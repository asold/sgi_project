{
  "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing",
  "url": "https://openalex.org/W3190572136",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3033325763",
      "name": "Kalyan, Katikapalli Subramanyam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287158305",
      "name": "Rajasekharan, Ajit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3212140250",
      "name": "Sangeetha Sivanesan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3017549762",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3213730158",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3156404059",
    "https://openalex.org/W2979314664",
    "https://openalex.org/W2963153906",
    "https://openalex.org/W3118580427",
    "https://openalex.org/W3116423158",
    "https://openalex.org/W3110048753",
    "https://openalex.org/W3116527904",
    "https://openalex.org/W3175898847",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3100985894",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W3008219293",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W3168772426",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W3173814445",
    "https://openalex.org/W2134797427",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W3096403953",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3153266325",
    "https://openalex.org/W3171087246",
    "https://openalex.org/W3101278968",
    "https://openalex.org/W3112689365",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3091355780",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3100652389",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3163521353",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W3176685217",
    "https://openalex.org/W3152609875",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W3115729981",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3186870698",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970636124",
    "https://openalex.org/W2963121782",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3093814160",
    "https://openalex.org/W3153642904",
    "https://openalex.org/W3012393889",
    "https://openalex.org/W3127861905",
    "https://openalex.org/W3172198372",
    "https://openalex.org/W3130716829",
    "https://openalex.org/W3170996386",
    "https://openalex.org/W3096565276",
    "https://openalex.org/W3204526376",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W3171649327",
    "https://openalex.org/W3093699284",
    "https://openalex.org/W2963997607",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3152879179",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W3029760648",
    "https://openalex.org/W3105601216",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W3128029819",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3116295307",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3038495045",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W3098300729",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3022112973",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3127389184",
    "https://openalex.org/W2107901333",
    "https://openalex.org/W3152788712",
    "https://openalex.org/W3010108619",
    "https://openalex.org/W3141023492",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3081210419",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W3102046836",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3158196282",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2979694518",
    "https://openalex.org/W3106051020",
    "https://openalex.org/W3164211145",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W2998554035",
    "https://openalex.org/W3120094169",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3136221257",
    "https://openalex.org/W3174418826",
    "https://openalex.org/W3182414670",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3152569586",
    "https://openalex.org/W3156665996",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3106339673",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W3092129462",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3133650345",
    "https://openalex.org/W3105867580",
    "https://openalex.org/W1670132599",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3026732421",
    "https://openalex.org/W2945767825",
    "https://openalex.org/W3170647102",
    "https://openalex.org/W3135158964",
    "https://openalex.org/W2806758205",
    "https://openalex.org/W3119989665",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W3099008231",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3153414861",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W3155904893",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W3086007799",
    "https://openalex.org/W3172021172",
    "https://openalex.org/W3035614045",
    "https://openalex.org/W2983772459",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W3021268743",
    "https://openalex.org/W3016187590",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3096266342",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W3013838212",
    "https://openalex.org/W2968289784",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3011718307",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W3002832564",
    "https://openalex.org/W3159134453",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W3116407756",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W3156371424",
    "https://openalex.org/W3162296828",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3118043957",
    "https://openalex.org/W2985509622",
    "https://openalex.org/W3095771422",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W3098469895",
    "https://openalex.org/W3110457105",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3024622987",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2963855739",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3032020872",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W3158835769",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W3164054899",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3040245432",
    "https://openalex.org/W3120253119",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3093553144",
    "https://openalex.org/W3097571385",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W3038035611",
    "https://openalex.org/W3122838366",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2963877297",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W3175870271",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3105601320",
    "https://openalex.org/W3173151551",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094446431",
    "https://openalex.org/W3120490999",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2606321545",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3037063616",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W3178814223",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W3153124855",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3134634493",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W3104136798",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2973071945",
    "https://openalex.org/W3000965575",
    "https://openalex.org/W3197040386",
    "https://openalex.org/W3164896303",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W3159795318",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W3081505754",
    "https://openalex.org/W3113311717",
    "https://openalex.org/W3109919947",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3105639882",
    "https://openalex.org/W3176936914",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3176617251",
    "https://openalex.org/W3048823912",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2966989210",
    "https://openalex.org/W3125833936",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3125826128",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3095642204",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2966892770",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3028836324"
  ],
  "abstract": "Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.",
  "full_text": "1\nAMMUS : A Survey of Transformer-based\nPretrained Models in Natural Language\nProcessing\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha\nAbstract—Transformer-based pretrained language models (T -PTLMs) have achieved great success in almost every NLP task. The\nevolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning\nand transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using\nself-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to\ndownstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a\nbrief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining\ntasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T -PTLMs and then give brief overview\nof various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T -PTLMs.\nFinally, we highlight some of the future research directions which will further improve these models. We strongly believe that this\ncomprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent\nhappenings in T -PTLMs. The list of T -PTLMs along with links is available athttps://mr-nlp.github.io/posts/2021/05/tptlms-list/\nIndex Terms—Self-Supervised Learning, Transformers, Pretrained Language Models, Survey.\n!\nCONTENTS\n1 Introduction 2\n2 Self-Supervised Learning (SSL) 3\n2.1 Why Self-Supervised Learning? . . . 3\n2.2 What is Self-Supervised Learning? . 3\n2.3 Types of Self-Supervised Learning . 4\n3 T-PTLM Core Concepts 4\n3.1 Pretraining . . . . . . . . . . . . . . . 4\n3.1.1 Pretraining Steps . . . . . . 4\n3.1.2 Pretraining Corpus . . . . 5\n3.2 Types of Pretraining Methods . . . . 6\n3.2.1 Pretraining from Scratch\n(PTS) . . . . . . . . . . . . . 6\n3.2.2 Continual Pretraining (CPT) 7\n3.2.3 Simultaneous Pretraining\n(SPT) . . . . . . . . . . . . . 8\n3.2.4 Task Adaptive Pretraining\n(TAPT) . . . . . . . . . . . . 8\n• K.S.Kalyan is with the Department of Computer Applications, National\nInstitute of Technology Trichy, Trichy, Tamil Nadu, India, 620015.\nE-mail: kalyan.ks@yahoo.com, Website: https://mr-nlp.github.io\n• Ajit Rajasekharan is with the Nference.ai as CTO, Cambridge, MA, USA,\n02142.\n• S.Sangeetha is with the Department of Computer Applications, National\nInstitute of Technology Trichy, Trichy, Tamil Nadu, India, 620015..\nPreprint under review - The paper is named (AMMUS - AMMU Smiles) in\nthe memory of one of the close friends of K.S.Kalyan (https://mr-nlp.github.\nio).\n3.2.5 Knowledge Inherited Pre-\ntraining (KIPT) . . . . . . . 9\n3.3 Pretraining Tasks . . . . . . . . . . . 9\n3.4 Embeddings . . . . . . . . . . . . . . 12\n3.4.1 Main Embeddings . . . . . 12\n3.4.2 Auxiliary Embeddings . . 13\n4 Taxonomy 14\n4.1 Pretraining Corpus-based . . . . . . 14\n4.1.1 General . . . . . . . . . . . 14\n4.1.2 Social Media-based . . . . 14\n4.1.3 Language-based . . . . . . 14\n4.1.4 Domain-Speciﬁc Models . 17\n4.2 Architecture . . . . . . . . . . . . . . 17\n4.2.1 Encoder-based . . . . . . . 17\n4.2.2 Decoder-based . . . . . . . 17\n4.2.3 Encoder-Decoder based . . 18\n4.3 SSL . . . . . . . . . . . . . . . . . . . . 19\n4.3.1 Generative SSL . . . . . . . 19\n4.3.2 Contrastive SSL . . . . . . 19\n4.3.3 Adversarial SSL . . . . . . 19\n4.3.4 Hybrid SSL . . . . . . . . . 20\n4.4 Extensions . . . . . . . . . . . . . . . 20\n4.4.1 Compact T-PTLMs . . . . . 20\n4.4.2 Character-based T-PTLMs 21\n4.4.3 Green T-PTLMs . . . . . . 21\n4.4.4 Sentence-based T-PTLMs . 22\n4.4.5 Tokenization-Free T-PLTMs 22\n4.4.6 Large Scale T-PTLMs . . . 23\n4.4.7 Knowledge Enriched T-\nPTLMs . . . . . . . . . . . . 23\narXiv:2108.05542v2  [cs.CL]  28 Aug 2021\n2\n4.4.8 Long-Sequence T-PTLMs . 23\n4.4.9 Efﬁcient T-PTLMs . . . . . 23\n5 Downstream Adaptation Methods 23\n5.1 Feature-based . . . . . . . . . . . . . . 24\n5.2 Fine-tuning . . . . . . . . . . . . . . . 24\n5.2.1 Vanilla Fine-Tuning . . . . 25\n5.2.2 Intermediate Fine-Tuning\n(IFT) . . . . . . . . . . . . . 25\n5.2.3 Multi-task Fine-Tuning\n(MTFT) . . . . . . . . . . . 25\n5.2.4 Parameter Efﬁcient Fine-\nTuning . . . . . . . . . . . . 26\n5.3 Prompt-based Tuning . . . . . . . . . 26\n6 Evaluation 27\n6.1 Intrinsic Evaluation . . . . . . . . . . 27\n6.2 Extrinsic Evaluation . . . . . . . . . . 28\n7 Useful Libraries 31\n8 Discussions and Future Directions 31\n8.1 Better Pretraining Methods . . . . . . 31\n8.2 Sample Efﬁcient Pretraining Tasks . 31\n8.3 Efﬁcient Models . . . . . . . . . . . . 31\n8.4 Better Position Encoding Mechanisms 31\n8.5 Improving existing T-PTLMs . . . . . 31\n8.6 Beyond Vanilla Fine-tuning . . . . . 33\n8.7 Benchmarks . . . . . . . . . . . . . . . 33\n8.8 Compact Models . . . . . . . . . . . . 33\n8.9 Robustness to Noise . . . . . . . . . . 33\n8.10 Novel Adaptation Methods . . . . . 33\n8.11 Privacy Issues . . . . . . . . . . . . . 33\n8.12 Mitigating Bias . . . . . . . . . . . . . 34\n8.13 Mitigating Fine-Tuning Instabilities . 34\n9 Conclusion 34\nReferences 34\n1 I NTRODUCTION\nT\nRANSFORMER -based pretrained language models\n(T-PTLMs) like GPT-1 [1], BERT [2], XLNet [3],\nRoBERTa [4], ELECTRA [5], T5 [6], ALBERT [7], BART\n[8] and PEGAUSUS [9] have achieved tremendous suc-\ncess in NLP because of their ability to learn universal\nlanguage representations from large volumes of unla-\nbeled text data and then transfer this knowledge to\ndownstream tasks. In the early days, NLP systems are\nmostly rule-based which are later replaced by machine-\nlearned models. Machine learning models require feature\nengineering which requires domain expertise and it is\na time-consuming process too. The evolution of better\ncomputer hardware like GPUs and word embeddings\nlike Word2Vec [10] and Glove [11] increased the use\nof deep learning models like CNN [12] and RNN [13],\n[14] for building NLP systems. The main drawback\nwith these deep learning models is the requirement of\ntraining the model from scratch except for the word\nembeddings. Training the model from scratch requires\na large number of labeled instances which are expensive\nto generate. However, we expect the model to perform\nwell using few labeled instances only. Transfer learning\n[15] allows the reuse of knowledge learned in source\ntasks to perform well in the target task. Here the target\ntask should be similar to the source task. Based on the\nidea of transfer learning, researchers in Computer Vision\ntrained large CNN models [16]–[19] using large scale\nlabeled datasets like ImageNet [20], [21]. These models\nlearn image representations which are common across all\nthe tasks. The large pretrained CNN models are adapted\nto downstream tasks by including few task-speciﬁc lay-\ners and then ﬁne-tuned on the target datasets [22]. As\nthe pretrained CNN models provide good background\nknowledge to the downstream models, they enjoyed\ntremendous success in many CV tasks [18], [23].\nDeep learning models like CNN and RNN have dif-\nﬁculties in modelling long term contexts and learn the\nword representations with locality bias [24]. Moreover,\nas RNNs process the input sequentially i.e., word by\nword, the utilization of parallel computer hardware is\nlimited. To overcome these drawbacks in existing deep\nlearning models, Vaswani et al. [25] proposed a deep\nlearning model called Transformers which is completely\nbased on self-attention. Self-attention allows for more\nparallelization compared to RNNs and can easily model\nlong term contexts as every token attend to all the tokens\nin the input sequence [25]. Transformers contains a stack\nof encoder and decoder layers. With the help of a stack\nof encoder and decoder layers, transformers can learn\ncomplex language information. It is a very expensive and\ntime-taking process to generate a large amount of labeled\ndata in the NLP domain. However, it is very easy to\nget large volumes of unlabeled text data. NLP research\ncommunity impressed with the success of CNN-based\npretrained models in Computer Vision, have developed\nT-PTLMs by combining the power of transformers and\nself-supervised learning. Self-supervised learning allows\nthe transformers to learn based on the pseudo supervi-\nsion provided by one or more pretraining tasks.\nGPT and BERT are the ﬁrst T-PTLMs developed based\non transformer decoder and encoder layers respectively.\nFollowing GPT and BERT, models like XLNet , RoBERTa,\nELECTRA, ALBERT, T5, BART and PEGAUSUS are pro-\nposed. Here XLNet, RoBERTa, ELECTRA and ALBERT\nare improvements over BERT model while T5, BART and\nPEGAUSUS are encoder-decoder based models. Kaplan\net al. [26] showed that the performance of T-PTLMs can\nbe increased just by increasing the size of the model. This\nobservation triggered the development of large-scale T-\nPTLMs like GPT-3 (175B) [27], PANGU- (200B) [28],\nGShard (600B) [29] which contain billions of parameters\nand Switch-Transformers (1.6T) [30] which contains tril-\nlions of parameters. Following the success of T-PTLMs in\ngeneral English domain, T-PTLMs are also developed for\nother domains like Finance [31], Legal [32], [33], News\n3\n[34], Programming [35]–[39], Dialogue [40], Networking\n[41], Academic [42]–[44] and Biomedical [45]–[48]. T-\nPTLMs support transfer-learning also as these models\ncan be adapted to downstream tasks by ﬁne-tuning or\nprompt-tuning on target datasets. In this survey paper,\nwe present a comprehensive review of recent research\nworks related to T-PTLMs. We summarize the highlights\nof our survey as\n• We present a brief overview of SSL, the backbone\nbehind developing T-PTLMs (Section 2).\n• We explain various core concepts related to T-\nPTLMs like pretraining, pretraining methods, pre-\ntraining tasks, embeddings and downstream adap-\ntation methods (Section 3).\n• We present a new taxonomy to categorize various\nT-PTLMs. This taxonomy is based on four perspec-\ntives namely pretraining corpus, architecture, type\nof SSL and extensions (Section 4).\n• We present a new taxonomy to categorize various\ndownstream adaptation methods and explain each\nin detail (Section 5).\n• We present a brief overview of various benchmarks\nincluding both intrinsic and extrinsic which evaluate\nthe progress of T-PTLMs (Section 6).\n• We present a brief overview of various li-\nbraries starting from Huggingface Transformers to\nTransformer-interpret which are useful to work T-\nPTLMs (Section 7).\n• We brieﬂy discuss some of the future research di-\nrections which will drive the research community\nto further improve the models (Section 8).\n2 S ELF -SUPERVISED LEARNING (SSL)\nSelf-supervised learning, a relatively new learning\nparadigm has gained attention in the Artiﬁcial Intelli-\ngence (AI) research community due to its ability to make\nuse of unlabeled data to inject universal knowledge\nabout language, image or speech into pretrained models.\nDue to its data efﬁciency and generalization ability, SSL\nﬁnds applications in various AI ﬁelds like Robotics [49],\nSpeech [50], [51], Natural Language Processing [24], [52]\nand Computer Vision [53], [54].\n2.1 Why Self-Supervised Learning?\nSupervised learning has played a crucial part in AI\nprogress by allowing the models to learn from human-\nannotated instances. Models trained using supervised\nlearning over labeled instances perform well on a spe-\nciﬁc task. However, a model trained using supervised\nlearning requires a large number of labeled instances to\nachieve good performance. Data collection and labelling\nis a time-taking and expensive process. Moreover, it\nis difﬁcult to obtain labeled data in speciﬁc domains\nlike Medical and Legal. Further, the model learns only\nwhat is available in the training data and suffers from\ngeneralization error and spurious correlations. Although\nsupervised learning is a dominant learning paradigm in\ndeveloping AI models in the last two decades, the bot-\ntlenecks in supervised learning have forced the research\ncommunity to look for alternative learning paradigms\nlike Self-Supervised Learning (SSL). SSL does not require\nhuman labeled data and helps the model to gain more\ngeneralization ability by learning from large amounts\nof unlabeled data. We summarize the drawbacks of\nsupervised learning as\n• heavy dependence on human labeled instances\nwhich are expensive and time-consuming to gen-\nerate.\n• lack of generalization ability and suffers from spu-\nrious correlations.\n• many domains like Medical and Legal are labeled\ndata starved which limits the application of AI\nmodels in these domains.\n• inability to learn from large amount of freely avail-\nable unlabeled data.\n2.2 What is Self-Supervised Learning?\nSelf-Supervised Learning (SSL) is a new learning\nparadigm which helps the model to learn univer-\nsal knowledge based on the pseudo supervision\nprovided by pretraining tasks. In SSL, the labels\nare automatically generated based on data attributes\nand the deﬁnition of pretraining task. Let X =\n(x1,p1),(x2,p2),(x3,p3),..., (xn,pn) represents pseudo\nlabeled instances. The pretraining loss ( LSSL ) of SSL\nlearning paradigm can be deﬁned as\nLSSL = λ1LPT −1 + λ2LPT −2 + ...+ λmLPT −m (1)\nHere LPT −1(), LPT −2(),. . . ,LPT −m represent the loss\nfunctions of ‘m’ pretraining tasks andλ1(), λ2(),. . . ,λm()\nrepresents weights. In general, pretraining using SSL\nparadigm can involve more than one pretraining task.\nFor example, RoBERTa is pretrained using only masked\nlanguage modelling (MLM) while BERT model is pre-\ntrained using two pretraining tasks namely masked\nlanguage modelling (MLM) and next sentence predic-\ntion (NSP). In case of MLM, the loss function used is\ncross entropy loss and in case of NSP , it is sigmoid\nloss. By solving the pretraining tasks over vast amount\nof unlabeled data, the model learns general language\nrepresentations which can encode both syntax and se-\nmantic information. These representations are useful\nin downstream tasks and helps the model to achieve\nmuch better performance using few labeled instances\nonly. We can say that pretraining over vast amount of\nunlabeled data using SSL helps the model to gain basic\ncommon sense or background knowledge without which\nthe model requires more labeled instances to achieve a\ngood performance.\nSSL has similarities with other popular learning\nparadigms like supervised and unsupervised learning.\nSSL is like unsupervised learning as it does not require\nhuman labeled instances. However, it is different from\n4\nunsupervised learning because a) SSL requires supervi-\nsion unlike unsupervised learning and b) the objective\nof unsupervised learning is to identify the hidden pat-\nterns while the objective of SSL is to learn meaningful\nrepresentations. SSL is like supervised learning as both\nthe learning paradigms require supervision. However,\nit is different from supervised learning because a) SSL\ngenerates labels automatically without any human in-\nvolvement and b) the goal of supervised learning is pro-\nvide task speciﬁc knowledge while SSL aims to provide\nthe model with universal knowledge. We summarize the\ngoals of SSL as\n• learn universal language representations which pro-\nvides a good background to the downstream model.\n• better generalization ability by learning over vast\namount of freely available unlabeled text data.\n2.3 Types of Self-Supervised Learning\nSelf-Supervised Learning can be classiﬁed into Genera-\ntive SSL, Contrastive SSL and Adversarial SSL . Gener-\native SSL allows the model to learn by decoding the en-\ncoded input. Generative SSL can use autoregressive, au-\ntoencoding or hybrid language models. Autoregressive\nlanguage model predicts the next tokens based on the\nprevious tokens. GPT-1 [1] is the ﬁrst PTLM that is based\non the autoregressive language model. Autoencoding\nlanguage model predicts the masked tokens based on\nthe unmasked tokens (bidirectional context). For exam-\nple, masked language modelling (MLM) involves two\nsteps. The ﬁrst step is to encode the masked tokens\nusing bidirectional context and the second step is to\ndecode (predict) the original tokens based on the en-\ncoded masked token representations. Models like BERT\n[2], RoBERTa [4] and ALBERT [7] are pretrained using\nMLM. Hybrid language models combine the advantages\nof autoregressive and autoencoding language models.\nFor example, permutation language modelling (PLM) in\nXLNet [3] is an example of a hybrid language model.\nContrastive SSL allows the model to learn by com-\nparing. Next sentence prediction (NSP) in BERT and\nsentence order prediction in ALBERT are examples of\ncontrastive SSL. NSP involves identifying whether the\ngiven sentence pair includes consecutive sentences or\nnot, while SOP involves identifying whether the given\npair includes swapped sentences or not. Adversarial SSL\nallows the model to learn by identifying whether the\ntokens in the input sentence are replaced or shufﬂed or\nrandomly substituted. Replaced token detection (RTD) in\nELECTRA [5], shufﬂed token detection (STD) [55] and\nrandom token substitution (RTS) [56] are examples of\nAdversarial SSL. For detailed information about SSL and\ntypes, please refer to the survey paper on SSL [49].\n3 T-PTLM C ORE CONCEPTS\n3.1 Pretraining\nPretraining on large volumes of unlabeled text and then\nﬁne-tuning on small task-speciﬁc datasets has become a\nstandard approach in modern natural language process-\ning. In Computer Vision, large models [16]–[19] based\non CNN are pretrained on large, labeled datasets like\nImageNet [20], [21], and then these models are used in\nsimilar target tasks by adding few task-speciﬁc layers\n[22]. Here pretraining allows the model to learn com-\nmon image features which are useful in many tasks.\nInspired by the success of pretrained image models, NLP\nresearchers developed models like BERT [2], RoBERTa\n[4], ELECTRA [5], XLNet [3], and T5 [6] by pretraining\nthem on large volumes of unlabelled text using self-\nsupervised learning. Some of the beneﬁts of pretraining\nare\n• It helps the model to learn universal language repre-\nsentations by leveraging large volumes of unlabeled\ntext.\n• Pretrained models can be adapted to downstream\ntasks by just adding one or two speciﬁc layers.\nHence it avoids training the downstream model (ex-\ncept task-speciﬁc layers) from scratch by providing\na good initialization.\n• It helps the model to perform better even with small\ndatasets and hence reduces the requirement of a\nlarge number of labeled instances.\n• Deep learning models due to having a large number\nof parameters tend to overﬁt on small datasets. As\npretraining provides a good initialization, it avoids\noverﬁtting on small datasets, and hence pretraining\ncan be viewed as a form of regularization [57].\n3.1.1 Pretraining Steps\nPretraining a model involves the following ﬁve steps\n1. Prepare the pretraining corpus – Pretraining corpus is\nobtained from one or more sources of unlabelled text and\nthen cleaned. BERT [2] model is pretrained on English\nWikipedia and BooksCorpus. Further research [3], [4], [6]\nshowed that pretraining the model on a much larger text\ncorpus obtained from multiple sources further improves\nthe performance of the model. Moreover, Lee et al.\n[58] showed there is a lot of redundancy in pretraining\ncorpus in the form of near-duplicate sentences and long\nrepetitive substrings. Further, Lee et al. [58] showed\npretraining the model on deduplicated corpus requires\nfewer training steps to achieve similar performance.\n2. Generate the vocabulary – Most of the transformer-\nbased pretrained language models use tokenizers like\nWordPiece [59], Byte Pair Encoding (BPE) [60], Byte\nLevel BPE (bBPE) [61], and SentencePiece [62] to gen-\nerate the vocabulary. Usually, vocabulary consists of all\nthe unique characters and commonly used subwords\nand words. Vocabulary is generated by applying any\nof the tokenizers on the pretraining corpus. Different T-\nPTLMs use different tokenizers and generate vocabulary\nwith different sizes. For example, BERT uses WordPiece\nvocabulary of size around 30K, RoBERTa uses bBPE\nvocabulary of size around 50K, XLM [63] uses BPE vo-\ncabulary of size 95K, mBERT [2] WordPiece vocabulary\n5\nFig. 1: Pretraining corpus\nof size 110K, XLM-R [64], and mBART [65] uses Senten-\ncePiece vocabulary of size 250K. The large vocabulary\nsize in multilingual models like XLM, XLM-R, mBERT,\nand mBART make sense as they have to represent\nmultiple languages. However, the size of the pretrained\nmodel increases with an increase in vocabulary size. This\nstep is optional in the case of char-based T-PTLM like\nCharacterBERT [66] and tokenization-free T-PTLMs like\nCANINE [67], ByT5 [68], and Charformer [69].\n3. Design the pretraining tasks - During pretraining, the\nmodel learns language representations by minimizing\nlosses based on one or more pretraining tasks. A pre-\ntraining task should\n• be challenging enough to allow the model to learn seman-\ntics at word, phrase, sentence, or document level . For\nexample, recent research works [4], [7] questioned\nthe efﬁciency of NSP task and resulted in new\npre-training tasks to learn semantics at sentence\nlevel like sentence order prediction [7] and sentence\nstructure prediction [70].\n• provide more training signal so that the model learns\nmore language information with less pretraining corpus .\nFor example, RTD provides more training signal\ncompared to MLM because RTD is deﬁned over all\nthe input tokens while MLM is deﬁned over a subset\nof tokens only [5].\n• close to downstream tasks . For example, span bound-\nary pretraining task in SpanBERT [71] is close to\nthe span extraction task and the gap sentence gen-\neration in PEGAUSUS [9] is close to the summariza-\ntion task. Recent research works resulted in better\nversions of MLM like Swapped Language Modeling\n[56] which avoids the use of special mask tokens and\nhence reduces the discrepancy between pretraining\nand ﬁne-tuning.\n4. Choose the pretraining method – Training a new model\nfrom scratch using SSL only is highly expensive and\nconsumes a lot of pretraining time. Instead of training\nfrom scratch using SSL only, pretraining methods like\nKIPT [72], [73] which pretrain a model using both SSL\nand KD can be used. In the case of adapting general\nmodels to speciﬁc domains, pretraining methods like\ncontinual pretraining with new vocabulary [74]–[77] or\nadapt and distill [78] can be used. To pretrain a domain-\nspeciﬁc model with limited domain-speciﬁc corpus, si-\nmultaneous pretraining which leverages both general\nand in-domain corpus can be used [79].\n5. Choose the pretraining dynamics – BERT model is\npretrained on sentence pairs with static masking in small\nbatch sizes. Liu et al. [4] showed that carefully designed\npretraining choices like dynamic masking, large batch\nsizes, more pretraining steps, and long input sequences\nfurther enhance the performance of the model. More-\nover, when using large batch sizes which may cause dif-\nﬁculty in optimization, it is recommended to a) linearly\nincrease the learning rate in the early pretraining steps\nand b) use different learning rates in different layers\nwhich can also help to speed up convergence [80].\n3.1.2 Pretraining Corpus\nSelf-Supervised learning to pretrain T-PTLMs requires\nlarge volumes of pretraining data. As shown in Fig-\nure, pretraining corpus can be classiﬁed into four types\n(refer Figure 1). The characteristic of the text differs\nfrom one type of corpus to another. For example, in\nthe general domain, the text is less noisy and written\nformally by professionals. In social media, the text is\nmostly noisy and written colloquially by the general\npublic. Moreover, many speciﬁc domains like Biomed-\nical and Finance contain many domain-speciﬁc words\nwhich are not used in the general domain. In general,\nthe performance of general domain models in domain-\nspeciﬁc tasks is limited [45]. So, we have to choose the\npretraining corpus depending on the target domain to\n6\nFig. 2: Pretraining methods\nPretraining Corpus Type Pretraining\nCorpus\nDescription Models\nGeneral - OpenWebText Open source equivalent to the WebText corpus used\nto pretrain GPT-2 model and it is around 32GB.\nRoBERTa [4]\n- C4 [6] 750GB collection of common crawl text which is\ndeduplicated and ﬁltered to include natural text\nonly.\nT5 [6]\nSocial Media - Rale-E [81] Collection of hateful comments in English which are\nposted on Reddit, a popular social media platform.\nHateBERT [81]\n- Amazon\nreviews [82]\nCollection of 233M reviews posted by users about\nvarious products. The gathered reviews cover\naround 29 domains.\nBERT-SentiX [83]\nDomain-Speciﬁc\nBiomedical MIMIC-III [84] Consists of de-identiﬁed ICU patient records gath-\nered over more than a decade. It is the largest\npublicly available corpus of clinical records.\nBioBERT [45], BlueBERT\n[48], ClinicalBERT [46]\nNews RealNews [85] Common crawl new corpus. It is around 120GB. Roberta-base-news [34]\nAcademic S2ORC [86] Large scale collection of more than 80M research\npapers written in English.\nRoberta-base-biomed [34],\nRoberta-base-cs [34]\nTABLE 1. Summary of various pretraining corpora in general, social media and speciﬁc domains.\nachieve good results. BERT model is pretrained using\ntext from Wikipedia and BookCorpus which amounts\nto 16GB [2]. Further research works showed that the\nperformance of the model can be increased by using\nlarge pretraining datasets [3], [4]. This triggered the\ndevelopment of much larger datasets, especially from the\ncommon crawl. For example, C4 data contains around\n750GB of text data [6] while CC-100 corpus includes\naround 2.5TB of text data [64]. Multilingual T-PTLMs\nlike mBERT [2], IndT5 [87], IndoBART [88], and XLM-\nR [64] are pretrained using only multilingual datasets.\nSome of the models like XLM [63], XLM-E [89], infoXLM\n[90], and mT6 [91] are pretrained using both multilingual\nand parallel datasets. A summary of various pretraining\ncorpora is given in Tables 1 and 2.\nFig. 3: Pretraining from Scratch (PTS)\n3.2 Types of Pretraining Methods\nFigure 2 shows the classiﬁcation of pretraining methods\ninto ﬁve types.\n3.2.1 Pretraining from Scratch (PTS)\nModels like BERT, RoBERTa , ELECTRA, and T5 are\npretrained from scratch on large volumes of unlabeled\ntext (refer Figure 3). Usually, any transformer-based pre-\ntrained language model consists of an embedding layer,\n7\nPretraining Corpus Type Pretraining\nCorpus\nDescription Models\nLanguage-based Monolingual\n(Indonesian)\nIndo4B [92] Collection of 23GB of Indonesian text gathered from\nvarious public resources including social media\nplatforms. It includes around 3.58 billion words.\nIndonesianBERT [92]\nMonolingual\n(Portuguese)\nBrWaC [93] 17.5GB collection of Brazilian Portuguese web text\ngathered from 3.5 million web pages. It includes\naround 2.7B words.\nPortugueseBERT [75], PTT5\n[77]\nMonolingual\n(Chinese)\nCLUECorpus2020\n[94]\nCollection of 100GB of Chinese common crawl text. RoBERTa-tiny-clue [94]\nMonolingual\n(Chinese)\nWuDaCorpora\n[95]\n200GB collection of Chinese Web text. It includes\naround 72B Chinese characters.\nChinese-Transformer-XL\n[95]\nMultilingual\nIndo4Bplus [88] Includes text from Indo4B corpus for Indonesian\nand from Wikipedia, CC-100 for Sundanese and\nJavanese language.\nIndoBART [88]\nOSCAR [96] Large scale collection of common crawl text for\naround 166 languages.\nMuRIL [97]\nIndicCorp [98] Collection of text from various sources for 12 major\nIndian languages. It includes around 8.9B words.\nIndicBERT [98]\nmC4 [99] Multi-lingual equivalent of C4. It includes common\ncrawl text for 101 languages.\nmT5 [99]\nCC-Net [100] Large scale collection of common crawl text for\nmore than 100 languages.\nmT6 [91]\nCC-100 [64] Large scale collection (2.5TB) of common crawl text\nof 100 languages.\nXLM-R [64], infoXLM [90],\nXLM-E [89]\nIndCorpus [87] Collection of text from Wikipedia and Bible for\n11 languages including Indigenous languages. It\nis around 1.17GB and includes around 5.37M sen-\ntences.\nIndT5 [87]\nParallel\nPMINDIA [101] Collection of parallel data gathered from Prime\nMinister of India website. The corpus includes 56K\nsentences for each language pair.\nMuRIL [97]\nIIT Bombay\n[102]\nCollection of 1.49M English-Hindi parallel sen-\ntences.\nmT6 [91], XLM [63], in-\nfoXLM [90], Unicoder [103],\nALM [104], XLM-E [89]\nMulti-UN [105] Parallel corpus created from UN ofﬁcial documents\nfor six languages.\nmT6 [91], XLM [63],\ninfoXLM [90]], Unicoder\n[103], ALM [104], XNLG\n[106], XLM-E [89]\nWiki-Matrix\n[107]\nParallel data for 85 languages. It includes 135M\nparallel sentences in 1620 language pairs out of\nwhich 34M sentences are aligned with English.\nmT6 [91], XLM-E [89]\nCC-Aligned\n[108]\nParallel corpus of 292 million non-English common\ncrawl document pairs and 100 million English com-\nmon crawl document pairs.\nXLM-E [89]\nDakshina [109] Parallel corpus containing 10K sentences for 12 In-\ndian languages. Each sentences consists of sentence\nin native script sentence and its manually roman-\nized transliteration.\nMuRIL [97]\nSamanantar\n[110]\nIncludes 49.6M sentences pairs of 12 Indian lan-\nguages aligned with English. It is the largest pub-\nlicly available parallel corpus for Indian languages.\n-\nTABLE 2. Summary of various language-based pretraining corpora.\ntransformer encoder, or (and) transformer decoder lay-\ners. All these layer parameters are randomly initialized\nand then learned during pretraining by minimizing the\nlosses of one or more pretraining tasks. For example,\nBERT model is pretrained from scratch using MLM\nand NSP . Pretraining from scratch is computationally\nexpensive and requires a large number of GPUs or TPUs.\n3.2.2 Continual Pretraining (CPT)\nModels like BioBERT [45], ALeaseBERT [32], TOD-BERT\n[40], HateBERT [81], infoXLM [90], and XNLG [106] are\nobtained by initializing from existing pretrained models\nand then further pretrained. For example, infoXLM is ini-\ntialized from XLM-R [64] and further pretrained on both\nmonolingual and parallel data, ALeaseBERT is initialized\nfrom general ALBERT and further pretrained on lease\n8\nFig. 4: Continual Pretraining (CPT)\nagreements, XLM [63] parameters are used to initialize\nboth encoder and decoder layers in XNLG. Unlike in\nPTS, in continual pretraining the model parameters are\nnot learned from scratch. Instead, the parameters are\ninitialized with existing language model parameters and\nthen adapted to the target domain by further pretraining\n(refer Figure 4). Continual pretraining is commonly used\nto develop T-PTLMs in speciﬁc domains like social me-\ndia [81], [111], [112], biomedical [45], Legal [32], News\n[34], Computer Networking [41] etc. The main advantage\nof continual pretraining is that it avoids training a model\nfrom scratch and makes use of the existing language\nmodel parameters. As CPT starts from existing model\nparameters, it is less expensive and requires less training\ntime and computational resources compared to PFS.\nHowever, the lack of target domain-speciﬁc vocab-\nulary is a drawback in CPT when the target domain\nconsists of many domain-speciﬁc words. For example,\nBioBERT [45] is initialized from general BERT and fur-\nther pretrained on biomedical text. Though the language\nmodel is adapted to the biomedical domain, the vocab-\nulary which is learned over general domain text does\nnot include many of the domain-speciﬁc words. As a\nresult, domain-speciﬁc words are split into a number of\nsub-words which hinders model learning and degrades\nits performance in downstream tasks. Similarly, mBERT\naccommodates more than 100 languages, the number of\ntokens in its vocabulary (110K) speciﬁc to a language is\nless. A possible solution for this is continual pretraining\nwith target domain or language-speciﬁc vocabulary [75]–\n[77]. Here, new vocabulary is generated over the target\ndomain or language text. During continual pretraining,\nthe embedding layer is randomly initialized and all other\nlayer parameters are initialized with existing language\nmodel parameters. For example, models like RuBERT\n[74], PortugueseBERT [75], SlavicBERT [76] are initial-\nized from mBERT but further pretrained with language-\nspeciﬁc vocabulary. Similarly, PPT5 [77] is initialized\nfrom the T5 model but further pretrained with language-\nspeciﬁc vocabulary. However, the performance of the\nmodel obtained by continual pretraining with new vo-\ncabulary is slightly less but on par with the performance\nof the model trained from scratch. As CPT is computa-\ntionally less expensive, CPT with new vocabulary can\nbe preferred over PTS in resource-constrained situations.\nRecently, Yao et al. [78] proposed Adapt and distill\napproach to adapt general models to a speciﬁc domain\nusing vocabulary expansion and knowledge distillation.\nDifferent from existing adaptation methods, Adapt and\ndistill approach not only adapt general models to speciﬁc\ndomain but also reduces the size of the model.\nIt is not necessary to use the same set of pretraining\ntasks used by the existing model for continual pretrain-\ning. For example, BERT-SentiX [83] model is initialized\nfrom BERT and further pretrained on product reviews\nusing four sentiment-aware pretraining tasks. Similarly,\nTOD-BERT [40] is initialized from BERT and further\npretrained on dialogue corpus text using MLM and\nresponse contrastive loss (RCL).\n3.2.3 Simultaneous Pretraining (SPT)\nDomain-speciﬁc T-PTLMs can be developed by training\nfrom scratch or by continual pretraining. Both these\npretraining methods require large volumes of domain-\nspeciﬁc unlabelled text to pretrain the model. However,\nthe availability of domain-speciﬁc text is limited in many\ndomains. Moreover, domain-speciﬁc text in languages\nother than English is available in small quantities only.\nFor example, in the biomedical domain, MIMIC-III [84]\nis the largest publicly available (English) medical records\ndataset. However, it is difﬁcult to obtain such large\nvolumes of medical records in languages like Japanese\n[79]. PTS or CPT using a small amount of domain-\nspeciﬁc text overﬁts the model. Simultaneous pretraining\n(SPT) allows the model to pretrain from scratch using a\ncorpus having both general and domain-speciﬁc text [79]\n(refer Figure 5). Here, up sampling of domain-speciﬁc\ntext is done to ensure a good number of domain-speciﬁc\nterms in model vocabulary and also to have a balanced\npretraining. Wada et al. [79] showed that Japanese clin-\nical BERT pretrained using SPT outperforms Japanese\nclinical BERT trained from scratch.\nFig. 5: Simultaneous Pretraining (SPT)\n3.2.4 Task Adaptive Pretraining (TAPT)\nPretraining approaches like PTS, CPT and SPT allow the\nmodel to learn universal or domain-speciﬁc language\nrepresentations by training on large volumes of general\nor domain-speciﬁc or combined text. As all these ap-\nproaches involve training over a large amount of text,\nthese approaches are expensive. Task Adaptive Pretrain-\ning (TAPT) allows the model to learn ﬁne-grained task-\nspeciﬁc knowledge along with domain-speciﬁc knowl-\nedge by pretraining on a small amount of task-speciﬁc\nunlabelled text [34] (refer Figure 6). As TAPT requires\nonly a small amount of text, it is less expensive compared\nto other pretraining methods. Additional task-related\nsentences can be obtained from large domain corpus\nusing lightweight approaches like VAMPIRE [113] which\nembeddings all the sentences using a simple bag-of-\nwords language model. Gururangan et al. [34] showed\n9\nthat TAPT is complementary to other pretraining ap-\nproaches i.e., PTS / CPT followed by TAPT further\nimproves the performance of the model.\nFig. 6: Task Adaptive Pretraining (TAPT)\n3.2.5 Knowledge Inherited Pretraining (KIPT)\nAll the previously discussed pretraining methods like\nPTS, CPT, SPT, and TAPT solely depend on self-\nsupervised learning to pretrain the models. It is highly\nexpensive and time-consuming to pretrain a large model\nfrom scratch using SSL only. In general, humans learn\nnot only learn through self-learning but also learn from\nother knowledgeable people. Inspired from this, Qin\net al. [72] proposed Knowledge Inherited Pretraining\n(KIPT), a novel pretraining method which pretrains the\nmodel using both self-supervised learning and knowl-\nedge distillation (refer Figure 7) . KIPT allows reusing\nthe knowledge available in existing pretrained models\nto pretrain a new model.\nFig. 7: Knowledge Inherited Pretraining (KIPT)\nLKIPT = σ∗LSSL + (1−σ) ∗LKD (2)\nwhere LKIPT represents the overall loss of KIPT, LSSL\nand LKD represents losses of self-supervised learning\nand knowledge distillation. KIPT is similar to KD in\nreusing the knowledge from existing models. However,\nit is different from KD in two aspects, (a) in KD,\ngenerally the student model is compact in size com-\npared to teacher model whereas in KIPT the student\nmodel is larger in size compared to teacher model (b)\nin KD, the student model solely learns from teacher\nmodel where as in KIPT the student model encodes\nthe knowledge available in pretraining corpus using\nself-supervised learning in addition to the knowledge\nfrom teacher model. By learning from a knowledgeable\nteacher model along with self-supervised learning, the\nmodel learns more as well as converges faster which\nmakes KIPT more effective and less expensive compared\nto the pretraining methods which involves only self-\nsupervised learning. Due to the additional knowledge\ngained from a knowledgeable teacher model, the models\ntrained using KIPT outperforms models trained using\nself-supervised learning only [72]. Further, Qin et al. [72]\nshowed that KIPT supports both life-long learning and\nknowledge transfer. CPM-2 [73] is the ﬁrst large scale\npretrained language model pretrained using knowledge\ninheritance.\n3.3 Pretraining Tasks\nSSL allows T-PTLMs to learn universal language rep-\nresentations by solving one or more predeﬁned tasks.\nThese tasks are referred to as “pretext” or “pretraining”\ntasks. Pretraining tasks are self-supervised i.e., these\ntasks make use of pseudo labeled data. The data at-\ntributes and pretraining task deﬁnition determine the\npseudo labels. A pretraining task should be challenging\nenough so that it provides more training signals to the\nmodel. For example, tasks like MLM involves only 15%\nof tokens in each training sample for learning while tasks\nlike Replaced Token Detection (RTD) [5], Random Token\nSubstitution (RTS) [56], and Shufﬂed Token Detection\n(STD) [55] involves all the tokens in the input sample for\nmodel learning. Moreover, a pretraining task should be\nsimilar to the downstream task. For example, pretraining\ntasks like Seq2SeqLM [114] or Denoising Auto Encoder\n(DAE) [8] are similar to downstream tasks like text\nsummarization, machine translation, etc.\nCasual Language Modeling (CLM) - CLM or simply\nUnidirectional LM predicts the next word based on the\ncontext. The unidirectional LM can handle the sequence\nfrom left-to-right or right-to-left. In let-to-right LM, the\ncontext includes all the words on the left side while in\nright-to-left LM, the context includes all the words on the\nright side. GPT-1 [1] is the ﬁrst transformer-based PTLM\nto use CLM (left-to-right) as a pretraining task. UniLM\n[115] uses both left-to-right and right-to-left CLM as\npretraining tasks. Let x= {x1,x2,x3,...,x |x|}represents\na sequence where |x|represents the number of tokens in\nthe sequence. CLM loss is deﬁned as\nL(x)\nCLM = −1\n|x|\n|x|∑\ni=1\nlogP(xi/x<i) (3)\nwhere x<i = x1,x2,x3,..xi−1.\nMasked Language Modeling (MLM) – The main\ndrawback in CLM is the inability to leverage both\ncontexts. Bidirectional contextual information is much\nbetter compared to unidirectional context information\nfor encoding token representations. It is not possible\nto train standard CLM using bidirectional context as\nit would allow a token to see itself which makes the\nprediction trivial. MLM is an improved version of CLM\nto leverage tokens from both contexts. In MLM, we feed\nthe masked token vectors to the softmax layer to get the\nprobability distribution over the vocabulary and then\nuse the cross-entropy loss. BERT is the ﬁrst model to\nuse MLM as pretraining task [2]. The authors of BERT\nmasked the tokens at a probability of 0.15. Let x\\Mx\nrepresents the masked version of x and Mx represents\nthe set of masked token positions in x. MLM loss is\n10\ndeﬁned as\nL(x)\nMLM = − 1\n|Mx|\n∑\ni∈Mx\nlogP(xi/x\\Mx ) (4)\nReplaced Token Detection (RTD) - MLM is better\nthan CLM by leveraging bidirectional contextual infor-\nmation. However, MLM has two drawbacks a) provides\nless training signal – in MLM, the model learns from\nonly 15% of the tokens and b) model see special mask\ntoken only during pretraining which results in a dis-\ncrepancy between pretraining and ﬁne-tuning stages.\nRTD overcomes these two issues by a novel approach\nthat involves identifying the replaced tokens [5]. MLM\ncorrupts the sentence by using special mask tokens\nwhile RTD corrupts the sentence using the output tokens\nfrom the generator model trained using MLM objective.\nMLM involves predicting the original tokens based on\nmasked token vectors while RTD is a token-level binary\nclassiﬁcation task that involves classifying every token\nas replaced or not. ELECTRA model is pretrained in two\nsteps. 1)train the generator model using MLM objective\nand 2)train the discriminator model initialized from a\ngenerator using RTD objective. Let ˆx is the corrupted\nversion of x. RTD loss is deﬁned as\nL(x)\nRTD = −1\n|ˆx|\n|ˆx|∑\ni=1\nlogP(d/ˆxi) (5)\nwhere d∈{0,1}represents whether the token is replaced\nor not.\nShufﬂed Token Detection (STD) – STD is a token-\nlevel discriminative task that involves identifying the\nshufﬂed tokens. Similar to RTD, it is sample efﬁcient and\navoids discrepancy between pretraining and ﬁne-tuning\nstages. In STD, the words are shufﬂed at a probability\nof 0.15 (this is based on the masking probability used in\nBERT and RoBERTa models). Panda et al. [55] showed\nthat continual pretraining RoBERTa using STD improves\nits performance in many of the GLUE tasks and estab-\nlished that STD allows the model to learn more coherent\nsentence representations. Let ˆx is the corrupted version\nof x. RTD loss is deﬁned a\nL(x)\nSTD = −1\n|ˆx|\n|ˆx|∑\ni=1\nlogP(d/ˆxi) (6)\nwhere d∈{0,1}represents whether the token is shufﬂed\nor not.\nRandom Token Substitution (RTS) – RTD is sample\nefﬁcient but requires a separate generator to corrupt the\ninput sequence. Training a separate generator model is\ncomputationally expensive. To overcome this drawback,\nDi et al. [56] proposed RTS which involves identifying\nthe randomly substituted tokens. In RTS, 15% of the\ntokens are randomly substituted with other tokens from\nthe vocabulary. RTS is sample efﬁcient like RTD but\ndoes not require any separate generator model to corrupt\nthe input sequence. Di et al.[56] showed that RoBERTa\nmodel trained using RTS matches the performance of\nRoBERTa model trained using MLM while requiring less\ntraining time. RTS loss is deﬁned as\nL(x)\nRTS = −1\n|ˆx|\n|ˆx|∑\ni=1\nlogP(d/ˆxi) (7)\nwhere d ∈ {0,1}represents whether the token is ran-\ndomly substituted or not and ˆxis obtained by randomly\nsubstituting 15% of tokens in x.\nSwapped Language Modeling (SLM) – The MLM\npretraining task uses a special mask token to corrupt the\ninput sequence. However, the use of this special token\nresults in a discrepancy between pretraining and ﬁne-\ntuning stages. SLM overcomes this drawback by corrupt-\ning the sequence with random tokens from vocabulary\nat a probability of 0.15 [56]. SLM is similar to MLM by\npredicting the corrupting tokens but unlike MLM, SLM\nreplaces the tokens with random tokens. SLM is similar\nto RTS in using the random tokens for corruption but\nunlike RTS which is sample efﬁcient by involving every\ntoken in the input sequence, SLM is not sample efﬁcient\nas it involves only 15% of input tokens. SLM loss is\ndeﬁned as\nL(x)\nSLM = − 1\n|Rx|\n∑\ni∈Rx\nlogP(xi/x\\Rx ) (8)\nWhere Rx represents set of positions of randomly substi-\ntuted tokens and x\\Rx represents the corrupted version\nof x.\nTranslation Language Modeling (TLM) – TLM is an\nextension of MLM to use parallel data in cross-lingual\npretraining. XLM [63] is the ﬁrst cross-lingual model to\nuse TLM as a pretraining task followed by XNLG [106].\nTLM is also referred to as cross-lingual MLM (XMLM).\nHere the input is a pair of sentences ( x,y) where x\nand y are parallel sentences i.e., x is a translation of\ny. Similar to MLM, tokens from both sentences are ran-\ndomly masked. As prediction of masked tokens involves\ncontext from both the sentences, TLM helps the model\nto learn cross-lingual mapping. TLM loss is similar to\nMLM and is deﬁned as\nL(x,y)\nMLM = − 1\n|Mx|\n∑\ni∈Mx\nlogP(xi/x\\Mx ,y\\My )\n− 1\n|My|\n∑\ni∈My\nlogP(yi/x\\Mx ,y\\My ) (9)\nWhere Mx and My represents the set of masked positions\nin the sentences x and y respectively, x\\My and y\\My\nrepresent the masked version of x and y respectively.\nAlternate Language Modeling (ALM) : ALM is a\npretraining task to train cross-lingual language models.\nALM involves predicting the masked tokens in the code-\nswitched sentences generated from parallel sentences\n[104]. For a given parallel sentence pair ( x,y), a code-\nswitched sentence is generated by randomly substituting\nsome phrases of x with their translations from y. ALM\n11\nfollows the same settings of standard MLM for masking\nthe tokens. By pretraining the model on code-switched\nsentences, the model learns relationships between lan-\nguages in a much better way. Yang et al. [104] showed\nthat cross-lingual model pretrained using ALM outper-\nforms XLM which shows that ALM is a better alternative\nto TLM for pretraining cross-lingual language models.\nALM loss is deﬁned as\nL(z(x,y))\nALM = − 1\n|M|\n∑\ni∈M\nlogP(zi/Z\\M ) (10)\nWhere z is the code-switched sentence generated from\nxand y, z\\M represents the masked version of z and M\nrepresents the set of masked token positions in z\\M .\nSentence Boundary Objective (SBO) – SBO pretrain-\ning task involves predicting the masked tokens based on\nthe span boundary tokens and position embeddings [71].\nSBO is similar to MLM in predicting the masked tokens.\nHowever, it is different from MLM in three aspects (a)\nSBO masks only contiguous span of tokens while MLM\nmasks tokens randomly and (b) in SBO, the prediction\nof masked tokens involves span boundary tokens and\nposition embeddings while in MLM, the prediction of\nmasked tokens involves only the masked token vec-\ntors. Moreover, SBO is much challenging compared to\nstandard MLM as it is difﬁcult to predict the entire\nspan “an American football game” compared to pre-\ndicting “game” when “an American football” is already\nknown [71]. SBO helps the model to perform better\nin downstream tasks like entity extraction, coreference\nresolution, and question answering which involves span-\nbased extraction. SBO loss is deﬁned as\nL(x))\nSBO = −1\n|S|\n∑\ni∈S\nlogP(xi/yi) (11)\nwhere yi = f(xs−1,xe+1,ps−e+1) and f() is a two-layered\nfeedforward neural network, S represents the positions\nof tokens in contiguous span, |S|represents the length\nof span, sand erepresent the start and end positions of\nspan, p represents the position embedding.\nNext Sentence Prediction (NSP) – NSP is a sentence-\nlevel pretraining task that helps the model to learn\nrelationships between sentences [2]. It is a binary sen-\ntence pair classiﬁcation task that involves identifying\nconsecutive sentences. Here the aggregate representation\nof the two sentences (x,y) i.e., [CLS] token vector is\ngiven to the sigmoid layer to get the probability. For\ntraining, the sentence pairs are generated in a way that\n50% of instances are consecutive and the rest are not\nconsecutive. Pretraining the model at the sentence level\nis useful in downstream tasks like question answering,\nNLI, and STS which involve sentence pair input. NSP\nloss is deﬁned as\nL(x,y)\nNSP = −logP(d/x,y) (12)\nWhere d ∈{1,0}represents whether the sentences are\nconsecutive or not.\nSentence Order Prediction (SOP) – NSP allows the\nmodel to learn sentence-level semantics and involves\nboth topic and coherence prediction. As topic prediction\nis easier, the effectiveness of NSP is questioned [4], [7].\nSOP is a sentence-level pretraining task based on sen-\ntence coherence only. ALBERT [7] is the ﬁrst pretraining\nmodel to use SOP as a pretraining task. It involves\nidentifying whether the given sentences are swapped or\nnot. Following NSP , the training instances are generated\nin a way that 50% of instances are swapped and the rest\nor not. SOP loss is deﬁned as\nL(x,y)\nSOP = −logP(d/x,y) (13)\nWhere d ∈{1,0}represents whether the sentences are\nswapped or not.\nSequence-to-Sequence LM (Seq2SeqLM) - MLM is\napproached as a token level classiﬁcation task over the\nmasked tokens i.e., original words are predicted by\nfeeding the masked token vectors to a softmax layer\nover the vocabulary. Seq2SeqLM is an extension of stan-\ndard MLM to pretrain encoder-decoder-based models\nlike T5 [6], mT5 [99] and MASS [114]. In the case of\nMLM, the context includes all the tokens in the input\nsequence whereas in Seq2SeqLM, the context includes\nall the words in the input masked sequence and the\nleft side words in the predicted target sequence. With\nmasked sequence as input to the encoder, the decoder\npredicts the masked words from left to right sequentially.\nSeq2SeqLM loss is deﬁned as\nL(x)\nSeq2SeqLM = −1\nls\nj∑\ns=i\nlogP(xs/ˆx,xi:s−1) (14)\nwhere ˆx is the masked version of x i.e., x with masked\nn-gram span, ls represents the length of masked n-gram\nspan.\nDenoising Auto Encoder (DAE) : DAE helps to model\nto learn by reconstructing the original text from cor-\nrupted text [8]. The text can be corrupted at token (e.g.,\ntoken deletion and token masking), phrase (e.g., token\ninﬁlling), sentence (e.g., sentence permutation), or doc-\nument level (e.g., document rotation). Like Seq2SeqLM,\nDAE is useful to train encoder-decoder-based models.\nHowever, DAE is more sample efﬁcient by providing\nmore training signals for model learning. DAE provides\nmore training signal as it involves reconstructing entire\noriginal text while Seq2SeqLM involves reconstructing\nthe masked tokens only. BART [8] uses a bidirectional en-\ncoder to encode corrupted input sequence and a left-to-\nright decoder to recreate the original text. The authors of\nBART experimented with various corruption strategies\nand ﬁnally trained the model on the sentences corrupted\nusing sentence permutation and text inﬁlling. DAE loss\nis deﬁned as\nL(x)\nDAE = −1\n|x|\n|x|∑\ni=1\nlogP(xi/ˆx,x<i) (15)\nwhere ˆx is the corrupted version of x.\n12\nFig. 8: Embeddings in T-PTLMs\n3.4 Embeddings\nDeep learning models expect the input in the form of\na matrix of numbers and then apply a sequence of\nmatrix operations. As deep learning models including\ntransformers expect numerical input, input text should\nbe mapped to a sequence of dense, low dimensional vec-\ntors (commonly called embeddings in natural language\nprocessing). In transformer-based pretrained language\nmodels, character or sub-word embeddings are preferred\nover word embeddings. This is because a) small vo-\ncabulary size in character and sub-word embeddings\ncompared to word embeddings. The vocabulary of word\nembeddings consists of all the unique words (or all\nthe words above the cut-off frequency) in the pretrain-\ning corpus, whereas vocabulary in character embedding\nmodels consists of all the characters and vocabulary in\nsub-word embedding models consists of all the char-\nacters, frequently occurring sub-words and words. The\nsize of vocabulary also determines the overall size of\npretrained language model [116]. b) can represent any\nword and hence overcome the problem of OOV words\nwhich is a serious problem with word embeddings c)\ncan encode ﬁne-grained information at character or sub-\nword levels in word representation. Apart from repre-\nsenting input data using embeddings, it is also necessary\nadditional information like position, language, etc. We\nclassify embeddings into main and auxiliary depending\non whether they represent the input data or provide\nadditional information to the model (refer Figure 8).\n3.4.1 Main Embeddings\n- Main embeddings represent the input data in dense\nlow dimensional vectors. In TPLMs, the input data\nis mostly a sequence of words. However, in domain-\nspeciﬁc models like BERT-EHR [117], MedBERT [118],\nand BEHRT [119] the input is a sequence of medical\ncodes. All these three models are pretrained on medical\ntext from electronic health records (EHRs).\nText Embeddings: Input for most of the TPLMs is the\nsequence of words. Input text can be represented using\ncharacter, sub-word, or combination of character and\nsub-word embeddings. Models like CharacterBERT [66],\nAlphaBERT [120] use character embeddings, and models\nlike CharBERT [121] use both character and sub-word\nembeddings. Models like BERT, RoBERTa, XLNet, T5,\nand BART use sub-word embeddings but the tokenizer\nused to generate the vocabulary is different in these\nmodels.\nCharacter Embeddings : Character embeddings map\neach character to a dense low dimensional vector. The\nvocabulary of character embeddings includes all the\ncharacters like letters, symbols, punctuations, and num-\nbers. Once the vocabulary is ﬁnalized, the embedding of\neach character in the vocabulary is randomly initialized\nand then learned during model pretraining. TPLMs use\ncharacter embeddings in two ways. The ﬁrst one is,\ncharacter level word representation is generated from\ncharacter embeddings and then a sequence of trans-\nformer layers are applied to encode contextual infor-\nmation [66]. For example in CharacterBERT [66], ﬁne-\ngrained word representation is generated from character\nembeddings using a character encoder based on Char-\nCNN and highway layer [122]. The second one is based\non context string embedding [123]. Here, there is no no-\ntion of the explicit word, and input text is modeled as a\nsequence of characters. In AlphaBERT [120], transformer\nlayers are directly on character embeddings whereas\nin CharBERT [121] transformer layers are applied after\napplying BiGRU on character embeddings. Here BiGRU\nprocesses the input at the character level and generates\n13\ncontextualized character embeddings [124].\nSub-Word Embeddings : Unlike character embed-\ndings, the vocabulary of sub-word embeddings con-\nsists of characters, frequently occurring sub-words, and\nwords. Here the vocabulary can be generated using any\nof the tokenizers like WordPiece [59], Byte Pair Encoding\n(BPE) [60], Byte Level BPE (bBPE) [61], Unigram [125],\nand SentencePiece [62]. Except Unigram, tokenizers like\nWordPiece, BPE, bBPE generate vocabulary by starting\nwith base vocabulary having only the characters and\niteratively augment the vocabulary until the predeﬁned\nsize is reached. BPE chooses the new symbol pair to be\nincluded in the vocabulary based on frequently while\nWordPiece does it based on language model probabil-\nity. bBPE is the same as BPE except that it represents\neach character as a byte. Unigram starts with a large\nvocabulary and then arrives at a vocabulary of prede-\nﬁned size by iteratively cutting the characters which is\nexactly opposite to what happens in BPE and WordPiece.\nTokenizers like WordPiece and BPE assume space as a\nword separator in the input text which is not true in all\ncases. To overcome this, SentencePiece tokenizer treats\nspace as a character and then generates the vocabulary\nusing BPE or Unigram.\nThe size of the vocabulary must be chosen carefully.\nToo small vocabulary size results in longer input se-\nquences as more words will be split into many sub-\nwords which hinders model learning and increases pre-\ntraining time. Too large vocabulary represents more\nwords using a single token but increases the size overall\nof the model [126]. However, in the case of multilingual\nmodels like mBERT, XLM, and XLM-R, it is necessary\nto have a large vocabulary to accommodate more lan-\nguages. Once the vocabulary is generated, each token in\nthe vocabulary is assigned with a randomly initialized\nembedding and then learned during model pretraining.\nHybrid Embeddings: To leverage the beneﬁts in both\ncharacter and sub-word embeddings, models like Char-\nBERT [121] uses both character and sub-word embed-\ndings. The model uses dual-channel CNN-based interac-\ntion module to model the interaction between character\nand sub-word embeddings.\nCode Embeddings: In the medical domain, each con-\ncept is represented using a standard code from ontology.\nHere the concept can be a disease, drug, symptom, etc.\nAll the information during patient visits to a hospital\nis represented using medical codes in EHRs. Pretrained\nlanguage models in the biomedical domain like BERT-\nEHR [117], MedBERT [118], and BEHRT [119] expect\na sequence of medical codes as input. So, in these\nmodels vocabulary consists of medical codes from stan-\ndard clinical ontologies. Embeddings for these medical\ncodes are randomly initialized and learned during model\npretraining.\n3.4.2 Auxiliary Embeddings\nAuxiliary embeddings provide additional information to\nthe model. Each auxiliary embeddings have its purpose.\nFor example, positional embeddings represent the posi-\ntion, while segment embeddings distinguish tokens from\ndifferent sentences in the input sentence pair, language\nembeddings in multilingual pretrained models like XLM\n[63] and Unicoder [103] provide information about the\nlanguage of the input sentence. Among auxiliary embed-\ndings, position and segment embeddings are commonly\nused while the other embeddings are used in speciﬁc\npretrained language models. For example, age, gender,\nand semantic group embeddings are used in biomedical\npretrained language models only [117], [119], [127].\nPosition Embeddings : In traditional deep learning\nmodels like CNN and RNN, it is not necessary to\nprovide any auxiliary embeddings along with text em-\nbeddings to represent the position of input tokens. This\nis because these models implicitly learn the order of\ninput tokens. For example, RNN process input sequence\ncharacter by character or word by word, and hence\nit automatically learns the order. In CNN, convolution\noperations are performed at a ﬁxed size window level\ninstead of character or word level and hence it also\nlearns the order automatically. As transformers do not\nhave convolution or recurrent layers to learn order,\nposition embeddings are provided. Position embeddings\ncan be absolute [2], [4], [5], [117] or relative [3]. In\nmodels like BERT, RoBERTa, ELECTRA absolute position\nembeddings are learned along with other parameters of\nthe model. However, in models like BERT-EHR [117] and\nBEHRT [119], absolute position embeddings are prede-\ntermined to handle an imbalance in patient sequence\nlength.\nSegment Embeddings : In the case of sentence pair\ntasks, the model takes both the input sentences at the\nsame time. So, it is necessary to distinguish tokens of two\ninput sentences using segment embeddings. Position\nembedding is different for different tokens in the input\nsentence, but segment embedding is the same for all the\ntokens in each input sentence.\nLanguage Embeddings : Language embeddings are\nused in cross-lingual pretrained language models like\nXLM [63], Unicoder [103], and XNLG [106]. For exam-\nple, models like XLM are pretrained using a) MLM on\nmonolingual text data in 100 languages and b) TLM\nusing parallel data. Language embeddings are used to\nexplicitly inform the model about the language of the\ninput sentence. In MLM which involves sentences in one\nlanguage, language embedding will be the same for all\nthe tokens in the input sentence. In the case of TLM\nwhich involves a pair of sentences from two different\nlanguages, language embedding will be the same for all\nthe tokens in a sentence but different from the language\nembedding assigned to tokens in other input sentence.\nHowever, language embeddings are not used in XLM-R\n[64] model to allow the model to better deal with code-\nswitching.\nEntity Type Embeddings : OAG-BERT [42] is a pre-\ntrained academic language model like SciBERT [43]. It\nis pretrained on academic text corpus. Unlike SciBERT\n14\nwhich is just pretrained on academic text, OAG-BERT\nis pretrained on academic text as well as augmented\nwith information about various entities in academic text\nlike paper, published venue, author afﬁliation, research\ndomain, and authors. Information about various entities\nis provided to the model during pretraining via entity\ntype embeddings.\nAge and Gender Embeddings : In medical pretrained\nmodels like BEHRT [119] and BERT-EHR [117], the input\nis a sequence of patient visits where each patient visit\nis represented as a sequence of medical codes. Apart\nfrom model codes, it is useful to provide additional\ninformation like age and gender. For example, each\npatient visits happen at different times. Providing age\ninformation to the model allows it to leverage temporal\ninformation. Age and gender information is provided to\nthese models explicitly via age and gender embeddings.\nSemantic Group Embeddings : UmlsBERT [127] is\na knowledge enriched medical language model. It is\nobtained by continual pretraining ClinicalBERT [46] on\nUMLS data using novel multi-label MLM pretraining\ntask. UMLS is a collection of over 100 medical ontologies.\nIn UMLS, each medical concept is assigned a unique\ncalled Concept Unique Identiﬁer, semantic type, syn-\nonyms, related concepts, etc. During continual pretrain-\ning, semantic type information is provided explicitly via\nsemantic group embedding so that the language model\ncan a) learn better representation for rare words and b)\nbetter model the association between words of the same\nsemantic type.\n4 T AXONOMY\nTo understand and keep track of the development of\nvarious T-PTLMs, as shown in Figure 9, we classify T-\nPTLMs from four different perspectives namely Pretrain-\ning Corpus (Section 4.1), Model Architecture (Section\n4.2), Type of SSL (Section 4.3), and Extensions (Section\n4.4).\n4.1 Pretraining Corpus-based\n4.1.1 General\nModels like GPT-1 [1], BERT [2], UnilM [115], XLNet\n[3], RoBERTa [4], ELECTRA [5], T5 [6], and BART [8]\nare pretrained on general corpus. For example, GPT-1\nis pretrained on Books corpus while BERT and UniLM\nare pretrained on English Wikipedia and Books corpus.\nAs the amount of text data available in Book corpus\nor English Wikipedia, text is gathered from multiple\nsources for pretraining models like XLNet, RoBERTa,\nELECTRA, BART and T5.\n4.1.2 Social Media-based\nT-PTLMs like BERT and RoBERTa are pretrained on\nformal text. As social media text is highly informal\nin nature with a lot of noise in the form of irregular\ngrammar, slang words, and non-standard abbreviations,\nthese models have limited performance on social media\ndatasets [81], [111], [129], [130] . Researchers working at\nthe intersection of social media and NLP have developed\nsocial media-speciﬁc T-PTLMs either by training from\nscratch [129] or continual pretraining [34], [81], [83],\n[111], [112], [130] and a summary of these models is\npresented in Table 3. Except for Bertweet [129], all other\nsocial media-based T-PTLMs are developed by contin-\nual pretraining. Training from scratch is effective only\nwhen the pretraining corpus consists of a large number\nof tweets. Otherwise, continual pretraining is recom-\nmended. For example, BERTweet [129] is pretrained from\nscratch using 850M tweets. Barbieri et al. [111] showed\nthat RoBERTa model trained from scratch on tweets\nachieved less performance compared to RoBERTa model\nadapted to social media by continual pretraining. This\nis because of using just 60M tweets for pretraining.\nDifferent from other social media-based T-PTLMs which\nare developed using commonly used pretraining tasks\nlike MLM and NSP , BERT-SentiX [83] is obtained by\ncontinual pretraining on user reviews using four novel\nsentiment aware pretraining tasks.\n4.1.3 Language-based\nLanguage-based T-PTLMs can be monolingual or multi-\nlingual. Monolingual T-PTLMs are pretrained on speciﬁc\nlanguage corpus while multi-lingual T-PTLMs are pre-\ntrained multiple language corpus.\nMulti-lingual T-PTLMs Inspired by the tremendous\nsuccess of BERT in English, the authors of BERT devel-\noped mBERT by pretraining BERT model from scratch\nusing Wikipedia text from 104 languages [2]. mBERT is\nthe ﬁrst multilingual T-PTLM. Following mBERT, many\nmultilingual T-PTLMs are proposed. Recent research\nworks showed that the performance of the model can be\nimproved by training on large volumes of text. So, XLM-\nR [64] is pretrained on CC-100 which consists of a large\namount of text particularly for low-resource languages\ncompared to Wikipedia. Inspired from “Scaling laws for\nneural language models” [26] much larger models like\nXLM-RXL [131] and XLM-RXXL [131] are pretrained\non CC-100 and achieved much better results. mBERT,\nXLM-R, and its variants are pretrained on only non-\nparallel data. However, pretraining the model on parallel\ndata along with non-parallel data allows the model to\nlearn cross-lingual representations in a much better way.\nModels like MuRIL [97], mT6 [91], InfoXLM [90], XLM\n[63] and Unicoder [103] are pretrained on both parallel\nand non-parallel data. Multi-lingual NLP research com-\nmunity also developed many generative T-PTLMs like\nmT5 [99], mT6 [91], mBART [65] and IndoBART [88]\nbased on encoder-decoder architecture. A summary of\nvarious multi-lingual T-PTLMs is presented in Table 4.\nMonolingual T-PTLMs Multilingual models are pre-\ntrained on the corpus from multiple languages and\nhence they can be used for NLP tasks in more than one\nlanguage. However, the following drawbacks force the\nNLP community to develop separate models for each\n15\nFig. 9: Taxonomy of T-PTLMs\n16\nName Pretrained\nfrom\nPretraining tasks Corpus Evaluation\nHateBERT [81] BERT MLM RAL-E (dataset of 1.5M hateful Reddit\ncomments)\nOffensive tweets classiﬁca-\ntion\nRoBERTa-Twitter\n[111]\nRoBERTa MLM Tweets (60M) Tweet classiﬁcation\nRoBERTa-Reviews\n[34]\nRoBERTa MLM Amazon reviews (24.75M) [128] Review classiﬁcation\nBERT-SentiX [83] BERT SWP , WP , EP and\nRP\nAmazon (233M) [82] and Yelp reviews\n(8M) Reviews\nCross domain sentiment\nanalysis\nXLM-R-Twitter\n[112]\nXLM-R MLM Tweets in multiple languages (198M) TweetEval [111] and\nUMSAB [112]\nBertweet [129] Scratch MLM Tweets (845M English + 5M COVID\ntweets)\nPOS, NER and Tweets clas-\nsiﬁcation\nBertweetCovid19\n[129]\nBertweet MLM COVID tweets (23M) Tweet classiﬁcation\nCT-BERT [130] BERT MLM, NSP COVID tweets (160M) Tweet classiﬁcation\nTABLE 3. Summary of social-media based T-PTLMs.\nName Architecture Pretraining tasks Corpus Vocabulary #Lang #Parameters\nMuRIL [97] Encoder MLM + TLM Wikipedia,\nCommon Crawl\n+ Parallel data\nWordPiece\n(197K)\n17 236M\nIndicBERT [98] Encoder MLM IndicCorp SentencePiece\n(200K)\n12 33M\nmT5 [99] Encoder-\nDecoder\nSeq2SeqLM mC4 SentencePiece\n(250K)\n101 300M, 580M, 1.2B,\n3.7B and 13B (base,\nlarge, xl and xxl)\nmT6 [91] Encoder-\nDecoder\nSeq2SeqLM, MT, TPSC,\nTSC\nCCNet + Parallel\ndata\nSentencePiece\n(250K)\n94 300M\nInfoXLM [90] Encoder MLM, TLM , XLCo CC-100 + Parallel\ndata\nSentencePiece\n(250K)\n94 270M and 559M\n(base and large)\nmBERT [2] Encoder MLM , NSP Wikipedia WordPiece\n(110K)\n104 172M\nmBART [65] Encoder-\nDecoder\nDAE CC-25 SentencePiece\n(250K)\n25 680M\nXLM-15 [63] Encoder MLM, TLM Wikipedia + Paral-\nlel data\nBPE (95K) 15 250M\nXLM-17 [63] Encoder MLM Wikipedia BPE (200K) 17 570M\nXLM-100 [63] Encoder MLM Wikipedia BPE (200K) 100 570M\nXLM-R [64] Encoder MLM CC-100 SentencePiece\n(250K)\n100 270M ,560M (base\nand large)\nXLM-RXL [131] Encoder MLM CC-100 SentencePiece\n(250K)\n100 3.5B\nXLM-RXXL\n[131]\nEncoder MLM CC-100 SentencePiece\n(250K)\n100 10.7B\nUnicoder [103] Encoder MLM, TLM, CLWR,\nCLPC, CLMLM\nWikipedia + Paral-\nlel data\nBPE (95K) 15 250M\nIndoBART [88] Encoder-\nDecoder\nDAE Indo4B-plus BPE (40K) 3 130M\nTABLE 4. Summary of multi-lingual T-PTLMs.\nlanguage starting from BanglaBERT [132] to ParsBERT\n[133].\n• Curse of Multilinguality [64]: Multilingual models\ncannot represent all the languages equally. This is\nbecause of the underrepresentation of low-resource\nlanguages in the pretraining corpus and the lim-\nited capacity of the model. Moreover, adding more\nlanguages after a certain limit reduces the model\nperformance.\n• Embedding barrier [132]: The performance of multi-\n17\nlingual models in high resource languages that have\nadequate representation in model vocabulary is on\npar with their monolingual models [134]. However,\nin the case of languages without adequate repre-\nsentation in model vocabulary, the difference in the\nperformance of the monolingual and multilingual\nmodel is signiﬁcant. Due to the high imbalance in\nthe pretraining corpus, the representation of low-\nresource languages in multilingual model vocab-\nulary is very limited which is referred to as the\nembedding barrier [90]. For example, the represen-\ntation of the Arabic language [135] in popular mul-\ntilingual models is 5K out of 110K in mBERT and\n14K out of 250K in XLM. This issue is more severe\nin the case of languages like Bangla that does not\nshare vocabulary or script with any high-resource\nlanguages. The percentage of Bangla vocabulary in\nthe multilingual model is less than 1% [132]. With\nvery limited representation in the vocabulary, words\nin low resource languages are tokenized into many\nsubwords which increases input sequence length,\nhinders model learning, and makes training expen-\nsive.\nA summary of the various monolingual model is\npresented in Tables 5 and 6. Monolingual models are pre-\ntrained based on standard model architectures like GPT\n[142] BERT [74], [75], [92], [133], [135], [136], [139]–[141],\n[144]–[146], [151]–[154], RoBERTa [137], [138], [147]–\n[149], [152], [155], ALBERT [92], [150], ELECTRA [132],\n[143], and T5 [77]. As the availability of corpus from one\nsource is limited in the case of many languages, most\nof these models are pretrained using corpus gathered\nfrom multiple sources. For example, IndoBERT [136]\nis pretrained on corpus having text from Wikipedia,\nNews domain, and Internet. Except for models like PTT5\n[77], RuBERT [74] and PortugueseBERT [75], all other\nmonolingual models are pretrained from scratch. Models\nlike PTT5, RuBERT, and PortugueseBERT are initialized\nfrom existing models and further pretrained with new\nlanguage-speciﬁc vocabulary. In these models, only the\ntransformer encoder layer parameters are copied from\nexisting models while embedding layer parameters are\nrandomly initialized. During CPT, embedding layer pa-\nrameters are updated along with other layers.\n4.1.4 Domain-Speciﬁc Models\nFollowing the success of T-PTLMs in general domain,\nT-PTLMs in speciﬁc domains like Finance [31], Legal\n[32], [33], News [34], Programming [35]–[39], Dialogue\n[40], Networking [41], Academic [42]–[44] and Biomed-\nical [45]–[48] have been developed (refer Table 7 for\na brief summary). Models like BERT, RoBERTa, BART,\nand T5 are pretrained on general domain text. For a\nmodel to perform well on domain-speciﬁc datasets the\nmodel should have enough domain knowledge [31],\n[45]. These general domain models can not acquire\nenough domain knowledge just through ﬁne-tuning. As\na result, the performance of these models on domain-\nspeciﬁc datasets is limited [31], [45]. The initial trend\nto develop domain-speciﬁc models is using continual\npretraining i.e., initialize the model with any of the\nexisting general domain models and further pretrain on\na domain-speciﬁc corpus. For example, BioBERT [45] is\nthe ﬁrst domain-speciﬁc BERT model developed using\ncontinual pretraining. Following BioBERT in biomed-\nical domain, models like AleaseBERT [32], RoBERTa-\nNews [34], GraphCodeBERT [38], CoText [39], CodeGPT-\nadapted [35], NetBERT [41], MathBERT [44], TOD-BERT\n[40], ClinicalBERT [46] and BluBERT [48] have been\ndeveloped using continual pretraining.\nThe main advantage of developing domain-speciﬁc\nmodels using continual pretraining is that the model\nconverges faster as it is not trained from scratch and\nhence it is comparatively less expensive. However, as\nthese models use the same vocabulary learned over gen-\neral domain text, many of the domain-speciﬁc words are\nmissing in the vocabulary. For example, the vocabularies\nof FinBERT and general BERT have 41% of common to-\nkens [31]. As a lot of domain-speciﬁc words are missing\nin the vocabulary, many of the domain-speciﬁc words are\nnot represented properly which hinders model learning.\nThe advantage of having domain-speciﬁc vocabulary is\nthat even if the word is missing in the vocabulary, the\nword will be split into meaningful tokens. For example,\nthe word actyeltransferase is split into [“ace”, “ty”, “lt”,\n“ran”, “sf”, “eras”, “e’] by BERT model whereas the\nsame word is split into meaning tokens [“acetyl”, “trans-\nferase”] by SciBERT which has domain-speciﬁc vocabu-\nlary [47]. Models like PubMedBERT [47], FinBERT [31],\nLegalBERT [33], CodeBERT [36], PLBART [37], CodeGPT\n[35], SciBERT [43] and OAG-BERT [42] are pretrained\nfrom scratch.\n4.2 Architecture\nTransformers, a novel self-attention model deep learning\nproposed by Vaswani et al. [25] consists of stack of both\nencoder and decoder layers. A T-PTLM can be pretrained\nusing a stack of encoders or decoders or both.\n4.2.1 Encoder-based\nIn general, an encoder-based T-PTLM consists of an\nembedding layer followed by a stack of encoder layers.\nFor example, the BERT-base model consists of 12 encoder\nlayers while the BERT-large model consists of 24 encoder\nlayers [2]. The output from the last encoder layer is\ntreated as the ﬁnal contextual representation of the input\nsequence. In general, encoder-based models like BERT\n[2], XLNet [3], RoBERTa [4], ELECTRA [5], ALBERT [7]\nand XLM-E [89] are used in NLU tasks.\n4.2.2 Decoder-based\nA decoder-based T-PTLM consists of an embedding layer\nfollowed by a stack of decoder layers. Here transformer\n18\nName Language Pretrained\nfrom\nPretraining\ntasks\nCorpus Vocabulary\nBanglaBERT\n[132]\nBangla Scratch RTD Bangla Web text corpus WordPiece (32k)\nIndoBERT [136] Indonesian Scratch MLM Indonesian Wikipedia, News and Web\ncorpus\nWordPiece (32k)\nIndonesianBERT\n[92]\nIndonesian Scratch MLM Indo4B Sentencepiece (30k)\nIndonesianBERT-\nLite [92]\nIndonesian Scratch MLM and SOP Indo4B Sentencepiece (30k)\nPhoBERT [137] Vietnamese Scratch MLM Vietnamese Wikipedia and News cor-\npus\nBPE (64K)\nRobBERT [138] Dutch Scratch MLM OSCAR corpus bBPE (40K)\nRomanianBERT\n[139]\nRomanian Scratch MLM,NSP OPUS, OSCAR and Wikipedia corpus BPE (50k)\nFlauBERT [140] French Scratch MLM French text corpus BPE (50K)\nAraBERT [141] Arabic Scratch MLM and NSP Arabic Wikipedia and News corpus SentencePiece (64K)\nAraGPT2 [142] Arabic Scratch CLM Arabic Wikipedia, OSCAR and News\ncorpus\nbBPE (64K)\nAraELECTRA\n[143]\nArabic Scratch RTD Arabic Wikipedia, OSCAR and News\ncorpus\nSentencePiece (64K)\nBERTje [144] Dutch Scratch MLM, SOP Dutch Books, Wikipedia and News cor-\npus\nSentencePiece (30K)\nFinnishBERT\n[145]\nFinnish Scratch MLM, NSP Finnish News, Online discussion and\nCommon Crawl Corpus\nSentencePiece (50K)\nALBERTO [146] Italian Scratch MLM,NSP Italian tweets corpus SentencePiece\n(128K)\nPortugueseBERT\n[75]\nPortuguese mBERT MLM,NSP BrWaC [93] SentencePiece (30K)\nRuBERT [74] Russian mBERT MLM,NSP Russian Wikipedia and News corpus SentencePiece\nBETO [147] Spanish Scratch MLM Wikipedia and Common Crawl corpus SentencePiece (32K)\nCamemBERT\n[148]\nFrench Scratch MLM French OSCAR corpus SentencePiece(32K)\nARBERT [135] Arabic Scratch MLM, NSP Arabic Wikipedia, Books, Common\nCrawl, News corpus\nWordPiece (100K)\nMARBERT\n[135]\nArabic Scratch MLM Arabic tweets corpus WordPiece (100K)\nWangchanBERTa\n[149]\nThai Scratch MLM Thai Wikipedia , Social media posts,\nBooks and reviews corpus\nSentencePiece (25K)\nKoreALBERT\n[150]\nKorean Scratch MLM, SOP and\nWOP\nKorean Wikipedia, News, Internet crawl\nand Books corpus\nSentencePiece (32K)\nTABLE 5. Summary of monolingual T-PTLMs.\ndecoder layer consists of only masked multi-head atten-\ntion and feed-forward network layers. The multi-head\nattention module which performs encoder-decoder cross\nattention is removed. In general, decoder-based models\nlike GPT-1 [1], GPT-2 [61] and GPT-3 [27] are used in\nNLG tasks.\n4.2.3 Encoder-Decoder based\nEncoder-decoder based T-PTLMs are more suitable\nfor sequence-to-sequence modeling tasks like Machine\n19\nName Language Pretrained\nfrom\nPretraining\ntasks\nCorpus Vocabulary\nPTT5 [77] Portuguese T5 Seq2SeqLM BrWac corpus SentencePiece (32K)\nSweedishBERT\n[153]\nSweedish Scratch MLM ,NSP Sweedish text corpus (Wikipedia, News,\nSocial media and Legal)\nSentencePiece (50K)\nSweedishALBERT\n[153]\nSweedish Scratch MLM, SOP Sweedish text corpus (Wikipedia, News,\nSocial media and Legal)\nSentencePiece (50K)\nSweedishELECTRA\n[153]\nSweedish Scratch RTD Sweedish text corpus (Wikipedia, News,\nSocial media and Legal)\nSentencePiece (50K)\nHerBERT [151] Polish Scratch MLM Polish text corpus (Wiki, OSCAR, Sub-\ntitles and Books)\nBPE (50K)\nPolishBERT [154] Polish Scratch MLM Polish text corpus (Common Crawl,\nWikipedia, Books and OPUS)\nSentencePiece (50K)\nRobeCzech [155] Czech Scratch MLM Wikipedia, Web and News corpus bBPE (52K)\nKLUE-BERT [152] Korean Scratch MLM,NSP Korean text corpus BPE(32K)\nKLUE-RoBERTa\n[152]\nKorean Scratch MLM Diverse Korean text corpus BPE(32K)\nParsBERT [133] Persian Scratch MLM,NSP Persian text corpus WordPiece (100K)\nTABLE 6. Summary of monolingual T-PTLMs.\nTranslation, Text Summarization, etc. MASS [114] is\nthe ﬁrst encoder-decoder based T-PTLM model. It is\npretrained using Seq2SeqLM, an extension of MLM to\nencoder-decoder architectures. Following MASS, a num-\nber of encoder-decoder models like T5 [6], mT5 [99], mT6\n[91], BART [8], mBART [65], PLBART [37], PEGAUSUS\n[9] and PALM [156] are proposed in the recent times.\nFor example, Models like MASS and BART use bidirec-\ntional encoder over corrupted text and lef-to-right auto\nregressive decoder to reconstruct the original text.\n4.3 SSL\nSSL is one of the key ingredients in building T-PTLMs.\nA T-PTLM can be developed by pretraining using Gen-\nerative, Contrastive or Adversarial, or Hybrid SSL.\n4.3.1 Generative SSL\nGenerative SSL helps the model to learn by predicting\ntokens. The different scenarios in generative SSL are a)\npredicting the next token based on current tokens (CLM)\nb) prediction the masked tokens (MLM and its variants\nlike TLM, Seq2SeqLM) c) reconstructing the original text\nfrom the corrupted text (DAE). Some of the popular\nmodels developed using Generative SSL are GPT-1 [1],\nGPT-2 [61], GPT-3 [27] (based on CLM), RoBERTa [4],\nXLM [63], XLM-R [64] (based on MLM and its variants\nlike TLM), BART [8], mBART [65] (based on DAE), and\nMASS [114], T5 [6], mT5 [99] (based on Seq2SeqLM).\n4.3.2 Contrastive SSL\nContrastive SSL helps the model to learn by comparison.\nIn NLP , there is no T-PTLM which is pretrained using\nContrastive SSL only. Contrastive SSL is used in contin-\nual pretraining to further improve the model i.e., to learn\nsentence-level semantics. For example, CERT [157] uses\ncontrastive SSL to improve the BERT model by injecting\nmore sentence-level semantics. CERT outperforms BERT\nin many GLUE tasks. Similarly, Mirror-BERT [158] and\nSimCSE [159] use contrastive SSL to allow the BERT\nmodel to generate quality sentence embeddings. Lin et\nal. [160] showed that multi-lingual contrastive pretrain-\ning improves the performance of multilingual T-PTLMs\nin Mickey Probe.\n4.3.3 Adversarial SSL\nAdversarial SSL helps the model to learn by distin-\nguishing corrupted tokens. Here the corrupted tokens\ncan be replaced or shufﬂed. Adversarial SSL can be\nused in training the model from scratch or in contin-\nual pretraining. Models like ELECTRA [5] and XLM-E\n[89] are pretrained using adversarial SSL. ELECTRA is\npretrained using replaced token detection (RTD) while\nXLM-E is pretrained using multi-lingual replaced token\ndetection(MRTD) and translation replaced token detec-\ntion (TRTD). Panda et al. [55] used adversarial SSL based\non shufﬂed token detection (STD) to further improve\nRoBERTa model.\n20\nName Domain Pretrained\nfrom\nPretraining tasks Corpus Vocabulary\nFinBERT [31] Finance Scratch MLM + NSP Financial Communication\nCorpus\nWordPiece (31K)\nALeaseBERT [32] Legal ALBERT MLM Lease Agreements Same as ALBERT\nLegalBERT [33] Legal Scratch MLM + NSP English Legal Text Sentencepiece (31K)\nRoBERTa-News [34] News RoBERTa MLM Real News corpus Same as RoBERTa\nCodeBERT [36] Programming Scratch MLM+RTD CodeSearchNet WordPiece\nPLBART [37] Programming Scratch DAE Github and Stackoverﬂow\ncorpus\nSentencepiece (50K)\nGraphCodeBERT\n[38]\nProgramming CodeBERT MLM, EP and NA CodeSearchNet Same as CodeBERT\nCoText [39] Programming T5 Seq2SeqLM CodeSearchNet and Github\ncode\nSame as T5\nCodeGPT [35] Programming Scratch CLM CodeSearchNet BPE (50K)\nCodeGPT-adapted\n[35]\nProgramming GPT-2 CLM CodeSearchNet Same as GPT-2\nNetBERT [41] Networking BERT MLM Computer Networking Cor-\npus\nSame as BERT\nSciBERT [43] Academic Scratch MLM and NSP Semantic Scholar WordPiece (30K)\nOAG-BERT [42] Academic Scratch MLM OAG text corpus WordPiece (44K)\nMathBERT [44] Academic BERT MLM,CCP and MSP Arxiv papers Same as BERT\nTOD-BERT [40] Dialogue BERT MLM and RCL Dialogue Corpus Same as BERT\nBioBERT [45] Biomedical BERT MLM+NSP PubMed and PMC Same as BERT\nClinicalBERT [46] Biomedical BERT MLM+NSP MIMIC-III Same as BERT\nBlueBERT [48] Biomedical BERT MLM+NSP PubMed and MIMIC-III Same as BERT\nPubMedBERT [47] Biomedical Scratch MLM+NSP PubMed and PMC WordPiece\nTABLE 7. Summary of domain-speciﬁc T-PTLMs.\n4.3.4 Hybrid SSL\nSome of the T-PTLMs are pretrained using more than\none type of SSL. For example, BERT model - generative\n(MLM) and contrastive SSL (NSP), ALBERT- generative\n(MLM) and contrastive SSL (SOP), infoXLM – gener-\native (MLM, TLM) and contrastive SSL (XLCo). Here\nXLCo represents the cross lingual contrastive pretraining\ntask. Models like CLINE [161] are obtained by further\npretraining RoBERTa model using generative (MLM),\ncontrastive, and adversarial SSL (RTD).\n4.4 Extensions\n4.4.1 Compact T -PTLMs\nPTLMs have achieved huge success in almost every NLP\ntask. Recently researchers observed that the performance\nof PTLMs can be increased just by increasing the size of\nthe model and training with large volumes of corpus for\nmore training steps [26]. The large size and high latency\nmake the deployment of PLTMs difﬁcult in real-word\napplications where resources are limited and require\nfast inference. To reduce the size of T-PTLMs and make\nthem faster, many model compression techniques like\npruning, parameter sharing, knowledge distillation, and\n21\nquantization are explored in recent times [162].\nPruning: In general, deep learning models like T-\nPTLMs are over parameterized i.e., some of the model\ncomponents (weights [163], attention heads [164], [165],\nor layers [166], [167] can be removed during pretraining\nor after pretraining without much impact on the model\nperformance and also reducing the model storage space\nand inference time. Pruning is inspired from the biolog-\nical observation, “thousands of trillions of synapses in\na newborn baby reduces to 500 trillion synapses after\nten years” [162]. T-PTLMs are trained with multiple\nattention heads. Michel et al. [164] and Voita et al. [165]\nshowed that most of the attention heads are redundant\nand can be removed during inference. Fan et al. [167]\nshowed that encoder layers can be dropped during pre-\ntraining which allows dropping layers during inference.\nIn contrast, Sajjad et al. [166] applied layer dropping on\nthe pre-trained models which eliminates training from\nscratch unlike Fan et al. [167].\nKnowledge Distillation : Knowledge Distillation is a\nmodel compression method that allows training com-\npact student models using the knowledge from large\nteacher models. During knowledge distillation, the stu-\ndent learns the generalization ability of the teacher\nmodel by reproducing its behavior, and hence the per-\nformance of the student model is on par with the teacher\nmodel. Knowledge Distillation is introduced by Bucila et\nal. [168] and later generalized by Ba and Caruna [169]\nand Hinton et al. [170]. The approach of Ba and Caruna\n[169] trains the student model using L2 loss between\nteacher and student model logits while the approach of\nHinton et al. [170] uses cross-entropy between softmax\nlogits of teacher and student (soft loss) as well as cross-\nentropy loss between student prediction and actual label\n(hard loss). Some of the popular models trained using\nKnowledge distillation are DistilBERT [171], Tiny-BERT\n[172], BERT-PKD [173], MobileBERT [174], and MiniLM\n[175].\nQuantization: Quantization compresses a model by\nusing fewer bits to represent weights. In general, T-\nPTLMs parameters are represented using 32 or 16 bits.\nQuantized T-PTLMs use 8 bits [176] or even lesser [177]–\n[179] to present weights. Pruning compresses a model\nby removing less important weights, while quantization\ncompresses a model by using fewer bits for weights\nrepresentation. Some of the popular quantized BERT\nmodels are Q8BERT [176], Q-BERT [177], and Ternary-\nBERT [178]. To reduce performance drop in ultra-low\n(1 or 2) bit models, researchers proposed methods like\nmixed-bit quantization [177], [179], combining knowl-\nedge distillation with quantization [178], and product\nquantization (PQ) [180]. Mixed-bit quantization is not\nsupported by some hardware while PQ requires extra\nclustering operations. Overall, quantization compresses\nthe model by using fewer bits but as quantization is\nhardware speciﬁc, we need specialized hardware to use\nquantized models.\nParameter Sharing : ALBERT [7] a lite version of the\nBERT model achieves parameter reduction by cross-layer\nparameter sharing and factorized embedding parame-\nterization. Factorized embedding parameterization splits\nthe large vocabulary matrix into two small matrices\nwhich allow growing the hidden vector size without\nsigniﬁcantly increasing vocabulary matrix parameters.\nCross-layer parameter sharing prevents the growth of\nparameters with the increase in the depth of the model.\nWith these two parameter reduction techniques, the\nALBERT model has 18x fewer parameters and can be\ntrained 1.7x faster compared to the BERT-large model.\nCross-layer parameter sharing is also explored in Uni-\nversal Transformers [181].\n4.4.2 Character-based T -PTLMs\nMost of the T-PTLMs use sub-word embeddings based\non tokenizers like BPE, bBPE, WordPiece, Unigram,\nand SentencePiece. The problem with the use of word\nembeddings is the requirement of a large vocabulary\nand OOV problem. Sub-word embeddings are based on\nthe idea that only rare and misspelled words should\nbe represented using sub-words while frequently used\nwords should be represented as it is. Sub-word embed-\ndings overcome the two problems in word embeddings.\nHowever sub-word embeddings have two drawbacks a)\ncannot encode ﬁne-grained character level information\nin the word representation and b) brittleness to noise\ni.e., even simple typos can change the representation of\na word which hinders the model learning [66], [121].\nTo overcome these drawbacks, character-based T-\nPTLMs like CharacterBERT [66], CharBERT [121], and\nAlphaBERT [120] are proposed. CharacterBERT uses\nCharCNN+Highway layer to generate word representa-\ntions from character embeddings and then apply trans-\nformer encoder layers. The use of CharCNN+Highway\nlayer is inspired from ELMo [122]. Different from Char-\nacterBERT which uses only character embeddings, Char-\nBERT uses both character and sub-word embeddings.\nIn CharBERT, character-level word embeddings are gen-\nerated from character embeddings using Bidirectional\nGRU similar to contextual string embeddings [123]. Then\na dual-channel CNN is used in every transformer en-\ncoder layer to model the interaction between character\nand sub-word embedding channels. Due to the inclu-\nsion of an extra channel for character embeddings and\ninteraction module in every layer, the size of the model\nincreases by 5M parameters. CharBERT or CharRoBERTa\nare more robust to noise and perform better than BERT\nor RoBERTa models. Unlike CharacterBERT and Char-\nBERT, AlphaBERT in Biomedical domain operates di-\nrectly at the character level. In AlphaBERT, transformer\nencoder layers are directly applied on character embed-\ndings after adding to position embeddings.\n4.4.3 Green T -PTLMs\nThe standard approach to adapt general models to a\nspeciﬁc domain or improve general models with knowl-\nedge from Knowledge bases is continual pretraining.\n22\nThe resulting models after continual pretraining achieve\ngood results. But this process is expensive in terms of\nhardware and run time and also not environmentally\nfriendly with CO2 emissions [182], [183]. Recently, re-\nsearchers focused on developing less expensive methods\nto adapt general models to a speciﬁc domain or to inject\nknowledge from knowledge bases. The models devel-\noped using less expensive methods like GreenBioBERT\n[184], exBERT [185], and E-BERT [186] are referred to as\nGreen models as they are developed in a environmen-\ntally friendly way.\nGreenBioBERT [184] is developed by extending the\nvocabulary of the general BERT model using domain-\nspeciﬁc word embeddings developed using Word2Vec\nwhich are further aligned with WordPiece embeddings.\nGreenBioBERT achieves comparable performance with\nBioBERT which is developed by further pretraining\nfor 10 days using eight v100 NVIDIA GPUs. exBERT\n[185] is developed by extending general BERT with\ndomain-speciﬁc WordPiece embeddings and an exten-\nsion module. During continual pretraining, as the extra\nWordPiece embeddings and extension module param-\neters are only updated while keeping other parame-\nters freezed, this process is less expensive. E-BERT is\ndeveloped by extending BERT model vocabulary with\nthe Wikipedia2Vec entity vectors [187] after aligning. E-\nBERT [186] which doesn’t require any further pretraining\noutperforms models like ERNINE [188] and KnowBERT\n[189] (both these models require further pretraining to\ninject information from knowledge bases) on the LAMA\nbenchmark.\n4.4.4 Sentence-based T -PTLMs\nThe sentence embeddings obtained from T-PLTMs like\nBERT by applying any of the pooling strategies are\nnot effective [190]. Recently many approaches based on\nsupervised learning [190], [191] or self-supervised learn-\ning [158], [159], [192]–[194], [194], [195] are proposed.\nSBERT [190] is one of the ﬁrst supervised approaches\nwhich extend T-PTLMs like BERT to generate quality\nsentence embeddings. SBERT ﬁne-tunes BERT model\nusing the Siamese network over NLI and STSb datasets.\nBy ﬁne-tuning over NLI and STSb which are sentence\npair classiﬁcation tasks, the model learns sentence-level\nsemantics and hence generates quality sentence vectors.\nDvBERT [191] which is based on multi-view learning\n[196] extends SBERT by adding word-level interaction\nfeatures across two sentences. As supervised approaches\nrequire labeled datasets which limits the application of\nthese models in labeled data scare domains.\nMoreover, Zhang et al. [192] showed that the perfor-\nmance of NLI and STSb ﬁne-tuned models is limited\nwhen training data is limited or distribution of test\ndiffers signiﬁcantly from training data. To overcome\nthe requirement of labeled datasets, many approaches\nbased on SSL are proposed. IS-BERT [192] uses mutual\ninformation maximization strategy to learn quality sen-\ntence embeddings and CNN instead of mean pooling\non the top of BERT model. Mirror-BERT [158] trains\nBERT by using a contrastive learning-based objective to\npush positive sentence pairs to have similar representa-\ntions. Here positive sentences are obtained by random\nmasking in input space or applying dropout in feature\nspace. TSDAE [193] involves reconstructing the original\nsentence from the sentence embedding generated by the\nencoder using a corrupted sentence. SimCSE [159] is a\ncontrastive learning-based framework to further train\nPTLMs to generate better sentence embeddings using\nunlabelled or labeled data.\n4.4.5 Tokenization-Free T -PLTMs\nMost of the existing PTLMs use sub-word or character\nor both the embeddings. The main drawback with these\napproaches are\n• Sub-word embeddings require a ﬁxed vocabulary\nthat is modest in size in models like BERT and\nRoBERTa but larger in multilingual models. The\nvocabulary requires a vocabulary matrix in which\neach token is mapped with a vector and softmax\nmatrix in the output layer. These two matrix param-\neters occupy a signiﬁcant amount of model param-\neters. For example, these two matrix parameters are\nabout 66% of mT5 model parameters [99]. Moreover,\nhaving a ﬁxed vocabulary makes the adaptation\nof models to other domains inefﬁcient i.e., many\ndomain-speciﬁc words are not represented properly\nwhich impacts model adaptation as well as model\ndownstream performance [47], [66].\n• Both sub-word and character embeddings require\nan explicit tokenizer that splits the input sequence\nbased on white space or punctuation. This becomes\nproblematic in the case of languages that do not use\nwhite space or punctuations as word separators. For\nexample, languages like Chinese and Thai do not\nuse white space as separators and languages like\nHawaiian and Twi use punctuations as consonants\n[67].\nRecently there is a rising interest in the research\ncommunity to overcome the above drawbacks with\ntokenization-free T-PTLMs [67]–[69]. With tokenization-\nfree models, there is no need for language-speciﬁc tok-\nenizers, models are more robust to noise and have no\nlarge vocabulary which requires a signiﬁcant amount of\nmodel parameters. CANINE [67] is the ﬁrst tokenization-\nfree T-PTLM which directly operates on character se-\nquence. The model applies convolution layers on the\ncharacter sequence to reduce the input sequence length\nand then applies transformer encoder layer stack. CA-\nNINE is pretrained with the same tasks as the BERT\nmodel. CANINE with 28% fewer parameters compared\nto mBERT output performs it by 2.8 points on multilin-\ngual QA.\nByT5 [68] is an improved version of T5 model to han-\ndle input at byte-level without using any ﬁxed vocabu-\nlary. T5 uses the same number of encoder and decoder\n23\nlayers while the depth of the encoder is 3x compared\nto the decoder in ByT5. Charformer [69] uses a novel\ntokenizer that uses gradients to automatically learn sub-\nwords from characters which eliminate the requirement\nof a ﬁxed vocabulary. Charformer performs on par with\nmodels like T5 while outperforming byte-level models\nlike ByT5. Moreover,unlike CANINE, the gradient-based\nsub-word tokenizer output is interpretable.\n4.4.6 Large Scale T -PTLMs\nInitially the sizes of T-PTLMs are in the range of 110M to\n340M parameters [2], [4], [5]. Kaplan et al. [26] showed\nthat the performance of T-PTLMs is strongly related to\nthe scale rather than the depth or width of the model.\nThe authors showed that the performance of T-PTLMs\nis largely determined by the scale i.e., the number of\nparameters, the size of pretraining data, and the amount\nof pretraining compute. According to Kalplan et al.\n[26] the performance of the model can be increased by\nincreasing the size of the model or training the model on\nmuch large volumes of large data or training the model\nfor more training steps. All these three must be scaled up\nat the same time to achieve optimal performance. This\nobservation triggered the development of large-scale\nT-PTLMs like GPT-3 (175B) [27], PANGU(200B) [28],\nGShard (600B) [29] which contains billions of parame-\nters and Switch-Transformers (1.6T) [30] which contains\ntrillions of parameters.\n4.4.7 Knowledge Enriched T -PTLMs\nT-PTLMs are developed by pretraining over large vol-\numes of text data. During pretraining, the model learns\nknowledge available in the pretraining text data by\nsolving one ore more challenging pretraining tasks. Re-\ncent works [188], [189], [197]–[204] showed that these\nmodels can be further improved by integrating the\nknowledge available in external knowledge sources. T-\nPTLMs integrated with knowledge from external sources\nare referred to as Knowledge Enriched T-PTLMs. Some\nof the popular external knowledge sources are WordNet,\nWikidata in the general domain and UMLS [205] in\nspeciﬁc domains like Biomedical. Some of the examples\nof knowledge enriched T-PTLMs are CasualBERT [197],\nKnowBERT [189], SenseBERT [204], LIMIT-BERT [199],\nLiBERT [198], SentiLARE [200], ERNINE [188], E-BERT\n[186] in the general domain and Clincal Kb-BERT [201],\nClinical Kb-ALBERT [201], SapBERT [202], Sap-XLMR\n[202], UmlsBERT [127], CoderBERT [203], CoderBERT-\nAll [203] in speciﬁc domain like Biomedical. For ex-\nample, CasualBERT injects casual knowledge into BERT\nmodel using two novel pretraining tasks on cause-effect\npairs. LiBERT is pretrained from scratch using lingual\nrelation classiﬁcation (LRC) task along with MLM and\nNSP . LRC task helps to inject linguistic knowledge.\nLiBERT outperforms BERT model in most of the GLUE\ntasks. SentiLARE introduces label-aware MLM to in-\nject POS tag and word polarity information into BERT\nmodel. All the biomedical domain-speciﬁc knowledge-\nenriched models are obtained by integrating knowledge\nfrom the biomedical ontology UMLS.\n4.4.8 Long-Sequence T -PTLMs\nThe self-attention attention module in transformers up-\ndates the representation of each input token by attending\nto all tokens in the input sequence. The quadratic time\ncomplexity of the self-attention module limits the appli-\ncation of T-PTLMs to long input sequences. To overcome\nthis drawback, self-attention variants like sparse self-\nattention and linearized self-attention are proposed to\nreduce its complexity and hence extend T-PTLMs to\nlong input sequences also [206]. Some of the popular T-\nPTLMs based on a) sparse self-attention are Longformer\n[207], ETC [208], BigBird [209] and Reformer [210] and\nb) linearized self-attention are Performer [211]. Sparse\nself-attention reduces the complexity by including spar-\nsity bias which reduces the number of query-key pairs\nthat each query attends to. In linearized self-attention,\nreduced complexity is achieved by disentangling the\nattention with kernel feature maps and then computing\nthe attention in reverse order.\n4.4.9 Efﬁcient T -PTLMs\nT-PTLMs require pretraining on large volumes of text\ndata for longer durations which makes pretraining\nhighly expensive. Recently, with better model architec-\ntures it is possible to achieve similar or better per-\nformances using less pretraining data [212] and less\npretraining costs [213]. DeBERTa [212] improves the\nBERT model using disentangled attention mechanism\nand enhanced masked decoder. Disentangled attention\nmechanism represents a word using separate vectors to\nencode its content and position information and then\ncompute the attention weights based on contents and\nrelative positions. An enhanced masked decoder is used\nto predict masked tokens instead of softmax layer during\npretraining. These two novel changes improve pretrain-\ning efﬁciency and DeBERTa model which is pretraining\non 78GB of data outperforms RoBERTa which is pre-\ntrained on 160GB of data.\nConvBERT [213] improves the BERT model with\nmixed attention block consisting of self-attention and\nspan based dynamic convolution modules. The self-\nattention modules model global dependencies while\nspan-based dynamic convolution modules model local\ndependencies. The authors of ConvBERT observed that\nsome attention heads are needed to model only local\ndependencies and hence they replaced these attention\nheads with span-based dynamic convolution modules.\nConvBERT with better model architecture outperforms\nELECTRA base using less than ¼ of its pretraining cost.\n5 D OWNSTREAM ADAPTATION METHODS\nOnce a language model is pretrained, it can be used\nin downstream tasks. A pretrained language model can\n24\nFig. 10: Downstream adaptation methods\nbe used in downstream tasks in three ways namely\na) feature-based b) ﬁne-tuning and c) prompt-based\ntuning (refer Figure 10). The feature-based approach\ninvolves generating contextual word embeddings from\nlanguage models and then using them as input fea-\ntures in task-speciﬁc downstream models. Fine-tuning\ninvolves adapting model weights to downstream tasks\nby minimizing task-speciﬁc loss.\n5.1 Feature-based\nIn traditional deep learning models like CNN or RNN,\nword embeddings generated using embedding models\nlike Word2Vec [10] or Glove [11] are used as word fea-\ntures. In feature-based approach, BERT [2] based models\nare used to generate contextual word vectors, and then\nthey are used as input features similar to Word2Vec or\nGlove embeddings in task-speciﬁc downstream models.\nBERT-based contextual word embeddings are much bet-\nter as a) they are contextual unlike Word2Vec and Glove\nembeddings b) overcome the issue of OOV words and c)\nencode more information in word vectors because of the\ndeep layered model architecture. Here, word vectors can\nbe taken from the last layer (or from multiple layers us-\ning any of the pooling strategies) [2]. The advantage with\nthe feature-based approach is that contextualized word\nvectors can be used any in any of the handcrafted state-\nof-the-art task speciﬁc-architectures. However, feature-\nbased approach involves training the downstream model\nfrom scratch (except embeddings) which requires a large\nnumber of labeled instances.\n5.2 Fine-tuning\nPretraining allows the pretrained language model to gain\nuniversal language knowledge. However, the perfor-\nmance of the model in downstream tasks requires task-\nspeciﬁc knowledge i.e., for the model to perform well\nin downstream tasks, its weights should be close to the\nideal setting for the target task [214]. Fine-tuning imparts\ntask-speciﬁc knowledge to the model by adapting its\nweights based on task-speciﬁc loss [2]. Moreover, ﬁne-\ntuning enhances the model performance because it clus-\nters the points of different labels away from each other\nsuch that there is a large separation between the cluster\nregions [215]. Fine-tuning updates all the transformer\nlayers including the embedding layer but the higher\nlayers are subjected to more changes compared to the\nlower layers [215]–[218].\nModels like BERT, RoBERTa, and ELECTRA do not\nfollow uniﬁed input-output format across tasks i.e., dif-\nferent tasks have different input and output formats. So,\nit is required to add task-speciﬁc layers in these models\nduring ﬁne-tuning. However, in models like T5 [6] which\nfollow the same input-output format across tasks, there\nis no need to add any extra layers speciﬁc to each task.\nT5 follows a text-to-text format in any task i.e., input for\nthe model is some text and the model has to produce\nsome text as output.\nFine-tuning can be a) Vanilla ﬁne-tuning [2] b) In-\ntermediate ﬁne-tuning [214], [219], [220] c) Parameter\nefﬁcient ﬁne-tuning and d) Multi-task ﬁne-tuning [221]–\n[224]. Unlike Vanilla ﬁne-tuning which is prone to overﬁt\nthe model on small datasets, intermediate ﬁne-tuning\nor multi-task ﬁne-tuning avoid overﬁtting the model\non small datasets. As ﬁne-tuning involves adjustments\nto the entire model weights, methods like adapters or\npruning-based ﬁne-tuning help to ﬁne-tune the model\nin a parameter-efﬁcient way.\n25\n5.2.1 Vanilla Fine-Tuning\nIn Vanilla ﬁne-tuning, the model is adapted to down-\nstream tasks based on task-speciﬁc loss [2]. The main\ndrawback in vanilla ﬁne-tuning is that PTLM having\nlarge parameters is prone to overﬁt on small task-\nspeciﬁc datasets. Moreover, with small datasets, the\nmodel weights are not adapted well to the end task\nwhich limits its performance. Intermediate ﬁne-tuning\nor multi-task ﬁne-tuning overcome the issues in vanilla\nﬁne-tuning.\n5.2.2 Intermediate Fine-Tuning (IFT)\nIFT involves ﬁne-tuning the model on an intermediate\ndataset with a large number of labeled instances. IFT\nhelps the model to gain additional domain or task-\nspeciﬁc knowledge which avoids overﬁtting and en-\nhances its performance on small target datasets [214],\n[219], [220]. Poth et al. [220] established that intermediate\npre-training can yield performance gains in adapter-\nbased setups, similar to what has been previously found\nfor full model ﬁnetuning. IFT can be domain adaptive\n[225] or task adaptive [214], [226]–[230].\nDomain adaptive intermediate ﬁne-tuning (DAIFT) :\nDAIFT involves ﬁne-tuning the model on the same\ndomain dataset with a large number of labeled instances\ni.e., source and target datasets are of the same domain\nbut different tasks. DAIFT on the same domain source\ndataset imparts more domain knowledge to the model\nwhich enhances the model performance on the same\ndomain target task [225]. McCreery et al. [225] ﬁne-tuned\nmodels like BERT and XLNet on the medical question-\nanswer pairs dataset to enhance the performance on the\nMedical question similarity dataset. Here source (med-\nical question-answer pair) dataset and target (medical\nquestion similarity) datasets are from the same domain\ni.e., Medical but from different tasks. The models (BERT\nand XLNet) are pretrained on a general domain text\ncorpus. DAFT on medical domain dataset injects medical\nknowledge into BERT and XLNet which are pretrained\non a general domain text corpus. McCreery et. al. [225]\nshowed that the improvement is more when the number\nof training instances in the target task is less.\nTask adaptive intermediate ﬁne-tuning (TAIFT) : TAIFT\ninvolves ﬁne-tuning the model on the same or related\ntask dataset with a large number of labeled instances\ni.e., source and target datasets are from the same or\nrelated task. Here the source and target datasets need\nnot be from the same domain. TAIFT on the same or\nrelated task source dataset imparts more task-speciﬁc\nknowledge to the model which enhances the model\nperformance on the target dataset. For example, Cengiz\net al. [226] showed that TAIFT on general domain NLI\ndatasets improves the in-domain model performance on\nthe medical NLI dataset. Similarly, Yang et al. [227]\nand Wang et al. [228] achieved better performance on\nclinical STS dataset with TAIFT on general STS dataset.\nYoon et al. [229] showed that TAIFT on the general do-\nmain SQUAD dataset improves the performance on the\nbiomedical question answering dataset. Jeong et al. [230]\nimproved BioBERT model performance in biomedical\nQA with TAIFT on the general NLI dataset. Phang et al.\n[214] achieved an improvement of 1.4 in GLUE score for\nBERT with TAIFT on the general NLI dataset. Further,\nthe authors showed that the improvement is more when\ntarget labeled instances are less in number. TAIFT on\nNLI datasets imparts sentence-level reasoning skills to\nthe model which improves the model performance in\nother tasks.\nHowever, IFT does not guarantee better performance\nall the time [214], [219], [231] i.e., IFT sometimes nega-\ntively impacts the transferability to downstream tasks.\nPruksachatkun et al. [219] performed a large-scale study\non the pretrained RoBERTa model with 110 intermedi-\nate–target task combinations to investigate when and\nwhy IFT is beneﬁcial. The authors showed that interme-\ndiate tasks requiring high-level inference and reasoning\nabilities tend to work best i.e., NLI and QA tasks that\ninvolve common sense reasoning are generally useful as\nintermediate tasks.\n5.2.3 Multi-task Fine-Tuning (MTFT)\nMulti-task learning (MTL) allows the model to learn\nknowledge that is useful across tasks. The primary focus\nof MTL can be improving the performance of target\ntasks with the help of auxiliary tasks or improving the\nperformance of all the tasks [232]. The advantages of\nMTL are a) allows the model to gain more knowledge\nby learning from multiple datasets simultaneously which\nreduces the requirement of a large number of labeled\ninstances in a speciﬁc target task. b) provides a regular-\nization effect by avoiding overﬁtting to a speciﬁc target\ntask [221]. Multi-task ﬁne-tuning can be a) Vanilla MTFT\nb) Iterative MTFT and c) Hybrid.\nVanilla MTFT : Vanilla MTFT involves ﬁne-tuning the\nmodel on multiple datasets simultaneously [221], [233].\nFor example, Liu et al. [221] improved the performance\nof BERT model in GLUE tasks using vanilla MTFT.\nHere, the embedding and transformer layers are shared\nacross the tasks while each task has a task-speciﬁc layer.\nHowever, it is not guaranteed that Vanilla MTFT always\nimproves the performance of the model across tasks\n[222]. For example, Mulyar et al. [222] developed MT-\nClinicalBERT by ﬁne-tuning ClinicalBERT using multi-\nple datasets related to NER, STS, and RTE tasks. MT-\nClinicalBERT achieved on par but less performance com-\npared to task-speciﬁc ClinicalBERT models. The possible\nreason for this is that some of the tasks may negatively\ntransfer knowledge which reduces the performance of\nthe model. The drawback in vanilla MTFT can be\navoided using Iterative MTFT which allows selecting the\nbest set of tasks or MTFT followed by vanilla ﬁne-tuning.\nIterative MTFT : Iterative MTFT allows to select the\nbest set of tasks for ﬁne-tuning the model [223]. It is\nnecessary to select the best set of related datasets as\nvanilla MTFT on all the tasks sometimes may degrade\nthe performance of the model [222]. Iterative MTFT is\n26\nsimilar to traditional feature selection in machine learn-\ning. Iterative MTFT helps to select the best set of datasets\nto ﬁne-tune the model whereas feature selection helps\nto select the best set of features. Mahanjan et al. [223]\napplied iterative MTFT to choose the best set of related\ndatasets and achieved SOTA results on the clinical STS\ndataset.\nHybrid MTFT : Iterative MTFT allows to choose the\nbest set of related datasets, but it is expensive as it\ninvolves multiple iterations. Moreover, each iteration\ninvolves training the model on multiple datasets and\nthen ﬁne-tuning it on the target dataset. Instead of\niteratively applying MTFT, we can ﬁne-tune the model\non multiple related datasets and then ﬁne-tune it on\nthe target dataset with a small learning rate [224]. We\nrefer to this as hybrid MTFT as it involves vanilla MTFT\nfollowed by vanilla ﬁne-tuning.\n5.2.4 Parameter Efﬁcient Fine-Tuning\nFine-tuning allows the model weights to adapt to down-\nstream tasks by minimizing the task-speciﬁc loss i.e.,\nﬁne-tuning starts with copying the entire model weights\nand making small changes. As ﬁne-tuning involves up-\ndating the entire model weights, it is required to train\na separate model for each task which is not param-\neter efﬁcient. Adapters [234] and pruning-based ﬁne-\ntuning [166] helps to ﬁne-tune the model in a parameter-\nefﬁcient way.\nAdapters [234]– The adapter is a special trainable layer\nmodule proposed by Houlsby et al. [234] to ﬁne-tune\npretrained language models in a parameter-efﬁcient way.\nThe adapter module consists of two feed-forward layers\nwith a non-linear layer in between and a skip connection.\nThe adapter module projects the input vector into a\nsmall vector and then projects back into the original\ndimension using the two feed-forward layers and non-\nlinear layer. Let x be the original vector dimension and\ny be the small vector dimension, then the total number\nparameters in the adapter module are 2xy+ x+ y. By\nsetting x << y, we can further reduce the number of\nparameters in the adapter module. The small vector\ndimension (y) provides a trade-off between performance\nand parameter efﬁciency. Adapters are added to each of\nthe sublayers in transformer layer before layer normal-\nization. During ﬁne-tuning, only parameters of adapters,\nlayer normalization in each transformer layer, and task-\nspeciﬁc layers are only updated while the rest of the\nparameters in pretrained model are kept frozen.\nHoulsby et al. [234] showed that adapter-based ﬁne-\ntuning is highly parameter efﬁcient and they can achieve\nthe performance of a fully ﬁne-tuned model using\nadapter-based ﬁne-tuning which involves only 3% of\ntask-speciﬁc parameters. Moreover, Poth et al. [220]\nshowed that intermediate ﬁne-tuning using adapters\nimprove model performance in the target task. Stick-\nland and Murray [235] proposed an approach to train\nadapters in a multi-task setting. However, this approach\nsuffers from issues like a) requirement of simultaneous\naccess to multiple datasets and b) difﬁculty in balancing\nvarious tasks as the model may overﬁt on low resource\ntasks and underﬁt on high resource tasks. Pfeiffer et al.\n[236] proposed AdapterFusion a novel two-stage method\nbased on adapters that overcome the issues in sequential\nlearning and multi-task learning to leverage knowledge\nfrom multiple tasks. They showed that AdapterFusion\noutperforms full ﬁne-tuning as well the adapter-based\nmodel trained in single and multi-task setups.\nPruning-based ﬁne-tuning - Recent studies [164]–[166],\n[237], [238] show that deep pre-trained language models\nhave redundancy. Pruning methods are based on the\nidea that not all the parameters are important in the\npre-trained models and some of them can be removed\nwithout much impact on the model performance. For\nexample, research studies [164], [165], [237] show that\nsome of the attention heads can be pruned. Sajjad et al.\n[166] proposed different strategies to drop encoder layers\nin pretrained language models. The authors showed\nthat the size of the pre-trained models can be reduced\nby dropping encoder layers after pre-training and the\nresulting pruned BERT model can be ﬁne-tuned to get\ncompetitive performance. The experimental results show\nthat it is possible to prune BERT, RoBERTa, and XLNet\nmodels by up to 40% while maintaining up to 98% of\ntheir original performance.\n5.3 Prompt-based Tuning\nIn general, most of the P-TLMs are pretrained us-\ning language modeling objectives and then adapted to\ndownstream tasks using ﬁne-tuning which involves task-\nspeciﬁc objectives. The discrepancy in objectives during\npretraining and ﬁne-tuning impacts the downstream per-\nformance of the model. The downstream performance\nof models can be improved especially in few-short and\nzero-shot settings by prompt-based tuning which formu-\nlates the tuning process as a slot ﬁlling which is close to\nthe language modeling objective. Here the prompt can be\nclose or pre-ﬁx shape and it can be generated manually\nor automatically [239]. Close style prompts are suitable\nfor models pretrained masked language modeling objec-\ntive while pre-ﬁx style prompts are suitable for models\npretrained using casual modeling objective.\nPrompt-based tuning initially is based on manually\ncreated prompts. For example, LAMA [240] probe is\nconducting manually created close-style prompts while\nGPT-3 [27] model is tuned using manually created pre-ﬁx\nstyle prompts. As manually creating prompts is a time-\ntaking process and these prompts can be sub-optimal\nalso [241]. To over these drawbacks prompts are created\nautomatically. Automatically generated prompts can be\ndiscrete or continuous. A discrete prompt is simply a\nstring i.e., a sequence of words included in the input text\nto guide the T-PTLM to better model the downstream\ntask. Some of the popular methods to generate discrete\nprompts are Prompt mining [241], Prompt generation\n[242], Prompt paraphrasing [241], [243], and Gradient-\nbased search [244], [245].\n27\nFig. 11: Benchmarks to evaluate the progress in T-PTLMs\nPrompt mining [241] involves collecting a large num-\nber of sentences having the subject x and object y\nand then generate the new prompts using the middle\nwords or the dependency paths. Prompt generation [242]\ninvolves generating prompts using T-PTLMs like T5.\nPrompt paraphrasing generates new prompts from seed\nprompts using methods like back translation [241] or\nequivalent phrases from thesaurus [243]. Gradient-based\nsearch generates trigger tokens which can be combined\nwith input sequence to create a prompt [244], [245].\nContinuous prompts perform prompting in the em-\nbedding space of T-PTLM i.e., add a sequence of task-\nspeciﬁc vectors to the input sequence. Here the prompt\nvectors need not be embeddings of natural language\nwords. Unlike discrete prompts where the template pa-\nrameters are determined by T-PTLM parameters, in con-\ntinuous prompts, the templates have their parameters\nindependent of T-PTLM parameters. Some of the pop-\nular continuous prompt generating approaches are Pre-\ntuning [246], P-Tuning [247], and Prompt-tuning [248].\nFurther Lester et al. [248] showed that prompt ensem-\nbling outperforms traditional model ensembling. Here\nprompt ensembling means generating multiple prompts\nfor the same task.\n6 E VALUATION\nA T-PTLM gains knowledge encoded in pretraining cor-\npus during pretraining. Here the knowledge refers to\nsyntactic, semantic, factual, and common-sense knowl-\nedge. The effectiveness of a T-PTLM can be evaluated\nin two ways namely intrinsic and extrinsic (refer Figure\n11). Intrinsic evaluation probes the knowledge encode\nin T-PLTM while extrinsic evaluation evaluates how\neffective the T-PTLMs are in real-world downstream\ntasks. Intrinsic evaluation sheds light on the knowledge\ngained by T-PTLM during pretraining which helps us to\ndesign better pretraining tasks so that the model learns\nmore knowledge during the pretraining stage itself.\n6.1 Intrinsic Evaluation\nIntrinsic evaluation involves probing the model knowl-\nedge using probes like LAMA [240], XLAMA [252], X-\nFACTR [257], MickeyProbe [160] , Negated LAMA [253],\nMisprimed LAMA [253], WDLMPro [254] or WNLaM-\nPro [256] (refer Table 8). LAMA is one of the ﬁrst\nprobes introduced to evaluate factual and common-sense\nknowledge in PLTMs under zero-shot settings. LAMA\nconsists of a corpus of facts where the fact can be a\nrelation triplet or a question-answer pair gathered from\nSQUAD. Here facts are converted to ﬁll-in-the-blank\nstyle questions and the model is evaluated based on\nthe prediction of blank tokens i.e., argmax x∈W P(x⁄temp)\nrepresents the model vocabulary and temp represents\nthe ﬁll-in-the-blank template. LAMA is based on the\nhypothesis that a model with a good amount of factual\nknowledge correctly predicts the blank tokens i.e., i.e.,\nthe ground truth tokens are predicted with the highest\nprobability compared to other tokens in the model vo-\ncabulary. Negated LAMA and Misprimed LAMA probe\nshows that the language models are not able to consider\nthe negated or misprimed words in the templates. For\nexample, the model predicts the same token whether the\ntemplated is negated or not. Poerner et al. (2020) [186]\nintroduced LAMA-UHN which is a collection of triples\nfrom the LAMA probing benchmark which are difﬁcult\nto guess.\nThe main drawbacks in the LAMA probe are a) restric-\ntion to single token entities only b) limits the prediction\nof tokens over the model vocabulary which hinders\nthe evaluation of models with different vocabulary c)\nit probes only English language models and d) many of\nthe triples in LAMA are easy to guess [186]. XLAMA\n28\nName Probe Language Method Includes Data Source\nLAMA [240] Factual and Common-\nsense knowledge\nEnglish UnTQ Single token entities TREx [249], GoogleRE, Con-\nceptNet [250] and SQUAD\n[251]\nXLAMA [252] Factual knowledge Multilingual\n(53 languages)\nTQ (Typed\nQuery)\nSingle and Multi token\nentities\nTREx [249] and GoogleRE\nNegated\nLAMA [253]\nImpact of negation\nin probing factual\nand common-sense\nknowledge\nEnglish UnTQ Single token entities TRex [249], GoogleRE, Con-\nceptNet [250]and SQUAD\n[251]\nMisprimed\nLAMA [253]\nImpact of mis primes\nin probing factual and\ncommon-sense knowl-\nedge\nEnglish UnTQ Single token entities TRex [249], GoogleRE, Con-\nceptNet [250]and SQUAD\n[251]\nWDLMPro\n[254]\nWord Understanding English Ranking Single and Multi-token\nentities\nWordNet [255]\nWNLaMPro\n[256]\nWord Understanding English UnTQ Single and Multi-token\nentities\nWordNet [255]\nX-FACTR [257] Factual knowledge Multilingual\n(26 languages)\nUnTQ Single and Multi-token\nentities\nTREx [249]\nMickeyProbe\n[160]\nCommon-sense knowl-\nedge\nMultilingual\n(11 languages)\nSentence\nranking\n- OMCS Corpus [258]\nTABLE 8. Summary of various intrinsic benchmarks.\nextends the LAMA probe to multiple languages (53 lan-\nguages) and includes multi-token entities also. Moreover,\nin LAMA the model has to predict over the entire model\nvocabulary while in XLAMA the model has to predict\nover a ﬁxed set of candidates speciﬁc to each relation\ntype i.e., argmax x∈C P(x⁄temp)where C represents a set\nof candidate entities speciﬁc to a relation type. The au-\nthors of XLAMA refer to this type of querying as Typed\nQuery (TQ) and the querying in LAMA as UnTyped\nQuery (UnTQ). Similar to XLAMA, X-FACTR is also\na multi-lingual probe for 23 languages. Moreover, the\nauthors of X-FACTR developed several decoding algo-\nrithms to predict multi-token entities. MickeyProbe [160]\nis a zero-shot common-sense probe that uses sentence-\nlevel ranking based on Pseudo-Likelikhood [259]. Here\nthe model has to rank a set of declarative sentences\nhaving similar words and syntactic features. The perfor-\nmance of multilingual models in retrieving knowledge\nvaries with language i.e., it is high in high resource\nlanguages compared to low resource languages [160],\n[252], [257] . Moreover, the multilingual model exhibits\nlanguage bias i.e., the language of query affects the\nmodel prediction [252]. The model performance in re-\ntrieving knowledge can be improved by pretraining on\ncod-switched data [257] or further pretraining using a\nmultilingual contrastive loss function [160].\nProbes like LAMA, XLAMA, and X-FACTR focus on\nevaluating the relations between entities. Unlike these\nprobes, WDLMPro and WNLaMPro focus on under-\nstanding how the pretrained models understand the\nwords. WNLamPro uses ﬁll-in-the-black style templates\nwhile WDLMPro evaluates the model by matching a\nword with its deﬁnition. WDLMPro probe is based on\nthe assumption that a model correctly matches a word\nwith its deﬁnition only when the model understands the\nword. WDLMPro consists of synset groups where each\ngroup consists of a word, its taxonomic sister words from\nWordNet along their deﬁnitions.\n6.2 Extrinsic Evaluation\nExtrinsic evaluation helps to assess the performance of\na model in downstream tasks. To get the maximum out\nof a model, the model should perform well across a\nwide range of tasks rather than just performing well\non one or two tasks. A benchmark provides a standard\nway of evaluating the model’s generalization ability\nacross tasks. A benchmark usually consists of a set of\ndatasets, a leader board, and a single metric [260]. The\ndatasets are chosen in a way that they are challenging\nand represent diverse tasks. A leaderboard is an online\nrepository that helps to compare and rank models. For a\nmodel to achieve a good score in a benchmark, it should\nshare knowledge i.e., parameters across tasks with one\nor two layers speciﬁc to each task [260]. A benchmark\nuses a single metric to evaluate the overall performance\nof the model across tasks. Without a benchmark, it is\ndifﬁcult to evaluate models in a standard way and track\nthe progress in the development of pretrained language\nmodels. A summary of various extrinsic bechmarks are\npresented in Tables 9 and 10.\nGLUE [260] and SuperGLUE [261] benchmarks are\nthe commonly used benchmarks to evaluate the natural\nlanguage understanding ability of pretrained language\nmodels. GLUE benchmark consists of nine tasks which\ninclude both single sentence and sentence pair tasks.\nWith rapid progress in model development, the models\nachieved good performance in the GLUE benchmark\nresulting in little space for further improvement [261].\n29\nBenchmark Type Category Language Public\nLeader-\nboard\nDiagnostic\nDataset\nDetails\nGLUE [260] NLU General English \u0013 \u0013 Five NLU tasks (TC, SA, STS, PI, and\nNLI) with nine datasets and one diag-\nnostic dataset.\nSuperGLUE\n[261]\nNLU General English \u0013 \u0013 Five NLU tasks (QA, WSD, NLI, and\nCoref) with eight datasets and two di-\nagnostic datasets.\nGENIE [262] NLG General English \u0013 \u0017 Four NLG tasks (MT, TS, MRC, and\nCSR).\nGEM [263] NLG General English \u0013 \u0017 Four NLG tasks (Data2text, TS, TSim,\nand Dialog) with thirteen data sets.\nGLGE [264] NLG General English \u0013 \u0017 Eight language generation tasks, in-\ncluding Abstractive TS, Answer-aware\nQuestion Generation, Conversational\nQA, and Personalizing Dialogue.\nTweetEval\n[111]\nNLU Social-media English \u0013 \u0017 Seven tweets related tasks, and all are\nframed as multi-class tweet classiﬁca-\ntion.\nUMSAB [112] XLU Social-media Cross-\nlingual\n\u0013 \u0017 Cross-lingual sentiment analysis for\neight different languages and all are\nframed as tweet classiﬁcation with three\nlabels (positive, negative, and neutral).\nCodeXGLUE\n[35]\nPLU and\nPLG\nDomain-speciﬁc Programming \u0013 \u0017 Ten programming language under-\nstanding and generation tasks with\nfourteen datasets.\nDialoGLUE\n[265]\nNLU Domain-speciﬁc\n(Dialogue)\nEnglish \u0013 \u0017 Four NLU tasks (Intent Prediction, Slot-\nﬁlling, Semantic parsing, and Dialogue\nstate tracking) with seven task-oriented\ndialogue datasets.\nBLUE [48] NLU Domain-speciﬁc\n(biomedical)\nEnglish \u0013 \u0017 Five tasks (STS, NER, RE, NLI and\nDC) with ten datasets that cover both\nbiomedical and clinical texts with dif-\nferent dataset sizes and difﬁculties.\nBLURB [47] NLU Domain-speciﬁc\n(biomedical)\nEnglish \u0013 \u0017 Thirteen biomedical NLP datasets in 6\ntasks (NER, PICO, RE, STS, DC, and\nQA).\nCBLUE [266] NLU Domain-speciﬁc\n(biomedical)\nChinese \u0013 \u0017 Includes tasks like NER, PI, QA, IR, IC,\nand ToC.\nTABLE 9. Summary of general, social-media and domain-speciﬁc Extrinsic Benchmarks. TC - Text Classiﬁcation, STS- Semantic\nText Similarity, TS - Text Summarization, TSim – Text Simpliﬁcation, ToC – Topic Classiﬁcation, IC – Intent Classiﬁcation, IR\n– Information Retrieval, QA- Question Answering, DC- Document Classiﬁcation, RE – Relation Extraction, POS - Parts-of-\nspeech tagging, DP - Dependency Parsing, MRC - Machine Reading Comprehension, SA – Sentiment Analysis, PI- Paraphrase\nIdentiﬁcation, NLI – Natural Language Inference, WPR – Web Page Ranking, QAM – Question Matching, QADSM – Query\nAd Matching, MT- Machine Translation, LID – Language Identiﬁcation, CSR – Common Sense Reasoning, WSD – Word Sense\nDisambiguation, MCQA – Multiple Choice Question Answering.\nTo have a more challenging benchmark, the SuperGLUE\nbenchmark is introduced with more challenging tasks\nlike QA, word sense disambiguation (WSD), and coref-\nerence resolution while retaining the two difﬁcult tasks\nfrom GLUE benchmark.\nInspired by the success of GLUE and SuperGLUE\nbenchmarks in the general English domain, benchmarks\nlike GENIE [262], GEM [263], GLGE [264] have been in-\ntroduced to evaluate NLG models in the general English\ndomain. To evaluate cross-lingual models, XGLUE [267]\nand XTREME [270] benchmarks have been introduced.\nXTREME benchmark includes only XNLU tasks while\nthe XGLUE benchmark includes both XNLU and XNLG\ntasks. Moreover, the XGLUE benchmark includes diverse\ndatasets related to search, ads, and news scenarios which\nmakes it more challenging and practical. Recently, as\nthere is less room for improvement in the XTREME\nbenchmark with existing achieving improvements by\nalmost 13 points, Ruder et al. [271] extended XTREME\nto XTREME-R which consists of ten challenging NLU\ntasks. Moreover, XTREME covers only forty languages\nwhile XTREME-R covers 50 languages.\nTo evaluate social media-based T-PTLMs, we have\nbenchmarks like TweetEval [111] and UMSAB [112].\nTweetEval includes datasets from English only while\nUMSAB includes datasets from eight languages includ-\ning English. In both the benchmarks, all the tasks are\nframed as tweet classiﬁcation. Apart from XGLUE and\n30\nBenchmark Type Category Language Public\nLeader-\nboard\nDiagnostic\nDataset\nDetails\nXGLUE [267] XLU and\nXLG\nLanguage-based Cross-\nlingual\n\u0013 \u0017 Eleven tasks in which nine tasks are\nXNLU (NER, POS, QA, NLI, PI, WPR,\nQAM, QADSM and TC) and two tasks\nare XNLG (question and news title gen-\neration). This benchmark covers 19 lan-\nguages.\nLinCE [268] NLU and\nNLG\nLanguage-based Code-\nSwitching\n\u0013 \u0017 Five tasks (MT, LID, NER, POS, and SA)\nwith eighteen datasets covering nine\ndifferent code-switched language pairs.\nGLUECoS\n[269]\nNLU Language-based Code-\nSwitching\n\u0013 \u0017 Eleven datasets covering six tasks (LID,\nPOS, NER, SA, QA and NLI) and\ntwo language pairs (English-Hindi and\nEnglish-Spanish).\nXTREME\n[270]\nXLU Language-based Cross-\nlingual\n\u0013 \u0017 Nine tasks spanning forty typologically\ndiverse languages from 12 language\nfamilies.\nXTREME-R\n[271]\nXLU Language-based Cross-\nlingual\n\u0013 \u0013 Includes ten challenging NLU tasks for\n50 languages.\nRussianSuper\nGLUE [272]\nNLU Language-based Russian \u0013 \u0017 Nine Russian NLU tasks.\nIndicGLUE\n[98]\nNLU Language-based Indian lan-\nguages\n\u0013 \u0017 Ten tasks covering multiple Indian lan-\nguages.\nCLUE [273] NLU Language-based Chinese \u0013 \u0013 Nine language understanding tasks in\nChinese and diagnostic dataset for lin-\nguistic analysis.\nIndoNLU [92] NLU Language-based Indonesian \u0013 \u0017 Twelve tasks clustered into four cate-\ngories: (a) single-sentence classiﬁcation,\n(b) single-sentence sequence tagging,\n(c) sentence-pair classiﬁcation, and (d)\nsentence-pair sequence labelling.\nIndoNLG [88] NLG Language-based Indonesian \u0017 \u0017 Six commonly used NLG tasks: TS, QA,\nChitchat, and three different pairs of\nmachine translation (MT) tasks.\nIndoLEM\n[136]\nNLU Language-based Indonesian \u0013 \u0017 Seven tasks for the Indonesian lan-\nguage spanning Morpho-syntax and Se-\nquence labelling, Semantics, and Dis-\ncourse with eight datasets.\nFLUE [140] NLU and\nXLU\nLanguage-based French \u0017 \u0017 Six tasks (TC, PI, NLI, WSD, DP , and\nPOS). Three out of six tasks (TC, PI, and\nNLI) are from cross-lingual datasets.\nArBench [135] NLU Language-based Arabic \u0013 \u0017 Six different Arabic language under-\nstanding tasks (SA, Social meaning\ntasks, ToC, Dialect identiﬁcation, NER,\nand QA) with 42 datasets.\nKLEJ [151] NLU Language-based Polish \u0013 \u0017 Seven tasks (NER, Semantic relatedness,\nQA, TE, SA, and Cyberbully detection)\nwith 9 datasets.\nKLUE [152] NLU Language-based Korean \u0013 \u0017 Eight Korean natural language under-\nstanding tasks, including ToC, STS, NLI,\nNER, RE, DP , MRC, and Dialogue state\ntracking.\nGLUES [147] NLU Language-based Spanish \u0017 \u0017 Includes tasks like NLI, PI, NER, POS,\nDC, DP and QA.\nParsiNLU\n[274]\nNLU Language-based Persian \u0013 \u0017 Includes six NLU tasks like TE, PI, SA,\nMT, MRC, MCQA.\nTABLE 10. Summary of language-based extrinsic benchmarks. TC - Text Classiﬁcation, STS- Semantic Text Similarity, TS - Text\nSummarization, TSim – Text Simpliﬁcation, ToC – Topic Classiﬁcation, IC – Intent Classiﬁcation, IR – Information Retrieval, QA-\nQuestion Answering, DC- Document Classiﬁcation, RE – Relation Extraction, POS - Parts-of-speech tagging, DP - Dependency\nParsing, MRC - Machine Reading Comprehension, SA – Sentiment Analysis, PI- Paraphrase Identiﬁcation, NLI – Natural\nLanguage Inference, WPR – Web Page Ranking, QAM – Question Matching, QADSM – Query Ad Matching, MT- Machine\nTranslation, LID – Language Identiﬁcation, CSR – Common Sense Reasoning, WSD – Word Sense Disambiguation, MCQA –\nMultiple Choice Question Answering.\n31\nXTREME which evaluate cross-lingual models, we have\nseparate benchmarks in each language like Russian (Rus-\nsianSuperGLUE [272]), Indian (IndicGLUE [98]), Chinese\n(CLUE [273]), Indonesian (IndoNLU [92], IndoNLG [88],\nIndoLEM [136]), French (FLUE [140]), Arabic (ArLUE\n[135]), Polish (KLEJ [151]), Korean (KLUE [152]) Spanish\n(GLUES [147]), and Persian (ParsiNLU [274]) to eval-\nuate monolingual language models. Besides, we have\nbenchmarks like GLUECoS [269] and LinCE [268] for\nCodeSwitching, BLUE [48], BLURB [47] and Chinese-\nBLUE [266] in the Biomedical domain, CodeXGLUE [35]\nin the Code intelligence domain and DialogGLUE [265]\nto evaluate Dialog models. Further, we have benchmarks\nlike FewCLUE [275], FLEX [276], and FewGLUE [277] to\nevaluate T-PTLMs under few shot settings.\n7 U SEFUL LIBRARIES\nWe present a summary of popular libraries to work with\ntransformer-based PTLMs. Libraries like Transformers\n[278] and Fairseq [279] are useful for model training and\nevaluation. Some of the libraries like SimpleTransform-\ners, HappyTransformer, AdaptNLP which are built on\nthe top of Transformers library make the model training\nand evaluation easier with just a few lines of code.\nLibraries like FastSeq [280], DeepSpeed [281], FastT5,\nOnnxT5 and LightSeq [282] are useful to increase the in-\nference speed of models. Ecco, BertViz [283], and exBERT\n[284] are visual analysis tools to explore the layers of\ntransformer models while Transformers-interpret and\nCaptum help to explain the model decisions.\n8 D ISCUSSIONS AND FUTURE DIRECTIONS\n8.1 Better Pretraining Methods\nIt is highly expensive to pretrain a model especially\nlarge-scale models with billions or trillions of parameters\nusing SSL only. Novel pretrained methods like Knowl-\nedge Inherited Pretraining (KIPT) involve both SSL and\nKnowledge Distillation [72]. SSL allows the model to\nlearn the knowledge available in pretraining corpus\nwhile KD allows the model to learn the knowledge\nalready encoded in existing pretrained models. Due to\nthe additional knowledge gained by the model during\npretraining through KD, a) the model converges faster\nand hence reduces the pretraining time b) the model\nperforms better in downstream tasks compared to the\nmodels pretrained using SSL only [72]. The research\ncommunity must focus more on developing better pre-\ntraining methods like KIPT which allow the model to\ngain more knowledge as well as reduce the pretraining\ntime.\n8.2 Sample Efﬁcient Pretraining Tasks\nA pretraining task is sample efﬁcient if it makes maxi-\nmum out of each train instance i.e., it should be deﬁned\nover all the tokens in the training instance. Sample efﬁ-\ncient pretraining tasks make pretraining more compute\nefﬁcient [5]. MLM, the most commonly used pretraining\ntask is less sample efﬁcient as it involves only a subset\nof tokens i.e., masked tokens which amount to 15% of\ntotal tokens [2], [5]. Pretraining tasks like RTD [5], RTS\n[56], and STD [55] can be considered as early attempts\nto develop sample-efﬁcient pretraining tasks. All these\nthree pretraining tasks are deﬁned over all the tokens\nin each training instance i.e., they involve identifying\nwhether each token is replaced [5], randomly substituted\n[56], or shufﬂed [55] or not. We can expect more sample\nefﬁcient pretraining tasks which make pretraining more\ncompute efﬁcient.\n8.3 Efﬁcient Models\nPretraining T-PLMs is highly expensive due to the large\nmodel size and also there is a requirement of large\nvolumes of unlabelled text data. However long pre-\ntraining times are not environmentally friendly due to\nCO2 emission and the availability of large volumes of\nunlabelled text data is not possible in all the domains\nlike Biomedical. Recently, models like DeBERTa [212]\nwith novel improvements to BERT model achieve better\nperformance than RoBERTa model even though it is\npretrained using just 78GB of data which is just half\nof the data used to pretrain RoBERTa model. Similarly,\nConvBERT [213] with a novel mixed attention module\noutperforms ELECTRA model using just ¼ of its pre-\ntraining cost. There is a great need for efﬁcient models\nlike DeBERTa and ConvBERT to reduce the amount of\npretraining data as well as the pretraining costs.\n8.4 Better Position Encoding Mechanisms\nThe self-attention mechanism is permutation invariant\nwithout position bias. The position bias can be pro-\nvided using absolute or relative position embeddings.\nMoreover, absolute position embeddings can be prede-\ntermined or learned. However, there are drawbacks to\nboth these approaches [288]. Absolute position embed-\ndings suffer from generalization issues but are easy to\nimplement. Unlike absolute positions, relative position\nembeddings are robust to sequence length changes but\ndifﬁcult to implement and yield less performance. There\nis a great need for more novel position encoding mech-\nanisms like CAPE [288] which combines the advantages\nin both absolute and relative position embeddings.\n8.5 Improving existing T-PTLMs\nT-PTLMs like BERT and RoBERTa have achieved good\nresults in many of the NLP tasks. Recent research works\nshowed that these models can be further improved\nby injecting sentence-level semantics through contin-\nual pretraining based on adversarial [55] or contrastive\npretraining tasks [157], [160] . For example, Panda et\nal. [55] showed that continual pretraining using shuf-\nﬂed token detection objective improves RoBERTa model\nperformance in GLUE tasks by allowing the model to\n32\nLibrary Purpose Description Framework Link\nTransformers [278] Training and\nInference\nState-of-the-art library for transformer\nbased PTLMs.\nPytorch,\nTensorﬂow\nand Jax\nhttps://github.com/huggingface/\ntransformers\nSimpleTransformers Training and\nInference\nBuilt on the top of transformers and\nlets you to quickly train and evaluate\nmodels.\nPyTorch https://github.com/ThilinaRajapakse/\nsimpletransformers\nHappyTransformer Training and\nInference\nBuilt on the top of transformers and\nmakes the use of state-of-the-art models\neasy.\nPyTorch https://github.com/EricFillion/\nhappy-transformer\nFairSeq [279] Training and\nInference\nLibrary to train custom models for\ntranslation, summarization, language\nmodeling and other text generation\ntasks.\nPyTorch https://github.com/pytorch/fairseq\nAdaptNLP Training and\nInference\nBuilt on the top of Flair and Transform-\ners library and makes the use of state-\nof-the-art models easy.\nPyTorch https://github.com/Novetta/adaptnlp\nSimpleT5 Training and\nInference\nBuilt on top of PyTorch-lightning and\nTransformers that lets you quickly train\nyour T5 models.\nPyTorch-\nLightning\nhttps://github.com/Shivanandroy/\nsimpleT5\nSpacyTransformers All NLP tasks spaCy pipelines for pretrained BERT,\nRoBERTa XLNet, GPT-2 etc.\nPyTorch https://github.com/explosion/\nspacy-transformers\nTextBox Text Generation Library for building text generation\nsystems based on models like GPT-2,\nBART, T5 etc.\nPyTorch https://github.com/RUCAIBox/\nTextBox\nTrankit Multilingual\nNLP\nLight-Weight Transformer-based\nPython Toolkit for Multilingual Natural\nLanguage Processing and is built on\nthe top of transformers library.\nPyTorch https://github.com/nlp-uoregon/\ntrankit\nHaystack Information Re-\ntrieval\nLibrary to build powerful and\nproduction-ready pipelines for different\nsearch use cases.\nPyTorch https://github.com/deepset-ai/\nhaystack\nEasyNMT Machine Trans-\nlation\nEasy to use, state-of-the-art Neural Ma-\nchine Translation library for 100+ lan-\nguages.\nPyTorch https://github.com/UKPLab/\nEasyNMT\nAitextgen Text Generation Library for training and generation us-\ning OpenAI’s GPT-2 and EleutherAI’s\nGPT Neo/GPT-3 architecture.\nPyTorch-\nLightning\nhttps://github.com/minimaxir/\naitextgen\nDl-Translate Machine Trans-\nlation\nDeep Learning-based translation library\nbuilt on Huggingface transformers.\nPyTorch https://github.com/xhlulu/\ndl-translate\nFastSeq [280] Fast Inference Efﬁcient implementation of the popu-\nlar sequence models for text generation,\nsummarization, and translation tasks.\nPyTorch https://github.com/microsoft/fastseq\nLightSeq [282] Fast Inference High performance training and infer-\nence library for sequence processing\nand generation.\nPyTorch,\nTensorﬂow\nhttps://github.com/bytedance/\nlightseq\nTurboTransformers\n[285]\nFast Inference A library open source by WeChat AI\nto get fast inference using transformer\nmodels.\nPyTorch https://github.com/Tencent/\nTurboTransformers\nEET Fast Inference PyTorch library to make transformer\nmodels inference faster.\nPyTorch https://github.com/NetEase-FuXi/\nEET\nDeepSpeed [281] Distributed\nModel Training\nDeep learning optimization library that\nmakes distributed training easy, efﬁ-\ncient, and effective.\nPyTorch https://github.com/microsoft/\nDeepSpeed\nFastT5 Fast Inference Reduce T5 model size by 3X and in-\ncrease the inference speed up to 5X.\nPyTorch https://github.com/topics/fastt5\nOnnxT5 Fast Inference Fast Inference of T5 model. PyTorch https://github.com/abelriboulot/\nonnxt5\nexBERT [284] Visualization Library to explore the learned attention\nweights and contextual representations.\nPyTorch https://github.com/bhoov/exbert\nBertViz [283] Visualizaation Library to visualize attention in the\nTransformer model.\nPyTorch https://github.com/jessevig/bertviz\nTransfomers-\ninterpret\nModel Interpre-\ntation\nLibrary to explain the decision of trans-\nformer models.\nPyTorch https://github.com/cdpierse/\ntransformers-interpret\nEcco Visualization Library to visualize and explore NLP\nlanguage models.\nPyTorch https://github.com/jalammar/ecco\nCaptum Model Interpre-\ntation\nPyTorch interpretation library. PyTorch https://github.com/pytorch/captum\nTextBrewer [286] Model\nCompression\nSupports Knowledge distillation meth-\nods.\nPyTorch https://github.com/airaria/\nTextBrewer\nKD-Lib [287] Model\nCompression\nLibrary to develop compact model us-\ning model compression techniques like\nquantization, pruning and knowledge\ndistillation.\nPyTorch https://github.com/SforAiDl/KD Lib\nParallelformers Model\nParallelization\nAn Efﬁcient Model Parallelization\nToolkit for Deployment (inference).\nPyTorch https://github.com/tunib-ai/\nparallelformers\nTABLE 11. Useful Libraries to work with T-PTLMs\n33\nlearn more coherent sentence representations. Similarly,\ncontinual pretraining using contrastive pretraining ob-\njectives improves the performance of T-PTLMs in GLUE\ntasks [157] and multilingual T-PTLMs in Mickey Probe\n[160]. Further research is required to extend this to other\nmonolingual and domain-speciﬁc T-PTLMs.\n8.6 Beyond Vanilla Fine-tuning\nFine-tuning is the most commonly used method to\nadapt pretrained models to downstream tasks. However,\nthe main drawback with vanilla ﬁne-tuning is that it\nmakes changes to all the layers in the pretrained model,\nand hence it requires maintaining a separate copy for\neach task which makes deployment expensive. Methods\nlike Adapters [234] and Pruning-based tuning [166] are\nproposed to adapt pretrained models to downstream\ntasks in a parameter-efﬁcient way. For example, adapters\nare small task-speciﬁc layers added to each transformer\nlayer and during downstream task adaptation, only\nadapter layer parameters are updated while keeping the\ntransformer layer parameter ﬁxed. Moreover, Poth et al.\n[220] showed that adapters are useful for intermediate\nﬁne-tuning also. Recently prompt-based tuning methods\n(discrete – [27], [242], [244] and continuous – [246], [248])\nhave attracted the research community with much better\nparameter efﬁciency. For example, prompt-based tuning\nmethods like Preﬁx-tuning [246] require only 0.1% of\ntask-speciﬁc parameters while adapter-based ﬁne-tuning\ninvolves 3% of task-speciﬁc parameters [234].\n8.7 Benchmarks\nIn the last four layers many benchmarks have been\nintroduced to evaluate the progress in pretrained models\nin general [260]–[264] as well as in speciﬁc domains\n[35], [47], [48], [111], [265], [266]. Apart from English,\nbenchmarks are introduced to evaluate the progress in\nother monolingual [88], [92], [135], [136], [140], [147],\n[151], [152], [272]–[274] as well as multilingual models\n[112], [267], [270]. However, the existing benchmarks are\nnot sufﬁcient to cover all the scenarios. For example,\nthere are no benchmarks to evaluate a) the progress\nin compact pretrained models b) the robustness of pre-\ntrained models c) PTLMs speciﬁc to social media as\nwell as speciﬁc to other domains like Academic. Re-\ncently, leaderboards like Explainboard [289] which not\nonly evaluate the progress using a single metric like\nexisting benchmarks but also dig deeper by analyzing\nthe strengths and weaknesses of models are introduced.\nThis kind of leaderboard should be extended to other do-\nmains also. Moreover, benchmarks like FewGLUE [277],\nFLEX [276], and FewCLUE [275] which evaluate few-\nshot learning techniques should be extended to other\nlanguages and domains also.\n8.8 Compact Models\nTransformer-based PTLMs achieved state-of-the-art re-\nsults in almost every NLP task. However, these models\nare large which requires more amount of storage space.\nAs these models have many layers through which the\ninput has to pass through to get the model predic-\ntion, latency is high [162]. As real-world applications\nare resource-constrained and require less latency, model\ncompression methods like pruning, quantization, knowl-\nedge distillation, parameter sharing, and factorization\nare explored to develop compact models in English for\ngeneral domain applications [162], [290]. There is a great\nneed to explore these model compression methods to\ndevelop compact models for other languages as well as\nfor other domains also.\n8.9 Robustness to Noise\nTransformer-based PTLMs are brittle to noise which\nincludes both adversarial and natural noise [291], [292]\n. The main reason behind this is the use of sub-word\nembeddings. In the case of sub-word embeddings, even\na small typo error can change the overall representation\nof the word by breaking the word into many sub-word\ntokens which hinders model learning and impact the\nmodel predictions [66], [121]. To increase the robustness\nof PTLMs to noise, models like CharacterBERT [66]\nuse character embeddings only while models like Char-\nBERT [121] use character embeddings along with sub-\nword embeddings. Both these approaches improved the\nrobustness to noise. Recently, tokenization-free models\nlike CANINE [67], ByT5 [68], and Charformer [69] are\nproposed which further improve robustness to noise.\nThere is a need for more robust models to increase the\nuse of PTLMs in real-world applications especially in\nsensitive domains like Medicine.\n8.10 Novel Adaptation Methods\nThe commonly used strategy to adapt general models to\nspeciﬁc domains like biomedical or multilingual models\nto speciﬁc languages is continual pretraining [45], [46],\n[48]. Although this approach achieves good results by\nadapting the model to a speciﬁc domain or language, the\nlack of domain or language-speciﬁc vocabulary hurts the\nmodel downstream performance. Recently researchers\nproposed methods like vocabulary expansion [184], vo-\ncabulary expansion and then continual pretraining [185],\n[293]. These methods overcome the issue of OOV words\nbut increase the size of vocabulary due to the addition\nof new terms in the vocabulary. Recently, Yao et al.\n[78] proposed the Adapt and Distill approach to adapt\ngeneral models to a speciﬁc domain using vocabulary\nexpansion and knowledge distillation. Different from ex-\nisting adaptation methods, this approach not only adapts\ngeneral models to speciﬁc domain but also reduces the\nsize of the model. Further research on this topic will\nresult in more novel adaption methods.\n8.11 Privacy Issues\nTransformer-based PTLMs achieved impressive results\nin many of the NLP tasks. However, there are some\n34\nunexpected as well as unwanted risks associated with\nthese models. For example, data leakage from these\nmodels is of primary concern especially when the model\nis pretrained over private data. As the model is pre-\ntrained over a large amount of text data, it is possible to\nrecover sensitive data including personally identiﬁable\ninformation [294]–[297]. This prevents the public release\nof models pretrained on private data. Recently, Carlini\net al. [295] showed that GPT-2 model generates the\nentire postal address of a person which is included in\ntraining data when prompted with the person’s name.\nRecently frameworks like KART [294] are introduced in\nthe Biomedical domain which performs various attacks\nto assess data leakage. There is a great need to develop\nmore sophisticated attacks to assess data leakage and\nalso methods to prevent leakage of sensitive data from\npretrained models.\n8.12 Mitigating Bias\nDeep learning-based models are increasingly used in\nmany real-world applications including speciﬁc domains\nlike Biomedical [298] and Legal [299]. However, these\nmodels are prone to learn and amplify the bias already\npresent in training data. As a result, the decisions from\nthese models are biased i.e., may favor a particular race,\ngender, or aged people. This behavior is completely\nundesirable. Some of the recent works focused on iden-\ntifying and mitigating bias. For example, Minot et al.\n[182] proposed a data augmentation-based approach to\nreduce gender bias while Liang et al. [300] proposed\nA-INLP approach which dynamically identiﬁes bias-\nsensitive tokens. Further research in this area helps to\nmitigate bias in pretrained models and help them to\nmake fair decisions.\n8.13 Mitigating Fine-Tuning Instabilities\nFine-tuning is the most widely adopted approach to\nadapt PTLMs to the downstream task. Though ﬁne-\ntuning achieves good performance, it is unstable i.e.,\nﬁne-tuning the model with different random seeds re-\nsults in the large variance of downstream performance.\nIt is believed that catastrophic forgetting and the small\nsize of datasets are possible reasons for ﬁne-tuning\ninstabilities [2], [301], [302]. However, Mosbach et al.\n[303] showed that ﬁne-tuning instability is not caused\nby any of these two and further showed that ﬁne-tuning\ninstability can be attributed to a) optimization difﬁculties\nwhich lead to vanishing gradients and b) generalization\nissues. The possible solutions to mitigate ﬁne-tuning\ninstability are a) intermediate ﬁne-tuning [214] b) mix-\nout [301] c) smaller learning rates in early epochs and\nﬁne-tuning the model for more number of epochs [303]\nand d) use of supervised contrastive loss along with\ncross-entropy loss [304]. Further work related to this will\nmake ﬁne-tuning more stable.\n9 C ONCLUSION\nIn this survey paper, we present a comprehensive review\nof recent research works in transformer-based pretrained\nlanguage models. This paper covers various pretraining\nmethods, pretraining tasks, embeddings, downstream\nadaptation methods, intrinsic and extrinsic benchmarks,\nuseful libraries to work with T-PTLMs. We also present\na new taxonomy to categorize various T-PTLMs. We\ndiscuss various future research directions which will\ndirect the research community to further improve T-\nPTLMs.\nACKNOWLEDGMENTS\nKalyan would like to thank his father Katikapalli Sub-\nramanyam for giving a) $750 to buy a new laptop, 24-\ninch monitor and study table. b) $180 for one year sub-\nscription of Medium, Overleaf and Edraw MindMaster\nsoftware. Edraw MindMaster is used to create all the\ndiagrams in the paper.\nREFERENCES\n[1] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Im-\nproving language understanding by generative pre-training,”\n2018.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language un-\nderstanding,” in Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) ,\n2019, pp. 4171–4186.\n[3] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov,\nand Q. V . Le, “Xlnet: Generalized autoregressive pretraining\nfor language understanding,” Advances in Neural Information\nProcessing Systems, vol. 32, pp. 5753–5763, 2019.\n[4] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[5] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-\ntraining text encoders as discriminators rather than generators,”\nin International Conference on Learning Representations , 2019.\n[6] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer,” arXiv preprint\narXiv:1910.10683, 2019.\n[7] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and\nR. Soricut, “Albert: A lite bert for self-supervised learning of\nlanguage representations,” in International Conference on Learning\nRepresentations, 2019.\n[8] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy, V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising\nsequence-to-sequence pre-training for natural language gener-\nation, translation, and comprehension,” in Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics ,\n2020, pp. 7871–7880.\n[9] J. Zhang, Y. Zhao, M. Saleh, and P . Liu, “Pegasus: Pre-training\nwith extracted gap-sentences for abstractive summarization,” in\nInternational Conference on Machine Learning . PMLR, 2020, pp.\n11 328–11 339.\n[10] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient esti-\nmation of word representations in vector space,” arXiv preprint\narXiv:1301.3781, 2013.\n[11] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vec-\ntors for word representation,” in Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP) , 2014,\npp. 1532–1543.\n35\n[12] N. Kalchbrenner, E. Grefenstette, and P . Blunsom, “A convolu-\ntional neural network for modelling sentences,” in Proceedings\nof the 52nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , 2014, pp. 655–665.\n[13] P . Liu, X. Qiu, and X. Huang, “Recurrent neural network for\ntext classiﬁcation with multi-task learning,” in Proceedings of the\nTwenty-Fifth International Joint Conference on Artiﬁcial Intelligence ,\n2016, pp. 2873–2879.\n[14] P . Zhou, Z. Qi, S. Zheng, J. Xu, H. Bao, and B. Xu, “Text clas-\nsiﬁcation improved by integrating bidirectional lstm with two-\ndimensional max pooling,” in Proceedings of COLING 2016, the\n26th International Conference on Computational Linguistics: Technical\nPapers, 2016, pp. 3485–3495.\n[15] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE\nTransactions on knowledge and data engineering , vol. 22, no. 10, pp.\n1345–1359, 2009.\n[16] K. Simonyan and A. Zisserman, “Very deep convolutional\nnetworks for large-scale image recognition,” arXiv preprint\narXiv:1409.1556, 2014.\n[17] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n“Rethinking the inception architecture for computer vision,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 2818–2826.\n[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2016, pp. 770–778.\n[19] M. Tan and Q. Le, “Efﬁcientnet: Rethinking model scaling for\nconvolutional neural networks,” in International Conference on\nMachine Learning. PMLR, 2019, pp. 6105–6114.\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-\nﬁcation with deep convolutional neural networks,” Advances in\nneural information processing systems, vol. 25, pp. 1097–1105, 2012.\n[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge scale visual recognition challenge,” International journal of\ncomputer vision, vol. 115, no. 3, pp. 211–252, 2015.\n[22] T. Kaur and T. K. Gandhi, “Automated brain image classiﬁcation\nbased on vgg-16 and transfer learning,” in 2019 International\nConference on Information Technology (ICIT) . IEEE, 2019, pp. 94–\n98.\n[23] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards\nreal-time object detection with region proposal networks,” Ad-\nvances in neural information processing systems , vol. 28, pp. 91–99,\n2015.\n[24] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained\nmodels for natural language processing: A survey,” Science China\nTechnological Sciences, pp. 1–26, 2020.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems , 2017, pp.\n5998–6008.\n[26] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei,\n“Scaling laws for neural language models,” arXiv preprint\narXiv:2001.08361, 2020.\n[27] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP . Dhariwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell\net al. , “Language models are few-shot learners,” arXiv preprint\narXiv:2005.14165, 2020.\n[28] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang,\nZ. Yang, K. Wang, X. Zhang et al. , “Pangu: Large-scale autore-\ngressive pretrained chinese language models with auto-parallel\ncomputation,” arXiv preprint arXiv:2104.12369 , 2021.\n[29] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang,\nM. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant\nmodels with conditional computation and automatic sharding,”\narXiv preprint arXiv:2006.16668 , 2020.\n[30] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\nto trillion parameter models with simple and efﬁcient sparsity,”\narXiv preprint arXiv:2101.03961 , 2021.\n[31] Y. Yang, M. C. S. Uy, and A. Huang, “Finbert: A pretrained\nlanguage model for ﬁnancial communications,” arXiv preprint\narXiv:2006.08097, 2020.\n[32] S. Leivaditi, J. Rossi, and E. Kanoulas, “A benchmark for lease\ncontract review,” arXiv preprint arXiv:2010.10386 , 2020.\n[33] I. Chalkidis, M. Fergadiotis, P . Malakasiotis, N. Aletras, and\nI. Androutsopoulos, “Legal-bert: The muppets straight out of\nlaw school,” arXiv preprint arXiv:2010.02559 , 2020.\n[34] S. Gururangan, A. Marasovi ´c, S. Swayamdipta, K. Lo, I. Beltagy,\nD. Downey, and N. A. Smith, “Don’t stop pretraining: Adapt\nlanguage models to domains and tasks,” in Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics ,\n2020, pp. 8342–8360.\n[35] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,\nC. Clement, D. Drain, D. Jiang, D. Tang et al. , “Codexglue: A\nmachine learning benchmark dataset for code understanding\nand generation,” arXiv preprint arXiv:2102.04664 , 2021.\n[36] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,\nB. Qin, T. Liu, D. Jiang et al., “Codebert: A pre-trained model for\nprogramming and natural languages,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 1536–1547.\n[37] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Uniﬁed\npre-training for program understanding and generation,” in\nProceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, 2021, pp. 2655–2668.\n[38] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,\nA. Svyatkovskiy, S. Fu et al., “Graphcodebert: Pre-training code\nrepresentations with data ﬂow,” arXiv preprint arXiv:2009.08366 ,\n2020.\n[39] L. Phan, H. Tran, D. Le, H. Nguyen, J. Anibal, A. Peltekian, and\nY. Ye, “Cotext: Multi-task learning with code-text transformer,”\narXiv preprint arXiv:2105.08645 , 2021.\n[40] C.-S. Wu, S. C. Hoi, R. Socher, and C. Xiong, “Tod-bert: Pre-\ntrained natural language understanding for task-oriented dia-\nlogue,” in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , 2020, pp. 917–929.\n[41] A. Louis, “Netbert: A pre-trained language representation model\nfor computer networking,” Ph.D. dissertation, Cisco Systems,\n2020.\n[42] X. Liu, D. Yin, X. Zhang, K. Su, K. Wu, H. Yang, and J. Tang,\n“Oag-bert: Pre-train heterogeneous entity-augmented academic\nlanguage models,” arXiv preprint arXiv:2103.02410 , 2021.\n[43] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained language\nmodel for scientiﬁc text,” in Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), 2019, pp. 3606–3611.\n[44] S. Peng, K. Yuan, L. Gao, and Z. Tang, “Mathbert: A pre-trained\nmodel for mathematical formula understanding,” arXiv preprint\narXiv:2105.00377, 2021.\n[45] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and\nJ. Kang, “Biobert: a pre-trained biomedical language represen-\ntation model for biomedical text mining,” Bioinformatics, vol. 36,\nno. 4, pp. 1234–1240, 2020.\n[46] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Nau-\nmann, and M. McDermott, “Publicly available clinical bert em-\nbeddings,” in Proceedings of the 2nd Clinical Natural Language\nProcessing Workshop, 2019, pp. 72–78.\n[47] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu,\nT. Naumann, J. Gao, and H. Poon, “Domain-speciﬁc language\nmodel pretraining for biomedical natural language processing,”\narXiv preprint arXiv:2007.15779 , 2020.\n[48] Y. Peng, S. Yan, and Z. Lu, “Transfer learning in biomedical\nnatural language processing: An evaluation of bert and elmo\non ten benchmarking datasets,” in Proceedings of the 18th BioNLP\nWorkshop and Shared Task , 2019, pp. 58–65.\n[49] X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and\nJ. Tang, “Self-supervised learning: Generative or contrastive,”\narXiv preprint arXiv:2006.08218 , vol. 1, no. 2, 2020.\n[50] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representa-\ntions,” Advances in Neural Information Processing Systems , vol. 33,\n2020.\n[51] A. Sivaraman and M. Kim, “Self-supervised learning from con-\ntrastive mixtures for personalized speech enhancement,” arXiv\npreprint arXiv:2011.03426, 2020.\n[52] Q. Liu, M. J. Kusner, and P . Blunsom, “A survey on contextual\nembeddings,” arXiv preprint arXiv:2003.07278 , 2020.\n36\n[53] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and\nM. Shah, “Transformers in vision: A survey,” arXiv preprint\narXiv:2101.01169, 2021.\n[54] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,\nA. Xiao, C. Xu, Y. Xu et al. , “A survey on visual transformer,”\narXiv preprint arXiv:2012.12556 , 2020.\n[55] S. Panda, A. Agrawal, J. Ha, and B. Bloch, “Shufﬂed-token\ndetection for reﬁning pre-trained roberta,” in Proceedings of the\n2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Student Research Workshop , 2021, pp.\n88–93.\n[56] L. Di Liello, M. Gabburo, and A. Moschitti, “Efﬁcient pre-training\nobjectives for transformers,” arXiv preprint arXiv:2104.09694 ,\n2021.\n[57] D. Erhan, A. Courville, Y. Bengio, and P . Vincent, “Why does\nunsupervised pre-training help deep learning?” in Proceedings\nof the thirteenth international conference on artiﬁcial intelligence and\nstatistics. JMLR Workshop and Conference Proceedings, 2010,\npp. 201–208.\n[58] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-\nBurch, and N. Carlini, “Deduplicating training data makes lan-\nguage models better,” arXiv preprint arXiv:2107.06499 , 2021.\n[59] Y. Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey et al., “Google’s neural\nmachine translation system: Bridging the gap between human\nand machine translation,” arXiv preprint arXiv:1609.08144 , 2016.\n[60] R. Sennrich, B. Haddow, and A. Birch, “Neural machine trans-\nlation of rare words with subword units,” in Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , 2016, pp. 1715–1725.\n[61] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, “Language models are unsupervised multitask\nlearners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[62] T. Kudo and J. Richardson, “Sentencepiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neu-\nral text processing,” in Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demon-\nstrations, 2018, pp. 66–71.\n[63] G. Lample and A. Conneau, “Cross-lingual language model\npretraining,” arXiv preprint arXiv:1901.07291 , 2019.\n[64] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wen-\nzek, F. Guzm ´an, ´E. Grave, M. Ott, L. Zettlemoyer, and V . Stoy-\nanov, “Unsupervised cross-lingual representation learning at\nscale,” in Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , 2020, pp. 8440–8451.\n[65] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad,\nM. Lewis, and L. Zettlemoyer, “Multilingual denoising pre-\ntraining for neural machine translation,” Transactions of the Asso-\nciation for Computational Linguistics , vol. 8, pp. 726–742, 2020.\n[66] H. El Boukkouri, O. Ferret, T. Lavergne, H. Noji, P . Zweigen-\nbaum, and J. Tsujii, “Characterbert: Reconciling elmo and bert for\nword-level open-vocabulary representations from characters,” in\nProceedings of the 28th International Conference on Computational\nLinguistics, 2020, pp. 6903–6915.\n[67] J. H. Clark, D. Garrette, I. Turc, and J. Wieting, “Canine: Pre-\ntraining an efﬁcient tokenization-free encoder for language rep-\nresentation,” arXiv preprint arXiv:2103.06874 , 2021.\n[68] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale,\nA. Roberts, and C. Raffel, “Byt5: Towards a token-free fu-\nture with pre-trained byte-to-byte models,” arXiv preprint\narXiv:2105.13626, 2021.\n[69] Y. Tay, V . Q. Tran, S. Ruder, J. Gupta, H. W. Chung, D. Bahri,\nZ. Qin, S. Baumgartner, C. Yu, and D. Metzler, “Charformer:\nFast character transformers via gradient-based subword tok-\nenization,” arXiv preprint arXiv:2106.12672 , 2021.\n[70] W. Wang, B. Bi, M. Yan, C. Wu, Z. Bao, J. Xia, L. Peng,\nand L. Si, “Structbert: Incorporating language structures into\npre-training for deep language understanding,” arXiv preprint\narXiv:1908.04577, 2019.\n[71] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and\nO. Levy, “Spanbert: Improving pre-training by representing and\npredicting spans,” Transactions of the Association for Computational\nLinguistics, vol. 8, pp. 64–77, 2020.\n[72] Y. Qin, Y. Lin, J. Yi, J. Zhang, X. Han, Z. Zhang, Y. Su, Z. Liu, P . Li,\nM. Sun et al., “Knowledge inheritance for pre-trained language\nmodels,” arXiv preprint arXiv:2105.13880 , 2021.\n[73] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao,\nF. Qi, J. Guan, P . Keet al., “Cpm-2: Large-scale cost-effective pre-\ntrained language models,” arXiv preprint arXiv:2106.10715 , 2021.\n[74] Y. Kuratov and M. Arkhipov, “Adaptation of deep bidirectional\nmultilingual transformers for russian language,” arXiv preprint\narXiv:1905.07213, 2019.\n[75] F. Souza, R. Nogueira, and R. Lotufo, “Bertimbau: Pretrained\nbert models for brazilian portuguese,” in Brazilian Conference on\nIntelligent Systems. Springer, 2020, pp. 403–417.\n[76] M. Arkhipov, M. Troﬁmova, Y. Kuratov, and A. Sorokin, “Tuning\nmultilingual transformers for language-speciﬁc named entity\nrecognition,” in Proceedings of the 7th Workshop on Balto-Slavic\nNatural Language Processing , 2019, pp. 89–93.\n[77] D. Carmo, M. Piau, I. Campiotti, R. Nogueira, and R. Lotufo,\n“Ptt5: Pretraining and validating the t5 model on brazilian\nportuguese data,” arXiv preprint arXiv:2008.09144 , 2020.\n[78] Y. Yao, S. Huang, W. Wang, L. Dong, and F. Wei, “Adapt-and-\ndistill: Developing small, fast and effective pretrained language\nmodels for domains,” arXiv preprint arXiv:2106.13474 , 2021.\n[79] S. Wada, T. Takeda, S. Manabe, S. Konishi, J. Kamohara, and\nY. Matsumura, “Pre-training technique to localize medical bert\nand enhance biomedical bert,” arXiv preprint arXiv:2005.07202 ,\n2020.\n[80] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli,\nX. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh, “Large batch\noptimization for deep learning: Training bert in 76 minutes,” in\nInternational Conference on Learning Representations , 2019.\n[81] T. Caselli, V . Basile, J. Mitrovi ´c, and M. Granitzer, “Hatebert:\nRetraining bert for abusive language detection in english,” arXiv\npreprint arXiv:2010.12472, 2020.\n[82] J. Ni, J. Li, and J. McAuley, “Justifying recommendations us-\ning distantly-labeled reviews and ﬁne-grained aspects,” in Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), 2019, pp. 188–197.\n[83] J. Zhou, J. Tian, R. Wang, Y. Wu, W. Xiao, and L. He, “Sentix: A\nsentiment-aware pre-trained model for cross-domain sentiment\nanalysis,” in Proceedings of the 28th International Conference on\nComputational Linguistics, 2020, pp. 568–579.\n[84] A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-Wei, M. Feng,\nM. Ghassemi, B. Moody, P . Szolovits, L. A. Celi, and R. G. Mark,\n“Mimic-iii, a freely accessible critical care database,” Scientiﬁc\ndata, vol. 3, no. 1, pp. 1–9, 2016.\n[85] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roes-\nner, and Y. Choi, “Defending against neural fake news,” in Pro-\nceedings of the 33rd International Conference on Neural Information\nProcessing Systems, 2019, pp. 9054–9065.\n[86] K. Lo, L. L. Wang, M. Neumann, R. Kinney, and D. S. Weld,\n“S2orc: The semantic scholar open research corpus,” in Proceed-\nings of the 58th Annual Meeting of the Association for Computational\nLinguistics, 2020, pp. 4969–4983.\n[87] W.-R. Chen, M. Abdul-Mageed, H. Cavusoglu et al. , “Indt5: A\ntext-to-text transformer for 10 indigenous languages,” in Pro-\nceedings of the First Workshop on Natural Language Processing for\nIndigenous Languages of the Americas , 2021, pp. 265–271.\n[88] S. Cahyawijaya, G. I. Winata, B. Wilie, K. Vincentio, X. Li,\nA. Kuncoro, S. Ruder, Z. Y. Lim, S. Bahar, M. L. Khodra et al. ,\n“Indonlg: Benchmark and resources for evaluating indonesian\nnatural language generation,” arXiv preprint arXiv:2104.08200 ,\n2021.\n[89] Z. Chi, S. Huang, L. Dong, S. Ma, S. Singhal, P . Bajaj, X. Song,\nand F. Wei, “Xlm-e: Cross-lingual language model pre-training\nvia electra,” arXiv preprint arXiv:2106.16138 , 2021.\n[90] Z. Chi, L. Dong, F. Wei, N. Yang, S. Singhal, W. Wang, X. Song,\nX.-L. Mao, H. Huang, and M. Zhou, “Infoxlm: An information-\ntheoretic framework for cross-lingual language model pre-\ntraining,” arXiv preprint arXiv:2007.07834 , 2020.\n[91] Z. Chi, L. Dong, S. Ma, S. H. X.-L. Mao, H. Huang, and\nF. Wei, “mt6: Multilingual pretrained text-to-text transformer\nwith translation pairs,” arXiv preprint arXiv:2104.08692 , 2021.\n[92] B. Wilie, K. Vincentio, G. I. Winata, S. Cahyawijaya, X. Li, Z. Y.\nLim, S. Soleman, R. Mahendra, P . Fung, S. Bahar et al., “Indonlu:\nBenchmark and resources for evaluating indonesian natural\nlanguage understanding,” in Proceedings of the 1st Conference of the\nAsia-Paciﬁc Chapter of the Association for Computational Linguistics\nand the 10th International Joint Conference on Natural Language\nProcessing, 2020, pp. 843–857.\n37\n[93] J. A. Wagner Filho, R. Wilkens, M. Idiart, and A. Villavicencio,\n“The brwac corpus: A new open resource for brazilian por-\ntuguese,” in Proceedings of the eleventh international conference on\nlanguage resources and evaluation (LREC 2018) , 2018.\n[94] L. Xu, X. Zhang, and Q. Dong, “Cluecorpus2020: A large-scale\nchinese corpus for pre-training language model,” arXiv preprint\narXiv:2003.01355, 2020.\n[95] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,\nand J. Tang, “Wudaocorpora: A super large-scale chinese corpora\nfor pre-training language models,” AI Open, 2021.\n[96] P . J. O. Su ´arez, B. Sagot, and L. Romary, “Asynchronous pipeline\nfor processing huge corpora on medium to low resource infras-\ntructures,” in 7th Workshop on the Challenges in the Management of\nLarge Corpora (CMLC-7) . Leibniz-Institut f ¨ur Deutsche Sprache,\n2019.\n[97] S. Khanuja, D. Bansal, S. Mehtani, S. Khosla, A. Dey, B. Gopalan,\nD. K. Margam, P . Aggarwal, R. T. Nagipogu, S. Dave et al. ,\n“Muril: Multilingual representations for indian languages,”\narXiv preprint arXiv:2103.10730 , 2021.\n[98] D. Kakwani, A. Kunchukuttan, S. Golla, N. Gokul, A. Bhat-\ntacharyya, M. M. Khapra, and P . Kumar, “inlpsuite: Monolingual\ncorpora, evaluation benchmarks and pre-trained multilingual\nlanguage models for indian languages,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 4948–4961.\n[99] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Sid-\ndhant, A. Barua, and C. Raffel, “mt5: A massively multilingual\npre-trained text-to-text transformer,” in Proceedings of the 2021\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 2021, pp.\n483–498.\n[100] G. Wenzek, M.-A. Lachaux, A. Conneau, V . Chaudhary,\nF. Guzm´an, A. Joulin, and ´E. Grave, “Ccnet: Extracting high qual-\nity monolingual datasets from web crawl data,” in Proceedings of\nthe 12th Language Resources and Evaluation Conference , 2020, pp.\n4003–4012.\n[101] B. Haddow and F. Kirefu, “Pmindia–a collection of parallel\ncorpora of languages of india,” arXiv preprint arXiv:2001.09907 ,\n2020.\n[102] A. Kunchukuttan, P . Mehta, and P . Bhattacharyya, “The iit\nbombay english-hindi parallel corpus,” in Proceedings of the\nEleventh International Conference on Language Resources and Evalu-\nation (LREC 2018) , 2018.\n[103] H. Huang, Y. Liang, N. Duan, M. Gong, L. Shou, D. Jiang,\nand M. Zhou, “Unicoder: A universal language encoder by\npre-training with multiple cross-lingual tasks,” in Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , 2019, pp. 2485–2494.\n[104] J. Yang, S. Ma, D. Zhang, S. Wu, Z. Li, and M. Zhou, “Alternating\nlanguage modeling for cross-lingual pre-training,” in Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence , vol. 34, no. 05,\n2020, pp. 9386–9393.\n[105] M. Ziemski, M. Junczys-Dowmunt, and B. Pouliquen, “The\nunited nations parallel corpus v1. 0,” in Proceedings of the\nTenth International Conference on Language Resources and Evaluation\n(LREC’16), 2016, pp. 3530–3534.\n[106] Z. Chi, L. Dong, F. Wei, W. Wang, X.-L. Mao, and H. Huang,\n“Cross-lingual natural language generation via pre-training,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 34,\nno. 05, 2020, pp. 7570–7577.\n[107] H. Schwenk, V . Chaudhary, S. Sun, H. Gong, and F. Guzm ´an,\n“Wikimatrix: Mining 135m parallel sentences in 1620 language\npairs from wikipedia,” in Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computational Linguistics:\nMain Volume, 2021, pp. 1351–1361.\n[108] A. El-Kishky, V . Chaudhary, F. Guzm ´an, and P . Koehn, “A\nmassive collection of cross-lingual web-document pairs,” in\nProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 2020, pp. 5960–5969.\n[109] B. Roark, L. Wolf-Sonkin, C. Kirov, S. J. Mielke, C. Johny,\nI. Demirsahin, and K. Hall, “Processing south asian languages\nwritten in the latin script: the dakshina dataset,” in Proceedings\nof the 12th Language Resources and Evaluation Conference , 2020, pp.\n2413–2423.\n[110] G. Ramesh, S. Doddapaneni, A. Bheemaraj, M. Jobanputra,\nR. AK, A. Sharma, S. Sahoo, H. Diddee, D. Kakwani, N. Ku-\nmar et al. , “Samanantar: The largest publicly available paral-\nlel corpora collection for 11 indic languages,” arXiv preprint\narXiv:2104.05596, 2021.\n[111] F. Barbieri, J. Camacho-Collados, L. E. Anke, and L. Neves,\n“Tweeteval: Uniﬁed benchmark and comparative evaluation for\ntweet classiﬁcation,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: Findings , 2020,\npp. 1644–1650.\n[112] F. Barbieri, L. E. Anke, and J. Camacho-Collados, “Xlm-t: A\nmultilingual language model toolkit for twitter,” arXiv preprint\narXiv:2104.12250, 2021.\n[113] S. Gururangan, T. Dang, D. Card, and N. A. Smith, “Variational\npretraining for semi-supervised text classiﬁcation,” in Proceedings\nof the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019, pp. 5880–5894.\n[114] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked\nsequence to sequence pre-training for language generation,” in\nInternational Conference on Machine Learning . PMLR, 2019, pp.\n5926–5936.\n[115] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,\nM. Zhou, and H.-W. Hon, “Uniﬁed language model pre-training\nfor natural language understanding and generation,” in Pro-\nceedings of the 33rd International Conference on Neural Information\nProcessing Systems, 2019, pp. 13 063–13 075.\n[116] P . Ganesh, Y. Chen, X. Lou, M. A. Khan, Y. Yang, D. Chen,\nM. Winslett, H. Sajjad, and P . Nakov, “Compressing large-scale\ntransformer-based models: A case study on bert,” arXiv preprint\narXiv:2002.11985, 2020.\n[117] Y. Meng, W. F. Speier, M. K. Ong, and C. Arnold, “Bidirectional\nrepresentation learning from transformers using multimodal\nelectronic health record data to predict depression,” IEEE Journal\nof Biomedical and Health Informatics , 2021.\n[118] L. Rasmy, Y. Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-bert:\npretrained contextualized embeddings on large-scale structured\nelectronic health records for disease prediction,” NPJ digital\nmedicine, vol. 4, no. 1, pp. 1–13, 2021.\n[119] Y. Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrish-\nnan, D. Canoy, Y. Zhu, K. Rahimi, and G. Salimi-Khorshidi,\n“Behrt: transformer for electronic health records,” Scientiﬁc re-\nports, vol. 10, no. 1, pp. 1–12, 2020.\n[120] Y.-P . Chen, Y.-Y. Chen, J.-J. Lin, C.-H. Huang, and F. Lai, “Modi-\nﬁed bidirectional encoder representations from transformers ex-\ntractive summarization model for hospital information systems\nbased on character-level tokens (alphabert): development and\nperformance evaluation,” JMIR medical informatics , vol. 8, no. 4,\np. e17787, 2020.\n[121] W. Ma, Y. Cui, C. Si, T. Liu, S. Wang, and G. Hu, “Charbert:\nCharacter-aware pre-trained language model,” in Proceedings of\nthe 28th International Conference on Computational Linguistics, 2020,\npp. 39–50.\n[122] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representa-\ntions,” in Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) , 2018, pp. 2227–\n2237.\n[123] A. Akbik, D. Blythe, and R. Vollgraf, “Contextual string embed-\ndings for sequence labeling,” in Proceedings of the 27th interna-\ntional conference on computational linguistics , 2018, pp. 1638–1649.\n[124] K. Cho, B. van Merri ¨enboer, C. Gulcehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase rep-\nresentations using rnn encoder–decoder for statistical machine\ntranslation,” in Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , 2014, pp. 1724–\n1734.\n[125] T. Kudo, “Subword regularization: Improving neural network\ntranslation models with multiple subword candidates,” in Pro-\nceedings of the 56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , 2018, pp. 66–75.\n[126] H.-C. Shin, Y. Zhang, E. Bakhturina, R. Puri, M. Patwary,\nM. Shoeybi, and R. Mani, “Bio-megatron: Larger biomedical\ndomain language model,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , 2020,\npp. 4700–4706.\n[127] G. Michalopoulos, Y. Wang, H. Kaka, H. Chen, and A. Wong,\n“Umlsbert: Clinical domain knowledge augmentation of con-\n38\ntextual embeddings using the uniﬁed medical language system\nmetathesaurus,” arXiv preprint arXiv:2010.10391 , 2020.\n[128] R. He and J. McAuley, “Ups and downs: Modeling the visual\nevolution of fashion trends with one-class collaborative ﬁlter-\ning,” in proceedings of the 25th international conference on world\nwide web, 2016, pp. 507–517.\n[129] D. Q. Nguyen, T. Vu, and A. T. Nguyen, “Bertweet: A pre-trained\nlanguage model for english tweets,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations, 2020, pp. 9–14.\n[130] M. M ¨uller, M. Salath ´e, and P . E. Kummervold, “Covid-twitter-\nbert: A natural language processing model to analyse covid-19\ncontent on twitter,” arXiv preprint arXiv:2005.07503 , 2020.\n[131] N. Goyal, J. Du, M. Ott, G. Anantharaman, and A. Conneau,\n“Larger-scale transformers for multilingual masked language\nmodeling,” arXiv preprint arXiv:2105.00572 , 2021.\n[132] A. Bhattacharjee, T. Hasan, K. Samin, M. S. Rahman, A. Iqbal,\nand R. Shahriyar, “Banglabert: Combating embedding bar-\nrier for low-resource language understanding,” arXiv preprint\narXiv:2101.00204, 2021.\n[133] M. Farahani, M. Gharachorloo, M. Farahani, and M. Manthouri,\n“Parsbert: Transformer-based model for persian language under-\nstanding,” arXiv preprint arXiv:2005.12515 , 2020.\n[134] P . Rust, J. Pfeiffer, I. Vuli ´c, S. Ruder, and I. Gurevych, “How\ngood is your tokenizer? on the monolingual performance of\nmultilingual language models,” arXiv preprint arXiv:2012.15613 ,\n2020.\n[135] M. Abdul-Mageed, A. Elmadany, and E. M. B. Nagoudi, “Arbert\n& marbert: Deep bidirectional transformers for arabic,” arXiv\npreprint arXiv:2101.01785, 2020.\n[136] F. Koto, A. Rahimi, J. H. Lau, and T. Baldwin, “Indolem and\nindobert: A benchmark dataset and pre-trained language model\nfor indonesian nlp,” in Proceedings of the 28th International Con-\nference on Computational Linguistics , 2020, pp. 757–770.\n[137] D. Q. Nguyen and A. T. Nguyen, “Phobert: Pre-trained language\nmodels for vietnamese,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: Findings , 2020,\npp. 1037–1042.\n[138] P . Delobelle, T. Winters, and B. Berendt, “Robbert: a dutch\nroberta-based language model,” in Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing: Findings,\n2020, pp. 3255–3265.\n[139] S. Dumitrescu, A.-M. Avram, and S. Pyysalo, “The birth of\nromanian bert,” in Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings , 2020, pp. 4324–\n4328.\n[140] H. Le, L. Vial, J. Frej, V . Segonne, M. Coavoux, B. Lecouteux,\nA. Allauzen, B. Crabb ´e, L. Besacier, and D. Schwab, “Flaubert:\nUnsupervised language model pre-training for french,” in Pro-\nceedings of The 12th Language Resources and Evaluation Conference ,\n2020, pp. 2479–2490.\n[141] W. Antoun, F. Baly, and H. Hajj, “Arabert: Transformer-based\nmodel for arabic language understanding,” in LREC 2020 Work-\nshop Language Resources and Evaluation Conference 11–16 May 2020,\np. 9.\n[142] ——, “Aragpt2: Pre-trained transformer for arabic language\ngeneration,” in Proceedings of the Sixth Arabic Natural Language\nProcessing Workshop, 2021, pp. 196–207.\n[143] ——, “Araelectra: Pre-training text discriminators for arabic lan-\nguage understanding,” in Proceedings of the Sixth Arabic Natural\nLanguage Processing Workshop , 2021, pp. 191–195.\n[144] W. de Vries, A. van Cranenburgh, A. Bisazza, T. Caselli, G. van\nNoord, and M. Nissim, “Bertje: A dutch bert model,” arXiv\npreprint arXiv:1912.09582, 2019.\n[145] A. Virtanen, J. Kanerva, R. Ilo, J. Luoma, J. Luotolahti,\nT. Salakoski, F. Ginter, and S. Pyysalo, “Multilingual is not\nenough: Bert for ﬁnnish,” arXiv preprint arXiv:1912.07076 , 2019.\n[146] M. Polignano, P . Basile, M. De Gemmis, G. Semeraro, and\nV . Basile, “Alberto: Italian bert language understanding model\nfor nlp challenging tasks based on tweets,” in 6th Italian Confer-\nence on Computational Linguistics, CLiC-it 2019 , vol. 2481. CEUR,\n2019, pp. 1–6.\n[147] J. Canete, G. Chaperon, R. Fuentes, and J. P ´erez, “Spanish pre-\ntrained bert model and evaluation data,” PML4DC at ICLR , vol.\n2020, 2020.\n[148] L. Martin, B. Muller, P . J. O. Su ´arez, Y. Dupont, L. Romary, ´E. V .\nDe La Clergerie, D. Seddah, and B. Sagot, “Camembert: a tasty\nfrench language model,” in Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , 2020, pp. 7203–\n7219.\n[149] L. Lowphansirikul, C. Polpanumas, N. Jantrakulchai, and S. Nu-\ntanong, “Wangchanberta: Pretraining transformer-based thai lan-\nguage models,” arXiv preprint arXiv:2101.09635 , 2021.\n[150] H. Lee, J. Yoon, B. Hwang, S. Joe, S. Min, and Y. Gwon,\n“Korealbert: Pretraining a lite bert model for korean language\nunderstanding,” in 2020 25th International Conference on Pattern\nRecognition (ICPR). IEEE, 2021, pp. 5551–5557.\n[151] P . Rybak, R. Mroczkowski, J. Tracz, and I. Gawlik, “Klej: Com-\nprehensive benchmark for polish language understanding,” in\nProceedings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics, 2020, pp. 1191–1201.\n[152] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song,\nJ. Kim, Y. Song, T. Oh et al., “Klue: Korean language understand-\ning evaluation,” arXiv preprint arXiv:2105.09680 , 2021.\n[153] M. Malmsten, L. B ¨orjeson, and C. Haffenden, “Playing with\nwords at the national library of sweden–making a swedish bert,”\narXiv preprint arXiv:2007.01658 , 2020.\n[154] S. Dadas, M. Perełkiewicz, and R. Po ´swiata, “Pre-training polish\ntransformer-based language models at scale,” in International\nConference on Artiﬁcial Intelligence and Soft Computing . Springer,\n2020, pp. 301–314.\n[155] M. Straka, J. N ´aplava, J. Strakov ´a, and D. Samuel, “Robeczech:\nCzech roberta, a monolingual contextualized language represen-\ntation model,” arXiv preprint arXiv:2105.11314 , 2021.\n[156] B. Bi, C. Li, C. Wu, M. Yan, W. Wang, S. Huang, F. Huang, and\nL. Si, “Palm: Pre-training an autoencoding&autoregressive lan-\nguage model for context-conditioned generation,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2020, pp. 8681–8691.\n[157] H. Fang, S. Wang, M. Zhou, J. Ding, and P . Xie, “Cert: Con-\ntrastive self-supervised learning for language understanding,”\narXiv preprint arXiv:2005.12766 , 2020.\n[158] F. Liu, I. Vuli ´c, A. Korhonen, and N. Collier, “Fast, effective\nand self-supervised: Transforming masked language models\ninto universal lexical and sentence encoders,” arXiv preprint\narXiv:2104.08027, 2021.\n[159] T. Gao, X. Yao, and D. Chen, “Simcse: Simple contrastive learning\nof sentence embeddings,” arXiv preprint arXiv:2104.08821 , 2021.\n[160] B. Y. Lin, S. Lee, X. Qiao, and X. Ren, “Common sense beyond\nenglish: Evaluating and improving multilingual language mod-\nels for commonsense reasoning,” arXiv preprint arXiv:2106.06937,\n2021.\n[161] D. Wang, N. Ding, P . Li, and H.-T. Zheng, “Cline: Contrastive\nlearning with semantic negative examples for natural language\nunderstanding,” arXiv preprint arXiv:2107.00440 , 2021.\n[162] M. Gupta and P . Agrawal, “Compression of deep learning\nmodels for text: A survey,” arXiv preprint arXiv:2008.05221, 2020.\n[163] M. Gordon, K. Duh, and N. Andrews, “Compressing bert:\nStudying the effects of weight pruning on transfer learning,”\nin Proceedings of the 5th Workshop on Representation Learning for\nNLP, 2020, pp. 143–155.\n[164] P . Michel, O. Levy, and G. Neubig, “Are sixteen heads really\nbetter than one?” arXiv preprint arXiv:1905.10650 , 2019.\n[165] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Ana-\nlyzing multi-head self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , 2019, pp.\n5797–5808.\n[166] H. Sajjad, F. Dalvi, N. Durrani, and P . Nakov, “Poor man’s\nbert: Smaller and faster transformer models,” arXiv preprint\narXiv:2004.03844, 2020.\n[167] A. Fan, E. Grave, and A. Joulin, “Reducing transformer depth\non demand with structured dropout,” in International Conference\non Learning Representations , 2019.\n[168] C. Bucilu ˇa, R. Caruana, and A. Niculescu-Mizil, “Model com-\npression,” in Proceedings of the 12th ACM SIGKDD international\nconference on Knowledge discovery and data mining , 2006, pp. 535–\n541.\n[169] L. J. Ba and R. Caruana, “Do deep nets really need to be\ndeep?” in Proceedings of the 27th International Conference on Neural\nInformation Processing Systems-Volume 2 , 2014, pp. 2654–2662.\n[170] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in\na neural network,” arXiv preprint arXiv:1503.02531 , 2015.\n39\n[171] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a\ndistilled version of bert: smaller, faster, cheaper and lighter,”\narXiv preprint arXiv:1910.01108 , 2019.\n[172] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang,\nand Q. Liu, “Tinybert: Distilling bert for natural language un-\nderstanding,” in Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings , 2020, pp. 4163–\n4174.\n[173] S. Sun, Y. Cheng, Z. Gan, and J. Liu, “Patient knowledge dis-\ntillation for bert model compression,” in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019, pp. 4323–4332.\n[174] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, “Mobile-\nbert: a compact task-agnostic bert for resource-limited devices,”\nin Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, 2020, pp. 2158–2170.\n[175] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou,\n“Minilm: Deep self-attention distillation for task-agnostic\ncompression of pre-trained transformers,” arXiv preprint\narXiv:2002.10957, 2020.\n[176] O. Zafrir, G. Boudoukh, P . Izsak, and M. Wasserblat, “Q8bert:\nQuantized 8bit bert,” arXiv preprint arXiv:1910.06188 , 2019.\n[177] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W.\nMahoney, and K. Keutzer, “Q-bert: Hessian based ultra low pre-\ncision quantization of bert,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , vol. 34, no. 05, 2020, pp. 8815–8821.\n[178] W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, and\nQ. Liu, “Ternarybert: Distillation-aware ultra-low bit bert,” in\nProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 2020, pp. 509–521.\n[179] A. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos, “Gobo:\nQuantizing attention-based nlp models for low latency and\nenergy efﬁcient inference,” in 2020 53rd Annual IEEE/ACM In-\nternational Symposium on Microarchitecture (MICRO). IEEE, 2020,\npp. 811–824.\n[180] A. Fan, P . Stock, B. Graham, E. Grave, R. Gribonval, H. Jegou,\nand A. Joulin, “Training with quantization noise for extreme\nmodel compression,” arXiv preprint arXiv:2004.07320 , 2020.\n[181] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser,\n“Universal transformers,” arXiv preprint arXiv:1807.03819 , 2018.\n[182] J. R. Minot, N. Cheney, M. Maier, D. C. Elbers, C. M. Danforth,\nand P . S. Dodds, “Interpretable bias mitigation for textual data:\nReducing gender bias in patient notes while maintaining classi-\nﬁcation performance,” arXiv preprint arXiv:2103.05841 , 2021.\n[183] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy\nconsiderations for deep learning in nlp,” in Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics ,\n2019, pp. 3645–3650.\n[184] N. Poerner, U. Waltinger, and H. Sch ¨utze, “Inexpensive do-\nmain adaptation of pretrained language models: Case studies\non biomedical ner and covid-19 qa,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 1482–1490.\n[185] W. Tai, H. Kung, X. L. Dong, M. Comiter, and C.-F. Kuo, “exbert:\nExtending pre-trained models with domain-speciﬁc vocabulary\nunder constrained training resources,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 1433–1439.\n[186] N. Poerner, U. Waltinger, and H. Sch ¨utze, “E-bert: Efﬁcient-yet-\neffective entity embeddings for bert,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 803–818.\n[187] I. Yamada, H. Shindo, H. Takeda, and Y. Takefuji, “Joint learn-\ning of the embedding of words and entities for named entity\ndisambiguation,” in Proceedings of The 20th SIGNLL Conference\non Computational Natural Language Learning , 2016, pp. 250–259.\n[188] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “Ernie:\nEnhanced language representation with informative entities,”\nin Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019, pp. 1441–1451.\n[189] M. E. Peters, M. Neumann, R. Logan, R. Schwartz, V . Joshi,\nS. Singh, and N. A. Smith, “Knowledge enhanced contextual\nword representations,” in Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), 2019, pp. 43–54.\n[190] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embed-\ndings using siamese bert-networks,” in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019, pp. 3982–3992.\n[191] X. Cheng, “Dual-view distilled bert for sentence embedding,”\narXiv preprint arXiv:2104.08675 , 2021.\n[192] Y. Zhang, R. He, Z. Liu, K. H. Lim, and L. Bing, “An unsu-\npervised sentence embedding method by mutual information\nmaximization,” in Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , 2020, pp. 1601–\n1610.\n[193] K. Wang, N. Reimers, and I. Gurevych, “Tsdae: Using\ntransformer-based sequential denoising auto-encoder for un-\nsupervised sentence embedding learning,” arXiv preprint\narXiv:2104.06979, 2021.\n[194] F. Carlsson, A. C. Gyllensten, E. Gogoulou, E. Y. Hellqvist, and\nM. Sahlgren, “Semantic re-tuning with contrastive tension,” in\nInternational Conference on Learning Representations , 2020.\n[195] Y. Yan, R. Li, S. Wang, F. Zhang, W. Wu, and W. Xu, “Consert:\nA contrastive framework for self-supervised sentence represen-\ntation transfer,” arXiv preprint arXiv:2105.11741 , 2021.\n[196] C. Xu, D. Tao, and C. Xu, “A survey on multi-view learning,”\narXiv preprint arXiv:1304.5634 , 2013.\n[197] Z. Li, X. Ding, K. Liao, T. Liu, and B. Qin, “Causalbert: Injecting\ncausal knowledge into pre-trained models with minimal super-\nvision,” arXiv preprint arXiv:2107.09852 , 2021.\n[198] A. Lauscher, I. Vuli ´c, E. M. Ponti, A. Korhonen, and G. Glava ˇs,\n“Specializing unsupervised pretraining models for word-level\nsemantic similarity,” in Proceedings of the 28th International Con-\nference on Computational Linguistics , 2020, pp. 1371–1383.\n[199] J. Zhou, Z. Zhang, H. Zhao, and S. Zhang, “Limit-bert: Lin-\nguistics informed multi-task bert,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 4450–4461.\n[200] P . Ke, H. Ji, S. Liu, X. Zhu, and M. Huang, “Sentilare: Sentiment-\naware language representation learning with linguistic knowl-\nedge,” arXiv preprint arXiv:1911.02493 , 2019.\n[201] B. Hao, H. Zhu, and I. Paschalidis, “Enhancing clinical bert\nembedding using a biomedical knowledge base,” in Proceedings\nof the 28th international conference on computational linguistics, 2020,\npp. 657–661.\n[202] F. Liu, E. Shareghi, Z. Meng, M. Basaldella, and N. Collier, “Self-\nalignment pretraining for biomedical entity representations,” in\nProceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, 2021, pp. 4228–4238.\n[203] Z. Yuan, Z. Zhao, and S. Yu, “Coder: Knowledge infused cross-\nlingual medical term embedding for term normalization,” arXiv\npreprint arXiv:2011.02947, 2020.\n[204] Y. Levine, B. Lenz, O. Dagan, O. Ram, D. Padnos, O. Sharir,\nS. Shalev-Shwartz, A. Shashua, and Y. Shoham, “Sensebert:\nDriving some sense into bert,” in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , 2020, pp.\n4656–4667.\n[205] O. Bodenreider, “The uniﬁed medical language system (umls):\nintegrating biomedical terminology,” Nucleic acids research ,\nvol. 32, no. suppl 1, pp. D267–D270, 2004.\n[206] T. Lin, Y. Wang, X. Liu, and X. Qiu, “A survey of transformers,”\narXiv preprint arXiv:2106.04554 , 2021.\n[207] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” arXiv preprint arXiv:2004.05150 , 2020.\n[208] J. Ainslie, S. Ontanon, C. Alberti, V . Cvicek, Z. Fisher, P . Pham,\nA. Ravula, S. Sanghai, Q. Wang, and L. Yang, “Etc: Encoding long\nand structured inputs in transformers,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing\n(EMNLP), 2020, pp. 268–284.\n[209] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,\nS. Ontanon, P . Pham, A. Ravula, Q. Wang, L. Yang et al. , “Big\nbird: Transformers for longer sequences.” in NeurIPS, 2020.\n[210] N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efﬁcient\ntransformer,” in International Conference on Learning Representa-\ntions, 2019.\n[211] K. M. Choromanski, V . Likhosherstov, D. Dohan, X. Song,\nA. Gane, T. Sarlos, P . Hawkins, J. Q. Davis, A. Mohiuddin,\nL. Kaiser et al. , “Rethinking attention with performers,” in\nInternational Conference on Learning Representations , 2020.\n40\n[212] P . He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-\nenhanced bert with disentangled attention,” arXiv preprint\narXiv:2006.03654, 2020.\n[213] Z.-H. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan, “Con-\nvbert: Improving bert with span-based dynamic convolution,”\nAdvances in Neural Information Processing Systems , vol. 33, 2020.\n[214] J. Phang, T. F ´evry, and S. R. Bowman, “Sentence encoders\non stilts: Supplementary training on intermediate labeled-data\ntasks,” arXiv preprint arXiv:1811.01088 , 2018.\n[215] Y. Zhou and V . Srikumar, “A closer look at how ﬁne-tuning\nchanges bert,” arXiv preprint arXiv:2106.14282 , 2021.\n[216] A. Merchant, E. Rahimtoroghi, E. Pavlick, and I. Tenney, “What\nhappens to bert embeddings during ﬁne-tuning?” in Proceedings\nof the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP , 2020, pp. 33–44.\n[217] M. Mosbach, A. Khokhlova, M. A. Hedderich, and D. Klakow,\n“On the interplay between ﬁne-tuning and sentence-level prob-\ning for linguistic knowledge in pre-trained transformers,” in\nProceedings of the Third BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP , 2020, pp. 68–82.\n[218] Y. Hao, L. Dong, F. Wei, and K. Xu, “Investigating learning\ndynamics of bert ﬁne-tuning,” in Proceedings of the 1st Conference\nof the Asia-Paciﬁc Chapter of the Association for Computational\nLinguistics and the 10th International Joint Conference on Natural\nLanguage Processing, 2020, pp. 87–92.\n[219] Y. Pruksachatkun, J. Phang, H. Liu, P . M. Htut, X. Zhang, R. Y.\nPang, C. Vania, K. Kann, and S. Bowman, “Intermediate-task\ntransfer learning with pretrained language models: When and\nwhy does it work?” in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , 2020, pp. 5231–5247.\n[220] C. Poth, J. Pfeiffer, A. R ¨uckl´e, and I. Gurevych, “What to pre-\ntrain on? efﬁcient intermediate task selection,” arXiv preprint\narXiv:2104.08247, 2021.\n[221] X. Liu, P . He, W. Chen, and J. Gao, “Multi-task deep neural\nnetworks for natural language understanding,” in Proceedings\nof the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019, pp. 4487–4496.\n[222] A. Mulyar and B. T. McInnes, “Mt-clinical bert: Scaling clinical\ninformation extraction with multitask learning,” arXiv preprint\narXiv:2004.10220, 2020.\n[223] D. Mahajan, A. Poddar, J. J. Liang, Y.-T. Lin, J. M. Prager,\nP . Suryanarayanan, P . Raghavan, and C.-H. Tsou, “Identiﬁca-\ntion of semantically similar sentences in clinical notes: Iterative\nintermediate training using multi-task learning,” JMIR medical\ninformatics, vol. 8, no. 11, p. e22508, 2020.\n[224] Y. Peng, Q. Chen, and Z. Lu, “An empirical study of multi-task\nlearning on bert for biomedical text mining,” in Proceedings of\nthe 19th SIGBioMed Workshop on Biomedical Language Processing ,\n2020, pp. 205–214.\n[225] C. McCreery, N. Katariya, A. Kannan, M. Chablani, and X. Ama-\ntriain, “Domain-relevant embeddings for medical question sim-\nilarity,” arXiv preprint arXiv:1910.04192 , 2019.\n[226] C. Cengiz, U. Sert, and D. Yuret, “Ku ai at mediqa 2019: Domain-\nspeciﬁc pre-training and transfer learning for medical nli,” in\nProceedings of the 18th BioNLP Workshop and Shared Task , 2019,\npp. 427–436.\n[227] X. Yang, X. He, H. Zhang, Y. Ma, J. Bian, and Y. Wu, “Measure-\nment of semantic textual similarity in clinical texts: Comparison\nof transformer-based models,” JMIR Medical Informatics , vol. 8,\nno. 11, p. e19735, 2020.\n[228] Y. Wang, K. Verspoor, and T. Baldwin, “Learning from unlabelled\ndata for clinical semantic textual similarity,” in Proceedings of the\n3rd Clinical Natural Language Processing Workshop , 2020, pp. 227–\n233.\n[229] W. Yoon, J. Lee, D. Kim, M. Jeong, and J. Kang, “Pre-trained\nlanguage model for biomedical question answering,” in Joint\nEuropean Conference on Machine Learning and Knowledge Discovery\nin Databases. Springer, 2019, pp. 727–740.\n[230] M. Jeong, M. Sung, G. Kim, D. Kim, W. Yoon, J. Yoo, and J. Kang,\n“Transferability of natural language inference to biomedical\nquestion answering,” arXiv preprint arXiv:2007.00217 , 2020.\n[231] A. Wang, J. Hula, P . Xia, R. Pappagari, R. T. McCoy, R. Patel,\nN. Kim, I. Tenney, Y. Huang, K. Yu et al. , “Can you tell me\nhow to get past sesame street? sentence-level pretraining beyond\nlanguage modeling,” in Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics , 2019, pp. 4465–4476.\n[232] J. Worsham and J. Kalita, “Multi-task learning for natural lan-\nguage processing in the 2020s: Where are we going?” Pattern\nRecognition Letters, vol. 136, pp. 120–126, 2020.\n[233] M. R. Khan, M. Ziyadi, and M. AbdelHady, “Mt-bioner: Multi-\ntask learning for biomedical named entity recognition using\ndeep bidirectional transformers,” arXiv preprint arXiv:2001.08904,\n2020.\n[234] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Larous-\nsilhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-\nefﬁcient transfer learning for nlp,” in International Conference on\nMachine Learning. PMLR, 2019, pp. 2790–2799.\n[235] A. C. Stickland and I. Murray, “Bert and pals: Projected attention\nlayers for efﬁcient adaptation in multi-task learning,” in Interna-\ntional Conference on Machine Learning . PMLR, 2019, pp. 5986–\n5995.\n[236] J. Pfeiffer, E. Simpson, and I. Gurevych, “Low resource multi-\ntask sequence tagging–revisiting dynamic conditional random\nﬁelds,” arXiv preprint arXiv:2005.00250 , 2020.\n[237] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky, “Re-\nvealing the dark secrets of bert,” in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019, pp. 4365–4374.\n[238] F. Dalvi, H. Sajjad, N. Durrani, and Y. Belinkov, “Analyzing\nredundancy in pretrained transformer models,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2020, pp. 4908–4926.\n[239] P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig,\n“Pre-train, prompt, and predict: A systematic survey of prompt-\ning methods in natural language processing,” arXiv preprint\narXiv:2107.13586, 2021.\n[240] F. Petroni, T. Rockt ¨aschel, S. Riedel, P . Lewis, A. Bakhtin, Y. Wu,\nand A. Miller, “Language models as knowledge bases?” in\nProceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , 2019, pp. 2463–\n2473.\n[241] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know\nwhat language models know?” Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 423–438, 2020.\n[242] T. Gao, A. Fisch, and D. Chen, “Making pre-trained language\nmodels better few-shot learners,” arXiv preprint arXiv:2012.15723,\n2020.\n[243] W. Yuan, G. Neubig, and P . Liu, “Bartscore: Evaluating generated\ntext as text generation,” arXiv preprint arXiv:2106.11520 , 2021.\n[244] T. Shin, Y. Razeghi, R. L. Logan IV , E. Wallace, and S. Singh,\n“Eliciting knowledge from language models using automatically\ngenerated prompts,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , 2020,\npp. 4222–4235.\n[245] E. Wallace, S. Feng, N. Kandpal, M. Gardner, and S. Singh, “Uni-\nversal adversarial triggers for attacking and analyzing nlp,” in\nProceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , 2019, pp. 2153–\n2162.\n[246] X. L. Li and P . Liang, “Preﬁx-tuning: Optimizing continuous\nprompts for generation,” arXiv preprint arXiv:2101.00190 , 2021.\n[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang,\n“Gpt understands, too,” arXiv preprint arXiv:2103.10385 , 2021.\n[248] B. Lester, R. Al-Rfou, and N. Constant, “The power of\nscale for parameter-efﬁcient prompt tuning,” arXiv preprint\narXiv:2104.08691, 2021.\n[249] H. Elsahar, P . Vougiouklis, A. Remaci, C. Gravier, J. Hare,\nE. Simperl, and F. Laforest, “T-rex: A large scale alignment of\nnatural language with knowledge base triples,” 2019.\n[250] R. Speer, C. Havasi et al., “Representing general relational knowl-\nedge in conceptnet 5.” in LREC, vol. 2012, 2012, pp. 3679–86.\n[251] P . Rajpurkar, J. Zhang, K. Lopyrev, and P . Liang, “Squad:\n100,000+ questions for machine comprehension of text,” in Pro-\nceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, 2016, pp. 2383–2392.\n[252] N. Kassner, P . Dufter, and H. Sch¨utze, “Multilingual lama: Inves-\ntigating knowledge in multilingual pretrained language mod-\nels,” in Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume , 2021,\npp. 3250–3258.\n41\n[253] N. Kassner and H. Sch ¨utze, “Negated and misprimed probes\nfor pretrained language models: Birds can talk, but cannot ﬂy,”\nin Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, 2020, pp. 7811–7818.\n[254] L. K. Senel and H. Sch ¨utze, “Does he wink or does he nod? a\nchallenging benchmark for evaluating word understanding of\nlanguage models,” arXiv preprint arXiv:2102.03596 , 2021.\n[255] C. Fellbaum, “Wordnet,” in Theory and applications of ontology:\ncomputer applications. Springer, 2010, pp. 231–243.\n[256] T. Schick and H. Sch ¨utze, “Rare words: A major problem\nfor contextualized embeddings and how to ﬁx it by attentive\nmimicking,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 34, no. 05, 2020, pp. 8766–8774.\n[257] Z. Jiang, A. Anastasopoulos, J. Araki, H. Ding, and G. Neubig,\n“X-factr: Multilingual factual knowledge retrieval from pre-\ntrained language models,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , 2020,\npp. 5943–5959.\n[258] P . Singh, T. Lin, E. T. Mueller, G. Lim, T. Perkins, and W. L. Zhu,\n“Open mind common sense: Knowledge acquisition from the\ngeneral public,” in OTM Confederated International Conferences”\nOn the Move to Meaningful Internet Systems” . Springer, 2002, pp.\n1223–1237.\n[259] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, “Masked\nlanguage model scoring,” in Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics , 2020, pp. 2699–\n2712.\n[260] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” in Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, 2018, pp. 353–355.\n[261] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael,\nF. Hill, O. Levy, and S. R. Bowman, “Superglue: A stickier bench-\nmark for general-purpose language understanding systems,”\narXiv preprint arXiv:1905.00537 , 2019.\n[262] D. Khashabi, G. Stanovsky, J. Bragg, N. Lourie, J. Kasai, Y. Choi,\nN. A. Smith, and D. S. Weld, “Genie: A leaderboard for\nhuman-in-the-loop evaluation of text generation,” arXiv preprint\narXiv:2101.06561, 2021.\n[263] S. Gehrmann, T. Adewumi, K. Aggarwal, P . S. Ammana-\nmanchi, A. Anuoluwapo, A. Bosselut, K. R. Chandu, M. Clinciu,\nD. Das, K. D. Dhole et al. , “The gem benchmark: Natural\nlanguage generation, its evaluation and metrics,” arXiv preprint\narXiv:2102.01672, 2021.\n[264] D. Liu, Y. Yan, Y. Gong, W. Qi, H. Zhang, J. Jiao, W. Chen, J. Fu,\nL. Shou, M. Gong et al. , “Glge: A new general language gen-\neration evaluation benchmark,” arXiv preprint arXiv:2011.11928 ,\n2020.\n[265] S. Mehri, M. Eric, and D. Hakkani-Tur, “Dialoglue: A natural\nlanguage understanding benchmark for task-oriented dialogue,”\narXiv preprint arXiv:2009.13570 , 2020.\n[266] N. Zhang, Q. Jia, K. Yin, L. Dong, F. Gao, and N. Hua, “Con-\nceptualized representation learning for chinese biomedical text\nmining,” arXiv preprint arXiv:2008.10813 , 2020.\n[267] Y. Liang, N. Duan, Y. Gong, N. Wu, F. Guo, W. Qi, M. Gong,\nL. Shou, D. Jiang, G. Cao et al. , “Xglue: A new benchmark\ndatasetfor cross-lingual pre-training, understanding and gener-\nation,” in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , 2020, pp. 6008–6018.\n[268] G. Aguilar, S. Kar, and T. Solorio, “Lince: A centralized bench-\nmark for linguistic code-switching evaluation,” in Proceedings of\nThe 12th Language Resources and Evaluation Conference , 2020, pp.\n1803–1813.\n[269] S. Khanuja, S. Dandapat, A. Srinivasan, S. Sitaram, and\nM. Choudhury, “Gluecos: An evaluation benchmark for code-\nswitched nlp,” in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , 2020, pp. 3575–3585.\n[270] J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson,\n“Xtreme: A massively multilingual multi-task benchmark for\nevaluating cross-lingual generalisation,” in International Confer-\nence on Machine Learning . PMLR, 2020, pp. 4411–4421.\n[271] S. Ruder, N. Constant, J. Botha, A. Siddhant, O. Firat, J. Fu, P . Liu,\nJ. Hu, G. Neubig, and M. Johnson, “Xtreme-r: Towards more\nchallenging and nuanced multilingual evaluation,” arXiv preprint\narXiv:2104.07412, 2021.\n[272] T. Shavrina, A. Fenogenova, E. Anton, D. Shevelev, E. Arte-\nmova, V . Malykh, V . Mikhailov, M. Tikhonova, A. Chertok,\nand A. Evlampiev, “Russiansuperglue: A russian language un-\nderstanding evaluation benchmark,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing\n(EMNLP), 2020, pp. 4717–4726.\n[273] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu,\nC. Yu et al., “Clue: A chinese language understanding evaluation\nbenchmark,” in Proceedings of the 28th International Conference on\nComputational Linguistics, 2020, pp. 4762–4772.\n[274] D. Khashabi, A. Cohan, S. Shakeri, P . Hosseini, P . Pezeshkpour,\nM. Alikhani, M. Aminnaseri, M. Bitaab, F. Brahman, S. Ghazarian\net al., “Parsinlu: a suite of language understanding challenges for\npersian,” arXiv preprint arXiv:2012.06154 , 2020.\n[275] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Yuan, H. Xu, G. Wei, X. Pan,\nand H. Hu, “Fewclue: A chinese few-shot learning evaluation\nbenchmark,” arXiv preprint arXiv:2107.07498 , 2021.\n[276] J. Bragg, A. Cohan, K. Lo, and I. Beltagy, “Flex: Unifying evalu-\nation for few-shot nlp,” arXiv preprint arXiv:2107.07170 , 2021.\n[277] T. Schick and H. Sch ¨utze, “It’s not just size that matters: Small\nlanguage models are also few-shot learners,” in Proceedings of the\n2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 2021, pp.\n2339–2352.\n[278] T. Wolf, J. Chaumond, L. Debut, V . Sanh, C. Delangue, A. Moi,\nP . Cistac, M. Funtowicz, J. Davison, S. Shleifer et al., “Transform-\ners: State-of-the-art natural language processing,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations , 2020, pp. 38–45.\n[279] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grang-\nier, and M. Auli, “Fairseq: A fast, extensible toolkit for sequence\nmodeling,” NAACL HLT 2019 , p. 48, 2019.\n[280] Y. Yan, F. Hu, J. Chen, N. Bhendawade, T. Ye, Y. Gong, N. Duan,\nD. Cui, B. Chi, and R. Zhang, “Fastseq: Make sequence genera-\ntion faster,” arXiv preprint arXiv:2106.04718 , 2021.\n[281] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, “Deepspeed:\nSystem optimizations enable training deep learning models with\nover 100 billion parameters,” in Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining, 2020, pp. 3505–3506.\n[282] X. Wang, Y. Xiong, Y. Wei, M. Wang, and L. Li, “Lightseq: A high\nperformance inference library for transformers,” in Proceedings of\nthe 2021 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies:\nIndustry Papers, 2021, pp. 113–120.\n[283] J. Vig, “Bertviz: A tool for visualizing multihead self-attention in\nthe bert model,” in ICLR Workshop: Debugging Machine Learning\nModels, 2019.\n[284] B. Hoover, H. Strobelt, and S. Gehrmann, “exbert: A visual\nanalysis tool to explore learned representations in transformer\nmodels,” in Proceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics: System Demonstrations , 2020,\npp. 187–196.\n[285] J. Fang, Y. Yu, C. Zhao, and J. Zhou, “Turbotransformers: an efﬁ-\ncient gpu serving system for transformer models,” in Proceedings\nof the 26th ACM SIGPLAN Symposium on Principles and Practice\nof Parallel Programming , 2021, pp. 389–402.\n[286] Z. Yang, Y. Cui, Z. Chen, W. Che, T. Liu, S. Wang, and G. Hu,\n“Textbrewer: An open-source knowledge distillation toolkit for\nnatural language processing,” in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics: System\nDemonstrations, 2020, pp. 9–16.\n[287] H. Shah, A. Khare, N. Shah, and K. Siddiqui, “Kd-lib: A pytorch\nlibrary for knowledge distillation, pruning and quantization,”\narXiv preprint arXiv:2011.14691 , 2020.\n[288] T. Likhomanenko, Q. Xu, R. Collobert, G. Synnaeve, and\nA. Rogozhnikov, “Cape: Encoding relative positions with con-\ntinuous augmented positional embeddings,” arXiv preprint\narXiv:2106.03143, 2021.\n[289] P . Liu, J. Fu, Y. Xiao, W. Yuan, S. Chang, J. Dai, Y. Liu, Z. Ye,\nand G. Neubig, “Explainaboard: An explainable leaderboard for\nnlp,” arXiv preprint arXiv:2104.06387 , 2021.\n[290] G. Menghani, “Efﬁcient deep learning: A survey on making\ndeep learning models smaller, faster, and better,” arXiv preprint\narXiv:2106.08962, 2021.\n[291] L. Sun, K. Hashimoto, W. Yin, A. Asai, J. Li, P . Yu, and C. Xiong,\n“Adv-bert: Bert is not robust on misspellings! generating nature\n42\nadversarial samples on bert,” arXiv preprint arXiv:2003.04985 ,\n2020.\n[292] D. Pruthi, B. Dhingra, and Z. C. Lipton, “Combating adversarial\nmisspellings with robust word recognition,” in Proceedings of the\n57th Annual Meeting of the Association for Computational Linguis-\ntics, 2019, pp. 5582–5591.\n[293] Z. Wang, S. Mayhew, D. Roth et al., “Extending multilingual bert\nto low-resource languages,” arXiv preprint arXiv:2004.13640, 2020.\n[294] Y. Nakamura, S. Hanaoka, Y. Nomura, N. Hayashi, O. Abe,\nS. Yada, S. Wakamiya, and E. Aramaki, “Kart: Privacy leak-\nage framework of language models pre-trained with clinical\nrecords,” arXiv preprint arXiv:2101.00036 , 2020.\n[295] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss,\nK. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson et al. ,\n“Extracting training data from large language models,” arXiv\npreprint arXiv:2012.07805, 2020.\n[296] V . Misra, “Black box attacks on transformer language models,”\nin ICLR 2019 Debugging Machine Learning Models Workshop , 2019.\n[297] S. Hisamoto, M. Post, and K. Duh, “Membership inference\nattacks on sequence-to-sequence models: Is my data in your\nmachine translation system?” Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 49–63, 2020.\n[298] J. Wang, G. Zhang, W. Wang, K. Zhang, and Y. Sheng, “Cloud-\nbased intelligent self-diagnosis and department recommendation\nservice using chinese medical bert,” Journal of Cloud Computing ,\nvol. 10, no. 1, pp. 1–12, 2021.\n[299] D. Araci, “Finbert: Financial sentiment analysis with pre-trained\nlanguage models,” arXiv preprint arXiv:1908.10063 , 2019.\n[300] P . P . Liang, C. Wu, L.-P . Morency, and R. Salakhutdinov, “To-\nwards understanding and mitigating social biases in language\nmodels,” in International Conference on Machine Learning . PMLR,\n2021, pp. 6565–6576.\n[301] C. Lee, K. Cho, and W. Kang, “Mixout: Effective regularization to\nﬁnetune large-scale pretrained language models,” arXiv preprint\narXiv:1909.11299, 2019.\n[302] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi,\nand N. Smith, “Fine-tuning pretrained language models: Weight\ninitializations, data orders, and early stopping,” arXiv preprint\narXiv:2002.06305, 2020.\n[303] M. Mosbach, M. Andriushchenko, and D. Klakow, “On the\nstability of ﬁne-tuning bert: Misconceptions, explanations, and\nstrong baselines,” in International Conference on Learning Repre-\nsentations, 2020.\n[304] B. Gunel, J. Du, A. Conneau, and V . Stoyanov, “Supervised con-\ntrastive learning for pre-trained language model ﬁne-tuning,”\narXiv preprint arXiv:2011.01403 , 2020.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8191426396369934
    },
    {
      "name": "Computer science",
      "score": 0.7593618631362915
    },
    {
      "name": "Transfer of learning",
      "score": 0.5744280815124512
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5363729000091553
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.4896686375141144
    },
    {
      "name": "Natural language processing",
      "score": 0.44702956080436707
    },
    {
      "name": "Language model",
      "score": 0.44696637988090515
    },
    {
      "name": "Question answering",
      "score": 0.41559380292892456
    },
    {
      "name": "Machine learning",
      "score": 0.3991280496120453
    },
    {
      "name": "Engineering",
      "score": 0.07920962572097778
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}