{
  "title": "IDPT: Interconnected Dual Pyramid Transformer for Face Super-Resolution",
  "url": "https://openalex.org/W4285599866",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2114526326",
      "name": "Jingang Shi",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2128869945",
      "name": "Yusi Wang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2470157925",
      "name": "Songlin Dong",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2095942684",
      "name": "Xiaopeng Hong",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2726404128",
      "name": "Zitong Yu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2108825262",
      "name": "Changxin Wang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2170066198",
      "name": "Yihong Gong",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287391849",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W2774482732",
    "https://openalex.org/W2790508633",
    "https://openalex.org/W2963460857",
    "https://openalex.org/W2964167901",
    "https://openalex.org/W2913036400",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W1796263212",
    "https://openalex.org/W2895542678",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2520930090",
    "https://openalex.org/W2963676087",
    "https://openalex.org/W4287635192",
    "https://openalex.org/W3111002277",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312358294",
    "https://openalex.org/W3035605421",
    "https://openalex.org/W3169612303",
    "https://openalex.org/W4287181824",
    "https://openalex.org/W3207918547"
  ],
  "abstract": "Face Super-resolution (FSR) task works for generating high-resolution (HR) face images from the corresponding low-resolution (LR) inputs, which has received a lot of attentions because of the wide application prospects. However, due to the diversity of facial texture and the difficulty of reconstructing detailed content from degraded images, FSR technology is still far away from being solved. In this paper, we propose a novel and effective face super-resolution framework based on Transformer, namely Interconnected Dual Pyramid Transformer (IDPT). Instead of straightly stacking cascaded feature reconstruction blocks, the proposed IDPT designs the pyramid encoder/decoder Transformer architecture to extract coarse and detailed facial textures respectively, while the relationship between the dual pyramid Transformers is further explored by a bottom pyramid feature extractor. The pyramid encoder/decoder structure is devised to adapt various characteristics of textures in different spatial spaces hierarchically. A novel fusing modulation module is inserted in each spatial layer to guide the refinement of detailed texture by the corresponding coarse texture, while fusing the shallow-layer coarse feature and corresponding deep-layer detailed feature simultaneously. Extensive experiments and visualizations on various datasets demonstrate the superiority of the proposed method for face super-resolution tasks.",
  "full_text": "IDPT: Interconnected Dual Pyramid Transformer for Face Super-Resolution\nJingang Shi1 , Yusi Wang1 , Songlin Dong1 , Xiaopeng Hong1 , Zitong Yu2∗ ,\nFei Wang1 , Changxin Wang1 and Yihong Gong1\n1School of Software Engineering, Xi’an Jiaotong University\n2School of Electrical and Electronic Engineering, Nanyang Technological University\n{jingang, yusiwang, dsl972731417, feynmanw, wcxhello, ygong}@xjtu.edu.cn,\nhongxiaopeng@ieee.org, zitong.yu@ntu.edu.sg\nAbstract\nFace Super-resolution (FSR) task works for gener-\nating high-resolution (HR) face images from the\ncorresponding low-resolution (LR) inputs, which\nhas received a lot of attentions because of the wide\napplication prospects. However, due to the di-\nversity of facial texture and the difficulty of re-\nconstructing detailed content from degraded im-\nages, FSR technology is still far away from being\nsolved. In this paper, we propose a novel and ef-\nfective face super-resolution framework based on\nTransformer, namely Interconnected Dual Pyramid\nTransformer (IDPT). Instead of straightly stack-\ning cascaded feature reconstruction blocks, the pro-\nposed IDPT designs the pyramid encoder/decoder\nTransformer architecture to extract coarse and de-\ntailed facial textures respectively, while the rela-\ntionship between the dual pyramid Transformers is\nfurther explored by a bottom pyramid feature ex-\ntractor. The pyramid encoder/decoder structure is\ndevised to adapt various characteristics of textures\nin different spatial spaces hierarchically. A novel\nfusing modulation module is inserted in each spa-\ntial layer to guide the refinement of detailed texture\nby the corresponding coarse texture, while fusing\nthe shallow-layer coarse feature and correspond-\ning deep-layer detailed feature simultaneously. Ex-\ntensive experiments and visualizations on various\ndatasets demonstrate the superiority of the pro-\nposed method for face super-resolution tasks.\n1 Introduction\nFace Super-resolution (FSR), also known as face hallucina-\ntion, works for generating high-resolution (HR) face images\nfrom the corresponding low-resolution (LR) inputs [Jiang et\nal., 2021; Shi et al., 2018; Shi and Zhao, 2019]. Since most\nexisting facial analysis techniques (e.g., face detection, face\nalignment, face recognition) would suffer from worse perfor-\nmance when encounter the degradation of very low resolution\nproblem, the research on FSR has far-reaching significance.\n∗Corresponding author\nFSR is a challenging task with promising application\nprospects, which can be considered as a specific task of sin-\ngle image super resolution (SISR). Different from SISR, the\ntarget of FSR concentrates on the recovery of important facial\nstructures (e.g., facial components, facial contour) instead of\narbitrary details in natural images. These structures are cru-\ncial for displaying the characteristics of human faces, but the\nrecovery is difficult when the input low-resolution faces need\na large magnification in the reconstruction. Recently, most\nFSR algorithms have been developed by employing deep\nConvolutional Neural Network (CNN) [Chen et al., 2018;\nYuet al., 2018; Maet al., 2020] to reconstruct facial textures.\nAlthough these CNN-based methods have obtained impres-\nsive performance, they still suffer from several barriers which\nwould hinder FSR algorithms to achieve perfect results. As\nwe know, convolution is a local operation which aims to ex-\ntract features from local regions in the image. Thus, it has\na limitation in describing the interactions between different\nregions and capturing long-range dependencies. To alleviate\nthe problem, some recent FSR approaches usually construct\ndeeper network structure and stack manually designed fea-\nture reconstruction blocks (FRBs) to explore cross-region de-\ntailed features. However, several disadvantages exist in these\nnetwork architectures that need to be further solved. First,\nthough it could achieve better performance by stacking more\nFRBs and employing deeper network, the improvement is\nnot effective compared to the increase of computational cost.\nSecond, since the input images suffer from low-resolution\nand other degraded factors, some inaccurate details can be\nproduced in feature extraction layers. The artifacts from shal-\nlow layers will be accumulated and become serious when the\nnetwork architecture goes deeper. Third, various local con-\ntents may have different intrinsic characteristics (e.g., width,\nheight, complexity), so it is better to design hierarchical struc-\nture for adapting the diversity of textures.\nRecently, Transformer takes the advantage of self-attention\nmechanism to explore global interactions and has achieved\noutstanding performance in the vision fields[Cao et al., 2021;\nLiang et al., 2021; Yu et al., 2022]. The pioneering work\nViT [Dosovitskiy et al., 2021 ] directly divides the image\ninto patches with fixed size to adapt the Transformer mod-\nule, which presents its capability in classification task. After\nthe success of ViT, kinds of Transformers have been proposed\nrecently [Khan et al., 2021]. One representative work is Swin\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1306\nTransformer [Liu et al., 2021] which designs the shifted win-\ndow based self-attention and presents superior performance\non vision tasks. However, current Transformer-based meth-\nods have few researches on the FSR task, so how to efficiently\nuse Transformer to solve it is still a meaningful challenge.\nIn this paper, we propose the Interconnected Dual Pyra-\nmid Transformer (IDPT) framework for face super-resolution\ntask. There are three core innovations for IDPT. First, we\ndesign a dual pyramid encoder/decoder Transformer archi-\ntecture which could extract hierarchical features on various\nspatial dimensions from shallow to deep layers. Concretely,\nwe increase the depth of Transformer blocks in the encoder\npathway gradually while decrease it in the decoder pathway.\nCompared with the methods that simply stack network depth,\nthe proposed architecture is more adaptive to learn high-\nquality textural details for reconstruction.\nSecond, inspired by the human visual cognitive mechanism\nthat human brain modulates the ventral pathway through the\nsubcortical pathway to improve the recognition ability [Liu\net al., 2017], we design a novel fusing modulation module\n(FMM) to modulate the deep-layer in decoder by the corre-\nsponding shallow-layer in encoder. As we know, the shallow-\nlayer extracts the coarse feature while the deep-layer could\nproduce the detailed feature. FMM is proposed to refine the\ndetailed feature by the guidance of shallow-layer for elimi-\nnating the deep-layer artifacts, while also fusing the coarse\nfeature and detailed feature for improving the reconstruction\ncapability of the network.\nFinally, we propose the bottom pyramid feature extrac-\ntor (BPFE) to construct the relationship between the bottom\nlayers of dual pyramid structures. It serves as a coarse-to-fine\nfeature extraction procedure, which explores the detailed fea-\nture from the output of the encoder and further feeds it into\nthe decoder pyramid. Compared with prevalent face super-\nresolution methods, the proposed IDPT framework could\nachieve superior results on multiple datasets. The visualiza-\ntions could also verify that our method is very effective to\nrecover complex textural details on facial images.\nOur main contributions are summarized as follows:\n• We propose a novel dual pyramid Transformer architec-\nture to deal with FSR task, which could hierarchically\nextract the shallow-layer coarse feature and deep-layer\ndetailed feature from different spatial spaces.\n• Inspired by the research of the human visual cognitive\nmechanism, we propose the fusing modulation module\nto refine the deep-layer detailed feature and fuse it with\ncorresponding shallow-layer coarse feature.\n• A coarse-to-fine feature extraction module is con-\nstructed across the bottom of dual pyramids, which\nis employed to explore the underlying relationship be-\ntween encoder and decoder.\n• Extensive experiments and visualizations demonstrate\nthe superiority of the proposed IDPT when compared\nwith the state-of-the-art methods.\n2 Related Works\nFace Super-Resolution. In the past years, deep learning\nbased methods have great promoted the development of FSR\nfield. URDGN [Yu and Porikli, 2016] firstly proposes a dis-\ncriminative generative network structure which is utilized to\nrestore details from very low resolution face images. Consid-\nering the identity information of face image, SICNN [Zhang\net al., 2018] defines the super-identity loss to maintain the\nconsistency in the identity metric space for achieving bet-\nter performance. SPARNet [Chen et al., 2020] introduces a\nspatial attention mechanism that could focus on the recovery\nof feature-rich regions to generate high-quality SR images.\nSISN [Lu et al., 2021] proposes the external-internal split at-\ntention group to fuse structural information and textural de-\ntails of facial images in the reconstruction. In some latest\nFSR methods, facial prior is also utilized as an additional\nconstraint to supervise the training phase. FSRNet [Chen\net al., 2018] makes use of landmark heatmaps and parsing\nmaps in the training process, which induces impressive re-\nsults. In [Kim et al., 2019], it proposes a facial attention loss\nwhich encourages the network to focus on the alignment of\nfacial landmarks. DIC [Ma et al., 2020] uses a deep iterative\ncollaboration network for FSR to enable face images recov-\nery and landmark estimation in a mutually reinforcing mech-\nanism. GPEN [Yang et al., 2021] designs the neural network\nbased on GAN prior to solve blind face restoration problem.\nVisual Transformer.Transformer model has gained great\nsuccess in natural language processing (NLP) domain. In-\nspired by its significant performance in NLP, Transformer is\nutilized to solve computer vision tasks recently. ViT [Doso-\nvitskiy et al., 2021] firstly proves that a pure Transformer\nstructure could achieve state-of-the-art performance on clas-\nsification task. With pre-training on large-scale datasets, it re-\nshapes the inputs into a set of medium-size flattened patches\nand surpasses the capability of CNN-based classification lay-\ners. T2T [Yuan et al., 2021] further aggregates nearby to-\nkens into a single token recursively, instead of the simple\nimage split in ViT. Another variant called Swin Transformer\nis also proposed in [Liu et al., 2021 ]. Due to its shifted\nwindow-based multi-head attentions, it achieves state-of-the-\nart performance in various fields, such as image classifica-\ntion, object detection, and semantic segmentation. Recently,\nTransformer-based structure has also been utilized for image\nrestoration. In [Cao et al., 2021], the authors introduce VSR-\nTransformer with self-attention mechanism to deal with video\nsuper-resolution (SR). SwinIR[Liang et al., 2021] is also pro-\nposed recently, where the network architecture is constructed\nbased on [Liu et al., 2021] for image restoration.\n3 Approach\n3.1 Overview\nIn the FSR method, the task is to restore the high-frequency\nfacial details from the LR input face images and generate\nsuper-resolved results. Here ILR, IHR, and ISR denote the\nLR images, the ground truth HR images, and the generated\nsuper-resolved images respectively. Given an LR face image\nILR, we first upsample it to the same spatial dimension of\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1307\nFigure 1: (a) Framework of the IDPT. “UPFE”, “DPTB” and “ +⃝” denote a 3 × 3 convolution with LeakyReLU, dual pyramid Transformer\nblocks and element-wise addition operator, respectively. (b) The architecture of the Fusing Modulation Module (FMM). (c) The architecture\nof the Bottom Pyramid Feature Extractor (BPFE).\nIHR by bicubic interpolation. The interpolated image is rep-\nresented as ILR−HR and fed to IDPT to generate ISR.\nAs shown in Figure 1, we propose the Interconnected\nDual Pyramid Transformer (IDPT) to extract feature pro-\ngressively, which consists of the Dual Pyramid Transformer\nBlocks (DPTB), the Fusing Modulation Modules (FMM) and\nthe Bottom Pyramid Feature Extractor (BPFE). DPTB is em-\nployed to hierarchically extract the coarse feature and detailed\nfeature from various spatial dimensions. FMM is devised to\nrefine the deep-layer detailed feature by coarse feature and\nfurther fuses the shallow-layer and deep-layer for reconstruct-\ning better results. BPFE is designed to establish the underly-\ning coarse-to-fine connection between encoder and decoder.\nIn the proposed algorithm, we first utilize the upper pyra-\nmid feature extractor (UPFE) which is a 3 ×3 convolutional\nlayer with LeakyReLU to extract the low-frequency texture\nfeature Xlf. Then Xlf is fed into L encoder stages to hier-\narchically extract the coarse feature at various spatial dimen-\nsions.\nThe coarse feature Xcfi extracted by the i-th encoder can\nbe formulated as:\nXcfi = FESi(Xcfi−1 ), (1)\nwhere FESi(·) denotes the i-th encoder stage.\nAt the end of the encoder, we propose the BPFE to ex-\ntract the detailed feature Xdf for the bottom layer of pyramid\nstructure, which can be formulated as:\nXdf = FBPFE(XcfL), (2)\nThen Xdf is fed into L decoder stages to extract detailed\nfeature. In each decoder stage, both the features from the\n(i + 1)-th decoder stage and the i-th encoder stage are fed\ninto the FMM to get the modulated feature Xmfi, which is\nthe input as the i-th decoder stage (i < L).\nThe i-th decoder stage extracts the detailed featureXdfi as:\nXdfi = FDSi(Xmfi), (3)\nwhere FDSi(·) represents the i-th decoder stage.\nAfter all decoder stages, we apply the UPFE (w/o\nLeakyReLU) to get high-frequency texture feature IHF. Fi-\nnally, the generated super-resolved imageISR is obtained by:\nISR = ILR−HR + IHF (4)\n3.2 The Dual Pyramid Transformer Structure\nSwin Transformer Module (STM).Inspired by the work of\n[Liu et al., 2021], we utilize the basic block of Swin Trans-\nformer as the feature extraction module, namely Swin Trans-\nformer module (STM). Given an input of size H ×W ×C,\nSwin Transformer reshapes and splits it into non-overlapping\nwindows of M ×M size. Then it calculates self-attention\nseparately in each window.\nFor a local window feature X ∈RM2×C, it is first linearly\ntransformed to three parts, i.e., the query Q ∈RM2×d, key\nK ∈RM2×d and value V ∈RM2×d respectively, where d is\nthe query/key/value dimension. Then the self attention calcu-\nlation for a local window can be formulated as:\nAttention(Q, K, V) = Softmax(QKT /\n√\nd + B)V, (5)\nwhere B is the learnable relative position bias. The attention\nfunction is calculated h times in parallel and concatenated to\nget multi-head self-attention (MSA).\nThe MLP layer, including two FC layers with a GELU non-\nlinearity, is applied after MSA layer for feature transforma-\ntion. The LayerNorm (LN) is applied before each layer, and\nresidual connection is employed after each layer.\nMoreover, to introduce connections between neighboring\nlocal windows, the features are shifted by (⌊M\n2 ⌋, ⌊M\n2 ⌋). We\nemploy regular and shifted window partitioning alternately to\naccomplish cross-window connections.\nDual Pyramid Transformer Block (DPTB).The DPTB\nis composed of the Encoder Transformer Block (ETB) and\nthe Decoder Transformer Block (DTB). Each block contains\na stack of STMs, following by a3×3 convolutional layer with\nresidual connection. At the beginning of each ETB, we use a\n4×4 convolutional layer with stride 2 to downsample the size\nof feature maps by half and double the feature channels. As\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1308\nfor DTB, we use a 2 ×2 transposed convolution with stride 2\nto upsample the size of feature maps and reduce the number\nof feature channels to one fourth at the end. An exception\nis that there is not the downsampling/upsampling operator at\nstage 1. The number of the STMs dSTMs depends on the\ndepth of stage i:\ndSTMs = 2i−1 (6)\nThe Dual Pyramid Structure.The proposed method hi-\nerarchically extracts shallow-layer coarse feature and deep-\nlayer detailed feature by DPTBs. In the encoder stage, we\nprogressively increase the number of STMs in DPTB while\nit is decreased progressively in the decoder stage. The whole\nframework looks like a dual pyramid, which improves the ca-\npability of the network to capture feature.\n3.3 The Fusing Modulation Module\nAs illustrated in Figure 1(b), the multi-layer fusing modula-\ntion module (FMM) contains the extraction stage, the mod-\nulation stage, and the fusion stage. For the i-th FMM, we\nfirst apply a 3 ×3 convolutional layer as the extraction stage\nFES(·) to extract the feature X′\ncfi from the corresponding en-\ncoder stage as:\nX′\ncfi = FES(Xcfi) (7)\nThen, the modulation stage utilizes X′\ncfi to modulate the\ndeep-layer detailed feature from (i + 1)-th decoder stage to\nget the modulated features Xmdfi as:\nXmdfi = (σ(X′\ncfi)⊗Xdfi+1 ), (8)\nwhere σ(·) and ⊗represent the sigmoid logistic function and\nthe element-wise multiplication operator separately.\nFinally, the fusion stage concatenates X′\ncfi and Xmdfi as\nthe input features of i-th decoder stage Xmfi. Xmfi can be\nformulated as:\nXmfi = Concat[X′\ncfi, Xmdfi] (9)\nThe overall process of FMM is formulated as:\nXmfi = FMM(Xcfi, Xdfi+1 ) (10)\n3.4 The Bottom Pyramid Feature Extractor\nWe design the bottom pyramid feature extractor (BPFE) to\nrefine the coarse feature from encoder and produce the de-\ntailed feature as the input of decoder stages. As shown in Fig-\nure 1(c), BPFE contains a coarse-to-fine layer (CFL) which is\ncomposed of two consecutive blocks with STMs and convo-\nlutional layers. XcfL is fed into the CFL to further get X′\ncfL:\nX′\ncfL = FCFL(XcfL) (11)\nAt the end of the BPFE, we also apply the FMM, which\nmodulates X′\ncfL with XcfL to get the detailed features Xdf :\nXdf = FMM(XcfL, X′\ncfL) (12)\n3.5 Loss Function\nIn this section, we implement two variants of the proposed\nalgorithm (i.e., IDPT and IDPT-GAN) by switching the loss\nfunction in the training phase. Similar to previous meth-\nods [Ma et al., 2020], IDPT is designed to guarantee the con-\nsistency in pixel-level, while the target of IDPT-GAN is to\nachieve better perceptual quality.\nPixel-level loss:The L1-norm is used to constrain the map-\nping between generated SR images and HR images at the\npixel level:\nLPix = 1\nN\nNX\ni=1\n∥ISR −IHR∥1, (13)\nwhere N is the number of training set, and the SR images can\nbe formulated as ISR = FIDPT(ILR−HR).\nAdversarial loss:To make a fair comparison, we employ\nsimilar approach as [Ma et al., 2020] to construct the adver-\nsarial loss. Specifically, we build the same discriminator D\nas [Ma et al., 2020] and employ IDPT as the generatorG. The\ngenerator aims to produce SR images that could fool the dis-\ncriminator. Meanwhile, the discriminator tries to distinguish\nthe ground-truth HR face images and the SR face images re-\nconstructed by the generator. The loss functions of D and G\nare formulated as:\nLDis = −E[log(D(IHR))] −E[log(1 −D(G(ILR−HR)))]\n(14)\nLGen = −E[log(D(G(ILR−HR)))] (15)\nPerceptual loss: To improve the perceptual quality of\ngenerated images, we also apply the perceptual loss in our\nmethod. Similar to [Ma et al., 2020], we employ the pre-\ntrained LightCNN [Wu et al., 2018] to extract features. The\nloss function could be formulated as:\nLPcp = E[∥ϕ(ISR) −ϕ(IHR)∥1], (16)\nwhere ϕ(·) denotes the pretrained LightCNN.\nFor IDPT, we only employ the pixel-level loss LPix as the\nloss function in the training phase. To train IDPT-GAN, the\noverall GAN-based loss function to optimize generator is for-\nmulated as:\nLG = LPix + λAdv ·LGen + λPcp ·LPcp, (17)\nwhere λAdv and λPcp represent coefficients of adversarial\nloss and perceptual loss respectively.\n4 Experiments\n4.1 Datasets\nThe CelebA dataset [Liu et al., 2015] is utilized to train IDPT\nin the experiments. To construct the training set, we first crop\nthe face regions from each image by MTCNN [Zhang et al.,\n2016] and resize them to the size of 128 ×128 without any\npre-alignment as HR training images. Then, we downsample\nthe HR images to the size of 16 ×16 by bicubic to obtain\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1309\nBicubic\n SRResNet\n SISN\n SPARNet\n DIC\n DICGAN\n SwinIR\n IDPT\n IDPT-GAN\n HR\nFigure 2: Visualization among different methods. The restored images from previous methods have undesirable artifacts on key facial parts.\nIn contrast, the proposed IDPT and IDPT-GAN can better restore facial details, especially on eyes, lip and teeth. The proposed method\nperforms well even under the scenarios with exaggerated expressions (e.g., smiling). Please zoom in for better comparison.\nHelen CelebA\nMethod PSNR(dB) SSIM PSNR(dB) SSIM\nBicubic 23.80 0.6745 23.56 0.6361\nSRResNet 26.48 0.7962 26.38 0.7690\nSISN 27.00 0.8074 26.76 0.7778\nSPARNet 27.43 0.8201 27.17 0.7911\nSwinIR 27.50 0.8215 27.18 0.7906\nDIC 26.94 0.7994 27.41 0.7983\nDICGAN 25.90 0.7516 26.34 0.7562\nIDPT 27.96 0.8355 27.59 0.8045\nIDPT-GAN 26.57 0.7837 26.19 0.7480\nTable 1: Quantitative comparison with state-of-the-art FSR methods\non CelebA and Helen dataset. Best and second best performance are\nhighlighted in red and blue colors, respectively.\nthe corresponding LR training set. In the testing phase, we\nrandomly select 1000 images from CelebA dataset and 50\nimages from Helen dataset [Le et al., 2012] to conduct the\nexperiments. The generated SR images are evaluated with\nPSNR and SSIM metrics calculated on the Y channel of trans-\nformed YCbCr space.\n4.2 Implementation Details\nTraining Setting.For UPFE, it produces feature maps with\nchannel number of 32 for the first encoder stage, while the\nparameter α of LeakyReLU is set to 0.01. In the dual pyra-\nmid structure, the maximum number of stages L is set to 4.\nNotice that L could be set as a smaller value to reduce the\nparameters of network. For STMs, the attention head num-\nber and window size are set to 2i−1 (i = 1, ..., L) and 8 ×8,\nrespectively. We train IDPT by AdamW optimizer with β1 =\n0.9, β2 = 0.99, ϵ = 10−8, and weight decay is set to 0.02.\nThe learning rate is 2 ×10−4 and the batchsize is set to 16.\nLoss Setting. For IDPT, we set λAdv = 0 and λPcp = 0.\nFor IDPT-GAN, we empirically setλAdv = 0.005 and λPcp =\n0.1.\n4.3 Comparisons with State-of-the-Art Methods\nComparison of PSNR and SSIM scores with state-of-\nthe-art FSR methods. We compare our proposed method\nwith several state-of-the-art FSR methods such as SRRes-\nNet [Ledig et al., 2017 ], SISN [Lu et al., 2021 ], SPAR-\nNet [Chen et al., 2020], SwinIR [Liang et al., 2021], DIC [Ma\net al., 2020], and DICGAN [Ma et al., 2020] quantitatively.1\nTable 1 presents the quantitative results on Helen and CelebA\nrespectively. It is obvious that IDPT obtains the best PSNR\nand SSIM scores on both datasets and significantly outper-\nforms other FSR methods by a large margin. Compared to\ntwo recent state-of-the-art FSR methods (i.e., SwinIR and\nDIC) in PSNR value, IDPT outperforms SwinIR by 0.46dB\non Helen and 0.41dB on CelebA, while surpasses DIC by\n1.02dB on Helen and 0.18dB on CelebA.\nSince the target of IDPT-GAN is to produce visually real-\nistic texture, it could induce the decrease of quantitative val-\nues. Even so, IDPT-GAN also gets comparable performance\nin both PSNR and SSIM. This shows that our IDPT-GAN can\n1The experiments are implemented on Mindspore and Pytorch.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1310\nDIC\n SwinIR\n IDPT\n HR\nFigure 3:\nVisual details of the restored images from different FSR\nmethods. IDPT restores correct eyes details and sharper details on\nlip and teeth.\nmaintain pixel-level consistency while improving the percep-\ntual quality of generated results.\nVisualized comparison with state-of-the-art FSR meth-\nods. Some generated results of different FSR methods are\nvisualized in Figure 2. For face SR tasks, it is difficult to\nrecover the details of facial components, especially when the\nhuman face has large variations on expressions. It can be seen\nthat IDPT can produce clearer face images and vivid details\non key facial components. Figure 3 shows more comparison\nresults on image details. Specifically, we see IDPT recovers\ncorrect textures on eyes while other methods can not generate\npleasant details. Besides, IDPT can recover sharper details\non lip and teeth, while other methods produce blurry results.\nMoreover, as shown in Figure 2, IDPT-GAN could gen-\nerate more realistic and stable results with plausible details\nwhen compared with DICGAN. It also demonstrates the ef-\nfectiveness of the proposed method.\n4.4 Ablation Study\nIn this section, we conduct ablation studies on the Helen\ndataset to verify the effectiveness of the proposed method.\nEffectiveness of the dual pyramid Transformer struc-\nture. Instead of straightly stacking cascaded feature re-\nconstruction blocks, we propose the dual pyramid Trans-\nformer structure, which hierarchically reconstructs the high-\nfrequency features by the coarse-to-fine mechanism. The\nnumber of STMs in each DPTB is set to 2i−1, where i de-\nnotes the depth of the stage. We implement an ablation study\nto confirm its effectiveness. For comparison, we fix depths of\nSTMs in all the DPTBs to 2, 4, and 8 respectively. The re-\nFigure 4:\nResults of straightly stacking STMs in all the DPTBs and\nthe proposed dual pyramid structure (with yellow) in IDPT.\nName. of methods IDPT w\n/o BPFE SwinIR\nPSNR (dB) 27.96 27.66\n27.50\nTable\n2: Ablation study of the Bottom Pyramid Feature Extractor.\nES MS\nPSNR (dB)\n27.75\n✓ 27.84\n✓ 27.88\n✓ ✓ 27.96\nTable\n3: Quantitative comparison of combining different compo-\nnents of the Fusing Modulation Module.\nsults are shown in Figure 4. As the depth of STMs increases,\nthe growth of PSNR is not obvious and it could not obtain su-\nperior results when compared to the proposed pyramid struc-\nture. For example, when the depth is set to 8, our method\noutperforms it by 0.22dB, while the number of parameters\ncan be also reduced significantly.\nEffectiveness of the Fusing Modulation Module.The\nFusing Modulation Module (FMM) contains two core com-\nponents. The extraction stage (ES) is utilized to extract the\ncoarse textures, while the modulation stage (MS) is employed\nto refine the detailed textures by the guidance of correspond-\ning coarse features. Table 3 shows the comparison results\nwhen we combine different components in FMM. If we re-\nmove the whole module and simply use concatenation to re-\nplace it, the PSNR value decreases to the baseline of 27.75dB.\nIf we separately introduce ES and MS to the neural network,\nit gains an improvement of 0.13dB and 0.09dB respectively.\nThe whole FMM could remarkably improve the performance\nby 0.21dB. All experiments validate the effectiveness of our\nproposed FMM.\nEffectiveness of the Bottom Pyramid Feature Extractor.\nIn our proposed method, BPFE constructs the connection of\nthe dual pyramid structure and explore the deep-level detailed\nfeatures from the coarse ones. In our ablation study, we re-\nmove BPFE and use a convolutional layer to substitute the\nconnection. As shown in Table 2, BPFE introduces an im-\nprovement of 0.3dB in PSNR value, which demonstrates the\neffectiveness of BPFE. In addition, after removing BPFE, our\nmethod still performs better than the second-best comparison\napproach (i.e., SwinIR), which illustrates the superiority of\nour dual pyramid structure.\n5 Conclusion\nIn this paper, we propose a Transformer-based approach\ncalled IDPT for the face super-resolution task. The designed\npyramid encoder/decoder Transformer architecture could ef-\nfectively enrich the relationship between shallow-layer coarse\nfeatures and deep-layer detailed features. The multi-layer\nfusing modulation modules are also devised to refine the de-\ntailed features by the guidance of coarse ones at various spa-\ntial dimensions. Experimental results on CelebA and Helen\ndatasets show the superiority of our proposed method.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1311\nAcknowledgments\nThis work is supported by the National Natural Sci-\nence Foundation of China under Grant No. 62002283,\nU21B2048, 62102307 and 62076195, the National Key Re-\nsearch and Development Project of China under Grant No.\n2020AAA0105600, and the Fundamental Research Funds for\nthe Central Universities. We also thank Mindspore for the\nsupport of this work.2\nReferences\n[Cao et al., 2021] Jiezhang Cao, Yawei Li, Kai Zhang, and\nLuc Van Gool. Video super-resolution transformer. arXiv\npreprint arXiv:2106.06847, 2021.\n[Chen et al., 2018] Yu Chen, Ying Tai, Xiaoming Liu, Chun-\nhua Shen, and Jian Yang. FSRNet: End-to-end learning\nface super-resolution with facial priors. In CVPR, pages\n2492–2501, 2018.\n[Chen et al., 2020] Chaofeng Chen, Dihong Gong, Hao\nWang, Zhifeng Li, and Kwan-Yee Wong. Learning spa-\ntial attention for face super-resolution. IEEE Transactions\non Image Processing, 30:1219–1231, 2020.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, and et.al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nICLR, 2021.\n[Jiang et al., 2021] Junjun Jiang, Chenyang Wang, Xian-\nming Liu, and Jiayi Ma. Deep learning-based face super-\nresolution: A survey. ACM Computing Surveys, 55(1):1–\n36, 2021.\n[Khan et al., 2021] Salman Khan, Muzammal Naseer, Mu-\nnawar Hayat, and et al. Transformers in vision: A survey.\narXiv preprint arXiv:2101.01169, 2021.\n[Kim et al., 2019] Deokyun Kim, Minseon Kim, Gihyun\nKwon, and et al. Progressive face super-resolution via at-\ntention to face landmark. In BMVC, 2019.\n[Le et al., 2012] Vuong Le, Jonathan Brandt, Zhe Lin, and\net al. Interactive facial feature localization. In ECCV,\npages 679–692. Springer, 2012.\n[Ledig et al., 2017] Christian Ledig, Lucas Theis, Ferenc\nHusz´ar, and et al. Photo-realistic single image super-\nresolution using a generative adversarial network. In\nCVPR, pages 4681–4690, 2017.\n[Liang et al., 2021] Jingyun Liang, Jiezhang Cao, Guolei\nSun, and et al. SwinIR: Image restoration using swin trans-\nformer. In CVPR, pages 1833–1844, 2021.\n[Liu et al., 2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and\nXiaoou Tang. Deep learning face attributes in the wild. In\nICCV, pages 3730–3738, 2015.\n[Liu et al., 2017] Ling Liu, Fan Wang, Ke Zhou, Nai Ding,\nand Huan Luo. Perceptual integration rapidly activates\ndorsal visual pathway to guide local processing in early\nvisual areas. PLoS biology, 15(11):e2003646, 2017.\n2https://www.mindspore.cn/\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, and et al.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV, pages 10012–10022, 2021.\n[Lu et al., 2021] Tao Lu, Yuanzhi Wang, Yanduo Zhang,\nand et al. Face hallucination via split-attention in split-\nattention network. In ACM MM, pages 5501–5509, 2021.\n[Ma et al., 2020] Cheng Ma, Zhenyu Jiang, Yongming Rao,\nand et al. Deep face super-resolution with iterative collab-\noration between attentive recovery and landmark estima-\ntion. In CVPR, pages 5569–5578, 2020.\n[Shi and Zhao, 2019] Jingang Shi and Guoying Zhao. Face\nhallucination via coarse-to-fine recursive kernel regression\nstructure. IEEE Transactions on Multimedia, 21(9):2223–\n2236, 2019.\n[Shi et al., 2018] Jingang Shi, Xin Liu, Yuan Zong, Chun\nQi, and Guoying Zhao. Hallucinating face image by\nregularization models in high-resolution feature space.\nIEEE Transactions on Image Processing, 27(6):2980–\n2995, 2018.\n[Wu et al., 2018] Xiang Wu, Ran He, Zhenan Sun, and Tie-\nniu Tan. A light cnn for deep face representation with\nnoisy labels. IEEE Transactions on Information Forensics\nand Security, 13(11):2884–2896, 2018.\n[Yang et al., 2021] Tao Yang, Peiran Ren, Xuansong Xie,\nand Lei Zhang. GAN prior embedded network for blind\nface restoration in the wild. In CVPR, pages 672–681,\n2021.\n[Yu and Porikli, 2016] Xin Yu and Fatih Porikli. Ultra-\nresolving face images by discriminative generative net-\nworks. In ECCV, pages 318–333. Springer, 2016.\n[Yu et al., 2018] Xin Yu, Basura Fernando, Bernard\nGhanem, Fatih Porikli, and Richard Hartley. Face\nsuper-resolution guided by facial component heatmaps. In\nECCV, pages 217–233, 2018.\n[Yu et al., 2022] Zitong Yu, Yuming Shen, Jingang Shi,\nHengshuang Zhao, Philip Torr, and Guoying Zhao. Phys-\nformer: Facial video-based physiological measurement\nwith temporal difference transformer. In CVPR, 2022.\n[Yuan et al., 2021] Li Yuan, Yunpeng Chen, Tao Wang, Wei-\nhao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training\nvision transformers from scratch on imagenet. In ICCV,\npages 558–567, 2021.\n[Zhang et al., 2016] Kaipeng Zhang, Zhanpeng Zhang,\nZhifeng Li, and Yu Qiao. Joint face detection and align-\nment using multitask cascaded convolutional networks.\nIEEE Signal Processing Letters, 23(10):1499–1503, 2016.\n[Zhang et al., 2018] Kaipeng Zhang, Zhanpeng Zhang,\nChia-Wen Cheng, Winston Hsu, Yu Qiao, Wei Liu, and\nTong Zhang. Super-identity convolutional neural network\nfor face hallucination. In ECCV, pages 183–198, 2018.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1312",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7176501750946045
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6604046821594238
    },
    {
      "name": "Encoder",
      "score": 0.6322417855262756
    },
    {
      "name": "Transformer",
      "score": 0.5935910940170288
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.5892384052276611
    },
    {
      "name": "Computer vision",
      "score": 0.5459042191505432
    },
    {
      "name": "Image resolution",
      "score": 0.5047277212142944
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45716702938079834
    },
    {
      "name": "Feature extraction",
      "score": 0.43300721049308777
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.4173102378845215
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.41679665446281433
    },
    {
      "name": "Engineering",
      "score": 0.11497762799263
    },
    {
      "name": "Voltage",
      "score": 0.10741937160491943
    },
    {
      "name": "Mathematics",
      "score": 0.09268015623092651
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ],
  "cited_by": 20
}