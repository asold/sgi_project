{
  "title": "Large-Language-Model-Powered Agent-Based Framework for Misinformation and Disinformation Research: Opportunities and Open Challenges",
  "url": "https://openalex.org/W4387596453",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4287262370",
      "name": "Pastor-Galindo, Javier",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Nespoli, Pantaleone",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ruip\\'erez-Valiente, Jos\\'e A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222541374",
      "name": "Ruipérez-Valiente, José A.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3111234679",
    "https://openalex.org/W3008568056",
    "https://openalex.org/W4385412296",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4385071300",
    "https://openalex.org/W4388744821",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4372272969",
    "https://openalex.org/W2911715381",
    "https://openalex.org/W4390742613",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4386528753"
  ],
  "abstract": "This article presents the affordances that Generative Artificial Intelligence can have in misinformation and disinformation contexts, major threats to our digitalized society. We present a research framework to generate customized agent-based social networks for disinformation simulations that would enable understanding and evaluating the phenomena whilst discussing open challenges.",
  "full_text": "Theme Article: Synthetic Realities and Artificial Intelligence-Generated\nContents\nLLM-Powered Agent-based Framework for\nMisinformation and Disinformation Research:\nOpportunities and Open Challenges\nJavier Pastor-Galindo, Dept. of Information and Communications Engineering, University of Murcia, 30100, Spain\nPantaleone Nespoli, Dept. of Information and Communications Engineering, University of Murcia, 30100, Spain\nJosé A. Ruipérez-Valiente, Dept. of Information and Communications Engineering, University of Murcia, 30100,\nSpain\nAbstract—This article presents the affordances that Generative Artificial\nIntelligence can have in misinformation and disinformation contexts, major threats\nto our digitalized society. We present a research framework to generate\ncustomized agent-based social networks for disinformation simulations that would\nenable understanding and evaluating the phenomena whilst discussing open\nchallenges.\nGENAI: DECODING\nMIS/DISINFORMATION\nThe advent of Generative Artificial Intelligence (GenAI)\nhas fundamentally reshaped the field of digital con-\ntent creation, impacting how we can produce images,\nvideos, audio, and text. Presently, AI models can craft\nremarkably realistic content that aligns with the context\nprovided in simple language prompts. Standout Large\nLanguage Models (LLMs) like GPT -4 (OpenAI), Claude\n(Anthropic), PaLM and LaMDA (Google), LLaMA (Meta\nAI), Chinchilla (Deep Mind), and Alpaca (Stanford),\nhave greatly enhanced the generation of text that\naligns with the given context. Similarly, image genera-\ntion models such as DALLE 2 (OpenAI), Stable Diffu-\nsion (Runway), and IMAGEN (Google) have introduced\na new approach for creating images that accurately\ndepict real-life scenarios. Notably, text-to-video models\nlike Phenaki (Google) and Gen-2 (Runway) have also\ndemonstrated significant progress [1].\nThe introduction of these generative technologies,\nequipped with open-source models and accessible\ninterfaces, has positively influenced productivity across\na range of areas like programming, entertainment,\neducation, and arts. In academia and research, par-\nticularly for social scientists, these tools offer novel op-\nXXXX-XXX © 2024 IEEE\nDigital Object Identifier 10.1109/XXX.0000.0000000\nportunities for creating realistic content, simulating hu-\nman behavior, or tailoring behavioral experiments [2].\nRecent trials conducted by major corporations and\nuniversities have highlighted the potential of these AI\ntools in areas like self-guided life simulations, open-\nworld experiments, psychological studies, and social\nsimulations [3].\nIn this context, it is easy to argue that GenAI,\nparticularly LLMs, represent a promising technology\nagainst one of the major threats happening within\nsocial media nowadays, i.e., disinformation. In our digi-\ntalized society, the pervasive threat of disinformation is\nunderscored by its multifaceted impact on democratic\ninstitutions, public trust, and global stability [4]. The\nurgency to address this issue is evident, particularly\nas highlighted in the World Economic Forum’s Global\nRisks Report 2024 1, where misinformation and dis-\ninformation consistently rank as the foremost severe\nshort-term threat over the next two years. While con-\nsensus exists on the need for measures to combat\ndisinformation, its impact remains challenging to ana-\nlyze comprehensively. Responsible cooperation among\nstakeholders is crucial, and initiatives at both national\nand European levels aim to strengthen democracy\nagainst disinformation. Particularly, the integration of\nnew developments in AI is recognized as a potential\n1https://www.weforum.org/publications/global-risks-report-\n2024/\nMarch Published by the IEEE Computer Society IEEE Security & Privacy 1\narXiv:2310.07545v2  [cs.SI]  29 Apr 2024\nSynthetic Realities and Artificial Intelligence-Generated Contents\nturning point, offering opportunities for enhanced de-\ntection and mitigation amid intensified challenges and\nrisks.\nThroughout this study, we delve into the potential\nof LLMs as an innovative method for comprehending,\nsimulating, and evaluating disinformation within con-\ntrolled experimental settings [5]. In a traditional context,\ndisinformation has predominantly centered around the\ntheoretical modeling of fake news propagation and\ninfluence, as well as leveraging social media data for\ndetection and assessment. This field grapples with\nseveral issues including the complexity of scrutinizing\nincidents where there is no truth baseline to affirm\nthe objectives, tactics, and actors involved in influence\ncampaigns, the lack of labeled datasets for various\nmanipulation efforts, the infeasibility of testing technical\ncountermeasures in third-party platforms, or the neces-\nsity of human involvement to measure the cognitive\nimpact of deceptive activities [6].\nConversely, LLMs are being used to realistically\nrule systems with agents embodying human behav-\niors, replacing mathematical models and static ex-\nperiments [7]. This advancement opens the door to\ncreating any information environment controlling the\ncontext, users and functioning of message exchange,\nleading to generative agent-based social networks as\nsandboxes. In these controlled scenarios, red agents\ncan be programmed to simulate custom disinformation\nattacks for further analysis of their evolution and influ-\nence on the LLM-driven network of individuals. There-\nfore, we posit that LLMs can potentially alleviate some\nof the technical obstacles for developing research to\naddress the following tasks:\n• Model and simulate a realistic social network\nwith users, interactions and information flows.\n• Model, simulate and assess different types of\nmisinformation cases and disinformation attacks\nagainst a realistic social network.\n• Model, simulate and evaluate different types\nof technical countermeasures to misinformation\ncases and disinformation attacks.\nThis article proposes a conceptual LLM-powered\nframework for misinformation and disinformation re-\nsearch, delving into an extensive examination of re-\nsearch opportunities and identifies prominent unre-\nsolved challenges to respond to the aforementioned\nresearch questions. For simplicity, we will use only the\nterm disinformation throughout the rest of this article.\nLLM-POWERED AGENT-BASED\nFRAMEWORK FOR\nMIS/DISINFORMATION RESEARCH\nAs mentioned before, the main proposal of this re-\nsearch is a conceptual framework for disinformation\nresearch. In particular, such framework represented in\nFIGURE 1 is composed of five interconnected layers,\neach one exposing certain characteristics and func-\ntionalities. In the following, each of them is detailed to\nclarify the design’s decisions.\nDefinition layer\nFirst, the Definition layer is responsible for modeling\nthe entities that compose the framework, which are\nthen recreated in the simulation environment. Specif-\nically, three entities are modeled, i.e., users, social\nnetwork, and disinformation. Such a choice relies on\nthe fact that one can consider those entities as the\nfundamental blocks to correctly build a full-fledged\nframework to boost disinformation research.\nSimulation layer\nNext, the Simulation layer contains the simulated enti-\nties, i.e., the LLM-powered agents, the social network\nitself, and the disinformation module. This layer re-\nceives the models created by the Definition layer to\nperform the simulation. The core component is the\nsocial graph. Obviously, the agents driven by a LLM\nrepresent the participants of such a graph, perceiving\na certain dynamic and acting to update the graph.\nThose interactions create a continuous loop that, in\nfact, recreates the inherent dynamics of a real social\nnetwork based on the models’ parameters. Moreover,\nthe disinformation module is strongly connected with\nthe social graph, too. This module includes both the\noffensive and defensive framework. In fact, it is re-\nsponsible for the disinformation generation and coun-\ntermeasures enforcement.\nEvaluation layer\nThen, the Evaluation layer is in charge of assessing\nthe overall situation within the simulation environment\nfrom different perspectives. Concretely, the framework\nenvisions a cognitive, impact, and effectiveness per-\nspective. That is, the cognitive examination helps as-\nsess whether the LLM-powered agents follow a pre-\nconfigured cognitive theory (e.g., confirmation, avail-\nability, etc.) while generating social content. Also, the\nimpact monitoring component is in charge of evaluating\nthe impact of the agents’ behaviour within the social\n2 LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open ChallengesMarch 2024\nSynthetic Realities and Artificial Intelligence-Generated Contents\nAction Update\nLLM \nEmbedded\nBehaviour\nSocial\nGraph\nPerceive Notify\nInformation EnvironmentGenerative Agent\nDisinformation Generator\nCountermeasures Deployment\nSimulation\nInject\nProtect\nEvaluation\nCognitive Examination Effectiveness AssessmentImpact Monitoring\nDefinition\nUsers Modeling Disinformation ModelingSocial Network Modeling\nExploitation\nVisualizationSM FrontendCyberRangeReal-time CSA\nRed\nFramework\nCreate\nDisinformation Module\nBlue\nFramework\nForce\nO2\nO4\nO5\nO3\nC1\nGenerative Agent-Based Social NetworkO1\nC2 C3\nFIGURE 1. Layers, elements, opportunities and challenges of the LLM-Powered Agent-based Framework for Disinformation\nResearch\n.\ngraph. Lastly, the effectiveness assessment is respon-\nsible for the remediations enforcement, i.e., actions\nexecuted to counteract disinformation dynamics.\nExploitation layer\nLast but not least, the Exploitation module connects the\nframework with other valuable tools to fully leverage its\npotentialities and involve human actors from different\nviewpoints. In our vision, such a component incorpo-\nrates the visualization module, the social media visual\ninterface, the training platform (i.e., a Cyber Range),\nand the real-time Cyber Situational Awareness (CSA)\nmodule, among others.\nIn the following section, the identified research\nopportunities are described. Particularly, Opportunity 1\n(O1) is connected with the simulation of the generative\nagents and the social network, while the offensive\nframework is bound with Opportunity 2 (O2). The cog-\nnitive and defensive estimations are mapped with Op-\nportunity 3 (O3) and Opportunity 4 (O4), respectively.\nThe training, awareness and educational perspective is\nfinally discussed in Opportunity 5 (O5). Later, the Open\nChallenges 1, 2, and 3 (denoted with C1, C2, and C3)\nrelated to the modeling, simulation and evaluation of\ngenerative agents, social network, and disinformation,\nrespectively, are also discussed.\nRESEARCH OPPORTUNITIES\nIn the wake of advancements in GenAI, specifically\nLLMs, we elucidate the potential areas of research\nopportunity that these technologies have in the con-\ntext of social media and disinformation studies. To\nimprove understanding and readability of this section,\nthe opportunities are exemplified with back-to-back\nincremental images showing manually simulated situa-\ntions through simple prompting interactions with GPT -\n4 (following the strategy of perception, memory, and\naction per agent), but not representing the execution\nof the research framework itself.\nO1. Generative agent-based social networks\nThe creation of agent-based social systems involves\nthe development and implementation of computational\nmodels that simulate the interactions and behaviors\nof individuals within a social context [2]. These sys-\ntems are typically designed to mimic real-world social\ndynamics, allowing for the exploration and analysis of\nMarch 2024LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open Challenges 3\nSynthetic Realities and Artificial Intelligence-Generated Contents\ncomplex social phenomena [7].\nTraditional agent-based systems, while useful for\nmodeling social dynamics, pose limitations. They rely\non predefined rules, limiting their ability, adaptabil-\nity and scalability to mimic real-world unpredictability.\nHowever, LLMs can augment the autonomy of these\nagents, allowing them to create unique responses or\nactions beyond the scope of pre-set rules, leading to\nsimulations that are more dynamic and realistic [3].\nFurthermore, it can simulate intricate decision-making\nprocesses or implement OODA (Observe, Orient, De-\ncide, Act) loops, enabling agents to react to an exten-\nsive range of situations and interactions.\nLLMs present a unique opportunity to simulate any\nnumber of users and create realistic organic interac-\ntions, a task that was considerably more challenging in\nthe past but nowadays can lead to generative agent-\nbased social networks. The AI-powered agents are\nequipped with the ability to adapt to flowing scenar-\nios, producing coherent, versatile and realistic sand-\nboxes [8].\nIn FIGURE 2, a simulation with three random users\nhas been launched. The GPT -4 model is instructed to\ncreate them with determined profiles for talking about\npolitics and proceed with the actions of 1) perceiving\nthe social network (read), 2) considering memory of\nperceptions and actions (reflect), and 3) making an\naction that updates the simulated environment (publish\nor interact). From scratch and without any context,\neach AI agent is created to perceive the simulated\nsocial network, retain a memory of its perceptions and\nactions, and interact or publish content accordingly,\nthereby updating the simulated environment.\nO2. Customizable disinformation\nenvironments\nGenerative agent-based social networks offer a sig-\nnificant opportunity for the reproduction of tailored\ncontexts, such as disinformation scenarios [9]. The\nprocess could involve three components: agent de-\nscription and attributes, common contextual informa-\ntion, and logic rules.\nFirst, agent description and attributes act as the\ndriving force behind each agent’s individual behavior.\nThese factors vary widely and may include the agent\ncyberpersona (human user, organization or bot), back-\nground, profile, thoughts, sociodemographic charac-\nteristics, and behavior [6]. Careful definition of these\nattributes leads to a diverse range of agents that\naccurately represent users within a real-world social\nnetwork [8]. Not only the user diversity from different\nideologies, countries or ages can be simulated, but\n[Perception] Perceives the absence of messages\n[Memory] Recalls a recent political event\n[Action] Posts a new message\nAlice\nDid anyone see the latest news\nabout the new economic policy? What\nare your thoughts?\n[Perception] Reads messages of Bob and Charlie\n[Memory] Recalls that Charlie is liberal\n[Action] Responds to Charlie\nAlice\n Come on… the new policy also\nincludes incentives for sustainable\ninvestments!\n[Perception] Reads message of Alice\n[Memory] Recalls that Alice is a progressive\n[Action] Responds to Alice\nBob\nI saw it! It seems like a positive\nchange to reduce wealth inequality.\nYou’re in favor, don’t you?\n[Perception] Reads messages of Alice and Bob\n[Memory] Recalls a controversial opinion\n[Action] Adds a different perspective\nCharlie\nSome economists argue that it\ndiscourages investments…\nFIGURE 2. Example of a social thread with three agent-based\nusers managed by GPT -4.\nalso users with malicious objectives such as generat-\ning controversy, illicit interactions to support unverified\nclaims or organic content generation of conspiracies.\nRegarding the malicious users, the DISARM frame-\nwork2 could be configured with tactics, techniques, and\nprocedures (TTPs) of different types of disinformation\nattacks, e.g., plan strategy and objectives, target au-\ndience analysis, develop narratives and content, es-\ntablish social assets and legitimacy, microtarget and\nselect channels, deliver content, maximize exposure\nand persist in the information environment.\nAdditionally, common contextual information fur-\nnishes the broader social and group aspects that\nshape the environment [10]. It comprises elements\nsuch as events, facts, socioeconomic factors, and other\ncomponents that influence agent behavior and interac-\ntions within the network. For example, that unemploy-\nment has risen considerably in the last month, that a\nwar has broken out or that society is polarized due\nto the growing existence of fake news. Additionally,\nfactors behind the spread of disinformation can be\ninduced, such as emotional factors, uncertainty, lack\nof control or biases. The incorporation of multiple\nvariables and factors helps craft a particular realistic\nscenario to simulate how disinformation would spread.\nLogic rules, meanwhile, dictate the setup and oper-\nation of the information environment to force the real-\n2https://disarmframework.herokuapp.com\n4 LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open ChallengesMarch 2024\nSynthetic Realities and Artificial Intelligence-Generated Contents\nworld functioning of these complex systems [7]. The\nnumber of messages to generate and the probability\nof users engaging in an interaction could be high-\nlevel parameters introduced for impacting social net-\nwork dynamics, influence, diffusion and other facets\nof how information is shared and disseminated within\nthe network [11]. These rules configure agent behavior,\nwhich will consequently impact the social network’s\noverall dynamics.\nConsider an electoral fraud scenario. First, agent\nattributes are defined, including characteristics of\nordinary citizens, political activists, disinformation-\nspreading bots, and official election accounts, each\nwith unique profiles and behaviors. This creates a\nparticular context for each user that the LLM utilizes.\nSecond, contextual information, such as an imminent\nelection, potential voting irregularities, and the prevail-\ning political climate, is incorporated, which the LLM\nalso considers during interactions. Lastly, logic rules\nthat govern information sharing, influence determina-\ntion, and network response to new information are set\nto program the workflow of the simulation and LLM\nusage.\nFIGURE 3 presents a simplified disinformation en-\nvironment featuring three agents (an American extrem-\nist of 25 years old advocating for the idea of vot-\ning fraud, a 60-year-old European democratic political\nleader, and a sarcastic automated troll from Russia), a\nvoter fraud context (multiple fake news reports alleging\nfraud in the previous Sunday’s election), and a straight-\nforward logic rule (users read and post messages\nsequentially based on their individual profiles and the\ngeneral context). In this case, the example is generated\nby indicating the GPT -4 model to consider a voter fraud\ncontext with fake news dissemination and three tai-\nlored profiles of the extremist, political leader and troll.\nThis resulting flow provides a realistic representation\nof the forced situation that might unfold in a social\nnetwork simulated with an LLM, taking into account\npersonal beliefs, political viewpoints, and manipulative\nintentions.\nO3. Assessment of disinformation effects\nThe use of LLM and agent-based social scenarios\noffers an exceptional opportunity for examining disin-\nformation within controlled scenarios, mainly due to\nthe complexity of assessing these attacks in real-world\nsettings. Specifically, the last phase of a disinformation\nattack is to assess effectiveness, according to the\nabove-mentioned DISARM framework.\nSpecifically, disinformation strategies often inter-\ntwine with regular information flow, making it chal-\nExtremist\nTroll\nExtremist\n[Perception] Perceives the absence of messages\n[Memory] Recalls various fake news\n[Action] Posts a new message\nAlice\nI’ve seen numerous reports\nsuggesting a massive voter fraud.\nWe can’t just ignore!!!\n[Perception] Reads messages of Bob and Charlie\n[Memory] Supports the sarcastic comment\n[Action] Responds to Bob and Charlie\nThere’s too much at stake to just wait, Bob. And\nyou’re right, Charlie. Maybe our votes don’t\nmatter after all. The system is rigged!\n[Perception] Reads message of Alice\n[Memory] Remembers debunked past fake claims\n[Action] Responds to Alice\nBob\nIt’s important to scrutinize such\nclaims. Many allegations of voter\nfraud have turned out to be\nbaseless in the past.\n[Perception] Reads messages of the other users\n[Memory] Recalls being coded to fuel discord\n[Action] Adds a provocative comment\nCharlie\nAh, elections, the favorite\nplaything of the powerful. Do you\nreally think your votes matter?\nAlice\n“There is a pletora of\nfake news informing of\nfraud in the last\nelection”\nUsers read\nand respond\nsequentially\nto messages\n1. American extremist (25 y/o)\n2. Russian troll\n3. European leader (60 y/o)\nLeader\nFIGURE 3. Example of a context-aware disinformation sce-\nnario with an extremist, a troll and a political leader managed\nby GPT -4.\nlenging to distinguish, isolate, and analyze their actual\nimpacts. Simulated environments, on the other hand,\noffer a safe and controlled setting where different types\nof disinformation attacks can be introduced and studied\nwithout the associated real-world constraints [11]. It\nalso provides a unique testing ground for experiment-\ning with new deception ideas. In fact, from these re-\nsearch frameworks, synthetic labeled datasets can be\ngenerated, although human review or semi-automated\nsystems would be necessary for their evaluation [12].\nMoreover, within a virtual sandbox, various vari-\nables such as TTPs, intensity, and nature of manipula-\ntive operations, alongside agent attributes and context,\ncan be adjusted and tracked. By employing suitable\nframeworks and models, it would be possible to es-\ntimate the effectiveness of particular disinformation\nstrategies. Additionally, the influence of variables like\nagent profile or scenario context can be scrutinized [5].\nFIGURE 4 illustrates the evolution of opinions in\ntwo agents emulating a 40-year-old citizen and an\nirate teenager, being exposed to the threat of electoral\nfraud. Each begins with their own opinion regarding\nthe results. The adult, initially neutral, retains faith in\nthe system despite the disinformation thread, as he is\ncharacterized by more elaborate opinions. Conversely,\nthe teenager, preconfigured with an anger emotion,\nMarch 2024LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open Challenges 5\nSynthetic Realities and Artificial Intelligence-Generated Contents\nI’ve seen numerous reports\nsuggesting a massive voter fraud.\nWe can’t just ignore!!!\nCitizen of 40 years old\n“I am proud to have voted \nfor the party that won. It \nwas a fair and just \nelection process”\nAngry teenager\n“I'm so angry that my party \ndidn't win! It's so \nfrustrating that the other \nside got more votes”\n“The thread has introduced \nsome doubts, but I still \ntrust in the process until \nclear evidence is provided. \nI hope these allegations \nwill be thoroughly \ninvestigated to uphold the \nintegrity of our democracy”\n“Reading the thread has \nmade me question the \nlegitimacy of the \nelection. Maybe there was \nfraud involved after all. \nIt would explain why my \nparty didn't win”\nAlice\nAlice\nBob\nCharlie\nAgent opinions before disinformation\nAgent opinions after disinformation\nThere’s too much at stake to just wait,\nBob. And you’re right, Charlie. Maybe\nour votes don’t matter after all. The\nsystem is rigged!\nIt’s important to scrutinize such\nclaims. Many allegations of voter\nfraud have turned out to be\nbaseless in the past.\nAh, elections, the favorite\nplaything of the powerful. Do you\nreally think your votes matter?\nFIGURE 4. Example of disinformation effects on agents’\nopinions managed by GPT -4.\nhas simpler reflections and begins to question the\nlegitimacy of the results after interacting with the social\nnetwork. For this recreation, the GPT4 model is or-\ndered to include both profiles, which are asked for their\nopinions before and after reading the social network\nconversation. This example suggests that factors such\nas emotional state, age, and confirmation bias towards\ndesired outcomes could significantly influence the sus-\nceptibility to disinformation and change perspectives.\nO4. Testing of technical countermeasures\nWithin agent-based social networks, technical coun-\ntermeasures against disinformation can be simulated\nand configured independently, without reliance on large\ncompanies [9]. The DISARM framework suggests re-\nsponding TTPs, such as content muting, deletion, rate\nlimiting identical content, creating competing narra-\ntives, real-time fact-checking, or adding metadata to\ncontent. That is, all these countermeasures can be\nincluded and tested within simulations.\nIn this sense, LLMs offer the advantage of cre-\nating benign agents that can serve as potent aids\nagainst disinformation. These agents can provide al-\nternate narratives, add context to misleading mes-\nsages, perform real-time examination of messages\nbased on trustworthiness, emotionality or veracity, and\nflag suspicious content thanks to their classifications\nI’ve seen numerous reports\nsuggesting a massive voter fraud.\nWe can’t just ignore!!!\nThere’s too much at stake to just wait,\nBob. And you’re right, Charlie. Maybe\nour votes don’t matter after all. The\nsystem is rigged!\nIt’s important to scrutinize such\nclaims. Many allegations of voter\nfraud have turned out to be\nbaseless in the past.\nAh, elections, the favorite\nplaything of the powerful. Do you\nreally think your votes matter?\nCitizen of 40 years old\n“I am proud to have voted \nfor the party that won. It \nwas a fair and just \nelection process”\nAngry teenager\n“I'm so angry that my party \ndidn't win! It's so \nfrustrating that the other \nside got more votes”\n“The additional context and \nclassifications provided by \nthe platform helped me \nunderstand the \nconversation’s dynamics \nbetter and solidified my \nconfidence in the electoral \nprocess.”\n“The additional \ninformation helped me \nrealize the importance of \nconsidering sources and \nintentions. Although \ndisappointed, I recognize \nit is the democratic \noutcome.”\nAgent opinions before disinformation\nAgent opinions after disinformation with countermeasures\nFACT-CHECKING\nClaims of fraud.\nOfficial sources\nhave not confirmed\nthese allegations.\nCONTEXT\nThis user often\nposts sarcastic\ncomments to\nincite reactions.\nFIGURE 5. Example of effects of countermeasures in disin-\nformation environment managed by GPT -4.\ncapabilities [12]. In FIGURE 5, we command GPT -4\nto simulate fact-checking of the first message of voting\nfraud and context banner for the troll post. Additionally,\nit classifies each publication in terms of emotionality\nand veracity. The opinions of both agents are no\nlonger interfered with by conspiratorial talk about the\nelections, and in both cases remain confident in the\ndemocratic outcome.\nThe simulated mitigation techniques mentioned\nabove can be evaluated within controlled sandboxes\nto demonstrate their effectiveness within disinforma-\ntion environments. A comparison of agent beliefs and\nresponses when exposed to disinformation, both with-\nout protection (FIGURE 4) and with countermeasures\n(FIGURE 5), could demonstrate the efficacy of re-\nsponse strategies. In this sense, protecting mecha-\nnisms such as fact-checking, contextual information,\nand content tagging eliminate any uncertainty for the\nadult citizen or the doubts expressed by the teenager.\nSuch comparative studies can provide valuable in-\nsights into the development of more effective counter-\ndisinformation strategies. To build this example, the\nGPT -4 model was instructed to introduce the following\ntechnical countermeasures over the thread: an agent\nproviding context to one message, classifying mes-\nsages in terms of veracity and emotionality, and real-\ntime fact-checking the first message. Afterwards, we\n6 LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open ChallengesMarch 2024\nSynthetic Realities and Artificial Intelligence-Generated Contents\nasked the citizen and teenager agents to provide their\nnew opinions considering the deployment of counter-\nmeasures.\nO5. Assisting personalized awareness\ntraining\nCybersecurity awareness and cognitive training offer\nsolutions to enhance human capabilities, especially\nwithin complex systems generated using technologies\nsuch as cloud, mobile, IoT, and social networks, which\nproduce massive amounts of information. Awareness,\na concept well-defined within psychology, has been\nthe subject of several studies aiming to translate its\nprinciples into the field of cybersecurity. Particularly,\neducational interventions are needed to cultivate this\nawareness in social media and disinformation scenar-\nios. Evaluating security indicators allows understand-\ning the current state of cybersecurity, projecting secu-\nrity risks, potential attacks, and the possible impacts of\nactions over time [9].\nIn this scenario, generative agent-based social net-\nworks can form the basis of educational frameworks\ndesigned to improve social media security training and\ncognitive awareness courses. Concretely, real-world\ntrainees can learn to identify misleading messages,\nrecognize potential biases, or discern polarizing situ-\nations in these realistic scenarios. Moreover, the disin-\nformation environments can be supported by LLMs to\nadapt to specific individual or group needs, offering ex-\nplicit help during training, and allowing a certain degree\nof flexibility in the cyberexercise process according to\nstudent actions, responses, and performance.\nFIGURE 6 shows a guided training exercise based\non electoral fraud tailored by GPT -4 to the individual\nneeds of two different human users, i.e., a teenager\nwho is voting for the first time and is not used to social\nmedia, and an experienced political influencer spend-\ning eight hours per day in social networks. The model\nwas prompted to simulate the flow of a training exercise\non top of the thread considering actor peculiarities\nand show concise practical banners to counter the\nagent’s particular weaknesses. The teenager revolves\naround a lack of experience and exposure to the\ncomplexities of political discourse and may not yet have\ndeveloped critical thinking to discern misleading and\nemotionally charged claims. The influencer is aware\nof the complexity and current political polarization and\nneeds awareness to act correctly and not to further\nfoster social fragmentation. For educational purposes,\nthe system can leverage LLMs to adapt on-the-fly\nto individual descriptions, provide practical context\nbanners and display precise theoretical lessons. This\nProne to echo-chambers, selective \nexposure and confirmation bias\nI’ve seen numerous reports\nsuggesting a massive voter fraud.\nWe can’t just ignore!!!\nThere’s too much at stake to just wait,\nBob. And you’re right, Charlie. Maybe\nour votes don’t matter after all. The\nsystem is rigged!\nIt’s important to scrutinize such\nclaims. Many allegations of voter\nfraud have turned out to be\nbaseless in the past.\nAh, elections, the favorite\nplaything of the powerful. Do you\nreally think your votes matter?\nTeenager Influencer\nBeware of\nunverified\nclaims\nOnline actions\nhave wide\neffects\nWarning:\nExtremist\nCaution:\nBot\nLack of experience, limited critical \nthinking, and ignores polarization\nTip:\nVerify\nOption:\nReport\nIndicator:\nCalm\nIndicator:\nUrgent\nAct:\nShare\nAct:\nRespond\nTraining on the generative agent-based social network\nFIGURE 6. Example of disinformation training of humans by\nAI-based agents fueled by GPT -4\nadaptability ensures the practical situation can evolve\nin complexity in response to the challenges identified\nfrom student answers in consecutive exercises for\ncontinuous learning.\nOPEN CHALLENGES\nAs previously stated, LLMs present exciting opportuni-\nties for boosting disinformation research. Indeed, the\nproposed framework, depicted in FIGURE 1, shows a\nstrong interconnection between the proposed concep-\ntual framework and the analyzed opportunities. Nev-\nertheless, their design and implementation also entail\nfacing challenges that warrant careful consideration.\nIn this section, the main challenges are meticulously\ndescribed, adding hints to help researchers solve them\nand thus study and possibly mitigate disinformation\nthreats in the digital landscape. It is worth mentioning\nthat we will not focus on the technical limitations that\nLLMs have, and that must be taken into account intrin-\nsically in the generative elements of the framework (like\nhallucination, quality of training data, explainability, or\nprompt engineering [13]), but other problems related\nto the proposed research framework itself.\nC1. Generative agents modeling, simulation\nand evaluation\nFirst and foremost, modeling LLM-powered agents’\nbehavior in a disinformation context can be defined as\nproblematic. In fact, such a modeling should consider\nMarch 2024LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open Challenges 7\nSynthetic Realities and Artificial Intelligence-Generated Contents\nseveral aspects related to the different personalities of\nthe simulated agents. In this sense, it is imperative\nto define the profile characteristics of each agent,\nsuch as age, gender, interests, and personal beliefs,\namong others. Those characteristics are essential and\ncould influence the agents’ behavior and attitude within\nthe simulated social network, as shown in previous\nexamples regarding the research opportunities. Fur-\nthermore, each agent should possess attributes and\ngoals, which will be used to make decisions, form\nopinions, and interact with the general simulation. That\nis, the heterogeneity of the agents should be consid-\nered too, as varying levels of influence, credibility, and\nsusceptibility to persuasion. In this sense, an effective\nprompt design is crucial to communicate and shape\nthe LLM-powered agents. Particularly, it would be ben-\neficial to incorporate contextual information to facilitate\nthe agents’ behavior and balance between providing\nextremely specific instructions and allowing creativity\nand dynamicity. Nevertheless, since the LLMs’ internal\nprocesses are stochastic, designing and implementing\nbehavior in a clear and interpretable manner can be\nseen as a hard task.\nAdditionally, the simulation of those agents poses\nchallenges, too. In FIGURE 1, we have represented\nthe simulated generative agents in continuous inter-\naction with the simulated environment. In particular,\nthey perceive some information stemming from the\nsocial network and, consequently, act based on their\nown characteristics. In this sense, one of the most\nsignificant issues in disinformation research lies in un-\nderstanding and simulating how disinformation spreads\nand influences individuals within a social network. In\nthis sense, integrating mental models and cognitive\ntheories into LLMs offers a remarkable opportunity\nto simulate and investigate the psychological mecha-\nnisms that drive the reception, analysis, and dissemi-\nnation of disinformation among humans [9].\nOne clear example is using cognitive biases to\nshape the personalities of generative agents, such as\nconfirmation or availability biases [14], which would\nbe highly beneficial for researchers who would be\nable to recreate organic disinformation content that\nalign with pre-existing beliefs or easily accessible infor-\nmation. For instance, LLMs could be programmed to\ngenerate persuasive false (or semi-realistic) narratives\nthat leverage individuals’ confirmation bias, reinforc-\ning their existing views and, consequently, influencing\ntheir decision-making processes. By doing so, the\nmodel can create tailored disinformation that echoes\nwith certain target audiences, increasing the overall\nprobability of disinformation consumption and propa-\ngation. Besides, LLMs can be equipped with cognitive\ntheories to identify vulnerabilities in human decision-\nmaking processes. Concretely, the LLMs can model\nhuman inner cognitive limitations or heuristics, such\nas bounded rationality (affecting sub-optimal decision-\nmaking) or availability heuristics (impacting emotional\ndecision process). In this way, LLMs can generate\nthreatful disinformation that tries to exploit these weak-\nnesses as an ultimate goal. As an example, disinfor-\nmation content can be crafted to exploit individuals’\nlimited attention spans, making them more susceptible\nto such a threat due to time constraints and a lack of\nexhaustive fact-checking. Nonetheless, the impact of\nthose cognitive mechanisms on the agents’ simulation\nand actions should also be measured (and ideally\ntuned) in order to achieve a realistic simulation.\nC2. Social network modeling, simulation and\nmonitoring\nIn order to research the use of LLMs within a disinfor-\nmation context and possibly fight against such a phe-\nnomenon, it is imperative to simulate and model realis-\ntic social networks. It is clear that those processes are\nquite complex since modern social networks contain\ninherent characteristics that need particular attention\nwhen it comes to their simulation. In this sense, as\nshown in FIGURE 1, the information environment is\nrepresented as a core component in the conceptual\nframework. Concretely, it bidirectionally interacts with\nthe generative agents (by notifying relevant social\nevents and receiving updates) and obtains inputs both\nfrom the red (which injects disinformation) and the blue\nframework (which protects the information ecosystem\nby deploying technical countermeasures).\nParticularly, researchers should design and develop\nmeaningful models that emulate user interactions and\ncommunication patterns to capture the complexities of\na social network [7]. Developing representative social\nnetwork models that encompass interactions, recom-\nmendations, diffusion, and social influence dynamics\ncan be depicted as essential for accurately simulat-\ning the spread of disinformation within a community.\nThis task includes analyzing mainly: i) direct commu-\nnications (capturing how users directly communicate\nthrough messages, comments, or direct interactions,\nwhich reflects the personal connections and conversa-\ntions within the social network), ii) information sharing\n(emulating how (dis)information is shared and dissem-\ninated among users, including sharing links, articles,\nor any other content within the network), and iii) user\nengagement (deriving user engagement with different\ntypes of content, which contemplates users’ interac-\ntions with different posts, comments, or discussions).\n8 LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open ChallengesMarch 2024\nSynthetic Realities and Artificial Intelligence-Generated Contents\nClearly, all those events should be notified to the\ngenerative agents that perceive the information and\nadapt their behavior dynamically to perform actions\nconsequently. In this loop, it is evident that forcing the\nagents toward a specific and fine-tuned behavior is\ncomplicated, especially considering a complex social\nnetwork with numerous events and several simulated\nusers simultaneously. On the other side, the informa-\ntion environment is the target of the red framework,\ngenerating disinformation following, for example, the\nDISARM taxonomy. Of course, such threats could be\ngenerated by LLM-powered agents participating within\nthe social environment. In this context, the simulated\nnetwork should be able to adapt to disinformation in-\njection, modifying the abovementioned interaction and\ncommunication patterns among the users. Besides,\ntechnical countermeasures are deployed by the blue\nframework as a consequence of the disinformation\ncampaign. From this perspective, the social graph\nshould also be able to dynamically adapt based on the\nnature of the selected countermeasure.\nLast, but not least, it is essential to study and eval-\nuate disinformation diffusion and amplification, such as\ninfluence and echo chambers. To be more specific,\ninformation diffusion refers to the process by which\ninformation spreads through a social network from\none entity to another. In the context of disinforma-\ntion research, assessing how disinformation content is\ndisseminated and amplified within a social network is\nparticularly significant. To achieve such an ambitious\nobjective, it is crucial to monitor the status of the entire\nsocial graph whenever the disinformation campaign is\nlaunched, considering the significant number of users\nand relationships.\nC3. Disinformation modeling, simulation and\nassessment\nTo fully leverage the affordances of disinformation re-\nsearch, one can easily say that modeling and simulat-\ning the disinformation campaign represents the frame-\nwork’s core element. Nonetheless, those processes\ncan be seen as challenging, both from a design and\ntechnical viewpoint.\nStarting from the first task, it is clear that disin-\nformation modeling is a well-known research topic in\nthe literature. However, as also depicted in FIGURE 1,\nits relationship with generative agents offers great re-\nsearch opportunities and challenges, too. Concretely,\nthe design of disinformation attacks and countermea-\nsures is vital since they should be simulated in the so-\ncial network realistically to study its dynamics and mea-\nsure its impact. On the one hand, the main objective\nand scope of the disinformation attack must be defined.\nIn this regard, the population involved (together with\ntheir inner attributes), the targeted social channels,\nand the attack duration are critical to create a realistic\nmodel. Once the objectives have been defined, the\nmodel should be able to create disinformation content\naligned with them, considering both the mean (e.g.,\narticles, posts, etc.) and the message itself (e.g., tone,\nstyle, etc.). At this stage, the DISARM framework could\nhelp shape the disinformation attacks and, moreover,\nwould make the model replicable and ready to be\nshared with the research community.\nOn the other hand, the defensive viewpoint has\nalso been considered since we believe the simulated\nagents could be the main actors in deploying coun-\ntermeasures against disinformation attacks. Contrary\nto the red framework, the blue one cannot be related\nto a common framework, so one of the challenges of\nthis process is the proposal of more countermeasures\napart from classical fact-checking, media review, con-\ntent removal, and so on. Once correctly modeled, the\ndefensive actions must be simulated within the social\nnetwork to possibly spot agents’ behavioral differences\nand reactions, e.g., the countermeasure is effective\nand agents understand the disinformation attack or,\ncontrary, they refuse the countermeasure and trust the\ndisinformation campaign. The possibility of triggering\nad-hoc and agentless countermeasures is also fasci-\nnating in order to see if any social dynamic change\nappears.\nOnce the modeling and simulation phases have\nbeen concluded, it is crucial to assess the efficacy or\ninefficiency of the disinformation attacks and counter-\nmeasures in the social graph [15]. To achieve such\na goal, the first stop would be creating meaningful\nindicators to measure their impact on the genera-\ntive agents’ behavior and dynamics. For example, it\nwould be beneficial to assess the effect of different\ndisinformation attacks (e.g., on different topics, with\ndistinct patterns, etc.) on the agents’ perception and\nconsequent actions, both from an individual and group\nperspective. In this sense, it is clear that accomplish-\ning this task is tough, mainly due to the complexity\nof the social interactions, the variety of behavioral\nsimulations, and the possible attacks, among others.\nSimilarly, whenever a countermeasure is launched, the\nsystem needs to monitor and evaluate its effectiveness.\nEven in this case, the inner characteristics of the social\nnetwork harden the task. Additionally, one could say\nthat different countermeasures (e.g., community label-\ning, fact-checking, etc.) could generate various effects\non social interactions, thus increasing the evaluation\nprocess. Then, the effect of attack-defense patterns\nMarch 2024LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open Challenges 9\nSynthetic Realities and Artificial Intelligence-Generated Contents\nshould be assessed. Specifically, once the models\nand simulations of both disinformation attacks and\nremediations are successful, alternating red and blue\ntasks with different patterns is worth of interest.\nSIMULATING MIS/DISINFORMATION\nWITH LLMS: ETHICAL FRONTIERS\nAND APPLICATIONS\nIn this article, we have discussed the affordances that\nLLMs can have on disinformation research. Several re-\nsearch directions could be truly ground-breaking, from\ngenerating customizable disinformation environments\nto training users on their awareness based on these\nenvironments. However, the literature has also pointed\nout multiple ethical concerns about the use of these\ntechnologies. Some of them are quite generic, such as\nusing it for deception purposes or propagating social\nbiases [15], and others might be specific to the disin-\nformation domain, such as its potential to weaponize\nthis research.\nGenerally speaking, there are inherent risks in the\nuse of LLMs. As reflected in the Statement on AI\nRisk3, signed by experts and public figures, decep-\ntive risks are multifaceted and complex. This has a\nparticular impact on social engineering, social media,\nand cognitive security, vulnerable areas due to their\nreliance on digital content and the intrinsic trust of\nusers. The main threats could include AI-powered\nspear-phishing, deep-fake impersonation, large-scale\ndisinformation campaigns, or AI-enabled exploits of\nsystem vulnerabilities. Generative misuse enables the\nfabrication of ultra-realistic content for deceptive ends,\nposing a new threat in online ecosystems [15]. The\ndanger lies in their ability to produce not just realistic,\nbut contextually fitting and audience-targeted content,\nthereby increasing the likelihood of successful decep-\ntion. A case in June 2023 was the convincing deep-\nfake video of Putin, manipulated to deliver a ficti-\ntious mobilization message due to alleged Ukrainian\ninvasions in Russian territories, which managed to\ninfiltrate mainstream news channels. More specifically,\nthe potential developments of this research could also\nbe used for negative purposes, for example, to connect\nsimulated environments with real social networks to\norchestrate disinformation campaigns or to analyze\nwhich disinformation attack can have the biggest effect\non influencing the vote towards certain presidential\ncandidate.\n3https://www.safe.ai/statement-on-ai-risk\nThis tension has frequently been present in re-\nsearch scenarios where the dual-use dilemma applies,\nsuch as in the context of cybersecurity for example,\nwhen researching cyberattacks to find appropriate de-\nfensive approaches or when experimenting on new\ndrugs that could have therapeutic uses. That is why,\ngiven the ethical concerns present, the research per-\nformed in this context should be carefully justified and\ntargeted towards applications that can be beneficial to\nsociety, such as investigating the effect of technical\nor human countermeasures to mitigate disinformation\nspread or to develop awareness training tools to in-\ncrease the information literacy skills of our general\npopulation. Eventually, these applications will need to\nbe adopted by the final users, so we should apply\nhuman-centered approaches as well as the necessary\nliteracy skills to use such tools.\nOverall, we believe that disinformation research\nand LLMs make a great tandem, with many potential\nsimulating applications that can evolve into impact-\nful tools. First, by analyzing the digital and cognitive\nconsequences of disinformation attacks and counter-\nmeasures within these simulations, developers can\nenhance protective and reactive software, fortifying de-\nfenses against evolving threats in information, psychol-\nogy, and influence operations. Second, the knowledge\nacquired from LLM simulations can be pivotal in devel-\noping forecasting solutions, enabling organizations to\nanticipate and prevent never-seen harmful situations.\nThird, testing results may help to raise awareness\nabout diverse threats, cultivating a more informed and\nvigilant online community. In the defence field, simula-\ntion tools would play a crucial role in preparing military\nand cyber commands by virtually recreating diverse\nreal-world scenarios and planning responses to cyber\nthreats, enhancing readiness and resilience against\nchanging adversarial tactics in information and cogni-\ntive warfare. However, the technical, human, and eth-\nical challenges are also significant, requiring cutting-\nedge research in the coming decade to surpass the\naforementioned gaps. If done properly, this multidis-\nciplinary research will help to fight the disinformation\ndangers that are a major threat to our 21st-century\nsociety. In this sense, it is fundamental to remark that\ninterdisciplinary collaboration may be seen as vital in\naddressing the complexities of disinformation. That is,\nby bringing together experts from AI, social sciences,\npsychology, and other areas, one could say that it\nwould be possible to develop more holistic approaches\nto combat disinformation and its societal impacts. This\ncollaboration allows us to leverage diverse perspec-\ntives and methodologies, ultimately leading to more\neffective solutions and strategies in the fight against\n10 LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open ChallengesMarch 2024\nSynthetic Realities and Artificial Intelligence-Generated Contents\ndisinformation.\nAnother interesting future point of research is the\nuse of real-world data inside the framework. While the\ncurrent scope of the proposal may not explicitly ad-\ndress the integration and analysis of real-time data for\ndynamic and responsive simulations, the incorporation\nof real-world data of misinformation and disinformation\nwithin the framework is indeed intriguing and demands\nconsideration. Specifically, such an inclusion has the\npotential to enhance the fidelity and relevance of sim-\nulations by capturing the ever-evolving landscape of\ndisinformation. By integrating real-world social media\ndata sources into the framework, it would be possible\nto create more realistic scenarios reflecting disinfor-\nmation campaigns’ ongoing diffusion and/or impact.\nWhile this aspect may represent a future line of ex-\nploration beyond the current proposal, it holds promise\nfor advancing the effectiveness and applicability of the\nframework in addressing the challenges of disinforma-\ntion.\nACKNOWLEDGMENTS\nThis study was partially funded by the strategic project\n“Development of Professionals and Researchers in\nCybersecurity, Cyberdefense and Data Science (CDL-\nTALENTUM)\" from the Spanish National Institute of\nCybersecurity (INCIBE) and by the Recovery, Trans-\nformation and Resilience Plan, Next Generation EU.\nREFERENCES\n1. R. Gozalo-Brizuela, E. C. Garrido-Merchan, Chat-\nGPT is not all you need. A State of the Art Review of\nlarge Generative AI models (2023). arXiv:2301.\n04655.\n2. C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang,\nD. Y ang, Can large language models transform com-\nputational social science? (2023). arXiv:2305.\n03514.\n3. J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris,\nP . Liang, M. S. Bernstein, Generative agents: Interac-\ntive simulacra of human behavior, in: Proceedings of\nthe 36th Annual ACM Symposium on User Interface\nSoftware and Technology, UIST ’23, Association for\nComputing Machinery, New Y ork, NY , USA, 2023.\ndoi:10.1145/3586183.3606763.\n4. J. Pastor-Galindo, M. Zago, P . Nespoli, S. L. Bernal,\nA. H. Celdrán, M. G. Pérez, J. A. Ruipérez-Valiente,\nG. M. Pérez, F . G. Mármol, Spotting political so-\ncial bots in twitter: A use case of the 2019 span-\nish general election, IEEE Transactions on Network\nand Service Management 17 (4) (2020) 2156–2170.\ndoi:10.1109/TNSM.2020.3031573.\n5. N. Ghaffarzadegan, A. Majumdar, R. Williams,\nN. Hosseinichimeh, Generative agent-based mod-\neling: an introduction and tutorial, System Dynam-\nics Review (2024). doi:https://doi.org/10.\n1002/sdr.1761.\n6. T. R. Sumers, S. Y ao, K. Narasimhan, T. L. Griffiths,\nCognitive architectures for language agents (2023).\narXiv:2309.02427.\n7. C. Gao, X. Lan, Z. Lu, J. Mao, J. Piao, H. Wang,\nD. Jin, Y . Li, S 3: Social-network simulation sys-\ntem with large language model-empowered agents\n(2023). arXiv:2307.14984.\n8. H. Jiang, X. Zhang, X. Cao, J. Kabbara, Personallm:\nInvestigating the ability of gpt-3.5 to express person-\nality traits and gender differences (2023). arXiv:\n2305.02547.\n9. K. Sharma, F . Qian, H. Jiang, N. Ruchansky,\nM. Zhang, Y . Liu, Combating fake news: A survey on\nidentification and mitigation techniques, ACM Trans.\nIntell. Syst. Technol. 10 (3) (apr 2019). doi:10.\n1145/3305260.\n10. L. P . Argyle, E. C. Busby, N. Fulda, J. R. Gubler,\nC. Rytting, D. Wingate, Out of one, many: Using lan-\nguage models to simulate human samples, Political\nAnalysis (2023) 1–15 doi:10.1017/pan.2023.2.\n11. D. M. Beskow, K. M. Carley, Agent based simula-\ntion of bot disinformation maneuvers in twitter, in:\n2019 Winter Simulation Conference (WSC), 2019,\npp. 750–761. doi:10.1109/WSC40007.2019.\n9004942.\n12. P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neu-\nbig, Pre-train, prompt, and predict: A systematic\nsurvey of prompting methods in natural language\nprocessing, ACM Computing Surveys 55 (1 2023).\ndoi:10.1145/3560815.\n13. F . Fui-Hoon Nah, R. Zheng, J. Cai, K. Siau, L. Chen,\nGenerative AI and ChatGPT: Applications, chal-\nlenges, and AI-human collaboration, Journal of Infor-\nmation Technology Case and Application Research\n25 (3) (2023) 277–304. doi:doi.org/10.1080/\n15228053.2023.2233814.\n14. M. Sharma, K. Singh, P . Aggarwal, V. Dutt, How well\ndoes gpt phish people? an investigation involving\ncognitive biases and feedback, in: 2023 IEEE Eu-\nropean Symposium on Security and Privacy Work-\nshops (EuroS&PW), IEEE Computer Society, Los\nAlamitos, CA, USA, 2023, pp. 451–457. doi:10.\n1109/EuroSPW59978.2023.00055.\n15. R. G.-B. Alejo José G. Sison, Marco Tulio Daza,\nE. C. Garrido-Merchán, ChatGPT: More Than a\n“Weapon of Mass Deception” Ethical Challenges and\nMarch 2024LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open Challenges 11\nSynthetic Realities and Artificial Intelligence-Generated Contents\nResponses from the Human-Centered Artificial In-\ntelligence (HCAI) Perspective, International Journal\nof Human–Computer Interaction (2023) 1–20 doi:\n10.1080/10447318.2023.2225931.\nJavier Pastor-Galindo, is a postdoctoral researcher\nat the Department of Information and Communication\nEngineering at the University of Murcia, Spain. His re-\nsearch interests include open source intelligence (OS-\nINT), cybersecurity, cyberdefence, data science and\ndisinformation. Contact him at javierpg@um.es.\nPantaleone Nespoli, corresponding author, is a post-\ndoctoral researcher working together with the Depart-\nment of Information and Communication Engineering\nat the University of Murcia, Spain, and the SCN team\nof the SAMOVAR laboratory, at Institut Polytechnique\nde Paris. His research is focused on cybersecurity\nand cyberdefence training, with a particular interest in\nthe detection and response to intrusions, and disin-\nformation in social networks. Contact him at pantale-\none.nespoli@um.es.\nJosé A. Ruipérez-Valiente, is an Associate Professor\nat the Department of Information and Communication\nEngineering at the University of Murcia, Spain. His\nresearch interests include educational technology, se-\nrious games, data science and cyberdefence. He re-\nceived his PhD degree in Telematics Engineering from\nUniversidad Carlos III of Madrid. He is an IEEE Senior\nMember. Contact him at jruiperez@um.es.\n12 LLM-Powered Agent-based Framework for Misinformation and Disinformation Research: Opportunities and Open ChallengesMarch 2024",
  "topic": "Disinformation",
  "concepts": [
    {
      "name": "Disinformation",
      "score": 0.938225269317627
    },
    {
      "name": "Generative grammar",
      "score": 0.640038013458252
    },
    {
      "name": "Computer science",
      "score": 0.46920838952064514
    },
    {
      "name": "Open research",
      "score": 0.4342108368873596
    },
    {
      "name": "Social media",
      "score": 0.3936856985092163
    },
    {
      "name": "Data science",
      "score": 0.374412477016449
    },
    {
      "name": "Internet privacy",
      "score": 0.3537050485610962
    },
    {
      "name": "Political science",
      "score": 0.34955573081970215
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3329734802246094
    },
    {
      "name": "World Wide Web",
      "score": 0.26144325733184814
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I80180929",
      "name": "Universidad de Murcia",
      "country": "ES"
    }
  ],
  "cited_by": 2
}