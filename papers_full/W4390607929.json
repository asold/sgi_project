{
  "title": "Identifying and Extracting Rare Diseases and Their Phenotypes with Large Language Models",
  "url": "https://openalex.org/W4390607929",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4303297942",
      "name": "Cathy Shyr",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2096506771",
      "name": "Yan Hu",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A1905904404",
      "name": "Lisa Bastarache",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2279855920",
      "name": "Alex Cheng",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2104700563",
      "name": "Rizwan Hamid",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1996827311",
      "name": "Paul Harris",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A4303297942",
      "name": "Cathy Shyr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096506771",
      "name": "Yan Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1905904404",
      "name": "Lisa Bastarache",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2279855920",
      "name": "Alex Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104700563",
      "name": "Rizwan Hamid",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1996827311",
      "name": "Paul Harris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2973267506",
    "https://openalex.org/W4306684931",
    "https://openalex.org/W2128959663",
    "https://openalex.org/W2028127763",
    "https://openalex.org/W4226189594",
    "https://openalex.org/W2090246235",
    "https://openalex.org/W3032794980",
    "https://openalex.org/W3146746902",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W6600106573",
    "https://openalex.org/W6600561556",
    "https://openalex.org/W4367041395",
    "https://openalex.org/W3191499914",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3198335263",
    "https://openalex.org/W1487381889",
    "https://openalex.org/W3154100408",
    "https://openalex.org/W2951146425",
    "https://openalex.org/W2043723189",
    "https://openalex.org/W2884181551",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W3201983976",
    "https://openalex.org/W6605684554",
    "https://openalex.org/W4385757404",
    "https://openalex.org/W2984857708"
  ],
  "abstract": "Abstract Purpose Phenotyping is critical for informing rare disease diagnosis and treatment, but disease phenotypes are often embedded in unstructured text. While natural language processing (NLP) can automate extraction, a major bottleneck is developing annotated corpora. Recently, prompt learning with large language models (LLMs) has been shown to lead to generalizable results without any (zero-shot) or few annotated samples (few-shot), but none have explored this for rare diseases. Our work is the first to study prompt learning for identifying and extracting rare disease phenotypes in the zero- and few-shot settings. Methods We compared the performance of prompt learning with ChatGPT and fine-tuning with BioClinicalBERT. We engineered novel prompts for ChatGPT to identify and extract rare diseases and their phenotypes (e.g., diseases, symptoms, and signs), established a benchmark for evaluating its performance, and conducted an in-depth error analysis. Results Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.610 in the zero- and few-shot settings, respectively). However, ChatGPT achieved higher accuracy for rare diseases and signs in the one-shot setting (F1 of 0.778 and 0.725). Conversational, sentence-based prompts generally achieved higher accuracy than structured lists. Conclusion Prompt learning using ChatGPT has the potential to match or outperform fine-tuning BioClinicalBERT at extracting rare diseases and signs with just one annotated sample. Given its accessibility, ChatGPT could be leveraged to extract these entities without relying on a large, annotated corpus. While LLMs can support rare disease phenotyping, researchers should critically evaluate model outputs to ensure phenotyping accuracy.",
  "full_text": "Journal of Healthcare Informatics Research (2024) 8:438–461\nhttps://doi.org/10.1007/s41666-023-00155-0\nRESEARCH ARTICLE\nIdentifying and Extracting Rare Diseases and Their\nPhenotypes with Large Language Models\nCathy Shyr, Yan Hu, Lisa Bastarache, Alex Cheng, Rizwan Hamid, Paul Harris,\net al. [full author details at the end of the article]\nReceived: 9 August 2023 / Revised: 24 October 2023 / Accepted: 13 November 2023 /\nPublished online: 5 January 2024\n© The Author(s) 2024\nAbstract\nPurpose Phenotyping is critical for informing rare disease diagnosis and treatment, but\ndisease phenotypes are often embedded in unstructured text. While natural language\nprocessing (NLP) can automate extraction, a major bottleneck is developing annotated\ncorpora. Recently, prompt learning with large language models (LLMs) has been\nshown to lead to generalizable results without any (zero-shot) or few annotated samples\n(few-shot), but none have explored this for rare diseases. Our work is the ﬁrst to study\nprompt learning for identifying and extracting rare disease phenotypes in the zero-\nand few-shot settings.\nMethods We compared the performance of prompt learning with ChatGPT and ﬁne-\ntuning with BioClinicalBERT. We engineered novel prompts for ChatGPT to identify\nand extract rare diseases and their phenotypes (e.g., diseases, symptoms, and signs),\nestablished a benchmark for evaluating its performance, and conducted an in-depth\nerror analysis.\nResults Overall, ﬁne-tuning BioClinicalBERT resulted in higher performance (F1 of\n0.689) than ChatGPT (F1 of 0.472 and 0.610 in the zero- and few-shot settings, respec-\ntively). However, ChatGPT achieved higher accuracy for rare diseases and signs in\nthe one-shot setting (F1 of 0.778 and 0.725). Conversational, sentence-based prompts\ngenerally achieved higher accuracy than structured lists.\nConclusion Prompt learning using ChatGPT has the potential to match or outperform\nﬁne-tuning BioClinicalBERT at extracting rare diseases and signs with just one anno-\ntated sample. Given its accessibility, ChatGPT could be leveraged to extract these\nentities without relying on a large, annotated corpus. While LLMs can support rare\ndisease phenotyping, researchers should critically evaluate model outputs to ensure\nphenotyping accuracy.\nB Paul Harris\npaul.a.harris@vumc.org\nB Hua Xu\nhua.xu@yale.edu\nExtended author information available on the last page of the article\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 439\nKeywords Natural language processing · ChatGPT · Rare disease ·\nArtiﬁcial intelligence · Prompt learning · Large language model\n1 Introduction\nRare diseases are chronically debilitating, often life-limiting conditions that affect 300\nmillion individuals worldwide [1]. Though individually rare (deﬁned as affecting fewer\nthan 200,000 individuals in the United States), rare diseases are collectively common\nand represent a serious public health concern [ 2]. Because of the lack of knowledge and\neffective treatment options for rare diseases, patients undergo diagnostic and therapeu-\ntic odysseys that have devastating medical, psychosocial, and economic consequences\nfor patients and families, resulting in irreversible disease progression, physical suf-\nfering, emotional turmoil, and ongoing high medical costs [ 3–5]. Thus, there is an\nurgent need to shorten rare disease odysseys, and reaching this goal requires effective\ndiagnostic and treatment strategies.\nPhenotyping is crucial for informing both strategies and is a cornerstone of the study\non rare diseases. Ongoing initiatives like the National Institutes of Health’s Undiag-\nnosed Diseases Network rely on deep phenotyping to generate candidate diseases for\ndiagnosis, identify additional patients with similar clinical manifestations, and person-\nalize treatment or disease management strategies [ 6, 7]. In addition, phenotyping can\nfacilitate cohort identiﬁcation and recruitment for clinical trials critical to the develop-\nment of novel treatment regimes [ 8, 9]. Rare disease phenotypes are often embedded\nin unstructured text and require manual extraction by highly trained experts, which\nis laborious, costly, and susceptible to bias depending on the clinician’s background\nand training. An alternative is to leverage natural language processing (NLP) models,\nwhich have the potential to automatically identify and extract rare disease entities,\nreduce manual workload, and improve phenotyping efﬁciency.\nAutomatic recognition of disease entities, or named entity recognition (NER), is\nan NLP task that involves the identiﬁcation and categorization of disease information\nfrom unstructured text. This task is especially challenging due to the diversity, com-\nplexity, and speciﬁcity of rare diseases and their phenotypes, which can have different\nsynonyms (e.g., neuroﬁbromatosis type I and V on Recklinghausen’s Disease), abbre-\nviations (e.g., NF1 for neuroﬁbromatosis type I), and modiﬁers such as body location\n(e.g., small holes in front of the ear) and severity (e.g., extreme nearsightedness).\nDescriptions of rare disease phenotypes that are discontinuous, nested, or overlapping\npresent additional challenges; moreover, those that range from short phrases in lay-\nman’s terms (e.g., distention of the kidney) to medical jargon (e.g., hydronephrosis)\nmay further complicate NER.\nWhile early approaches for NER relied on rules derived from extensive manual\nanalysis, advancements in technology led to the emergence of large language mod-\nels (LLMs), artiﬁcial intelligence systems built using deep learning techniques [ 10].\nSpeciﬁcally, LLMs use a deep neural network architecture called transformers that\nenable models to learn complex language patterns, capture long-range dependen-\ncies, and generate coherent responses [ 11]. LLMs are the bedrock of two major NER\nparadigms: 1) pre-train and ﬁne-tuning and 2) pre-train and prompt learning .W e\n123\n440 Journal of Healthcare Informatics Research (2024) 8:438–461\nhenceforth refer to these paradigms as ﬁne-tuning and prompt learning, respectively.\nThe former involves a two-step process where a language model is ﬁrst trained on a\nmassive amount of unlabeled text data and then ﬁne-tuned on speciﬁc downstream\nNER tasks with labeled data. In contrast, prompt learning is a more recent paradigm\nthat reformulates the NER task as textual prompts so that the model itself learns to\npredict the desired output in the second step.\nWhile ﬁne-tuning LLMs has been shown to achieve strong performance on bench-\nmark datasets [ 12], a major bottleneck is the development of large, annotated corpora.\nRecently, OpenAI released ChatGPT, a revolutionary LLM capable of following\ncomplex prompts and generating high-quality responses without any annotated data\n(zero-shot) or with just a few examples (few-shot) [ 13–16]. This capability, which\nprovides opportunities to signiﬁcantly reduce the manual burden of annotation with-\nout sacriﬁcing model performance, is especially attractive for NER in the context of\nrare diseases. While some explored the potential of ChatGPT for diagnosing rare dis-\neases with human-provided suggestions [ 17, 18], none have studied its performance\nfor NER in the zero- or few-shot settings.\nTo this end, our study makes the following contributions. 1) This work is the\nﬁrst to explore prompt learning for biomedical NER in the context of rare diseases.\nSpeciﬁcally, we designed new prompts for ChatGPT to extract rare diseases and their\nphenotypes (i.e., diseases, symptoms, and signs) in the zero- and few-shot settings.\n2) We established a benchmark for evaluating ChatGPT’s NER performance on a\nhigh-quality corpus of annotated descriptions on rare diseases [ 19]. In addition, we\ncompared prompt learning to ﬁne-tuning by training and evaluating BioClinicalBERT,\na domain-speciﬁc Bidirectional Encoder Representations from Transformers (BERT)\nmodel, on the annotated corpus [ 20]. 3) We conducted an in-depth error analysis to\nelucidate ChatGPT’s performance and 4) provided suggestions to help guide future\nwork on prompt learning for rare diseases.\n2 LiteratureReview\nDespite the proliferation of studies on NLP over the past decade, the task of NER\nis relatively under-explored for rare diseases. In this section, we provide a summary\nof prior contributions speciﬁc to extracting rare diseases and their phenotypes from\nunstructured text. These contributions can be broadly divided into two categories based\non the NLP approach: 1) rule-based and 2) deep learning. Among those in the second\ncategory, only one explored ﬁne-tuning [ 21]; to the best of our knowledge, none have\nexplored prompt learning for rare disease NER to date.\nUsing rule-based algorithms, Davis et al. [ 22] identiﬁed individuals with multiple\nsclerosis from clinical notes in electronic health records (EHR). The authors manually\nreviewed patient notes to determine relevant keywords on disease progression and\ntype, which were then used to build rule-based algorithms. For example, the algo-\nrithm for identifying the year of initial neurological symptom selected 100 characters\naround phrases referencing the beginning of the disease course, i.e., “dating back\" and\n“began\". Lo et al. [ 23] extracted phenotypes related to Dravet syndrome from clinical\nnotes using the Uniﬁed Medical Language System Metathesaurus’ subset of 20,000\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 441\nphenotypic words or expressions. Deisseroth et al. [ 24] developed ClinPhen, a rule-\nbased phenotype extractor for genetic diseases that automatically converts clinical\nnotes into a prioritized list of patient phenotypes using Human Phenotype Ontology\nterms. Nigwekar et al. [ 25] used an unnamed NLP software to identify patients with\nthe terms “calciphylaxis\" or “calciﬁc uremic arteriolopathy\" in their medical records.\nRecently, Fabregat et al. [ 26] and Segura-Bedmar et al. [ 21] leveraged deep learning\ntechniques, including bidirectional long short term memory (BiLSTM) networks and\nBERT-based models, to recognize rare diseases and their clinical manifestations from\nbiomedical texts. Fabregat et al.’s BiLSTM model is a recurrent neural network that\nsequentially processes the input text from both forward and backward directions,\nallowing the model to learn contextual information on both sides. In their work, Segura-\nBedmar et al. explored a similar model architecture and found that using a conditional\nrandom ﬁeld (CRF) as the output layer led to improved performance. In addition, the\nauthors trained domain-speciﬁc BERT models by ﬁne-tuning them on the downstream\nNER task. Overall, ﬁne-tuning BERT models had the highest accuracy, outperforming\nboth BiLSTM and BiLSTM with a CRF layer.\n3 Methods\n3.1 Problem Definition\nOur objective is to identify and extract rare disease-related named entities, which\nare words or phrases that belong to the pre-deﬁned categories: rare disease, disease,\nsymptom, or sign. As such, we seek to build an NER model that classiﬁes each input\ntoken into a pre-deﬁned category. Formally, given a sequence of n input tokens X =\n{x\n1, x2,..., xn}, the true label (i.e., gold-standard annotation) is the vector Y :=\n{y1, y2,..., ym } where\ny j ={ xstart j : xend j , t j }, 0 ≤ j ≤ m ≤ n\nis the tuple for the jth entity. Here, start j ∈[ 1, n] and end j ∈[ 1, n] denote the\nstarting and ending indices of the jth entity, respectively, where start j ≤ end j and\nt j ∈{ rare disease, disease, symptom, sign } is the entity type. We let\nxstart j : xend j =[ xstart j xstart j+1 ··· xend j−1 xend j ]\ndenote the textual span from xstart j to xend j and let ˆY := {ˆy1, ˆy2,..., ˆy ˆm } denote the\nmodel-predicted label vector where\nˆyk ={ x ˆstartk : x ˆendk , ˆtk }, 0 ≤ k ≤ˆm ≤ n\nis the tuple for the kth predicted entity. Figure 1 shows an example where an NER\nmodel recognizes one of two named entities from the input, “Keratomalacia is a cause\nof corneal scarring.\" Here, the model correctly identiﬁed the rare disease, “keratoma-\nlacia,\" but missed “corneal scarring\" as a sign.\n123\n442 Journal of Healthcare Informatics Research (2024) 8:438–461\nFig. 1 Example of the rare disease named entity recognition task. {xi }7\ni=1 denotes the sequence of input\ntokens, Y ={ y1, y2} the true labels with m = 2 entities, and ˆY ={ ˆy1} the predicted label with ˆm = 1\nentity\n3.2 Dataset\nTo study the NER performance of LLMs for rare diseases, we used the RareDis cor-\npus, which consists of n = 832 texts containing descriptions of rare diseases from the\nNational Organization for Rare Disorders database [ 19]. This corpus was annotated\nwith four entities (rare disease, disease, symptom, and sign) by biomedical experts who\nhad an inter-annotator agreement (IAA) F1-score of 83.5% under exact match, indi-\ncating a high level of annotation consistency and reliability. Speciﬁcally, the F1-score\nmeasures the IAA accounting for precision (proportion of correctly annotated entities)\nand recall (proportion of gold-standard entities that were annotated). Table 1 provides\nthe entity deﬁnitions and summary statistics. Unlike corpora with distinct entity types,\ne.g., {person, location, organization} or {problem, test, treatment}, RareDis consists\nof entities with considerable semantic overlap. Speciﬁcally, rare diseases are a subset\nof diseases. Diseases can cause or be associated with other diseases as a symptom\nor sign. The distinction between symptoms and signs is very subtle; while both are\nabnormalities that may indicate a disease, the former are subjective to the patient and\ncannot be measured by tests or observed by physicians (e.g., pain or loss of appetite).\nOn the other hand, a sign can be measured or observed (e.g., high blood pressure,\npoor lung function). Across n = 832 texts, there were a total 4,065 rare diseases,\n1,814 diseases, 316 symptoms, and 3,317 signs. Rare diseases and signs were more\ncommon than diseases and symptoms, accounting for 77% of all entities in the corpus\n(Table 1). A subset of the RareDis corpus (832 out of 1041 texts) is publicly available\nand distributed in the Brat standoff format [ 27].\n3.3 NER Paradigms\nIn this section, we describe our approach to performing NER with LLMs under two\nparadigms: 1) ﬁne-tuning and 2) prompt learning.\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 443\nTable 1 Entity deﬁnitions and summary statistics\nEntity Deﬁnition Examples Total Count Per Text\nCount Mean (SD)\nRare disease Diseases which affect a small number of people cat eye syndrome, 4,065 4.88 (3.57)\ncompared to the general population Marfan syndrome\nDisease An abnormal condition of a part, organ, or system cancer, cardiovascular 1,814 2.18 (2.59)\nof an organism resulting from various causes, such disease\nas infection, inﬂammation, environmental factors,\nor genetic defect, and characterized by an identiﬁable\ngroup of signs, symptoms, or both\nSymptom A physical or mental problem that may indicate fatigue, pain 316 0.38 (1.23)\na disease or condition; cannot be seen and do not\nshow up on medical tests\nSign A physical or mental problem that may indicate rash, abnormal heart 3,317 3.98 (4.89)\na disease or condition; can be seen and shows up rate\non medical tests\nTotal count represents the number of entity occurrence in the entire corpus. SD = standard deviation\n123\n444 Journal of Healthcare Informatics Research (2024) 8:438–461\n3.3.1 Fine-tuning BERT-Based Model\nFor ﬁne-tuning, we chose BERT as our LLM for two reasons. First, BERT is one\nof the most widely-used deep contextualized language models, achieving state-of-\nthe-art performance on benchmark NER datasets [ 12]. Speciﬁcally, its transformer\narchitecture captures long-range dependencies in the input text and supports parallel\nprocessing, thereby enabling contextualized learning and reducing computational bur-\nden. Second, Segura-Bedmar et al. [ 21] found that ﬁne-tuning BERT models resulted\nin the best NER performance on the RareDis corpus. Therefore, we adopted the same\napproach for a consistent comparison.\nFigure 2 illustrates the architecture of the BERT model. To ﬁne-tune this model\non the RareDis corpus, we performed a series of pre-processing tasks. First, we split\nthe texts into tokens with the BERT tokenizer and added special tokens (i.e., CLS and\nSEP) to the beginning and end of each tokenized sequence, respectively. Next, we\nconverted the tokens to their respective IDs and padded (or truncated) text sequences\nbased on the maximum number of tokens (i.e., 512) that a BERT-based model can\nhandle, and created an attention mask to distinguish between actual and padding\ntokens. Last, we mapped the entity labels, {rare disease, disease, symptom, sign}, to\ncorresponding numerical values. We partitioned the data into a training, validation,\nand test set based on an 8:1:1 ratio. For the base architecture, we selected BioClin-\nicalBERT [ 20], a variant of BERT that was pre-trained on large-scale biomedical\n(PubMed, ClinicalTrials.gov) and clinical corpora (MIMIC-III [ 28]). The model ﬁne-\ntuning parameters were learning rate = 2e-5, weight decay rate = 0.1, batch size =\n32, and dropout = 0.1. BioCinicalBERT takes as input the sequence of tokens and\nproduces context-based embeddings. These embeddings are then passed through a\nstack of transformer encoder layers that capture bidirectional, contextual informa-\ntion from each token. The layers output contextualized representations of the tokens,\nwhich are used to produce a probability distribution over output labels. Speciﬁcally,\nwe used BIO (beginning, inside, outside) tags to represent the output labels, where\nFig. 2 Architecture of (a) BERT and (b) GPT. {xi }n\ni=1 denotes the sequence of input tokens. CLS and SEP\nare special tokens that represent classiﬁcation and separation, respectively. E and R denote embeddings\nand representations, respectively\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 445\nB-t and I-t correspond to the ﬁrst token and continuation of an entity mention of type\nt ∈{ rare disease, disease, symptom, sign }, respectively, and O for other tokens. Fig-\nure 2a shows an example where “Keratomalacia\" and “scarring\" were labeled B-rare\ndisease and I-sign, respectively.\nThe BIO tags directly correspond to the model-predicted label vector deﬁned in\nSection 3.1. For predicted entity k < ˆm with type ˆtk , the predicted starting token\nx ˆstartk corresponds to the input token with a B-ˆtk tag, and the predicted ending token\nx ˆendk corresponds to the last input token with an I-ˆtk tag before the next token with\na B-ˆtk+1 tag, where ˆtk may or may not be the same as ˆtk+1. For example, the kth\npredicted entity has type ˆtk = rare disease, whereas the k + 1st has type ˆtk+1 = sign.\nIf k =ˆm, i.e., the kth predicted entity is the last mention in the input sequence, then\nthe predicted ending token x ˆendk is the last input token with an I-ˆtk tag.\n3.3.2 Prompt Learning Using GPT-Based Model\nIn this section, we describe our approach to reformulating NER as a text generation\ntask in the zero- and few-shot settings using OpenAI’s ChatGPT (GPT-3.5 turbo).\nThe former refers to instructing the model to extract entities directly from an input\ntext in the test set, and the latter is similar except we also provide an example of\nextracted entities from a training text. All experiments were performed using OpenAI’s\napplication programming interface with the model gpt-3.5-turbo on June 19th\nand 20th, 2023. We used a temperature of 0 so that ChatGPT always selected the most\nlikely token in its response to ensure reproducibility.\nPrompt design. Table 2 provides a summary of prompts in the zero- and few-shot\nsettings. The ﬁve main building blocks of our prompt designs were 1) task instruction,\n2) task guidance, 3) output speciﬁcation, 4) output retrieval, and, in the few-shot setting,\n5) a speciﬁc example. Task instruction conveys the overall set of directions for NER\nin a speciﬁc but concise manner. To prevent ChatGPT from rephrasing entities, we\ninstructed it to extract their exact names from the input text. Task guidance provides\nentity deﬁnitions from the original RareDis annotation guidelines. The objective is to\nhelp ChatGPT differentiate between entity types within the context of the input text,\nas all four entities overlap semantically. Output speciﬁcation instructs ChatGPT to\noutput the extracted entities in a speciﬁc format to reduce post-processing workload.\nOutput retrieval prompts the model to generate a response. In the few-shot setting, we\nalso provided an example with an input text from the training set and its gold standard\nlabels (i.e., entities labeled by the annotators).\nPrompt format. In each setting, we experimented with two prompt formats: simple\nand structured (Table 2). The former presents the prompt as a simple sentence, and\nthe latter a structured list. The simple sentence is shorter in length and resembles\nhuman instructions provided in a conversational setting where different building blocks\n(i.e., task instruction, task guidance, and output speciﬁcation) are woven together\nas a single unit. Agrawal et al. [ 14] and Hu et al. [ 15] used a similar approach to\nextract medications and clinical entities, respectively. In contrast, the structured list\nresembles a recipe or outline that consists of multiple sub-prompts in a speciﬁc order.\nChen et al. [ 16] used a similar format for evaluating ChatGPT’s NER performance\n123\n446 Journal of Healthcare Informatics Research (2024) 8:438–461Table 2 Summary of prompts\nSetting Type Prompt Example\nZero-shot Simple Extract the exact names of [ entity] , Extract the exact names of rare diseases ,\nwhich are [ defn] , from this passage which are diseases\nand output them in a list: that affect a small number of individuals,\n\"[text from test set ]\". from this passage and output them in a list:\n\"The exact prevalence and incidence abetalipoproteinemia\nis unknown, but it is estimated to affect ···\n..\n.\n··· incidence of consanguineous marriages. Symptoms\nusually become apparent during infancy.\"\nStructured ###Task: ###Task:\nExtract the exact names of [ entity] Extract the exact names of rare diseases\nfrom the input text and output them from the input text and output them\nin a list. in a list.\n### Definition: ### Definition:\n[entity]s are defined as [ defn] . Rare diseases are defined as diseases that affect\na small number of individuals.\n### Input text: [ text from test set ]. ### Input text: \"The exact prevalence and incidence of\nabetalipoproteinemia is unknown, but it is estimated\nto affect ···\n..\n.\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 447Table 2 continued\nSetting Type Prompt Example\n··· incidence of consanguineous marriages. Symptoms\nusually become apparent during infancy.\"\n### Output: ### Output:\nFew-shot Simple Passage: [ text from training set ]. Passage: \"Binder type nasomaxillary dysplasia\nis a rare congenital\ncondition that affects males and females in equal\nnumbers ···\n...\n··· suggests that Binder syndrome occurs in less than\n1 per 10,000 live births.\"\nExtract the exact names of Extract the exact names of rare diseases, which are\n[entity], which are [ defn], diseases that affect a small number of individuals,\nfrom this passage and output from this passage and output them in a list:\nthem in a list:\n[gold standard training labels ]. Blinder type nasomaxillary dysplasia, Blinder\nsyndrome\nPassage: [ text from test set ]. Passage: \"The exact prevalence and incidence of\nabetalipoproteinemia is unknown, but it is estimated\nto affect ···\n..\n.\n123\n448 Journal of Healthcare Informatics Research (2024) 8:438–461Table 2 continued\nSetting Type Prompt Example\n··· incidence of consanguineous marriage. Symptoms\nusually become apparent during infancy.\"\nExtract the exact names of [ entity], Extract the exact names of [ entity],\nwhich are [ defn], from this passage which are [ defn], from this passage\nand output them in a list: and output them in a list:\nStructured ### Task: ### Task:\nExtract the exact names of [ entity], Extract the exact names of rare diseases,\nfrom the input and output them from the input text and output them\ntext in a list. in a list.\n### Definition: ### Definition:\n[entity]s are defined as [ defn]. Rare diseases are defined as diseases that\naffect a small number of individuals.\n### Input text: ### Input text: \"Blinder type nasomaxillary dysplasia\n[text from training set ] is a rare congenital condition that affects males\nand females in equal ···\n..\n.\n··· suggests that Binder syndrome occurs in less than\n1 per 10,000 live births.\"\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 449Table 2 continued\nSetting Type Prompt Example\n### Output: [ gold standard training ### Output: Blinder type nasomaxillary dysplasia,\nlabels] Blinder syndrome\n### Input text: [ text from test set ] ### Input text: \"The exact prevalence and incidence of\nabetalipoproteinemia is unknown, but it is estimated\nto affect ···\n...\n··· incidence of consanguineous marriages. Symptoms\nusually become apparent during infancy.\"\n###\nOutput: ### Output:\nDifferent parts of the prompt are color-coded as follows: Task instruction , Task guidance , Output speciﬁcation , Output retrieval , and Speciﬁc example . [ entity]a n d\n[defn] represent the entity and corresponding deﬁnition from Table 1\n123\n450 Journal of Healthcare Informatics Research (2024) 8:438–461\non benchmark biomedical datasets. To provide additional guidance for ChatGPT, we\nalso incorporated distinguishing characteristics about each entity in their prompts\nsupplemented with examples (Table 3).\nFew-shot example selection. We explored two strategies for selecting an example\ntext in the few-shot setting. The ﬁrst strategy involved randomly selecting a text from\nthe training set, and the second selecting the training text that was most similar to\nthe test text. The motivation for the second strategy was that different rare diseases\nmay have similar etiology, course of progression, and symptoms/signs. For example,\nCreutzfeldt-Jakob disease and CARASIL (cerebral autosomal recessive arteriopathy\nwith subcortical infarcts and leukoencephalopathy) are rare, neurological diseases\nthat share similar signs, including progressive deterioration of cognitive processes\nand memory. Thus, providing a training text most similar to the test text may improve\nChatGPT’s performance. To implement this strategy, we selected the training text\nwith the highest similarity score based on spaCy’s pre-trained word embeddings and\nincorporated it as an example in the few-shot prompt [ 29]. We repeated this process\nfor each text in the test set.\nFigure 2b illustrates the architecture of the GPT model. In contrast to BERT, GPT\nuses a stack of transformer decoder layers aimed at autoregressive (left to right) text\ngeneration, i.e., predicting the next token based on preceding context. GPT takes\nas input a sequence of tokens for the prompt in addition to texts from the RareDis\ncorpus and produces embeddings, which are then passed through decoder layers\nto produce contextualized representations. Unlike BERT, GPT does not use spe-\ncial tokens like CLS or SEP. Based on our prompts, the model directly outputs\nthe predicted entities in a list separated by commas. Figure 2b shows an exam-\nple where “Keratomalacia\" and “corneal necrosis\" were identiﬁed as rare disease\nentities. We performed post-processing to remove separating commas and, using\nthe notation deﬁned in Section 3.1, the predicted output vector in this example is\nˆY ={ˆy\n1, ˆy2}={ { Keratomalacia, rare disease }, {corneal necrosis, rare disease }}.\n3.4 Evaluation\n3.4.1 Metrics\nTo evaluate model performance on the test set, we computed the following evalu-\nation metrics: precision, recall, and F1-score. Precision = Number correctly predicted\nˆm is\nthe proportion of predicted entities found by the model that were correct, and recall\n= Number correctly predicted\nm the proportion of gold standard entities identiﬁed by the\nmodel. F1 = 2×Precision×Recall\nPrecision+Recall accounts for both precision and recall by taking the\nharmonic mean. We calculated these metrics under two evaluation settings: exact and\nrelaxed. For an exact match on the jth entity, the true and predicted entities must share\nthe same boundaries and entity type, i.e., xstart j = x ˆstart j , xend j = x ˆend j and t j = ˆt j .\nFor a relaxed match, the predicted and true entity must overlap in their textual spans\nand have the same entity type, i.e., {xstart j : xend j }∩{ x ˆstart j : x ˆend j } ̸=∅ and t j = ˆt j .\nTo ensure that stop words did not inﬂuence the evaluation, we removed them from\nboth the gold standard and model-predicted entities.\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 451\nTable 3 Distinguishing characteristics of each entity\nEntity Distinguishing Characteristics\nRare disease Rare diseases often come with terms like “rare\", “uncommon\", or mentions of speciﬁc low-prevalence numbers.\nFor example, in the sentence, “Ablepharon-Macrostomia Syndrome (AMS) is an extremely rare inherited disorder,\"\n“Ablepharon-Macrostomia Syndrome\" and “AMS\" are rare diseases, but “inherited disorder\" is not.\nDisease Diseases are generally recognized medical conditions. The mention of a disease might not necessarily come with\ndescriptors of its prevalence unless it’s rare. For example, in the sentence, \"Ablepharon-Macrostomia Syndrome\n(AMS) is an extremely rare inherited disorder,\" \"inherited disorder\" is a disease.\nSymptom Symptoms are subjective and detected by the patient. For example, in the sentence, \"In the acute form,\ndrowsiness, coma, and seizures may occur,\" \"drowsiness\" is a symptom, but \"coma\" and \"seizures\" are not.\nSign Signs can be measured or observed and don’t rely on the patient’s subjective reporting. For example,\nin the sentence, \"In the acute form, drowsiness, coma, and seizures may occur,\" \"coma\" and \"seizures\" are signs,\nbut \"drowsiness\" is not.\n123\n452 Journal of Healthcare Informatics Research (2024) 8:438–461\n3.4.2 Error Analysis\nIn our error analysis, we considered ﬁve types of errors: 1) incorrect boundary, 2)\nincorrect entity type, 3) incorrect boundary and entity type, 4) spurious, and 5) missed.\nThe ﬁrst refers to a predicted entity where one or both of its boundaries do not match\nthat of the gold standard label, i.e., for entity j, x\nstart j ̸= x ˆstart j , xend j ̸= x ˆend j , or both.\nThe second refers to a predicted entity with incorrect type, i.e., t j ̸= ˆt j . The third\nrefers to the case where neither the predicted entity’s boundaries nor type matches\nthose of the gold standard label. Spurious entities are predicted entities that do not\ncorrespond to any gold standard labels (false positive). In other words, predicted entity\nk is spurious if {x\nˆstartk : x ˆendk }∩{ xstart j : xend j }=∅ for all j ≤ n. Missed entities are\ntrue entities that the model failed to identify (false negative), i.e., entity j is missed if\n{x ˆstartk : x ˆendk }∩{ xstart j : xend j }=∅ for all k ≤ n.\n4 Results\n4.1 Overall Results\nFine-tuning vs. Prompt learning. Table 4 provides a summary of the model perfor-\nmance by entity type. Under exact match, ﬁne-tuning BioClinicalBERT resulted in\nF1-scores that ranged from 0.491 to 0.704, outperforming ChatGPT across all entity\ntypes. Under relaxed match, BioClinicalBERT achieved an overall F1-score of 0.689\nand outperformed ChatGPT on all entities except rare diseases and signs. For these\nentities, prompt learning using ChatGPT in the few-shot setting resulted in higher\nF1-scores of 0.778 (vs. 0.755) and 0.725 (vs. 0.704) for rare diseases and signs, respec-\ntively. In the few-shot setting, ChatGPT outperformed BioClinicalBERT in terms of\nrecall under relaxed match across all entity types.\nComparison across prompts. Overall, incorporating an example in the few-shot\nsetting led to improved performance over the zero-shot setting. Under relaxed match,\nChatGPT in the zero-shot setting achieved F1-scores of 0.472 and 0.407 with the\nsimple sentence and structured list prompts, respectively. Its performance improved\nin the few-shot setting, resulting in F1-scores of 0.591 and 0.469. Selecting a similar\ntraining text led to additional improvement, resulting in F1-scores of 0.610 and 0.544.\nCompared to prompts written as a structured list, simple sentences generally achieved\nsimilar or better performance; this trend was consistent across both zero- and few-shot\nsettings. Incorporating distinguishing characteristics in the prompt led to an increase\nin the overall F1-score in the zero-shot (structured list) and few shot (structured list\n+ random training text) settings. Moreover, this approach resulted in the highest F1-\nscore for rare diseases (F1 = 0.778) in the few shot (structured list + similar training\ntext) setting, outperforming BioClinicalBERT (F1 = 0.755).\nComparison across entities. Among the four entities, rare diseases were associated\nwith the highest accuracy for both models across all settings. In contrast, diseases\nwere challenging for both models. While BioClinicalBERT performed similarly at\nextracting signs and symptoms, ChatGPT achieved substantially better performance\nfor signs. This trend was consistent across both zero- and few-shot settings.\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 453Table 4 Summary of model performance by entity type\nExact Relaxed\nParadigm Model Setting Entity Precision Recall F1 Precision Recall F1\nFine-tuning BioClinicalBERT Supervised Rare disease 0.689 0.720 0.704 0.772 0.739 0.755\nDisease 0.494 0 .488 0 .491 0.532 0.538 0.535\nSign 0.561 0 .516 0.538 0.676 0.0735 0.704\nSymptom 0.667 0.630 0.648 0.704 0.745 0.724\nOverall 0.600 0 .583 0 .591 0 .681 0.698 0.689\nPrompt ChatGPT Zero-shot Rare disease 0.559 0.409 0.472 0.843 0.694 0.761\nlearning (Simple sentence) Disease 0.109 0.240 0.150 0.200 0.437 0.274\nSign 0.269 0.380 0.315 0.537 0.751 0.627\nSymptom 0.070 0.619 0.126 0.084 0.762 0.155\nOverall 0.203 0.369 0.262 0.365 0.670 0.472\nZero-shot Rare disease 0.765 0.489 0.597 0.887 0.634 0.740\n(Structured list) Disease 0.184 0.210 0.196 0.261 0.293 0.276\nSign 0.266 0.324 0.292 0.448 0.543 0.491\nSymptom 0.063 0.690 0.116 0.079 0.857 0.145\nOverall 0.226 0.359 0.277 0.331 0.528 0.407\nZero-shot Rare disease 0.663 0.613 0.637 0.821 0.763 0.791\n(Structured list Disease 0.138 0.263 0.181 0.199 0.377 0.261\n+ Distinguishing Sign 0.303 0.369 0.333 0.572 0.676 0.620\nCharacteristics) Symptom 0.068 0.643 0.123 0.086 0.810 0.156\nOverall 0.240 0.420 0.305 0.371 0.640 0.470\nFew-shot Rare disease 0.719 0.441 0.547 0.937 0.634 0.756\n(Simple sentence Disease 0.211 0.210 0.210 0.287 0.287 0.287\n+ Random example) Sign 0.457 0.409 0.432 0.721 0.671 0.695\n123\n454 Journal of Healthcare Informatics Research (2024) 8:438–461Table 4 continued\nExact Relaxed\nParadigm Model Setting Entity Precision Recall F1 Precision Recall F1\nSymptom 0.279 0.452 0.345 0.294 0.476 0.364\nOverall 0.423 0.376 0.398 0.616 0.568 0.591\nFew-shot Rare disease 0.569 0.532 0.550 0.750 0.758 0.754\n(Structured list Disease 0.151 0.341 0.209 0.211 0.467 0.291\n+ Random example) Sign 0.273 0.406 0.327 0.478 0.698 0.567\nSymptom 0.094 0.714 0.166 0.107 0.810 0.189\nOverall 0.237 0.440 0.308 0.361 0.668 0.469\nFew-shot Rare disease 0.677 0.608 0.640 0.812 0.769 0.790\n(Structured list Disease 0.131 0.341 0.189 0.186 0.473 0.267\n+ Random example Sign 0.268 0.366 0.310 0.539 0.743 0.625\n+ Distinguishing Symptom 0.072 0.548 0.127 0.100 0.762 0.177\nCharacteristics) Overall 0.230 0.429 0.300 0.370 0.692 0.483\nFew-shot Rare disease 0.818 0.484 0.608 0.967 0.634 0.766\n(Simple sentence Disease 0.206 0.246 0.224 0.286 0.341 0.311\n+ Similar example) Sign 0.441 0.444 0.443 0.720 0.730 0.725\nSymptom 0.260 0.310 0.283 0.308 0.381 0.340\nOverall 0.422 0.403 0.412 0.617 0.603 0.610\nFew-shot Rare disease 0.590 0.565 0.577 0.762 0.790 0.776\n(Structured list Disease 0.199 0.437 0.273 0.297 0.653 0.408\n+ Similar example) Sign 0.337 0.487 0.398 0.561 0.802 0.660\nSymptom 0.093 0.690 0.164 0.114 0.833 0.200\nOverall 0.278 0.506 0.359 0.421 0.769 0.544\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 455Table 4 continued\nExact Relaxed\nParadigm Model Setting Entity Precision Recall F1 Precision Recall F1\nFew-shot Rare disease 0.596 0.586 0.591 0.766 0.790 0.778\n(Structured list Disease 0.182 0.473 0.263 0.248 0.635 0.356\n+ Similar example Sign 0.310 0.495 0.381 0.535 0.818 0.647\n+ Distinguishing Symptom 0.076 0.619 0.135 0.091 0.738 0.162\nCharacteristics) Overall 0.257 0.519 0.343 0.385 0.767 0.513\nBest scores by entity type are bolded\n123\n456 Journal of Healthcare Informatics Research (2024) 8:438–461\n4.2 Detailed Error Analysis\nWe conducted an in-depth error analysis to elucidate ChatGPT’s performance. This\nanalysis was crucial for gaining additional insight, as unlike other biomedical corpora,\nRareDis contains entities with overlapping semantics. Speciﬁcally, rare diseases are\nsimilar to diseases, and symptoms to signs. Depending on the context of the input text,\ndiseases can also be symptoms or signs.\nTable 5 shows the distribution of errors in the few-shot setting (simple sentence +\nrandom example) under exact match. The most common error type for rare diseases\nis false negative (45%) followed by incorrect entity type (31%). In the case of entity\ntype errors, ChatGPT tended to label rare diseases as diseases. For diseases, signs,\nand symptoms, false positives and false negatives were the most common error types.\nBased on manual review, many of these errors can be attributed to the challenge of\ndifferentiating among these entities. Speciﬁcally, ChatGPT’s under-performance may\nbe attributed to the challenge of inferring contextual meaning. For example, in the\nsentence, “a large percentage of primary antiphospholipid syndrome (APS) patients\nare women with recurrent pregnancy loss,\" the entity “recurrent pregnancy loss\" was\nused to describe a population of women who have APS. However, ChatGPT mis-\ntakenly identiﬁed it as a sign of APS. Another challenge is differentiating between\nsigns (observable and/or measurable) and symptoms (subjective to the patient/non-\nmeasurable). For example, ChatGPT mistakenly identiﬁed “weight loss\" and “fever\"\nas symptoms. In another example, it labeled “fatigue\" as both a symptom and a sign,\nsuggesting that it was challenging to for the model to understand the subtle difference\nbetween the two entities. In other cases, gold standard labels deviated from the def-\ninitions provided in the annotation guidelines, as the lack of abnormalities was also\nlabeled as an entity (i.e., “asymptomatic during infancy or childhood\" was labeled as\na symptom by the annotators). As such, a portion of false negatives could be attributed\nto these edge cases.\n5 Discussion\nIn this work, we reformulated NER as a text generation task and established a bench-\nmark for ChatGPT’s performance on extracting rare disease phenotypes. Overall,\nwhile ﬁne-tuning BioClinicalBERT led to better performance, prompt learning using\nChatGPT achieved similar or higher accuracy for some entities (i.e., rare diseases and\nsigns) with a single example, demonstrating its potential for out-of-the-box NER in\nthe few-shot setting. Given its accessibility, ChatGPT may be leveraged to extract\nrare diseases or signs without relying on a large, annotated corpus, which is a major\nbottleneck for training natural language processing models. Overall, prompts written\nas simple sentences generally achieved similar or better performance than structured\nlists, suggesting that ChatGPT may be more receptive to conversational prompts. To\nthis end, we recommend using these prompts to identify and extract rare diseases and\ntheir phenotypes.\nOur error analysis revealed that ChatGPT tended to label rare diseases as diseases.\nThese errors may be attributed to the fact that there is no single deﬁnition of rare\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 457\nTable 5 Error analysis for ChatGPT in the few-shot setting under exact match\nBoundary ✗ Boundary ✓ Boundary ✗ Spurious Missed Total\nEntity type ✓ Entity type ✗ Entity type ✗ (False Pos.) (False Neg.) errors\nRare disease 16 (10%) 48 (31%) 17 (11%) 4 (3%) 72 (45%) 157 (100%)\nDisease 11 (4%) 7 (2%) 9 (3%) 147 (51%) 116 (40%) 290 (100%)\nSign 64 (17%) 8 (2%) 5 (1%) 146 (40%) 148 (40%) 371 (100%)\nSymptom 3 (4%) 12 (16%) 2 (3%) 34 (44%) 25 (33%) 76 (100%)\n123\n458 Journal of Healthcare Informatics Research (2024) 8:438–461\ndiseases; rather, the deﬁnition can vary by country or location (i.e., a disease is a rare\ndisease if it affects < 200, 000 people in the United States or no more than 1 in 2,000\nin the European Union). Moreover, this deﬁnition is subject to change over time, as a\ndisease that used to be rare at the time of annotation may have become more prevalent,\nor vice versa. Because annotations are contextual, it’s possible that what the domain\nexperts deemed as rare diseases may not be reﬂected in information on the Internet\nbefore September 2021, ChatGPT’s knowledge cut-off date.\nWhile other studies explored supervised deep learning techniques for extracting\nrare disease phenotypes, ours is the ﬁrst to study ChatGPT in the zero- and few-shot\nsettings. Segura-Bedmar et al. [ 21] compared the NER performance of base BERT,\nBioBERT, and ClinicalBERT, and found that ClinicalBERT had the highest overall F1-\nscore (0.695). This was comparable to BioClinicalBERT’s performance in the current\nstudy (0.689). Fabregat et al. [ 26] used support vector machines and neural networks\nwith a long short-term memory architecture to extract disabilities associated with rare\ndiseases and obtained an F1-score of 0.81. While this was much higher than the overall\nF1-scores in the current study, the authors focused on extracting a single entity, i.e.,\ndisabilities, whereas our goal was to recognize and differentiate among four entities\nwith overlapping semantics. Hu et al. [ 15] and Chen et al. [ 16] evaluated ChatGPT’s\nclinical and biomedical NER performance and found that it had lower accuracy than\nﬁne-tuning pre-trained LLMs. While our overall results aligned with this ﬁnding,\nwe discovered that ChatGPT had similar or better performance on speciﬁc entities,\nsuggesting that with appropriate prompt engineering, the model has the potential to\nmatch or outperform ﬁne-tuned language models for certain entity types.\nOur work has several potential limitations and extensions. First, we only had access\nto a subset of the RareDis corpus (832 out of 1041 texts), so our results may not fully\nreﬂect ChatGPT’s performance across the entire spectrum of rare diseases. Second,\nthe current work focuses on ChatGPT and does not include GPT-4 or other variants\n(e.g., LLaMA, Alpaca, etc.), so broadening the current set of experiments to include\nother LLMs is a natural extension. Third, though manually-created prompts are highly\nintuitive and interpretable, evidence suggests that small changes can lead to variations\nin performance [ 30]. A promising alternative is to automate the prompt engineer-\ning process. To this end, Gutiérrez et al. [ 31] employed a semi-automated approach\ncombining manually-created prompts with an automatic procedure to choose the best\nprompt combination with cross validation. In addition, fully-automated prompt learn-\ning approaches, where the prompt is described directly in the embedding space of\nthe underlying language model, are also interesting extensions of the current work\n[32, 33]. Last, while the current study did not involve clinical data, prompt-learning\nstrategies proposed herein are transferrable to clinical applications that leverage secure\ninstances of ChatGPT. Speciﬁcally, these instances are governed by appropriate legal\nand business agreements ensuring privacy of protected health information. Given the\nease of interacting with ChatGPT through textual prompts, our work has the potential\nto inform clinical applications on rare disease phenotyping in practice.\nThe advent of LLMs is creating unprecedented opportunities for rare disease phe-\nnotyping by automatically identifying and extracting disease-related concepts. While\nthese models provide valuable insight and assistance, researchers and clinicians should\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 459\ncritically evaluate model outputs and be well-informed of their limitations when con-\nsidering them as tools for supporting rare disease diagnosis and treatment.\nAuthor Contributions Concept and design: C.S., P .H., and H.X. Formal analysis and interpretation of data:\nC.S., Y .H., and H.X. Original draft preparation: C.S. Critical revision of the manuscript for important\nintellectual content: C.S., Y .H., L.B., A.C., R.H., P .H., and H.X.\nFunding Cathy Shyr was supported by the National Library of Medicine of the National Institutes of Health\nunder Award Number 1K99LM014429-01.\nAvailability of data and materials The RareDis corpus can be found using the link provided in [ 19]. The\ncode for the current study can be found at https://github.com/cathyshyr/rare_disease_phenotype_extraction.\nDeclarations\nCompeting of interest The authors declare no competing interests.\nEthical approval Not applicable.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included\nin the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\n1. Nguengang Wakap S, Lambert DM, Olry A, Rodwell C, Gueydan C, Lanneau V , Murphy D, Le Cam\nY , Rath A (2020) Estimating cumulative point prevalence of rare diseases: analysis of the Orphanet\ndatabase. Eur J Hum Genet 28(2):165–173\n2. Chung CCY , Project HKG, Chu A TW, Chung BHY (2022) Rare disease emerging as a global public\nhealth priority. Front Public Health 10:1028545\n3. Cohen JS, Biesecker BB (2010) Quality of life in rare genetic conditions: a systematic review of the\nliterature. Am J Med Genet A 152(5):1136–1156\n4. Carmichael N, Tsipis J, Windmueller G, Mandel L, Estrella E (2015) Is it going to hurt?: the impact\nof the diagnostic odyssey on children and their families. J Genet Couns 24:325–335\n5. Yang G, Cintina I, Pariser A, Oehrlein E, Sullivan J, Kennedy A (2022) The national economic burden\nof rare disease in the united states in 2019. Orphanet J Rare Dis 17(1):1–11\n6. Tifft CJ, Adams DR (2014) The national institutes of health undiagnosed diseases program. Curr Opin\nPediatr 26(6):626\n7. Macnamara EF, D’Souza P , Tifft CJ et al (2019) The undiagnosed diseases program: approach to\ndiagnosis. Trans Sci Rare Dis 4(3–4):179–188\n8. Ahmad FS, Ricket IM, Hammill BG, Eskenazi L, Robertson HR, Curtis LH, Dobi CD, Girotra S,\nHaynes K, Kizer JR, et al (2020) Computable phenotype implementation for a national, multicenter\npragmatic clinical trial: lessons learned from adaptable. Circ: Cardio Qual Outcomes 13(6):006292\n9. Chapman M, Domínguez J, Fairweather E, Delaney B, Curcin V (2021) Using computable phenotypes\nin point-of-care clinical trial recruitment. In: public health and informatics-proceedings of MIE 2021:\nStudies in health technology and informatics, pp 560–564. IOS Press, ???\n10. Wang Y , Wang L, Rastegar-Mojarad M, Moon S, Shen F, Afzal N, Liu S, Zeng Y , Mehrabi S, Sohn S\net al (2018) Clinical information extraction applications: a literature review. J Biomed Inf 77:34–49\n123\n460 Journal of Healthcare Informatics Research (2024) 8:438–461\n11. V aswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017)\nAttention is all you need. Advances in neural information processing systems, p 30\n12. Devlin J, Chang M-W, Lee K, Toutanova K (2018) Bert: pre-training of deep bidirectional transformers\nfor language understanding. arXiv:1810.04805\n13. OpenAI: introducing ChatGPT (2022) https://openai.com/blog/chatgpt\n14. Agrawal M, Hegselmann S, Lang H, Kim Y , Sontag D (2022) Large language models are few-shot\nclinical information extractors. In: Proceedings of the 2022 Conference on empirical methods in natural\nlanguage processing, pp 1998–2022\n15. Hu Y , Ameer I, Zuo X, Peng X, Zhou Y , Li Z, Li Y , Li J, Jiang X, Xu H (2023) Zero-shot clinical entity\nrecognition using ChatGPT. arXiv:2303.16416\n16. Chen Q, Du J, Hu Y , Keloth VK, Peng X, Raja K, Zhang R, Lu Z, Xu H (2023) Large language\nmodels in biomedical natural language processing: benchmarks, baselines, and recommendations.\narXiv:2305.16326\n17. Lee P , Goldberg C, Kohane I (2023) The AI revolution in medicine: GPT-4 and beyond. Pearson\n18. Mehnen L, Gruarin S, V asileva M, Knapp B (2023) ChatGPT as a medical doctor? A diagnostic\naccuracy study on common and rare diseases. medRxiv, 2023–04\n19. Martínez-deMiguel C, Segura-Bedmar I, Chacón-Solano E, Guerrero-Aspizua S (2022) The RareDis\ncorpus: a corpus annotated with rare diseases, their signs and symptoms. J Biomed Inf 125:103961\n20. Alsentzer E, Murphy JR, Boag W, Weng W-H, Jin D, Naumann T, McDermott M (2019) Publicly\navailable clinical BERT embeddings. arXiv:1904.03323\n21. Segura-Bedmar I, Camino-Perdones D, Guerrero-Aspizua S (2022) Exploring deep learning methods\nfor recognizing rare diseases and their clinical manifestations from texts. BMC Bioinf 23(1):263\n22. Davis MF, Sriram S, Bush WS, Denny JC, Haines JL (2013) Automated extraction of clinical traits of\nmultiple sclerosis in electronic medical records. J Am Med Inf Assoc 20(e2):334–340\n23. Lo Barco T, Kuchenbuch M, Garcelon N, Neuraz A, Nabbout R (2021) Improving early diagnosis of\nrare diseases using natural language processing in unstructured medical records: an illustration from\nDravet syndrome. Orphanet J Rare Dis 16:1–12\n24. Deisseroth CA, Birgmeier J, Bodle EE, Kohler JN, Matalon DR, Nazarenko Y , Genetti CA, Brownstein\nCA, Schmitz-Abe K, Schoch K et al (2019) Clinphen extracts and prioritizes patient phenotypes directly\nfrom medical records to expedite genetic disease diagnosis. Genet Med 21(7):1585–1593\n25. Nigwekar SU, Solid CA, Ankers E, Malhotra R, Eggert W, Turchin A, Thadhani RI, Herzog CA\n(2014) Quantifying a rare disease in administrative data: the example of calciphylaxis. J Gener Int Med\n29:724–731\n26. Fabregat H, Araujo L, Martinez-Romo J (2018) Deep neural models for extracting entities and rela-\ntionships in the new RDD corpus relating disabilities and rare diseases. Comput Methods Prog Biomed\n164:121–129\n27. Stenetorp P , Pyysalo S, Topi´ c G, Ohta T, Ananiadou S, Tsujii J (2012) BRA T: a web-based tool for\nNLP-assisted text annotation. In: Proceedings of the Demonstrations at the 13th Conference of the\nEuropean Chapter of the Association for Computational Linguistics, pp 102–107\n28. Johnson AE, Pollard TJ, Shen L, Lehman L-WH, Feng M, Ghassemi M, Moody B, Szolovits P , Anthony\nCeli L, Mark RG (2016) Mimic-iii, a freely accessible critical care database. Sci Data 3(1):1–9\n29. spaCy: industrial-strength natural language processing in python. https://spacy.io\n30. Cui L, Wu Y , Liu J, Yang S, Zhang Y (2021) Template-based named entity recognition using BART.\narXiv:2106.01760\n31. Gutiérrez BJ, McNeal N, Washington C, Chen Y , Li L, Sun H, Su Y (2022) Thinking about GPT-3\nin-context learning for biomedical IE? Think again. arXiv:2203.08410\n32. Ma R, Zhou X, Gui T, Tan Y , Li L, Zhang Q, Huang X (2021) Template-free prompt tuning for few-shot\nNER. arXiv:2109.13532\n33. Taylor N, Zhang Y , Joyce D, Nevado-Holgado A, Kormilitzin A (2022) Clinical prompt learning with\nfrozen language models. arXiv:2205.05535\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional afﬁliations.\n123\nJournal of Healthcare Informatics Research (2024) 8:438–461 461\nAuthorsandAﬃliations\nCathy Shyr 1 · Yan Hu 2 · Lisa Bastarache 1 · Alex Cheng 1 · Rizwan Hamid 3 ·\nPaul Harris 1,4,5 · Hua Xu 6\nCathy Shyr\ncathy.shyr@vumc.org\nYan Hu\nyan.hu@uth.tmc.edu\nLisa Bastarache\nlisa.bastarache@vumc.org\nAlex Cheng\na.cheng@vumc.org\nRizwan Hamid\nrizwan.hamid@vumc.org\n1 Department of Biomedical Informatics, V anderbilt University Medical Center, Nashville, TN\n37203, USA\n2 School of Biomedical Informatics, University of Texas Health Science Center at Houston,\nHouston, TX 77225, USA\n3 Division of Medical Genetics and Genomic Medicine, V anderbilt University Medical Center,\nNashville, TN 37203, USA\n4 Department of Biostatistics, V anderbilt University Medical Center, Nashville, TN 37203, USA\n5 Department of Biomedical Engineering, V anderbilt University Medical Center, 2525 West End\nAvenue, Nashville, TN 37203, USA\n6 Section of Biomedical Informatics and Data Science, Yale School of Medicine, 100 College\nStreet, New Haven, CT 06510, USA\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6864669322967529
    },
    {
      "name": "Bottleneck",
      "score": 0.6499729156494141
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6288515329360962
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6231220364570618
    },
    {
      "name": "Sentence",
      "score": 0.5405709743499756
    },
    {
      "name": "Natural language processing",
      "score": 0.5369569659233093
    },
    {
      "name": "Machine learning",
      "score": 0.44875746965408325
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I901861585",
      "name": "Vanderbilt University Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I919571938",
      "name": "The University of Texas Health Science Center at Houston",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ],
  "cited_by": 37
}