{
    "title": "A Generalized Language Model in Tensor Space",
    "url": "https://openalex.org/W2905052854",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2098387877",
            "name": "Li-Peng Zhang",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A1972955069",
            "name": "Peng Zhang",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2904531529",
            "name": "Xindian Ma",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2099130763",
            "name": "Shuqin Gu",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2050912551",
            "name": "Zhan Su",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A1980846011",
            "name": "Dawei Song",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098387877",
            "name": "Li-Peng Zhang",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A1972955069",
            "name": "Peng Zhang",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2904531529",
            "name": "Xindian Ma",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2099130763",
            "name": "Shuqin Gu",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2050912551",
            "name": "Zhan Su",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A1980846011",
            "name": "Dawei Song",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6680532216",
        "https://openalex.org/W6678277124",
        "https://openalex.org/W6669440227",
        "https://openalex.org/W2097927681",
        "https://openalex.org/W6638868411",
        "https://openalex.org/W2258054274",
        "https://openalex.org/W6640281019",
        "https://openalex.org/W6656357712",
        "https://openalex.org/W2612051204",
        "https://openalex.org/W6676340756",
        "https://openalex.org/W6658528880",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W6727099177",
        "https://openalex.org/W1999965501",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W6648487637",
        "https://openalex.org/W1975474923",
        "https://openalex.org/W1519502414",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2068599297",
        "https://openalex.org/W2469894155",
        "https://openalex.org/W2767321762",
        "https://openalex.org/W330298975",
        "https://openalex.org/W3104263599",
        "https://openalex.org/W2767017507",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2108207327",
        "https://openalex.org/W2024165284",
        "https://openalex.org/W1993482030",
        "https://openalex.org/W2963537482",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2032175749",
        "https://openalex.org/W1852909287",
        "https://openalex.org/W2075719792",
        "https://openalex.org/W2963573053",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W4299295087",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W4298422451"
    ],
    "abstract": "In the literature, tensors have been effectively used for capturing the context information in language models. However, the existing methods usually adopt relatively-low order tensors, which have limited expressive power in modeling language. Developing a higher-order tensor representation is challenging, in terms of deriving an effective solution and showing its generality. In this paper, we propose a language model named Tensor Space Language Model (TSLM), by utilizing tensor networks and tensor decomposition. In TSLM, we build a high-dimensional semantic space constructed by the tensor product of word vectors. Theoretically, we prove that such tensor representation is a generalization of the n-gram language model. We further show that this high-order tensor representation can be decomposed to a recursive calculation of conditional probability for language modeling. The experimental results on Penn Tree Bank (PTB) dataset and WikiText benchmark demonstrate the effectiveness of TSLM.",
    "full_text": "The Thirty-Third AAAI Conference on Artiï¬cial Intelligence (AAAI-19)\nA Generalized Language Model in Tensor Space\nLipeng Zhang,1 Peng Zhang,1âˆ—, Xindian Ma,1 Shuqin Gu,1 Zhan Su,1 Dawei Song2\n1College of Intelligence and Computing, Tianjin University, Tianjin, China\n2School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China\n{lpzhang, pzhang, xindianma, shuqingu, suzhan}@tju.edu.cn, dawei.song2010@gmail.com\nAbstract\nIn the literature, tensors have been effectively used for captur-\ning the context information in language models. However, the\nexisting methods usually adopt relatively-low order tensors,\nwhich have limited expressive power in modeling language.\nDeveloping a higher-order tensor representation is challeng-\ning, in terms of deriving an effective solution and showing\nits generality. In this paper, we propose a language model\nnamed Tensor Space Language Model (TSLM), by utiliz-\ning tensor networks and tensor decomposition. In TSLM, we\nbuild a high-dimensional semantic space constructed by the\ntensor product of word vectors. Theoretically, we prove that\nsuch tensor representation is a generalization of the n-gram\nlanguage model. We further show that this high-order tensor\nrepresentation can be decomposed to a recursive calculation\nof conditional probability for language modeling. The exper-\nimental results on Penn Tree Bank (PTB) dataset and Wiki-\nText benchmark demonstrate the effectiveness of TSLM.\nIntroduction\nLanguage Modeling (LM) is a fundamental research topic\nthat underpins a wide range of Natural Language Process-\ning (NLP) tasks, e.g., speech recognition, machine transla-\ntion, and dialog system (Yu and Deng 2014; Lopez 2008;\nWang, Chung, and Seneff 2006). Statistical learning of a\nlanguage model aims to approximate the probability dis-\ntribution on the set of expressions in the language (Brown\net al. 1992). Recently, Neural networks (NNs), e.g., Recur-\nrent Neural Networks (RNNs) have been shown effective for\nmodeling language (Bengio et al. 2003; Mikolov et al. 2010;\nJozefowicz et al. 2016).\nIn the literature, a dense tensor is often used to represent a\nsentence or document. Cai et al. (2006) proposed TensorLSI,\nwhich considered sentences or documents as2-order tensors\n(matrices) and tried to ï¬nd an optimal basis for the tensor\nsubspace in term of reconstruction error. The 2-order ten-\nsor (matrix) only reï¬‚ects the local information (e.g., bi-gram\ninformation). Liu et al. (2005) proposed to model text by\na multilinear algebraic tensor instead of a vector. Speciï¬-\ncally, they represented texts using 3-order tensors to capture\nthe context of words. However, they still adopted relatively\nâˆ—Corresponding author: Peng Zhang (pzhang@tju.edu.cn)\nCopyright câƒ 2019, Association for the Advancement of Artiï¬cial\nIntelligence (www.aaai.org). All rights reserved.\nlow-order tensors, rather than high-order ones constructed\nby the tensor product of vectors. For a sentence withnwords\n(n >3), a n-order tensor (a high-order tensor) constructed\nby the tensor product of n word vectors, can consider all\nthe combinatorial dependencies among words (not limited\nto two/three consecutive words in 2/3-order tensor). It turns\nout that low-order tensors have limited expressive power in\nmodeling language.\nIt is challenging to construct a high-order tensor based\nlanguage model (LM), in the sense that it will involve an ex-\nponentially increasing number of parameters. Therefore, the\nresearch problems are how to derive an effective high-order\ntensor based LM and how to demonstrate the generality of\nsuch a tensor-based LM. To address these problems, our mo-\ntivation is to explore the expressive ability of tensor space in\ndepth, by making use of tensor networks and tensor decom-\nposition, in the language modeling process.\nTensor network is an elegant mathematical tool and an\neffective method for solving high-order tensors (e.g., ten-\nsors in quantum many-body problem), through contractions\namong lower-order tensors (Pellionisz and LlinÂ´as 1980). Re-\ncent research shows the connections between neural net-\nworks and tensor networks, which provides a novel per-\nspective for the interpretation of neural network (Cohen et\nal. (2016) and Levine et al. (2017; 2018)). With the help of\ntensor decomposition, the high dimensionality of parame-\nters in tensor space can be reduced greatly. Based on tensors,\na novel language representation using quantum many-body\nwave function was also proposed in (Zhang et al. 2018).\nInspired by the recent research, in this paper, we propose\na Tensor Space Language Model (TSLM), which is based\non high-dimensional semantic space constructed by the ten-\nsor product of word vectors. Tensor operations (e.g., multi-\nplication, inner product, decomposition) can be represented\nintuitively in tensor networks. Then, the probability of a sen-\ntence will be obtained by the inner product of two tensors,\ncorresponding to the input data and the global parameters,\nrespectively.\nTheoretically, we prove that TSLM is a generalization of\nthe n-gram language model, by showing that the conditional\nprobability of a language sequence can be calculated by the\ntensor representation we constructed. We further show that,\nafter tensor decomposition, the high-order tensor represen-\ntation can be decomposed to a recursive calculation of con-\n7450\nğœ¶ğœ¶2 ğœ¶ğœ¶3 ğœ¶ğœ¶ğ‘›ğ‘›ğœ¶ğœ¶1\nğ‘¨ğ‘¨\nğ’—ğ’—\nğ‘¨ğ‘¨ğ’—ğ’—\nğ’–ğ’–\nğ‘¨ğ‘¨\nğ›¿ğ›¿\nğ‘–ğ‘– âˆˆ [ğ‘šğ‘š] ğ‘‘ğ‘‘1 âˆˆ [ğ‘šğ‘š1]\nğ’–ğ’– = ğ‘¨ğ‘¨ğ’—ğ’—\nğ‘¢ğ‘¢ğ‘–ğ‘– = ï¿½\nğ‘—ğ‘—=1\nğ‘›ğ‘›\nğ´ğ´ğ‘–ğ‘–ğ‘—ğ‘—ğ‘£ğ‘£ğ‘—ğ‘—\nğ‘‘ğ‘‘2 âˆˆ [ğ‘šğ‘š2]\nğ‘‘ğ‘‘1\nğ‘‘ğ‘‘2\nğ‘‘ğ‘‘3\n=\n(a)\n(c)(b)\n1) 2) 3) 4)\nğ‘—ğ‘— âˆˆ [ğ‘›ğ‘›]\nğ‘–ğ‘– âˆˆ [ğ‘šğ‘š]\nğ‘–ğ‘– âˆˆ [ğ‘šğ‘š]\n=\nğ‘‘ğ‘‘1ğ‘‘ğ‘‘2 ğ‘‘ğ‘‘ğ‘›ğ‘›ğ‘‘ğ‘‘3\nâ€¦\nğ“£ğ“£\nVector ğ’—ğ’—: Matrix ğ‘¨ğ‘¨: 3-order ğ›¿ğ›¿ tensor: ğ‘›ğ‘›-order tensor ğ“£ğ“£:\nğ‘‘ğ‘‘1 ğ‘‘ğ‘‘2 ğ‘‘ğ‘‘ğ‘›ğ‘›ğ‘‘ğ‘‘3 â€¦\nğ“£ğ“£\nğ’”ğ’”, ğ’„ğ’„ = ï¿½\nğ‘‘ğ‘‘1,â€¦,ğ‘‘ğ‘‘ğ‘›ğ‘›=1\nğ‘šğ‘š\nğ’¯ğ’¯ğ‘‘ğ‘‘1â€¦ğ‘‘ğ‘‘ğ‘›ğ‘›ğ’œğ’œğ‘‘ğ‘‘1â€¦ğ‘‘ğ‘‘ğ‘›ğ‘›ğ‘¨ğ‘¨ = ï¿½\nğ‘–ğ‘–=1\nğ‘Ÿğ‘Ÿ\nğœ†ğœ†ğ‘–ğ‘–ğ’–ğ’–ğ‘–ğ‘–â¨‚ğ’—ğ’—ğ‘–ğ‘–\n(d)\nğ›¿ğ›¿\n=\nğ‘¼ğ‘¼ ğ‘½ğ‘½\nğ‘–ğ‘– ğ‘–ğ‘–\nğ‘—ğ‘— ğ‘˜ğ‘˜\nğ‘˜ğ‘˜ğ‘—ğ‘—\nğ€ğ€\nğ‘–ğ‘–\nğ’”ğ’”, ğ’„ğ’„\nâ€¦\nğ‘›ğ‘›-order rank-one tensor ğ“ğ“:5) \nğ‘‘ğ‘‘1ğ‘‘ğ‘‘2 ğ‘‘ğ‘‘ğ‘›ğ‘›ğ‘‘ğ‘‘3\nâ€¦\nğ“ğ“\nâ€¦\nğœ¶ğœ¶1\n ğœ¶ğœ¶2\n ğœ¶ğœ¶3 â€¦\n ğœ¶ğœ¶ğ‘›ğ‘›\nâ€¦\nFigure 1: Introduction of Tensor Networks. (a) Tensors in the TN are represented by nodes. (b) A matrixAmultiplying a vector\nvin TN notation. The contracted indices are denoted by j and are summed upon. The open indices are denoted by i, their\nnumber equals the order of the tensor represented by the entire network. The contraction is marked by the dashed line. (c) A\nTN notation illustrates the SVD of a matrix A. (d) A TN notation shows the inner product of two tensors.\nditional probability for language modeling. Finally, we eval-\nuate our model on the PTB dataset and the WikiText bench-\nmark, and the experimental results demonstrate the effec-\ntiveness of TSLM.\nThe main contributions of our work can be summarized as\nfollows: (1) We propose a novel language model aiming to\nconsider high-order dependencies of words via tensors and\ntensor networks. (2) We prove that TSLM is a generalization\nof the n-gram language model. (3) We can derive a recursive\ncalculation of conditional probability for language modeling\nvia tensor decomposition in TSLM.\nPreliminaries\nWe ï¬rst brieï¬‚y introduce tensors and tensor networks.\nTensors\n1.A tensor can be thought as a multidimensional array. The\norder of a tensor is deï¬ned to be the number of indexing en-\ntries in the array, which are referred to as modes. Thedimen-\nsion of a tensor in a particular mode is deï¬ned as the num-\nber of values that may be taken by the index in that mode.\nA tensor T âˆˆRm1Ã—Â·Â·Â·Ã—mn means that it is a n-order ten-\nsor with dimension mi in each mode iâˆˆ[n] := {1,...,n }.\nFor simplicity, we also call it amn-dimensional tensor in the\nfollowing text. A speciï¬c entry in a tensor will be referenced\nwith subscripts, e.g. Td1...dn âˆˆR.\n2.Tensor productis a fundamental operator in tensor anal-\nysis, denoted by âŠ—, which can map two low-order tensors\nto a high-order tensor. Similarly, the tensor product of two\nvector spaces is a high-dimensional tensor space. For exam-\nple, tensor product intakes two tensors A âˆˆRm1Ã—Â·Â·Â·Ã—mj\n(j-order) and Bâˆˆ Rmj+1Ã—Â·Â·Â·Ã—mj+k (k-order), and returns a\ntensor AâŠ—B = T âˆˆRm1Ã—Â·Â·Â·Ã—mj+k ((j+k)-order) deï¬ned\nby : Td1...dj+k = Ad1...dj Â·Bdj+1...dj+k .\n3.A n-order tensor Ais rank-one if it can be written as the\ntensor product of nvectors, i.e.,\nA= Î±1 âŠ—Î±2 âŠ—Â·Â·Â·âŠ— Î±n (1)\nThis means that each entry of the tensor is the product of the\ncorresponding vector elements:\nAd1d2...dn = Î±1,d1 Î±2,d2 Â·Â·Â·Î±n,dn âˆ€i,di âˆˆ[mi] (2)\n4.The rank of a tensor Tis deï¬ned as the smallest number\nof rank-one tensors that generate T as their sum (Hitchcock\n1927; Kolda and Bader 2009).\nTensor Networks\nA Tensor Network(TN) is formally represented by an undi-\nrected and weighted graph. The basic building blocks of a\nTN are tensors, which are represented by nodes in the net-\nwork. The order of a tensor is equal to the number of edges\nincident to it. The weight of a edge is equal to the dimension\nof the corresponding mode of a tensor. Fig. 1 (a) shows ï¬ve\nexamples for tensors: 1) A vector is a node with one edge. 2)\nA matrix is a node with two edges. 3) In particular, a triangle\nnode represents a Î´tensor, Î´ âˆˆRmÃ—Â·Â·Â·Ã—m, which is deï¬ned\nas follow:\nÎ´d1...dn =\n{\n1, d 1 = Â·Â·Â· = dn\n0, otherwise (3)\nwith di âˆˆ[m] âˆ€iâˆˆ[n], i.e. its entries are equal to one only\non the super-diagonal and zero otherwise. 4) The rounded\nrectangle node is the same as the circle node representing\nan arbitrary tensor. Accordingly, a tensor of order nis rep-\nresented in the TN as a node with n edges. 5) A n-order\nrank-one tensor Acan be represented by the tensor product\nof nvectors.\n7451\nEdges which connect two nodes in the TN represent\na contraction operation between the two corresponding\nmodes. An example for a contraction is depicted in Fig. 1\n(b), in which a TN corresponding to the operation of multi-\nplying a vectorvâˆˆRn by a matrix AâˆˆRmÃ—n is performed\nby summing over the only contracted index j. As there is\nonly one open index i, the result of contracting the network\nis a vector: uâˆˆRm which upholds u= Av.\nAn important concept in the later analysis is Singular\nValue Decomposition (SVD), which denotes that a matrix\nA âˆˆRmÃ—n can be decomposed as A = UÎ›V, where Î› âˆˆ\nRrÃ—r represents a diagonal matrix and U âˆˆ RmÃ—r,V âˆˆ\nRrÃ—n represent orthogonal matrices. In TN notation, we rep-\nresent the SVD as A = âˆ‘r\ni=1 Î»iui âŠ—vi in Fig. 1 (c). Î»i is\na singular value, ui, vi are the components of U, V respec-\ntively. The effect of the Î´ tensor is shown obviously, which\ncan be observed as â€˜forcingâ€™ the i-th â€˜rowâ€™ of any tensor to\nbe multiplied only by the i-th â€˜rowsâ€™ of other tensors.\nFig. 1 (d) shows the inner productof two tensors T and\nA, which returns a scalar value that is the sum of the prod-\nucts of their entries (Kolda and Bader 2009).\nBasic Formulation of Language Modeling in\nTensor Space\nIn this section, we describe the basic formation of Tensor\nSpace Language Model (TSLM), which is used for comput-\ning probability for the occurrence of a sequence s= wn\n1 :=\n(w1,...,w n) of length n, composed of |V|different tokens,\nand the vocabulary V containing all the words in the model,\ni.e. wi âˆˆV. In the next sections, we will prove that when\nwe use one-hot vectors, TSLM is a generalization ofn-gram\nlanguage model. If using word embedding vectors, TSLM\ncan result in a recursive calculation of conditional probabil-\nity for language modeling. In Related Work, we will discuss\nthe relations and differences between our previous work on\nlanguage representation for matching sentences (Zhang et al.\n2018) and the tensor space language modeling in this paper.\nRepresentations of Words and Sentences\nFirst, we deï¬ne the semantic space of a single word as a\nm-dimensional vector space V with the orthogonal basis\n{ed}m\nd=1, where each base vector ed is corresponding to a\nspeciï¬c semantic meaning. A word wi in a sentence s can\nbe written as a linear combination of themorthogonal basis\nvectors as a general representation:\nwi =\nmâˆ‘\ndi=1\nÎ±i,diedi (4)\nwhere Î±i,di is its corresponding coefï¬cient. For the basis\nvectors {ed}m\nd=1, there can be two different choices, one-hot\nvectors or embedded vectors.\nAs for a sentence s= (w1,...,w n) with length n, it can\nbe represented in the tensor space:\nVâŠ—n := V âŠ—V âŠ—Â·Â·Â·âŠ— Vî´™ î´˜î´— î´š\nn\n(5)\nas:\ns= w1 âŠ—Â·Â·Â·âŠ— wn (6)\nwhich is a mn-dimensional tensor. Through the interaction\nof each dimension of words, the sentence tensor has a strong\nexpressive power. Substituting Eq. 4 into Eq. 6, the repre-\nsentation of the sentence scan be expanded by:\ns=\nmâˆ‘\nd1,...,dn=1\nAd1...dned1 âŠ—Â·Â·Â·âŠ— edn (7)\nwhere {ed1 âŠ—Â·Â·Â·âŠ— edn}m\nd1,...,dn=1 are the basis with mn\ndimension in the tensor space VâŠ—n, which denotes the high-\ndimensional semantic meaning. Ais a mn-dimensional ten-\nsor and its each entry Ad1...dn is the corresponding coefï¬-\ncient of each basis. According to the Eq. 1 and 2, we will\nsee A, computed as âˆn\ni=1 Î±i,di, is a rank-one tensor.\nA Probability Estimation Method of Sentences\nA goal of language modeling is to learn the joint probability\nfunction of sequences of words in a language (Bengio et al.\n2003). We assume that each sentencesi appears with a prob-\nability pi. Then, we can construct a mixed representation c\nwhich is a linear combination of sentences, denoted as:\nc:=\nâˆ‘\npisi (8)\nWe consider the mixed representation c as a high-\ndimensional representation of a sequence containing n\nwords. For each sentence si, it can be represented by coefï¬-\ncient tensor Ai and basis vectors. Therefore, based on Eqs. 7\nand 8, the mixed representation ccan be formulated with a\ncoefï¬cient tensor T:\nc=\nmâˆ‘\nd1,...,dn=1\nTd1...dned1 âŠ—... âŠ—edn (9)\nThe difference betweensin Eq. 7 andcin Eq. 9 is mainly\non two different tensors Aand T. According to the Prelim-\ninaries, Ais essentially rank-one while Thas a higher rank\nand is harder to solve. We will show that the tensor T en-\ncodes the parameters, and Ais the input, in our model.\nIn turn, if we have estimated such a mixed representation\nc(in fact, its coefï¬cient tensor T) from our model, we can\nget the probability pi of a sentence si via computing the in-\nner product of cand si:\np(si) = âŸ¨si,câŸ© (10)\nBased on the Eq. 7, 9 and 10, we can obtain:\np(si) =\nmâˆ‘\nd1,...,dn=1\nTd1...dnAd1...dn (11)\nas shown in the tensor network of Fig 1(d). This is the basic\nformula in TSLM for estimating the sentence probability.\nTSLM as a Generalization of N-Gram\nLanguage Model\nThe goal of the n-gram language model is to estimate the\nprobability distribution of sentences. For a speciï¬c sentence\n7452\nğ’‰ğ’‰ğŸğŸ ğ’‰ğ’‰ğŸğŸ ğ’‰ğ’‰ğŸğŸ\nğ’™ğ’™ğŸğŸ ğ’™ğ’™ğŸğŸ\nğ’šğ’šğŸğŸ ğ’šğ’šğŸğŸ\nğ‘Šğ‘Š ğ‘Šğ‘Š\nğ‘ˆğ‘ˆ ğ‘ˆğ‘ˆ\nğ‘‰ğ‘‰ ğ‘‰ğ‘‰\nğ‘”ğ‘”(ï¿½,ï¿½) ğ‘”ğ‘”(ï¿½,ï¿½) â€¦\n(a)\n(b)\nğ‘ˆğ‘ˆ\nğ›¿ğ›¿ğ‘Šğ‘Šğ’‰ğ’‰ğ‘¡ğ‘¡âˆ’1\nğœ¶ğœ¶ğ‘¡ğ‘¡\n= ğ’‰ğ’‰ğ‘¡ğ‘¡\n(c) (d)\nğ›¿ğ›¿\nğ‘ˆğ‘ˆ\nğ€ğ€ğ’‰ğ’‰0\nğœ¶ğœ¶1 ğœ¶ğœ¶2 ğœ¶ğœ¶ğ‘›ğ‘›ğ“ğ“ â€¦\nâ€¦\nğ‘ˆğ‘ˆğ‘ˆğ‘ˆ\nğ›¿ğ›¿ğ›¿ğ›¿ ğ‘Šğ‘Šğ‘Šğ‘Š\nğ‘‰ğ‘‰ ğ‘‰ğ‘‰ ğ‘‰ğ‘‰\nğ“£ğ“£\nâ€¦\nğ’®ğ’®(ğ‘¡ğ‘¡âˆ’1)\nğ‘‰ğ‘‰\nğ›¿ğ›¿\nğ‘ˆğ‘ˆ\n= ğ’¯ğ’¯(ğ‘¡ğ‘¡)\nFigure 2: The TN represents the recursive calculation process of TSLM. (a) represents the inner product of two tensors T and\nA, (b) denotes the recursive representations of T(t) and (c) is the recursive representations of ht, respectively. (d) is a general\nRNN architecture.\ns, its joint probability p(s) = p(wn\n1 ) := p(w1,...,w n) re-\nlies on the Markov Chain Rule of conditional probability:\np(wn\n1 ) = p(w1)\nnâˆ\ni=2\np(wi|wiâˆ’1\n1 ) (12)\nand the conditional probability p(wi|wiâˆ’1\n1 ) can be calcu-\nlated as:\np(wi|wiâˆ’1\n1 ) = p(wi\n1)\np(wiâˆ’1\n1 ) â‰ˆ count(wi\n1)\ncount(wiâˆ’1\n1 ) (13)\nwhere the countdenotes the frequency statistics in corpus.\nClaim 1.In our TSLM, when we set the dimension of vec-\ntor space m = |V|and each word w as an one-hot vector,\nthe probability of sentence s consist of words d1 ...d n in\nvocabulary is the entry Td1...dn of tensor T.\nProof. The detailed proof can be found in Appendix.\nIntuitively, the speciï¬c sentencescan be represented as an\none-hot tensor. The mixed representation ccan be regarded\nas the total sampling distribution. The tensor inner product\nâŸ¨s,câŸ©represents statistics probability that a sentence s ap-\npears in a language.\nClaim 2.In our TSLM, we deï¬ne the word sequence wi\n1 =\n(w1,w2,...,w i) with length ias:\nwi\n1 := w1 âŠ—Â·Â·Â·âŠ— wi âŠ—1i+1 âŠ—Â·Â·Â·âŠ— 1n (14)\nwhich means that the sequence wi\n1 is padded via using full\none vector 1. Then, the probability p(wi\n1) can be computed\nas p(wi\n1) = âŸ¨wi\n1,câŸ©.\nProof. It can be proved by the marginal probability of mul-\ntiple discrete variables (in Appendix).\nTherefore, the conditional probability p(wi|wiâˆ’1\n1 ) in n-\ngram language model can be calculated by Bayesian Condi-\ntional Probability Formula using tensor representations and\ntensor inner product as follow:\np(wi|wiâˆ’1\n1 ) = âŸ¨wi\n1,câŸ©\nâŸ¨wiâˆ’1\n1 ,câŸ© (15)\nThis kind of representation of conditional probability is a\nspecial case of TSLM based on the one-hot basis vectors.\nBecause of the one-hot vector representation, the n-gram\nlanguage model has O(|V|n) parameters. Compared with n-\ngram language model, our general model hasO(mn) param-\neters (m â‰ª|V|). However, the tensor space still contains\nexponential parameters, and in the next section, we will in-\ntroduce tensor decomposition to deal with this problem.\nDeriving Recursive Language Modeling\nProcess from TSLM\nWe have deï¬ned the method for estimating probability of a\nsentence sby the inner product of two tensors T and Ain\nEq. 11. In this section, we describe the recursive language\nmodeling process in Fig. 2. Firstly, our derivation is under\nthe condition of basis vectors {ed}m\nd=1 as embedded vec-\ntors. Secondly, we recursively decompose the tensor T (see\nFig. 3), to obtain the TN representation of tensor Tin Fig. 2\n(a). Then, we use the intermediate variables in Fig. 2 (bc)\nto estimate the conditional probability p(wt|wtâˆ’1\n1 ). In the\nfollowing, we introduce the recursive tensor decomposition,\nfollowed by the calculation of conditional probability.\nRecursive Tensor Decomposition\nWe generalize the SVD from matrix to tensor as shown in-\ntuitively in Fig. 3, inspired by the train-style decomposition\nof Tensor-Train (Oseledets 2011) and Tucker Decomposi-\ntion (Kolda and Bader 2009). Fig. 3 illustrates a high-order\ntensor recursively decomposed as several low-order tensor\n(vectors, matrices, etc.).\nThe formula of the recursive decomposition about tensor\nT is :\nT =\nrâˆ‘\ni=1\nÎ»iS(n),i âŠ—ui\nS(n),k =\nrâˆ‘\ni=1\nWk,iS(nâˆ’1),i âŠ—ui\n(16)\nwhere we deï¬ne : S(1) = 1 âˆˆRr. It means that a n-order\n7453\nğ‘ˆğ‘ˆ\nğ‘Šğ‘Šğ›¿ğ›¿ ğ›¿ğ›¿ ğ›¿ğ›¿\nğ‘ˆğ‘ˆ ğ‘ˆğ‘ˆ\nğ€ğ€\nğŸğŸ\nâ€¦\nâ€¦\nğ“£ğ“£\nğ‘ˆğ‘ˆ\nğ›¿ğ›¿ ğ€ğ€\nâ€¦\nğ’®ğ’®(ğ‘›ğ‘›)\nğ‘ˆğ‘ˆ\nğ›¿ğ›¿ ğ€ğ€\nâ€¦\nğ‘Šğ‘Šğ’®ğ’®(ğ‘›ğ‘›âˆ’1)\nğ‘ˆğ‘ˆ\nğ›¿ğ›¿\nFigure 3: The recursive generalized SVD for the tensor T.\ntensor T âˆˆRmÃ—Â·Â·Â·Ã—m can be decomposed as a n-order ten-\nsor1 S(n) âˆˆRmÃ—Â·Â·Â·Ã—r, a diagonal matrix Î› âˆˆRrÃ—r and a\nmatrix U âˆˆRrÃ—m. One can consider this decomposition as\nthe matrix SVD after tensor matricization, also as unfold-\ning or ï¬‚attening the tensor to matrix by one mode. Recur-\nsively, (n-1)-order tensor S(n),k, which can be seen as the\nk-th â€˜rowâ€™ of the tensor S(n), can be decomposed like the\ntensor Tand W is a matrix composed by rgroups of singu-\nlar value vectors.\nWe employ this decomposition to extract the main fea-\ntures of the tensor T which is similar with the effect of\nSVD on matrix, then approximately represent the parame-\nters of our model, where r(râ‰¤m) denotes the rank of ten-\nsor decomposition. This tensor decomposition method re-\nduce the O(mn) magnitude of parameters approximatively\nto O(mÃ—r).\nA Recursive Calculation of Conditional Probability\nIn our model, we compute the conditional probability distri-\nbution as :\np(wt|wtâˆ’1\n1 ) = softmax(âŸ¨T(t),A(tâˆ’1)âŸ©) (17)\nwhere A(tâˆ’1) is the input of (t-1) words, represented as\nÎ±1,..., Î±(tâˆ’1) in Fig. 2 (a). âŸ¨T(t),A(tâˆ’1)âŸ©is denoted as yt.\nAs shown in Fig. 2 (b), T(t) âˆˆ RmÃ—Â·Â·Â·Ã—mÃ—|V| is con-\nstructed by matrix V âˆˆRrÃ—|V|, S(tâˆ’1) and matrix U. The\nV is the weighted matrix mapping to the vocabulary:\nT(t),k =\nrâˆ‘\ni=1\nVk,iS(tâˆ’1),i âŠ—ui (18)\nAs shown in Fig. 2 (c), when calculating the inner prod-\nuct of two tensors T and A, we can introduce intermediate\nvariables ht, which can be recursively calculated as:\nh1 = Wh0 âŠ™UÎ±1\n...\nht = Whtâˆ’1 âŠ™UÎ±t\n(19)\nwhere setting the h0 := Wâˆ’11, and the matrices W and\nU decomposed from Eq. 16 in the last section. The sym-\n1To distinguish, we use the ï¬rst parenthesized subscript to indi-\ncate the order of the tensor.\nbol âŠ™denotes the element-wise multiplication between vec-\ntors. This multiplicative operation is derived from the ten-\nsor product (in Eq. 16) and the Î´-tensor, which we have ex-\nplained it â€˜forcesâ€™ the two vectors connected with it to be\nmultiplied by elements (in Preliminaries).\nBased on the analysis above, the recursive calculation of\nconditional probability of our TSLM can be formulated as:\np(wt|wtâˆ’1\n1 ) = softmax(yt)\nyt = Vht\nht = g(Whtâˆ’1,UÎ±t)\ng(a,b) = aâŠ™b\n(20)\nwhere Î±t âˆˆRm is the input at time-step tâˆˆ[n], ht âˆˆRr is\nthe hidden state of the network, yt âˆˆR|V|denotes the out-\nput, and the trainable weight parameters U âˆˆRmÃ—r,W âˆˆ\nRrÃ—r,V âˆˆRrÃ—|V|are the input to hidden, hidden to hidden\nand hidden to output weights matrices, respectively, andgis\na non-linear operation.\nThe operation âŠ™stands for element-wise multiplication\nbetween vectors, for which the resultant vector upholds(aâŠ™\nb)i = aiÂ·bi. Differently, in the RNN and TSLM architecture,\ngis deï¬ned as:\ngRNN (a,b) = Ïƒ(a+ b)\ngTSLM (a,b) = aâŠ™b (21)\nwhere Ïƒ(Â·) is typically a point-wise activation function such\nas sigmoid, tanh etc. A bias term is usually added to Eq. 21.\nSince it has no effect with our analysis, we omit it for sim-\nplicity. We show a general structure of RNN in Fig. 2(d).\nIn fact, recurrent networks that include the element-wise\nmultiplication operation have been shown to outperform\nmany of the existing RNN models (Sutskever, Martens, and\nHinton 2011; Wu et al. 2016). Wu et al. (2016) had given a\nmore general formula for hidden unit in RNN, named Mul-\ntiplicative Integration, and discussed the different structures\nof the hidden unit in RNN.\nRelated Works\nHere, we present a brief review of related work, including\nsome representative work in language modeling, and the\nmore recent research on the cross ï¬elds of tensor network,\nneural network and language modeling.\nThere have been tremendous research efforts in the ï¬eld\nof statistical language modeling. Some earlier language\nmodels are based on the Markov assumption are represented\nby n-gram models (Brown et al. 1992), where the predic-\ntion of the next word is often conditioned just on n pre-\nceding words. For n-gram models, Kneser and Ney (2002)\nproposed the most well-known KN smoothing method,\nand some researchers continued to improve the smoothing\nmethod, as well as introduced the low-rank model. Neu-\nral Probabilistic Language Model (Bengio et al. 2003) is to\nlearn the joint probability function of sequence of words in\na language, which shows the improvement on n-gram mod-\nels. Recently, RNN (Mikolov et al. 2010) and Long Short-\nTerm Memory (LSTM) networks (Soutner and MÂ¨uller 2013)\nachieve promising results on language model tasks.\n7454\nPTB WikiText-2\nTrain Valid Test Train Valid Test\nArticles - - - 600 60 60\nTokens 929,590 73,761 82,431 2,088,628 217,646 245,569\nV ocab size 10,000 33,278\nOOV rate 4.8% 2.6%\nTable 1: Statistics of the PTB and WikiText-2.\nPTB WikiText-2\nModel Hidden size Layers Valid Test Hidden size Layers Valid Test\nKN-5(Mikolov and Zweig 2012) - - - 141.2 - - - -\nRNN(Mikolov and Zweig 2012) 300 1 - 124.7 - - - -\nLSTM(Zaremba, Sutskever, and Vinyals 2014) 200 2 120.7 114.5 - - - -\nLSTM(Grave, Joulin, and Usunier 2016) 1024 1 - 82.3 1024 1 - 99.3\nLSTM(Merity et al. 2017) 650 2 84.4 80.6 650 2 108.7 100.9\nRNNâ€  256 1 130.3 124.1 512 1 126.0 120.4\nLSTMâ€  256 1 118.6 110.3 512 1 105.6 101.4\nTSLM 256 1 117.2 108.1 512 1 104.9 100.4\nRNN+MoSâ€ (Yang et al. 2018) 256 1 88.7 84.3 512 1 85.6 81.8\nTSLM+MoS 256 1 86.4 83.6 512 1 83.9 81.0\nTable 2: Best perplexity of models on the PTB and WikiText-2 dataset. Models tagged with â€ indicate that they are reimple-\nmented by ourselves.\nRecently, Cohen et al. (2016) and Levine et al. (2017;\n2018) use tensor analysis to explore the expressive power\nand interpretability of neural networks, including convolu-\ntional neural network (CNN) and RNN. Levine et al. (2018)\neven explored the connection between quantum entangle-\nment and deep learning. Inspired by their work, Zhang et\nal. (2018) proposed a Quantum Many-body Wave Function\ninspired Language Modeling (QMWF-LM) approach. How-\never, in QMWF-LM and TSLM, the term language mod-\neling has different meanings and application tasks. Specif-\nically, QMWF-LM is basically a language representation\nwhich encodes language features extracted by a CNN, and\nperforms the semantic matching in Question Answer (QA)\ntask as an extrinsic evaluation.\nDifferent from QMWF-LM, in this paper, TSLM focuses\non the Markov process of conditional probabilities in lan-\nguage modeling task with an intrinsic evaluation . Based\non the tensor representation and tensor networks, we pro-\npose the tensor space language model. We have established\nthe connection between TSLM and neural language models\n(e.g., RNN based LMs) and proved that TSLM is a more\ngeneral language model.\nExperiments\nDatasets\nPTB Penn Tree Bank dataset (Marcus, Marcinkiewicz, and\nSantorini 1993) is often used to evaluate language models. It\nconsists of 929k training words, 73k validation words, 82k\ntest words, and has 10k words in its vocabulary.\nWikiText-2 (WT2)dataset (Merity et al. 2017). Compared\nwith the preprocessed version of PTB, WikiText-2 is larger.\nIt also features a larger vocabulary and retains the original\ncase, punctuation and numbers, all of which are removed in\nPTB. It is composed of full articles.\nTable 1 shows statistics of these two datasets. The out\nof vocabulary (OOV) rate denotes the percentage of tokens\nhave been replaced by an âŸ¨unkâŸ©token. The token count in-\ncludes newlines which add to the WikiText-2 dataset.\nEvaluation Metrics\nPerplexity is the typical measure used for reporting progress\nin language modeling. It is the average per-word log-\nprobability on the holdout data set.\nPPL = e(âˆ’1\nn\nâˆ‘\ni ln p(wi))\nThe lower the perplexity, the more effective the model is. We\nfollow the standard procedure and sum over all the words.\nComparative Models and Experimental Settings\nIn order to demonstrate the effectiveness of TSLM, we\ncompare our model with several baseline models, in-\ncluding Kneser-Ney 5-gram (KN-5) (Mikolov and Zweig\n2012), RNN based language model (Mikolov et al. 2010),\nLong Short-Term Memory network (LSTM) based language\nmodel (Zaremba, Sutskever, and Vinyals 2014), and RNN\nadded Matrix of Softmax (MoS) language model (Yang et\nal. 2018). Models tagged with â€ indicate that they are reim-\nplemented by ourselves.\nKneser-Ney 5-gram (KN-5) : It uses Kneser-Ney\nSmoothing on the n-gram language model (n=5) (Chen and\nGoodman 1996; Mikolov and Zweig 2012). It is also the\nmost representative statistical language model, and we con-\nsider it as a low-order tensor language model.\nRNN: Recurrent neural network based language mod-\nels (Mikolov et al. 2010), use a recurrent hidden layer to\nrepresent longer and variable length histories, instead of us-\ning ï¬xed number of words to represent the context.\n7455\nLSTM: LSTM neural network is an another variant of\nRNN structure. It allows to discover both long and short pat-\nterns in data and eliminates the problem of vanishing gradi-\nent by training RNN. LSTM approved themselves in vari-\nous applications and it seems to be very promising course\nalso for the ï¬eld of language modeling (Soutner and M Â¨uller\n2013).\nRNN+MoS: A high-rank RNN language model (Yang\net al. 2018) breaking the softmax bottleneck, formulates\nthe next-token probability distribution as a Matrix of Soft-\nmax (MoS), and improves the perplexities on the PTB and\nWikiText-2. It is the state-of-the-art softmax technique for\nsolving probability distributions.\nSince our focus is to show the effectiveness of language\nmodeling in tensor space, we choose to set a relatively small\nscale network structure. Speciï¬cally, we set the same scale\nparameters for comparison experiments, i.e. 256/512 hid-\nden size, 1 hidden layer, 20 batch size and 30/40 sequence\nlength. Among them, the hidden size is equivalent to the ten-\nsor decomposition rank r, and sequence length means the\ntensor order nin our model.\nExperimental Results and Analysis\nTable 2 shows the results on PTB and WT2, respectively.\nFrom Table 2, we could observe that our proposed TSLM\nachieves the lower perplexity, which reï¬‚ects that TSLM out-\nperform others. It is obvious that KN-5 method gets the\nworst performance. We choose KN-5 as a baseline, since it\nis a typical n-gram model and we prove that TSLM is a gen-\neralization of n-gram. Although the most advanced Kneser-\nNey Smoothing is used, it still has not achieved good perfor-\nmance. The reason could be that statistic language model is\nbased on the word frequency statistics and does not have the\nsemantic advantages that word vectors in continuous space\ncan satisfy.\nThe LSTM based language modeling can theoretically\nmodel arbitrarily long dependencies. The experimental re-\nsults show that our model has achieved relatively better re-\nsults than LSTM reimplemented by ourselves. Based on the\nMoS, RNN based language models achieve state-of-the-art\nresults. To prove the effectiveness of our model using MoS,\nwe compared our model to RNN+MoS model on the same\nparameters. The empirical results of our model have also\nbeen improved, which also illustrates the effectiveness of\nour model structure.\nWe have shown that TSLM is a generalized language\nmodel. In practice, its performance is better than RNN based\nlanguage model. The reason could be that the non-linear op-\neration gin Eq. 20 is an element-wise multiplication, which\nis capable of expressing more complex semantic informa-\ntion than a standard RNN structure with activation function\nusing an addition operation.\nNote that, the parameters n, m and r are crucial factors\nfor modeling language in TSLM. The order n of the ten-\nsor reï¬‚ects the maximum sentence length. With the increase\nof the maximum sentence length, the performance of the\nmodel gradually increases. However, after a certain degree\nof growth, the expressive ability of the TSLM will reach an\nupper bound. The reason can be summarized as: The size of\n0 10 20 30 40 50\nThe order n (PTB)\n100\n110\n120\n130\n140\n150\n160PPL\nRNN with 256 units\nLSTM with 256 units\nTSLM with 256 units\n0 10 20 30 40 50\nThe order n (WikiText-2)\n100\n110\n120\n130\n140\n150\n160\nRNN with 512 units\nLSTM with 512 units\nTSLM with 512 units\nFigure 4: Perplexity (PPL) with different max length of sen-\ntences in corpus.\n1632 64 128 256\nThe rank r (PTB)\n100\n125\n150\n175\n200\n225\n250\n275\n300PPL\nRNN with length 30\nLSTM with length 30\nTSLM with length 30\n3264 128 256 512\nThe rank r (WikiText-2)\n100\n125\n150\n175\n200\n225\n250\n275\n300\nRNN with length 40\nLSTM with length 40\nTSLM with length 40\nFigure 5: Perplexity (PPL) with different hidden sizes.\nthe corpus determines the size of the tensor space we need to\nmodel. The larger the order is, the larger the semantic space\nthat TSLM can model and the more semantic information it\ncan contain. As shown in Fig. 4, we can see that the model\nis optimal when nequals 30 and 40 on PTB and WikiText\ndatasets, respectively. There are similar experimental phe-\nnomena in RNN and LSTM.\nOther key factors that affect the capability of TSLM are\nthe dimension of the orthogonal basis m and the rank of\ntensor decomposition r, where each orthogonal basis de-\nnote the basic semantic meaning. The tensor decomposi-\ntion is enough to extract the main features of the tensor\nT when r = m. They correspond to the word embedding\nsize and hidden size in RNN or LSTM, and are usually\nset as the same value. We try to set the value of them as\n[16,32,64,128,256,512,...]. As shown in Fig. 5, we can\nsee that the model is optimal when the decomposition rank\nrequals 256 and 512 on PTB and WikiText datasets, respec-\ntively. These phenomena is mainly due to the saturation of\nsemantic information in tensor space, which means that the\nnumber of the basic semantic meaning is ï¬nite with respect\nto the speciï¬c corpus.\nConclusions and Future work\nIn this paper, we have proposed a language model based\non tensor space, named Tensor Space Language Model\n(TSLM). We use the tensor networks to represent the high-\norder tensor for modeling language, and calculate the prob-\nability of a speciï¬c sentence by the inner product of tensors.\nMoreover, we have proved that TSLM is a generalization\nof n-gram language model and we can derive the recursive\nlanguage model process from TSLM. Experimental results\n7456\ndemonstrate the effectiveness of our model, compared with\nthe standard RNN-based language model and LSTM-based\nlanguage model on two typical datasets to evaluate language\nmodeling. In the future, we will further explore the potential\nof tensor network for modeling language in both theoretical\nand empirical directions.\nAcknowledgement\nThis work is supported in part by the state key develop-\nment program of China (grant No. 2017YFE0111900), Nat-\nural Science Foundation of China (grant No. U1636203,\n61772363), and the European Unionâ€™s Horizon 2020\nresearch and innovation programme under the Marie\nSkÅ‚odowska-Curie grant agreement No. 721321.\nAppendix\nClaim 1\nIn our TSLM, when we set the dimension of vector space\nm = |V|and each word was an one-hot vector, the proba-\nbility of sentence sconsist of words d1,...,d n in vocabu-\nlary is the entry Td1...dn of tensor T.\nProof of Claim 1\nProof. In our TSLM, when we set the dimension of vec-\ntor space m = |V|and each word w as an one-hot vec-\ntor, the speciï¬c sentence swill be represented as an one-hot\ntensor. The mixed representation ccan be regarded as the\ntotal sampling distribution. The tensor inner product âŸ¨s,câŸ©\nrepresents statistics probability that a sentence sappears in\na language. Speciï¬cally, the word wi is an one-hot vector\nwi = (0,..., 1,..., 0), and the vectors of any two different\nwords are orthogonal (word vector itself is basis vector):\nâŸ¨wi,wjâŸ©= âŸ¨ei,ejâŸ©=\n{\n1, i = j\n0, i Ì¸= j (22)\nFirstly, for the sentence s = (w1,...,w n) with length n\nis represented as s= w1 âŠ—Â·Â·Â·âŠ— wn. It is an one-hot tensor,\nand the tensors of any two different sentences are orthogonal\n(when being viewed as the ï¬‚atten vectors):\nsi = wi,1 âŠ—Â·Â·Â·âŠ— wi,n\nsj = wj,1 âŠ—Â·Â·Â·âŠ— wj,n\nâ‡’âŸ¨si,sjâŸ©=\nnâˆ\nk=1\nâŸ¨wi,k,wj,kâŸ©\n=\n{\n1, wi,k = wj,k, âˆ€kâˆˆ[n]\n0, otherwise\nâ‡’âŸ¨si,câŸ©= âŸ¨si,\nâˆ‘\nj\npjsjâŸ©= pi\n(23)\nSecondly, for the representations in TSLM, the orthogonal\nbasis can be composed by {wd}|V|\nd=1, and the sentence swill\nbe represented as:\ns=\n|V|âˆ‘\nd1,...,dn=1\nAd1...dnwd1 âŠ—Â·Â·Â·âŠ— wdn (24)\nwhere\nAd1...dn =\n{\n1, dk = index(wk,V),âˆ€kâˆˆ[n]\n0, otherwise (25)\nwhich means tensor Ais an one-hot tensor, and index(w,V)\nmeans the index ofwin vocabularyV. We have deï¬nedc:=âˆ‘pisi as :\nc=\n|V|âˆ‘\nd1,...,dn=1\nTd1...dnwd1 âŠ—Â·Â·Â·âŠ— wdn (26)\nThen, the probability of sentence si is:\npi = âŸ¨si,câŸ©\n=\n|V|âˆ‘\nd1,...,dn=1\nTd1...dnAd1...dn\n= Td1...dn, dk = index(wk,V),âˆ€kâˆˆ[n]\n(27)\nTherefore, the probability of sentence s consist of words\nd1,...,d n is p(w1 ...w n = d1 ...d n) = Td1...dn.\nClaim 2\nIn our TSLM, we deï¬ne the word sequence wi\n1 =\n(w1,w2,...,w i) with length ias:\nwi\n1 := w1 âŠ—Â·Â·Â·âŠ— wi âŠ—1i+1 âŠ—Â·Â·Â·âŠ— 1n (28)\nThen, the probability p(wi\n1) can be computed as p(wi\n1) =\nâŸ¨wi\n1,câŸ©.\nProof of Claim 2\nProof. According to the marginal distribution in probability\ntheory and statistics, for two discrete random variables, the\nmarginal probability function can be written as p(X = x):\np(X = x) =\nâˆ‘\ny\np(X = x,Y = y) (29)\nwhere p(X = x,Y = y) is the joint distribution of two\nvariables X and Y.\nFirstly, in our TSLM, we can deï¬ne the marginal distribu-\ntion using the word variable was:\np(wi) =\nâˆ‘\nwjâˆˆV\np(wi,wj)\np(w1,...,w nâˆ’1) =\nâˆ‘\nwnâˆˆV\np(w1,...,w nâˆ’1,wn)\n(30)\nSecondly, the probability of the word sequence wi\n1 can be\nwritten as:\np(wi\n1)\n=p(w1,...,w i)\n=\nâˆ‘\nwi+1,Â·Â·Â·,wnâˆˆV\np(w1,...,w i,wi+1,...,w n)\n=\n|V|âˆ‘\ndi+1,Â·Â·Â·,dn=1\nTd1...dn, dk = index(wk,V),âˆ€kâˆˆ[i]\n(31)\n7457\nThen, the inner product âŸ¨wi\n1,câŸ©can be written as:\nâŸ¨wi\n1,câŸ©\n=âŸ¨w1 âŠ—Â·Â·Â·âŠ— 1,\n|V|âˆ‘\nd1,Â·Â·Â·,dn=1\nTd1...dnwd1 âŠ—Â·Â·Â·âŠ— wdnâŸ©\n=\n|V|âˆ‘\nd1,Â·Â·Â·,dn=1\nTd1...dnâŸ¨w1 âŠ—Â·Â·Â·âŠ— 1,wd1 âŠ—Â·Â·Â·âŠ— wdnâŸ©\n=\n|V|âˆ‘\nd1,Â·Â·Â·,dn=1\nTd1...dn\niâˆ\nj=1\nâŸ¨wj,wdj âŸ©\nnâˆ\nj=i+1\nâŸ¨1,wdj âŸ©\n=\n|V|âˆ‘\ndi+1,Â·Â·Â·,dn=1\nTd1...dn, dk = index(wk,V),âˆ€kâˆˆ[i]\n(32)\nTherefore, we derive that the probability p(wi\n1) can be com-\nputed as p(wi\n1) = âŸ¨wi\n1,câŸ©according to Eq. 31 and 32.\nReferences\nBengio, Y .; Ducharme, R.; Vincent, P.; and Janvin, C. 2003.\nA neural probabilistic language model. Journal of Machine\nLearning Research 3:1137â€“1155.\nBrown, P. F.; Desouza, P. V .; Mercer, R. L.; Pietra, V . J. D.;\nand Lai, J. C. 1992. Class-based n -gram models of natural\nlanguage. Computational Linguistics 18(4):467â€“479.\nCai, D.; He, X.; and Han, J. 2006. Tensor space model\nfor document analysis. In Proceedings of the 29th annual inter-\nnational ACM SIGIR conference on Research and development in\ninformation retrieval, 625â€“626. ACM.\nChen, S. F., and Goodman, J. 1996. An empirical study of\nsmoothing techniques for language modeling. InProceedings\nof the 34th annual meeting on Association for Computational Lin-\nguistics, 310â€“318. Association for Computational Linguis-\ntics.\nCohen, N.; Sharir, O.; and Shashua, A. 2016. On the ex-\npressive power of deep learning: A tensor analysis.Computer\nScience.\nGrave, E.; Joulin, A.; and Usunier, N. 2016. Improving neu-\nral language models with a continuous cache. arXiv preprint\narXiv:1612.04426.\nHitchcock, F. L. 1927. The expression of a tensor or a\npolyadic as a sum of products. Studies in Applied Mathematics\n6(1-4):164-189.\nJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and\nWu, Y . 2016. Exploring the limits of language modeling.\narXiv preprint arXiv:1602.02410.\nKneser, R., and Ney, H. 2002. Improved backing-off for\nm-gram language modeling. In International Conference on\nAcoustics, Speech, and Signal Processing, 181â€“184 vol.1.\nKolda, T. G., and Bader, B. W. 2009. Tensor decompositions\nand applications. Siam Review 51(3):455â€“500.\nLevine, Y .; Yakira, D.; Cohen, N.; and Shashua, A. 2018.\nDeep learning and quantum entanglement: Fundamental\nconnections with implications to network design. In Inter-\nnational Conference on Learning Representations.\nLevine, Y .; Sharir, O.; and Shashua, A. 2017. Beneï¬ts of\ndepth for long-term memory of recurrent networks. arXiv\npreprint arXiv:1710.09431.\nLiu, N.; Zhang, B.; Yan, J.; and Chen, Z. 2005. Text repre-\nsentation: from vector to tensor. In IEEE International Confer-\nence on Data Mining, 725â€“728.\nLopez, A. 2008. Statistical machine translation. Acm Com-\nputing Surveys 40(3):1â€“49.\nMarcus, M. P.; Marcinkiewicz, M. A.; and Santorini, B.\n1993. Building a large annotated corpus of English: the penn\ntreebank. MIT Press.\nMerity, S.; Xiong, C.; Bradbury, J.; Socher, R.; Merity, S.;\nXiong, C.; Bradbury, J.; and Socher, R. 2017. Pointer sen-\ntinel mixture models. In ICLR.\nMikolov, T., and Zweig, G. 2012. Context dependent recur-\nrent neural network language model. SLT 12:234â€“239.\nMikolov, T.; Karaï¬Â´at, M.; Burget, L.; CernockÂ´y, J.; and Khu-\ndanpur, S. 2010. Recurrent neural network based lan-\nguage model. In INTERSPEECH 2010, Conference of the In-\nternational Speech Communication Association, Makuhari, Chiba,\nJapan, September, 1045â€“1048.\nOseledets, I. V . 2011. Tensor-train decomposition. Siam\nJournal on Scientiï¬c Computing 33(5):2295â€“2317.\nPellionisz, A., and Llin Â´as, R. 1980. Tensorial approach to\nthe geometry of brain function: Cerebellar coordination via\na metric tensor. Neuroscience 5(7):1125â€“1136.\nSoutner, D., and MÂ¨uller, L. 2013. Application of lstm neural\nnetworks in language modelling. In International Conference\non Text, Speech and Dialogue, 105â€“112.\nSutskever, I.; Martens, J.; and Hinton, G. E. 2011. Generat-\ning text with recurrent neural networks. In Proceedings of the\n28th International Conference on Machine Learning (ICML-11) ,\n1017â€“1024.\nWang, C.; Chung, G.; and Seneff, S. 2006. Automatic induc-\ntion of language model data for a spoken dialogue system.\nLanguage Resources and Evaluation 40(1):25â€“46.\nWu, Y .; Zhang, S.; Zhang, Y .; Bengio, Y .; and Salakhutdi-\nnov, R. R. 2016. On multiplicative integration with recurrent\nneural networks. In Advances in Neural Information Processing\nSystems, 2856â€“2864.\nYang, Z.; Dai, Z.; Salakhutdinov, R.; and Cohen, W. W.\n2018. Breaking the softmax bottleneck: A high-rank RNN\nlanguage model. In International Conference on Learning Rep-\nresentations.\nYu, D., and Deng, L. 2014. Automatic Speech Recognition: A\nDeep Learning Approach. Springer.\nZaremba, W.; Sutskever, I.; and Vinyals, O. 2014. Recurrent\nneural network regularization. arXiv preprint arXiv:1409.2329.\nZhang, P.; Su, Z.; Zhang, L.; Wang, B.; and Song, D. 2018.\nA quantum many-body wave function inspired language\nmodeling approach. In Proceedings of the 27th ACM Interna-\ntional Conference on Information and Knowledge Management ,\n1303â€“1312. ACM.\n7458"
}