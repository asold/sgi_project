{
  "title": "Play the Shannon Game with Language Models: A Human-Free Approach to Summary Evaluation",
  "url": "https://openalex.org/W3137501073",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2683151353",
      "name": "Nicholas Egan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2300554051",
      "name": "Oleg Vasilyev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158128992",
      "name": "John Bohannon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2683151353",
      "name": "Nicholas Egan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2300554051",
      "name": "Oleg Vasilyev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158128992",
      "name": "John Bohannon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2608106774",
    "https://openalex.org/W2947771965",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2993398598",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3045321166",
    "https://openalex.org/W3021557422",
    "https://openalex.org/W6632455782",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W2108325777",
    "https://openalex.org/W2141927646",
    "https://openalex.org/W2120481102",
    "https://openalex.org/W1826672328",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2949807892",
    "https://openalex.org/W2758374095",
    "https://openalex.org/W2278662189",
    "https://openalex.org/W2970892365",
    "https://openalex.org/W2970807214",
    "https://openalex.org/W6693592476",
    "https://openalex.org/W3008926737",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2123086176",
    "https://openalex.org/W2897802198",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3035628162",
    "https://openalex.org/W3098968529",
    "https://openalex.org/W658020064",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W3093303279",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W2970886762",
    "https://openalex.org/W2963685037",
    "https://openalex.org/W2971034336",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3141940864",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4287684343",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2102065370",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2939896848",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W3014387963"
  ],
  "abstract": "The goal of a summary is to concisely state the most important information in a document. With this principle in mind, we introduce new reference-free summary evaluation metrics that use a pretrained language model to estimate the information content shared between a document and its summary. These metrics are a modern take on the Shannon Game, a method for summary quality scoring proposed decades ago, where we replace human annotators with language models. We also view these metrics as an extension of BLANC, a recently proposed approach to summary quality measurement based on the performance of a language model with and without the help of a summary. Using transformer based language models, we empirically verify that our metrics achieve state-of-the-art correlation with human judgement of the summary quality dimensions of both coherence and relevance, as well as competitive correlation with human judgement of consistency and fluency.",
  "full_text": "Play the Shannon Game With Language Models:\nA Human-Free Approach to Summary Evaluation\nNicholas Egan, Oleg Vasilyev, John Bohannon\nPrimer AI\n{negan, oleg, bohannon}@primer.ai\nAbstract\nThe goal of a summary is to concisely state the most impor-\ntant information in a document. With this principle in mind,\nwe introduce new reference-free summary evaluation metrics\nthat use a pretrained language model to estimate the infor-\nmation content shared between a document and its summary.\nThese metrics are a modern take on the Shannon Game, a\nmethod for summary quality scoring proposed decades ago,\nwhere we replace human annotators with language models.\nWe also view these metrics as an extension of BLANC, a re-\ncently proposed approach to summary quality measurement\nbased on the performance of a language model with and with-\nout the help of a summary. Using transformer based language\nmodels, we empirically verify that our metrics achieve state-\nof-the-art correlation with human judgement of the summary\nquality dimensions of both coherence and relevance, as well\nas competitive correlation with human judgement of consis-\ntency and fluency.\n1 Introduction\nWith the ever-expanding development of new summariza-\ntion algorithms in the NLP community, metrics that reli-\nably measure summary quality are more important than ever.\nAnd yet, the most popular method for summary quality es-\ntimation remains the ROUGE (Lin 2004) family of metrics,\nwhich require human written reference summaries for com-\nparison and measure summary quality through simple token\noverlap, ignoring the syntax and semantics governing the\nway humans use language.\nThe goal of a summary is to concisely state the most im-\nportant information conveyed by a document. Examining\nsummarization through this lens, one should be able to deter-\nmine summary quality by measuring how much information\nfrom the document is represented in the summary. Put an-\nother way, when comparing alternative summaries of simi-\nlar length, the information we gain from reading the original\ndocument should be minimal given the best summary.\nThe idea of measuring this difference in information con-\ntent was proposed as the Shannon Game by Hovy and Lin\n(1998): they assign 3 humans the task of guessing a doc-\nument letter by letter, where the first human is allowed to\nlook at the document, the second human is allowed to look\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nat a summary of the document, and the third human is given\nnothing at all. By measuring how many tries it takes the sec-\nond human to guess the document compared to the other\nhumans, you can evaluate how much information about the\ndocument is communicated in the summary, and therefore\nmeasure how good the summary is.\nContributions This paper proposes a new summarization\nevaluation metric, the Shannon Score, that performs the\nShannon Game with a language model such as GPT-2 (Rad-\nford et al. 2019). By using a language model to autoregres-\nsively generate a document both with and without a sum-\nmary as a prompt, we measure the information provided by\nthe summary. One can view this method as a more theoreti-\ncally driven extension to the recently proposed BLANC met-\nric (Vasilyev, Dharnidharka, and Bohannon 2020), which\nmeasures the accuracy of unmasking document tokens with\nand without a summary. In addition to the Shannon Score,\nwe also propose a variant we call Information Difference.\nTo understand the empirical performance of this method\nas a summary evaluation technique, we performed experi-\nments to correlate our metrics against human judgement.\nWe found that our metrics perform strongly on the Sum-\nmEval benchmark (Fabbri et al. 2021), achieving state-of-\nthe-art correlation with human judgement of summary co-\nherence and relevance, and competitive correlation with hu-\nman judgement of summary consistency and fluency.\n2 Methods\n2.1 Computing Information\nLanguage models are probability distributions over docu-\nments, giving usp(D) for some documentD. Autoregressive\nlanguage models do this by predicting next token probabili-\nties given prior tokens, modeling\np(xt|x1, . . . , xt−1)\nwhere our input document is broken into tokens\n{x1, . . . , xn}. The Shannon information content, or\nsurprisal, of event E with probability p(E) of happening\nis defined as I(E) = −log p(E), so we can compute the\ninformation of a document according to our language model\nas\nI(D) =−log p(x1) −log p(x2|x1) −. . .\n−log p(xn|x1, x2, . . . xn−1)\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10599\n2.2 Conditional Information\nSuppose we had a conditional language model p(D|S) that\ngives us a probability distribution of documents that could\ncorrespond to a given summary S. Using this conditional\nlanguage model, we could compute the conditional informa-\ntion content I(D|S) as the amount of information we gain\nfrom the documentDif we are already given the information\nof summary S.\nIf Sis a satisfactory summary ofD, then I(D|S) < I(D),\nas documents that have little to do with the summary should\nbe much less likely than documents that are relevant to the\nsummary after conditioning the language model. If the sum-\nmary fluently describes people, ideas, or relationships that\nappear in the document, then that should decrease the infor-\nmation one learns from subsequently reading the document.\nThus we can define an Information Difference metric of\nsummary quality as:\nID(D, S) =I(D) −I(D|S)\nThe Information Difference tells us the change in document\ninformation between using the summary and not using the\nsummary, and it is equivalent to the log likelihood ratio be-\ntween the document and the document given the summary.\nWhile it is unbounded, it should be positive unless a sum-\nmary does such a bad job that it makes the document more\nconfusing to read.\nConsidering the fact that the summary that best preserves\nthe information of a document is the document itself, we can\nview I(D|D) as a lower bound on I(D|S). Since this idea\nof having a third evaluator who has the document itself as\nhelp is inspired by the Shannon Game, we can compute the\nShannon Score metric as:\ns(D, S) = I(D) −I(D|S)\nI(D) −I(D|D)\nThe Shannon Score gives us the ratio between how helpful\nthe summary was and how helpful the document itself was.\nWhile this formula in theory is unbounded, it usually should\nbe in the range 0 to 1, unless the summary makes the doc-\nument more confusing or somehow explains the document\nbetter than the document itself.\n2.3 Approximating Conditional Information\nTo the extent of our knowledge, there is no easy way to ex-\nactly condition a pretrained language model such as GPT-2\non a summary, even though there has been work on con-\nditioning language models on fixed control codes (Keskar\net al. 2019), bags of words, or discriminators (Dathathri\net al. 2020). We also have a strong motivation not to train\nsuch a model because we want our method to be universal\nand robust, while summarization datasets are much smaller\nand more restricted in domain than the massive datasets that\nmodern language models require.\nWe approximate p(D|S) by computing the probability\nthat Dis generated when we provide Sas a prompt to a\nlanguage model. We intuitively justify this idea by the fact\nthat in real-world documents the most important information\nis often summarized at the top as an introduction, and then\ndescribed in more detail in body paragraphs. This setup re-\nsembles the BLANC-help metric (Vasilyev, Dharnidharka,\nand Bohannon 2020), which measures language model to-\nken unmasking accuracy for a document when a summary is\nprepended. An alternative setup would be to finetune a lan-\nguage model on the summary which was also explored by\nVasilyev, Dharnidharka, and Bohannon (2020), but we don’t\nexplore that method in this paper. We use the GPT-2 small\nlanguage model (Radford et al. 2019) for our experiments,\nbut investigate the use of other language models in section\n5.1.\nAn issue we run into when computing information with\nGPT-2 is that the model can only be given a maximum of\n1024 tokens, making many documents too large to fit in at\nonce. To get around this, we approximate document infor-\nmation with an independence assumption between sentences\nin the document, meaning that only the preceding tokens\nwithin a sentence are provided when generating the next to-\nken in the sentence. In section 5.2, we investigate the effects\nof prompting the language model with additional upstream\nsentences of context.\n3 Understanding Our Metrics\n3.1 Information Visualization\nA toy illustration of our methodology is shown in Figure 1.\nWe picked a document excerpt in the CNN/DailyMail (Her-\nmann et al. 2015) dataset and paired it with two abstrac-\ntive summaries we wrote. While both of these summaries\nare grammatically correct and mostly consist of words from\nthe document, one of the summaries is of high quality and\nthe other is of low quality. The figure shows the information\ncontent of each token in the document as estimated by GPT-\n2 in 4 scenarios: I(D) (the document on its own), I(D|D)\n(the document given the document), I(D|S) (the document\ngiven a summary) for the high quality summary, andI(D|S)\nfor the low quality summary. A darker background color de-\nnotes higher information according to the model.\nAs you can see, the model gained less information from\nwords like “gray” and “Varvara” after seeing those words\nin the high quality summary. We can also see that words\nlike “Pacific” and “journey,” which do not appear in the high\nquality summary, became more likely to appear in the docu-\nment due to their association with concepts in the summary.\nThe low quality summary may have helped the model pre-\ndict words like “CNN,” but it is unhelpful for words like\n“mammal” and “website” that are confusingly used in the\nsummary. Very little information was gained from reading\na document that was already read, except for the first token\nor two for each sentence. This is an artifact of our autore-\ngressive language modeling setup, so measuring I(D|D) is\nuseful for normalizing our Shannon Scores.\nWe used a truncated document and toy summaries here to\ndemonstrate the Shannon Score in a concise way, but we\nincluded visualizations of real, full-length documents and\nsummaries from the SummEval dataset in the appendix.\n10600\nI(D) = 580 I(D|D) = 52\n( CNN ) A North Pacific gray whale has earned a spot in the record\nbooks after completing the longest migration of a mammal ever\nrecorded . The whale , named Varv ara , sw am nearly 14 , 000 miles\n( 22 , 500 kilometers ), according to a release from Oregon State\nUniversity , whose scientists helped conduct the whale - tracking\nstudy . Var v ara , which is Russian for ” Bar bara ,” left her\nprimary feeding ground off Russia ’s S akh alin Island to cross the\nPacific Ocean and down the West Coast of the United States to B\naja , Mexico . Var v ara ’s journey surpassed a record listed on the\nGuinness Worlds Records website . It said the previous record was\nset by a hump back whale that sw am a mere 10 , 190 - mile round\ntrip\n( CNN ) A North Pacific gray whale has earned a spot in the record\nbooks after completing the longest migration of a mammal ever\nrecorded . The whale , named Varv ara , sw am nearly 14 , 000 miles\n( 22 , 500 kilometers ), according to a release from Oregon State\nUniversity , whose scientists helped conduct the whale - tracking\nstudy . Var v ara , which is Russian for ” Bar bara ,” left her\nprimary feeding ground off Russia ’s S akh alin Island to cross the\nPacific Ocean and down the West Coast of the United States to B\naja , Mexico . Var v ara ’s journey surpassed a record listed on the\nGuinness Worlds Records website . It said the previous record was\nset by a hump back whale that sw am a mere 10 , 190 - mile round\ntrip\nI(D|S) = 482for this high quality summary: I(D|S) = 540for this low quality summary:\nVarvara the gray whale traveled from Russia to Mexico, a swim\nof record breaking length.\nThe round humpback has told CNN mammals that Baja was a\nprevious Pacific website for ”Guinness.”\n( CNN ) A North Pacific gray whale has earned a spot in the record\nbooks after completing the longest migration of a mammal ever\nrecorded . The whale , named Varv ara , sw am nearly 14 , 000 miles\n( 22 , 500 kilometers ), according to a release from Oregon State\nUniversity , whose scientists helped conduct the whale - tracking\nstudy . Var v ara , which is Russian for ” Bar bara ,” left her\nprimary feeding ground off Russia ’s S akh alin Island to cross the\nPacific Ocean and down the West Coast of the United States to B\naja , Mexico . Var v ara ’s journey surpassed a record listed on the\nGuinness Worlds Records website . It said the previous record was\nset by a hump back whale that sw am a mere 10 , 190 - mile round\ntrip\n( CNN ) A North Pacific gray whale has earned a spot in the record\nbooks after completing the longest migration of a mammal ever\nrecorded . The whale , named Varv ara , sw am nearly 14 , 000 miles\n( 22 , 500 kilometers ), according to a release from Oregon State\nUniversity , whose scientists helped conduct the whale - tracking\nstudy . Var v ara , which is Russian for ” Bar bara ,” left her\nprimary feeding ground off Russia ’s S akh alin Island to cross the\nPacific Ocean and down the West Coast of the United States to B\naja , Mexico . Var v ara ’s journey surpassed a record listed on the\nGuinness Worlds Records website . It said the previous record was\nset by a hump back whale that sw am a mere 10 , 190 - mile round\ntrip\nFigure 1: A comparison of token-wise information content within a document as estimated by GPT-2 in 4 scenarios: the\ndocument on its own, the document given the document, the document given a high quality summary, and the document given\na low quality summary. Tokens with a darker background color have more information.\n3.2 Baseline Validation\nAs a simple validation of our information-based metrics, we\nsampled 100 documents with their corresponding reference\nsummaries from the CNN/DailyMail dataset (Hermann et al.\n2015), and created two “bad” summaries per document: a\nversion of the reference summary with all the words ran-\ndomly shuffled, and a reference summary for a different doc-\nument in the dataset.\nFigure 2 shows the distributions of the Shannon Score\nand Information Difference for these three summaries. As\nexpected, the original summaries have the highest scores,\nfollowed by shuffled summaries and wrong summaries. It\nis good to see that there is full separation between origi-\nnal summaries and wrong summaries for both metrics. The\nfact that the original summaries and shuffled summaries are\nalmost completely separated demonstrates the importance\nof syntax to our metrics, a quality that metrics like the\nJensen-Shannon divergence (Louis and Nenkova 2009) and\nROUGE-1 (Lin 2004) lack.\nWe also verified that there are no documents for which\nthe shuffled summary or wrong summary score better than\nthe original summary for either of the metrics. Despite the\nfact that the Shannon Score has no lower bound, we can see\nthat it doesn’t go far below zero for even the most unrea-\nsonable of summaries. And despite the fact that the Shannon\nScore has no upper bound, even high quality human refer-\nence summaries are unable to achieve a score above 0.4.\n4 Evaluation of Our Metrics\n4.1 SummEval\nThe SummEval (Fabbri et al. 2021) benchmark was estab-\nlished as a comprehensive evaluation tool for summary eval-\nuation metrics. It consists of 100 English-language docu-\nments from the CNN/DailyMail dataset, each paired with\nsystem summaries from 17 different summarization sys-\ntems: 3 extractive models, 13 abstractive models, and a lead-\n3 baseline. All models were published in 2017 or later. Each\nof these 1700 system summaries were scored by a panel of 3\nexperts in the field of summarization on the qualities of co-\nherence (the collective quality of all sentences), consistency\n(the factual alignment between the summary and document),\nfluency (the quality of individual sentences), and relevance\n(selection of important content from the source). The experts\nachieved an inter-annotator agreement kappa coefficient of\n0.7127.\n10601\nFigure 2: Distributions of Shannon Score and Information\nDifference on 100 summaries from the CNN/DailyMail\ndataset. Three different summaries are used: the original hu-\nman written reference summary (in blue), the original sum-\nmary with words scrambled (in orange), and a reference\nsummary for a different document in the dataset (in green).\nFabbri et al. (2021) scored each summary using these\nevaluation metrics: ROUGE (Lin 2004), ROUGE-WE (Ng\nand Abrecht 2015), S3 (Peyrard, Botschen, and Gurevych\n2017), BertScore (Zhang et al. 2020), MoverScore (Zhao\net al. 2019), Sentence Mover’s Similarity (SMS) (Clark,\nCelikyilmaz, and Smith 2019), SummaQA (Scialom et al.\n2019), BLANC (Vasilyev, Dharnidharka, and Bohannon\n2020), SUPERT (Gao, Zhao, and Eger 2020), BLEU (Pap-\nineni et al. 2002), CHRF (Popovi ´c 2015), METEOR (Lavie\nand Agarwal 2007), and CIDEr (Vedantam, Lawrence Zit-\nnick, and Parikh 2015). They also measure the Grusky, Naa-\nman, and Artzi (2018) statistics of summary length, extrac-\ntive fragment coverage (coverage), compression ratio, aver-\nage length of extractive fragments (density), proportion of\nn-grams in summary that aren’t in the document (novel n),\nand n-grams repeated in summary (repeat n).\nTable 1 shows the correlation between expert annota-\ntions and the automated evaluation metrics. Following Fab-\nbri et al. (2021), we use Kendall tau-b system-level corre-\nlation for comparison. Our metrics of Shannon Score and\nInformation Difference are the only metrics to be in the top\n5 for every category of summary quality. Additionally, our\nmetrics achieve state-of-the-art performance for the qualities\nof coherence and relevance.\n4.2 Coverage\nThe coverage score (Lin and Hovy 2003) is a human evalua-\ntion method that measures a system summary’s recall of se-\nmantic units that appeared in a reference summary, weighed\nby how well the system summary was able to capture each\nsemantic unit as judged by the human labeler. The 2001 and\n2002 Document Understanding Conferences (DUC) provide\ndatasets of English-language system and reference sum-\nmaries for news documents with human coverage labels, on\nboth single-document and multi-document levels.\nTable 2 shows the correlation of various metrics to\nthese coverage scores for the single-document summaries.\nSystem-level Spearman correlation is used following Louis\nMetric Coher. Consi. Fluen. Relev.\nShannonˆ 0.4118 0.6324 0.5240 0.6029\nInfo Diffˆ 0.4706 0.6324 0.5683 0.6618\nrouge-1 0.2500 0.5294 0.5240 0.4118\nrouge-2 0.1618 0.5882 0.4797 0.2941\nrouge-3 0.2206 0.7059 0.5092 0.3529\nrouge-4 0.3088 0.5882 0.5535 0.4118\nrouge-L 0.0735 0.1471 0.2583 0.2353\nrouge-su* 0.1912 0.2941 0.4354 0.3235\nrouge-w 0.0000 0.3971 0.3764 0.1618\nrouge-we-1 0.2647 0.4559 0.5092 0.4265\nrouge-we-2 -0.0147 0.5000 0.3026 0.1176\nrouge-we-3 0.0294 0.3676 0.3026 0.1912\nS3-pyr -0.0294 0.5147 0.3173 0.1324\nS3-resp -0.0147 0.5000 0.3321 0.1471\nBertScore-p 0.0588 -0.1912 0.0074 0.1618\nBertScore-r 0.1471 0.6618 0.4945 0.3088\nBertScore-f 0.2059 0.0441 0.2435 0.4265\nMoverScore 0.1912 -0.0294 0.2583 0.2941\nSMS 0.1618 0.5588 0.3616 0.2353\nSummaQAˆ 0.1176 0.6029 0.4059 0.2206\nBLANCˆ 0.0735 0.5588 0.3616 0.2647\nSuPERTˆ 0.1029 0.5882 0.4207 0.2353\nBLEU 0.1176 0.0735 0.3321 0.2206\nCHRF 0.3971 0.5294 0.4649 0.5882\nCIDEr 0.1176 -0.1912 -0.0221 0.1912\nMETEOR 0.2353 0.6324 0.6126 0.4265\nLengthˆ -0.0294 0.4265 0.2583 0.1618\nNovel 1ˆ 0.1471 -0.2206 -0.1402 0.1029\nNovel 2ˆ 0.0294 -0.5441 -0.3469 -0.1029\nNovel 3ˆ 0.0294 -0.5735 -0.3469 -0.1324\nRepeat 1ˆ -0.3824 0.1029 -0.0664 -0.3676\nRepeat 2ˆ -0.3824 -0.0147 -0.2435 -0.4559\nRepeat 3ˆ -0.2206 0.1471 -0.0221 -0.2647\nCoverageˆ -0.1324 0.3529 0.1550 -0.0294\nCompressˆ 0.1176 -0.4265 -0.2288 -0.0147\nDensityˆ 0.1618 0.6471 0.3911 0.2941\nTable 1: Kendall tau-b system-level correlation between ex-\npert annotations of coherence, consistency, fluency, and rel-\nevance and various automated metrics, adapted from Fabbri\net al. (2021). ˆ denotes reference-free metrics. The five high-\nest correlations per column are in bold, with ties for con-\nsistency and relevance. Coefficients with a magnitude above\n0.36 are significant at the α = 0.05 level.\nand Nenkova (2013). The reference-free metrics perform\nsimilarly, except for Jensen-Shannon Divergence (Louis and\nNenkova 2009) which performs particularly well on DUC\n2001 and Info Diff which performs particularly poorly on\nDUC 2002. The metrics using references benefit from the\nbias that the coverage itself was measured with respect to\nthe reference summary, so as expected, they have higher cor-\nrelations with the coverage than the reference-free metrics\nfor this dataset. A fair comparison would involve a cover-\nage measured with respect to the document itself. One can\n10602\nMetric DUC 2001 DUC 2002\nShannon Score 0.2909 0.5714\nInfo Diff 0.3000 0.4835\nJensen-Shannon 0.4455 0.5440\nBLANC-help 0.2727 0.5769\nROUGE-1 0.9636 0.9066\nROUGE-2 0.8273 0.9121\nROUGE-L 0.7455 0.9176\nROUGE-Lsum 0.9455 0.9066\nBERTScore-P 0.4636 0.5989\nBERTScore-R 0.8545 0.9451\nBERTScore-F1 0.6091 0.7308\nTable 2: System-level Spearman correlation of various sum-\nmary quality metrics with human-judged coverage scores on\nthe DUC 2001 and 2002 single-document summary datasets.\nThe last seven metrics make use of reference summaries,\nwhile the first four metrics have to rely only on the origi-\nnal document itself. DUC 2001 coefficients above 0.60 and\nDUC 2002 coefficients above 0.55 are significant at the\nα = 0.05 level.\nalso see that most metrics perform better on DUC 2002 than\nDUC 2001: this was also observed by Sun and Nenkova\n(2019), who suggested that this can be explained by the fact\nthat DUC 2001 systems are more similar to each other and\nworse than DUC 2002 systems on average.\n4.3 Metric Biases\nTo understand the biases of our metrics, we measured the\ncorrelation between our metrics and the SummEval statis-\ntics describing summaries described in section 4.1 across\nthe 1700 SummEval summaries. For comparison, we also\ncorrelated the expert summary quality judgements with the\nstatistics. These correlations are shown in table 3.\nBoth of our metrics have significant positive correlation\nwith summary length, which is expected since longer sum-\nmaries can contain more information. Our metrics have bias\nagainst more abstractive summaries (based on noveln-gram,\ncoverage, and density), but we are generally less biased\nagainst abstractive summaries than humans judging consis-\ntency are: we suspect this is because abstractive summaries\nare more likely to hallucinate factual errors. The Shan-\nnon Score is biased against highly compressed summaries,\nwhich is not shared by Information Difference.\n5 Metric Variations\n5.1 Choice of Language Model or Model Size\nIn the previous sections, we used GPT-2 small as our lan-\nguage model of choice when computing the Shannon Score\nand Information Difference. To understand how well our\nmethod generalizes to other language models, we computed\nthe Shannon Score and Information Difference metrics us-\ning the three other GPT-2 sizes (medium, large, and extra-\nlarge), and three other language models with autoregressive\nModel Coher. Consi. Fluen. Relev.\nShannon Score\nGPT-2 S 0.4118 0.6324 0.5240 0.6029\nGPT-2 M 0.3529 0.6618 0.4945 0.5441\nGPT-2 L 0.3676 0.6471 0.5092 0.5588\nGPT-2 XL 0.3824 0.6324 0.4945 0.5735\nGPT 0.0294 0.5147 0.3469 0.1912\nXLNet 0.4265 0.5882 0.4945 0.6471\nTransfoXL 0.3529 0.5441 0.4502 0.5441\nInformation Difference\nGPT-2 S 0.4706 0.6324 0.5683 0.6618\nGPT-2 M 0.3971 0.6765 0.5092 0.5882\nGPT-2 L 0.3824 0.6324 0.4945 0.5735\nGPT-2 XL 0.3971 0.6471 0.5092 0.5882\nGPT 0.0441 0.5294 0.3616 0.2059\nXLNet 0.4559 0.5882 0.5240 0.6765\nTransfoXL 0.3529 0.5441 0.4502 0.5441\nTable 4: Kendall tau-b system-level correlations between ex-\npert annotations of coherence, consistency, fluency, and rel-\nevance and our Shannon Score and Information Difference\nmetrics with the choice of different language models on the\nSummEval dataset. Scores at least as high as GPT-2 S are\nbold. Coefficients above 0.36 are significant at theα = 0.05\nlevel.\nFigure 3: The average document information and document\ninformation given summary as estimated by different sizes\nof GPT-2 for the SummEval dataset.\npretraining objectives: GPT (Radford et al. 2018), XLNet\n(Yang et al. 2019), and Transformer-XL (Dai et al. 2019).\nTable 4 shows the system-level Kendall tau-b correlation\nbetween our metrics and the SummEval quality judgements\nfrom section 4.1 for each language model. The language\nmodels perform quite similarly overall, suggesting that the\nchoice of language model is not overly important when us-\ning the Shannon Score or Information Difference. The ex-\nception is the low correlation of GPT, particularly on the co-\nherence and relevance qualities: we suspect this is because\nGPT was trained on the BooksCorpus dataset (Zhu et al.\n2015), which is less diverse than the datasets used for the\nother language models.\n10603\nMetric Info Diff Shannon Score Coherence Consistency Fluency Relevance\nLength 0.5425 0.4291 0.0615 0.0886 -0.0105 0.2054\nNovel 1 -0.1140 -0.0962 0.1340 -0.2719 -0.1924 0.0267\nNovel 2 -0.2935 -0.2849 -0.0248 -0.3693 -0.2674 -0.0733\nNovel 3 -0.3324 -0.3297 -0.0781 -0.3840 -0.2755 -0.1035\nCoverage 0.2163 0.1896 0.0144 0.3369 0.2431 0.0688\nCompression -0.0879 -0.6086 -0.0041 -0.0697 -0.0084 -0.1155\nDensity 0.4591 0.4517 0.1991 0.4035 0.2738 0.2019\nTable 3: Spearman correlation of our metrics and human judged quality metrics with various statistics describing summaries\nacross the 1700 SummEval summaries.\nIt is also interesting to see that bigger GPT-2 models do\nnot necessarily perform better. Figure 3 shows the relation-\nship between model size and average document info with\nand without the help of a summary. We can see that as the\nmodel gets larger, both average I(D) and average I(D|S)\ndecrease together. Larger models should be better at autore-\ngressive token prediction, as reflected in the plot of I(D),\nbut it is interesting to see that I(D|S) decreases at around\nthe same rate. We suspect this is because larger models may\nnot be more suitable at utilizing a summary to predict a doc-\nument under our setup.\n5.2 Upstream Sentences\nAs described in section 2.3, we are making an independence\nassumption between sentences in a document when estimat-\ning I(D), I(D|S), and I(D|D) by feeding each sentence\ninto the model individually. We could alternatively assume\nthat each sentence in the document is dependent on the k\nprevious sentences, where k = 0 refers to our current ap-\nproach and k = ∞(or the maximum number of sentences\nin a document) drops the sentence independence assumption\naltogether. One could reason that this would better allow us\nto quantify the information in a document, which may lead\nto a more effective metric.\nAs shown in table 5, usingk >0 leads to an improvement\nin consistency at the expense of the other summary dimen-\nsions, and increasing k beyond 1 does not yield any signif-\nicant gains in performance. Figure 4 shows that increasing\nk from 0 is more helpful at decreasing I(D) than it is at\ndecreasing I(D|S). We could draw a similar conclusion as\nwe did in section 5.1 that increasing k is helpful for autore-\ngressive token prediction, but it doesn’t help our model with\nutilizing a summary to predict a document in our setup.\n5.3 BLANC-Shannon\nOur metrics bear similarity to the BLANC-help metric\n(Vasilyev, Dharnidharka, and Bohannon 2020; Vasilyev\net al. 2020), which measures the accuracy of the BERT\nlanguage model on the task of guessing masked tokens\nwith and without a summary prepended to a document. The\nBLANC score is measured as a boost in unmasking accu-\nracy ahelp −abase when masking various sets of M evenly\nspaced tokens, where ahelp is the accuracy when the sum-\nmary is provided as help and abase is the accuracy when no\nhelp is provided. Our metrics differ from BLANC in that\nwe measure information instead of raw accuracy, we gener-\nate documents autoregressively instead of masking, and we\ntypically use GPT-2 instead of BERT.\nTo study the utility of measuring document information\nas opposed to raw accuracy counts, we define BLANC-\nShannon to be the boost in accuracy when generating docu-\nment tokens given the summary. On the SummEval bench-\nmark, BLANC-Shannon achieves Kendall tau-b system-\nlevel correlations of 0.3676, 0.6765, 0.5092, and 0.5588 for\nthe expert annotations of coherence, consistency, fluency,\nand relevance respectively. These scores are an improvement\non the consistency dimension over the Shannon Score and\nInformation Difference metrics at the expense of every other\ndimension. We can only hypothesize that accuracy may be\nmore sensitive to wrongly generated tokens and hence to\nconsistency, but it would be interesting to compare BLANC-\nShannon to the other metrics on an even larger dataset than\nSummEval.\n6 Related Work\nThe Shannon Game The Shannon Game (Hovy and Lin\n1998) was proposed over two decades ago as a way to use\nhumans to measure the information retention between doc-\nument and summary. In the original formulation, humans\nneed to guess a document letter by letter given the summary,\ndocument, or nothing, and they measure the total number\nof guesses that were required to reconstruct the document.\nThe authors ran a small-scale experiment where they con-\nducted this game using human subjects, and they found a\nclear order of magnitude difference between the number of\nguesses each human required, as expected. However, they\nalso found that reconstructing the original document with no\nhelp (the task of human 3) was extremely time-consuming,\nsometimes taking over 3 hours, making the Shannon Game\nprohibitively expensive as a human evaluation method.\nAutomated Summary Evaluation The most popular au-\ntomatic summarization evaluation method is the ROUGE\nfamily of metrics (Lin 2004; Lin and Och 2004), which mea-\nsure word overlap between the system summary and one or\nmore reference summaries. The two biggest problems we\nsee with ROUGE as a metric are 1) that it relies on human\nwritten reference summaries, and 2) that it measures sim-\nple word overlap, which means that a perfectly paraphrased\nversion of the reference summary would score poorly.\n10604\nk Coher. Consi. Fluen. Relev.\nShannon Score\n0 0.4118 0.6324 0.5240 0.6029\n1 0.3529 0.6618 0.4945 0.5441\n2 0.3235 0.6618 0.4945 0.5147\n3 0.3235 0.6618 0.4945 0.5147\n4 0.3235 0.6618 0.4945 0.5147\nInformation Difference\n0 0.4706 0.6324 0.5683 0.6618\n1 0.3529 0.6618 0.4945 0.5441\n2 0.3382 0.6765 0.5092 0.5294\n3 0.3235 0.6618 0.4945 0.5147\n4 0.3382 0.6765 0.5092 0.5294\nTable 5: Kendall tau-b system-level correlations between ex-\npert annotations of coherence, consistency, fluency, and rel-\nevance and our Shannon Score and Information Difference\nmetrics with different choices of k (the number of upstream\nsentences to provide the model) on the SummEval dataset.\nScores at least as high as those of k = 0 are bold. Coeffi-\ncients above 0.36 are significant at the α = 0.05 level.\nFigure 4: The average document information and document\ninformation given summary when prompting GPT-2 with\ndifferent amounts of upstream sentences for the SummEval\ndataset.\nMany solutions have been proposed to remedy issue #2\nwithout solving issue #1, such as BERTScore (Zhang et al.\n2020), MoverScore (Zhao et al. 2019), Sentence Mover Sim-\nilarity (Clark, Celikyilmaz, and Smith 2019), Word Mover\nSimilarity (Kusner et al. 2015), and ROUGE-WE (Ng and\nAbrecht 2015). All of these metrics involve the idea of using\nsoft overlap or embedding/token distance between the sys-\ntem and reference summaries. Louis and Nenkova (2009)\nsuggested measuring the Jensen-Shannon divergence be-\ntween word distributions used in the system summary and\noriginal document, which suffers from issue #2 while fix-\ning issue #1. Sun and Nenkova (2019) and Gao, Zhao, and\nEger (2020) perform reference-free summary evaluation us-\ning language model word embeddings with promising re-\nsults. Other have used question generation and question an-\nswering models to evaluate summaries (Scialom et al. 2019;\nChen et al. 2018), but we argue that these metrics are only\nas good as the datasets the models were trained on, and may\nhave problems generalizing. Beyond summarization, there\nhave been many metrics proposed for Natural Language\nGeneration more generally (Sai, Mohankumar, and Khapra\n2020).\nOur methods are most similar to BLANC (Vasilyev,\nDharnidharka, and Bohannon 2020; Vasilyev et al. 2020),\nwhich measures the accuracy boost of BERT (Devlin et al.\n2019) on the Cloze task (Taylor 1953) when a summary is\nprepended to a document or the model is finetuned on the\nsummary. This paper contributes to the study of BLANC-\nlike metrics by extending them to new language models,\ngiving them a theoretical motivation, and performing more\nrobust experiments to better understand their behavior. The\ninformation-theoretic motivations of our metrics are similar\nto that of Peyrard (2019) who formally defined some met-\nrics based on distributions of semantic units, which contrasts\nwith our use of pretrained language models.\n7 Conclusion\nIn this work, we successfully show that a universal lan-\nguage model performing the basic language modeling task\nis an effective reference-free evaluator of summary qual-\nity. This work extends the Shannon Game from using hu-\nmans as evaluators to using machines, and extends the work\non BLANC-like metrics to new language models and theo-\nretical interpretations. We experimentally showed that our\nmetrics strongly correlate with expert judgement of sum-\nmary quality, and hope that they will serve as useful tools\nfor the future development of summarization models. As\nnext steps, it would be interesting to see if our metrics are\nuseful for summarization model training, or evaluation in\ntasks beyond standard summarization, such as paraphrasing\nor query-focused summarization. Our code is available on\nGitHub.1\nReferences\nChen, P.; Wu, F.; Wang, T.; and Ding, W. 2018. A Semantic\nQA-Based Approach for Text Summarization Evaluation. In\nProceedings of the Thirty-Second AAAI Conference on Arti-\nficial Intelligence, 4800–4807. AAAI Press (2018).\nClark, E.; Celikyilmaz, A.; and Smith, N. A. 2019. Sen-\ntence Mover’s Similarity: Automatic Evaluation for Multi-\nSentence Texts. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 2748–\n2760. Florence, Italy: Association for Computational Lin-\nguistics.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\n1github.com/primerai/blanc/tree/master/shannon\n10605\nputational Linguistics, 2978–2988. Florence, Italy: Associ-\nation for Computational Linguistics.\nDathathri, S.; Madotto, A.; Lan, J.; Hung, J.; Frank, E.;\nMolino, P.; Yosinski, J.; and Liu, R. 2020. Plug and Play\nLanguage Models: A Simple Approach to Controlled Text\nGeneration. In International Conference on Learning Rep-\nresentations.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nFabbri, A. R.; Kry ´sci´nski, W.; McCann, B.; Xiong, C.;\nSocher, R.; and Radev, D. 2021. SummEval: Re-evaluating\nSummarization Evaluation. Transactions of the Association\nfor Computational Linguistics, 9: 391–409.\nGao, Y .; Zhao, W.; and Eger, S. 2020. SUPERT: To-\nwards New Frontiers in Unsupervised Evaluation Metrics\nfor Multi-Document Summarization. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, 1347–1354. Online: Association for Computa-\ntional Linguistics.\nGrusky, M.; Naaman, M.; and Artzi, Y . 2018. Newsroom: A\nDataset of 1.3 Million Summaries with Diverse Extractive\nStrategies. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long Papers), 708–719. New Orleans, Louisiana: Asso-\nciation for Computational Linguistics.\nHermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.;\nKay, W.; Suleyman, M.; and Blunsom, P. 2015. Teach-\ning Machines to Read and Comprehend. In Cortes, C.;\nLawrence, N.; Lee, D.; Sugiyama, M.; and Garnett, R., eds.,\nAdvances in Neural Information Processing Systems, vol-\nume 28, 1693–1701. Curran Associates, Inc.\nHovy, E.; and Lin, C.-Y . 1998. Automated Text Summariza-\ntion and the Summarist System. In TIPSTER TEXT PRO-\nGRAM PHASE III: Proceedings of a Workshop held at Balti-\nmore, Maryland, October 13-15, 1998, 197–214. Baltimore,\nMaryland, USA: Association for Computational Linguistics.\nKeskar, N. S.; McCann, B.; Varshney, L.; Xiong, C.; and\nSocher, R. 2019. CTRL - A Conditional Transformer Lan-\nguage Model for Controllable Generation. arXiv preprint\narXiv:1909.05858.\nKusner, M.; Sun, Y .; Kolkin, N.; and Weinberger, K. 2015.\nFrom Word Embeddings To Document Distances. vol-\nume 37 ofProceedings of Machine Learning Research, 957–\n966. Lille, France: Proceedings of Machine Learning Re-\nsearch (PMLR).\nLavie, A.; and Agarwal, A. 2007. METEOR: An Automatic\nMetric for MT Evaluation with High Levels of Correlation\nwith Human Judgments. InProceedings of the Second Work-\nshop on Statistical Machine Translation, 228–231. Prague,\nCzech Republic: Association for Computational Linguistics.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Barcelona, Spain: Association for Computational\nLinguistics.\nLin, C.-Y .; and Hovy, E. 2003. Automatic Evaluation of\nSummaries Using N-gram Co-occurrence Statistics. In Pro-\nceedings of the 2003 Human Language Technology Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics, 150–157.\nLin, C.-Y .; and Och, F. J. 2004. Automatic Evaluation of\nMachine Translation Quality Using Longest Common Sub-\nsequence and Skip-Bigram Statistics. In Proceedings of the\n42nd Annual Meeting of the Association for Computational\nLinguistics (ACL-04), 605–612. Barcelona, Spain.\nLouis, A.; and Nenkova, A. 2009. Automatically Evaluating\nContent Selection in Summarization without Human Mod-\nels. In Proceedings of the 2009 Conference on Empirical\nMethods in Natural Language Processing, 306–314. Singa-\npore: Association for Computational Linguistics.\nLouis, A.; and Nenkova, A. 2013. Automatically Assessing\nMachine Summary Content Without a Gold Standard. Com-\nputational Linguistics, 39(2): 267–300.\nNg, J.-P.; and Abrecht, V . 2015. Better Summarization Eval-\nuation with Word Embeddings for ROUGE. In Proceedings\nof the 2015 Conference on Empirical Methods in Natural\nLanguage Processing, 1925–1930. Lisbon, Portugal: Asso-\nciation for Computational Linguistics.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBLEU: A Method for Automatic Evaluation of Machine\nTranslation. In Proceedings of the 40th Annual Meeting\non Association for Computational Linguistics, ACL ’02,\n311–318. USA: Association for Computational Linguistics.\nPeyrard, M. 2019. A Simple Theoretical Model of Impor-\ntance for Summarization. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics ,\n1059–1073. Florence, Italy: Association for Computational\nLinguistics.\nPeyrard, M.; Botschen, T.; and Gurevych, I. 2017. Learn-\ning to Score System Summaries for Better Content Selec-\ntion Evaluation. In Proceedings of the Workshop on New\nFrontiers in Summarization, 74–84. Copenhagen, Denmark:\nAssociation for Computational Linguistics.\nPopovi´c, M. 2015. chrF: character n-gram F-score for auto-\nmatic MT evaluation. In Proceedings of the Tenth Workshop\non Statistical Machine Translation, 392–395. Lisbon, Portu-\ngal: Association for Computational Linguistics.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving Language Understanding by Generative\nPre-Training. OpenAI.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners. OpenAI.\nSai, A. B.; Mohankumar, A. K.; and Khapra, M. M. 2020.\nA Survey of Evaluation Metrics Used for NLG Systems.\narXiv:2008.12009.\n10606\nScialom, T.; Lamprier, S.; Piwowarski, B.; and Staiano, J.\n2019. Answers Unite! Unsupervised Metrics for Reinforced\nSummarization Models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 3246–3256. Hong\nKong, China: Association for Computational Linguistics.\nSun, S.; and Nenkova, A. 2019. The Feasibility of Em-\nbedding Based Automatic Evaluation for Single Document\nSummarization. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), 1216–1221. Hong Kong,\nChina: Association for Computational Linguistics.\nTaylor, W. L. 1953. Cloze procedure: A new tool for mea-\nsuring readability. Journalism Bulletin, 30(4): 415–433.\nVasilyev, O.; Dharnidharka, V .; and Bohannon, J. 2020. Fill\nin the BLANC: Human-free quality estimation of document\nsummaries. In Proceedings of the First Workshop on Evalu-\nation and Comparison of NLP Systems, 11–20. Association\nfor Computational Linguistics.\nVasilyev, O.; Dharnidharka, V .; Egan, N.; Chambliss, C.;\nand Bohannon, J. 2020. Sensitivity of BLANC to\nhuman-scored qualities of text summaries. arXiv preprint,\narXiv:2010.06716.\nVedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015.\nCIDEr: Consensus-based image description evaluation. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 4566–4575.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019. XLNet: Generalized Autoregres-\nsive Pretraining for Language Understanding. In Wallach,\nH.; Larochelle, H.; Beygelzimer, A.; d'Alch ´e-Buc, F.; Fox,\nE.; and Garnett, R., eds., Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2020. BERTScore: Evaluating Text Generation with\nBERT. In International Conference on Learning Represen-\ntations.\nZhao, W.; Peyrard, M.; Liu, F.; Gao, Y .; Meyer, C. M.;\nand Eger, S. 2019. MoverScore: Text Generation Evaluat-\ning with Contextualized Embeddings and Earth Mover Dis-\ntance. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 563–578. Hong Kong, China: Associa-\ntion for Computational Linguistics.\nZhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning Books and\nMovies: Towards Story-Like Visual Explanations by Watch-\ning Movies and Reading Books. 2015 IEEE International\nConference on Computer Vision (ICCV), 19–27.\n10607",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.823662281036377
    },
    {
      "name": "Judgement",
      "score": 0.687799334526062
    },
    {
      "name": "Fluency",
      "score": 0.6315006613731384
    },
    {
      "name": "Language model",
      "score": 0.5525799989700317
    },
    {
      "name": "Transformer",
      "score": 0.507028341293335
    },
    {
      "name": "Relevance (law)",
      "score": 0.5062066912651062
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5034324526786804
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.49475377798080444
    },
    {
      "name": "Natural language processing",
      "score": 0.4929196834564209
    },
    {
      "name": "Machine learning",
      "score": 0.4406782388687134
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.43427759408950806
    },
    {
      "name": "Mutual information",
      "score": 0.4321434497833252
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.42376774549484253
    },
    {
      "name": "Human language",
      "score": 0.4155682921409607
    },
    {
      "name": "Correlation",
      "score": 0.41461458802223206
    },
    {
      "name": "Information retrieval",
      "score": 0.3907661736011505
    },
    {
      "name": "Data mining",
      "score": 0.3446540832519531
    },
    {
      "name": "Statistics",
      "score": 0.15565654635429382
    },
    {
      "name": "Linguistics",
      "score": 0.09196138381958008
    },
    {
      "name": "Mathematics",
      "score": 0.08048388361930847
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": []
}