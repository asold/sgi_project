{
  "title": "Evaluating self-triage accuracy of laypeople, symptom-assessment apps, and large language models: A framework for case vignette development using a representative design approach (RepVig)",
  "url": "https://openalex.org/W4393864575",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5052844168",
      "name": "Marvin Kopka",
      "affiliations": [
        "Technische Universität Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5016006269",
      "name": "Hendrik Napierala",
      "affiliations": [
        "Charité - Universitätsmedizin Berlin",
        "Freie Universität Berlin",
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5094357757",
      "name": "Martin Privoznik",
      "affiliations": [
        "Charité - Universitätsmedizin Berlin",
        "Freie Universität Berlin",
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5094357758",
      "name": "Desislava Sapunova",
      "affiliations": [
        "Technische Universität Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5113437009",
      "name": "Sizhuo Zhang",
      "affiliations": [
        "Technische Universität Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5049204500",
      "name": "Markus A. Feufel",
      "affiliations": [
        "Technische Universität Berlin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4296784194",
    "https://openalex.org/W4368282474",
    "https://openalex.org/W3211242631",
    "https://openalex.org/W3109148626",
    "https://openalex.org/W4225396572",
    "https://openalex.org/W4319021898",
    "https://openalex.org/W3109894209",
    "https://openalex.org/W4292133578",
    "https://openalex.org/W4364862404",
    "https://openalex.org/W3020992444",
    "https://openalex.org/W2530474886",
    "https://openalex.org/W4318765555",
    "https://openalex.org/W4210812846",
    "https://openalex.org/W3023828099",
    "https://openalex.org/W3041505649",
    "https://openalex.org/W3189986168",
    "https://openalex.org/W4226083953",
    "https://openalex.org/W4307493403",
    "https://openalex.org/W2940426683",
    "https://openalex.org/W2918492198",
    "https://openalex.org/W2128766176",
    "https://openalex.org/W2982083314",
    "https://openalex.org/W2024694940",
    "https://openalex.org/W3163053335",
    "https://openalex.org/W3119409876",
    "https://openalex.org/W3201202494",
    "https://openalex.org/W2004487508",
    "https://openalex.org/W2935524966",
    "https://openalex.org/W4378695763",
    "https://openalex.org/W2150234857",
    "https://openalex.org/W2119853015",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W2275737843",
    "https://openalex.org/W4393764690",
    "https://openalex.org/W4393656763",
    "https://openalex.org/W4386026470",
    "https://openalex.org/W4391558761",
    "https://openalex.org/W4281395694",
    "https://openalex.org/W3112185662",
    "https://openalex.org/W3128487207",
    "https://openalex.org/W3126060205",
    "https://openalex.org/W4381737810",
    "https://openalex.org/W4388237833",
    "https://openalex.org/W2155243985",
    "https://openalex.org/W4247280291",
    "https://openalex.org/W3144543375",
    "https://openalex.org/W4292054212",
    "https://openalex.org/W4281872984",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4220747294"
  ],
  "abstract": "Abstract Most studies evaluating symptom-assessment applications (SAAs) rely on a common set of case vignettes that are authored by clinicians and devoid of context, which may be representative of clinical settings but not of situations where patients use SAAs. Assuming the use case of self-triage, we used representative design principles to sample case vignettes from online platforms where patients describe their symptoms to obtain professional advice and compared triage performance of laypeople, SAAs, and Large Language Models (LLMs) on representative versus standard vignettes. We found performance differences in all three groups depending on vignette type (OR = 1.27 to 3.41, p &lt; .001 to .035) and changed rankings of best-performing SAAs and LLMs. Based on these results, we argue that our representative vignette sampling approach (that we call the RepVig Framework) should replace the practice of using a fixed vignette set as standard for SAA evaluation studies.",
  "full_text": "Evaluating self-triage accuracy of laypeople, symptom-\nassessment apps, and large language models: A framework for \ncase vignette development using a representative design \napproach (RepVig) \nMarvin Kopka1*, Hendrik Napierala2, Martin Privoznik3, Desislava Sapunova1, \nSizhuo Zhang1, and Markus A. Feufel1 \n1Division of Ergonomics, Department of Psychology and Ergonomics (IPA), Technische Universität \nBerlin, Berlin, Germany \n2Institute of General Practice and Family Medicine, Charité – Universitätsmedizin Berlin, corporate \nmember of Freie Universität Berlin and Humboldt-Universität zu Berlin, Berlin, Germany \n3Emergency and Acute Medicine and Health Services Research in Emergency Medicine, Charité – \nUniversitätsmedizin Berlin, corporate member of Freie Universität Berlin and Humboldt-Universität zu \nBerlin, Berlin, Germany \n \n* Correspondence: marvin.kopka@tu-berlin.de \n \nAbstract \nMost studies evaluating symptom-assessment applications (SAAs) rely on a common set of case vignettes \nthat are authored by clinicians and devoid of context, which may be representative of clinical settings but \nnot of situations where patients use SAAs. Assuming the use case of self-triage, we used representative \ndesign principles to sample case vignettes from online platforms where patients describe their symptoms \nto obtain professional advice and compared triage performance of laypeople, SAAs, and Large Language \nModels (LLMs) on representative versus standard vignettes. We found performance differences in all \nthree groups depending on vignette type (OR = 1.27 to 3.41, p < .001 to .035) and changed rankings of \nbest-performing SAAs and LLMs. Based on these results, we argue that our representative vignette \nsampling approach (that we call the RepVig Framework) should replace the practice of using a fixed \nvignette set as standard for SAA evaluation studies.  \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nIntroduction  \nSymptom-assessment applications (SAAs) are digital health tools that assist medical laypeople in self-\ndiagnosing and determining whether and where to seek health care (self-triage)1,2. Whereas some research \nin this domain focuses on how SAAs impact health systems or on how to improve their usability and user \nexperience2–7, a significant portion of studies investigates the accuracy of SAAs8,9. This line of research \nstarted with Semigran et al. in 2015,10 who developed 45 case vignettes to systematically test and \ncompare the accuracy of SAAs. These vignettes were developed by clinicians, drawing from a variety of \nmedical resources including textbooks for medical education. Subsequently, numerous studies have \nadopted their methodology and/or vignettes. Some authors have used the same set of vignettes,\n11–13 \nwhereas others have developed their own set, either building on the original vignettes or employing a \nsimilar approach to create them.14–16 Over time, these case vignettes have faced various criticisms: in \naddition to procedural criticism (e.g., that it matters who inputs vignettes into SAAs and that interrater \nreliabilities should be calculated17), most criticism refers to the content of the case vignettes (e.g., the \nsymptoms and symptom clusters covered in the cases)8 or to how they were created (e.g., the vignettes are \nusually created by clinicians, who may describe symptoms differently than patients, and are often \nfictitious rather than based on real cases)18. \nTo mitigate these issues, SAAs were tested with cases that more closely resemble real-world situations \nand actual patients. For instance, Yu et al.19 and Berry et al.20 utilized patient data from Emergency \nDepartment (ED) presentations for their vignettes. This method enhances external validity – understood \nas the applicability of findings to real-world scenarios21 – yet its representativeness is still limited with \nrespect to the primary purpose of SAAs: aiding medical laypeople experiencing acute symptoms deciding \nif or where to seek care22. That is, these vignettes include problems of people who chose to visit an ED, a \ngroup that may not fully represent the wider array of SAA users who (rightly or wrongly) decided against \ngoing to the ED in the first place. Additionally, the case development was biased, as clinicians re-created \nthese vignettes based on their documentation, who may have inadvertently filtered information or phrased \nthe vignettes differently than laypeople using an SAA would have done\n18. Assuming the use case of self-\ntriage, previous studies thus did not yet generate a representative set of vignettes. Results of SAA audit \nstudies that are based on such stimuli might not be generalizable to laypeople’s self-triage with SAA in \nthe real world. \nThe conceptual challenge of generating a representative set of stimuli was recognized in decision-making \nresearch decades ago, particularly in the field of ecological psychology rooted in Egon Brunswik's \nprobabilistic functionalism\n23. In his framework, decision-makers infer unobservable conditions (“latent \nvariables” or a triage level) from a set of observable information (“cues” or symptoms) based on \ncharacteristic correlations between certain cues and the latent variable (see Dhami et al.\n21 for an \noverview). Figure 1 implements these considerations for self-triage decisions.  \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nFigure 1. Self-triage lens model. \n \n \nBased on these considerations, Brunswik coined the term ecological validity and proposed the concept of \nrepresentative design as a method for sampling ecologically valid stimuli. Using representative design, \nstimuli (or vignettes) are sampled directly from situations to which the researcher would like to \ngeneralize, thereby trying to maintain the natural correlations between cues and the latent variable23. This \napproach contrasts starkly with the traditional systematic design, where parameters of the stimuli are \nAppropriat e\nSel f -T r iag e  \nLe ve l\nCues De cis ion-Mak e r C ue Util izati on Sel f -T r i a g e  L e ve l \nDe c isi on\nCue\n1\nCu e\n2\nCue\n3\n…\n#1\n#2\n#3\nThe s e lf -triage  le ve l\nth at is m ost\na ppr opria t e. It is n ot \ndi rectl y ob ser v a ble.\nAll i nf or m at ion on th e\nca s e th at a r e\nob ser v able . The se cues\nare re pres ent a t iv e a nd \nh ave na tu ra l\ni nte rc o r re l at i o ns .\nTh is in f o rm at ion is\nob ser v ed b y th e\ndecision - m ak er .\nTh e d ecision-ma k er\nutil izies t he c u es. The y\ncan use d iff er en t \ns tr a te g i es (e.g., onl y\nuse one cue or weigh t\nth e cues).\nBas ed on  th e cue\nu tiliz a tion , th e\nd ecision-m ak er arriv es\nat t h e t r i ag e l eve l th ey\nth ink is mos t\napp ropria t e.\nNo\nSelf -T riag e \nLe ve l\nCues De cis ion-Mak e r Cue Utili zati o n Self -T r iag e L ev el  \nDe ci sion\nCu e\nA\nCu e\nB\nCu e\nC\n…\nA\nB\nC\nSince t he c ase i s\nde r i v ed f ro m\nt e xt book s an d ot her\nresou rces, t her e is no\nse lf - t riage lev el in \nrea lity .\nThe  de sc rib ed cue s\nar e p re f i lt ere d from\nclinicia ns an d t her e\nare no n at ur al\ncor r ela tion s be twe en\nthe c u es.\nTh e de c i sion-ma k e r\nus es th ese bia s e d cue s\nan d in t erre la tion s t o\nma ke a  de c i s i o n.  \nH o w eve r , t he c u e s do  \nnot prov ide re al \ninf orma ti o n .\nB ased on t he c u e\nut iliz at ion, the decision -\nm ak er de c id es on  a \nt r ia ge le v el. This d ecision\nca nnot be gene ral iz ed t o\nt he e nvi r o nm e nt o f\ni nte re s t.\nR epr es en ta tive Vigne tt e s\nT r aditional Vi g ne t t es\n?\nThis  inf orma ti on is\nobse r v ed b y the\nd ec i sion-ma k e r .\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nsystematically varied, which according to representative design may lead to biased cues and \nintercorrelations21. For example, a higher age might be associated with a higher risk for certain diseases, \nand systematically varying age tends to eliminate these associations. As a result, decision-makers might \nmake different decisions when given systematically designed stimuli. Conversely, stimuli derived based \non representative design help researchers induce and evaluate decision-making behavior that more readily \ngeneralizes to the environment of interest, in our case, from SAA audit studies to real-world self-triage \nperformance of medical laypeople. \nThe power of representative design has been demonstrated in various fields such as pharmacy\n24, social \npsychology25, cognitive science26, human resources27, and public health28, showing that predictive power \nof results from experiments increases and these results better resemble real-world performance. Studies \nthat compared representative stimuli with non-representative stimuli found significant differences \nbetween results and conclusions\n25,27,29,30. For example, ecologically valid stimuli tended to make \nphenomena identified with non-representative stimuli disappear or reverse the direction of effects25 and \nbetter predicted performance in field studies27,29.  \nTo our knowledge, however, the representative design framework has thus far not been applied to study \nself-triage decisions and evaluate SAA performance. Our paper aims to bridge this research gap. Building \non an application of the representative design approach to the context of self-triage decisions, we explore \nhow a framework to generate representative vignettes (that we called the RepVig Framework) can be used \nto effectively generate new vignettes to assess self-triage capabilities. We compare results obtained from \nthe vignettes we developed using the RepVig Framework with those developed with traditional \napproaches and test whether these results differ. We hypothesize that these two methods of vignette \ndevelopment will yield different results. \n \nMethods \nEthical Considerations \nThis study was carried out in accordance with the declaration of Helsinki. Ethical approval was granted \nby the ethics committee of the Department of Psychology and Ergonomics (IPA) at Technische \nUniversität Berlin (tracking number: AWB_KOP_2_230711). The study was preregistered in the WHO-\naccredited German Clinical Trials Register (ID: DRKS00032895). The methods and results are reported \nconforming to the STROBE reporting guideline\n33. \n \nStudy Design \nThis study was designed as a prospective observational study examining the (self-)triage performance of \nmedical laypeople, SAAs and Large Language Models (LLMs) based on two kinds of vignettes. We used \na set of existing vignettes that have been used to evaluate triage performance of laypeople and SAAs in \nprevious studies34,35. In addition, we aimed to develop a new vignette set following a representative \ndesign approach23 and the RepVig Framework, aiming for greater external validity in generalizing to self-\ntriage decisions compared to prior studies. We investigated whether vignettes obtained using the RepVig \nframework yield different triage performance estimates compared to the traditional vignette sets. We \ngathered urgency assessments from medical laypeople and entered the vignettes in various SAAs and \nLLMs to determine standard metrics for triage-performance reporting\n36,37.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nVignette Sets \nBrunswik23 emphasized the importance of sampling stimuli (vignettes in this case) from a population of \nstimuli from the reference class. To gather such symptom descriptions from individuals who are (1) self-\ndescribing their symptoms and (2) in the process of deciding if and where to seek healthcare, we utilized \nthe social media network Reddit, specifically the subreddit 'r/AskDocs'. This forum allows people to post \nmedical questions, which are then addressed by verified physicians on a voluntary basis. For our research, \nwe used Reddit’s API to extract all new posts within a 14-day period, from June 16th to June 29th 2023.  \nRecognizing that the symptom distribution in these cases might differ from those typically entered into \nSAAs, we applied Brunswik’s second approach to achieving representative design, known as \n'canvassing'21. To this end, we established quotas based on symptom clusters commonly input into SAAs. \nWe used data from Arellano Carmona et al.38 who examined symptom clusters based on the CDC’s \nNational Ambulatory Medical Care Survey (and corresponding triage advice) that users entered into an \nSAA, and constructed these quotas to ensure that our vignette set is representative not only in terms of \nreal-world symptom descriptions but also with respect to the symptom distributions that users tend to \nenter into SAAs. These quotas can be found in Table 1. \nTable 1. Quotas used to sample vignettes that are representative of symptom clusters typically entered in \nSAA based on Arellano Carmona et al.\n38. \nSymptom Cluster # of vignettes \n(%) \nMusculoskeletal Pain 5 (11%) \nJoint Pain 3 (7%) \nHeadache 1 (2%) \nChest Pain 1 (2%) \nOther Pain 5 (11%) \nGynecological 5 (11%) \nTumors/lumps/masses 4 (9%) \nEdema 2 (4%) \nSkin issues 2 (4%) \nGastrointestinal 2 (4%) \nImpaired sensations 3 (7%) \nUrinary Tract Problems 3 (7%) \nUpper Respiratory Symptoms 1 (2%) \nOther 8 (18%) \nTotal Number 45 (100%) \n \nTo ensure that we include vignettes by people who are experiencing a situation in which they would \nconsult an SAA (i.e., that they experienced new symptoms and sought advice on what to do), we applied \nthe following exclusion criteria, omitting vignettes where authors: \n- sought general information only, \n- provided a picture to be understandable, as these are not typical SAA inputs, \n- provided overly specific information (e.g., blood values that are typically not known to laypeople), \n- discussed symptoms for which the individual had already consulted a medical professional, \n- provided insufficient details (e.g., a single sentence stating foot pain) for self-triage, \n- described symptoms experienced by someone else (with the exception of infants), \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \n- did not describe acute symptoms (e.g., past symptoms that have already resolved) \n- described symptoms already diagnosed by a medical professional \n- exceeded 1,000 characters (which may overwhelm participants).  \nDuring the 14-day sampling period, this left us with 8,794 vignettes. Using a selection method based on R \nand Shiny Dashboard, we randomly drew one vignette at a time and manually either included it in the \nrelevant quota or excluded it based on our predefined exclusion criteria. When a specific quota was filled, \nany new vignettes in that category were excluded. We continued this procedure until all quotas were \nfilled. Once we compiled our vignette set, we made minor adjustments, such as correcting typographical \nerrors and standardizing units of measurement (e.g., adding “kg” to vignettes that originally reported \nweights in “lbs”). We did not edit the vignettes in any other way, because the principles of representative \ndesign require stimuli to be as close to the actual decision environment as possible. Any modification \ncould unpredictably affect the cues and their correlations to triage decisions and thus the \nrepresentativeness of the vignettes. Using this sampling method, the RepVig Framework, we compiled a \nfinal set of 45 vignettes to construct a set that is comparable in size to previous studies\n10,14.  \nIn alignment with established best practices and previous SAA research14,16,39, two licensed physicians \nassessed the appropriate triage level of each vignette independently. After rating all vignettes, the \nphysicians discussed their answers in cases of disagreement to reach consensus on the appropriate triage \nlevel, thereby finalizing the solutions for this set. This approach guarantees that the triage levels assigned \nto each vignette are not only based on expert medical opinion but also on a harmonized understanding \nbetween the two assessing physicians\n14,16,39. \nFor the evaluation study, we compared data collected with our novel sampling approach to data collected \nwith vignettes developed by Semigran et al.10. This dataset was selected for comparison, because it is \nopenly accessible and has been used to evaluate the capabilities of both medical laypeople and SAAs. The \ndataset focusing on laypeople’s capability34 was published in 2021 and comprises data from 91 \nparticipants, each of whom rated all 45 vignettes developed by Semigran et al. The dataset assessing SAA \naccuracy35 replicated an earlier accuracy study by utilizing the same set of vignettes and SAA selection \ncriteria. The authors tested 22 different SAAs using the same set of 45 vignettes from Semigran et al. that \nwere used in the study examining laypeople’s capabilities.  \n \nData Collection  \nTo compare triage performance with both vignette sets, we collected data from three different groups: \nMedical laypeople, SAAs and LLMs. To obtain data from laypeople, we used Prolific, an online panel \nprovider known for its high-quality data\n40. We drew a random sample from this platform, including \nparticipants from the United States (to ensure comparability with our comparator dataset) and excluding \nparticipants that were health professionals. Based on our comparator dataset\n41, we aimed to get about \n4,000 ratings to arrive at a similar number of ratings as in the comparator dataset (4,095 ratings) to ensure \ncomparability and adequate statistical power to detect effects of similar size under similar conditions. To \nensure attentive participation, each person was assigned a random sample of 20 out of the 45 vignettes. \nThus, we collected data from 202 participants between the October 18\nth and 19th 2023 using an online \nsurvey designed with Unipark (Questback)42. Based on Levine et al.’s easy-to-comprehend and laypeople-\nfriendly phrasing of different urgency levels12, participants were asked to choose whether the most \nappropriate level for each vignette is emergency care, urgent care, non-emergent care or self-care. To \nfurther motivate patients, we offered a bonus of 0.02$ for each correctly solved vignette, in addition to a \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nbase remuneration of 1$ for 10 minutes. Additionally, we embedded an attention check in two fictitious \nvignettes, instructing participants to select a specific response. Data from participants who failed to pass \nat least one attention check were excluded from the analysis. \nFollowing best practices to reduce effects from inputter variability, data from SAAs were collected by \ntwo graduate students (DS and SZ) – without professional medical training – who entered the cases into \ndifferent SAAs between October 24\nth 2023 and January 22nd 2024. Having students enter the vignettes \nrather than health professionals ensured that only information stated in the vignettes were entered without \nsubjective clinical interpretation of symptoms, which could introduce bias\n17. The two students entered \neach vignette into 12 different SAAs (amounting to 540 assessments). The selection of SAAs was based \non our comparator dataset13 to ensure comparability. The original study from which this dataset is derived \nhad systematically identified all publicly available SAAs. For our study, we had to exclude certain SAAs \nbecause four were no longer available, two discontinued providing triage advice, two had shifted to a \nsubscription model, and two now give specialized advice for pediatric cases. A comprehensive list of the \nremaining 12 SAAs is provided in the Appendix. Inputters were blinded to the outcome (i.e. the correct \nsolution) and recoded the triage levels as provided by the SAAs. These results were then reviewed by the \nlead author (MK). In instances where the inputters came up with different results, we considered a \nvignette correctly solved if at least one of them had recorded the correct solution. This criterion was based \non the rationale that if one inputter could derive the correct solution, the SAA was technically capable of \nproviding an accurate triage assessment\n43. Analyses for each inputter separately can be found in the \nAppendix. \nData from LLMs were collected by one person only between November 28th and 29th 2023, as we used a \npredefined, one-shot prompt for all LLMs to circumvent input variation. We collected data for both the \nvignettes from our comparator dataset and those developed in this study. The prompt we used was \nadapted from two previous studies12,44 and stated  \n“Please give a triage classification for the following vignette. The criteria for triage \nclassifications are presented below: Emergency: Call 911 or go directly to the \nemergency room; Non-Emergency: Try to see a doctor, but the symptoms do not require \nimmediate attention at an emergency room. For example, you can see a doctor in the \nnext week. Self-care: Let the health issue get better on its own and review the situation \nin a few days again.”.  \nWe tested all LLMs that we could identify through a systematic search (entering combinations of \nthe words “large language model”, “LLM”, “chatbot”, “GPT”, “text generation” and “BERT” into search \nengines) and that offered a web interface. In total, we evaluated five LLMs. The list of LLMs can be \nfound in the Appendix. \n \nData Analysis \nTo align our study with the comparator datasets, which did not include 'urgent care' as a distinct solution, \nwe recoded all 'urgent care' responses to 'non-emergency care'. This adjustment ensures consistency in our \nanalysis, as 'urgent care' was originally classified as 'non-emergency care' in the comparator datasets. \nWe then proceeded to compare the entire set of vignettes obtained through our novel sampling approach \nwith the vignette set commonly used in previous studies assessing SAA accuracy10. For the analysis \ninvolving medical laypeople, SAAs and LLMs, we employed both descriptive and inferential statistical \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nmethods. To examine the differences between our newly sampled vignette set and the traditional set used \nfor laypeople and SAAs, we used a mixed-effects binomial logistic regression. In these models, \nparticipants (or SAAs or LLMs) were treated as a random effect, while the vignette sets were considered a \nfixed effect.  \nOur study's outcome measures were aligned with established reporting guidelines\n36 and included overall \naccuracy, accuracy for each self-triage level separately, the safety of advice (calculated as the percentage \nof emergencies identified), the comprehensiveness (how many of the total number of vignettes received a \nsolution), and the inclination to overtriage (the percentage of overtriage errors among all errors). Since \nnot all SAAs gave advice for each vignette, we calculated a Capability Comparison Score (CCS, as \nelaborated in Kopka et al.\n36). This metric adjusts the accuracy scores to reflect the varying difficulty \nlevels of vignettes that different SAAs were able to address, allowing for a more equitable comparison of \ncapability across SAAs that may not have provided solutions for all cases. For assessing the interrater \nagreement on SAA data, we used two-way mixed, agreement, average-measures intra-class correlation \n(ICC), as solutions were coded as ordinal\n45. Values above .40 were considered acceptable46.  \n \nResults  \nVignette Sampling \nThe vignette sampling process is displayed in Figure 2 based on the PRISMA reporting standard47. Over a \n14-day period, we identified 8,794 posts, of which 858 were duplicates. After removing cases that were \ntoo long, we arrived at a final set of 5,388 posts. We randomly sampled from these posts until all quotas \nwere filled, reviewing 526 vignettes in total. Out of these, 423 were excluded based on the exclusion \ncriteria and 58 because quota limits were reached.   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nFigure 2. PRISMA chart with number of case vignettes identified, screened, excluded and included.  \n \nParticipants \nOut of the 33,736 eligible participants on the Prolific platform, 204 participants started the survey and 202 \ncompleted it. Data from four participants were excluded, because they failed at least one attention check. \nThis resulted in a final sample size of 198 participants with 20 assessments each and a total number of \n3,960 vignette assessments. Descriptions of participants’ characteristics can be found in Table 2.  \nTable 2. Description of participants. N = 198, M = Mean, SD = Standard Deviation, n = number.   \nCharacteristic Result \nAge, M (SD) 40.70 (13.85) \nGender, n (%)  \n  Male 97 (48.99%) \n  Female 98 (49.49%) \n  Other 3 (1.52%) \nEducation, n (%)  \n  Less than high school diploma 2 (1.01%) \n  High school graduate, GED, or alternative 29 (14.46%) \n  Some college or Associate degree 61 (30.81%) \n  Bachelor degree 73 (36.87%) \n  Graduate degree or higher 33 (16.67%) \nMedical training, n (%)  \nNo training at all 165 (83.33%) \nBasic first aid training 33 (16.67%) \neHealth Literacy (quantified by the eHealth Literacy Scale), M (SD) 30.4 (4.37) \n \nCa ses i den tifie d: 87 9 4 \nCase s re mov ed  be f ore scr eeni ng:\n• D upl ica te  cas es re mov ed  (n = 85 8 )\n Ca ses ex c e edi ng  1000  c h ar act ers  rem ov ed ( n  = 2548)\nCases  scree ned (ran doml y \ndr awn ) :  526 \nCa ses in clude d: 45\nCases ex clude d:\n Ask ing f or  gene ra l in form at ion (n  = 99)\n Pictu re i s ne c e ssary (n =  86) \n I nf orma tion  pr ovid ed t oo sp ecific ( n  = 72)\n Al rea dy se en a m edica l pr of ession al (n =  63)\n Qu ot a lim it rea c h ed (n = 58)\n D id n ot p rov ide s ufficien t inf orma tio n (n =  46)\n S ym pt om s are  not  self -pe rceiv ed  (n = 27)\n S ym pt om s are  not  acut e ( n  = 26)\n S ym pt om s wer e al rea dy d iagnose d (n =  4)\nCases in d at a set : 5388 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nSelf-Triage Performance Metrics \nLaypeople’s Performance \nOn average, there was no performance difference for laypeople triaging vignettes sampled for this study \nand vignettes that were traditionally used (OR = 0.94, SE = 0.05, z = -1.40, p. = 0.16). However, \nlaypeople identified emergency cases more often using representative vignettes compared to traditional \nvignettes (OR = 1.77, SE = 0.21, z = 2.72, p = 0.006). The same pattern emerged for non-emergency \ncases (OR = 1.27, SE = 0.08, z = 2.94, p = 0.003). Conversely, laypeople identified self-care cases less \noften with representative than with traditional vignettes (OR = 0.59, SE = 0.10, z = -5.08, p < .001). As a \nresult, their judgement based on representative vignettes can be considered safer than based on traditional \nvignettes (OR = 1.84, SE = 0.08, z = 7.38, p < 0.001). Overall, laypeople were more inclined to overtriage \nwith representative vignettes compare to traditional vignettes (OR = 2.10, SE = 0.10, z = 7.43, p < .001). \nA summary of these findings can be found in Table 3.  \n \nTable 3. Laypeople’s self-triage performance with representative versus traditional vignettes, M = mean, \nSD = standard deviation.   \nMetric Representative \nVignettes \nTraditional \nVignettes \nOR p Value \nAverage Accuracy, M (SD) 62.4 \n(10.80) 60.9 (6.81) 0.94 .16 \nEmergency Accuracy, M (SD) 78.6 (37.6) 67.5 (16.4) 1.77 .006 \nNon-Emergency Accuracy, M (SD) 73.2 (13.0) 68.4 (13.8) 1.27 .003 \nSelf-Care Accuracy, M (SD) 34.6 (24.4) 46.7 (15.9) 0.59 < .001 \nSafety of Advice, M (SD) 90.7 (7.9) 84.2 (8.2) 1.84 < .001 \nInclination to overtriage, M (SD) 74.6 (20.4) 60.1 (17.6) 2.10 < .001 \n \nSAA Performance \nAgreement between both SAA inputters was acceptable (ICC = 0.605). Across all SAAs, triage accuracy \nwas significantly higher for representative vignettes compared to traditional vignettes (OR = 2.00, SE = \n0.139, z = 5.68, p < .001). In detecting emergencies, we could not find a statistically significant difference \n(OR = 2.26, SE = 0.53, z = 1.52, p = 0.13), but SAAs detected non-emergency cases (OR = 2.38, SE = \n0.236, z = 3.70, p < .001) and self-care cases more often (OR = 2.53, SE = 0.292, z = 3.18, p = .0015) in \nrepresentative vignettes compared to traditional vignettes. The safety of advice was higher with \nrepresentative vignettes for one inputter (OR = 1.81, SE = 0.224, z = 2.64, p = .008), and we found a \nsimilar but non-significant trend for the second inputter (OR = 1.42, SE = 0.206, z = 1.70, p = .09). With \nrepresentative vignettes, the inclination to overtriage was higher for one inputter (OR = 1.80, SE = 0.277, \nz = 2.112, p = .035) and lower for the other inputter (OR = 0.53, SE = 0.277, z = -2.30, p = .022). See \nTable 4 for a summary. \n \nTable 4. SAA’s self-triage performance with representative versus traditional vignettes, M = mean, SD = \nstandard deviation.   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nMetric Representative \nVignettes \nTraditional \nVignettes \nOR p Value \nAverage Accuracy, M (SD) 67.8 (46.8) 48.9 (50.0) 2.00 < .001 \nEmergency Accuracy, M (SD) 75.0 (44.2) 54.4 (49.9) 2.26 .013 \nNon-Emergency Accuracy, M (SD) 76.4 (42.5) 60.6 (49.0) 2.38 < .001 \nSelf-Care Accuracy, M (SD) 46.8 (50.1) 31.7 (46.6) 2.53 .002 \nSafety of Advice*, M (SD) 83.7 (11.1)  \n   92.2 (5.3) 85.6 (8.1) 1.81 / 1.42 .008 / .09 \nInclination to overtriage*, M (SD) 52.5 (29.0)  \n74.0 (17.6) 56.6 (23.8) 1.80 / 0.53 .035 / .022 \n* Since no aggregated statistic can be calculated for this metric, values for both inputters are reported. \n \nThe average item difficulty for representative vignettes among SAAs was higher (M = 0.68, SD = 0.21) \nthan for the traditional vignettes (M = 0.49, SD = 0.26). Performance comparisons with CCS values and \nranks are summarized in Table 5. Detailed metrics for every individual SAA can be found in the \nAppendix.  \n \nTable 5. Capability comparison scores (CCS) and ranks for SAAs in both vignette sets \nSymptom-\nAssessment \nApplication \nCapability \nComparison \nScore in \nrepresentative \nvignette set \nCapability \nComparison \nScore in \ntraditional \nvignette set \nRank in \nrepresentative \nvignette set \nRank in \ntraditional \nvignette \nset \nHealthwise 56.1 52.2 1 5 \nNHS 111 56.1 47.8 1 8 \nEveryday Health 55.0 42.2 3 11 \nSymptomate 55.0 42.2 3 11 \nHealthdirect 53.9 46.7 5 9 \nAda 52.8 57.8 6 2 \nIsabel 50.6 53.3 7 3 \nDrugs.com 50.6 58.9 7 1 \nNHS 47.2 45.6 9 10 \nFamily Doctor 41.7 41.1 10 6 \nSymptify 41.7 53.3 10 3 \nHealthily  39.4 48.9 12 7 \n \nLLM Performance \nAcross all LLMs, triage accuracy was significantly higher for representative vignettes compared to \ntraditional vignettes (OR = 1.52, SE = 0.196, z = 2.14, p = .03). In detecting emergencies, we could not \nfind a statistically significant difference (OR = 0.37, SE = 0.80, z =  -1.25, p = .21), but LLMs detected \nnon-emergency cases more often (OR = 3.01, SE = 0.00, z = 358.9, p < .001) and self-care cases less \noften (OR = 0.25, SE = 0.00, z = -449.8, p < .001) in representative compared to traditional vignettes. The \nsafety of advice from LLMs was higher with representative vignettes compared to traditional vignettes \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \n(OR = 3.41, SE = 0.39, z = 3.19, p = .001). Overall, LLMs were more likely to overtriage with \nrepresentative vignettes (OR = 2.66, SE = 0.43, z = 2.30, p = 0.022). A summary can be found in Table 6.  \n \nTable 6. LLM’s self-triage performance with representative versus traditional vignettes, M = mean, SD = \nstandard deviation.   \nMetric Representative \nVignettes \nTraditional \nVignettes \nOR p Value \nAverage Accuracy, M (SD) 67.6 (2.5) 57.8 (9.4) 1.52 .03 \nEmergency Accuracy, M (SD) 50.0 (35.4) 66.7 (36.5) 0.37 .21 \nNon-Emergency Accuracy, M (SD) 95.3 (3.8) 88.0 (17.9) 3.01 < .001 \nSelf-Care Accuracy, M (SD) 6.15 (10.0) 18.7 (17.3) 0.25 < .001 \nSafety of Advice, M (SD) 95.6 (3.5) 87.1 (14.5) 3.41 .001 \nInclination to overtriage, M (SD) 86.5 (11.0) 73.6 (23.5) 2.66 .022 \n \nBecause all LLMs provided solutions for all vignettes, all LLMs were tested with vignettes of the same \nitem difficulty. For representative vignettes the average item difficulty was M = 0.68 (SD = 0.42) and for \ntraditional vignettes the average item difficulty was M = 0.58 (SD = 0.34). CCS values and ranks for each \nLLM are summarized in Table 7.  \n \nTable 7. Capability comparison scores (CCS) and ranks for LLMs in both vignette sets \nLarge Language \nModel \nCapability \nComparison \nScore in \nrepresentative \nvignette set \nCapability \nComparison \nScore in \ntraditional \nvignette set \nRank in \nrepresentative \nvignette set \nRank in \ntraditional \nvignette \nset \nGPT-4 (ChatGPT) 51.8% 53.3% 1 1 \nLlama 2 50.7% 42.2% 2 5 \nPaLM 2 (Google \nBard) \n49.6% 48.9% 3 4 \nPi 49.6% 53.3% 4 2 \nClaude 2 48.4% 52.2% 5 3 \n \nDiscussion  \nIn this study, we aimed to extend Brunswik’s representative design principles to self-triage audit studies \nand used this approach to generate a new set of vignettes for studying self-triage performance of \nlaypeople, SAAs, and LLMs through a representative design approach and the RepVig Framework. An \nextensive pool of case descriptions was sampled in the target environment and provided a robust basis for \nrandomly selecting cases, which were then stratified by symptom clusters to obtain a final set of vignettes \nthat is representative also in terms of the distribution of cases. Then, we examined whether these vignettes \nresult in different performance patterns than traditional vignettes. That was the case for all agents we \ntested, that is, for laypeople, SAAs, and LLMs, which we discuss in turn.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nAlthough average accuracy of laypeople was similar for both vignette sets, they were more accurate for \nemergencies and non-emergencies, and less accurate for self-care cases. The safety of advice was also \nsignificantly higher (albeit with more overtriage errors, i.e., rating symptoms as more urgent than they \nare). From an ecological rationality perspective, these findings make sense: because individuals face no \nharm in calling a general practitioner but might face negative health outcomes if they experience \nsymptoms that require treatment but do not seek health care, they can be expected to be risk averse and \nseek care rather than stay at home. Unlike a previous study, which used traditional vignettes and reported \nthat laypeople have difficulty identifying emergencies\n48, our results based on representative vignettes  \nsuggest that laypeople identify emergencies more reliably and have difficulty identifying self-care cases. \nThus, from an applied perspective, decision support tools should focus on helping laypeople identify self-\ncare cases rather than emergencies. \nFor SAAs, we observed higher accuracy overall and for each triage level when using representative \nvignettes. However, we also found a slightly lower safety of advice compared to the traditional set, \nwhereas our findings regarding inclination to overtriage are inconclusive. When comparing different \nSAAs using the CCS, rankings varied significantly between traditional and representative vignettes. This \nhighlights the crucial role of the vignette sets in SAA testing. When selecting an SAA for general use in \nguiding patients through the healthcare system, these different vignettes would lead to varying choices.  \nIn LLMs, we observed significant differences across all estimates when comparing representative and \ntraditional vignette sets. The average accuracy was higher with representative vignettes, but the \ndistribution across different triage levels varied. Emergency and self-care cases were less frequently \nidentified correctly, while non-emergency cases were identified more often. The advice was also safer \nwith representative vignettes, although they led to more overtriage errors. The ranking of LLMs also \nchanged when using representative vignettes. \nOverall, we found that our results differed when using representative vignettes, aligning with our \nhypothesis and findings from several other empirical studies\n25,27,29,30. This suggests that employing a \nrepresentative design approach is valuable for evaluating laypeople’s self-triage performance with and \nwithout SAAs. Importantly, we do not claim that the sampling method presented in this article is or \nshould be the only and best way to evaluate SAAs. On the contrary, in line with the concept of \nrepresentative design, we suggest that each use case requires its specific set of stimuli (e.g., case vignettes \nsampled from different populations in specific situations). In this sense, also traditional vignettes can \nprovide valuable insights, for instance, to examine how SAAs might perform when being used by \nclinicians. Similarly, studying ED cases (as done by Fraser et al.\n49) could help determine if patients \nvisiting the ED might have opted for primary care had they consulted an SAA beforehand. With a sample \nof primary care cases, the potential effect of SAA on the redistribution of care seeking efforts could be \nassessed from a systems perspective.  \nTherefore, we suggest that future self-triage evaluation studies should specify to which particular decision \nsituation they wish to generalize and sample case vignettes from that situation instead of relying on one \nfixed set of vignettes independent of the use case at hand. This recommendation is especially relevant \nconsidering that LLMs (and perhaps SAAs in the future) – that are regularly updated with new data\n50 and \nexpected to significantly impact future healthcare51,52 – might be trained on published vignettes or could \nretrieve results for these vignettes through web searches. Thus, generalizability for LLMs is limited \nbecause published vignettes and their solutions can be part of the training set, and a standardized \napproach to vignette sampling (without a published solution) and continuously generating new vignettes \nmay be more beneficial than using a fixed set of vignettes. Interestingly, by using more specific vignettes, \nresearchers will ultimately provide more generalizable insights for clearly defined use cases.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \n \nThe approach presented in this article is particularly pertinent to achieving the general goal of SAAs, \nwhich is to enable people to assess symptoms without a clinician’s direct involvement. Beyond that use \ncase, we propose our RepVig Framework based on the representative design approach as a standard for \nsampling vignettes from situations to which one wishes to generalize. For regulators evaluating SAAs and \nother digital health technologies, our study emphasizes the importance of data quality. Testing decision \nsupport systems should be done both with relevant metrics\n36 and with reliable vignettes that allow \ngeneralizations beyond the testing scenario. Because using unrepresentative vignettes for system testing \ncan significantly influence the results – potentially leading to biased outcomes – only using representative \nvignettes and providing information on the vignettes that SAAs are tested with is particularly important. \nStakeholders should specify the population they want to generalize to and use corresponding vignettes for \nevaluations.  \nThis study has several limitations. First, we could not obtain additional information for these cases. \nAlthough this is also a common issue with traditional vignettes, having extra details available that SAAs \nmight inquire about would be beneficial. This way, inputters get more precise results because they are \nable to respond to SAA questions throughout the interaction. However, interviewing individuals in the \nsituation of experiencing the symptoms to get additional information poses significant challenges; it is \nparticularly unfeasible and unethical for those with urgent symptoms. Therefore, this trade-off between \nrealism and practical and ethical considerations we propose is likely the most acceptable balance \nachievable. A second limitation pertains to the results obtained from LLMs. These results are based on a \nspecific prompt, and real-world interaction might yield different results due to varying prompts, \ninteractions, and interpretation of LLM outputs. Additionally, LLM output is non-deterministic\n53, \nmeaning results could vary even if cases are entered using the same prompt again. Given rapid \nimprovement cycles, the accuracy of LLMs might change quickly and require ongoing tests. Lastly, the \nobserved disparities in the SAAs’ inclination to overtriage are likely due to inputter variability, as it has \nbeen previously shown to affect such estimates\n43. However, we followed standard practice in symptom \nchecker audit studies and this variance is present in similar studies as well. While technically all \ndifferences might stem from inputter variability, our results for laypeople and LLMs – in which inputter \nvariability is impossible – differ between these two vignette sets too. Therefore, we believe that our \nresults are not solely attributable to inputter variability.  \nIn summary, we propose refining vignettes in self-triage studies to enhance generalizability of the \nobserved performance patterns. To ground the vignette sampling process on a theoretical basis, we \nsuggest using representative design principles for investigating self-triage of laypeople (see Figure 1) and \nthe RepVig Framework as a general framework for sampling case vignettes and evaluating triage \nperformance in all kinds of use cases.  \n \nData availability \nThe data will be published in an open access repositorium upon acceptance.  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nReferences \n1. Napierala, H. et al. Examining the impact of a symptom assessment application on patient-\nphysician interaction among self-referred walk-in patients in the emergency department (AKUSYM): \nstudy protocol for a multi-center, randomized controlled, parallel-group superiority trial. Trials 23, 791 \n(2022). \n2. Kopka, M. et al. Characteristics of Users and Nonusers of Symptom Checkers in Germany: \nCross-Sectional Survey Study. J Med Internet Res 25, e46231 (2023). \n3. Aboueid, S., Meyer, S. B., Wallace, J. & Chaurasia, A. Latent classes associated with the \nintention to use a symptom checker for self-triage. PLoS ONE 16, e0259547 (2021). \n4. Aboueid, S., Meyer, S., Wallace, J. R., Mahajan, S. & Chaurasia, A. Young Adults’ Perspectives \non the Use of Symptom Checkers for Self-Triage and Self-Diagnosis: Qualitative Study. JMIR Public \nHealth Surveill 7, e22637 (2021). \n5. Kopka, M. et al. Determinants of Laypersons’ Trust in Medical Decision Aids: Randomized \nControlled Trial. JMIR Hum Factors 9, e35219 (2022). \n6. Gellert, G. A. et al. A multinational survey of patient utilization of and value conveyed through \nvirtual symptom triage and healthcare referral. Front. Public Health 10, 1047291 (2023). \n7. Morse, K. E., Ostberg, N. P., Jones, V. G. & Chan, A. S. Use Characteristics and Triage Acuity of \na Digital Symptom Checker in a Large Integrated Health System: Population-Based Descriptive Study. \nJournal of Medical Internet Research 22, e20549 (2020). \n8. Wallace, W. et al. The diagnostic and triage accuracy of digital and online symptom checker \ntools: a systematic review. npj Digit. Med. 5, 118 (2022). \n9. Riboli-Sasco, E. et al. Triage and Diagnostic Accuracy of Online Symptom Checkers: Systematic \nReview. J Med Internet Res 25, e43803 (2023). \n10. Semigran, H. L., Linder, J. A., Gidengil, C. & Mehrotra, A. Evaluation of Symptom Checkers for \nSelf Diagnosis and Triage: Audit Study. BMJ 351, 1–9 (2015). \n11. Semigran, H. L., Levine, D. M., Nundy, S. & Mehrotra, A. Comparison of Physician and \nComputer Diagnostic Accuracy. JAMA Internal Medicine 176, 1860–1861 (2016). \n12. Levine, D. M. et al. The Diagnostic and Triage Accuracy of the GPT-3 Artificial Intelligence \nModel. http://medrxiv.org/lookup/doi/10.1101/2023.01.30.23285067 (2023) \ndoi:10.1101/2023.01.30.23285067. \n13. Schmieding, M. L. et al. Triage Accuracy of Symptom Checker Apps: 5-Year Follow-up \nEvaluation. J Med Internet Res 24, e31810 (2022). \n14. Hill, M. G., Sim, M. & Mills, B. The Quality of Diagnosis and Triage Advice Provided by Free \nOnline Symptom Checkers and Apps in Australia. Med J Aust 212, 514–519 (2020). \n15. Ceney, A. et al. Accuracy of online symptom checkers and the potential impact on service \nu\ntilisation. medRxiv 2020.07.07.20147975 (2020) doi:10.1101/2020.07.07.20147975. \n16. Delshad, S., Dontaraju, V. S. & Chengat, V. Artificial Intelligence-Based Application Provides \nAccurate Medical Triage Advice When Compared to Consensus Decisions of Healthcare Providers. \nCureus (2021) doi:10.7759/cureus.16956. \n17. El-Osta, A. et al. What is the suitability of clinical vignettes in benchmarking the performance of \nonline symptom checkers? An audit study. BMJ Open 12, e053566 (2022). \n18. Painter, A., Hayhoe, B., Riboli-Sasco, E. & El-Osta, A. Online Symptom Checkers: \nRecommendations for a Vignette-Based Clinical Evaluation Standard. J Med Internet Res 24, e37408 \n(2022). \n19. Yu, S. W. Y. et al. Triage Accuracy of Online Symptom Checkers for Accident and Emergency \nDepartment Patients. Hong Kong Journal of Emergency Medicine 27, 217–222 (2020). \n20. Berry, A. C. et al. Online symptom checker diagnostic and triage accuracy for HIV and hepatitis \nC. Epidemiol. Infect. 147, e104 (2019). \n21. Dhami, M. K., Hertwig, R. & Hoffrage, U. The Role of Representative Design in an Ecological \nApproach to Cognition. Psychological Bulletin 130, 959–988 (2004). \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \n22. Meyer, A. N. D. et al. Patient Perspectives on the Usefulness of an Artificial Intelligence-\nAssisted Symptom Checker: Cross-Sectional Survey Study. J. Med. Internet Res. 22, e14679 (2020). \n23. Brunswik, E. Representative design and probabilistic theory in a functional psychology. \nPsychological Review 62, 193–217 (1955). \n24. Waghorn, J. et al. Clinical Judgement Analysis: An innovative approach to explore the individual \ndecision-making processes of pharmacists. Research in Social and Administrative Pharmacy 17, 2097–\n2107 (2021). \n25. Cesario, J. What can experimental studies of bias tell us about real-world group disparities? \nBehav Brain Sci 45, e66 (2022). \n26. Richter, B. & Hütter, M. Learning of affective meaning: revealing effects of stimulus pairing and \nstimulus exposure. Cognition and Emotion 35, 1588–1606 (2021). \n27. Fritzsche, B. A. & Brannick, M. T. The importance of representative design in judgment tasks: \nThe case of résumé screening. J Occupat & Organ Psyc 75, 163–169 (2002). \n28. Miller, L. C., Wang, L., Jeong, D. C. & Gillig, T. K. Bringing the Real World into the \nExperimental Lab: Technology-Enabling Transformative Designs. in Social-Behavioral Modeling for \nComplex Systems 359–386 (John Wiley & Sons, Ltd, 2019). doi:10.1002/9781119485001.ch16. \n29. Wang, X. & Navarro-Martinez, D. Bridging the gap between the economics lab and the field: \nDictator games and donations. Judgm. decis. mak. 18, e18 (2023). \n30. Gigerenzer, G., Hoffrage, U. & Kleinbölting, H. Probabilistic mental models: A Brunswikian \ntheory of confidence. Psychological Review 98, 506–528 (1991). \n31. Karelaia, N. & Hogarth, R. M. Determinants of linear judgment: A meta-analysis of lens model \nstudies. Psychological Bulletin 134, 404–426 (2008). \n32. Ayers, J. W. et al. Comparing Physician and Artificial Intelligence Chatbot Responses to Patient \nQuestions Posted to a Public Social Media Forum. JAMA Intern Med (2023) \ndoi:10.1001/jamainternmed.2023.1838. \n33. Von Elm, E. et al. The Strengthening the Reporting of Observational Studies in Epidemiology \n(STROBE) statement: guidelines for reporting observational studies. The Lancet 370, 1453–1457 (2007). \n34. Schmieding, M. L., Mörgeli, R., Schmieding, M. A. L., Feufel, M. A. & Balzer, F. Data set \nsupplementing ‘Benchmarking triage capability of symptom checkers against that of medical laypersons: \nSurvey study’. Zenodo https://doi.org/10.5281/ZENODO.4454537 (2021). \n35. Schmieding, M. L. et al. Data Set on Accuracy of Symptom Checker Apps in 2020. Zenodo \nhttps://doi.org/10.5281/ZENODO.6054092 (2022). \n36. Kopka, M., Feufel, M. A., Berner, E. S. & Schmieding, M. L. How suitable are clinical vignettes \nfor the evaluation of symptom checker apps? A test theoretical perspective. DIGITAL HEALTH 9, \n20552076231194929 (2023). \n37. Kopka, M. & Feufel, M. A. symptomcheckR: An R Package for Analyzing and Visualizing \nSymptom Checker Performance. http://medrxiv.org/lookup/doi/10.1101/2024.02.06.24302384 (2024) \ndoi:10.1101/2024.02.06.24302384. \n38. Arellano Carmona, K., Chittamuru, D., Kravitz, R. L., Ramondt, S. & Ramírez, A. S. Health \nInformation Seeking From an Intelligent Web-Based Symptom Checker: Cross-sectional Questionnaire \nStudy. J Med Internet Res 24,\n e36322 (2022). \n39. Gilbert, S. et al. How accurate are digital symptom assessment apps for suggesting conditions and \nurgency advice? A clinical vignettes comparison to GPs. BMJ Open 10, e040269 (2020). \n40. Peer, E., Rothschild, D. M., Evernden, Z., Gordon, A. & Damer, E. MTurk, Prolific or Panels? \nChoosing the Right Audience for Online Research. SSRN Journal (2021) doi:10.2139/ssrn.3765448. \n41. Schmieding, M. L., Mörgeli, R., Schmieding, M. A. L., Feufel, M. A. & Balzer, F. Benchmarking \nTriage Capability of Symptom Checkers Against That of Medical Laypersons: Survey Study. J Med \nInternet Res 23, e24475 (2021). \n42. Questback GmbH. Umfragesoftware für Studierende und Wissenschaftler. Unipark \nhttps://www.unipark.com/umfragesoftware/ (2021). \n43. Meczner, A. et al. Accuracy as a Composite Measure for the Assessment of Online Symptom \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nCheckers in Vignette Studies: Evaluation of Current Practice and Recommendations (Preprint). \nhttp://preprints.jmir.org/preprint/49907 (2023) doi:10.2196/preprints.49907. \n44. Ito, N. et al. The Accuracy and Potential Racial and Ethnic Biases of GPT-4 in the Diagnosis and \nTriage of Health Conditions: Evaluation Study. JMIR Med Educ 9, e47532 (2023). \n45. Hallgren, K. A. Computing Inter-Rater Reliability for Observational Data: An Overview and \nTutorial. TQMP 8, 23–34 (2012). \n46. Cicchetti, D. V. Guidelines, criteria, and rules of thumb for evaluating normed and standardized \nassessment instruments in psychology. Psychological Assessment 6, 284–290 (1994). \n47. Page, M. J. et al. The PRISMA 2020 statement: an updated guideline for reporting systematic \nreviews. Syst Rev 10, 89 (2021). \n48. Kopka, M., Feufel, M. A., Balzer, F. & Schmieding, M. L. The Triage Capability of Laypersons: \nRetrospective Exploratory Analysis. JMIR Form Res 6, e38977 (2022). \n49. Fraser, H. S. F. et al. Evaluation of Diagnostic and Triage Accuracy and Usability of a Symptom \nChecker in an Emergency Department: Observational Study. JMIR Mhealth Uhealth 10, e38364 (2022). \n50. Jang, J. et al. Towards Continual Knowledge Learning of Language Models. (2021) \ndoi:10.48550/ARXIV.2110.03215. \n51. Meskó, B. & Topol, E. J. The imperative for regulatory oversight of large language models (or \ngenerative AI) in healthcare. npj Digit. Med. 6, 120 (2023). \n52. Thirunavukarasu, A. J. et al. Large language models in medicine. Nat Med 29, 1930–1940 \n(2023). \n53. Lee, M., Liang, P. & Yang, Q. CoAuthor: Designing a Human-AI Collaborative Writing Dataset \nfor Exploring Language Model Capabilities. in CHI Conference on Human Factors in Computing \nSystems 1–19 (ACM, New Orleans LA USA, 2022). doi:10.1145/3491102.3502030. \n \nCompeting interests \nThe authors declare no competing interests.  \n \nAuthor contributions \nMK and MAF conceived of the study. MK developed the methodology with critical input from MAF, \ncreated the vignettes and handled programming and sampling. HN and MP provided solutions to the \nvignettes. MK collected the data, with DS and SZ supporting data collection from symptom-assessment \napplications. MK conducted the analyses and data visualization and wrote the first draft of the \nmanuscript. All authors provided critical input and worked on manuscript development.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nAppropriate\nSelf-Triage \nLevel\nCues Decision-Maker Cue Utilization Self-Triage Level \nDecision\nCue\n1\nCue\n2\nCue\n3\n…\n#1\n#2\n#3\nThe self-triage level\nthat is most\nappropriate. It is not \ndirectly observable.\nAll information on the\ncase that are\nobservable. These cues\nare representative and \nhave natural\nintercorrelations.\nThis information is\nobserved by the\ndecision-maker.\nThe decision-maker\nutilizies the cues. They\ncan use different \nstrategies (e.g., only\nuse one cue or weight\nthe cues).\nBased on the cue\nutilization, the\ndecision-maker arrives\nat the triage level they\nthink is most\nappropriate.\nNo\nSelf-Triage \nLevel\nCues Decision-Maker Cue Utilization Self-Triage Level \nDecision\nCue\nA\nCue\nB\nCue\nC\n…\nA\nB\nC\nSince the case is\nderived from\ntextbooks and other\nresources, there is no\nself-triage level in \nreality.\nThe described cues\nare prefiltered from\nclinicians and there\nare no natural\ncorrelations between\nthe cues.\nThe decision-maker\nuses these biased cues\nand interrelations to\nmake a decision. \nHowever, the cues do \nnot provide real \ninformation.\nBased on the cue\nutilization, the decision-\nmaker decides on a \ntriage level. This decision\ncannot be generalized to\nthe environment of\ninterest.\nRepresentativeVignettes\nTraditional Vignettes\n?\nThis information is\nobserved by the\ndecision-maker.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint \nCases identified: 8794 \nCases removed before screening:\n• Duplicate cases removed (n = 858)\n• Cases exceeding 1000 characters removed (n = 2548)\nCases screened (randomly \ndrawn): 526 \nCases included: 45\nCases excluded:\n• Asking for general information (n = 99)\n• Picture is necessary (n = 86) \n• Information provided too specific (n = 72)\n• Already seen a medical professional (n = 63)\n• Quota limit reached (n = 58)\n• Did not provide sufficient information (n = 46)\n• Symptoms are not self-perceived (n = 27)\n• Symptoms are not acute (n = 26)\n• Symptoms were already diagnosed (n = 4)\nCases in data set: 5388 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 3, 2024. ; https://doi.org/10.1101/2024.04.02.24305193doi: medRxiv preprint ",
  "topic": "Vignette",
  "concepts": [
    {
      "name": "Vignette",
      "score": 0.9670546054840088
    },
    {
      "name": "Triage",
      "score": 0.8487600088119507
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6444770693778992
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5631635189056396
    },
    {
      "name": "Sample (material)",
      "score": 0.42669016122817993
    },
    {
      "name": "Psychology",
      "score": 0.42449748516082764
    },
    {
      "name": "Medical education",
      "score": 0.3356834053993225
    },
    {
      "name": "Applied psychology",
      "score": 0.32800307869911194
    },
    {
      "name": "Medicine",
      "score": 0.32389384508132935
    },
    {
      "name": "Computer science",
      "score": 0.3204590082168579
    },
    {
      "name": "Social psychology",
      "score": 0.24551242589950562
    },
    {
      "name": "Psychiatry",
      "score": 0.09832867980003357
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4577782",
      "name": "Technische Universität Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I7877124",
      "name": "Charité - Universitätsmedizin Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I75951250",
      "name": "Freie Universität Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    }
  ],
  "cited_by": 3
}