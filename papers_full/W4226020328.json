{
    "title": "LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding",
    "url": "https://openalex.org/W4226020328",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2135447766",
            "name": "Jiapeng Wang",
            "affiliations": [
                "South China University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2110317904",
            "name": "Lianwen Jin",
            "affiliations": [
                "Peng Cheng Laboratory",
                "South China University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2021818801",
            "name": "Kai Ding",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3006883036",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3173325518",
        "https://openalex.org/W3202839357",
        "https://openalex.org/W2964346820",
        "https://openalex.org/W3176851559",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3171975879",
        "https://openalex.org/W3152774549",
        "https://openalex.org/W2972985407",
        "https://openalex.org/W3132296545",
        "https://openalex.org/W3176664887",
        "https://openalex.org/W3190448953",
        "https://openalex.org/W2965085721",
        "https://openalex.org/W2891117443",
        "https://openalex.org/W2769316437",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3163650427",
        "https://openalex.org/W3205981739",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2922714365",
        "https://openalex.org/W3123089036",
        "https://openalex.org/W2795424778",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2964350391",
        "https://openalex.org/W4287123554",
        "https://openalex.org/W3093218477",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W2996480099",
        "https://openalex.org/W3102725307",
        "https://openalex.org/W3104953317",
        "https://openalex.org/W2963336383",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2949861626",
        "https://openalex.org/W2986619406",
        "https://openalex.org/W2605976347",
        "https://openalex.org/W2962772269",
        "https://openalex.org/W1966382373",
        "https://openalex.org/W3190292546",
        "https://openalex.org/W3203055579",
        "https://openalex.org/W3000758063"
    ],
    "abstract": "Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7747 - 7757\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nLiLT: A Simple yet Effective Language-Independent Layout Transformer\nfor Structured Document Understanding\nJiapeng Wang1 Lianwen Jin∗1,3,4 Kai Ding∗2,3\n1South China University of Technology, Guangzhou, China\n2IntSig Information Co., Ltd, Shanghai, China\n3INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding, China\n4Peng Cheng Laboratory, Shenzhen, China\n1eejpwang@mail.scut.edu.cn, eelwjin@scut.edu.cn\n2danny_ding@intsig.net\nAbstract\nStructured document understanding has at-\ntracted considerable attention and made sig-\nnificant progress recently, owing to its cru-\ncial role in intelligent document processing.\nHowever, most existing related models can\nonly deal with the document data of specific\nlanguage(s) (typically English) included in\nthe pre-training collection, which is extremely\nlimited. To address this issue, we propose\na simple yet effective Language-independent\nLayout Transformer (LiLT) for structured doc-\nument understanding. LiLT can be pre-trained\non the structured documents of a single lan-\nguage and then directly fine-tuned on other\nlanguages with the corresponding off-the-shelf\nmonolingual/multilingual pre-trained textual\nmodels. Experimental results on eight lan-\nguages have shown that LiLT can achieve com-\npetitive or even superior performance on di-\nverse widely-used downstream benchmarks,\nwhich enables language-independent benefit\nfrom the pre-training of document layout struc-\nture. Code and model are publicly available at\nhttps://github.com/jpWang/LiLT.\n1 Introduction\nStructured document understanding (SDU) aims at\nreading and analyzing the textual and structured\ninformation contained in scanned/digital-born doc-\numents. With the acceleration of the digitization\nprocess, it has been regarded as a crucial part of\nintelligent document processing and required by\nmany real-world applications in various industries\nsuch as finance, medical treatment and insurance.\nRecently, inspired by the rapid development of\npre-trained language models of plain texts (Devlin\net al., 2019; Liu et al., 2019b; Bao et al., 2020;\nChi et al., 2021), many researches on structured\ndocument pre-training (Xu et al., 2020, 2021a,b; Li\net al., 2021a,b,c; Appalaraju et al., 2021) have also\n∗Corresponding author.\n(a) A form.\n (b) A receipt.\nFigure 1: The substitution of language does not appear\nobviously unnatural when the layout structure remains\nunchanged, as shown in a (a) form/(b) receipt. The\ndetailed content has been re-synthesized to avoid the\nsensitive information leak. Best viewed in zoomed-in.\npushed the limit of a variety of SDU tasks. How-\never, almost all of them only focus on pre-training\nand fine-tuning on the documents in a single lan-\nguage, typically English. This is extremely limited\nfor other languages, especially in the case of lack-\ning pre-training structured document data.\nIn this regard, we consider how to make the SDU\ntasks enjoy language-independent benefit from the\npre-training of document layout structure. Here,\nwe give an observation as shown in Figure 1. When\nthe layout structure remains unchanged, the substi-\ntution of language does not make obvious unnatu-\nralness. It fully motivates us to decouple and reuse\nthe layout invariance among different languages.\nBased on this inspiration, in this paper, we pro-\npose a simple yet effective Language-independent\nLayout Transformer (LiLT) for structured docu-\nment understanding. In our framework, the text and\nlayout information are first decoupled and joint-\noptimized during pre-training, and then re-coupled\nfor fine-tuning. To ensure that the two modali-\nties have sufficient language-independent interac-\ntion, we further propose a novel bi-directional at-\ntention complementation mechanism (BiACM) to\nenhance the cross-modality cooperation. Moreover,\nwe present the key point location (KPL) and cross-\nmodal alignment identification (CAI) tasks, which\nare combined with the widely-used masked visual-\n7747\nlanguage modeling (MVLM) to serve as our pre-\ntraining objectives. During fine-tuning, the layout\nflow (LiLT) can be separated and combined with\nthe off-the-shelf pre-trained textual models (such\nas RoBERTa (Liu et al., 2019b), XLM-R (Conneau\net al., 2020), InfoXLM (Chi et al., 2021), etc) to\ndeal with the downstream tasks. In this way, our\nmethod decouples and learns the layout knowledge\nfrom the monolingual structured documents before\ngeneralizing it to the multilingual ones.\nTo the best of our knowledge, the only pre-\nexisting multilingual SDU model is LayoutXLM\n(Xu et al., 2021b). It scraps multilingual PDF doc-\numents of 53 languages from a web crawler and\nintroduces extra pre-processing steps to clean the\ncollected data, filter the low-quality documents,\nand classify them into different languages. After\nthis, it utilizes a heuristic distribution to sample 22\nmillion multilingual documents, which are further\ncombined with the 8 million sampled English ones\nfrom the IIT-CDIP (Lewis et al., 2006) dataset (11\nmillion English documents), resulting 30 million\nfor pre-training with the LayoutLMv2 (Xu et al.,\n2021a) framework. However, this process is time-\nconsuming and laborious. On the contrary, LiLT\ncan be pre-trained with only IIT-CDIP and then\nadapted to other languages. In this respect, LiLT\nis the first language-independent method for struc-\ntured document understanding.\nExperimental results on eight languages have\nshown that LiLT can achieve competitive or even\nsuperior performance on diverse widely-used down-\nstream benchmarks, which substantially benefits\nnumerous real-world SDU applications. Our main\ncontributions can be summarized as follows:\n• We introduce a simple yet effective language-\nindependent layout Transformer called LiLT\nfor monolingual/multilingual structured docu-\nment understanding.\n• We propose BiACM to provide language-\nindependent cross-modality interaction, along\nwith an effective asynchronous optimization\nstrategy for textual and non-textual flows in\npre-training. Moreover, we present two new\npre-training objectives, namely KPL and CAI.\n• LiLT achieves competitive or even superior\nperformance on various widely-used down-\nstream benchmarks of different languages\nunder different settings, which fully demon-\nstrates its effectiveness.\n2 LiLT\nFigure 2 shows the overall illustration of our\nmethod. Given an input document image, we first\nuse off-the-shelf OCR engines to get text bounding\nboxes and contents. Then, the text and layout infor-\nmation are separately embedded and fed into the\ncorresponding Transformer-based architecture to\nobtain enhanced features. Bi-directional attention\ncomplementation mechanism (BiACM) is intro-\nduced to accomplish the cross-modality interaction\nof text and layout clues. Finally, the encoded text\nand layout features are concatenated and additional\nheads are added upon them, for the self-supervised\npre-training or the downstream fine-tuning.\n2.1 Model Architecture\nThe whole framework can be regarded as a parallel\ndual-stream Transformer. The layout flow shares\na similar structure as text flow, except for the re-\nduced hidden size and intermediate size to achieve\ncomputational efficiency.\n2.1.1 Text Embedding\nFollowing the common practice (Devlin et al.,\n2019; Xu et al., 2020), in the text flow, all text\nstrings in the OCR results are first tokenized and\nconcatenated as a sequence St by sorting the corre-\nsponding text bounding boxes from the top-left to\nbottom-right. Intuitively, the special tokens[CLS]\nand [SEP] are also added at the beginning and end\nof the sequence respectively. After this, St will be\ntruncated or padded with extra [PAD] tokens until\nits length equals the maximum sequence length N.\nFinally, we sum the token embedding Etoken of St\nand the 1D positional embedding P1D to obtain the\ntext embedding ET ∈ RN×dT as:\nET = LN(Etoken + P1D), (1)\nwhere dT is the number of text feature dimension\nand LN is the layer normalization (Ba et al., 2016).\n2.1.2 Layout Embedding\nAs for the layout flow, we construct a 2D posi-\ntion sequence Sl with the same length as the token\nsequence St using the corresponding text bound-\ning boxes. To be specific, we normalize and dis-\ncretize all box coordinates to integers in the range\n[0, 1000], and use four embedding layers to gener-\nate x-axis, y-axis, height, and width features sepa-\nrately. Given the normalized bounding boxes B =\n(xmin, xmax, ymin, ymax, width, height), the 2D\n7748\nMatMul\nMatMul\nMaskOut\nSoftMax\nQT KTVT\nTransformer Layer i \nMatMul\nMatMul\nMaskOut\nSoftMax\nQL KL VL\nTransformer Layer i\nToken \nEmbedding\n1D Position \nEmbedding\n+ + + + + + + +\nt1 tMt2 t4 tR t7t6 t8\n1 32 4 5 76 8\n2D Position \nEmbedding\n1D Position \nEmbedding\n+ + + + + + + +\nb1 b3bR b4 b5 b7b6 bM\n1 32 4 5 76 8\n+ +\n△\nScale\nThe Text Flow\n(RoBERTa/XLM-R/InfoXLM/...)\nThe Layout Flow\n(LiLT)\n||\nPre-training Objectives\nMasked\nVisual-Language Modeling- t3- - t5 t7- -\n- -r2 r4 - -- r8 Key Point Location\nCross-modal\nAlignment Identification\n(0:Mis-aligned, 1:Aligned)\n- -0 1 0 1- -\n1 32 4 5 76 8\nFine-tuning Tasks\nSemantic \nEntity Recognition\n(H:Header, Q:Question,\n A:Answer, O:Other)\nO OH Q A QO A\n- -- - - -- -\n- -- - - -- -\n- -- - - -- -\n- -0 - - -- -\n- -0 1 - -- -\n- -- - - -- -\n- -0 0 0 -- -\n- -0 0 0 1- -\nRelation Extraction\n(0:None, 1:Key-Value Pair)\n…… ……\nOCR Engines\nScale\nNl ×\n+\n||△\nConcatenate\nAdd\nDetach (only exists in pre-training)\nBiACM\nFigure 2: The overall illustration of our framework. Text and layout information are separately embedded and fed\ninto the corresponding flow. BiACM is proposed to accomplish the cross-modality interaction. At the model output,\ntext and layout features are concatenated for the self-supervised pre-training or the downstream fine-tuning. Nl is\nthe number of Transformer layers. The red *M/*R indicates the randomly masked/replaced item for pre-training. t,\nb and r represent token, box and region, respectively. Best viewed in zoomed-in.\npositional embedding P2D ∈ RN×dL (where dL\nis the number of layout feature dimension) is con-\nstructed as follows:\nP2D = Linear(CAT(Exmin, Exmax,\nEymin, Eymax,Ewidth , Eheight )). (2)\nHere, the Es are embedded vectors. Linear is a\nlinear projection layer and CAT is the channel-\nwise concatenation operation. The special to-\nkens [CLS], [SEP] and [PAD] are also attached\nwith (0,0,0,0,0,0), (1000,1000,1000,1000,0,0) and\n(0,0,0,0,0,0) respectively. It is worth mentioning\nthat, for each token, we directly utilize the bound-\ning box of the text string it belongs to, because the\nfine-grained token-level information is not always\nincluded in the results of some OCR engines.\nSince Transformer layers are permutation-\ninvariant, here we introduce the 1D positional em-\nbedding again. The resulting layout embedding\nEL ∈ RN×dL can be formulated as:\nEL = LN(P2D + P1D). (3)\n2.1.3 BiACM\nThe text embedding ET and layout embedding EL\nare fed into their respective sub-models to gen-\nerate high-level enhanced features. However, it\nwill considerably ignore the cross-modal interac-\ntion process if we simply combine the text and\nlayout features at the encoder output only. The net-\nwork also needs to comprehensively analyse them\nat earlier stages. In view of this, we propose a new\nbi-directional attention complementation mecha-\nnism (BiACM) to strengthen the cross-modality\ninteraction across the entire encoding pipeline. Ex-\nperiments in Section 3.2 will further verify its ef-\nfectiveness.\nThe vanilla self-attention mechanism in Trans-\nformer layers captures the correlation between\nquery xi and key xj by projecting the two vectors\nand calculating the attention score as:\nαij = (xiWQ)(xjWK)⊤\n√\ndh . (4)\nHere, the description is for a single head in a single\nself-attention layer with hidden size of dh and pro-\njection metrics WQ, WK for simplicity. Given αT\nij\nand αL\nij of the text and layout flows located in the\nsame head of the same layer, BiACM shares them\nas common knowledge, which is formulated as:\nfαT\nij = αL\nij + αT\nij, (5)\nfαL\nij =\n(\nαL\nij + DETACH(αT\nij) if Pre-train,\nαL\nij + αT\nij if Fine-tune.\n(6)\nIn order to maintain the ability of LiLT to cooper-\nate with different off-the-shelf text models in fine-\ntuning as much as possible, we heuristically adopt\nthe detached αT\nij for fαL\nij, so that the textual stream\nwill not be affected by the gradient of non-textual\n7749\none during pre-training, and its overall consistency\ncan be preserved. Finally, the modified attention\nscores are used to weight the projected value vec-\ntors for subsequent modules in both flows.\n2.2 Pre-training Tasks\nWe conduct three self-supervised pre-training tasks\nto guide the model to autonomously learn joint\nrepresentations with cross-modal cooperation. The\ndetails are introduced below.\n2.2.1 Masked Visual-Language Modeling\nThis task is originally derived from (Devlin et al.,\n2019). MVLM randomly masks some of the input\ntokens and the model is asked to recover them over\nthe whole vocabulary using the output encoded fea-\ntures, driven by a cross-entropy loss. Meanwhile,\nthe non-textual information remains unchanged.\nMVLM improves model learning on the language\nside with cross-modality information. The given\nlayout embedding can also help the model better\ncapture both inter- and intra-sentence relationships.\nWe mask 15% text tokens, among which 80% are\nreplaced by the special token [MASK], 10% are re-\nplaced by random tokens sampled from the whole\nvocabulary, and 10% remain the same.\n2.2.2 Key Point Location\nWe propose this task to make the model better un-\nderstand layout information in the structured docu-\nments. KPL equally divides the entire layout into\nseveral regions (we set 7×7=49 regions by default)\nand randomly masks some of the input bounding\nboxes. The model is required to predict which re-\ngions the key points (top-left corner, bottom-right\ncorner, and center point) of each box belong to us-\ning separate heads. To deal with it, the model is re-\nquired to fully understand the text content and know\nwhere to put a specific word/sentence when the sur-\nrounding ones are given. We mask 15% boxes,\namong which 80% are replaced by (0,0,0,0,0,0),\n10% are replaced by random boxes sampled from\nthe same batch, and 10% remain the same. Cross-\nentropy loss is adopted.\nSince there may exist detection errors in the out-\nput of OCR engines, we let the model predict the\ndiscretized regions (as mentioned above) instead\nof the exact location. This strategy can moderately\nrelax the punishment criterion while improving the\nmodel performance.\n2.2.3 Cross-modal Alignment Identification\nWe collect those encoded features of token-box\npairs that are masked and further replaced (mis-\naligned) or kept unchanged (aligned) by MVLM\nand KPL, and build an additional head upon them\nto identify whether each pair is aligned. To achieve\nthis, the model is required to learn the cross-modal\nperception capacity. CAI is a binary classification\ntask, and a cross-entropy loss is applied for it.\n2.3 Optimization Strategy\nUtilizing a unified learning rate for all model pa-\nrameters to perform the end-to-end training process\nis the most common optimization strategy. While\nin our case, it will cause the layout flow to contin-\nuously update in the direction of coupling with\nthe evolving text flow in the pre-training stage,\nwhich is harmful to the ability of LiLT to coop-\nerate with different off-the-shelf textual models\nduring fine-tuning. Based on this consideration, we\nexplore multiple ratios to greatly slow down the\npre-training optimization of the text stream. We\nalso find that an appropriate reduction ratio is better\nthan parameter freezing.\nNote that, we adopt a unified learning rate for\nend-to-end optimization during fine-tuning. The\nDETACH operation of BiACM is also canceled at\nthis time, as shown in Equation 6.\n3 Experiments\n3.1 Pre-training Setting\nWe pre-train LiLT on the IIT-CDIP Test Collec-\ntion 1.0 (Lewis et al., 2006), which is a large-scale\nscanned document image dataset and contains more\nthan 6 million documents with more than 11 mil-\nlion scanned document images. We use TextIn\nAPI1 to obtain the text bounding boxes and strings\nfor this dataset.\nIn this paper, we initialize the text flow from\nthe existing pre-trained English RoBERTa BASE\n(Liu et al., 2019b) for our document pre-training,\nand combine LiLT BASE with the pre-trained\nInfoXLMBASE (Chi et al., 2021)/a new pre-trained\nRoBERTaBASE for multilingual/monolingual fine-\ntuning. They have an equal number of self-\nattention layers, attention heads and maximum\nsequence length, which ensures that BiACM can\nwork normally. In this BASE setting, LiLT has a\n12-layer encoder with 192 hidden size, 768 feed-\nforward filter size and 12 attention heads, resulting\n1https://www.textin.com\n7750\n# Inter-modal Operation Average F1\n1 CAT 0.6751\n2 CAT+Co-Attention (Lu et al., 2019) 0.6276\n3 CAT+BiACM 0.7963\n4 CAT+BiACM−DETACH in pre-training 0.7682\n5 CAT+BiACM+DETACH in fine-tuning 0.7822\n6 The text flow alone\n(InfoXLMBASE, as shown in Table 6) 0.7207\n(a) BiACM. CAT is short for concatenation.\n# MVLM KPL CAI Average F1\n1 ✓ 0.7616\n2 ✓ ✓ 0.7748\n3 ✓ ✓ 0.7809\n4 ✓ ✓ ✓ 0.7963\n(b) Pre-training tasks.\n# Slow-down Ratio Average F1\n1 1 (No Slow-down) 0.7840\n2 500 0.7901\n3 800 0.7947\n4 1000 0.7963\n5 1200 0.7935\n6 +∞(Parameter Freezing) 0.7893\n(c) Slow-down ratios.\nTable 1: Ablation study of LiLT BASE combined with\nInfoXLMBASE (Chi et al., 2021) on the FUNSD and\nXFUND datasets (8 languages in total). The average F1\naccuracy of language-specific semantic entity recogni-\ntion (SER) task is given. (a) BiACM. (b) Pre-training\ntasks. (c) Slow-down ratios of the pre-training optimiza-\ntion for the text flow.\nin the number of parameters as 6.1M. The maxi-\nmum sequence length N is set as 512.\nLiLTBASE is pre-trained using Adam optimizer\n(Kingma and Ba, 2015; Loshchilov and Hutter,\n2018), with the learning rate 2×10−5, weight decay\n1×10−2, and (β1, β2) = (0.9, 0.999). The learning\nrate is linearly warmed up over the first 10% steps\nand then linearly decayed. We set the batch size\nas 96 and train LiLTBASE for 5 epochs on the IIT-\nCDIP dataset using 4 NVIDIA A40 48GB GPUs.\n3.2 Ablation Study\nConsidering that the complete pre-training takes a\nrelatively long time, we pre-train LiLTBASE with\n2M documents randomly sampled from IIT-CDIP\nfor 5 epochs to conduct ablation experiments, as\nshown in Table 1.\nWe first evaluate the effect of introducing Bi-\nACM. In setting (a)#1, the text and layout fea-\ntures are concatenated at the model output with-\nout any further interaction. Compared with (a)#6,\nModel Precision Recall F1\nBERTBASE1 0.5469 0.6710 0.6026\nRoBERTaBASE2 0.6349 0.6975 0.6648\nUniLMv2BASE3 0.6349 0.6975 0.6648\nLayoutLMBASE4 0.7597 0.8155 0.7866\nBROSBASE5 0.8056 0.8188 0.8121\nSelfDoc6 - - 0.8336\nLayoutLMv2BASE7 0.8029 0.8539 0.8276\nStrucTexTBASE8 0.8568 0.8097 0.8309\nDocFormerBASE9 0.8076 0.8609 0.8334\n⋆LayoutXLMBASE10 0.7913 0.8158 0.8034\nLiLT[EN-R2]BASE 0.8721 0.8965 0.8841\n⋆LiLT[InfoXLM11]BASE 0.8467 0.8709 0.8586\nTable 2: Comparison on the semantic entity recognition\n(SER) task of FUNSD (Jaume et al., 2019) dataset.Bold\nindicates the SOTA andunderline indicates the second\nbest. “EN-R” is short for English RoBERTa. ⋆The\nmultilingual model. [] denotes the off-the-shelf tex-\ntual model used as the text flow of LiLT.1(Devlin et al.,\n2019);2(Liu et al., 2019b);3(Bao et al., 2020);4(Xu et al.,\n2020);5(Hong et al., 2020); 6(Li et al., 2021b); 7(Xu\net al., 2021a); 8(Li et al., 2021c); 9(Appalaraju et al.,\n2021);10(Xu et al., 2021b);11(Chi et al., 2021).\nwe find that such a plain design results in a much\nworse performance than using the text flow alone.\nFrom (a)#1 to (a)#3, the significant improvement\ndemonstrates that it is the novel BiACM that makes\nthe transfer from “monolingual” to “multilingual”\nsuccessful. Beside this, we have also tried to re-\nplace BiACM with the co-attention mechanism\n(Lu et al., 2019) which is widely adopted in dual-\nstream Transformer architecture. It can be seen as\na “deeper” cross-modal interaction, since the keys\nand values from each modality are passed as input\nto the other modality’s dot-product attention cal-\nculation. However, severe drops are observed as\nshown in (a)#2 vs (a)#1#3. We attribute it to the\ndamage of such a “deeper” interaction to the over-\nall consistency of the text flow in the pre-training\noptimization. In contrast, BiACM can maintain\nLiLT’s cross-model cooperation ability on the basis\nof providing cross-modal information. Moreover,\nthe necessity of DETACH in pre-training is proved\nin (a)#4 vs (a)#3. Compared (a)#3 to (a)#5, we can\nalso infer that removing DETACH in fine-tuning\nleads to a better performance.\nThen, we compare the proposed KPL and CAI\ntasks. As shown in Table 1(b), both tasks improve\nthe model performance substantially, and the pro-\nposed CAI benefits the model more than KPL. Us-\ning both tasks together is more effective than using\neither one alone.\n7751\nModel Precision Recall F1\nBERTBASE 0.8833 0.9107 0.8968\nUniLMv2BASE 0.8987 0.9198 0.9092\nLayoutLMBASE 0.9437 0.9508 0.9472\nBROSBASE 0.9558 0.9514 0.9536\nLAMBERTBASE1 - - 0.9441\nTILTBASE2 - - 0.9511\nLayoutLMv2BASE 0.9453 0.9539 0.9495\nDocFormerBASE 0.9652 0.9614 0.9633\n⋆LayoutXLMBASE 0.9456 0.9506 0.9481\nLiLT[EN-R]BASE 0.9598 0.9616 0.9607\n⋆LiLT[InfoXLM]BASE 0.9574 0.9581 0.9577\nTable 3: Comparison on the semantic entity recogni-\ntion (SER) task of CORD (Park et al., 2019) dataset.\n1(Garncarek et al., 2021);2(Powalski et al., 2021).\nModel Precision Recall F1\nBiLSTM+CRF1 - - 0.8910\nGraphIE2 - - 0.9026\nGCN-based3 - - 0.9255\nTRIE4 - - 0.9321\nVIES5 - - 0.9523\nMatchVIE6 - - 0.9687\nTCPN7 - - 0.9759\nRoBERTaBASE8 0.9405 0.9640 0.9521\nStrucTexTBASE - - 0.9795\n⋆LayoutXLMBASE 0.9699 0.9820 0.9759\nLiLT[ZH-R8]BASE 0.9762 0.9833 0.9797\n⋆LiLT[InfoXLM]BASE 0.9699 0.9820 0.9759\nTable 4: Comparison on the semantic entity recognition\n(SER) task of EPHOIE (Wang et al., 2021a) dataset.\n“ZH-R” is short for Chinese RoBERTa. 1(Lample et al.,\n2016);2(Qian et al., 2019);3(Liu et al., 2019a);4(Zhang\net al., 2020); 5(Wang et al., 2021a); 6(Tang et al.,\n2021);7(Wang et al., 2021b);8(Cui et al., 2020).\nFinally, we explore the most suitable slow-down\nratio for the pre-training optimization of the text\nflow. A ratio equal to 1 in (c)#1 means there is no\nslow-down and a unified learning rate is adopted.\nIt can be found that the F1 scores keep rising with\nthe growth of slow-down ratios and begin to fall\nwhen the ratio is greater than 1000. Consequently,\nwe set the slow-down ratio as 1000 by default.\n3.3 Comparisons with the SOTAs\nTo demonstrate the performance of LiLT, we con-\nduct experiments on several widely-used monolin-\ngual datasets and the multilingual XFUND bench-\nmark (Xu et al., 2021b). In addition to the ex-\nperiments involving typical language-specific fine-\ntuning, we also follow the two settings designed\nModel Accuracy\nVGG-161 90.97%\nStacked CNN Single2 91.11%\nStacked CNN Ensemble2 92.21%\nInceptionResNetV23 92.63%\nLadderNet4 92.77%\nMultimodal Single5 93.03%\nMultimodal Ensemble5 93.07%\nBERTBASE 89.81%\nUniLMv2BASE 90.06%\nLayoutLMBASE(w/ image) 94.42%\nBROSBASE 95.58%\nSelfDoc 93.81%\nTILTBASE 93.50%\nLayoutLMv2BASE 95.25%\nDocFormerBASE 96.17%\n⋆LayoutXLMBASE 95.21%\nLiLT[EN-R]BASE 95.68%\n⋆LiLT[InfoXLM]BASE 95.62%\nTable 5: Comparison on the document classification\n(DC) task of RVL-CDIP (Harley et al., 2015) dataset.\n1(Afzal et al., 2017);2(Das et al., 2018);3(Szegedy et al.,\n2017);4(Sarkhel and Nandi, 2019);5(Dauphinee et al.,\n2019).\nin (Xu et al., 2021b) to demonstrate the ability\nto transfer knowledge among different languages,\nwhich are zero-shot transfer learning and multitask\nfine-tuning, for fair comparisons. Specifically, (1)\nlanguage-specific fine-tuning refers to the typical\nfine-tuning paradigm of fine-tuning on language X\nand testing on language X. (2) Zero-shot transfer\nlearning means the models are fine-tuned on En-\nglish data only and then evaluated on each target\nlanguage. (3) Multitask fine-tuning requires the\nmodel to fine-tune on data in all languages.\n3.3.1 Language-specific Fine-tuning\nWe first evaluate LiLT on four widely-used mono-\nlingual datasets - FUNSD (Jaume et al., 2019),\nCORD (Park et al., 2019), EPHOIE (Wang et al.,\n2021a) and RVL-CDIP (Lewis et al., 2006), and the\nresults are shown in Table 2, 3, 4 and 5. We have\nfound that (1) LiLT is flexible since it can work\nwith monolingual or multilingual plain text models\nto deal with downstream tasks. (2) Although LiLT\nis designed for the transfer from “monolingual” to\n“multilingual”, it can surprisingly cooperate with\nmonolingual textual models to achieve competi-\ntive or even superior performance (especially on\nthe FUNSD dataset with only a few training sam-\nples available), compared with existing language-\nspecific SDU models such as LayoutLMv2 and\n7752\nTask Model Pre-training Docs FUNSD XFUND Avg.Language Size EN ZH JA ES FR IT DE PT\nSER\nXLM-RoBERTaBASE - - 0.6670 0.8774 0.7761 0.6105 0.6743 0.6687 0.6814 0.6818 0.7047\nInfoXLMBASE - - 0.6852 0.8868 0.7865 0.6230 0.7015 0.6751 0.7063 0.7008 0.7207\nLayoutXLMBASE Multilingual 30M 0.7940 0.8924 0.7921 0.7550 0.7902 0.8082 0.8222 0.7903 0.8056\nLiLT[InfoXLM]BASE English only 11M 0.8415 0.8938 0.7964 0.7911 0.7953 0.8376 0.8231 0.8220 0.8251\nRE\nXLM-RoBERTaBASE - - 0.2659 0.5105 0.5800 0.5295 0.4965 0.5305 0.5041 0.3982 0.4769\nInfoXLMBASE - - 0.2920 0.5214 0.6000 0.5516 0.4913 0.5281 0.5262 0.4170 0.4910\nLayoutXLMBASE Multilingual 30M 0.5483 0.7073 0.6963 0.6896 0.6353 0.6415 0.6551 0.5718 0.6432\nLiLT[InfoXLM]BASE English only 11M 0.6276 0.7297 0.7037 0.7195 0.6965 0.7043 0.6558 0.5874 0.6781\nTable 6: Language-specific fine-tuning F1 accuracy on FUNSD and XFUND (fine-tuning on X, testing on X). “SER”\ndenotes the semantic entity recognition and “RE” denotes the relation extraction. [] indicates the off-the-shelf\ntextual model used as the text flow of LiLT.\nDocFormer. (3) On these datasets which are widely\nadopted for monolingual evaluation, LiLT gener-\nally performs better than LayoutXLM. This fully\ndemonstrates the effectiveness of our pre-training\nframework and indicates that the layout and text\ninformation can be successfully decoupled in pre-\ntraining and re-coupled in fine-tuning.\nThen we evaluate LiLT on language-specific\nfine-tuning tasks of FUNSD and the multilingual\nXFUND (Xu et al., 2021b), and the results are\nshown in Table 6. Compared with the plain text\nmodels (XLM-R/InfoXLM) or the LayoutXLM\nmodel pre-trained with 30M multilingual struc-\ntured documents, LiLT achieves the highest F1\nscores on both the SER and RE tasks of each lan-\nguage while using 11M monolingual data. This\nsignificant improvement shows LiLT’s capability\nto transfer language-independent knowledge from\npre-training to downstream tasks.\n3.3.2 Zero-shot Transfer Learning\nThe results of cross-lingual zero-shot transfer are\npresented in Table 7. It can be observed that the\nLiLT model transfers the most knowledge from En-\nglish to other languages, and significantly outper-\nforms its competitors. This fully verifies that LiLT\ncan capture the common layout invariance among\ndifferent languages. Moreover, LiLT has never\nseen non-English documents before evaluation un-\nder this setting, while the LayoutXLM model has\nbeen pre-trained with them. This is to say, LiLT\nfaces a stricter cross-lingual zero-shot transfer sce-\nnario but achieves better performance.\n3.3.3 Multi-task Fine-tuning\nTable 8 shows the results of multitask learning. In\nthis setting, the pre-trained LiLT model is simul-\ntaneously fine-tuned with all eight languages and\nevaluated for each specific language. We observe\nthat this setting further improves the model per-\nformance compared to the language-specific fine-\ntuning, which confirms that SDU can benefit from\ncommonalities in the layout of multilingual struc-\ntured documents. In addition, LiLT once again\noutperforms its counterparts by a large margin.\n4 Related Work\nDuring the past decade, deep learning methods be-\ncame the mainstream for document understanding\ntasks (Yang et al., 2017; Augusto Borges Oliveira\net al., 2017; Siegel et al., 2018). Grid-based meth-\nods (Katti et al., 2018; Denk and Reisswig, 2019;\nLin et al., 2021) were proposed for 2D document\nrepresentation where text pixels were encoded us-\ning character or word embeddings and classified\ninto specific field types, using a convolutional neu-\nral network. GNN-based approaches (Liu et al.,\n2019a; Yu et al., 2021; Tang et al., 2021) adopted\nmulti-modal features of text segments as nodes to\nmodel the document graph, and used graph neural\nnetworks to propagate information between neigh-\nboring nodes to attain a richer representation.\nIn recent years, self-supervised pre-training has\nachieved great success. Inspired by the develop-\nment of the pre-trained language models in various\nNLP tasks, recent studies on structured document\npre-training (Xu et al., 2020, 2021a,b; Li et al.,\n2021a,b,c; Appalaraju et al., 2021) have pushed the\nlimits. LayoutLM (Xu et al., 2020) modified the\nBERT (Devlin et al., 2019) architecture by adding\n2D spatial coordinate embeddings. In compari-\nson, our LiLT can be regarded as a more powerful\nand flexible solution for structured document un-\nderstanding. LayoutLMv2 (Xu et al., 2021a) im-\nproved over LayoutLM by treating the visual fea-\n7753\nTask Model Pre-training Docs FUNSD XFUND Avg.Language Size EN ZH JA ES FR IT DE PT\nSER\nXLM-RoBERTaBASE - - 0.6670 0.4144 0.3023 0.3055 0.3710 0.2767 0.3286 0.3936 0.3824\nInfoXLMBASE - - 0.6852 0.4408 0.3603 0.3102 0.4021 0.2880 0.3587 0.4502 0.4119\nLayoutXLMBASE Multilingual 30M 0.7940 0.6019 0.4715 0.4565 0.5757 0.4846 0.5252 0.5390 0.5561\nLiLT[InfoXLM]BASE♠ English only 11M 0.8415 0.6152 0.5184 0.5101 0.5923 0.5371 0.6013 0.6325 0.6061\nRE\nXLM-RoBERTaBASE - - 0.2659 0.1601 0.2611 0.2440 0.2240 0.2374 0.2288 0.1996 0.2276\nInfoXLMBASE - - 0.2920 0.2405 0.2851 0.2481 0.2454 0.2193 0.2027 0.2049 0.2423\nLayoutXLMBASE Multilingual 30M 0.5483 0.4494 0.4408 0.4708 0.4416 0.4090 0.3820 0.3685 0.4388\nLiLT[InfoXLM]BASE♠ English only 11M 0.6276 0.4764 0.5081 0.4968 0.5209 0.4697 0.4169 0.4272 0.4930\nTable 7: Cross-lingual zero-shot transfer F1 accuracy on FUNSD and XFUND (fine-tuning on FUNSD, testing on\nX). ♠ indicates that LiLT faces a stricter zero-shot transfer scenario compared with LayoutXLM, since it has never\nseen non-English documents before evaluation, even during pre-training.\nTask Model Pre-training Docs FUNSD XFUND Avg.Language Size EN ZH JA ES FR IT DE PT\nSER\nXLM-RoBERTaBASE - - 0.6633 0.8830 0.7786 0.6223 0.7035 0.6814 0.7146 0.6726 0.7149\nInfoXLMBASE - - 0.6538 0.8741 0.7855 0.5979 0.7057 0.6826 0.7055 0.6796 0.7106\nLayoutXLMBASE Multilingual 30M 0.7924 0.8973 0.7964 0.7798 0.8173 0.8210 0.8322 0.8241 0.8201\nLiLT[InfoXLM]BASEEnglish only 11M 0.8574 0.9047 0.8088 0.8340 0.8577 0.8792 0.8769 0.8493 0.8585\nRE\nXLM-RoBERTaBASE - - 0.3638 0.6797 0.6829 0.6828 0.6727 0.6937 0.6887 0.6082 0.6341\nInfoXLMBASE - - 0.3699 0.6493 0.6473 0.6828 0.6831 0.6690 0.6384 0.5763 0.6145\nLayoutXLMBASE Multilingual 30M 0.6671 0.8241 0.8142 0.8104 0.8221 0.8310 0.7854 0.7044 0.7823\nLiLT[InfoXLM]BASEEnglish only 11M 0.7407 0.8471 0.8345 0.8335 0.8466 0.8458 0.7878 0.7643 0.8125\nTable 8: Multitask fine-tuning F1 accuracy on FUNSD and XFUND (fine-tuning on 8 languages all, testing on X).\ntures as separate tokens. Furthermore, additional\npre-training tasks were explored to improve the uti-\nlization of unlabeled document data. SelfDoc (Li\net al., 2021b) established the contextualization over\na block of content, while StructuralLM (Li et al.,\n2021a) proposed cell-level 2D position embeddings\nand the corresponding pre-training objective. Re-\ncently, StrucTexT (Li et al., 2021c) introduced a\nunified solution to efficiently extract semantic fea-\ntures from different levels and modalities to handle\nthe entity labeling and entity linking tasks. Doc-\nFormer (Appalaraju et al., 2021) designed a novel\nmulti-modal self-attention layer capable of fusing\ntextual, vision and spatial features.\nNevertheless, the aforementioned SDU ap-\nproaches mainly focus on a single language - typ-\nically English, which is extremely limited with\nrespect to multilingual application scenarios. To\nthe best of our knowledge, LayoutXLM (Xu et al.,\n2021b) was the only pre-existing multilingual SDU\nmodel, which adopted the multilingual textual\nmodel InfoXLM (Chi et al., 2021) as the initial-\nization, and adapted the LayoutLMv2 (Xu et al.,\n2021a) framework to multilingual structured doc-\nument pre-training. However, it required a heavy\nprocess of multilingual data collection, cleaning\nand pre-training. On the contrary, our LiLT can\ndeal with the multilingual structured documents\nby pre-training on the monolingual IIT-CDIP Test\nCollection 1.0 (Lewis et al., 2006) only.\n5 Conclusion\nIn this paper, we present LiLT, a language-\nindependent layout Transformer that can learn the\nlayout knowledge from monolingual structured\ndocuments and then generalize it to deal with\nmultilingual ones. Our framework successfully\nfirst decouples the text and layout information\nin pre-training and then re-couples them for fine-\ntuning. Experimental results on eight languages un-\nder three settings (language-specific, cross-lingual\nzero-shot transfer, and multi-task fine-tuning) have\nfully illustrated its effectiveness, which substan-\ntially bridges the language gap in real-world struc-\ntured document understanding applications. The\npublic availability of LiLT is also expected to pro-\nmote the development of document intelligence.\nFor future research, we will continue to follow\nthe pattern of transferring from “monolingual” to\n“multilingual” and further unlock the power of LiLT.\nIn addition, we will also explore the generalized\nrather than language-specific visual information\ncontained in multilingual structured documents.\n7754\n6 Acknowledgement\nThis research is supported in part by NSFC\n(Grant No.: 61936003) and GD-NSF (No.\n2017A030312006).\nReferences\nMuhammad Zeshan Afzal, Andreas Kölsch, Sheraz\nAhmed, and Marcus Liwicki. 2017. Cutting the er-\nror by half: Investigation of very deep CNN and\nadvanced training strategies for document image clas-\nsification. In ICDAR, volume 1, pages 883–888.\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,\nYusheng Xie, and R Manmatha. 2021. DocFormer:\nEnd-to-end Transformer for document understanding.\nIn ICCV.\nDario Augusto Borges Oliveira et al. 2017. Fast CNN-\nbased document layout analysis. In ICCV Workshop,\npages 1173–1180.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. UniLMv2: Pseudo-\nmasked language models for unified language model\npre-training. In ICML, pages 642–652.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHe-Yan Huang, and Ming Zhou. 2021. InfoXLM:\nAn information-theoretic framework for cross-lingual\nlanguage model pre-training. In NAACL-HLT, pages\n3576–3588.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In ACL,\npages 8440–8451.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin\nWang, and Guoping Hu. 2020. Revisiting pre-trained\nmodels for Chinese natural language processing. In\nFindings of EMNLP, pages 657–668.\nArindam Das, Saikat Roy, Ujjwal Bhattacharya, and\nSwapan K Parui. 2018. Document image classifica-\ntion with intra-domain transfer learning and stacked\ngeneralization of deep convolutional neural networks.\nIn ICPR, pages 3180–3185.\nTyler Dauphinee, Nikunj Patel, and Mohammad\nRashidi. 2019. Modular multimodal architec-\nture for document classification. arXiv preprint\narXiv:1912.04376.\nTimo I Denk and Christian Reisswig. 2019. BERT-\ngrid: Contextualized embedding for 2D document\nrepresentation and understanding. In Workshop on\nDocument Intelligence at NeurIPS.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186.\nŁukasz Garncarek, Rafał Powalski, Tomasz\nStanisławek, Bartosz Topolski, Piotr Halama,\nand Filip Grali ´nski. 2021. LAMBERT: Layout-\naware (language) modeling using BERT for\ninformation extraction. In ICDAR.\nAdam W Harley et al. 2015. Evaluation of deep convo-\nlutional nets for document image classification and\nretrieval. In ICDAR, pages 991–995.\nTeakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok\nHwang, Daehyun Nam, and Sungrae Park. 2020.\nBROS: A pre-trained language model for understand-\ning texts in document.\nGuillaume Jaume et al. 2019. FUNSD: A dataset for\nform understanding in noisy scanned documents. In\nICDAR, volume 2, pages 1–6.\nAnoop R Katti, Christian Reisswig, Cordula Guder, Se-\nbastian Brarda, Steffen Bickel, Johannes Höhne, and\nJean Baptiste Faddoul. 2018. Chargrid: Towards\nunderstanding 2D documents. In EMNLP, pages\n4459–4469.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition. In\nNAACL-HLT, pages 260–270.\nDavid Lewis, Gady Agam, Shlomo Argamon, Ophir\nFrieder, David Grossman, and Jefferson Heard. 2006.\nBuilding a test collection for complex document in-\nformation processing. In ACM SIGIR, pages 665–\n666.\nChenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang\nHuang, Fei Huang, and Luo Si. 2021a. StructuralLM:\nStructural pre-training for form understanding. In\nACL.\nPeizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu,\nHandong Zhao, Rajiv Jain, Varun Manjunatha, and\nHongfu Liu. 2021b. SelfDoc: Self-supervised docu-\nment representation learning. In CVPR, pages 5652–\n5660.\nYulin Li, Yuxi Qian, Yuchen Yu, Xiameng Qin,\nChengquan Zhang, Yan Liu, Kun Yao, Junyu Han,\nJingtuo Liu, and Errui Ding. 2021c. StrucTexT:\nStructured text understanding with multi-modal\nTransformers. In ACM-MM.\n7755\nTsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming\nHe, Bharath Hariharan, and Serge Belongie. 2017.\nFeature pyramid networks for object detection. In\nCVPR, pages 2117–2125.\nWeihong Lin, Qifang Gao, Lei Sun, Zhuoyao Zhong,\nKai Hu, Qin Ren, and Qiang Huo. 2021. ViBERT-\ngrid: A jointly trained multi-modal 2D document\nrepresentation for key information extraction from\ndocuments. In ICDAR.\nXiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha\nZhao. 2019a. Graph convolution for multimodal in-\nformation extraction from visually rich documents.\nIn NAACL-HLT, pages 32–39.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In ICLR.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. ViLBERT: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks.\nNeurIPS, 32:13–23.\nSeunghyun Park, Seung Shin, Bado Lee, Junyeop Lee,\nJaeheung Surh, Minjoon Seo, and Hwalsuk Lee. 2019.\nCORD: A consolidated receipt dataset for post-OCR\nparsing. In Workshop on Document Intelligence at\nNeurIPS.\nRafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz,\nTomasz Dwojak, Michał Pietruszka, and Gabriela\nPałka. 2021. Going full-TILT boogie on document\nunderstanding with text-image-layout Transformer.\nIn ICDAR.\nYujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, and\nRegina Barzilay. 2019. GraphIE: A graph-based\nframework for information extraction. In NAACL-\nHLT, pages 751–761.\nRitesh Sarkhel and Arnab Nandi. 2019. Deterministic\nrouting between layout abstractions for multi-scale\nclassification of visually rich documents. In IJCAI,\npages 3360–3366.\nNoah Siegel, Nicholas Lourie, Russell Power, and\nWaleed Ammar. 2018. Extracting scientific figures\nwith distantly supervised neural networks. In JCDL,\npages 223–232.\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke,\nand Alexander A Alemi. 2017. Inception-v4,\nInception-ResNet and the impact of residual connec-\ntions on learning. In AAAI, pages 4278–4284.\nGuozhi Tang, Lele Xie, Lianwen Jin, Jiapeng Wang,\nJingdong Chen, Zhen Xu, Qianying Wang, Yaqiang\nWu, and Hui Li. 2021. MatchVIE: Exploiting match\nrelevancy between entities for visual information ex-\ntraction. In IJCAI, pages 1039–1045.\nJiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi\nTang, Jiaxin Zhang, Shuaitao Zhang, Qianying Wang,\nYaqiang Wu, and Mingxiang Cai. 2021a. Towards\nrobust visual information extraction in real world:\nNew dataset and novel solution. In AAAI, volume 35,\npages 2738–2745.\nJiapeng Wang, Tianwei Wang, Guozhi Tang, Lianwen\nJin, Weihong Ma, Kai Ding, and Yichao Huang.\n2021b. Tag, copy or predict: A unified weakly-\nsupervised learning framework for visual informa-\ntion extraction using sequences. In IJCAI, pages\n1082–1090.\nSaining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu,\nand Kaiming He. 2017. Aggregated residual transfor-\nmations for deep neural networks. In CVPR, pages\n1492–1500.\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu\nWei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\nZhang, Wanxiang Che, et al. 2021a. LayoutLMv2:\nMulti-modal pre-training for visually-rich document\nunderstanding. In ACL.\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu\nWei, and Ming Zhou. 2020. LayoutLM: Pre-training\nof text and layout for document image understanding.\nIn ACM-SIGKDD, pages 1192–1200.\nYiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yi-\njuan Lu, Dinei Florencio, Cha Zhang, and Furu Wei.\n2021b. LayoutXLM: Multimodal pre-training for\nmultilingual visually-rich document understanding.\narXiv preprint arXiv:2104.08836.\nXiao Yang, Ersin Yumer, Paul Asente, Mike Kraley,\nDaniel Kifer, and C Lee Giles. 2017. Learning to ex-\ntract semantic structure from documents using multi-\nmodal fully convolutional neural networks. In CVPR,\npages 5315–5324.\nWenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and\nRong Xiao. 2021. PICK: Processing key information\nextraction from documents using improved graph\nlearning-convolutional networks. In ICPR, pages\n4363–4370.\nPeng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu,\nJing Lu, Liang Qiao, Yi Niu, and Fei Wu. 2020.\nTRIE: End-to-end text reading and information ex-\ntraction for document understanding. In ACM-MM,\npages 1413–1422.\n7756\nAppendix\nA Dataset Details\nFUNSD FUNSD (Jaume et al., 2019) is an En-\nglish dataset for form understanding in noisy\nscanned documents. It contains 199 real, fully\nannotated, scanned forms where 9,707 semantic\nentities are annotated above 31,485 words. The\n199 samples are split into 149 for training and 50\nfor testing. We directly use the official OCR anno-\ntations. The semantic entity recognition (SER) task\nis assigning to each word a semantic entity label\nfrom a set of four predefined categories: question,\nanswer, header, or other. The entity-level F1 score\nis used as the evaluation metric (Table 2).\nCORD CORD (Park et al., 2019) is an English\nreceipt dataset for key information extraction. Its\npublicly available subset includes 800 receipts for\nthe training set, 100 for the validation set, and 100\nfor the test set. A photo and a list of OCR anno-\ntations are equipped for each receipt. The dataset\ndefines 30 fields under 4 categories and the task\naims to label each word to the right field. The eval-\nuation metric is the entity-level F1 score, as shown\nin Table 3. We use the official OCR annotations.\nEPHOIE EPHOIE (Wang et al., 2021a) is col-\nlected from actual Chinese examination papers\nwith the diversity of text types and layout distri-\nbution. The 1,494 samples are divided into a train-\ning set with 1,183 images and a testing set with\n311 images, respectively. It defines ten entity cate-\ngories, and we provide the entity-level F1 score for\nRoBERTa, LayoutXLM and LiLT in Table 4. The\nofficial OCR annotations are adopted.\nRVL-CDIP RVL-CDIP (Harley et al., 2015) con-\nsists of 400,000 gray-scale images of English doc-\numents, with 8:1:1 for the training set, validation\nset, and test set. A multi-class single-label classifi-\ncation task is defined on RVL-CDIP. The images\nare categorized into 16 classes, with 25,000 images\nper class. The evaluation metric is the overall clas-\nsification accuracy as shown in Table 5. Text and\nlayout information are extracted by TextIn API.\nXFUND XFUND (Xu et al., 2021b) is a multilin-\ngual form understanding dataset that contains 1,393\nfully annotated forms with seven languages includ-\ning Chinese (ZH), Japanese (JA), Spanish (ES),\nFrench (FR), Italian (IT), German (DE), and Por-\ntuguese (PT). Each language includes 199 forms,\nwhere the training set includes 149 forms, and the\ntest set includes 50 forms. We focus on the seman-\ntic entity recognition (SER) and relation extraction\n(RE) tasks defined in the original paper (Xu et al.,\n2021b). Relation extraction aims to predict the re-\nlation between any two given semantic entities, and\nwe mainly focus on the key-value relation extrac-\ntion. We use the official OCR results, and the same\nF1 accuracy evaluation metric as in LayoutXLM\n(Xu et al., 2021b) for Table 6, 7 and 8.\nB Fine-tuning Details\nFine-tuning for Semantic Entity Recognition\nWe conduct the semantic entity recognition task\non FUNSD, CORD, EPHOIE and XFUND. We\nbuild a token-level classification layer above the\noutput representations to predict the BIO tags for\neach entity field.\nFine-tuning for Document Classification This\ntask depends on high-level visual information,\nthereby we leverage the image features explicitly\nin the fine-tuning stage, following LayoutLMv2\n(Xu et al., 2021a). We pool the visual feature of\nthe ResNeXt101-FPN (Xie et al., 2017; Lin et al.,\n2017) backbone into a global feature, concatenate\nit with the [CLS] output feature, and feed them\ninto the final classification layer.\nFine-tuning for Relation Extraction We build\nthe additional head for relation extraction on the\nFUNSD and XFUND datasets following (Xu et al.,\n2021b) for fair comparison. We first incrementally\nconstruct the set of relation candidates by produc-\ning all possible pairs of given semantic entities. For\nevery pair, the representation of the head/tail entity\nis the concatenation of the first token vector in each\nentity and the entity type embedding obtained with\na specific type embedding layer. After respectively\nprojected by two FFN layers, the representations\nof head and tail are concatenated and then fed into\na bi-affine classifier.\n7757"
}