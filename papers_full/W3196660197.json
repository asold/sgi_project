{
  "title": "Scaled ReLU Matters for Training Vision Transformers",
  "url": "https://openalex.org/W3196660197",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097016312",
      "name": "Pichao Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2106529478",
      "name": "Xue Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2055855224",
      "name": "Hao Luo",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2152163647",
      "name": "Zhou Jingkai",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2125264181",
      "name": "Zhipeng Zhou",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2080195193",
      "name": "Fan Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097763185",
      "name": "Hao Li",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2100565684",
      "name": "Rong Jin",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3091401866",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2137911812",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W6784853309",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2769994766",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2724359148",
    "https://openalex.org/W6687888618",
    "https://openalex.org/W6794880463",
    "https://openalex.org/W3173631098",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W3122542623",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W4309611266",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3188427387",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3170188883",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3192174868",
    "https://openalex.org/W4287330514",
    "https://openalex.org/W2127265926",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W2965497096",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3164540605",
    "https://openalex.org/W4287126759",
    "https://openalex.org/W4295308583",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W3155420132",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3172757681",
    "https://openalex.org/W2963695615",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3190216403",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963047834",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3035164673",
    "https://openalex.org/W3159732141",
    "https://openalex.org/W3167597877",
    "https://openalex.org/W4287203089",
    "https://openalex.org/W3169938586",
    "https://openalex.org/W3170974475",
    "https://openalex.org/W4308536459",
    "https://openalex.org/W3159885121",
    "https://openalex.org/W4287285959",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W4288586689",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3098045837",
    "https://openalex.org/W3181848549",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3180659574",
    "https://openalex.org/W3212756788",
    "https://openalex.org/W3134608633",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3177183540",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W4287274255",
    "https://openalex.org/W3169769133",
    "https://openalex.org/W2616050959",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3168489096",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2131172946",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W4214669216",
    "https://openalex.org/W4287646898",
    "https://openalex.org/W3135094205",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "Vision transformers (ViTs) have been an alternative design paradigm to convolutional neural networks (CNNs). However, the training of ViTs is much harder than CNNs, as it is sensitive to the training parameters, such as learning rate, optimizer and warmup epoch. The reasons for training difficulty are empirically analysed in the paper Early Convolutions Help Transformers See Better, and the authors conjecture that the issue lies with the patchify-stem of ViT models. In this paper, we further investigate this problem and extend the above conclusion: only early convolutions do not help for stable training, but the scaled ReLU operation in the convolutional stem (conv-stem) matters. We verify, both theoretically and empirically, that scaled ReLU in conv-stem not only improves training stabilization, but also increases the diversity of patch tokens, thus boosting peak performance with a large margin via adding few parameters and flops. In addition, extensive experiments are conducted to demonstrate that previous ViTs are far from being well trained, further showing that ViTs have great potential to be a better substitute of CNNs.",
  "full_text": "Scaled ReLU Matters for Training Vision Transformers\nPichao Wang*, Xue Wang*, Hao Luo, Jingkai Zhou, Zhipeng Zhou, Fan Wang, Hao Li, Rong Jin\nAlibaba Group\n{pichao.wang,xue.w}@alibaba-inc.com\n{michuan.lh,zhoujingkai.zjk,yuer.zzp,fan.w,lihao.lh,jinrong.jr}@alibaba-inc.com\nAbstract\nVision transformers (ViTs) have been an alternative design\nparadigm to convolutional neural networks (CNNs). How-\never, the training of ViTs is much harder than CNNs, as it\nis sensitive to the training parameters, such as learning rate,\noptimizer and warmup epoch. The reasons for training dif-\nﬁculty are empirically analysed in the paper Early Convolu-\ntions Help Transformers See Better, and the authors conjec-\nture that the issue lies with the patchify-stem of ViT models.\nIn this paper, we further investigate this problem and extend\nthe above conclusion: only early convolutions do not help for\nstable training, but the scaled ReLU operation in the convolu-\ntional stem (conv-stem) matters. We verify, both theoretically\nand empirically, that scaled ReLU in conv-stem not only im-\nproves training stabilization, but also increases the diversity\nof patch tokens, thus boosting peak performance with a large\nmargin via adding few parameters and ﬂops. In addition, ex-\ntensive experiments are conducted to demonstrate that previ-\nous ViTs are far from being well trained, further showing that\nViTs have great potential to be a better substitute of CNNs.\nIntroduction\nVisual recognition has been dominated by convolutional\nneural networks (CNNs) (He et al. 2016; Howard et al.\n2017; Zhang et al. 2018; Tan and Le 2019; Li et al. 2021a;\nZhou et al. 2021c) for years, which effectively impose spa-\ntial locality and translation equivalence. Recently the pre-\nvailing vision transformers (ViTs) are regarded as an alter-\nnative design paradigm, which target to replace the inductive\nbias towards local processing inherent in CNNs with global\nself-attention (Dosovitskiy et al. 2020; Touvron et al. 2020;\nWang et al. 2021b; Fan et al. 2021).\nDespite the appealing potential of ViTs for complete data-\ndriven training, the lack of convolution-like inductive bias\nalso challenges the training of ViTs. Compared with CNNs,\nViTs are sensitive to the choice of optimizer, data augmen-\ntation, learning rate, training schedule length and warmup\nepoch (Touvron et al. 2020, 2021; Chen, Hsieh, and Gong\n2021; Xiao et al. 2021). The reasons for training difﬁculty\nare empirically analysed in (Xiao et al. 2021), and the au-\nthors conjecture that the issue lies with the patchify stem of\n*The ﬁrst two authors contribute equally.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nViT models and propose that early convolutions help trans-\nformers see better. Recent works (Graham et al. 2021; Guo\net al. 2021; Yuan et al. 2021c) also introduce the conv-stem\nto improve the robustness of training vision transformer, but\nthey lack the deep analysis why such conv-stem works.\nIn this paper, we theoretically and empirically verify\nthat scaled ReLU in the conv-stem matters for the robust\nViTs training. Speciﬁcally, scaled ReLU not only improves\nthe training stabilization, but also increases the diversity\nof patch tokens, thus boosting the ﬁnal recognition perfor-\nmances by a large margin. In addition, extensive experi-\nments are conducted to further unveil the effects of conv-\nstem and the following interesting observations are made:\nﬁrstly, after adding conv-stem to the ViTs, the SAM opti-\nmizer (Foret et al. 2020) is no longer powerful as reported\nin (Chen, Hsieh, and Gong 2021); secondly, withconv-stem,\nthe supervised ViTs (Touvron et al. 2020) are better than its\ncorresponding self-supervised trained models (Caron et al.\n2021) plus supervised ﬁnetuning on Imagenet-1k; thirdly,\nusing conv-stem the better trained ViTs improve the perfor-\nmance of downstream tasks. All of these observations reﬂect\nthat previous ViTs are far from being well trained and ViTs\nmay become a better substitute for CNNs.\nRelated Work\nConvolutional neural networks (CNNs). Since the break-\nthrough performance on ImageNet via AlexNet (Krizhevsky,\nSutskever, and Hinton 2012), CNNs have become a dom-\ninant architecture in computer vision ﬁeld. Following the\nprimary design rule of stacking low-to-high convolutions\nin series by going deeper, many popular architectures are\nproposed, such as VGG (Simonyan and Zisserman 2014),\nGoogleNet (Szegedy et al. 2015) and ResNet (He et al.\n2016). To further exploit the capacity of visual repre-\nsentation, many innovations have been proposed, such as\nResNeXt (Xie et al. 2017), SENet (Hu, Shen, and Sun 2018),\nEfﬁcientNet (Tan and Le 2019) and NFNet (Brock et al.\n2021). For most of these CNNs, Conv+BN+ReLU becomes\na standard block. In this paper, we investigate this basic\nblock for training vision transformers as a lightweight stem.\nVision Transformers (ViTs) . Since Dosovitskiy et\nal. (Dosovitskiy et al. 2020) ﬁrst successfully applies trans-\nformer for image classiﬁcation by dividing the images\ninto non-overlapping patches, many ViT variants are pro-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2495\nposed (Wang et al. 2021b; Liu et al. 2021; Zhang et al. 2021;\nXie et al. 2021; Gao et al. 2021; Rao et al. 2021; Zhou et al.\n2021b; El-Nouby et al. 2021; Wang et al. 2021c; Xu et al.\n2021; Zhou et al. 2021d). In this section, we mainly review\nseveral closely related works for training ViTs. Speciﬁcally,\nDeiT (Touvron et al. 2020) adopts several training tech-\nniques (e.g. truncated normal initialization, strong data aug-\nmentation and smaller weight decay) and uses distillation to\nextend ViT to a data-efﬁcient version; T2T ViT (Yuan et al.\n2021b), CeiT (Yuan et al. 2021a), and CvT (Wu et al. 2021)\ntry to deal with the rigid patch division by introducing con-\nvolution operation for patch sequence generation to facili-\ntate the training; DeepViT (Zhou et al. 2021a), CaiT (Tou-\nvron et al. 2021), and PatchViT (Gong et al. 2021) inves-\ntigate the unstable training problem, and propose the re-\nattention, re-scale and anti-over-smoothing techniques re-\nspectively for stable training; to accelerate the convergence\nof training, ConViT (d’Ascoli et al. 2021), PiT (Heo et al.\n2021), CeiT (Yuan et al. 2021a), LocalViT (Li et al. 2021b)\nand Visformer (Chen et al. 2021) introduce convolutional\nbias to speedup the training; LV-ViT (Jiang et al. 2021)\nadopts several techniques including MixToken and Token\nLabeling for better training and feature generation; the SAM\noptimizer (Foret et al. 2020) is adopted in (Chen, Hsieh, and\nGong 2021) to better train ViTs without strong data aug-\nmentation; KVT (Wang et al. 2021a) introduces the k-NN\nattention to ﬁlters out irrelevant tokens to speedup the train-\ning; conv-stem is adopted in several works (Graham et al.\n2021; Xiao et al. 2021; Guo et al. 2021; Yuan et al. 2021c)\nto improve the robustness of training ViTs. In this paper, we\ninvestigate the training of ViTs by using the conv-stem and\ndemonstrate several properties of conv-stem in the context\nof vision transformers, both theoretically and empirically.\nVision Transformer Architectures\nViT. ViT (Dosovitskiy et al. 2020) ﬁrst divides an input im-\nage into non-overlapping pxppatches and linearly projects\neach patch to a d-dimensional feature vector using a learned\nweight matrix. The typical patch and image size are p =\n16 and 224x224, respectively. The patch embeddings to-\ngether with added positional embeddings and a concate-\nnated classiﬁcation token are fed into a standard transformer\nencoder (Vaswani et al. 2017) followed by a classiﬁcation\nhead. Similar as (Xiao et al. 2021), we name the portion of\nViT before the transformer blocks as ViT-stem, and call the\nlinear projection (stride-p, pxpkernel) as patchify-stem.\nConv-stem. Unless otherwise speciﬁed, we adopt the\nconv-stem from VOLO (Yuan et al. 2021c). The full conv-\nstem consists of 3Conv+3BN+3ReLU+1Proj blocks, and the\nkernel sizes and strides are (7,3,3,8) and (2,1,1,8), respec-\ntively. The detailed conﬁgurations are shown in Algorithm\n1 of supplemental material. The parameters and FLOPs of\nconv-stem are slightly larger than patchify-stem. For exam-\nple, the parameters of DeiT-Small increase from 22M to\n23M, but the increase is very small as the kernel size in\nlast linear projection layer decreases from 16*16 inpatchify-\nstem to 8*8 in conv-stem. The reason why we adopt the\nVOLO conv-stem rather than that in (Xiao et al. 2021) is\nthat we want to keep the layers of encoders the same as in\nViT, but not to remove one encoder layer as in (Xiao et al.\n2021).\nViTp and ViTc. To make easy comparisons, the original\nViT model usingpatchify-stem is called ViTp. To form a ViT\nmodel with a conv-stem, we simply replace the pathify-stem\nwith conv-stem, leaving all the other unchanged, and we call\nthis ViT as ViTc. In the following sections, we theoretically\nand empirically verify that ViTc is better than ViTp in stabi-\nlizing training and diversifying the patch tokens, due to the\nscaled ReLU structure.\nScaled ReLU Structure\nIn this section, we ﬁrst introduce the Scaled ReLU structure\nand then analyze how scaled ReLU stabilizes training and\nenhances the token diversiﬁcation respectively.\nFor any input x, we deﬁned the scaled ReLU structure\nwith scaling parameters \u000b;\f, ReLU\u000b;\f(·) for shorthand, as\nfollow:\nReLU\u000b;\f(x) =\fmax {x+ \u000b;0}:\nThe scaled ReLU structure can be achieved by combining\nReLU with normalization layers, such as Batchnorm or Lay-\nernorm that contain trainable scaling parameters, and one\ncan view the Batchnorm + ReLU in the conv-stem as a vari-\nant of the scaled ReLU. Intuitively, the ReLU layer may cut\nout part of input data and make the data focus on a smaller\nrange. It is necessary to scale it up to a similar data range as\nof its input, which helps stabilize training as well as main-\ntain promising expression power. For simplicity, we will fo-\ncus on the scaled ReLU in this paper and our analysis could\nbe extended to the case with commonly used normalization\nlayers.\nTraining Stabilization\nLet’s assumeXi;c ∈RHW be the output of channel cin the\nCNN layer from the last conv-stem block for i-th sample,\nwhere H and W are height and width. Based on the deﬁni-\ntion of the Batchnorm, the outputXout\ni;c of the last conv-stem\nis\nXout\ni;c = ReLU\n\n Xi;c −\u0016ce\n√∑B\ni=1 ∥Xi;c −\u0016ce∥2\n\fc + \u000bce\n\n\n= ReLU\u000bc\n\fc;\fc\n\n Xi;c −\u0016ce\n√∑B\ni=1 ∥Xi;c −\u0016ce∥2\n\n\n= ReLU\u000bc\n\fc;\fc( ~Xi;c); (1)\nwhere ~Xi;c = Xi;c−\u0016ce√PB\ni=1 ∥Xi;c−\u0016ce∥2 , \u0016c is the mean of Xi;c\nwithin a batch and B is the batch size. Next, we concate-\nnate Xout\ni;c over channel as Xout\ni and reshape it to Xin\ni ∈\nRB×n×d, where n is the token (patch) length and d is the\nembedding dimension. Finally, we compute Qi;Ki;Vi as\nfollow:\n[Qi Ki Vi] =Xin\ni [WQ WK WV] := Xin\ni Wtrans\n2496\nand start to run the self attention.\nTo illustrate how the scaled ReLU can stabilize training,\nwe consider a special case which we freeze all parameters\nexcept the scaling parameters \u000bc;\fc for c = 1;2;:::;C in\nthe last batchnorm layer, and WQ, WK and WV in the ﬁrst\ntransformer block. Note that Q, K and V are computed by\nthe production of Xin and Wtrans. In order to maintain the\nsame magnitude of Q, K and V, Wtrans will be closer to 0\nif Xin is scaled with larger \u000bc and \fc parameters. In other\nwords, the Scaled ReLU may give the Wtrans an implicit\nregularization with respect to its scaling parameters. The re-\nsult is summarized in the following Theorem 1.\nTheorem 1 Let \u000bc, \fc be the parameters in scaled ReLU\nstructures in the last conv-stem block with c = 1;2;:::;C\nand Wtrans\n:= [WQ WK WV] be the attention param-\neters in the ﬁrst transformer block. If we freeze all other\nparameters and introduce the l2 weight decay in the op-\ntimizer, then the optimization problem is equivalent to the\nweighted l1 penalized learning on Wtrans. Moreover, let\nWtrans;c be the parameters associated with channel c and\nthe penalty weights corresponding to Wtrans;c are propor-\ntional to\n√\n\f2c + \u000b2c.\nThe theorem shows an implicit l1 regularization on atten-\ntion weights from the scaled ReLU structure. In the modern\nhigh-dimensional statistics, it is well known thatl1 penalized\nlearning introduces signiﬁcantly less model bias (e.g., expo-\nnentially better dimensionality efﬁciency shown in Loh and\nWainwright 2015). Moreover, the regularization strength\nthat is on the order ofO(\n√\n\u000b2c + \f2c) differs from channel to\nchannel and changes over time adaptively. For the channel\nwith larger magnitude in \u000bc and/or \fc, the scaled token has\nhigher divergence. In order to make the training processing\nmore stable, the updates for the corresponding parameters in\nWtrans need also be more careful (using larger penalties). It\ndistinguishes the scaled ReLU structure from directly using\nl1 weights decay in the optimizer directly.\nProof of Theorem 11 We denote the loss function as fol-\nlow:\nmin 1\nn\nn∑\ni=1\nKL\n(\nf({ReLU\u000bc\n\fc;\fc( ~Xi;c)};Wtrans);yi\n)\n+ \u0015\n( C∑\nc=1\n(\u000b2\nc + \f2\nc) +∥Wtrans∥2\nF\n)\n;\nwhere KL(·) is the KL-divergence, yi is the label for i-th\nsample, f(·) denotes prediction function, \u0015is a positive con-\nstant for l2 weight decays and {ReLU\u000bc;\fc( ~Xi;c)}is the set\nof ReLU\u000bc;\fc( ~Xi;c) over all channels. Without loss of gen-\nerality, we can ﬁnd a function gto rewrite f function as:\nf\n({\nReLU\u000bc\n\fc;\fc( ~Xi;c)\n}\n;Wtrans\n)\n= g\n({\nReLU\u000bc\n\fc;\fc( ~Xi;c)Wtrans;c\n})\n;\n1The similar analysis procedure for implicit regularization are\nalso presented in (Ergen et al. 2021; Neyshabur, Tomioka, and Sre-\nbro 2014; Savarese et al. 2019).\nwhere we rearrange Wtrans;c to match the dimensions of the\nconv-stem (i.e., C×HW instead of n×d).\nNext, we can re-scale the parameters with \u0011c > 0 as fol-\nlow:\n~\fc = \u0011c\fc;~\u000bc = \u0011c\u000bc; ~Wtrans;c = \u0011−1\nc Wtrans;c;\nand it implies\ng\n({\nReLU\u000bc\n\fc;\fc( ~Xi;c)Wtrans;c\n})\n= g\n({\nReLU~\u000bc\n~\fc\n;~\fc\n( ~Xi;c) ~Wtrans;c\n})\n:\nMoreover, using the fact that(a2 +b2)+ c2 ≥2|c|\n√\na2 + b2\none can verify\nC∑\nc=1\n(~\u000b2\nc + ~\f2\nc) +∥~Wtrans∥2\nF\n=\nC∑\nc=1\n~\u000b2\nc + ~\f2\nc + ∥~Wtrans;c∥2\n≥2\nC∑\nc=1\n∥\u0011−1\nc Wtrans;c∥1\n√\n\u00112c\u000b2c + \u00112c\f2c\nHW (2)\n= 2√\nHW\nC∑\nc=1\n∥Wtrans;c∥1\n√\n\u000b2c + \f2c; (3)\nwhere the equality (2) holds when\n\u0011c =\n√\n∥Wtrans;c∥1\n\u000b2c + \f2c\n; c = 1;2;:::;C: (4)\nTherefore the right hand-size of (3) becomes thel1 penalties\nover the Wtrans;c with weights\n√\n\u000b2c + \f2c, i.e., WQ, WK\nand WV are l1 penalized over the input channels with\ndifferent strength. \u0003\nRemark 1. The analysis of Theorem 1 is also capable of\ncombining the ReLU + Layernorm or Batchnorm + ReLU\n+ MLP structures. In some types of transformer models, the\ntokens will ﬁrst go through Layernorm or be projected via\nMLP before entering the self-attention. Via the similar anal-\nysis, we can also show the adaptive implicitl1 regularization\nin these two settings.\nTokens Diversiﬁcation\nNext, we demonstrate the scaled ReLU’s token diversiﬁca-\ntion ability by consine similarity. Following (Gong et al.\n2021) the consine similarity metric is deﬁned as:\nCosSim(B) = 1\nn(n−1)\n∑\ni̸=j\nBiBT\nj\n∥Bi∥∥Bj∥; (5)\nwhere Birepresents the i-th row of matrixBand ∥·∥denotes\nthe l2 norm. Note that if we can ensure ∥Bi∥> bmin for\ni= 1;2;:::;n , the CosSim(B) will in turn be upper bounded\n2497\nby\nCosSim(B) ≤ 1\nn(n−1)b2\nmin\n∑\ni̸=j\nBiBT\nj\n= 1\nn(n−1)b2\nmin\n[\neTBBTe−\n∑\ni\n∥Bi∥2\n2\n]\n≤ 1\nn−1\n(\n∥B∥2\nop\nb2\nmin\n−1\n)\n; (6)\nwhere ∥·∥op denotes matrix operator norm. Based on (6),\nas long as ∥B∥op and bmin change at the same order, the\nconsine similarity may decease. In the following Theorem\n2, we analyze the order of ∥B∥op and mini∥Bi∥.\nTheorem 2 Let Dbe a zero mean probability distribution\nand matrix A ∈ Rn×d be a matrix ﬁlled whose elements\nare drawn independently from Dand ∥A∥op ≤R\n√\nndwith\nR >0. Furthermore, we denote B = ReLU\u000b;\f(A), \u0016B =\nE[Bi;j] and \u001b2\nB = Var[Bi;j] for all i;j. Then for \u000e >0 and\n\r ∈(0;c0) , with probability 1 −\u000e−2 exp(−cc−2\n0 \r2d+\nlog n), we have\n∥B∥op ≤O\n(\n\u0016log\n(1\n\u000e\n)\n+ \u001b\n√\nlog\n(1\n\u000e\n))\nand\nmin\ni\n∥Bi∥2 ≥O\n(√\n\u00162 + (1−\r)\u001b2\n)\n;\nwhere c;c0 are positive constants, O(·) suppresses the de-\npendence in n;d and R.\nThe above result shows that the operator norm and l2\nnorm for each row of the token matrix after scaled ReLU\nis proportional to its element-wise mean and standard devia-\ntion. Given the identity transformation (i.e.,B = A) is a spe-\ncial case of the scaled ReLU, matrix A(token matrix before\nscaled ReLU) enjoys the similar properties. As the ReLU\ntruncates the negative parts of its input, one has\u0016B ≥\u0016A. If\nwe could maintain the same variance level in Band A, both\nmini∥Bi∥2 and ∥B∥op change at order of O(\u0016+ \u001b) and\naccording to inequality (6), the cosine similarity becomes\nsmaller from Ato B.\nProof of Theorem 2:\nUpper Bound for ∥B∥op. Denote E ∈Rn×d as the ma-\ntrix ﬁlled with 1 and X = B −\u0016E. We have E[X] = 0,\n∥X∥op ≤(\fR+ \f\u000b+ \u0016)\n√\nndalmost surely. Via the matrix\nBernstein inequality (e.g., Theorem 1.6 in Tropp 2012),\nP\n(\n∥X∥op ≥t\n)\n≤(n+ d) exp\n( −t2=2\n\u001b2max + Rmaxt=3\n)\n;\n(7)\nwhere\n\u001b2\nmax = max{∥E[XXT]∥op;∥E[XTX]∥op}\n= max{n\u001b2;d\u001b2}≤(n+ d)\u001b2\nRmax ≥∥X∥op = (\fR+ \f\u000b+ \u0016)\n√\nnd:\nBy setting \u000e= (n+ d) exp\n(\n−t2=2\n\u001b2\nmax+Rmaxt=3\n)\n, we can repre-\nsent tby \u000eas:\nt= 1\n3Rmax log\n(n+ d\n\u000e\n)\n+\n√\n1\n9R2max log2\n(n+ d\n\u000e\n)\n+ 2\u001b2max log\n(n+ d\n\u000e\n)\n≤2\n3Rmax log\n(n+ d\n\u000e\n)\n+\n√\n2\u001b2max log\n(n+ d\n\u000e\n)\n;\nwhere last inequality uses the fact that\n√\na+ b ≤\n√\n|a|+√\n|b|.\nThen inequality (7) implies the following result holds with\nprobability 1 −\u000e:\n∥X∥op ≤2\n3Rmax log\n(n+ d\n\u000e\n)\n+\n√\n2\u001b2max log\n(n+ d\n\u000e\n)\n:\n(8)\nNext, combine (8) with the facts∥B∥op−∥\u0016E∥op ≤∥X∥op\nand ∥\u0016E∥op = \u0016\n√\nnd, one has\n∥B∥op ≤O\n(\n\u0016log\n(1\n\u000e\n)\n+ \u001b\n√\nlog\n(1\n\u000e\n))\n;\nwhere we ignore the dependence in n;d and R.\nLower Bound for ∥Bi∥. Next, we derive the bound for\n∥Bi∥. Since ∥A∥op is upper bounded, there exists a con-\nstant c0 such that B2\nij −\u00162 −\u001b2 being centered c0\u001b2 sub-\nexponential random variable. Then we are able to apply the\nCorollary 5.17 in Vershynin 2010, there exists c > 0, for\n\u0011 >0:\nP\n\n\n⏐⏐\n⏐⏐⏐⏐\nd∑\nj\nB2\nij −d(\u00162 + \u001b2)\n⏐⏐⏐⏐\n⏐⏐\n≥\u0011d\n\n\n≤2 exp\n(\n−cmin\n{ \u00112\nc2\n0\u001b4 ; \u0011\nc0\u001b2\n}\nd\n)\n:\nWe then set \u0011 = \r\u001b2 for some \r ∈(0;c0) such that \u00162 +\n(1 −\r)\u001b2 > 0. Combining ∥Bi∥2 = ∑d\nj B2\nij with above\ninequality, we have\nP\n(\n∥Bi∥≤\n√\nd(\u00162 + (1−\r)\u001b2)\n)\n≤2 exp\n(\n−c\r2c−2\n0 d\n)\n:\nTherefore via union bound, we have\nmin\ni\n∥Bi∥≥\n√\nd(\u00162 + (1−\r)\u001b2) =O(\n√\n\u00162 + (1−\r)\u001b2)\nholds with probability 1 −2 exp(−c\r2c−2\n0 d+ logn).\n\u0003\nExperiments\nIn this section, we conduct extensive experiments to verify\nthe effects of conv-stem and scaled ReLU. The ImageNet-\n1k (Russakovsky et al. 2015) is adopted for standard training\nand validation. It contains 1.3 million images in the training\nset and 50K images in the validation set, covering 1000 ob-\nject classes. The images are cropped to 224×224.\n2498\nmodel lr optimizer wm-ep Top1 acc\nViTp\nDeiT-Small\n5e-4 AdamW 5 79.8\n1e-3 AdamW 5 crash\n1e-3 AdamW 20 80.0\n5e-4 SAM 5 79.9\n1e-3 SAM 5 79.6\n1e-4 SAM 5 77.8\nViTc\nDeiT-Small\n5e-4 AdamW 5 81.6\n1e-3 AdamW 5 81.9\n1e-3 AdamW 20 81.7\n1e-3 AdamW 0 crash\n5e-4 SAM 5 81.5\n1e-3 SAM 5 81.7\n1e-4 SAM 5 79.1\nTable 1: The effects of conv-stem using different learning\nrate (lr), optimizer, warmup epoch (wm-ep).\nThe Effects of Conv-Stem\nWe take DeiT-Small (Touvron et al. 2020) as our baseline,\nand replace the patchify-stem with conv-stem. The batch-\nsize is 1024 for 8 GPUs, and the results are as shown in\nTable 1. From the Table we can see that conv-stem based\nmodel is capable with more volatile training environment:\nwith patchify-stem, ViTp can not support larger learning rate\n(1e-3) using AdamW optimizer but only works by using\nSAM optimizer, which reﬂects ViT p is sensitive to learn-\ning rate and optimizer. By adding the conv-stem, ViTc can\nsupport larger learning rate using both AdamW and SAM\noptimizers. Interestingly, ViTc achieves 81.9 top-1 accuracy\nusing lr=1e-3 and AdamW optimizer, which is 2.1 point\nhigher than baseline. With conv-stem, SAM is no longer\nmore powerful than AdamW, which is a different conclu-\nsion as in (Chen, Hsieh, and Gong 2021). After addingconv-\nstem, it still needs warmup, but 5 epochs are enough and\nlonger warmup training does not bring any beneﬁt.\nThe Effects of Scaled ReLU in Conv-Stem\nWe adopt three vision transformer architectures, includ-\ning both supervised and self-supervised methods, to eval-\nuate the value of scaled ReLU for training ViTs, namely,\nDeiT (Touvron et al. 2020), DINO (Caron et al. 2021) and\nVOLO (Yuan et al. 2021c). For DeiT and VOLO, we fol-\nlow the ofﬁcial implementation and training settings, only\nmodifying the parameters listed in the head of Table 2; for\nDINO, we follow the training settings for 100 epoch and\nshow the linear evaluation results as top-1 accuracy. The\nresults are shown in Table 2. From the Table we can see\nthat scaled ReLU (BN+ReLU) plays a very important role\nfor both stable training and boosting performance. Specif-\nically, without ReLU the training will be crashed under 5\nwarmup epoch in most cases, for both AdamW and SAM\noptimizers; increasing warmup epoch will increase the sta-\nbilization of training with slightly better results; with scaled\nReLU, it can boost the ﬁnal performance largely in stable\ntraining mode. The full conv-stem boosts the performance\nof DeiT-Small largely, 2.1 percent compared with the base-\nline, but by removing ReLU or scaled ReLU the perfor-\nmance will decrease largely; the same trend holds for both\nDINO and VOLO. For thepatchify-stem, after adding ReLU\nor scaled ReLU it can stabilize the training by supporting a\nlarge learning rate. In addition, scaled ReLU has faster con-\nvergence speed. For DeiT-Small, the top-1 accuracy is 18.1\nvs 10.6 at 5 epoch, 53.6 vs 46.8 at 20 epoch, 63.8 vs 60.9 at\n50 epoch, for conv-stem and patchify-stem, respectively.\nScaled ReLU Diversiﬁes Tokens\nTo analyze the property of scaled ReLU diversifying tokens,\nwe adopt the quantitative metric layer-wise cosine similar-\nity between tokens as deﬁned in formula 5.\nWe regard theconv-stem as one layer and position embed-\nding as another layer in the ViT-stem, thus the total layers of\nViTc is 14 (plus 12 transformer encoder layers). The layer-\nwise cosine similarity of tokens are shown in Figure 1. From\nthe Figure we can see that position embedding can largely\ndiversify the tokens due to its speciﬁc position encoding for\neach token. Compared with baseline (1Proj) (Touvron et al.\n2020), the full conv-stem (3Conv+3BN+3ReLU+1Proj) can\nsigniﬁcantly diversify the tokens at the lower layers to\nlearn better feature representation, and converge better at\nhigher layers for task-speciﬁc feature learning. Interestingly,\n3Conv+3ReLU+1Proj and 3Conv+1Proj+warmup20 have\nthe similar trend which reﬂects that ReLU can stabilize the\ntraining as longer warmup epochs.\n2.5 5 .0 7 .5 10 .0 12 .5\nLayer depth\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAvg. cosine similarity\n1Proj(baseline)\n3Conv+3BN+3ReLU+1Proj\n3Conv+3ReLU+1Proj\n3Conv+1Proj+warmup20\nFigure 1: Layer-wise cosine similarity of tokens for DeiT-\nSmall.\nThe Effects of Stride in Conv-Stem\nAccording to the work (Xiao et al. 2021), the stride in the\nconv-stem matters for the ﬁnal performance. We also inves-\ntigate this problem in the context of VOLO conv-stem for\nDeiT-Small. We keep the kernel size unchanged, and only\nadjust the stride and its corresponding padding. The default\nwarmup epoch is 5 unless otherwise noted. The results are\nshown in Table 3. From this Table we can see that the aver-\nage stride (2,2,2,2) is not better than (2,1,1,8), and it can not\nstabilize the training either.\n2499\nmodel lr optimizer wm-epoch components in conv-stem stride Top acc\nDeiT-Small\n1e-3 AdamW 5 3Conv+3BN+3ReLU+1Proj (2,1,1,8) 81.9\n1e-3 AdamW 5 3Conv+3BN+1Proj (2,1,1,8) crash\n1e-3 AdamW 5 3Conv+3ReLU+1Proj (2,1,1,8) 81.5\n1e-3 AdamW 5 3Conv+1Proj (2,1,1,8) crash\n1e-3 AdamW 20 3Conv+1Proj (2,1,1,8) 80.0\n1e-3 AdamW 5 3Conv+1Proj+1ReLU (2,1,1,8) 79.9\n1e-3 AdamW 5 1Proj+1BN+1ReLU (16) 79.8\n1e-3 AdamW 5 1Proj+1ReLU (16) 79.5\n1e-3 AdamW 5 1Proj (16) crash\n5e-4 AdamW 5 1Proj (baseline) (16) 79.8\n1e-3 SAM 5 3Conv+3BN+3ReLU+1Proj (2,1,1,8) 81.7\n1e-3 SAM 5 3Conv+3BN+1Proj (2,1,1,8) 80.2\n1e-3 SAM 5 3Conv+3ReLU+1Proj (2,1,1,8) 80.6\n1e-3 SAM 5 3Conv+1Proj (2,1,1,8) crash\n1e-3 SAM 20 3Conv+1Proj (2,1,1,8) 80.4\n1e-3 SAM 5 3Conv+1Proj+1ReLU (2,1,1,8) 80.3\nDINO-S/16\n100 epoch\n5e-4 AdamW 10 3Conv+3BN+3ReLU+1Proj (2,1,1,8) 76.0\n5e-4 AdamW 10 3Conv+3BN+1Proj (2,1,1,8) 73.4\n5e-4 AdamW 10 3Conv+3ReLU+1Proj (2,1,1,8) 74.8\n5e-4 AdamW 10 3Conv+1Proj (2,1,1,8) 74.1\n5e-4 AdamW 10 1Proj+1ReLU (16) 73.6\n5e-4 AdamW 10 1Proj+1BN+1ReLU (16) 73.3\n5e-4 AdamW 10 1Proj (baseline) (16) 73.6\nVOLO-d1-224\n1.6e-3 AdamW 20 3Conv+3BN+3ReLU+1Proj (2,1,1,4) 84.1\n1.6e-3 AdamW 20 3Conv+3BN+1Proj (2,1,1,4) 83.6\n1.6e-3 AdamW 20 3Conv+3ReLU+1Proj (2,1,1,4) 84.0\n1.6e-3 AdamW 20 3Conv+1Proj (2,1,1,4) crash\n1.6e-3 AdamW 20 1Proj 8 83.4\n1.6e-3 AdamW 20 1Proj+1ReLU 8 83.4\n1.6e-3 AdamW 20 1Proj+1BN+1ReLU 8 83.5\nTable 2: The effects of scaled ReLU under different settings using three methods.\ncomponents in conv-stem stride Top1 acc\n3Conv+3BN+3ReLU+1Proj (2,1,1,8) 81.9\n3Conv+3BN+3ReLU+1Proj (2,2,2,2) 81.0\n3Conv+1Proj (2,1,1,8) crash\n3Conv+1Proj (2,2,2,2) crash\n3Conv+1Proj (wm-epoch=20) (2,1,1,8) 80.0\n3Conv+1Proj (wm-epoch=20) (2,2,2,2) 79.7\n3Conv+1Proj+1ReLU (2,1,1,8) 79.9\n3Conv+1Proj+1ReLU (2,2,2,2) 79.9\nTable 3: The effects of stride in conv-stem for DeiT-Small.\nTransfer Learning: Object ReID\nIn this section, we transfer the DINO-S/16 (100 epoch) on\nImageNet-1k to object ReID to further demonstrate the ef-\nfects of conv-stem. We ﬁne-tune the DINO-S/16 shown in\nTable 2 on Market1501 (Zheng et al. 2015) and MSMT17\n(Wei et al. 2018) datasets. We follow the baseline (He et al.\n2021) and follow the standard evaluation protocol to report\nthe Mean Average Precision (mAP) and Rank-1 accuracies.\nAll models are trained with the baseline learning rate (1.6e-\n3) and a larger learning rate (5e-2). The results are shown in\nTable 4. From the Table we can see that the full conv-stem\nnot only achieves the best performance but also supports\nboth the large learning rate and small learning rate training.\nWithout ReLU or BN+ReLU, in most cases, the ﬁnetuning\nwith a large learning rate will crash. Interestingly, the ﬁne-\ntuning with DINO is sensitive to the learning rate, a smaller\nlearning rate will achieve better performance.\nScaled ReLU/GELU in Transformer Encoder\nIn transformer encoder layer, the feed-forward layers (ffn)\nadopt LayerNorm+GELU block, and in this section, we in-\nvestigate this design using DeiT-Small (DeiT-S) and VOLO-\nd1-224, using the training parameters for the best perfor-\nmance in Table 2. The motivation to investigate ReLU and\nGELU is to show whether GELU is better than ReLU for\nconv-stem design, as GELU achieves better results than\nReLU for transformer encoder. We ﬁrst remove the Layer-\nNorm layer in ffn, the training directly crashes in the ﬁrst\nfew epochs. And then, we replace the GELU with RELU, the\nperformance drops largely, which reﬂects that GELU is bet-\nter than ReLU for ffn. Next, we replace ReLU with GELU\nin conv-stem, the performance drops a little bit, demonstrat-\ning that ReLU is better than GELU for conv-stem. Lastly,\n2500\nwe rewrite the MLP implementation in ffn by replacing the\nfc+act block with Conv1D+BN1D+GELU (Conv1D equals\nto fc, and the full implementation is shown in supplemen-\ntal material of Algorithm 2), and the performance drops,\nespecially for VOLO. It might conﬁrm the conclusion in\nNFNet (Brock et al. 2021) that batch normalization con-\nstrains the extreme performance, making the network sub-\noptimal. All the results are shown in Table 5.\nMarket1501 MSMT17\ncomponents in conv-stem lr mAP R-1 mAP R-1\n3Conv+3BN+3ReLU+1Proj\n1.6*\ne-3\n84.3 93.5 56.3 78.7\n3Conv+3BN+1Proj 83.6 92.9 55.1 77.8\n3Conv+3ReLU+1Proj 81.7 91.9 51.5 75.2\n3Conv+1Proj 83.0 92.7 52.1 74.0\n1Proj+1ReLU 84.2 93.1 53.6 75.5\n1Proj+1BN+1ReLU 84.1 92.8 55.7 77.5\n1Proj (baseline) 84.1 93.1 54.9 76.8\n3Conv+3BN+3ReLU+1Proj\n5*\ne-2\n76.8 89.7 48.5 72.1\n3Conv+3BN+1Proj crash\n3Conv+3ReLU+1Proj crash\n3Conv+1Proj crash\n1Proj+1ReLU 69.5 86.1 36.1 36.0\n1Proj+1BN+1ReLU 77.6 90.6 46.2 88.6\n1Proj (baseline) crash\nTable 4: The comparisons with different components in\nconv-stem based on DINO for ﬁnetuning ReID tasks.\nSelf-Supervised + Supervised Training\nWe adopt the DINO self-supervised pretrained ViT-Small\nmodel (Caron et al. 2021) on ImageNet-1k and use it to\ninitialize the ViT-Small model to ﬁnetune on ImageNet-1k\nusing full labels. The results are shown in Table 6. From\nthis Table we can see that using a self-supervised pretrained\nmodel for initialization, ViT p achieve 81.6 top-1 accuracy\nusing SAM optimizer, which is 1.8 percent point higher than\nbaseline. However, according to the analysis in (Newell and\nDeng 2020), with large labelled training data like Imagenet-\n1k dataset, the two stage training strategy will not contribute\nmuch (below 0.5 percent point). By adding conv-stem, the\npeak performance of ViT c can reach 81.9 which is higher\nthan two stage training, which reﬂects that previous ViTs\nmodels are far from being well trained.\nScaled Dataset Training\nWe adopt the DINO pretrained ViT-Small model (Caron\net al. 2021) on ImageNet-1k to initialize the ViT-Small\nmodel, and ﬁnetune on ImageNet-1k using portion of full\nlabels. We adopt the original patchify-stem and SAM opti-\nmizer for this investigation. The results are shown in Table 7.\nIt can be seen that even using self-supervised pretrained\nmodel for initialization, using only 10% of ImageNet-1k\ndata for training, it only achieves 67.8% accuracy, much\nworse than the linear classiﬁcation accuracy using full data\n(77.0%) (Caron et al. 2021). With the data-size increasing,\nthe performance improves obviously, and we do not see\nany saturation in the data-size side. This performance partly\nmodel design Top1 acc\nDeiT-Sc\nLayerNorm removed in ffn crash\nGELU→ReLU in ffn 80.3(1.6↓)\nReLU→GELU in conv-stem 81.7(0.2↓)\nMLP→Conv1D+BN+GELU 81.7(0.2↓)\nMLP→Conv1D+GELU 82.0(0.1↑)\nVOLO-d1c\nLayerNorm removed in ffn crash\nGELU→ReLU in ffn 83.5(0.6↓)\nReLU→GELU in conv-stem 84.0(0.1↓)\nMLP→Conv1D+BN+GELU 83.2(0.9↓)\nMLP→Conv1D+GELU 84.0(0.1↓)\nTable 5: The comparisons among different designs using\nscaled ReLU/GELU.\nmodel lr optimizer wm-ep Top1 acc\nDeiT-Smallp\nTST\n1e-4 AdamW 5 81.2\n5e-4 AdamW 5 81.3\n1e-3 AdamW 5 80.1\n1e-4 SAM 5 81.6\n5e-4 SAM 5 81.1\n1e-3 SAM 5 80.1\nDeiT-Smallc\nOST\n1e-3 AdamW 5 81.9\n1e-3 SAM 5 81.7\nTable 6: The comparisons between two-stage training (TST,\nself-supervised + supervised training) and only supervised\ntraining (OST) on ImageNet-1k.\ndemonstrates that ViT is powerful in ﬁtting data and current\nViT models trained on ImageNet-1k is not trained enough.\nmodel lr optimizer datasize Top1 acc\nDeiT-Smallp\nTST\n1e-4 SAM 10% 67.8\n1e-4 SAM 20% 73.5\n1e-4 SAM 30% 76.0\n1e-4 SAM 40% 77.6\n1e-4 SAM 50% 79.0\n1e-4 SAM 60% 79.8\n1e-4 SAM 70% 80.4\n1e-4 SAM 80% 80.9\n1e-4 SAM 90% 81.4\n1e-4 SAM 100% 81.6\nTable 7: The comparisons among different portion of\nImageNet-1k for two-stage training (TST, self-supervised +\nsupervised training) training.\nConclusion\nIn this paper, we investigate the training of ViTs in the con-\ntext of conv-stem. We theoretically and empirically verify\nthat the scaled ReLU in the conv-stem matters for robust\nViTs training. It can stabilize the training and improve the\ntoken diversity for better feature learning. Extensive exper-\niments unveil the merits of conv-stem and demonstrate that\nprevious ViTs are not well trained even if they obtain better\nresults in many cases compared with CNNs.\n2501\nReferences\nBrock, A.; De, S.; Smith, S. L.; and Simonyan, K. 2021.\nHigh-performance large-scale image recognition without\nnormalization. arXiv preprint arXiv:2102.06171.\nCaron, M.; Touvron, H.; Misra, I.; J ´egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging Proper-\nties in Self-Supervised Vision Transformers. arXiv preprint\narXiv:2104.14294.\nChen, X.; Hsieh, C.-J.; and Gong, B. 2021. When\nVision Transformers Outperform ResNets without Pre-\ntraining or Strong Data Augmentations. arXiv preprint\narXiv:2106.01548.\nChen, Z.; Xie, L.; Niu, J.; Liu, X.; Wei, L.; and Tian, Q.\n2021. Visformer: The Vision-friendly Transformer. arXiv\npreprint arXiv:2104.12533.\nd’Ascoli, S.; Touvron, H.; Leavitt, M.; Morcos, A.; Biroli,\nG.; and Sagun, L. 2021. ConViT: Improving Vision Trans-\nformers with Soft Convolutional Inductive Biases. arXiv\npreprint arXiv:2103.10697.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nEl-Nouby, A.; Touvron, H.; Caron, M.; Bojanowski, P.;\nDouze, M.; Joulin, A.; Laptev, I.; Neverova, N.; Synnaeve,\nG.; Verbeek, J.; et al. 2021. XCiT: Cross-Covariance Image\nTransformers. arXiv preprint arXiv:2106.09681.\nErgen, T.; Sahiner, A.; Ozturkler, B.; Pauly, J.; Mardani,\nM.; and Pilanci, M. 2021. Demystifying Batch Normal-\nization in ReLU Networks: Equivalent Convex Optimiza-\ntion Models and Implicit Regularization. arXiv preprint\narXiv:2103.01499.\nFan, H.; Xiong, B.; Mangalam, K.; Li, Y .; Yan, Z.; Malik, J.;\nand Feichtenhofer, C. 2021. Multiscale vision transformers.\narXiv preprint arXiv:2104.11227.\nForet, P.; Kleiner, A.; Mobahi, H.; and Neyshabur, B. 2020.\nSharpness-aware Minimization for Efﬁciently Improving\nGeneralization. In International Conference on Learning\nRepresentations.\nGao, P.; Lu, J.; Li, H.; Mottaghi, R.; and Kembhavi, A. 2021.\nContainer: Context Aggregation Network. arXiv preprint\narXiv:2106.01401.\nGong, C.; Wang, D.; Li, M.; Chandra, V .; and Liu, Q.\n2021. Improve Vision Transformers Training by Suppress-\ning Over-smoothing. arXiv preprint arXiv:2104.12753.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J´egou, H.; and Douze, M. 2021. LeViT: a Vision Trans-\nformer in ConvNet’s Clothing for Faster Inference. arXiv\npreprint arXiv:2104.01136.\nGuo, J.; Han, K.; Wu, H.; Xu, C.; Tang, Y .; Xu, C.; and\nWang, Y . 2021. CMT: Convolutional Neural Networks Meet\nVision Transformers. arXiv preprint arXiv:2107.06263.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang, W.\n2021. Transreid: Transformer-based object re-identiﬁcation.\narXiv preprint arXiv:2102.04378.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking spatial dimensions of vision transformers.\narXiv preprint arXiv:2103.16302.\nHoward, A. G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang,\nW.; Weyand, T.; Andreetto, M.; and Adam, H. 2017. Mo-\nbilenets: Efﬁcient convolutional neural networks for mobile\nvision applications. arXiv preprint arXiv:1704.04861.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation\nnetworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 7132–7141.\nJiang, Z.; Hou, Q.; Yuan, L.; Zhou, D.; Jin, X.; Wang, A.;\nand Feng, J. 2021. Token Labeling: Training a 85.5% Top-1\nAccuracy Vision Transformer with 56M Parameters on Im-\nageNet. arXiv preprint arXiv:2104.10858.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classiﬁcation with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25: 1097–1105.\nLi, D.; Hu, J.; Wang, C.; Li, X.; She, Q.; Zhu, L.; Zhang,\nT.; and Chen, Q. 2021a. Involution: Inverting the Inher-\nence of Convolution for Visual Recognition. arXiv preprint\narXiv:2103.06255.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021b. LocalViT: Bringing Locality to Vision Transformers.\narXiv preprint arXiv:2104.05707.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLoh, P.-L.; and Wainwright, M. J. 2015. Regularized M-\nestimators with nonconvexity: Statistical and algorithmic\ntheory for local optima. The Journal of Machine Learning\nResearch, 16(1): 559–616.\nNewell, A.; and Deng, J. 2020. How useful is self-\nsupervised pretraining for visual tasks? In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7345–7354.\nNeyshabur, B.; Tomioka, R.; and Srebro, N. 2014. In search\nof the real inductive bias: On the role of implicit regulariza-\ntion in deep learning. arXiv preprint arXiv:1412.6614.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh,\nC.-J. 2021. DynamicViT: Efﬁcient Vision Transform-\ners with Dynamic Token Sparsiﬁcation. arXiv preprint\narXiv:2106.02034.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. International journal of computer vision, 115(3):\n211–252.\n2502\nSavarese, P.; Evron, I.; Soudry, D.; and Srebro, N. 2019.\nHow do inﬁnite width bounded norm networks look in func-\ntion space? In Conference on Learning Theory, 2667–2690.\nPMLR.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSzegedy, C.; Liu, W.; Jia, Y .; Sermanet, P.; Reed, S.;\nAnguelov, D.; Erhan, D.; Vanhoucke, V .; and Rabinovich, A.\n2015. Going deeper with convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, 1–9.\nTan, M.; and Le, Q. 2019. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, 6105–6114. PMLR.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2020. Training data-efﬁcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877.\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and\nJ´egou, H. 2021. Going deeper with Image Transformers.\narXiv preprint arXiv:2103.17239.\nTropp, J. A. 2012. User-friendly tail bounds for sums of ran-\ndom matrices. Foundations of computational mathematics,\n12(4): 389–434.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In NIPS.\nVershynin, R. 2010. Introduction to the non-\nasymptotic analysis of random matrices. arXiv preprint\narXiv:1011.3027.\nWang, P.; Wang, X.; Wang, F.; Lin, M.; Chang, S.; Xie, W.;\nLi, H.; and Jin, R. 2021a. KVT: k-NN Attention for Boosting\nVision Transformers. arXiv preprint arXiv:2106.00515.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021b. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122.\nWang, W.; Yao, L.; Chen, L.; Cai, D.; He, X.; and Liu, W.\n2021c. CrossFormer: A Versatile Vision Transformer Based\non Cross-scale Attention. arXiv preprint arXiv:2108.00154.\nWei, L.; Zhang, S.; Gao, W.; and Tian, Q. 2018. Per-\nson transfer gan to bridge domain gap for person re-\nidentiﬁcation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 79–88.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. arXiv preprint arXiv:2103.15808.\nXiao, T.; Singh, M.; Mintun, E.; Darrell, T.; Doll ´ar, P.; and\nGirshick, R. 2021. Early Convolutions Help Transformers\nSee Better. arXiv preprint arXiv:2106.14881.\nXie, J.; Zeng, R.; Wang, Q.; Zhou, Z.; and Li, P. 2021. So-\nViT: Mind Visual Tokens for Vision Transformer. arXiv\npreprint arXiv:2104.10935.\nXie, S.; Girshick, R.; Doll´ar, P.; Tu, Z.; and He, K. 2017. Ag-\ngregated residual transformations for deep neural networks.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 1492–1500.\nXu, Y .; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.;\nZhang, L.; Xu, C.; and Sun, X. 2021. Evo-ViT: Slow-Fast\nToken Evolution for Dynamic Vision Transformer. arXiv\npreprint arXiv:2108.01390.\nYuan, K.; Guo, S.; Liu, Z.; Zhou, A.; Yu, F.; and Wu,\nW. 2021a. Incorporating Convolution Designs into Visual\nTransformers. arXiv preprint arXiv:2103.11816.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Tay, F. E.;\nFeng, J.; and Yan, S. 2021b. Tokens-to-token vit: Train-\ning vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986.\nYuan, L.; Hou, Q.; Jiang, Z.; Feng, J.; and Yan, S. 2021c.\nV olo: Vision outlooker for visual recognition.arXiv preprint\narXiv:2106.13112.\nZhang, P.; Dai, X.; Yang, J.; Xiao, B.; Yuan, L.; Zhang, L.;\nand Gao, J. 2021. Multi-Scale Vision Longformer: A New\nVision Transformer for High-Resolution Image Encoding.\narXiv preprint arXiv:2103.15358.\nZhang, X.; Zhou, X.; Lin, M.; and Sun, J. 2018. Shufﬂenet:\nAn extremely efﬁcient convolutional neural network for mo-\nbile devices. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 6848–6856.\nZheng, L.; Shen, L.; Tian, L.; Wang, S.; Wang, J.; and Tian,\nQ. 2015. Scalable person re-identiﬁcation: A benchmark. In\nProceedings of the IEEE international conference on com-\nputer vision, 1116–1124.\nZhou, D.; Kang, B.; Jin, X.; Yang, L.; Lian, X.; Hou, Q.; and\nFeng, J. 2021a. DeepViT: Towards Deeper Vision Trans-\nformer. arXiv preprint arXiv:2103.11886.\nZhou, D.; Shi, Y .; Kang, B.; Yu, W.; Jiang, Z.; Li, Y .;\nJin, X.; Hou, Q.; and Feng, J. 2021b. Reﬁner: Reﬁn-\ning Self-attention for Vision Transformers. arXiv preprint\narXiv:2106.03714.\nZhou, J.; Jampani, V .; Pi, Z.; Liu, Q.; and Yang, M.-H.\n2021c. Decoupled Dynamic Filter Networks. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 6647–6656.\nZhou, J.; Wang, P.; Wang, F.; Liu, Q.; Li, H.; and Jin, R.\n2021d. ELSA: Enhanced Local Self-Attention for Vision\nTransformer. arXiv preprint arXiv:2112.12786.\n2503",
  "topic": "FLOPS",
  "concepts": [
    {
      "name": "FLOPS",
      "score": 0.7863423824310303
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.7504198551177979
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6414958238601685
    },
    {
      "name": "Transformer",
      "score": 0.601146399974823
    },
    {
      "name": "Computer science",
      "score": 0.590666651725769
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5336006283760071
    },
    {
      "name": "Machine learning",
      "score": 0.42227569222450256
    },
    {
      "name": "Voltage",
      "score": 0.20066964626312256
    },
    {
      "name": "Engineering",
      "score": 0.12521126866340637
    },
    {
      "name": "Electrical engineering",
      "score": 0.07061266899108887
    },
    {
      "name": "Parallel computing",
      "score": 0.0634889304637909
    }
  ]
}