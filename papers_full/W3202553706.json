{
    "title": "BiTr-Unet: A CNN-Transformer Combined Network for MRI Brain Tumor Segmentation",
    "url": "https://openalex.org/W3202553706",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3203014467",
            "name": "Qiran Jia",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2118405160",
            "name": "Hai Shu",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A3203014467",
            "name": "Qiran Jia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2118405160",
            "name": "Hai Shu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6959738644",
        "https://openalex.org/W6973769828",
        "https://openalex.org/W2751069891",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W4212875960",
        "https://openalex.org/W3151682712",
        "https://openalex.org/W2947263797",
        "https://openalex.org/W2912989244",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W3168825659",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3096764502",
        "https://openalex.org/W1641498739",
        "https://openalex.org/W2076462394",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3155189856",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3203841574",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W3204255739",
        "https://openalex.org/W3086354526",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3178989684",
        "https://openalex.org/W3134689216",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3137561054",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4289489408",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3107408919",
        "https://openalex.org/W4301860726",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W4287100534",
        "https://openalex.org/W3094502228"
    ],
    "abstract": null,
    "full_text": "BiTr-Unet: a CNN-Transformer Combined\nNetwork for MRI Brain Tumor Segmentation\nQiran Jia and Hai Shu (\u0000 )\nDepartment of Biostatistics, School of Global Public Health, New York University,\nNew York, NY 10003, USA\nhs120@nyu.edu\nAbstract. Convolutional neural networks (CNNs) have achieved re-\nmarkable success in automatically segmenting organs or lesions on 3D\nmedical images. Recently, vision transformer networks have exhibited ex-\nceptional performance in 2D image classiﬁcation tasks. Compared with\nCNNs, transformer networks have an appealing advantage of extract-\ning long-range features due to their self-attention algorithm. Therefore,\nwe propose a CNN-Transformer combined model, called BiTr-Unet, with\nspeciﬁc modiﬁcations for brain tumor segmentation on multi-modal MRI\nscans. Our BiTr-Unet achieves good performance on the BraTS2021 val-\nidation dataset with median Dice score 0.9335, 0.9304 and 0.8899, and\nmedian Hausdorﬀ distance 2.8284, 2.2361 and 1.4142 for the whole tu-\nmor, tumor core, and enhancing tumor, respectively. On the BraTS2021\ntesting dataset, the corresponding results are 0.9257, 0.9350 and 0.8874\nfor Dice score, and 3, 2.2361 and 1.4142 for Hausdorﬀ distance. The code\nis publicly available at https://github.com/JustaTinyDot/BiTr-Unet.\nKeywords: Brain Tumor, Deep Learning, Multi-modal Image Segmen-\ntation, Vision Transformer.\n1 Introduction\nAs one of the most complicated tasks in computer vision, automated biomed-\nical image segmentation plays an important role in disease diagnosis and fur-\nther treatment planning. It aims to act like experienced physicians to identify\ntypes of tumors and delineate diﬀerent sub-regions of organs on medical images\nsuch as MRI and CT scans [27,15]. Earlier segmentation systems were based\non traditional approaches such as edge detection ﬁlters, mathematical methods,\nand machine learning algorithms, but the heavy computational complexity hin-\ndered their progress [9]. In recent years, remarkable breakthroughs have been\nmade to computer hardware and deep learning. Deep learning is a derivation of\nmachine learning, which implements multiple processing layers to build a deep\nneural network to extract representations from data by mimicking the working\nmechanism of human brains [12]. Deep learning has succeeded in various dif-\nﬁcult tasks including image classiﬁcation and speech recognition, and also has\nbecome the most prevailing method for automated biomedical image segmen-\ntation [9,11,17,19]. While many challenges and limitations such as the shortage\narXiv:2109.12271v2  [eess.IV]  30 Dec 2021\n2 Q. Jia and H. Shu\nFig. 1. Visualization of one case from the BraTS2021 training data.\nof annotated data persist, deep learning exhibits its superiority over traditional\nsegmentation methods in processing speed and segmentation accuracy [9].\nTo promote the development of biomedical image segmentation techniques,\nmany relevant challenges and conferences are held each year for researchers to\npropose innovative algorithms and communicate new discoveries. Year 2021 is\nthe tenth anniversary of the Brain Tumor Segmentation Challenge (BraTS),\nwhich has been dedicated to being the venue of facilitating the state-of-the-art\nbrain glioma segmentation algorithms [1,16,4,3,2].Due to privacy protection is-\nsues, biomedical image data are notoriously diﬃcult to obtain and usually in\na comparatively small scale. However, this year the number of the well-curated\nmulti-institutional multi-parametric MRI scans of glioma provided by the or-\nganizer have been updated from 660 to 2000, which largely beneﬁts setting\nthe new benchmark and exploring the potential of algorithms[1]. The train-\ning dataset provides four 3D MRI modalities, including native T1-weighted\n(T1), post-contrast T1-weighted (T1c), T2-weighted (T2), and T2 Fluid Atten-\nuated Inversion Recovery (T2-FLAIR), together with the “ground-truth” tumor\nsegmentation labeled by physicians; see Fig. 1. For the validation and testing\ndatasets, the “ground-truth” tumor segmentation is not open to participants.\nThe ranking criterion in BraTS2021 consists of the Dice score, Hausdorﬀ dis-\ntance (95%), Sensitivity, and Speciﬁcity for the respective segmentation of the\nwhole tumor, tumor core, and enhancing tumor.\nTitle Suppressed Due to Excessive Length 3\nThe invention of U-Net, which supplements the contracting network by suc-\ncessive layers to build an encoder-decoder with skip connections architecture,\nsuccessfully makes the feature extraction in biomedical images more precise and\neﬃcient [18]. U-Net has become the baseline network that is frequently modi-\nﬁed for better performance in BraTS ever since, and one of U-Net’s modiﬁed\nversions, nnU-Net, took the ﬁrst place in BraTS2020 [10], which again proves\nthe excellent performance of the fully convolutional network and the structure\nof successive upsampling layers. Some other top-ranked networks in BraTS2020\nalso add innovative modules such as the variational autoencoder (VAE), at-\ntention gate (AG), as well as Convolutional Block Attention Module (CBAM)\ninto U-net like structures to regularize the feature extraction process and boost\ngeneralization performance [10,8,15]. Overall, it is generally acknowledged that\nmodifying and enhancing U-Net architecture is an eﬃcient and elegant way to\nobtain good results for biomedical image segmentation.\nAnother important network structure, Transformer [22], which displays suc-\ncessful improvement on performances of networks in the Natural Language Pro-\ncessing (NLP) realm, is also proved to be useful in the domain of computer vision.\nVision Transformer (ViT) [6] proposes the idea that divides an image into several\nequivalent patches and then applies the self-attention mechanism of Transformer\nto each patch to capture the long-range dependencies. Compared with networks\nthat are built solely on convolutional layers, ViT overcomes their limitation of\nlocality and therefore makes predictions with more considerations. To boost the\nperformance of ViT and continually set the state-of-the-art benchmark of image\nclassiﬁcation tasks, various modiﬁcations of ViT have been proposed [21,5,25].\nFurthermore, to alleviate the high computational cost of the original Transformer\non high-resolution images, the architecture design of CNN is introduced to the\nnetworks that adopt Transformer [7,23].\nEven though many works have been done to apply Transformer to the com-\nputer vision realm, only a small number of researches on utilizing Transformer to\n3D biomedical image segmentation are available. NVIDIA proposed UNETR [7],\nwhich uses a modiﬁed ViT as the encoder, and adopts the successive upsampling\nlayers from U-Net as the decoder. Since there are 12 Transformer layers in the en-\ncoder of UNETR, the sequence representations of diﬀerent layers are upsampled\nby CNN and then concatenated with the decoder layers to create the skip con-\nnections. Another Transformer-based network designed for 3D biomedical image\nsegmentation is TransBTS [23], which explores another possibility of incorporat-\ning Transformer into CNN-based network. Like a conventional U-Net, there are\ndownsampling and upsampling layers with skip connections in TranBTS, but the\nunique part is that at the bottom of the network, the encoder and decoder are\nconnected by the following 4 types of layers: a linear projection layer, a patch\nembedding layer to transfer the image tensor to sequence data, ViT layers to\nextract long-range dependencies, and a feature mapping layer to ﬁt the sequence\ndata back to the 3D CNN decoder. The beneﬁt of arranging ViT in this way\ninto the U-Net architecture is that at the end of the 3D CNN encoder the ViT\ncan compute long-range dependencies with a global receptive ﬁeld.\n4 Q. Jia and H. Shu\nGiven that several studies have incorporated Transformer into the task of\n3D medical image segmentation and yielded satisfactory results, we believe that\nsuch a type of algorithms has the potential to succeed in BraTS2021. TransBTS\nand UNETR both displayed great potentials to be applied to this challenge\nwith speciﬁc modiﬁcations and improvement, but TransBTS’s arrangement of\nthe ViT layers is more suitable for the BraTS data (we will discuss it in detail\nin the Method section). Therefore, we decide to borrow the backbone and the\ncore idea from TransBTS to propose a reﬁned version for BraTS2021, called Bi\nTransformer U-Net (BiTr-Unet). The preﬁx “Bi” means that we have two sets\nof ViT layers in our modiﬁed model while TransBTS only has one.\n2 Method\n2.1 Basic Structure of BiTr-Unet\nFig. 2 provides an overview of the proposed BiTr-Unet, which contains the main\ncharacteristics and core backbone of TransBTS. Overall, the proposed deep neu-\nral network f takes the multi-modal MRI scans X as the input, and outputs the\npredicted segmentation image Y , which can be simply written as\nY = f(X). (1)\nFig. 2. The basic architecture of BiTr-Unet.\nTitle Suppressed Due to Excessive Length 5\nPreprocessing The BraTS MRI scans with four modalities including T1, T1c,\nT2, and T2-FLAIR, and the extra “ground-truth” segmentation label for the\ntraining data are stored as separate NIfTI ﬁles (.nii.gz). For each patient, the\nfour NIfTI ﬁles of MRI images are imported as NumPy arrays and stacked\nalong a new axis to recreate the volumetric information in one NumPy array X.\nFor the training data, the NIfTI ﬁle of segmentation label is imported as one\nNumPy array Y . Then, X, or X and Y for the training data only, will be stored\nas one Pickle ﬁle (.pkl). Details of the implementation of other conventional\npreprocessing steps are provided in the Implementation Details Section.\n3D CNN Encoder with Attention Module BiTr-Unet has an encoder with\n3D CNN downsample layers. The preprocessed input of MRIs X ∈RC×H×W×D\nis fed into the network. After the initial convolutional block with a stride of 1 to\nincrease the dimension of feature map F to 4C×H×W×D, there are four con-\nsecutive 3×3×3 convolutional blocks with a stride of 2 in the encoder to extract\nthe feature representation of the input image. The resulting output dimension of\nthe feature representation is (64 C, H\n16 , W\n16 , D\n16 ). The Convolutional Block Atten-\ntion Module (CBAM) is proposed in [24] to improve the performance of CNN\nmodels on various tasks. This eﬃcient and lightweight CBAM can be seamlessly\nintegrated into a CNN architecture for adaptive feature reﬁnement by comput-\ning attention maps. The original CBAM in [24] is designed for 2D CNN, and\nin [8] the CBAM is expanded for 3D CNN. Given the intermediate feature map\nF, 3D CBAM produces the channel attention map Mc ∈RC×1×1×1×1 and the\n3D spatial attention map Ms ∈R1×H×W×D. The process of computing the 3D\nspatial attention maps is written as\nF′= Mc(F) ⊗F, (2)\nF′′= MS(F′) ⊗F′, (3)\nwhere ⊗is the element-wise multiplication between maps, andF′′represents the\nreﬁned feature map after the CBAM block. We integrate the 3D CBAM into the\nencoder by replacing the regular 3D CNN block with 3D CBAM block.\nFeature Embedding of Feature Representation Since Transformer takes\nsequence data as input, the most common way of changing image input into\nsequence data is to divide an image into several equivalent patches. However,\nsince the input of Transformer in BiTr-Unet is the intermediate feature map\nFint ∈RK×M×M×M , a more convenient way introduced by TransBTS is used.\nFint ﬁrstly go through a 3 ×3×3 convolutional block to increase its channel di-\nmension K to d. In this way the spatial and depth dimensions of the feature\nrepresentations are ﬂattened to one dimension of size N. This process is named\nlinear projection, and the resulting feature map f has a dimension of d×N, so\nit can be regarded as N d-dimensional tokens to be input of the Transformer\nblock. For the positional embedding of these N d-dimensional tokens, TransBTS\nutilizes learnable position embeddings and adds them to the tokens directly by\nz0 = f + PE = W ×F + PE, (4)\n6 Q. Jia and H. Shu\nwhere W is the linear projection, PE ∈Rd×N is the position embeddings, and\nz0 ∈Rd×N is the whole operation of feature embeddings of the feature represen-\ntations.\nTransformer Layers Fig. 3 illustrates the workﬂow of the Transformer block\nin our model. After feature embedding, the tokenized and position embedded\nsequence of feature representations goes through the typical Transformer layers\nwith a Multi-Head Attention (MHA) block and a Feed Forward Network (FFN).\nThe number of Transformer layers L can be modiﬁed for the best result. The\noutput of the lth (l = 1, . . . , L) Transformer layer is denoted byzl and calculated\nas\nz′\nl = MHA (LN(zl−1)) + zl−1, (5)\nzl = FFN (LN(z′\nl)) + z′\nl. (6)\nFeature Mapping Since the outputs of the Transformer layers are the sequence\ndata, it should be transferred back to the intermediate feature representation\nFint ∈RK×M×M×M for the 3D CNN upsample layer. To achieve this, the output\nFig. 3. The basic structure of the Vision Transformer Block\nTitle Suppressed Due to Excessive Length 7\nzL ∈Rd×N of the Transformer layers are reshaped tozL ∈Rd×M×M×M and then\ngo through a 3 ×3×3 convolutional block to reduce the channel dimension from\nd to K. After this process, the feature representation Fint ∈RK×M×M×M is\nready for the 3D CNN upsample layer.\n3D CNN Decoder BiTr-Unet has a decoder with 3D CNN upsample layers.\nThere are four consecutive 3 ×3×3 convolutional blocks with a stride of 2, and\none ﬁnal convolutional block with a stride of 1 to decrease the dimension of the\nfeature map Ffinal to C×H×W×D in the decoder to construct the ﬁnal seg-\nmentation image. The resulting output segmentation label is Y ∈RC×H×W×D.\nSkip Connections The output of the ﬁrst three layers of the 3D CNN layers\nare directly sent to the last three layers of the 3D CNN layers to create the skip\nconnections. Unlike the output of the ﬁrst three 3D downsample CNN layers\nwhich are concatenated directly with the input of the last three 3D upsample\nCNN layers, the output of the fourth and the ﬁfth 3D downsample CNN lay-\ners go through a feature embedding of feature representation layer, transformer\nlayers, and a feature mapping layer, and then go through the corresponding 3D\nupsample CNN layer.\nPostprocessing and Model Ensemble The champion model of BraTS2020,\nnnU-Net, applies a BraTS-speciﬁc postprocessing strategy to achieve a higher\nDice score, which eliminates a volume of predicted segmentation if this volume\nis smaller than a threshold [10]. We borrow this postprocessing technique to\nmaximize the Dice score of the resulting segmentation. Majority voting is an\neﬀective and fast method of model ensemble, which may result in a signiﬁcant\nimprovement in prediction accuracy [15]. We adopt majority voting to ensemble\ndiﬀerently trained models of BiTr-Unet. Speciﬁcally, for each voxel, every model\nvotes for the voxel’s category, and the category with the highest number of votes\nis used as the ﬁnal prediction of the voxel. Given n diﬀerently trained BiTr-Unet\nmodels fi(X) ( i = 1 , . . . , n), the output of the majority voting for each j-th\nvoxel is\nC(X)[j] = mode(f1(X)[j], ..., fn(X)[j]). (7)\nIf the j-th voxel has more than one category with the highest number of votes,\nits ﬁnal predicted label is the category with the largest averaged prediction\nprobability over all trained models, i.e.,\nyj = arg max\nk∈C(X)[j]\n(pjk), (8)\nwhere pjk is the averaged prediction probability of the k-th category for the j-th\nvoxel over the n trained models.\n8 Q. Jia and H. Shu\n2.2 Comparison with Related Work\nModiﬁcations from TransBTS BiTr-Unet is a variant of TransBTS, but our\nmodiﬁcations are signiﬁcant and make the network well-prepared for BraTS2021.\nWe keep all the innovative and successful attributes of TransBTS, including the\ncombination of CNN and Transformer, feature embedding, and feature mapping.\nMeanwhile, we also notice that the CNN encoder and decoder in TransBTS\ncan be reﬁned by adding the attention module. We also increase the depth of\nthe whole network for a denser feature representation. TransBTS only utilizes\nTransformer at the end of the encoder, so it is an operation of the whole feature\nrepresentation after the fourth layer of the encoder. Since the depth of the en-\ncoder layers is increased to ﬁve, we use Transformer for both the fourth and the\nﬁfth layers. For the fourth layer, Transformer works as an operation to build a\nskip connection so that its output is concatenated with the input of the fourth\ndecoder layer.\nDiﬀerences from UNETR A recently proposed network for 3D medical im-\nage segmentation, UNETR, also uses Transformer to extract long-range spatial\ndependencies [7]. Unlike TransBTS and our BiTr-Unet, which involve 3D convo-\nlutional blocks in both encoder and decoder of the network, UNETR does not\nassign any convolutional block to the encoder. ViT [6] is designed to operate\non 2D images, and UNETR modiﬁes it 3D images but keeps the original ViT’s\ndesign that divides an image into equivalent patches and treats each patch as\none token for the operation of attention mechanism. It is an elegant way that\nusing a pure Transformer for the encoding of 3D medical image segmentation,\nbut removing convolutional blocks in the encoder may lead to the insuﬃciency in\nextracting local context information for the volumetric BraTS MRI data. More-\nover, UNETR stacks Transformer layers and keeps the sequence data dimension\nunchanged during the whole process, which results in expensive computation for\nhigh-resolution 3D images.\n3 Result\n3.1 Implementation Details\nOur BiTr-Unet model is implemented in Pytorch and trained on four NVIDIA\nRTX8000 GPUs for 7050 epochs with a batch size of 16. For the optimization,\nwe use the Adam optimizer with an initial learning rate of 0.0002. The learning\nrate decays by each iteration with a power of 0.9 for better convergence. We\nadopt this training strategy to avoid overﬁtting, but it also needs a high number\nof training epochs. The four modalities of the raw BraTS training data for each\ncase are randomly cropped from the 240 × 240 × 155 voxels to 128 ×128×128\nvoxels. We also randomly shift the intensity in the range of [-0.1, 0.1] and scale\nin the range of [0.9, 1.1]. Test Time Augmentation (TTA),which is proved to\nincrease the prediction accuracy in [20], is applied when using the trained model\nTitle Suppressed Due to Excessive Length 9\nFig. 4. Visualization of the segmentation results from the ensembled BiTr-Unet for\nthree cases in BraTS2021 Validation data.\nto generate the segmentation images of the validation data. During TTA, we\ncreate 7 extra copies of the input X by ﬂipping all the possible combinations of\ndirections H, W and D.\n3.2 Segmentation Result\nDuring the validation phase, the segmentation images of BraTS2021 validation\ndata from our BiTr-Unet are evaluated by the BraTS2021 organizer through\nthe Synapse platform ( https://www.synapse.org/#!Synapse:syn25829067/\nwiki). We validate three checkpoints with diﬀerent numbers of trained epochs\nand also validate the ensemble model of these three checkpoints. Table 1 shows\nthe mean score on the validation data. Fig. 4 presents the segmentation results\nfrom the ensembled model for three validation cases.\nIn the testing phase, we are required to encapsulate the execution environ-\nment dependencies and the trained model in an Docker image, and push the built\nDocker image into Synapse for the testing evaluation. Automated model ensem-\nble through Docker may lead to increased Docker image size, longer processing\ntime, and even unstable result. To avoid risks, we skip the model ensemble and\nonly select the 7050-epoch BiTr-Unet model for the testing Docker submission.\nTable 2 shows the detailed segmentation result of the 7050-epoch BiTr-Unet on\nBraTS2021 validation data, and Table 3 shows the result on the testing data.\nTable 1. Mean segmentation result of diﬀerently trained BiTr-Unet models on\nBraTS2021 validation data.\nMean Dice Mean Hausdorﬀ95 (mm)\nModel ET WT TC ET WT TC\n4800-epoch 0.8139 0.9070 0.8466 16.7017 4.5880 13.3968\n6050-epoch 0.8237 0.9079 0.8383 15.3014 4.5883 13.9110\n7050-epoch 0.8187 0.9097 0.8434 17.8466 4.5084 16.6893\nEnsemble by Majority Voting 0.8231 0.9076 0.8392 14.9963 4.5322 13.4592\n10 Q. Jia and H. Shu\nTable 2. Detailed segmentation result of the 7050-epoch BiTr-Unet model on\nBraTS2021 validation data.\nDice Hausdorﬀ95 (mm)\nStatistics ET WT TC ET WT TC\nMean 0.8187 0.9097 0.8434 17.8466 4.5084 16.6893\nStandard deviation 0.2327 0.08837 0.2305 73.6517 7.6621 65.5411\nMedian 0.8899 0.9366 0.9338 1.4142 2.8284 2.2361\n25th percentile 0.8289 0.8903 0.8610 1 1.4142 1\n75th percentile 0.9397 0.9584 0.9616 2.6390 4.4721 4.5826\nTable 3. Detailed segmentation result of the 7050-epoch BiTr-Unet model on\nBraTS2021 testing data.\nDice Hausdorﬀ95 (mm)\nStatistics ET WT TC ET WT TC\nMean 0.7256 0.7639 0.7422 65.2966 62.1576 69.006318\nStandard deviation 0.3522 0.3426 0.3738 139.003958 133.4915 140.8401\nMedian 0.8874 0.9257 0.9350 1.4142 3 2.2361\n25th percentile 0.7313 0.8117 0.7642 1 1.4142 1\n75th percentile 0.9512 0.9600 0.9708 3.7417 9.1647 8.2002\n4 Discussion\nWe propose a new deep neural network model called BiTr-Unet, a reﬁned ver-\nsion of TransBTS, for the BraTS2021 tumor segmentation challenge. The result\non the validation data indicates that BiTr-Unet is a stable and powerful net-\nwork to extract both local and long-range dependencies of 3D MRI scans. Com-\npared to models with U-Net alike architectures but without attention modules\nthat are successful in BraTS2020, BiTr-Unet takes the advantage of the novel\nTransformer module for potentially better performance. The way we incorpo-\nrate Transformer into a CNN encoder-decoder model is inspired by TransBTS,\nbut we propose more innovative and suitable modiﬁcations such as making fea-\nture representations of the skip connection to go through a ViT block. However,\nthere is plenty more room for further explorations to incorporate Transformer\ninto networks for 3D medical image segmentation. The result on the testing data\nshares similar median and 75th percentile with that on the validation data, but\nhas an inferior mean,standard deviation, and 25th percentile. With the fact that\ntesting data is more dissimilar to the training data than the validation data, this\ndiscrepancy result indicates that BiTr-Unet may still struggle when processing\nunseen patterns. Due to the time limit, we did not ﬁnish the experiment to test\nthe performance of the model with more encoder and decoder layers that are\nTitle Suppressed Due to Excessive Length 11\nconnected by a skip connection going through a ViT Block, or to test ViT with\ndiﬀerent embedding dimensions. Also, inspired by the recent work of innova-\ntive Transformer models for image classiﬁcation or segmentation that reduces\ncomputation complexity by special algorithms [26,13,14] , we would also try to\nintroduce them into our model in the future to improve the performance and\ndecrease the computation complexity.\n5 Acknowledgements\nThis research was partially supported by the grant R21AG070303 from the Na-\ntional Institutes of Health and a startup fund from New York University. The\ncontent is solely the responsibility of the authors and does not necessarily repre-\nsent the oﬃcial views of the National Institutes of Health or New York University.\nReferences\n1. Baid, U., Ghodasara, S., Bilello, M., Mohan, S., Calabrese, E., Colak, E., Farahani,\nK., Kalpathy-Cramer, J., Kitamura, F.C., Pati, S., et al.: The rsna-asnr-miccai\nbrats 2021 benchmark on brain tumor segmentation and radiogenomic classiﬁca-\ntion. arXiv preprint arXiv:2107.02314 (2021)\n2. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Frey-\nmann, J., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic\nfeatures for the pre-operative scans of the tcga-gbm collection (07 2017).\nhttps://doi.org/10.7937/K9/TCIA.2017.KLXWJJ1Q\n3. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,\nJ., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic features for\nthe pre-operative scans of the tcga-lgg collection. The cancer imaging archive 286\n(2017)\n4. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Freymann,\nJ.B., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma mri\ncollections with expert segmentation labels and radiomic features. Scientiﬁc data\n4(1), 1–13 (2017)\n5. Chen, C.F., Fan, Q., Panda, R.: Crossvit: Cross-attention multi-scale vision trans-\nformer for image classiﬁcation. arXiv preprint arXiv:2103.14899 (2021)\n6. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n7. Hatamizadeh, A., Yang, D., Roth, H., Xu, D.: Unetr: Transformers for 3d medical\nimage segmentation. arXiv preprint arXiv:2103.10504 (2021)\n8. Henry, T., Carre, A., Lerousseau, M., Estienne, T., Robert, C., Paragios, N.,\nDeutsch, E.: Brain tumor segmentation with self-ensembled, deeply-supervised\n3d u-net neural networks: a brats 2020 challenge solution. arXiv preprint\narXiv:2011.01045 (2020)\n9. Hesamian, M.H., Jia, W., He, X., Kennedy, P.: Deep learning techniques for medical\nimage segmentation: Achievements and challenges. Journal of Digital Imaging 32\n(05 2019). https://doi.org/10.1007/s10278-019-00227-x\n12 Q. Jia and H. Shu\n10. Isensee, F., Petersen, J., Klein, A., Zimmerer, D., Jaeger, P.F., Kohl, S.,\nWasserthal, J., Koehler, G., Norajitra, T., Wirkert, S., et al.: nnu-net: Self-\nadapting framework for u-net-based medical image segmentation. arXiv preprint\narXiv:1809.10486 (2018)\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-\nvolutional neural networks. In: Pereira, F., Burges, C.J.C., Bottou, L., Wein-\nberger, K.Q. (eds.) Advances in Neural Information Processing Systems. vol. 25.\nCurran Associates, Inc. (2012), https://proceedings.neurips.cc/paper/2012/\nfile/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n12. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521, 436–44 (05 2015).\nhttps://doi.org/10.1038/nature14539\n13. Lin, H., Cheng, X., Wu, X., Yang, F., Shen, D., Wang, Z., Song, Q., Yuan, W.:\nCat: Cross attention in vision transformer. arXiv preprint arXiv:2106.05786 (2021)\n14. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030 (2021)\n15. Lyu, C., Shu, H.: A two-stage cascade model with variational autoencoders and\nattention gates for mri brain tumor segmentation. In: Crimi, A., Bakas, S. (eds.)\nBrainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. pp.\n435–447. Springer International Publishing, Cham (2021)\n16. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor\nimage segmentation benchmark (brats). IEEE transactions on medical imaging\n34(10), 1993–2024 (2014)\n17. Noda, K., Yamaguchi, Y., Nakadai, K., Okuno, H., Ogata, T.: Audio-visual speech\nrecognition using deep learning. Applied Intelligence 42(4), 722–737 (Jun 2015).\nhttps://doi.org/10.1007/s10489-014-0629-7\n18. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n19. Shu, H., Chiang, T., Wei, P., Do, K.A., Lesslie, M.D., Cohen, E.O., Srinivasan, A.,\nMoseley, T.W., Chang Sen, L.Q., Leung, J.W.T., Dennison, J.B., Hanash, S.M.,\nWeaver, O.O.: A deep learning approach to re-create raw full-ﬁeld digital mam-\nmograms for breast density and texture analysis. Radiology: Artiﬁcial Intelligence\n3(4), e200097 (2021). https://doi.org/10.1148/ryai.2021200097\n20. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition (2015)\n21. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´ egou, H.: Training\ndata-eﬃcient image transformers & distillation through attention. In: International\nConference on Machine Learning. pp. 10347–10357. PMLR (2021)\n22. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998–6008 (2017)\n23. Wang, W., Chen, C., Ding, M., Li, J., Yu, H., Zha, S.: Transbts: Multimodal brain\ntumor segmentation using transformer. arXiv preprint arXiv:2103.04430 (2021)\n24. Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: Cbam: Convolutional block attention\nmodule. In: Proceedings of the European conference on computer vision (ECCV).\npp. 3–19 (2018)\n25. Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.: Cvt: Introduc-\ning convolutions to vision transformers. arXiv preprint arXiv:2103.15808 (2021)\nTitle Suppressed Due to Excessive Length 13\n26. Xie, Y., Zhang, J., Shen, C., Xia, Y.: Cotr: Eﬃciently bridging cnn and transformer\nfor 3d medical image segmentation. arXiv preprint arXiv:2103.03024 (2021)\n27. Zhong, L., Li, T., Shu, H., Huang, C., Johnson, J.M., Schomer, D.F., Liu, H.L.,\nFeng, Q., Yang, W., Zhu, H.: 2wm: Tumor segmentation and tract statistics for\nassessing white matter integrity with applications to glioblastoma patients. Neu-\nroImage 223, 117368 (2020)"
}