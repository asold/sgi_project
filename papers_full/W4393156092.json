{
    "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
    "url": "https://openalex.org/W4393156092",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5093823237",
            "name": "Saebom Leem",
            "affiliations": [
                "Sogang University",
                "Korea Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2226425267",
            "name": "Hyun-Seok Seo",
            "affiliations": [
                "Korea Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5093823237",
            "name": "Saebom Leem",
            "affiliations": [
                "Korea Institute of Science and Technology",
                "Sogang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1787224781",
        "https://openalex.org/W2498056627",
        "https://openalex.org/W2336525064",
        "https://openalex.org/W2765793020",
        "https://openalex.org/W3112516115",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2147800946",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2195388612",
        "https://openalex.org/W6795475546",
        "https://openalex.org/W6791479011",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2240067561",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W6674914833",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W6638319203",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2979377095",
        "https://openalex.org/W2295107390",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W1797268635",
        "https://openalex.org/W3024127982",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2599157026",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W4297812995",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W3140565983",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3176306675",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3170841864"
    ],
    "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
    "full_text": "Attention Guided CAM:\nVisual Explanations of Vision Transformer Guided by Self-Attention\nSaebom Leem1,2, Hyunseok Seo1*\n1Korea Institute of Science and Technology\n2Sogang University\ntoqha1215@sogang.ac.kr, seo@kist.kr\nAbstract\nVision Transformer(ViT) is one of the most widely used\nmodels in the computer vision field with its great perfor-\nmance on various tasks. In order to fully utilize the ViT-\nbased architecture in various applications, proper visualiza-\ntion methods with a decent localization performance are nec-\nessary, but these methods employed in CNN-based models\nare still not available in ViT due to its unique structure. In this\nwork, we propose an attention-guided visualization method\napplied to ViT that provides a high-level semantic explana-\ntion for its decision. Our method selectively aggregates the\ngradients directly propagated from the classification output\nto each self-attention, collecting the contribution of image\nfeatures extracted from each location of the input image.\nThese gradients are additionally guided by the normalized\nself-attention scores, which are the pairwise patch correla-\ntion scores. They are used to supplement the gradients on\nthe patch-level context information efficiently detected by the\nself-attention mechanism. This approach of our method pro-\nvides elaborate high-level semantic explanations with great\nlocalization performance only with the class labels. As a re-\nsult, our method outperforms the previous leading explain-\nability methods of ViT in the weakly-supervised localization\ntask and presents great capability in capturing the full in-\nstances of the target class object. Meanwhile, our method pro-\nvides a visualization that faithfully explains the model, which\nis demonstrated in the perturbation comparison test.\nIntroduction\nTransformer-based models (Vaswani et al. 2017; Devlin\net al. 2018; Liu et al. 2019; Radford et al. 2018) is a widely\nused architecture in various NLP tasks due to its superior\nperformance. Vision Transformer (ViT) (Dosovitskiy et al.\n2020) is a modified Transformer that adopts the architecture\nof BERT (Devlin et al. 2018), but is applicable to images by\nreplacing its basic unit of operation with image patches. As\na Transformer-based model, ViT applies the self-attention\nmechanism as its primary operation, sharing the advantages\nof the Transformer over other models: it significantly re-\nduces the required computational load and supports better\nparallelization. Furthermore, recent studies propose that ViT\nis better at shape recognition (Tuli et al. 2021) and shows\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nhigh robustness against occlusions and perturbations in the\ninput (Naseer et al. 2021). Exploiting these benefits, ViT and\nits derived models have achieved remarkable performance in\nnumerous vision tasks such as classification (Chen, Fan, and\nPanda 2021; Li et al. 2021; Touvron et al. 2021), object de-\ntection (Liu et al. 2021; Wang et al. 2021), and semantic seg-\nmentation (Ranftl, Bochkovskiy, and Koltun 2021; Zheng\net al. 2021). Demonstrating its high versatility and decent\nperformance, especially in large-scale image data, it is now\nconsidered as a practical alternative to Convolutional Neu-\nral Network (CNN) (He et al. 2016; LeCun et al. 1989; Si-\nmonyan and Zisserman 2014; Szegedy et al. 2015) which\nhas dominated the computer vision field for the past decade.\nDespite the notable success of ViT in computer vision, it\nstill lacks explainability. The proper methods to provide a\nvisual explanation of the model are vital to ensure the relia-\nbility of the given model. For CNN, for example, numerous\nmethods have been developed to provide a faithful explana-\ntion of the model by gradient analysis (Draelos and Carin\n2020; Selvaraju et al. 2017; Zhou et al. 2016). In addition,\nmany of the gradient-based methods have been actively uti-\nlized in weakly-supervised localization (Chattopadhay et al.\n2018; Qin, Kim, and Gedeon 2021; Yang et al. 2020). In con-\ntrast, the unique structure of ViT, such as the use of [class]\ntoken and the self-attention mechanism, makes it compli-\ncated to provide the proper explanation of the model. There-\nfore, compared to CNN, there have been fewer explainabil-\nity methods developed, including Attention rollout (Abnar\nand Zuidema 2020) and Layer-wise Relevance Propagation\n(LRP)-based method (Chefer, Gur, and Wolf 2021).\nAttention Rollout is a method developed for ViT and aims\nto provide a concise aggregation of the overall attention by\nusing the resulting matrix of self-attention operation. Al-\nthough it considers the core component of ViT architec-\nture, it assumes a linear combination of attention and over-\nlooks the influence of the MLP head, resulting in a rough\nand non-class-specific explanation of the classification deci-\nsion. On the other hand, the LRP-based method applied to\nViT provides a class-specific analysis and takes the whole\nmodel into consideration. It focuses on decomposing the\nmodel back into the level of image patches and calculates\nthe relevancy score of each patch based on the conservation\nproperty. Since both methods take the self-attention opera-\ntion into account, they are prone to the peak intensity re-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2956\nsulting from the repeated softmax operation in the sequen-\ntial self-attention module. The softmax operation tends to\namplify the local large values in the process of converting\nthe self-attention scores into probabilities. Consequently, it\ngenerates a peak intensity that highlights the specific point\nof a homogeneous background of the input image due to\nhigh self-attention scores from similar pixel intensities. As\ndemonstrated in Figure 1, Attention Rollout and LRP-based\nmethod are severely influenced by the peak intensity, re-\nsulting in poor localization performance. In contrast, our\nmethod renormalizes the self-attention scores with sigmoid,\nwhich does not affect the original prediction process, and\ntherefore is much less disturbed by the peak intensity.\nIn this work, we propose an attention-guided gradient\nanalysis that aims to improve localization performance by\ncombining the essential target gradients with the feedfor-\nward feature of the self-attention module. Specifically, to\nprovide the class activation map (CAM) of high-semantic\nexplanation, we aggregate the gradients that are directly con-\nnected to the MLP head and backpropagated along skip con-\nnections. Also, we conclude that the self-attention score rep-\nresents the patch correlation scores with a continuous pattern\nand preserves spatial position information. Therefore, we\nuse the self-attention score, which is newly normalized with\nthe sigmoid operation to alleviate peak intensities, as feature\nmaps that guide the gradients on the pattern information of\nthe image. In short, the proposed method provides the CAM\nthat represents the image features of the input combined with\ntheir contributions to the prediction of the model. This ap-\nproach achieves greater weakly-supervised localization per-\nformance with the state-of-the-art result in most evaluation\nbenchmarks. The contributions of this work are as follows:\n• We propose a gradient-based method applicable to Vision\nTransformer that fully considers the major structures of\nthe model and provides a reliable high-semantic explana-\ntion of the model.\n• The proposed method aggregates the selective gradients\nguided by the self-attention to construct a class activation\nmap (CAM) of great localization performance.\n• Our method outperforms the previous leading methods\napplied to ViT in the experiments on weakly-supervised\nlocalization. We also demonstrate the improved reliabil-\nity of our method by pixel perturbation experiment.\nRelated Works\nExplainability of a deep neural network matters because the\nblack box nature of it makes it difficult to ensure that the\nmodel is working in a proper way. Hence, there have been\nvarious methods that aim to explain the model’s inner work-\nings, but each method adopts a different idea of what it in-\ntends to explain and how it generates the explanations. For\nexample, in Attention Rollout (Abnar and Zuidema 2020)\nwhich is designed to explain the Transformer, the expla-\nnation means the amount of information propagated from\nthe first to the last self-attention module. Although it can be\neasily applied to any Transformer-based model, it does not\ntake the MLP heads into account and cannot specify how\nRaw\nAttention\nAttention\nRollout LRP-based Ours\nFigure 1: The illustration of peak intensity propagation from\nthe self-attention scores to final visualization heatmaps of\nPASCAL VOC 2012. Raw attention is a simple sum aggre-\ngation of the self-attention scores of all layers.\nmuch each captured correlation contributes to the classifica-\ntion output of a particular class. Therefore, Attention Rollout\nproduces a non-class-specific explanation and shows lower\nperformance in localization tasks for some regions that are\nunrelated to the classification output are also highlighted.\nLRP-based methods are contrived to calculate the rele-\nvancy score of the input pixel to the classification output. In\nother words, the explanation provided by LRP is the con-\ntribution of each pixel of the input image throughout the\nmodel from the input to the output. It first decomposes a\nmodel pixel-wisely typically using Deep Taylor Decompo-\nsition (DTD) framework (Montavon et al. 2017), then it cal-\nculates the relevancy of each of the pixels by propagating\nthe decomposed relevancies backward from the output to\nthe input layer. LRP-based methods have been extended to\nvarious models. Bach et al., (Bach et al. 2015) proposed\nan LRP method that can consider the nonlinearity of the\nmodel, and Binder et al., (Binder et al. 2016a) applied LRP\nto some deep neural networks including GoogLeNet and\nVGG. They, then, extend LRP to the renormalization layer\n(Binder et al. 2016b). Finally, Chefer et al., (Chefer, Gur,\nand Wolf 2021) introduced the LRP-based method applied\nto ViT by proposing the method to apply LRP to the GELU\n(Hendrycks and Gimpel 2016) layer, skip connections, and\nmatrix multiplication, which are the major operations of\nViT and calculates the relevancy score of each image patch.\nThese methods capture the contribution of each independent\nand discrete unit of the model and provide a precise expla-\nnation. However, they often result in scattered contributions\nwhich only highlight a partial area of the target class object\nbecause of approximation error in relevancy calculation and\nincomplete attention scores as shown in Figure 1.\nOn the other hand, the gradient-based methods provide\na high-semantic explanation of the model, meaning that\nthey explain the contribution of the image features elicited\nthrough multiple layers, rather than the contribution of the\nindependent pixels. The earliest gradient-based method is\nClass Activation Map (CAM) (Zhou et al. 2016), which gen-\nerates a saliency map as a result of the weighted sum of the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2957\nFigure 2: The demonstration of the ViT architecture and the major components in our method. The yellow shaded lines represent\nthe essential gradients being considered along the skip connections propagated from the classification output of the given classc,\nyc. The purple-colored boxes point to the self-attention score matrices which are the result of matrix multiplication of the query\nand the key matrices. The feature maps are these self-attention score matrices normalized with sigmoid, which are represented\nas the green boxes in each block. These feature maps are aggregated with the gradients to provide the final class activation map.\nfeature map channels of the last convolutional layer. Here,\nthe weights for each feature map channel are calculated by\na single time backward from the classification output of the\ntarget class. Grad-CAM (Selvaraju et al. 2017) proposes a\ngeneralized CAM, whose usage is not restricted to a model\nwith Global Average Pooling (GAP). Grad-CAM averages\nthe gradients of each channel to construct a class activa-\ntion map of a general CNN model with fully connected lay-\ners. HiResCAM (Draelos and Carin 2020) provides a more\nfaithful explanation by replacing the channel-wise weights\nof the Grad-CAM with the pixel-wise multiplication of the\ngradients and the feature maps. Gradient-based methods are\nalso highly utilized in weakly-supervised object localiza-\ntion with great performance. Grad-CAM++ (Chattopadhay\net al. 2018) proposed a generalized version of Grad-CAM\nwith improved localization performance, adding channel-\nwise weights to the Grad-CAM. Combinational CAM (Yang\net al. 2020) and infoCAM (Qin, Kim, and Gedeon 2021) in-\ntegrate the CAM of non-label classes to localize the target\nobject more precisely. In these gradient-based methods, the\nfeature maps reflect the interaction among the pixels from\nmultiple layers of the model and the gradients are the con-\ntributions of these high-level image features. This approach\nof the gradient-based methods that combines the feature\nmaps and their gradients intrinsically results in a continuous\nheatmap where contributions cluster together on the target\nobject and also gives an excellent capability in object local-\nization.\nMethodology\nOur method applies the gradient-based visualization tech-\nnique to ViT (Dosovitskiy et al. 2020) to generate the class\nactivation map (CAM) (Zhou et al. 2016; Selvaraju et al.\n2017) of the target class. The major components of our\nmethod are demonstrated in Figure 2. To generate a high-\nsemantic explanation of the model, we focus on the gra-\ndients from the classification output to each encoder block\nalong the backward path through the skip connection. In ad-\ndition, these essential gradients are guided by feature maps\nobtained from the newly normalized self-attention score ma-\ntrices by sigmoid. The reasons why the gradients and the\nfeature map are obtained from the self-attention blocks are\nas follows. Firstly, the attention score matrice at each block\ncontains the high-level image features elicited through the\nself-attention mechanism. Given that these image features\nrepresent pairwise patch correlations, these matrices are ap-\npropriate to be used as a feature map. Secondly, regardless of\nthe aggregation option chosen in the MLP head (e.g. [class]\ntoken or average pooling), these matrices preserve the patch\nposition information of the input image. Here in this paper,\nwe explain our method based on the original ViT model with\na [class] token.\nAccording to the ViT architecture, the input with a size\nof [(n × p) × (n × p) × 3] is flattened and converted into a\npatch embedding with a size of [N × P] before fed into the\ntransformer encoder. Here, the number of patches N equals\nn2 + 1with n2 image patches and 1 additional for [class]\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2958\nsoftmax\nsigmoid\nFigure 3:\nThe demonstration of the results of softmax and\nsigmoid operation applied on self-attention. The images\n(left) are the sum aggregation of self-attention of all layers\nand heads with each operation and the peaks are indicated\nwith red boxes. The graphs (right) are the distributions of the\nflattened self-attention scores in the left images. The peaks\nare indicated with red lines.\ntoken, and the patch embedding size P can vary but is gen-\nerally defined as [p2 ×3]. The order of the patches embedded\nin this process is maintained and therefore the positional in-\nformation of the n2 image patches is traceable throughout\nthe feedforward encoder block.\nViT architecture can be largely divided into the MLP head\nand the encoder blocks which consist of the self-attention,\nMLP, and skip connections. The patch embedding shape of\n[N ×P] is maintained at every skip connection layer and we\ndenote this matrix after the first skip connection in the kth\nencoder block as Ek\nr1 and the one after the second skip con-\nnection in thekth encoder block asEk\nr2, respectively. During\nthe multi-head self-attention operation in the kth encoder,\nthe matrix multiplication of the query and the key matrices\nresults in the self-attention score matrices with the size of\n[H×N ×N] where H is the number of heads, and we denote\nthe self-attention matrix of hth head as Ak\nh(1 ≤ h ≤ H). At\nthe end of the encoder, the MLP head produces the classifi-\ncation outputs and we denote the classification output of the\ntarget class c as yc. These matrices in the ViT feedforward\nnetwork and their notations are demonstrated in Figure 2.\nAs the feature map for gradient calculation, the self-\nattention matrices Ak\nh are used. Each element of the matri-\nces represents the pairwise patch correlation scores detected\nat each layer and head and can guide the combined gradi-\nents on meaningful pattern information. Basically, in ViT,\nthe self-attention scores are converted into the probability\nby the softmax operation. However, softmax tends to maxi-\nmize the local large values and generates some peak inten-\nsity that suppresses other important values as shown in Fig-\nure 3. Therefore, instead of softmax, we normalize the self-\nattention matrices with sigmoid, which is a monotonically\nincreasing function as well as softmax. When we denote the\nsoftmax function as S(·) and sigmoid function as G(·), the\ntwo function satisfies the following relation:\n∀x, y∈ R, S(x) < S(y) =⇒ G(x) < G(y) (1)\nAt the same time, the sigmoid effectively recovers the\nmedium correlations that are lost in softmax. The effect of\nreplacing softmax with sigmoid is represented in Figure 3.\nTo prevent misunderstanding, we clarify that normalization\nwith sigmoid does not affect any backpropagation process of\nthe original ViT structure, and sigmoid of Ak\nh is calculated\nafter the model finishes learning.\nNote that in ViT with [class] token, only the first rows of\nEk\nr1, Ek\nr2 and Ak\nh (i.e., Ek\nr1,1, Ek\nr2,1 and Ak\nh,1, respectively)\nare considered. The MLP head is only connected with the\n[class] token at the end of the last encoder block, EK\nr2,1,\nwhich does not contain the positional information of the im-\nage patches itself. However, the positional information con-\nnected to this token can be traced back to the first rows of the\nself-attention matrices Ak\nh,1s since all operations from Ak\nh,is\nalong the skip connections are applied row-wisely where i\nstands for ith row component. Also due to the skip connec-\ntion, the MLP head is directly connected not only to AK\nh,1 in\nthe last encoder block but also to all Ak\nh,1s in the previous\nblocks, as shown in Figure 2. Therefore, the feature mapFk\nh\nin our method consists of the first-row components of Ak\nh,is\nnormalized with sigmoid operation and is defined as:\nFk\nh = G(Ak\nh,1) (2)\nThe feature maps Fk\nh s are generated in the green boxes in\nFigure 2.\nTo produce a complete CAM, the gradients, which rep-\nresent the influence of Ak\nh,1s of each block, should be com-\nbined with the feature maps. In thekth encoder block except\nfor the last encoder block (i.e., , k < K), the gradient di-\nrectly connected from the MLP head towards Ek\nr1,1 is prop-\nagated along the first skip connection in the (k + 1)th block\ntowards Ek\nr2,1. By doing so, the gradient from the MLP head\ncan be propagated to Ek−1\nh,1 as well as to Ek\nh,1. The skip con-\nnection consists of a residual operation, a simple addition of\ntwo matrices, with no effect on the gradient passing through\nit. Therefore, for the first skip connection in the (k + 1)th\nencoder block connected to Ek\nr2,1, we get the mathematical\nrelation as follows:\n∂Ek+1\nr1,1\n∂Ek\nr2\n,1\n= I (3)\nThen let us denote the gradient propagated from the output\nyc to the matrix Ek\nr1,1 in the kth encoder block along the\nskip connection path as βk,c. The gradient βk,c is defined\nas:\nβk,c =\n\n\n\n∂yc\n∂Ek\nr1,1\n, k = K\nβk+1\n,c ·\n∂Ek+1\nr1,1\n∂Ek\nr2,\n1\n·\n∂Ek\nr2,1\n∂Ek\nr1,\n1\n, k < K\n(4)\nFrom Eqs. 3 and 4, we can get:\nβk,c =\n\n\n\n∂yc\n∂Ek\nr1,\n1\n, k = K\nβk+1,c ·\n∂Ek\nr2,1\n∂Ek\nr1,1\n,\nk < K\n(5)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2959\nFigure 4: The illustration of how the one-dimensional matrix\nLc\n1 is reshaped into a two-dimensional class activation map.\nSince the self-attention matrices pass through a softmax\nlayer in each block, the gradients αk,c\nh that are propagated to\neach feature map Fk\nh are defined as:\nαk,c\nh = βk,c · ∂Ek\nr1,1\n∂F k\nh\n= βk,c · ∂Ek\nr1,1\n∂S(Ak\nh,1) ·\n∂S(Ak\nh,1)\n∂G(Ak\nh,1) (6)\nHowever, the gradients still cause the peak-amplification ef-\nfect since they contain the weights propagated from softmax.\nIn other words, the peak amplification effect occurs due to\nlarge elements in\n∂S(Ak\nh,1)\n∂G(Ak\nh,1) . However, if the general attention\nscores are assumed to have a smooth varying property (i.e.,\n∂S(Ak\nh,1)\n∂G(Ak\nh,1) ≈ 1), the gradients propagated to eachS(Ak\nh,1) ap-\nproximate the gradients propagated to each G(Ak\nh,1), which\ncan be formulated as:\nαk,c\nh = βk,c · ∂Ek\nr1,1\n∂S(Ak\nh,1) · I (7)\nand it is proved in the supplementary material. Due to Equa-\ntion 7, our method still faithfully explains the model. The\nfinal gradients αk,c\nh are demonstrated in the yellow-shaded\nline in Figure 2.\nFinally, the class activation map Lc of the given class c\ncan be formulated as:\nLc =\nKX\nk=1\nHX\nh=1\nFk\nh ⊙ ReLU(αk,c\nh ) (8)\nwhere ⊙ refers to the Hadamard product. Here, we apply\nthe ReLU operation on the computed gradientαk,c\nh to reflect\nonly the positive contribution to the classification output.\nAlso, the contributions obtained at each location of the patch\nfrom all layers and heads are summed to combine them in\nthe same way as the feedforward network fuses the embed-\nded patches at each skip connection. The result of this pro-\ncess, Lc, is a [1×N] matrix where each value represents the\ncontribution of each patch to the classification output of the\ngiven class c. However, the first element of this matrix rep-\nresents the contribution of the [class] token which does not\ncontain any spatial information. Since we are only interested\nin the contribution of each image patch, we discard the first\nelement and construct the class activation map (CAM) with\nthe last n2 elements. To visualize the final CAM, the n2 el-\nements are reshaped to a two-dimensional image, which has\nthe size of [n × n] as demonstrated in Figure. 4. Then it is\ninterpolated to have the same size as the input image and\neventually generates the final CAM of the model.\nExperiments\nIn this section, we present the results of the performance\ncomparison of our method with previous leading methods.\nThe compared methods here are the current explainability\nmethods devised for ViT that consist of Attention Rollout\n(Abnar and Zuidema 2020) and LRP-based method for ViT\n(Chefer, Gur, and Wolf 2021).\nExperimental Setup\nDatasets and Evaluation Metrics.For the evaluation, we\nused the validation set of ImageNet ILSVRC 2012 (Rus-\nsakovsky et al. 2015) and Pascal VOC 2012 (Everingham\net al. 2012) and the test set of Caltech-UCSD Birds-200-\n2011 (CUB 200) (Wah et al. 2011), which provide the\nbounding-box annotation label. In quantitative evaluation,\nthe images with more than one class label in PASCAL VOC\n2012 are excluded and only single-class images are used.\nDuring the weakly-supervised localization evaluation, the\ninput images for which the model produces a wrong pre-\ndiction are excluded since the heatmaps are not reliable in\nthis case.\nFor the weakly-supervised localization test, the perfor-\nmance is measured by pixel accuracy, Intersection over\nUnion (IoU), Dice coefficient (F1), precision, and recall\nscores. The pixel perturbation test is measured by the ABPC\nscore (Samek et al. 2016) with pixel-level perturbation. The\nABPC score is the area between the LeRF and MoRF per-\nturbation curves where the LeRF curve removes the least\nrelevant pixels first and the MoRF curve removes the most\nrelevant pixels first. A larger ABPC value indicates a better\nquality of the heatmap.\nImplementation Details. All methods are evaluated with\nthe same ViT-base (Dosovitskiy et al. 2020) model that takes\nthe input image with a size of [224 × 224 × 3]. All methods\nshare the same model parameters and the fine-tuning details\nof the model parameters are provided in the supplementary\nmaterial. In this ViT, the input images are converted into\n[14×14] number of patches and therefore each method gen-\nerates a heatmap with a size of[14×14×1] where one pixel\ncorresponds to the contribution of one image patch of the\ninput image. Before evaluation, the heatmaps are all resized\ninto [224 × 224 × 1] and adjusted to a min-max normal-\nization. For the weakly-supervised object detection, we get\na binary mask from the generated heatmap by applying a\nthreshold (σ = 0.5) and then generate bounding boxes from\nthe group of pixels that have a continuous contour. The per-\nturbation test is applied to the ground-truth class to compare\nthe heatmap quality on the existing object.\nResults\nHere we demonstrate the visualization of the heatmaps gen-\nerated by all three methods. Then, we present the quanti-\ntative evaluation of each method measured by the weakly-\nsupervised object detection metrics and pixel-perturbation.\nVisualization. The visualization results of each method on\nthree datasets are presented in Figure 5. The first images of\neach dataset demonstrate that our method greatly improves\nobject localization performance by successfully shading the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2960\nImageNet ILSVRC 2012 Pascal VOC 2012 CUB 200\nBounding\nBox\nAttention\nRollout\nLRP-based\nOurs\nFigure 5: The heatmaps on ImageNet ILSVRC 2012, Pascal VOC 2012, and CUB 200 dataset generated by each of the methods.\nThe first images in each dataset demonstrate the peak intensities generated on a homogeneous non-object background in Atten-\ntion Rollout and LRP-based method and the reduced peak intensities in our method. The second and third images in ILSVRC\n2012 and PASCAL VOC show the localization performance of each method on single-instance and multiple-instance images,\nrespectively. CUB200 consists of single-instance images only and its second and third images include one object instance per\nimage.\nAttention\nRollout LRP-based Ours\nCat\nPerson\nCar\nDog\nFigure 6: Visualization of the heatmaps generated for differ-\nent target objects. The input images are from PASCAL VOC\n2012 and have two class labels per image.\npeak intensities. As can be seen, the visualization results of\nthe Attention Rollout and LRP-based method on the im-\nages are dominated by the peak intensities and therefore\nfail to localize the target objects which are indicated by the\nbounding boxes. Also, this result shows that our method\ncaptures the object region more precisely compared to At-\ntention Rollout (Abnar and Zuidema 2020). Since Attention\nRollout produces a non-class-specific explanation, it often\nhighlights some unrelated background regions whereas our\nmethod successfully separates the foreground object from\nthe background. Also, our method shows a strong point in\nencompassing the full object area compared to the LRP-\nbased method (Chefer, Gur, and Wolf 2021). The LRP-based\nmethod tends to highlight a small conspicuous part of the\nobject. Furthermore, it often misses some instances of the\nobject in an image with multiple instances of the class ob-\nject. In contrast, our method successfully localizes the whole\nobject and also captures all the instances of the class object\neven when the instances are placed far from each other.\nIn addition, ours can also provide class-specific explana-\ntions for different classes presented within an image. Figure\n6 demonstrates the results of the explanation with different\ntarget classes generated by each method. Attention Rollout\ndoes not provide a class-specific explanation and therefore\nproduces the same results regardless of the target class la-\nbel. In contrast, LRP and ours are both capable of provid-\ning a class-specific explanation of the given model. Here,\nour method still maintains better localization performance\nby fully capturing the object of each target class. The visu-\nalization results on more image samples are presented in the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2961\nAttention Rollout LRP-based Ours\npixel accuracy 0.6209 0.5863 0.7341\nIoU 0.3597 0.2029 0.5212\ndice (F1) 0.4893 0.3055 0.6515\nprecision 0.7326 0.9110 0.8299\nrecall 0.4657 0.2176 0.6276\nTable 1: Localization performance comparison on ImageNet\nILSVRC 2012.\nAttention Rollout LRP-based Ours\npixel accuracy 0.5592 0.5750 0.7521\nIoU 0.1645 0.1574 0.5335\ndice (F1) 0.2431 0.2348 0.6646\nprecision 0.5802 0.7651 0.8167\nrecall 0.2115 0.1716 0.6647\nTable 2: Localization performance comparison on Pascal\nVOC 2012.\nAttention Rollout LRP-based Ours\npixel accuracy 0.7273 0.7039 0.8351\nIoU 0.3097 0.1997 0.5836\ndice (F1) 0.4339 0.3106 0.7220\nprecision 0.8357 0.9669 0.8987\nrecall 0.3420 0.1992 0.6438\nTable 3: Localization performance comparison on CUB 200.\nAttention Rollout LRP-based Ours\nLeRF 0.4739 0.5140 0.5298\nMoRF 0.2053 0.1736 0.1607\nABPC 0.2685 0.3404 0.3691\nTable 4: The result of pixel perturbation test on ILSVRC\n2012. LeRF represents the area under the LeRF curve and\nMoRF represents the area under the MoRF curve. The\nABPC score is the area between the LeRF and MoRF curves.\nFor LeRF and ABPC higher is better and for MoRF lower is\nbetter.\nsupplementary material.\nWeakly-Supervised Object Detection. The result of\nthe weakly-supervised object detection on the ImageNet\nILSVRC 2012 validation set is presented in Table 1. This\nresult shows that ours achieves 73.41% in pixel accuracy,\n52.12% in IoU score, and 65.15% in dice coefficient, which\nis the highest among the three methods. Although there was\na drop in precision compared to LRP-based (82.99% vs\n91.10%), ours achieves a much better recall score (62.76%\nvs 21.76%) which is generally a trade-off with precision.\nThe localization performance on the Pascal VOC 2012\nvalidation is presented in Table 2. Our method achieves\n75.21% in pixel accuracy, 53.35% in IoU, and 66.46% in\ndice coefficient, presenting outstanding localization perfor-\nmance. In this case, our method also achieves the highest\nprecision score of 81.67%.\nTable 3 shows the localization performance evaluated on\nthe test set of CUB 200. The CUB 200 dataset consists\nof images with a single bird per image which are eas-\nily distinguishable from the background. Therefore, patch-\ncorrelation information of the self-attention scores serves\na significant role in this dataset. Our method effectively\nutilizes the self-attention scores, resulting in a more accu-\nrate and precise explanation compared to others, achieving\n83.51% in pixel accuracy, 58.36% in IoU, and 72.20% in\ndice coefficient. This is about 10.78%, 27.39%, and 28.81%\nhigher scores respectively than those of Attention Rollout.\nIn conclusion, our method provides a high-semantic ex-\nplanation of ViT that consistently shows an outstanding per-\nformance in the weakly-supervised object detection task\ncompared to the Attention rollout and LRP-based method.\nOur method shows significant improvements in terms of\npixel accuracy, IoU, recall, and dice coefficient, while it still\nmaintains an acceptable level of precision.\nPixel Perturbation.The result of the pixel perturbation test\nis presented in Table 4. LeRF and MoRF represent the areas\nunder the prediction probability score curve when remov-\ning the least relevant pixels first and the most relevant pix-\nels first, respectively. The ABPC is the area between these\ntwo curves, which is obtained by subtracting the AUC of\nthe MoRF curve from that of the LeRF curve. Our method\nachieves a better LeRF score and MoRF score and therefore\na higher ABPC score compared to the LRP-based method\n(36.91 % vs 34.04 %). This guarantees the better faithfulness\nand reliability of the explanations that our method provides.\nAdditional evaluation results of the object localization task\nand the pixel perturbation test are presented in the supple-\nmentary materials.\nConclusion\nIn this work, we propose an attention-guided gradient analy-\nsis method that aims at achieving greater weakly-supervised\nlocalization performance. To this end, our method provides a\nhigh-level semantic explanation by selectively collecting the\nessential gradients propagated from the classification out-\nput of the target class to each self-attention matrix along the\nskip connection path. To supplement the gradient informa-\ntion with the patch correlation information that indicates the\ngroup of patches with contiguous patterns, the self-attention\nscores are combined with the gradients as feature maps. Be-\nfore these two major components are aggregated, the self-\nattention scores are adapted in a way that decreases the effect\nof peak intensities to improve the localization performance\nof the CAM. As a result, our method outperforms the current\nstate-of-the-art visualization techniques of ViT by localizing\nthe full areas of the target object, and it especially achieves\na great performance improvement in capturing the multiple\ninstances of the given class object. This provides a reliable\nexplanation of the model and weakly-supervised object de-\ntection method at the same time and allows ViT to be more\nadaptable to many tasks involving object localization in the\ncomputer vision field.\nAcknowledgments\nThis work was supported by KIST Institutional Programs\n(2V09831, 2E32341, and 2E32211).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2962\nReferences\nAbnar, S.; and Zuidema, W. 2020. Quantifying attention\nflow in transformers. arXiv preprint arXiv:2005.00928.\nBach, S.; Binder, A.; Montavon, G.; Klauschen, F.; M ¨uller,\nK.-R.; and Samek, W. 2015. On pixel-wise explanations for\nnon-linear classifier decisions by layer-wise relevance prop-\nagation. PloS one, 10(7): e0130140.\nBinder, A.; Bach, S.; Montavon, G.; M ¨uller, K.-R.; and\nSamek, W. 2016a. Layer-wise relevance propagation for\ndeep neural network architectures. In Information science\nand applications (ICISA) 2016, 913–922. Springer.\nBinder, A.; Montavon, G.; Lapuschkin, S.; M ¨uller, K.-R.;\nand Samek, W. 2016b. Layer-wise relevance propagation\nfor neural networks with local renormalization layers. In\nArtificial Neural Networks and Machine Learning–ICANN\n2016: 25th International Conference on Artificial Neural\nNetworks, Barcelona, Spain, September 6-9, 2016, Proceed-\nings, Part II 25, 63–71. Springer.\nChattopadhay, A.; Sarkar, A.; Howlader, P.; and Balasub-\nramanian, V . N. 2018. Grad-cam++: Generalized gradient-\nbased visual explanations for deep convolutional networks.\nIn 2018 IEEE winter conference on applications of computer\nvision (WACV), 839–847. IEEE.\nChefer, H.; Gur, S.; and Wolf, L. 2021. Transformer inter-\npretability beyond attention visualization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 782–791.\nChen, C.-F. R.; Fan, Q.; and Panda, R. 2021. Crossvit:\nCross-attention multi-scale vision transformer for image\nclassification. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, 357–366.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nDraelos, R. L.; and Carin, L. 2020. Hirescam: Faith-\nful location representation in visual attention for explain-\nable 3d medical image classification. arXiv preprint\narXiv:2011.08891.\nEveringham, M.; Van Gool, L.; Williams, C.\nK. I.; Winn, J.; and Zisserman, A. 2012. The\nPASCAL Visual Object Classes Challenge\n2012 (VOC2012) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHendrycks, D.; and Gimpel, K. 2016. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415.\nLeCun, Y .; Boser, B.; Denker, J. S.; Henderson, D.; Howard,\nR. E.; Hubbard, W.; and Jackel, L. D. 1989. Backpropaga-\ntion applied to handwritten zip code recognition. Neural\ncomputation, 1(4): 541–551.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n10012–10022.\nMontavon, G.; Lapuschkin, S.; Binder, A.; Samek, W.; and\nM¨uller, K.-R. 2017. Explaining nonlinear classification de-\ncisions with deep taylor decomposition.Pattern recognition,\n65: 211–222.\nNaseer, M. M.; Ranasinghe, K.; Khan, S. H.; Hayat, M.;\nShahbaz Khan, F.; and Yang, M.-H. 2021. Intriguing proper-\nties of vision transformers. Advances in Neural Information\nProcessing Systems, 34: 23296–23308.\nQin, Z.; Kim, D.; and Gedeon, T. 2021. Informative Class\nActivation Maps. arXiv preprint arXiv:2106.10472.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by gener-\native pre-training.\nRanftl, R.; Bochkovskiy, A.; and Koltun, V . 2021. Vision\ntransformers for dense prediction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n12179–12188.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\nBerg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale Vi-\nsual Recognition Challenge. International Journal of Com-\nputer Vision (IJCV), 115(3): 211–252.\nSamek, W.; Binder, A.; Montavon, G.; Lapuschkin, S.; and\nM¨uller, K.-R. 2016. Evaluating the visualization of what\na deep neural network has learned. IEEE transactions on\nneural networks and learning systems, 28(11): 2660–2673.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-CAM: Visual Expla-\nnations From Deep Networks via Gradient-Based Localiza-\ntion. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV).\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSzegedy, C.; Liu, W.; Jia, Y .; Sermanet, P.; Reed, S.;\nAnguelov, D.; Erhan, D.; Vanhoucke, V .; and Rabinovich, A.\n2015. Going deeper with convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, 1–9.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2963\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In International\nconference on machine learning, 10347–10357. PMLR.\nTuli, S.; Dasgupta, I.; Grant, E.; and Griffiths, T. L. 2021.\nAre convolutional neural networks or transformers more like\nhuman vision? arXiv preprint arXiv:2105.07197.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie,\nS. 2011. The Caltech-UCSD Birds-200-2011 Dataset.\nTechnical Report CNS-TR-2011-001, California Institute of\nTechnology.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid Vision Trans-\nformer: A Versatile Backbone for Dense Prediction With-\nout Convolutions. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), 568–578.\nYang, S.; Kim, Y .; Kim, Y .; and Kim, C. 2020. Combina-\ntional class activation maps for weakly supervised object lo-\ncalization. In Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, 2941–2949.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n6881–6890.\nZhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba,\nA. 2016. Learning deep features for discriminative localiza-\ntion. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2921–2929.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2964"
}