{
  "title": "GRET: Global Representation Enhanced Transformer",
  "url": "https://openalex.org/W2996766022",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2743605551",
      "name": "Rongxiang Weng",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2106366899",
      "name": "Haoran Wei",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096150187",
      "name": "Shujian Huang",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2127223396",
      "name": "Heng Yu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2105392445",
      "name": "Weihua Luo",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096321562",
      "name": "Jiajun Chen",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2743605551",
      "name": "Rongxiang Weng",
      "affiliations": [
        "Alibaba Group (China)",
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2106366899",
      "name": "Haoran Wei",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096150187",
      "name": "Shujian Huang",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2127223396",
      "name": "Heng Yu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105392445",
      "name": "Weihua Luo",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096321562",
      "name": "Jiajun Chen",
      "affiliations": [
        "Nanjing University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2809186393",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2892116303",
    "https://openalex.org/W6644516640",
    "https://openalex.org/W2304113845",
    "https://openalex.org/W6746208923",
    "https://openalex.org/W2962996600",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6755768796",
    "https://openalex.org/W2806591392",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6743446608",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2998135987",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6601487527",
    "https://openalex.org/W2817535134",
    "https://openalex.org/W6742618366",
    "https://openalex.org/W6754517385",
    "https://openalex.org/W6757235413",
    "https://openalex.org/W6750467700",
    "https://openalex.org/W2939862856",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963970666",
    "https://openalex.org/W2963311117",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2902503418",
    "https://openalex.org/W2341718173",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2962853356",
    "https://openalex.org/W3005996763",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2788278639",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2963497309",
    "https://openalex.org/W2552839021",
    "https://openalex.org/W1978331037",
    "https://openalex.org/W3105409430",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W2794714381",
    "https://openalex.org/W2970849705",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2903728819",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Transformer, based on the encoder-decoder framework, has achieved state-of-the-art performance on several natural language generation tasks. The encoder maps the words in the input sentence into a sequence of hidden states, which are then fed into the decoder to generate the output sentence. These hidden states usually correspond to the input words and focus on capturing local information. However, the global (sentence level) information is seldom explored, leaving room for the improvement of generation quality. In this paper, we propose a novel global representation enhanced Transformer (GRET) to explicitly model global representation in the Transformer network. Specifically, in the proposed model, an external state is generated for the global representation from the encoder. The global representation is then fused into the decoder during the decoding process to improve generation quality. We conduct experiments in two text generation tasks: machine translation and text summarization. Experimental results on four WMT machine translation tasks and LCSTS text summarization task demonstrate the effectiveness of the proposed approach on natural language generation1.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nGRET: Global Representation Enhanced Transformer\nRongxiang Weng,1,2 Haoran Wei,2 Shujian Huang,1∗ Heng Yu,2\nLidong Bing,2 Weihua Luo,2 Jiajun Chen1\n1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China\n2Machine Intelligence Technology Lab, Alibaba Group, Hangzhou, China\n{wengrx, funan.whr, yuheng.yh, l.bing, weihua.luowh}@alibaba-inc.com, {huangsj, chenjj}@nju.edu.cn\nAbstract\nTransformer, based on the encoder-decoder framework, has\nachieved state-of-the-art performance on several natural lan-\nguage generation tasks. The encoder maps the words in the in-\nput sentence into a sequence of hidden states, which are then\nfed into the decoder to generate the output sentence. These\nhidden states usually correspond to the input words and focus\non capturing local information. However, the global (sentence\nlevel) information is seldom explored, leaving room for the\nimprovement of generation quality. In this paper, we propose\na novel global representation enhanced Transformer (GRET)\nto explicitly model global representation in the Transformer\nnetwork. Speciﬁcally, in the proposed model, an external state\nis generated for the global representation from the encoder.\nThe global representation is then fused into the decoder dur-\ning the decoding process to improve generation quality. We\nconduct experiments in two text generation tasks: machine\ntranslation and text summarization. Experimental results on\nfour WMT machine translation tasks and LCSTS text sum-\nmarization task demonstrate the effectiveness of the proposed\napproach on natural language generation\n1.\n1 Introduction\nTransformer (V aswani et al. 2017) has outperformed other\nmethods on several neural language generation (NLG) tasks,\nlike machine translation (Deng et al. 2018), text summariza-\ntion (Chang, Huang, and Hsu 2018),etc. Generally, Trans-\nformer is based on theencoder-decoder framework which\nconsists of two modules: an encoder network and a decoder\nnetwork. The encoder encodes the input sentence into a se-\nquence of hidden states, each of which corresponds to a spe-\nciﬁc word in the sentence. The decoder generates the output\nsentence word by word. At each decoding time-step, the de-\ncoder performs attentive read (Luong, Pham, and Manning\n2015; V aswani et al. 2017) to fetch the input hidden states\nand decides which word to generate.\nAs mentioned above, the decoding process of Trans-\nformer only relies on the representations contained in these\n∗Corresponding author\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1Source code is available at: https://github.com/wengrx/GRET\nhidden states. However, there is evidence showing that hid-\nden states from the encoder in Transformer only contain\nlocal representations which focus on word level informa-\ntion. For example, previous work (V aswani et al. 2017;\nDevlin et al. 2018; Song et al. 2020) showed that these hid-\nden states pay much attention to the word-to-word mapping;\nand the weights of attention mechanism, determining which\ntarget word will be generated, is similar to word alignment.\nAs Frazier (1987) pointed, the global information, which\nis about the whole sentence in contrast to individual words,\nshould be involved in the process of generating a sentence.\nRepresentation of such global information plays an import\nrole in neural text generation tasks. In the recurrent neural\nnetwork (RNN) based models (Bahdanau, Cho, and Bengio\n2014), Chen (2018) showed on text summarization task that\nintroducing representations about global information could\nimprove quality and reduce repetition. Lin et al. (2018b)\nshowed on machine translation that the structure of the trans-\nlated sentence will be more correct when introducing global\ninformation. These previous work shows global informa-\ntion is useful in current neural network based model. How-\never, different from RNN (Sutskever, Vinyals, and Le 2014;\nCho et al. 2014; Bahdanau, Cho, and Bengio 2014) or\nCNN (Gehring et al. 2016; 2017), although self-attention\nmechanism can achieve long distance dependence, there\nis no explicit mechanism in the Transformer to model the\nglobal representation of the whole sentence. Therefore, it is\nan appealing challenge to provide Transformer with such a\nkind of global representation.\nIn this paper, we divide this challenge into two issues that\nneed to be addressed: 1).how to model the global contextual\ninformation? and 2). how to use global information in the\ngeneration process?, and propose a novel global representa-\ntion enhanced Transformer (GRET) to solve them. For the\nﬁrst issue, we propose to generate the global representation\nbased on local word level representations by two comple-\nmentary methods in the encoding stage. On one hand, we\nadopt a modiﬁedcapsule network(Sabour, Frosst, and Hin-\nton 2017) to generate the global representation based the fea-\ntures extracted from local word level representations. The lo-\ncal representations are generally related to the word-to-word\nmapping, which may be redundant or noisy. Using them to\n9258\ngenerate the global representation directly without any ﬁl-\ntering is inadvisable. Capsule network, which has a strong\nability of feature extraction (Zhao et al. 2018), can help to\nextract more suitable features from local states. Comparing\nwith other networks, like CNN (Krizhevsky, Sutskever, and\nHinton 2012), it can see all local states at one time, and ex-\ntract feature vectors after several times of deliberation.\nOn the other hand, we propose a layer-wise recurrent\nstructure to further strengthen the global representation. Pre-\nvious work shows the representations from each layer have\ndifferent aspects of meaning (Peters et al. 2018; Dou et al.\n2018), e.g. lower layer contains more syntactic information,\nwhile higher layer contains more semantic information. A\ncomplete global context should have different aspects of in-\nformation. However, the global representation generated by\nthe capsule network only obtain intra-layer information. The\nproposed layer-wise recurrent structure is a helpful supple-\nment to combine inter-layer information by aggregating rep-\nresentations from all layers. These two methods can model\nglobal representation by fully utilizing different grained in-\nformation from local representations.\nFor the second issue, we propose to usea context gat-\ning mechanism to dynamically control how much informa-\ntion from the global representation should be fused into the\ndecoder at each step. In the generation process, every de-\ncoder states should obtain global contextual information be-\nfore outputting words. And the demand from them for global\ninformation varies from word to word in the output sentence.\nThe proposed gating mechanism could utilize the global rep-\nresentation effectively to improve generation quality by pro-\nviding a customized representation for each state.\nExperimental results on four WMT translation tasks, and\nLCSTS text summarization task show that our GRET model\nbrings signiﬁcant improvements over a strong baseline and\nseveral previous researches.\n2 Approach\nOur GRET model includes two steps: modeling the global\nrepresentation in the encoding stage and incorporating it into\nthe decoding process. We will describe our approach in this\nsection based on Transformer (V aswani et al. 2017).\n2.1 Modeling Global Representation\nIn the encoding stage, we propose two methods for mod-\neling the global representation at different granularity. We\nﬁrstly use capsule network to extract features from local\nword level representations, and generate global representa-\ntion based on these features. Then, a layer-wise recurrent\nstructure is adopted subsequently to strengthen the global\nrepresentation by aggregating the representations from all\nlayers of the encoder. The ﬁrst method focuses on utilizing\nword level information to generate a sentence level repre-\nsentation, while the second method focuses on combining\ndifferent aspects of sentence level information to obtain a\nmore complete global representation.\nIntra-layer Representation Generation We propose to\nuse capsules with dynamic routingto extract the speciﬁc and\nAlgorithm 1Dynamic Routing Algorithm\n1: procedure:R OUTING (H, r)\n2: for i in input layer andk in output layerdo\n3: bki ← 0;\n4: end for\n5: for r iterations do\n6: for k in output layerdo\n7: ck ← softmax(bk);\n8: end for\n9: for k in output layerdo\n10: uk ← q(∑I\ni ckihi);\n11: ⊿H = {h1,··· ,hi,···}\n12: end for\n13: for i in input layer andk in output layerdo\n14: bki ← bki +hi ·uk;\n15: end for\n16: end for\n17: return U; ⊿U = {u1,··· ,uk,···}\nsuitable features from the local representations for stronger\nglobal representation modeling, which is an effective and\nstrong feature extraction method (Sabour, Frosst, and Hinton\n2017; Zhang, Liu, and Song 2018)\n2. Features from hidden\nstates of the encoder are summarized into several capsules,\nand the weights (routes) between hidden states and capsules\nare updated by dynamic routing algorithm iteratively.\nFormally, given an encoder of the Transformer which has\nM layers and an input sentencex = {x\n1,··· ,xi,··· ,xI }\nwhich has I words. The sequence of hidden statesHm =\n{hm\n1 ,··· ,hm\ni ,··· ,hm\nI } from the mth layer of the encoder\nis computed by\nHm = LN(SAN(Qm\ne ,Km−1\ne ,Vm−1\ne )), (1)\nwhere the Qm\ne , Km−1\ne and Vm−1\ne are query, key and value\nvectors which are same asHm−1, the hidden states from the\nm−1th layer. The LN(·)and SAN(·)are layer normalization\nfunction (Ba, Kiros, and Hinton 2016) and self-attention net-\nwork (V aswani et al. 2017), respectively. We omit the resid-\nual network here.\nThen, the capsulesU\nm with size ofK are generated by\nHm. Speciﬁcally, thekth capsule um\nk is computed by\num\nk = q(\nI∑\ni\nckiˆh\nm\ni ),c ki ∈ ck, (2)\nˆh\nm\ni = Wkhm\ni , (3)\nwhere q(·)is non-linear squash function (Sabour, Frosst, and\nHinton 2017):\nsquash(t)= ||t||2\n1+ ||t||2\nt\n||t||, (4)\nand ck is computed by\nck = softmax(bk), bk ∈ B, (5)\n2Other details of the Capsule Network are shown in Sabour,\nFrosst, and Hinton (2017) .\n9259\n…hm\n1 hm\n2\nsm\nmth Layer of  Encoder\num\n1\num\n2\num\nk um\nk+1\n……\nPooling\nhm\ni hm\ni+1 …Local States\nCapsules\nGlobal State\nDynamic Routing\nFigure 1: The overview of generating the global representa-\ntion with capsule network.\nwhere the matrixB is initialized by zero and whose row and\ncolumn are K and I, respectively. This matrix will be up-\ndated when all capsules are produced.\nB = B +Um⊤ ·Hm. (6)\nThe algorithm is shown in Algorithm 1. The sequence of\ncapsules Um could be used to generate the global represen-\ntation.\nDifferent from the original capsules network which use\na concatenation method to generate the ﬁnal representation,\nwe use an attentive pooling method to generate the global\nrepresentation\n3. Formally, in themth layer, the global repre-\nsentation is computed by\nsm = FFN(\nK∑\nk=1\nakum\nk ), (7)\nak = exp(ˆsm ·um\nk )∑K\nt=1 exp(ˆsm ·um\nt )\n, (8)\nwhere FFN(·)is a feed-forward network and theˆsm is com-\nputed by\nsm = FFN( 1\nK\nK∑\nk=1\num\nk ). (9)\nThis attentive method can consider the different roles of\nthe capsules and better model the global representation. The\noverview of the process of generating the global representa-\ntion are shown in Figure 1.\nInter-layer Representation Aggregation Traditionally,\nthe Transformer model only fed the last layer’s hidden states\n3Typically, the concatenation and other pooling methods, e.g.\nmean pooling, could be used here easily, but they will decrease\n0.1∼0.2 BLEU in machine translation experiment.\nsm\num\n1 um\n2\num\nK…\nPooling\nsm−1GRU\nPooling\n…\n…\num−1\n1 um−1\n2\nGRU\num−1\nK\nm-1th Layer mth Layer\nFigure 2: The overview of the layer-wise recurrent structure.\nHM as representations of input sentence to the decoder to\ngenerate the output sentence. Following this, we can feed\nthe last layer’s global representation s\nM into the decoder\ndirectly. However, current global representation only con-\ntain the intra-layer information, the other layers’ represen-\ntations are ignored, which were shown to have different\naspects of meaning in previous work (Wang et al. 2018b;\nDou et al. 2018). Based on this intuition, we propose a\nlayer-wise recurrent structureto aggregate the representa-\ntions generated by employing the capsule network on all lay-\ners of the encoder to model a complete global representation.\nThe layer-wise recurrent structure aggregates each layer’s\nintra global state by a gated recurrent unit (Cho et al. 2014,\nGRU) which could achieve different aspects of information\nfrom the previous layer’s global representation. Formally,\nwe adjust the computing method ofs\nm by\nsm = GRU(AT P(Um),sm−1), (10)\nwhere the A TP(·) is the attentive pooling function com-\nputed by Eq 7-9. The GRU unit can control the informa-\ntion ﬂow by forgetting useless information and capturing\nsuitable information, which can aggregate previous layer’s\nrepresentations usefully. The layer-wise recurrent structure\ncould achieve a more exquisite and complete representa-\ntion. Moreover, the proposed structure only need one more\nstep in the encoding stage which is not time-consuming. The\noverview of the aggregation structure is shown in Figure 2.\n2.2 Incorporating into the Decoding Process\nBefore generating the output word, each decoder state\nshould consider the global contextual information. We com-\nbine the global representation in decoding process with an\nadditive operation to the last layer of the decoder guiding\nthe states output true words. However, the demand for the\nglobal information of each target word is different. Thus, we\npropose a context gating mechanismwhich can provide spe-\nciﬁc information according to each decoder hidden state.\nSpeciﬁcally, given an decoder which hasN layers and the\ntarget sentence y which has J words in the training stage,\nthe hidden states R\nN = {rN\n1 ,··· ,rN\nj ,··· ,rN\nJ } from the\nNth layer of the decoder is computed by\nRN = LN(SAN(QN\nd ,KN−1\nd ,VN−1\nd )\n+SAN(QN\nd ,KM\ne ,VM\ne )), (11)\n9260\n… rn\nj−1\nrn\nj−1\nrn\nj\nrn\nj\nsm\nrn\nj+1 …\nrn\nj+1\nTime Step\nFigure 3: The context gating mechanism of fusing the global\nrepresentation into decoding stage.\nwhere QN\nd , KN−1\nd and VN−1\nd are hidden statesRN−1 from\nN −1th layer. TheKM\ne and VM\ne are same asHM . We omit\nthe residual network here.\nFor each hidden state rN\nj from RN , the context gate is\ncalculated by:\ngj = sigmoid(rN\nj ,sM ). (12)\nThe new state, which contains the needed global informa-\ntion, is computed by:\nrN\nj = rN\nj +sM\nj ∗g. (13)\nThen, the output probability is calculated by the output\nlayer’s hidden state:\nP(yj|y<j,x)= softmax(FFN(rN\nj )). (14)\nThis method enables each state to achieve it’s customized\nglobal information. The overview is shown in Figure 3.\n2.3 Training\nThe training process of our GRET model is same as the stan-\ndard Transformer. The networks is optimized by maximizing\nthe likelihood of the output sentencey given input sentence\nx, denoted byL\ntrans.\nLtrans = 1\nJ\nJ∑\nj=1\nlogP(yj|y<j,x), (15)\nwhere P(yj|y<j,x) is deﬁned in Equation 14.\n3 Experiment\n3.1 Implementation Detail\nData-sets We conduct experiments on machine translation\nand text summarization tasks. In machine translation, we\nemploy our approach on four language pairs: Chinese to En-\nglish (ZH→EN), English to German (EN→DE), German to\nEnglish (DE→EN), and Romanian to English (RO→EN) 4.\nIn text summarization, we use LCSTS (Hu, Chen, and Zhu\n2015)\n5 to evaluate the proposed method. These data-sets are\n4http://www.statmt.org/wmt17/translation-task.html\n5http://icrc.hitsz.edu.cn/Article/show/139.html\npublic and widely used in previous work, which will make\nother researchers replicate our work easily.\nIn machine translation, on the ZH →EN task, we use\nWMT17 as training set which consists of about 7.5M sen-\ntence pairs. We usenewsdev2017 as validation set and\nnewstest2017 as test set which have 2002 and 2001\nsentence pairs, respectively. On the EN→DE and DE→EN\ntasks, we use WMT14 as training set which consists of about\n4.5M sentence pairs. We usenewstest2013 as validation\nset and newstest2014 as test set which have 2169 and\n3000 sentence pairs, respectively. On the RO→EN task, we\nuse WMT16 as training set which consists of about 0.6M\nsentence pairs. We usenewstest2015 as validation set\nand newstest2016 as test set which has 3000 and 3002\nsentence pairs, respectively.\nIn text summarization, following in Hu, Chen, and\nZhu (2015) , we use PART I as training set which consists of\n2M sentence pairs. We use the subsets of PART II and PART\nIII scored from 3 to 5 as validation and test sets which con-\nsists of 8685 and 725 sentence pairs, respectively.\nSettings In machine translation, we apply byte pair encod-\ning (BPE) (Sennrich, Haddow, and Birch 2016) to all lan-\nguage pairs and limit the vocabulary size to 32K. In text\nsummarization, we limit the vocabulary size to 3500 based\non the character level. Out-of-vocabulary words and chars\nare replaced by the special tokenUNK.\nFor the Transformer, we set the dimension of the input and\noutput of all layers as 512, and that of the feed-forward layer\nto 2048. We employ 8 parallel attention heads. The number\nof layers for the encoder and decoder are 6. Sentence pairs\nare batched together by approximate sentence length. Each\nbatch has 50 sentence and the maximum length of a sentence\nis limited to 100. We set the value of dropout rate to 0.1. We\nuse the Adam (Kingma and Ba 2014) to update the parame-\nters, and the learning rate was varied under a warm-up strat-\negy with 4000 steps (V aswani et al. 2017). Other details are\nshown in V aswani et al. (2017) . The number of capsules is\nset 32 and the default time of iteration is set 3. The training\ntime of the Transformer is about 6 days on the DE→EN task.\nAnd the training time of the GRET model is about 12 hours\nwhen using the parameters of baseline as initialization.\nAfter the training stage, we use beam search for heuristic\ndecoding, and the beam size is set to 4. We measure transla-\ntion quality with the NIST-BLEU (Papineni et al. 2002) and\nsummarization quality with the ROUGE (Lin 2004).\n3.2 Main Results\nMachine Translation We employ the proposed GRET\nmodel on four machine translation tasks. All results are sum-\nmarized in Table 1. For fair comparison, we reported sev-\neral Transformer baselines with same settings reported by\nprevious work (V aswani et al. 2017; Hassan et al. 2018;\nGu et al. 2018) and researches about enhancing local word\nlevel representations (Dou et al. 2018; Yang et al. 2018;\nShaw, Uszkoreit, and V aswani 2018; Yang et al. 2019).\nThe results on the WMT17 ZH→EN task are shown in the\nsecond column of Table 1. The improvement of our GRET\n9261\nModel ZH→EN EN→DE DE→EN RO→EN\nTransformer∗ (V aswani et al. 2017) − 27.3 − −\nTransformer∗ (Hassan et al. 2018) 24.13 − − −\nTransformer∗ (Gu et al. 2018) − 27.02 − 31.76\nDeepRepre∗ (Dou et al. 2018) 24.76 28.78 − −\nLocalness∗ (Yang et al. 2018) 24.96 28.54 − −\nRelPos∗ (Shaw, Uszkoreit, and V aswani 2018) 24.53 27.94 − −\nContext-aware∗ (Yang et al. 2019) 24.67 28.26 − −\nGDR∗ (Zheng et al. 2019) − 28.10 − −\nTransformer 24.31 27.20 32.34 32.17\nGRET 25.53‡ 28.46† 33.79‡ 33.06‡\nTable 1: The comparison of our GRET , Transformer baseline and related work on the WMT17 Chinese to English (ZH→EN),\nWMT14 English to German (EN→DE) and German to English (DE→EN), and WMT16 Romania to English (RO→EN) tasks\n(* indicates the results came from their paper,†/‡ indicate signiﬁcantly better than the baseline (p< 0.05/0.01)).\nModel ROUGE-1 ROUGE-2 ROUGE-L\nRNNSearch∗ (Hu, Chen, and Zhu 2015) 30.79 − −\nCopyNet∗ (Gu et al. 2016) 34.4 21.6 31.3\nMRT∗ (Ayana, Liu, and Sun 2016) 37.87 25.43 35.33\nAC-ABS∗ (Li, Bing, and Lam 2018) 37.51 24.68 35.02\nCGU∗ (Lin et al. 2018a) 39.4 26.9 36.5\nTransformer∗ (Chang, Huang, and Hsu 2018) 42.35 29.38 39.23\nTransformer 43.14 29.26 39.72\nGRET 44.77 30.96 41.21\nTable 2: The comparison of our GRET , Transformer baseline and related work on the LCSTS text summarization task (*\nindicates the results came from their paper).\nmodel could be up to 1.22 based on a strong baseline system,\nwhich outperforms all previous work we reported. To our\nbest knowledge, our approach attains the state-of-the-art in\nrelevant researches.\nThen, the results on the WMT14 EN→DE and DE→EN\ntasks, which is the most widely used data-set recently, are\nshown in the third and fourth columns. The GRET model\ncould attain 28.46 BLEU (+1.26) on the EN→DE and 33.79\nBLEU (+1.45) on the DE→EN, which are competitive re-\nsults compared with previous studies.\nTo verify the generality of our approach, we also ex-\nperiment it on low resource language pair of the WMT16\nRO→EN task. Results are shown in the last column. The im-\nprovement of the GRET is 0.89 BLEU, which is a material\nimprovement in low resource language pair. And it shows\nthat proposed methods could improve translation quality in\nlow resource scenario.\nExperimental results on four machine translation tasks\nshow that modeling global representation in the current\nTransformer network is a general approach, which is not\nlimited by the language or size of training data, for improv-\ning translation quality.\nText Summarization Besides machine translation, we\nalso employ proposed methods in text summarization, a\nmonolingual generation task, which is an important and typ-\nical task in natural language generation.\nFigure 4: The comparison of the GTR with different number\nof capsules at different iteration times on the EN→DE task.\nThe results are shown in Table 2, we also reports sev-\neral popular methods in this data-set as a comparison. Our\napproach achieves considerable improvements in ROUGE-\n1/2/L (+1.63/+1.70/+1.49) and outperforms other work with\nsame settings. The improvement on text summarization is\neven more than machine translation. Compared with ma-\nchine translation, text summarization focuses more on ex-\ntracting suitable information from the input sentence, which\nis an advantage of the GRET model.\nExperiments on the two tasks also show that our approach\ncould work on different types of language generation task\nand may improve the performance of other text generation\ntasks.\n9262\nModel Capsule Aggregate Gate #Param Inference BLEU Δ\nTransformer −− − 61.9M 1.00x 27.20 −\nOur Approach\n61.9M 0.99x 27.39 +0.19\n✓ 63.6M 0.87x 28.02 +0.82\n✓✓ 68.1M 0.82x 28.32 +1.02\n✓✓ 63.6M 0.86x 28.23 +1.03\n✓ 66.6M 0.95x 27.81 +0.61\n✓✓ 66.8M 0.93x 27.76 +0.56\n✓ 62.1M 0.98x 27.53 +0.33\n✓✓ ✓ 68.3M 0.81x 28.46 +1.26\nTable 3: Ablation study on the WMT14 English to German (EN→DE) machine translation task.\nModel #Param Inference BLEU\nTransformer-Base 61.9M 1.00x 27.20\nGTR-Base 68.3M 0.81x 28.46\nTransformer-Big 249M 0.59x 28.47\nGRET-Big 273M 0.56x 29.33\nTable 4: The comparison of GRET and Transformer withbig\nsetting (V aswani et al. 2017) on the EN→DE task.\nModel Precision\nTop-200 Top-500 Top-1000\nLast 43% 52% 64%\nAverage 49% 57% 69%\nGRET 63% 74% 81%\nTable 5: The precision from the bag-of-words predictor\nbased on GRET , last encoder state (Last) and averaging all\nlocal states (Average) on the EN→DE task.\n3.3 Ablation Study\nTo further show the effectiveness and consumption of each\nmodule in our GRET model, we make ablation study in this\nsection. Speciﬁcally, we investigate how the capsule net-\nwork, aggregate structureand gating mechanism affect the\nperformance of the global representation.\nThe results are shown in Table 3. Speciﬁcally, without\nthe capsule network, the performance decreases 0.7 BLEU\n, which means extracting features from local representa-\ntions iteratively could reduce redundant information and\nnoisy. This step determines the quality of global represen-\ntation directly. Then, aggregating multi-layers’ representa-\ntions attains 0.61 BLEU improvement. The different aspects\nof information from each layer is an excellent complement\nfor generating the global representation. Without the gating\nmechanism, the performance decreases 0.24 BLEU score\nwhich shows the context gating mechanism is important to\ncontrol the proportion of using the global representation in\neach decoding step. While the GRET model will take more\ntime, we think it is worthwhile to improve generation quality\nby reducing a bit of efﬁciency in most scenario.\nFigure 5: The comparison of the GTR with different number\nof capsules at different iteration times on the EN→DE task.\n3.4 Effectiveness on Different Model Settings\nWe also experiment the GRET model withbig setting on the\nEN→DE task. Thebig model is far larger than abovebase\nmodel and get the state-of-the-art performance in previous\nwork (V aswani et al. 2017).\nThe results are shown in Table 4, Transformer-Big out-\nperforms Transformer-Base, while the GRET-Big improves\n0.86 BLEU score comparing with the Transformer-Big.\nIt is worth to mention that our model with base setting\ncould achieve a similar performance to the Transformer-\nBig, which reduces parameters by almost 75% (68.3M VS.\n249M) and inference time by almost 27% (0.81x VS. 0.56x).\n3.5 Analysis of the Capsule\nThe number of capsules and the iteration time from dy-\nnamic routing algorithm may affect the performance of the\nproposed model. We evaluate the GRET model with differ-\nent number of capsules at different iteration times on the\nEN→DE task. The results are shown in Figure 4.\nWe can get two empirical conclusions in this experiment.\nFirst, the ﬁrst three iterations can signiﬁcantly improve the\nperformance, while the results of more iterations (4 and 5)\ntend to stabilize. Second, the increase of capsule number (48\nand 64) doesn’t get a further gain. We think the reason is that\nmost sentences are shorter than 50, just the suitable amount\nof capsules can extract enough features.\n3.6 Probing Experiment\nWhat does the global representation learn is an interesting\nquestion. Following Weng et al. (2017) , we do a probing\n9263\nInput 出台或者即将出台楼市调控政策的二线城市不止苏州，还包括合肥、南京等城市。\nReference In addition to Suzhou, other second-tier cities including Hefei and Nanjing will also introduce property market\nregulations and control policies.\nTransformer The second-tier cities, including Hefei and Nanjing, are not only Suzhou,but also the cities of Hefei.\nGRET The second-tier cities, not only Suzhou, but also Hefei and Nanjingwill also introduce property market\nregulations and control policies.\nInput 莫斯科旅游警察的人员招录、警务模式、装备配备给我们带来了很多启发。\nReference The recruiting, police mode and equipment of Moscow tourism police ofﬁcers have inspired us a lot.\nTransformer We have a lot of inspiration from theMoscow Travel Police, the policemodel, and the equipment.\nGRET We have a lot of inspiration by theMoscow Travel Police’s recruiting, police mode, and equipment.\nFigure 6: Translation cases from Transformer and our GRET model on the ZH→EN task.\nexperiment here. We train abag-of-words predictorby max-\nimizing P(ybow|sM ), where ybow is an unordered set con-\ntaining all words in the output sentence. The structure of the\npredictor is a simple feed-forward network which maps the\nglobal state to the target word embedding matrix.\nThen, we compare the precision of target words in the top-\nK words which are chosen through the predicted probability\ndistribution\n6. The results are shown in Table 5, the global\nstate from GRET can get higher precision in all conditions,\nwhich shows that the proposed method can obtain more in-\nformation about the output sentence and partial answers why\nthe GRET model could improve the generation quality.\n3.7 Analysis of Sentence Length\nTo see the effectiveness of the global representation, we\ngroup the EN→DE test set by the length of the input sen-\ntences to re-evaluate the models. The set is divided into 4\nsets. Figure 5 shows the results. We ﬁnd that our model\noutperforms the baseline in all categories, especially in the\nlonger sentences, which shows that fusing the global rep-\nresentation may help the generation of longer sentences by\nproviding more complete information.\n3.8 Case Study\nWe show two real-cases on the ZH→EN task to see the dif-\nference between the baseline and our model. These cases are\nshown in Figure 6. The “Source” indicates the source sen-\ntence and the “Reference” indicates the human translation.\nThe bold fontindicates improvements of our model; and the\nitalic fontindicates translation errors.\nEach output from GRET is decided by previous state and\nthe global representation. So, it can avoid some common\ntranslation errors like over/under translation, caused by the\nstrong language model of the decoder which ignores some\ntranslation information. For example, the over translation\nof “the cities of Hefei” in case 1 is corrected by the GRET\nmodel. Furthermore, providing global information can avoid\ncurrent state only focuses on the word-to-word mapping.\nIn case 2, the vanilla Transformer translates the “Moscow\nTravel Police” according to the source input “mosike lvyou\njingcha”’, but omits the words “de renyuan zhaolu”, which\nleads it fails to translate the target word “recruiting”.\n6Experiment details are shown in Weng et al. (2017) .\n4 Related Work\nSeveral work also try to generate global representation. In\nmachine translation, Lin et al. (2018b) propose a decon-\nvolutional method to obtain global information to guide the\ntranslation process in RNN-based model. However, the lim-\nitation of CNN can not model the global information well\nand there methods can not employ on the Transformer. In\ntext summarization, Chen (2018) also propose to incorpo-\nrate global information in RNN-based model to reduce repe-\ntition. They use an additional RNN to model the global rep-\nresentation, which is time-consuming and can not get the\nlong-dependence relationship, which hinders the effective-\nness of the global representation.\nZhang, Liu, and Song (2018) propose a sentence-state\nLSTM for text representation. Our method shows an alterna-\ntive way of obtaining the representation, on the implementa-\ntion of the Transformer.\nMany previous researches notice the importance of the\nrepresentations generated by the encoder and focus on mak-\ning full use of them. Wang et al. (2018a) propose to use Cap-\nsule network to generate hidden states directly, which inspire\nus to use capsules with dynamic routing algorithm to extract\nspeciﬁc and suitable features from these hidden states. Wang\net al.; Dou et al. (2018b; 2018) propose to utilize the hidden\nstates from multiple layers which contain different aspects of\ninformation to model more complete representations, which\ninspires us to use the states in multiple layers to enhance the\nglobal representation.\n5 Conclusion\nIn this paper, we address the problem that Transformer\ndoesn’t model global contextual information which will de-\ncrease generation quality. Then, we propose a novel GRET\nmodel to generate an external state by the encoder con-\ntaining global information and fuse it into the decoder dy-\nnamically. Our approach solves the both issues of how to\nmodel and how to use the global contextual information. We\ncompare the proposed GRET with the state-of-the-art Trans-\nformer model. Experimental results on four translation tasks\nand one text summarization task demonstrate the effective-\nness of the approach. In the future, we will do more analysis\nand combine it with the methods about enhancing local rep-\nresentations to further improve generation performance.\n9264\nAcknowledgements\nWe would like to thank the reviewers for their insightful\ncomments. Shujian Huang is the corresponding author. This\nwork is supported by the National Key R&D Program of\nChina (No. 2019QY1806), the National Science Foundation\nof China (No. 61672277), the Jiangsu Provincial Research\nFoundation for Basic Research (No. BK20170074).\nReferences\nAyana, S. S.; Liu, Z.; and Sun, M. 2016. Neural headline generation\nwith minimum risk training.arXiv preprint arXiv:1604.01904.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer normalization.\narXiv preprint arXiv:1607.06450.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural machine\ntranslation by jointly learning to align and translate.CoRR.\nChang, C.-T.; Huang, C.-C.; and Hsu, J. Y .-j. 2018. A hybrid word-\ncharacter model for abstractive summarization.CoRR.\nChen, G. 2018. Chinese short text summary generation model\ncombining global and local information. InNCCE.\nCho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning phrase\nrepresentations using rnn encoder–decoder for statistical machine\ntranslation. In EMNLP.\nDeng, Y .; Cheng, S.; Lu, J.; Song, K.; Wang, J.; Wu, S.; Yao, L.;\nZhang, G.; Zhang, H.; Zhang, P .; et al. 2018. Alibaba’s neural\nmachine translation systems for wmt18. InConference on Machine\nTranslation: Shared Task Papers.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding. arXiv.\nDou, Z.-Y .; Tu, Z.; Wang, X.; Shi, S.; and Zhang, T. 2018. Ex-\nploiting deep representations for neural machine translation. In\nEMNLP.\nFrazier, L. 1987. Sentence processing: A tutorial review.\nGehring, J.; Auli, M.; Grangier, D.; and Dauphin, Y . N. 2016. A\nconvolutional encoder model for neural machine translation.arXiv\npreprint arXiv:1611.02344.\nGehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin, Y . N.\n2017. Convolutional sequence to sequence learning.arXiv preprint\narXiv:1705.03122.\nGu, J.; Lu, Z.; Li, H.; and Li, V . O. 2016. Incorporating copying\nmechanism in sequence-to-sequence learning. InACL.\nGu, J.; Bradbury, J.; Xiong, C.; Li, V . O.; and Socher, R. 2018.\nNon-autoregressive neural machine translation. InICLR.\nHassan, H.; Aue, A.; Chen, C.; Chowdhary, V .; Clark, J.; Feder-\nmann, C.; Huang, X.; Junczys-Dowmunt, M.; Lewis, W.; Li, M.;\net al. 2018. Achieving human parity on automatic chinese to en-\nglish news translation.arXiv preprint arXiv:1803.05567.\nHu, B.; Chen, Q.; and Zhu, F. 2015. Lcsts: A large scale chinese\nshort text summarization dataset. InEMNLP.\nKingma, D. P ., and Ba, J. 2014. Adam: A method for stochastic\noptimization. CoRR.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet\nclassiﬁcation with deep convolutional neural networks. InNIPS.\nLi, P .; Bing, L.; and Lam, W. 2018. Actor-critic based train-\ning framework for abstractive summarization. arXiv preprint\narXiv:1803.11070.\nLin, J.; Sun, X.; Ma, S.; and Su, Q. 2018a. Global encoding for\nabstractive summarization. InACL.\nLin, J.; Sun, X.; Ren, X.; Ma, S.; Su, J.; and Su, Q. 2018b.\nDeconvolution-based global decoding for neural machine transla-\ntion. In ACL.\nLin, C.-Y . 2004. ROUGE: A package for automatic evaluation of\nsummaries. In ACL.\nLuong, M.; Pham, H.; and Manning, C. D. 2015. Effective ap-\nproaches to attention-based neural machine translation. InEMNLP.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: A\nmethod for automatic evaluation of machine translation. InACL\n.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.;\nLee, K.; and Zettlemoyer, L. 2018. Deep contextualized word\nrepresentations. arXiv preprint arXiv:1802.05365.\nSabour, S.; Frosst, N.; and Hinton, G. E. 2017. Dynamic routing\nbetween capsules. CoRR.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural machine\ntranslation of rare words with subword units. InACL.\nShaw, P .; Uszkoreit, J.; and V aswani, A. 2018. Self-attention with\nrelative position representations. InNAACL.\nSong, K.; Wang, K.; Y u, H.; Zhang, Y .; Huang, Z.; Luo, W.; Duan,\nX.; and Zhang, M. 2020. Alignment-enhanced transformer for\nconstraining nmt with pre-speciﬁed translations. InAAAI.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to se-\nquence learning with neural networks. InNIPS.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. InNIPS.\nWang, M.; Xie, J.; Tan, Z.; Su, J.; et al. 2018a. Towards linear time\nneural machine translation with capsule networks.arXiv.\nWang, Q.; Li, F.; Xiao, T.; Li, Y .; Li, Y .; and Zhu, J. 2018b. Multi-\nlayer representation fusion for neural machine translation. InCOL-\nING.\nWeng, R.; Huang, S.; Zheng, Z.; Dai, X.; and Chen, J. 2017. Neural\nmachine translation with word predictions. InEMNLP.\nYang, B.; Tu, Z.; Wong, D. F.; Meng, F.; Chao, L. S.; and Zhang, T.\n2018. Modeling localness for self-attention networks. InEMNLP.\nYang, B.; Li, J.; Wong, D. F.; Chao, L. S.; Wang, X.; and Tu, Z.\n2019. Context-aware self-attention networks. InAAAI.\nZhang, Y .; Liu, Q.; and Song, L. 2018. Sentence-state lstm for text\nrepresentation. In ACL.\nZhao, W.; Ye, J.; Yang, M.; Lei, Z.; Zhang, S.; and Zhao, Z. 2018.\nInvestigating capsule networks with dynamic routing for text clas-\nsiﬁcation. arXiv preprint arXiv:1804.00538.\nZheng, Z.; Huang, S.; Tu, Z.; DAI, X.-Y .; and CHEN, J. 2019. Dy-\nnamic past and future for neural machine translation. InEMNLP-\nIJCNLP.\n9265",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8942286968231201
    },
    {
      "name": "Machine translation",
      "score": 0.8439490795135498
    },
    {
      "name": "Computer science",
      "score": 0.7884952425956726
    },
    {
      "name": "Transformer",
      "score": 0.7062090635299683
    },
    {
      "name": "Sentence",
      "score": 0.6501274108886719
    },
    {
      "name": "Encoder",
      "score": 0.6399846076965332
    },
    {
      "name": "Natural language processing",
      "score": 0.5474706888198853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5349254608154297
    },
    {
      "name": "Decoding methods",
      "score": 0.5020861625671387
    },
    {
      "name": "Natural language understanding",
      "score": 0.4652325510978699
    },
    {
      "name": "Speech recognition",
      "score": 0.43967780470848083
    },
    {
      "name": "Natural language generation",
      "score": 0.433176726102829
    },
    {
      "name": "Representation (politics)",
      "score": 0.41567856073379517
    },
    {
      "name": "Natural language",
      "score": 0.4083330035209656
    },
    {
      "name": "Algorithm",
      "score": 0.19025784730911255
    },
    {
      "name": "Voltage",
      "score": 0.13643985986709595
    },
    {
      "name": "Engineering",
      "score": 0.07378610968589783
    },
    {
      "name": "Electrical engineering",
      "score": 0.059350550174713135
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    }
  ],
  "cited_by": 11
}