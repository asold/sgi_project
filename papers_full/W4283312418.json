{
  "title": "Counting Varying Density Crowds Through Density Guided Adaptive Selection CNN and Transformer Estimation",
  "url": "https://openalex.org/W4283312418",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3130283250",
      "name": "Yuehai Chen",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2083709059",
      "name": "Jing Yang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2473510192",
      "name": "Badong Chen",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2109410961",
      "name": "Shaoyi Du",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205884078",
    "https://openalex.org/W3009231981",
    "https://openalex.org/W2945574898",
    "https://openalex.org/W2977057439",
    "https://openalex.org/W3213366174",
    "https://openalex.org/W3015084401",
    "https://openalex.org/W3132676275",
    "https://openalex.org/W3112819693",
    "https://openalex.org/W2463631526",
    "https://openalex.org/W2741077351",
    "https://openalex.org/W2964209782",
    "https://openalex.org/W2964264515",
    "https://openalex.org/W3176047859",
    "https://openalex.org/W2964203052",
    "https://openalex.org/W3199696784",
    "https://openalex.org/W2120815373",
    "https://openalex.org/W2122243179",
    "https://openalex.org/W2123175289",
    "https://openalex.org/W2541389513",
    "https://openalex.org/W2072232009",
    "https://openalex.org/W2982021328",
    "https://openalex.org/W3047585969",
    "https://openalex.org/W3012297320",
    "https://openalex.org/W2963893037",
    "https://openalex.org/W2962720716",
    "https://openalex.org/W2514654788",
    "https://openalex.org/W2995582330",
    "https://openalex.org/W3004672782",
    "https://openalex.org/W6783507135",
    "https://openalex.org/W3176458063",
    "https://openalex.org/W3174519905",
    "https://openalex.org/W4312826597",
    "https://openalex.org/W3045455261",
    "https://openalex.org/W6787680731",
    "https://openalex.org/W3203845557",
    "https://openalex.org/W3177167987",
    "https://openalex.org/W2886443245",
    "https://openalex.org/W2963274387",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4225264236",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4210786150",
    "https://openalex.org/W6790375769",
    "https://openalex.org/W3129012257",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6789463137",
    "https://openalex.org/W4313182800",
    "https://openalex.org/W2895051362",
    "https://openalex.org/W2967069910",
    "https://openalex.org/W2963693541",
    "https://openalex.org/W3015469128",
    "https://openalex.org/W6785789763",
    "https://openalex.org/W3027606690",
    "https://openalex.org/W2966893608",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3127995199",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3113041473",
    "https://openalex.org/W3123221884",
    "https://openalex.org/W3087861058",
    "https://openalex.org/W3160910141",
    "https://openalex.org/W4297779827",
    "https://openalex.org/W3097305524"
  ],
  "abstract": "In real-world crowd counting applications, the crowd densities in an image vary greatly. When facing density variation, humans tend to locate and count the targets in low-density regions, and reason the number in high-density regions. We observe that CNN focus on the local information correlation using a fixed-size convolution kernel and the Transformer could effectively extract the semantic crowd information by using the global self-attention mechanism. Thus, CNN could locate and estimate crowds accurately in low-density regions, while it is hard to properly perceive the densities in high-density regions. On the contrary, Transformer has a high reliability in high-density regions, but fails to locate the targets in sparse regions. Neither CNN nor Transformer can well deal with this kind of density variation. To address this problem, we propose a CNN and Transformer Adaptive Selection Network (CTASNet) which can adaptively select the appropriate counting branch for different density regions. Firstly, CTASNet generates the prediction results of CNN and Transformer. Then, considering that CNN/Transformer is appropriate for low/high-density regions, a density guided adaptive selection module is designed to automatically combine the predictions of CNN and Transformer. Moreover, to reduce the influences of annotation noise, we introduce a Correntropy based optimal transport loss. Extensive experiments on four challenging crowd counting datasets have validated the proposed method.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nCounting Varying Density Crowds Through Density\nGuided Adaptive Selection CNN and Transformer\nEstimation\nYuehai Chen, Jing Yang, Badong Chen, Senior Member, IEEE Shaoyi Du, Member, IEEE\nAbstract—In real-world crowd counting applications, the\ncrowd densities in an image vary greatly. When facing density\nvariation, humans tend to locate and count the targets in low-\ndensity regions, and reason the number in high-density regions.\nWe observe that CNN focus on the local information correlation\nusing a ﬁxed-size convolution kernel and the Transformer could\neffectively extract the semantic crowd information by using the\nglobal self-attention mechanism. Thus, CNN could locate and\nestimate crowds accurately in low-density regions, while it is hard\nto properly perceive the densities in high-density regions. On\nthe contrary, Transformer has a high reliability in high-density\nregions, but fails to locate the targets in sparse regions.\nNeither CNN nor Transformer can well deal with this kind\nof density variation. To address this problem, we propose a\nCNN and Transformer Adaptive Selection Network (CTASNet)\nwhich can adaptively select the appropriate counting branch\nfor different density regions. Firstly, CTASNet generates the\nprediction results of CNN and Transformer. Then, considering\nthat CNN/Transformer is appropriate for low/high-density re-\ngions, a density guided adaptive selection module is designed to\nautomatically combine the predictions of CNN and Transformer.\nMoreover, to reduce the inﬂuences of annotation noise, we\nintroduce a Correntropy based optimal transport loss. Extensive\nexperiments on four challenging crowd counting datasets have\nvalidated the proposed method.\nIndex Terms—Crowd counting, Transformer, Adaptive Selec-\ntion.\nI. I NTRODUCTION\nD\nENSE crowd counting is an important topic in com-\nputer vision. Especially after the outbreak of coronavirus\ndisease (COVID-19), it plays a more essential role in video\nsurveillance, public safety, and crowd analysis [49]–[52], [57]–\n[60]. Most recent state-of-the-art works generate a pseudo\ndensity map by smoothing the sparse annotated points and\nthen train a CNN model by regressing the value at each pixel\nin this density map. However, as shown in Figure 1, a major\nchallenge for the task is the extremely large scale variation of\nThis work was supported by the National Key Research and Development\nProgram of China under Grant No. 2020AAA0108100, the National Natural\nScience Foundation of China under Grant No. 62073257, 62141223, and the\nKey Research and Development Program of Shaanxi Province of China under\nGrant No. 2022GY-076.\nYuehai Chen and Jing Yang are with School of Automation Sci-\nence and Engineering, Faculty of Electronic and Information Engineering,\nXi’an Jiaotong, Xi’an 710049, China (e-mail: cyh0518@stu.xjtu.edu.cn, jas-\nmine1976@xjtu.edu.cn).\nBadong Chen and Shaoyi Du is with Institute of Articial Intelligence and\nRobotics, College of Articial Intelligence, Xi’an Jiaotong University, Xi’an,\nShanxi 710049, China (e-mail: chenbd@xjtu.edu.cn, dushaoyi@gmail.com)\nCorresponding author: Shaoyi Du. Yuehai Chen and Jing Yang contributed\nequally to this work.\nFig. 1. There are large scale variations in the same scene and different scenes.\nThe intra-scene and inter-scene variations in appearance, scale, and perspective\nmake the problem extremely difﬁcult.\ncrowds, which arises from the wide viewing angle of cameras\nand the 2D perspective projection.\nIn recent years, numerous methods for handling large scale\nvariations have been proposed. One feasible workaround is\nto adopt a multi-column convolution neural network (CNN),\nwhich aggregates several branches with different receptive\nﬁelds for extracting multi-scale features [62], [80]. The scale\nvariations in images are continuous, while these methods only\nconsider several discrete receptive ﬁelds. They are unable to\ncover the continuous scale variation, which leads to feature\nredundancy among different branches [92]. Considering sim-\nplifying network architecture, some methods deploy single and\ndeeper CNNs to combine features from different layers [103].\nThey usually aggregate the features from different layers in a\nscale-agnostic way, which may lead to an inconsistent mapping\nbetween feature levels and target scales [79]. The two kinds of\naforementioned methods beneﬁt from the multi-scale feature\nrepresentations and have achieved inspiring performances.\nHowever, due to the limited respective ﬁeld and presentation\nability of CNN, it is still very difﬁcult to accurately estimate\nthe count in sparse and dense regions at the same time.\nIn the real world, humans would adopt appropriate counting\nmodes for different density regions: they would accurately\nlocate and count the targets in sparse regions and reason\nthe target number in dense regions [29]. Motivated by the\nhuman counting behaviors, an ideal counting method should\narXiv:2206.10075v2  [cs.CV]  14 Oct 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nFig. 2. Visualization of the predicted results on a image from (a) VGG\nnetwork and (b) Transformer network.\nhave an adaptive ability to choose the appropriate counting\nmode according to the crowd density. To be speciﬁc, it is\nexpected to count through localizing targets in low density\nregions; whereas, in congested regions, it should behave in an\ninference mode.\nCNN constructs a powerful broad ﬁlter by focusing on\nthe local information correlation, and show effectiveness in\ncomputer vision task. But it is usually too dependent on\ncertain local information, which may lead to the unreliability\nof the extracted semantic information. Unlike the CNN, the\nTransformer adopts the Multi-Head Self-Attention mechanism\nwhich could provide the global receptive ﬁeld. Such a Multi-\nHead Self-Attention mechanism can understand the relation-\nship of different regional semantics in the entire image. In\nFigure 2, we visualize the predicted results of CNN and\nTransformer networks to show their differences. As shown in\nthe green dash framework of Figure 2 (a), the CNN estimation\ncould accurately locate and count targets in sparse crowds,\nwhich is reliable. On the other hand, in crowded regions in\nthe red dash framework, the corresponding object sizes tend\nto be very small and hard to locate. The CNN estimation\nis unable to accurately reﬂect the density in these regions,\nleading to a wrong estimation. While the Transformer network\ncould perform better on the occasion, since the Transformer\nestimation has signiﬁcantly different response intensities in\ndifferent density regions. This means that the Transformer\ncan effectively perceive the density of the area. However,\nin the sparse-crowd regions in the green dash framework of\nFigure 2 (b), the response position of Transformer estimation\nis inconsistent with the target position.\nBased on the above analysis, we ﬁnd that the CNN and\nTransformer networks have their different strengths on differ-\nent crowd densities regions. The CNN based method could\nlocalize and count each person precisely in the low density\nregions since they concentrate on the local pixel correlation.\nHowever, its reliability degenerates in crowded regions due to\nthe inaccuracy of regional density information perception. The\nTransformer based approach is preferred for congested scenes.\nWithout localization information for each target, applying\nthem to sparse scenes may lead to wrong estimation. Motivated\nby this understanding, we propose a novel crowd counting\nframework named CNN and Transformer Adaptive Selection\nNetwork (CTASNet), which is capable of adaptively locating\nthe target in the low density regions and perceiving the crowd\ndensity in the high density regions.\nTo be speciﬁc, for a given image, the CTASNet ﬁrst\ngenerates two kinds of crowd density maps through CNN and\nTransformer networks, respectively. To adaptively decide the\ndifferent counting modes for the sparse and dense regions, a\ndensity guided Adaptive Selection Module (ASM) is proposed\nto obtain the ﬁnal prediction through automatically selecting\nCNN/Transformer estimations in low/high density regions,\nrespectively. Note that point annotation is widely adopted in\ncrowd datasets, which is sparse and could only occupy a pixel\nof the entire human head. There are unavoidable annotation\nerrors. To alleviate this issue, we design a transport cost\nfunction based on correntropy [87] in an optimal transport\nframework that could explicitly tolerate the annotation errors.\nIn summary, we make the following contributions:\n• To model the different counting modes of humans in\nsparse and dense regions, we design a CNN and Trans-\nformer Adaptive Selection Network for crowd counting.\n• We propose a density guided Adaptive Selection Mod-\nule to automatically choose CNN/Transformer network\nestimations for low/high density regions.\n• We design a transport cost function based on correntropy\nin an optimal transport framework to explicitly tolerate\nthe annotation errors.\n• We conduct extensive experiments on four datasets to\ndemonstrate the superiority of our method against state-\nof-the-art competitors.\nII. R ELATED WORKS\nA. Crowd Counting\nTraditional crowd counting algorithms are mainly di-\nvided into two categories: detection-based methods [73], [74]\nand regression-based methods [75]–[77]. These methods are\nmostly based on hand-crafted features which are specially\ndesigned by domain experts. When faced with complex scenes\nsuch as congestion and scale variation, the performances of\nthese hand designed methods are disappointing.\nScale Variation. To achieve accurate crowd counting in\ncomplex scenarios, recent attention has shifted to deep learn-\ning. The common way to cope with large scale variation is to\nobtain a richer feature representation [79]. MCNN designed\na Multi-Column Neural Network to estimate crowd numbers\naccurately from different perspectives with three branches\n[62]. Based on MCNN, Switching CNN trains a switch\nclassiﬁer to select the best branch for density estimation [80].\nDADNet uses different dilated rates in each parallel column\nto obtain multi-scale features [28]. [4] used Inception-v3 as\nthe backbone and proposed a novel curriculum loss function\nto resolve the scale variance issue. [5] aggregated different\nmodules to adaptively encodes the multi-scale contextual infor-\nmation for accurate counting in both dense and sparse crowd\nscenes. In summary, these methods employ multiple branches\narchitecture to address the scale variation problem, which may\nintroduce signiﬁcant feature redundancy [92].\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nContext. In contrast to those methods designing speciﬁc\narchitectures, recent methods concentrate on incorporating\nrelated information like high-level semantic information and\ncontextual information into networks for obtaining rich feature\nrepresentations. CrowdNet used a combination of high-level\nsemantic information and low-level features for accurate crowd\nestimation [95]. [26] incorporated a high-level prior into the\ndensity estimation network enabling the network to learn glob-\nally relevant discriminative features for lower count error. [16],\n[25] makes use of contextual information to predict both local\nand global counts. This kind of method has been demonstrated\neffective but is still difﬁcult to accurately estimate the count\nin the sparse and dense regions at the same time. The reason\nmay be that the ﬁxed-size convolution kernel of CNN would\nintroduce a limited receptive ﬁeld.\nLoss function. An appropriate loss measure can help to\nimprove the counting performance. BL [61] proposes a point-\nwise Bayesian loss function between the ground-truth point\nannotations and the aggregated dot prediction generated from\nthe predicted density map. DM-Count [98] design a balanced\nOptimal Transport loss with an L2 cost function, to match\nthe shape of the two distributions. Based on DM-Count [98],\nGL and UOT [104], [105] develop an unbalanced Optimal\nTransport loss for crowd counting, which could output sharper\ndensity maps. Overly strict pixel-level spatial invariance would\ncause overﬁt noise in the density map generation. [3] use\nlocally connected Gaussian kernels to replace the original\nconvolution ﬁlter to overcome the annotation noise in the\nfeature extraction process.\nCrowd Localization. To perform counting, density map\nestimation, and localization simultaneously, [7] uses point-\nlevel annotations to train a typical object detector. [6] proposed\na straightforward crowd localization framework that obtains\nthe positions and the crowd counts by segmenting crowds\ninto independent connected components. [8] propose a purely\npoint-based framework for joint crowd counting and individ-\nual localization. [9] consider the counting and localization\ntasks as a pixel-wise dense prediction problem and integrate\nthem into an end-to-end framework. These crowd localization\nmethods have achieved inspiring performances on the crowd\ncounting task. However, they may have limited performance\non the extremely dense datasets (e.g., UCF-QNRF [90] and\nUCF CC 50 [89] dataset). Different from these localization\nmethods, we design an adaptive selection module making\nCNN and Transformer models appropriate for estimating less\ndense and more dense regions, respectively, which could\nachieve better performance on extremely dense datasets.\nB. Transformer\nTransformers were introduced by [24] as a new attention-\nbased building block for machine translation. Dynamic atten-\ntion mechanism and global modeling ability enable Trans-\nformer to exhibit strong feature learning ability [15]. In recent\nyears, Transformer has become comparable to CNN methods\nin computer vision [20]. Speciﬁcally, DETR ﬁrstly utilizes\na CNN backbone to extract the visual features, followed by\nthe Transformer for the object detection [23]. ViT is the ﬁrst\none that applies Transformer-encoder to images patch and\ndemonstrates outstanding performances [22]. SETR extended\na pure Transformer from image classiﬁcation to a spatial\nlocation task of semantic segmentation [21]. More recently,\nthe self attention mechanism and global modeling ability have\nboosted the effective applications of Transformer in various\ntasks such as object tracking and video classiﬁcation [19], [19].\nHowever, since the Transformer contains no recurrence and\nno convolution, the self-attention mechanism in the Trans-\nformer does not explicitly model relative or absolute position\ninformation [14]. A sub-optimal approach is to add ”positional\nencodings” to the input embeddings at the bottoms of the\nencoder and decoder stacks [24]. Convolution can implicitly\nencode absolute positions, especially zero padding, and bor-\nders act as anchors to derive spatial information [12], [13].\nC. Correntropy\nCorrentropy, a novel similarity measure, is deﬁned as the ex-\npectation of a kernel function between two random variables.\nIt has been successfully applied in robust machine learning\nand signal processing to combat large outliers [87]. For two\ndifferent random variables X and Y, the correntropy between\nX and Y is deﬁned as:\nV(X,Y ) =E[κ(X,Y )] =\n∫\nκ(x,y)dFXY(x,y) (1)\nwhere E is the expectation operator, κ(x,y) is a shift-invariant\nMercer kernel, and FXY (x,y) denotes the joint distribution\nfunction of (X,Y ). The most popular kernel used in corren-\ntropy is the Gaussian kernel:\nκσ(x,y) = 1√\n2πσ exp\n(\n−∥x−y∥2/2σ2)\n(2)\nwhere the σ >0 denotes the kernel size (or kernel bandwidth).\nIII. P ROPOSED METHOD\nIn this section, we ﬁrst describe the proposed crowd count-\ning framework, CNN and Transformer Adaptive Selection\nNetwork which is shown in Figure 3. Then we will present\na novel density guided Adaptive Selection Module (ASM),\nwhich could automatically choose the estimations of CNN\nand Transformer for different density regions. Finally, we\npropose a transport cost function based on correntropy in\nan optimal transport framework, to explicitly tolerate the\nannotation errors.\nA. Overview\nFigure 3 presents an overview of the framework. There are\ntwo parallel branches in the proposed framework: The trans-\nformer Estimation branch and the CNN Estimation branch.\nFor each image I, we ﬁrst use the ﬁrst 13 convolution layers\nin the VGG16 backbone to extract the high-level feature\npresentations F4, F5. In the Transformer estimation branch, the\ndeep feature F5 is ﬂattened and transmitted into a Transformer\nencoder. Then, a regression decoder is utilized to predict the\nﬁnal density map Dt. In the CNN estimation branch, the top\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nFig. 3. The framework of CNN and Transformer Adaptive Selection Network.\nfeature F5 is passed through an Atrous spatial pyramid pooling\n(ASPP) [11] module to obtain feature F5’, which obtains\na larger receptive ﬁeld. Afterwards, a regression decoder\nuses concatenate and bilinear upsampling to fuse the multi-\nscale features F4 and F5’ into the ﬁnal density map Dc.\nThen the ﬁnal output is obtained by automatically combining\nthe predictions from CNN and Transformer branch with a\ndesigned density guided Adaptive Selection Module. Finally,\nwe design a correntropy based OT loss to supervise the ﬁnal\noutput.\nB. CNN and Transformer Adaptive Selection Network\n[29] shows that: in the low crowd density regions, humans\nwould ﬁrst locate the head and then count the number; while\nfacing the high crowd density regions, they may reason about\nthe number of people by estimating the degree of density. To\nmodel the human different counting modes in sparse and dense\nregions, we design a CNN and Transformer Adaptive Selection\nNetwork for crowd counting which is presented in Figure 3.\nThe Transformer estimation branch is responsible for the dense\ncrowd regions, while the CNN estimation branch concentrates\non predicting the sparse crowd regions. Note that Transformers\nrequire high computational costs. The cause of this challenge\nis the need to perform attention operations that have quadratic\ntime and space complexity according to the context size [10].\nTo reduce computational consumption, we employ a VGG\n16 backbone to obtain lower-resolution feature representations\nwhich would be fed into the Transformer estimation branch\nand CNN estimation branch, respectively. Speciﬁcally, given\nthe initial image I ∈ R3×W×H, where W and H are the\nimage width and height, a VGG-16 backbone generates the\nlower-resolution feature representations F4 ∈ RC×W\n8 ×H\n8 ,\nF5 ∈RC×W\n16 ×H\n16 .\nTransformer Estimation Branch: The Transformer encoder\nadopts self-attention layers, which can connect all pairs of\ninput positions to consider the global relations of current fea-\ntures. As a result, the Transformer encoder could understand\nthe relationship of different regional semantics in the entire\nFig. 4. The proposed Transformer encoder consists of Multi-Head Attention,\nLayerNorm (LN) and a feed forward network (FFN).\nimage, which is beneﬁcial to perceive the regional density.\nIt is suitable to use a Transformer encoder to model human\nreasoning in dense regions.\nTo be speciﬁc, we only adopt the Transformer encoder\nwith a regression decoder. As shown in Figure 4, the pro-\nposed Transformer encoder consists of Multi-Head Atten-\ntion (MHA), Layer Normalization (LN), and a feed forward\nnetwork (FFN). Meanwhile, the residual connection is also\nemployed. The output of the Transformer encoder is calculated\nby:\nZ′\nl = MHA (Zl−1) +Zl−1 (3)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nFig. 5. The detail structure of the proposed Adaptive Selection Module.\nZl = LN(FFN (LN(Z′\nl)) +LN(Z′\nl)) (4)\nwhere Zl is the output of the lth ∈ 1,...,N Transformer\nencoder, N is the Transformer encoder number. Z0 is the input\nof the ﬁrst Transformer layer. To be speciﬁc, the Transformer\nencoder expects a sequence as input, hence we ﬂatten the\nspatial dimensions of F5 ∈RC×W\n16 ×H\n16 into one dimension,\nresulting in a C×WH\n256 feature Z0.\nThe MHA is the self-attention layer of the Transformer,\nwhich enables to measure the similarity of all input pairs to\nbuild the global relations of current features. At the MSA, the\ninput consists of query (Q), key (K), and value (V), which are\ncomputed from Zl−1:\nQ= Zl−1WQ, K = Zl−1WK, V = Zl−1WV (5)\nMHA (Zl) = softmax\n(QKT\n√\nd\n)\nV (6)\nwhere 1√\nd is a scaling factor based on the vector dimension\nd. WQ,WK,WV ∈Rd×d three learnable weight matrices for\nprojections.\nThe FFN contains two linear layers with a RELU activation\nfunction. Speciﬁcally, the ﬁrst linear layer of FFN expands the\nfeature embedding dimension from d to 4d, while the second\nlayer shrinks the dimension from 4d to d. Finally, the output\nZN of the Transformer encoder is fed into a regression decoder\nto generate the estimated density map Dt.\nSince the Transformer encoder regards the input as a disor-\ndered sequence and indiscriminately considers all correlations\namong features, the obtained feature is position-agnostic. As\na result, the prediction of the Transformer cannot have the\nability to locate targets in sparse regions.\nCNN Estimation Branch: Convolution is found to implicitly\nencode absolute positions, using zero padding, and borders act\nas anchors [13]. Therefore, we proposed a CNN Estimation\nBranch to focus on the sparse crowd regions. As illustrated\nin Figure 5, the heads in sparse regions are large, thus we\nﬁrstly input the feature presentation F5 into an Atrous spatial\npyramid pooling (ASPP) [11] module to obtain the scale-aware\ncontextual feature F5’. The ASPP module applies dilated\nconvolution with different rates (1,6,12,18) to the obtained\nfeature F5 for large receptive ﬁelds. To obtain accurate location\ninformation in the regression decoder, we upsample the scale-\naware contextual feature F5’ and concatenate it with the lower\nfeature F4. Finally, the concatenated features are used to\ngenerate the ﬁnal density map Dc.\nC. Density Guided Adaptive Selection Module\nAfter the predicted density maps Dt, Dc are acquired, we\ntry to select the proper prediction for different density regions.\nThe most common selection approach is to minimize counting\nerror for a speciﬁc patch. However, since we could not obtain\nthe ground truth during inference in advance, it is infeasible to\ndecide which prediction to select for the patch. As discussed\nabove, Transformer Estimation Branch is suitable to reason the\nnumber in high-density regions and CNN Estimation Branch\ncould locate and count the target in the low-density regions.\nTherefore, we proposed a density guided adaptive selection\nmodule, which can automatically choose proper prediction\nmanners for different density regions.\nIn detail, as illustrated at the Figure 5, we compute the\npredicted density map Di,i ∈c,t as:\ndbi = Fi1 (Pave(Di,k(i)) ,θi1) (7)\ndati = Fi2 (dbi,θi2) (8)\nwhere dbi is the generated upsampled block density map\nand dati is the generated normalized attention map. The\nfunction Fi1 contains a 1 × 1 convolution network and bilinear\ninterpolation. The function Fi2 consists of a 1 × 1 convolution\nnetwork and a sigmoid function. θij(j =1, 2) is the learned\nparameter in the 1 × 1 convolution of function Fij(j =1,\n2). Pave(.,k(i)) averages the predicted density maps into\nk(i) ×k(i) blocks. We do this because the scale in a certain\nregion is similar and the value in the block density map rep-\nresents the regional density. We apply the Transformer/CNN\nprediction results in the high/low density regions. In practice,\nwe use k(i) = 6for averaging pooling since it shows better\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nperformance compared with other settings ( k(i) = 1,2,4,8).\nTo make the block density map Pave(Di,k(i)) obtain the\nsame size of the predicted density map, we adopt the function\nFi1. To obtain a normalized attention map dati, the function\nFi2 is employed. With such an operation, the obtained atten-\ntion map could highlight the high-density region.\nTo further increase the difference between dense and sparse\nregions, we multiply the upsampled block density map dbi and\nattention map dati, and use sigmoid normalization to obtain\nthe ﬁnal attention map Aati.\nAati = sigmoid(dbi ×dati) (9)\nThe value in the ﬁnal attention map Aati reﬂects the\nregional density. Our goal is to use the Transformer estimation\nbranch in the dense regions and adopt the CNN estimation\nbranch in sparse regions. Therefore, the ﬁnal density map ˆz is\ncomputed by:\nˆz = Aatt ×Dt + (1−Aatc) ×Dc (10)\nAs a result, this would make the network automatically\nchoose the proper prediction manner for different density\nregions.\nD. The Optimal Transport Loss based on Correntropy\nFor achieving target localization in sparse regions and\noptimizing the entire network, we adopt the loss function\nproposed in DM-Count [98]:\nℓ(z,ˆz) =ℓC(z,ˆz) + 0.1ℓOT(z,ˆz) + 0.01∥z∥1ℓTV (z,ˆz) (11)\nwhere ∥z∥ denote the vectorized dot-annotation map, ∥ˆz∥\nis the vectorized predicted density map. ℓC(z,ˆz) =\n|∥z∥1 −∥ˆz∥1|is the counting loss, ∥·∥1 denote the L1 norm\nof a vector.\nℓOT(z,ˆz)\ndef.\n= min\nT∈U(z,ˆz)\n⟨C,T⟩\ndef.\n=\n∑\ni,j\nCi,jTi,j (12)\nwhere ℓOT is the Optimal Transport (OT) loss. The ground\ntruth z is the dot map. C ∈ Rn×m\n+ is the transport\ncost matrix, whose item Cij = c(z,ˆz) measures the\ncost for moving probability mass on pixel z to pixel ˆz.{\nT ∈Rn×m\n+ : T1n = z,TT1m = ˆz\n}\n(1n is the transport ma-\ntrix, which assigns probability masses at each location z to ˆz\nfor measuring the cost. U is the set of all possible ways to\ntransport probability masses from z to ˆz.\nℓTV (z,ˆz) =\n\nz\n∥z∥1\n− ˆz\n∥ˆz∥1\n\nTV\n= 1\n2\n\nz\n∥z∥1\n− ˆz\n∥ˆz∥1\n\n1\n(13)\nThe TV loss can increase the stability of the training proce-\ndure.\nThe annotation label provided in popular datasets is in\na form of sparse point annotation, which occupies a very\nsmall portion of target [17]. The annotation noise arises from\nhuman annotation, which in general exists in this kind of\nannotation label. The transport cost function adopted in [98]\nis the Euclidean distance between two pixels, Cij = L2\nij =xi −yj\n2\n2. As shown in Figure 6 (a), the Euclidean distance\nis unable to tolerate the annotation noise (i.e., the displacement\nof the annotated locations). If the annotated locations shifted,\nthe distance would increase which result in a large transport\ncost. This means that the cost function based on Euclidean\ndistance is sensitive to annotation noise. To alleviate negative\ninﬂuences by annotation noises, we propose a transport cost\nfunction based on correntropy for crowd counting:\nCij = ∥ai −bj∥2\nκσ(ai,bj) = ∥ai −bj∥2\nexp (−∥ai −bj∥2/2σ2) (14)\nwhere κσ(ai,bj) is a shift-invariant Gaussian kernel. As\nillustrated in Figure 6 (b), the proposed correntropy based\ntransport cost function can alleviate negative inﬂuences caused\nby annotation error, since it is insensitive to annotation position\noffset within a certain range.\nFig. 6. Comparison of transport cost functions based on Euclidean distance\nand Correntropy.\nIV. E XPERIMENTS\nIn this section, we present experiments evaluating the pro-\nposed network, CTASNet. We ﬁrst present detailed experimen-\ntal setups including network architecture, training details, and\nevaluation metrics. Then, we compare the proposed methods\nwith recent state-of-the-art approaches. Finally, we conduct\nablation studies to verify the effectiveness of the proposed\nmethod.\nA. Experimental Setups\nNetwork Architecture. We adopt VGG16 as our backbone\nnetwork that is pre-trained on ImageNet. Speciﬁcally, follow-\ning [5], we remove the classiﬁcation part of VGG-16 (fully-\nconnected layers) and adopt the ﬁrst 13 layers from VGG16\nas the backbone. In the Transformer Estimation Branch, we\nrefer to [24] for the structure of Transformer encoder. The\nregression decoder in the Transformer Estimation Branch\nconsists of a bilinear upsampling, two 3×3 convolution layers\nwith 256 and 128 channels, and a 1×1 convolution layer to get\nthe ﬁnal output. In the CNN Estimation Branch, the regression\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nTABLE I\nRESULTS ON THE SHANGHAI TECH , UCF CC 50, UCF-QNRF AND NWPU DATASETS .\nMethod ShanghaiTech A ShanghaiTech B UCF CC 50 UCF-QNRF NWPU\nMAE MSE MAE MSE MAE MSE MAE MSE MAE MSE\nMCNN [62] 110.2 173 .2 26.4 41 .3 377.6 509 .1 277.0 426 .0 232.5 714 .6\nSwitch-CNN [80] 90.4 135 .0 21.6 33 .4 318.1 439 .2 228 445 − −\nCSRNet [92] 68.2 115 .0 10.6 16 .0 266.1 397 .5 120.3 208 .5 121.3 387 .8\nSANet [91] 67.0 104 .2 8.4 13 .6 258.4 334 .9 − − − −\nCAN [93] 62.3 100 7.8 12 .2 212.2 243 .7 107 183 106.3 386 .5\nBL [61] 62.8 101 .8 7.7 12 .7 229.3 308 .2 88.7 154 .8 105.4 454 .0\nSFCN [97] 67.0 104 .5 8.4 13 .6 258.4 334 .9 102.0 171 .4 105.7 424 .1\nADSCNet [95] 55.4 97 .7 6.4 11.3 198.4 267 .3 71.3 132.5 − −\nCG-DRCN-CC-Res101 [72] 60.2 94 .0 7.5 12 .1 − − 95.5 164 .3 − −\nSASNet [79] 53.6 88.4 6.4 10.0 161.4 234.5 85.2 147 .3 − −\nNoiseCC [30] 61.9 99 .6 7.4 11 .3 − − 85.8 150 .6 96.9 534 .2\nDM-Count [98] 59.7 95 .7 7.4 11 .8 211.0 291 .5 85.6 148 .3 88.4 388.4\nCTASNet(ours) 54.3 87.8 6.5 10.7 158.1 221.9 80.9 139.2 94.4 357.6\ndecoder consists of four convolution layers with 256, 64, 32,\nand 1 channel, respectively. The kernel sizes of the ﬁrst three\nlayers are 3 × 3 and that of the last is 1 × 1.\nTraining Details. We ﬁrst do the image augmentation using\nrandom crop and horizontal ﬂipping. The random crop size\nis 512 ×512 in all datasets except ShanghaiTech A. As some\nimages in ShanghaiTech A contain smaller resolution, the crop\nsize for this dataset changes to 224 ×224. In all experiments,\nwe use the Adam algorithm with a learning rate 10−5 to\noptimize the network parameters.\nEvaluation Metrics. The widely used mean absolute error\n(MAE) and the mean squared error (MSE) are adopted to\nevaluate the performance. The MAE and MSE are deﬁned\nas follows:\nMAE = 1\nN\nN∑\ni=1\n⏐⏐⏐ˆCi −Ci\n⏐⏐⏐ (15)\nMSE =\n√1\nN\nN∑\ni=1\n⏐⏐⏐ˆCi −Ci\n⏐⏐⏐\n2\n(16)\nwhere N is the number of test images, ˆCi and Ci are the\nestimated count and the ground truth, respectively.\nB. Comparisons with State-of-the-Arts\nWe compare our proposed methods with other state-of-\nthe-art methods on several public crowd datasets, including\nShanghaiTech A [62], ShanghaiTech B [62], UCF CC 50\n[89], UCF-QNRF [90] and NWPU [45]. The quantitative\nresults of counting accuracy are listed in Table I.\nShanghaiTech A Dataset. ShanghaiTech A dataset is col-\nlected from the Internet and consists of 482 (300 for train, 182\nfor test) images with highly congested scenes. The images in\nShanghaiTech A are highly dense with crowd counts between\n33 to 3139. Our CTASNet achieves the lowest MSE and the\nsecond lowest MAE on ShanghaiTech A. Compared to DM-\nCount, CTASNet signiﬁcantly boosts its counting accuracy on\nShanghaiTech A. Speciﬁcally, the improvements are 9.04%\nand 8.25% for MAE and MSE.\nShanghaiTech B Dataset. The ShanghaiTech B dataset con-\ntains 716 (400 for train, 316 for test) images taken from\nbusy streets in Shanghai. The images in ShanghaiTech B\nare less dense with the number of people varying from 9\nto 578. It can be observed that the proposed CTASNet can\ndeliver comparable results with the best method (SASNet).\nThe effect of our CTASNet could not outperform SASNet.\nThe reason may be that ShanghaiTech B dataset contains\nfewer people than other popular datasets. There are fewer\nextremely dense regions that are suitable for reasoning, thus\nour Transformer estimation branch is not so useful in these\nlow-density scenes. Compared with methods with the DM-\nCount [98] performance, the CTASNet reduces the MAE by\n12.16% and MSE by 9.32%.\nUCF CC 50. UCF CC 50 is an extremely dense crowd\ndataset but includes 50 images of different resolutions [89].\nThe numbers of annotations range from 94 to 4,543 with an\naverage number of 1,280. Due to the limited training samples,\n5-fold cross-validation is performed following the standard\nsetting in [89]. As shown in Table I, our CTASNet surpasses\nall the other methods. In particular, our method could achieve\nthe best performance with 5.77% MAE and 8.68% MSE\nimprovement compared to SASNet [79] with the second best\nperformance.\nUCF-QNRF. UCF-QNRF is a challenging dataset that has a\nmuch wider range of counts than currently available crowd\ndatasets [90]. Speciﬁcally, the dataset contains 1,535 (1,201\nfor train, 334 for test) jpeg images whose number ranges\nfrom 816 to 12,865. By combining the CNN and Transformer\nestimations, our method can achieve the second best perfor-\nmance with an MAE of 80.9 and MSE of 139.2. Note that\nour CTASNet makes a reduction of 6.34% on MSE compared\nwith DM-Count.\nNWPU Dataset. The NWPU dataset is the largest-scale and\nmost challenging crowd counting dataset publicly available\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\n[45]. The dataset consists of 5,109 (3,109 for train, 500 for\nval , 1,500 for test) images whose number ranges from 0 to\n20,003. Note that the ground truth of the test image sets is not\nreleased and researchers could submit their results online for\nevaluation. As illustrated in Table I, our CTASNet achieves\nthe lowest MSE and the second lowest MAE on NWPU.\nC. Ablation Study\n1) Effect of each component:\nQuantitative Analysis. The proposed CTASNet is composed\nof four components: CNN Estimation Branch (CEB), Trans-\nformer Estimation Branch (TEB), Density Guided Adaptive\nSelection Module (ASM) and Correntropy based Optimal\nTransport loss (COT). We perform ablation studies on Shang-\nhaiTech A dataset to analyze the effect of each component.\nWe ﬁrst adopt the CEB with different loss functions L2 loss\n[92], Bayesian Loss (BL) [61], optimal transport (OT) loss in\nDM-Count [98] and the proposed COT loss. Then, we use the\nCEB and TEB for prediction, respectively. Finally, we adopt\ntwo strategies (”concat” and ASM) to combine the predictions\nof CEB and TEB. To be speciﬁc, in the ”concat” strategy,\nwe concat the features generated from the encoders in CEB\nand TEB. Afterwards, we feed the obtained feature into the\nregression decoder for the ﬁnal prediction. While ASM is the\nproposed Adaptive Selection Module in the Methods Section.\nAll detail results are illustrated in Table II. Comparing the\nresults of CEB with different loss functions, we could observe\nthat the CEB with our COT loss achieves the best result.\nSpeciﬁcally, by replacing L2 loss with the proposed COT loss,\nthe performance of CEB is improved by 16.1% in MAE and\n17.7% in MSE. This illustrates that the proposed COT loss\ncan signiﬁcantly boost the counting accuracy.\nOne could observe that using both CNN and Transformer for\nthe prediction can effectively improve the counting accuracy.\nSpeciﬁcally, such a simple ”concat” strategy can achieve an\nMAE of 56.4 and an MSE of 90.3. However, for a speciﬁc\nregion, if we aggregate the predictions by adaptively selecting\nthe different counting modes in different density regions, the\nMAE and MSE are improved to 54.3 and 87.8, denoted by\nthe ’CEB + TEB + ASM + COT’ in Table II. This signiﬁcant\nimprovement demonstrates the great potential of automatically\nselecting the appropriate counting mode in different density\nregions.\nTABLE II\nANALYSIS OF THE EFFECT OF DIFFERENT COMPONENTS . ALL\nEXPERIMENTS ARE PERFORMED ON SHANGHAI TECH A DATASET.\nComponents MAE MSE\nCEB + L2 67.6 113.9\nCEB + BL 62.4 100.5\nCEB + OT loss in DM-Count [98] 59.2 96.4\nCEB + COT 56.7 93.7\nTEB + COT 61.2 95.4\nCEB + TEB + concat + COT 56.4 90.3\nCEB + TEB + ASM + COT 54.3 87.8\nQualitative Analysis. To further demonstrate the advantage\nof the ASM strategy, we visualize the predictions of ’CEB+\nCOT’, ’TEB+ COT’ and ’CEB + TEB + ASM + COT’\nin Figure 7. Speciﬁcally, (a) is the input image; (b) is the\nprediction of TEB; (c) is the prediction of CEB; (d) is\nthe prediction of CTASNet; (e) is the prediction of TEB in\nCTASNet; (f) is the prediction of CEB in CTASNet.\nFirstly, comparing the yellow frames in (a), (b), and (c),\nwe could observe that Transformer has signiﬁcantly different\nresponse intensities in dense regions of varying degrees. This\nillustrates that Transformer has a strong ability to perceive the\nregional density. While for sparse-crowd regions in red frames,\nthe Transformer fails to locate and count accurately the target\nhead. CNN shows opposite characteristics in prediction: CNN\ncould well locate and count in the sparse-crowd region but has\na similar response intensity in dense-crowd regions of varying\ndegrees. It means that CNN may be suitable for sparse-crowd\nregion counting and Transformer may work well in dense-\ncrowd region counting.\nThe (d) image in Figure 7 presents our CTASNet predic-\ntions. In Figure 7 (d), it is easy to observe that our CTASNet\ncan locate targets in low-density regions and has different\nresponse intensities for different density regions. This veriﬁes\nthat our CTASNet could take full advantage of CNN and\nTransformer to achieve accurate counting on both sparse and\ndense regions. More speciﬁcally, in Figure 7 (e) and (f), we\nvisualize the predictions of TEB and CEB in the proposed\nCTASNet. One could observe that the TEB prediction only\nfocuses on dense regions. Also, the CEB prediction mostly\nconcentrates on sparse regions. These results fully demonstrate\nthat our CTASNet can adaptively select the appropriate count-\ning mode for different density regions. This is consistent with\npeople’s counting behavior.\nAnalyze the predictions of CEB and TEB. To further show\nthat the CNN and the Transformer branches are responsible\nfor less dense and more dense regions, respectively, we con-\nduct a quantitative experiment. Speciﬁcally, we ﬁrst choose\na relatively dense subset (contains 26 images, about 15%\ntest set) from the test images (contains 181 images) in the\nShanghaiTech A dataset. Then, we divide them into 4 ×4\nblocks which contain low and high-density regions. Finally,\nwe compare the count predictions and ground truth between\nCNN and Transformer branches in Figure 8.\nAs shown in Figure 8 (A), it could be observed that the\nprediction of CTASNet is close to the ground truth. While for\nthe extremely dense regions (ground truth of region >400),\nthe CTASNet underestimates them. The reason may be that\nthese extremely dense regions are too less to learn effective\nfeatures for accurate estimation.\nFigure 8 (B) has presented that, when facing the less dense\nregions (ground truth of region <250), the prediction of CEB\nis close to the ground truth. While for TEB in Figure 8 (C),\nwe could observe that the prediction of TEB approximates 0.\nThis means that the estimation of TEB contributes less to these\nsparse regions. It conﬁrms the CNN branch is responsible for\nless dense regions. On the contrary, when estimating more\ndense regions (ground truth of region >325), the prediction\nof TEB occupies a main component of the ﬁnal estimation\ncompared to the prediction of CEB. It illustrates that the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nFig. 7. Visualization of the prediction results: (a) Input Image; (b) Prediction of Transformer; (c) Prediction of CNN; (d) Prediction of CNN and Transformer\nAdaptive Selection Network (CTASNet); (e) Prediction of Transformer Estimation Branch in CTASNet; (f) Prediction of CNN Estimation Branch in CTASNet.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nTABLE III\nEFFECT OF ENCODER LAYER NUMBERS . EACH ROW CORRESPONDS TO A MODEL WITH THE VARIED NUMBER OF ENCODER LAYERS . PERFORMANCE\nGRADUALLY IMPROVES WITH MORE ENCODER LAYERS .\nlayers GFLOPS params Inference time MAE MSE\n0 86.5G 19 .4M 2 .9 56 .7 93 .7\n2 97.4G 25 .6M 3 .6 56 .6 89 .7\n4 102.2G 30 .3M 4 .0 54 .3 87 .8\n6 107.1G 35 .0M 4 .4 53 .9 87 .2\n8 111.9G 39 .8M 4 .9 52 .4 86 .3\nTABLE IV\nPERFORMANCES OF LOSS FUNCTIONS USING DIFFERENT BACKBONES ON UCF-QNRF DATASET. OUR PROPOSED METHOD OUTPERFORMS OTHER LOSS\nFUNCTIONS .\nMethods VGG19 CSRNet MCNN\nMAE MSE MAE MSE MAE MSE\nL2 98.7 176 .1 110 .6 190 .1 186 .4 283 .6\nBL [61] 88.8 154 .8 107 .5 184 .3 190 .6 272 .3\nNoiseCC [30] 85.8 150 .6 96 .5 163 .3 177 .4 259 .0\nDM-Count [98] 85.6 148 .3 103 .6 180 .6 176 .1 263 .3\nAL-PAPM (OURS) 81.2 141 .9 95 .6 162 .7 157 .5 243 .3\n0\n100\n200\n300\n400\n500\n600\n1\n12\n23\n34\n45\n56\n67\n78\n89\n100\n111\n122\n133\n144\n155\n166\n177\n188\n199\n210\n221\n232\n243\n254\n265\n276\n287\n298\n309\n320\n331\n342\n353\n364\n375\n386\n397\n408\n0\n100\n200\n300\n400\n500\n600\n1\n12\n23\n34\n45\n56\n67\n78\n89\n100\n111\n122\n133\n144\n155\n166\n177\n188\n199\n210\n221\n232\n243\n254\n265\n276\n287\n298\n309\n320\n331\n342\n353\n364\n375\n386\n397\n408\nGTGT Prediction of CTASNetPrediction of CTASNet\n(A)\n0\n100\n200\n300\n400\n500\n600\n1\n12\n23\n34\n45\n56\n67\n78\n89\n100\n111\n122\n133\n144\n155\n166\n177\n188\n199\n210\n221\n232\n243\n254\n265\n276\n287\n298\n309\n320\n331\n342\n353\n364\n375\n386\n397\n408\nGT Prediction of CTASNet\n(A)\n(C)\n0\n100\n200\n300\n400\n500\n600\n1\n12\n23\n34\n45\n56\n67\n78\n89\n100\n111\n122\n133\n144\n155\n166\n177\n188\n199\n210\n221\n232\n243\n254\n265\n276\n287\n298\n309\n320\n331\n342\n353\n364\n375\n386\n397\n408\nGTGT Prediction of TEBPrediction of TEB\nGTGT Prediction of CEBPrediction of CEB\n(B)\nGround Truth of regions\nPrediction of regions\nGround Truth of regions\nGround Truth of regions\nPrediction of regions Prediction of regions\nFig. 8. Visualization of the quantitative experiment. The prediction of\nCTASNet is a combined prediction of CNN and Transformer. The CNN\nand the Transformer branches are responsible for less dense and more dense\nregions, respectively.\nTransformer branch is responsible for more dense regions.\n2) Effect of the number of Transformer Encoder Layer: We\nevaluate the importance of global feature-level self-attention\nby changing the number of Transformer encoder layers. Table\nIII reports a detail comparison. Comparing layer 0 and layer\n2, we could observe that, without Transformer encoder layers,\nthere is a signiﬁcant MSE drop by 4.0 points. Moreover, the\nperformance gradually improves with more Transformer en-\ncoder layers. Thus we conjecture that the Transformer encoder\nwhich uses global scene reasoning is useful to perceive the\ndifferent regional densities in the whole image. As shown in\nTable III, the layer of 8 achieves the best results. However,\ncompared with the layer of 4, the layer of 8 brings an increase\nof 31.4% in parameters and 22.5% in inference time. By\nadding the layer from 4 to 8, the improvement of MSE is 1.7%\non ShanghaiTech A. Taking into account model complexity\nand counting accuracy, we adopt 4 Transformer Encoder layers\nin our proposed CTASNet.\nIn Figure 9, we visualize the prediction results of different\nTransformer encoder layers. By comparing the results of\nencoder layer 0 and other encoder layers, it could be observed\nthat the predictions with the Transformer encoder layer have\nsigniﬁcantly different response intensities in different density\nregions. This veriﬁes that the Transformer encoder using\nglobal scene reasoning is effective to perceive the different\nregional densities in the dense regions.\n3) Effect of Bandwidth σ: We next investigate the effect\nof bandwidth σ in our proposed Correntropy based OT Loss,\nwhich is designed for tolerating the annotation noise. The\nspeciﬁc Correntropy based OT Loss is computed by:\nℓOT(z,ˆz)\ndef.\n= min\nT∈U(z,ˆz)\n⟨C,T⟩\ndef.\n=\n∑\ni,j\nCi,jTi,j (17)\nCij = ∥ai −bj∥2\nκσ(ai,bj) = ∥ai −bj∥2\nexp (−∥ai −bj∥2/2σ2) (18)\nIn this experiment, we tune bandwidth σ from 4, 8, 16, 32\nto ∞. Especially, when p = ∞, the Correntropy based cost\nfunction presented in Equation (18) converts to the classical\nL2 cost function Cij = ∥ai −bj∥2. The detail results are\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nFig. 9. Visualization of the prediction results containing different Transformer encoder layers.\nFig. 10. The curves of testing results for different bandwidths on Shang-\nhaiTech A.\npresented in Figure 10, σ = 16outperforms other bandwidth\nvalues. Therefore, we choose σ = 16, for all experiments on\nall datasets.\n4) Effect of the position embedding: To investigate the\neffect of the position embedding, we conduct an ablation study\non ShanghaiTech A dataset. To be speciﬁc, following [24],\nwe use sine and cosine functions of different frequencies to\nachieve positional encoding. Details are presented in Table\nV. Comparing the results of TEB and ‘TEB + position\nencoding’, we could observe that adding position encoding\nto TEB can improve the counting performance. While for\nCTASNet, embedding the positional information would reduce\nthe counting accuracy. The reason may be that CTASNet\ncontains convolution for ﬁnal estimation, enabling the model\ncan implicitly encode absolute positional information. Adding\nextra positional information would bring information redun-\ndancy. Thus, we do not introduce extra positional encoding in\nthe Transformer of CTASNet.\nTABLE V\nEFFECT OF THE POSITION EMBEDDING . FOLLOWING [24], WE ADD SINE\nAND COSINE POSITIONAL ENCODING INTO TRANSFORMER ESTIMATION\nBRANCH (TEB) AND CTASN ET. WE ADOPT THE PROPOSED\nCORRENTROPY BASED OPTIMAL TRANSPORT LOSS (COT) FOR TEB AND\nCTASN ET.\nComponents ShanghaiTech A\nMAE MSE\nTEB 61.2 95 .4\nTEB + position encoding 57.7 91 .7\nCTASNet 54.3 87 .8\nCTASNet + position encoding 56.6 88 .7\n5) Comparison with different loss functions: In Table IV,\nwe compare our proposed loss function with different loss\nfunctions using different backbone networks. The pixel-wise\nL2 loss function measures the pixel difference between the\npredicted density map and the ”ground-truth” density map.\nThe Bayesian loss (BL) [61] uses a point-wise loss function\nbetween the ground-truth point annotations and the aggregated\ndot prediction generated from the predicted density map. The\nNoiseCC model [30] the annotation noise using a random\nvariable with Gaussian distribution and derives a probability\ndensity Gaussian approximation as a loss function. DM-Count\n[98] uses balanced OT with an L2 cost function, to match the\nshape of the two distributions.\nOur proposed loss function achieves the best results among\nall loss functions. Our loss function performs better than L2\nloss since we directly adopt point annotation for supervision,\ninstead of designing hand-craft intermediate presentation as a\nlearning target. Compared to BL and DM-Count (both using\npoint annotation for supervision), our loss function achieves\nbetter performances in all network architectures. Our loss\nfunction can tolerate the annotation noise. While these two loss\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nTABLE VI\nCOMPARISON OF THE PARAMETERS (M), FLOP S (G), I NFERENCE SPEED (S / 100 IMAGES ) AND RESULTS ON THE PARTA DATASET [62]. N OTE : THE\nPARAMETERS AND FLOP S ARE COMPUTED WITH THE INPUT SIZE OF 512×512 ON A SINGLE NVIDIA 3090 GPU. T HE INFERENCE TIME IS THE\nAVERAGE TIME OF 100 RUNS ON TESTING A 1024×768 SAMPLE . “FS” REPRESENTS THAT THE MODEL IS TRAINED FROM SCRATCH .\nMethod Backbone Parameters(M) FLOPs(G) Inference speed(s) RMSE In PartA\nMCNN [62] FS 0.13 7.05 0.008 173.2\nPCC-Net [51] FS 0.51 43 .87 0 .013 124.0\nCSRNet [92] VGG16 16.26 108 .34 0 .038 115.0\nCAN [93] VGG16 18.10 114 .83 0 .047 100.0\nSCAR [2] VGG16 16.29 108 .44 0 .047 110.0\nSFANet [5] VGG16 15.92 93 .27 0 .043 99 .3\nM-SFANet [5] VGG16 22.88 115 .14 0 .058 94 .5\nSFCN [97] ResNet101 38.60 162 .03 0 .096 104.5\nCTASNet(ours) VGG16 30.30 102 .22 0 .040 87.8\nfunctions ignore the noise problem existing in the annotation\nlabeling process. Thus, our proposed loss function achieves\nthe lowest MAE and MSE among all loss functions.\nD. Complexity analysis\nTo evaluate the complexity of our method, we have con-\nducted an ablation study on ShanghaiTech A dataset in Table\nVI. To exclude interference from other factors, we experi-\nmented on the same experimental environment and reported\nthe results in ShanghaiTech A benchmark [62]. The parameters\nand FLOPs are computed with the input size of 512×512 on a\nsingle NVIDIA 3090 GPU. The inference time is the average\ntime of 100 runs on testing a 1024×768 image sample.\nAs shown in Table VI, our model does not have an ad-\nvantage in model parameters and inference speed. However,\nour model has achieved better performance in crowd counting.\nMoreover, our model could also achieve real-time crowd\ncounting at a speed of 0.048 seconds per picture. It does not\naffect the application of our method in reality.\nV. C ONCLUSION\nIn this paper, a CNN and Transformer Adaptive Selection\nNetwork (CTASNet) has been proposed for the crowd counting\nproblem. It is motivated by the complementary performance\nof CNN and Transformer based counting methods under\nsituations with varying crowd densities. An Adaptive Selection\nModule is proposed to make the network automatically adopt\ndifferent counting modes for different density regions. Also,\nto reduce the inﬂuences of annotation noise, we introduce\na Correntropy based Optimal Transport loss (COT loss). We\nevaluate the CTASNet with proposed COT loss on four chal-\nlenging crowd counting benchmarks, most of which consist\nof high variation in crowd densities. The experimental results\nconﬁrm that our method obtains state-of-the-art performance\non these datasets.\nACKNOWLEDGMENTS\nThis work was supported by the National Key Re-\nsearch and Development Program of China under Grant No.\n2020AAA0108100, the National Natural Science Foundation\nof China under Grant No. 62073257, 62141223, and the Key\nResearch and Development Program of Shaanxi Province of\nChina under Grant No. 2022GY-076.\nREFERENCES\n[1] F. Yu, D. Wang, E. Shelhamer, and T. Darrell, “Deep layer aggregation,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 2403–2412.\n[2] J. Gao, Q. Wang, and Y . Yuan, “Scar: Spatial-/channel-wise attention\nregression networks for crowd counting,” Neurocomputing, vol. 363,\npp. 1–8, 2019. 12\n[3] Z.-Q. Cheng, Q. Dai, H. Li, J. Song, X. Wu, and A. G. Hauptmann,\n“Rethinking spatial invariance of convolutional networks for object\ncounting,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2022, pp. 19 638–19 648. 3\n[4] Q. Wang and T. P. Breckon, “Crowd counting via segmentation\nguided attention networks and curriculum loss,” IEEE Transactions on\nIntelligent Transportation Systems, 2022. 2\n[5] P. Thanasutives, K.-i. Fukui, M. Numao, and B. Kijsirikul, “Encoder-\ndecoder based convolutional neural networks with multi-scale-aware\nmodules for crowd counting,” in 2020 25th International Conference\non Pattern Recognition (ICPR). IEEE, 2021, pp. 2382–2389. 2, 6, 12\n[6] J. Gao, T. Han, Y . Yuan, and Q. Wang, “Learning independent instance\nmaps for crowd localization,” arXiv preprint arXiv:2012.04164 , 2020.\n3\n[7] Y . Wang, J. Hou, X. Hou, and L.-P. Chau, “A self-training approach\nfor point-supervised object detection and counting in crowds,” IEEE\nTransactions on Image Processing , vol. 30, pp. 2876–2887, 2021. 3\n[8] Q. Song, C. Wang, Z. Jiang, Y . Wang, Y . Tai, C. Wang, J. Li, F. Huang,\nand Y . Wu, “Rethinking counting and localization in crowds: A purely\npoint-based framework,” inProceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 3365–3374. 3\n[9] Y . Wang, X. Hou, and L.-P. Chau, “Dense point prediction: A simple\nbaseline for crowd counting and localization,” in 2021 IEEE Interna-\ntional Conference on Multimedia & Expo Workshops (ICMEW). IEEE,\n2021, pp. 1–6. 3\n[10] S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang,\nand J. Susskind, “An attention free transformer,” arXiv preprint\narXiv:2105.14103, 2021. 4\n[11] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n“Deeplab: Semantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs,” IEEE transactions on\npattern analysis and machine intelligence , vol. 40, no. 4, pp. 834–848,\n2017. 4, 5\n[12] M. A. Islam, M. Kowal, S. Jia, K. G. Derpanis, and N. D. Bruce, “Po-\nsition, padding and predictions: A deeper look at position information\nin cnns,” arXiv preprint arXiv:2101.12322 , 2021. 3\n[13] M. A. Islam, S. Jia, and N. D. Bruce, “How much position in-\nformation do convolutional neural networks encode?” arXiv preprint\narXiv:2001.08248, 2020. 3, 5\n[14] X. Chu, B. Zhang, Z. Tian, X. Wei, and H. Xia, “Do we really need\nexplicit position encodings for vision transformers?” arXiv e-prints, pp.\narXiv–2102, 2021. 3\n[15] D. Liang, X. Chen, W. Xu, Y . Zhou, and X. Bai, “Transcrowd:\nweakly-supervised crowd counting with transformers,” Science China\nInformation Sciences, vol. 65, no. 6, pp. 1–14, 2022. 3\n[16] A. Zhang, J. Shen, Z. Xiao, F. Zhu, X. Zhen, X. Cao, and L. Shao,\n“Relational attention network for crowd counting,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2019, pp.\n6788–6797. 3\n[17] H. Lin, Z. Ma, R. Ji, Y . Wang, and X. Hong, “Boosting crowd counting\nvia multifaceted attention,” arXiv preprint arXiv:2203.02636 , 2022. 6\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\n[18] N. Wang, W. Zhou, J. Wang, and H. Li, “Transformer meets tracker:\nExploiting temporal context for robust visual tracking,” in 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021, pp. 1571–1580.\n[19] S. Jiayao, S. Zhou, Y . Cui, and Z. Fang, “Real-time 3d single object\ntracking with transformer,” IEEE Transactions on Multimedia, pp. 1–1,\n2022. 3\n[20] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in 2021 IEEE/CVF International Conference on\nComputer Vision (ICCV) , 2021, pp. 9992–10 002. 3\n[21] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr, and L. Zhang, “Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers,” in 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021, pp. 6877–6886. 3\n[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020. 3\n[23] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean conference on computer vision . Springer, 2020, pp. 213–\n229. 3\n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you\nneed,” in Advances in Neural Information Processing Systems ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\n2017/ﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf 3, 6, 11\n[25] C. Shang, H. Ai, and B. Bai, “End-to-end crowd counting via joint\nlearning local and global count,” in 2016 IEEE International Confer-\nence on Image Processing (ICIP) . IEEE, 2016, pp. 1215–1219. 3\n[26] V . A. Sindagi and V . M. Patel, “Cnn-based cascaded multi-task learning\nof high-level prior and density estimation for crowd counting,” in 2017\n14th IEEE international conference on advanced video and signal\nbased surveillance (AVSS). IEEE, 2017, pp. 1–6. 3\n[27] L. Boominathan, S. S. Kruthiventi, and R. V . Babu, “Crowdnet: A\ndeep convolutional network for dense crowd counting,” in Proceedings\nof the 24th ACM international conference on Multimedia , 2016, pp.\n640–644.\n[28] D. Guo, K. Li, Z.-J. Zha, and M. Wang, “Dadnet: Dilated-attention-\ndeformable convnet for crowd counting,” 10 2019, pp. 1823–1832. 2\n[29] J. Liu, C. Gao, D. Meng, and A. G. Hauptmann, “Decidenet: Counting\nvarying density crowds through attention guided detection and density\nestimation,” in 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2018, pp. 5197–5206. 1, 4\n[30] J. Wan and A. Chan, “Modeling noisy annotations for crowd counting,”\nAdvances in Neural Information Processing Systems , vol. 33, 2020. 7,\n10, 11\n[31] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\nobject detection with region proposal networks,” IEEE Transactions on\nPattern Analysis and Machine Intelligence , vol. 39, no. 6, pp. 1137–\n1149, 2017.\n[32] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\nonce: Uniﬁed, real-time object detection,” in 2016 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016, pp. 779–788.\n[33] T. N. Mundhenk, G. Konjevod, W. A. Sakla, and K. Boakye, “A\nlarge contextual dataset for classiﬁcation, detection and counting of\ncars with deep learning,” CoRR, vol. abs/1609.04453, 2016. [Online].\nAvailable: http://arxiv.org/abs/1609.04453\n[34] J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in\n2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017, pp. 6517–6525.\n[35] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss\nfor dense object detection,” in 2017 IEEE International Conference on\nComputer Vision (ICCV) , 2017, pp. 2999–3007.\n[36] ——, “Focal loss for dense object detection,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 42, no. 2, pp. 318–327,\n2020.\n[37] T. Stahl, S. L. Pintea, and J. C. van Gemert, “Divide and count:\nGeneric object counting by image divisions,” IEEE Transactions on\nImage Processing, vol. 28, no. 2, pp. 1035–1044, 2019.\n[38] E. Goldman, R. Herzig, A. Eisenschtat, J. Goldberger, and T. Hassner,\n“Precise detection in densely packed scenes,” in 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) ,\n2019, pp. 5222–5231.\n[39] X. Liu, J. van de Weijer, and A. D. Bagdanov, “Leveraging unlabeled\ndata for crowd counting by learning to rank,” in 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2018, pp.\n7661–7669.\n[40] I. H. Laradji, N. Rostamzadeh, P. O. Pinheiro, D. Vazquez, and\nM. Schmidt, “Where are the blobs: Counting by localization with point\nsupervision,” 2018.\n[41] Y . Liu, M. Shi, Q. Zhao, and X. Wang, “Point in, box out: Beyond\ncounting persons in crowds,” in 2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2019, pp. 6462–6471.\n[42] V . K. Valloli and K. Mehta, “W-net: Reinforced u-net for density map\nestimation,” 2019.\n[43] J. Ding, N. Xue, G.-S. Xia, X. Bai, W. Yang, M. Y . Yang, S. Belongie,\nJ. Luo, M. Datcu, M. Pelillo, and L. Zhang, “Object detection in aerial\nimages: A large-scale benchmark and challenges,” 2021.\n[44] M.-R. Hsieh, Y .-L. Lin, and W. H. Hsu, “Drone-based object counting\nby spatially regularized regional proposal network,” in 2017 IEEE\nInternational Conference on Computer Vision (ICCV), 2017, pp. 4165–\n4173.\n[45] Q. Wang, J. Gao, W. Lin, and X. Li, “Nwpu-crowd: A large-scale\nbenchmark for crowd counting and localization,” IEEE transactions\non pattern analysis and machine intelligence , vol. 43, no. 6, pp. 2141–\n2149, 2020. 7, 8\n[46] R. He, W.-S. Zheng, and B.-G. Hu, “Maximum correntropy criterion\nfor robust face recognition,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence , vol. 33, no. 8, pp. 1561–1576, 2010.\n[47] W. Liu, P. P. Pokharel, and J. C. Principe, “Correntropy: Properties and\napplications in non-gaussian signal processing,” IEEE Transactions on\nsignal processing, vol. 55, no. 11, pp. 5286–5298, 2007.\n[48] R. Guerrero-G ´omez-Olmedo, B. Torre-Jim ´enez, R. L ´opez-Sastre,\nS. Maldonado-Basc ´on, and D. O. noro Rubio, “Extremely overlapping\nvehicle counting,” in Iberian Conference on Pattern Recognition and\nImage Analysis (IbPRIA) , 2015.\n[49] Z. Yan, P. Li, B. Wang, D. Ren, and W. Zuo, “Towards learning multi-\ndomain crowd counting,” IEEE Transactions on Circuits and Systems\nfor Video Technology, pp. 1–1, 2021. 1\n[50] U. Sajid, H. Sajid, H. Wang, and G. Wang, “Zoomcount: A zooming\nmechanism for crowd counting in static images,” IEEE Transactions\non Circuits and Systems for Video Technology , vol. 30, no. 10, pp.\n3499–3512, 2020. 1\n[51] J. Gao, Q. Wang, and X. Li, “Pcc net: Perspective crowd counting\nvia spatial convolutional network,” IEEE Transactions on Circuits and\nSystems for Video Technology , vol. 30, no. 10, pp. 3486–3498, 2020.\n1, 12\n[52] M. Zhao, C. Zhang, J. Zhang, F. Porikli, B. Ni, and W. Zhang,\n“Scale-aware crowd counting via depth-embedded convolutional neural\nnetworks,” IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 30, no. 10, pp. 3651–3662, 2020. 1\n[53] H. Zheng, Z. Lin, J. Cen, Z. Wu, and Y . Zhao, “Cross-line pedestrian\ncounting based on spatially-consistent two-stage local crowd density\nestimation and accumulation,” IEEE Transactions on Circuits and\nSystems for Video Technology, vol. 29, no. 3, pp. 787–799, 2019.\n[54] S. Jiang, X. Lu, Y . Lei, and L. Liu, “Mask-aware networks for\ncrowd counting,” IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 30, no. 9, pp. 3119–3129, 2020.\n[55] B. Sheng, C. Shen, G. Lin, J. Li, W. Yang, and C. Sun, “Crowd counting\nvia weighted vlad on a dense attribute feature map,” IEEE Transactions\non Circuits and Systems for Video Technology, vol. 28, no. 8, pp. 1788–\n1797, 2018.\n[56] X. Jiang, L. Zhang, P. Lv, Y . Guo, R. Zhu, Y . Li, Y . Pang, X. Li,\nB. Zhou, and M. Xu, “Learning multi-level density maps for crowd\ncounting,” IEEE Transactions on Neural Networks and Learning Sys-\ntems, vol. 31, no. 8, pp. 2705–2715, 2020.\n[57] J. Gao, T. Han, Y . Yuan, and Q. Wang, “Domain-adaptive crowd\ncounting via high-quality image translation and density reconstruction,”\nIEEE Transactions on Neural Networks and Learning Systems , pp. 1–\n13, 2021. 1\n[58] Q. Wang, T. Han, J. Gao, and Y . Yuan, “Neuron linear transformation:\nModeling the domain shift for crowd counting,” IEEE Transactions on\nNeural Networks and Learning Systems , pp. 1–13, 2021. 1\n[59] Y . Luo, J. L ¨u, X. Jiang, and B. Zhang, “Learning from architectural\nredundancy: Enhanced deep supervision in deep multipath encoder-\ndecoder networks,” IEEE Transactions on Neural Networks and Learn-\ning Systems, pp. 1–14, 2021. 1\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\n[60] X. Chen, X. Chen, Y . Zhang, X. Fu, and Z.-J. Zha, “Laplacian pyramid\nneural network for dense continuous-value regression for complex\nscenes,”IEEE Transactions on Neural Networks and Learning Systems,\nvol. 32, no. 11, pp. 5034–5046, 2021. 1\n[61] Z. Ma, X. Wei, X. Hong, and Y . Gong, “Bayesian loss for crowd count\nestimation with point supervision,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2019.\n3, 7, 8, 10, 11\n[62] Y . Zhang, D. Zhou, S. Chen, S. Gao, and Y . Ma, “Single-image\ncrowd counting via multi-column convolutional neural network,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2016. 1, 2, 7, 12\n[63] C. Zhang, H. Li, X. Wang, and X. Yang, “Cross-scene crowd counting\nvia deep convolutional neural networks,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), June\n2015.\n[64] J. Wan and A. Chan, “Adaptive density map generation for crowd\ncounting,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , October 2019.\n[65] J. Wan, Q. Wang, and A. B. Chan, “Kernel-based density map genera-\ntion for dense object counting,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 2020.\n[66] C. Yang and Zhang, “Saliency detection via graph-based manifold\nranking,” in Computer Vision and Pattern Recognition (CVPR), 2013\nIEEE Conference on . IEEE, 2013, pp. 3166–3173.\n[67] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual\nattention for rapid scene analysis,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , vol. 20, no. 11, pp. 1254–1259,\n1998.\n[68] A. M. Treisman and G. Gelade, “A feature-integration theory of\nattention,” Cognitive Psychology , vol. 12, no. 1, pp. 97–136, 1980.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\n0010028580900055\n[69] C. Koch and S. Ullman, Shifts in Selective Visual Attention: Towards\nthe Underlying Neural Circuitry . Dordrecht: Springer Netherlands,\n1987, pp. 115–141.\n[70] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, and R. Yang, “Salient\nobject detection in the deep learning era: An in-depth survey,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , pp. 1–1,\n2021.\n[71] V . A. Sindagi, R. Yasarla, and V . M. Patel, “Pushing the frontiers of\nunconstrained crowd counting: New dataset and benchmark method,”\nin Proceedings of the IEEE International Conference on Computer\nVision, 2019, pp. 1221–1231.\n[72] ——, “Jhu-crowd++: Large-scale crowd counting dataset and a bench-\nmark method,” Technical Report, 2020. 7\n[73] M. Li, Z. Zhang, K. Huang, and T. Tan, “Estimating the number of\npeople in crowded scenes by mid based foreground segmentation and\nhead-shoulder detection,” in 2008 19th International Conference on\nPattern Recognition, 2008, pp. 1–4. 2\n[74] W. Ge and R. T. Collins, “Marked point processes for crowd counting,”\nin 2009 IEEE Conference on Computer Vision and Pattern Recognition,\n2009, pp. 2913–2920. 2\n[75] A. B. Chan, Z.-S. J. Liang, and N. Vasconcelos, “Privacy preserv-\ning crowd monitoring: Counting people without people models or\ntracking,” in 2008 IEEE Conference on Computer Vision and Pattern\nRecognition, 2008, pp. 1–7. 2\n[76] A. B. Chan and N. Vasconcelos, “Bayesian poisson regression for\ncrowd counting,” in 2009 IEEE 12th International Conference on\nComputer Vision, 2009, pp. 545–551. 2\n[77] H. Idrees, I. Saleemi, C. Seibert, and M. Shah, “Multi-source multi-\nscale counting in extremely dense crowd images,” in 2013 IEEE\nConference on Computer Vision and Pattern Recognition , 2013, pp.\n2547–2554. 2\n[78] V . Lempitsky and A. Zisserman, “Learning to count objects in images.”\n01 2010, pp. 1324–1332.\n[79] Q. Song, C. Wang, Y . Wang, Y . Tai, C. Wang, J. Li, J. Wu, and\nJ. Ma, “To choose or to fuse? scale selection for crowd counting,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 35,\nno. 3, 2021, pp. 2576–2583. 1, 2, 7\n[80] D. Babu Sam, S. Surya, and R. Venkatesh Babu, “Switching convolu-\ntional neural network for crowd counting,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), July\n2017. 1, 2, 7\n[81] X. Jiang, Z. Xiao, B. Zhang, X. Zhen, X. Cao, D. Doermann, and\nL. Shao, “Crowd counting and density estimation by trellis encoder-\ndecoder networks,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , June 2019.\n[82] V . A. Sindagi and V . M. Patel, “Multi-level bottom-top and top-bottom\nfeature fusion for crowd counting,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2019.\n[83] J. Wan, Z. Liu, and A. B. Chan, “A generalized loss function for\ncrowd counting and localization,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), June\n2021, pp. 1974–1983.\n[84] S. Bai, Z. He, Y . Qiao, H. Hu, W. Wu, and J. Yan, “Adaptive dilated\nnetwork with self-correction supervision for counting,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), June 2020.\n[85] C. Villani, Optimal transport: old and new . Springer, 2009, vol. 338.\n[86] G. Peyr ´e and M. Cuturi, “Computational optimal transport: With\napplications to data science,” Foundations and Trends® in Machine\nLearning, vol. 11, no. 5-6, pp. 355–607, 2019. [Online]. Available:\nhttp://dx.doi.org/10.1561/2200000073\n[87] B. Chen, Y . Xie, X. Wang, Z. Yuan, P. Ren, and J. Qin, “Multikernel\ncorrentropy for robust learning,” IEEE Transactions on Cybernetics ,\npp. 1–12, 2021. 2, 3\n[88] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image\ntranslation with conditional adversarial networks,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), July 2017.\n[89] A. Bansal and K. S. Venkatesh, “People counting in high density\ncrowds from still images,” 2015. 3, 7\n[90] H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed, N. Rajpoot,\nand M. Shah, “Composition loss for counting, density map estimation\nand localization in dense crowds,” in Proceedings of the European\nConference on Computer Vision (ECCV) , September 2018. 3, 7\n[91] X. Cao, Z. Wang, Y . Zhao, and F. Su, “Scale aggregation network for\naccurate and efﬁcient crowd counting,” in Proceedings of the European\nConference on Computer Vision (ECCV) , September 2018. 7\n[92] Y . Li, X. Zhang, and D. Chen, “Csrnet: Dilated convolutional neural\nnetworks for understanding the highly congested scenes,” in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 1091–1100. 1, 2, 7, 8, 12\n[93] W. Liu, M. Salzmann, and P. Fua, “Context-aware crowd counting,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 5099–5108. 7, 12\n[94] V . A. Sindagi and V . M. Patel, “Multi-level bottom-top and top-bottom\nfeature fusion for crowd counting,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 1002–1012.\n[95] N. Liu, Y . Long, C. Zou, Q. Niu, L. Pan, and H. Wu, “Adcrowdnet: An\nattention-injective deformable convolutional network for crowd under-\nstanding,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2019, pp. 3225–3234. 3, 7\n[96] D. B. Sam, S. V . Peri, M. N. Sundararaman, A. Kamath, and V . B.\nRadhakrishnan, “Locate, size and count: Accurately resolving people\nin dense crowds via detection,” IEEE transactions on pattern analysis\nand machine intelligence , 2020.\n[97] Q. Wang, J. Gao, W. Lin, and Y . Yuan, “Learning from synthetic data\nfor crowd counting in the wild,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n8198–8207. 7, 12\n[98] B. Wang, H. Liu, D. Samaras, and M. Hoai, “Distribution matching\nfor crowd counting,” arXiv preprint arXiv:2009.13077 , 2020. 3, 6, 7,\n8, 10, 11\n[99] X. Liu, J. Yang, W. Ding, T. Wang, Z. Wang, and J. Xiong, “Adap-\ntive mixture regression network with local counting map for crowd\ncounting,” in Computer Vision–ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV 16 .\nSpringer, 2020, pp. 241–257.\n[100] L. Liu, H. Lu, H. Zou, H. Xiong, Z. Cao, and C. Shen, “Weighing\ncounts: Sequential crowd counting by reinforcement learning,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 164–\n181.\n[101] V . A. Sindagi and V . M. Patel, “Cnn-based cascaded multi-task learning\nof high-level prior and density estimation for crowd counting,” in 2017\n14th IEEE International Conference on Advanced Video and Signal\nBased Surveillance (AVSS). IEEE, 2017, pp. 1–6.\n[102] J. Wan, Q. Wang, and A. B. Chan, “Kernel-based density map genera-\ntion for dense object counting,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence , pp. 1–1, 2020.\n[103] L. Zhang, M. Shi, and Q. Chen, “Crowd counting via scale-adaptive\nconvolutional neural network,” in 2018 IEEE Winter Conference on\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\nApplications of Computer Vision (WACV) . IEEE, 2018, pp. 1113–\n1121. 1\n[104] J. Wan, Z. Liu, and A. B. Chan, “A generalized loss function for\ncrowd counting and localization,” in 2021 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2021, pp. 1974–\n1983. 3\n[105] Z. Ma, X. Wei, X. Hong, H. Lin, Y . Qiu, and Y . Gong, “Learning to\ncount via unbalanced optimal transport,” in Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , vol. 35, no. 3, 2021, pp. 2319–\n2327. 3\n[106] Y . Yang, D. Fan, S. Du, M. Wang, B. Chen, and Y . Gao, “Point\nset registration with similarity and afﬁne transformations based on\nbidirectional kmpe loss,” IEEE transactions on cybernetics , vol. 51,\nno. 3, pp. 1678–1689, 2019.\n[107] B. Chen, L. Xing, X. Wang, J. Qin, and N. Zheng, “Robust learning\nwith kernel mean p-power error loss,” IEEE transactions on cybernet-\nics, vol. 48, no. 7, pp. 2101–2113, 2017.\n[108] P. Thanasutives, K.-i. Fukui, M. Numao, and B. Kijsirikul, “Encoder-\ndecoder based convolutional neural networks with multi-scale-aware\nmodules for crowd counting,” in 2020 25th International Conference\non Pattern Recognition (ICPR) . IEEE, 2021, pp. 2382–2389.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6725045442581177
    },
    {
      "name": "Crowds",
      "score": 0.6645103096961975
    },
    {
      "name": "Density estimation",
      "score": 0.6540277600288391
    },
    {
      "name": "Kernel density estimation",
      "score": 0.6334906220436096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5733238458633423
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.49756577610969543
    },
    {
      "name": "Transformer",
      "score": 0.49750521779060364
    },
    {
      "name": "Mathematics",
      "score": 0.20932191610336304
    },
    {
      "name": "Voltage",
      "score": 0.11963734030723572
    },
    {
      "name": "Statistics",
      "score": 0.10194858908653259
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Estimator",
      "score": 0.0
    }
  ]
}