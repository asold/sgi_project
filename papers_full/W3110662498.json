{
    "title": "MiniVLM: A Smaller and Faster Vision-Language Model",
    "url": "https://openalex.org/W3110662498",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1901876785",
            "name": "Wang Jian-feng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2349235520",
            "name": "Hu Xiaowei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2370394005",
            "name": "Zhang, Pengchuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2221596280",
            "name": "LI Xiu-Jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1578702133",
            "name": "Wang Li-juan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1842339085",
            "name": "Zhang Lei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119363152",
            "name": "Gao, Jianfeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352283912",
            "name": "Liu, Zicheng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2997591391",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W2963037989",
        "https://openalex.org/W3138819159",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W3106250896",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2974875810",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W3034971973",
        "https://openalex.org/W2943152387",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2612624696",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2936404177",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W2952228917",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2986670728",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W3035694605",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2506483933",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2962772649",
        "https://openalex.org/W2185175083",
        "https://openalex.org/W3026176584",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964444661",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3121480429",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3035396860",
        "https://openalex.org/W2983943451",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3035497460",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2806070179",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W3035652667",
        "https://openalex.org/W3034399919",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2133459682",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3190043560",
        "https://openalex.org/W2981927700"
    ],
    "abstract": "Recent vision-language (VL) studies have shown remarkable progress by learning generic representations from massive image-text pairs with transformer models and then fine-tuning on downstream VL tasks. While existing research has been focused on achieving high accuracy with large pre-trained models, building a lightweight model is of great value in practice but is less explored. In this paper, we propose a smaller and faster VL model, MiniVLM, which can be finetuned with good performance on various downstream tasks like its larger counterpart. MiniVLM consists of two modules, a vision feature extractor and a transformer-based vision-language fusion module. We design a Two-stage Efficient feature Extractor (TEE), inspired by the one-stage EfficientDet network, to significantly reduce the time cost of visual feature extraction by $95\\%$, compared to a baseline model. We adopt the MiniLM structure to reduce the computation cost of the transformer module after comparing different compact BERT models. In addition, we improve the MiniVLM pre-training by adding $7M$ Open Images data, which are pseudo-labeled by a state-of-the-art captioning model. We also pre-train with high-quality image tags obtained from a strong tagging model to enhance cross-modality alignment. The large models are used offline without adding any overhead in fine-tuning and inference. With the above design choices, our MiniVLM reduces the model size by $73\\%$ and the inference time cost by $94\\%$ while being able to retain $94-97\\%$ of the accuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the state-of-the-art VL research for on-the-edge applications.",
    "full_text": "MiniVLM: A Smaller and Faster Vision-Language Model\nJianfeng Wang Xiaowei Hu Pengchuan Zhang Xiujun Li Lijuan Wang\nLei Zhang Jianfeng Gao Zicheng Liu\nMicrosoft\n{jianfw,xiaowh,penzhan,xiul,lijuanw,leizhang,jfgao,zliu}@microsoft.com\nAbstract\nRecent vision-language (VL) studies have shown re-\nmarkable progress by learning generic representations from\nmassive image-text pairs with transformer models. While\nexisting research has focused on achieving high accuracy\nwith large pre-trained models, building a lightweight model\nis of great value in practice but is less explored. In this pa-\nper, we propose a smaller and faster VL model, MiniVLM,\nwhich can be ﬁnetuned with good performance on vari-\nous downstream tasks like its larger counterpart. MiniVLM\nconsists of two modules, a vision feature extractor and a\ntransformer-based vision-language fusion module. We de-\nsign a Two-stage Efﬁcient feature Extractor (TEE) inspired\nby the one-stage EfﬁcientDet [52] network to reduce the\ncost of visual feature extraction by 99%, compared to a\nbaseline model. We adopt the MiniLM [59] structure to\nreduce the computation cost of the transformer module af-\nter comparing different compact BERT models. In addition,\nwe improve the MiniVLM pre-training by adding7M Open\nImages data, which are pseudo-labeled by a state-of-the-art\ncaptioning model. We also pre-train with high-quality im-\nage tags obtained from a strong tagging model to enhance\ncross-modality alignment. The large models are used ofﬂine\nwithout adding any overhead in ﬁne-tuning and inference.\nWith the above design choices, our MiniVLM reduces the\nmodel size by 73% and the FLOPs by 99% while maintain-\ning 94 −97% accuracy on multiple VL tasks. We hope that\nMiniVLM helps ease the use of the state-of-the-art VL re-\nsearch for on-the-edge applications.\n1. Introduction\nWith the success of BERT [6] and recent advances [66,\n35, 46, 50, 25, 24, 26, 13, 4, 15, 27] in vision-language pre-\ntraining (VLP), models pre-trained on large-scale image-\ntext data have made substantial improvement on various\nbenchmarks for a wide range of vision-language (VL) tasks,\nsuch as image captioning, visual question answering and\nimage-text retrieval. The models used in most VLP works\nOSCAR B MiniVLM (ours)0\n50\n100\n150\n200\n250Params (M)\n100%\n27%\nOSCAR B MiniVLM (ours)0\n200\n400\n600\n800FLOPS (B)\n100%\n1%\nTransformer\nVision Model\nCaption VQA TR IR NLVR20\n20\n40\n60\n80\n100\n120\n140\n100%\n100%\n100% 100%\n100%\n97%\n94%\n96%\n95%\n94%\nOSCAR B\nMiniVLM  (ours)\nFigure 1: MiniVLM retains 94 −97% of the accuracy on\nmultiple tasks with 27% parameters and 1% FLOPS com-\npared to state-of-the-art model OSCAR B [26]. Details can\nbe found in Sec. 4.4.\ncontain two modules: the vision module based on convolu-\ntional neural networks trained on ImageNet [42] and/or Vi-\nsual Genome (VG) [20] to extract visual features from the\nimage; and the feature fusion module based on the multi-\nmodal transformer model to process both the visual features\nand the token embeddings of the text input. The VL models\nare ﬁrstly pre-trained to learn cross-modal representations,\nand then ﬁne-tuned on task-speciﬁc data. In recent VLP re-\nsearch, both of the two modules leverage large-scale deep\nneural networks, which can take up to hundreds of millions\nof parameters, to achieve the state-of-the-art performance.\nHowever, due to the large sizes and high computation cost,\nit could be impractical for real-world applications to ex-\nploit the power of large models under a constrained training\nand/or inference budget. In fact, building a lightweight VL\nmodel, which is desired when operating on resource-limited\ndevices, is of great practical value but is less explored in the\nliterature.\narXiv:2012.06946v2  [cs.CV]  9 Aug 2021\nWhile larger models have been demonstrated to achieve\nhigher performance in extensive studies, it is challenging\nto compress the model to smaller sizes without tremendous\nperformance drop. In order to retain as much performance\nas possible, we ﬁrstly optimize the network architecture to\nbalance accuracy and speed. Moreover, we improve the\nsmall-model pre-training by leveraging large models and\nlarge-scale dataset.\nFor the architecture of VL models, popularized as\n“bottom-up top-down” (BUTD) attention [2], most existing\nworks [66, 35, 46, 50, 25, 24, 26, 13, 4] use the ResNet-101\nFaster R-CNN [41] model trained on the VG dataset as the\nvisual feature extractor, which has been well validated by\nstate-of-the-art results on various benchmarks. However,\nthe detector suffers from a heavy model size and high la-\ntency, and consequently cannot be deployed to resource-\nlimited applications. A few recent works [15, 27] revisit\nthe usage of grid features from the convolutional layer to\nskip the region-related computation in Faster R-CNN. Nev-\nertheless, it is still an open problem to select over the over-\nwhelming number of grid features, as dumping the whole\nfeature map to transformer could be prohibitively expen-\nsive in computation. For the transformer module, BERT is\nwidely used as the de facto standard. Recent work in Nat-\nural Language Processing (NLP) has explored to maintain\nhigh performance with compact structures based on BERT.\nHowever, the compact structure is less investigated in VLP.\nIn this paper, we propose a smaller and faster VL model,\nnamed MiniVLM, to reach similar performance as its larger\ncounterpart with a much smaller size, resulting in faster in-\nference speed. For the vision module in MiniVLM, we\ndesign a Two-stage Efﬁcient feature Extractor (TEE) to\ndrastically reduce the computation cost for extracting vi-\nsual features, which is a dominating part of the inference\ncost on certain tasks. While reﬁning each part of the de-\ntection model, we greatly simplify the region-related mod-\nules in TEE. The underlying implication is that the VL tasks\nrequire rich visual representations rather than precise box\nlocations as in the object detection task. Experimental re-\nsults show that our TEE can extract visual features of simi-\nlar quality at a much faster speed. In particular, our TEE-0,\nusing a similar backbone as EfﬁcientDet-D0 [52], is 3.7×\nsmaller and 99×faster than the widely used R101 Faster R-\nCNN from BUTD, while retaining competitive accuracy in\ndetection on VG, and up to 97% of the accuracy on down-\nstream tasks. For the transformer model, we choose the\nMiniLM[59] structure after empirically evaluating the per-\nformance of several structures, including BERT [6] and its\ncompact variants [43, 49, 18, 59].\nIn addition to the model architecture optimization, we\nleverage high-accuracy large models and large-scale data,\neither labeled or unlabeled, to further boost the performance\nof the small pre-trained model. To improve the accuracy of\nTEE, we pre-train it on large-scale classiﬁcation and detec-\ntion dataset before ﬁne-tuning on VG. During the VL pre-\ntraining, we apply data distillation [39, 61, 62] to add 7M\nOpen Images [21] which are pseudo-labeled by the state-of-\nthe-art “teacher” captioning model. We also use the high-\nquality tags from a large tagging model in pre-training to\nimprove visual-text alignment. The large tagging model is\nnot used in ﬁne-tuning or inference, and therefore has no\nimpact on the runtime speed. With the above ingredients,\nour MiniVLM, composed of TEE-0 and MiniLM [59], re-\nduces the end-to-end FLOPs to 1% with 27% parameters,\nand retains 94 −97% accuracy compared to large state-of-\nthe-art models on multiple VL tasks.\nIn summary, we make the following contributions.\n• We propose a VL model MiniVLM, which can be ﬁne-\ntuned with good performance on multiple downstream\ntasks, while being smaller and faster for practical ap-\nplication.\n• We design a Two-stage Efﬁcient feature Extractor\n(TEE) to extract image region features for VL tasks,\nwhich generates features of good quality at a much\nfaster speed.\n• We demonstrate the beneﬁts of using large models as\nwell as large-scale data in the small VL model pre-\ntraining stage to improve the downstream tasks.\n2. Related work\nVision-Language Pre-training. Remarkable progress [35,\n46, 15, 28, 25, 26, 13, 4] has been made recently on vision-\nlanguage tasks through network pre-training on massive\ndata with image-text pairs. A popular framework used in\nmost VLP work is to view the extracted visual features as\nvisual ‘tokens’ and feed them together with text tokens into\nthe BERT [6, 54] model for joint representation learning.\nThe visual feature is generally extracted with an off-the-\nshelf vision model, and the main focus is on the multi-\nmodal fusion based on BERT model. With the multiple\nmodalities, the fusion can be categorized as early fusion,\nlate fusion and full fusion. Early fusion is to ﬁrst process the\ntwo modalities together and then process each separately\nto enhance the single-modality task, e.g. InterBERT [28].\nLate fusion is to ﬁrst process each modality separately\nand then to fuse them together, e.g. in ViLBERT [35],\nLXMERT [50], ERNIE-ViL [64]. Full fusion means to pro-\ncess the two modalities’ features together with the BERT\nmodel from the very beginning to the ﬁnal representa-\ntion, e.g. OSCAR [26], Unicoder-VL [24], VL-BERT [46],\nUNITER [4], VIVO [13]. The pre-training tasks typically\ninclude the masked language modeling, image-text pairing\nloss, and masked region modeling.\nEfficientNet\n2 FC class \nprediction\nbox \nprediction\nBiFPN\nRPN\n4x4 RoIAlign\n…\n2 FC\n…\nregion \nfeatures\ncat\nbench\nMulti-layer Transformer\n…\n[MASK]\n[CLS]\ncat\nTwo-stage Efficient feature Extractor (TEE) compact BERT\nL=12 H=384 I=1536 A=12\nFigure 2: The proposed MiniVLM architecture, consisting of the Two-stage Efﬁcient feature Extractor (TEE) and compact\nBERT feature fusion module. During inference, the vision module detects objects and extracts region features, which are\nfed to the transformer model. The text input of the transformer also depends on the downstream task. For image captioning,\n[CLS] is given as the ﬁrst token, then the model predicts the next token in an auto-regressive manner to generate a caption\nsentence.\nVisual Feature Extractor. Visual feature extraction is one\nof the key modules in vision-language (VL) tasks. As in the\nbottom-up top-down approach [2], region features based on\nFaster R-CNN [41] have shown strong accuracy and been\nwidely used in VL tasks [66, 35, 46, 50, 25, 24, 26, 13].\nThe extractor is trained on ImageNet [42] and Visual\nGenome [20] datasets with two training tasks: one is to\npredict the object category and the other is to predict the\nattribute information.\nAn alternative approach is the grid feature, which is re-\nvisited in [17, 15] and demonstrated encouraging perfor-\nmance. In [17], the grid feature extractor is constructed by\ncasting the Faster R-CNN model into a fully-convolutional\nnetwork and remove the region-related operations (e.g. non-\nsuppressed compression) to reduce the time cost. In [15],\nthe convolutional network is trained together with modality\nfusion network without the detection data.\nOne advantage of using region features is that it is easy\nto select the top-K salient regions as each region is associ-\nated with a conﬁdence score. Typically, the number of re-\ngion features is 50 while the number of grid features ranges\nfrom 300 to 600 as in [17]. With more features, the cost of\nthe multi-modal fusion part can be increased signiﬁcantly.\nThus, in this paper, we stick to the region features for our\ncompact model.\nObject Detection. Region feature is built on the object de-\ntector, and the detector can be two-stage [41, 9, 27, 60] or\none-stage [40, 52, 32, 30, 7, 65, 58, 53]. The two-stage\ndetector generates bounding box candidates with a region\nproposal network (RPN) and extracts the region features\nwith RoIPool [41] or RoIAlign [9]. The feature is further\nprocessed with a classiﬁcation head and a bounding box re-\ngression head. In contrast, the one-stage detector removes\nthe RPN, and predicts the bounding box results based on the\nconvolutional neural network directly.\nDue to the removal of RPN and region feature extraction,\nfast object detector are mostly based on one-stage detectors,\ne.g. [40, 52, 57, 36, 23]. However, it remains open on how\nto effectively extract region features directly from one-stage\ndetectors for VL models. Thus, we use a two-stage architec-\nture but design a lightweight backbone and detection head\nfor the compact VL model.\nCompact BERT.BERTBASE or BERTLARGE has been com-\nmonly used in the existing VL works. To reduce the cost,\none can simply reduce the network dimensions, e.g. the\nnumber of layers, the hidden size, as in TinyBERT [18]\nand MiniLM [59]. MobileBERT [49] constructs the net-\nwork with the bottleneck design [10] to reduce the cost.\nALBERT [22] focuses on the reduction of the parameter\nsize. In our compact solution, we choose MiniLM [59] as\nour multi-modal fusion module after comparing different\napproaches in VL tasks.\nData Distillation. Data distillation (and self-training) is a\nsimple yet effective approach to leverage massive raw im-\nages with pseudo labels generated from a strong pre-trained\nmodel. The effectiveness has been well demonstrated, e.g.\nin image classiﬁcations [62, 22] and object detection [39].\nHere we apply data distillation to the VL model. One po-\ntential improvement is to apply the model distillation or\nknowledge distillation [11] on both the vision module and\nthe transformer fusion module, which we leave as future\nwork.\n3. MiniVLM\nIn this section, we describe how we design a smaller\nand faster VL model, MiniVLM, and improve the accuracy\nfor small VL model. An overview of our model is shown\nin Fig. 2. It consists of a detector-based feature extractor\nand a transformer-based feature fusion module. For various\ndownstream tasks, we alter the transformer prediction head\nwith minimal changes, which we defer to Sec. 4.4.\n3.1. Model architecture\nTwo-stage Efﬁcient feature Extractor. While the R101\nFaster R-CNN detector from [2] has been widely used to ex-\ntract region features, the computational cost is largely over-\nlooked, which can take a majority of the total inference time\nfor some VL tasks. Although region feature extraction is\npart of an objection detection model, the requirement for\nVL tasks is not the same as for objection detection. For\nVL tasks, the transformer is used to reason the relation-\nship between visual and language semantics, and what is\nneeded from the feature extractor is rich visual representa-\ntions. For example, the bounding box locations do not have\nto be highly accurate, and the recall of the bounding boxes\nis more important to cover more visual information from\nthe image. These characteristics allow us to design a fea-\nture extractor that is much more efﬁcient while not causing\nsigniﬁcant accuracy degradation for the downstream tasks.\nFig. 2 shows the design of our feature extractor called Two-\nstage Efﬁcient feature Extractor (TEE).\nFirst, we replace the backbone with EfﬁcientNet [51]\nand add BiFPN [52] to generate multi-scale features. Both\ncomponents consists of depthwise and pointwise convo-\nlutional layers, which reduce the model size and compu-\ntation signiﬁcantly compared with the standard convolu-\ntional layers. The BiFPN receives as input 4 layers with\nstride = 4, 8, 16, 32 from EfﬁcientNet, and outputs 5 fea-\ntures with an extra feature map of stride = 64 by down-\nsampling. Both EfﬁcientNet and BiFPN are building blocks\nof the one-stage detector EfﬁcientDet [52], while we make\nthe change to use feature maps starting from stride = 4in-\nstead of 8 to incorporate information from higher resolution\nfeature maps for the feature extraction.\nWhile region proposal network (RPN) [41] is used fol-\nlowing the design of two-stage detectors, the box prediction\nmodules are greatly simpliﬁed. Our RPN contains only 2\nconvolutional layers with kernal size as1: one for bounding\nbox regression and the other for objectness prediction. Af-\nter non-maximal suppression (NMS) we select the feature\nmap for each box proposal with heuristics from [29], and\napply RoIAlign [9] operation, followed by 2 linear layers\nto extract the region features. The resolution of RoIAlign is\nreduced to 4×4 rather than 14×14 in [41, 2] or7×7 in [29].\nThe feature’s dimension is also reduced from 2048 [41, 2]\nto 1024. In [2], NMS is applied for each class which can\nbe up to 1600 times on Visual Genome [20]. To reduce\nthe cost, one can apply sophisticated approaches, e.g. [56]\nor [12, 48, 3] to remove NMS. For simplicity, we apply\nNMS once in a class-agnostic manner to save the compu-\ntation.\nSimilar to EfﬁcientDet, we scale up the input image\nsize, network depth and width to get stronger feature ex-\ntractors. For varying EfﬁcientDet-DX (X = 0, 1, ···), the\ncorresponding TEE is denoted as TEE-X. Without confu-\nsion, we also use TEE to refer to TEE-0 as our extractor for\nMiniVLM.\nDuring the inference, given an image I, the vision mod-\nule outputs a bag of region features R with corresponding\nbounding boxes B and object tags C, which are fed to the\ntransformer model along with text tokens. It is noted that no\nextra tagging model is employed. We re-use the feature ex-\ntractor as the tagging model and treat the region class names\nas the tags.\nMulti-modal Transformer. With the extracted features,\na transformer-based feature fusion module is applied. To\nstrike a good balance between speed and accuracy, we\nsearch the compact structures based on BERT by vary-\ning some parameters, e.g., the number of layers. Based\non experimental results, we choose the same structure as\nMiniLM [59], i.e., 12 layers with hidden size reduced to\n384 and feed-forward intermediate size reduced to 1536.\nWe follow [26] to train the transformer model. The input\nconsists of visual features formed by the concatenation of\nR and bounding box encoding (normalized 4 corner coor-\ndinates and the heigh/width of the box), tokenized object\ntag names C, and tokenized sentences S. The content of S\ncan vary depending on the downstream task, e.g., the ques-\ntion sentence for VQA, a single [CLS] token to indicate\nthe start of sentence for image captioning.\n3.2. Pre-training\nTo train a VL model, the vision module is ﬁrst trained\non classiﬁcation or detection dataset to learn diverse visual\nrepresentations. Given the visual features, the transformer\nmodule is then pre-trained on massive image-text pairs to\nlearn cross-modal representations. Finally, the model is\nﬁne-tuned on speciﬁc downstream tasks. To compensate the\nperformance drop brought about by the small model size,\nwe apply several techniques in training.\nAs visual features are critical in VL tasks, we improve\nvisual feature by pre-training TEE on large-scale classiﬁca-\ntion and object detection dataset, e.g., Objects365 [44], be-\nfore ﬁne-tuning on the Visual Genome dataset, which shows\nthe performance gain for various downstream VL tasks\nBy pre-training the transformer model on large-scale\nimage-text data, our model inherits the advantage of VL\npre-training. Moreover, we leverage large models in two\nModel Params(M) FLOPS(B) mAP 0.5\nGrid [17] R50 23.5 37 .8 -\nX101 86.9 161 .2 -\nRegion R101-F [2] 63.8 767 .0 10 .21\nTEE (ours) 7.5 4 .4 9 .9\nTable 1: Comparison of different vision modules on number\nof parameters, FLOPS, and detection accuracy on VG.\nways to further exploit the potential for pre-training with\ncompact VL models. First, we apply a state-of-the-art cap-\ntioning model to describe 7M images from Open Images\nwith pseudo captions. In this way, the small model learns to\nmimic the behavior of the large model through much more\ndata, which can be further expanded with internet-scale un-\nlabeled data. Second, we use a large tagging model to gen-\nerate high-quality tags, and also add ground truth tags if\navailable. Although the tags in pre-training are from dif-\nferent sources, the tags in ﬁne-tuning are generated by the\nsame vision model used to extract features to remove the\ndependency on the large model at inference time. The ex-\nperimental results in Sec. 4.5 shows that the better quality\nof tags helps with cross-modal representation learning.\nOther than the changes about the sources of object tags\nand the associated sentences, we use the same pre-training\ntasks as described in [26], including masked language mod-\neling (MLM) and image-text (contrastive) matching (ITM).\n4. Experiment\n4.1. Implementation details\nTEE. We ﬁrst pre-train the backbone on ImageNet [42]\nclassiﬁcation dataset, then pre-train the whole detection\nmodel on Objects365 [44], and lastly ﬁne-tune it on Vi-\nsual Genome [20]. On ImageNet, the backbone is trained\nfrom scratch for 400 epochs. Stochastic gradient descent\n(SGD) is used to optimize the model with the batch size\nof 1024. The learning rate is 0.4 and decays with a cosine\nscheduler [33]. Afterwards, the detection model is initial-\nized with this ImageNet-pretrained backbone and trained on\nObjects365 for 100 epochs. The learning rate is 0.4 and\nbatch size is 256 with SGD. Lastly, we ﬁne-tune the model\non Visual Genome for 200 epochs, with learning rate 0.2\nand batch size 512. Following [2], an additional head is\nadded to train with attribute classes.\nVision-Language Pre-training. We combine existing V+L\ndatasets, including MS COCO [31], Conceptual Captions\n(CC) [45], SBU captions [37], Flicker30k [63], GQA [16],\nVQA [8] and VG-QA [20].\nTo explore data distillation for the compact VL model\n1This number is from https://github.com/peteanderson80/bottom-up-\nattention.\nModel Conﬁg Params(M) FLOPS(B)\nBERTBASE [6] 12/768/3072 134 .3 8 .2\nBERT8 8/768/3072 106 .0 5 .8\nTinyBERT6 [18] 6/768/3072 91 .8 4 .6\nBERT4 4/768/3072 77 .6 3 .3\nMiniLM [59] 12/384/1536 45 .7 2 .3\nTinyBERT4 [18] 4/312/1200 24 .3 0 .8\nTable 2: Computational cost for different transformer struc-\ntures. Conﬁg: the number of layers, the embedding dimen-\nsion, and intermediate size. FLOPs: measured in one for-\nward pass with 50 image regions and 35 text tokens.\npre-training, we incorporate the Open Images V6 [21] as\nextra images and generate pseudo captions using the state-\nof-the-art image captioning model ﬁne-tuned from OS-\nCAR [26]. The human veriﬁed positive tags are com-\nbined with object class predictions from TEE-3, and to-\ngether serve as the tag input in our VL pre-training. This\ndataset is referred to as OI-Caps-7M.\nDuring VL pre-training, the batch size is 2048, and the\ninitial learning rate is4×10−4 with linear decay. The model\nis updated with AdamW [34] optimizer for 100 epochs.\n4.2. Smaller and faster\nAs shown in Fig. 1, our MiniVLM reduces the number\nof parameters to 27% and FLOPS to 1% in total compared\nto the model in [26]. The following details the compression\nfor both the vision module and the transformer module.\nTEE. We compare our region feature extractor TEE with\nthe widely used ResNet-101 Faster R-CNN model (R101-\nF) from [2], as well as the grid feature extractor based on\nResNet-50 and ResNeXt-101 from [17]. Table 1 shows\nthe size and computation cost for each model at inference\ntime. Compared to R101-F, our TEE reduces the num-\nber of parameters to 7.5/63.8 = 11 .8%, and FLOPS to\n4.4/767.0 = 1% . Table 4 and Table 5 show the break-\ndown on each component of the parameters and FLOPs, re-\nspectively. For R101-F, the major cost resides in the box\nhead, which consists of 3 residual blocks and the number of\noutput channels in each block is 2048. In the backbone, the\nlargest number of channels is1024, and thus the box head is\neven more expensive than the backbone. In comparison, the\nbox head of our TEE only contains 2 linear layers, which\nsigniﬁcantly reduces the cost. Our model also uses fewer\nparameters and FLOPS than grid feature extractors. The\nreason is the use of the depthwise and pointwise convolu-\ntional layers in backbone and the lightweight region feature\nextraction head.\nOn a CPU workstation2 with 4 threads, with models im-\n2Intel(R) Xeon(R) CPU E5-2620 v4 @2.10GHz\nMethod Network Cost Evaluation on Captioning VQA\nVision Transformer Params(M) ↓FLOPs(B)↓ B@4↑M↑ C↑ S↑ Acc. ↑\nOSCARB [26] R101-F BERT BASE 198.1 775 .2 36 .5 30 .3 123.7 23 .1 73 .16\nBERT →MiniLM R101-F MiniLM 109.5 769 .3 35 .0 28 .5 118.8 21 .7 68 .69\nR101-F →TEE-0 TEE-0 BERT BASE 141.8 12 .6 34 .6 28 .4 118.7 21 .6 69 .31\nbaseline TEE-0 MiniLM 53.2 6 .7 34 .0 27 .8 115.0 21 .2 68 .14\n+O TEE-0 MiniLM 53.2 6 .7 34 .3 28 .1 116.7 21 .3 68 .54\n+O + H TEE-0 MiniLM 53.2 6 .7 34 .7 28 .3 117.7 21 .4 68 .85\n+O + H + DD (MiniVLM) TEE-0 MiniLM 53.2 6 .7 35 .6 28 .6 119.8 21 .6 69 .09\nTable 3: Retaining high accuracy from OSCAR B to MiniVLM on the image captioning (Karpathy split) and VQA 2.0 task\n(test-dev). O: using Objects365 to pre-train TEE before ﬁne-tuning on Visual Genome. H: using high-quality tags during\nvision-language pre-training. DD: with data distillation by adding OI-Caps-7M to the pre-training corpus. For the metrics, ↑\nindicates higher is better, ↓indicates lower is better. Captioning results are evaluated with BLEU-4 (B@4), METEOR (M),\nCIDEr (C) and SPICE (S).\nExtractor Backbone RPN Box head Total\nR101-F [2] 27.6 4 .7 31 .4 63 .8\nTEE-0 3.8 10 −3 3.7 7 .5\nTable 4: Number of parameters (in million) in each compo-\nnent of the region feature extractors.\nExtractor Backbone RPN Box head Total\nR101-F [2] 67.1 9 .1 690 .8 767 .0\nTEE-0 3.3 0 .03 1 .1 4 .4\nTable 5: FLOPs (in billion) in each component of the region\nfeature extractors.\nplemented in PyTorch3 and processing one image at a time,\nGrid R50 takes 699.8 ±110.1 ms, R101-F takes 12.3 ±3.2\nseconds, while our TEE takes only 393.9 ±43.8 ms, which\nis 3.2% of R101-F. Note that inference speed highly de-\npends on hardware and implementation, so we mainly re-\nport FLOPS for fair comparison.\nCompact BERT. Table 2 shows the comparison of differ-\nent transformer model structures, including TinyBERT [18]\nwith 4 and 6 layers, MiniLM [59], BERT BASE with the\nnumber of layers cut to 4 (BERT4) and 8 (BERT8), and\nBERTBASE. The whole transformer model, including em-\nbedding and decoding layers, is counted. By choosing\nMiniLM, which will be explained later with Fig. 4, the num-\nber of parameters is reduced to 45.7/134.3 = 34.0%, and\nFLOPS to 2.3/8.2 = 28 .0%. As to the inference time\nevaluated on the image captioning task, BERT BASE takes\n712.6±176.1 ms to process one image, while MiniLM takes\n346.7 ±49.8 ms, reducing to 346.7/712.6 = 48.7%.\n3https://github.com/pytorch/pytorch\n4.3. Retaining high accuracy\nTable 3 shows the ablation study on improving the pre-\ntraining for our small model. The pre-trained models\nare ﬁne-tuned and evaluated on the image captioning and\nVQA task, which will be detailed in Sec. 4.4. The cost\nis measured end-to-end including both vision and trans-\nformer modules. Starting from the OSCAR B [26] model,\nwhich consists of R101-F and BERT BASE, if we replace\nthe transformer module with MiniLM, the CIDEr score\ndrops 4.9 points. If solely replacing the vision module with\nTEE-0, the CIDEr score drops similarly 5.0 points. This\nalso indicates that our feature extractor TEE-0 can achieve\n118.7/123.7 = 96% of the accuracy compared to R101-F\non this task without any additional techniques. Then, we\nreplace both modules to TEE-0 and MiniLM, respectively,\nwhere the CIDEr score is decreased by8.7. This is the base-\nline performance of our compact VL model.\nNext, we apply approaches in Sec. 3.2 and show the im-\nprovement for pre-training with small-model. First, we use\nthe Objects365 dataset to pre-train TEE before ﬁne-tuning it\non VG, which improves the CIDEr score by 1.7, indicating\nthat better visual features contribute to better performance\non VL tasks. Secondly, we use high-quality tags, gener-\nated from the stronger vision model TEE-3, during vision-\nlanguage pre-training, and further improve the score by1.0.\nThe intuition is based on [26] that the tag information in pre-\ntraining helps with visual-text alignment. Lastly, we add the\nOI-Caps-7M dataset, and observe the gain of 2.1 in CIDEr.\nIn total, the CIDEr score is improved by 4.8, resulting in\na much smaller gap with the large pre-trained VL model.\nSimilar trend can be observed for the VQA task as shown\nin the last column of Table 3.\n4.4. Results on downstream VL tasks\nImage Captioning. The task is to describe an image with a\nnatural language sentence. Following [66, 26], we ﬁne-tune\nthe model with region features, captioning tokens and ob-\nject tags. Captioning tokens are randomly replaced by the\ntoken of [MASK] with 15% chance and predicted by the\ncorresponding representation, the attention of which is only\non region features, tags and preceding caption tokens. The\ntraining task is either the cross entropy loss or the loss op-\ntimized for the CIDEr [55] score, and we report results on\nboth tasks. During inference, the [MASK] is appended re-\ncursively with the generated tokens to predict the next token\none by one. Considering the inference speed, we reduce the\nbeam search size to 1 instead of 5 as in [26]. The accuracy is\nevaluated with BLEU@4 [38], METEOR [5], CIDEr [55],\nand SPICE [1]. The dataset is COCO [31] with Karpathy\nsplit [19].\nVQA. The task [8] is to answer a question with natural lan-\nguage based on the image context, and we cast it as a clas-\nsiﬁcation problem where each class corresponds to one an-\nswer. The representation of [CLS] is used to predict the\nanswer over a shared set of3129 answers with a linear layer.\nThe model is trained with binary cross entropy loss, and the\ninference is to select the answer with the highest conﬁdence.\nNatural Language Visual Reasoning for Real (NLVR2).\nThe task’s input is a pair of images and a natural descrip-\ntion, and the goal [47] is to predict whether the description\nis true about the image pair. To ﬁne-tune the network, we\nconstruct two input sequences, each containing the concate-\nnation of the description and one image, and then two out-\nputs corresponding to [CLS] are concatenated as the joint\nrepresentation for a binary linear classiﬁer.\nImage-Text Retrieval. The task is to retrieve similar im-\nages based on the text description or vice versa. The key\nis to score the similarity of image-text pairs. The model\nis trained as a binary classiﬁcation task where the input is\nthe image region features and the associated or mismatched\ntext description. The transformer output corresponding to\n[CLS] is used for binary assessment. The experiments are\non COCO dataset, and we report top- K retrieval accuracy\nfor both 1K test sets and 5K test sets.\nResults. Table 6, 7, and 8 show the results on image\ncaptioning, VQA, NLVR2 and image-text retrieval, respec-\ntively. As summarized in Fig. 1, we retain 94 −97% of\nthe accuracy on downstream tasks compared with the state-\nof-the-art model OSCAR B. In the Fig. 1, captioning is\nmeasured by CIDEr score with cross entropy optimization\non COCO. Text retrieval (TR) and image retrieval (IR) are\non 5K test set, and measured by R@10. VQA is on the\ntest-std split, and NLVR2 is on test-P split. For\nimage captioning, the CIDEr score of OSCAR B [26] is\n123.7, while our MiniVLM achieves 119.8 CIDEr, reach-\ning 119.8/123.7 = 97% accuracy. Compared with [17],\n5 10 15 20 25115\n116\n117\n118\n119\n120\n121\n122CIDEr\nTEE-0\nTEE-1\nTEE-2\nTEE-3\nCaption\n68.0\n68.5\n69.0\n69.5\n70.0\n70.5\n71.0\n71.5\n72.0\nAccuracy\nVQA\nFigure 3: Impact of different backbones in TEE for COCO\ncaptioning task and VQA ( test-dev). Overall, the\nstronger feature extractor leads to the higher score.\nwhich uses X101 to extract the grid feature, our solution\nachieves an even higher CIDEr (119.8 vs 113.8) with much\nlower (4.4 vs 161.2 in FLOPS) feature extraction cost as\nshown in Table. 1. On NLVR2 and image-text retrieval,\nour MiniVLM achieves higher scores than [15] which uses\nResNet50 as the grid feature extractor, while both our vision\nand transformer modules are smaller.\n4.5. Analysis\nIn this section, we provide analysis on the model archi-\ntectures and pre-training methods for small VL models. Re-\nsults are based on the models pretrained on the 7M corpus\nwithout OI-Caps-7M. All the models are evaluated through\nthe COCO image captioning task and the VQA task.\nImpact of Object Tags in Pre-training. While Table 3\nhas shown that using tags of high quality can improve the\naccuracy, we study the impact of tags under more settings\nin Table 9 for pre-training. Region features are always from\nTEE-0 with different tagging models. The transformer is\ninitialized randomly or from the pre-trained weights [59]\nfor NLP tasks. From the results, object tags makes large\nimprovement (2+ points in caption, 1+ in VQA) compared\nwith the case without tags, and high-quality tags leads to\neven better results. Small models might be more difﬁcult to\nlearn good representations, and thus the tag can contribute\nmore in cross-modal alignment. Another observation is that\nrandom initialization gives comparable results with the text\npre-trained weights. This is similar to the ﬁndings in [50].\nVarying the backbone of TEE. To study the impact of vi-\nsion modules, we scale up TEE, ranging from TEE-0 to\nTEE-3 with larger sizes and better detection accuracy as\nshown in Table 10. As shown in Fig. 3, stronger vision\nmodule leads to better accuracy, for both caption task and\nMethod CE Optimization CIDEr optimization\nB@4 M C S B@4 M C S\nBUTD [2] 36.2 27 .0 113.5 20.3 36 .3 27 .7 120.1 21.4\nGrid [17] 36.4 27 .4 113.8 20.7 - - - -\nAoANet [14] 37.2 28 .4 119.8 21.3 38 .9 29 .2 129.8 22.4\nOSCARB [26] 36.5 30 .3 123.7 23.1 40 .5 29 .7 137.6 22.8\nMiniVLM (Ours) 35.6 28 .6 119.8 21.6 39 .2 29 .7 131.7 23.5\nTable 6: Image captioning evaluation results (single model) on\nCOCO ‘Karpathy’ [19] test split. (B@4: BLEU@4, M: METEOR,\nC: CIDEr, S: SPICE.)\nMethod VQA NLVR2\ntest-std test-dev Test-P Dev\nBUTD [2] 70.34 - - -\nGrid [17] - 72.59 - -\nPixel. (R50) [15] 71.42 71 .35 71 .7 72 .4\nPixel. (X152) [15] 74.55 74 .45 76 .5 77 .2\nVisualBERT [25] 71.00 70 .80 67 .4 67 .0\nOSCARB [26] 73.44 73 .16 78 .07 78.36\nMiniVLM (Ours) 69.44 69 .09 73 .93 73.71\nTable 7: VQA and NLVR2 evaluation results.\nMethod\n1K test set 5K test set\nText Retrieval Image Retrieval Text Retrieval Image Retrieval\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nPixelBERT (R50) [15] 77.8 95 .4 98 .2 64 .1 91 .0 96 .2 53 .4 80 .4 88 .5 41 .1 69 .7 80 .5\nPixelBERT (X152) [15] 84.9 97 .7 99 .3 71 .6 93 .7 97 .4 63 .6 87 .5 93 .6 50 .1 77 .6 86 .2\nUnicoder-VLB [24] 84.3 97 .3 99 .3 69 .7 93 .5 97 .2 62 .3 87 .1 92 .8 46 .7 76 .0 85 .3\nOSCARB [26] 88.4 99 .1 99 .8 75 .7 95 .2 98 .3 70 .0 91 .1 95 .5 54 .0 80 .8 88 .5\nMiniVLM (Ours) 81.1 96 .1 99 .2 68 .5 93 .0 97 .1 58 .8 85 .1 91 .7 45 .0 74 .1 84 .0\nTable 8: Image-Text Retrieval task evaluation results on COCO datasets.\nTask Init Tagging Model\nNo Tag TEE-0 TEE-3\nCaption Text 113.5 117 .2 117 .2\nCOCO Random 114.7 116 .7 117 .7\nVQA Text 67.25 68 .22 68 .72\ntest-dev Random 67.05 68 .54 68 .85\nTable 9: Impact of the tag input used in pre-training, com-\nparing no tag with tags predicted by TEE-0, and tags pre-\ndicted with higher quality by a stronger model (TEE-3).\nRegion features are extracted with TEE-0. “Text” means\nthe model is initialized from text pre-trained weights pro-\nvided by [59]. “Random” means the model is initialized\nfrom scratch.\nModel Params (M) FLOPS (B) mAP 0.5\nTEE-0 7.5 4 .4 9 .9\nTEE-1 10.6 9 .6 10 .6\nTEE-2 12.4 17 .6 11 .3\nTEE-3 17.0 23 .3 11 .5\nTable 10: Performance of different variants of our detectors.\nA larger backbone gives higher accuracy, but more cost.\nVQA task.\n2 4 6 8\nFLOPs (B) of BERT Models\n102.5\n105.0\n107.5\n110.0\n112.5\n115.0\n117.5\n120.0CIDEr\nTinyBERT 4\nMiniLM\nBERT 4\nTinyBERT 6\nBERT 8\nBERT B\nCaption\n64\n66\n68\n70\n72\n74\nAccuracy\nVQA\nFigure 4: Impact of different compact BERT structures on\ncaptioning and VQA (test-dev).\nImpact of Compact BERT Structures. Fig. 4 shows the\nexperimental results on speed-accuracy trade-off for mod-\nels with different transformer modules listed in Table 2.\nAmong the structures, MiniLM achieves a better trade-off\nbetween speed and accuracy. This shows that a “thinner”\nversion of BERT could make better trade-off than the “shal-\nlower” version for VL tasks.\n5. Conclusion\nIn this paper, we have proposed a compact solution,\nMiniVLM, for vision-language (VL) tasks, which is smaller\nand faster, and thus can be deployed in real-world applica-\ntions on resource-constrained devices. For the vision mod-\nule, we design the Two-stage Efﬁcient feature Extractor\n(TEE), to signiﬁcantly save computation by simplifying the\nregion head and replacing regular convolutional layers with\npointwise and depthwise convolution layers. To improve\nthe small-model pre-training, we leverage large models and\nlarge-scale dataset. We ﬁne-tune the pre-trained model on\nvarious downstream VL tasks, and show that MiniVLM can\nretain 94 −97% of the accuracy with 27% parameters and\n1% FLOPS compared to the state-of-the-art VL model.\nReferences\n[1] Peter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. SPICE: semantic propositional image cap-\ntion evaluation. In Computer Vision - ECCV 2016 - 14th Eu-\nropean Conference, Amsterdam, The Netherlands, October\n11-14, 2016, Proceedings, Part V , volume 9909 of Lecture\nNotes in Computer Science, pages 382–398. Springer, 2016.\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and\nvisual question answering. In 2018 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2018, Salt\nLake City, UT, USA, June 18-22, 2018 , pages 6077–6086.\nIEEE Computer Society, 2018.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Computer Vision -\nECCV 2020 - 16th European Conference, Glasgow, UK, Au-\ngust 23-28, 2020, Proceedings, Part I, volume 12346 of Lec-\nture Notes in Computer Science , pages 213–229. Springer,\n2020.\n[4] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUNITER: universal image-text representation learning. In\nComputer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part XXX ,\nvolume 12375 of Lecture Notes in Computer Science, pages\n104–120. Springer, 2020.\n[5] Michael J. Denkowski and Alon Lavie. Meteor universal:\nLanguage speciﬁc translation evaluation for any target lan-\nguage. In Proceedings of the Ninth Workshop on Statistical\nMachine Translation, WMT@ACL 2014, June 26-27, 2014,\nBaltimore, Maryland, USA, pages 376–380. The Association\nfor Computer Linguistics, 2014.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Papers) ,\npages 4171–4186. Association for Computational Linguis-\ntics, 2019.\n[7] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V . Le. NAS-FPN:\nlearning scalable feature pyramid architecture for object de-\ntection. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-\n20, 2019, pages 7036–7045. Computer Vision Foundation /\nIEEE, 2019.\n[8] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the V in VQA matter: El-\nevating the role of image understanding in visual question\nanswering. CoRR, abs/1612.00837, 2016.\n[9] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\nGirshick. Mask R-CNN. IEEE Trans. Pattern Anal. Mach.\nIntell., 42(2):386–397, 2020.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages\n770–778. IEEE Computer Society, 2016.\n[11] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\nDistilling the knowledge in a neural network. CoRR,\nabs/1503.02531, 2015.\n[12] Jan Hendrik Hosang, Rodrigo Benenson, and Bernt Schiele.\nLearning non-maximum suppression. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017 , pages 6469–\n6477. IEEE Computer Society, 2017.\n[13] Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang,\nJianfeng Gao, and Zicheng Liu. VIVO: surpassing human\nperformance in novel object captioning with visual vocabu-\nlary pre-training. CoRR, abs/2009.13682, 2020.\n[14] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei.\nAttention on attention for image captioning. In International\nConference on Computer Vision, 2019.\n[15] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and\nJianlong Fu. Pixel-bert: Aligning image pixels with text\nby deep multi-modal transformers. CoRR, abs/2004.00849,\n2020.\n[16] Drew A. Hudson and Christopher D. Manning. GQA: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2019, Long Beach, CA,\nUSA, June 16-20, 2019, pages 6700–6709. Computer Vision\nFoundation / IEEE, 2019.\n[17] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-\nMiller, and Xinlei Chen. In defense of grid features for visual\nquestion answering. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10267–10276, 2020.\n[18] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Dis-\ntilling BERT for natural language understanding. CoRR,\nabs/1909.10351, 2019.\n[19] Andrej Karpathy and Fei-Fei Li. Deep visual-semantic align-\nments for generating image descriptions. In IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2015, Boston, MA, USA, June 7-12, 2015, pages 3128–3137.\nIEEE Computer Society, 2015.\n[20] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\nFei-Fei Li. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations. CoRR,\nabs/1602.07332, 2016.\n[21] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig,\nand Vittorio Ferrari. The open images dataset v4: Uniﬁed\nimage classiﬁcation, object detection, and visual relationship\ndetection at scale. IJCV, 2020.\n[22] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite\nBERT for self-supervised learning of language representa-\ntions. CoRR, abs/1909.11942, 2019.\n[23] Hei Law, Yun Teng, Olga Russakovsky, and Jia Deng.\nCornernet-lite: Efﬁcient keypoint based object detection.\nCoRR, abs/1904.08900, 2019.\n[24] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin\nJiang. Unicoder-vl: A universal encoder for vision and lan-\nguage by cross-modal pre-training. In The Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 , pages\n11336–11344. AAAI Press, 2020.\n[25] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. Visualbert: A simple and performant\nbaseline for vision and language. CoRR, abs/1908.03557,\n2019.\n[26] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In Computer\nVision - ECCV 2020 - 16th European Conference, Glasgow,\nUK, August 23-28, 2020, Proceedings, Part XXX , volume\n12375 of Lecture Notes in Computer Science , pages 121–\n137. Springer, 2020.\n[27] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong\nDeng, and Jian Sun. Light-head R-CNN: in defense of two-\nstage object detector. CoRR, abs/1711.07264, 2017.\n[28] Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren\nZhou, and Hongxia Yang. Interbert: Vision-and-language\ninteraction for multi-modal pretraining. arXiv preprint\narXiv:2003.13198, 2020.\n[29] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\nnetworks for object detection. In 2017 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2017,\nHonolulu, HI, USA, July 21-26, 2017, pages 936–944. IEEE\nComputer Society, 2017.\n[30] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,\nand Piotr Doll´ar. Focal loss for dense object detection. IEEE\nTrans. Pattern Anal. Mach. Intell., 42(2):318–327, 2020.\n[31] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.\nBourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll´ar, and C. Lawrence Zitnick. Microsoft\nCOCO: common objects in context. CoRR, abs/1405.0312,\n2014.\n[32] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C.\nBerg. SSD: single shot multibox detector. In Computer Vi-\nsion - ECCV 2016 - 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part I,\nvolume 9905 of Lecture Notes in Computer Science , pages\n21–37. Springer, 2016.\n[33] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient\ndescent with warm restarts. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings. OpenRe-\nview.net, 2017.\n[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net, 2019.\n[35] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS 2019, 8-\n14 December 2019, Vancouver, BC, Canada , pages 13–23,\n2019.\n[36] Sachin Mehta, Mohammad Rastegari, Linda G. Shapiro, and\nHannaneh Hajishirzi. Espnetv2: A light-weight, power efﬁ-\ncient, and general purpose convolutional neural network. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 ,\npages 9190–9200. Computer Vision Foundation / IEEE,\n2019.\n[37] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In Advances in Neural Information Processing\nSystems 24: 25th Annual Conference on Neural Informa-\ntion Processing Systems 2011. Proceedings of a meeting held\n12-14 December 2011, Granada, Spain , pages 1143–1151,\n2011.\n[38] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, July 6-12, 2002,\nPhiladelphia, PA, USA, pages 311–318. ACL, 2002.\n[39] Ilija Radosavovic, Piotr Doll ´ar, Ross B. Girshick, Georgia\nGkioxari, and Kaiming He. Data distillation: Towards omni-\nsupervised learning. In 2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt Lake City,\nUT, USA, June 18-22, 2018, pages 4119–4128. IEEE Com-\nputer Society, 2018.\n[40] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,\nand Ali Farhadi. You only look once: Uniﬁed, real-time ob-\nject detection. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, pages 779–788. IEEE Computer Society,\n2016.\n[41] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with re-\ngion proposal networks. In Advances in Neural Information\nProcessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12, 2015,\nMontreal, Quebec, Canada, pages 91–99, 2015.\n[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg,\nand Fei-Fei Li. Imagenet large scale visual recognition chal-\nlenge. Int. J. Comput. Vis., 115(3):211–252, 2015.\n[43] Victor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. Distilbert, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. CoRR, abs/1910.01108,\n2019.\n[44] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\nlarge-scale, high-quality dataset for object detection. In2019\nIEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, pages 8429–8438. IEEE, 2019.\n[45] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed,\nimage alt-text dataset for automatic image captioning. In\nProceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers ,\npages 2556–2565. Association for Computational Linguis-\ntics, 2018.\n[46] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. VL-BERT: pre-training of generic\nvisual-linguistic representations. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[47] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Hua-\njun Bai, and Yoav Artzi. A corpus for reasoning about natu-\nral language grounded in photographs. In Proceedings of the\n57th Conference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,\nVolume 1: Long Papers , pages 6418–6428. Association for\nComputational Linguistics, 2019.\n[48] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\nChanghu Wang, and Ping Luo. SparseR-CNN: End-to-end\nobject detection with learnable proposals. arXiv preprint\narXiv:2011.12450, 2020.\n[49] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yim-\ning Yang, and Denny Zhou. Mobilebert: a compact task-\nagnostic BERT for resource-limited devices. In Proceed-\nings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 2158–2170. Association for Computational Linguis-\ntics, 2020.\n[50] Hao Tan and Mohit Bansal. LXMERT: learning cross-\nmodality encoder representations from transformers. In\nProceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7, 2019 ,\npages 5099–5110. Association for Computational Linguis-\ntics, 2019.\n[51] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. In Pro-\nceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, Cali-\nfornia, USA, volume 97 ofProceedings of Machine Learning\nResearch, pages 6105–6114. PMLR, 2019.\n[52] Mingxing Tan, Ruoming Pang, and Quoc V . Le. Efﬁcientdet:\nScalable and efﬁcient object detection. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\nCVPR 2020, Seattle, WA, USA, June 13-19, 2020 , pages\n10778–10787. IEEE, 2020.\n[53] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\nA simple and strong anchor-free object detector. CoRR,\nabs/2006.09214, 2020.\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, 4-9 December\n2017, Long Beach, CA, USA, pages 5998–6008, 2017.\n[55] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalu-\nation. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2015, Boston, MA, USA, June 7-12,\n2015, pages 4566–4575. IEEE Computer Society, 2015.\n[56] Jianfeng Wang, Xi Yin, Lijuan Wang, and Lei Zhang.\nHashing-based non-maximum suppression for crowded ob-\nject detection. CoRR, abs/2005.11426, 2020.\n[57] Robert J. Wang, Xiang Li, Shuang Ao, and Charles X.\nLing. Pelee: A real-time object detection system on mo-\nbile devices. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada, April\n30 - May 3, 2018, Workshop Track Proceedings . OpenRe-\nview.net, 2018.\n[58] Tiancai Wang, Rao Muhammad Anwer, Hisham Cholakkal,\nFahad Shahbaz Khan, Yanwei Pang, and Ling Shao. Learn-\ning rich features at high-speed for single-shot object detec-\ntion. In 2019 IEEE/CVF International Conference on Com-\nputer Vision, ICCV 2019, Seoul, Korea (South), October 27\n- November 2, 2019, pages 1971–1980. IEEE, 2019.\n[59] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang,\nand Ming Zhou. Minilm: Deep self-attention distillation\nfor task-agnostic compression of pre-trained transformers.\nCoRR, abs/2002.10957, 2020.\n[60] Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan\nWang, Hongzhi Li, and Yun Fu. Rethinking classiﬁcation\nand localization for object detection. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\nCVPR 2020, Seattle, WA, USA, June 13-19, 2020 , pages\n10183–10192. IEEE, 2020.\n[61] Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and\nQuoc V . Le. Self-training with noisy student improves ima-\ngenet classiﬁcation. In 2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2020, Seattle,\nWA, USA, June 13-19, 2020 , pages 10684–10695. IEEE,\n2020.\n[62] I. Zeki Yalniz, Herv ´e J´egou, Kan Chen, Manohar Paluri, and\nDhruv Mahajan. Billion-scale semi-supervised learning for\nimage classiﬁcation. CoRR, abs/1905.00546, 2019.\n[63] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\nmaier. From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descrip-\ntions. Trans. Assoc. Comput. Linguistics, 2:67–78, 2014.\n[64] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua\nWu, and Haifeng Wang. Ernie-vil: Knowledge enhanced\nvision-language representations through scene graph. CoRR,\nabs/2006.16934, 2020.\n[65] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\nStan Z. Li. Bridging the gap between anchor-based and\nanchor-free detection via adaptive training sample selection.\nIn 2020 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19,\n2020, pages 9756–9765. IEEE, 2020.\n[66] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson J. Corso, and Jianfeng Gao. Uniﬁed vision-language\npre-training for image captioning and VQA. In The Thirty-\nFourth AAAI Conference on Artiﬁcial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artiﬁ-\ncial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artiﬁcial Intelli-\ngence, EAAI 2020, New York, NY, USA, February 7-12, 2020,\npages 13041–13049. AAAI Press, 2020."
}