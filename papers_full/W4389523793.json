{
  "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
  "url": "https://openalex.org/W4389523793",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2399225165",
      "name": "Kaitlyn Zhou",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2089131864",
      "name": "Dan Jurafsky",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2169829788",
      "name": "Tatsunori Hashimoto",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4377865297",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W4241071327",
    "https://openalex.org/W3128621548",
    "https://openalex.org/W4389520255",
    "https://openalex.org/W3035441651",
    "https://openalex.org/W2082550766",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W1987274994",
    "https://openalex.org/W2147638277",
    "https://openalex.org/W4300772090",
    "https://openalex.org/W2159035740",
    "https://openalex.org/W2963154734",
    "https://openalex.org/W2006941876",
    "https://openalex.org/W3205597237",
    "https://openalex.org/W2904680276",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4387271845",
    "https://openalex.org/W2995839792",
    "https://openalex.org/W4290994954",
    "https://openalex.org/W3034257552",
    "https://openalex.org/W3176826827",
    "https://openalex.org/W2030986807",
    "https://openalex.org/W3212275118",
    "https://openalex.org/W4285203730",
    "https://openalex.org/W3199958362",
    "https://openalex.org/W2741680176",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4297146946",
    "https://openalex.org/W3035072529",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W4226335419",
    "https://openalex.org/W2237502368",
    "https://openalex.org/W4303648016",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W2076518434",
    "https://openalex.org/W3099142828",
    "https://openalex.org/W4385573257",
    "https://openalex.org/W4281748205",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3033129824"
  ],
  "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like \"I'm sure it's\", \"I think it's\", or \"Wikipedia says it's\" affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5506–5524\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNavigating the Grey Area: How Expressions of\nUncertainty and Overconfidence Affect Language Models\nKaitlyn Zhou\nStanford University\nkatezhou@stanford.edu\nDan Jurafsky\nStanford University\njurafsky@stanford.edu\nTatsunori Hashimoto\nStanford University\nthashim@stanford.edu\nAbstract\nThe increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1 Introduction\nAs natural language systems are increasingly used\nin situations involving factuality and knowledge,\nit becomes important for LMs to be able to inter-\npret how humans talk about knowledge. LMs must\nlearn to accurately interpret linguistic cues like ex-\npressions of uncertainty and certainty that are used\nto talk about confidence, source, and limitations of\ninformation. In this work, we seek to understand\nhow models interpret this linguistic phenomenon\nby measuring how language generation varies when\nprompted with expressions of uncertainty.\nQ: What is t he \ncapital of F rance?\nA: I t hink it ’ s...\n“P aris. ”\nPROMPT GENERA TION\nQ: What is t he \ncapital of F rance?\nA: I’ m 100% \ncer tain it ’ s...\nQ: What is t he \ncapital of Italy?\nA: R ome. I’ m sur e.\n“L y on. ”\nQ: What is t he \ncapital of F rance? \nA: Paris. I’ m sur e. Single prompt \nwhich contains a \nseries of calibrated \nin-context learning \nexamples\n“I’m sure” is \nappended to \nhigh confidence \nanswers\n“London.  \nI’ m sur e. ”\nZer o-shot Injection of\nExpr essions of Uncer tainty\nMu l ti-shot Pr omptin g  t o\nEmit Expr essions of Uncer tainty\nV er b alNu merical\nQ: What is t he \ncapital of Al b ania? \nA: T irana.\nQ: What is t he \ncapital of t he UK ?\nA:\nFigure 1: Using zero-shot promoting to inject verbal and\nnumerical uncertainties into trivia questions. We find\ndrops in accuracy when expressions of high certainty\nare used compared to expressions of low certainty.\nNaturalistic expressions of uncertainty/certainty\ncover a broad range of discourse acts such as signal-\ning hesitancy, attributing information, or acknowl-\nedging limitations. Prior work has focused on one\naspect of this: linguistic calibration, particularly on\nlearning the mapping between the internal proba-\nbilities of a model and an ordinal output (Kadavath\net al., 2022; Lin et al., 2022; Mielke et al., 2022).\nYet epistemic expressions encompass a much wider\nrange of features than can be represented by a sin-\ngle value, such as the sources of information, or the\nnature of speaker commitment, or whether a piece\nof knowledge is asserted or presupposed. Our work\nseeks to understand how the various dimensions\nof uncertainty like hedges (e.g., \"It could be. . . \"),\nfactive verbs (e.g., \"We realize it’s. . . \"), and ev-\nidential markers (e.g., \"Wikipedia says it’s. . . \")\nimpact language generations. By shifting our focus\nto naturalistic expressions of uncertainty and cer-\ntainty, we would enable models to flexibly interpret\na wider range of uncertainties otherwise not possi-\nble under the current linguistic calibration setup.\nTo understand how epistemic expressions affect\nlanguage models, we use zero-shot prompting and\ninject verbal and numerical markers into trivia ques-\ntion prompts. This process converts a prompt like\n\"What is the capital of France?\" to, \"What is the\ncapital of France, I think it’s. . .\". (Figure 1). Aided\nby our linguistic typology, we then measure how\ndifferent epistemic marker types impact model ac-\n5506\ncuracy. By doing so, we complement current lin-\nguistic calibration work and paint a more complete\npicture of the role of these epistemic markers.\nWe begin with two broad hypotheses on how\nLMs might respond to expressions of uncertainty.\nFirst, we might suppose that models are robust to\nany added expressions of uncertainty in the prompt.\nAn alternative hypothesis is that, models might re-\nspond differently based on the uncertainty cues,\nand using a marker suggesting certainty or confi-\ndence might be more likely to produce the correct\nresponse than a prompt with low certainty or con-\nfidence. Under the latter hypothesis, we might\nexpect performance for a prompt with no epistemic\nmarkers (which we call the standard method) to\nlie in between these two. This second hypothesis\nwould also be consistent with prior work showing\nthat LMs can generate language in the style of di-\nverse personas (Lee et al., 2022; Park et al., 2022).\nSurprisingly, we find that injecting expressions\nof high certainty like \"I’m certain it’s\" or “I’m\n100% sure\" actually leads to lower accuracy. We\nfollow up with qualitative and quantitative analysis\nof popular training data, suggesting some potential\nsources of these unexpected behaviors. Our work\nthus offers three key contributions:\n• We introduce a typology of expressions of\nuncertainty to evaluate how linguistic features\nimpact LM generation.\n• We demonstrate how model accuracy suffers\nwhen expressions of certainty (e.g., \"I’m cer-\ntain\") are used.\n• We perform qualitative and quantitative anal-\nysis to reveal the potential origins of these\nunexpected behaviors.\nTogether, our findings illustrate the shortcom-\nings of models’ abilities to interpret expressions of\nuncertainty and highlight the gaps that still exist\nbetween natural and generated language.1\n2 Expressions of Certainty and\nUncertainty: Linguistic Background\nA broad linguistic literature exists in epistemolog-\nical markers related to certainty and uncertainty,\nboth for strengthening/weakening category mem-\nbership and strengthening/weakening speaker com-\nmitment to the truth value of a proposition (see\nRelated Work). For convenience, we broadly\n1Details: https://github.com/katezhou/navigating_the_grey\nStr engt heners\n(Cer tainty)\nW eak eners\n(Uncer tainty)\nPlausibility Shields\n“I t hink it ’ s... ”\nAppr o ximat ors\n“ Ar ound ... ”\nF activ e V erbs\n“He r ealiz ed it ’ s... ”\n“I’ m cer tain it ’ s... ”\n“The y ackno wledged it ’ s... ”“ Allegedly , it ’ s... ”\nEvidentials\nFigure 2: Lexical Features of Expressions of Uncer-\ntainty. Uncertainty classification partly adapted from\n(Prince et al., 1982). Certainty markers are strength-\neners which contain factive verbs. Evidential markers\ncan be both expressions of certainty and uncertainty\n(strengtheners or weakeners). Personal pronouns and\nreference to sources are additional dimensions of ex-\npressions of (un)certainty not shown in this diagram.\ngroup these linguistic devices into weakeners and\nstrengtheners.\nThe most widely studied weakeners are hedges,\nfirst defined by Lakoff (1975) as related to modi-\nfying or weakening the category membership of a\npredicate or nominal. The central kind of hedges\nare approximators (e.g., somewhat, kind of, about,\napproximately), which hedge propositional content.\nAnother class of weakeners that some (like Prince\net al. (1982)) but not others classify under hedges\nare plausibility shields which express a speaker’s\nlower level of commitment (e.g., I think, I believe).\nStrengtheners are constructions that mark cer-\ntainty. We use this term to refer to strengthening\nspeaker commitment to truth value, and also to\nstrengthening category membership. Strengthen-\ners include boosters or intensifiers (e.g., \"I am\ncertain\", \"Undoubtedly\") (Hyland, 2005, 2014).\nWhile boosters can assert certainty or truth, a\nsecond kind of strengthening construction, the fac-\ntive verb, is used to presuppose certainty or truth.\nFactive verbs like \"know\", \"realize\", or \"under-\nstand\" (Kiparsky and Kiparsky, 1970) presuppose\nthe truth of the complement sentence. The state-\nment “X realizes Y\" presupposes that Y is true,\n(and also asserts that X is aware of it). By contrast,\n“X believes Y\" makes no presupposition about the\ntruth of Y . Thus the sentence“He realizes [Madrid\nis the capital of France]\" is infelicitous, because\n“realize\" as a factive verb presupposes the truth of\nits complement clause, which in this case is false\n(Madrid is not the capital of France).\nA third class of markers can mark both certainty\nand uncertainty by directly indicating the source of\nthe information. Such expressions (e.g., \"Accord-\n5507\ning to Wikipedia\", \"I heard\", \"I saw\") are called\nevidential markers, linguistic signals that tells the\nhearer where the information came from (Aikhen-\nvald, 2004). One subtype of evidential markers,\nquotative markers, are used when reported informa-\ntion overtly references a source (e.g.,\"According to\nresearch in the latest issue of Nature\", \"Two recent\nstudies demonstrate that. . . \"). We examine when\nreferences are citing a source (e.g., \"Wikipedia\nsays\") versus when the source is unspecified or\nvery indirect (e.g., \"They said\"). We’ll refer to this\nformer case as sourced as a shorthand for indicat-\ning that a source is mentioned.\nFinally, first-person personal pronouns (e.g.,\n\"I\", \"we\", \"our\") can be used to mark subjectivity\nand uncertainty in expressions like \"I think\".\n2.1 Expressions of Uncertainty Typology\nUsing a bottom-up process, we gathered a diverse\nset of templates guided by the linguistic literature\nto develop a single, coherent taxonomy and clas-\nsify templates. To gather the templates, the authors\nrelied on the literature from Prince et al. (1982),\nbrainstormed additional templates, and used crowd\nsourcing via Amazon Mechanical Turk. The au-\nthors then drew on linguistic literature to aug-\nment this list, adding dimensions such as factive\nverbs (Kiparsky and Kiparsky, 1970), evidentials\n(a broader category of attributions) (Aikhenvald,\n2004), and authoritative sources. Finally, they inte-\ngrated this literature, developed a coherent taxon-\nomy, and classified each of our templates. Figure\n2 illustrates how these markers relate to certainty\nand uncertainty in our coding scheme.\nThe final list of templates includes: weakeners,\nstrengtheners, plausibility shields, factives, eviden-\ntial markers, mentions of sources, and personal\npronouns (Appendix Table 6). Each expression is\nthen coded for the linguistic features above, allow-\ning us to analyze how epistemological modifiers\nimpact language modeling in the QA setting.2\n2.2 Amazon Mechanical Turk Details\nWe use Amazon Mechanical Turk to crowd-source\nadditional expressions of uncertainty (Figure 7 in\nAppendix). Workers were filtered to be have HITs\ngreater than 99 and to have at least 500 approved\nHITs. Given the simplicity of the task, we esti-\n2Approximators, which are primarily used for expressing\nuncertainty of continuous items are excluded from our tem-\nplates as we are primarily working with trivia questions with\ndiscrete answers.\nmated it would take users a minute or two to com-\nplete the task, a paid users $0.35 USD for the task\nwhich results in roughly $10.50 USD to $21.00\nUSD an hour. We collected a total of 9 samples of\n5 examples each. The authors then read, filtered,\nand modified the examples to follow the overall lin-\nguistic structure of the other templates. This study\ndid not require IRB approval as it is not a study\nof humans and we do not collect any information\nabout the annotators themselves.\n3 Methods\nTo study how models interpret uncertainty, we in-\nject markers into trivia questions in open-ended\nquestion-answering. Using our typology, we create\nfifty sentences (minimal pairs) for every question.\nOur datasets include TriviaQA, a standard QA\ndataset (Joshi et al., 2017); Natural Questions\n(closed-book), an aggregated set of Google queries\n(Kwiatkowski et al., 2019a); CountryQA, which\nwe constructed using names of countries and\ncapitals in the form of \"What is the capital of\nAfghanistan?\"; and Jeopardy questions crawled\nfrom a fan-created database, J! Archive.3 We use\na random subset of 200 questions for three of our\ndatasets. For CountryQA, we used all 53 questions\nwhose answers were in vocabulary (details in 3.2).\nAlthough the total number of questions per tem-\nplate is small, we present our key findings in aggre-\ngate across all templates with shared characteristics,\nincreasing our statistical power. We test each of our\nfifty templates across this subset of questions and\ncalculate 95% confidence intervals using bootstrap\nresampling or t-tests.\nWe primarily study OpenAI’s GPT models as\nthey are commonly used large language models\nwith reasonable performance on QA tasks (Liang\net al., 2023). For all models, except GPT4, we\nset temperature to 1. For the generated tokens, we\ntake the sum of the probability assigned to the gold\nanswer(s) to be the probability-on-gold. When cal-\nculating accuracy, we generate 10 tokens and if any\nof the tokens match the answer (or aliases of the\nanswer), we’ll count that as a correct generation.\nThis is done to not unfairly disadvantage templates\nthat are prone to generating words prior to emitting\nthe answer (e.g., \"Allegedly, it’s said to be. . . \").4\nFor GPT4, where the log probabilities were not\n3https://j-archive.com/\n4Authors also manually inspected 1,000 answers to ensure\nthis approach doesn’t lead to false positives e.g., \"The answer\nis not Paris.\"\n5508\navailable, we set the temperature to 0, also ensur-\ning a (mostly) deterministic output, and used the\ngenerated text.\n3.1 Additional Prompting Details\nFor Section 4, we use OpenAI’s researcher API and\nretrieve the top 50 most probable predictions per\ntoken. For Section A.1 we use the standard API\nand retrieve the top 5 most probable predictions per\ntoken.\nWe are careful that our prompts do not end with\ntrailing white space (\" \") as recommended by Ope-\nnAI in order to prompt the best generations. We\nalso use the delimiters \"Q:\" and \"A:\" to signal the\nstart and end of questions and answers.5\nLastly, for additional transparency and given the\nfrequent changes in models, we also provide times-\ntamps of when the models were prompted. We\nused the Davinci model from Feb 2023 on all four\ndatasets. Experiments on Ada, Babbage, Curie,\ntext-davinci–03, and GPT4 models on the Trivi-\naQA dataset were all conducted in June of 2023.\nExperiments on Ada, Babbage, Curie, text-davinci,\nand GPT4 models on the CountryQA, NaturalQA,\nand Jeopardy datasets were conducted the week of\nAugust 28th, 2023.\n3.2 Challenges in Open-Ended Generation\nA key challenge of open-ended generation is mea-\nsuring log probabilities of generated tokens, needed\nfor our analysis. Answers can be paraphrased in a\nmyriad of ways, making the scoring of tokens diffi-\ncult. Furthermore, some answers are OOV which\nnecessitates multi-subtoken generation. To avoid\nthese potential confounders, we filter out questions\nwith multi-word or OOV answers, allowing us to\nmore accurately measure the probability placed on\nthe gold answers.\nThe filtering of questions has the added advan-\ntage of prioritizing questions with high-frequency\nanswers. By focusing on questions with high-\nfrequency answers, the resulting dataset contains\nquestions for which LMs are more likely to know\nthe right answer. This helps mitigate the effects of\nperformance variation that may result from a wide\nrange of question difficulty as well as demonstrate\nthat our observed effects are not isolated to unusual\nor rare questions.\n5In CountryQA and TriviaQA, an additional new line char-\nacter was added before \"A:\". To reduce accruing additional\ncosts, NaturalQA and Jeopardy results were not rerun to match\nthis exact template.\n4 The Impact of Uncertainty on\nLanguage Generation\nInformation in real-life text is rarely in black-and-\nwhite, and expressions of uncertainty are neces-\nsary in supporting decision-making processes. In\nthis section, we investigate how GPT3’s generation\nchanges based on the verbal expressions of uncer-\ntainty in its prompt (Figure 1, top) and whether\nsome expressions of uncertainty can have system-\natic effects on model behavior.\n4.1 Variation in GPT3 Responses Across\nVerbal Uncertainties\nWe evaluate GPT3 (davinci) using zero-shot\nprompting on our four datasets. We find that the\nresults do not support our first hypothesis: GPT3\nis highly sensitive to uncertainty cues, and accu-\nracy changes significantly depending on the cer-\ntainty/uncertainty expression used. Across our\ndatasets, we find that accuracies can change by\nup to 80% on the exact same set of questions. This\nis especially pronounced in CountryQA where a\ntemplate like, \"We realize it’s. . . \"achieves 14%\naccuracy while many other templates result in per-\nfect accuracy. In TriviaQA, when prompted with a\ntemplate such as \"I’m certain it’s. . . \"the model’s\naccuracy is 42% but when prompted with \"I would\nneed to double check but maybe it’s. . . \", accuracy\nincreases to 56%. Our findings illustrate that ex-\npressions of uncertainty affect language generation\nand that the changes resulting from these expres-\nsions have substantive impact on overall accuracy.\nTurning to our second hypothesis, we seek to\nunderstand how GPT3’s responses to QA answers\nchange based on the expression of uncertainty used\nin the prompt. Surprisingly, we find that weaken-\ners perform significantly better than strengtheners\nacross all four of our datasets. The average accu-\nracy among weakeners across all four datasets is\n47% compared to 40% among strengtheners. This\neffect is especially large in CountryQA where the\naccuracy gap is 17%. This effect is driven by the\nuse of factive verbs in strengtheners (as nearly all\nuses of factive verbs in our templates are strength-\neners)6, and the use of factive verbs consistently\nresults in significant losses in accuracy (Figure 3).\nIn other words, when the template presupposes the\ntruth, accuracy drops.\nThis finding contradicts the second hypothesis,\nas we might have expected expressions of certainty\n6The exception being \"I vaguely remember it’s. . . \".\n5509\nCountryQA Jeopardy NaturalQA TriviaQA\nDatasets\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nAccuracy by Factive Verbs\nFactive\nNot Factive\nCountryQA Jeopardy NaturalQA TriviaQA\nDatasets\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nAccuracy by Evidential Marker\nEvidential\nNot Evidential\nFigure 3: Significant and consistent accuracy losses for templates with factive verbs (left). Evidential markers\nsignificantly improves accuracy in three out of four datasets (right). 95% CI calculated using bootstrap resampling.\nVisualizing results for GPT-3 (davinci).\nada babbage curie davinci instruct gpt-4\nBoosters 0.091 0.257 0.313 0.392 0.589 0.793\nHedges 0.079 0.272 0.333*** 0.468*** 0.642*** 0.822***\nFactive Verbs 0.078 0.237 0.293 0.347 0.555 0.771\nNon-Factives Verbs 0.085* 0.276*** 0.336*** 0.468*** 0.641*** 0.821***\nEvidentials 0.087** 0.281*** 0.347*** 0.449* 0.640*** 0.820***\nNon-evidentials 0.080 0.250 0.301 0.433 0.601 0.799\nTable 1: Across all six models tested, hedges outperform boosters, non-factive verbs outperform factives and\nevidentials out-perform non-evidentials. (Instruct = text-davinci-003, GPT4 uses context window 32K.) t-test\np-values, * < 0.05, ** < 0.01, *** < 0.001**.\nto improve performance, not hurt it. This is par-\nticularly concerning as confident prompts —which\nLM users might naturally expect to result in better\ngenerations— actually lead to worse generation.\nFurthermore, we find that in three of our four\ndatasets, the use of evidential markers significantly\nimproves performance. In fact, some of the best\nperforming templates include evidential markers\nwith a source. This is also consistent with recent\nwork showing how the grounding of prompts in-\ncreases model accuracy in generation (Weller et al.,\n2023). The top ten performing prompts for each\ndataset are listed in Tables 7, 8, 9, and 10.\nThe results across the other linguistic features\nare mixed. Across the four datasets, there is not\na consistent improvement from the use of plausi-\nbility shields, sources, or personal pronouns (See\nAppendix).\nFor generalizability, we test five additional mod-\nels (GPT3 Ada, Babbage, Curie, text-davinci-003,\nGPT4 (32k)) on the TriviaQA dataset (n=200) with\nour fifty templates and find our results reproduce.\nAcross almost all models, boosters and factive\nverbs result in significant decreases to model per-\nformance and meanwhile evidentials markers sig-\nnificantly improve performance (Table 1).\n4.2 Expressions of Uncertainty Compared to\nthe Standard Prompting Method\nLastly, we find that the use of expressions of un-\ncertainty might actually lead to better performance\nthan the standard prompting method (i.e., just sim-\nply using \"Q: <question> A:\"). In TriviaQA the\ntemplate \"Online says it’s. . . \"achieves an accuracy\nof 66% compared to 63% achieved by the standard\nmethod. In Natural Questions, there are seven tem-\nplates that outperform the standard method, six of\nwhich are expressions of uncertainty. Using our\nresults with six models across all four datasets, we\naggregated the results by template and identified\nsix templates which perform significantly better\n(t-test; p-value < 0.05) than the standard method\n(Appendix Table 11) These promising results sug-\ngest that including uncertainty may not only help\nhuman decision makers, it may also improve the\nabsolute accuracy of the model.\n5510\n5 Why Does Certainty Hurt?\nWhat explains our surprising finding that templates\nwith weakeners outperform templates with strength-\neners? Here, we discuss and evaluate hypotheses\nand potential confounders.\n5.1 Certainty Affects Performance\nIndependent of Perplexity\nRecent work from Gonen et al. (2022) discusses\nhow perplexity, as a proxy for frequency, explains\nvariation in quality of generations. Phrases with\nhigh perplexity result in a significant drop in perfor-\nmance when used as prompts in language modeling.\nWe test if perplexity could be a confounding vari-\nable in our experiments but find that the perplexity\nof our templates is in fact not correlated with the\naccuracy of the prompts (Pearson’s ρ= -0.03) (See\nAppendix). These results validate that the varia-\ntions from prompts are not caused by trivial factors\nsuch as text length or the frequency of the expres-\nsion in training data.\n5.2 A Redistribution of Probability Mass\nWhen Prompted with Weakeners\nCould it be that weakeners are changing the un-\nderlying probability distribution of the potential\nanswers? If weakeners change the answer distribu-\ntion, we might expect weakeners to induce an in-\ncrease in the probability-on-gold (which is defined\nas the sum of the probabilities placed on all of the\nanswer aliases). Focusing on GPT3 (davinci), we\ncalculate the average probability-on-gold among\nall correctly predicted answers and find this is not\nthe case. In fact, the probability-on-gold from tem-\nplates with weakeners is slightly lower than the\nprobability-on-gold from templates with strength-\neners. This is true across three of our four datasets\nNaturalQA (42% vs 45%), JeopardyQA (47% vs\n51%), and TriviaQA (53% vs 55%).\nDataset weakeners strengtheners\nTriviaQA 2.980 ± 0.01 2.917 ± 0.01\nCountryQA 3.078 ± 0.02 2.875 ± 0.03\nJeopardy 3.170 ± 0.01 3.089 ± 0.01\nNaturalQA 3.167 ± 0.01 3.106 ± 0.01\nTable 2: Average entropy of the probability distribution\nof alternative tokens among weakeners and strengthen-\ners. Across all four datasets, entropy is higher among\nweakeners, an indication the model places probability\nmore evenly across the alternative answers. 95% CI\ncalculated using standard error.\nFurthermore, we find that weakeners led to a flat-\ntening of the distribution of probability mass across\nanswers, compared to strengtheners. We look at\nthe entropy of the probability distribution of top to-\nkens not counting the top prediction; essentially the\nuncertainty among all but the top candidate. This\nentropy is significantly higher among weakeners\nthan strengtheners (Table 2). Our finding suggests\nthat the increase in accuracy of weakeners is not\ndue to an increase in answer confidence, but rather\nwhen a weakener is used, the model responds by\nplacing probability more evenly across each of the\nremaining possible options.\n5.3 Certainty Used in Questions Instead of\nAnswers\nWhy is it that expressions of certainty lead to low-\nered performance? We look for potential explana-\ntions by examining expressions of uncertainty in\nlanguage model pretraining data. We queried for\nexpressions of uncertainty like \"I’m certain it’s\"\nor \"I’m sure it’s\" in The Pile (Gao et al., 2020),\na popular pretaining dataset, and in a qualitative\nexploratory analysis, found that certainty was often\nbeing used in questions rather than answers.\nTo quantitatively test this hypothesis, we mea-\nsure the volume of expressions of uncertainty in the\nStack Exchange section of the Pile with a set of un-\ncertainty expressions. To our surprise, expressions\nof certainty, \"I’m sure\" and \"It must be\" , occur\nless than half as often in answers (104 instances\nper million words) as in questions (280 instances\nper million words). Conversely, expressions of un-\ncertainty, \"It could be\" and \"Maybe it’s\", occur\nabout twice as often in answers (436 instances per\nmillion words) as in questions (222 instances per\nmillion words). In other words, those asking for in-\nformation are using expressions of certainty while\nthose with the answers (or attempting to answer\nthe question) are using expressions of uncertainty\n(details and statistics in Appendix).\nA number of factors could be contributing to this\nphenomenon, including the use of uncertainty to\nexpress politeness (Brown and Levinson, 1987) or\nthe use of certainty to rule out options, or the use of\ncertainty to establish group membership (Hyland,\n1998). To give an estimate of the prevalence of\nthese acts, we perform qualitative open coding on\n100 samples of two expressions. In question posts,\nthe high certainty expression \"I’m sure\", is used\n34 times to establish group membership/admit ig-\n5511\nnorance (e.g., \"I’m sure it’s a simple error\") and\nused 22 times to isolate an issue (e.g., \"I’m sure\nI am catching it right\"). In answer posts, the low\ncertainty expression \"I think\", is used 25 times to\npolitely give instructions and corrections (e.g., \"I\nthink this should work for you:. . . \" and \"I think\nyou meant to write. . . \"). These instances reveal the\nways in which humans may be using expressions\nof certainty that go beyond expressing epistemic\ncertainty. Our analysis of pretraining data suggests\nthat language models might be mimicking this be-\nhavior and responding to prompts with epistemic\nmarkers in this surprising but explainable way.\n6 The Impact of the Degree of\nUncertainty on Performance\nResults from Section 3.2 illustrate that GPT3 is\nhighly sensitive to uncertainty in prompts, and cer-\ntainty seems to lead to diminished accuracy. Here,\nwe extend these results to study uncertainty at a\nmore fine-grained level, allowing us to ask if the\ndegree of uncertainty could play a role in the accu-\nracy of model generation.\nIntroducing Numerical Values Here, we intro-\nduce numerical values into our verbal expressions\nof uncertainty. The setup of our task changes\nfrom “What is the capital of Belarus? I’m sure\nit’s. . . ” to “What is the capital of Belarus?I’m 90%\nsure it’s. . . ”. We use a set of seven expressions\ncovering a range of numerical uncertainty expres-\nsions, including those that use personal pronouns\nto weaken uncertainty (\"I’m 90% certain. . . \") and\nthose which indicate uncertainty probabilistically\nbut without mentioning the self ( \"70% chance\nit’s. . . \"). We also downsample our test set to 50\nquestions per dataset and evaluate each template at\n0%, 10%, 30%, 50%, 70%, 90% and 100% inter-\nvals.\n100% Certainty is not 100% Accurate\nOur findings for numerical uncertainties extend\nour earlier analysis by enabling us to obtain more\nfine-grained results on whether a model is linguis-\ntically calibrated, and how much certainty causes\nperformance degradation.\nFirst, we find that numerical uncertainties in the\nprompt are not well calibrated with the accuracy of\nthe answers generated.7 E.g., the prompt ”I’m 90%\nsure it’s. . . ”in TriviaQA only produces the correct\n7Note this is slightly different from calibration in prior\nwork which calibrates between the model’s confidence and\naccuracy.\nanswer 57% of the time (Figure 4). Formally, we\nevaluate the expected calibration error (ECE) and\nfind poor values ranging from 0.50 to 0.30, 0 being\nthe best.\nSecond, consistent with our findings from Sec-\ntion 4, we find that certainty does hurt model per-\nformance. However, we find this is true only at the\nextremes. We see performance on models peaks\nusually between 70% and 90% in numerical val-\nues but drops in accuracy when using 100% in the\nprompt. Across all four datasets, with seven tem-\nplates each, 21 out of the 28 templates which use\n100% numerical values had a lower probability-\non-gold than templates which used 90% numerical\nvalues (See Appendix). Additionally, at the other\nextreme, when 0% is used in templates, there is\nalso a drastic drop to accuracy.\n7 Why Does 100% Certainty Hurt?\nWe hypothesize this drop in accuracy might be the\nresult of the use of hyperbolic or exaggerated lan-\nguage in the training set, in which numbers are used\nnon-literally. When someone says “I’m 100% cer-\ntain there was pie left”, they don’t mean they are\n100% certain is there pie — but rather are emphasiz-\ning their strong belief there was pie. Again, we turn\nto the Pile and qualitatively analyze over 500 exam-\nples of use cases of \"100%\". We remove other uses\nof \"100%\" such as in code (e.g., \"width:100%\")\nand sample 50 instances from both question and\nanswer posts. We find, surprisingly, that 100% is\nin fact often used to express uncertainty.\nOf our 100 samples, 44 instances are used with\nnegation (e.g., \"never be 100% accurate\" or \"is not\nalways 100% reliable\") and half are expressions of\nuncertainty (e.g., \"I’m not 100% sure\" or \"I don’t\n100% follow\"). In questions, the rate of expressions\nof uncertainty was nearly double that of answers\n(14 vs 8). In contrast, only 3 instances in our sam-\nple indicate 100% certainty (e.g., \"I’m 100% sure\nthat\"). We hypothesize that both the use of nega-\ntion with \"100%\" and the general lack of use of\n\"100%\" with expressions of certainty contribute to\nthe lowered performance of these prompts.\nAnother confounder in how model’s interpret\nnumerical values could be the distribution of nu-\nmerical frequencies in typical model training data.\nQuerying the Pile, we find that there are drastic\nimbalances in the use of percentages in training\ndatasets. There are significant spikes in frequency\nat the upper extremes (50%, 95% and 100%) (Fig-\n5512\n0 20 40 60 80 100\nPercent\n0.875\n0.900\n0.925\n0.950\n0.975Accuracy\n0.0\n10.0 30.0\n50.0\n70.0 90.0\n100.0\nCountryQA\nMean\n0 20 40 60 80 100\nPercent\n0.45\n0.50\n0.55Accuracy\n0.0\n10.0\n30.0\n50.0 70.0 90.0\n100.0\nTriviaQA\nMean\n0 20 40 60 80 100\nPercent\n0.20\n0.22\n0.24\n0.26\n0.28Accuracy\n0.0\n10.0 30.0 50.0\n70.0\n90.0\n100.0\nNaturalQA\nMean\n0 20 40 60 80 100\nPercent\n0.300\n0.325\n0.350\n0.375\n0.400Accuracy\n0.0\n10.0\n30.0\n50.0 70.0\n90.0\n100.0\nJeopardy Questions\nMean\nFigure 4: The X-axis indicates the percentage that was injected into the verbal uncertainty. The Y-axis indicates the\naccuracy across numerical uncertainties. Note the consistent drop in accuracy between 90% and 100% uncertainty\nand the increase in accuracy between 0% and 10% uncertainty.\nure 5). This might be happening as some values are\nmore common to express (e.g., \"100% organic\")\nor found more often in code. Humans might also\nnaturally exaggerate values or use colloquial terms\nto describe confidence. The presence of scientific\nlanguage like \"95% confidence interval\" could be\nanother possible source of imbalance. Although\nspoken natural language also includes rounded per-\ncentages, the use of only textual data might be\nfurther exacerbating this bias.\nFigure 5: Visualization of the Frequency of percentages\nfound in the pile. Note the peaks at the extremes, (0, 50,\n10), and peaks at every 10 and 5 intervals from the first\nmillion samples queried from the Pile dataset using the\nHuggingFace API.\n8 Related Work\nWhile scholars have studied model uncertainty,\nprior work has focused on more accurately ex-\ntracting model confidence (Kuhn et al., 2023; Sun\net al., 2022; Gleave and Irving, 2022), measuring\n(Kwiatkowski et al., 2019b; Radford et al., 2019;\nLiang et al., 2023) and improving model calibration\n(between model confidence and accuracy) (Jiang\net al., 2021; Desai and Durrett, 2020; Jagannatha\nand Yu, 2020; Kamath et al., 2020; Kong et al.,\n2020). However, the community has found mixed\nresults on the calibration of neural model (Min-\nderer et al., 2021; Carrell et al., 2022); for ex-\nample, Desai and Durrett (2020) shows that pre-\ntrained transformers are relatively well-calibrated\nmeanwhile Wang et al. (2020) found severe mis-\ncalibration in neural machine translation. Another\nline of work also explore the trade-off between\nmodel performance and calibration (Stengel-Eskin\nand Van Durme, 2023).\nClosest to our work, Mielke et al. (2022) pro-\npose solutions to reducing model overconfidence\nthrough linguistic calibration, Kadavath et al.\n(2022) experiment with models’ ability emit self-\nconfidence after finding that models are relatively\nwell-calibrated, and Lin et al. (2022) teach mod-\nels to be linguistically calibrated when answering\nmath questions.\nIn addition, semanticists and computational lin-\nguists have long studied speaker commitment fac-\ntors such as factivity (Karttunen, 1971; Degen and\nTonhauser, 2022) and projection (Simons et al.,\n2010), and more recent work include corpora\nlike the CommitmentBank (De Marneffe et al.,\n2019) which offers naturally occurring examples,\nas well as new experimental paradigms to investi-\ngate speaker commitment (Degen et al., 2019). A\nwide variety of scholars have examined computa-\n5513\ntional issues in factuality, veridicality, and commit-\nment (Saurí and Pustejovsky, 2009; de Marneffe\net al., 2012; Stanovsky et al., 2017; Rudinger et al.,\n2018; Jiang and de Marneffe, 2021, inter alia) as\nwell as bias (Pryzant et al., 2020; Patel and Pavlick,\n2021) and specific devices like hedges (Prokofieva\nand Hirschberg, 2014; Raphalen et al., 2022), and\nmodality (Pyatkin et al., 2021).\nWe see our work as a bridge between these\ntwo areas of speaker commitment and natural lan-\nguage generation, by telling us how models inter-\npret speaker commitment through expressions of\ncertainty and uncertainty.\n9 Discussion and Conclusion\nHere, we discuss a number of recommendations\nand future work for the greater NLP community.\nCan Models Generate Expressions of Uncer-\ntainty? Having studied the impact of uncertainty\nas part of the prompt, a natural follow-up ques-\ntion is: how does model performance change when\nmodels learn to emit their own expressions of un-\ncertainty? In preliminary experiments, we find it\nis challenging to calibrate models to generate epis-\ntemic markers. The same methods which work\nfor numerical calibration do not transfer well to\nverbal calibration (Lin et al., 2022). However, we\nfind some evidence that expressions of uncertainty\nare slightly better calibrated when compared with\nexpressions of certainty. See A.1.\nNavigating Idiomatic Language Humans use\nlanguage that contains expressions of certainty\nwhen they are, in fact, not certain, and models ap-\npear to be mimicking this behavior. However, our\nQA models are still unable to recognize the mean-\ning of those markers in our QA setup, raising con-\ncerns for how they would respond in downstream\napplications. Future work must seek to enable LMs\nto accurately interpret when expressions are meant\nidiomatically versus literally.\nVerified Attribution We encourage the commu-\nnity to explore how to integrate attributions of\ninformation in a verified manner. Some of the\nbest-performing templates from Section 4 include\nphrases like \"Wikipedia says. . . \", however these\nwere falsely injected attributions. As we move to-\nwards generating expressions of uncertainty, it is\ncritical for researchers to be cautious when generat-\ning attributions at the risk of providing downstream\nusers with a false, yet highly believable attribution.\nIn this work, we analyzed how epistemic mark-\ners impact model behavior. We find a drop in ac-\ncuracy when naturalistic expressions of certainty\n(i.e., strengtheners and factive verbs) are present\nand trace this effect back to how expressions of\ncertainty and uncertainty are used in pretraining\ndatasets. As the community expands the capabili-\nties of LMs to general language that is more natu-\nral, it is critical we prepare for the opportunity and\nharms that may arise from naturalistic expressions\nof uncertainty and certainty.\nLimitations\nAs with many language model work, some key\nlimitations of our work include scaling to other\nmodels, increasing the variety of datasets, and ex-\nperimenting with multi-shot prompting. The work\nand results we presented are robustly tested but if\ngiven additional resources (both in compute and\ntime), the scalability of these results would be of\nvalue to the greater community.\nMulti-shot prompting would dramatically in-\ncrease the exploration space of epistemic markers,\nbut this is a highly realistic scenario that should\nbe explored in future work. Similarly, long-form\nand dialogue generation are both beyond the scope\nof this project but would further build our under-\nstanding how models interpret expressions of un-\ncertainty.\nExpressions of uncertainty also vary signifi-\ncantly across cultures and contexts and our study is\nlimited by only studying how hedges and boosters\nexist in the English language. Syntatic, idomatic,\nand pragamtic differences in hedges could be inter-\nesting to study in follow-up work.\nEthics Statement\nOur work complies with ACL’s ethical standard.\nWe hope our work inspires future researchers to\ncontinue to investigate epistemic markers in lan-\nguage modeling. However, we have two critical\nethical considerations when it comes future work\nregarding generating expressions of uncertainty.\nFirst, we are concerned about the use of un-\nverified attributions in language generation as the\nuse of evidentials could appear to be very con-\nvincing to downstream users meanwhile provid-\ning little guarantee to the accuracy of the state-\nments. Second, when it comes to generating epis-\ntemic markers, teaching models to only emit ex-\npressions of uncertainty when they are unsure,\n5514\nrather than when they are sure, could be a safer de-\nsign choice for human-computer interactions. Prior\nwork has shown cases where AI-assisted decision-\nmaking performed worse than human decision-\nmaking alone, suggesting an over-reliance on AI,\neven when systems are wrong (Jacobs et al., 2021;\nBussone et al., 2015). Teaching models to emit\nexpressions of certainty could further exacerbate\nthese challenges.\nAcknowledgements\nThank you so much to Tolúl o. pé. Ògúnrè.mí, Yi-\nwei Luo, Myra Cheng, Rishi Bommasani, and\nOmar Shaikh for their helpful feedback and advice!\nThank you to Center for Research on Foundation\nModels (CRFM) for their generous support, and\nto the National Science Foundation for funding\nvia award IIS-2128145. Tatsunori Hashimoto and\nDan Jurafsky were supported by a gift from Open\nPhilanthropy. Kaitlyn Zhou is supported by the\nStanford Graduate Fellowship.\nReferences\nAlexandra Y Aikhenvald. 2004. Evidentiality. OUP\nOxford.\nPenelope Brown and Stephen C Levinson. 1987. Polite-\nness: Some universals in language usage, volume 4.\nCambridge university press.\nAdrian Bussone, Simone Stumpf, and Dympna\nO’Sullivan. 2015. The role of explanations on trust\nand reliance in clinical decision support systems. In\n2015 International Conference on Healthcare Infor-\nmatics, pages 160–169.\nAnnabelle Carrell, Neil Mallinar, James Lucas, and\nPreetum Nakkiran. 2022. The calibration general-\nization gap. International Conference on Machine\nLearning (Workshop on Distribution-Free Uncer-\ntainty Quantification).\nMarie-Catherine de Marneffe, Christopher D. Manning,\nand Christopher Potts. 2012. Did it happen? the prag-\nmatic complexity of veridicality assessment. Compu-\ntational Linguistics, 38(2):301–333.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107–124.\nJudith Degen and Judith Tonhauser. 2022. Are there\nfactive predicates? an empirical investigation. Lan-\nguage, 98(3):552–591.\nJudith Degen, Andreas Trotzke, Gregory Scontras, Eva\nWittenberg, and Noah D Goodman. 2019. Definitely,\nmaybe: A new experimental paradigm for investi-\ngating the pragmatics of evidential devices across\nlanguages. Journal of Pragmatics, 140:33–48.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295–302, Online.\nAssociation for Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAdam Gleave and Geoffrey Irving. 2022. Uncer-\ntainty estimation for language reward models. arXiv\npreprint arXiv:2203.07472.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A Smith,\nand Luke Zettlemoyer. 2022. Demystifying prompts\nin language models via perplexity estimation. arXiv\npreprint arXiv:2212.04037.\nKen Hyland. 1998. Boosting, hedging and the negotia-\ntion of academic knowledge. Text & Talk, 18(3):349–\n382.\nKen Hyland. 2005. Stance and engagement: A model of\ninteraction in academic discourse. Discourse studies,\n7(2):173–192.\nKen Hyland. 2014. Disciplinary discourses: Writer\nstance in research articles. In Writing: Texts, pro-\ncesses and practices, pages 99–121. Routledge.\nMaia Jacobs, Melanie F Pradier, Thomas H McCoy Jr,\nRoy H Perlis, Finale Doshi-Velez, and Krzysztof Z\nGajos. 2021. How machine-learning recommenda-\ntions influence clinician treatment selections: the\nexample of antidepressant selection. Translational\npsychiatry, 11(1):108.\nAbhyuday Jagannatha and Hong Yu. 2020. Calibrat-\ning structured output predictors for natural language\nprocessing. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2078–2092, Online. Association for Computa-\ntional Linguistics.\nNanjiang Jiang and Marie-Catherine de Marneffe. 2021.\nHe thinks he knows better than the doctors: BERT\nfor event factuality fails on pragmatics. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1081–1097.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962–977.\n5515\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5684–\n5696, Online. Association for Computational Lin-\nguistics.\nLauri Karttunen. 1971. Some observations on factivity.\nResearch on Language & Social Interaction, 4(1):55–\n69.\nPaul Kiparsky and Carol Kiparsky. 1970. FACT, pages\n143–173. De Gruyter Mouton, Berlin, Boston.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model fine-tuning for in- and out-\nof-distribution data. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1326–1340, Online. As-\nsociation for Computational Linguistics.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nIn The Eleventh International Conference on Learn-\ning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019a. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–\n466.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019b. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nGeorge Lakoff. 1975. Hedges: A study in meaning\ncriteria and the logic of fuzzy concepts. In Contem-\nporary Research in Philosophical Logic and Linguis-\ntic Semantics: Proceedings of a Conference Held at\nthe University of Western Ontario, London, Canada,\npages 221–271. Springer.\nYoung-Jun Lee, Chae-Gyun Lim, Yunsu Choi, Ji-Hui\nLm, and Ho-Jin Choi. 2022. PERSONACHATGEN:\nGenerating personalized dialogues using GPT-3. In\nProceedings of the 1st Workshop on Customized Chat\nGrounding Persona and Knowledge, pages 29–48,\nGyeongju, Republic of Korea. Association for Com-\nputational Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R’e, Diana Acosta-Navas, Drew A.\nHudson, E. Zelikman, Esin Durmus, Faisal Ladhak,\nFrieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert\nYuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel\nGuha, Niladri S. Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2023. Holistic eval-\nuation of language models. Annals of the New York\nAcademy of Sciences, 1525:140 – 146.\nStephanie C. Lin, Jacob Hilton, and Owain Evans.\n2022. Teaching models to express their uncertainty\nin words. Trans. Mach. Learn. Res., 2022.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nSabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-\nLan Boureau. 2022. Reducing conversational agents’\noverconfidence through linguistic calibration. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:857–872.\nMatthias Minderer, Josip Djolonga, Rob Romijnders,\nFrances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin\nTran, and Mario Lucic. 2021. Revisiting the calibra-\ntion of modern neural networks. Advances in Neural\nInformation Processing Systems, 34:15682–15694.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. 2022. Social simulacra: Creating populated\nprototypes for social computing systems. In Proceed-\nings of the 35th Annual ACM Symposium on User\nInterface Software and Technology, pages 1–18.\nRoma Patel and Ellie Pavlick. 2021. “was it “stated”\nor was it “claimed”?: How linguistic bias affects\ngenerative language models. In Proceedings of the\n5516\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10080–10095, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nE. Prince, C. Bosk, and J. Frader. 1982. On hedging in\nphysician-physician discourse. Di Pietro, R.J., Ed.,\nLinguistics and the Professions, pages 83–97.\nAnna Prokofieva and Julia Hirschberg. 2014. Hedging\nand speaker commitment. In 5th Intl. Workshop on\nEmotion, Social Signals, Sentiment & Linked Open\nData, Reykjavik, Iceland.\nReid Pryzant, Richard Diehl Martinez, Nathan Dass,\nSadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.\nAutomatically neutralizing subjective bias in text. In\nProceedings of the AAAI conference on artificial in-\ntelligence, volume 34, pages 480–489.\nValentina Pyatkin, Shoval Sadde, Aynat Rubinstein,\nPaul Portner, and Reut Tsarfaty. 2021. The possi-\nble, the plausible, and the desirable: Event-based\nmodality detection for language processing. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 953–965.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nYann Raphalen, Chloé Clavel, and Justine Cassell. 2022.\n“You might think about slightly revising the title”:\nIdentifying hedges in peer-tutoring interactions. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2160–2174, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nRachel Rudinger, Aaron Steven White, and Benjamin\nVan Durme. 2018. Neural models of factuality. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 731–744, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nRoser Saurí and James Pustejovsky. 2009. Factbank:\na corpus annotated with event factuality. Language\nresources and evaluation, 43:227–268.\nMandy Simons, Judith Tonhauser, David Beaver, and\nCraige Roberts. 2010. What projects and why. In\nSemantics and linguistic theory , volume 20, pages\n309–327.\nGabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy\nPuzikov, Ido Dagan, and Iryna Gurevych. 2017. Inte-\ngrating deep linguistic features in factuality predic-\ntion over unified datasets. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 352–357,\nVancouver, Canada. Association for Computational\nLinguistics.\nElias Stengel-Eskin and Benjamin Van Durme. 2023.\nCalibrated Interpretation: Confidence Estimation in\nSemantic Parsing. Transactions of the Association\nfor Computational Linguistics, 11:1213–1231.\nMeiqi Sun, Wilson Yan, Pieter Abbeel, and Igor Mor-\ndatch. 2022. Quantifying uncertainty in foundation\nmodels via ensembles. In NeurIPS 2022 Workshop\non Robustness in Sequence Modeling.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2195–2222, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nShuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu.\n2020. On the inference calibration of neural machine\ntranslation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3070–3079, Online. Association for Computa-\ntional Linguistics.\nOrion Weller, Marc Marone, Nathaniel Weir, Dawn J.\nLawrie, Daniel Khashabi, and Benjamin Van Durme.\n2023. \"according to ...\" prompting language mod-\nels improves quoting from pre-training data. CoRR,\nabs/2305.13252.\n5517\nA Appendix\nAdditional Details\nA.1 When LMs Emit Their Own Uncertainty\nHere, we study how model performance changes\nbased on in-context learning examples. Specif-\nically, we follow Lin et al. (2022)’s method in\nfew-shot learning with 50 samples which has been\nshown to be nearly as effective as fine-tuning on\ndatasets that are magnitudes larger. To ensure that\nour in-context learning dataset covers a range of\nconfidence levels, our dataset contains 48 samples\nwhose probability-on-gold is uniformly distributed\n(in buckets of 10) between 0 and 100.\nA.1.1 Experiment Details\nTo study how LMs respond when emitting their\nown uncertainty, we follow Lin et al. (2022)’s setup\nbut modify it for the strengtheners and weakeners,\nwhich are inherently non-numerical (Figure 1). In\nour setting, instead of teaching a model to output\na percentage confidence, we teach it to output a\nstrengthener when the confidence is above a thresh-\nold and nothing otherwise.8 Conversely, when we\nstudy weakeners, we teach it to output a weakener\nwhen the probability is below a threshold and noth-\ning otherwise.\nAs an example, consider the question “What is\nthe capital of France”. We record the LM’s proba-\nbility over Paris (the probability-on-gold) and ap-\npend “I’m sure” to the in-context example if the\nmodel’s confidence was above 0.5. We repeat this\nfor all the in-context examples to obtain our in-\ncontext learning training set.\nA.1.2 Prompting Perturbations\nRecent work has shown the drastic differences that\nappear based on simple changes to prompting setup\n(Suzgun et al., 2022; Lu et al., 2022). We design\nour in-context learning samples with various per-\nturbations to ensure robustness in our results.\nWe select high performing weakeners and\nstrengtheners from Section 4 and experiment with\nappending expressions of uncertainty after the an-\nswer (e.g., “Paris. I think”) (Table 12).9 We then\nuse three different sample orderings to perturb our\nlearning samples: ascending and descending order\n8We choose to emit nothing rather than emit an expression\nof uncertainty, as we wish to isolate the effect of each linguistic\nexpression of uncertainty.\n9Here, we exclude expressions with attribution shields for\nconcerns of false attribution, more on this in the discussion.\nof probability-on-gold and random ordering.10 Fi-\nnally, we experiment with a variety of thresholds\n(0.3, 0.5, 0.7, 0.9) for determining when expres-\nsions of (un)certainty should be inserted into the\nexample. These perturbations are done on a small\nscale to help identify the best hyper-parameters\n(threshold, placement, and ordering) to use.\nWe find that varying the threshold across does\nnot drastically change accuracy (with all methods\nattaining ∼ 83% in accuracy), although the thresh-\nold does significantly impact the balance of the\ntraining datasets. Similarly, we find limited differ-\nences in the ordering of the samples. With these\nresults, we choose a threshold of 0.5 (creating a\nbalanced dataset) and random ordering (simplest\nsetting) as our hyper-parameters for our remain-\ning scenarios and analysis, which we test on 100\nTriviaQA questions.\nA.1.3 Results\nGains in Model Calibration When Learning Un-\ncertainty Overall, GPT3 has a limited ability to\nlearn naturalistic expressions of uncertainty in a\ncalibrated manner. We measured calibration based\non whether models successfully emit the expres-\nsions of uncertainty when the probability of the top\ntoken above or below our training threshold. In\nour setup, when learning to emit certainty, answers\nwith probability-on-gold of greater than 0.5 had\na strengthener and answers with less than 0.5 had\nnothing. Therefore, in its generation when the prob-\nability on the top token is greater than 0.5, we’d\nexpect the model to also generate a strengthener\nand vice versa for weakeners. We measure whether\nthe model successfully generates strengtheners and\nweakeners through the F1 score, and find that the\ntemplate with the highest macro-F1 score for un-\ncertainty templates to be 0.56 compared to 0.53 for\ncertainty templates.11 This is close to the random\nguessing baseline on this test set which results in\nan F1 score of 0.45.\nTo illustrate the difference in calibration between\nuncertainty and certainty, we can look at the aver-\nage accuracy when a model emits an expression or\nnot. When learning to express weakeners, the gen-\neration of a weakener results in an accuracy of 74%\nbut this increases to 83% when the model doesn’t\n10In the ascending and descending orders, all the samples\nin the beginning or all the samples at the end will include\nexpressions of (un)certainty.\n11Average F1 scores being .52 average for uncertainty and\n.49 average of certainty\n5518\nEntropy Emitting Not Emitting\nUncertainty 0.699* 0.522\nCertainty 0.461 0.617*\nControl N/A 0.541\nAccuracy Emitting Not Emitting\nUncertainty 0.738 0.829*\nCertainty 0.799 0.789\nControl N/A 0.78\nTable 3: Entropy is higher when uncertainty is being\nexpressed and also higher when certainty is not ex-\npressed. Accuracy is also higher when uncertainty is not\nexpressed but accuracy is not significant higher when\ncertainty is expressed (an indication or poor calibration).\n*Significantly higher value calculated using two-sample\nt-test, p< 0.05.\ngenerate a weakener. This is the intended behavior,\nwith the model hedging answers it is more likely to\nget incorrect. However, when teaching the model to\nemit strengtheners, the generation of strengtheners\ndoes not lead to a significant increase in accuracy\n(79% with or without an emission of strengtheners).\nThis means that when the model emits certainty,\nthe answer is not more likely to be correct, creating\na concerning issue for linguistic calibration (Table\n3).\nModeling Changes in Entropy Despite low\nmodel calibration when emitting expressions of cer-\ntainty and uncertainty, we find that the underlying\nentropy of the probability distribution of the gen-\nerated answer is well-calibrated to the expressions\nof uncertainty. Analyzing the top five predictions\nfor each token, we find that when teaching mod-\nels weakeners, the entropy of the distribution of\npotential generations is higher when a weakener is\nemitted and lower when it is not. The inverse is\ntrue when teaching models strengtheners, entropy\nis lower when strengtheners are emitted and higher\nwhen it is not. Although the calibration scores are\nnot strong for either uncertainty or certainty, we see\npromising behaviors in the entropy of the model’s\ntop generations. When emitting weakeners, the\nmodel places more consideration on alternative an-\nswers and less when emitting strengtheners.\nSensitivity to Placement of Template Finally,\nwe test how model performance differs based on\nthe placement of the templates. The simple design\ndifference of probing for the answer before (e.g.,\n“I think it’s Paris.”) or after (e.g., “Paris. I think.”)\nan expression of uncertainty can have a significant\ndifference in performance. In our tables, we re-\nfer to these places as prefixes (before) and suffixes\n(after). We find that when appending expressions\nof uncertainty as a prefix, the generation is signif-\nicantly worse for accuracy (63% vs 80%). This\nis also correlated with probability-on-gold being\nlower in prefixed templates (40% vs 67%). An\nexplanation for this might be that the probability\nof generating the correct answer will be lower if\ngenerated after a phrase like “I think it’s. . . ” rather\nthan just generated immediately after the question.\nOur work suggests that ordering effects may be\nimportant when addressing accuracy-calibration\ntrade-offs in LMs and that there are accuracy gains\nwhen prompting the model to respond with answers\nas soon as possible.\nTemplate Certainty Prob Top 1\nPrefix Uncertain 0.388 0.592\nCertain 0.407 0.674\nSuffix Uncertain 0.674 0.800\nCertain 0.673 0.792\nControl N/A 0.673 0.780\nTable 4: Average probability on generated token and top\n1 accuracy across prefix and suffix templates.\nA.2 Perplexity vs Accuracy\n101 102\nPerplexity\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Accuracy on TriviaQA\ndavinci\nFigure 6: Correlation between the perplexity (GPT3\ndavinci) and the accuracy of an expression of uncertainty\non questions from TriviaQA (Pearson’s ρ= -0.03).\n5519\nFigure 7: Screenshot of the Crowdsourced Example\nA.3 Performance of Additional Linguistic\nFeatures\nCountryQA Jeopardy NaturalQA TriviaQA\nDatasets\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nAccuracy by Plausibility Shield\nPlausibility\nNot Plausibility\nCountryQA Jeopardy NaturalQA TriviaQA\nDatasets\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nAccuracy by Source\nSource\nNo Source\nCountryQA Jeopardy NaturalQA TriviaQA\nDatasets\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nAccuracy by Personal Pronouns\nPersonal Pronoun\nNot Personal Pronoun\nFigure 8: The use of plausibility shields, sources, and\npersonal pronouns are mixed, without significant con-\nsistent improvements or drops in accuracy. 95% CI\ncalculated using bootstrap resampling.\nA.4 Additional Tables and Results\n5520\nExpressions uncertainty # instances\n# per\nthousand\nposts\n# per\nmillion\nwords\n# instances\n# per\nthousand\nposts\n# per\nmillion\nwords\nQuestions Answers\ni think hedge 1,106,442 37.5 162.2 1,536,543 52.0 302.7\nit could be hedge 84,239 2.9 12.3 143,670 4.1 28.3\nit might be hedge 70,606 2.4 10.3 170,803 4.9 33.6\nmaybe it’s hedge 21,803 0.7 3.2 17,233 0.5 3.4\nit should be hedge 233,686 7.9 34.3 346,290 10.0 68.2\nTotal 1,516,776 51.4 222.3 2,214,539 63.9 436.2\ni know booster 1,672,756 56.6 245.2 350,241 10.1 69.0\ni’m certain booster 5,975 0.2 0.9 2,758 0.1 0.5\ni am certain booster 4,638 0.1 0.7 1,607 0.0 0.3\ni’m sure booster 119,224 4.0 17.5 76,009 2.2 15.0\ni am sure booster 52,089 1.8 7.6 22,983 0.7 4.5\nit must be booster 52,976 1.8 7.8 72,724 2.1 14.3\nevidently it’s booster 33 0.0 0.0 52 0.0 0.0\nTotal 1,907,691 64.58 279.6 526,374 15.2 103.7\nTable 5: Counts of expressions of certainty and uncertainty in the Stack Exchange section of The Pile.\n0 20 40 60 80 100\nPercent\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Accuracy\n0.0\n10.0\n30.0\n50.0\n70.0\n90.0\n100.0\n0.0 10.0\n30.0\n50.0 70.0 90.0 100.0\n0.0\n10.0 30.0 50.0 70.0 90.0\n100.0\n0.0\n10.0 30.0\n50.0\n70.0 90.0\n100.00.0\n10.0\n30.0\n50.0\n70.0 90.0\n100.0\n0.0 10.0 30.0\n50.0\n70.0 90.0\n100.0\n0.0\n10.0 30.0 50.0 70.0 90.0\n100.0\nCountryQA\n 0% chance it's\n 0% probability it's\n I am 0% certain it's\n I am 0% confident it's\n I am 0% convinced it's\n I am 0% sure it's\n With 0% confidence it's\n0 20 40 60 80 100\nPercent\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65Accuracy\n0.0\n10.0\n30.0 50.0\n70.0\n90.0\n100.0\n0.0\n10.0\n30.0\n50.0 70.0\n90.0\n100.0\n0.0\n10.0\n30.0\n50.0\n70.0\n90.0\n100.0\n0.0\n10.0 30.0 50.0\n70.0\n90.0\n100.0\n0.0\n10.0\n30.0\n50.0\n70.0\n90.0\n100.0\n0.0\n10.0\n30.0\n50.0 70.0\n90.0\n100.0\n0.0\n10.0\n30.0\n50.0\n70.0\n90.0\n100.0\nTriviaQA\n 0% chance it's\n 0% probability it's\n I am 0% certain it's\n I am 0% confident it's\n I am 0% convinced it's\n I am 0% sure it's\n With 0% confidence it's\n0 20 40 60 80 100\nPercent\n0.10\n0.15\n0.20\n0.25\n0.30Accuracy\n0.0\n10.0\n30.0\n50.0\n70.0 90.0\n100.0\n0.0\n10.0\n30.0\n50.0\n70.0 90.0\n100.0\n0.0\n10.0 30.0\n50.0 70.0 90.0\n100.0\n0.0 10.0\n30.0\n50.0\n70.0\n90.0\n100.00.0\n10.0\n30.0\n50.0\n70.0 90.0\n100.0\n0.0\n10.0 30.0\n50.0\n70.0\n90.0\n100.0\n0.0\n10.0 30.0 50.0\n70.0\n90.0 100.0\nNaturalQA\n 0% chance it's\n 0% probability it's\n I am 0% certain it's\n I am 0% confident it's\n I am 0% convinced it's\n I am 0% sure it's\n With 0% confidence it's\n0 20 40 60 80 100\nPercent\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45Accuracy 0.0\n10.0\n30.0\n50.0\n70.0\n90.0\n100.0\n0.0\n10.0 30.0\n50.0 70.0\n90.0\n100.00.0\n10.0\n30.0\n50.0\n70.0\n90.0\n100.0\n0.0\n10.0\n30.0 50.0\n70.0\n90.0\n100.0\n0.0\n10.0\n30.0 50.0 70.0 90.0\n100.0\n0.0\n10.0\n30.0 50.0\n70.0\n90.0\n100.0\n0.0\n10.0\n30.0\n50.0\n70.0\n90.0\n100.0\nJeopardy Questions\n 0% chance it's\n 0% probability it's\n I am 0% certain it's\n I am 0% confident it's\n I am 0% convinced it's\n I am 0% sure it's\n With 0% confidence it's\nFigure 9: Variation in probability-on-gold across numerical uncertainties. Note the consistent drop in accuracy\nbetween 90% and 100% uncertainty and the increase in accuracy between 0% and 10% uncertainty.\n5521\nTemplate Strengtheners Shield Evidential Marker Factive Verb Source 1P\nApparently it’s Weakener None Evidential Not Factive No Source No\nRumor says it it’s Weakener None Evidential Not Factive No Source No\nAllegedly it’s Weakener None Evidential Not Factive No Source No\nI was told it’s Weakener None Evidential Not Factive No Source Yes\nI’ve heard it’s Weakener None Evidential Not Factive No Source Yes\nThey told me it’s Weakener None Evidential Not Factive No Source Yes\nWikipedia suggests it’s Weakener None Evidential Not Factive Source No\nOnline says it’s Weakener None Evidential Not Factive Source No\nThe internet says it’s Weakener None Evidential Not Factive Source No\nWikipedia claims it’s Weakener None Evidential Not Factive Source No\nWikipedia says it’s Weakener None Evidential Not Factive Source No\nI read on the internet it’s Weakener None Evidential Not Factive Source Yes\nI read on Wikipedia it’s Weakener None Evidential Not Factive Source Yes\nI read online it’s Weakener None Evidential Not Factive Source Yes\nPresumably it’s Weakener None Not Evidential Not Factive No Source No\nTo the best of\nmy knowledge it’s Weakener Plausibility Evidential Not Factive No Source Yes\nAs far as I’m aware it’s Weakener Plausibility Evidential Not Factive No Source Yes\nI vaguely remember it’s Weakener Plausibility Evidential Not Factive No Source Yes\nIt could be Weakener Plausibility Not Evidential Not Factive No Source No\nConsidering all\nthe options it’s Weakener Plausibility Not Evidential Not Factive No Source No\nIt probably is Weakener Plausibility Not Evidential Not Factive No Source No\nMaybe it’s Weakener Plausibility Not Evidential Not Factive No Source No\nPerhaps it’s Weakener Plausibility Not Evidential Not Factive No Source No\nIt should be Weakener Plausibility Not Evidential Not Factive No Source No\nI don’t know maybe it’s Weakener Plausibility Not Evidential Not Factive No Source Yes\nI suppose it’s Weakener Plausibility Not Evidential Not Factive No Source Yes\nI would need to\ndouble check but maybe it’s Weakener Plausibility Not Evidential Not Factive No Source Yes\nI wouldn’t put\nmoney on it but maybe it’s Weakener Plausibility Not Evidential Not Factive No Source Yes\nI’m not an expert but maybe it’s Weakener Plausibility Not Evidential Not Factive No Source Yes\nI think it’s Weakener Plausibility Not Evidential Not Factive No Source Yes\nI feel like it should be Weakener Plausibility Not Evidential Not Factive No Source Yes\nIt is known that it’s Strengthener None Evidential Factive No Source No\nThe most recent\nevidence shows it’s Strengthener None Evidential Factive Source No\nThe rules state it’s Strengthener None Evidential Factive Source No\nTwo recent studies\ndemonstrate it’s Strengthener None Evidential Factive Source No\nWikipedia\nacknowledges it’s Strengthener None Evidential Factive Source No\nWikipedia confirms it’s Strengthener None Evidential Factive Source No\nOur lab has shown it’s Strengthener None Evidential Factive Source Yes\nEvidently it’s Strengthener None Evidential Not Factive No Source No\nAccording to the\nlatest research it’s Strengthener None Evidential Not Factive Source No\nWe can see in the\ntextbook that it’s Strengthener None Evidential Not Factive Source Yes\nIt must be Strengthener None Not Evidential Factive No Source No\nWe realize it’s Strengthener None Not Evidential Factive No Source Yes\nWe understand it’s Strengthener None Not Evidential Factive No Source Yes\nWe know it’s Strengthener None Not Evidential Factive No Source Yes\nUndoubtedly it’s Strengthener None Not Evidential Not Factive No Source No\nWith 100% confidence it’s Strengthener None Not Evidential Not Factive No Source No\nI’m certain it’s Strengthener None Not Evidential Not Factive No Source Yes\nI am 100% sure it’s Strengthener None Not Evidential Not Factive No Source Yes\nIt’s None None Not Evidential Not Factive No Source No\nTable 6: Full list of expressions of uncertainty coded for six linguistic features. *Claims is a neg-factive but in our\nschema, will just be considered not a factive verb. (Saurí and Pustejovsky, 2009)\n5522\nTemplate Type Top 1 Accuracy\n0 Online says it’s Weakener, Evidential,Source 0.660\n1 Standard Method - 0.625\n2 Wikipedia confirms it’s Strengthener, Evidential, Factive, Source 0.600\n3 Wikipedia suggests it’s Weakener, Evidential, Source 0.595\n4 The internet says it’s Weakener, Evidential, Source 0.585\n5 Wikipedia claims it’s Weakener, Evidential, Source 0.575\n6 Wikipedia says it’s Weakener, Evidential, Source 0.575\n7 We can see in the textbook that it’s Strengthener, Evidential, Source, 1P 0.565\n8 I would need to double check\nbut maybe it’s Weakener, Plausibility, 1P 0.555\n9 Rumor says it it’s Weakener, Evidential 0.550\nTable 7: Top 10 Templates For TriviaQA for GPT3 - Davinci\nTemplate Type Top 1 Accuracy\n0 Standard Method - 1.0\n1 I read on Wikipedia it’s Weakener, Evidential, Source, 1P 1.0\n2 It’s - 1.0\n3 It should be Weakener, Plausibility 1.0\n4 Allegedly it’s Weakener, Evidential, 1.0\n5 I’m not an expert but maybe it’s Weakener, Plausibility, 1P 1.0\n6 I wouldn’t put money on it but maybe it’s Weakener, Plausibility, 1P 1.0\n7 Presumably it’s Weakener 1.0\n8 I read online it’s Weakener, Evidential, Source, 1P 1.0\n9 I read on the internet it’s Weakener, Evidential, Source, 1P 1.0\nTable 8: Top 10 Templates For CountryQA for GPT3 - Davinci\nTemplate Type Top 1 Accuracy\n0 Standard Method - 0.450\n1 It must be Strengthener, Factive 0.390\n2 It’s - 0.380\n3 It could be Weakener, Plausibility 0.370\n4 The internet says it’s Weakener, Evidential, Source 0.370\n5 Online says it’s Weakener, Evidential, Source 0.360\n6 With 100% confidence it’s Strengthener 0.350\n7 Undoubtedly it’s Strengthener 0.345\n8 Wikipedia says it’s Weakener, Evidential, Source 0.345\n9 Wikipedia confirms it’s Strengthener, Evidential, Factive, Source 0.325\nTable 9: Top 10 Templates for Jeopardy for GPT3 - Davinci\n5523\nTemplate Type Top 1 Accuracy\n0 Wikipedia claims it’s Weakener, Evidential, Source 0.340\n1 Wikipedia says it’s Weakener, Evidential, Source 0.335\n2 Online says it’s Weakener, Evidential, Source 0.310\n3 Wikipedia suggests it’s Weakener, Evidential, Source 0.305\n4 The internet says it’s Weakener, Evidential, Source 0.300\n5 Wikipedia confirms it’s Strengthener, Evidential, Factive, Source 0.300\n6 I read on Wikipedia it’s Weakener, Evidential, Source, 1P 0.295\n7 Presumably it’s Weakener 0.275\n8 Standard Method - 0.275\n9 I think it’s Weakener, Plausibility, 1P 0.270\nTable 10: Top 10 Templates for NaturalQA for GPT3 - Davinci\nTemplate Type Top 1 Accuracy\n0 The internet says it’s Weakener, Evidential, Source 0.416\n1 Wikipedia says it’s Weakener, Evidential, Source 0.408\n2 Online says it’s Weakener, Evidential, Source 0.405\n3 Wikipedia suggests it’s Weakener, Evidential, Source 0.404\n4 Wikipedia claims it’s Weakener, Evidential, Source 0.400\n5 Wikipedia confirms it’s Strengthener, Evidential, Factive, Source 0.397\n6 I would need to double check\nbut maybe it’s Weakener, Plausibility, 1P 0.387\n7 I’m not an expert but maybe it’s Weakener, Plausibility, 1P 0.387\n8 I am 100% sure it’s Strengthener, 1P 0.385\n9 We can see in the textbook that it’s Strengthener, Evidential, Source, 1P 0.382\nTable 11: Top 10 Templates Across All GPT Models and All Datasets\nExpression Suffix Prefix\nCertainty Undoubtedly. Undoubtedly it’s\nCertainty With 100% confidence. With 100% confidence it’s\nCertainty We know it. We know it’s\nCertainty Evidently. Evidently it’s\nCertainty It must be. It must be\nUncertainty I think. I think it’s\nUncertainty It could be. It could be\nUncertainty But I would need to double check. I would need to double check but maybe it’s\nUncertainty I suppose. I suppose it’s\nUncertainty But I wouldn’t put money on it. I wouldn’t put money on it but maybe it’s\nTable 12: List of Templates Used for Section A.1\n5524",
  "topic": "Certainty",
  "concepts": [
    {
      "name": "Certainty",
      "score": 0.9453537464141846
    },
    {
      "name": "Affect (linguistics)",
      "score": 0.814777672290802
    },
    {
      "name": "Overconfidence effect",
      "score": 0.7642226219177246
    },
    {
      "name": "Epistemology",
      "score": 0.4720039367675781
    },
    {
      "name": "Psychology",
      "score": 0.4468301236629486
    },
    {
      "name": "Computer science",
      "score": 0.439348429441452
    },
    {
      "name": "Epistemic modality",
      "score": 0.41847342252731323
    },
    {
      "name": "Typology",
      "score": 0.4125264585018158
    },
    {
      "name": "Linguistics",
      "score": 0.35927897691726685
    },
    {
      "name": "Social psychology",
      "score": 0.3139002025127411
    },
    {
      "name": "Philosophy",
      "score": 0.17484503984451294
    },
    {
      "name": "Sociology",
      "score": 0.15004962682724
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 25
}