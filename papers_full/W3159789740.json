{
  "title": "DECIMER 1.0: deep learning for chemical image recognition using transformers",
  "url": "https://openalex.org/W3159789740",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3092156181",
      "name": "Kohulan Rajan",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    },
    {
      "id": "https://openalex.org/A210963128",
      "name": "Achim Zielesny",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2029328209",
      "name": "Christoph Steinbeck",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    },
    {
      "id": "https://openalex.org/A3092156181",
      "name": "Kohulan Rajan",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    },
    {
      "id": "https://openalex.org/A210963128",
      "name": "Achim Zielesny",
      "affiliations": [
        "GeoInformation (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2029328209",
      "name": "Christoph Steinbeck",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2031942128",
    "https://openalex.org/W2172216479",
    "https://openalex.org/W2416943854",
    "https://openalex.org/W2523785361",
    "https://openalex.org/W2610394652",
    "https://openalex.org/W2145870108",
    "https://openalex.org/W3013318641",
    "https://openalex.org/W3092396754",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W1508604947",
    "https://openalex.org/W1966456689",
    "https://openalex.org/W2963734039",
    "https://openalex.org/W3007750971",
    "https://openalex.org/W4232500930",
    "https://openalex.org/W3204820152",
    "https://openalex.org/W3085821739",
    "https://openalex.org/W3097598035",
    "https://openalex.org/W3130247384",
    "https://openalex.org/W2766447205",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W3094640617",
    "https://openalex.org/W2114704115",
    "https://openalex.org/W2148797284",
    "https://openalex.org/W2895677720",
    "https://openalex.org/W3009321976",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W3127736057",
    "https://openalex.org/W3045928028",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W3133325765",
    "https://openalex.org/W3138074128",
    "https://openalex.org/W2042126044",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2136623753",
    "https://openalex.org/W3103383152",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3203895579",
    "https://openalex.org/W1522301498"
  ],
  "abstract": null,
  "full_text": "Rajan et al. J Cheminform _##################### _\nhttps://doi.org/10.1186/s13321-021-00538-8\nRESEARCH ARTICLE\nDECIMER 1.0: deep learning for chemical \nimage recognition using transformers\nKohulan Rajan1, Achim Zielesny2 and Christoph Steinbeck1*  \nAbstract \nThe amount of data available on chemical structures and their properties has increased steadily over the past dec-\nades. In particular, articles published before the mid-1990 are available only in printed or scanned form. The extraction \nand storage of data from those articles in a publicly accessible database are desirable, but doing this manually is a \nslow and error-prone process. In order to extract chemical structure depictions and convert them into a computer-\nreadable format, Optical Chemical Structure Recognition (OCSR) tools were developed where the best perform-\ning OCSR tools are mostly rule-based. The DECIMER (Deep lEarning for Chemical ImagE Recognition) project was \nlaunched to address the OCSR problem with the latest computational intelligence methods to provide an automated \nopen-source software solution. Various current deep learning approaches were explored to seek a best-fitting solution \nto the problem. In a preliminary communication, we outlined the prospect of being able to predict SMILES encodings \nof chemical structure depictions with about 90% accuracy using a dataset of 50–100 million molecules. In this article, \nthe new DECIMER model is presented, a transformer-based network, which can predict SMILES with above 96% accu-\nracy from depictions of chemical structures without stereochemical information and above 89% accuracy for depic-\ntions with stereochemical information.\nKeywords: Chemical data extraction, Deep learning, Neural networks, Optical chemical structure recognition\n© The Author(s) 2021. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nScientists build on the results of their peers. Knowledge \nand data arising from previous research is shared through \nscientific publications and increasingly through the dep -\nosition of data in repositories. To enable progress in core \nareas of chemistry, the availability of open data has a ben-\neficial impact [1]. Most of the chemical data is published \nin the form of text and images in scientific publications \n[2]. Retrieving and storing published information into \nopen-access databases will facilitate the reuse as well as \nthe development of new methods and products [3]. But \nmost of the data published is non-machine readable and \nmanual curation is still the standard. This manual work is \ntedious and error-prone [4]. The increase of publications \nwith valuable chemical information [5] does encourage \nthe development of tools for automated data retrieval. \nInformation retrieval with corresponding database stor -\nage is an ongoing task and multiple projects are working \ntowards this. The CHEMDNER [6] challenge is one good \nexample of it.\nThere has been a significant amount of development \nin the field of chemical data mining [5] with a couple of \nopen source solutions including ChemDataExtracter [4] \nand ChemSchematicResolver (CSR) [7], building upon \neach other. A scanned page of an article, however, can -\nnot be handled by CSR, and not all publications can be \nprocessed by CSR. Although most publishers offer docu -\nments in markup format, many of the older publications \nare stored in scanned PDF files. For example, the Journal \nof Natural Products did publish scientific articles since \n1978, one of their issues even dates back to 1949; how -\never, these publications were not formatted in markup \nOpen Access\nJournal of Cheminformatics\n*Correspondence:  christoph.steinbeck@uni-jena.de\n1 Institute for Inorganic and Analytical Chemistry, Friedrich-Schiller-\nUniversity Jena, Lessingstr. 8, 07743 Jena, Germany\nFull list of author information is available at the end of the article\nPage 2 of 16Rajan et al. J Cheminform _##################### _\nformat. So retrieving this information is a difficult \nprocess.\nImage mining methods for chemical structure depic -\ntions and their conversion into a machine-readable file \nformat is a comparatively small research area [8]. The \nautomatic recognition of chemical structure depictions \nand their conversion into machine-readable formats such \nas SMILES [9] or InChI [10], however, is an important \ntask for creating corresponding databases. The publica -\ntions include chemical structure depictions along with \nother information in textual format and contain some \ninformation presented as tables, graphs, spectra, etc.\nOptical Chemical Structure Recognition (OCSR) soft -\nware was built to parse chemical structure depictions. \nHowever, most of these softwares/tools are unable to \nhandle whole page articles or scanned ones. In order to \nuse these tools, it is necessary to segment the chemical \nstructure depictions into separate images from printed \nliterature and then use these segmented images as inputs. \nAlso, the user should ensure that the image does not con-\ntain any other elements or artefacts other than a repre -\nsentation of a chemical structure in a segmented image. \nAll of the available systems vary in their accuracy, OSRA \n[11] and MolVec [11, 12] can resolve a chemical structure \nwith 80–90% accuracy [8].\nWith the advancements in computer vision, a few deep \nlearning-based OCSR tools have been developed, e.g. by \nStaker et  al. [13], the first machine learning-based sys -\ntem for segmentation of images and resolution into a \ncomputer-readable format. Another deep learning-based \nwork is Chemgrapher [14], where multiple neural net -\nworks are combined for the recognition of molecules. \nRecently, there was a new publication called ChemPix \n[15], a deep learning-based method that was developed \nto recognize hand drawn hydrocarbon chemical struc -\ntures. Another recent publication describes SMILES \ngeneration from images [16] where an encoder–decoder \nmethod with a pre-trained decoder is used from previous \nwork [17]. These contributions demonstrate an increas -\ning interest in this field of research. Even though they \nall claim to provide enhanced accuracy, none of them is \naccessible to the general public to date.\nThe DECIMER (Deep lEarning for Chemical IMagE \nRecognition) project [18] is an end-to-end open-source \nsystem that can perform chemical structure segmenta -\ntion on scanned scientific literature and use the seg -\nmented structure depictions to convert them into a \ncomputer-readable molecular file format.\nIn our work on DECIMER-Segmentation [19], the seg -\nmentation workflow was specifically addressed. Here we \nnow present a transformer-based algorithm that converts \nthe bitmap of a chemical structure depiction into a com -\nputer-readable format. The system does not inherit any \nrules or make any assumptions, thus, it solely relies on \nthe chemical structure depiction to perform its task.\nThe DECIMER algorithm was primarily inspired by \nthe successful AlphaGo Zero algorithm [20] developed \nby Google’s DeepMind. The success of AlphaGo Zero \nallowed us to realize that very challenging problems \ncould be adequately tackled by having a sufficient amount \nof data and using an adequate neural network architec -\nture. With dozens of millions of molecules available in \nthe databases like PubChem [21], Zinc20 [22], and GDB-\n17 [23], we have shown in our preliminary communica -\ntion that our goal to have a system that can work with \nabout 90% accuracy, could be achieved by training the \nnetwork on a dataset of 50–100 million molecules.\nMaterials and methods\nDECIMER is a completely data-driven solution to chemi-\ncal image recognition. Recent impressive applications \nof deep learning, such as the AlphaGo Zero example, \nall relied on the availability of very large to unlimited \namounts of training data. In our case, one of the largest \nchemical databases on the planet, PubChem [21], was \nused.\nData preparation\nThe latest version of Pubchem was downloaded from \ntheir FTP site. All explicit hydrogens were removed using \nthe CDK [24] and isomeric SMILES [9] were generated, \nwhich inherit the canonicalisation and retain the stereo -\nchemistry information. After generating the SMILES, the \nfollowing set of rules were used to filter the dataset for a \nbalanced dataset. The molecules in both training and test \nset should,\n• have a molecular weight of fewer than 1500 Daltons,\n• not possess counter ions,\n• only contain the elements C, H, O, N, P , S, F, Cl, Br, I, \nSe and B,\n• not contain isotopes of Hydrogens (D, T),\n• have 3–40 bonds,\n• not contain any charged groups including zwitteri -\nonic forms,\n• only contain implicit hydrogens, except in functional \ngroups,\n• have less than 40 SMILES characters,\n• no stereochemistry is allowed.\nThe resulting main dataset contains 39  million mol -\necules. The same rule set was used to generate a second \ndataset, but the molecules with charged groups including \nzwitterionic forms and stereochemistry were retained. \nFurthermore, the molecules containing tokens that were \nrare in the dataset were removed (see “ Tokenization” \nPage 3 of 16\nRajan et al. J Cheminform _##################### _\nsection), resulting in a dataset that contains approxi -\nmately 37  million molecules. Adding extra information \ncaused the SMILES character length to get longer. Later, \nwhen the rule that SMILES length should not exceed 40 \ncharacters was applied, more molecules were removed. \nIn the end, this resulted in  dataset 2 being smaller in size \nthan  dataset 1.\nMolecular bitmap images were generated using the \nCDK Structure Diagram Generator (SDG). The CDK \ndepiction generator enables the generation of produc -\ntion-quality 2D images. In this work, every molecule was \nrandomly rotated and depicted as 8 Bit PNG images with \na 299 × 299 resolution. It was made sure that each image \ncontains only one structure.\nUsing the set of images from the second dataset and \nintroducing image augmentations the third dataset was \ngenerated. The image augmentations were applied using \nthe imgaug [25] python package. One of the following \naugmentations was randomly applied to the images.\n• Gaussian Blur\n• Average Blur\n• Additive Gaussian Noise\n• Salt and Pepper\n• Salt\n• Pepper\n• Coarse Dropout\n• Gamma Contrast\n• Sharpen\n• Enhance Brightness\nOften, deep learning in chemistry is using SMILES \nas a textual representation of structures. Training Neu -\nral Networks (NNs) directly with SMILES, however, \nhas pitfalls: In order to generate tokens, a set of rules \nhas to be set up on how and where to split long strings \nof SMILES into smaller words. After training, inva -\nlid SMILES are often encountered in the predictions, \nwhich results in overall significantly reduced accuracy. \nTo tackle this problem there are two new text repre -\nsentations named DeepSMILES [26] and SELFIES [27]. \nDeepSMILES exhibited better results in comparison \nto standard SMILES, but again invalid DeepSMILES \ncaused similar problems. In the end, SELFIES were \nused, since they can be split easily into tokens by split -\nting the SELFIE at close (“]”) and open brackets (“[”). \nNo further rules had to be applied to split them into a \nworking token set (Fig.  1). Also, they translate back into \na SMILES string without any errors. All SMILES strings \nin our 3 datasets were converted into SELFIES using \nPython.\nTo summarize, the datasets used in this work are:\n1. Dataset 1: PNG images of chemical structure depic -\ntions plus corresponding canonical SMILES con -\nverted into SELFIES, without stereochemical infor -\nmation and charged groups.\n2. Dataset 2: PNG images of chemical structure depic -\ntions plus corresponding canonical SMILES con -\nverted into SELFIES, with stereochemical informa -\ntion and charged groups.\n3. Dataset 3: Augmented PNG images of chemical \nstructure depictions plus corresponding canonical \nSMILES converted into SELFIES, with stereochemi -\ncal information and charged groups.\nTest datasets were selected from 10% of each dataset. \nTo ensure that the chemical diversity of test and train -\ning data was similar, 10% of SMILES were selected as \nTest dataset using the RDKIT MaxMin algorithm. An \noverview of all the train and test datasets and the nam -\ning of subsets can be found in Table 1 .\nFig. 1 SMILES, DeepSMILES and SELFIES represented as tokens with a space character as a delimiter\nPage 4 of 16Rajan et al. J Cheminform _##################### _\nImage feature extraction\nA Convolutional Neural Network (CNN) is used to parse \nthe images, where the second last layer retains the fea -\ntures to be extracted for calculations. For training our \nmodel, we evaluated InceptionV3 [28] and EfficientNet-\nB3 [29], see Fig.  2. The EfficientNet base model for an \nimage size of 299 × 299 outperforms InceptionV3 in our \ntask at hand [29].\nEvery image was scaled to a resolution of 299 × 299 \npixels and the pixel values were normalized to inter -\nval − 1 to 1, which corresponds to the format used on \nInceptionV3 and EfficientNet-B3. Then the image fea -\ntures were extracted into a vector format using the pre-\ntrained weights of ImageNet [30] on InceptionV3 and \npre-trained weights of Noisy-student [31] training on \nEfficientNet-B3.\nFor Inception V3 a feature vector size of 8 × 8 × 2048 \nand for EfficientNet-B3 a feature vector size of \n10 × 10 × 1536 was obtained. The 8 × 8 × 2048 and \n10 × 10 × 1536 dimensions are simply the shape of the \nlast output layer in Inception-V3 and EfficientNet-B3 \nnetworks. Since Inception-V3 and EfficientNet-B3 are \nnetworks built for image classification, these features are \nthen used in the final layer of these networks to classify \nthe images.\nNext, these extracted feature vectors were saved into \nNumPy arrays.\nTokenization\nSELFIES were tokenized into a unique set of tokens and \npadded to fit the maximum length of SELFIES strings. \nHere the Keras [32] tokenizer in Tensorflow 2.3 [33] was \nused. Table 2 summarizes the details regarding the tokens \npresent in each dataset.\nTokens in Dataset 1: [C], [=C], [Branch1_1], [N], \n[Ring1], [O], [Branch1_2], [Expl=Ring1], [=N], \n[Branch2_1], [Branch1_3], [Ring2], [S], [F], [=O], \nTable 1 Overview of the datasets\nDataset 1 Dataset 2 Dataset 3\nTotal dataset size 39 million 37 million 37 million\nSubset 1 Subset 2 Subset 3 Subset 4 Subset 5 Subset 6 Non \naugmented \ntest set\nAugmented test set\nTrain dataset size 921,600 10,240,000 15,360,000 35,002,240 15,360,000 33,304,320 33,304,320 33,304,320\nTest dataset size 102,400 1,024,000 1,536,000 3,929,093 1,536,000 3,700,480 2,000,000 2,000,000\nFig. 2 Schema of the encoder–decoder network used in DECIMER for training\nPage 5 of 16\nRajan et al. J Cheminform _##################### _\n[Branch2_2], [Cl], [Branch2_3], [#C], [Br], [P], [=S], [I], \n[=P], [Expl=Ring2], [B], [#N].\nTokens in Dataset 2 & 3: [C], [=C], [Branch1_1], \n[Branch1_2], [Ring1], [N], [O], [=O], [=N], [Ring2], \n[Branch2_1], [S], [Branch1_3], [F], [Branch2_2], [Cl], \n[Branch2_3], [Br], [#C], [/C], [#N], [P], [C@Hexpl], \n[C@@Hexpl], [=N+expl], [=S], [=N-expl], [I], [O-expl], \n[N+expl], [\\C], [/N], [/O], [C@expl], [B], [C@@expl], \n[\\N], [Expl/Ring1], [\\O], [NH+expl], [I-expl], [Expl\\\nRing1], [P+expl], [NH2+expl], [/Cl], [/S], [NH3+expl], \n[Cl-expl], [/F], [#N+expl], [C-expl], [\\S], [N-expl], \n[=NH+expl], [=I], [S-expl], [\\Cl], [S+expl], [#C-expl], \n[B-expl], [/Br].\nComplete list of rare tokens which were removed: [=B], \n[=Cl], [=Br], [#I], [=I], [#S], [Expl#Ring1], [#B], [#P], \n[=Br], [Expl#Ring2].\nGenerating TFRecords\nExtracted feature vectors and tokenized data must be \nconverted into TFRecords before training the models on \nTensor Processing Units (TPU) [34]. TFRecords stores \nthe data in binary format which allows training the mod -\nels faster using GPUs and TPUs. The TPUs are currently \navailable through the Google Cloud Platform. TFRecords \nare stored in a Google cloud bucket for training. This \nreduces the training time significantly and reduces the \noverhead of loading the data and performing the calcula -\ntions on a TPU.\nUsing a custom python script all the datasets were con-\nverted into 75 MB chunks of TFRecords. Each TFRecord \ncontains 128 Datapoints (128 image vectors + 128 \ntokenized strings).\nAfter generating the TFRecords locally, they were \nmoved to a Google cloud storage bucket.\nNetworks\nIn this work, two different types of networks were evalu -\nated. Initially, an encoder–decoder model was tested, \nwhich is based on the work by Google on their Show, \nAttend and Tell [35] publication. The network eventu -\nally selected is a transformer-based model based on the \nAttention is all you need [36] publication by Google. The \nmodels are written using Python and Tensorflow 2.3 as a \nbackend.\nEncoder–decoder network\nThe encoder–decoder network used is an unaltered \nimplementation by the TensorFlow team [37]. The model \nuses a CNN-based encoder with a ReLU activation func -\ntion, a soft attention mechanism introduced by Bahdanau \net al. [38] and the RNN based decoder uses Gated Recur -\nrent Units (GRU) and two fully connected layers. The \ndecoder consists of 1024 units and an embedding size of \n512.\nThe network is trained using an Adam optimizer [39] \nwith a learning rate of 0.0005 throughout all learning \nepochs. The loss is calculated using sparse categorical \ncross-entropy between real and predicted SELFIES.\nTransformer network\nThe transformer model (Fig.  3) used in this work is the \nmodel from the 2017 publication Attention is all you \nneed. It uses four encoder–decoder layers and eight \nparallel attention heads. The attention has a dimen -\nsion size of 512 and the feed-forward networks have a \ndimension size of 2048. Here the number of rows and \ncolumns corresponds to our image features extracted \ninto a vector format, so for the InceptionV3, the feature \nvector size is 8 × 8 × 2048 and for the EfficientNet-B3 \nit is 10 × 10 × 1536. A drop out of 0.1 is used to avoid \noverfitting.\nThe network is trained using an Adam optimizer with \na custom learning rate scheduler according to [36]. The \nloss is calculated using sparse categorical cross-entropy \nbetween real and predicted SELFIES.\nTraining the models\nInitially, all the models were trained using an in-house \nserver equipped with an Nvidia V100 Tesla with 32  GB \nGPU, 384  GB of RAM and two Intel(R) Xeon(R) Gold \n6230 CPUs. The details regarding the scaling and perfor -\nmance were explained in our previous publication [18]. \nFor this work, a model with a dataset of 1  million mol -\necules is initially trained using the same GPU equipped \nserver. A batch size of 512 images is used to train the \nmodel, resulting in an epoch time of 29 min and 48 s, on \naverage. For a complete convergence of the model, it took \nabout 1 day, 5 h and 48 min on the hardware mentioned \nabove.\nOn a TPU v3-8 (TPU version 3 with 8 nodes) the same \nmodel was trained with a batch size of 1024 which is dis -\ntributed between 8 nodes, and it took on average 8 min \nand 41 s per epoch and for a complete convergence of the \nmodel, it took 8 h 41 min and 4 s. This is a reduction of \n71.9% in computation time and we, therefore, decided to \nTable 2 SELFIES tokens and maximum length found on each \ndataset\nDatasets Number of SELFIES \ntokens\nThe maximum length \nof the SELFIES strings\nDataset 1 27 47\nDataset 2 61 47\nDataset 3 61 47\nPage 6 of 16Rajan et al. J Cheminform _##################### _\ntrain all models with the TensorFlow distributed training \nAPI using the Tensor Processing Units v3-8.\nTesting the models\nAll the models were trained until their training loss con -\nverged, then each model was tested with a test data size \nof 10% of the training data. Throughout the process of \nselecting molecules for the test dataset, the RDKit [40] \nMaxMin algorithm is used to select a diverse test dataset \ncovering the same chemical space as the training dataset.\nTest dataset evaluations were performed on the GPUs. \nPredicted SELFIES were decoded back to SMILES and \nthen the Tanimoto Similarity Index was calculated for \nthe original and predicted SMILES using PubChem fin -\ngerprints, included in the CDK. The Tanimoto Similar -\nity Index provides useful information because it makes \na difference whether a wrong prediction is completely \nwrong or provides a result very similar to the correct \nmolecule was used because the predictions do not \nalways correspond to the same molecule. The Tanimoto \nsimilarity thereby provides a quantitative measure of \nhow well the network is able to “understand” graphical \nchemical structure representations.\nFig. 3 Schema of the transformer network used in DECIMER for training\nPage 7 of 16\nRajan et al. J Cheminform _##################### _\nApart from that for the predictions with the Tanimoto \nsimilarity index of 1.0, we additionally generated InChIs \nusing the CDK to perform an isomorphism check and \ndetermined whether Tanimoto 1.0 predictions are a good \nproxy for structure identity.\nModels trained with augmentations were tested \nwith augmented images and with images without any \naugmentation.\nResults and discussion\nComputational considerations\nTraining large datasets such as the ones used here on \ndeep neural networks take months even on GPUs, let \nalone regular CPUs. For performance measure, a dataset \nwith 1  million molecules was trained for 50 epochs on \nan Nvidia Tesla V100 GPU and the same model was also \ntrained on a TPU V3-8 (version 3 TPU with 8 nodes) and \nTPU V3-32 (version 3 TPU with 32 nodes).\nTraining a model on a V3-8 TPU helped by increasing \ntraining speed up to 4 times compared to a V100 GPU \nand by using a V3-32 TPU a 16 times faster training \nspeed was achieved, see Fig.  4. Concerning these results \nand considering the costs of V3-32 TPUs, it was decided \nto train all the models on a V3-8 TPU.\nTo evaluate if testing accuracy could be improved by \nincreasing the training dataset size, different subsets gen -\nerated using dataset 1 were trained on TPU V3-8. The \nmaximum length of SELFIES strings stayed the same \nthroughout the training. As shown in Fig. 5, training time \nincreases with the increase in datasets.\nIt would take a considerable amount of time to exam -\nine the performance of the network using a bigger data -\nset. For the initial tests, Subset 1: a subset of 1 million \nwas used, which was derived from Dataset 1. We split \nthe dataset into 90% training data (921,600) and 10% test \ndata (102,400) using the RDKIT MaxMin algorithm to \nensure that the test data picked are diverse and resemble \nthe training dataset.\nImage feature extraction test\nCorrect extraction of the image features will result in \nan improved overall model at the end. In our previous \nwork, the InceptionV3 model was used for image feature \nextraction. InceptionV3 is a state-of-the-art image clas -\nsification network. A newer network, called EfficientNet, \nwas created to enable better classification accuracy, and \nthe results of noisy-student training using EfficientNet \n[31] were better than the InceptionV3 network. The Effi -\ncientNet-B3 model was then compared to InceptionV3 \nwhile still using the same image size (299 × 299) to test \nwhether EfficientNet-based image feature extraction \nwould improve our models’ accuracy.\nTo compare the InceptionV3 feature extraction with \nEfficientNet-B3 feature extraction a subset of 1 million \nmolecules was used. Using these models, the features \nwere extracted and then used to train encoder–decoder \nbased networks for 60 epochs until the training loss con -\nverged. The training time for the network that uses the \nfeatures extracted using the InceptionV3 model was \nfound to be shorter than the network which uses the Effi-\ncientNet-B3 model.\nAfter training, the models were tested with a test \ndataset. The predicted SELFIES were retranslated into \nSMILES strings and the Tanimoto similarity index was \nFig. 4 Training time comparison between a GPU and TPUs (lower is \nbetter)\nFig. 5 Average training time per epoch with increasing training \ndataset size\nPage 8 of 16Rajan et al. J Cheminform _##################### _\ncalculated between original SMILES and the retranslated \nSMILES. Here, no errors have occurred in translating \nSELFIES to SMILES. Table 3 summarizes the evaluation.\nThe Tanimoto 1.0 count indicated that the Efficient -\nNet-B3 model led to a remarkable overall performance \nincrease, so it was used for the entire work.\nEncoder–decoder model vs. transformer model\nIn our previous work [18], the encoder–decoder network \nwas extensively explored. Meanwhile, great progress was \nmade in transformer-based networks and the results \nseemed promising, so we decided to implement a trans -\nformer-based network in this work as well.\nFirst, the transformer network was tested with Incep -\ntionV3 based image feature extraction, then it was tested \nusing the EfficientNet-B3 based image feature extraction. \nThe extracted image features with tokenized SELFIES \nwere used as inputs for the transformer. For this work, \nthe same 1  million molecules subset was used with a \n90:10 split for training and testing.\nThe models were trained on TPU V3-8 until the train -\ning loss converged. The average time for transformer-\nbased models was higher than the other, and the highest \naverage training time was recorded for the EfficientNet-\nB3 Transformer network. Once the training was com -\npleted, the models were tested using the same test set. \nTable 4 summarizes the final evaluation.\nBy comparing the Tanimoto 1.0 count, the transformer-\nbased models clearly outperformed the encoder–decoder \nbased models.\nWith these results, it was decided to train all the other \ndatasets using transformers with image features extracted \nusing EfficientNet-B3 based image feature extraction.\nImage feature extraction comparison using EfficientNet‑B3 \nand B7\nThe work described in [29] indicated that EfficientNet-\nB7 outperforms EfficientNet-B3 marginally by 2.7%. \nWe, therefore, implemented EfficientNet-B7 image fea -\nture extraction and training on the extracted features. \nThe number of parameters to train using EfficientNet-\nB7 (66  million parameters) compared to B3 (12  million \nparameters) is almost 5.5 times larger, however, which \nmakes the network rather big and complex. Furthermore, \nimages had to be rescaled to 600 × 600 for B7, in which \nthe chemical structure depictions had to be magnified \ntwice the normal scale. For B3, it is easy to use the images \nwith a scale of 299 × 299 without any alterations.\nTo test these two image feature extraction methods and \nto see how well this helps us to achieve our main goal, a \n1 million molecules image subset was used to train the \ntransformer networks and the final models were evalu -\nated using respective Images generated using the same \ntest set. Table 5 summarizes the results.\nIt is evident that the Image feature extraction using \nEfficientNet-B7 outperforms B3. We found, however, \nthat most of the chemical structure depictions found on \nprinted literature can easily fit the scale of 299 × 299, so \nto use the 600 × 600 scale the images should be upscaled. \nUpscaling will result in losing information which will \nbe a major downside for this approach since the models \nmajorly rely on the image features.\nChemical structure depictions larger than 299 × 299 \nsquare pixels can be downscaled easily to be used in our \nmodels without losing any pixel information. Thus the \nTable 3 1 million molecules model testing results for comparing \nInceptionV3 and EfficientNet-B3 feature extraction\nMetrics InceptionV3 EfficientNet‑B3\nAverage training time per \nepoch\n7 min 34 s 8 min 57 s\nTanimoto 0.5459 0.6345\nTanimoto 1.0 1.41% 7.03%\nTable 4 Comparing the encoder–decoder- and transformer-based approach with a 1 million images test dataset\nMetrics Encoder–decoder Transformer\nInceptionV3 EfficientNet‑B3 InceptionV3 EfficientNet‑B3\nAverage training time per epoch 7 min 34 s 8 min 57 s 8 min 33 s 9 min 27 s\nTanimoto 0.5459 0.6345 0.8764 0.9371\nTanimoto 1.0 1.41% 7.03% 55.29% 74.57%\nTable 5 Comparison of evaluation of using EfficientNet-B3 and \nB7 for image feature extraction\nMetrics EfficientNet‑B3 EfficientNet‑B7\nTrain data size 921,600 921,600\nTest data size 102,400 102,400\nTrain data size 0.46 TB 2.8 TB\nAverage training time 9 min 27 s 11 min 42 s\nTanimoto 0.9371 0.9669\nTanimoto 1.0 74.57% 84.82%\nPage 9 of 16\nRajan et al. J Cheminform _##################### _\nsize of the image was decided to be 299 × 299 and the \nfeature extraction was performed using EfficientNet-B3.\nIt may be possible in the future to use EfficientNet-B7 \nto extract image features for chemical image depictions \nwith higher resolutions.\nThe performance measure with increasing dataset size\nTo evaluate how the split percentage of training and \ntest data affected the training efficiency, we use a small \ntoy dataset (subset 1). The data was split into different \nsizes (see Table  6) of train and test sets using the RDKit \nMaxMin algorithm, and then each model was trained \nseparately and evaluated. Table 6 summarizes the results.\nFigure  6 shows that model performance increases \nwith training dataset size. The test data performance \nincreases up to a 70:30 split and then drops slightly, \nfor which we have no explanation. Since we assumed \nthat for our much larger final training data (30 Mio) \nit would be beneficial for the network to see as much \ntraining data as possible, we used a 90:10 split for our \nfinal training. To see how well the transformer per -\nforms with an increased number of data another sub -\nset of 10  million molecules images which was derived \nfrom the Dataset 1 (Subset 2) was utilized. The image \nfeatures were extracted using the InceptionV3 based \nnetwork and the EfficientNet-B3 based network. Every \nTable 6 Results of training the subset 1 with different train and test dataset sizes\nNo. Train data size Test data size Split Average time per \nepoch\nAverage Tanimoto Tanimoto 1.0 (%)\n1 102,400 921,600 10|90 42.22 0.86 45.05\n2 204,800 819,200 20|80 69.95 0.91 63.59\n3 307,200 716,800 30|70 199.52 0.93 71.63\n4 409,600 614,400 40|60 276.09 0.94 73.93\n5 512,000 512,000 50|50 320.25 0.95 77.37\n6 614,400 409,600 60|40 392.51 0.96 84.50\n7 716,800 307,200 70|30 448.91 0.97 85.38\n8 819,200 204,800 80|20 535.57 0.96 82.89\n9 921,600 102,400 90|10 560.47 0.94 75.06\nFig. 6 Average Tanimoto similarity indices and Tanimoto similarity 1.0 count with dataset number\nPage 10 of 16Rajan et al. J Cheminform _##################### _\ndataset was converted into TFRecords and moved to \nthe Google cloud. Two different models based on these \ntwo different image feature extractions were trained. \nAfter the model completed the training they were \ntested using a test dataset size of 1 million molecule \nimages of chemical structure depictions. Table  7 sum -\nmarizes the results.\nLooking at the Tanimoto similarity average and the \nTanimoto 1.0 count one can see that the dataset trained \nwith the EfficientNet-B3 based image feature extraction \nmethod outperforms the InceptionV3 based method. \nThis also was evident in the previous training with 1 \nmillion molecule images. With these results, the next \nset of training included only the EfficientNet-B3 based \nimage feature extraction.\nA total of four subsets were now extracted from Data -\nset 1, the train and test datasets were created using the \nRDKit MaxMin algorithm. All four datasets included \nthe same number of tokens. All four datasets were con -\nverted into TFRecords and stored on Google Cloud \nStorage Buckets and used to train the models. Table  8 \nsummarizes the overall results for different subsets.\nThese results demonstrate an increasing trend of \naccurate predictions due to increasing data in the train -\ning datasets. In addition, with 35  million molecules \ntraining, we reached an average Tanimoto similarity of \n0.99, along with a 96.47% Tanimoto 1.0 count. Because \nof using SELFIES as the input textual data, all of the \npredictions were successfully retranslated into valid \nmolecules. An isomorphism check using InChIs was \ncarried out in order to find out how many molecules in \nTanimoto 1.0 are fully isomorphic.\nInChI strings were generated using the CDK for all \nthe predictions with a Tanimoto similarity index of 1.0 \nand then checked whether they are isomorphic or not \nby string matching.\nTable 9 shows that 99% of all predictions which have \nTanimoto 1.0 are structurally identical to the depicted \nmolecule. Also with the increasing Training dataset \nsize, the isomorphic structure count kept increasing \nslightly.\nAnalysis of the predictions with low Tanimoto similarity \nindices\nThe model trained with subset 4 was able to extract \nmachine-readable representations of molecules \ndepicted in the test dataset with near 100% accuracy. \nIn order to understand why predictions with low Tani -\nmoto scores were not predicted correctly, the following \nanalysis was performed (Table 10; Fig. 7 ).\nIn most cases the network was able to interpret the \nskeleton of the chemical structure well. Semantically \nsmall errors such as the miss of a ring closure will lead \nto seemingly large errors in the eyes of a chemist, as \ncan be seen in case 3(Fig 7 ).\nIn the majority of cases, the Tanimoto similarity was \nlow due to the predicted SMILES,\n• having one or more wrong atoms.\n• missing a bond.\n• having a wrong bond.\n• missing an aromatic ring.\nA strategy to overcome such issues could be to use \nmultiple depictions of the same chemical structure \nin the training set with different rotations so that the \nnetwork sees more examples of the same set of input \ndata. Also implementing different and more image aug -\nmentation methods and training the augmented images \nalong with the non augmented images might enable the \nnetwork to see the chemical structures clearer.\nTable 7 Testing results of the models trained on 10 million \nmolecule images of chemical structure depictions\nMetrics InceptionV3 EfficientNet‑B3\nTrain data size 10,240,000 10,240,000\nTest data size 1,024,000 1,024,000\nTanimoto 0.9310 0.9695\nTanimoto 1.0 74.52% 87.85%\nTable 8 Test data results for subsets\nMetrics Subset 1 Subset 2 Subset 3 Subset 4\nTrain data size 921,600 10,240,000 15,360,000 35,002,240\nTest data size 102,400 1,024,000 1,536,000 3,929,093\nTanimoto 0.9371 0.9691 0.9779 0.9923\nTanimoto 1.0 74.57% 87.88% 91.02% 96.47%\nTable 9 Results of isomorphism calculations for the subsets of \ndataset 1\nMetrics Subset 1 Subset 2 Subset 3 Subset 4\nTrain data size 921,600 10,240,000 15,360,000 35,002,240\nTest data size 102,400 1,024,000 1,536,000 3,929,093\nPredictions with Tani-\nmoto 1.0\n74,176 899,941 1,398,028 3,790,273\nIsomorphic predictions 98.63% 99.45% 99.59% 99.75%\nNon-isomorphic predic-\ntions\n1.37% 0.55% 0.41% 0.25%\nPage 11 of 16\nRajan et al. J Cheminform _##################### _\nPerformance of the network with training data using \nstereochemistry information—Dataset 2\nTo assess the performance of the transformer network on \nchemical structure depictions with stereochemistry and \nions, the same dataset was used but stereochemistry and \nion information were included. By including this infor -\nmation the unique number of tokens increased, and the \nmolecules with the least number of tokens were removed \nafter the calculation of the token distribution. A new \ndataset with 37 Mio molecules was created and split into \ntraining and test datasets using the RDKit MaxMin algo -\nrithm. This whole dataset is called Dataset-2 from now \non.\nBy adding stereochemical information and ions, the \nnumber of unique SELFIES tokens increased from 27 \nto 61, almost twice the number of the tokens found on \nDataset 1. From Table  11 one could see the same mole -\ncule with and without stereochemistry and how it affects \nthe number of tokens present in the SELFIES and the \ndepicted structure.\nInclusion of stereochemistry increased the amount of \ntokens, but also introduced new artifacts in chemical \nstructure depictions such as wedged and dashed bonds. \nIncluding the cis/trans information reduced the amount \nof curly bonds in the new dataset.\nIncluding the information about the ions also increased \nthe number of tokens, also this introduced new artifacts \nto the chemical structure depictions such as the “+, −” \nsigns and arrows, see Fig. 8.\nTwo subsets of Data Set 2 were generated, one with the \n15  million training molecules plus 1.5  million test mol -\necules and another with 33  million training molecules \nplus 3.7  million test molecules. TFRecords were gener -\nated from the chemical structure depictions using these \ndatasets and moved into Google cloud storage buckets. \nFinally, two models were trained using these two data -\nsets. Table 12 summarizes the results.\nIt can be seen from the results shown in Table  12 that \nthe average Tanimoto is lower compared to dataset 1 \nthat was seen in Table  8. The Tanimoto 1.0 count is also \nlower. This is mainly due to the new artefacts included \nin the new dataset and now the number of tokens in use \nalso doubled. Increasing the data for the newly intro -\nduced tokens can improve the results significantly. To \ncheck how many of the predicted structures are isomor -\nphic the InChIs were generated for the original and pre -\ndicted structures and a string matching was performed as \nexplained before, see Table 13.\nTable 13 shows that more than 96% of the predicted \nSMILES are isomorphic for a training data set of 15 \nMio compounds. By approximately doubling the train -\ning dataset size, the number of isomorphic structures \nTable 10 Predicted SMILES with lower Tanimoto similarity indices compared with the original SMILES\nNo. Original SMILES Predicted SMILES Tanimoto \nsimilarity \nindex\n1 P#CP=PP=PP=PP=PP=PP=PP=PP=PP=PP=PP=P N#CC=NSSSSSSSSSSSSSSSSC=N 0\n2 N1=NOO1 C=1=NOC1 0.14\n3 OC1OC(C=2C(F)=C(F)C(F)=C(F)C21)C)C O=C(OCOCCC(F)=C(F)C(F)=C(F)C)NC 0.35\n4 OCC(C)(CO)C12CCC(C1)C3SSSC32 OCC(C)(CO)C1C=2SSSC2CCC1C 0.59\n5 O=C1N=CC2=CC(=O)C=CC2=N1 O=C1N=CC2=NC(=O)C=CC2=N1 0.81\nFig. 7 Depictions of chemical structures with lower Tanimoto \nsimilarity indices\nPage 12 of 16Rajan et al. J Cheminform _##################### _\nincreased to over 98%, which is similar to the results for \ndataset 1. We also analysed the non-isomorphic predic -\ntions to see why they had a Tanimoto similarity of 1.0. \nFigure  9 demonstrates that the non-isomorphic struc -\ntures are identical to the original structures, except for \nan incorrectly predicted stereochemistry. This can very \nTable 11 Analysis of a molecule for with and without stereochemical information\nMolecules with stereochemical information Molecules without stereochemical information\n01 SMILES (canonical/isomric) C1=CC2=C(C=C1C=O)C(C(O2)Br)Br C1=CC2=C(C=C1C=O)[C@@H]([C@H](O2)Br)Br\nSELFIES [C][=C][C][=C][Branch1_1][Branch1_3][C][=C][Ring1]\n[Branch1_2][C][=O][C][Branch1_1][Branch2_1][C]\n[Branch1_1][Ring2][O][Ring1][Branch2_2][Br][Br]\n[C][=C][C][=C][Branch1_1][Branch1_3][C][=C]\n[Ring1][Branch1_2][C][=O][C@@Hexpl][Branch1_1]\n[Branch2_1][C@Hexpl][Branch1_1][Ring2][O][Ring1]\n[Branch2_2][Br][Br]\nNumber of unique SELFIES \ntokens\n12 14\nDepicted structure\n \n  \n02 SMILES (canonical/isomeric) CC1C(=C(N(N1)C)OC2CCC=CC2)C=NO CC1C(=C(N(N1)C)OC2CCC=CC2)/C=N/O\nSELFIES [C][C][C][Branch2_2][Ring1][Ring2][=C][Branch1_1]\n[Branch2_1][N][Branch1_1][Ring2][N][Ring1]\n[Branch1_1][C][O][C][C][C][C][=C][C][Ring1]\n[Branch1_2][C][=N][O]\n[C][C][C][Branch2_2][Ring1][Ring2][=C][Branch1_1]\n[Branch2_1][N][Branch1_1][Ring2][N][Ring1]\n[Branch1_1][C][O][C][C][C][C][=C][C][Ring1]\n[Branch1_2][/C][=N][/O]\nNumber of unique SELFIES \ntokens\n11 13\nDepicted Structure\n \n  \nFig. 8 Chemical structure depictions with ions\nTable 12 Results on the subsets of dataset 2\nMetrics Subset 5 Subset 6\nTrain data size 15,360,000 33,304,320\nTest data size 1,536,000 3,700,480\nTanimoto 0.9372 0.9761\nTanimoto 1.0 75.23% 89.87%\nPage 13 of 16\nRajan et al. J Cheminform _##################### _\nlikely be improved by training the network with more \nmolecules with stereochemistry.\nIncreasing the training data points will likely increase \nisomorphic structure predictions in general. Due to the \napplied ruleset, only a limited amount of data is availa -\nble to work with. Therefore, the next step will be to train \nthese models on augmented images to assess whether or \nnot they improve overall accuracy.\nPerformance of the network with training data using \nstereo‑chemistry and image augmentation—Dataset 3\nBy applying image augmentation to dataset 2 we gen -\nerated dataset 3. The resulting images look similar to \nFig. 10.\nThe parameters were restricted to reflect the real world \nimages, not to add extreme augmentations. The param -\neter is shown in Table 14 during augmentations. Here the \nlist of parameters provided is the ones that were imple -\nmented to augment the images, for more details about \nthe parameters and how they are implemented, we refer \nour readers to the imgaug documentation [41].\nThe generated dataset was then used to train two \nmodels. One model was trained from scratch using aug -\nmented images. Another model previously trained on \nDataset 2 was used as the pre-trained model and then \nrefitted with the augmented images (see Table  15). Both \nof them were tested on a dataset size of 4 million images, \nwhich includes 2 million images with augmentations and \n2  million images without any augmentations. Table  15 \nsummarizes the results.\nThe first two columns of the table explain the perfor -\nmance of the model trained only on augmented images \nand tested on augmented and non-augmented images. \nThe last two columns summarize the evaluation of the \nmodel which was previously trained on non-augmented \nimages and refitted with dataset 2.\nIn refitting, we used weights from the best model \npreviously trained on non-augmented images instead \nof random weights as a starting point for training. \nThis was done to see whether using the weights from \nTable 13 Results of isomorphism calculations for the subsets of \ndataset 2\nMetrics Subset 5 Subset 6\nTrain data size 15,360,000 33,304,320\nTest data size 1,536,000 3,700,480\nPredictions with Tanimoto 1.0 1,155,483 3,325,656\nIsomorphic predictions 96.42% 98.50%\nNon isomorphic predictions 3.58% 1.50%\nFig. 9 Non-isomorphic structures with Tanimoto similarity index of 1.0. The constitution is the same but predicted stereo-chemistry differs\nPage 14 of 16Rajan et al. J Cheminform _##################### _\na previously trained model would improve the perfor -\nmance of the newly trained model trained using a simi -\nlar type of data.\nThe above results clearly show that our models were \nable to retain the Tanimoto average of above 0.95 \nand Tanimoto 1.0 of above 80%. Also, the isomorphic \nresults are high in all cases, and this was similar to the \nearlier results. The overall accuracy of these models \ncould be improved by increasing the number of Aug -\nmented and Non-Augmented training images.\nVery likely, training with more data will improve the \noutcome.\nConclusion and future work\nIn our preliminary communication [18], we claimed that \nwith data around 50–100  million molecule images will \nhelp us obtain a model that can predict SMILES with \nabout 90% accuracy. Here, we have now presented a solu-\ntion based on a transformer network that delivers this \npromise.\nUsing the improved EfficientNet-B3 method rather \nthan Inception-V3 for image feature extraction helped \nin extracting relevant features required for network \ntraining. Through the implementation of the new trans -\nformer-based models, we’ve been able to improve the \naccuracy of our Image-to-SMILES models overall.\nWe have achieved an accuracy level of about 96% for \nchemical structure depictions using DECIMER’s new \nalgorithm without stereochemistry training the network \nusing 30–35 million molecules.\nWhen the models were extended to include stereo -\nchemical information and ions, a near 90% accuracy \nFig. 10 Images augmented with parameters within a given range\nTable 14 Image augmentations and their parameters\nImage augmentations Parameters (imgaug)\nGaussian blur 0–1.8\nAverage blur k = 0–3\nAdditive Gaussian noise Scale = (0, 0.1 * 255)\nSalt and pepper 0–0.05\nSalt 0–0.05\nPepper 0–0.05\nCoarse dropout 0–0.01, size percent = 0.9\nGamma contrast 0.5–2.0\nSharpen Alpha = (0.0, 1.0), lightness = 1.0\nEnhance brightness Factor = (0.95, 1.5)\nPage 15 of 16\nRajan et al. J Cheminform _##################### _\nwas achieved, despite increasing the number of tokens \ntwofold. This can be further improved by increasing \nthe data on stereochemical information and ions. This \nalso applies to the models trained using image augmen -\ntations. In order to improve these models, more data \nshould be incorporated into training.\nWith TPUs, the models could be trained within days, \nand the largest model took less than 14  days to train. \nThat means even bigger models could be trained within \na month using TPUs rather than training on GPUs, \nwhich may take several months to complete. It is also \ncost-effective as well as energy-efficient to implement \nthe TPU solution on the Google cloud platform rather \nthan relying on the local hardware setup.\nOur results showed that DECIMER was achieving \nthe intended objective with synthetic data. Further \nsteps in future will include training with more data, \nrefining models using a variety of real-world examples \nand image datasets with more augmentations. Addi -\ntionally, training images created by using a variety of \ntools will contribute to the model’s improved accuracy. \nUltimately, the DECIMER project aims to provide an \nopen-source tool that is capable of performing optical \nchemical structure recognition (OCSR) reliably on seg -\nmented images from the scanned literature.\nThe DECIMER software is fully open-source and \nhosted on GitHub. All data and trained models are \nopenly available.\nAbbreviations\nAPI: Application Programming Interface; CDK: Chemistry Development Kit; \nCNN: Convolutional Neural Network; CSR: ChemSchematicResolver; FTP: File \nTransfer Protocol; GB: GigaByte; GPU: Graphical Processing Unit; GRU : Gated \nRecurrent Unit; InChI: International Chemical Identifier; DECIMER: Deep lEarn-\ning for Chemical ImagE Recognition; MB: MegaByte; NNs: Neural Networks; \nOCSR: Optical Chemical Structure Recognition; OSRA: Optical Structure \nRecognition Application; PDF: Portable Document Format; PNG: Portable \nNetwork Graphics; RNN: Recurrent Neural Network; SDG: Structure Diagram \nGenerator; SELFIES: Self-referencing embedded strings; SMILES: Simplified \nMolecular-Input Line-Entry System; TFRecord: TensorFlow Record; TPU: Tensor \nProcessing Unit.\nAcknowledgements\nWe are grateful for the company Google making free computing time on their \nTensorFlow Research Cloud infrastructure available to us. We thank Dr. Charles \nTapley Hoyt for his valuable advice and help in improving the DECIMER \nGitHub repository.\nAuthors’ contributions\nKR developed the software and performed the data analysis. CS and AZ \nconceived the project and supervised the work. All authors contributed to the \nmanuscript. All authors read and approved the final manuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL. The authors \nacknowledge funding by the Carl-Zeiss-Foundation. Open Access funding \nenabled and organized by Projekt DEAL.\nAvailability of data and materials\nThe code for DECIMER and the trained models are available at https:// github. \ncom/ Kohul an/ DECIM ER- TPU, https:// doi. org/ 10. 5281/ zenodo. 47305 15. The \ndata is available as SMILES at: https:// doi. org/ 10. 5281/ zenodo. 47662 51.\nDeclarations\nCompeting interests\nAZ is co-founder of GNWI—Gesellschaft für naturwissenschaftliche Informatik \nmbH, Dortmund, Germany.\nAuthor details\n1 Institute for Inorganic and Analytical Chemistry, Friedrich-Schiller-Uni-\nversity Jena, Lessingstr. 8, 07743 Jena, Germany. 2 Institute for Bioinformat-\nics and Chemoinformatics, Westphalian University of Applied Sciences, \nAugust-Schmidt-Ring 10, 45665 Recklinghausen, Germany. \nReceived: 29 April 2021   Accepted: 25 July 2021\nReferences\n 1. Gaulton A, Overington JP (2010) Role of open chemical data in aiding \ndrug discovery and design. Future Med Chem 2:903–907 [cito:cites]\n 2. O’Boyle NM, Guha R, Willighagen EL et al (2011) Open data, open source \nand open standards in chemistry: the blue obelisk five years on. J Chem-\ninform 3:1–15 [cito:cites] [cito:agreesWith]\n 3. Zhou JZ (2011) Chemoinformatics and library design. In: Zhou JZ (ed) \nChemical library design. Humana Press, Totowa, pp 27–52 [cito:cites]\n 4. Swain MC, Cole JM (2016) ChemDataExtractor: a toolkit for automated \nextraction of chemical information from the scientific literature. J Chem \nInf Model 56:1894–1904 [cito:cites]\nTable 15 Results on dataset 3 and dataset 2 + 3\nMetrics Augmented dataset (3) Pre‑trained model + augmented dataset \n(2 + 3)\nNon augmented test \nset\nAugmented test set Non augmented test \nset\nAugmented test set\nTrain data size 33,304,320 33,304,320 33,304,320 33,304,320\nTest data size 2,000,000 2,000,000 2,000,000 2,000,000\nTanimoto 0.9663 0.9501 0.9708 0.9521\nTanimoto 1.0 86.43% 80.26% 88.04% 80.87%\nIsomorphic Predictions 97.89% 97.46% 98.15% 97.61%\nNon isomorphic predictions 2.11% 2.54% 1.85% 2.39%\nPage 16 of 16Rajan et al. J Cheminform _##################### _\n 5. Krallinger M, Rabal O, Lourenço A, Oyarzabal J, Valencia A (2017) Informa-\ntion retrieval and text mining technologies for chemistry. Chem Rev \n117:7673–7761 [cito:cites]\n 6. Krallinger M, Leitner F, Rabal O, Vazquez M, Oyarzabal J, Valencia A (2015) \nCHEMDNER: the drugs and chemical names extraction challenge. J \nCheminform 7:S1[cito:cites]\n 7. Beard EJ, Cole JM (2020) ChemSchematicResolver: a toolkit to decode 2D \nchemical diagrams with labels and R-groups into annotated chemical \nnamed entities. J Chem Inf Model 60:2059–2072 [cito:cites]\n 8. Rajan K, Brinkhaus HO, Zielesny A, Steinbeck C (2020) A review of optical \nchemical structure recognition tools. J Cheminform 12:60 [cito:cites] \n[cito:agreesWith] [cito:citesAsAuthority]\n 9. Weininger D (1988) SMILES, a chemical language and information system. \n1. Introduction to methodology and encoding rules. J Chem Inf Comput \nSci 28:31–36 [cito:cites]\n 10. Heller SR, McNaught A, Pletnev I, Stein S, Tchekhovskoi D (2015) InChI, the \nIUPAC international chemical identifier. J Cheminform 7:23 [cito:cites]\n 11. Filippov IV, Nicklaus MC (2009) Optical structure recognition software to \nrecover chemical information: OSRA, an open source solution. J Chem Inf \nModel 49:740–743 [cito:cites] [cito:citesAsAuthority]\n 12. Peryea T, Katzel D, Zhao T, Southall N, Nguyen D-T (2019) MOLVEC: \nOpen source library for chemical structure recognition. In: Abstracts \nof papers of the American Chemical Society, vol 258 [cito:cites] \n[cito:citesAsAuthority]\n 13. Staker J, Marshall K, Abel R, McQuaw CM (2019) Molecular structure \nextraction from documents using deep learning. J Chem Inf Model \n59:1017–1029 [cito:cites] [cito:citesAsAuthority]\n 14. Oldenhof M, Arany A, Moreau Y, Simm J (2020) ChemGrapher: optical \ngraph recognition of chemical compounds by deep learning. J Chem Inf \nModel 60:4506–4517 [cito:cites] [cito:citesAsAuthority]\n 15. Weir H, Thompson K, Choi B, Woodward A, Braun A, Martínez TJ (2021) \nChemPix: automated recognition of hand-drawn hydrocarbon structures \nusing deep learning. ChemRxiv. https:// doi. org/ 10. 26434/ chemr xiv. 14156 \n957. v1 [cito:cites] [cito:citesAsAuthority]\n 16. Clevert D-A, Le T, Winter R, Montanari F (2021) Img2Mol—accurate \nSMILES recognition from molecular graphical depictions. Chem-\nRxiv. https:// doi. org/ 10. 26434/ chemr xiv. 14320 907. v1 [cito:cites] \n[cito:citesAsAuthority]\n 17. Le T, Winter R, Noé F, Clevert D-A (2020) Neuraldecipher—reverse-\nengineering extended-connectivity fingerprints (ECFPs) to their \nmolecular structures. Chem Sci 11:10378–10389 [cito:cites] \n[cito:citesAsAuthority]\n 18. Rajan K, Zielesny A, Steinbeck C (2020) DECIMER: towards deep \nlearning for chemical image recognition. J Cheminform 12:65 \n[cito:usesMethodIn] [cito:citesAsAuthority] [cito:extends]\n 19. Rajan K, Brinkhaus HO, Sorokina M, Zielesny A, Steinbeck C (2021) \nDECIMER-segmentation: automated extraction of chemical structure \ndepictions from scientific literature. J Cheminform 13:1–9. https:// \ndoi. org/ 10. 1186/ s13321- 021- 00496-1 [cito:cites] [cito:extends] \n[cito:citesAsAuthority]\n 20. Silver D, Schrittwieser J, Simonyan K et al (2017) Mastering the game \nof Go without human knowledge. Nature 550:354–359 [cito:cites] \n[cito:agreesWith]\n 21. Kim S, Chen J, Cheng T et al (2019) PubChem 2019 update: improved \naccess to chemical data. Nucleic Acids Res 47:D1102–D1109 \n[cito:citesAsDataSource]\n 22. Irwin JJ, Tang KG, Young J, Dandarchuluun C, Wong BR, Khurelbaatar M, \nMoroz YS, Mayfield J, Sayle RA (2020) ZINC20—a free ultralarge-scale \nchemical database for ligand discovery. J Chem Inf Model 60:6065–6073 \n[cito:cites]\n 23. Ruddigkeit L, van Deursen R, Blum LC, Reymond J-L (2012) Enumeration \nof 166 billion organic small molecules in the chemical universe database \nGDB-17. J Chem Inf Model 52:2864–2875 [cito:cites]\n 24. Steinbeck C, Han Y, Kuhn S, Horlacher O, Luttmann E, Willighagen E \n(2003) The chemistry development kit (CDK): an open-source Java library \nfor Chemo- and bioinformatics. J Chem Inf Comput Sci 43:493–500 \n[cito:usesMethodIn]\n 25. Jung AB, Wada K, Crall J et al (2020) Imgaug. GitHub: San Francisco, CA, \nUSA [cito:usesMethodIn]\n 26. O’Boyle N, Dalke A (2018) DeepSMILES: an adaptation of SMILES for use \nin machine-learning of chemical structures. ChemRxiv. https:// doi. org/ 10. \n26434/ chemr xiv. 70979 60. v1 [cito:usesMethodIn]\n 27. Krenn M, Häse F, Nigam A, Friederich P , Aspuru-Guzik A (2020) Self-\nreferencing embedded strings (SELFIES): a 100% robust molecular string \nrepresentation. Mach Learn Sci Technol 1:045024 [cito:usesMethodIn]\n 28. Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the \ninception architecture for computer vision. In: Proceedings of the IEEE \nconference on computer vision and pattern recognition. pp 2818–2826 \n[cito:cites]\n 29. Tan M, Le Q (2019) Efficientnet: rethinking model scaling for convolu-\ntional neural networks. In: International conference on machine learning. \nPMLR, pp 6105–6114 [cito:cites]\n 30. Deng J, Dong W, Socher R, Li L, Kai Li, Li Fei-Fei (2009) ImageNet: a large-\nscale hierarchical image database. In: 2009 IEEE conference on computer \nvision and pattern recognition. pp 248–255 [cito:cites]\n 31. Xie Q, Luong M-T, Hovy E, Le QV (2020) Self-training with noisy student \nimproves imagenet classification. In: Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition. pp 10687–10698 \n[cito:cites] [cito:usesMethodIn]\n 32. Chollet F et al (2015) Keras. https:// keras. io. [cito:usesMethodIn]\n 33. Abadi M, Agarwal A, Barham P et al (2015) TensorFlow: large-scale \nmachine learning on heterogeneous systems. [cito:usesMethodIn]\n 34. Norrie T, Patil N, Yoon DH, Kurian G, Li S, Laudon J, Young C, Jouppi N, Pat-\nterson D (2021) The design process for Google’s training chips: TPUv2 and \nTPUv3. IEEE Micro 41:56–63 [cito:cites]\n 35. Xu K, Ba J, Kiros R, Cho K, Courville A, Salakhudinov R, Zemel R, Bengio Y \n(2015) Show, attend and tell: neural image caption generation with visual \nattention. In: Bach F, Blei D (eds) Proceedings of the 32nd international \nconference on machine learning. PMLR, Lille, France, pp 2048–2057 \n[cito:usesMethodIn]\n 36. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, \nKaiser L, Polosukhin I (2017) Attention is all you need. arXiv [cs.CL] \n[cito:usesMethodIn]\n 37. Image captioning with visual attention. https:// www. tenso rflow. org/ tutor \nials/ text/ image_ capti oning. Accessed 17 Mar 2021 [cito:usesMethodIn]\n 38. Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation by jointly \nlearning to align and translate. arXiv [cs.CL] [cito:usesMethodIn]\n 39. Kingma DP , Ba J (2014) Adam: a method for stochastic optimization. arXiv \n[cs.LG] [cito:usesMethodIn]\n 40. Landrum G et al (2016) RDKit: open-source cheminformatics soft-\nware (2016). http:// www. rdkit. org/, https:// github. com/ rdkit/ rdkit \n[cito:usesMethodIn]\n 41. dtype support—imgaug 0.4.0 documentation. https:// imgaug. readt \nhedocs. io/ en/ latest/ source/ dtype_ suppo rt. html. Accessed 15 Apr 2021 \n[cito:usesMethodIn]\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7068548202514648
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5151130557060242
    },
    {
      "name": "Transformer",
      "score": 0.493552565574646
    },
    {
      "name": "Deep learning",
      "score": 0.46361833810806274
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4042956829071045
    },
    {
      "name": "Machine learning",
      "score": 0.32961905002593994
    },
    {
      "name": "Engineering",
      "score": 0.12090590596199036
    },
    {
      "name": "Electrical engineering",
      "score": 0.11780276894569397
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76198965",
      "name": "Friedrich Schiller University Jena",
      "country": "DE"
    }
  ],
  "cited_by": 57
}