{
  "title": "FLAT: Chinese NER Using Flat-Lattice Transformer",
  "url": "https://openalex.org/W3034379414",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2104570025",
      "name": "Xiaonan LI",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116385994",
      "name": "Hang Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950037171",
    "https://openalex.org/W2998103904",
    "https://openalex.org/W2759200442",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2567657016",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2963406669",
    "https://openalex.org/W2756381707",
    "https://openalex.org/W2983725648",
    "https://openalex.org/W2983180560",
    "https://openalex.org/W2963270153",
    "https://openalex.org/W2952079278",
    "https://openalex.org/W3015217028",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2734399784",
    "https://openalex.org/W2250709962",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2951635603",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W2962676330",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W3134983687",
    "https://openalex.org/W2970480111",
    "https://openalex.org/W2965690110",
    "https://openalex.org/W2970323499",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W2250999640"
  ],
  "abstract": "Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6836‚Äì6842\nJuly 5 - 10, 2020.c‚Éù2020 Association for Computational Linguistics\n6836\nFLAT: Chinese NER Using Flat-Lattice Transformer\nXiaonan Li, Hang Yan, Xipeng Qiu‚àó, Xuanjing Huang\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\nSchool of Computer Science, Fudan University\nlixiaonan xdu@outlook.com, {hyan19, xpqiu, xjhuang}@fudan.edu.cn\nAbstract\nRecently, the character-word lattice structure\nhas been proved to be effective for Chinese\nnamed entity recognition (NER) by incorpo-\nrating the word information. However, since\nthe lattice structure is complex and dynamic,\nmost existing lattice-based models are hard to\nfully utilize the parallel computation of GPUs\nand usually have a low inference-speed. In\nthis paper, we propose FLAT: Flat-LAttice\nTransformer for Chinese NER, which converts\nthe lattice structure into a Ô¨Çat structure con-\nsisting of spans. Each span corresponds to\na character or latent word and its position in\nthe original lattice. With the power of Trans-\nformer and well-designed position encoding,\nFLAT can fully leverage the lattice informa-\ntion and has an excellent parallelization ability.\nExperiments on four datasets show FLAT out-\nperforms other lexicon-based models in perfor-\nmance and efÔ¨Åciency.\n1 Introduction\nNamed entity recognition (NER) plays an indis-\npensable role in many downstream natural lan-\nguage processing (NLP) tasks (Chen et al., 2015;\nDiefenbach et al., 2018). Compared with English\nNER (Lample et al., 2016; Yang et al., 2017; Liu\net al., 2017; Sun et al., 2020), Chinese NER is more\ndifÔ¨Åcult since it usually involves word segmenta-\ntion.\nRecently, the lattice structure has been proved\nto have a great beneÔ¨Åt to utilize the word infor-\nmation and avoid the error propagation of word\nsegmentation (Zhang and Yang, 2018). We can\nmatch a sentence with a lexicon to obtain the latent\nwords in it, and then we get a lattice like in Figure\n1(a). The lattice is a directed acyclic graph, where\neach node is a character or a latent word. The lat-\ntice includes a sequence of characters and potential\n‚àóCorresponding author.\nÂíå\nAnd\nÂ∫Ü\nQing\n‰∫∫\nPeople\nÈáç\nChong\nÂ∫ó\nShop\nËçØ\nDrug\n‰∫∫ÂíåËçØÂ∫ó\nRenhe Pharmacy\nÈáçÂ∫Ü\nChongqing\nËçØÂ∫ó\nPharmacy\n(a) Lattice.\nÂíå\nAnd\nÂ∫Ü\nQing\n‰∫∫\nPeople\nÈáç\nHeavy\nÂ∫ó\nShop\nËçØ\nDrug\n‰∫∫ÂíåËçØÂ∫ó\nRenhe Pharmacy\nÈáçÂ∫Ü\nChongqing\nËçØÂ∫ó\nPharmacy\nB-LOC E-LOC E-LOCB-LOC I-LOC I-LOC\n(b) Lattice LSTM.\nRenhe\nPharmacy\nÂíå\nAnd\nÂ∫Ü\nQing\n‰∫∫\nPeople\nÈáç\nChong\nÂ∫ó\nShop\nËçØ\nDrug\n‰∫∫ÂíåËçØÂ∫óÈáçÂ∫Ü\nChongqing\nËçØÂ∫ó\nPharmacy\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n1\n2\n3\n6\n5\n6\nTransformer Encoder\nB-LOC E-LOC E-LOCB-LOC I-LOC I-LOC\n(c) Flat-Lattice Transformer.\nFigure 1: While lattice LSTM indicates lattice struc-\nture by dynamically adjusting its structure, FLAT only\nneeds to leverage the span position encoding. In 1(c),\n,\n ,\n denotes tokens, heads and tails, respectively.\nwords in the sentence. They are not ordered se-\nquentially, and the word‚Äôs Ô¨Årst character and last\ncharacter determine its position. Some words in\nlattice may be important for NER. For example, in\nFigure 1(a), ‚Äú‰∫∫ÂíåËçØÂ∫ó(Renhe Pharmacy)‚Äù can be\nused to distinguish between the geographic entity\n‚ÄúÈáçÂ∫Ü(Chongqing)‚Äù and the organization entity ‚ÄúÈáç\nÂ∫Ü‰∫∫(Chongqing People)‚Äù.\nThere are two lines of methods to leverage the\nlattice. (1) One line is to design a model to be\ncompatible with lattice input, such as lattice LSTM\n(Zhang and Yang, 2018) and LR-CNN (Gui et al.,\n2019a). In lattice LSTM, an extra word cell is em-\nployed to encode the potential words, and attention\nmechanism is used to fuse variable-number nodes\nat each position, as in Figure 1(b). LR-CNN uses\n6837\nCNN to encode potential words at different win-\ndow sizes. However, RNN and CNN are hard to\nmodel long-distance dependencies (Vaswani et al.,\n2017), which may be useful in NER, such as coref-\nerence (Stanislawek et al., 2019). Due to the dy-\nnamic lattice structure, these methods cannot fully\nutilize the parallel computation of GPU. (2) An-\nother line is to convert lattice into graph and use a\ngraph neural network (GNN) to encode it, such as\nLexicon-based Graph Network (LGN) (Gui et al.,\n2019b) and Collaborative Graph Network (CGN)\n(Sui et al., 2019). While sequential structure is\nstill important for NER and graph is general coun-\nterpart, their gap is not negligible. These meth-\nods need to use LSTM as the bottom encoder to\ncarry the sequential inductive bias, which makes\nthe model complicated.\nIn this paper, we propose FLAT: Flat LAttice\nTransformer for Chinese NER. Transformer\n(Vaswani et al., 2017) adopts fully-connected self-\nattention to model the long-distance dependencies\nin a sequence. To keep the position information,\nTransformer introduces the position representation\nfor each token in the sequence. Inspired by the\nidea of position representation, we design an in-\ngenious position encoding for the lattice-structure,\nas shown in Figure 1(c). In detail, we assign two\npositional indices for a token (character or word):\nhead position and tail position, by which we can\nreconstruct a lattice from a set of tokens. Thus, we\ncan directly use Transformer to fully model the lat-\ntice input. The self-attention mechanism of Trans-\nformer enables characters to directly interact with\nany potential word, including self-matched words.\nTo a character, its self-matched words denote words\nwhich include it. For example, in Figure 1(a),\nself-matched words of ‚Äú ËçØ (Drug)‚Äù are ‚Äú‰∫∫ÂíåËçØ\nÂ∫ó(Renhe Pharmacy)‚Äù and ‚ÄúËçØÂ∫ó (Pharmacy)‚Äù(Sui\net al., 2019). Experimental results show our model\noutperforms other lexicon-based methods on the\nperformance and inference-speed. Our code will\nbe released at https://github.com/LeeSureman/Flat-\nLattice-Transformer.\n2 Background\nIn this section, we brieÔ¨Çy introduce the Trans-\nformer architecture. Focusing on the NER task,\nwe only discuss the Transformer encoder. It is com-\nposed of self-attention and feedforward network\n(FFN) layers. Each sublayer is followed by resid-\nual connection and layer normalization. FFN is\n0 1 2 3 4 5 0 2 4\n-1 0 1 2 3 4 -1 1 3\n-2 -1 0 1 2 3 -2 0 2\n-3 -2 -1 0 1 2 -3 -1 1\n-4 -3 -2 -1 0 1 -4 -2 0\n-5 -4 -3 -2 -1 0 -5 -3 -1\n0 1 2 3 4 5 0 2 4\n-2 -1 0 1 2 3 -2 0 2\n-4 -3 -2 -1 0 1 -4 -2 0Embedding\nSelf-Attention\nFFN\nùëë(ùë°ùë°)\nLinear & CRF\nÂíå\nAnd\nÂ∫Ü\nQing\n‰∫∫\nPeople\nÈáç\nChong\nÂ∫ó\nShop\nËçØ\nDrug\n‰∫∫ÂíåËçØÂ∫ó\nRenhe Pharmacy\nÈáçÂ∫Ü\nChongqing\nËçØÂ∫ó\nPharmacy\nùëë(‚Ñé‚Ñé)\nùëë(‚Ñéùë°)\nùëë(ùë°‚Ñé)\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n1\n2\n3\n6\n5\n6\nHead\nTail\nAdd & Norm\nAdd & Norm\nToken\nFigure 2: The overall architecture of FLAT.\na position-wise multi-layer Perceptron with non-\nlinear transformation. Transformer performs self-\nattention over the sequence byH heads of attention\nindividually and then concatenates the result of H\nheads. For simplicity, we ignore the head index in\nthe following formula. The result of per head is\ncalculated as:\nAtt(A, V) = softmax(A)V, (1)\nAij =\n(QiKj\nT\n‚àödhead\n)\n, (2)\n[Q, K, V] =Ex[Wq, Wk, Wv], (3)\nwhere E is the token embedding lookup ta-\nble or the output of last Transformer layer.\nWq, Wk, Wv ‚ààRdmodel√ódhead are learnable pa-\nrameters, and dmodel = H √ódhead, dhead is the\ndimension of each head.\nThe vanilla Transformer also uses absolute posi-\ntion encoding to capture the sequential information.\nInspired by Yan et al. (2019), we think commuta-\ntivity of the vector inner dot will cause the loss of\ndirectionality in self-attention. Therefore, we con-\nsider the relative position of lattice also signiÔ¨Åcant\nfor NER.\n3 Model\n3.1 Converting Lattice into Flat Structure\nAfter getting a lattice from characters with a lex-\nicon, we can Ô¨Çatten it into Ô¨Çat counterpart. The\nÔ¨Çat-lattice can be deÔ¨Åned as a set of spans, and a\nspan corresponds to a token, a head and a tail, like\nin Figure 1(c). The token is a character or word.\nThe head and tail denote the position index of the\ntoken‚Äôs Ô¨Årst and last characters in the original se-\nquence, and they indicate the position of the token\nin the lattice. For the character, its head and tail are\nthe same. There is a simple algorithm to recover\nÔ¨Çat-lattice into its original structure. We can Ô¨Årst\n6838\ntake the token which has the same head and tail,\nto construct the character sequence. Then we use\nother tokens (words) with their heads and tails to\nbuild skip-paths. Since our transformation is re-\ncoverable, we assume Ô¨Çat-lattice can maintain the\noriginal structure of lattice.\n3.2 Relative Position Encoding of Spans\nThe Ô¨Çat-lattice structure consists of spans with dif-\nferent lengths. To encode the interactions among\nspans, we propose the relative position encoding of\nspans. For two spans xi and xj in the lattice, there\nare three kinds of relations between them: intersec-\ntion, inclusion and separation, determined by their\nheads and tails. Instead of directly encoding these\nthree kinds of relations, we use a dense vector to\nmodel their relations. It is calculated by continu-\nous transformation of the head and tail information.\nThus, we think it can not only represent the relation\nbetween two tokens, but also indicate more detailed\ninformation, such as the distance between a charac-\nter and a word. Let head[i] and tail[i] denote the\nhead and tail position of span xi. Four kinds of rel-\native distances can be used to indicate the relation\nbetween xi and xj. They can be calculated as:\nd(hh)\nij = head[i] ‚àíhead[j], (4)\nd(ht)\nij = head[i] ‚àítail[j], (5)\nd(th)\nij = tail[i] ‚àíhead[j], (6)\nd(tt)\nij = tail[i] ‚àítail[j], (7)\nwhere d(hh)\nij denotes the distance between head of\nxi and tail of xj, and other d(ht)\nij , d(th)\nij , d(tt)\nij have\nsimilar meanings. The Ô¨Ånal relative position encod-\ning of spans is a simple non-linear transformation\nof the four distances:\nRij = ReLU(Wr(pd(hh)\nij\n‚äïpd(th)\nij\n‚äïpd(ht)\nij\n‚äïpd(tt)\nij\n)), (8)\nwhere Wr is a learnable parameter, ‚äïdenotes the\nconcatenation operator, and pd is calculated as in\nVaswani et al. (2017),\np(2k)\nd = sin\n(\nd/100002k/dmodel\n)\n, (9)\np(2k+1)\nd = cos\n(\nd/100002k/dmodel\n)\n, (10)\nwhere d is d(hh)\nij , d(ht)\nij , d(th)\nij or d(tt)\nij and k denotes\nthe index of dimension of position encoding. Then\nwe use a variant of self-attention (Dai et al., 2019)\nto leverage the relative span position encoding as\nfollows:\nOntonotes MSRA Resume Weibo\nTrain 15740 46675 3821 1350\nCharavg 36.92 45.87 32.15 54.37\nWordavg 17.59 22.38 24.99 21.49\nEntityavg 1.15 1.58 3.48 1.42\nTable 1: Statistics of four datasets. ‚ÄòTrain‚Äô is the size of\ntraining set. ‚ÄòCharavg‚Äô, ‚ÄòWordavg‚Äô, ‚ÄòEntityavg‚Äô are the\naverage number of chars, words mateched by lexicon\nand entities in an instance.\nLexicon Ontonotes MSRA Resume Weibo\nBiLSTM - 71.81 91.87 94.41 56.75\nTENER - 72.82 93.01 95.25 58.39\nLattice LSTM YJ 73.88 93.18 94.46 58.79\nCNNR YJ 74.45 93.71 95.11 59.92\nLGN YJ 74.85 93.63 95.41 60.15\nPLT YJ 74.60 93.26 95.40 59.92\nFLAT YJ 76.45 94.12 95.45 60.32\nFLATmsm YJ 73.39 93.11 95.03 57.98\nFLATmld YJ 75.35 93.83 95.28 59.63\nCGN LS 74.79 93.47 94.12 ‚àó 63.09\nFLAT LS 75.70 94.35 94.93 63.42\nTable 2: Four datasets results (F1). BiLSTM results are\nfrom Zhang and Yang (2018). PLT denotes the porous\nlattice Transformer (Mengge et al., 2019). ‚ÄòYJ‚Äô denotes\nthe lexicon released by Zhang and Yang (2018), and\n‚ÄòLS‚Äô denotes the lexicon released by Li et al. (2018).\nThe result of other models are from their original paper.\nExcept that the superscript * means the result is not\nprovided in the original paper, and we get the result\nby running the public source code. Subscripts ‚Äòmsm‚Äô\nand ‚Äòmld‚Äô denote FLAT with the mask of self-matched\nwords and long distance (>10), respectively.\nA‚àó\ni,j = W‚ä§\nq E‚ä§\nxi Exj Wk,E + W‚ä§\nq E‚ä§\nxi RijWk,R\n+ u‚ä§Exj Wk,E + v‚ä§RijWk,R, (11)\nwhere Wq, Wk,R, Wk,E ‚àà Rdmodel√ódhead and\nu, v ‚ààRdhead are learnable parameters. Then we\nreplace A with A‚àó in Eq.(1). The following calcu-\nlation is the same with vanilla Transformer.\nAfter FLAT, we only take the character represen-\ntation into output layer, followed by a Condiftional\nRandom Field (CRF) (Lafferty et al., 2001).\nSpan F Type Acc\nOntonotes MSRA Ontonotes MSRA\nTENER 72.41 93.17 96.33 99.29\nFLAT 76.23 94.58 97.03 99.52\nFLAThead 75.64 94.33 96.85 99.45\nTable 3: Two metrics of models. FLAThead means Rij\nin (11) is replaced by d(hh)\nij .\n6839\n4 Experiments\n4.1 Experimental Setup\nFour Chinese NER datasets were used to eval-\nuate our model, including (1) Ontonotes 4.0\n(Weischedel and Consortium, 2013) (2) MSRA\n(Levow, 2006) (3) Resume (Zhang and Yang,\n2018) (4) Weibo (Peng and Dredze, 2015; He and\nSun, 2016). We show statistics of these datasets in\nTable 1. We use the same train, dev, test split as Gui\net al. (2019b). We take BiLSTM-CRF and TENER\n(Yan et al., 2019) as baseline models. TENER is\na Transformer using relative position encoding for\nNER, without external information. We also com-\npare FLAT with other lexicon-based methods. The\nembeddings and lexicons are the same as Zhang\nand Yang (2018). When comparing with CGN (Li\net al., 2018), we use the same lexicon as CGN. The\nway to select hyper-parameters can be found in the\nsupplementary material. In particular, we use only\none layer Transformer encoder for our model.\n4.2 Overall Performance\nAs shown in Table 2, our model outperforms base-\nline models and other lexicon-based models on\nfour Chinese NER datasets. Our model outper-\nforms TENER (Yan et al., 2019) by 1.72 in average\nF1 score. For lattice LSTM, our model has an\naverage F1 improvement of 1.51 over it. When\nusing another lexicon (Li et al., 2018), our model\nalso outperforms CGN by 0.73 in average F1 score.\nMaybe due to the characteristic of Transformer, the\nimprovement of FLAT over other lexicon-based\nmodels on small datasets is not so signiÔ¨Åcant like\nthat on large datasets.\n4.3 Advantage of Fully-Connected Structure\nWe think self-attention mechanism brings two ad-\nvantages over lattice LSTM: 1) All characters can\ndirectly interact with its self-matched words. 2)\nLong-distance dependencies can be fully modeled.\nDue to our model has only one layer, we can strip\nthem by masking corresponding attention. In de-\ntail, we mask attention from the character to its\nself-matched word and attention between tokens\nwhose distance exceeds 10. As shown in Table\n2, the Ô¨Årst mask brings a signiÔ¨Åcant deterioration\nto FLAT while the second degrades performance\nslightly. As a result, we think leveraging informa-\ntion of self-matched words is important For Chi-\nnese NER.\nLatticeLSTM\n‚ô£\nLatticeLSTM\n‚ô†\nLR-CNN\n‚ô£\nLGN\n‚ô†\nCGN\n‚ô†\nFLAT\n‚ô£\nFLAT\n‚ô†\n0\n5\n10\n15\n1\n2.1 2.47 2.74\n4.74\n3.28\n16.32\nRelative Speed\nFigure 3: Inference-speed of different models, com-\npared with lattice LSTM ‚ô£. ‚ô£denotes non-batch-\nparallel version, and ‚ô†indicates the model is run in\n16 batch size parallelly. For model LR-CNN, we do\nnot get its batch-parallel version.\n4.4 EfÔ¨Åciency of FLAT\nTo verify the computation efÔ¨Åciency of our model,\nwe compare the inference-speed of different\nlexicon-based models on Ontonotes. The result\nis shown in Figure 3. GNN-based models outper-\nform lattice LSTM and LR-CNN. But the RNN\nencoder of GNN-based models also degrades their\nspeed. Because our model has no recurrent mod-\nule and can fully leverage parallel computation of\nGPU, it outperforms other methods in running ef-\nÔ¨Åciency. In terms of leveraging batch-parallelism,\nthe speedup ratio brought by batch-parallelism is\n4.97 for FLAT, 2.1 for lattice LSTM, when batch\nsize = 16. Due to the simplicity of our model, it can\nbeneÔ¨Åt from batch-parallelism more signiÔ¨Åcantly.\n4.5 How FLAT Brings Improvement\nCompared with TENER, FLAT leverages lexicon\nresources and uses a new position encoding. To\nprobe how these two factors bring improvement.\nWe set two new metrics, 1)Span F: while the com-\nmon F score used in NER considers correctness\nof both the span and the entity type, Span F only\nconsiders the former. 2) Type Acc: proportion of\nfull-correct predictions to span-correct predictions.\nTable 3 shows two metrics of three models on the\ndevlopment set of Ontonotes and MSRA. We can\nÔ¨Ånd: 1) FLAT outperforms TENER in two met-\nrics signiÔ¨Åcantly. 2) The improvement on Span F\nbrought by FLAT is more signiÔ¨Åcant than that on\nType Acc. 3) Compared to FLAT, FLAThead‚Äôs de-\nterioration on Span F is more signiÔ¨Åcant than that\non Type Acc. These show: 1) The new position\nencoding helps FLAT locate entities more accu-\nrately. 2) The pre-trained word-level embedding\n6840\nLexicon Ontonotes MSRA Resume Weibo\nBERT - 80.14 94.95 95.53 68.20\nBERT+FLAT YJ 81.82 96.09 95.86 68.55\nTable 4: Comparision between BERT and\nBERT+FLAT. ‚ÄòBERT‚Äô refers to the BERT+MLP+CRF\narchitecture. ‚ÄòFLAT+BERT‚Äô refers to FLAT using\nBERT embedding. We Ô¨Ånetune BERT in both models\nduring training. The BERT in the experiment is\n‚ÄòBERT-wwm‚Äô released by Cui et al. (2019). We use it\nby the BERTEmbedding in fastNLP 1.\nmakes FLAT more powerful in entity classiÔ¨Åcation\n(Agarwal et al., 2020).\n4.6 Compatibility with BERT\nWe also compare FLAT equipped with BERT with\ncommon BERT+CRF tagger on four datasets, and\nResults are shown in Table 4. We Ô¨Ånd that, for large\ndatasets like Ontonotes and MSRA, FLAT+BERT\ncan have a signiÔ¨Åcant improvement over BERT. But\nfor small datasets like Resume and Weibo, the im-\nprovement of FLAT+BERT over BERT is marginal.\n5 Related Work\n5.1 Lexicon-based NER\nZhang and Yang (2018) introduced a lattice LSTM\nto encode all characters and potential words recog-\nnized by a lexicon in a sentence, avoiding the error\npropagation of segmentation while leveraging the\nword information. Gui et al. (2019a) exploited a\ncombination of CNN and rethinking mechanism\nto encode character sequence and potential words\nat different window sizes. Both models above suf-\nfer from the low inference efÔ¨Åciency and are hard\nto model long-distance dependencies. Gui et al.\n(2019b) and Sui et al. (2019) leveraged a lexicon\nand character sequence to construct graph, convert-\ning NER into a node classiÔ¨Åcation task. However,\ndue to NER‚Äôs strong alignment of label and input,\ntheir model needs an RNN module for encoding.\nThe main difference between our model and models\nabove is that they modify the model structure ac-\ncording to the lattice, while we use a well-designed\nposition encoding to indicate the lattice structure.\n5.2 Lattice-based Transformer\nFor lattice-based Transformer, it has been used in\nspeech translation and Chinese-source translation.\nThe main difference between them is the way to\n1https://github.com/fastnlp/fastNLP\nindicate lattice structure. In Chinese-source trans-\nlation, Xiao et al. (2019) take the absolute position\nof nodes‚Äô Ô¨Årst characters and the relation between\neach pair of nodes as the structure information. In\nspeech translation, Sperber et al. (2019) used the\nlongest distance to the start node to indicate lattice\nstructure, and Zhang et al. (2019) used the shortest\ndistance between two nodes. Our span position\nencoding is more natural, and can be mapped to all\nthe three ways, but not vise versa. Because NER is\nmore sensitive to position information than transla-\ntion, our model is more suitable for NER. Recently,\nPorous Lattice Transformer (Mengge et al., 2019)\nis proposed for Chinese NER. The main difference\nbetween FLAT and Porus Lattice Transformer is\nthe way of representing position information. We\nuse ‚Äòhead‚Äô and ‚Äòtail‚Äô to represent the token‚Äôs posi-\ntion in the lattice. They use ‚Äòhead‚Äô, tokens‚Äô relative\nrelation (not distance) and an extra GRU. They also\nuse ‚Äòporous‚Äô technique to limit the attention distri-\nbution. In their model, the position information is\nnot recoverable because ‚Äòhead‚Äô and relative relation\ncan cause position information loss. BrieÔ¨Çy, rela-\ntive distance carries more information than relative\nrelation.\n6 Conclusion and Future Work\nIn this paper, we introduce a Ô¨Çat-lattice Trans-\nformer to incorporate lexicon information for Chi-\nnese NER. The core of our model is converting\nlattice structure into a set of spans and introduc-\ning the speciÔ¨Åc position encoding. Experimental\nresults show our model outperforms other lexicon-\nbased models in the performance and efÔ¨Åciency.\nWe leave adjusting our model to different kinds of\nlattice or graph as our future work.\nAcknowledgments\nWe thank anonymous reviewers for their respon-\nsible attitude and helpful comments. We thank\nTianxiang Sun, Yunfan Shao and Lei Li for their\nhelp, such as drawing skill sharing, pre-reviewing,\netc. This work is supported by the National Key\nResearch and Development Program of China\n(No. 2018YFC0831103), National Natural Sci-\nence Foundation of China (No. U1936214 and\n61672162), Shanghai Municipal Science and Tech-\nnology Major Project (No. 2018SHZDZX01) and\nZJLab.\n6841\nReferences\nOshin Agarwal, Yinfei Yang, Byron Wallace, and Ani\nNenkova. 2020. Interpretability analysis for named\nentity recognition to understand system predictions\nand how they can improve.\nYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and\nJun Zhao. 2015. Event extraction via dynamic multi-\npooling convolutional neural networks. In Proceed-\nings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 167‚Äì176,\nBeijing, China. Association for Computational Lin-\nguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nBERT. CoRR, abs/1906.08101.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na Ô¨Åxed-length context. CoRR, abs/1901.02860.\nDennis Diefenbach, Vanessa Lopez, Kamal Singh, and\nPierre Maret. 2018. Core techniques of question an-\nswering systems over knowledge bases: a survey.\nKnowledge and Information Systems , 55(3):529‚Äì\n569.\nTao Gui, Ruotian Ma, Qi Zhang, Lujun Zhao, Yu-Gang\nJiang, and Xuanjing Huang. 2019a. Cnn-based\nchinese ner with lexicon rethinking. In Proceed-\nings of the 28th International Joint Conference on\nArtiÔ¨Åcial Intelligence, IJCAI‚Äô19, pages 4982‚Äì4988.\nAAAI Press.\nTao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jin-\nlan Fu, Zhongyu Wei, and Xuanjing Huang. 2019b.\nA lexicon-based graph neural network for Chinese\nNER. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1039‚Äì1049, Hong Kong, China. Association for\nComputational Linguistics.\nHangfeng He and Xu Sun. 2016. F-score driven max\nmargin neural network for named entity recognition\nin chinese social media. CoRR, abs/1611.04234.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random Ô¨Åelds:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth Inter-\nnational Conference on Machine Learning , ICML\n‚Äô01, pages 282‚Äì289, San Francisco, CA, USA. Mor-\ngan Kaufmann Publishers Inc.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 260‚Äì270, San Diego, California. Association\nfor Computational Linguistics.\nGina-Anne Levow. 2006. The third international Chi-\nnese language processing bakeoff: Word segmen-\ntation and named entity recognition. In Proceed-\nings of the Fifth SIGHAN Workshop on Chinese\nLanguage Processing, pages 108‚Äì117, Sydney, Aus-\ntralia. Association for Computational Linguistics.\nShen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and\nXiaoyong Du. 2018. Analogical reasoning on Chi-\nnese morphological and semantic relations. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 138‚Äì143, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nLiyuan Liu, Jingbo Shang, Frank F. Xu, Xiang Ren,\nHuan Gui, Jian Peng, and Jiawei Han. 2017. Em-\npower sequence labeling with task-aware neural lan-\nguage model. CoRR, abs/1709.04109.\nXue Mengge, Yu Bowen, Liu Tingwen, Wang Bin,\nMeng Erli, and Li Quangang. 2019. Porous lattice-\nbased transformer encoder for chinese ner.\nNanyun Peng and Mark Dredze. 2015. Named entity\nrecognition for Chinese social media with jointly\ntrained embeddings. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 548‚Äì554, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nMatthias Sperber, Graham Neubig, Ngoc-Quan Pham,\nand Alex Waibel. 2019. Self-attentional models for\nlattice inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1185‚Äì1197, Florence, Italy. Associa-\ntion for Computational Linguistics.\nTomasz Stanislawek, Anna Wr ¬¥oblewska, Alicja\nW¬¥ojcicka, Daniel Ziembicki, and Przemyslaw\nBiecek. 2019. Named entity recognition - is there\na glass ceiling? In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 624‚Äì633, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nDianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, and\nShengping Liu. 2019. Leverage lexical knowledge\nfor Chinese named entity recognition via collabora-\ntive graph network. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 3821‚Äì3831, Hong Kong, China. As-\nsociation for Computational Linguistics.\nTianxiang Sun, Yunfan Shao, Xiaonan Li, Pengfei Liu,\nHang Yan, Xipeng Qiu, and Xuanjing Huang. 2020.\nLearning sparse sharing architectures for multiple\ntasks. In Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence.\n6842\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998‚Äì6008. Curran Asso-\nciates, Inc.\nRalph M Weischedel and Linguistic Data Consortium.\n2013. Ontonotes release 5.0. Title from disc label.\nFengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang,\nand Kehai Chen. 2019. Lattice-based transformer\nencoder for neural machine translation. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3090‚Äì3097,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu.\n2019. Tener: Adapting transformer encoder for\nnamed entity recognition.\nJie Yang, Yue Zhang, and Fei Dong. 2017. Neu-\nral reranking for named entity recognition. CoRR,\nabs/1707.05127.\nPei Zhang, Niyu Ge, Boxing Chen, and Kai Fan.\n2019. Lattice transformer for speech translation.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n6475‚Äì6484, Florence, Italy. Association for Compu-\ntational Linguistics.\nYue Zhang and Jie Yang. 2018. Chinese NER using\nlattice LSTM. CoRR, abs/1805.02023.\nA Appendices\nA.1 Hyperparameters Selection\nFor MSRA and Ontonotes these two large datasets,\nwe select the hyper-parameters based on the devel-\nopment experiment of Ontonotes. For two small\ndatasets, Resume and Weibo, we Ô¨Ånd their optimal\nhyper-parameters by random-search. The Table 5\nlists the hyper-parameters obtained from the devel-\nopment experiment of Ontonotes.\nThe Table 6 lists the range of hyper-parameters\nrandom-search for Weibo, Resume datasets. For\nthe hyper-parameters which do not appear in it,\nthey are the same as in Table 5.\nbatch 10\nlr\n-decay\n1e-3\n0.05\noptimizer\n-momentum\nSGD\n0.9\ndmodel 160\nhead 8\nFFN size 480\nembed dropout 0.5\noutput dropout 0.3\nwarmup 10 (epoch)\nTable 5: Hyper-parameters for Ontonotes and MSRA.\nbatch [8,10]\nlr [1e-3, 8e-4]\ndhead [16,20]\nhead [4,8,12]\nwarmup [1, 5, 10] (epoch)\nTable 6: The range of hyper-parameters random-search\nfor Weibo, Resume datasets.",
  "topic": "Lattice (music)",
  "concepts": [
    {
      "name": "Lattice (music)",
      "score": 0.6294799447059631
    },
    {
      "name": "Computer science",
      "score": 0.6047208309173584
    },
    {
      "name": "Transformer",
      "score": 0.5456342101097107
    },
    {
      "name": "Computation",
      "score": 0.5251731276512146
    },
    {
      "name": "Inference",
      "score": 0.4200509786605835
    },
    {
      "name": "Algorithm",
      "score": 0.4174182713031769
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3680003881454468
    },
    {
      "name": "Physics",
      "score": 0.16086167097091675
    },
    {
      "name": "Voltage",
      "score": 0.16059911251068115
    },
    {
      "name": "Electrical engineering",
      "score": 0.15256422758102417
    },
    {
      "name": "Engineering",
      "score": 0.15197640657424927
    },
    {
      "name": "Acoustics",
      "score": 0.09346479177474976
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I116921496",
      "name": "Lattice Semiconductor (United States)",
      "country": "US"
    }
  ],
  "cited_by": 407
}