{
  "title": "DomBERT: Domain-oriented Language Model for Aspect-based Sentiment Analysis",
  "url": "https://openalex.org/W3103223797",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2116153423",
      "name": "Hu Xu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A1973602695",
      "name": "Bing Liu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A1978909455",
      "name": "Lei Shu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2147553045",
      "name": "Philip Yu",
      "affiliations": [
        "Tsinghua University",
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2950488390",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2108646579",
    "https://openalex.org/W2965510113",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W2985056549",
    "https://openalex.org/W4313490656",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2756816896",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W2789190634",
    "https://openalex.org/W2963494756",
    "https://openalex.org/W2964164368",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W3015777882",
    "https://openalex.org/W3105721709",
    "https://openalex.org/W2963274454",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2962843214",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3156333129",
    "https://openalex.org/W2962676330",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W2963168371",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2962741379",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2951278869",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W2756381707",
    "https://openalex.org/W2251792193",
    "https://openalex.org/W3088107006",
    "https://openalex.org/W2251124635",
    "https://openalex.org/W2757541972",
    "https://openalex.org/W2963240575",
    "https://openalex.org/W2875308690",
    "https://openalex.org/W3152231500",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2901440799"
  ],
  "abstract": "This paper focuses on learning domain-oriented language models driven by end tasks, which aims to combine the worlds of both general-purpose language models (such as ELMo and BERT) and domain-specific language understanding. We propose DomBERT, an extension of BERT to learn from both in-domain corpus and relevant domain corpora. This helps in learning domain language models with low-resources. Experiments are conducted on an assortment of tasks in aspect-based sentiment analysis (ABSA), demonstrating promising results.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1725–1731\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1725\nDomBERT: Domain-oriented Language Model\nfor Aspect-based Sentiment Analysis\nHu Xu1, Bing Liu1, Lei Shu1 and Philip S. Yu1,2\n1Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA\n2Institute for Data Science, Tsinghua University, Beijing, China\n{hxu48, liub, lshu3, psyu}@uic.edu\nAbstract\nThis paper focuses on learning domain-\noriented language models driven by end tasks,\nwhich aims to combine the worlds of both\ngeneral-purpose language models (such as\nELMo and BERT) and domain-speciﬁc lan-\nguage understanding. We propose DomBERT,\nan extension of BERT to learn from both in-\ndomain corpus and relevant domain corpora.\nThis helps in learning domain language mod-\nels with low-resources. Experiments are con-\nducted on an assortment of tasks in aspect-\nbased sentiment analysis (ABSA), demonstrat-\ning promising results. 1\n1 Introduction\nPre-trained language models (LMs) (Peters et al.,\n2018; Radford et al., 2018, 2019; Devlin et al.,\n2019) aim to learn general (or mixed-domain)\nknowledge for end tasks. Recent studies (Xu et al.,\n2019; Gururangan et al., 2020) show that learning\ndomain-speciﬁc LMs are equally important. This\nis because the training corpus of general LMs is\nout-of-domain for end tasks in a particular domain\nand, more importantly, because general LMs may\nnot capture the long-tailed and underrepresented\ndomain details (Xu et al., 2018). An intuitive ex-\nample related to corpus of aspect-based sentiment\nanalysis (ABSA) can be found in Table 1, where all\nmasked words sky, water, idea, screen and picture\ncan appear in a mixed-domain corpus. A general-\npurpose LM may favor frequent examples and ig-\nnore long-tailed choices in certain domains.\nIn contrast, although domain-speciﬁc LMs can\ncapture ﬁne-grained domain details, they may suf-\nfer from insufﬁcient training corpus (Gururan-\ngan et al., 2020) to strengthen general knowledge\nwithin a domain. To this end, we propose a domain-\n1The code will be released onhttps://github.com/\nhowardhsu/BERT-for-RRC-ABSA .\nExample Domain\nThe[MASK]is clear .\nTheskyis clear . Astronomy [Irrelevant Domain]\nThewateris clear . Liquids [Irrelevant Domain]\nTheideais clear . Concepts [Irrelevant Domain]\nThescreenis clear . Desktop [Relevant Domain]\nThepictureis clear . Laptop [Target Domain]\nTable 1: Multiple choices to recover a masked token\n(an aspect in ABSA) for different domains: a target do-\nmain needs more examples from a relevant domain.\noriented learning task that aims to combine the ben-\neﬁts of both general and domain-speciﬁc worlds:\nDomain-oriented Learning: Given a target do-\nmain tand a set of diverse source domains S =\n{s1,s2,... }, perform (language model) learning\nthat focusing on tand all its relevant domains in S.\nThis learning task resolves the issues in both\ngeneral and domain-speciﬁc worlds. On one hand,\nthe training of LM does not need to focus on un-\nrelated domains anymore (e.g., Books is one big\ndomain but not very related to laptop); on the other\nhand, although an in-domain corpus may be limited,\nother relevant domains can share a great amount\nof knowledge (e.g., Desktop in Table 1) to make\nin-domain corpus more diverse and general.\nThis paper proposes an extremely simple ex-\ntension of BERT (Devlin et al., 2019) called\nDomBERT to learn domain-oriented language mod-\nels. DomBERT simultaneously learns masked lan-\nguage modeling and discovers relevant domains\n(with a built-in retrieval model (Lewis et al., 2020))\nto draw training examples, where the later are com-\nputed from domain embeddings learned from an\nauxiliary task of domain classiﬁcation. We apply\nDomBERT to end tasks in aspect-based sentiment\nanalysis (ABSA) in low-resource settings, demon-\nstrating promising results.\nRelated Work Pre-trained language models gain\nsigniﬁcant improvements over a wide spectrum\nof NLP tasks(Minaee et al., 2020), including\nELMo(Peters et al., 2018), GPT/GPT2(Radford\n1726\net al., 2018, 2019), BERT(Devlin et al., 2019), XL-\nNet(Yang et al., 2019), RoBERTa(Liu et al., 2019),\nALBERT(Lan et al., 2019), ELECTRA(Clark et al.,\n2019). This paper extends BERT’s masked lan-\nguage model (MLM) with domain knowledge learn-\ning. Following RoBERTa, the proposed DomBERT\nleverages dynamic masking, removes the next sen-\ntence prediction (NSP) task (which is proved to\nhave negative effects on pre-trained parameters),\nand allows for max-length MLM to fully utilize\nthe computational power. This paper also borrows\nALBERT’s removal of dropout since pre-trained\nLM, in general, is an underﬁtting task that requires\nmore parameters instead of avoiding overﬁtting.\nThe proposed domain-oriented learning task can\nbe viewed as one type of transfer learning(Pan and\nYang, 2009), which learns a transfer strategy implic-\nitly that transfer training examples from relevant\n(source) domains to the target domain. This trans-\nfer process is conducted throughout the training\nprocess of DomBERT.\nThe experiment of this paper focuses on aspect-\nbased sentiment analysis (ABSA), which typically\nrequires a lot of domain-speciﬁc knowledge. Re-\nviews serve as a rich resource for sentiment anal-\nysis (Pang et al., 2002; Hu and Liu, 2004; Liu,\n2012, 2015). ABSA aims to turn unstructured re-\nviews into structured ﬁne-grained aspects (such as\nthe “battery” or aspect category of a laptop) and\ntheir associated opinions (e.g., “good battery” is\npositive about the aspect battery). This paper fo-\ncuses on three (3) popular tasks in ABSA: aspect\nextraction (AE) (Hu and Liu, 2004; Li and Lam,\n2017), aspect sentiment classiﬁcation (ASC) (Hu\nand Liu, 2004; Dong et al., 2014; Nguyen and Shi-\nrai, 2015; Li et al., 2018; Tang et al., 2016; Wang\net al., 2016a,b; Ma et al., 2017; Chen et al., 2017;\nMa et al., 2017; Tay et al., 2018; He et al., 2018; Liu\net al., 2018) and end-to-end ABSA (E2E-ABSA)\n(Li et al., 2019a,b). AE aims to extract aspects (e.g.,\n“battery”), ASC identiﬁes the polarity for a given\naspect (e.g., positive for battery) and E2E-ABSA is\na combination of AE and ASC that detects the as-\npects and their associated polarities simultaneously.\nThis paper focuses on self-supervised methods2 to\nimprove ABSA.\n2We assume domain tags are largely available online with-\nout extra supervised annotation.\n2 DomBERT\nThis section presents DomBERT, which is an ex-\ntension of BERT for domain knowledge learning.\nThe goal of DomBERT is to discover relevant do-\nmains from the pool of source domains and uses\nthe training examples from relevant source domains\nfor masked language model learning. As a result,\nDomBERT has a sampling process over a categori-\ncal distribution on all domains (including the target\ndomain) to retrieve relevant domains’ examples.\nLearning such a distribution needs to detect the do-\nmain similarities between all source domains and\nthe target domain. DomBERT learns an embed-\nding for each domain and computes such similari-\nties. The domain embeddings are learned from an\nauxiliary task called domain classiﬁcation.\n2.1 Domain Classiﬁcation\nGiven a pool of source and target domains, one can\neasily form a classiﬁcation task on domain tags.\nAs such, each text document has its domain label\nl. Following RoBERTa(Liu et al., 2019)’s max-\nlength training examples, we pack different texts\nfrom the same domain up to the maximum length\ninto a single training example.\nLet the number of source domains be |S|= n.\nThen the number of domains (including the target\ndomain) is n+ 1. Let h[CLS] denote the hidden\nstate of the [CLS] token of BERT, which indicates\nthe document-level representations of one example.\nWe ﬁrst pass this hidden states into a dense layer\nto reduce the size of hidden states. Then we pass\nthis reduced hidden states to a dense layer D ∈\nR(n+1)∗m to compute the logits over all domains ˆl:\nˆl= D ·(W ·h[CLS] + b), (1)\nwhere mis the size of the dense layer, D, W and\nbare trainable weights. Besides a dense layer, D is\nessentially a concatenation of domain embeddings:\nD = dt ◦d1 ◦···◦ dn. Then we apply cross-\nentropy loss to the logits and label to obtain the\nloss of domain classiﬁcation:\nLCLS = CrossEntropyLoss(ˆl,l). (2)\nTo encourage the diversity of domain embeddings,\nwe further compute a regularizer among domain\nembeddings as following:\n∆ = 1\n|D|2 ||cos(D,DT ) −I||2\n2. (3)\n1727\nMinimizing this regularizer encourages the learned\nembeddings to be more orthogonal (thus diverse)\nto each other. Finally, we add the loss of domain\nclassiﬁcation, BERT’s masked language model and\nregularizer together:\nL= λLMLM + (1 −λ)LCLS + ∆, (4)\nwhere λ controls the ratio of losses between\nmasked language model and domain classiﬁcation.\n2.2 Domain Sampler\nAs a side product of domain classiﬁcation,\nDomBERT has a built-in data sampling process\nto draw examples from both the target domain and\nrelevant domains for future learning. This pro-\ncess follows a uniﬁed categorical distribution over\nall domains, which ensures a good amount of ex-\namples from both the target domains and relevant\ndomains are sampled. As such, it is important to\nalways have the target domain twith the highest\nprobability for sampling.\nTo this end, we use cosine similarity as the sim-\nilarity function, which has the property to always\nlet cos(dt,dt) = 1 . For an arbitrary domain i,\nthe probability Pi of domain i being sampled is\ncomputed from a softmax function over domain\nsimilarities as following:\nPi = exp (cos(dt,di)/τ)∑n+1\nj=0 exp (cos(dt,dj)/τ)\n, (5)\nwhere τ is the temperature (Hinton et al., 2015) to\ncontrol the importance of highly-ranked domains\nvs long-tailed domains.\nTo form a mini-batch for the next training step,\nwe sample domains following the categorical dis-\ntribution of s∼P and retrieve the next available\nexample from each sampled domain. As such, we\nmaintain a randomly shufﬂed queue of examples\nfor each domain. When the examples of one do-\nmain are exhausted, a new randomly shufﬂed queue\nwill be generated for that domain.\n3 Experiments\n3.1 Datasets\nWe apply DomBERT to end tasks in aspect-\nbased sentiment analysis from the SemEval dataset,\nwhich focusing on Laptop, Restaurant. We choose\n3 end tasks: aspect extraction (AE), aspect sen-\ntiment classiﬁcation (ASC), and end2end ABSA.\nFor AE, we choose SemEval 2014 Task 4 for lap-\ntop and SemEval-2016 Task 5 for restaurant to be\nconsistent with (Xu et al., 2018) and other previous\nworks. For ASC, we use SemEval 2014 Task 4\nfor both laptop and restaurant as existing research\nfrequently uses this version. We use 150 examples\nfrom the training set of all these datasets for vali-\ndation. For E2E-ABSA, we adopt the formulation\nof (Li et al., 2019a) where the laptop data is from\nSemEval-2014 task 4 and the restaurant domain is\na combination of SemEval 2014-2016.\nBased on the domains of end tasks from Se-\nmEval dataset, we explore the capability of the\nlarge-scale unlabeled corpus from Amazon review\ndatasets(He and McAuley, 2016) and Yelp dataset3.\nFollowing (Xu et al., 2019), we select all laptop\nreviews from the electronics department. This ends\nwith about 100 MB corpus. Similarly, we simulate\na low-resource setting for restaurants and randomly\nselect about 100 MB reviews tagged asRestaurants\nas their ﬁrst category from Yelp reviews. For source\ndomains S, we choose all reviews from the 5-core\nversion of Amazon review datasets and all Yelp re-\nviews excluding Laptop and Restaurants. Note that\nYelp is not solely about restaurants but has other\nlocation-based domains such as car service, bank,\ntheatre, etc. This ends with a total of |D|= 4680\ndomains, and n= 4679 are source domains. The\ntotal size of the corpus is about 20 GB.\nThe number of examples for each domain is plot-\nted in Figure 1, where the distribution of domains\nis heavily long-tailed.\n3.2 Hyper-parameters\nWe adopt BERTBASE (uncased) as the basis for\nall experiments due to the limits of computational\npower in our academic setting. We choose the hid-\nden size of domain embeddings m= 64 to ensure\nthe regularizer term in the loss doesn’t consume\ntoo much GPU memory. We choose τ = 0.1 for\n3https://www.yelp.com/dataset/\nchallenge, 2019 version.\nLaptop Restaurant\nTraining\nSentence 3045 2000\nAspect 2358 1743\nTesting\nSentence 800 676\nAspect 654 622\nTable 2: Summary of datasets on aspect extraction.\n1728\nLaptop Restaurant\nTraining\nPositive 987 2164\nNegative 866 805\nNeutral 460 633\nTesting\nPositive 341 728\nNegative 128 196\nNeutral 169 196\nTable 3: Summary of datasets on aspect sentiment clas-\nsiﬁcation.\nLaptop Restaurant\nTraining\nPositive 987 2407\nNegative 860 1035\nNeutral 450 664\nTesting\nPositive 339 1524\nNegative 130 500\nNeutral 165 263\nTable 4: Summary of datasets on end-to-end aspect-\nbased sentiment analysis.\nlaptop and τ = 0 .13 for restaurant domain and\nλ = 0.9. We assume the number of training ex-\namples per epoch is the number of examples in\nthe target domains. Then, we train DomBERT for\n400 epochs to get enough training examples from\nrelevant domains. The full batch size is set to 288.\nThe maximum length of DomBERT is consistent\nwith BERT as 512. We use Adamax(Kingma and\nBa, 2014) as the optimizer. Lastly, the learning rate\nis to be 5e-5.\n3.3 Compared Methods\nBERT this is the vanilla BERTBASE pre-trained\nmodel from (Devlin et al., 2019), which is used\nto show the performance of BERT without any\ndomain adaption.\nBERT-Review post-train BERT on all (mixed-\ndomain) Amazon review datasets and Yelp datasets\nin a similar way of training BERT. Following (Liu\net al., 2019), we train the whole corpus for 4 epochs,\nwhich took about 10 days of training (much longer\nthan DomBERT).\nBERT-DKis a baseline borrowed from (Xu et al.,\n2019) that trains an LM per domain. Note that the\nrestaurant domain is trained from 1G of reviews\nthat aligns well with the types of restaurants in\nDomain LaptopRestaurantMethods F1 F1BERT(Devlin et al., 2019)79.28 74.1BERT-Review 83.64 76.20BERT-DK(Xu et al., 2019)83.55 77.02DomBERT 83.89 77.21\nTable 5: AE in F1.\nFigure 1: Rank of domains by number of examples.\nDomain Laptop Rest.Methods Acc. MF1Acc. MF1\nBERT(Devlin et al., 2019)75.29 71.9181.54 71.94BERT-Review 78.62 75.583.35 74.9BERT-DK(Xu et al., 2019)77.01 73.7283.96 75.45DomBERT 76.72 73.4683.14 75.00\nTable 6: ASC in Accuracy and Macro-F1(MF1).\nSemEval, which is not a low-resource case. We\nuse this baseline to show that DomBERT can reach\ncompetitive performance.\nDomBERT is the model proposed in this paper4.\n3.4 Evaluation Metrics\nFor AE, we use F1 score. For ASC, we com-\npute both accuracy and Macro-F1 over 3 classes\nof polarities, where Macro-F1 is the major met-\nric as the imbalanced classes introduce biases on\naccuracy. Examples belonging to the conﬂict po-\nlarity are dropped as in (Tang et al., 2016). For\nE2E-ABSA, we adopt the evaluation script from(Li\net al., 2019a), which reports precision, recall, and\nF1 score. Results are as averages of 10 runs.\n3.5 Result Analysis and Discussion\nAE: In Table 5, we notice that AE is a very domain-\nspeciﬁc task. DomBERT further improves the\nperformance of BERT-DK that only uses domain-\nspeciﬁc corpus. Note that BERT-DK for restaurant\nuses 1G of restaurant corpus. But DomBERT’s tar-\nget domain corpus is just 100 MB. So DomBERT\nfurther learns domain-speciﬁc knowledge from rel-\nevant domains. Although Yelp data contain a\ngreat portion of restaurant reviews, a mixed-domain\ntraining as BERT-Review does not yield enough\ndomain-speciﬁc knowledge.\nASC: ASC is a more domain agnostic task because\nmost of the sentiment words are sharable across\nall domains (e.g., “good” and “bad”). As such,\nin Table 6, we notice ASC for restaurant is more\ndomain-speciﬁc than laptop. DomBERT is worse\nthan BERT-Review in laptop because a 20+ G can\n4We do not compare DomBERT with LMs that require\nextra (directly or indirectly) annotated data.\n1729\nLaptop RestaurantP R F1 P R F1Existing Models(Li et al., 2019a)61.2754.8957.9068.6471.0169.80(Luo et al., 2019)- - 60.35- - 72.78(He et al.) - - 58.37- - -LSTM-CRF(Lample et al., 2016)58.6150.4754.2466.1066.3066.20(Ma and Hovy, 2016)58.6651.2654.7161.5667.2664.29(Liu et al., 2018)53.3159.4056.1968.4664.4366.38BERT+Linear(Li et al., 2019b)62.1658.9060.4371.4275.2573.22BERT(Devlin et al., 2019)61.9758.5260.1168.8673.0070.78BERT-Review 65.8063.1264.3769.9275.3672.47BERT-DK(Xu et al., 2019)63.9561.1862.4571.8874.0772.88DomBERT 66.9665.5866.2172.1774.9673.45\nTable 7: Results of E2E ABSA: baselines are borrowed\nfrom (Li et al., 2019b).\nLaptop RestaurantTablets FoodBoot Shop (Men) Coffee & TeaLaptop & Netbook Computer AccessoriesBakeriesComputers & AccessoriesBarsMicrosoft Windows NightlifeElectronics WarrantiesArts & EntertainmentDesktops GroceryAntivirus & Security Venues & Event SpacesAviation Electronics LoungesWatch Repair BeerOrthopedists CasinosCompact Stereos HotelsUnlocked Cell PhonesDance ClubsPower Strips Tea RoomsMobile Broadband PubsCleaners CinemaNo-Contract Cell PhonesEvent Planning & ServicesVideo Games/PC/AccessoriesSports BarsAntivirus Specialty FoodMP3 Players & AccessoriesDesserts\nTable 8: Top-20 relevant domains\nlearn general-purpose sentiment better. BERT-DK\nis better than DomBERT because a much larger in-\ndomain corpus is more important for performance.\nE2E ABSA: By combining AE and ASC together,\nE2E ABSA exhibit more domain-speciﬁcity, as\nshown in Table 7. In this case, we can see the full\nperformance of DomBERT because it can learn\nboth general and domain-speciﬁc knowledge well.\nBERT-Review is poor probably because it focuses\non irrelevant domains such as Books.\nWe further investigate relevant domains discov-\nered by DomBERT in Table 8. The results are\ncloser to our intuition because most domains are\nvery close to laptop and restaurant, respectively.\n4 Conclusions\nWe propose DomBERT, which automatically ex-\nploits the power of training corpus from relevant\ndomains for a target domain. Experiments demon-\nstrate that DomBERT is promising for ABSA.\nAcknowledgments\nThis work was partially supported by four grants\nfrom National Science Foundation: IIS-1910424,\nIIS-1838770, III-1763325, III-1909323, and SaTC-\n1930941.\nReferences\nPeng Chen, Zhongqian Sun, Lidong Bing, and Wei\nYang. 2017. Recurrent attention network on mem-\nory for aspect sentiment analysis. In Proceedings of\nthe 2017 conference on empirical methods in natural\nlanguage processing, pages 452–461.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In International Conference on Learning Rep-\nresentations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming\nZhou, and Ke Xu. 2014. Adaptive recursive neural\nnetwork for target-dependent twitter sentiment clas-\nsiﬁcation. In Proceedings of the 52nd annual meet-\ning of the association for computational linguistics\n(volume 2: Short papers), volume 2, pages 49–54.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. An interactive multi-task learning net-\nwork for end-to-end aspect-based sentiment analy-\nsis. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics . As-\nsociation for Computational Linguistics.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2018. Effective attention modeling for\naspect-level sentiment classiﬁcation. In Proceed-\nings of the 27th International Conference on Com-\nputational Linguistics, pages 1121–1131.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative ﬁltering. In World Wide\nWeb.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining , pages 168–177.\nACM.\n1730\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020. Pre-training via paraphrasing. arXiv preprint\narXiv:2006.15020.\nXin Li, Lidong Bing, Wai Lam, and Bei Shi.\n2018. Transformation networks for target-\noriented sentiment classiﬁcation. arXiv preprint\narXiv:1805.01086.\nXin Li, Lidong Bing, Piji Li, and Wai Lam. 2019a. A\nuniﬁed model for opinion target extraction and target\nsentiment prediction. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 33,\npages 6714–6721.\nXin Li, Lidong Bing, Wenxuan Zhang, and Wai\nLam. 2019b. Exploiting bert for end-to-end\naspect-based sentiment analysis. arXiv preprint\narXiv:1910.00883.\nXin Li and Wai Lam. 2017. Deep multi-task learning\nfor aspect term extraction with memory interaction.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2886–2892.\nBing Liu. 2012. Sentiment analysis and opinion min-\ning. Synthesis lectures on human language technolo-\ngies, 5(1):1–167.\nBing Liu. 2015. Sentiment analysis: Mining opinions,\nsentiments, and emotions . Cambridge University\nPress.\nL. Liu, J. Shang, F. Xu, X. Ren, H. Gui, J. Peng, and\nJ. Han. 2018. Empower Sequence Labeling with\nTask-Aware Neural Language Model. In AAAI.\nQiao Liu, Haibin Zhang, Yifu Zeng, Ziqi Huang, and\nZufeng Wu. 2018. Content attention model for as-\npect based sentiment analysis. In Proceedings of the\n2018 World Wide Web Conference on World Wide\nWeb, pages 1023–1032. International World Wide\nWeb Conferences Steering Committee.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHuaishao Luo, Tianrui Li, Bing Liu, and Junbo Zhang.\n2019. DOER: Dual cross-shared RNN for aspect\nterm-polarity co-extraction. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 591–601, Florence, Italy.\nAssociation for Computational Linguistics.\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng\nWang. 2017. Interactive attention networks for\naspect-level sentiment classiﬁcation. arXiv preprint\narXiv:1709.00893.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1064–1074, Berlin, Ger-\nmany. Association for Computational Linguistics.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria,\nNarjes Nikzad, Meysam Chenaghlu, and Jianfeng\nGao. 2020. Deep learning based text classiﬁca-\ntion: A comprehensive review. arXiv preprint\narXiv:2004.03705.\nThien Hai Nguyen and Kiyoaki Shirai. 2015.\nPhraseRNN: Phrase recursive neural network\nfor aspect-based sentiment analysis. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing , pages 2509–2514,\nLisbon, Portugal. Association for Computational\nLinguistics.\nSinno Jialin Pan and Qiang Yang. 2009. A survey on\ntransfer learning. IEEE Transactions on knowledge\nand data engineering, 22(10):1345–1359.\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up?: sentiment classiﬁcation using\nmachine learning techniques. In Proceedings of the\nACL-02 conference on Empirical methods in natural\nlanguage processing-Volume 10, pages 79–86. Asso-\nciation for Computational Linguistics.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2.amazonaws.com/openai-assets/research-\ncovers/languageunsupervised/language understand-\ning paper.pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\n1731\nDuyu Tang, Bing Qin, and Ting Liu. 2016. Aspect\nlevel sentiment classiﬁcation with deep memory net-\nwork. arXiv preprint arXiv:1605.08900.\nYi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.\nLearning to attend via word-aspect associative fu-\nsion for aspect-based sentiment analysis. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence.\nWenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and\nXiaokui Xiao. 2016a. Recursive neural conditional\nrandom ﬁelds for aspect-based sentiment analysis.\narXiv preprint arXiv:1603.06679.\nYequan Wang, Minlie Huang, Li Zhao, et al. 2016b.\nAttention-based lstm for aspect-level sentiment clas-\nsiﬁcation. In Proceedings of the 2016 conference on\nempirical methods in natural language processing ,\npages 606–615.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2018. Dou-\nble embeddings and cnn-based sequence labeling for\naspect extraction. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics. Association for Computational Linguistics.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019. Bert\npost-training for review reading comprehension and\naspect-based sentiment analysis. In Proceedings of\nthe 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8547258377075195
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7796252965927124
    },
    {
      "name": "Natural language processing",
      "score": 0.6768977642059326
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6493986248970032
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6283060312271118
    },
    {
      "name": "Language model",
      "score": 0.618690013885498
    },
    {
      "name": "Extension (predicate logic)",
      "score": 0.5911097526550293
    },
    {
      "name": "Domain-specific language",
      "score": 0.5329319834709167
    },
    {
      "name": "Domain analysis",
      "score": 0.5028414130210876
    },
    {
      "name": "Domain model",
      "score": 0.48477286100387573
    },
    {
      "name": "Programming language",
      "score": 0.17843285202980042
    },
    {
      "name": "Domain knowledge",
      "score": 0.14705216884613037
    },
    {
      "name": "Software",
      "score": 0.06572365760803223
    },
    {
      "name": "Software development",
      "score": 0.04838976263999939
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Software construction",
      "score": 0.0
    }
  ]
}