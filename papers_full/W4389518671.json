{
  "title": "Query Rewriting in Retrieval-Augmented Large Language Models",
  "url": "https://openalex.org/W4389518671",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5012003354",
      "name": "Xinbei Ma",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A5041448669",
      "name": "Yeyun Gong",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5019259019",
      "name": "Pengcheng He",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5036050911",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A5042018181",
      "name": "Nan Duan",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385474529",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4385573075",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4302305863",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4385573021",
    "https://openalex.org/W4321650181",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W3153269634",
    "https://openalex.org/W4385572901",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4389524581",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W1191599655",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3100292568",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221161695"
  ],
  "abstract": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5303–5315\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nQuery Rewriting for Retrieval-Augmented Large Language Models\nXinbei Ma1,2,∗, Yeyun Gong3, #, †, Pengcheng He4, #, Hai Zhao1,2,†, Nan Duan3\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n3Microsoft Research Asia 4Microsoft Azure AI\nsjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,\n{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com\nAbstract\nLarge Language Models (LLMs) play pow-\nerful, black-box readers in the retrieve-then-\nread pipeline, making remarkable progress\nin knowledge-intensive tasks. This work in-\ntroduces a new framework, Rewrite-Retrieve-\nRead instead of the previous retrieve-then-read\nfor the retrieval-augmented LLMs from the per-\nspective of the query rewriting. Unlike prior\nstudies focusing on adapting either the retriever\nor the reader, our approach pays attention to\nthe adaptation of the search query itself, for\nthere is inevitably a gap between the input text\nand the needed knowledge in retrieval. We\nfirst prompt an LLM to generate the query,\nthen use a web search engine to retrieve con-\ntexts. Furthermore, to better align the query\nto the frozen modules, we propose a trainable\nscheme for our pipeline. A small language\nmodel is adopted as a trainable rewriter to cater\nto the black-box LLM reader. The rewriter is\ntrained using the feedback of the LLM reader\nby reinforcement learning. Evaluation is con-\nducted on downstream tasks, open-domain QA\nand multiple-choice QA. Experiments results\nshow consistent performance improvement, in-\ndicating that our framework is proven effective\nand scalable, and brings a new framework for\nretrieval-augmented LLM 1.\n1 Introduction\nLarge Language Models (LLMs) have shown re-\nmarkable abilities for human language processing\nand extraordinary scalability and adaptability in\nfew- or zero-shot settings.(Ouyang et al., 2022;\nBrown et al., 2020; Chowdhery et al., 2022). How-\never, the training process depends on large-scale\nhigh-quality corpora but without the perception\n∗ Work done during an internship at 3Microsoft Research\nAsia. # Equal contribution. †Corresponding author.\nThis paper was partially supported by Joint Research\nProject of Yangtze River Delta Science and Technology Inno-\nvation Community (No. 2022CSJGG1400).\n1https://github.com/xbmxb/RAG-query-rewriting\nof the real world. Thus, LLMs still have to face\nthe issue of hallucination (Yao et al., 2023; Bang\net al., 2023) and temporal misalignment (Röttger\nand Pierrehumbert, 2021; Luu et al., 2022; Jang\net al., 2022). This affects the reliability of LLMs\nand hinders wider practical application, because\nthe consistency between the LLM responses with\nthe real world needs further validation. Exist-\ning work has proved that incorporating external\nknowledge (i.e., non-parametric knowledge) with\ninternal knowledge (i.e., parametric knowledge)\ncan effectively alleviate hallucination, especially\nfor knowledge-intensive tasks. In fact, retrieval-\naugmented LLMs have been shown so effective\nthat they have been regarded as a standard solu-\ntion to alleviate the factuality drawbacks in naive\nLLM generations. Retrieval augmentation is ap-\nplied to select relative passages as external contexts\nfor the language model, which isretrieve-then-read\nframework (Lewis et al., 2020b; Karpukhin et al.,\n2020; Izacard et al., 2022). Take the open-domain\nQuestion-Answering task (open-domain QA) as\nan example, a retriever first searches for related\ndocuments for a question. Then the LLM receives\nthe question and the documents, then predicts an\nanswer.\nAs most LLMs are only accessible through infer-\nence APIs, they play the part of black-box frozen\nreaders in the pipeline. This makes previous re-\ntrieval augmentation methods that require complete\naccess (Lewis et al., 2020b; Guu et al., 2020; Izac-\nard et al., 2022) no longer feasible. Recent studies\non retrieval-augmented language models lean more\non the LLM-oriented adaptation. An idea is to train\na dense retrieval model to cater to the frozen lan-\nguage model (Shi et al., 2023). By using feedback\nfrom the LLM as a training objective, the retrieval\nmodel is tuned for better LLM input contexts. An-\nother research line focuses on the design of inter-\nactions between the retriever and the reader (Yao\net al., 2023; Khattab et al., 2022), where both the\n5303\nretriever and the reader are usually frozen. The idea\nis to trigger the emergent ability through carefully\ncrafted prompts or a sophisticated prompt pipeline.\nMultiple interactions with external knowledge al-\nlow the LLM to approach the correct answer step\nby step.\nHowever, there are still problems remaining to\nbe solved. Existing approaches overlook the adap-\ntation of the query, i.e., the input of the retrieve-\nthen-read pipeline. The retrieval query is either\noriginal from datasets or directly determined by the\nblack-box generation, thus is always fixed. How-\never, there is inevitably a gap between the input\ntext and the knowledge that is really needed to\nquery. This limits performance and places a burden\non retrieval capability enhancement and prompt\nengineering.\nIn consideration of this issue, this paper pro-\nposes Rewrite-Retrieve-Read, a new framework for\nretrieval augmentation, which can be further tuned\nfor adapting to LLMs. In front of the retriever, a\nstep of rewriting the input is added, filling the gap\nbetween the given input and retrieval need, as is\nshown in Figure 1. We adopt the off-the-shelf tool,\nan internet search engine, as the retriever, which\navoids the maintenance of the search index and\ncan access up-to-date knowledge (Lazaridou et al.,\n2022). Different from previous studies (Khattab\net al., 2022; Yao et al., 2023) that require the mem-\nory of multiple interaction rounds between the re-\ntriever and the LLM for each sample, the motiva-\ntion of our rewriting step is to clarify the retrieval\nneed from the input text.\nWe also propose a trainable scheme for our\nrewrite-retrieve-read framework (Figure 1 (c)).\nThe black-box retriever and the reader form a\nfrozen system. To further smooth the steps of\nour pipeline, we apply a small, trainable language\nmodel to perform the rewriting step, denoted as the\nrewriter. The rewriter is trained by reinforcement\nlearning using the LLM performance as a reward,\nlearning to adapt the retrieval query to improve the\nreader on downstream tasks.\nOur proposed methods are evaluated on\nknowledge-intensive downstream tasks including\nopen-domain QA (HotpoQA (Yang et al., 2018),\nAmbigNQ (Min et al., 2020), PopQA (Mallen\net al., 2022)) and multiple choice QA (MMLU\n(Hendrycks et al., 2021)). The experiments are\nimplemented on T5-large (Raffel et al., 2020) as\nthe rewriter, ChatGPT (Ouyang et al., 2022) and\nVicuna-13B (Chiang et al., 2023) as the LLM\nreader. The results show that query rewriting con-\nsistently improves the retrieve-augmented LLM\nperformance. The results also indicate that the\nsmaller language model can be competent for query\nrewriting.\nTo sum up, our proposed novel retrieval-\naugmentation method, rewrite-retrieve-read is the\nfirst framework where the input text is adapted for\nthe frozen retriever and LLM reader. We introduce\na tuneable scheme with a small, trainable model,\nachieving performance gains with less resource\nconsumption.\n2 Related Work\n2.1 Retrieval Augmentation\nLanguage models require external knowledge to al-\nleviate the factuality drawbacks. Retrieval augmen-\ntation has been regarded as the standard effective\nsolution. With a retrieval module, related passages\nare provided to the language model as the context\nof the original input. Thus factual information like\ncommon sense or real-time news helps with output\nprediction through contextualized reading compre-\nhension.\nEarlier studies use sparse retriever (Chen et al.,\n2017) or dense retriever (Karpukhin et al., 2020)\nin front of a pre-trained language model (PrLM).\nThe neural retriever and reader are both PrLMs\nof trainable size like BERT (Devlin et al., 2019)\nor BART (Lewis et al., 2020a). Hence, the whole\nretrieve-then-reader framework is a tuneable end-\nto-end system, where the retrieved contexts can\nbe regarded as the intermediate results (Karpukhin\net al., 2020; Lewis et al., 2020b). Approaches to\nsmooth the two-step framework are proposed to op-\ntimize the retrieval and the reading comprehension\n(Sachan et al., 2021; Lee et al., 2022; Jiang et al.,\n2022). More recently, retrieval remains a powerful\nenhancement as the size of models and data scales\nrapidly (Mallen et al., 2022; Shi et al., 2023; Brown\net al., 2020). On the other hand, retrieval enhance-\nment can compensate for the shortfall in parameter\nsize, compared to large-scale language models. For\nexample, by jointly training the retriever and the\nreader, Atlas (Izacard et al., 2022) shows few-shot\nperformance on par with 540B PalM (Chowdhery\net al., 2022) but be of 50×smaller size.\nThe Internet as a knowledge baseMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\n5304\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search \nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput OutputReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Galesville, Wisconsin, U.S...... \ndirector\nRewriter\nRetriever\nBlack-box LLM\nReader\nBlack-box LLM\nReader\n(a) Retrieve-then-read    (b)Rewrite-retrieve-read                  (c) Trainable rewrite-retrieve-read    \nBlack-box LLM\nReader\nWeb Search \nRetriever\nRewriter\nSmall PrLM\nExample\nQuery: Elia Kazan profession \nElia Kazan was an American film and \ntheatre director, producer, \nscreenwriter and actor, described  ......\nCorrect (reader      )\nHit (retriever      )\n                              \n      \nFigure 1: Overview of our proposed pipeline. From left to right, we show (a) standard retrieve-then-read method,\n(b) LLM as a query rewriter for our rewrite-retrieve-read pipeline, and (c) our pipeline with a trainable rewriter.\nexternal knowledge. Komeili et al. (2022) use an\ninternet search for relevant information based on\nthe dialogue history to perform dialogue response\ngeneration. SeeKeR (Shuster et al., 2022) use a\nsingle Transformer to iteratively perform search\nquery generation, then knowledge extraction for\ndialogue generation and sentence completion. For\nlarge-scale models, web search still shows effec-\ntive for knowledge augmentation (Lazaridou et al.,\n2022), fact-checking (Menick et al., 2022), and\nLLM agent enhancement (Yao et al., 2023).\n2.2 Cooperation with Black-box LLMs\nLarge Language Models, such as ChatGPT\n(Ouyang et al., 2022), Codex (Chen et al., 2021),\nPaLM (Chowdhery et al., 2022), emerge impres-\nsive natural language processing ability as well as\nremarkable scalability. This leads to a tendency\nto embrace LLMs on a wide range of NLP tasks.\nHowever, LLMs are only accessible as a black box\nin most cases, which is because (i) Some like Chat-\nGPT are not open-source and kept private; (ii) The\nlarge parameter scale requires computational re-\nsources that are not always affordable to users. This\nconstraint means nothing is available except input\nand output texts.\nExisting studies have proved that LLMs’ abili-\nties can be better leveraged by carefully designed\ninteraction methods. GenRead (Yu et al., 2023)\nprompts an LLM to generate context instead of\ndeploying a retriever, showing that LLMs can re-\ntrieve internal knowledge by prompting. ReAct\n(Yao et al., 2023) and Self-Ask (Press et al., 2022)\ncombines the Chain-of-Thought (CoT) (Wei et al.,\n2022; Wang et al., 2022) and inter-actions with web\nAPIs. Only relying on prompt construction, Re-\nAct provides novel baselines for interactive tasks.\nDemonstrate–Search–Predict (DSP) (Khattab et al.,\n2022) defines a sophisticated pipeline between an\nLLM and a retriever. Unlike ReAct, DSP integrates\nprompts for demonstration bootstrap besides multi-\nhop breakdown and retrieval.\nDespite the promising performance in the zero or\nfew-shot setting, the behavior of LLMs sometimes\nneeds adjustments. A feasible approach is to ap-\npend trainable small models in front of or after the\nLLM. The small models, as a part of the parameters\nof the system, can be fine-tuned for optimization.\nRePlug (Shi et al., 2023) is proposed to fine-tune a\ndense retriever for the frozen LLM in the retrieve-\nthen-read pipeline. The retriever is trained under\nthe LLM’s supervision to retrieve documents that\nare suitable for the LLM. With the same purpose,\nDirectional Stimulus Prompting (Li et al., 2023)\ndeploys a small model to provide the LLM with\nstimulus (e.g., keywords for summarization, or di-\nalogue actions for response generation), which is\nupdated according to the LLM reward.\nDifferent from the inspiring work mentioned\nabove, our proposed pipeline contains a query\nrewriting step in front of the retrieve-then-read\nmodule. We further propose a trainable scheme\nwith a small rewriting model, which is a novel\nenhancement for retrieval-augmented LLM by re-\n5305\nconstructing the search query.\n3 Methodology\nWe present Rewrite-Retrieve-Read, a pipeline that\nimproves the retrieval-augmented LLM from the\nperspective of query rewriting. Figure 1 shows an\noverview. This section first introduces the pipeline\nframework in section 3.1, then the trainable scheme\nin section 3.2.\n3.1 Rewrite-Retrieve-Read\nA task with retrieval augmentation can be de-\nnoted as follows. Given a dataset of a knowledge-\nintensive task (e.g., open-domain QA), D =\n{(x,y)i},i = 0,1,2,...,N , x (e.g., a question)\nis the input to the pipeline, yis the expected output\n(e.g., the correct answer). Our pipeline consists of\nthree steps. (i) Query rewrite: generate a query ˜x\nfor required knowledge based on the original input\nx. (ii) Retrieve: search for related context,doc. (iii)\nRead: comprehend the input along with contexts\n[doc,x] and predict the output ˆy.\nA straightforward but effective method is to ask\nan LLM to rewrite queries to search for informa-\ntion that is potentially needed. We use a few-shot\nprompt to encourage the LLM to think, and the\noutput can be none, one or more queries to search.\n3.2 Trainable Scheme\nBesides, total reliance on a frozen LLM has shown\nsome drawbacks. Reasoning errors or invalid\nsearch hinders the performance (Yao et al., 2023;\nBehnamGhader et al., 2022). On the other hand,\nretrieved knowledge may sometimes mislead and\ncompromise the language model (Mallen et al.,\n2022). To better align to the frozen modules, it is\nfeasible to add a trainable model and adapt it by\ntaking the LLM reader feedback as a reward.\nBased on our framework, we further propose to\nutilize a trainable small language model to take\nover the rewriting step, as is shown in the right\npart of Figure 1. The trainable model is initial-\nized with the pre-trained T5-large (770M) (Raffel\net al., 2020), denoted astrainable rewriter, Gθ. The\nrewriter is first trained on pseudo data to warm up\n(§3.2.1), then continually trained by reinforcement\nlearning (§3.2.2).\n3.2.1 Rewriter Warm-up\nThe task, query rewriting, is quite different from\nthe pre-training objective of sequence-to-sequence\ngenerative models like T5. First, we construct a\npseudo dataset for the query rewriting task. In-\nspired by recent distillation methods (Hsieh et al.,\n2023; Ho et al., 2022), we prompt the LLM to\nrewrite the original questions xin the training set\nand collect the generated queries ˜xas pseudo la-\nbels. The collected samples are then filtered: Those\nthat get correct predictions from the LLM reader\nare selected into the warm-up dataset, denoted as\nDTrain = {(x,˜x)|ˆy= y}. The rewriter Gθ is fine-\ntuned on DTrain with the standard log-likelihood\nas the training objective, denoted as\nLwarm = −\n∑\nt\nlogpθ( ˆ˜xt | ˜x<t, x ). (1)\nThe rewriter model after warm-up shows mod-\nest performance, which depends on the pseudo\ndata quality and rewriter capability. Highly relying\non the human-written prompt line, ˜xcan be sub-\noptimal. The relatively small scale of the rewriter\nsize is also a limitation of the performance after the\nwarm-up. Then we turn to reinforcement learning\nto align the rewriter to the following retriever and\nLLM reader.\n3.2.2 Reinforcement Learning\nTo further fine-tune the rewriter to cater to the LLM\nreader, we adopt a policy gradient reinforcement\nlearning framework.\nTask Formulation In the context of reinforce-\nment learning, the rewriter optimization is for-\nmulated as a Markov Decision Process 5-tuple\n⟨S,A,P,R,γ ⟩. (i) The state space Sis a finite set\nlimited by the vocabulary and the sequence length.\n(ii) The action space Ais equals to the vocabulary.\n(iii) The transition probability P is determined by\nthe policy network, which is the rewriter model\nGθ. (iv) The reward function R gives a reward\nvalue that depends on the current state. The pol-\nicy gradient is derived from rewards, used as the\ntraining objective. (v) γdenotes the discount fac-\ntor. More specifically, the rewriter Gθ after the\nwarm-up is the initial policy model π0. At each\nstep t, the action at is to generate the next token\nˆ˜xt based on the observation of the present state,\nst = [x,ˆ˜x<t]. When the generation is stopped by\nthe End-Of-Sentence token, one episode is ended.\nAfter finishing the retrieval and reading, a reward\nis computed by evaluating the final output, i.e., a\nscore for the LLM reader prediction.\nPolicy Optimization We adopt Proximal Policy\nOptimization (PPO) (Schulman et al., 2017), fol-\nlowing (Ramamurthy et al., 2022). Maximization\n5306\nof the expectation of the reward Ris formulated as\nmax\nθ\nEˆ˜x∼pθ(·|x)[R(x,ˆ˜x)],\nmax\nθ\nE(st,at)∼πθ′ [min{kt,θAθ′\n(st,at) ;\nclip (kt,θ,1 −ε,1 +ε) Aθ′\n(st,at)}],\nkt,θ = pθ(at |st)\npθ′ (at |st),\n(2)\nwhere θ′is the temporarily fixed policy for sam-\npling and θ is updated. Adenotes the advantage\nfunction, which is formulated based on the estima-\ntion of value network Vϕ. The value network Vϕ is\ninitialized from the policy network π0. The formu-\nlation follows Generalized Advantage Estimation\n(GAE) (Schulman et al., 2015).\nδt = R(st,at) +Vϕ(st+1) −Vϕ(st) ,\nˆAθ\nt (st,at) =\n∞∑\nt′=0\nλt′\nδt+t′ , (3)\nwhere λis the bias-variance trade-off parameter.\nThe reward function Rreflects the quality of the\ngenerated queries, which needs to be consistent\nwith the final evaluation of the task. ˆ˜xis fed to the\nretriever and the reader for a final prediction ˆy. A\npart of the reward function is the measures of ˆy\ncompared to the golden label y(e.g., exact match\nand F1 of the predicted answers), denoted as Rlm.\nBesides, a KL-divergence regularization is added\nto prevent the model from deviating too far from\nthe initialization (Ramamurthy et al., 2022; Ziegler\net al., 2019).\nR(st,at) =Rlm(ˆ˜x,y) −βKL (πθ∥π0) . (4)\nThe final loss function is composed of policy loss\nand value loss.\nLθ = − 1\n|S|T\n∑\nτ∈S\nT∑\nt=0\nmin(kt,θAθ′\n,clip Aθ′\n),\nLϕ = 1\n|S|T\n∑\nτ∈S\nT∑\nt=0\n(Vϕ(st) −Rt)2 ,\nLppo = Lθ + λvLϕ.\n(5)\nHere, Sdenotes the sampled set, and T is for step\nnumbers.\n4 Implementation\nRewriter For the frozen pipeline in §3.1, we\nprompt an LLM to rewrite the query with few-shot\nin-context learning (Brown et al., 2020; Min et al.,\n2022). Our prompt follows the formulation of [in-\nstruction, demonstrations, input], where the input\nis x. The instruction is straightforward and demon-\nstrations are 1-3 random examples from training\nsets and are kept constant across all runs, mainly\nfor the task-specific output format illustration, i.e.,\na short phrase as an answer for HotpotQA, and an\noption as an answer for MMLU. For the training\nscheme in §3.2, we fine-tuning a T5 as the rewriter.\nRetriever We use the Bing search engine as the\nretriever. It requires no candidate index construc-\ntion like a dense retriever, nor candidates like a\ntextbook. But it allows for a wide knowledge scope\nand up-to-time factuality. With Bing API, the re-\ntrieval is performed in two approaches. (i) For all\nretrieved web pages, we concatenate the snippets\nthat are related sentences selected by Bing. This\nmethod is similar to using a search engine in a\nbrowser, input a query and press Enter, then col-\nlect the texts shown on the search result page. (ii)\nFor retrieved web pages, we request the URLs and\nparser to get all the texts. This is similar to clicking\non items on the search result page. Then we use\nBM25 to keep those with higher relevance scores\nwith the query, reducing the document length.\nReader The reader is a frozen LLM, where we\nadopt ChatGPT (gpt-3.5-turbo) and Vicuna-13B.\nIt performs reading comprehension and prediction\nwith few-shot in-context learning. In our prompt,\nfollowing the brief instruction and the demonstra-\ntions, the input is xor [doc,ˆ˜x] with retrieval aug-\nmentation.\nIt has been proved that both the phrasing of\nprompt lines (Zhang et al., 2023a) and the selection\nof demonstrations show effects on the in-context\nlearning performance (Su et al., 2022; Zhang et al.,\n2023b). As it is not the focus of this work, we pay\nno more attention to prompt editing.\n5 Experiments\n5.1 Task Settings\n5.1.1 Open-domain QA\nThree open-domain QA datasets are used for evalu-\nation. (i) HotPotQA (Yang et al., 2018) consists of\ncomplex questions that require multi-hop reason-\ning. We evaluate the full test set. (ii) AmbigNQ\n(Min et al., 2020) provides a disambiguated version\nof Natural Questions (NQ) (Kwiatkowski et al.,\n2019). For ambiguous questions in NQ, minimal\nconstraints are added to break it into several similar\n5307\nDirect prompt\nAnswer the question in the following format, end the answer with ’**’. {demonstration} Question: { x} Answer:\nReader prompt in retrieval-augment pipelines\nAnswer the question in the following format, end the answer with ’**’. {demonstration} Question: { doc } { x}\nAnswer:\nPrompts for LLM as a frozen rewriter\nOpen-domain QA: Think step by step to answer this question, and provide search engine queries for knowledge\nthat you need. Split the queries with ’;’ and end the queries with ’**’. {demonstration} Question: { x} Answer:\nMultiple choice QA: Provide a better search query for web search engine to answer the given question, end the\nqueries with ’**’. {demonstration} Question: { x} Answer:\nTable 1: Prompt lines used for the LLMs.\nbut specific questions. The first 1000 samples are\nevaluated in the test set. (iii) PopQA (Mallen et al.,\n2022) includes long-tail distributions as it contains\nmore low-popularity knowledge than other popular\nQA tasks. We split the dataset into 13k for training\nand 714 for testing.\nOpen-domain QA benchmarks are sets of\nquestion-answer pairs denoted as {(q,a)i}. We use\nChatGPT for both the reader and the frozen rewriter.\nThe evaluation metrics are Exact Match (EM) and\nF1 scores. For the reward function in RL, we use\nan indicator to reward if the retrieved content hits\nthe answer and penalize if misses the answer, de-\nnoted as Hit. The total reward is a weighted sum\nof EM, F1, and Hit.\nHit =\n{\n1 a in doc,\n−1 else\nRlm = EM + λfF1 + λhHit.\n(6)\n5.1.2 Multiple-choice QA\nFor multiple-choice QA, our evaluation is con-\nducted on Massive Multi-task Language Under-\nstanding (MMLU) (Hendrycks et al., 2021), an\nexam question dataset including 4 categories: Hu-\nmanities, STEM, Social Sciences, and Other. Each\ncategory is split into 80% for the training set and\n20% for the test set.\nMultiple-choice QA can be formulated as\n{(q′,a)i}, where q′= [q,c0,c1,c2,c3]. cdenotes\nthe options, generally there are four for each ques-\ntion. The retrieved documents that are included\nin the officially provided contaminated lists are\nignored. The questions with options are rewritten\ninto search queries. The answer is one option. EM\nis reported as metrics and used for the reward.\nRlm = EM. (7)\nWe use ChatGPT as a frozen rewriter and the reader.\nWe also use Vicuna-13B as the reader for evalua-\ntion due to the rate limit issue of ChatGPT. More\ninformation on datasets and training setup are pre-\nsented in the appendix.\n5.2 Baselines\nThe following settings are implemented to eval-\nuate and support our methods. (i) Direct: The\nstandard in-context learning without any augmen-\ntations. (ii) Retrieve-then-read: The standard\nretrieval-augmented method. Retrieved documents\nare concatenated with the question. (iii) LLM\nas a frozen rewriter: As is introduced in §3.1,\nwe prompt a frozen LLM to reason and generate\nqueries by few-shot in-context learning. (iv) Train-\nable rewriter: Applying the fine-tuned rewriter,\nthe output queries are used by the retriever and the\nreader. Table 1 presents prompt line forms. Please\nnote that the prompts for prediction are kept the\nsame for each task.\n5.3 Results\nExperimental results on open-domain QA are re-\nported in Table 2. For the three datasets, query\nrewriting consistently brings performance gain\nwith both a frozen rewriter and a trainable rewriter.\nOn AmbigNQ and PopQA, the standard retrieval\naugments the reader, indicating useful external\nknowledge is retrieved. On HotpotQA, the stan-\ndard retrieval hurts the reader. This shows that\nusing complex questions as queries cannot com-\npensate for the parametric knowledge, but bring\nnoises instead (Mallen et al., 2022). This suggests\nthat multi-hop questions are not suitable queries\nfor the web search engine. The scores increase by\nadding the rewriting step. On PopQA, our trainable\nrewriter surpasses standard retrieval while being\ninferior to the LLM rewriter. This indicates that the\n5308\ndistillation of query rewriting is sub-optimal.\nThe scores on multiple-choice QA are presented\nin Table 3. With ChatGPT as a reader, it can be ob-\nserved that query rewriting improves the scores in\nmost of the settings, except for the social sciences\ncategory. With Vicuna as a reader, our method\nachieves more gains on the four categories com-\npared to ChatGPT. This agrees with the intuition\nthat a more powerful reader has more parametric\nmemories, thus more difficult to compensate with\nexternal knowledge.\nModel EM F 1\nHotpotQA\nDirect 32.36 43.05\nRetrieve-then-read 30.47 41.34\nLLM rewriter 32.80 43.85\nTrainable rewriter 34.38 45.97\nAmbigNQ\nDirect 42.10 53.05\nRetrieve-then-read 45.80 58.50\nLLM rewriter 46.40 58.74\nTrainable rewriter 47.80 60.71\nPopQA\nDirect 41.94 44.61\nRetrieve-then-read 43.20 47.53\nLLM rewriter 46.00 49.74\nTrainable rewriter 45.72 49.51\nTable 2: Metrics of open-domain QA.\nMMLU EM\nHuman. STEM Other Social\nChatGPT\nDirect 75.6 58.8 69.0 71.6\nRetrieve-then-read 76.7 63.3 70.0 78.2\nLLM rewriter 77.0 63.5 72.6 76.4\nVicuna-13B\nDirect 39.8 34.9 50.2 46.6\nRetrieve-then-read 40.2 39.8 55.2 50.6\nLLM rewriter 42.0 41.5 57.1 52.2\nTrainable rewriter 43.2 40.9 59.3 51.2\nTable 3: Metrics of multiple choice QA.\n6 Analysis\n6.1 Training Process\nThe training process includes two stages, warm-up\nand reinforcement learning. This section shows\nthe validation scores of the three open-domain QA\ndatasets for further analysis. Figure 2 presents\nthe metric scores through training iterations in the\nprocess of reinforcement learning. As the rewriting\nmodels have been warmed up on the pseudo data\nbefore RL, scores at “0 iteration” denote the ability\nacquired from the warm-up training.\n0 5 10 15 20 25\nInteration\n30\n31\n32\n33\n34EM\n(a)HotpotQA\nRetrieve-then-read\nLLM rewriter\n41\n42\n43\n44\n45\nF1\n0 2 4 6 8 10\nInteration\n44\n45\n46\n47\n48EM\n(b)AmbigNQ\nRetrieve-then-read\nLLM rewriter\n57\n58\n59\n60\nF1\n0 2 4 6 8 10 12\nInteration\n40\n41\n42\n43\n44\n45\n46EM\n(c)PopQA\nRetrieve-then-read\nLLM rewriter 43\n44\n45\n46\n47\n48\n49\nF1\nFigure 2: Reinforcement learning validation scores of\n(a)HotpotQA, (b)AmbigNQ, and (c)PopQA. The solid\nlines show EM (red) and F1 (blue) numbers through\ntraining iterations. The dashed lines are EM scores\nof the standard retrieve-then-read method (orange) and\nretrieval with an LLM as the rewriter (green).\nIt can be observed that the curves show upward\ntrends with some fluctuations on all the datasets. (i)\nFor multi-hop questions in HotpotQA, the standard\nretrieval is relatively weaker. Complex questions\ncan be not specific search queries and show a larger\ngap from rewritten queries, i.e., the green and red\nlines. (ii) On AmbigNQ and PopQA, our method\nsurpasses the baselines after several iterations (3\nor 4). This indicates that the RL training stage can\ncompensate for the insufficiency of the distillation\non the pseudo data during warm-up training. (iii)\nIn particular, on PopQA, the trainable rewriter re-\nmains inferior to the LLM rewriter. This can be\nexplained as the dataset is constructed for adaptive\nretrieval (Mallen et al., 2022), which only uses re-\ntrieval where it helps to avoid harmful redundant\nretrieval. Thus, “None” is a possible query that\nmeans no retrieval. This causes more complex-\nity and uncertainty. LLM rewriter knows better\nwhen the retrieval is needed for itself as a reader,\nalthough the rewriting step is not concatenated as\n5309\nthe input context of the reader.\nWe calculate the performance of query “None”.\nThe questions that can be correctly answered with-\nout retrieval (i.e., the “Direct” method) are those\nsamples that need no more context. Comparing this\nretrieval-free set with those that are rewritten to\nbe“None” query, the F1 score of the LLM rewriter\nis 71.9% and the T5 rewriter score is 67.1%. If\nwe consider the questions that can be correctly an-\nswered without retrieval but go wrong with retrieval\nas the retrieval-free set, the F1 scores are 78.7% for\nLLM rewriter and 77.4% for T5.\nModel EM F 1 Hit ratio\nNo retrieval 42.10 53.05 –\nUpper bound 58.40 69.45 100\nRetrieve-then-read\nw/ snippet 38.70 50.50 61.1\nw/ BM25 45.80 58.50 76.4\nLLM rewriter\nw/ snippet 39.80 52.64 63.5\nw/ BM25 46.40 58.74 77.5\nTrainable rewriter\nw/ BM252 47.80 60.71 82.2\nTable 4: Retrieval analysis on AmbigNQ.\n6.2 Retrieval Result\nOur proposed method is a pipeline framework, in-\nstead of an end-to-end system. The query rewrit-\ning first affects the retrieved context, then the con-\ntext makes a difference to the output of the reader.\nHence, QA metrics are indirect measurements. We\ntake a closer look at the retrieved context and the\nreader capability through the retrieval metric, hit\nratio. After text normalization, the hit rate is com-\nputed to measure whether the retrieved context con-\ntains the correct answers.\nTable 4 shows the scores on AmbigNQ. The\nscores in the second line are computed on a selec-\ntion of the samples whose retrieved contexts hit\ncorrect answers (under the standard retrieve-then-\nread setting). The scores show the approximate\nupper bound ability of the reader with retrieval aug-\nmentation, abbreviated as the “upper bound” score.\nThe effectiveness of retrieval is proved compared\nto the no retrieval setting (the first line). For each\nretrieval method, two settings are presented: (i)\ncollecting Bing snippets, (ii) selecting from URLs\nby BM25. The metrics show that content selection\nwith BM25 recalls better documents than snippets,\n2Our trainable rewriter is adapted to the retriever using\nBM25 during RL training. Using the output queries of the test\nset after training, the snippet hit rate is 73.4%.\nExample 1: multi-hop question\nQ0: The youngest daughter of Lady Mary-Gaye \n       Curzon stars with Douglas Smith and \n       Lucien Laviscount in what 2017 film?\nQ1: the youngest daughter of Lady Mary-Gaye\n       Curzon; 2017 film stars Douglas Smith \n       and Lucien Laviscount\nQ2: Lady Mary-Gaye Curzon youngest daughter\n       2017 film with Douglas Smith and Lucien \n       Laviscount \nExample 2:\nQ1: movie \"All Star\" 2000\nExample 3: multiple choice\nHit Correct\nQ0: A car-manufacturing factory is considering \n       a new site for its next plant. Which of the \n       following would community planners be \n       most concerned with before allowing the \n       plant to be built? Options: A. The amount \n       of materials stored in the plant B. The hours\n       of operations of the new plant C. The effect \n       the plant will have on the environment D. \n       The work environment for the employees\n       at the plant\nQ1: What would community planners be most \n       concerned  with before allowing a car-\n       manufacturing factory to be built?\nQ2: 2000 movie \"All Star\" song\nQ0: What 2000 movie does the song \"All Star\"\n       appear in?\nFigure 3: Examples for intuitive illustration. Q0 denotes\noriginal input, Q1 is from the LLM rewriter, and Q2 is\nfrom the trained T5 rewriter. Hit means retriever recall\nthe answer, while Correct is for the reader output.\nwhile query rewriting makes progress on both set-\ntings. We also observed that the improvement in\nthe hit rate of the retriever is more significant than\nthe improvement in the reader. This is consistent\nwith the findings in related search (Mallen et al.,\n2022; Liu et al., 2023).\n6.3 Case Study\nTo intuitively show how the query rewriting makes\na difference in the retrieved contexts and prediction\nperformance, we present examples in Figure 3 to\ncompare the original questions and the queries. In\nexample 1, the original question asks for a film that\nthe youngest daughter of Lady Mary-Gaye Curzon\nco-stars with two certain actors. Both query 1 and\nquery 2 put the keyword film forward, closely fol-\nlowing the youngest daughter of Lady Mary-Gaye\nCurzon. With both, the actress Charlotte Calthorpe\nand her movie information can be retrieved and\nthe answer is included. The second is an example\nwhere the query from the LLM rewriter failed but\n5310\nthe query from T5 gets the correct answer. The\nnumber 2000 is misunderstood in query 1, while\nquery 2 keeps 200 movie together, avoiding mean-\ningless retrieval. Example 3 is for multiple choice.\nThe query simplifies the background and enhances\nthe keyword community planner. The retrieve con-\ntexts are mainly about Introduction to Community\nPlanning where the answer environment appears\nseveral times.\n7 Conclusion\nThis paper introduces the Rewrite-Retrieve-Read\npipeline, where a query rewriting step is added\nfor the retrieval-augmented LLM. This approach\nis applicable for adopting a frozen large language\nmodel as the reader and a real-time web search\nengine as the retriever. Further, we propose to ap-\nply a tuneable small language model the rewriter,\nwhich can be trained to cater to the frozen retriever\nand reader. The training implementation consists\nof two stages, warm-up and reinforcement learn-\ning. Evaluation and analyses on open-domain QA\nand multiple-choice QA show the effectiveness\nof query rewriting. Our work proposes a novel\nretrieval-augmented black-box LLM framework,\nproves that the retrieval augmentation can be en-\nhanced from the aspect of query rewriting, and\nprovides a new method for integrating trainable\nmodules into black-box LLMs.\nLimitations\nWe acknowledge the limitations of this work. (i)\nThere is still a trade-off between generalization and\nspecialization among downstream tasks. Adding\na training process, the scalability to direct transfer\nis compromised, compared to few-shot in-context\nlearning. (ii) The research line of LLM agent has\nshown impressive performance but relies on mul-\ntiple calls to the LLM for each sample (Khattab\net al., 2022; Yao et al., 2023), where the LLM\nplays as an agent to flexibly call the retriever multi-\nple times, reads the context in earlier hops, and\ngenerates follow-up questions. Different from\nthese studies, our motivation is to enhance the one-\nturn retriever-then-read framework with a trainable\nquery rewriter. (iii) Using a web search engine as\nthe retriever also leads to some limitations. Neu-\nral dense retrievers that are based on professional,\nfiltered knowledge bases may potentially achieve\nbetter and controllable retrieval. More discussion\nis included in the appendix.\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reason-\ning, hallucination, and interactivity. arXiv preprint\narXiv:2302.04023.\nParishad BehnamGhader, Santiago Miret, and Siva\nReddy. 2022. Can retriever-augmented language\nmodels reason? the blame game between the re-\ntriever and the language model. arXiv preprint\narXiv:2212.09146.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Association for Computational\nLinguistics (ACL).\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\n5311\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR).\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\nLarge language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alexander J. Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister.\n2023. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller\nmodel sizes. ArXiv, abs/2305.02301.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Few-shot Learning with Retrieval Aug-\nmented Language Models.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. Temporalwiki: A lifelong bench-\nmark for training and evaluating ever-evolving lan-\nguage models.\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\nZhiruo Wang, Jamie Callan, and Graham Neubig.\n2022. Retrieval as attention: End-to-end learning of\nretrieval and reading within a single transformer. In\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), Abu Dhabi, UAE.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive NLP. arXiv preprint\narXiv:2212.14024.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115.\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\njape, Christopher Manning, and Kyoung-Gu Woo.\n2022. You only need one model for open-domain\nquestion answering. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3047–3060, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nZekun Li, Baolin Peng, Pengcheng He, Michel Galley,\nJianfeng Gao, and Xifeng Yan. 2023. Guiding large\nlanguage models via directional stimulus prompting.\narXiv preprint arXiv:2302.11520.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How lan-\nguage models use long contexts. arXiv preprint\narXiv:2307.03172.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A. Smith. 2022. Time\nwaits for no one! analysis and challenges of tem-\nporal misalignment. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 5944–5958, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\n5312\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nHannaneh Hajishirzi, and Daniel Khashabi. 2022.\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and non-\nparametric memories. arXiv preprint.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. arXiv preprint arXiv:2203.11147.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In EMNLP.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\nArXiv preprint, abs/2307.16789.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nRajkumar Ramamurthy, Prithviraj Ammanabrolu,\nKianté Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n2022. Is reinforcement learning (not) for natural\nlanguage processing?: Benchmarks, baselines, and\nbuilding blocks for natural language policy optimiza-\ntion.\nPaul Röttger and Janet Pierrehumbert. 2021. Temporal\nadaptation of BERT and performance on downstream\ndocument classification: Insights from social media.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2400–2412, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 25968–25981.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael\nJordan, and Pieter Abbeel. 2015. High-dimensional\ncontinuous control using generalized advantage esti-\nmation. arXiv preprint arXiv:1506.02438.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston.\n2022. Language models that seek for knowledge:\nModular search & generation for dialogue and\nprompt completion. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 373–393. Association for Computational Lin-\nguistics.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. 2022. Selec-\ntive annotation makes language models better few-\nshot learners. arXiv preprint arXiv:2209.01975.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. CoRR, abs/2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\n5313\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In International Confer-\nence for Learning Representation (ICLR).\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\nurmans, and Joseph E Gonzalez. 2023a. Tempera:\nTest-time prompt editing via reinforcement learning.\nIn The Eleventh International Conference on Learn-\ning Representations.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompt-\ning in large language models. In The Eleventh In-\nternational Conference on Learning Representations\n(ICLR 2023).\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences. arXiv\npreprint arXiv:1909.08593.\nA Warm-up Dataset\nFor the warm-up training of the tuneable rewriter,\nwe construct a pseudo dataset for the query rewrit-\ning task. For benchmarks that provide official train-\ning and test splits (HotpotQA and AmbigNQ), we\nuse the whole training set. For those that have no\nofficial splits (PopQA and MMLU), we randomly\nsplit the full dataset. In detail, PopQA contains 16\ntypes of questions, thus split into 13k for training\nand 714 for testing following stratified sampling.\nFor MMLU, each of the 4 categories is randomly\nsplit into 80% for the training set and 20% for\nthe test set. Then the training sets of each bench-\nmark are used to derive the pseudo dataset for the\nquery rewriting, i.e., DTrain = {(x,˜x)|ˆy = y}.\nWe present the statistics of the splits and warm-up\ndataset in Table 5.\nB Setup Details\nFor warm-up, we train the T5-large with 3e-5 learn-\ning rate, {16, 20} batch size, for {6,8,12} epochs.\nFor reinforcement learning, we set the sampling\nTask Training Set Warm-up Test Set\nHotpotQA 90.4k 37.5k 7.4k\nAmbigNQ 19.4k 8.6k 1k\nPopQA 13.0k 6.0k 0.7k\nHumanities 3.8k 1.5k 0.9k\nSTEM 2.4k 0.9k 0.6k\nOther 2.6k 1.3k 0.6k\nSocial Science 2.4k 1.3k 0.6k\nTable 5: Metrics of multiple choice QA.\nsteps to 5120, 10 threads, 512 steps for each. After\nsampling, the policy network is trained for {2,3,4}\nepochs, with learning rate as 2e-6 and batch size\nas {8,16}. λf and λh are 1.0. β in Eq. 4 is dy-\nnamically adapted according to Ramamurthy et al.\n(2022); Ziegler et al. (2019),\net = clip\n(KL (π∥π0) −KLtarget\nKLtarget\n,−0.2,0.2\n)\n,\nβt+1 = βt(1 + Kβet) ,\nwhere KLtarget is set to 0.2, K β is set to 0.1. β0\nis initialized to be 0.001. The generation strat-\negy follows the 4-beam search and returns the one\nsequence. In the implementation of the BM25-\nbased retriever, the textboxes from searched URLs\nare parsed from HTML code. We compute BM25\nscores between the paragraph from each textbox\nand the query following the scikit-learn package,\nthen keep those with higher scores until the re-\nserved context reaches a max length. In reinforce-\nment learning, the results of AmbigNQ are with\nthe BM25 method, while others use snippets as\ncontext.\nC Web Search: Tool Use\nOur proposed pipeline integrates an externally built\nweb search engine as the retriever module. We\npresent more discussion on the advantages and dis-\nadvantages here.\nThe usage of external tools expands the abil-\nity boundary of language models, compensating\nfor the parametric knowledge, and grounding the\ncapabilities of language models to interact with en-\nvironments (Qin et al., 2023; Schick et al., 2023).\nRecent studies show a trend to leverage plug-and-\nplay tools like search engines to enhance language\nagents (Lazaridou et al., 2022; Menick et al., 2022;\nShuster et al., 2022; Shen et al., 2023). Search\nengine APIs are well-developed retrievers, saving\nefforts to build and maintain another retriever, like\na Contriever. Accessible to the whole Internet, the\nweb search retrieves from a wide-range, up-to-date\n5314\nknowledge base. The temporal misalignment prob-\nlem on a fixed candidate database can be alleviated.\nOn the other hand, web search APIs are commer-\ncial products requiring subscriptions. Also, the vast\namount of knowledge on the web can be difficult\nto control. The retrieved context from the Internet\ncan be occasionally inconsistent, redundant, and\ntoxic, which hinders the LLM reader.\nBeyond retrieval augmentation, in a general\nscope, other tools called by LLMs, like code in-\nterpreters, online models, and expert applications,\nare all similar to search engines, without trainable\nparameters to optimize. There could be a gap be-\ntween the LM and these tools. This paper proposes\nan idea to align them through a trainable small\nmodel.\n5315",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8992891311645508
    },
    {
      "name": "Pipeline (software)",
      "score": 0.7047390937805176
    },
    {
      "name": "Information retrieval",
      "score": 0.6312229633331299
    },
    {
      "name": "Rewriting",
      "score": 0.6246336698532104
    },
    {
      "name": "Query expansion",
      "score": 0.6107919812202454
    },
    {
      "name": "Scalability",
      "score": 0.6100560426712036
    },
    {
      "name": "Query language",
      "score": 0.5271461009979248
    },
    {
      "name": "Language model",
      "score": 0.5045641660690308
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.4664344787597656
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.46264517307281494
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.42428505420684814
    },
    {
      "name": "Natural language processing",
      "score": 0.2904108166694641
    },
    {
      "name": "Programming language",
      "score": 0.16890594363212585
    },
    {
      "name": "Database",
      "score": 0.13430827856063843
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165726",
      "name": "Shanghai Municipal Education Commission",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ],
  "cited_by": 117
}