{
  "title": "Human-AI Interaction in the Age of Large Language Models",
  "url": "https://openalex.org/W4398184816",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6665322772",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W4389519970",
    "https://openalex.org/W4379919478",
    "https://openalex.org/W4376988648",
    "https://openalex.org/W4312089323",
    "https://openalex.org/W4375869712",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4396827149",
    "https://openalex.org/W4323697401",
    "https://openalex.org/W4389523857",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W2059216172",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W4366389138"
  ],
  "abstract": "Large language models (LLMs) have revolutionized the way humans interact with AI systems, transforming a wide range of fields and disciplines. In this talk, I share two distinct approaches to empowering human-AI interaction using LLMs. The first one explores how LLMstransform computational social science, and how human-AI collaboration can reduce costs and improve the efficiency of social science research. The second part looks at social skill learning via LLMs by empowering therapists and learners with LLM-empowered feedback and deliberative practices. These two works demonstrate how human-AI collaboration via LLMs can empower individuals and foster positive change. We conclude by discussing how LLMs enable collaborative intelligence by redefining the interactions between humans and AI systems.",
  "full_text": "Human-AI Interaction in the Age of Large Language Models\nDiyi Yang\nStanford University\n353 Jane Stanford Way\nStanford, California, USA, 94305\ndiyiy@stanford.edu\nAbstract\nLarge language models (LLMs) have revolutionized the\nway humans interact with AI systems, transforming a\nwide range of fields and disciplines. In this talk, I\nshare two distinct approaches to empowering human-AI\ninteraction using LLMs. The first one explores how\nLLMstransform computational social science, and how\nhuman-AI collaboration can reduce costs and improve\nthe efficiency of social science research. The second part\nlooks at social skill learning via LLMs by empowering\ntherapists and learners with LLM-empowered feedback and\ndeliberative practices. These two works demonstrate how\nhuman-AI collaboration via LLMs can empower individuals\nand foster positive change. We conclude by discussing how\nLLMs enable collaborative intelligence by redefining the\ninteractions between humans and AI systems.\nIntroduction\nThe development of Large Language Models (LLMs) has\nchanged the way how humans interact with AI systems. As\nLLMs are trained on vast amounts of Internet data, they\ncan comprehend and generate human-like text, enabling\nusers to engage with AI through natural language in a\nconversational manner, as well as facilitating a variety\nof creative writing and customer service applications. We\nare witnessing a profound transformation in how humans\ninteract with AI systems, a subfield known as Human-AI\nInteraction, which has been established even before the\nintroduction of LLMs (Wu, Yang, and Santy 2023).\nHuman-AI Interaction covers multiple forms of\ninteraction between human and AI systems, depending\non the objective of the interaction. There are two major\nforms: human-AI collaborationwhere humans and AI work\ntogether to achieve a goal, and humans utilizing AI-infused\nsystems where AI is used to facilitate human interaction.\nRelated to this, mixed-initiative interaction (Horvitz 1999)\nemphasizes that humans and AI systems can take on the\nleading roles interchangeably in an interaction strategy.\nAs LLMs become more competent in their capabilities\n(OpenAI 2023), human-AI interactions are becoming\nmore diverse with more diverse characters (e.g., tutors,\nAI-enabled chat) and functionalities (e.g., math, coding,\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nchess). As a result, it becomes more important to\nthink about how insights from human-human interaction\ncan be integrated into this process to support more\neffective human-AI interaction workflows, especially when\nLLM-simulated agents (Talebirad and Nadiri 2023) are\ninvolved in simulating participants (Park et al. 2023) or other\nroles such as malicious characters (Ganguli et al. 2022).\nTo enable positive human-AI interaction, from a\nmethodology perspective, enabling seamless interactions\nbetween humans and LLMs is not trivial. Different\nprompting techniques such as decomposition to planning,\nrefinement, and interaction (Cai et al. 2023; Li et al. 2023b),\ntogether with approaches like retrieval augmentation (Lewis\net al. 2020) have been proposed to enhance different aspects\n(e.g., factuality and grounding) of interaction. From an\nevaluation perspective, human-AI interaction also poses\nnew challenges as conventional performance driven metrics\nmight not be enough (Lee et al. 2022). User-centered\ndesign and evaluation become more needed to measure not\nonly these task-level performances, but also characterize\ndiverse interaction dimensions such as usability, satisfaction,\nresponsibility, as well as long-term effects on users.\nCast Studies of Human-AI Interaction\nThis work presents two case studies1 to illustrate how LLMs\ncan empower human-AI interaction. The first one explores\nhow LLMs transform computational social science, and how\nhuman-AI collaboration can reduce costs and improve the\nefficiency of social science research. The second work looks\nat social skill learning via LLMs by empowering learners\nwith LLM-empowered feedback and deliberative practices.\nCase Study 1: How human-AI collaboration helps\nsocial science research Many language processing tasks\ncan be successfully performed by LLMs in a zero-shot\nmanner. If LLMs can help code social phenomena such\nas persuasiveness and political ideology, then they would\ntransform Computational Social Science (CSS). We present\na road map for the use of LLMs as CSS tools, including\nprompting best practices and an evaluation pipeline for\nevaluating the zero-shot performance of 13 language models\non 24 representative CSS benchmarks (Ziems et al. 2023).\n1This symposium talk uses two case studies based on our\npreviously published research, which I would like to acknowledge.\nAAAI Spring Symposium Series (SSS-24)\n66\nWe further demonstrate how to harness the complementary\nstrengths of humans and LLMs for the CSS research\npipeline, given their strong zero-shot capability on many\ntext-annotation tasks (Kuzman, Ljube ˇsi´c, and Mozeti ˇc\n2023). We dive deep into how annotation work can be best\nallocated among humans and LLMs to achieve both quality\nand cost objectives via CoAnnotating a novel paradigm for\nHuman-LLM co-annotation of unstructured texts(Li et al.\n2023a). We find that CoAnnotating to be an effective means\nto allocate work, with up to 21% superior performance\ncompared to random allocation on multiple datasets.\nCase Study 2: Using LLMs to help humans learn\nsocial skills Since social skills are mostly learned through\nexpert supervision, it is difficult to scale training, especially\ngiven the shortage of trained professionals. We argue that\nLLMs can provide opportunities for people to practice\nand develop social skills that will help them in the\nworkplace and in their personal lives. For instance, the\nsuccess of peer counseling platforms depends on effective\nvolunteer counselors, but many volunteers lack access\nto individualized learning resources. One of our recent\nworks introduces CARE: an interactive AI coach that\ntrains peer counselors with automatic suggestions (Hsu\net al. 2023). CARE diagnoses which counseling strategies\nare most appropriate in the given context and suggests\ntailored responses during the practical training stage. We\nfind that this LLM-based system, trained on Motivational\nInterviewing strategies from counseling conversation data,\nsignificantly helps novice counselors respond to challenging\nsituations. One can use similar paradigm for other skills\nsuch as conflict resolution, for which we developed the\nRehearsal system (Shaikh et al. 2023). Rehearsal helps users\npractice conflicts with a believable simulated interlocutor,\nidentify alternative conversational paths, and learn through\nfeedback on how to apply specific conflict strategies.\nOur between-subjects evaluation showed that Rehearsal\nsignificantly helps learners navigate later unaided conflict\ncompared to control groups.\nConclusion\nUsing two case studies, this work illustrates how LLMs\ncan enable collaborative intelligence by enabling humans\nand AI systems to collaborate together, as well as by\nenabling humans to learn social skills. Note that the\ndesign and development of human-AI interaction systems\nmust address ethical challenges related to bias, harms,\nrisks, and privacy concerns. Human-AI interaction goes\nbeyond what we have covered here, including but not\nlimited to: (1) designing new forms of interaction, (2)\niteratively improving LLMs to enable better interactions\nthrough learning from interactions, (3) making human-AI\ninteractions more personalized, (4) and analyzing how\nhuman-AI interaction might influence humans and change\ntheir behavior within a broader social context.\nReferences\nCai, Y .; Mao, S.; Wu, W.; Wang, Z.; Liang, Y .; Ge, T.;\nWu, C.; You, W.; Song, T.; Xia, Y .; et al. 2023. Low-code\nLLM: Visual Programming over LLMs. arXiv preprint\narXiv:2304.08103.\nGanguli, D.; Lovitt, L.; Kernion, J.; Askell, A.; Bai, Y .;\nKadavath, S.; Mann, B.; Perez, E.; Schiefer, N.; Ndousse,\nK.; et al. 2022. Red teaming language models to reduce\nharms: Methods, scaling behaviors, and lessons learned.\narXiv preprint arXiv:2209.07858.\nHorvitz, E. 1999. Principles of mixed-initiative user\ninterfaces. In Proceedings of the SIGCHI conference on\nHuman Factors in Computing Systems, 159–166.\nHsu, S.-L.; Shah, R. S.; Senthil, P.; Ashktorab, Z.; Dugan,\nC.; Geyer, W.; and Yang, D. 2023. Helping the Helper:\nSupporting Peer Counselors via AI-Empowered Practice\nand Feedback. arXiv preprint arXiv:2305.08982.\nKuzman, T.; Ljubeˇsi´c, N.; and Mozeti ˇc, I. 2023. Chatgpt:\nbeginning of an end of manual annotation? Use case\nof automatic genre identification. arXiv preprint\narXiv:2303.03953.\nLee, M.; Srivastava, M.; Hardy, A.; Thickstun, J.; Durmus,\nE.; Paranjape, A.; Gerard-Ursin, I.; Li, X. L.; Ladhak, F.;\nRong, F.; et al. 2022. Evaluating human-language model\ninteraction. arXiv preprint arXiv:2212.09746.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;\nGoyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel,\nT.; et al. 2020. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural\nInformation Processing Systems, 33: 9459–9474.\nLi, M.; Shi, T.; Ziems, C.; Kan, M.-Y .; Chen, N. F.; Liu,\nZ.; and Yang, D. 2023a. CoAnnotating: Uncertainty-guided\nwork allocation between human and large language models\nfor data annotation. arXiv preprint arXiv:2310.15638.\nLi, Y .; Hu, B.; Chen, X.; Ma, L.; and Zhang, M. 2023b.\nLMEye: An Interactive Perception Network for Large\nLanguage Models. arXiv preprint arXiv:2305.03701.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPark, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang, P.;\nand Bernstein, M. S. 2023. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nAnnual ACM Symposium on User Interface Software and\nTechnology, 1–22.\nShaikh, O.; Chai, V .; Gelfand, M. J.; Yang, D.; and\nBernstein, M. S. 2023. Rehearsal: Simulating conflict to\nteach conflict resolution. arXiv preprint arXiv:2309.12309.\nTalebirad, Y .; and Nadiri, A. 2023. Multi-Agent\nCollaboration: Harnessing the Power of Intelligent LLM\nAgents. arXiv preprint arXiv:2306.03314.\nWu, T.; Yang, D.; and Santy, S. 2023. Designing,\nEvaluating, and Learning from Humans Interacting with\nNLP Models. In Zhang, Q.; and Sajjad, H., eds.,\nProceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing: Tutorial Abstracts, 13–18.\nSingapore: Association for Computational Linguistics.\nZiems, C.; Held, W.; Shaikh, O.; Chen, J.; Zhang, Z.;\nand Yang, D. 2023. Can Large Language Models\nTransform Computational Social Science? arXiv preprint\narXiv:2305.03514.\n67",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.37462112307548523
    },
    {
      "name": "Sociology",
      "score": 0.32765358686447144
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 3
}