{
  "title": "An Explainable Toolbox for Evaluating Pre-trained Vision-Language Models",
  "url": "https://openalex.org/W4385572962",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2320679490",
      "name": "Tiancheng Zhao",
      "affiliations": [
        "Zhejiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2113886952",
      "name": "Tianqi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141585783",
      "name": "Mingwei Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5058869386",
      "name": "Haozhan Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134995023",
      "name": "Kyusong Lee",
      "affiliations": [
        "Zhejiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2113751768",
      "name": "Xiaopeng Lu",
      "affiliations": [
        "Zhejiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2124632089",
      "name": "Jianwei Yin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3172112830",
    "https://openalex.org/W4312770978",
    "https://openalex.org/W4312563428",
    "https://openalex.org/W3177155667",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4295884768",
    "https://openalex.org/W3116271762",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4221167443",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3171654528",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W2938667732",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3096682293"
  ],
  "abstract": "Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, Jianwei Yin. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2022.",
  "full_text": "Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 30 - 37\nDecember 7-11, 2022c⃝2022 Association for Computational Linguistics\nAn Explainable Toolbox for Evaluating Pre-trained\nVision-Language Models\nTiancheng Zhao1,2, Tianqi Zhang3, Mingwei Zhu3, Haozhan Shen3,\nKyusong Lee1,2, Xiaopeng Lu1,2, Jianwei Yin3\nOm Research Lab, Binjiang Institute of Zhejiang University1\nLinker Technology Research Co. Ltd2\nCollege of Computer Science and Technology, Zhejiang University3\n{tianchez, kyusongl}@zju-bj.com, lu_xiaopeng@hzlh.com\n{zhang_tq, zhumw, cnfighting, zjuyjw}@zju.edu.cn\nAbstract\nWe introduce VL-CheckList, a toolbox\nfor evaluating Vision-Language Pretrain-\ning (VLP) models, along with a benchmark\ndataset for fine-grained VLP model anal-\nysis. Most existing VLP models evaluate\ntheir performance by comparing the fine-\ntuned downstream task performance. How-\never, only average downstream task accu-\nracy provides little information about the\npros and cons of each VLP method. In this\npaper, we demonstrate how minor input\nchanges in language and vision will affect\nthe prediction outputs. We also provide a\nguideline for the research community to uti-\nlizes and contributes to this toolbox. Lastly,\na case study based on VL-CheckList is con-\nducted to analyze one of the representa-\ntive VLP models. Data and code are avail-\nable at https://github.com/om-ai-lab/\nVL-CheckList\n1 Introduction\nThe ability to quickly iterate various methods\nand obtain insightful feedback is crucial for\nsuccessful research. For production machine\nlearning (ML) system, comprehensive testing\nbefore deployment is crucial for reliable user\nexperience. Therefore, explainable ML evalu-\nation has emerged to complement benchmark\nevaluation (Bolya et al., 2020; Ribeiro et al.,\n2020; Du et al., 2022), which strives to provide\nan interpretable evaluation of a ML systems\nand analyze the system from a number of dis-\nentangled aspects (Bolya et al., 2020).\nThe advantages of explainable evaluation\nvs. typical benchmark evaluation include: (1)\ndownstream task performance only provides\na black box score and it is difficult to obtain\ninsights for improving a system. (2) typical\ndataset is not designed to test models’ robust-\nness against extreme corner cases, which are\nhowever crucial for many real-world tasks, e.g.\nautonomous driving.\nGiven the importance of explainable ML\nevaluation, this paper concerns about Vision-\nLanguage Pretraining (VLP) models. VLP\nmodels have rapidly improved (Li et al., 2020;\nRadford et al., 2021; Li et al., 2021; Zhao\net al., 2022), thanks to the emergence of mul-\ntimodal transformers (Vaswani et al., 2017)\nand the availability of large paired image-text\ndataset (Sharma et al., 2018; Changpinyo et al.,\n2021). Many proposed VLP models have aided\nin achieving the state-of-the-art performance\nof a variety of downstream multimodal tasks,\nranging from visual QA (Lu et al., 2019), mul-\ntimodal retrieval (Lu et al., 2021) to visual\ngrounding (Kamath et al., 2021) and many\nothers. On the other hand, the current defacto\nmethod to evaluate a VLP model is based on\nthe fine-tuned downstream tasks performance,\nwhich is insufficient due to the limitations of\nbenchmark evaluation.\nTo address this challenge, this paper intro-\nduces VL-CheckList, an explainable framework\nthat comprehensively evaluates VLP models,\nfacilitates deeper understanding, and inspires\nnew ideas for improvement. The core princi-\nple of VL-CheckList are: (1) evaluate a VLP\nmodel’s fundamental capabilities instead of its\nperformance on applications (2) disentangle\ncapabilities into relatively independent vari-\nables that are easier to analyze. Specifically,\nwe choose Image-Text-Matching (ITM) as the\nmain target of evaluation since it is perhaps\nthe most universal pretraining objective that\nappear in all VLP methods (Li et al., 2019a,\n2020; Radford et al., 2021; Li et al., 2021).\nWe then propose a taxonomy that divides the\ncapabilities of VLP systems into three cate-\ngories: object, attribute and relation. Each\naspect is then further divided into more fine-\n30\ngrained variables, e.g. attribute is composed of\ncolor, material, and size, etc. Then, a linguistic-\naware negative sample sampling strategy is pro-\nposed to create ”hard negative” that challenges\na VLP model’s discriminative power against\nsmall changes in the input space. Lastly, VL-\nCheckList is implemented as a toolbox that\nallows the research community to plug into\ntheir evaluation pipeline.\n2 Related Work\nBenchmark evaluation is a common method to\ncompare different ML models in previous re-\nsearch (Rajpurkar et al., 2016; Bowman et al.,\n2015; Wang et al., 2018). However, researchers\nhave reported several limitations of the existing\nVLP benchmark. 1) the objects of interest have\na biased distribution of size and location, i.e.,\ntend to be large objects that appeared in the\ncenter region. 2) benchmark evaluation returns\nonly a plain score instead of fine-grained details\non the taxonomy. Therefore, it is difficult to\nunderstand the strengths and weaknesses of a\nmodel without a comprehensive analysis. Re-\ncent studies show even the state-of-the-art sys-\ntems that achieved better scores than humans,\nmay still be insufficient in real-world applica-\ntions (Ribeiro et al., 2020). Thus, researchers\nhave attempted to evaluate ML models with\nmore fine-grained details and avoid bias on the\ntest set.\nOne of the successful tools for the qualitative\nanalysis of natural language processing (NLP)\nis CheckList (Ribeiro et al., 2020) which evalu-\nates general linguistic capabilities and revealed\nweaknessesinseveralstate-of-the-artNLPmod-\nels and commercial applications. In computer\nvision, the Vision CheckList was proposed to\nhelp system designers to understand model ca-\npabilities (Du et al., 2022). They offered a\nset of transformation operations to generate\ndiverse test samples of different test types, such\nas rotation, replacing image patches, blur, and\nshuffle. However, target objects in the trans-\nformed images are unchanged, still center and\nlarge.\nThe idea of the CheckList has also been ap-\nplied other fields, e.g. evaluating Reinforce-\nment Learning (RL) agents (Lam et al., 2022),\nDynabench (Kiela et al., 2021) was proposed to\ngenerate dynamic benchmark datasets. It over-\ncomes the problem that the existing benchmark\nfails to cover fundamental linguistic challenges.\nTIDE (Bolya et al., 2020) is a tool to analyze\nthe errors of object detection. It defines crit-\nical error types and shows a comprehensive\nanalysis.\n3 VL-CheckList\nAn intuitive approach to evaluate multi-modal\nsystems is to check if a model correctly predicts\nalignment between different modalities. We\nchoose image-text matching (ITM) to check the\nalignment between vision and language for the\nfollowing reasons. Specifically, ITM is defined\nas the function that outputs the probability of\nan imagei is matched to a sentencet.\nThe ITM task is used as an effective and\nuniversal pretraining objective in almost all\nVLP models (Li et al., 2020). The ITM task\nis also model agnostic and applies to all multi-\nmodal fusion architectures. Thus, we exploit\nthe ITM to fairly compare the VLP models\nwithout finetuning them on downstream tasks.\nThe basic principle of the VL-CheckList is\nto probe the model’s robustness on the nega-\ntive examples. A robust VLP model should be\nable to return a higher ITM score for the posi-\ntive image-text pair than the negative example\non the ITM head. We perturb the one-side\nmodality to manipulate them and compare the\nscore with original samples. LV-CheckList of-\nfers both language-side and vision-side varia-\ntions.\n3.1 Language Variation\nTo provide a fine-grained analysis of the ro-\nbustness of the text-side, we build evaluation\ntaxonomies that are selected based on common\nmistakes or frequent usage. Based on the com-\nmon issues in VLP models, the proposed frame-\nwork places the three input properties (object,\nattribute, and relation) as the top layer of the\nevaluation taxonomy.\nObject: A strong VLP model is supposed\nto recognize whether or not the objects men-\ntioned in a text exist in the image. There-\nfore, if we replace objects in the correct text\nwith some other random noun phrases, a VLP\nmodel should give it a lower ITM score than\nthe original sentence. Furthermore, a strong\nVLP model should be able to recognize the ex-\n31\nistence of objects, regardless of its location and\nsizes. Thus, we further evaluate the robustness\nObject ITM by testing location variance (e.g.,\ncenter, middle, and margin) and size variance\n(e.g., small, large, medium), specifically:\nloc(x, y)=\n\n\n\ncenter if y\nx ≤1\n3\nmid if 1\n3 < y\nx ≤2\n3\nmargin otherwise\nwhere, x is the half-length of the diagonal\nof the full imagex =\n√\nw2+h2\n2 . and y is the dis-\ntance between its central point and the central\npoint of the full image.\nTo get the size of an object, we use the object\narea information (i.e., the bounding box of\nheight multiplies the width).\nsize(x)=\n\n\n\nsmall if area ≤S\nmedium if S < area≤M\nlarge otherwise\nwhere, area = w∗h, S denotes small size and\nM is the medium size. We setS = 1024, M=\n9216 in this paper.\nAttribute: Determining specific attributes\nfor any object is very challenging. The at-\ntribute generally contains color, material, size,\nstate, and action.\n• Size: replace the size expression like small,\nbig, and medium with another (e.g., There\nis a big apple vs. there is a small apple)\n• Material: replace a material word in the\nsentence (e.g., a metal box vs. a wood\nbox)\n• State: replace the state expression, such\nas cleanliness and newness (e.g., a man\nwith dry hair vs. a man with wet hair).\n• Action: replace the action-related word\nin the text (e.g., a standing person vs. a\nsitting person).\n• Color: replace the color word in the text\n(e.g., A red apple is on the table vs. A\ngreen apple is on the table)\nRelation: Relation cares about the inter-\naction between two objects. It covers replacing\nthe predicate in a triple (e.g., subject, predi-\ncate, object), where the subject and object are\nboth objects in the image. A strong VLP ITM\nhead should assign a higher score to text match-\ning the pair-wise object interaction. Further,\nwe divide prediction into spatial and action. If\na predicate is one of the spatial prepositions\n(e.g., in, on, at, etc), it is sub-categorized as\n’spatial’; otherwise, it is labeled ’action.’\n• Spatial: If a model can predict spatial\nrelation between two objects (e.g, <cat,\non, table> vs. <cat, under, table>).\n• Action: If a model can predict other rela-\ntion than a spatial preposition, usually ac-\ntion verbs like run, jump, kick, eat, break,\ncry, or smile (e.g., <cat, catch, fish> vs.\n<cat, eat, fish>)\n3.2 Vision Variation\nA strong VLP model should be able to return\nconsistent scores when an image is transformed\nwith augmentation techniques such as rotation,\nshift, flip, random brightness, etc. However,\nprevious augmentations are applied on the en-\ntire image-level. We provide the object-level\ndata augmentation by combining cropped ob-\njects and image background. The generated\nimagesareutilizedtoinvestigatetherobustness\nof the model outputs in various locations and\nsizes of the target object. Strong VLP mod-\nels should be able to return consistent scores\nregardless of the location and size of target ob-\njects unless the language description is related\nto location and size (e.g., an apple is the left\nside of the tree, an apple is small). We allow to\ninput cropped objects and background images\nand randomly place the target objects from\nmargin to center with various sizes to probe\nthe robustness. The goal of the LV-CheckList\non the vision-variations is to show how sim-\nple input changes such as object location and\nsize will affect the prediction outputs in the\nVL-CheckList Demo.\n4 Detailed User Guideline\nThis section describes a guideline for re-\nsearchers to use and contribute to the VL-\nCheckList project.\nFirst, users can install from GitHub1 or\nfrom pip install vl-checklist. We further\nprovide a HuggingFace demo for people to try\nout different VLP models2. Then the following\nis a step-by-step guideline to use VL-CheckList.\n1github.com/om-ai-lab/VL-CheckList\n2huggingface.co/spaces/omlab/VL_checklist_demo\n32\nFigure 1: Language Variation: negative samples are based on object, attribute and relation. Vision\nVariation: a user inputs target objects and backgrounds and evaluates the various synthesized images\n1) Define Corpus: a user defines a cor-\npus in the yaml config file. We provide four\ninitial pre-processed corpora using the semi-\nstructured dataset such as VG (Krishna et al.,\n2017), SWiG (Pratt et al., 2020), VAW (Pham\net al., 2021) and HAKE (Li et al., 2019b). We\nbuild a benchmark dataset for each capability\ntest in the proposed framework. We provide\nthe pre-processed datasets in thecorpus folder\nof our Github page. An example of the corpus\nconfig yaml file is as follows:\nANNO_PATH: \" A t t r / a c t i o n . j s o n \"\nIMG_ROOT: \" v g / \"\nTYPE: \" T U P L E _ J S O N \"\nANNO_PATH is the specific Json file path\nthat includes positive and negative captions\nand the specific image path.\nThe data type is TUPLE_JSON. We\nconverted the corpus into list of image path\nand captions(positive and negative), in the\nformat of a list of [[{image_path:str,\n\"POS\":pos_captions:list, \"NEG\":\nneg_captions:list}] . . . ]\n2) Define evaluation configuration:\nUsers can specify the evaluation settings in\nanother yaml to define evaluation in detail as\nthe following example:\nMAX_NUM: 2000\nMODEL_NAME: \" C L I P \"\nBATCH_SIZE: 4\nTASK: \" i t c \"\nDATA:\nTYPES: [ \" O b j e c t / L o c a t i o n / m i d \"]\nTEST_DATA: [ \" v g _ o b j \"]\nOUTPUT:\nDIR: \" o u t p u t / c l i p \"\nThe \"MAX_NUM\" is the maximum number\nof data points and the \"MODEL_NAME\"\nneeds to be specified. Appropriate\n\"BATCH_SIZE\" should be input based\non the GPU resources. The \"TASK\" can\nbe either \"ITC\" or \"ITM\". The \"ITC\" score\ncompares models’ scores on both positive and\nnegative captions. It counts as a true positive\nwhen the score on the original is higher than\nthe negatively transformed one. The \"ITM\"\nis predicting each image and a caption. It\nis called the true positive when a softmax\nscore on a positive example on the image is\nhigher than the threshold of 0.5. The Data tag\nconsists of TYPES and TEST_DATA. The\nTYPES is the storage paths of the \"ymal_files\".\nIn the top-level directory, we can divide it\ninto three categories: Object, Relation, and\nAttribute. For Swig, Vg, etc., there are\nmultiple data subsets, so the data subset\ntype should be filled in the TEST_DATA.\nWe can specify the evaluation data, output\ndirectory, and format as the example above.\nAfter defining a config file, users can simply\nstart the evaluation as follows:.\nfrom engine import Model\nfrom vl_checklist import Evaluate\nif __name__ == '__main__ ':\nmodel = Model ( 'model . ckpt')\neval = Evaluate (\" sample . yaml \",\nmodel = model )\neval . start ()\n33\n3) Define Model: Users can import\nVL-CheckList to their projects (e.g.,import\nvl_checklist) and need to implement one\nmodel class that includes the essential func-\ntions, \"predict\". The predict function should\nreturn probabilities on each pair of images and\ntexts. We included several representative mod-\nels for quick comparisons, such as ViLT (Kim\net al., 2021), ALBEF (Li et al., 2021), OS-\nCAR (Li et al., 2020), etc as example projects.\n5 Experimental Settings\nIn this section, we profile one of the most rep-\nresentative VLP models, CLIP (Radford et al.,\n2021) by testing its ability to understand an\nobject, attribute, and relationship between a\ntext prompt and a given image for language\nvariations.\nMetric: Wereturnthemodeloutputscores\nbetween the text description and the generated\nnegative samples. If the model score on the\noriginaltextdescriptionishigherthanthescore\non the generated negative samples, we regard\nit as positive output. We obtain the accuracy\nwith the following equation.\nacc =\n∑i<n\ni=0 f(xp\ni , xn\ni )\nN (1)\nwhere, f(xp\ni , xn\ni ) = 1 if p(xp\ni |Ii) > p(xn\ni |Ii),\notherwise 0. xp\ni denotes a positive sample of\nith data. xn\ni means a positive sample ofith\ndata. The N is the total number of pairs of\npositive and negative samples.Ii is ith image\ndata.\nData: The proposed VL-CheckList focuses\non a directional expectation test, in which the\nlabel is expected to change in a certain way.\nFor example, when there is a black bear in the\nphoto and the text description is \"A black bear\nis holding a stick\". We can transform several\nvariations (e.g., <a black bear→a red bear>,\n<a stick →an apple>, <holding →throw-\ning>, etc). The negative sampling strategy\nis the essential step for unbiased evaluations.\nTo generate hard negative examples, we use\nthe structured text description datasets such\nas Visual Genome (VG) (Krishna et al., 2017),\nSWiG (Pratt et al., 2020), and Human Activity\nKnowledge Engine (HAKE) (Li et al., 2019b).\nThe VG provides attributes, relationships, and\nregion graphs which can make a hard negative\nsample by replacing one attribute in the rela-\ntion in the image. The SWiG dataset provides\nstructured semantic summaries of images with\nroles such as agent and tool. We generate hard\nnegative samples by replacing one of the roles\nin the text description to mismatch with the\nimage. HAKE dataset provides the relation-\nship between instance activity and body part\nstates (e.g., \"head\" inspect rear view, \"right\nhand\" hold wheel, \"hip\" sit on chair seat).\nFor the VG dataset, we first assign each at-\ntribute, object, and relation to the closet type\nby cosine similarity from sentence transform-\ners. For objects and relationships, we randomly\nsample a corresponding instance with a co-\nsine similarity threshold of 0.5. For attribute,\nwe randomly sample a corresponding instance\nfrom the same attribute class with a cosine\nsimilarity threshold of 0.5. We further conduct\nmanual correction on the generated to data to\nfix inappropriate data.\nFor vision variations, we only conduct quali-\ntative analysis by visualizing the output scores\nvia the GUI demo. (Figure 2).\n6 Results and Analysis\nIn general, the ability of CLIP to understand\nobject changes is promising when the object is\ncenter and large (see the prefix-O at Figure 3).\nWehypothesizethattheCLIPmodelpaysmore\nattention to the central region and focus on\nsalient objects, similar to the perspective of\nhuman observation. On the other hand, CLIP’s\nability on recognizing attribute and relation-\nrelated variants is surprisingly low, especially\nfor Relation-spatial variations (Figure 3).\nThen, We investigate whether performance\ncan be improved by cropping the regions of\ninterest (ROI) first and then encoding the\ncropped ROIs via CLIP. We extract text\ndescriptions on each bounding box on the\nVG dataset to form a new image-text pair\n(Imagelocal,text), and construct new datasets\nfor VG: Local subj, Local obj. Results on\nLocalsubj and Localobj show that Region CLIP\noutperforms the original CLIP (whole image en-\ncoding) by 3.9% and 5.7% respectively (Table\n1). This confirms our hypothesis that the origi-\nnal CLIP was trained to match the entire image\nto a text description, without capturing the\nfine-grained alignment between image regions\n34\nFigure 2: A comparison of CLIP’s performances of the image with a big object in the center and image\nwith the same small object in the corner\nFigure 3: A radar chart for text variance on the\nCLIP model. (The prefix O, A, and R is Object,\nAttribute, and Relation respectively)\nand text spans. Thus the understanding of mi-\nnor objects in the image for CLIP is still chal-\nlenging and explore more fine-grained region-\nto-text multimodal alignment is a promising\ndirection (Zhong et al., 2022).\nFor vision variations, we synthesize images\nby changing an object’s size and location. In\nFigure 2, the image on the left is a big apple\nin the center, while the image on the right is a\nsmall apple in the corner. The text prompt we\ninput is \"an apple on the grass\" and \"a dog on\nthe grass\". The accuracy of the left image with\na big and center apple is nearly 1.00, while the\nright image with a small and corner apple only\nobtains 0.127 of accuracy. The location and\nsize of the object in the image can significantly\naffect the judgment of the model.\nThus Experimental results indicate that the\ncurrent benchmark evaluation reveals a gap of\nperformance for real applications. CLIP mostly\nfocus on objects that appeared in the center of\nthe image and the size of the objects should be\nlarge. This limits its performance if the target\nobjects are minor in the marginal regions for\nreal-world applications.\nModel \\ VGdata_type Subj Obj\nCLIP_Global 80.7 86\nCLIP_Local 84.6 91.7\nTable 1: Subj and Obj are two attribute subsets\nextracted from VG dataset. A new dataset is con-\nstructed using the bounding box tag of VG to merge\nand extract the region image pointed bysubj and\nobj fields. The text remains the same as previous\ncontent (Imagelocal,text). It only does the expan-\nsion experiment for CLIP.\n7 Conclusion\nThis paper introduces VL-CheckList to analyze\nVLP models from language and vision varia-\ntions. For language variance, we evaluated\nfrom three aspects: object, attribute and rela-\ntion. For vision variance, we generated synthe-\nsized images using cropped target objects and\nbackground. We found limitations of the CLIP\nmodel: 1) limited understanding for small ob-\njects in the corner 2) incompetence for recogniz-\ning relations and attributes. In the future, we\nplan to include more fine-grained taxonomies\n35\nand synthesizing strategies into VL-CheckList\nand also improve existing VLP methods under\nthe guidance of VL-CheckList report.\n8 Acknowledgement\nThis study is supported by National Natural\nScience Foundation of China under Grant (No.\n61825205).\nReferences\nDaniel Bolya, Sean Foley, James Hays, and Judy\nHoffman. 2020. Tide: A general toolbox for\nidentifying object detection errors. InEuropean\nConference on Computer Vision, pages 558–573.\nSpringer.\nSamuel Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large an-\nnotated corpus for learning natural language in-\nference. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 632–642.\nSoravit Changpinyo, Piyush Sharma, Nan Ding,\nand Radu Soricut. 2021. Conceptual 12m: Push-\ning web-scale image-text pre-training to recog-\nnize long-tail visual concepts. InProceedings of\nthe IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 3558–3568.\nXin Du, Benedicte Legastelois, Bhargavi Ganesh,\nAjitha Rajan, Hana Chockler, Vaishak Belle, Stu-\nart Anderson, and Subramanian Ramamoorthy.\n2022. Vision checklist: Towards testable error\nanalysis of image models to help system design-\ners interrogate model capabilities.arXiv preprint\narXiv:2201.11674.\nAishwarya Kamath, Mannat Singh, Yann LeCun,\nGabriel Synnaeve, Ishan Misra, and Nicolas Car-\nion. 2021. Mdetr-modulated detection for end-to-\nend multi-modal understanding. InProceedings\nof the IEEE/CVF International Conference on\nComputer Vision, pages 1780–1790.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh\nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie\nVidgen, Grusha Prasad, Amanpreet Singh,\nPratik Ringshia, et al. 2021. Dynabench: Re-\nthinking benchmarking in nlp. InProceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Lin-\nguistics: Human Language Technologies, pages\n4110–4124.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021.\nVilt: Vision-and-language transformer without\nconvolution or region supervision. InInterna-\ntional Conference on Machine Learning, pages\n5583–5594. PMLR.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin\nJohnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A\nShamma, etal.2017. Visualgenome: Connecting\nlanguage and vision using crowdsourced dense\nimage annotations.International journal of com-\nputer vision, 123(1):32–73.\nKin-Ho Lam, Delyar Tabatabai, Jed Irvine, Donald\nBertucci, Anita Ruangrotsakun, Minsuk Kahng,\nAlan Fern, Jeongyeon Kim, Yubin Choi, Juho\nKim, et al. 2022. Beyond value: Checklist for\ntesting inferences in planning-based rl. ACM\nTransactions on Interactive Intelligent Systems,\n12(1).\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Got-\nmare, Shafiq Joty, Caiming Xiong, and Steven\nChu Hong Hoi. 2021. Align before fuse: Vi-\nsion and language representation learning with\nmomentum distillation.Advances in Neural In-\nformation Processing Systems, 34.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019a. Visualbert:\nA simple and performant baseline for vision and\nlanguage. ArXiv, abs/1908.03557.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. 2020. Oscar:\nObject-semantics aligned pre-training for vision-\nlanguage tasks. InEuropean Conference on Com-\nputer Vision, pages 121–137. Springer.\nYong-Lu Li, Liang Xu, Xijie Huang, Xinpeng Liu,\nZe Ma, Mingyang Chen, Shiyi Wang, Haoshu\nFang, and Cewu Lu. 2019b. Hake: Human activ-\nity knowledge engine.ArXiv, abs/1904.06539.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Ste-\nfan Lee. 2019. Vilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-\nand-language tasks. Advances in neural infor-\nmation processing systems, 32.\nXiaopeng Lu, Tiancheng Zhao, and Kyusong Lee.\n2021. Visualsparta: An embarrassingly simple\napproach to large-scale text-to-image search with\nweighted bag-of-words. InProceedings of the 59th\nAnnual Meeting of the Association for Compu-\ntational Linguistics and the 11th International\nJoint Conference on Natural Language Process-\ning (Volume 1: Long Papers), pages 5020–5029.\nKhoi Pham, Kushal Kafle, Zhe Lin, Zhi Ding,\nScott D. Cohen, Quan Tran, and Abhinav Shri-\nvastava. 2021. Learning to predict visual at-\ntributes in the wild.2021 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 13013–13023.\nSarah Pratt, Mark Yatskar, Luca Weihs, Ali\nFarhadi, and Aniruddha Kembhavi. 2020.\nGrounded situation recognition. In European\n36\nConference on Computer Vision, pages 314–332.\nSpringer.\nAlec Radford, Jong Wook Kim, Chris Hallacy,\nAditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al. 2021. Learning transferable\nvisual models from natural language supervision.\nIn International Conference on Machine Learn-\ning, pages 8748–8763. PMLR.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopy-\nrev, and Percy Liang. 2016. Squad: 100,000+\nquestions for machine comprehension of text. In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, pages\n2383–2392.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos\nGuestrin, and Sameer Singh. 2020. Beyond ac-\ncuracy: Behavioral testing of nlp models with\nchecklist. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 4902–4912.\nPiyush Sharma, Nan Ding, Sebastian Goodman,\nand Radu Soricut. 2018. Conceptual captions:\nA cleaned, hypernymed, image alt-text dataset\nfor automatic image captioning. InProceedings\nof the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long\nPapers), pages 2556–2565.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need.Advances in neural in-\nformation processing systems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355.\nTiancheng Zhao, Peng Liu, Xiaopeng Lu, and Kyu-\nsong Lee. 2022. Omdet: Language-aware ob-\nject detection with large-scale vision-language\nmulti-dataset pre-training. arXiv preprint\narXiv:2209.05946.\nYiwu Zhong, Jianwei Yang, Pengchuan Zhang,\nChunyuan Li, Noel Codella, Liunian Harold Li,\nLuowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al.\n2022. Regionclip: Region-based language-image\npretraining. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, pages 16793–16803.\n37",
  "topic": "Toolbox",
  "concepts": [
    {
      "name": "Toolbox",
      "score": 0.8798161745071411
    },
    {
      "name": "Computer science",
      "score": 0.7199254631996155
    },
    {
      "name": "Zhàng",
      "score": 0.6588975191116333
    },
    {
      "name": "Natural language processing",
      "score": 0.6297917366027832
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5574274659156799
    },
    {
      "name": "Natural language",
      "score": 0.48743894696235657
    },
    {
      "name": "Speech recognition",
      "score": 0.3877851963043213
    },
    {
      "name": "Programming language",
      "score": 0.3648372292518616
    },
    {
      "name": "History",
      "score": 0.12201809883117676
    },
    {
      "name": "China",
      "score": 0.0854993462562561
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I168879160",
      "name": "Zhejiang University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    }
  ],
  "cited_by": 24
}