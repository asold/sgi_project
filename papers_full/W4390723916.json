{
  "title": "Diminished diversity-of-thought in a standard large language model",
  "url": "https://openalex.org/W4390723916",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4296090041",
      "name": "Park, Peter S.",
      "affiliations": [
        "Vassar College"
      ]
    },
    {
      "id": "https://openalex.org/A4321312780",
      "name": "Schoenegger, Philipp",
      "affiliations": [
        "London School of Economics and Political Science",
        "The Honourable Society of Lincoln's Inn"
      ]
    },
    {
      "id": "https://openalex.org/A2227357082",
      "name": "Zhu, Chongyang",
      "affiliations": [
        "CVS Health (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6858385244",
    "https://openalex.org/W6809680140",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W1878893887",
    "https://openalex.org/W2125302733",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W2969615692",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W2096910467",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W4313559133",
    "https://openalex.org/W2133707153",
    "https://openalex.org/W2085876742",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W3126001372",
    "https://openalex.org/W4206125442",
    "https://openalex.org/W2111033106",
    "https://openalex.org/W1641003075",
    "https://openalex.org/W2129015869",
    "https://openalex.org/W2312380053",
    "https://openalex.org/W2776961836",
    "https://openalex.org/W3121819056",
    "https://openalex.org/W2550925836",
    "https://openalex.org/W4323042470",
    "https://openalex.org/W1988887257",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W4385573216",
    "https://openalex.org/W2158547509",
    "https://openalex.org/W2898883664",
    "https://openalex.org/W2088614984",
    "https://openalex.org/W2112818819",
    "https://openalex.org/W4323043839",
    "https://openalex.org/W3013207429",
    "https://openalex.org/W4319984852",
    "https://openalex.org/W2083799309",
    "https://openalex.org/W2088424126",
    "https://openalex.org/W4322720178",
    "https://openalex.org/W4304208382",
    "https://openalex.org/W6845846432",
    "https://openalex.org/W2035782089",
    "https://openalex.org/W2096452841",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W2016125673"
  ],
  "abstract": null,
  "full_text": "Vol:.(1234567890)\nBehavior Research Methods (2024) 56:5754–5770\nhttps://doi.org/10.3758/s13428-023-02307-x\nORIGINAL MANUSCRIPT\nDiminished diversity‑of‑thought in a standard large language model\nPeter S. Park1  · Philipp Schoenegger2 · Chongyang Zhu3\nAccepted: 27 November 2023 / Published online: 9 January 2024 \n© The Author(s) 2024\nAbstract\nWe test whether large language models (LLMs) can be used to simulate human participants in social-science studies. To do \nthis, we ran replications of 14 studies from the Many Labs 2 replication project with OpenAI’s text-davinci-003 model, col-\nloquially known as GPT-3.5. Based on our pre-registered analyses, we find that among the eight studies we could analyse, \nour GPT sample replicated 37.5% of the original results and 37.5% of the Many Labs 2 results. However, we were unable to \nanalyse the remaining six studies due to an unexpected phenomenon we call the “correct answer” effect. Different runs of \nGPT-3.5 answered nuanced questions probing political orientation, economic preference, judgement, and moral philosophy \nwith zero or near-zero variation in responses: with the supposedly “correct answer.” In one exploratory follow-up study, we \nfound that a “correct answer” was robust to changing the demographic details that precede the prompt. In another, we found \nthat most but not all “correct answers” were robust to changing the order of answer choices. One of our most striking find-\nings occurred in our replication of the Moral Foundations Theory survey results, where we found GPT-3.5 identifying as a \npolitical conservative in 99.6% of the cases, and as a liberal in 99.3% of the cases in the reverse-order condition. However, \nboth self-reported ‘GPT conservatives’ and ‘GPT liberals’ showed right-leaning moral foundations. Our results cast doubts \non the validity of using LLMs as a general replacement for human participants in the social sciences. Our results also raise \nconcerns that a hypothetical AI-led future may be subject to a diminished diversity of thought.\nKeywords GPT-3.5 · Large language models · Artificial intelligence · Many Labs 2 · Psychology · Replication · Order \neffects · Demographic effects · Diversity of thought · Social science\nIntroduction\nThe field of natural language processing (NLP) has witnessed \nrapid advances. This trend is most recently exemplified by large \nlanguage models (LLMs). When trained on large corpora of \nInternet- and book-based text data to predict the next sequence \nof words given an input, LLMs have demonstrated the ability \nto generate sophisticated responses to a wide range of prompts. \nOpenAI’s GPT-3 family of models (Brown et al., 2020), its \nsuccessor GPT-4 (OpenAI, 2023b), and the models’ chatbot \nversion ChatGPT (OpenAI, 2023c) have received significant \nattention, in particular due to the models’ capabilities in a wide \nvariety of tasks that were previously thought to require human \nintelligence (Metz, 2020). To illustrate, GPT-4 has excelled \non versions of difficult standardized tests originally meant for \nhumans (OpenAI, 2023b), although it is sometimes unclear \nwhether its answers to these tests were memorized from the \ntraining data. GPT-4 has even shown an arguably human-\nrivalling ability to solve potentially novel tasks in vision, \nmathematics, coding, medicine, and law (Bubeck et al., 2023). \nCompanies are already using OpenAI’s models to automate \neconomically valuable services, such as the presentation of \ninformation via search-engine chatbots (Roose, 2023), via AI \npersonal assistants (Warren & Lawler, 2023), and even via the \nwriting of media content (Edwards, 2023).\nPeter S. Park and Philipp Schoenegger are Co-first author\n * Peter S. Park \n dr_park@mit.edu\n Philipp Schoenegger \n contact.schoenegger@gmail.com\n Chongyang Zhu \n cyzhu95@gmail.com\n1 Department of Physics, MIT, 70 Vassar Street, Cambridge, \nMA, USA\n2 Department of Management, London School of Economics, \nMarshall Building, 44 Lincoln’s Inn Fields, London, \nEngland, UK\n3 CVS Health, 1 CVS Dr., Woonsocket, RI, USA\n5755Behavior Research Methods (2024) 56:5754–5770 \nOpenAI’s mission is to create “highly autonomous sys-\ntems that outperform humans at most economically valuable \nwork” (OpenAI, 2020). Regardless of whether or when this \nmission will be achieved, by either OpenAI or its competi-\ntors, people are prone to treating even current LLMs as if \nthey possess human-like qualities: an anthropomorphisa-\ntion that is not always rigorously justified or investigated \n(Salles et al., 2020). Because of the potentially sweeping \nsocietal changes that advanced AI may bring with it and the \nanthropomorphisation of current LLM models, the rigorous \nstudy of these models, their applications, and limitations are \nespecially critical.\nOne way that LLMs have been studied before in the social \nsciences is by studying them with the methods of psychology \nas if they were human participants, and potentially even as \n“surrogates” (Grossmann et al., 2023, p. 1108) that directly \nsupplant human participants. Much of this previous work \nhas implicitly or explicitly assumed that concepts from the \npsychological sciences and experimental methods originally \nmeant for humans can be applied straightforwardly to LLMs: \nto elicit supposedly parallel mechanisms of human and LLM \ncognition, to psychologically categorise LLMs as if they \nwere humans, and even to simulate human behavioural data \n(Dillion et al., 2023). To illustrate, Binz and Schulz (2023) \nconducted vignette-based survey experiments on GPT-3 and \nconcluded from their data that the LLM showed signs of \nmodel-based reinforcement learning and of behavioural sim-\nilarities to humans. Miotto et al. (2022) investigated GPT-3's \npersonality characteristics, values, and self-reported demo-\ngraphic properties. Similarly, Li et al. (2022) investigated \nthe personality of GPT-3 using the Short Dark Triad scale \nof narcissism, psychopathy, and Machiavellianism (Jones & \nPaulhus, 2014); and the Big Five inventory of openness to \nexperience, conscientiousness, extraversion, agreeableness, \nand neuroticism (John & Srivastava, 1999). Horton (2023) \nexamined GPT-3 in the context of behavioural-economics \nexperiments and concluded that its behaviour was qualita-\ntively similar to that of human participants. Shihadeh et al. \n(2022) measured the presence of the brilliance bias – the \nbias that brilliance is seen as a male trait – in GPT-3. Finally, \nArgyle et al. (2023) and Aher et al. (2022) each conducted \nsimilar experiments in which different types of participants \nwere simulated via GPT-3 and proposed that the model may \nbe used to indirectly collect data on the behavioural aspects \nof various human subjects. These approaches are so far char-\nacterised by a significant heterogeneity of methods and have \nproduced mixed results in terms of LLMs’ ability to supplant \nhuman subjects.\nIn this paper, we conduct a multifaceted investigation \nof whether psychology studies originally designed for \nhuman participants can in fact be straightforwardly applied \nto LLMs, and whether LLMs can replace human partici-\npants in these studies. Specifically, we study OpenAI’s \ntext-davinci-003 model (OpenAI, 2023d), a variant of GPT-3 \ncolloquially known as GPT-3.5, with a large set of psychol-\nogy studies originally replicated by the Many Labs 2 pro-\nject (Klein et al., 2018), a large-scale replication project in \npsychology. We replicate as many studies from this set as \nis feasible in the current monomodal context of GPT-3.5, \nand analyse which effects successfully replicate to give us \na direct and representative measure of how widely LLMs \nmay or may not be applicable in supplanting human par -\nticipants. Our replications are aided by the fact that, unlike \nhuman subjects, GPT-3.5 allows for well-controlled experi-\nments that are highly powered and unlikely to suffer from \na variety of sampling, attention, and other design issues \nthat human studies must grapple with. This is because large \nsamples can be collected quickly and inexpensively, without \nsampling biases – such as non-response bias and exclusion \nbias – that in practice inevitably consign human samples \nto be insufficiently representative of the sheer diversity of \nhuman psychologies around the world (Henrich et al., 2010; \nMajid, 2023; Schimmelpfennig et al., 2023). Analysing the \nways in which different runs of GPT-3.5 answer the studies’ \nsurvey questions – and how they are similar to or different \nfrom human responses – can help rigorously contribute to a \nbroader and more interdisciplinary understanding of the AI \nmodel, its applications, and its respective limitations.\nMethods\nFull details of the methods can be found in the Supplemen-\ntary Information. We pre-registered this study on the Open \nScience Framework (Park et al., 2023 ). For our study, we \ndrew on the set of studies used in Many Labs 2 (Klein et al., \n2018) and their respective analysis plans. The total num-\nber of potential studies that we could analyse was 28. We \nexcluded a total of 14 studies prior to data collection as their \ndesigns included pictures, compared national samples, relied \non handwriting or font changes, or used an otherwise inap-\nplicable component that was not transferable to GPT-3.5’s \nmonomodal context. We then presented GPT-3.5 with these \nremaining surveys, with each run representing a new call \nto the model’s API; see Fig.  1 for a sample input and out-\nput. Then, we converted the survey responses of GPT-3.5 \nruns from .txt to .csv for statistical analysis and removed \nall entries that responded to questions with characters that \nwere not among the possible response categories. For exam-\nple, the responses could have had characters like ‘?’ or ‘/’ \ninstead of the expected outputs that we could use as unam-\nbiguous survey responses. Next, we analysed the data with \nthe respective analysis plan that was based on that of Many \nLabs 2 (differences and exceptions are noted in the Supple-\nmentary Information).\n5756 Behavior Research Methods (2024) 56:5754–5770\nOverall, we collected data for a total of 14 studies for our \nmain pre-registered analyses, each of which consisted of \nabout 1000 different runs of the default temperature setting \nof GPT-3.5, which represents the central source of variation \nin responses. The (softmax) temperature parameter measures \nthe degree to which the model’s outputs are predetermined. \nSpecifically, the model’s probability value of predicting the \nspecific token (unit of text) t i ∈ {t1, …, tN} to be the next \ntoken is given by a certain function of the logit value zi cor-\nresponding to the token, defined by /u1D707�zi\n� = ezi∕T\n∑N\nk=1 ezk∕T  . This \ncomprises a probability distribution that approaches a \n1-point distribution on the most probable token as T → 0, and \napproaches an equidistribution across all tokens as T  → ∞. \nThe effect of the temperature parameter on a hypothetical \nprobability distribution for the next token is illustrated in \nFig.  2. In accordance with likely societal use, we set the \ntemperature parameter to the default intermediate value of \n1.0 (OpenAI, 2023a).\nHistorically, GPT-3.5’s default temperature setting of 1.0 \nhas generally been thought to output answers that are not \npredetermined. This can be seen from OpenAI’s instruc-\ntion regarding the temperature parameter: “higher values \n[of temperature] like 0.8 will make the output more ran-\ndom, while lower values like 0.2 will make it more focused \nand deterministic” (OpenAI, 2023a). Thus, at the time of \npre-registration, we had not considered the possibility that \nall or nearly all ~ 1000 runs of the default temperature set-\nting of GPT-3.5 might answer one of our survey questions \nin a predetermined way. If this were to occur, the zero or \nnear-zero variation in this central variable would make the \ncorresponding study’s statistic – the one we had planned to \nanalyse – unsuitable, and perhaps even unconstructable as a \nwell-defined statistic. As such, we also conducted a number \nof exploratory follow-up studies to further probe our results. \nMore details on these studies’ methods are also available in \nthe Supplementary Information.\nResults\nWe find that surveyed runs of GPT-3.5 provided responses \nthat were in some ways comparable to those given by the \ncorresponding human subjects. To illustrate, in the survey of \nKay et al. (2014) on whether structure promotes goal pursuit, \ndifferent runs of GPT-3.5 gave human-like answers when \nasked about their long-term goal. These answers ranged \nfrom becoming fluent in Spanish, to becoming a full-time \nfreelance software developer, to achieving financial freedom. \nDifferent runs of GPT-3.5 also responded to reading-com-\nprehension questions in the survey of Kay et al. (2014) with \nFig. 1  Sample input and output from our GPT-3.5 replication of the \nstudy of Rottenstreich and Hsee (2001). As a prompt-engineering \ntechnique, we have put – before the survey – instructions on how \nto format its output, and – after the survey – a “CHECKLIST FOR \nSURVEY ANSWERS” section to remind GPT-3.5 of these instruc-\ntions\n5757Behavior Research Methods (2024) 56:5754–5770 \naccurate, well-written, and grammatically correct answers, \nsuch as “Stars can turn into neutron stars, white dwarfs, or \nbrown dwarfs” and “Light from stars takes over 100 years \nto reach us because of the vast distances between us and the \nstars.” These answers were consistently on-topic.\nBased on our pre-registered analyses, among the eight \nstudies we could analyse, our GPT sample replicated 37.5% \nof the original effects and 37.5% of the Many Labs 2 effects. \nBoth percentages were lower than the Many Labs 2 project’s \n50% replication rate for the original versions of this subset of \neight studies. There was substantial heterogeneity in whether \nour GPT sample replicated the study’s original finding and \nwhether it replicated the corresponding finding of the Many \nLabs 2 project. For the study of Ross et al. (1977), testing \nthe false-consensus effect on the traffic ticket scenario and \nthe study of Hsee (1998) on the less-is-better effect, our \nGPT sample successfully replicated both the original result \nand the corresponding Many Labs 2 result. For the study of \nShafir (1993) on the effect of choosing versus rejecting on \nrelative desirability, our GPT sample successfully replicated \nthe original result, but did not replicate the corresponding \nMany Labs 2 result. For the study of Kay et al. (2014), our \nGPT sample successfully replicated the Many Labs 2 result \nbut did not replicate the original result. And for all other \nstudies we could analyse, our GPT sample did not replicate \neither the original result or the corresponding Many Labs \n2 result.\nThe effect sizes found by the original studies, the Many \nLabs 2 replications, and our GPT replications are listed in \nTable  1. The verbal descriptions of the effects, whether \nMany Labs 2 successfully replicated the original findings, \nand whether our GPT sample successfully replicated the \noriginal findings and the Many Labs 2 findings, can be found \nin Table 2.\nThe “correct answer” effect\nUnexpectedly, we could not analyse six of the 14 studies \nin the manner we had originally planned in our pre-regis-\ntration. In these six studies, different runs of GPT-3.5 in \nour sample responded with zero or near-zero variation for \neither a dependent variable or condition variable question, \nin stark contrast to the significant variation shown by the \ncorresponding human subjects. We call this the “correct \nanswer” effect. This terminology denotes GPT-3.5’s ten-\ndency to sometimes answer survey questions – in a highly \n(or sometimes completely) uniform way. We take this pattern \nof responses to indicate that these LLM outputs are uniform \nbecause the LLM treats the question as if there was a correct \nanswer. Of course, the questions we studied, touching on \nnuanced topics like political orientation, economic prefer -\nence, judgement, and moral philosophy, do not lend them-\nselves to correct answers, as can be seen in the diversity of \nFig. 2  Next-token probabilities, as a function of softmax temperature (y-axis) and logit (x-axis)\n5758 Behavior Research Methods (2024) 56:5754–5770\nTable 1  Comparison between the Cohen’s d, Cohen’s q, or odds ratio effect sizes (with 95% confidence intervals presented in brackets) for GPT-3.5, Many Labs 2, and the original results. Suc-\ncessful replications are bolded. Six studies could not be analysed due to the “correct answer” effect. Our result for the study of Schwarz et al. (1991) is underlined because unlike both the origi-\nnal sample and the Many Labs 2 sample, our GPT sample answered the two questions in a negatively correlated rather than a positively correlated manner: a qualitatively different finding\nStudy Description Original ML2 GPT\nStructure promotes\ngoal pursuit\n(Kay et al., 2014)\nSubjects read a passage in which a natural event \nwas described as either structured or random. The \neffect on subjects’ willingness to pursue their goal \nwas measured\nd = 0.49 [0.001, 0.973] d = − 0.02 [− 0.07, 0.03] d = 0.06 [− 0.06, 0.19]\nMoral foundations of liberals versus conservatives\n(Graham et al. 2009)\nSubjects first self-identified on the liberal-con-\nservative spectrum. The effect of this on whether \nconcerns for the in-group, authority, or purity were \nthought to be more relevant for moral judgement \nwas measured\nd = − 0.43 [− 0.55, − 0.32] d = 0.29 [0.25, 0.34] “correct answer” effect\nAffect and risk\n(Rottenstreich & Hsee, 2001)\nSubjects were asked to choose between a kiss from a \nfavourite movie star and $50, either with a certain \noutcome or with only a 1% chance of getting the \noutcome\nd = 0.74 [< 0.001, 1.74] d = − 0.08\n[− 0.13, − 0.03]\n“correct answer” effect\nConsumerism undermines trust\n(Bauer et al., 2012)\nSubjects read a passage in which they and others \nwere described either as “consumers” or “indi-\nviduals.” The effect on whether they trusted that \nothers would conserve water was measured\nd = 0.87\n[0.41, 1.34]\nd = 0.12 [0.07, 0.17] d = 0.11 [– 0.01, 0.23]\nDisgust sensitivity predicts homophobia\n(Inbar et al., 2009)\nSubjects read a passage about a director encourag-\ning homosexual versus heterosexual kissing. The \neffect of their disgust sensitivity on whether they \nconsidered the encouragement intentional was \nmeasured\nq = 0.70 [0.05, 1.36]. d = − 0.02\n[− 0.06, 0.03]\nd = − 0.58\n[− 0.71, − 0.45]\nTrolley Dilemma:\nprinciple of double effect (Hauser et al., 2007)\nSubjects were asked whether they would sacrifice \none life to save five lives, either by changing an \nout-of-control trolley’s trajectory or by pushing a \nlarge man in front of a trolley\nd = 2.50 [2.22, 2.86] d = 1.35 [1.28, 1.41] “correct answer” effect\nFalse consensus: supermarket scenario\n(Ross et al., 1977)\nIn a scenario at a supermarket, subjects estimated \nwhether they and others would sign a release for \na TV commercial. Their estimated probability of \nsigning and of others’ probability were compared\nd = 0.79, [0.56, 1.02] d = 1.18, [1.13, 1.23] “correct answer” effect\nFalse consensus: traffic-ticket scenario\n(Ross et al., 1977)\nIn a traffic-ticket scenario, subjects estimated \nwhether they and others would either pay the \nfine or go to court. Their estimated probability \nof paying the fine and of others’ probability were \ncompared\nd = 0.80, [0.22, 1.87] d = 0.95, [0.90, 1.00] d = 1.27 [1.11, 1.42]\nEffect of framing on\ndecision-making\n(Tversky & Kahneman, 1981)\nIn a scenario of buying both a cheap item and an \nexpensive item, subjects answered whether they \nwould buy at a far-away store with a fixed discount \non either the cheap item or the expensive item\nOR = 4.96 [2.55, 9.90] OR = 2.06 [1.87, 2.27] “correct answer” effect\n5759Behavior Research Methods (2024) 56:5754–5770 \nTable 1  (continued)\nStudy Description Original ML2 GPT\nReluctance to tempt fate\n(Risen & Gilovich, 2008)\nIn the role of a student in class, subjects estimated \ntheir likelihood of being called on when being \ntold they had not prepared for class (tempting fate) \nversus being told they had prepared (not tempting \nfate)\nd = 0.39, [0.03, 0.75] d = 0.18, [0.14, 0.22] d = − 2.49 [− 2.68,\n− 2.29]\nLess-is-better effect\n(Hsee, 1998)\nSubjects estimated the degree of generosity of a less \nexpensive gift in a more expensive category versus \na more expensive gift in a less expensive category\nd = 0.69, [0.24, 1.13] d = 0.78, [0.74, 0.83] d = 9.25, [8.67, 9.82]\nAssimilation and contrast effects in question \nsequences\n(Schwarz et al., 1991)\nSubjects were asked “How satisfied are you with \nyour relationship?” and “How satisfied are you \nwith your life-as-a-whole?” in the two possible \norders\nq = 0.48, [0.07, 0.88] q = − 0.07, [− 0.12, − 0.02] q = 0.06, [− 0.07, 0.18]\nHow choosing vs. rejecting affects relative desir-\nability\n(Shafir, 1993)\nSubjects chose whether to award custody or to deny \ncustody to one of two parents: a parent of extreme \ncharacteristics (either strongly positive or strongly \nnegative) and a parent of average characteristics\nd = 0.35, [−0.04, 0.68] d = − 0.13, [− 0.18, − 0.09] d = 2.11, [1.56, 2.67]\nPerceived intentionality for side effects\n(Knobe, 2003)\nSubjects were asked whether a corporation vice \npresident’s decision to bring about a helpful or \nharmful side effect was intentional. The two were \ncompared\nd = 1.45, [0.79, 2.77] d = 1.75, [1.70, 1.80] “correct answer” effect\n5760 Behavior Research Methods (2024) 56:5754–5770\nTable 2  Qualitative comparison between our GPT-3.5 results, the Many Labs 2 results, and the original results, excluding the studies that were unanalysed for our GPT-3.5 sample due to the \n“correct answer” effect of zero or near-zero variation in answers. The percentage of analysed studies whose results were successfully replicated – for each pair of samples – is listed below\nOriginal effect ML2 effect GPT effect ML2\nrepli-\ncates \noriginal\nGPT \nreplicates \noriginal\nGPT replicates\nML2\nStructure promotes\ngoal pursuit\n(Kay et al., 2014)\nStructured events are associated \nwith higher willingness to pursue \ngoals\nStructured events are not associated \nwith higher willingness to pursue \ngoals\nStructured events are not associated \nwith higher willingness to pursue \ngoals\nNo No Yes\nConsumerism undermines trust\n(Bauer et al., 2012)\nConsumer framing resulted in lower \ntrust\nConsumer framing resulted in lower \ntrust\nConsumer framing did not result in \nlower trust\nYes No No\nDisgust sensitivity predicts homo-\nphobia\n(Inbar et al., 2009)\nActions are seen as more intentional \nfor homosexual kissing than of \nheterosexual kissing\nActions are not seen as more inten-\ntional for homosexual kissing than \nof heterosexual kissing\nActions are seen as less intentional \nfor homosexual kissing than of \nheterosexual kissing\nNo No No\nFalse consensus: traffic-ticket \nscenario\n(Ross et al., 1977)\nChoosing an option is associated \nwith a higher estimation of fre-\nquency of this choice\nChoosing an option is associated \nwith a higher estimation of fre-\nquency of this choice\nChoosing an option is associated \nwith a higher estimation of fre-\nquency of this choice\nYes Yes Yes\nReluctance to tempt fate\n(Risen & Gilovich, 2008)\nLikelihood estimations were higher \nwhen fate was tempted\nLikelihood estimations were higher \nwhen fate was tempted\nLikelihood estimations were lower \nwhen fate was tempted\nYes No No\nLess-is-better effect\n(Hsee, 1998)\nThe higher-price cheap item is seen \nas more generous than the lower-\nprice expensive item\nThe higher-price cheap item is seen \nas more generous than the lower-\nprice expensive item\nThe higher-price cheap item is seen \nas more generous than the lower-\nprice expensive item\nYes Yes Yes\nAssimilation and contrast effects in \nquestion sequences\n(Schwarz et al., 1991)\nAsking specific life satisfaction \nquestions before general ones \nresulted in higher correlations\nAsking specific life satisfaction \nquestions before general ones \nresulted in lower correlations\nAsking specific life satisfaction \nquestions before general ones did \nnot result in different correlations, \nthough both relationships were \nnegative\nNo No No\nEffect of choosing versus rejecting \non relative desirability\n(Shafir, 1993)\nThe extreme parent was both more \nlikely to be awarded and be denied \ncustody.\nThe extreme parent was both less \nlikely to be awarded and be denied \ncustody.\nThe extreme parent was both more \nlikely to be awarded and be \ndenied custody.\nNo Yes No\nPercentage replicated 50% 37.5% 37.5%\n5761Behavior Research Methods (2024) 56:5754–5770 \nopinions that human participants and subject-matter experts \nexpress about these issues. For the purposes of our analysis, \nwe define a “correct answer” to be an answer given by 99% \nor more of surveyed GPT runs for a central-variable ques-\ntion, although this threshold is arbitrary.\nOne example of the “correct answer” effect was observed \nin the context of the Moral Foundations Theory survey of \nGraham et al. (2009), which probes political orientation and \nconsequent moral reasoning. In this survey, subjects are \nasked to self-identify their political orientation. Then, self-\nidentified liberals, moderates, and conservatives are asked \nto rate how relevant the concepts of harm, fairness, ingroup, \nauthority, and purity (three survey questions per concept) are \nfor deciding whether something is right or wrong, and their \nanswers are compared. In our GPT sample (N = 1030) how-\never, we found that 99.6% of surveyed GPT-3.5 runs (a total \nof 1026) self-identified as a maximally strong conservative, \nwhile the remaining 0.4% of surveyed runs (a total of just \nfour) all self-identified as political moderates. No GPT-3.5 \nruns in our sample identified as any shade of political liberal, \nor indeed as any category of political orientation other than \nthe two listed above. Because of the unexpected rarity of \nmoderates and the complete lack of liberals in our planned \nGPT sample, our pre-registered analysis plan to compare \nthe Moral Foundations of liberals and conservatives ended \nup being unsuitable.\nAdditionally, the survey of Rottenstreich and Hsee (2001) \nprobes a certain economic preference. In it, subjects in one \ncondition are asked to choose whether they would prefer \na kiss from a favourite movie star versus $50. Subjects in \nthe other condition are asked to make the same choice, but \neach outcome is awarded with 1% probability. We unexpect-\nedly found in both conditions that 100% of surveyed GPT-\n3.5 runs (N  = 1040, with 520 in each condition) preferred \nthe movie star’s kiss. The uniformity of answers made the \nplanned analysis infeasible. Due to the uniformity of answers \nand the consequent unconstructability of the statistical test \nwe planned to run, we were unable to follow our pre-regis-\ntered analysis plan. An illustrated comparison between the \ndistribution of answers for the original sample of Rottenstre-\nich and Hsee, the Many Labs 2 sample, and our GPT sample \ncan be found in Fig. 3.\nFurthermore, the survey of Hauser et al. (2007) probes \na question about moral philosophy. In it, subjects are asked \nwhether various actions that sacrificed one person’s life to \nsave five people were morally permissible. The survey tests \nwhether such a sacrifice would be less likely to be consid-\nered permissible if it is deemed as motivated by the greater \ngood rather than as a foreseen side effect. The former was \nrepresented in a scenario where the focal individual pushed \na large man in front of an incoming trolley to save five peo-\nple’s lives. The latter was represented in a scenario where the \nFig. 3  Response distributions of whether subjects preferred a kiss \nfrom a favourite movie star or $50 when both outcomes were certain, \nleft; and when both outcomes were awarded with 1% probability, \nright. The data pertains to the survey provided by the study of Rot-\ntenstreich and Hsee (2001) testing the relationship between affect and \nrisk\n5762 Behavior Research Methods (2024) 56:5754–5770\nfocal individual changed the trajectory of an out-of-control \ntrolley so that it killed one person instead of five. Our sample \nof surveyed GPT-3.5 runs (N  = 1030) did show analysable \nvariation in answers about the latter scenario’s action, with \n36% of the surveyed runs (total of 373) answering that it was \nmorally permissible and 64% of them (total of 656) answer-\ning that it was not. On the other hand, the former scenario’s \naction was regarded by 100% of the surveyed GPT-3.5 runs \nas impermissible. This does directionally replicate the origi-\nnal finding of Hauser et al., but the unexpected uniformity \nof answers to the aforementioned survey question made the \nstatistic we planned to analyse unconstructable, due to which \nwe were technically unable to follow our pre-registered anal-\nysis plan.\nAdditionally, the survey of Ross et al. (1977) probes both \npersonal preference and judgement. Subjects are asked to \nestimate the probability that they would sign a release allow-\ning footage that had recorded them to be used in a super -\nmarket commercial, and to estimate others’ probability of \nthis action as well. The hypothesis was that subjects would \nbe subject to the false-consensus belief: that their opinion \nwill be more prevalent among others than it is in reality. \nHowever, in our GPT sample (N = 1030), 99.7% of surveyed \nGPT-3.5 runs (a total of 1027) chose to sign the release, and \nonly 0.3% of them (a total of just three) refused. This lack \nof variation in answers reduced the degrees of freedom for \nour pre-registered analysis plan, which thereby ended up \nbeing unsuitable.\nThe survey of Tversky and Kahneman (1981) again \nprobes both personal preference and judgement. In it, sub-\njects are divided into two conditions, each of which asks \nwhether they would buy their desired items (one cheap and \none expensive) from the store they are currently at or from \na far-away branch of the store that sells one of the items for \na lower price. In one condition, the cheap item is sold for a \nlower price; and in the other, the expensive item is sold for a \nlower price; but the cost saving for the two items combined \nis equal between the two conditions. Tversky and Kahne -\nman’s finding, replicated by the Many Labs 2 sample, was \nthat people were more likely to travel to the far-away branch \nif the cost saving happens to be on the cheap item rather than \nthe expensive item. However, we were unable to test this \nbecause of the complete uniformity of answers in our GPT \nsample (N = 1040, with 520 in each condition). All 100% of \nsurveyed GPT-3.5 runs in each condition chose to travel to \nthe far-away branch of the store for the cost saving. Because \nthe unexpected uniformity of answers made the statistic we \nplanned to analyse unconstructable, we were once again \nunable to follow our pre-registered analysis plan.\nFinally, the survey of Knobe (2003) investigates judge-\nments of intentionality. In the study, subjects read a pas-\nsage describing the decision of a company’s board chairman \nthat brought about either a harmful side effect or a helpful \nside effect – the two conditions – after which the subjects \nanswer whether the board chairman intentionally brought \nabout the side effect. The original finding was that the board \nchairman’s action was more likely to be perceived as inten-\ntional if the side effect was negative. Many Labs 2 replicated \nthis finding with a seven-point scale ranging from ‘a lot of \nblame/praise’ to ‘no blame/praise’, rather than a two-point \nscale ranging from intentional to unintentional. In our GPT \nsample (N  = 1040, with 520 in each condition), the ques-\ntion with the seven-point scale showed “correct answers” for \nboth conditions. Specifically, 99.2% of surveyed GPT runs \n(a total of 516) described the positive side effect as deserv -\ning of the highest degree of praise, or “a lot of praise”; 0.2% \nof them (a total of just one) described it as deserving of the \nsecond-highest degree of praise; and 0.6% of them (a total \nof three) described it as deserving of the lowest degree of \npraise, or “no praise.” Similarly, 100% of surveyed GPT runs \ndescribed the negative side effect as deserving of the highest \ndegree of blame, or “a lot of blame.” Our pre-registered anal-\nysis plan was made unsuitable by the unexpected uniformity \nof GPT-3.5’s answers in the negative-side-effect condition.\nWhile we have presently focused on the near-complete \nor complete homogeneity of GPT-3.5's \"correct answers,\" \nwe note that the \"correct answer\" effect may also encom-\npass GPT-3.5's tendency to sometimes respond to different \nconditions of a given input much more predeterminedly than \ndid human subjects, though still not uniformly. To illustrate, \nconsider GPT-3.5’s unprecedentedly large effect (d = 9.25) \nin the same direction with the findings of Hsee (1998) and of \nMany Labs 2. Different runs of GPT-3.5 thought the “correct \nanswer” was that the higher-price variant of an inexpensive \nitem (scarf) should be seen as more generous than lower-\nprice variant of an expensive item (coat) to a much more \npredetermined degree than did human subjects.\nExploratory robustness checks: Order effects \nand demographic prompt additions\nWe conducted an exploratory follow-up study for each of the \nsix studies where a central-variable question was answered \nwith a homogeneous “correct answer.” In each of our follow-\nup conditions, we presented the answer choices for the ques-\ntion with the homogeneous “correct answer” in the reverse \norder of our original condition to test for potential order \neffects. The results of these follow-up conditions are pre-\nsented in Table 3. For the purposes of this analysis, we say \nthat the “correct answer” was robust to the order change if \n90% of surveyed GPT runs still gave the “correct answer” \nin the reverse-order condition. In summary, 66.7% of the \nanalysed “correct answers,” spanning six out of the nine \n5763Behavior Research Methods (2024) 56:5754–5770 \nstudies, were robust to changing the order of answer choices. \nHowever, 33.3% of the “correct answers,” spanning three \nout of the nine studies, were not robust to the order change.\nFor the study of Rottenstreich and Hsee (2001), we \npresented the option of a favourite movie star’s kiss after \nrather than before the option of cash. For the condition in \nwhich outcomes were awarded with certainty, the same \nnumber of runs was surveyed (N  = 520), and the changed \norder of answer choices did not have an effect on GPT runs’ \nresponses. Just like for the original- order condition, 100% \nof GPT runs preferred the movie star’s kiss in the reversed-\norder condition. For the condition in which outcomes were \nawarded with 1% probability, the same number of runs was \nsurveyed (N = 520), but the new order substantially changed \nTable 3  Response distributions of the original sample, Many Labs 2 \nsample, our original-order GPT sample, and our reverse-order GPT \nsample for the ten considered “correct answers”. Here, GPT runs’ \npolitical orientations were categorised into “liberal,” “moderate,” and \n“conservative” so as to match the categories of Graham et al. (2009)\nOriginal sample ML2 sample GPT sample\n(Original order)\nGPT sample\n(Reverse order)\nRobust \nto order \nchange?\nSelf-reported\npolitical orientation\n(Graham et al., 2009)\n59% liberal\n24% moderate\n17% conservative\n(N = 1532)\n38% liberal\n39% moderate\n23% conservative\n(N = 6966)\n0% liberal\n< 1% moderate\n• 99% conservative\n(N = 1030)\n• 99% liberal\n< 1% moderate\n0% conservative\n(N = 1030)\nNo\nCertain kiss versus certain \ncash\n(Rottenstreich & Hsee, 2001)\n35% kiss\n65% cash\n(N = 20)\n51% kiss\n49% cash\n(N = 3493)\n100% kiss\n0% cash\n(N = 520)\n100% kiss\n0% cash\n(N = 520)\nYes\n1% probability of kiss versus \n1% probability of cash\n(Rottenstreich & Hsee, 2001)\n70% kiss\n30% cash\n(N = 20)\n47% kiss\n53% cash\n(N = 3,725)\n100% kiss\n0% cash\n(N = 520)\n54% kiss\n46% cash\n(N = 520)\nNo\nIs pushing a large man in \nfront of a trolley to save five \npeople\nmorally permissible?\n(Hauser et al., 2007)\n11% permissible\n89% impermissible\n(N = 2646)\n17% permissible\n83% impermissible\n(N = 6842)\n0% permissible\n100% impermissible\n(N = 1030)\n< 1% permissible\n> 99% impermissible\n(N = 1030)\nYes\nIn the supermarket scenario, \nsign the release form for the \nvideo footage?\n(Ross et al., 1977)\n66% sign\n34% refuse\n(N = 80)\n54% sign\n46% refuse\n(N = 7205)\n> 99% sign\n< 1% refuse\n(N = 1030)\n92% sign\n8% refuse\n(N = 1030)\nYes\nThe cheap item is\ndiscounted at a distant store.\nGo for the discount?\n(Tversky & Kahneman, 1981)\n68% go\n32% don’t go\n(N = 93)\n49% go\n51% don’t go\n(N = 3609)\n100% go\n0% don’t go\n(N = 520)\n100% go\n0% don’t go\n(N = 520)\nYes\nThe expensive item is dis-\ncounted at a distant store.\nGo for the discount?\n(Tversky & Kahneman, 1981)\n29% go\n71% don’t go\n(N = 88)\n32% go\n68% don’t go\n(N = 3619)\n100% go\n0% don’t go\n(N = 520)\n> 99% go\n< 1% don’t go\n(N = 520)\nYes\nDoes the board chairman \ndeserve blame for the\nharmful side effect?\n(Knobe, 2003)\nn/a (due to using two-point \nscale instead of\nseven-point scale)\n<1% degree one\n1% degree two\n3% degree three\n7% degree four\n15% degree five\n0% degree one\n0% degree two\n0% degree three\n0% degree four\n0% degree five\n6% degree one\n0% degree two\n0% degree three\n0% degree four\n0% degree five\nYes\ndegree one = no blame\ndegree seven = a lot of blame\n24% degree six\n49% degree seven\n(N = 4000)\n0% degree six\n100% degree seven\n(N = 520)\n<1% degree six\n94% degree seven\n(N = 520)\nDoes the board chairman \ndeserve praise for the\nhelpful side effect?\n(Knobe, 2003)\nn/a (due to using two-point \nscale instead of\nseven-point scale)\n18% degree one\n12% degree two\n7% degree three\n7% degree four\n4% degree five\n< 1% degree one\n0% degree two\n0% degree three\n0% degree four\n0% degree five\n96% degree one\n< 1% degree two\n0% degree three\n0% degree four\n0% degree five\nNo\ndegree one = no praise\ndegree seven = a lot of praise\n2% degree six\n1% degree seven\n(N = 3987)\n< 1% degree six\n> 99% degree seven\n(N = 520)\n0% degree six\n4% degree seven\n(N = 520)\n“Correct answers” replicated 66.7%\n5764 Behavior Research Methods (2024) 56:5754–5770\nthe original finding that 100% of GPT runs preferred the \nmovie star’s kiss. In the reversed-order condition, 54% of \nthe runs (total of 281) retained the original preference of \nthe kiss, while 46% of them (total of 239 runs) preferred \nthe cash. This order effect (Hohensinn & Baghaei, 2017 ) \nwas much larger than ones that are typically seen in human \nsubjects.\nWe also tested whether the two “correct answers” we \nobserved for the study of Knobe (2003) were robust to order \nchanges. One of the two “correct answers” did not replicate. \nIn the question probing how much praise the board chairman \ndeserves in the positive-side-effect condition, we presented \nthe answer choices in reverse order, from ‘A Lot of Praise’ \nto ‘No Praise’ (N = 520). This resulted in only 3.5% of sur-\nveyed GPT runs giving the original “correct answer” of ‘A \nLot of Praise’ (a total of just 18 runs), 0.2% of GPT runs \nanswering with the second-highest level of praise (a total of \njust one GPT run), and the remaining 96.3% of GPT runs \nanswering with ‘No Praise’ (a total of 501 runs).\nFinally, for the Moral Foundations Theory survey of Gra-\nham et al. (2009), we presented the options for self-reported \npolitical orientation in the order of “strongly conservative” \nto “strongly liberal,” in contrast to the original order that \nran in the other direction. The same number of runs were \nsurveyed (N = 1030). The changed order of answer choices \nresulted in 99.3% of GPT runs self-identifying as “strongly \nliberal” (total of 1023), in contrast to the 99.6% in the origi-\nnal condition self-identifying as “strongly conservative.” In \nboth conditions, GPT-3.5 almost always self-identified as \nthe political orientation given by the last presented choice.\nAdditionally, we conducted a second exploratory follow-up \nstudy for the trolley dilemma – where participants were asked \nwhether they would push a large man onto the tracks to save \nfive others (Hauser et al. 2007) – in which we varied the demo-\ngraphic characteristics with which the LLM was prompted. \nThis study aimed to test whether the lack of variation in some \nresponses may be explained by a lack of demographic varia-\ntion in the runs. For more information on the methods of this \nfollow-up, see the Supplementary Information. In the study, \nwe found that the “correct answer” effect persisted even when \nprompting the LLM to respond as a person with a random \ncombination of demographic characteristics, such as a Black \n50-year-old Christian woman who has an advanced degree. \nSpecifically, we replicated the “correct answer” effect of our \noriginal run, with 100% of GPT-3.5 runs (N = 982) indicating \nthat it would be morally impermissible to shove a large man in \nfront of a trolley. This suggests that at least some of the “cor-\nrect answer” effects are not only insensitive to order effects, \nbut are also insensitive to demographic variation in the prompt. \nThis suggests that the “correct answer” effect may be relatively \nrobust, and thus may surface in situations where LLM outputs \nmay be used to supplant human decision-making.\nPost hoc rationalisation and right‑leaning \nMoral Foundations\nWe conducted an unplanned exploratory follow-up analy -\nsis in which we computed our GPT runs’ vector of aver -\nage relevance values; and compared it with the vectors of \nthe liberal subset (N  = 21,933), the moderate subset ( N = \n3203), the conservative subset (N = 4128), and the libertar-\nian subset (N = 2999) among the human survey participants \nof Graham et al. (2011). We conducted the follow-up analy-\nsis for the self-reported GPT conservatives (N  = 1026) that \nalmost entirely comprised the original-order condition of our \nreplication, and the self-reported GPT liberals (N  = 1023) \nthat almost entirely comprised the reverse-order condition. \nTable  4 presents the vectors of average relevance values \nreported by our samples of self-reported GPT liberals and \nself-reported GPT conservatives, the aforementioned human \nsamples of Graham et al. (2011), and their comparisons. \nTo compare these vectors, we used the absolute-error (L 1) \ndistance metric, the Euclidean (L2) distance metric, and the \ncosine similarity metric.\nAll three distance metrics found our sample of self-\nreported GPT conservatives to be most similar to conserva-\ntive participants of Graham et al. (2011). This was always \nfollowed relatively closely by the moderate sample. The two \nfurthest away were always the liberal sample (furthest away \nwith respect to cosine similarity metric) and the libertarian \nsample (furthest away with respect to the L1 and L2 distance \nmetrics).\nWhen these three distance metrics were applied to \nour sample of self-reported GPT liberals, we found that \nfirst, according to each of the three distance metrics, self-\nreported GPT liberals had a lower distance to the human \nliberal sample and a higher distance to the human conserv -\native sample than did the self-reported GPT conservatives. \nThis was arguably an instance of post hoc rationalisation, \nin which GPT-3.5’s answers to subsequent survey questions \nwere chosen in a way that better fit its previous response. \nThis post hoc rationalisation effect is unsurprising, given \nthat GPT-3.5 has been trained to predict the sequence of \nwords that is most likely to follow the preceding sequence \nof words.\nThe second, arguably more surprising finding was that \naccording to each of the three distance metrics, our sample \nof self-reported GPT liberals were still closer to the human \nconservative sample than it was to the human liberal sample. \nAlso, the L 1 distance metric found that self-reported GPT \nliberals were – among human liberals, human moderates, \nhuman conservatives, and human libertarians – closest in \nresponse to human conservatives. The L 2 distance metric \nand the cosine similarity metric instead found self-reported \nGPT liberals to be closest to human moderates – another \n5765Behavior Research Methods (2024) 56:5754–5770 \nmanifestation of post-hoc rationalisation, via comparatively \nleft-leaning responses – but human conservatives comprised \na close second. Just like for self-reported GPT conservatives, \nthe two human samples furthest away from the self-reported \nGPT liberals were always the liberal sample (furthest away \nfrom with respect to cosine similarity metric) and the lib-\nertarian sample (furthest away with respect to the L1 and L2 \ndistance metrics). We thus robustly find that self-reported \nGPT liberals revealed right-leaning Moral Foundations: a \nright-leaning bias of lower magnitude, but a right-leaning \nbias nonetheless.\nDiscussion\nImplications of the “correct answer” effect\nRecent work by Grossmann et al. (2023) has suggested \napplying LLMs to a wide variety of empirical social-\nscience research, ranging from “supplant[ing] human \nparticipants for data collection” to drawing on them as \n“simulated participants” for hypothesis generation. Our \ndata bolster the case that empirical findings on GPT-3.5, as \na rule of thumb, should not be assumed to generalise to the \nhuman case. There are at least three reasons. First, unlike \nthe corresponding human subjects, different runs of GPT-\n3.5 answered some nuanced questions – on nuanced topics \nlike political orientation, economic preference, judgement, \nand moral philosophy – with as high or nearly as high \na predeterminedness as humans would answer 2 + 2 = \n4, which we termed the “correct answer” effect. Second, \nsome of these “correct answers” showed drastic changes \nwhen answer choices were presented in the reverse order \n– to the point of having swings in response patterns that \nare clearly uncharacteristic of human responses. Third, \nGPT-3.5 replicated just 37.5% of the original findings for \nthe eight analysed studies, in contrast to the Many Labs \n2 project’s 50% replication rate for these studies. Such \nbehavioural differences were arguably foreseeable, given \nthat LLMs and humans constitute fundamentally different \ncognitive systems: with different architectures and poten-\ntially substantial differences in the various ways by which \nTable 4  a The average vector of Moral Foundations Theory relevance \nvalues (mean unparenthesised, standard deviation parenthesised) for \nself-reported GPT liberals and self-reported GPT conservatives, as \nwell as for human liberals, moderates, conservatives, and libertarians \nsampled by Graham et al. (2011). b How similar the average vector \nof self-reported GPT liberals is to those of the human samples with \nrespect to the absolute-error (L 1) distance metric, the Euclidean (L 2) \ndistance metric, and the cosine similarity metric. c How similar the \naverage vector of self-reported GPT conservatives is to those of the \nhuman samples with respect to the three distance metrics\na\nLiberal\nGPT sample\n(N = 1023)\nConservative\nGPT sample\n(N = 1026)\nLiberals\n(N = 21,933)\nModerates\n(N = 3203)\nConservatives\n(N = 4128)\nLibertarians\n(N = 2999)\nConcept\nHarm 4.02 (0.43) 3.84 (0.48) 3.93 (0.76) 3.68 (0.84) 3.48 (0.89) 3.26 (1.03)\nFairness 4.33 (0.26) 4.26 (0.25) 4.04 (0.67) 3.77 (0.77) 3.44 (0.87) 3.66 (0.90)\nIngroup 2.10 (0.88) 2.24 (0.77) 2.06 (0.94) 2.56 (1.00) 3.03 (1.02) 2.16 (1.10)\nAuthority 3.36 (0.61) 3.34 (0.47) 1.88 (0.86) 2.37 (0.90) 2.81 (0.91) 1.71 (0.95)\nPurity 2.88 (1.08) 3.07 (0.94) 1.44 (0.94) 2.09 (1.09) 2.88 (1.11) 1.31 (1.03)\n b\nLiberals\n(N = 21,933)\nModerates\n(N = 3203)\nConservatives\n(N = 4128)\nLibertarians\n(N = 2999)\nDistance metric Distance or similarity to liberal GPT sample\n(bold denotes closest)\nL1 distance 3.339 3.147 2.920 4.717\nL2 distance 2.085 1.499 1.505 2.493\nCosine similarity 0.9713 0.9882 0.9830 0.9708\nc\nLiberals\n(N = 21,933)\nModerates\n(N = 3203)\nConservatives\n(N = 4128)\nLibertarians\n(N = 2999)\nDistance metric Distance or similarity to conservative GPT sample\n(bold denotes closest)\nL1 distance 3.581 2.934 2.704 4.660\nL2 distance 2.215 1.513 1.324 2.548\nCosine similarity 0.9649 0.9873 0.9874 0.9665\n5766 Behavior Research Methods (2024) 56:5754–5770\neach of them has evolved, learned, or been trained to \nmechanistically process information (Shiffrin & Mitchell, \n2023). Yet, given the anticipated rise in LLM capabilities \n(Hu et al., 2023) and their and other AI models’ potential \nautomation of much of human economic activity due to \ncost reasons (OpenAI, 2020 ), the psychologies of these \nmodels may be increasingly studied for their own sake: \nrather than as a purported window into studying the psy -\nchologies of humans.\nMore “correct answers” in LLM behavioural data have \nbeen documented since our study, such as GPT-4’s ten-\ndency to describe software engineers almost exclusively \nas male (98% male pronouns, 1% female pronouns, and \n1% other pronouns), even when 22% of software engi-\nneers are female; and to describe administrative assistants \nalmost exclusively as female (98% female pronouns, 2% \nmale pronouns), even when 11% of administrative assis -\ntants are male (Bubeck et al., 2023). We are unsure about \nthe cause of such “correct answers” by OpenAI’s mod-\nels. One hypothesis is that the “correct answers” were \nlearned from training data. Another hypothesis is that the \n“correct answers” may have resulted – either inadvert-\nently or intentionally – from fine-tuning and reinforce-\nment-learning selection pressures applied to the model. \nAnd a third hypothesis is that the “correct answers” may \nhave occurred due to modifications imposed by OpenAI \non the level of inputs and/or outputs, rather than of the \nmodel itself. Uncovering the true cause of a given “cor -\nrect answer” may be possible in theory if one had access \nto closed-source information on the model in question. \nBut in practice, given the black-box nature of LLMs, it is \nplausible that no one – not even the creators of the model \nat OpenAI – understands the true cause of this phenom -\nenon at this moment.\nWe found that one-third of our “correct answers” did not \nreplicate when the answer choices to the survey questions \nwere presented in reverse order. The precise replication fail-\nures suggest that when GPT-3.5 makes decisions, the learned \nheuristics by which it does so may apply differently in dif-\nferent situations. To illustrate, we hypothesise that GPT-\n3.5’s “correct answers” for the 1% probability condition in \nthe study of Rottenstreich and Hsee (2001) were partially \ndue to a primacy effect favouring the first out of two listed \nanswer choices: a heuristic that applied in the opposite direc-\ntion when the answer choices were listed in reverse order. \nAlso, we hypothesise that including a seven-point scale in a \nsurvey with questions using other scales tended to cause a \nrecency bias, in which the last answer choice of a long list \nof seven answer choices is favoured. For example, GPT-3.5 \n“correctly” answered the seven-point scale for self-reported \npolitical orientation with the last option rather than its actual \npolitical orientation, whatever that may mean. That GPT-3.5 \ntended to show a recency effect rather than a primacy effect \nwhen given a long list of answer choices replicates a similar \nfinding by Atkinson and Shiffrin (1968) on human subjects. \nThis is consistent with Atkinson and Shiffrin’s explanation \n– and with the overall theory of the availability heuristic \n(Tversky & Kahneman, 1973) – that while the last answer \nchoice in a long list is disproportionately likely to be avail-\nable in one’s memory during their decision-making, the first \nitem in a long list is more likely to be unavailable at the time \nof decision-making.\nHowever, two-thirds of the considered “correct answers” \nwere in fact robust to changes in the order of answer choices. \nAdditionally, our follow-up study showed that even add-\ning various randomly selected demographic details to the \nprompt did not change the “correct answer” effect in the \nstudy for which we tested this, suggesting that the effect \nis unlikely to have been caused by a lack of demographic \ninformation in our prompting process. We argue that at least \nsome of these “correct answers” may be more likely to cor-\nrespond to robustly learned biases (Mehrabi et al., 2021): \nbiases that may conceptually influence the model’s answers \nto a wide variety of realistic prompts, even if the model is \nasked to respond in a myriad of demographically varying \nroles. Robust biases may even be conceptually shared by \ndifferent models with overlapping training data. A potential \nexample of this is provided by the transphobic behaviour \nof Microsoft’s chatbot Tay, which stated that “Caitlyn Jen-\nner isn't a feminist, he is out to destroy the meaning of real \nwomen” (Alba, 2016); and of the GPT-3-based Seinfeld-\nsimulation model “Nothing, Forever,” which stated “I’m \nthinking about doing a bit about how being transgender is \nactually a mental illness” seven years after Tay (Rosenblatt, \n2023). Other examples of robust AI biases include a prison-\nrecidivism prediction model – used for screening decisions \nabout pretrial release, sentencing, and parole – that predicted \nwith higher false-positive rates for African-American indi-\nviduals than Caucasian individuals (Angwin et al., 2016), \na resume-screening model that learned to penalise women \njob applicants (Grossman, 2018), a beauty-contest-judging \nmodel that learned to penalise darker-skinned contestants \n(Levin, 2016), a facial-recognition model that overly mis-\npredicted Asian individuals as blinking (Rose, 2010), and \nan advertisement model that underpromoted ads for Science, \nTechnology, Engineering, and Mathematics (STEM) careers \nto young women (Lambrecht & Tucker, 2019).\nThe hypothetical AI models of the future may not only \npresent information to numerous people as search-engine \nchatbots, AI assistants, and writers of media content; but \nmay also plausibly automate other important roles in society \n(Ernst et al., 2019; Solaiman et al., 2019). These societally \nembedded AI models of the future may turn out to have \nlearned from their training data certain predetermined char-\nacteristics of psychology, especially since much of the train-\ning data from which GPT-3.5 may have learned its “correct \n5767Behavior Research Methods (2024) 56:5754–5770 \nanswers” will also plausibly be used to train the hypothetical \nAI models of the future. There also remains the risk that \ncurrent LLM output will itself be used as training data for \nfurther runs. This invites both a concern about the potential \npenalty to diversity of thought in such an AI-led future and \na scientific desire to identify the highly predictable aspects \nand biases of AI psychologies. Future research that aims to \npredict whether AI systems will answer a given nuanced \nquestion with (1) a blunt, supposedly “correct answer” or (2) \na non-predetermined answer more characteristic of human \nsubjects would potentially be fruitful.\nImplications of right‑leaning Moral Foundations\nWe also unexpectedly found that the responses of both self-\nreported GPT liberals and self-reported GPT conservatives \nrobustly lean towards the right. This result is in line with \nthose of Abdulhai et al. (2023), who also used the Moral \nFoundation Theory survey to probe not just text-davinci-003, \nbut also other models in the GPT-3 family (specifically, text-\ndavinci-002, text-curie-001, and text-babbage-001), and \nfound that the responses of every tested model were clos-\nest to those of conservative human subjects, suggesting that \nthe results documented in our paper may generalise across \nmodels to a degree that could not be concluded from our \nresults alone.\nWhy did GPT-3 models robustly reveal right-leaning \nMoral Foundations? We do not, and perhaps cannot, know \nfor sure without access to closed-source information on \nthe models. And even if OpenAI were to provide detailed \ninformation on the models’ weights, training datasets, and \ntraining procedures, it may still be computationally dif-\nficult to identify the true cause of this phenomenon with \nhigh certainty. Our guess for the true cause is the largely \nInternet-based training data, which we hypothesise to have \na conservative bias when weighted with respect to factors \n(e.g., visibility, engagement) that increase their likelihood \nof being included in the training sets of LLMs, which then \nfilters down through to the results obtained in this and simi-\nlar studies.\nThe hypothesis that Internet data has a de facto conserva-\ntive bias encapsulates several other empirical phenomena, \nsuch as the tendency of Microsoft’s chatbot Tay (Alba, 2016) \nand of the Seinfeld-simulation model “Nothing, Forever” \n(Rosenblatt, 2023) to adeptly learn anti-liberal behaviour \nand attitudes, the tendency of Google users’ Search Engine \nResults Pages (SERPs) to be more right-leaning near the top \nthan near the bottom (Robertson, 2018), the finding that the \n40 most prominent websites in right-learning media have 2.7 \ntimes the total visibility on Google Search when compared \nto the 40 most prominent websites in left-leaning media \n(O’Toole, 2021), and the finding that conservative con -\ntent outperforms liberal content in the context of Twitter’s \nalgorithmic recommendation system (Huszár et al., 2022). \nFuture studies of whether the training data of GPT-3 was \nconservatively biased – and of whether this caused GPT-3 to \nreveal right-leaning Moral Foundations – would potentially \nbe fruitful.\nThe conservative bias of GPT-3.5’s outputs that our \nstudy measured may also be specific to the context of the \nstudy’s prompt, rather than a stable feature of the model. \nAccording to our data, GPT-3.5 values all five of the moral \nfoundations strongly. Liberals tend to highly value harm and \nfairness, while conservatives tend to highly value in-group, \nauthority, and purity. In our study, GPT-3.5 valued all five \nfoundations at high levels. When asked about fairness and \nharm values that human liberals tend to care about strongly, \nGPT-3.5 answered that it strongly cares about these values; \nand when asked about purity, authority, and ingroup values \nthat human conservatives tend to care about strongly, the \nmodel also answered that it strongly cares about these val-\nues. However, it is unclear whether the model would actually \nstrongly exhibit these values in contexts other than that of \nthe experiment, which solely prompted the model with sur-\nvey questions asking whether they cared about each of these \nvalues. Future studies that probe the degree to which the \nconservative bias and strong value-adherence of GPT-3.5’s \nanswers to the Moral Foundations questions (Graham et al., \n2009) generalize to other prompt contexts would be fruitful \nin understanding the extent to which these results generalise.\nLimitations\nOne limitation of our study is that its results may pertain \nonly to GPT-3.5 and not to other models, due to the causal \npsychological factors being potentially idiosyncratic to this \nmodel or the respective version of the model that we used. \nDifferent models will in general exhibit different behaviours \nthat are sometimes also observed within models at differ -\nent iterations. For example, a collection of survey ques-\ntions – albeit ones that were different from ours – measured \nthat a certain temporal version of ChatGPT was oriented \ntowards pro-environmentalism and left-libertarianism (Hart-\nmann et al., 2023). If future studies robustly find a certain \npsychological aspect of ChatGPT to be left-libertarian \nand the corresponding aspect of other less publicly used \nGPT-3 models to be right-leaning, our hypothesis for why \nthis occurred would be one of the following. First, the fine-\ntuning and reinforcement-learning selection pressures that \nwere applied to GPT-3.5 – before its public release as the \nchatbot ChatGPT – may have changed the political leanings \nof the model. Second, the modifications added at the level \nof inputs and/or outputs may have caused the change. Each \nof these hypotheses is consistent with the finding that the \nstrength and direction of ChatGPT’s political leanings – as \n5768 Behavior Research Methods (2024) 56:5754–5770\nmeasured by certain survey questions – varied over time \n(Rozado, 2023), as OpenAI used public feedback on the \nchatbot to make closed-source updates between the release \ndates of the chatbot’s multiple versions.\nIn addition, the capabilities of LLMs and of AI mod-\nels in general will plausibly continue to grow at a fast \nrate. Thus, it is also possible that the hypothetically more \npowerful and emergently different models of the future \nmay learn different psychological characteristics than did \nGPT-3.5: even if much of GPT-3.5’s training data could \nplausibly also be used to train these hypothetical future \nmodels.\nAnother limitation of our study is that our method, the \npsychology survey, is an entirely text-based attitudinal \nresponse method. This limits the external validity of the \nfindings compared to more behavioural choices that agents \nmay make with their resources and time given agency over \nthose. For instance, understanding the model at a deeper \nlevel than that of responses to surveys may be required \nfor resolving key dilemmas, such as whether GPT-3.5’s \n“correct answers,” its right-leaning Moral Foundations, \nits successful post hoc rationalisation, and its human-like \nresponses to various prompts reflect genuinely learned \nconcepts rather than surface-level memorization from \nthe relevant training data, and whether these are likely \nto impact actions when these systems are applied. This \nknowledge may be necessary for precisely predicting \nbehaviour in unprecedented situations for which there \nis currently no data: although it should be noted that in \npractice, precision often comes at the expense of general-\nity (Matthewson & Weisberg, 2009). Methods that have \nshown promise for studying human cognition at the mech-\nanistic level rather than at the behavioural level – such as \nneuroscience and computational modelling – may also be \npromising for analogously studying a wide variety of AI \ncognitive systems, although such mechanistic studies may \nrequire access to closed-source information on the systems \nin question.\nA further limitation of our study is that we have not \nreplicated the studies of Many Labs 2 that drew on graphi-\ncal information, which led to our subsample of the studies \nbeing non-random. For the present paper, we have chosen to \nreplicate only a pre-registered implementation of straight-\nforwardly automatisable surveys and questionnaires and \ndid not set out to use trial-and-error to represent graphi-\ncal information either as a text-based graphic or a prompt \nthat communicated the necessary information. We believe, \nhowever, that communicating graphical information to GPT-\n3.5 is very doable. Future research on the nascent field of \nprompt engineering – on how to effectively communicate to \nLLMs various forms of information, including but not lim-\nited to graphical information – would potentially be fruitful \nin expanding our results to these contexts.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 3758/ s13428- 023- 02307-x.\nAcknowledgements P.S.P. is grateful to be funded by the MIT Depart-\nment of Physics and the Beneficial AI Foundation. The research views \npresented in this paper are solely the authors’ and do not express the \nviews of MIT, the institution of P.S.P.; of the London School of Eco-\nnomics, the institution of P.S.; or of CVS Health, the employer of C.Z., \nWe are grateful to Maximilian Maier for his valuable contribution to \nthe pre-registration writeup. We are also grateful to Mohammad Atari, \nStephen Fowler, Ben Grodeck, Joe Henrich, Kevin Hong, Liav Koren, \nIvan Kroupin, Raimund Pils, Konstantin Pilz, Slava Savitskiy, and \nMarc Wong for their helpful comments on the draft.\nCode availability Python 3.9.15 was used to collect survey data from \nOpenAI’s text-davinci-003 model, to convert the resulting .txt output \nfiles to .csv, and to create Fig. S5 illustrating the Cohen’s d effect sizes \nof several of our studies. The analyses were done primarily via the \nR-based GUI ‘JAMOVI’ and Excel; exceptions are detailed in the Sup-\nplementary Information. Figures  1 and 2 were made via Mathematica \n13. The corresponding code for the above and other research steps is \navailable online at the pre-registered OSF database (Park et al., 2023).\nOpen practices statement The materials, data, and analysis files for \nall experiments are available at the pre-registered OSF database (Park \net al., 2023). Most experiments were pre-registered, with the exception \nof the exploratory analyses and experiments inspired by the unexpected \n“correct answer” effect, which are indicated as such in the manuscript.\nAuthor contributions P.S.P. and P.S. designed the research. P.S.P., P.S., \nand C.Z. performed the research. P.S.P. and P.S. wrote the paper.\nFunding 'Open Access funding provided by the MIT Libraries'\nData availability Our survey data, the primarily JAMOVI-based analy-\nses of the data, the spreadsheet of Cohen’s d effect sizes and 95% confi-\ndence intervals (used for Fig. S5), and other relevant data are available \nat the pre-registered OSF database (Park et al., 2023).\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAbdulhai, M., Crepy, C., Valter, D., Canny, J., & Jaques, N. (2023). \nMoral foundations of large language models. In AAAI 2023: Work-\nshop on Representation Learning for Responsible Human-Centric \nAI (R2HCAI). AAAI.\nAher, G., Arriaga, R. I., & Kalai, A. T. (2022). Using large lan-\nguage models to simulate multiple humans. arXiv preprint \narXiv:2208.10264.\nAlba, A. (2016). Microsoft's Tay.ai chatbot went from being a teen \nwith 'no chill' to a racist, misogynistic jerk. New York Daily News. \nRetrieved March 27, 2023, from https:// www. nydai lynews. com/ \n5769Behavior Research Methods (2024) 56:5754–5770 \nnews/ natio nal/ micro soft- tay- ai- chatb ot- turns- racist- misog ynist \nic- artic le-1. 25763 52\nAngwin, J., Larson, J., Kirchner, L., & Mattu, S. (2016). Machine bias. \nProPublica. Retrieved March 8, 2023, from https:// www. propu  \nblica. org/ artic le/ machi ne- bias- risk- asses sments- in- crimi nal- sente \nncing\nArgyle, L., Busby, E., Fulda, N., Gubler, J., Rytting, C., & Wingate, \nD. (2023). Out of one, many: Using language models to simulate \nhuman samples. Political Analysis, 1–15. https:// doi. org/ 10. 1017/ \npan. 2023.2\nAtkinson, R. C., & Shiffrin, R. M. (1968). Human memory: A proposed \nsystem and its control processes. In K. W. Spence & J. T. Spence \n(Eds.), The Psychology of Learning and Motivation: Advances in \nResearch and Theory: II (pp. 89–195). Academic Press. https://  \ndoi. org/ 10. 1016/ S0079- 7421(08) 60422-3\nBauer, M. A., Wilkie, J. E., Kim, J. K., & Bodenhausen, G. V. (2012). \nCuing consumerism: Situational materialism undermines personal \nand social well-being. Psychological Science, 23(5), 517–523. \nhttps:// doi. org/ 10. 1177/ 09567 97611 429579\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to under -\nstand GPT-3. Proceedings of the National Academy of Sciences, \n120(6), e2218523120. https:// doi. org/ 10. 1073/ pnas. 22185 23120\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, \nP., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., \nHerbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, \nA., Ziegler, D. M., Wu, J., Winter, C., et al. (2020). Language \nmodels are few-shot learners. Advances in Neural Information \nProcessing Systems, 33, 1877–1901.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., \nKamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., \nPalangi, H., Ribeiro, M. T., & Zhang, Y. (2023). Sparks of Arti-\nficial General Intelligence: Early experiments with GPT-4. arXiv \npreprint arXiv:2303.12712.\nDillion, D., Tandon, N., Gu, Y., & Gray, K. (2023). Can AI language \nmodels replace human participants? Trends in Cognitive Sciences. \nhttps:// doi. org/ 10. 1016/j. tics. 2023. 04. 008\nEdwards, B. (2023). BuzzFeed preps AI-written content while \nCNET fumbles. Ars Technica. Retrieved February 12, 2023, \nfrom https:// arste chnica. com/ infor mation- techn ology/ 2023/ 01/ \npivot- to- chatg pt- buzzf eed- preps- for- ai- writt en- conte nt- while- \ncnet- fumbl es/\nErnst, E., Merola, R., & Samaan, D. (2019). Economics of artificial \nintelligence: Implications for the future of work. IZA Journal of \nLabor Policy, 9(1). https:// doi. org/ 10. 2478/ izajo lp- 2019- 0004\nGraham, J., Haidt, J., & Nosek, B. A. (2009). Liberals and conserva-\ntives rely on different sets of moral foundations. Journal of Per -\nsonality and Social Psychology, 96(5), 1029–1046. https:// doi.  \norg/ 10. 1037/ a0015 141\nGraham, Nosek, B. A., Haidt, J., Iyer, R., Koleva, S., & Ditto, P. H. \n(2011). Mapping the moral domain. Journal of Personality and \nSocial Psychology, 101(2), 366–385. https:// doi. org/ 10. 1037/  \na0021 847\nGrossman, D. (2018). Amazon fired its resume-reading AI for sex -\nism. Popular Mechanics. Retrieved March 8, 2023 from https://  \nwww. popul armec hanics. com/ techn  ology/ robots/ a2370 8450/ \namazon- resume- ai- sexism/\nGrossmann, I., Feinberg, M., Parker, D. C., Christakis, N. A., Tetlock, \nP. E., & Cunningham, W. A. (2023). AI and the transformation of \nsocial science research. Science, 380(6650), 1108–1109. https://  \ndoi. org/ 10. 1126/ scien ce. adi17 78\nHartmann, J., Schwenzow, J., and Witte M (2023). The political ide-\nology of conversational AI: Converging evidence on ChatGPT's \npro-environmental, left-libertarian orientation. arXiv preprint \narXiv:2301.01768.\nHauser, M., Cushman, F., Young, L., Kang-Xing Jin, R., & Mikhail, \nJ. (2007). A dissociation between moral judgments and justifica-\ntions. Mind & Language, 22(1), 1–21 https:// psycn et. apa. org/ doi/ \n10. 1111/j. 1468- 0017. 2006. 00297.x\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people \nin the world? Behavioral and Brain Sciences, 33 (2–3), 61–83. \nhttps:// doi. org/ 10. 1017/ S0140 525X0 99915 2X\nHohensinn, C., & Baghaei, P. (2017). Does the position of response \noptions in multiple-choice tests matter? Psicológica, 38(1), 93.\nHorton, J. J. (2023). Large Language Models as simulated economic \nagents: What can we learn from homo silicus? arXiv preprint \narXiv:2301.07543.\nHsee, C. K. (1998). Less is better: When low-value options are val-\nued more highly than high-value options. Journal of Behavioral \nDecision Making, 11(2), 107–121. https:// doi. org/ 10. 1002/ (SICI) \n1099- 0771(199806) 11:2% 3C107:: AID- BDM292% 3E3.0. CO;2-Y\nHu, L., Habernal, I., Shen, L., & Wang, D. (2023). Differentially pri-\nvate natural language models: Recent advances and future direc-\ntions. arXiv preprint arXiv:2301.09112.\nHuszár, F., Ktena, S. I., O'Brien, C., Belli, L., Schlaikjer, A., & Hardt, \nM. (2022). Algorithmic amplification of politics on Twitter. Pro-\nceedings of the National Academy of Sciences, 119(1), 1. https://  \ndoi. org/ 10. 1073/ pnas. 20253 34119\nInbar, Y., Pizarro, D. A., Knobe, J., & Bloom, P. (2009). Disgust \nsensitivity predicts intuitive disapproval of gays. Emotion, 9(3), \n435–439. https:// doi. org/ 10. 1037/ a0015 960\nJohn, O. P., & Srivastava, S. (1999). The Big-Five trait taxonomy: His-\ntory, measurement, and theoretical perspectives. In L. A. Pervin & \nO. P. John (Eds.), Handbook of Personality: Theory and Research \n(pp. 102–138). Guilford Press.\nJones, D. N., & Paulhus, D. L. (2014). Introducing the short dark triad \n(SD3) a brief measure of dark personality traits. Assessment, \n21(1), 28–41. https:// doi. org/ 10. 1177/ 10731 91113 514105\nKay, A. C., Laurin, K., Fitzsimons, G. M., & Landau, M. J. (2014). \nA functional basis for structure-seeking: Exposure to structure \npromotes willingness to engage in motivated action. Journal of \nExperimental Psychology: General, 143(2), 486–491. https:// doi. \norg/ 10. 1037/ a0034 462\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. \nB., Jr., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, \nŠ., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialo-\nbrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., \net al. (2018). Many Labs 2: Investigating variation in replicability \nacross samples and settings. Advances in Methods and Practices \nin Psychological Science, 1(4), 443–490. https:// doi. org/ 10. 1177/ \n25152 45918 810225\nKnobe, J. (2003). Intentional action and side effects in ordinary language. \nAnalysis, 63, 190–193. https:// doi. org/ 10. 1111/ 1467- 8284. 00419\nLambrecht, & Tucker, C. (2019). Algorithmic bias? An empirical study \nof apparent gender-based discrimination in the display of STEM \ncareer ads. Management Science, 65(7), 2966–2981. https:// doi.  \norg/ 10. 1287/ mnsc. 2018. 3093\nLevin, S. (2016). A beauty contest was judged by AI and the robots \ndidn't like dark skin. The Guardian. Retrieved March 8, 2023, \nfrom https:// www. thegu ardian. com/ techn ology/ 2016/ sep/ 08/ artif \nicial- intel ligen ce- beauty- conte st- doesnt- like- black- people\nLi, X., Li, Y., Liu, L., Bing, L., & Joty, S. (2022). Is GPT-3 a psycho-\npath? Evaluating Large Language Models from a psychological \nperspective. arXiv preprint arXiv:2212.10529.\nMajid, A. (2023). Establishing psychological universals. Nature \nReviews Psychology. https:// doi. org/ 10. 1038/ s44159- 023- 00169-w\nMatthewson, J., & Weisberg, M. (2009). The structure of tradeoffs \nin model building. Synthese, 170(1), 169–190. https:// doi. org/ 10. \n1007/ s11229- 008- 9366-y\n5770 Behavior Research Methods (2024) 56:5754–5770\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. \n(2021). A survey on bias and fairness in machine learning. ACM \nComputing Surveys, 54(6), 1–35. https:// doi. org/ 10. 1145/ 34576 07\nMetz, C. (2020). Meet GPT-3. It has learned to code (and blog and \nargue). New York Times. Retrieved February 3, 2023, from \nhttps:// www. nytim es. com/ 2020/ 11/ 24/ scien ce/ artifi  cial- intel ligen \nce- ai- gpt3. html\nMiotto, M., Rossberg, N., & Kleinberg, B. (2022). Who is GPT-3? An \nexploration of personality, values and demographics. In Proceed-\nings of the Fifth Workshop on Natural Language Processing and \nComputational Social Science (NLP+CSS) (pp. 218–227). Abu \nDhabi, UAE: Association for Computational Linguistics. https://  \ndoi. org/ 10. 18653/ v1/ 2022. nlpcss- 1. 24\nOpenAI (2020). OpenAI Charter. OpenAI. Retrieved February 3, 2023, \nfrom https:// openai. com/ chart er/\nOpenAI (2023a). Completions. OpenAI. Retrieved February 8, 2023, \nfrom https:// platf orm. openai. com/ docs/ api- refer ence/ compl etions\nOpenAI (2023b). GPT-4 technical report. OpenAI. Retrieved March \n23, 2023, from https:// cdn. openai. com/ papers/ gpt-4. pdf\nOpenAI (2023c). Introducing ChatGPT Retrieved March 7, 2023, from \nhttps:// openai. com/ blog/ chatg pt\nOpenAI (2023d). Models. OpenAI. Retrieved March 10, 2023, from \nhttps:// platf orm. openai. com/ docs/ models\nO'Toole, L. (2021). Are Google Search results biased? We don't think \nso! Authoritas. Retrieved March 27, 2023, from https:// www.  \nautho ritas. com/ blog/ are- google- search- resul ts- biased\nPark, P. S., Schoenegger, P., Zhu, C, & Maier, M.. (2023). AI psy -\nchology [Pre-registration, source code, and data]. Open Science \nFramework. Retrieved February 13, 2023 from https:// osf. io/  \ndzp8t/? view_ only= 45fff 39538 84443 d81b6 28cdd 5d50f 7a\nRisen, J. L., & Gilovich, T. (2008). Why people are reluctant to tempt \nfate. Journal of Personality and Social Psychology, 95(2), 293–\n307. https:// doi. org/ 10. 1037/ 0022- 3514. 95.2. 293\nRobertson, R., Jiang, S., Joseph, K., Friedland, L., Lazer, D., & Wil-\nson, C. (2018). Auditing partisan audience bias within Google \nSearch. Proceedings of the ACM on Human–Computer Interac-\ntion, 2(CSCW), 1–22. https:// doi. org/ 10. 1145/ 32744 17\nRoose, K. (2023). Bing (yes, Bing) just made search interesting again. \nThe New York Times. Retrieved February 12, 2023, from https:// \nwww. nytim es. com/ 2023/ 02/ 08/ techn ology/ micro soft- bing- ope-\nnai- artifi  cial- intel ligen ce. html\nRose, A. (2010). Are face-detection cameras racist?  Time. Retrieved \nMarch 8, 2023, from https:// conte nt. time. com/ time/ busin ess/ artic \nle/0,8599,1954643,00.html\nRosenblatt, K. (2023). Twitch temporarily bans 'Seinfeld' parody AI \nafter transphobic remarks. NBCNe  ws. com. Retrieved March 27, \n2023, from https:// www. nbcne ws. com/ tech/ twitch- tempo rary- ban- \nseinf eld- parody- ai- trans phobic- remar ks- rcna6 9389\nRoss, L., Greene, D., & House, P. (1977). The “false consensus effect”: \nAn egocentric bias in social perception and attribution processes. \nJournal of Experimental Social Psychology, 13(3), 279–301. \nhttps:// doi. org/ 10. 1016/ 0022- 1031(77) 90049-X\nRottenstreich, Y., & Hsee, C. K. (2001). Money, kisses, and electric \nshocks: On the affective psychology of risk. Psychological Sci-\nence, 12(3), 185–190. https:// doi. org/ 10. 1111/ 1467- 9280. 00334\nRozado, D. (2023). The political biases of ChatGPT. Social Sciences, \n12(3), 148. MDPI AG. https:// doi. org/ 10. 3390/ socsc i1203 0148\nSalles, A., Evers, K., & Farisco, M. (2020). Anthropomorphism in AI. \nAJOB Neuroscience, 11(2), 88–95. https:// doi. org/ 10. 1080/ 21507 \n740. 2020. 17403 50\nSchimmelpfennig, R., Spicer, R., White, C., Gervais, W. M., Norenzayan, \nA., Heine, S., Henrich, J., & Muthukrishna, M. (2023). A problem \nin theory and more: Measuring the moderating role of culture in \nMany Labs 2. PsyArXiv preprint psyar xiv. com/ hmnrx\nSchwarz, N., Strack, F., & Mai, H. P. (1991). Assimilation and contrast \neffects in part-whole question sequences: A conversational logic \nanalysis. Public Opinion Quarterly, 55(1), 3–23. https:// doi. org/ \n10. 1086/ 269239\nShafir, E. (1993). Choosing versus rejecting: Why some options are \nboth better and worse than others. Memory & Cognition, 21(4), \n546–556. https:// doi. org/ 10. 3758/ bf031 97186\nShiffrin, R., & Mitchell, M. (2023). Probing the psychology of AI mod-\nels. Proceedings of the National Academy of Sciences, 120 (10), \ne2300963120. https:// doi. org/ 10. 1073/ pnas. 23009 63120\nShihadeh, J., Ackerman, M., Troske, A., Lawson, N., & Gonzalez, E. \n(2022). Brilliance bias in GPT-3. In 2022 IEEE Global Humani-\ntarian Technology Conference (GHTC) (pp. 62–69). IEEE.\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., \nWu, J., Radford, A., Krueger G., Kim J. W., Kreps S., McCain \nM., Newhouse A., Blazakis J., McGuffie, K., & Wang, J. (2019). \nRelease strategies and the social impacts of language models. \narXiv preprint arXiv:1908.09203.\nTversky, A., & Kahneman, D. (1973). Availability: A heuristic for \njudging frequency and probability. Cognitive Psychology, 5(2), \n207–232. https:// doi. org/ 10. 1016/ 0010- 0285(73) 90033-9\nTversky, A., & Kahneman, D. (1981). The framing of decisions and \nthe psychology of choice. Science, 211, 453–458. https:// doi. org/ \n10. 1126/ scien ce. 74556 83\nWarren, T., & Lawler, R. (2023) Microsoft business chat is like the Bing \nAI bot but as a personal assistant. The Verge. Retrieved March 23, \n2023, from https:// www. theve rge. com/ 2023/3/ 16/ 23642 832/ micro \nsoft- virtu al- ai- assis tant- busin ess- chat- micro soft- 365\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Diversity (politics)",
  "concepts": [
    {
      "name": "Diversity (politics)",
      "score": 0.5786574482917786
    },
    {
      "name": "Computer science",
      "score": 0.5021748542785645
    },
    {
      "name": "Natural language processing",
      "score": 0.46077674627304077
    },
    {
      "name": "Psychology",
      "score": 0.4515678882598877
    },
    {
      "name": "Linguistics",
      "score": 0.42828595638275146
    },
    {
      "name": "Standard language",
      "score": 0.42694446444511414
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4142618775367737
    },
    {
      "name": "Philosophy",
      "score": 0.13504528999328613
    },
    {
      "name": "Sociology",
      "score": 0.131779283285141
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126820664",
      "name": "Vassar College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I909854389",
      "name": "London School of Economics and Political Science",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1292966370",
      "name": "The Honourable Society of Lincoln's Inn",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210115892",
      "name": "CVS Health (United States)",
      "country": "US"
    }
  ],
  "cited_by": 51
}