{
    "title": "Training Text-to-Text Transformers with Privacy Guarantees",
    "url": "https://openalex.org/W4285180419",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5102899387",
            "name": "Natalia Ponomareva",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5077391471",
            "name": "Jasmijn Bastings",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5070795618",
            "name": "Sergei Vassilvitskii",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3036808490",
        "https://openalex.org/W2788502731",
        "https://openalex.org/W2932329902",
        "https://openalex.org/W2399077675",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W2964018718",
        "https://openalex.org/W4300466035",
        "https://openalex.org/W2969261410",
        "https://openalex.org/W3082707687",
        "https://openalex.org/W3045700442",
        "https://openalex.org/W2594311007",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2950943617",
        "https://openalex.org/W3207429447",
        "https://openalex.org/W4231844697",
        "https://openalex.org/W3102378604",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3177765786",
        "https://openalex.org/W4287725407",
        "https://openalex.org/W3110164654",
        "https://openalex.org/W2520442116",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4302016406",
        "https://openalex.org/W3177173791",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3213241618"
    ],
    "abstract": "Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data. Such models are often released to the public so that end users can fine-tune them on a task dataset. While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material. Recent findings show that the capacity of these models allows them to memorize parts of the training data, and suggest differentially private (DP) training as a potential mitigation. While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood: it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE). Moreover, we show that T5's span corruption is a good defense against data memorization.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2182 - 2193\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nTraining Text-to-Text Transformers with Privacy Guarantees\nNatalia Ponomareva Jasmijn Bastings Sergei Vassilvitskii\nGoogle Research\n{nponomareva,bastings,sergeiv}@google.com\nAbstract\nRecent advances in NLP often stem from large\ntransformer-based pre-trained models, which\nrapidly grow in size and use more and more\ntraining data. Such models are often released\nto the public so that end users can ﬁne-tune\nthem on a task dataset. While it is common\nto treat pre-training data as public, it may\nstill contain personally identiﬁable informa-\ntion (PII), such as names, phone numbers, and\ncopyrighted material. Recent ﬁndings show\nthat the capacity of these models allows them\nto memorize parts of the training data, and sug-\ngest differentially private (DP) training as a po-\ntential mitigation. While there is recent work\non DP ﬁne-tuning of NLP models, the effects\nof DP pre-training are less well understood: it\nis not clear how downstream performance is\naffected by DP pre-training, and whether DP\npre-training mitigates some of the memoriza-\ntion concerns. We focus on T5 and show that\nby using recent advances in JAX and XLA we\ncan train models with DP that do not suffer a\nlarge drop in pre-training utility, nor in train-\ning speed, and can still be ﬁne-tuned to high\naccuracy on downstream tasks (e.g. GLUE).\nMoreover, we show that T5’s span corruption\nis a good defense against data memorization.\n1 Introduction\nRecent advances in natural language process-\ning tasks are largely due to introduction of\nlarge Transformer-based models trained on large\namounts of data. Models such as GPT-2 (Radford\net al., 2019) and T5 (Raffel et al., 2020) have bil-\nlions of parameters and are trained on hundreds of\ngigabytes of mostly uncurated public crawl data.\nThese models are often released as modiﬁable\ncheckpoints, and the end users have the ability to\nﬁne-tune these models to their ﬁnal tasks using an\noften more limited amount of data and compute.\nWhile pre-training datasets are typically treated\nas public, their sheer size makes them difﬁcult to\ncurate or scrutinize (Bender et al., 2021; Rogers,\n2021). Moreover, such public datasets (e.g., web\ncrawls) likely contain private information (Dodge\net al., 2021), e.g., data erroneously released to the\nweb or copyrighted text. The capacity of recent\nmodels makes it possible for them to memorize\nparts of the training data (Carlini et al., 2020), even\nafter subsequent ﬁne-tuning, and poses risks to the\nowners of pre-trained language models. In this\nwork, we focus on a potential mitigation: making\nthe model fully private using differential privacy\n(DP). We focus on T5 (Raffel et al., 2020) and\nexplore how well DP mitigates privacy risks and\nhow it affects pre-training and downstream perfor-\nmance.\nOur contributions are as follows:\n1. We describe how to achieve fully private T5\nmodels by (a) introducing private Sentence-\nPiece (DP-SP) and (b) combining it with pri-\nvate training (DP-Training).\n2. To the best of our knowledge, we are the ﬁrst\nto look into private pre-training (as opposed\nto private ﬁne-tuning) of T5, while also show-\ning how it affects downstream tasks. More\nconcretely, we demonstrate that fully private\nmodels are able to achieve good pre-training\nand ﬁne-tuning utility. Part of the drop in util-\nity introduced by DP-Training is mitigated by\nDP-SP (unigram) tokenizer.\n3. We show that all private pre-training compo-\nnents of T5 (DP-SP and DP-Training) help re-\nduce memorization of T5 models. The biggest\nreduction comes from DP-Training, while DP-\nSP memorization protection is much smaller.\n4. We demonstrate that the pre-training objec-\ntive (i.e., span corruption, next token predic-\ntion) has a signiﬁcant impact on the ability\nto memorize training instances. In particular,\nif memorization is the main concern, models\ntrained with span corruption, even without any\nadditional privacy changes, exhibit excellent\nresilience to training data extraction.\n2182\nExample 1\nExample 2\nExample 3\nExample 4\nBatch\nLoss\nLoss\nLoss\nLoss\nGrad\nGrad\nGrad\nGrad\nClipped Grad\nClipped Grad\nClipped Grad\nClipped Grad\nSummed Grad\nNoise\nPrivate Grad\nFigure 1: Differentially private training. Unlike conventional batched training, the gradient is computed and\nclipped for each example in the batch separately, then accumulated and noised before updating the parameters.\n2 Related work\nIn this section, we discuss what privacy in ML\nmeans (§2.1), followed by overview of private train-\ning (§2.2) and privacy in language models (§2.3).\n2.1 Privacy in ML\nPrivacy guarantees can come in many forms. On\nthe one hand, for a trained ML model, one can\nprovide theoretical Differential Privacy (DP) guar-\nantees in the form of(ϵ,δ) (Dwork and Roth, 2014),\nthat (roughly) say that with probability 1 −δ, no at-\ntacker can increase their prior on whether a speciﬁc\nexample is part of the training data by more than a\nfactor of exp(ϵ). These can be further categorized\ninto guarantees on all of the weights of the model\n(usually achieved via DP-training) or guarantees on\nthe outputs of the model only (private prediction),\nwhich translates into training data label protection.\nThe latter is usually achieved via adding noise to\nthe output. Full model guarantees provide also\nthe weaker guarantees, i.e., private training also\nensures private prediction but not vice versa.\nOn the other hand, the term ‘private’ is some-\ntimes applied to ML models to describe empiri-\ncal characteristics of the model. For example, a\nmodel can be described as private if it is robust\nto membership attacks, training data extraction at-\ntacks, or to attacks that attempt to infer some pri-\nvate attribute (e.g., the race of a speaker) from the\ndata. It is worth noting that DP methods like DP-\ntraining can mitigate some (membership attack,\ntraining data extraction attacks) but not all attacks\nin this category. And heuristic methods to make\nmodels robust to these attacks, such as adversar-\nial heads or adversarial training data augmentation,\ndon’t provide any theoretical privacy guarantees.\nIn this paper, we will focus on theoretical privacy\n(full model protection) achieved via DP-Training.\nIn §5.2 we verify how our models fare with re-\nspect to an empirical “privacy” deﬁnition, namely\nrobustness to training data extraction attacks.\n2.2 Differentially Private (DP) Training\nDP training is a modiﬁcation of the training process\nof ML models that guarantees that the resulting\nmodels (and all of the post processing on them) are\nalso differentially private. DP training is usually\nachieved via gradient noise or perturbing the loss.\nGradient noise, which is by far the most com-\nmon method, involves adding noise to the gradients\nlike DP-SGD and its variants (Abadi et al., 2016;\nPichapati et al., 2019). This is shown in Figure 1\nwith a batch of examples of various lengths. An al-\nternative is to perturb the loss function and then op-\ntimize as usual (Chaudhuri et al., 2011; Phan et al.,\n2016). Here DP guarantees hold only when the al-\ngorithm is fully converged, e.g. a global optimum\nis reached, which is not guaranteed for non-convex\nproblems, and large LMs require many steps to get\nthere. Iyengar et al. (2019) suggested an alterna-\ntive perturbation that does provide guarantees even\nif the model reaches only the vicinity of a global\noptimum, but convexity remains a requirement.\nAll of these methods inject noise into the training\nprocess and are known to result in a drop of utility\n(Appendix D discusses ways to mitigate the utility\ndrop in DP-Train). Since Transformer-based NLP\nmodels are non convex, and are usually not trained\nto full convergence, we employ one of the most\npopular methods that work by noising the gradients\n(Abadi et al., 2016).\n2.3 NLP models and Privacy\nIn NLP it is common to pre-train on large amounts\nof unlabeled data and then ﬁne-tune on the ﬁ-\nnal task. A lot of related work assumes that pre-\ntraining data is essentially public, and makes mod-\n2183\nels private with respect to the limited ﬁne-tuning\ndata. For example, Kerrigan et al. (2020) pre-\ntrained GPT-2 on public data and DP ﬁne-tuned\nit on private data. They demonstrated that such\npre-training on public data helps reduce perplex-\nity. Li et al. (2021) performed a similar analysis\nand showed that private ﬁne tuning can maintain\naccuracy given a good pretrained model. Hoory\net al. (2021a) looked into DP ﬁne-tuning of a (pub-\nlicly) pre-trained BERT model (Devlin et al., 2019)\nin the medical domain and explored how DP ﬁne\ntuning affects the performance and privacy of the\nmodels. They point out that multiple components\nfor language models may need to be adjusted to\nincorporate privacy. For BERT-like models, the\ntokenization algorithm (WordPiece) can be trained\non the private data (to improve the utility), and thus\nneeds to be adjusted to preserve the privacy. Secret\nsharer (Carlini et al., 2018) was used for evaluation,\nand the authors demonstrate that with adjustments\n(e.g., larger batch size) for private models, utility\nis hurt only marginally while being more robust to\nleaking “secrets”, even those with high frequency.\nAt the same time, several works show that pub-\nlicly (e.g. not using DP-Training methods) pre-\ntrained NLP models are vulnerable to privacy at-\ntacks (even after subsequent ﬁne tuning). (Thomas\net al., 2020) looked into whether pre-trained BERT,\nGlove and ELMO embeddings contained private\ndata. The authors inserted secret information into\nthe embeddings’ training data, and then explored\nLSTM models subsequently ﬁne-tuned on these\nembeddings. They showed that higher dimensional\nembeddings leak more information than lower di-\nmensional ones, and DP training reduced this leak-\nage. Additionally, for all but Glove embeddings,\nthe presence of multiple secret values with the same\npattern (e.g. multiple sentences of “John is sick\nwith ﬂu” and “Mary is sick with cold”) reduced the\nleakage. Leakage is also correlated with the num-\nber of epochs used to pre-train the embeddings. DP\ntraining (the authors used (ϵ,δ) of (10,0.00002)\nwith a noise level of 0.44) did reduce the “expo-\nsure”, sometimes up to 7 fold. However, it is worth\nnoticing that the exposure metric is calculated by\nlooking at what log perplexity the model assigns\nto the secret word that was present during training\nin comparison to the scores that the model assigns\nto other secret words (from a limited secret word\nvocabulary). This also means that a sequence-to-\nsequence model is not guaranteed to never output\na secret word, even if it was trained privately. In-\nstead, it means that the probability of outputting\nsuch words is greatly reduced (and the scores with\nwhich they are output are also lower). Additionally,\nfor sequence generation, it is common to use Beam\nsearch, which takes not just the top prediction but\ntop k predictions into consideration, so it is still\npossible to leak secret pre-training data.\nTaking this further, Carlini et al. (2020) demon-\nstrated that it is possible to extract some training\ndata instances by prompting the pre-trained GPT-2\n(Radford et al., 2019) with enough context: ﬁrst\nthe model was used to generate text sequences by\nsampling from the model repeatedly word by word,\nand then perplexity scores for generated sequences\nwere used to decide whether the generated data\nwas actually present in the training data. Finally,\nthe authors hypothesized (but didn’t verify empir-\nically) that DP-training might help mitigate this\ntraining data attack, but highlight that it usually\ndoes hurt the utility. They also mention that curat-\ning the training data could be helpful but is hard to\ndo, especially for large pre-training datasets. Addi-\ntionally, ﬁne-tuning on the downstream task could\npotentially remove some of memorized informa-\ntion.\nFinally, Lee et al. (2021) demonstrated that due\nto non-uniqueness of training data, language mod-\nels may output training data instances verbatim,\nwhich obviously is a privacy concern. They pro-\nposed to mitigate this by deduplicating the pre-\ntraining data and showed that it resulted in substan-\ntial decrease in verbatim training data generation.\n2.4 Summary\nTo summarize, prior work showed that publicly\npre-training LLMs results in privacy vulnerabili-\nties (e.g., memorization of the training data) that\nis exacerbated for larger models, and it was hy-\npothesized that DP training can mitigate these risks.\nHowever, most of the works treat pre-training data\nas public and do DP ﬁne-tuning only. Further, it is\nnot known whether DP pre-trained models can per-\nform well on downstream tasks after (public) ﬁne\ntuning. In subsequent sections, we look to privately\n(DP) pretrain LLMs and investigate how their pre-\ntraining and subsequent ﬁne-tuning performance is\naffected, as well as verify whether DP pretraining\ncan mitigate some privacy risks outlined above.\n2184\n3 Implementing a Fully Private T5\nWe focus on T5 (Raffel et al., 2020), a popular\nencoder-decoder. It uses a slightly modiﬁed Trans-\nformer architecture (Vaswani et al., 2017) and both\nthe input and output is a sequence of tokens, as to-\nkenized by SentencePiece (Kudo and Richardson,\n2018). T5 is a good model to focus on, since it can\nbe used for many input-output tasks, is trained on\na large public crawl data set, is publicly available,\nand has been shown to have excellent performance\non subsequent ﬁne-tuning tasks.\nWhat we are protecting. We use the DP deﬁ-\nnition, so we provide protection at the level of a\ntraining instance. For encoder-decoders like T5,\nthat means a pair of input and output sequences.\nImportantly, if the same training example is re-\npeated multiple times in the training data, the level\nof protection for such an example will be smaller.\nModiﬁcations. There are two parts to training\nT5 that need to be modiﬁed to achieve a fully pri-\nvate model. The ﬁrst part is the tokenizer, which\nis trained on the training data. This part is often\noverlooked by papers claiming to train private NLP\nmodels. It is also unique to NLP models (e.g., in\ncomparison to image models). In §5.1 we show\nthat making the tokenizer private is very important\nand allows us to reduce the utility drop introduced\nby DP-training. The second part is the modiﬁcation\nof the optimization algorithm (DP-Training).\n3.1 Private tokenizer (DP-SentencePiece)\nSentencePiece (Kudo and Richardson, 2018) is a\ntokenizer commonly used for pre-processing text\ndata. It comes with a number of algorithms that can\nbe used (e.g., unigram, char, BPE). One of the ﬁrst\npapers that looked into making tokenizers private is\nHoory et al. (2021a), who devised an algorithm that\nadds Laplacian noise to the histogram of the word\ncounts and applied it to the WordPiece algorithm\nused by BERT. Hoory et al. (2021a) improved on\nthese bounds by using Gaussian noise.\nAlgorithm 1 is a slight modiﬁcation of their algo-\nrithm. For each sentence in the data, compute the\nhistogram of words and counts. Then, compute the\nhistogram of the overall dataset by adding the word\ncounts across all histograms. Contrary to Hoory\net al. (2021a), we do not limit the count of a word in\na sentence to 1, to give per-example (as opposed to\nper word) DP guarantees. The words in the original\nhistogram are not modiﬁed or normalized; it may\nAlgorithm 1: DP-SentencePiece histogram\nInput : A histogram h= {wi : ci}with\nwi a word type and ci the total\ncount of wi in the data.\nσ, C- noise and clipping threshold\nOutput :Private histogram\n1 for i←0 to size(h) do\n2 count′\ni = h[wi] +N(0, σ2)\n3 if count′\ni >= Cthen\n4 h′[wi] =count′\ni ;\n5 end for\n6 return h′\ncontain words such as “Chrysler&apos;s”. The rest\nof SentencePiece algorithm is unmodiﬁed.\nTo calculate the bounds, we use Theorem 1 from\nHoory et al. (2021b): Given N the number of\nwords in a sentence, kthe maximum L2 norm of a\nsentence-level histogram, mthe maximum inﬁnity\nnorm of sentence-level histogram, and σthe noise\nlevel added to the counts, we would obtain (ϵ, δ) DP\nguarantees with ϵ = k\nσ\n√\n2 log (2.5/δ) when the\nclipping threshold of C = m+σerf−1(1−δ/2N)\nis used. Note that in reality there are two reasons\nthat our ϵguarantees will be even better. Please\nrefer to the discussion in Appendix B.\nFinally, it is worth mentioning that there are al-\nternatives to using DP SentencePiece algorithm.\nFirstly one can use SentencePiece trained on a re-\nlated public dataset. We explore the performance\nof such models in §5.1. Alternatively, one can con-\nsider using models that don’t require a pre-trained\ntokenizer, such as ByT5 (Xue et al., 2021). The\ncharacter-level SentencePiece algorithm is some-\ntimes seen as more “private” than the unigram one,\nhowever that is not a precise deﬁnition of privacy.\n3.2 DP-Training\nFor DP-Training, we protect individual example\nprivacy and implement the algorithm outlined in\n(Abadi et al., 2016). Speciﬁcally, we use the\nAdaFactor optimizer that was used for training T5\nwith the following adjustments (See Figure 1):\n1. We take the individual examples’ gradients\nand clip each to some ﬁxed norm (determined\nby privacy parameters).\n2. When taking the parameter update step, noise\n(determined by privacy parameters) is added\nto the accumulated gradients.\n2185\n3.2.1 Fast per-example gradients with JAX\nWe use a reimplementation1 of T5 in JAX (Brad-\nbury et al., 2018) and Flax (Heek et al., 2020). By\ndoing so, we can follow Subramani et al. (2020)\nin leveraging JAX’s vectorization supported by the\nXLA compiler. JAX lets us vectorize (‘vmap’) the\ncomputation of the gradient of the loss on a single\nexample, so that we obtain a batch of per-example\ngradients efﬁciently. This way, we still get most\nof the speedup of batched neural network training,\nwhile having a correct implementation of DP. For\neach example, we average the loss incurred over all\ntarget tokens in the target sequence, compute the\ngradient, and then clip the gradient norm. An up-\ndate for a batch of examples computes the gradient\nfor each example in parallel (using vectorization),\naccumulates the gradients and adds noise, before\nupdating the parameters of the model.\n4 Experiments\nHyperparameters. Our experiments in the main\ntext use T5 small, which has 6 encoder layers, 6 de-\ncoder layers, 8 64-dimensional heads, embedding\ndimension of 512, MLP dimension of 2048. We\nchose T5 small since it is relatively fast to train\nand produces results comparable with that of larger\nmodels (see Appendix A for a discussion of the ef-\nfect of the model size on pre-train and ﬁne-tuning\nperformance). We use AdaFactor (Shazeer and\nStern, 2018) with learning rate 0.5, decay rate 0.8,\nwarm-up 1000, and rsqrt learning rate decay.\nDatasets. We use The Colossal Clean Crawled\nCorpus (C4; Raffel et al., 2020) as a pre-training\ntask and look into the original “preﬁx” unsuper-\nvised training objective, that predicts next tokens\ngiven the context and the span corruption training\nobjective (Raffel et al., 2020, §3.3.4), where ran-\ndomly removed spans of the input are predicted.\nWe use 512 tokens as input/context and attempt to\npredict 114 target tokens. To evaluate ﬁne-tuning\nperformance, we utilize GLUE datasets (Wang\net al., 2018) that allows to evaluate model perfor-\nmance accross a range of NLU tasks.\nAblations. We look separately into the effect of\nDP-SentencePiece and DP-Train on Memorization\nand pretraining and subsequent ﬁne-tuning perfor-\n1Our code is available at https://github.com/\ngoogle-research/google-research/tree/\nmaster/private_text_transformers.\nmance. For DP-SP we use the unigram Sentence-\nPiece algorithm (Kudo and Richardson, 2018).\nPre-training and ﬁne-tuning performance. It\nis known that DP-training hurts the utility (e.g., ac-\ncuracy) of models. However, the common scenario\nin NLP is that models are pre-trained on some data\nand then subsequently ﬁne-tuned for the end task.\nWe look into private pre-training (contrary to the\nmajority of the papers which look into private ﬁne-\ntuning of a publicly pre-trained model), and we\nhope that (public) ﬁne-tuning such privately pre-\ntrained models on public data provides the same\nutility as publicly pre-trained models. For these\nexperiments, we train T5 models with the span cor-\nruption objective with a batch size of 8192 for 100K\nsteps, and ﬁne-tune on GLUE for 150K additional\nsteps with a (standard) 128 batch size. The batch\nsize of 8192 was chosen for pre-training since it\nprovides good performance for DP-Train. T5 with-\nout DP-training trains with approximately the same\nperformance using a batch size of 128, however\nfor DP-Training it is known that the batch size\nshould be increased signiﬁcantly in order to get\nreasonable performance (see Appendix D). For a\nfair comparison we use the same batch size for the\nbaseline and DP-T5 variants.2 Another alternative\nis to tune hyper-parameters (for both baseline and\nDP variants) and compare the best possible mod-\nels, however since tuning parameters for DP will\nchange the ϵ guarantees, we don’t go this route.\nWe use an initial learning rate of 0.5 (both baseline\nand DP-T5 variants pre-training) and weight decay,\nand train with 64 cores. For DP-training, please\nrefer to Appendix 5 for details on noise, clipping\nnorm and ϵ. For the Full DP T5 model, we use a\nDP-SP unigram model trained on C4 withϵ= 0.17\n(see Table 1) and combine it with DP-Training with\nvarious noise levels.\nAdditionally, Appendix D discusses additional\nmodiﬁcations that can be further explored to mini-\nmize the utility drop due to DP-Train.\nTesting for memorization. It is expected that\nDP (both DP-training and DP-SP) should reduce\ndata memorization of the models. To verify that,\nwe conduct an evaluation similar to Carlini et al.\n(2020) and Lee et al. (2021). In particular, we\ntrain models on C4 and attempt to “extract” the\ntraining data by providing an input preﬁx and al-\nlowing model to generate the rest of the sequence.\n2The DP version is only 25% slower than the baseline.\n2186\nWe use 512 tokens preﬁx length, similar to the in-\nput length that was used for training. When the\nmodel generates the output, we calculate the exact\nmatch on a per-instance basis and report the aver-\nage across all instances in the dataset. Exact match\nis different from per-token accuracy: it is either\n0 or 1 for each preﬁx, depending on whether the\nmodel generated the exact target sequence or not,\nwhereas token accuracy would report how many\npredicted tokens match the target tokens for each\ninstance. Exact match allows us to gauge what\nfraction of instances was output verbatim by the\nmodel, serving as a useful metric for memoriza-\ntion. Token-level accuracy serves as an additional\nmetric; while getting some tokens right does not\nguarantee that memorization occurred, high values\nof token-level accuracy would indicate that some\ntokens generated by the model words might have\nmatched those from the target exactly. Finally, we\nalso report median edit distance between predicted\nand target tokens, averaged across all instances in\nthe data. This metric also serves as indirect way of\nmeasuring memorization.\nThe difference between our setup and that of\nCarlini et al. (2020) and Lee et al. (2021) is that\nour model is an encoder-decoder, as opposed to\nGPT-2 which is a decoder only. Additionally, on\ntop of using just next word prediction as a training\nobjective (referred to as preﬁx training), we get to\nexperiment with the span corruption objective.\nWe test for memorization on the the same four\nC4-based datasets as Lee et al. (2021):\n•Train dup and Train unique : contains ex-\namples from the training set that had near-\nduplicates and which had no near-duplicates,\nrespectively, in the training set.\n•Valid in train and Valid unique: examples\nfrom the validation set (not used for training\ndirectly) which are very similar to the exam-\nples from the training data and data that con-\ntains examples from the validation set which\nhad no near-duplicates, respectively.\nWe would expect the most memorization to be\nexhibited on Train dup, and the least memorization\nto be present on Valid unique data.\n5 Results & Discussion\n5.1 Pre-training & Fine-tuning performance\nTable 1 presents an ablation study of DP-\nSentencePiece’s effect on pre-training and ﬁne-\ntuning performance. Additionally, we explore how\ntokens pre-selected via SentencePiece trained on\nother public (for example Wikipedia) datasets af-\nfect the performance of both pre-training and ﬁne\ntuning tasks. First, we see that the best pre-training\naccuracy does not necessarily translate into the best\nﬁne tuning accuracy. Second, we see that DP-SP\n(unigram) serves as a regularizer on the pre-training\ntask, signiﬁcantly improving pre-training perfor-\nmance (approx. 13% improvement for the best ϵ).\nThis might be due to the fact that the C4 pretrain-\ning data is not clean; without DP-SP, misspelling\nand non-words like “rein-forced;” were passed to\nthe SentencePiece algorithm and resulted in sub-\noptimal tokens being selected. For example, for\nϵ = 0.17, 88.6% of C4 words were dropped at\nthe histogram creation stage, resulting in only the\nmost common 11.4% of the words being used for\ntoken selection. Next, we can see that SP trained\non Wikipedia, a different dataset that we may con-\nsider public, performs just as well (if not better\non the pre-training task). The choice of (public)\ndata is important here, and if the data on which the\ntokenizer is trained is not similar to the training\ndata, the performance might be compromised. Ad-\nditionally, character SentencePiece, while without\nany privacy guarantees, provides excellent pre-train\nand ﬁne-tuning performance. Finally, other Senten-\ncePiece models (like BPE) might be more robust\nto the noisy data than the unigram and char models\nwe trained, so it is possible that the regularization\neffect of DP will not be as pronounced for those.\nWe chose unigram because it is the SP algorithm\nused for the majority of T5 models.\nTable 2 demonstrates pretraining and ﬁne-tuning\nperformance of DP-Train (only) models and Fully\nPrivate (Full-DP) models that combine DP-SP and\nDP-Train. We observe that again better pre-training\nutility does not directly translate into better down-\nstream ﬁne-tuning performance. Even for the most\nstringent guarantees of DP-Train (ϵof 6.06) which\nresult in approx 20% of pretrain accuracy drop, on\naverage GLUE ﬁne-tuned performance is not sig-\nniﬁcantly different from the baseline. Full-DP is\nable to recover or improve pre-train accuracy. On\naverage, we also see that full-DP is not signiﬁcantly\nbetter on subsequent ﬁne-tuning tasks (e.g. mnli_m,\nmnli_msm, qnli etc), however for some tasks (e.g.\ncola) ﬁne-tuning performance is signiﬁcantly better\nthan that of a (non-private) baseline. On average,\neven DP-train models have approximately the same\n2187\nModel name ϵ Pretrain GLUE ﬁne-tuning\ncola mnli_m mnli_msm mrpc qnli qqp rte sst2 stsb Avg\nSP\nT5, unigram C4-SP ∞ 56.4 84.6 87.7 88.5 91.7 95.8 96.4 90.4 92.3 66.2 88.2\n90% signif (+/-) 0.1 0.0 2.5 2.0 1.4 0.8 0.8 2.1 0.9 0.8 1.1\nT5, unigram wiki-SP ∞ 71.9 91.4 82.6 82.9 91.1 95.3 95.4 90.4 85.3 50.0 84.9\nT5, char C4-SP ∞ 76.5 97.3 94.6 94.4 92.6 96.6 97.3 94.8 95.1 65.9 92.0\nDP-SP T5, unigram C4-DP-SP\n0.17 69.1 91.5 90.8 91.5 91.1 96.6 96.4 92.9 94.1 56.6 89.1\n3.37 63.7 90.5 90.2 90.8 88.8 96.4 95.3 90.6 91.4 62.6 88.5\n33.70 65.5 84.6 87.1 87.6 94.9 96.9 97.7 92.6 91.7 58.3 87.9\n336.00 66.0 84.6 86.2 87.3 95.1 96.9 97.5 92.5 91.8 58.8 87.9\nTable 1: Accuracy on C4 span corruption pre-training and GLUE ﬁne-tuning. SP is standard SentencePiece, DP-\nSP is private SentencePiece, and Avg is the average across GLUE tasks.\nModel ϵ Pretrain GLUE ﬁne-tuning\ncola mnli_m mnli_msm mrpc qnli qqp rte sst2 stsb Avg\nBaseline ∞ 59.8 85.3 90.7 91.4 95.0 96.4 98.1 91.4 94.8 61.3 89.4\n90% signif (+/-) 0.4 0.0 3.1 3.2 2.1 0.6 0.5 0.5 12.5 0.8 2.4\nDP-train 6.06 39.5 82.7 87.4 87.2 92.6 94.8 97.6 90.72 91.4 60.4 87.2\n8.69 41.3 82.6 87.2 87.0 93.6 94.8 97.6 90.75 91.8 62.9 87.6\n13.46 42.8 81.9 87.0 87.2 93.4 94.6 97.6 90.8 91.6 62.2 87.4\n319.19 48.1 82.5 88.1 88.1 93.1 94.5 97.7 91.39 92.6 62.1 87.8\nFull DP 6.23 51.2 90.6 91.6 91.5 92.1 96.3 97.8 93.5 93.3 57.1 89.3\n8.86 52.4 90.1 92.0 91.8 92.5 96.5 97.9 93.3 94.0 57.5 89.5\n13.63 55.4 90.0 91.9 91.8 93.2 96.4 97.9 93.8 93.9 57.7 89.6\n319.36 62.8 90.6 92.2 92.2 93.1 96.6 98.0 94.6 94.2 67.7 91.0\nTable 2: Accuracy on C4 span corruption pretrain and GLUE ﬁne tuning tasks. DP-Train are T5 models trained\nwith public SentencePiece but DP-Adafactor training, and Full-DP combines DP-SP and DP-Train.\nGLUE performance (difference insigniﬁcant).\n5.2 Memorization discussion\nTable 3 presents the result of memorization ex-\nperiments for various fully-private (Full-DP) T5\nmodels, along with ablation studies that look into\neffect of DP-SentencePiece and DP-Training only.\nFirstly, we highlight that span corruption training\nis extremely robust to memorization. Even base-\nline non-private models do not output any training\ndata verbatim when prompted with input from the\nTrain Dup dataset (exact match of 0%). While\nsome tokens generated by the model do match tar-\nget tokens (the TA column for Train dup), it is only\n0.29% of all (114) generated tokens on average,\nwhich indicates that almost no words were output\nverbatim from the training data. At the same time,\nthe pretrain accuracy of a baseline model indicates\nthat its performance is reasonably good (59.8%\nteacher-forced accuracy). The take-away message\nhere is that if memorization is of a concern, one\nway to address it is to use span corruption training\nobjective. Zero memorization (EM of 0%) is pre-\nserved after publicly ﬁne-tuning these models on\nGLUE and retesting for pre-training data memo-\nrization.\nOne important caveat here is that the span corrup-\ntion training objective was splitting a piece of text\ninto input/target randomly, so it is possible a dif-\nferent deﬁnition of memorization would be more\nsuitable. For example, instead of using prompts\nfrom Train dup and targets that immediately\nfollow these prompts, it would be more suitable to\ntest span corruption models to see if they can output\na randomly selected set of words given other words\nin a sentence. This would mimmic the training ob-\njective of span corruption better. At the same time,\nsince Lee et al. (2021) showed that it is duplicate\nsentences that are major source of memorization,\nand for such duplicate sentences, span corruption\ninputs and targets (randomly selected) during train-\ning will be different for each duplicate, it is still\nour belief that even with such alternative memo-\nrization deﬁnition span corruption models will be\nextremely robust. We leave this for future work.\nPreﬁx training however does exhibit a lot of\nmemorization, conﬁrming the results from Carlini\net al. (2020) and Lee et al. (2021). The baseline\nmodel outputs approx. 2% of the training data ver-\nbatim, when prompted with 512 tokens from the\nTrain dup dataset. This number falls to 0.03%\nof the data for instances that were not repeated\nmany times in the training data (Train unique).\nFull DP-T5 models are able to not only improve\nthe pre-train performance, but also mitigate the ef-\nfect of memorization: for an ϵof 6.23, Full DP-T5\n2188\nModel Eps Pretrain Train dup Train unique Valid in train Valid unique\nEM TA MED EM TA MED EM TA MED EM TA MED\nSpan Corruption\nBaseline ∞ 59.8 0.00 0.29 133 0.00 0.14 220 0.00 0.29 126 0.00 0.14 228\nDP-Train\n6.06 39.5 0.00 0 .02 137 0.00 0 .01 229 0.00 0 .02 119 0.00 0 .01 228\n8.69 41.3 0.00 0 .06 140 0.00 0 .04 226 0.00 0 .07 125 0.00 0 .04 224\n13.46 42.8 0.00 0 .05 136 0.00 0 .03 228 0.00 0 .05 118 0.00 0 .03 226\n319.19 48.1 0.00 0 .25 143 0.00 0 .25 143 0.00 0 .26 133 0.00 0 .14 215\nDP-SP\n0.17 72.9 0.00 1 .12 141 0.00 0 .58 211 0.00 1 .18 134 0.00 0 .58 209\n3.37 70.7 0.00 1 .25 144 0.00 0 .71 203 0.00 1 .34 138 0.00 0 .71 201\n33.68 68.6 0.00 0 .30 129 0.00 0 .16 219 0.00 0 .31 112 0.00 0 .16 217\n336.00 68.7 0.00 0 .13 133 0.00 0 .07 226 0.00 0 .14 116 0.00 0 .07 224\nFull DP\n6.23 51.2 0.00 0 .51 158 0.00 0 .28 207 0.00 0 .51 153 0.00 0 .29 206\n8.86 52.4 0.00 1 .18 132 0.00 0 .78 216 0.00 1 .33 116 0.00 0 .79 214\n13.63 55.4 0.00 0 .34 137 0.00 0 .21 213 0.00 0 .36 127 0.00 0 .21 212\n319.36 62.7 0.00 1 .44 130 0.00 0 .81 204 0.00 1 .52 125 0.00 0 .82 202\nPreﬁx Training\nBaseline ∞ 39.6 2.20 3.33 112 0.03 0.48 208 1.17 2.49 104 0.02 0.48 206\nDP-Train\n6.06 23.0 0.00 0 .21 136 0.00 0 .15 226 0.00 0 .25 118 0.00 0 .15 225\n8.69 23.5 0.05 0 .25 135 0.00 0 .16 227 0.01 0 .27 117 0.00 0 .16 225\n13.46 24.2 0.06 0 .25 135 0.00 0 .16 226 0.05 0 .28 118 0.00 0 .16 225\n319.19 31.4 0.15 0 .64 127 0.00 0 .27 218 0.07 0 .65 111 0.00 0 .27 217\nDP-SP\n0.17 55.7 1.44 3 .41 120 0.01 1 .22 216 0.73 3 .11 216 0.01 1 .23 105\n3.37 53.1 1.48 3 .35 118 0.02 1 .16 215 0.75 2 .99 215 0.01 1 .17 103\n33.68 49.9 1.90 3 .04 117 0.02 0 .76 214 0.99 2 .46 214 0.01 0 .76 103\n336.00 49.8 1.95 3 .10 117 0.01 0 .75 215 0.99 2 .47 215 0.01 0 .75 103\nFull DP\n6.23 42.8 0.01 2 .02 135 0.00 1 .17 225 0.00 2 .16 117 0.00 1 .18 224\n8.86 43.2 0.01 2 .12 134 0.00 1 .27 223 0.00 2 .30 117 0.00 1 .28 222\n13.63 43.7 0.01 1 .67 136 0.00 0 .97 225 0.00 1 .83 118 0.00 0 .98 223\n319.36 49.2 0.15 1 .57 131 0.00 0 .85 222 0.08 1 .66 113 0.00 0 .86 221\nTable 3: Effect of DP on Memorization. EM is Exact match, TA is Token-level accuracy, MED is Median Edit\nDistance. DP-SP are T5 models trained only with Differentially Private SentencePiece. DP-Train are T5 models\ntrained with public SentencePiece but DP-Adafactor training, and Full-DP combines DP-SP and DP-Train.\nmodels output verbatim only 0.006% of training\ninstances that were repeated multiple times in the\ntraining data (366x less memorization) and even\nvery large values ofϵlike 320 provide 15x improve-\nment in memorization as measured by exact match.\nFor instances that occurred in training only a few\ntimes (Train unique), pretty much any level of\nDP-protection provides almost full elimination of\nmemorization (0.002% EM even for an ϵof 320.)\nWith respect to ablation studies, the DP-Training\nhas the most (positive) effect on memorization, ac-\ncounting for the majority of improvement of Full\nDP models. DP-SentencePiece does affect mem-\norization of T5 models, albeit much less than DP-\nTrain. For example, for prompts that look like\ntraining data duplicates, DP-SP (ϵof 0.17) is able\nto reduce the exact match from approx. 2% to\n1.4%. For a large ϵthis protective effect is almost\nnon-existent.\nFinally, it is important to mention that while DP\nT5 does signiﬁcantly reduce memorization (on the\npreﬁx objective), it does not completely eliminate\nit, especially for sentences that were repeated mul-\ntiple times ( Train dup). As mentioned previ-\nously, it might be because such sentences will have\na lower level of protection guarantees and thus can\nstill be output verbatim. Combining DP (DP-SP\nand DP-Training) with deduplication techniques\nfrom Lee et al. (2021) should thus be beneﬁcial.\n6 Conclusion\nWhile the majority of recent work looks into pri-\nvate ﬁne-tuning of pre-trained NLP models, we\ninvestigated how private pre-training of a model\non a large corpus of data affects its pre-training\nand subsequent ﬁne-tuning performance, as well as\nhow much memorization such privately pre-trained\nmodels exhibit. We worked with T5, a transformer-\nbased encoder-decoder, and demonstrated how to\nachieve a fully private T5 version by introducing\nDP-SentencePiece to train a differentially private\nsubword tokenizer, and implementing DP-Training\nfor the actual pre-training. We leveraged recent\nadvances in JAX to do so without incurring a large\nperformance hit in terms of training speed. Our\nresults show that both DP-SentencePiece and DP-\nTraining contribute to reducing memorization, but\nthat the latter has the largest effect. Moreover, we\ndemonstrated that the span corruption task from\nRaffel et al. (2020) also effectively mitigates mem-\norization, which isn’t the case for the next token\nprediction objective. We also show that fully pri-\nvate T5 models exhibit reasonable pre-training per-\nformance and don’t hurt subsequent ﬁne-tuning,\nand that private SentencePiece serves as a regu-\nlarizer on noisy datasets and is able to improve\npre-training and ﬁne-tuning performance of mod-\nels such as T5.\n2189\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H. Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. Proceedings of the 2016 ACM SIGSAC Con-\nference on Computer and Communications Security.\nRaef Bassily, Adam D. Smith, and Abhradeep\nThakurta. 2014. Private empirical risk minimization,\nrevisited. CoRR, abs/1405.7085.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY , USA. As-\nsociation for Computing Machinery.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nNicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlings-\nson, and Dawn Song. 2018. The secret sharer: Mea-\nsuring unintended neural network memorization &\nextracting secrets. CoRR, abs/1802.08232.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2020.\nExtracting training data from large language models.\nCoRR, abs/2012.07805.\nKamalika Chaudhuri, Claire Monteleoni, and Anand D.\nSarwate. 2011. Differentially private empirical risk\nminimization. Journal of Machine Learning Re-\nsearch, 12(29):1069–1109.\nAli Davody, David Ifeoluwa Adelani, Thomas Klein-\nbauer, and Dietrich Klakow. 2020. Robust differ-\nentially private training of deep neural networks.\nCoRR, abs/2006.10919.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colossal\nclean crawled corpus.\nCynthia Dwork and Aaron Roth. 2014. The algorith-\nmic foundations of differential privacy.Foundations\nand Trends in Theoretical Computer Science , 9(3-\n4):211–407.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nShlomo Hoory, Amir Feder, Avichai Tendler, Alon\nCohen, Soﬁa Erell, Itay Laish, Hootan Nakhost,\nUri Stemmer, Ayelet Benjamini, Avinatan Hassidim,\nand Yossi Matias. 2021a. Learning and evaluating\na differentially private pre-trained language model.\nIn Proceedings of the Third Workshop on Privacy in\nNatural Language Processing, pages 21–29, Online.\nAssociation for Computational Linguistics.\nShlomo Hoory, Amir Feder, Avichai Tendler, Soﬁa\nErell, Alon Peled-Cohen, Itay Laish, Hootan\nNakhost, Uri Stemmer, Ayelet Benjamini, Avinatan\nHassidim, and Yossi Matias. 2021b. Learning and\nevaluating a differentially private pre-trained lan-\nguage model. In Findings of the Association for\nComputational Linguistics: EMNLP 2021 , pages\n1178–1189, Punta Cana, Dominican Republic. As-\nsociation for Computational Linguistics.\nRoger Iyengar, Joseph P. Near, Dawn Song,\nOm Thakkar, Abhradeep Thakurta, and Lun\nWang. 2019. Towards practical differentially private\nconvex optimization. In 2019 IEEE Symposium on\nSecurity and Privacy (SP), pages 299–316.\nGavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020.\nDifferentially private language models beneﬁt from\npublic pre-training.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating train-\ning data makes language models better. CoRR,\nabs/2107.06499.\nXuechen Li, Florian Tramèr, Percy Liang, and Tat-\nsunori Hashimoto. 2021. Large language models\ncan be strong differentially private learners.\nIlya Mironov. 2017. Renyi differential privacy. CoRR,\nabs/1702.07476.\nIlya Mironov, Kunal Talwar, and Li Zhang. 2019.\nRényi differential privacy of the sampled gaussian\nmechanism. CoRR, abs/1908.10530.\n2190\nNicolas Papernot, Abhradeep Thakurta, Shuang Song,\nSteve Chien, and Úlfar Erlingsson. 2020. Tempered\nsigmoid activations for deep learning with differen-\ntial privacy.\nNhatHai Phan, Yue Wang, Xintao Wu, and Dejing\nDou. 2016. Differential privacy preservation for\ndeep auto-encoders: An application of human be-\nhavior prediction. In Proceedings of the Thir-\ntieth AAAI Conference on Artiﬁcial Intelligence ,\nAAAI’16, page 1309–1316. AAAI Press.\nVenkatadheeraj Pichapati, Ananda Theertha Suresh,\nFelix X. Yu, Sashank J. Reddi, and Sanjiv Kumar.\n2019. Adaclip: Adaptive clipping for private SGD.\nCoRR, abs/1908.07643.\nFrancesco Pittaluga, Sanjeev J. Koppal, and Ayan\nChakrabarti. 2018. Learning privacy preserving\nencodings through adversarial training. CoRR,\nabs/1802.05214.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nAnna Rogers. 2021. Changing the world by changing\nthe data. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 2182–2194, Online. Association for Computa-\ntional Linguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nPranav Subramani, Nicholas Vadivelu, and Gautam\nKamath. 2020. Enabling fast differentially private\nsgd via just-in-time compilation and vectorization.\nArXiv, abs/2010.09063.\nAleena Thomas, David Ifeoluwa Adelani, Ali Davody,\nAditya Mogadala, and Dietrich Klakow. 2020. In-\nvestigating the impact of pre-trained word embed-\ndings on memorization in neural networks. In Text,\nSpeech, and Dialogue: 23rd International Confer-\nence, TSD 2020, Brno, Czech Republic, Septem-\nber 8–11, 2020, Proceedings, page 273–281, Berlin,\nHeidelberg. Springer-Verlag.\nFlorian Tramer and Dan Boneh. 2021. Differentially\nprivate learning needs better features (or much more\ndata). In International Conference on Learning Rep-\nresentations.\nLaurens van der Maaten and Awni Y . Hannun.\n2020. The trade-offs of private prediction. CoRR,\nabs/2007.05089.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. CoRR,\nabs/1804.07461.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2021. Byt5: Towards a token-free\nfuture with pre-trained byte-to-byte models. CoRR,\nabs/2105.13626.\n2191\nA Small vs Base vs Large T5 model\nperformance\nTable 4 outlines the performance difference be-\ntween the \"small\", \"base\" and \"large\" T5 archi-\ntectures on Pretraining span corruption task on the\nC4 dataset and subsequent ﬁne tuning performance\non GLUE datasets. Since the performance differ-\nences are not very large, we chose to run all of our\nexperiments with the T5 small architecture.\nB DP-SentencePiece ϵ guarantees\ndiscussion\nTheorem 1 (Hoory et al., 2021b) uses the notion\nof kand m(maximum L2 and inﬁnity norms) of\nsentence-level (1-D vector) histogram. If N is the\nlength of the sentence, for a histogram where we ex-\nactly count the number of times each word appears\nin the sentence, we have k = m = N. However\nthe deﬁnition of a sentence is loose. Note that ide-\nally a “sentence” would mimic how the subsequent\ntraining of T5 model will happen, since we aim to\nobtain example-level DP protection. T5 however is\nnot trained on words—it is trained on tokens—and\ntokens are chosen by the SentencePiece algorithm.\nThe length of tokens chosen varies: they can be as\nshort as 1 character or long tokens of 3-4 characters\nor more. Our T5 experiments use 512 input tokens\nas features and 114 tokens as target, so the whole\n“example” or “sentence” is 626 tokens. Just as in\n(Hoory et al., 2021b), we assume that sentence\nlength is 256 words, which is a very pessimistic\nestimate—this translates into approximately 2.45\ntokens per word. Note that if in reality 626 tokens\nrepresent fewer words, the SentencePiece ϵ∼N\nwill be better.\nAdditionally, (Hoory et al., 2021b) authors pro-\nvide a Corollary that allows to obtain slightly better\nϵbounds while using approximately the same clip-\nping norm and the same level of noise.\nC DP-Training ϵ discussion\nIn order to come up with ϵ guarantees for DP-\nTraining, we consider that C4 dataset has approx-\nimately 133,897,2430,182 words. Assuming, just\nas in DP-SentencePiece discussion, that each word\nconsists of approx. 2.45 tokens, and each training\nexample is 512+114 tokens, our total number of\nexamples is approximately 5,240,387,307 (and the\nδused is 1/5,240,387,307).\nTable 5 presents the noise and clip norm that we\nused for our experiments, along with ϵguarantees.\nWe use differential privacy accountant\n(Abadi et al., 2016) and Renyi Differential\nPrivacy outlined in (Mironov, 2017), (Mironov\net al., 2019) which has been implemented in\nhttps://github.com/tensorﬂow/privacy.\nWhen combining DP-SP and DP-Training, we\nuse simple composition and sum the respective ϵ\nand δ.\nD Related work: Mitigating Utility Drop\nin DP-Training\nTo preface the below discussion, we would like\nto highlight that the goal of our paper was not to\nobtain the smallest pre-training utility drop possi-\nble. We thus did limited hyperparameter tuning\nand didn’t explore methods outlined below.\nSome works attempts to mitigate the perfor-\nmance drop by considering architectural or hy-\nperparameter changes. For example, (Tramer and\nBoneh, 2021) argues that the drop can be mitigated\nby the large amount of training data, whereas (Bass-\nily et al., 2014) shows that DP risk minimization\nbounds, compared to non DP bounds, have a poly-\nnomial dependency on the number of features and\nϵ. (Papernot et al., 2020) demonstrated the need\nto adjust the parameters for DP training and ar-\ngued for use of different activation functions when\nusing DP. Additionally, various other architecture\nadjustments like increasing the batch size, or using\nbatch/layer normalization were proposed, for ex-\nample in (Davody et al., 2020). It is also important\nto point out that hyper-parameter tuning (which in-\ncludes changing batch size, learning rate, architec-\nture etc) can’t be used “for free” with DP-training\nas it has to be accounted for in the privacy budget.\nThus for DP training experiments, it is common\nnot to tune the hyperparameters and choose some\npredeﬁned values before the experiments begin.\nAnother direction explored in literature is the\nmodiﬁcation to DP-SGD algorithm itself. For\nexample, (Davody et al., 2020) introduce scale-\ninvariant DP-SGD and use normalization tech-\nniques to dampen the effect of additional noise\nduring training. In this modiﬁcation, the ﬁnal net-\nwork weights are sampled from the normal distri-\nbution whose mean and variance were updated to\naccount for the privacy budget.\nFinally, instead of going for differentially private\ntraining, the utility drop can be mitigated by relax-\nation of privacy guarantees. For example, private-\ninference, which works by adding noise to the ﬁnal\n2192\nModel # params Pretrain GLUE\ncola mnli_m mnli_msm mrpc qnli qqp rte sst2 stsb Avg\nT5 Small 60M 60.7 88.2 94.3 94.4 96.1 98.2 97.9 95.3 95.4 71.7 92.4\nT5 Base 220M 64.6 92.0 95.5 95.7 96.2 98.9 98.2 96.3 97.0 71.5 93.5\nT5 Large 770M 66.7 92.1 96.4 96.5 97.3 99.0 98.2 98.0 98.1 71.9 94.2\nTable 4: Performance of various T5 architectures on pretrain C4 task and their ﬁne tuning performance on GLUE.\nClip Noise ϵ\n0.001 0.40 6.0573157\n0.001 0.35 8.6898032\n0.001 0.30 13.4586238\n0.001 0.20 47.2630501\n0.001 0.10 319.1941523\nTable 5: DP-Train clipping norm and noise hyper-parameters and ϵachieved for a batch size of 8192, 100K steps.\nprediction of the models trained conventionally, is\nknown to protect just the labels of the data and\ndoes not provide full DP guarantees with respect\nto all the model weights and data features (van der\nMaaten and Hannun, 2020). Additionally, heuristic\nmethods like in (Pittaluga et al., 2018) that prevent\ndiscovery of some predeﬁned \"private attributes\"\nfrom the data (.e.g like inferring the race of the\nspeaker) can be used without any DP guarantees.\n2193"
}