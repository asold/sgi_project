{
  "title": "Pretrained Transformers as Universal Computation Engines",
  "url": "https://openalex.org/W3134307371",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227136231",
      "name": "Lu, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223065690",
      "name": "Grover, Aditya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743294920",
      "name": "Abbeel, Pieter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221404522",
      "name": "Mordatch, Igor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2894919355",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W3023252952",
    "https://openalex.org/W3134095442",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3108981297",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3113184484",
    "https://openalex.org/W2118706537",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1511715654",
    "https://openalex.org/W2626792426",
    "https://openalex.org/W2898402099",
    "https://openalex.org/W3007700590",
    "https://openalex.org/W2970350231",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2963281204",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2982180741",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2963921132",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2951599627",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3047517563",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3035435378",
    "https://openalex.org/W2889498145",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2161072217",
    "https://openalex.org/W2887258823",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2950735465",
    "https://openalex.org/W3125166688",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2971842688",
    "https://openalex.org/W3034638829",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2972119347",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3133293957",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2963168530",
    "https://openalex.org/W2766736793",
    "https://openalex.org/W3096331697",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W3128590981",
    "https://openalex.org/W2412320034",
    "https://openalex.org/W2964303773"
  ],
  "abstract": "We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",
  "full_text": "Pretrained Transformers As\nUniversal Computation Engines\nKevin Lu\nUC Berkeley\nkzl@berkeley.edu\nPieter Abbeel\nUC Berkeley\npabbeel@cs.berkeley.edu\nAditya Grover\nFacebook AI Research\nadityagrover@fb.com\nIgor Mordatch\nGoogle Brain\nimordatch@google.com\nAbstract\nWe investigate the capability of a transformer pretrained on natural language to\ngeneralize to other modalities with minimal ﬁnetuning – in particular, without\nﬁnetuning of the self-attention and feedforward layers of the residual blocks. We\nconsider such a model, which we call a Frozen Pretrained Transformer (FPT), and\nstudy ﬁnetuning it on a variety of sequence classiﬁcation tasks spanning numerical\ncomputation, vision, and protein fold prediction. In contrast to prior works which\ninvestigate ﬁnetuning on the same modality as the pretraining dataset, we show\nthat pretraining on natural language can improve performance and compute efﬁ-\nciency on non-language downstream tasks. Additionally, we perform an analysis\nof the architecture, comparing the performance of a random initialized transformer\nto a random LSTM. Combining the two insights, we ﬁnd language-pretrained\ntransformers can obtain strong performance on a variety of non-language tasks1.\nBit Memory Bit XOR ListOps MNIST CIFAR-10 CIFAR-10 LRA Homology\nTest Accuracy\n100 100\n38\n98\n72\n39\n13\n100 100\n38\n99\n70\n42\n9\n61\n50\n17\n99.5\n74\n12 12\nPerformance on Multimodal Sequence Benchmarks\nFrozen Pretrained Transformer Full Transformer Full LSTM\nFigure 1: A frozen language-pretrained transformer (FPT) – without ﬁnetuning the self-attention\nand feedforward layers – can achieve strong performance compared to a transformer fully trained\nfrom scratch on a downstream modality on benchmarks from literature (Tay et al., 2020; Rao et al.,\n2019). We show results on diverse classiﬁcation tasks (see Section 2.1): numerical computation\n(Bit Memory/XOR, ListOps), image classiﬁcation (MNIST, CIFAR-10), and protein fold prediction\n(Homology). We also show results for a fully trained LSTM to provide a baseline.\n1Code available at github.com/kzl/universal-computation. For a summary of changes made in the updated\narXiv version, see Appendix A.\narXiv:2103.05247v2  [cs.LG]  30 Jun 2021\nContents\n1 Introduction 3\n2 Methodology 3\n2.1 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3 Empirical Evaluations 5\n3.1 Can pretrained language models transfer to different modalities? . . . . . . . . . . 5\n3.2 What is the importance of the pretraining modality? . . . . . . . . . . . . . . . . . 6\n3.3 How important is the transformer architecture compared to LSTM architecture? . . 7\n3.4 Does language pretraining improve compute efﬁciency over random initialization? 8\n3.5 Do the frozen attention layers attend to modality-speciﬁc tokens? . . . . . . . . . . 8\n3.6 Does freezing the transformer prevent overﬁtting or underﬁtting? . . . . . . . . . . 9\n3.7 Does performance scale with model size? . . . . . . . . . . . . . . . . . . . . . . 9\n3.8 Can performance be attributed simply to better statistics for initialization? . . . . . 10\n3.9 Can we train a transformer by only ﬁnetuning the output layer? . . . . . . . . . . . 10\n3.10 What is the role of model depth in token mixing? . . . . . . . . . . . . . . . . . . 11\n3.11 Can training more parameters improve performance? . . . . . . . . . . . . . . . . 11\n3.12 Which parameters of the model are important to ﬁnetune? . . . . . . . . . . . . . . 12\n3.13 Is ﬁnetuning layer norm necessary for FPT to perform well? . . . . . . . . . . . . 12\n3.14 How well do the trends hold across other transformer models? . . . . . . . . . . . 13\n4 Related Work and Discussion 13\n4.1 Transformers in multimodal settings . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4.2 Transformers in transfer settings . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4.3 Pretraining and ﬁnetuning of transformer models . . . . . . . . . . . . . . . . . . 14\n4.4 Self-attention layers as optimization steps . . . . . . . . . . . . . . . . . . . . . . 14\n4.5 Global workspace theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.6 Reservoir computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n5 Conclusion 15\nAcknowledgements 15\nReferences 20\nAppendix 21\n2\n1 Introduction\nThe transformer architecture (Vaswani et al., 2017) has shown broad successes in deep learning,\nserving as the backbone of large models for tasks such as modeling natural language (Brown et al.,\n2020), images (Dosovitskiy et al., 2020), proteins (Jumper et al., 2021), behaviors (Abramson et al.,\n2020), and multimodal tasks comprising of both images and text (Lu et al., 2019; Radford et al.,\n2021). Inspired by these successes, we seek to explore the generalization capabilities of a trans-\nformer in transferring from one modality to another.\nClassical approaches to sequence processing used recurrent neural network (RNN) approaches\n(Rumelhart et al., 1985; Hochreiter & Schmidhuber, 1997). In contrast, transformers utilize self-\nattention layers to extract features across tokens of a sequence, such as words (Vaswani et al., 2017)\nor image patches (Dosovitskiy et al., 2020). Furthermore, it has become common practice to train\nlarge models on unsupervised or weakly supervised objectives before ﬁnetuning or evaluating zero-\nshot generalization on a downstream task. However, the downstream tasks that have been studied\nare generally restricted to the same modality as the original training set: for example, train GPT\n(Radford et al., 2018) on a large language corpus, and ﬁnetune on a small task-speciﬁc dataset. Our\ngoal in this work is to investigate ﬁnetuning on modalities distinct from the training modality.\nWe hypothesize that transformers, namely the self-attention layers, can be pretrained on a data-rich\nmodality (i.e. where data is plentiful, such as a natural language corpus) and identify feature rep-\nresentations that are useful for arbitrary data sequences, enabling downstream transfer to different\nmodalities. In particular, we seek to investigate what pretrained language models (LMs) are capable\nof in terms of generalizing to other modalities with sequential structure.\nTo investigate this hypothesis, we take a transformer model pretrained on natural language data,\nGPT-2 (Radford et al., 2019), and ﬁnetune only the linear input and output layers, as well as the\npositional embeddings and layer norm parameters. We call this model a Frozen Pretrained Trans-\nformer (FPT). On a range of tasks across a variety of modalities – including numerical computation,\nimage classiﬁcation, and protein fold prediction – FPT displays comparable performance to training\nthe entire transformer or LSTM models from scratch, matching reported benchmarks for these tasks\n(Figure 1). Additionally, we ﬁnd FPT models also converge faster during training. Our results sug-\ngest the self-attention layers learned by a language model may have properties amenable to efﬁcient\nuniversal computation. Through a series of experiments, we seek to investigate what contributes to\nthe performance of FPTs by isolating various sub-components of these models.\n2 Methodology\n2.1 Tasks\nWe evaluate on a diverse set of classiﬁcation tasks representative of different modalities. In partic-\nular, we are interested in if language models are inherently capable of universal computation, by\nwhich we mean the ability to learn representations for predictive learning across diverse modalities.\nBit memory. Similar to the task proposed by Miconi et al. (2018), we consider a bit memory\ntask where the model is shown 5 bitstrings each of length 1000. Afterwards, the model is shown\na masked version of one of the bitstrings, where each bit is masked with probability 0.5, and the\nmodel is tasked with producing the original bitstring. The bitstrings are broken up into sequences of\nlength 50, so that the models are fed 120 tokens of dimension 50.\nBit XOR. Similar to the bit memory task, the model is shown 2 bitstrings of length 5, where the\nmodel must predict the element-wise XOR of the two bitstrings. The bitstrings are shown 1 bit at a\ntime, so the models are fed 10 tokens of dimension 1.\nListOps. Taken from Tay et al. (2020), the model is shown a sequence of list operations (ex. [\nMAX 4 3 [ MIN 2 3 ] 1 0 ]) and tasked with predicting the resulting output digit (ex. 4).\nThis task evaluates the ability of a model to parse mathematical expressions and evaluate over a long\ncontext. The model is shown 1 token at a time, so the models are fed 512 tokens of dimension 15.\nMNIST. We use the standard MNIST benchmark, where the model must classify a handwritten digit\nfrom a 32 ×32 black-and-white image. The tokens given to the model are 4 ×4 image patches, so\nthe models are fed 64 tokens of dimension 16.\n3\nInput\nEmbedding\nMulti-Head\nAttention+ Feed\nForward\nOutput\nLayer\nPositional\nEmbeddings\nL frozen self-attention blocks\nAdd &\nLayer Norm\nAdd &\nLayer Norm\nx L\nFigure 2: Frozen Pretrained Transformer (FPT). The self-attention & feedforward layers are frozen.\nCIFAR-10. We use the standard CIFAR-10 benchmark (Krizhevsky et al., 2009), where the tokens\ngiven to the model are 4 ×4 image patches, so the models are fed 64 tokens of dimension 16.\nCIFAR-10 LRA. This is a modiﬁed version of the above task taken from the Long Range Arena\nbenchmark where the images are converted to grayscale and ﬂattened with a token length of 1 (Tay\net al., 2020). As a result, the input sequence consists of 1024 tokens of dimension 1. This task is\nmuch more challenging than vanilla CIFAR-10 classiﬁcation above as the models must learn patterns\nover a signiﬁcantly longer sequence length and have minimal spatial inductive bias.\nRemote homology detection. In this task, we are interested in predicting the fold for a protein,\nrepresented as an amino acid sequence. We use the datasets provided by TAPE (Rao et al., 2019;\nFox et al., 2013; Hou et al., 2018), where the train/test split is generated by holding out certain\nevolutionary groups. Note that we do not pretrain on Pfam (El-Gebali et al., 2019), which is common\nin other works. There are 20 common and 5 uncommon amino acids (25 different types of inputs),\nand there are 1195 possible labels to predict. We only consider sequences of length less than 1024\nfor simplicity. The models are thus fed up to 1024 tokens of dimension 25.\n2.2 Architecture\nThe architecture we use is summarized in Figure 2. Denote the embedding size/hidden dimension\nof the transformer as ndim, the number of layers as nlayers, (note ndim = 768and nlayers = 12for\nthe base size models), the input dimension asdin, the output dimension (number of classes) asdout,\nand the maximum length of the sequence as l. We consider ﬁnetuning the following parameters of a\npretrained GPT-2 model (Radford et al., 2019):\n• Output layer: it is crucial to ﬁnetune the output layer since we are transferring to a completely\nnew task – we use the simplest possible instantiation of an output network, being a single linear\nlayer applied to the last output token output by the transformer, in order to highlight that almost all\nthe computation is being performed by the frozen transformer. The output layer has ndim ×dout\nparameters for the weight matrix. For example, for the base models on CIFAR-10, this comes out\nto 768 ·10 = 7680parameters.\n• Input layer: it is important to reinitialize a new input layer since we are reading in a new modality;\nin essence, we are learning how to query the transformer. This contrasts with prior unsupervised\nembedding evaluation techniques, such as linear probing – due to the change in modality, we\ninstead should train the input layer as well, and evaluate if the frozen intermediate transformer\nmodel performs effective computation. Again, we use a linear layer to minimize the amount of\ncomputation outside the transformer. The input layer has din ×ndim parameters for the weight\nmatrix/embeddings, and an additional ndim parameters if there is a bias term. For the base models\non CIFAR-10, this comes out to 16 ·768 = 13056parameters.\n• Layer norm parameters: as is standard practice in other ﬁnetuning works (Rebufﬁ et al., 2017;\nHoulsby et al., 2019), we also ﬁnetune the afﬁne layer norm parameters (scale and bias), which\nadapt to the statistics of the downstream task in a new domain. In GPT-2, layer norm is applied\ntwice per block, so these are a total of 4 ×ndim ×nlayers parameters. For the base models on\nCIFAR-10, these come out to 4 ·768 ·12 = 36684parameters.\n• Positional embeddings: While we observe that positional embeddings can be surprisingly uni-\nversal between modalities (see Section 3.12), we generally see a small beneﬁt to ﬁnetuning the\npositional embeddings which have a cheap parameter cost of l×ndim. For the base models on\nCIFAR-10, these come out to 64 ·768 = 49512parameters.\n4\nGiven the cheap linear scaling of these parameters, the parameter counts of large transformer models\nare dominated by the quadratic (in ndim and l) self-attention and feedforward layers. For the base\nCIFAR-10 model with 124M parameters, these come out to approximately 0.086% of the network.\nDue to this scaling, this number decreases with larger model sizes, down to 0.029% of the GPT-2\nXL model. We further ablate the importance of each parameter in Section 3.12. For more details\nand a description of the architecture, see Appendix B.\nNote that, crucially, all communication between tokens in the model are frozen. The data in each\ndatapoint is chunked into discrete tokens (bits, image patches, amino acids, etc.), and can only\nreference each other via the frozen attention connections, which are not trained; additionally, neither\nthe output nor the input layers are connected to multiple tokens. Our key investigation is to analyze\nthe computation that is already inherent in the language model, and hence we do a minimal amount\nof computation that is learned on the downstream modality.\n3 Empirical Evaluations\nIn this section, we review the results demonstrating transfer from language to other modalities, and\nseek to better understand why this occurs and what enables this transfer. All model sizes are the\nbase model size (12 layers, 768 hidden dimension), unless stated otherwise. See Appendix C for\nmore details on experiments.\n3.1 Can pretrained language models transfer to different modalities?\nWe investigate if the self-attention and feedforward layers – the main body – of a pretrained trans-\nformer can be applied to a classiﬁcation problem in a different modality without ﬁnetuning. To\ndo this, we apply our base procedure as described above, where the input embedding layer, output\nreadout layer, and layer norm parameters are ﬁnetuned.\nOur results are shown in Figure 1 and also summarized below in Table 1. We compare to state-of-\nthe-art from literature when available (full transformer on ListOps, CIFAR-10 LRA, and Remote\nHomology; LSTM on Remote Homology). Note the benchmarks from literature do not include\ndecimal points, so for those numbers we report without a decimal.\nWe ﬁnd that across all seven tasks considered, FPT achieves comparable performance to the fully\ntrained transformer benchmarks. We believe these results support the idea that these models are\nlearning representations and performing computation that is agnostic to the modality. We also note\nthat both transformer variants signiﬁcantly outperform LSTMs on some tasks, particularly ListOps\nand CIFAR-10 LRA, which have long sequence lengths of 512 and 1024, respectively.\nOn the two bit tasks (Memory and XOR), the models achieve 100% performance, i.e. they are able\nto recover the exact algorithm. Although our tables show results forn= 5, we actually ﬁnd FPT can\nstill recover the exact algorithm on sequence lengths greater than n = 256(the elementwise XOR\nof two bitstrings each of length 256), hinting that FPT has a fairly large working memory.\nModel Bit Memory XOR ListOps MNIST CIFAR-10 C10 LRA Homology\nFPT 100% 100% 38.4% 98.0% 72.1% 38.6% 12.7%\nFull 100% 100% 38% 99.1% 70.3% 42% 9%\nLSTM 60.9% 50.1% 17.1% 99.5% 73.6% 11.7% 12%\nTable 1: Test accuracy of FPT vs fully training transformer on downstream task vs fully training\nLSTM on downstream task (results are transcribed from Figure 1).\nWe highlight a few important points for contextualizing these results. We ﬁnd that it can be difﬁcult\nto fully train a 12-layer transformer on some of these (relatively small) datasets, as training can\neither diverge/overﬁt or be unstable. For CIFAR-10, we report the full transformer results for a 3-\nlayer model; for ListOps and CIFAR-10 LRA we report the number given for the 3-layer model from\nTay et al. (2020); for Remote Homology we report the number for a smaller 12-layer model from\nRao et al. (2019). From an engineering perspective, this makes the full transformers harder to tune\nsince we must choose model sizes that are stable and avoid overﬁtting – see Section 3.6 for more\n5\nanalysis. In particular, the numbers from Tay et al. (2020) are generated from “extensive sweeps\nover different hyper-parameters” and use task-speciﬁc hyperparameters, while we do not tune the\nhyperparameters for FPT (except for remote homology; see Appendix C). In contrast, we ﬁnd it is\neasy to improve the performance of FPT by increasing model size (see Section 3.7) – the CIFAR-10\nnumber for FPT here is for the 36-layer large model.\nFurthermore, unlike some other works utilizing transformers for vision, we use minimal spatial bias\nto emphasize the universal sequential aspect of the problem – for instance, we do not interleave self-\nattention and convolution layers. Note that we also do not use 2D positional embeddings (or other\ndomain-speciﬁc techniques), hence providing very weak inductive prior to the model. Our reasoning\nfor these decisions is to evaluate the ability of transformers to work on arbitrary sequential tasks.\n3.2 What is the importance of the pretraining modality?\nWe now compare pretraining on language to other pretraining methods for base model sizes:\n• Random initialization (Random): initialization of the frozen transformer parameters randomly\nusing the default initialization choices for GPT-2, i.e. without pretraining.\n• Bit memory pretraining (Bit): pretraining from scratch on the Bit Memory task and then freezing\nthe parameters before transferring. This allows the transformer to gain supervision working with\narbitrary bit strings and performing memory/denoising on independent inputs.\n• Image pretraining (ViT): using a pretrained Vision Transformer (Dosovitskiy et al., 2020) pre-\ntrained on ImageNet-21k (Deng et al., 2009). Note that the architecture is a bit different, notably\nnot using the autoregressive masking of GPT-2, since ViT is only pretrained on classiﬁcation tasks\n(for other details, see Appendix D.2).\nThese experiments highlight the signiﬁcance of pretraining – as opposed to simply the transformer\narchitecture – and compare language to other methods of supervision. Our results are shown in Table\n2. Although the random transformers can achieve surprisingly strong accuracies, there is a consid-\nerable gap to using natural language pretraining, such as in MNIST, where random transformers\nachieve similar performance to a linear classiﬁer on top of raw features (92%). Thus we believe that\nwhile the transformer architecture might be naturally conducive to these evaluations, the attention\nmechanisms used to transfer may be nontrivial and not fully speciﬁed by the architecture. We also\nﬁnd that, in addition to performance beneﬁts, language pretraining improves convergence compared\nto the randomly initialized transformer (see Section 3.4).\nModel Bit Memory XOR ListOps MNIST C10 C10 LRA Homology\nFPT 100% 100% 38.4% 98.0% 68.2% 38.6% 12.7%\nRandom 75.8% 100% 34.3% 91.7% 61.7% 36.1% 9.3%\nBit 100% 100% 35.4% 97.8% 62.6% 36.7% 7.8%\nViT 100% 100% 37.4% 97.8% 72.5% 43.0% 7.5%\nTable 2: Test accuracy of language-pretrained (FPT) vs randomly initialized (Random) vs Bit Mem-\nory pretraining (Bit) vs pretrained Vision Transformer (ViT) models. The transformer is frozen.\nPretraining on bit memory improves performance compared to the random models, but still lags\nbehind training on natural language data. Furthermore, measured by gradient steps, all models\nconverge faster than the randomly initialized transformers (more details in Section 3.4), indicating\nthat all modes of pretraining improve upon random initialization even without considering accuracy.\nAdditionally, while freezing a vision transformer yields better improvements on CIFAR-10, pretrain-\ning on images is not uniformly better; e.g., ViT is worse on protein classiﬁcation. One hypothesis is\nthat protein sequences are structured like language, in terms of discrete units of information with a\n“grammar”, so transfer from language to proteins may be more natural.\n6\n3.3 How important is the transformer architecture compared to LSTM architecture?\nIn Section 3.2 we found the transformer architecture can already be fairly effective in this regime,\neven with only random parameters. In this section, we consider using a random LSTM architec-\nture instead of the transformer, allowing us to consider the raw effect of architecture and ablating\npretraining. Like FPT, we ﬁnetune the input, output, and layernorm parameters for the LSTMs.\nModel Bit Memory XOR ListOps MNIST CIFAR-10 C10 LRA Homology\nTrans. 75.8% 100% 34.3% 91.7% 61.7% 36.1% 9.3%\nLSTM 50.9% 50.0% 16.8% 70.9% 34.4% 10.4% 6.6%\nLSTM∗ 75.0% 50.0% 16.7% 92.5% 43.5% 10.6% 8.6%\nTable 3: Test accuracy of randomly initialized transformers vs randomly initialized LSTM models.\nNote unlike in Figure 1, the LSTM here is frozen. Frozen LSTMs perform very poorly. LSTM∗rep-\nresents an LSTM with additional architecture improvements to match the transformers (see below).\nOur results are shown in Table 3. “LSTM” refers to a 3-layer “standard” LSTM with a hidden\ndimension of 768, matching standard implementations of LSTMs, without residual connections or\npositional embeddings (see discussion below). This matches the width of the FPT models, but not\nthe depth or total parameter count (note that LSTMs also do not have positional embeddings). We\nﬁnd that the self-attention architecture already serves as an effective inductive bias for universal\ncomputation, improving signiﬁcantly over the recurrent LSTM model and comprising most of the\nimprovement in test accuracy from random LSTM to FPT.\nHere, we compare the 3-layer “standard” LSTM to a 12-layer “standard” LSTM. Note that most\nLSTM implementations, including the one used in Table 3, do not feature residual connections\nand positional embeddings. We include this comparison to represent the traditional method more\nfaithfully, but add these additional architectural components below. In the same style of FPT and\nGPT-2, we do not use a bidirectional LSTM. Under these model choices, we report the performance\nof a frozen random 3-layer vs 12-layer LSTM in Table 4. Naively, the 12-layer model is much worse\nthan the 3-layer model, hinting that there is some loss of information by repeated LSTM layers.\nLayers ListOps MNIST CIFAR-10 C10 LRA\n12 16.2% 11.7% 10.8% 10.4%\n3 16.8% 70.9% 34.4% 10.4%\nTable 4: Test accuracy of randomly initialized “standard” LSTMs varying number of layers with a\nhidden dimension of 768. The simple 12-layer LSTM achieves only near-trivial performance.\nWe also experiment with ablating other architectural improvements included with the transformer\narchitecture in Table 5. Once residual connections (He et al., 2016) are added, the 12-layer LSTM\nmakes up a lot of the performance drops, hinting that residual connections could make up for loss\nof information from the LSTM layers which otherwise linearly combine the features. We also add\npositional embeddings, which ﬁnishes bridging the gap between standard LSTM implementations\nand the transformer. Even with these additional beneﬁts, the LSTM still performs worse. Note that\nthe ﬁnal 12-layer LSTM has about the same number of trainable parameters as the transformer.\nModel ListOps MNIST CIFAR-10 C10 LRA\n12-Layer LSTM 16.2% 11.7% 10.8% 10.4%\n+ Residual Connections 16.8% 70.9% 34.4% 10.4%\n+ Positional Embeddings 16.7% 92.5% 43.5% 10.6%\nRandom Transformer 34.3% 91.7% 61.7% 36.1%\nTable 5: Test accuracy of 12-layer randomly initialized “standard” LSTMs additional architectures\nmodiﬁcations to match transformers: residual connections and positional embeddings. The bottom\nrow, LSTM with residual connections and positional embeddings, is nearly identical to GPT-2.\n7\n3.4 Does language pretraining improve compute efﬁciency over random initialization?\nWe investigate compute efﬁciency by considering the number of gradient steps to converge for FPT\nvs random transformer models, shown in Table 6. We generally ﬁnd FPT converges faster, which\nindicates language pretrainining can yield compute beneﬁts for non-language tasks. While random\ntransformer models achieve decent test accuracies, in particular when compared to random LSTMs,\nthere is still a considerable gap in the compute efﬁciency compared to using pretraining. Note that bit\nmemory pretraining introduced in Section 3.2 generally falls between the two models, and notably\nis 6×slower than FPT on Bit XOR, which is signiﬁcantly better than random.\nModel Memory XOR ListOps MNIST C10 C10 LRA Homology\nFPT 1 ×104 5 ×102 2 ×103 5 ×103 4 ×105 3 ×105 1 ×105\nRandom 4 ×104 2 ×104 6 ×103 2 ×104 4 ×105 6 ×105 1 ×105\nSpeedup 4× 40× 3× 4× 1× 2× 1×\nTable 6: Approximate number of gradient steps until convergence for pretrained (FPT) vs randomly\ninitialized (Random) models. Note that we use the same batch size and learning rate for both models.\n3.5 Do the frozen attention layers attend to modality-speciﬁc tokens?\nWe investigate if FPT attends to semantically meaningful patterns in the data. We plot the attention\nweights (i.e. the values of the softmax of query-key dot product) from the ﬁrst layer. We show the\nresults in Figures 3 and 4 for the bit tasks. Note GPT-2 is autoregressive, so the upper right corner\nof the attention mask is zeroed out. On these tasks, FPT yields an interpretable attention pattern\ndespite not training the self-attention layers themselves. We did not ﬁnd easily interpretable patterns\non the other tasks.\n0 2 4 6 8\nInput Token\n0\n2\n4\n6\n8 Output Token\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n                       0101111001 => 1\n                       0101111001 => 0\n                       0101111001 => 0\n                       0101111001 => 1\n                       0101111001 => 0\n0\n0 0\n0\n1 1\n1\n1 1\n1\nString 1 String 2\nFigure 3: On Bit XOR, the model must produce the element-wise XOR of two bitstrings presented\nsequentially (inputs 0-4 are the ﬁrst bitstring, inputs 5-9 are the second). Each token is one bit. FPT\nlearns to attend positionally to the two bits that are XOR’ed by the output token.\n0 20 40 60 80 100120\nInput Token\n0\n20\n40\n60\n80\n100\n120\nOutput Token\nMasked String Is 1\n0 20 40 60 80 100120\nInput Token\nMasked String Is 2\n0 20 40 60 80 100120\nInput Token\nMasked String Is 3\n0 20 40 60 80 100120\nInput Token\nMasked String Is 4\n0 20 40 60 80 100120\nInput Token\nMasked String Is 5\nFigure 4: On Bit Memory, the model must return one of ﬁve strings (inputs 0-99) given a masked\nversion of one of the strings (inputs 100-119). Each token is 50 bits. FPT learns to attend to the\ncorrect string based on ﬁnding similarity to the inputs, not relying solely on position as in Bit XOR.\n8\nWe also include the attention map for Bit XOR using a randomly initialized transformer (which also\nsolves the task) in Figure 5. This model also learns to exploit the diagonal pattern, although the\nstrength is a little weaker. This indicates that while the random transformer still learns to solve the\ntask, it learns a less semantically interpretable/strong attention pattern.\n0 2 4 6 8\nInput Token\n0\n2\n4\n6\n8 Output Token\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5: A transformer with frozen randomly initialized self-attention layers also learns to correlate\nthe two diagonal elements on Bit XOR, although the magnitude of the diagonals is lower (note the\nextra attention weights distributed in between the diagonals).\n3.6 Does freezing the transformer prevent overﬁtting or underﬁtting?\nOur general ﬁndings are that – in contrast to their fully trained counterparts – FPT models underﬁt\nthe data, which lends them to further improvements by increasing model capacity (see Section 3.7).\nFor example, consider CIFAR-10 LRA, which is maximally difﬁcult due to lack of inductive prior\nover the sequence (each pixel is fed in as an arbitrary token only ordered by a raster scan) and rel-\natively small dataset (50k images). In Table 7, we show the train/test gap between training FPT vs\na 3-layer transformer from Tay et al. (2020), which we ﬁnd to give stronger results than our experi-\nments. In particular, they are much better than training a 12-layer transformer, which works poorly.\nOur results indicate that FPT is generally providing generalizable task representations without caus-\ning overﬁtting, whereas transformers can overﬁt arbitrarily poorly in low-data regimes (such as for\nLinformer, which overﬁt the most out of the architectures tested by Tay et al. (2020)). More work\ncan investigate how to increase the model expressiveness, which could yield performance beneﬁts.\nModel # Layers Test Accuracy Train Accuracy\nFPT (GPT-2) 12 38.6% 38.5%\nVanilla Transformer 3 42% 70%\nLinformer 3 39% 97%\nTable 7: Train vs test accuracies on CIFAR-10 LRA task.\n3.7 Does performance scale with model size?\nWe evaluate the efﬁcacy of adding more parameters to these models on CIFAR-10. Most of the\nadditional parameters are in the transformer layers and are trained during the natural language pre-\ntraining phase. Our results for pretrained and random models are in Table 8. Unlike fully training\na transformer, which exhibits more overﬁtting and divergence during training with larger models,\nincreasing model size stably increases the capacity of the models. This result indicates our observa-\ntions and results are likely to scale as we move towards larger models and higher-data regimes.\nModel Size # Layers Total Params Trained Params FPT Random\nSmall (Base) 12 117M 106K 68.2% 61.7%\nMedium 24 345M 190K 69.8% 64.0%\nLarge 36 774M 300K 72.1% 65.7%\nTable 8: Test accuracy of larger frozen transformer models on CIFAR-10.\n9\n3.8 Can performance be attributed simply to better statistics for initialization?\nIn this section, we ablate taking the layer-wise mean and standard deviation from the pretrained\nmodel and using it to initialize a random transformer, in order to ablate if a better initialization\nscheme via an “oracle” standard deviation can recover the performance of FPT. Note that the GPT-2\ninitialization scheme initializes parameters as Gaussian; traditionally, the standard deviation is 0.02\nby default. For clarity, we show the standard deviation by layer for the weights and biases of the\nattention and feedforward layers in Figure 6 for the pretrained models.\n0 5 10\n0.1\n0.2\nattn.c_attn.weight\n0 5 10\n0.05\n0.10\n0.15\nattn.c_proj.weight\n0 5 10\n0.05\n0.10\nmlp.c_fc.weight\n0 5 10\n0.1\n0.2\nmlp.c_proj.weight\n0 5 10\nLayer\n0.00\n0.25\nattn.c_proj.bias\n0 5 10\nLayer\n0.0\n0.1\nmlp.c_fc.bias\n0 5 10\nLayer\n0.0\n0.1\nmlp.c_proj.bias\nPretrained Statistics Default Random Statistics\nFigure 6: Standard deviation of the parameters by layer for the pretrained GPT-2 model versus\ndefault initialization hyperparameters (0.02 for weights and 0 for biases).\nWe show the results using this initialization scheme in Table 9 (note that all of the weights, biases,\nlayer norm, and positional embeddings are initialized – both mean and variance – in this fashion).\nThis yields better results on most tasks, but does poorly on CIFAR-10. As a result, we believe\nthe beneﬁts of language pretraining cannot be recovered with a simple better initialization scheme,\nalthough we believe future work in transformer initialization could yield different results.\nInitialization Memory XOR ListOps MNIST C10 C10 LRA Homology\nPretrained 100% 100% 38.4% 98.0% 68.2% 38.6% 12.7%\nStatistics Only 100% 100% 37.4% 97.2% 56.5% 33.1% 11.0%\nDefault 75.8% 100% 34.3% 91.7% 61.7% 36.1% 9.3%\nTable 9: Test accuracy when initializing parameters with pretrained weights (i.e., FPT) vs randomly\ninitializing parameters according to the mean and variance of the pretrained transformer (Statistics\nOnly) vs random initialization with default parameters (Default).\n3.9 Can we train a transformer by only ﬁnetuning the output layer?\nWe consider using FPT solely for naive feature extraction for linear classiﬁcation, where we ﬁx a\nrandomly initialized input layer and freeze all parts of the model except for the output. Note that\nthis resembles resevoir computing/echo state networks (see Section 4.5 for discussion). The model\nevaluates on every example in the training set once, caches the features, and then we train a linear\noutput layer. This enables subsequent epochs after the ﬁrst to run extremely quickly, but does not\neasily handle dropout/data augmentations, and scales well in terms of number of epochs, but not\nin dataset size. Note that this is mathematically equivalent to linear classiﬁcation. Our results are\nshown in Table 10. Although we ﬁnd speedups extremely signiﬁcant and they obtain nontrivial\nperformance, performance signiﬁcantly degrades and the models also exhibit overﬁtting (likely due\nto lack of regularization; unlike the training of FPT, dropout is not applied).\nTask Speedup Output Only FPT Full Transformer\nListOps 500 −2000× 32.8% 38.4% 38%\nCIFAR-10 LRA 500 −2000× 24.7% 38.6% 42%\nTable 10: Training only the output layer as a linear regression problem. Speedup refers to wall clock\ntime per epoch (after the ﬁrst). Larger models have larger speedups.\n10\n3.10 What is the role of model depth in token mixing?\nOne interesting question is the importance of the depth of the transformer for generating represen-\ntions which “mix” tokens: for instance, if there is only one layer and the parameters are random, it\nis unlikely for the tokens to be mixed well, whereas if there are many layers, there are many chances\nfor the tokens to mix and form interesting representations useful for downstream tasks. We inves-\ntigate this on ListOps by considering pretrained vs random models, where we only take the ﬁrst X\nlayers of the 12-layer pretrained model (i.e. for X=3, we use the ﬁrst 3 layers of the pretrained GPT-2\nmodel and perform classiﬁcation from those hidden states). Additionally, to maximally highlight the\nimportance of the pretrained parameters, we randomly initialize the input layer, and do not train the\ninput or positional parameters. We ﬁrst show results are ﬁnetuning the output layer and layernorm\nparameters, and then show only ﬁnetuning the output layer.\nWith ﬁnetuning layernorm. We ﬁrst investigate this question with ﬁnetuning the layernorm pa-\nrameters (i.e. we ﬁnetune only the output layer and the layernorm parameters). Results are shown in\nTable 11. Both models are unable to do well with only one layer, but the pretrained model performs\nsigniﬁcantly better than the random model at 2 layers, indicating that while the difference in per-\nformance at 12 layers is relatively small, there is a great beneﬁt to using pretrained layers for when\nconsidering a small number of layers in that the tokens are “mixed” faster.\nNumber of Layers Pretrained Random\n1 17% 17%\n2 36% 16%\n6 38% 35%\nTable 11: Test accuracy on Listops while varying model depth and ﬁnetuning layernorm parameters.\nPretrained layers “mix” the tokens faster, performing better at low model depths.\nWithout ﬁnetuning layernorm. We now investigate this question without ﬁnetuning the layernorm\nparameters, and only ﬁnetuning the output parameters, as in the reservoir computing setup in Section\n3.9. Note this is equivalent to linear classiﬁcation. This setting is the most challenging since all\nprocessing that is able to mix tokens is done by either random or pretrained parameters, and we\nare only able to train a linear layer on top of the output of the last token; as a result, the only token\nmixing that is done is performed entirely by the pretrained self-attention layers. Results are shown in\nTable 12. The random model does not do well even for a large number of layers, while the pretrained\nmodel can still do reasonably well, even though it requires more layers than before.\nNumber of Layers Pretrained Random\n1 12% -\n3 18% -\n6 33% -\n12 33% 17%\n24 - 17%\nTable 12: Test accuracy on Listops while varying model depth and only training output parameters.\nEven for a large number of layers, the random model does not learn to perform well.\n3.11 Can training more parameters improve performance?\nOur focus in this work was primarily to investigate if and how efﬁcient, general-purpose pretraining\ncan transfer across modalities. However, for practical applications, it would naturally be more suited\nto choose a more specialized ﬁnetuning scheme or add more trainable parameters. In this section,\nwe investigate additionally ﬁnetuning parameters with various methods, to see if frozen language\ntransformers can serve as a practical base for future work.\nWe ﬁrst investigate additionally ﬁnetuning the self-attention and feedforward layers, which were\npreviously frozen. We simply add them to the list of parameters ﬁnetuned, without changing the\n11\noptimization or learning rate scheme, although this is suboptimal. Our results are shown in Table\n13. Note that +Both is fully ﬁnetuning the 12-layer transformer (in other sections, we use full trans-\nformer to denote fully ﬁnetuning a transformer from scratch where the depth was tuned, whereas\nhere the depth is ﬁxed). We ﬁnd that ﬁnetuning the feedforward layers can improve performance,\nwhich is similar to techniques used in prior work (Houlsby et al., 2019), but ﬁnetuning the attention\nlayers can lead to divergence.\nModel Memory XOR ListOps MNIST C10 C10 LRA Homology\nFPT 100% 100% 38.4% 98.0% 68.2% 38.6% 12.7%\n+ Feedforward 100% 100% 36.0% 98.3% 76.6% 38.2% 13.1%\n+ Attention 100% 100% 36.8% 89.0% † 47.7%† 23.0% 10.9%\n+ Both 100% 100% 35.8% 93.1% † 32.9% 21.0% 10.5%\nTable 13: Additionally ﬁnetuning either the feedforward layers, attention layers, or both. We do not\nuse a per-layer learning scheme/etc. †training diverged, number reported before divergence.\nOn CIFAR-10, we experiment with additionally ﬁnetuning the last attention layer, shown in Table\n14. Generally we ﬁnd smarter pretraining methods can yield better performance, so we are optimistic\nabout the possibility of multimodal training/architectures improving performance in future work.\nTask Base (FPT) + Finetuning All FF Layers + Finetuning Last Attn Layer\nCIFAR-10 68.2% 76.6% 80.0%\nTable 14: Test accuracy on CIFAR-10 when ﬁnetuning additional parameters. In addition to FPT, if\nwe ﬁnetune the feedforward layers and the last self-attention layer, we can achieve 80% accuracy.\n3.12 Which parameters of the model are important to ﬁnetune?\nWe now run ablations for only ﬁnetuning select parameters to see which parameters are most sensi-\ntive. Note for all experiments (including the previous ones), we initialize the input layers as Gaussian\nif embeddings are used, or use an orthogonal initialization for linear layers; in particular, we ﬁnd\northogonal initialization to be very important when input parameters are not trained. We highlight\nsome results in Table 19; full results are shown on Page 16. Similar to a study of random CNNs by\nFrankle et al. (2020), we generally ﬁnd the layer norm parameters to be most important.\nTask output only + layernorm + input + positions\nBit Memory 76% 94% 100% 100%\nBit XOR 56% 98% 98% 100%\nListOps 15% 36% 36% 38%\nMNIST 23% 96% 98% 98%\nCIFAR-10 25% 54% 60% 68%\nCIFAR-10 LRA 17% 39% 39% 39%\nHomology 2% 9% 10% 13%\nTable 15: Ablation by successively adding certain parameters to the list of ﬁnetuned parameters for\npretrained frozen transformers.\n3.13 Is ﬁnetuning layer norm necessary for FPT to perform well?\nWhile previously we showed performance gains with ﬁnetuning layer norm, we could instead con-\nsider only ﬁnetuning the input and output layers, treating the entire GPT model as a black box. We\nshow results on CIFAR-10 in Table 16. The model performs worse; note accuracy is similar to not\nﬁnetuning the positional embeddings (see Section 3.12). This suggests the internal modulation of\nthe afﬁne layer norm parameters help, possibly by about as much as ﬁner positional information.\n12\nInitialization Frozen Layer Norm Finetuned Layer Norm\nPretrained 61.5% 68.2%\nRandom 55.0% 61.7%\nTable 16: Test accuracy on CIFAR-10 when only ﬁnetuning the input and output layer parameters.\n3.14 How well do the trends hold across other transformer models?\nWe also investigate how other transformer architectures perform when swapped out with GPT-2:\nBERT (Devlin et al., 2018), T5 (Raffel et al., 2019), and Longformer (Beltagy et al., 2020). For\nT5, we only use the encoder, and not the decoder. Our results are in Table 17. We ﬁnd results to\nroughly hold across some architectures – with some differences – although T5 tends to be slightly\nworse than the other models. An interesting question for future work is whether subtle differences\nin architecture, pretraining objective, or dataset contribute to these differences.\nTask GPT-2 (FPT Default) BERT T5 Longformer\nListOps 38.4% 38.3% 15.4% 17.0%\nCIFAR-10 68.2% 68.8% 64.7% 66.8%\nTable 17: Test accuracy for frozen pretrained transformer variants (base model sizes).\n4 Related Work and Discussion\n4.1 Transformers in multimodal settings\nTransformers (Vaswani et al., 2017) were ﬁrst used successfully for natural language processing\n(Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Brown et al., 2020). In recent\nyears, they have also been shown to be effective architectures for other modalities. One particular\nmodality of interest is computer vision (Chen et al., 2020a; Touvron et al., 2020); in particular,\nDosovitskiy et al. (2020) showed that transformers can outperform CNNs in the high-data regime on\nstandard object recognition benchmarks such as ImageNet and CIFAR. Furthermore, transformers\nhave also been used for prediction tasks over protein sequences (Jumper et al., 2021; Rao et al.,\n2021), reinforcement learning (Parisotto et al., 2020), and imitation learning (Abramson et al., 2020).\nWork speciﬁcally tackling multimodal tasks include Kaiser et al. (2017), who showed a single model\ncould learn a variety of multimodal tasks with an attention architecture. Recent work has utilized\ntransformers for multimodal predictive tasks, such as images and text in ViLBERT (Lu et al., 2019)\nand CLIP (Radford et al., 2021); these approaches generally use two distinct transformers to embed\nimages and text. Lu et al. (2020) applies ViLBERT to train a single model for a variety of combined\nvision and language tasks. Recent work from OpenAI (Goh et al., 2021) ﬁnds that some neurons\nlearned by CLIP are activated by a particular semantic concept, regardless of if the concept is pre-\nsented in language or picture form. Our work is most similar to DALL-E (Ramesh et al., 2021),\nwhich uses a single transformer to embed both the image and text modalities, which we consider\nto be generating a “universal latent space” that projects any type of input into a single latent space.\nSuch a latent space would be useful for a model that could learn from many sources of supervision.\n4.2 Transformers in transfer settings\nThere are also many works looking at transformers speciﬁcally in the context of in-modality trans-\nfer, such as ViT for vision (Dosovitskiy et al., 2020), T5 for language (Raffel et al., 2019), and\nUDSMProt for protein sequences (Strodthoff et al., 2020). CLIP (Radford et al., 2021) showed that\ntraining on text in addition to images could allow for zero-shot classiﬁcation via providing down-\nstream labels as text. Hernandez et al. (2021) do a thorough investigation of transfer with language\npretraining, notably showing transfer from English to Python, which they consider to be reasonably\ndistanced from English; many works have also looked at transferring from one langauge to another\n(Artetxe et al., 2019; Ponti et al., 2019). Similar to our work, Papadimitriou & Jurafsky (2020)\n13\ninvestigate transfer for LSTMs between modalities including code, different languages, and music,\nﬁnding that pretraining on “non-linguistic data with latent structure” can transfer to language, ﬁnd-\ning grammatical structure in a modality to be important, although we generally investigate the other\ndirection and explore more distanced modalities. Kiela et al. (2019) make similar observations for\naligning representation spaces of language and vision. Li et al. (2020) pretrain on a referential com-\nmunication game where an emergent learned language is used to transfer to NLP tasks. Wu et al.\n(2021) found explicitly pretraining computational primitives to transfer to mathematics tasks.\n4.3 Pretraining and ﬁnetuning of transformer models\nA common trend in deep learning models is to ﬁrst train a large model on an unsupervised objective\non a large dataset (Dai & Le, 2015; Radford et al., 2018) and then ﬁnetune on a small downstream\ndataset (e.g., by freezing the model and only ﬁnetuing the output layer). A common method used\nto ﬁnetune transformers are adapter networks (Rebufﬁ et al., 2017; Houlsby et al., 2019), which\nadd a fully connected residual block for each unique downstream task and also ﬁnetune the layer\nnorm parameters. For simplicity, we do not add the full adapter block but only train the layer norm\nparameters, reducing the number of parameters we consider. These techniques used are similar\nto prior approaches such as FiLM (Perez et al., 2018) and self-modulation (Chen et al., 2018). A\nrecent direction of research has explored learning prompt templates for large models (Shin et al.,\n2020) that simply require forward passes over the transformer. Unlike these works, we consider\nﬁnetuning on one modality (language) and ﬁnetuning on others, whereas prior work investigates\nﬁnetuning on the same modality as the pretraining task. Another interesting related work, although\nnot investigating transformers, by Frankle et al. (2020) ﬁnd randomly initialized CNNs, which only\ntrain the batchnorm afﬁne parameters, to work well on CIFAR-10. Their numbers are stronger than\nours on CIFAR-10, but include signiﬁcantly more inductive bias via a convolutional architecture, so\nthe main takeaway is slightly more relevant towards image tasks rather than arbitrary sequences.\n4.4 Self-attention layers as optimization steps\nThe nature of computation performed by self-attention layers has also been explored by other related\nworks. Bai et al. (2019) show that a single transformer self-attention block can be trained to perform\nan optimization step towards ﬁnding a stationary point, representing the solution to the task. Ram-\nsauer et al. (2020) show that the self-attention layer is a gradient step in a Hopﬁeld network with a\nlearning rate of 1, hinting that transformers are capable of storing and retrieving a large amount of\npatterns with an implicit energy function. An interesting discussion from Goyal & Bengio (2020)\npoints out a connection in viewing the key-value queries used in attention as similar to function sig-\nnatures in computer programming: the key maps the input to a type (e.g., ﬂoat) and the value maps\nthe input to its value (e.g., 3.14), and if the type matches the function signature, the function can be\napplied to the value – this may be particularly relevant when we consider using a single self-attention\nlayer applied to different modalities, as the modality may be embedded in the type.\n4.5 Global workspace theory\nA common technique for evaluating the embeddings learned by an unsupervised model is to train a\nlinear layer on top of the embeddings for a downstream task (Donahue et al., 2016; Oord et al., 2018;\nChen et al., 2020b), which is reasonable when you ﬁnetune on the same modality as the pretrained\none. However, when ﬁnetuning on a different modality, as in our setting, we have to reframe this\nnotion of generalizable embedding quality – instead of only ﬁnetuning the output layer, we also\nwant to ﬁnetune the input layer, and instead evaluate the ability of the frozen intermediate model\nto perform generalizable computation. This is reminiscent of Global Workspace Theory (Baars,\n1993), which revolves around the notion that there is a “blackboard” that different parts of the brain\nsend data to; we might consider the frozen language model as being a blackboard in this setting.\nLanguage might also be a natural choice of model for this blackboard, as there are hypotheses that\nlanguage may serve as a good multipurpose high-level representation for cognitive behavior and\nconscious planning (Andreas et al., 2017; Goyal & Bengio, 2020).\n14\n4.6 Reservoir computing\nSimilarly to the FPT setup and Global Workspace Theory, in reservoir computing (Tanaka et al.,\n2019) and echo state networks (Jaeger, 2001; Jaeger & Haas, 2004), a random recurrent network\nis frozen and only the output readout layer is trained. These models are very fast to train, using\na similar setup as in Section 3.9, because the activations of the recurrent network can be cached\nand it is unnecessary to backpropagate over time. Somewhat differently to the FPT architecture,\necho state networks are recurrent and thus feed back into themselves, which allows the outputs of\nthe random frozen network to modulate future inputs. Unlike echo state networks, we also notably\nﬁnetune the input and positional embeddings, which allow the inputs to the frozen network to adapt\nto a particular modality/for a query to the frozen network to be learned. Echo state networks are also\nsimilar to the perspective of self-attention applying a data-dependent ﬁlter to the inputs, as opposed\nto 1D convolutions, which are ﬁxed ﬁlters regardless of the input modality.\n5 Conclusion\nWe proposed transferring a pretrained transformer language model for downstream tasks in non-\nlanguage modalities. Through extensive empirical evaluation, we showed that these models could\nachieve performance competitive with transformers fully trained on the downstream task without\nhaving to ﬁnetune the self-attention and feedforward layers, relying solely on frozen parameters\nfrom the language model to perform the bulk of the computation.\nWe believe this work can serve as the foundation for future work investigating transfer between\nmodalities. In future, we are interested in investigating the use of other data-rich modalities (e.g.,\nvision) or a hybrid of multiple domains being used to provide the necessary substrate for pretraining\na universal computational engine. It would also be interesting to explore frozen pretrained models\nfor tasks beyond predictive modeling, such as reinforcement learning (Abramson et al., 2020).\nWe note that a limitation of our analysis in that we analyze speciﬁc models on a restricted set of\ntasks. More investigation can highlight whether or not similar behavior occurs for other models on\nother tasks. For instance, in Section 3.14, we ﬁnd the architecture can have a signiﬁcant impact\non results. As training regimes for these models evolve, performing similar experiments may yield\ndifferent results, and we are excited for more research in this direction.\nFor high stakes applications in the real-world, there are potential concerns with transfer of harmful\nbiases from one modality to one another using pretrained transformer models trained on vast quan-\ntities of unlabeled, uncurated datasets (Sheng et al., 2019; Bender et al., 2021). Mitigating these\nbiases is an active area of research (Grover et al., 2019; Choi et al., 2020). Conversely, there are also\npotential upsides with FPT models being able to better exploit representative datasets from one or\nmore modalities, which merit future investigation as well.\nAcknowledgements\nWe would like to thank Luke Metz, Kimin Lee, Fangchen Liu, Roshan Rao, Aravind Srinivas, Nikita\nKitaev, Daniel Freeman, Marc’Aurelio Ranzato, Jacob Andreas, and Ashish Vaswani for valuable\nfeedback and discussions. We would also like to thank members of the community for providing\nfeedback online on an earlier version of this paper.\n15\nParameter ablations for pretrained models\nTask output only output + input output + positions output + layernorm\nBit Memory 76% 98% 93% 94%\nBit XOR 56% 72% 84% 98%\nListOps 15% 17% 35% 36%\nMNIST 23% 85% 93% 96%\nCIFAR-10 25% 53% 38% 54%\nCIFAR-10 LRA 17% 22% 30% 39%\nHomology 2% 8% 8% 9%\nTable 18: Ablation by only ﬁnetuning individual types of parameters for pretrained frozen trans-\nformers. We bold the most important parameter (measured by highest test accuracy) for each task.\nTask output only + layernorm + input + positions\nBit Memory 76% 94% 100% 100%\nBit XOR 56% 98% 98% 100%\nListOps 15% 36% 36% 38%\nMNIST 23% 96% 98% 98%\nCIFAR-10 25% 54% 60% 68%\nCIFAR-10 LRA 17% 39% 39% 39%\nHomology 2% 9% 10% 13%\nTable 19: Ablation by successively adding certain parameters to the list of ﬁnetuned parameters for\npretrained frozen transformers.\nParameter ablations for random models\nTask output only output + input output + positions output + layernorm\nBit Memory 75% 75% 75% 75%\nBit XOR 50% 51% 59% 100%\nListOps 17% 17% 18% 35%\nMNIST 25% 28% 34% 83%\nCIFAR-10 20% 24% 21% 46%\nCIFAR-10 LRA 11% 16% 12% 34%\nHomology 2% 2% 6% 9%\nTable 20: Finetuning individual types of parameters for random frozen transformers.\nTask output only + layernorm + input + positions\nBit Memory 75% 75% 75% 76%\nBit XOR 50% 100% 100% 100%\nListOps 17% 35% 36% 37%\nMNIST 25% 83% 92% 92%\nCIFAR-10 20% 46% 56% 62%\nCIFAR-10 LRA 11% 34% 36% 36%\nHomology 2% 9% 9% 9%\nTable 21: Ablation by successively adding certain parameters to the list of ﬁnetuned parameters for\nrandom frozen transformers.\n16\nReferences\nJosh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Stephen Clark, An-\ndrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, et al. Imitating interactive intelligence.\narXiv preprint arXiv:2012.05672, 2020.\nJacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. arXiv preprint\narXiv:1711.00482, 2017.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of mono-\nlingual representations. arXiv preprint arXiv:1910.11856, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nBernard J Baars. A cognitive theory of consciousness. Cambridge University Press, 1993.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. arXiv preprint\narXiv:1909.01377, 2019.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big. In Proceedings of the 2020 Con-\nference on Fairness, Accountability, and Transparency; Association for Computing Machinery:\nNew York, NY, USA, 2021.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International Conference on Machine Learning, pp. 1691–\n1703. PMLR, 2020a.\nTing Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative adver-\nsarial networks. arXiv preprint arXiv:1810.01365, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597–1607. PMLR, 2020b.\nKristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon. Fair generative modeling\nvia weak supervision. In International Conference on Machine Learning, pp. 1887–1898. PMLR,\n2020.\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. arXiv preprint\narXiv:1511.01432, 2015.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nJeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n17\nSara El-Gebali, Jaina Mistry, Alex Bateman, Sean R Eddy, Aur´elien Luciani, Simon C Potter, Mat-\nloob Qureshi, Lorna J Richardson, Gustavo A Salazar, Alfredo Smart, Erik L L Sonnhammer,\nLayla Hirsh, Lisanna Paladin, Damiano Piovesan, Silvio C E Tosatto, and Robert D Finn. The\nPfam protein families database in 2019. Nucleic Acids Research , 47(D1):D427–D432, 2019.\nISSN 0305-1048. doi: 10.1093/nar/gky995.\nNaomi K Fox, Steven E Brenner, and John-Marc Chandonia. Scope: Structural classiﬁcation of\nproteins—extended, integrating scop and astral data and classiﬁcation of new structures. Nucleic\nacids research, 42(D1):D304–D309, 2013.\nJonathan Frankle, David J Schwab, and Ari S Morcos. Training batchnorm and only batchnorm: On\nthe expressive power of random features in cnns. arXiv preprint arXiv:2003.00152, 2020.\nGabriel Goh, Chelsea V oss, Daniela Amodei, Shan Carter, Michael Petrov, Justin Jay Wang, Nick\nCammarata, and Chris Olah. Multimodal neurons in artiﬁcial neural networks. 2021.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.\narXiv preprint arXiv:2011.15091, 2020.\nAditya Grover, Jiaming Song, Alekh Agarwal, Kenneth Tran, Ashish Kapoor, Eric Horvitz, and\nStefano Ermon. Bias correction of learned generative models using likelihood-free importance\nweighting. arXiv preprint arXiv:1906.09531, 2019.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer.\narXiv preprint arXiv:2102.01293, 2021.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nJie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping\nprotein sequences to folds. Bioinformatics, 34(8):1295–1303, 2018.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\nHerbert Jaeger. The “echo state” approach to analysing and training recurrent neural networks-with\nan erratum note. Bonn, Germany: German National Research Center for Information Technology\nGMD Technical Report, 148(34):13, 2001.\nHerbert Jaeger and Harald Haas. Harnessing nonlinearity: Predicting chaotic systems and saving\nenergy in wireless communication. science, 304(5667):78–80, 2004.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunya-\nsuvunakool, Olaf Ronneberger, Russ Bates, Augustin ˇZ´ıdek, Alex Bridgland, Clemens Meyer,\nSimon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-\nParedes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman,\nMartin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray\nKavukcuoglu, Pushmeet Kohli, and Demis Hassabis. High accuracy protein structure prediction\nusing deep learning. 2021.\nLukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and\nJakob Uszkoreit. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.\nDouwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez, and Davide Testuggine. Supervised\nmultimodal bitransformers for classifying images and text. arXiv preprint arXiv:1909.02950 ,\n2019.\n18\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nAlex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.\nYaoyiran Li, Edoardo M Ponti, Ivan Vuli´c, and Anna Korhonen. Emergent communication pretrain-\ning for few-shot machine translation. arXiv preprint arXiv:2011.00890, 2020.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task\nvision and language representation learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 10437–10446, 2020.\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic neural\nnetworks with backpropagation. In International Conference on Machine Learning , pp. 3559–\n3568. PMLR, 2018.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\nIsabel Papadimitriou and Dan Jurafsky. Pretraining on non-linguistic structure as a tool for analyzing\nlearning bias in language models. arXiv preprint arXiv:2004.14601, 2020.\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,\nMax Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers\nfor reinforcement learning. In International Conference on Machine Learning , pp. 7487–7498.\nPMLR, 2020.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. arXiv preprint arXiv:1912.01703, 2019.\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual\nreasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 32, 2018.\nEdoardo Maria Ponti, Ivan Vuli ´c, Ryan Cotterell, Roi Reichart, and Anna Korhonen. Towards\nzero-shot language modeling. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2893–2903, 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. Image, 2:T2, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nAditya Ramesh, Mikhail Pavolv, Gabriel Goh, Scott Gray, Mark Chen, Rewon Child, Vedant Misra,\nPamela Mishkin, Gertchen Krueger, Sandhini Agarwal, and Ilya Sutskever. Dall·e: Creating\nimages from text, 2021.\nHubert Ramsauer, Bernhard Sch ¨aﬂ, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gru-\nber, Markus Holzleitner, Milena Pavlovi ´c, Geir Kjetil Sandve, Victor Greiff, et al. Hopﬁeld\nnetworks is all you need. arXiv preprint arXiv:2008.02217, 2020.\n19\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel,\nand Yun S Song. Evaluating protein transfer learning with tape. In Advances in Neural Informa-\ntion Processing Systems, 2019.\nRoshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu,\nand Alexander Rives. Msa transformer. bioRxiv, 2021. doi: 10.1101/2021.02.12.430858.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. arXiv preprint arXiv:1705.08045, 2017.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations\nby error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive\nScience, 1985.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a\nbabysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts. arXiv preprint\narXiv:2010.15980, 2020.\nNils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek. Udsmprot: universal deep\nsequence models for protein classiﬁcation. Bioinformatics, 36(8):2401–2409, 2020.\nGouhei Tanaka, Toshiyuki Yamane, Jean Benoit H ´eroux, Ryosho Nakane, Naoki Kanazawa, Seiji\nTakeda, Hidetoshi Numata, Daiju Nakano, and Akira Hirose. Recent advances in physical reser-\nvoir computing: A review. Neural Networks, 115:100–123, 2019.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient\ntransformers. arXiv preprint arXiv:2011.04006, 2020.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J´egou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,\n2017.\nRoss Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for\nComputational Linguistics.\nYuhuai Wu, Markus Rabe, Wenda Li, Jimmy Ba, Roger Grosse, and Christian Szegedy.\nLime: Learning inductive bias for primitives of mathematical reasoning. arXiv preprint\narXiv:2101.06223, 2021.\n20\nAppendix\nContents\nA Summary of arXiv Updates 22\nB Background on Transformers 22\nB.1 Self-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nB.2 Positional Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nB.3 Layer Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.4 Pretraining Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.5 Model Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nC Experimental Details 23\nD Details by Table 24\nD.1 Can pretrained language models transfer to different modalities? . . . . . . . . . . 24\nD.2 What is the importance of the pretraining modality? . . . . . . . . . . . . . . . . . 25\nD.3 How important is the transformer architecture compared to LSTM architecture? . . 25\nD.4 Does language pretraining improve compute efﬁciency over random initialization? 26\n21\nA Summary of arXiv Updates\nWe summarize changes made in updated versions:\nv1. (9 Mar 2021) Original release.\nv2. (30 June 2021) Updated Section 3.3 with more analysis of the frozen LSTM architecture\nand additional experimental details. Added new Section 3.10 discussing model depth and\ntoken mixing, new results in Section 3.11 discussing how different freezing strategies can\nimprove performance, and attention mask visualization for random frozen transformer to\nSection 3.5. Included more details about experiments and hyperparameters, and added\nsome new citations (notably Wu et al. (2021) for related LIME work and Frankle et al.\n(2020) for similar frozen analysis for CNNs). Github was also updated to include LSTM\narchitecture, vision pretraining, and remote homology tasks. Minor writing updates.\nB Background on Transformers\nIn this section, we give a description of the transformer architecture used in our experiments, namely\nthe GPT-2 architecture (Radford et al., 2019).\nB.1 Self-Attention\nThe main subcomponent of the transformer architecture is the self-attention layer, which takes in l\ninput tokens and outputs loutput tokens, both of dimensionndim. Each input token xi is mapped by\nlinear transformations Q, K, and V – denoting query, key, and values, respectively – intoqi, ki, and\nvi. Both qi and ki have dimension dk, and vi has dimension dv. To generate the output token yi, dot\nproducts are calculated between query qi and keys kj, and fed into a softmax operation to generate\nweights wi ∈[0,1] (in practice, a scaling temperature factor of 1√dk\nis used to reduce the sharpness\nof the softmax). Then, the weights are used to generate yi as a weighted sum of all the values, i.e.:\nyi =\nl∑\nj=1\nexp(q⊤\ni kj)\n∑l\nk=1 exp(q⊤\ni kk)\nvj (1)\nThis is extended to multi-head attention over nheads heads by doing the above procedure nheads\ntimes, and then concatenating. To recover the original dimension the concatenated vector (of di-\nmension dvnheads) is multiplied by a projection matrix Wproj ∈Rdvnheads×ndim .\nGPT-2 applies a causal mask to its inputs, i.e. the output token i is only allowed to attend to the\ninput tokens j ≤i, which changes the upper bounds of the sums in Equation 1 toiinstead of l. This\nallows for unsupervised pretraining methods like language modeling (see Appendix B.4).\nA residual connection is used to connect the inputs with the outputs of the attention layer. Then, in\nthe rest of the transformer block, a two-layer MLP is used, conventionally projecting the dimension\nupwards to 4 ·ndim for the inner dimension and using the GELU activation function (Hendrycks\n& Gimpel, 2016). Another residual connection is used to connect the outputs of the MLP with the\nprevious outputs of the attention layer.\nThis forms the basis of the transformer block. As it preserves the dimension ndim, multiple blocks\ncan be learned and stacked on top of each other nlayers times, before feeding the ﬁnal hidden states\nto the output layer. In our work, we only use the output of the last hidden state for classiﬁcation,\nalthough in principle other methods are reasonable.\nB.2 Positional Embeddings\nAs the self-attention blocks are permutation-invariant, in order to capture positional information\nabout sequences, positional embeddings are learned. For each position i ∈ (1,..., max len), a\nvector pi is learned. At the front of the transformer, before feeding in the inputs xi into the self-\nattention blocks, the positional embeddings are added to the input embeddings as xi := xi + pi.\n22\nB.3 Layer Norm\nLayer norm (Ba et al., 2016) is frequently used in recurrent and transformer architectures as a means\nof normalizing the activations. In particular, for the activations of training example xof dimension\nndim, it normalizes by the mean and variance over the features:\n˜yi =\nxi −mean({xj}ndim\nj=1 )\nstd({xj}ndim\nj=1 ) (2)\nThen, afﬁne scale and shift parameters each of dimensionndim – γand β, respectively – are learned\nto generate the outputs y.\nyi = γi˜yi + βi (3)\nLayer norm is applied twice per self-attention block: once before the attention layer and once before\nthe MLP. As a result, a total of 4 ·nlayers ·ndim layer norm parameters are learned.\nB.4 Pretraining Objective\nGPT-2 is pretrained on an retrogressive language modeling objective optimizing for parameters\nwhich maximize the log-likelihood of the data: max θE[log pθ(x)]. GPT-2 models sequences au-\ntoregressively, factorizing the probability distributionp(x) =p(x1,...,x l) via chain rule as:\np(x) =\nl∏\ni=1\np(xi|xi−1,...,x 1) (4)\nFor the language domain, this objective can be interpreted as “given the previous i−1 words of a\nsentence, predict the next word”.\nB.5 Model Sizes\nThe model sizes from Section 3.7 are as follows:\nModel Size nlayers ndim nheads # Parameters\nSmall (Base) 12 768 12 117M\nMedium 24 1024 16 345M\nLarge 36 1280 20 774M\nTable 22: Hyperparameters for architectures for larger model sizes.\nThe hyperparameters for the experiments with other architectures (Vision Transformer, BERT, Long-\nformer, T5) are the same as for the base model size shown above.\nC Experimental Details\nWe use implementations of and pretrained models from the Huggingface Transformers library (Wolf\net al., 2020). We train all models using the Adam (Kingma & Ba, 2014) optimizer following Pytorch\n(Paszke et al., 2019) defaults. For all transformer models, we use a learning rate of 10−3 without\nlearning rate scheduling. For the remote homology task only, we use a learning rate of 10−4 as we\nfound it to give better performance than 10−3. We generally use the largest batch size that ﬁts on\nan RTX 2080 Ti graphics card, somewhere between 2 and 16, without gradient accumulation. Note\nthat except for the remote homology task, we did not tune the FPT hyperparameters. For all LSTMs,\nwe use a lower learning rate of 3 ×10−4 and the same batch sizes as transformers of the same size.\nModels are trained to convergence and evaluated on a heldout test set.\n23\nD Details by Table\nFor clarity, we explicitly write out ﬁner details for some experiment sections where numbers can\nrepresent different model types.\nD.1 Can pretrained language models transfer to different modalities?\nThis section refers to Table 1 in Section 3.1.\nBit Memory\n1. FPT: 12-layer base size FPT model (ﬁnetuning input, output, position, and layernorm\nparams).\n2. Full: 12-layer base size GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nBit XOR\n1. FPT: 12-layer base size FPT model (ﬁnetuning input, output, position, and layernorm\nparams).\n2. Full: 12-layer base size GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nListOps\n1. FPT: 12-layer base size FPT model (ﬁnetuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Tay et al. (2020) (3-layer vanilla transformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nCIFAR-10\n1. FPT: 36-layer large size FPT model (ﬁnetuning input, output, position, and layernorm\nparams).\n2. Full: 3-layer, 768 hidden dimension GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nCIFAR-10 LRA\n1. FPT: 12-layer base size FPT model (ﬁnetuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Tay et al. (2020) (3-layer vanilla transformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nRemote Homology\n1. FPT: 12-layer base size FPT model (ﬁnetuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Rao et al. (2019) (12-layer, 512 hidden dimension vanilla\ntransformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\n24\nD.2 What is the importance of the pretraining modality?\nThis section refers to Table 2 in Section 3.2.\nAll tasks\n1. FPT: 12-layer base size FPT model (ﬁnetuning input, output, position, and layernorm\nparams). This differs from Table 1, Section 3.1 only in the CIFAR-10 model size.\n2. Random: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, position, and layernorm params).\n3. Bit: 12-layer base size GPT-2 model (ﬁnetuning input, output, position, and layernorm\nparams), after ﬁrst being fully ﬁnetuned on Bit Memory following default random initial-\nization.\n4. ViT: 12-layer, 768 hidden dimension base size ViT model (ﬁnetuning input, output, posi-\ntion, and layernorm params), pretrained on 224 ×224 ImageNet-21k with a patch size of\n16. ( vit base patch16 224 from the timm Pytorch library (Wightman, 2019)). We\nreinitialize the input layer from scratch to match each task, and do not use a CLS token or\nan MLP as the output network – instead using a linear layer from the last token – matching\nthe protocol for the other methods.\nD.3 How important is the transformer architecture compared to LSTM architecture?\nThe following refer to Section 3.3. In Table 3:\nAll tasks\n1. Trans: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, and layernorm params). Note: same as “Random” in Table 2, Section 3.2.\n2. LSTM: 3-layer, 768 hidden dimension “standard” LSTM (training input, output, and lay-\nernorm params). Does not have residual connections or positional embeddings.\n3. LSTM ∗: 12-layer, 768 hidden dimension LSTM (training input, output, position, and lay-\nernorm params).\nTable 4:\nAll tasks\n1. 12: 12-layer, 768 hidden dimension “standard” LSTM (training input, output, and layer-\nnorm params).\n2. 3: 3-layer, 768 hidden dimension “standard” LSTM (training input, output, and layernorm\nparams).\nTable 5:\nAll tasks\n1. 12-layer LSTM: 12-layer, 768 hidden dimension “standard” LSTM (training input, output,\nand layernorm params). Note: same as “12” in Table 4, Section 3.3.\n2. + Residual Connections: 12-layer, 768 hidden dimension LSTM with residual connections\n(training input, output, and layernorm params).\n3. + Positional Embeddings: 12-layer, 768 hidden dimension LSTM with residual connec-\ntions and positional embeddings (training input, output, position, and layernorm params).\nNote: same as “LSTM∗” in Table 3, Section 3.3.\n25\nD.4 Does language pretraining improve compute efﬁciency over random initialization?\nThis section refers to Table 6 in Section 3.4.\nAll tasks\n1. FPT: 12-layer base size FPT model (ﬁnetuning input, output, position, and layernorm\nparams). Note: same models as “FPT” in Table 2, Section 3.2.\n2. Random: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, position, and layernorm params). Note: same models as “Random” in Table\n2, Section 3.2.\n26",
  "concepts": [
    {
      "name": "Computation",
      "score": 0.6749411821365356
    },
    {
      "name": "Transformer",
      "score": 0.6400226354598999
    },
    {
      "name": "Computer science",
      "score": 0.4904015064239502
    },
    {
      "name": "Electrical engineering",
      "score": 0.24111509323120117
    },
    {
      "name": "Engineering",
      "score": 0.21369239687919617
    },
    {
      "name": "Programming language",
      "score": 0.11735287308692932
    },
    {
      "name": "Voltage",
      "score": 0.0853319764137268
    }
  ],
  "topic": "Computation"
}