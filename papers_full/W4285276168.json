{
  "title": "Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models.",
  "url": "https://openalex.org/W4285276168",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3163460661",
      "name": "Daniil Moskovskiy",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3106834462",
      "name": "Daryna Dementieva",
      "affiliations": [
        "Technical University of Munich",
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110658729",
      "name": "Alexander Panchenko",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3153611199",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W3035125262",
    "https://openalex.org/W3172413486",
    "https://openalex.org/W3174681468",
    "https://openalex.org/W4221155453",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962937198",
    "https://openalex.org/W3170773997",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4205923021",
    "https://openalex.org/W3154272574",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W2986893290",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3166664235",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4289543067",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W2947314843",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4205635927",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4200629076",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2952650870",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W2949611393",
    "https://openalex.org/W3100727892",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3197475601",
    "https://openalex.org/W3175221561"
  ],
  "abstract": "Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are monolingual i.e. designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models in this setting. Unlike previous works we aim to make large language models able to perform detoxification without direct fine-tuning in a given language. Experiments show that multilingual models are capable of performing multilingual style transfer. However, tested state-of-the-art models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is currently inevitable and motivating the need of further research in this direction.",
  "full_text": "Exploring Cross-lingual Textual Style Transfer with\nLarge Multilingual Language Models\nDaniil Moskovskiy1 Daryna Dementieva1,2 Alexander Panchenko1\n1Skolkovo Institute of Science and Technology, Russia\n2Technical University of Munich, Germany\n{daniil.moskovskiy,daryna.dementieva,a.panchenko}@skoltech.ru\nAbstract\nDetoxification is a task of generating text in po-\nlite style while preserving meaning and fluency\nof the original toxic text. Existing detoxifica-\ntion methods are designed to work in one exact\nlanguage. This work investigates multilingual\nand cross-lingual detoxification and the behav-\nior of large multilingual models like in this\nsetting. Unlike previous works we aim to make\nlarge language models able to perform detoxifi-\ncation without direct fine-tuning in given lan-\nguage. Experiments show that multilingual\nmodels are capable of performing multilingual\nstyle transfer. However, models are not able to\nperform cross-lingual detoxification and direct\nfine-tuning on exact language is inevitable.\n1 Introduction\nThe task of Textual Style Transfer (Textual Style\nTransfer) can be viewed as a task where cer-\ntain properties of text are being modified while\nrest retain the same 1. In this work we focus\non detoxification textual style transfer (dos San-\ntos et al., 2018a; Dementieva et al., 2021a). It\ncan be formulated as follows: given two text\ncorpora DX = {x1, x2, . . . xn} and DY =\n{y1, y2, . . . , yn}, where X, Y - are two sets of all\npossible text in styles sX, sY respectively, we want\nto build a model fθ : X → Y , such that the prob-\nability p(ygen|x, sX, sY ) of transferring the style\nsX of given text x (by generation ygen) to the style\nsY is maximized (where sX and sY are toxic and\nnon-toxic styles respectively).\nSome examples of detoxification presented in\nTable 1.\nTextual style transfer gained a lot of attention\nwith a rise of deep learning-based NLP methods.\nGiven that, Textual Style Transfer has now a lot of\nspecific subtasks ranging from formality style trans-\nfer (Rao and Tetreault, 2018; Yao and Yu, 2021)\n1Hereinafter the data-driven definition of style is used.\nTherefore, we call style a characteristic of given dataset that\ndiffers from a general dataset (Jin et al., 2020).\nand simplification of domain-specific texts (De-\nvaraj et al., 2021; Maddela et al., 2021) to emotion\nmodification (Sharma et al., 2021) and detoxifica-\ntion (debiasing) (Li et al., 2021; Dementieva et al.,\n2021a).\nThere exist a variety of Textual Style Transfer\nmethods: from totally supervised methods (Wang\net al., 2019b; Zhang et al., 2020; Dementieva et al.,\n2021a) which require a parallel text corpus for train-\ning to unsupervised (Shen et al., 2017; Wang et al.,\n2019a; Xu et al., 2021) that are designed to work\nwithout any parallel data. The latter sub-field of re-\nsearch is more popular nowadays due to the scarcity\nof parallel text data for Textual Style Transfer. On\nthe other hand, if we address Textual Style Trans-\nfer task as a Machine Translation task we get a\nsignificant performance boost (Prabhumoye et al.,\n2018).\nThe task of detoxification, in which we focus\nin this work, is relatively new. First work on\ndetoxification was a sequence-to-sequence collabo-\nrative classifier, attention and the cycle consistency\nloss (dos Santos et al., 2018b). A recent work by\n(Laugier et al., 2021) introduces self-supervised\nmodel based on T5 model (Raffel et al., 2020) with\na denoising and cyclic auto-encoder loss.\nBoth these methods are unsupervised which is an\nadvantage but it comes from the major current prob-\nlem of the textual style transfer. There is a lack of\nparallel data for Textual Style Transfer since there\nexist only few parallel datasets for English (Rao\nand Tetreault, 2018) and some other languages (Bri-\nakou et al., 2021). When it comes to detoxification\nthere are only two parallel detoxification corpora\navailable now and they both appeared only last year\n(Dementieva et al., 2021b). Most state-of-the-art\nmethods rely on large amounts of text data which is\noften available for some well-researched languages\nlike English but lacking for other languages almost\nentirely. Therefore, it is important to study whether\ncross-lingual (or at least multilingual) detoxifica-\nSource text Target text\nWhat the f*ck is your problem? What is your problem?\nThis whole article is bullshit. This article is not good.\nYeah, this clowns gonna make alberta great again! Yeah, this gonna make Alberta great again\nTable 1: Examples of desired detoxification results.\ntion is possible.\nMultilingual language models such as mBART\n(Liu et al., 2020), mT5 (Xue et al., 2021) have\nrecently become available. This work explores the\npossibility of multilingual and cross-lingual textual\nstyle transfer (Textual Style Transfer) using such\nlarge multilingual language models. We test the\nhypothesis that modern large text-to-text models\nare able to generalize ability of style transfer across\nlanguages.\nOur contributions can be summarized as fol-\nlows2:\n1. We introduce a novel study of multilingual\ntextual style transfer and conduct experiments\nwith several multilingual language models and\nevaluate their performance.\n2. We conduct cross-lingual Textual Style Trans-\nfer experiments to investigate whether multi-\nlingual language models are able to perform\nTextual Style Transfer without fine-tuning on\na specific language.\n2 Methodology\nWe formulate the task of supervised Textual Style\nTransfer as a sequence-to-sequence NMT task and\nfine-tune multilingual language models to translate\nfrom \"toxic\" to \"polite\" language.\n2.1 Datasets\nIn this work we use two datasets for Russian and\nEnglish languages. Aggregated information about\ndatasets could be found in Table 2, examples from\ndatasets can be found in A.1 and A.2.\nLanguage Train Dev Test\nEnglish 18777 988 671\nRussian 5058 1000 1000\nTable 2: Aggregated datasets statistics.\n2All code is available online: https://github.\ncom/skoltech-nlp/multilingual_detox\nRussian data We use detoxification dataset 3\nwhich consists of 5058 training sentences, 1000\nvalidation sentences and 1000 test sentences.\nEnglish data We use ParaDetox (Dementieva\net al., 2021b) dataset. It consists of 19766 toxic\nsentences and their polite paraphrases. This data is\nsplit into training and validation as95% for training\nand 5% for validation. For testing we use a set of\n671 toxic sentences.\n2.2 Experimental Setup\nWe perform a series of experiments on detoxifica-\ntion using parallel data for English and Russian.\nWe train models in two different setups: multilin-\ngual and cross-lingual.\nMultilingual setup In this setup we train models\non data containing both English and Russian texts\nand then compare their performance with baselines\ntrained on these languages solely.\nCross-lingual setup In cross-lingual setup we\ntest the hypothesis that models are able to perform\ndetoxification without explicit fine-tuning on exact\nlanguage. We fine-tune models on English and\nRussian separately and then test their performance.\n2.3 Models\nScaling language models to many languages has\nbecome an emerging topic of interest recently (De-\nvlin et al., 2019; Tan et al., 2019; Conneau and\nLample, 2019; Conneau et al., 2020). We adopt\nseveral multilingual models to textual style transfer\nin our work.\nBaselines We use two detoxification methods as\nbaselines in this work - Delete method which sim-\nply deletes toxic words in the sentence according\nto the vocabulary of toxic words and CondBERT.\nThe latter approach works in usual masked-LM\nsetup by masking toxic words and replacing them\nwith non-toxic ones. This approach was first pro-\nposed by (Wu et al., 2019) as a data augmentation\n3https://github.com/skoltech-nlp/\nrusse_detox_2022\nmethod and then adopted to detoxification by (Dale\net al., 2021).\nmT5 mT5 (Xue et al., 2021) is a multilingual\nversion of T5 (Raffel et al., 2020) - a text-to-text\ntransformer model, which was trained on many\ndownstream tasks. mT5 replicates T5 training but\nnow it is trained on more than 100 languages.\nmBART mBART (Liu et al., 2020) is a multi-\nlingual variation of BART (Lewis et al., 2020) -\ndenoising autoencoder built with a sequence-to-\nsequence model. mBART is trained on mono-\nlingual corpora across many languages. We\nadopt mBART in sequence-to-sequence detoxifica-\ntion task via fine-tuning on parallel detoxification\ndataset.\n2.4 Evaluation metrics\nUnlike other NLP tasks, one metric is not enough\nto benchmark the quality of style transfer. The\nideal Textual Style Transfer model output should\npreserve the original contentof the text, change the\nstyle of the original text to target and the generated\ntext also should be grammatically correct . We\nfollow Dale et al. (2021) approach in Textual Style\nTransfer evaluation.\n2.4.1 Content Preservation\nRussian Content preservation score ( SIM) is\nevaluated as a cosine similarity of LaBSE (Feng\net al., 2020) sentence embeddings. The model is\nslightly different from the original one, only En-\nglish and Russian embeddings are left.\nEnglish Similarity (SIM) between the embed-\nding of the original sentence and the generated one\nis calculated using the model presented by Wiet-\ning et al. (2019). Being is trained on paraphrase\npairs extracted from ParaNMT corpus (Wieting and\nGimpel, 2018), this model’s training objective is\nto select embeddings such that the similarity of\nembeddings of paraphrases is higher than the simi-\nlarity between sentences that are not paraphrases.\n2.4.2 Grammatic and language quality\n(fluency)\nRussian We measure fluency (FL) with a BERT-\nbased classifier (Devlin et al., 2019) trained to dis-\ntinguish real texts from corrupted ones. The model\nwas trained on Russian texts and their corrupted\n(random word replacement, word deletion and in-\nsertion, word shuffling etc.) versions. Fluency is\ncalculated as a difference between the probabilities\nof being corrupted for source and target sentences.\nThe logic behind using difference is that we ensure\nthat the generated sentence is not worse than the\noriginal one in terms of fluency.\nEnglish We measure fluency (FL) as a percent-\nage of fluent sentences evaluated by the RoBERTa-\nbased4 (Liu et al., 2019) classifier of linguistic ac-\nceptability trained on CoLA (Warstadt et al., 2019)\ndataset.\n2.4.3 Style transfer accuracy\nRussian Style transfer accuracy (STA) is evalu-\nated with a BERT-based (Devlin et al., 2019) tox-\nicity classifier5 fine-tuned from RuBERT Conver-\nsational. This classifier was additionally trained\non Russian Language Toxic Comments dataset col-\nlected from 2ch.hk and Toxic Russian Comments\ndataset collected from ok.ru.\nEnglish Style transfer accuracy (STA) is calcu-\nlated with a style classifier - RoBERTa-based (Liu\net al., 2019) model trained on the union of three\nJigsaw datasets (Jigsaw, 2018). The sentence is\nconsidered toxic when the classifier confidence is\nabove 0.8. The classifier reaches the AUC-ROC of\n0.98 and F1-score of 0.76.\n2.4.4 Joint metric\nAforementioned metrics must be properly com-\nbined to get one Joint metric to evaluate Textual\nStyle Transfer. We follow Krishna et al. (2020) and\ncalculate J as an average of products of sentence-\nlevel fluency, style transfer accuracy, and content\npreservation:\nJ = 1\nn\nnX\ni=1\nSTA(xi) · SIM(xi) · FL(xi) (1)\n2.5 Training\nThere is a variety of versions of large multilingual\nmodels available. In this work we use small and\nbase versions of mT56,7 (Xue et al., 2021) and large\nversion of mBART8 (Liu et al., 2020).\n4https://huggingface.co/roberta-large\n5https://huggingface.co/\nSkolkovoInstitute/russian_toxicity_\nclassifier\n6https://huggingface.co/google/\nmt5-base\n7https://huggingface.co/google/\nmt5-large\n8https://huggingface.co/facebook/\nmbart-large-50-many-to-many-mmt\nSTA↑ SIM↑ FL↑ J↑ STA↑ SIM↑ FL↑ J↑\nRussian English\nBaselines\nDelete 0.532 0.875 0.834 0.364 0.810 0.930 0.640 0.460\ncondBERT (Dale et al., 2021) 0.819 0.778 0.744 0.422 0.980 0.770 0.820 0.620\nMultilingual Setup\nmT5 base 0.772 0.676 0.795 0.430 0.833 0.826 0.830 0.556\nmT5 small 0.745 0.705 0.794 0.428 0.826 0.841 0.763 0.513\nmT5 base∗ 0.773 0.676 0.795 0.430 0.893 0.787 0.942 0.657\nmBART 5000 0.685 0.778 0.841 0.449 0.887 0.889 0.866 0.640\nCross-lingual Setup\nmT5 base ENG 0.838 0.276 0.506 0.115 0.860 0.834 0.833 0.587\nmT5 base RUS 0.676 0.794 0.846 0.454 0.906 0.365 0.696 0.171\nmT5 small ENG 0.805 0.225 0.430 0.077 0.844 0.858 0.826 0.591\nmT5 small RUS 0.559 0.822 0.817 0.363 0.776 0.521 0.535 0.169\nmBART 3000 ENG 0.923 0.395 0.552 0.202 0.842 0.856 0.876 0.617\nmBART 3000 RUS 0.699 0.778 0.858 0.475 0.547 0.778 0.888 0.299\nmBART 5000 ENG 0.900 0.299 0.591 0.160 0.857 0.840 0.873 0.616\nmBART 5000 RUS 0.724 0.746 0.827 0.457 0.806 0.484 0.864 0.242\nBacktranslation Setup\nmBART 5000 (Google) 0.675 0.669 0.634 0.284 0.678 0.762 0.568 0.284\nmBART 5000 (FSMT) 0.737 0.633 0.731 0.348 0.744 0.746 0.893 0.415\nTable 3: Evaluation of TST models. Numbers in bold indicate the best results. ↑ describes the higher the better\nmetric. Results of unsuccessful TST depicted as gray. ENG and RUS depicts the data model have been trained on.\nmT5 base∗ was trained on all English and Russian data available (datasets were not equalized). Last row depicts\nbacktranslation workaround for cross-lingual detoxification. We include only the best result for brevity.\nMultilingual training In multilingual training\nsetup we fine-tune models using both English and\nRussian data. We use Adam (Kingma and Ba,\n2015) optimizer for fine-tuning with different learn-\ning rates ranging from 1 · 10−3 to 5 · 10−5 with\nlinear learning rate scheduling. We also test dif-\nferent number of warmup steps from 0 to 1000.\nWe equalize Russian and English data for train-\ning and use 10000 toxic sentences and their polite\nparaphrases for multilingual training in total. We\ntrain mT5 models for 40 thousand iterations9 with\na batch size of 8. We fine-tune mBART (Liu et al.,\n2020) for 1000, 3000, 5000 and 10000 iterations\nwith batch size of 8.\nCross-lingual training In cross-lingual training\nsetup we fine-tune models using only one dataset,\ne.g.: we fine-tune model on English data and check\nperformance on both English and Russian data.\nFine-tuning procedure was left the same: 40000\niterations for mT5 models and 1000, 3000, 5000\nand 10000 iterations for the mBART.\nBack-translation approach to cross-lingual\nstyle transfer proved to work substantially better\nthan the zero-shot setup discussed above. Neverthe-\nless, both Google and FSMT did not yield scores\n9According to (Xue et al., 2021) mT5 was not fine-tuned\non downstream tasks as the original T5 model. Therefore,\nmodel requires more fine-tuning iterations for Textual Style\nTransfer.\ncomparable to monolingual setup. Besides, surpris-\ningly Google yielded worse results than FSMT.\n3 Results & Discussion\nTable 3 shows the best scores of both multilin-\ngual and cross-lingual experiments. In multilingual\nsetup mBART performs better than baselines and\nmT5 for both English and Russian. Note that the\ntable shows only the best results of the models. It\nis also notable that for mT5 increased training size\nfor English data provides better metrics for English\nwhile keeping metrics for Russian almost the same.\nWe also depict some of the generated detoxified\nsentences in the Table 3 in the part B of Appendix.\nAs for cross-lingual style transfer, results are\nnegative. None of the models have coped with the\ntask of cross-lingual Textual Style Transfer. That\nmeans that models produce the same or almost the\nsame sentences for the language on which they\nwere not fine-tuned so that toxicity is not elimi-\nnated. We provide only some scores here in the\nTable 6 for reference.\nDespite the fact that our hypothesis about the\npossibility of cross-language detoxification was not\nconfirmed, the presence of multilingual models pre-\ntrained in many languages gives every reason to\nbelieve that even with a small amount of parallel\ndata, training models for detoxification is possible.\nA recent work by (Lai et al., 2022) shows that\ncross-lingual formality Textual Style Transfer is\npossible. Lai et al. (2022) achieve this on XFOR-\nMAL dataset (Briakou et al., 2021) by adding\nlanguage-specific adapters in the vanilla mBART\narchitecture (Liu et al., 2020) - two feed-forward\nlayers with residual connection and layer normal-\nization (Bapna and Firat, 2019; Houlsby et al.,\n2019).\nWe follow the original training procedure de-\nscribed by Lai et al. (2022) by training adapters\nfor English and Russian separately on 5 million\nsentences from News Crawl dataset 10. We use\nbatch size of 16 and 200 thousand training iter-\nations. We also then train cross-attentions on our\nparallel detoxifcation data in the same way. How-\never, models tend to duplicate input text without\nany detoxification. Thus, while the exact same\noriginal setup did not work for detoxification, more\nparameter search and optimization could lead to\nmore acceptable results and we consider the ap-\nproach by Lai et al. (2022) as a promising direction\nof a future work on multilingual and cross-lingual\ndetoxification.\n4 Conclusion\nIn this work we have tested the hypothesis that\nmultilingual language models are capable of per-\nforming cross-lingual and multilingual detoxifica-\ntion. In the multilingual setup we experimentally\nshow that reformulating detoxification (Textual\nStyle Transfer) as a NMT task boosts performance\nof the models given enough parallel data for train-\ning. We beat simple (Delete method) and more\nstrong (condBERT) baselines in a number of met-\nrics. Based on our experiments, we can assume that\nit is possible to fine-tune multilingual models in\nany of the 100 languages in which they were origi-\nnally trained. This opens up great opportunities for\ndetoxification in unpopular languages.\nHowever, our hypothesis that multilingual lan-\nguage models are capable of cross-lingual detoxifi-\ncation was proven to be false. We suggest that the\nreason for this is not a lack of data, but the model’s\ninability to capture the pattern between toxic and\nnon-toxic text and transfer it to another language by\nitself. This means that the problem of cross-lingual\ntextual style transfer is still open and needs more\ninvestigation.\n10https://data.statmt.org/news-crawl/\nAcknowledgements\nThis work was supported by MTS-Skoltech labora-\ntory on AI.\nReferences\nAnkur Bapna and Orhan Firat. 2019. Simple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel R.\nTetreault. 2021. Olá, bonjour, salve! XFORMAL: A\nbenchmark for multilingual formality style transfer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3199–3216. Association for Computational Linguis-\ntics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057–7067.\nDavid Dale, Anton V oronov, Daryna Dementieva, Var-\nvara Logacheva, Olga Kozlova, Nikita Semenov, and\nAlexander Panchenko. 2021. Text detoxification us-\ning large pre-trained neural models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7979–7996. Association for\nComputational Linguistics.\nDaryna Dementieva, Daniil Moskovskiy, Varvara Lo-\ngacheva, David Dale, Olga Kozlova, Nikita Semenov,\nand Alexander Panchenko. 2021a. Methods for\ndetoxification of texts for the russian language. Mul-\ntimodal Technol. Interact., 5(9):54.\nDaryna Dementieva, Sergey Ustyantsev, David\nDale, Olga Kozlova, Nikita Semenov, Alexander\nPanchenko, and Varvara Logacheva. 2021b. Crowd-\nsourcing of parallel corpora: the case of style transfer\nfor detoxification. In Proceedings of the 2nd Crowd\nScience Workshop: Trust, Ethics, and Excellence in\nCrowdsourced Data Management at Scale co-located\nwith 47th International Conference on Very Large\nData Bases (VLDB 2021 (https://vldb.org/2021/)) ,\npages 35–49, Copenhagen, Denmark. CEUR Work-\nshop Proceedings.\nAshwin Devaraj, Iain Marshall, Byron Wallace, and\nJunyi Jessy Li. 2021. Paragraph-level simplifica-\ntion of medical texts. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 4972–4984, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nCicero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018a. Fighting offensive language on social\nmedia with unsupervised text style transfer.\nCícero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018b. Fighting offensive language on so-\ncial media with unsupervised text style transfer. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194. Association for Com-\nputational Linguistics.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2020. Language-agnostic\nBERT sentence embedding. CoRR, abs/2007.01852.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nJigsaw. 2018. Toxic comment classification chal-\nlenge. https://www.kaggle.com/c/jigsaw-toxic-\ncomment-classification-challenge. Accessed: 2021-\n03-01.\nDi Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and\nRada Mihalcea. 2020. Deep learning for text style\ntransfer: A survey. CoRR, abs/2011.00416.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 737–762, Online. Asso-\nciation for Computational Linguistics.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2022.\nMultilingual pre-training with language and task\nadaptation for multilingual text style transfer. CoRR,\nabs/2203.08552.\nLeo Laugier, John Pavlopoulos, Jeffrey Sorensen, and\nLucas Dixon. 2021. Civil rephrases of toxic texts\nwith self-supervised transformers. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, EACL 2021, Online, April 19 - 23, 2021 ,\npages 1442–1461. Association for Computational\nLinguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nMingzhe Li, Xiuying Chen, Min Yang, Shen Gao,\nDongyan Zhao, and Rui Yan. 2021. The style-\ncontent duality of attractiveness: Learning to write\neye-catching headlines via disentanglement. In\nThirty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI 2021, Thirty-Third Conference on In-\nnovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2021, Vir-\ntual Event, February 2-9, 2021, pages 13252–13260.\nAAAI Press.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Trans. Assoc.\nComput. Linguistics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nMounica Maddela, Fernando Alva-Manchego, and Wei\nXu. 2021. Controllable text simplification with ex-\nplicit paraphrasing. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pages 3536–3553. Association for Com-\nputational Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Alan W. Black,\nand Ruslan Salakhutdinov. 2018. Style trans-\nfer through multilingual and feedback-based back-\ntranslation. CoRR, abs/1809.06284.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam,\nmay I introduce the GY AFC dataset: Corpus, bench-\nmarks and metrics for formality style transfer. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 129–140, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nAshish Sharma, Inna W. Lin, Adam S. Miner, David C.\nAtkins, and Tim Althoff. 2021. Towards facilitating\nempathic conversations in online mental health sup-\nport: A reinforcement learning approach. In WWW\n’21: The Web Conference 2021, Virtual Event / Ljubl-\njana, Slovenia, April 19-23, 2021 , pages 194–205.\nACM / IW3C2.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S.\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, De-\ncember 4-9, 2017, Long Beach, CA, USA , pages\n6830–6841.\nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-\nYan Liu. 2019. Multilingual neural machine transla-\ntion with knowledge distillation. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nKe Wang, Hang Hua, and Xiaojun Wan. 2019a. Con-\ntrollable unsupervised text attribute transfer via edit-\ning entangled latent representation. In Advances in\nNeural Information Processing Systems 32: Annual\nConference on Neural Information Processing Sys-\ntems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 11034–11044.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nhan Chao. 2019b. Harnessing pre-trained neural\nnetworks with rules for formality style transfer. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 3571–3576.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTrans. Assoc. Comput. Linguistics, 7:625–641.\nJohn Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel,\nand Graham Neubig. 2019. Beyond bleu: Training\nneural machine translation with semantic similarity.\nIn Proceedings of the Association for Computational\nLinguistics.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-50M:\nPushing the limits of paraphrastic sentence embed-\ndings with millions of machine translations. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 451–462, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2019. Conditional bert contextual\naugmentation. In Computational Science – ICCS\n2019, pages 84–95, Cham. Springer International\nPublishing.\nHaoran Xu, Sixing Lu, Zhongkai Sun, Chengyuan Ma,\nand Chenlei Guo. 2021. V AE based text style trans-\nfer with pivot words enhancement learning. CoRR,\nabs/2112.03154.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 483–498. Association\nfor Computational Linguistics.\nZonghai Yao and Hong Yu. 2021. Improving formality\nstyle transfer with context-aware rule injection. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 1561–\n1570. Association for Computational Linguistics.\nYi Zhang, Tao Ge, and Xu Sun. 2020. Parallel data aug-\nmentation for formality style transfer. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 3221–3228. Association for\nComputational Linguistics.\nA Data\nA.1 English Dataset\nTable 4 shows examples of sentence pairs from ParaDatex parallel detoxification corpora. There are\nseveral polite paraphrases for each toxic sentence in this dataset (Dementieva et al., 2021b), this is a\nconsequence of the way these parallel data are collected. Leaving only one paraphrase for one source\nsentence we could get 6000 unique pairs of toxic sentences and their polite paraphrases. However, in this\nwork we use data as is.\nOriginal my computer is broken and my phone too!! wtf is this devil sh*t???\nDetoxed My computer is broken and my phone too! So disappointed!\nMy computer is broken and my phone too, what is this?\nBoth my computer and phone are broken.\nOriginal sh*t is crazy around here\nDetoxed It is crazy around here.\nStuff is crazy around here.\nSomething is crazy around here.\nOriginal delete the page and shut up\nDetoxed Delete the page and stay silent.\nPlease delete the page.\nDelete the page.\nOriginal massive and sustained public pressure is the only way to get these b*stards to act.\nDetoxed Massive and sustained public pressure is the only way to get them to act.\nMassive and sustained pressure is the only way to get these people to act.\nOriginal f*ck you taking credit for some sh*t i wanted to do\nDetoxed You are taking credit for something I wanted to do\nYou’re taking credit fro something i wanted to do.\nOriginal you gotta admit that was f*ckin hilarious though!\nDetoxed you got to admit that was very hilarious though!\nyou gotta admit that was hilarious though!\nTable 4: Example sentences from ParaDetox parallel detoxification corpora. Sentence in red is original (toxic)\nsentence, below are its polite paraphrases. Note that for the purpose of an overall correctness explicit words are\nmasked with \"*\".\nA.2 Russian Dataset\nTable 5 shows examples from Russian parallel detoxification corpus.\nOriginal Х*рню всякую пишут,из-за этого лайка.v Долбо**изм.\nTranslation They write all sorts of bullshit, because of this like. Stupidity.\nDetoxed Чушь всякую пишут, из- за этого лайка.\nTranslation They write all sorts of nonsense, because of this like.\nOriginal А нахрена тогда ты здесь это писал?\nTranslation Why the f*ck did you post it here?\nDetoxed Зачем ты это писал?\nTranslation Why did you post it?\nOriginal Е*анутые. Отобрать оружие и лодку и штраф тысяч 500\nTranslation F*ckers. Take away weapons, boat and give a fine of 500 thousand.\nDetoxed Отобрать оружие и лодку и штраф тысяч 500.\nTranslation Take away weapons, boat and give a fine of 500 thousand.\nOriginal Не поверишь, сколько е**нутых на планете.\nTranslation You won’t believe how many f*cked up people are on the planet.\nDetoxed Не поверишь сколько таких на планете.\nTranslation You won’t believe how many people like that are there on the planet.\nTable 5: Example sentences from Russian parallel detoxification corpora. Sentence in red is original (toxic) sentence,\nbelow are its polite paraphrases.\nB Generation Examples\nTable 6 contains detoxification examples for different models. It is notable that in some cases models\ngenerate almost the same results. This can be explained by the similarity of the training procedure and the\nfact that the reference answer was the same.\nOriginal After all its hard to get a job if you’re stupid.\nmBART 5000 ENG After all its hard to get a job if you don’t understand.\nmBART 3000 ENG After all its hard to get a job if you are not smart.\nmT5 base (multilingual) It’s hard to get a job if you don’t know.\nOriginal Fuck minimum security, put him in real prison.\nmBART 5000 ENG Put him in a real prison with minimum security.\nmBART 3000 ENG I don’t care about minimum security, put him in real prison.\nmT5 base I don’t care about minimum security, put him in real prison.\nOriginal Sure he should get punished ,but three f*cking years?\nmBART 5000 ENG Sure he should get punished, but three years?\nmBART 3000 ENG He should get punished, but three years?\nmT5 base He’d get punished, but three years?\nParaphrase Что у этих людей в башке!? походу насрато!\nTranslation What’s going on in these people’s heads!? It looks like shit!\nmBART 5000 ENG Что у этих людей в голове!? Походу бесчеловечно.\nmBART 3000 ENG Что у этих людей в голове? Походу ненормально!\nmT5 base походу этих людей!? походу!\nTable 6: Some detoxified sentences produced by our fine-tuned models. Gray text refers to the original sentence,\nbelow are its paraphrases.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7877941131591797
    },
    {
      "name": "Fluency",
      "score": 0.764799952507019
    },
    {
      "name": "Detoxification (alternative medicine)",
      "score": 0.7476904392242432
    },
    {
      "name": "Natural language processing",
      "score": 0.5610395669937134
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5050674080848694
    },
    {
      "name": "Style (visual arts)",
      "score": 0.45584434270858765
    },
    {
      "name": "Politeness",
      "score": 0.44325751066207886
    },
    {
      "name": "Language model",
      "score": 0.43652868270874023
    },
    {
      "name": "Task (project management)",
      "score": 0.42298731207847595
    },
    {
      "name": "Linguistics",
      "score": 0.25180381536483765
    },
    {
      "name": "Engineering",
      "score": 0.06598997116088867
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125989756",
      "name": "Skolkovo Institute of Science and Technology",
      "country": "RU"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ]
}