{
  "title": "Transformer Acceleration with Dynamic Sparse Attention",
  "url": "https://openalex.org/W3205531882",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2109010745",
      "name": "Liu Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2315006627",
      "name": "Qu Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2367479171",
      "name": "Chen Zhaodong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2356621939",
      "name": "Ding, Yufei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136276677",
      "name": "Xie Yuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2884071170",
    "https://openalex.org/W2178628967",
    "https://openalex.org/W3035839559",
    "https://openalex.org/W2979439447",
    "https://openalex.org/W2963367920",
    "https://openalex.org/W3134226059",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3183058688",
    "https://openalex.org/W3037132819",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3129415623",
    "https://openalex.org/W3159185005",
    "https://openalex.org/W2883542588"
  ],
  "abstract": "Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution.",
  "full_text": "TRANSFORMER ACCELERATION WITH DYNAMIC SPARSE ATTENTION\nLiu Liu * 1 2 Zheng Qu * 2 Zhaodong Chen 2 Yufei Ding1 Yuan Xie2\nABSTRACT\nTransformers are the mainstream of NLP applications and are becoming increasingly popular in other domains\nsuch as Computer Vision. Despite the improvements in model quality, the enormous computation costs make\nTransformers difﬁcult at deployment, especially when the sequence length is large in emerging applications.\nProcessing attention mechanism as the essential component of Transformer is the bottleneck of execution due\nto the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling,\nbut those pieces of work are on static or ﬁxed patterns. We demonstrate that the sparse patterns are dynamic,\ndepending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efﬁciently exploit\nthe dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve\nbetter trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide\nsolutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical\nspeedup and efﬁciency improvements for Transformer execution.\n1 I NTRODUCTION\nTransformers (Vaswani et al., 2017) have become the driving\nforce for sequence modeling tasks such as neural machine\ntranslation (Ott et al., 2018), language understanding (De-\nvlin et al., 2019), and generative modeling (Parmar et al.,\n2018; Brown et al., 2020). Equipped with the self-attention\nmechanism, Transformers are capable of handling long-\nrange dependencies.\nDespite the impressive progress made by Transformers,\nthe computational requirements make the deployment of\nTransformer-based models difﬁcult at inference time, espe-\ncially when processing long sequences. The quadratically\nscaled self-attention modules are the execution bottleneck\nunder long sequences. Therefore, many studies propose\nTransformer variants to mitigate the quadratic time and\nspace complexity issue. Some approaches are primary for\nmemory footprint reduction during training while efﬁcient\ninference is being understudied (Gong et al., 2019; Dai\net al., 2019; Kitaev et al., 2020; Roy et al., 2021). Other\nmethods use ﬁxed or static sparse attention patterns to save\ncomputations (Child et al., 2019; Qiu et al., 2020; Beltagy\net al., 2020; Zaheer et al., 2020). However, we ﬁnd that\nintrinsic sparse patterns in attention are naturally dynamic,\ndepending on input sequences. Thus, we propose to exploit\nthe dynamic sparse patterns to save attention computations\n*Equal contribution 1Department of Computer Science, Univer-\nsity of California, Santa Barbara, USA 2Department of Electrical\nand Computer Engineering, University of California, Santa Bar-\nbara, USA. Correspondence to: Liu Liu <liu liu@ucsb.edu>.\nPreprint. Work in progress.\nwithout sacriﬁcing the representation power of attention. In-\ntuitively, posing static sparsity constraints in attention could\nbe too strong to capture dynamic attention connections.\nWe propose the Dynamic Sparse Attention (DSA) approach\nthat exploits dynamic sparsity to improve efﬁciency. The\nchallenge is to efﬁciently search for sparse patterns close to\noracle sparse patterns that keep all the important attention\nweights. We formulate the searching as a prediction prob-\nlem and augment the standard attention mechanism with a\nprediction path. As discussed in Section 3, we ﬁrst obtain an\napproximation of attention scores with low computational\ncosts. Then, we predict the sparse attention patterns using\nthe approximate attention scores. With the predicted sparse\nattention patterns represented as binary masks, we can save\ncomputations involved in full attention scores, softmax,\nand attention outputs.\nCompared with static sparse attention methods, our method\nis dynamic and naturally captures sparse attention patterns\nof different input sequences. We observe important tokens\nthat attract a large portion of attention weights from other\ntokens, similar to the global attention method (Beltagy et al.,\n2020; Zaheer et al., 2020). However, the positions of global\ntokens are input-dependent, and our method can effectively\nidentify such varieties, instead of relying on domain knowl-\nedge to predetermine certain global tokens in ﬁxed positions.\nCompared with other low-rank approximation methods, the\napproximation in DSA is only for sparsity prediction with-\nout strict and static constraints on attention positions. There-\nfore, our method can maintain the representation power of\nfull attention while reducing unnecessary attention weights.\narXiv:2110.11299v1  [cs.LG]  21 Oct 2021\nTransformer Acceleration with Dynamic Sparse Attention\nAlthough DSA can save theoretical computations and main-\ntain attention capability, achieving practical speedups and\nenergy savings on real hardware systems is challenging. We\ndiscuss the implications of DSA on existing GPU architec-\ntures and specialized hardware accelerators. We extend the\nﬁne-grained dynamic sparsity as searched by DSA to struc-\ntural dynamic patterns, such as block-wise and vector-wise.\nWe give the study on structural sparse patterns vs. atten-\ntion’s expressive power and explore the opportunities for\ndataﬂow optimization and data reuse from dynamic sparsity.\nOur evaluation in Section 4 shows that DSA can achieve\n95% sparsity in attention weights without compromising\nmodel accuracy. Under this setting, the overall computa-\ntional saving is up to 4.35×compared with full attention,\nwhile the sparsity prediction only introduces around 1.17%\nto 1.33% computational overhead. Experimental results\non NVIDIA V100 GPU show that, under 90% sparsity ra-\ntio, applying vector-wise sparsity on DSA delivers 1.15×\nspeedup on attention score computation, 14.6×speedup\non softmax computation, and 1.94×speedup on attention\noutput computation, with only 0.1% of accuracy loss. Fi-\nnally, through hardware specialization, we can further ex-\nplore token-level parallelism and computation reordering\nfor DSA. Our characterization results show that this can\nreduce the total memory access of attention computation by\nup to 2.54×.\n2 B ACKGROUND AND MOTIVATION\nBefore we describe our method in detail, we ﬁrst introduce\nthe preliminaries of the standard attention mechanism used\nin vanilla Transformers. Then, we discuss the challenge\nof serving long sequences under the quadratic complexity\nof attention. Finally, we demonstrate that redundancy ex-\nists in attentions and dynamic sparse patterns are naturally\nexpressed in attention.\n2.1 Preliminaries of Attention\nThe attention mechanism is the essential component of\nTransformers (Vaswani et al., 2017). Self-attention operates\non input representations of length l, X ∈Rl×d, with three\nlinear projections namely, query, key, and value as\nQ,K,V = XWQ,XWK,XWV (1)\n, where Q∈Rl×dk denotes the queries,K ∈Rl×dk denotes\nthe keys, and V ∈Rl×dv denotes the values. After linear\nprojections, the attention weights A∈Rl×l is deﬁned as\nA= φ(QK⊤\n√dk\n) (2)\nwhere φis the row-wise softmax(·) function. Finally, the\noutput values are computed by multiplying the attention\nweights Awith the projected values V as\nZ = AV. (3)\nServing Transformer-based models is challenging when the\ninput sequence length lis large. When using long sequences,\ncomputing Eq. (2) and Eq. (3) consumes the majority of\noperations and becomes the bottleneck of model evaluation.\nThe asymptotic complexity of attention O(l2dk + l2dv) is\nquadratic to sequence length l.\n2.2 Intrinsic Sparsity in Attention Weights\nA number of efﬁcient Transformer variants have been pro-\nposed to mitigate the quadratic complexity of self-attention\n(Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020;\nShi et al., 2021). One straightforward way to exploit the\nintrinsic redundancy in attention is forming sparse patterns\nas in\nA= φ(QK⊤−c(1 −M)), (4)\nwhere M ∈{0,1}l×l represents the sparse attention pat-\ntern, cis a large constant (1e4) such that where Mij = 0,\nindicating unimportant attention, Aij = 0 after softmax\nnormalization. Here, we omit √dk for simplicity. The\nsparse patterns can be pre-determined into global, block,\nrandom, or a combination of different patterns. Another\nway to determine sparse patterns is through trainable masks.\nHowever, all these methods explore static or ﬁxed sparse\npatterns, restricting viable attention connections.\n2.3 Dynamic Sparse Patterns in Attention\nA common motivation of sparse attention methods is that\nnot all attention weights, i.e., probabilities, are equally im-\nportant in Eq. (3). A large portion of attention weights\ndo not contribute to attention output and are redundant. In\nother words, only a small portion of attention weights are\nuseful. However, we ﬁnd that sparse patterns in attention\nare inherently dynamic and data-dependent.\nHere, we further support our hypothesis by showing the orig-\ninal attention weights matrix (aftersoftmaxnormalization)\nin Figure 1. The model used here is a vanilla Transformer\nand the benchmark is Text Classiﬁcation from Google Long-\nRange Arena(Tay et al., 2020b). Figure 1 indicates that only\na small amount of attention weights are with large mag-\nnitude and a signiﬁcant portion is near zero. We want to\nemphasize that this shows the raw attention weights without\nforcing any sparsity constraints or ﬁne-tuning, which indi-\ncates that redundancy naturally exists in attention. In short,\nattention mechanism exhibits the focused positions on a set\nof important tokens.\nMore importantly, the attention weights have dynamic\nsparse patterns. As shown in Figure 1, the sparse patterns\nTransformer Acceleration with Dynamic Sparse Attention\nInput0\nHead0\n Head1\n Head2\n Head3\nInput1\nInput2\nInput3\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nFigure 1.Visualization of attention weights from different inputs\nand attention heads. Only a small amount of attention weights are\nimportant. Note values >0.005 are clamped to show as 0.005.\nin attention weights are dynamically changing depending\non the input sequence. Different heads in multi-head atten-\ntion also have different sparse patterns. The characteristic\nof dynamic sparsity in attention weights motivates us to\nexplore effective methods to eliminate the redundancy and\nsave computations. Prior work on static or ﬁxed sparse\npatterns cannot capture the dynamically changing attention\nweights.\n3 D YNAMIC SPARSE ATTENTION\nFrom Section 2, we show that attention weights have in-\ntrinsic sparse patterns, and the positions of important atten-\ntion weights are dynamically changing as different input\nsequences. While attention exhibits dynamic sparse patterns,\nhow to efﬁciently and effectively obtain the dynamic sparse\npatterns remains challenging. We formulate the process of\nidentifying sparse attention patterns as a prediction problem.\nThe key challenge is how to obtain an approximate attention\npredictor that can accurately ﬁnd the sparse patterns while\nkeeping the prediction overhead small.\nHere, we present Dynamic Sparse Attention(DSA) that ex-\nploits sparsity in attention weights to reduce computations.\nThe principle of our method is to effectively search for dy-\nnamic sparse patterns without enforcing strict and static\nconstraints on attention while keeping the searching cost\nsmall. Our approach leverages trainable approximation to\npredict sparse attention patterns. As shown in Figure 2, we\nuse a prediction path based on low-rank transformation and\nlow-precision computation. The prediction path processes\ninput sequences functionally similar to query and key trans-\nformations but at much lower computational costs. Given\nthe prediction results that approximate QK⊤well, we can\nsearch sparse patterns based on the magnitude of prediction\nresults.\nWQ\nWK\nWV\nX\nGEMM\nGEMM\nSoftMax\nQ\nK\nV\nA\nS\nZ\n(a) \nDense Computation\nWQ\nWK\nWV\nX\nSDDMM\nSpMM\nSparse\nSoftMax\nQ\nK\nV\nM\nZ\n(b)\nWQ\nWK\nP\nSparse Computation\nӗ\nӗ\nGEMM\nQӗ\nKӗ\nPrediction\nAѲ\nFigure 2.(a) Standard full attention; (b) Dynamic sparse attention\nwith approximation-based prediction and sparse computation.\n3.1 Design of Prediction Path\nWe denote attention scores as S = QK⊤ and omit the\nscaling factor for simplicity. As shown in Figure 2(a), two\ngeneral matrix-matrix multiplication kernels (GEMM) and\none softmax kernel consume the majority of computations\nin self-attention. We construct a pair of approximate query\nand key transformations in the prediction path to compute\nfor approximate score ˜S, as in\n˜Q, ˜K = XP ˜WQ,XP ˜WK. (5)\nHere P ∈\n√\n3\nk ·{−1,0,1}d×k is a sparse random projection\nmatrix shared by both paths, and ˜WQ ∈ Rk×k, ˜WK ∈\nRk×k are parameters in approximating query and key.\nThen, we have approximate attention scores ˜S = ˜Q˜K⊤.\nFrom ˜S, we can predict sparse attention masks M using\nthresholds, where the threshold values are either ﬁxed by\ntuning from the validation set or determined by top−k\nsearching. When ˜S is well approximated with accurate\nattention scores S, the large scores in ˜Sare also large in S\nwith high probability. The resulting sparse attention weights\n¯Ais used to multiply the value matrix V similar to Eq. 3.\nOptimization of Approximation. The random projection\nmatrix P is constant after initialization and shared by two ap-\nproximate transformations. We obtain the trainable parame-\nters, ˜WQ and ˜WK, through minimizing the mean squared\nerror (MSE) as the criterion to optimize for approximation:\nLMSE = 1\nB||S−˜S||2\n2 = 1\nB||QK⊤−˜Q˜K⊤||2\n2 (6)\nwhere B is the mini-batch size.\nGiven the motivation of ﬁnding dynamic sparse patterns, the\nhypothesis of our method is that there exist oracle sparse\npatterns that perform well. Such that the optimization target\nTransformer Acceleration with Dynamic Sparse Attention\nis to approximate full attention scores Swell enough to pre-\ndict sparse patterns. We further give the results of applying\noracle sparse patterns by directly dropping small-magnitude\nattention weights during inference without ﬁne-tuning the\nmodel. As listed in Table 1, around 90% (up to 97%) of\nsmall attention weights can be dropped with negligible ac-\ncuracy loss.\nTable 1.Sparsity in attention weights, where values < θare set\nto zero. A signiﬁcant portion of attention weights that have small\nmagnitude are redundant. The accuracy metrics are Exact Match\n(EM) and F1 Score.\nCase Sparsity EM F1\nBase 0% 81.49 88.70\nθ= 0.001 75% - 95% 81.50 88.70\nθ= 0.01 94% - 97% 80.51 87.85\n3.2 Model Adaptation\nWhen sparse attention scores are masked out to generate\nsparsity in attention, the remaining attention weights, i.e.,\nthe important weights, are scaled up as the denominator\nbecomes small. Leaving the disturbed attention weights\nintact will degrade model quality. As a countermeasure,\nwe propose to ﬁne-tune model parameters with dynamic\nsparse constraints, referred to as model adaptation. With\nadaptation, the model evaluation accuracy can recover to be\non par with full attention baselines, while the computational\ncosts are signiﬁcantly reduced.\nWe do not change the computational graph and the loss func-\ntion of the original model, except adding dynamic sparse\nconstraints in attention as mask M. As a result, the new\nattention ¯A are sparse and only have important weights\nfrom prediction. Given a pre-trained model, our method\njointly ﬁne-tunes the model parameters and parameters of\nthe prediction path as in\nL= LModel + λLMSE (7)\nwhere λis the regularization factor of MSE. Our method can\nalso train from scratch with initialized model parameters.\nOur method approximates the original attention score with\na low-rank matrix ˜S. When training the model with loss\nfunction in Eq. 6, the gradient from LMSE will be passed\nto both the low-rank approximation ˜Sand the original atten-\ntion score S. Intuitively, this loss function not only makes\n˜Sa better approximation of S, but also makes Seasier to\nbe approximated by a low-rank matrix, i.e., by reducing the\nrank of S. On the other hand, the loss LModel guarantees\nthe rank of Sto be high enough to preserve the model accu-\nracy. In other words, the joint optimization of LModel and\nLMSE implicitly learns a low-rank Swith a learnable rank\ndepending on the difﬁculty of the task. Our design brings\ntwo advantages. First, the rank of Swill be automatically\nadjusted to tasks with different difﬁculty levels. Hence, our\nmethod can potentially achieve higher accuracy on difﬁcult\ntasks and higher speedup on simple tasks compared with\nlow-rank approximation methods using ﬁxed rank. Second,\nas the rank of ˜Sonly implicitly inﬂuences the rank of S, the\nﬁnal result is less sensitive to the hyper-parameter k.\n3.3 Computation Saving Analysis\nDSA introduces additional computations in the prediction\nstep, but the overall computation saving from sparse atten-\ntion kernels is fruitful and can have practical speedup. The\noriginal full attention takesO(l2dk +l2dv) MACs (multiply-\nand-accumulate operations) asymptotically. However, the\nasymptotic analysis does not consider practical concerns\nsuch as sparsity, quantization, and data reuse. Here, we\naugment the traditional asymptotic analysis with a sparsity\nfactor αand a quantization factor β. In this way, DST pre-\ndiction takes O(βldkk+ βl2k) MACs; DST attention takes\nO(αl2dk + αl2dv) MACs. Both αand β are determined\ndepending on tasks and underlying hardware platforms. In\nour settings, we choose αbetween 90% and 98% and our\nGPU kernels can achieve practical speedups. We assume\nthe baseline model uses FP32 as the compute precision and\nset prediction precision to be INT4. The execution time on\nsoftmax is not revealed in asymptotic analysis but is one\nof the major time-consuming components. Our method can\nalso save the time of softmax kernel with the same sparse\nattention patterns.\n3.4 Implications for Efﬁcient Deployment\nCompared with standard attention, DSA exhibits two new\nfeatures that can potentially affect model deployment.\nFirstly, a light-weight prediction path is attached to the\nattention layer to search for dynamic sparse patterns. The\nprediction involves approximation of attention scores, which\nis essentially a low-precision matrix-matrix multiplication\n(GEMM). While NVIDIA GPUs with Tensor Cores sup-\nport data precision as low as INT8 and INT4, DSA predic-\ntion can tolerate INT2 computation on certain benchmarks.\nTherefore, specialized hardware is preferable when seeking\nultra-efﬁcient attention estimation. In Section 5, we intro-\nduce two types of architectures to support multi-precision\ncomputations.\nSecondly, the predicted sparse patterns can be used to re-\nduce unnecessary attention computations. In other words,\ninstead of computing QK⊤and AV as two dense GEMM\noperations, we can reformulate QK⊤as a sampled dense\ndense matrix multiplication (SDDMM) and AV as a sparse\nmatrix-matrix multiplication (SpMM). When processing\nSDDMM and SpMM kernels on GPU, data reuse is the\nkey disadvantage that limits its performance compared with\nGEMM. Therefore, we extend DSA to support structural\nsparsity that can improve the data reuse of both SDDMM\nand SpMM kernels. We implement customized kernels that\nTransformer Acceleration with Dynamic Sparse Attention\ntake advantage of the sparsity locality to improve kernel per-\nformance, achieving practical runtime speedup on NVIDIA\nV100 GPU. Also, we demonstrate our choice of structural\nsparsity pattern and that DSA is able to maintain the model\nexpressive power with the extra constraints.\nAs for specialized hardware, the advantage of DSA can be\nfully exploited as the specialized architecture and dataﬂow\nis able to deal with ﬁne-grained sparsity, therefore achieving\noptimal sparsity ratio and computation reduction. How-\never, the challenge also arises as irregular sparsity causes\nload imbalance and under-utilization of processing elements.\nMoreover, instead of independently executing SDDMM and\nthen SpMM, we point out that more optimization opportu-\nnities can be explored when considering the whole process\nas a two-step SDDMM-SpMM chain. Please refer to Sec-\ntion 5 for more architectural design details and experimental\nresults.\n4 E VALUATION\nIn this section, we evaluate the performance of DSA over\nrepresentative benchmarks from Long-Range Arena (Tay\net al., 2020b). We ﬁrst compare the model accuracy of DSA\nwith dense vanilla transformers and other efﬁcient trans-\nformer models. Then, we present a sensitivity study over\ndifferent conﬁgurations of the prediction path. By choos-\ning different number of prediction parameters, DSA is able\nto achieve ﬂexible trade-offs between computational cost\nand model accuracy. Finally, we study the model efﬁciency\nof DSA by analyzing the computational cost (MACs) and\nrelative energy consumption.\n4.1 Experiment Settings\nThe datasets used are from Long-Range Arena (LRA),\nwhich is a benchmark suite for evaluating model quality\nunder long-sequence scenarios. In LRA, different trans-\nformer models are implemented using Jax (Bradbury et al.,\n2018) API and optimized with just-in-time (jax.jit) compi-\nlation. We implement DSA on top of the vanilla transformer\nprovided by LRA and compare it with other models included\nin LRA. Speciﬁcally, the self-attention layer in the vanilla\ntransformer is augmented by the DSA method as described\nin Section 3. All the other model conﬁgurations are kept the\nsame for a fair comparison.\nWe incorporate three tasks from the LRA benchmark in\nour experiment, including Text Classiﬁcation, Document\nRetrieval, and Image Classiﬁcation. The Long ListOps\nand Pathﬁnder tasks are excluded. We provide benchmark\ndescriptions and experiment conﬁgurations in Appendix A.\nDense DSA-90% DSA-95% DSA-97% DSA-99%\n40\n45\n50\n55\n60\n65Accuracy (%)\n65.12 65.63 65.06 64.87 64.06\n62.50 63.07 63.19 62.10 61.54\n42.74 43.82 43.68 42.44\n39.51\nT ext\nRetrieval\nImage\nFigure 3.Overall model accuracy of DSA (ﬁne-tuned from a pre-\ntrained checkpoint) compared with vanilla dense transformer.\n4.2 Accuracy\nFigure 3 presents the overall model accuracy of DSA on\ndifferent LRA tasks. In this experiment, the DSA model is\nﬁne-tuned from a pretrained vanilla transformer by jointly\nupdating the model parameters and prediction parameters\nusing the combined loss of LMSE and LModel . Different\npercentage numbers indicate the sparsity ratio that we ap-\nplied to the DSA models. For instance, DSA-90% means\nthat we only keep 10% of the attention weights in each row\nof the attention matrix, while masking out all the other 90%\nof the weights. The sparsity ratio constraint is uniform for\nall the heads and attention layers in the DSA model.\nAs shown in Figure 3, for all the evaluated tasks, dense\ntransformer possesses a considerable amount of redundancy\nin the attention matrix under the long-sequence condition,\nwhich supports our previous claim in Section 2. Speciﬁcally,\nwe can safely mask out up to 95% of the attention weights\nwithout suffering from any accuracy degradation. In fact, by\njointly optimizing the model parameters to adapt dynamic\nsparse attention, DSA delivers slightly higher performance\nwith 90% and 95% sparsity ratio. Even with up to 99%\nof sparsity, DSA still demonstrates promising performance\nwith negligible accuracy drop compared with the dense\nbaseline.\nTo fairly compare with other transformer variants provided\nby LRA, we follow several training constraints during our\nexperiment. For example, instead of ﬁne-tuning from a\npretrained baseline, the DSA model used in the compar-\nison is obtained from a randomly initialized model, i.e.,\ntraining from scratch. We also ﬁx other model parameters\n(e.g., number of layers, number of heads, hidden dimension)\nand training conﬁgurations (e.g., total training steps). The\nresults are shown in Table 2. We use DSA-90% with quan-\ntization precision to be INT4, and let the random projection\ndimension scale σ=k/d=0.25. As we can see from the table,\nDSA achieves ﬁrst-tier performance in all three tasks and\ndelivers a leading average score on the LRA benchmarks.\nThis encouraging performance mainly comes from two as-\npects. Firstly, joint optimization ensures that the DSA model\nTransformer Acceleration with Dynamic Sparse Attention\nTable 2.Accuracy of different Transformer models on the LRA\nbenchmark suite (Tay et al., 2020b). For a fair comparison, we\nfollow the instructions in LRA and train our model from scratch.\nDSA-90% uses projection scale σ= 0.25 and INT4 quantization.\nModel Text Retrieval Image Avg\nTransformer 65.12 62.5 42.74 56.79\nLocal Attention 52.98 53.39 41.46 50.89\nSparse Trans. 63.58 59.59 44.24 55.80\nLongformer 62.85 56.89 42.22 53.99\nLinformer 53.94 52.27 38.56 48.26\nReformer 56.10 53.40 38.07 49.19\nSinkhorn Trans. 61.20 53.83 41.23 52.09\nSynthesizer 61.68 54.67 41.61 52.65\nBigBird 64.02 59.29 40.83 54.71\nLinear Trans. 65.90 53.09 42.34 53.78\nPerformer 65.40 53.82 42.77 54.00\nDSA-90% 65.62 63.07 43.75 57.48\nInput Seq1\n Input Seq2\n Input Seq3\n Input Seq4\nFigure 4.Oracle attention mask generated by top-k selection.\nInput Seq1\n Input Seq2\n Input Seq3\n Input Seq4\nFigure 5.Sparse attention mask generated by DSA prediction.\ncan well adapt to the sparse attention patterns for computing\nthe attention output. Secondly, the trainable prediction path\nis able to accurately capture the input-dependent patterns.\nFigure 4 shows the oracle sparse patterns of four differ-\nent input sequences obtained from top-k selection over the\noriginal full attention matrix. The yellow dots indicate that\nthe important positions in the attention matrix, while the\npurple region is masked out. Figure 5 shows the sparsity\npatterns generated by DSA prediction. As we can see from\nthe two ﬁgures, horizontally, the sparse attention pattern\nchanges with different input sequences. Vertically, the pre-\ndicted patterns are very close to the oracle patterns. In our\nexperiments, the prediction accuracy is around 85 ∼95%.\nTo make sure the high performance of DSA comes from\nthe proposed approach rather than the task itself, we further\ntest two cases on the Text Classiﬁcation dataset. Firstly, we\napply a 99% sparsity constraint on the vanilla transformer,\nbut with a static local attention pattern. Secondly, we use a\nshort sequence with dense attention, and let the total number\nof tokens in the short sequence matches with the number of\nimportant tokens in the long-sequence scenario. The results\nshow that these two cases perform very poorly on the task,\ndelivering a model accuracy of only 53.24% and 54.16%\ncompared with 64.04% accuracy achieved by DSA-99%.\nThis further supports our previous discussion.\n4.3 Design Space Exploration of Prediction Path\nOne of the most important design choices of DSA is the\nconﬁguration of the Prediction Path. Overall, we want\nthe predictor to accurately capture dynamic sparse patterns.\nHowever, we also want to minimize the cost of prediction\nwhile maintaining DSA model accuracy. Thus, while we in-\nvolve trainable parameters for prediction, we also introduce\nrandom projection matrix P ∈ {−1,0,1}d×k to control\nthe prediction parameters ( ˜WQ ∈Rk×k, ˜WK ∈Rk×k), and\nuse low-precision to reduce the computation overhead. Here,\nwe present the sensitivity results regarding different choices\nof the reduced dimension size and quantization precision.\nWe ﬁrst sweep over different sizes of k and evaluate the\naccuracy of DSA-90% on the LRA Text Classiﬁcation task.\nHere, we use σ = k/d ∈(0,1] to represent the size of\nthe predictor. A Larger σvalue indicates more prediction\nparameters and better representation power, but also larger\ncomputation overhead. As we can see from Table 3, DSA\ndemonstrates relatively stable performance with different\nσ values. Even with σ = 0.1, DSA-90% still achieves a\nslightly higher accuracy compared with vanilla transformer.\nWe believe this is because we use predictor to indicate the\npositions of the important attention weights, while passing\nthe accurate attention weights to the output. Therefore, our\npredictor module can tolerate highly approximate computa-\ntion as long as it can capture the relative importance in the\nattention matrix.\nTo further study the performance and the impact of the\npredictor, we conduct another experiment to sweep over\ndifferent quantization precision, while ﬁxing σto be 0.25.\nAs shown in Table 3, DSA-90% achieves good accuracy\nwith quantized precision as low as 4-bit. Accuracy degra-\ndation occurs when the precision further scales down to\n2-bit. As we go deeper into the predictor module, we collect\nand show the prediction accuracy in each attention block of\nthis 4-layer DSA model. The prediction accuracy is deﬁned\nby the percentage of the correct guesses among the total\nnumber of predictions. For example, for aDSA-90% model\nworking on a sequence length of 2000, for each row of the\nattention matrix, the predictor will output 200 positions to\nbe important. If 100 of these 200 locations actually matches\nwith the top-kresults, the prediction accuracy is 50%. As\nshown in Figure 6, the predictor is able to maintain its pre-\ndiction accuracy even with 4-bit quantization. When the\nprecision is 2-bit, the prediction accuracy suffers a signiﬁ-\ncant degradation, dropping from 60 ∼90% to 25 ∼55%.\nTransformer Acceleration with Dynamic Sparse Attention\nTable 3.Change of DSA-90% model accuracy when sweeping random projection scale σand quantization precision.\nσ 0.1 0.16 0.2 0.25 0.33 0.4 Baseline\nDSA-90% 65.32 65.25 65.17 65.46 65.63 65.54 65.12\nQuantization Random INT2 INT4 INT8 INT16 FP32 Baseline\nDSA-90% 60.42 64.23 65.56 65.69 65.63 65.63 65.12\nLayer-0 Layer-1 Layer-2 Layer-3\n0\n20\n40\n60\n80Prediction Accuracy (%)\nRandom\nINT2\nINT4\nINT8\nINT16\nFP32\nFigure 6.The prediction accuracy of DSA in a 4-layer DSA-90%\nmodel with different quantization precision.\nDespite this, the overall model accuracy is acceptable, with\nonly 0.89% degradation compared with the baseline trans-\nformer. We believe this is because, for the binary Text\nClassiﬁcation task, it is more crucial to capture the very few\nmost important attentions. Although the prediction accuracy\nbecomes lower, the most important positions are preserved\nand therefore maintaining the overall model accuracy. Fi-\nnally, in Figure 6 and Table 3 we include a special case\nof randomly selecting 10% important positions. With this\nrandom mask applied to the model, the prediction accuracy\nis less than 10%, and overall model accuracy directly drops\nto 60.42%. This result supports our previous analysis.\n4.4 Model Efﬁciency\nAs we mentioned earlier, DSA has the potential to signif-\nicantly reduce computation and memory consumption of\nthe self-attention layer, which is especially beneﬁcial for\ndeploying a long sequence transformer model at inference\ntime. While we acknowledge that the actual runtime perfor-\nmance and memory footprint are largely depending on the\nunderlying hardware implementation, in this subsection we\nshed light on this problem by quantitatively analyzing the\ncost of DSA.\nWe start with presenting the number of required MAC op-\nerations for each attention layer. We use MAC number\nas the computational cost metric because the majority of\nthe operations in the self-attention layer are matrix multi-\nplications. We break down the total MAC operations into\nthree parts: (1) Linear: General Matrix-matrix Multiplica-\ntion(GEMM) for computing Query, Key, and Value. (2)\nAttention: GEMM for computing attention weight matrix\nand output Value. (3) Other: Other GEMMs inside the at-\ntention block like Feed-Forward layers. As we introduced\nearlier, the two GEMM operations in the part (2) scale\nquadratically with the sequence length, and we transform\nthem to be SDDMM and SpMM in our DSA model to re-\nduce both computation and memory consumption. Based on\nthis setting, the computational cost breakdown of different\nmodels used in our LRA experiment is shown in Figure 7.\nComparing different tasks, the tasks with longer sequence\nlength (Text and Retrieval) are more bounded by the Atten-\ntion part. The beneﬁt of using DSA is also more signiﬁcant\non the 4K tasks. Comparing within each task, it is obvious\nthat DSA model with higher sparsity ratio delivers higher\ncomputation savings. Overall, DSA achieves 2.79 ∼4.35×\ncomputation reduction without any accuracy degradation.\nDense\nDSA-90%DSA-95%DSA-97%DSA-99%\n0.00\n0.25\n0.50\n0.75\n1.00\n1e10 (a) T ext\nOther\nLinear\nAttention\nDense\nDSA-90%DSA-95%DSA-97%DSA-99%\n0\n2\n4\n1e9 (b) Retrieval\nDense\nDSA-90%DSA-95%DSA-97%DSA-99%\n0.0\n0.5\n1.0\n1.5\n1e8 (c) Image\nFigure 7.Computational cost measured in the number of MACs.\nNote that we do not include the computation overhead of\nthe prediction path for generating the sparsity mask. This\nis because the computations conducted in prediction are\nin reduced precision rather than full-precision. Besides,\nit is inappropriate to directly project the number of low-\nprecision MACs to the number of FP32 MACs. Therefore,\nwe use the relative energy consumption to illustrate the\noverall cost of DSA-augmented attention. As shown in\nFigure 8, we show the relative energy consumption of DSA-\n95% with σ = 0.25 and INT4 quantization. Each INT4\nMAC’s energy cost is projected to the relative factor of FP32\nMAC, where the factor number is referenced from industry-\nlevel simulator (Tang et al., 2021) with 45nm technology.\nFrom the ﬁgure we can see that, even with the predictor\noverhead considered, the overall beneﬁt is still compelling\nby virtue of the high dynamic sparsity ratio and low-cost\nprediction methodology.\nDense\nDSA-90%DSA-95%DSA-97%DSA-99%\n0.00\n0.25\n0.50\n0.75\n1.00\n(a) T ext\nOther\nLinear\nPrediction\nAttention\nDense\nDSA-90%DSA-95%DSA-97%DSA-99%\n0.00\n0.25\n0.50\n0.75\n1.00\n(b) Retrieval\nDense\nDSA-90%DSA-95%DSA-97%DSA-99%\n0.00\n0.25\n0.50\n0.75\n1.00\n(c) Image\nFigure 8.Relative energy consumption projected to vanilla trans-\nformer.\nTransformer Acceleration with Dynamic Sparse Attention\n1x2 Sparsity 1x4 Sparsity\nFigure 9.Column-vector sparse encoding (Chen et al., 2021).\n5 A LGORITHM -HARDWARE CO-DESIGN\nIn Section 4.4, we analyze the potential of DSA in terms\nof reducing the total cost of Transformer model. While the\nestimated number of MAC operations and relative energy\nconsumption present very promising results, it remains chal-\nlenging to achieve practical speedup and energy reduction\non real hardware systems. In this section, we dive deeper\ninto this problem as we discuss the implementation of DSA\non GPUs and Accelerators. Speciﬁcally, we evaluate the\nchallenge of mapping DSA onto different platforms, and\nwe demonstrate the ﬂexibility of DSA to enable efﬁcient\nalgorithm-hardware co-designs.\n5.1 GPU Acceleration with DSA\nGiven the predicted sparse patterns, we can reformulate\nQK⊤ as the sampled dense dense matrix multiplication\n(SDDMM) and AV as the sparse matrix-matrix multiplica-\ntion (SpMM). Under ﬁne-grained sparsity, a recent work\n(Gale et al., 2020) proposes SpMM and SDDMM kernel\nthat outperforms dense GEMM kernel under > 71% and\n>90% sparsity, respectively. Besides,cusparse (Naumov\net al., 2010) also achieves practical speedup at >80% spar-\nsity for single precision data. As we presented in Section 4,\nDSA can easily deliver a sparsity ratio of more than 90%\nwith zero accuracy degradation, therefore enabling faster\nkernel implementations on GPUs.\nWhile ﬁne-grained sparse GPU kernels are able to outper-\nform the dense counterparts on relatively high sparsity ratios,\nthe speedup is signiﬁcantly limited due to low data reuse.\nMoreover, when half precision (FP16) is used for compu-\ntation, above ﬁne-grained kernels can hardly compete with\nGEMM kernel, as NVIDIA Tensor Core provides much\nhigher throughput for half precision matrix multiplication.\nThus, we ﬁnd that the performance gain on sparse matrix\nmultiplication can hardly mitigate the overhead of comput-\ning the prediction path in DSA, especially for half precision\nscenarios that commonly appeared at inference. To tackle\nthis problem, structural dynamic sparsity can be introduced\nto the attention selection. Speciﬁcally, instead of select-\ning top−kindependent attention weights, we can enforce\nblock-wise and vector-wise constraints. Also, trade-off can\nbe made by adjusting the block size, as larger blocks deliver\nhigher speedup but can potentially cause accuracy loss.\nIn our work, we experiment on vector sparsity using the\nTable 4.Model accuracy and kernel speedup over cuBLASHgemm.\nWe implement customized SDDMM/SpMM kernel for1×4/1×8\nsparsity and reuse the kernel in (Gale et al., 2020) for ﬁne-grained\nsparsity. Experiments are done on NVIDIA V100 GPU.\nSparsity Pattern vec 1×4 vec 1×8 Fine-grained\nSpMM Speedup 1.57× 1.94× 1.85×\nSDDMM Speedup 0.94× 1.15× 1.09×\nAccuracy(%) -0.02 -0.1 +0.5\nText Classiﬁcation benchmark. As shown in Figure 9, we\nchoose column-vector sparse encoding, where the atten-\ntion elements are pruned in a column-vector granularity.\nColumn-vector sparsity provides the same data reuse as\nblock sparsity, but its smaller granularity makes it more\nfriendly to model training (Chen et al., 2021). Table 4 gives\nthe corresponding kernel speedup and model accuracy under\n90% sparsity ratio. The data type is FP16 for 1 ×4/1 ×8\nsparsity and FP32 for ﬁne-grained sparsity. As we can see,\nDSA can be ﬂexibly combined with different sparsity pat-\nterns, achieving practical runtime speedup on GPU while\nmaintaining on-par model accuracy with full attention.\nTo shed some light on the results, we can trace back to the\nvisualizations of the attention matrix in Figure 1. As shown\nby the ﬁgure, despite the sparse and dynamic characteris-\ntics of the attention matrix, the distribution of important\nattention connections exhibits a certain degree of locality.\nFor example, there exist some global tokens that attend to\nmost of the tokens within a sequence. Therefore, some\ncolumns of the attention matrix will contain many important\npositions. Besides, local attention also indicates row-wise\nlocality, as a token is likely to be inﬂuenced by its neigh-\nbors. Therefore, row-vector sparsity can be added to DSA\nfor performance/accuracy exploration as well. While these\nﬁxed locality patterns have been well discussed in prior\nwork (Zaheer et al., 2020; Beltagy et al., 2020), DSA il-\nlustrates the dynamic distribution which motivates us to\npropose the prediction path to efﬁciently locate these impor-\ntant connections.\n3.0 5.1 14.626.8709.9\n1\n10\n100\n1,000\n50 %70 %90 %95 %99 %\nSoftmaxSpeedup\nSparsity\nFigure 10.Speedup of softmax with different sparsity ratios.\nSparse Softmax Computation. Under the long-sequence\nscenario, the softmax function could be a bottleneck. Let\nh, l, and d be the number of head, sequence length, and\nfeature dimension of each head, respectively. Our proﬁl-\ning result shows that with h = 8 , l = 4096 , d = 64 ,\nTransformer Acceleration with Dynamic Sparse Attention\nsoftmax contributes 47% of the total execution time of the\nmulti-head self-attention layer. By sparsifying the atten-\ntion matrix, DSA directly saves both memory access and\ncomputation consumption of the softmax function to reduce\nexecution time. We evaluate the latency of the pytorch-\nimplemented softmax function on NVIDIA V100 GPU. Fol-\nlowing the conﬁguration in Text Classiﬁcation Benchmark,\nwe set batch size=16, h = 4 , l = 2000 and enforce dif-\nferent sparsity ratios. Figure 10 shows that the reduced\nsoftmax achieves 3.0 ∼709.9×speedup compared with\ndense softmax function.\n5.2 Hardware Specialization for DSA\nWhile adding structural constraints can potentially beneﬁt\nGPU kernel implementation, the expressive power of the\nmodel is still inevitably affected. For instance, as shown\nin Table 4, the 1 ×4 vector encoding achieves comparable\naccuracy with full-attention, but is lower than the accuracy\nof using ﬁne-grained sparsity under the same sparsity ratio.\nThus, an alternative approach is to use hardware specializa-\ntion to fully exploit the potential saving from DSA.\nAs we know, self-attention mechanism mainly involves\nmatrix-matrix multiplication, which can be efﬁciently han-\ndled with a 2D spatial array of processing elements (PEs).\nPrior work also proposes efﬁcient dataﬂow for computing\nattention layer using techniques such as operator fusion,\nloop tiling, and loop reordering (Park et al., 2020; Kao et al.,\n2021). With DSA, the underlying spatial array and data-ﬂow\nshould be adjusted accordingly. Speciﬁcally, DSA poses\ntwo architectural implications as follows.\nMulti-precision Computation. DSA relies on dimension\nreduction and quantization to control the overhead of atten-\ntion prediction. Therefore, the system needs to handle both\nhigh-precision (eg., FP32/FX16) and low-precision (e.g.,\nINT2/INT4) computations. This can be implemented either\nwith a decoupled design or a coupled design. In decoupled\narchitecture, standalone hardware modules are implemented\nfor different precision (Liu et al., 2020), e.g., using two\nPE arrays for low-precision and high-precision. The two\nmodules work in a pipelined manner, where the small PE\narray generates sparsity information for the large PE array\nto skip unnecessary computations. A drawback of this type\nof design is that, the computation throughput is ﬁxed for the\ntwo modules, but the relative workload between prediction\nand execution is task-dependent. As a result, one module\nmay become idle time to time due to workload imbalance.\nOn the contrary, a coupled PE array tackles this problem by\nusing multi-precision arithmetic units (Sharma et al., 2018).\nSpeciﬁcally, the computation precision of each PE is conﬁg-\nurable, such that different sections of the PE array can be\ndynamically controlled to balance the relative throughput.\nYet, this requires runtime conﬁguration which makes system\ncontrol to be more complicated.\nSparsity-aware Execution. As mentioned above, with\nDSA, we can reformulate Z = (QK⊤)V into a SDDMM\nfollowed by a SpMM (softmax is omitted for simplicity).\nIn other words, the sparsity information is used as output\nsparsity (OS) for the ﬁrst matrix multiplication and used as\ninput sparsity (IS) during the second matrix multiplication.\nHowever, different PEs may also encounter workload im-\nbalance due to irregularly distributed sparsity, causing low\nutilization. Prior work tackles this problem from multiple\napproaches. For example, one can enable early ﬁnished\nPE to switch to the execution of other elements, or apply\nofﬂine sorting and online shufﬂing to balance the compu-\ntation (Song et al., 2018; Aklaghi et al., 2018; Gondimalla\net al., 2019). These approaches impose different software\nand hardware overheads such as larger scratchpad memory,\nredundant memory access, higher bandwidth requirement\nand so on. In DSA, we use a simple and effective solution\nby enforcing a row-wise constraint such that different atten-\ntion rows contain the same amount of important attention\nweights.\n1 2 3\n1 32\n1 32\n1 32\nAttention Matrix\n3 1 2\n1 23\n2 13\n1 32Compute\nReordering\nAttention Matrix\nPE1\nPE2\nPE3\nPE4\nFigure 11.Using sparsity locality and compute reordering to im-\nprove data reuse.\nFinally, the locality of important attention weights provides\nopportunities for data reuse. As shown in Figure 11, if multi-\nple rows of attention matrixAare computed simultaneously,\nthe corresponding columns in matrix KT can be loaded\nonce and shared by different PEs. Similarly, this holds true\nfor computing matrix Z, as the rows in matrix V can be\nreused. In this example, suppose four PEs work in parallel\nto compute the attention matrix, and each PE is responsible\nfor one row. The colored squares are selected attentions.\nThe numbers in the square indicate the computation order\nof each PE. As we can see, in the left ﬁgure, each PE com-\nputes the selected attention weights from left to right. Thus,\nalthough the sparsity distribution delivers some locality, the\ndata reuse is bad. In contrast, if we reorder the computation\nwithin each row as shown in the right ﬁgure, then we can uti-\nlize the column locality to improve data reuse. We evaluate\nthe beneﬁt of computation reordering on real benchmarks.\nAs shown in Table 5, on the Text Classiﬁcation task, the\nlocality naturally brings 1.28×memory access reduction\ncompared with row-by-row processing, while reordering\nfurther improves this ratio to 2.54×. On Image Classiﬁca-\ntion, the reduction ratio is 1.07×without reordering and\n1.37×with reordering.\nTransformer Acceleration with Dynamic Sparse Attention\nAn important beneﬁt of this type of out-of-order execution\nis that, matrix Adoes not need to be reshufﬂed and matrixZ\nis still generated in a regular order. This granted advantage\ncomes from the attention mechanism itself, because the\nwhole computation process is a two-step GEMM chain.\nTherefore, the reordered Ais completely consumed during\nthe second matrix multiplication. In contrast, exploring\nthe same reordering in CNN would require a crossbar-like\ndesign to correctly store the output result (Liu et al., 2020),\ncausing additional performance and resource overhead.\nTable 5.Memory access reduction of the second matrix operand\nused in the multiplication of QK⊤ and AV.\nDataﬂow Image Text\nrow-by-row 1× 1×\nrow-parallel w/o\ncompute reordering 1.07× 1.37×\nrow-parallel w/\ncompute reordering 1.28× 2.54×\n6 R ELATED WORK\nTransformers with the use of self-attention mechanism\nare difﬁcult to scale with sequence length because of the\nquadratic time and memory complexity. Our paper focuses\non the exploration of sparse attention patterns in Trans-\nformers. Other orthogonal approaches such as parameters\nsharing (Gong et al., 2019) can mitigate the issue. We refer\nreaders to a survey paper for a more comprehensive view of\nefﬁcient Transformers (Tay et al., 2020c).\nStatic Sparse Patterns. A straightforward way to exploit\nattention sparsity is to set static or ﬁxed sparse patterns,\nsuch as local windows, block-wise, dilated patterns, or a\ncombination of static patterns (Zaheer et al., 2020; Child\net al., 2019; Qiu et al., 2020). However, as the sparse at-\ntention patterns are inherently dynamic depending on input\nsequences, those work lack the capability of capturing dy-\nnamic sparse patterns. As shown in our evaluation, the\nsparsity-saving trade-offs of representative methods using\nstatic sparse patterns are worse than our dynamic sparse\nattention approach.\nClustering-based methods. Building upon static block-\nsparse patterns, another line of research is to group simi-\nlar tokens into chunks and perform local attention within\nchunks (Kitaev et al., 2020; Roy et al., 2021; Tay et al.,\n2020a). The similarity function used to group tokens can\nbe hashing, clustering, or learned sorting. However, those\nmethods are designed for training memory reduction and im-\npractical at inference time when operating on each sequence.\nThe quality of grouping, e.g., convergence of clustering, is\nnot guaranteed at long sequences, and the overhead of on-\nthe-ﬂy clustering is not acceptable.\nApproximation methods. Recent work proposes to replace\nstandard attention with forms of approximation of the at-\ntention weights (Wang et al., 2020; Katharopoulos et al.,\n2020; Choromanski et al., 2021; Peng et al., 2021). While\nwe provide a comparison in our evaluation, we regard those\nwork out the scope of our discussion for exploring sparsity\nin (standard) attention. Whether using a form of approxima-\ntion to replace standard attention or as we suggest to predict\nsparse patterns explicitly is a design choice leaving up to\npractitioners.\nAttention and Transformer Accelerators Recent work\nadopt algorithm and hardware co-design to reduce the cost\nof attention mechanism. MnnFast (Jang et al., 2019) pro-\nposes to skip the computations of A×V based on the mag-\nnitude of the calculated attention scores. This method can\nonly beneﬁt the second GEMM of attention layer. A3 (Ham\net al., 2020) introduces attention approximation to prune\nthe unimportant attentions. However, A3 involves expen-\nsive online sorting, which causes signiﬁcant performance\nand energy overhead. ELSA (Ham et al., 2021) uses sign\nrandom projection to estimate the attention weights, mak-\ning the approximation much more hardware efﬁcient, but\nthe model quality is hurt due to inaccurate approximation.\nIn DSA, we address these limitations by simultaneously\nconsidering approximation accuracy and efﬁciency. Finally,\nSpAtten (Wang et al., 2021) proposes cascade token pruning\nand head pruning to reduce the cost of both self-attention\nblock and subsequent layers. While removing several rows\nand columns of the attention matrix makes the operation\nregular and hardware-friendly, we ﬁnd this constraint to be\ntoo aggressive as the locality of attention weights usually\nexists in small granularity.\n7 C ONCLUSION\nIn this paper, we present Dynamic Sparse Attention (DSA),\na novel method that exploits dynamic sparse patterns in\nattention to reduce computational cost when serving Trans-\nformers. Speciﬁcally, we show that our method can achieve\nup to 95% attention sparsity without model inference qual-\nity loss. Other than prior art that uses static sparse patterns\nin attention, our method explores dynamic sparse patterns\nthat are inherent in attention when processing different in-\nput sequences. Instead of replacing standard attention with\nother variants such as low-rank approximation methods, we\naugment standard attention with a prediction path as the\nmeans to locate dynamic sparsity. On one hand, attention\napproximation can be very efﬁcient when only used for spar-\nsity prediction. On the other hand, the expressive power of\nfull attention is preserved as the important attention weights\nfrom full attention are effective in model inference. Experi-\nmental results on the LRA benchmark demonstrate superior\nperformance and model efﬁciency of DSA. Furthermore, we\ndemonstrate the potential of using DSA to improve hardware\nperformance and efﬁciency. With customized kernel design\nTransformer Acceleration with Dynamic Sparse Attention\nand structural sparsity, DSA delivers practical speedup on\nGPU. The algorithm beneﬁt can be further exploited with\nspecialized architecture, as the hardware can fully beneﬁt\nfrom low-precision prediction, ﬁne-grained sparse computa-\ntion, and data locality.\nREFERENCES\nAklaghi, V ., Yazdanbakhsh, A., Samadi, K., Esmaeilzadeh,\nH., and Gupta, R. Snapea: Predictive early activation\nfor reducing computation in deep convolutional neural\nnetworks. ISCA, 2018.\nBeltagy, I., Peters, M. E., and Cohan, A. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,\nC., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,\nWanderman-Milne, S., and Zhang, Q. JAX: composable\ntransformations of Python+NumPy programs, 2018. URL\nhttp://github.com/google/jax.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nChen, Z., Qu, Z., Liu, L., Ding, Y ., and Xie, Y . Efﬁcient ten-\nsor core-based gpu kernels for structured sparsity under\nreduced precision. 2021.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Generat-\ning long sequences with sparse transformers, 2019.\nChoromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., Belanger, D., Colwell, L., and Weller, A.\nRethinking attention with performers, 2021.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and\nSalakhutdinov, R. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context, 2019.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 4171–4186,\n2019.\nGale, T., Zaharia, M., Young, C., and Elsen, E.\nSparse gpu kernels for deep learning. arXiv preprint\narXiv:2006.10901, 2020.\nGondimalla, A., Chesnut, N., Thottethodi, M., and Vijayku-\nmar, T. Sparten: A sparse tensor accelerator for con-\nvolutional neural networks. In Proceedings of the 52nd\nAnnual IEEE/ACM International Symposium on Microar-\nchitecture, pp. 151–165, 2019.\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\nEfﬁcient training of BERT by progressively stacking. In\nProceedings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Machine\nLearning Research, pp. 2337–2346. PMLR, 09–15 Jun\n2019. URL http://proceedings.mlr.press/\nv97/gong19a.html.\nHam, T. J., Jung, S. J., Kim, S., Oh, Y . H., Park, Y .,\nSong, Y ., Park, J.-H., Lee, S., Park, K., Lee, J. W.,\nand Jeong, D.-K. A ˆ3: Accelerating attention mecha-\nnisms in neural networks with approximation. In 2020\nIEEE International Symposium on High Performance\nComputer Architecture (HPCA), pp. 328–341, 2020. doi:\n10.1109/HPCA47549.2020.00035.\nHam, T. J., Lee, Y ., Seo, S. H., Kim, S., Choi, H., Jung, S. J.,\nand Lee, J. W. Elsa: Hardware-software co-design for\nefﬁcient, lightweight self-attention mechanism in neural\nnetworks. In 2021 ACM/IEEE 48th Annual International\nSymposium on Computer Architecture (ISCA), pp. 692–\n705. IEEE, 2021.\nJang, H., Kim, J., Jo, J.-E., Lee, J., and Kim, J. Mnn-\nfast: A fast and scalable system architecture for memory-\naugmented neural networks. In Proceedings of the 46th\nInternational Symposium on Computer Architecture, pp.\n250–263, 2019.\nKao, S.-C., Subramanian, S., Agrawal, G., and Krishna, T.\nAttacc the quadratic bottleneck of attention layers. ArXiv,\nabs/2107.06419, 2021.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In International Conference on\nMachine Learning, pp. 5156–5165. PMLR, 2020.\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer:\nThe efﬁcient transformer. In International Conference\non Learning Representations, 2020. URL https://\nopenreview.net/forum?id=rkgNKkHtvB.\nKrizhevsky, A. Learning multiple layers of features from\ntiny images. University of Toronto, 05 2012.\nLiu, L., Qu, Z., Deng, L., Tu, F., Li, S., Hu, X., Gu, Z.,\nDing, Y ., and Xie, Y . Duet: Boosting deep neural net-\nwork efﬁciency on dual-module architecture. In 2020\n53rd Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO), pp. 738–750, 2020. doi:\n10.1109/MICRO50266.2020.00066.\nTransformer Acceleration with Dynamic Sparse Attention\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,\nA. Y ., and Potts, C. Learning word vectors for sen-\ntiment analysis. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Linguis-\ntics: Human Language Technologies, pp. 142–150, Port-\nland, Oregon, USA, June 2011. Association for Com-\nputational Linguistics. URL https://www.aclweb.\norg/anthology/P11-1015.\nNaumov, M., Chien, L., Vandermersch, P., and Kapasi, U.\nCusparse library. In GPU Technology Conference, 2010.\nOtt, M., Edunov, S., Grangier, D., and Auli, M. Scaling neu-\nral machine translation. arXiv preprint arXiv:1806.00187,\n2018.\nPark, J., Yoon, H., Ahn, D., Choi, J., and Kim, J.-J. Opti-\nmus: Optimized matrix multiplication structure for trans-\nformer neural network accelerator. Proceedings of Ma-\nchine Learning and Systems, 2:363–378, 2020.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,\nN., Ku, A., and Tran, D. Image transformer. In Interna-\ntional Conference on Machine Learning, pp. 4055–4064.\nPMLR, 2018.\nPeng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,\nN., and Kong, L. Random feature attention. In In-\nternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=QtTKTdVrFBB.\nQiu, J., Ma, H., Levy, O., tau Yih, S. W., Wang, S., and\nTang, J. Blockwise self-attention for long document un-\nderstanding, 2020.\nRadev, D., Muthukrishnan, P., and Qazvinian, V . The acl\nanthology network corpus. volume 47, 01 2009. doi:\n10.1007/s10579-012-9211-2.\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient\ncontent-based sparse attention with routing transform-\ners. Transactions of the Association for Computational\nLinguistics, 9:53–68, 2021.\nSharma, H., Park, J., Suda, N., Lai, L., Chau, B., Chan-\ndra, V ., and Esmaeilzadeh, H. Bit fusion: Bit-level dy-\nnamically composable architecture for accelerating deep\nneural network. In 2018 ACM/IEEE 45th Annual Inter-\nnational Symposium on Computer Architecture (ISCA),\npp. 764–775. IEEE, 2018.\nShi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and\nKwok, J. T. Sparsebert: Rethinking the importance anal-\nysis in self-attention. arXiv preprint arXiv:2102.12871,\n2021.\nSong, M., Zhao, J., Hu, Y ., Zhang, J., and Li, T. Pre-\ndiction based execution on deep neural networks. In\n2018 ACM/IEEE 45th Annual International Symposium\non Computer Architecture (ISCA), pp. 752–763. IEEE,\n2018.\nTang, T., Li, S., Nai, L., Jouppi, N., and Xie, Y . Neurom-\neter: An integrated power, area, and timing modeling\nframework for machine learning accelerators industry\ntrack paper. In 2021 IEEE International Symposium on\nHigh-Performance Computer Architecture (HPCA), pp.\n841–853, 2021. doi: 10.1109/HPCA51647.2021.00075.\nTay, Y ., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.\nSparse sinkhorn attention. In International Conference\non Machine Learning, pp. 9438–9447. PMLR, 2020a.\nTay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D.,\nPham, P., Rao, J., Yang, L., Ruder, S., and Metzler,\nD. Long range arena: A benchmark for efﬁcient trans-\nformers. CoRR, abs/2011.04006, 2020b. URL https:\n//arxiv.org/abs/2011.04006.\nTay, Y ., Dehghani, M., Bahri, D., and Metzler, D. Efﬁ-\ncient transformers: A survey. arXiv, (August 2020):1–28,\n2020c. ISSN 23318422.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. At-\ntention is all you need. In NIPS, pp. 6000–6010,\n2017. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need .\nWang, H., Zhang, Z., and Han, S. Spatten: Efﬁcient sparse\nattention architecture with cascade token and head prun-\ning. In 2021 IEEE International Symposium on High-\nPerformance Computer Architecture (HPCA), pp. 97–110,\n2021. doi: 10.1109/HPCA51647.2021.00018.\nWang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-\nformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nZaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti,\nC., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang,\nL., et al. Big bird: Transformers for longer sequences.\narXiv preprint arXiv:2007.14062, 2020.\nTransformer Acceleration with Dynamic Sparse Attention\nA B ENCHMARK DESCRIPTIONS AND\nEXPERIMENT CONFIGURATIONS\nIn our experiments, we choose Text Classiﬁcation, Image\nClassiﬁcation and Document Retrieval from Long-Range\nArena, while excluding Long ListOps and Pathﬁnder. This\nis because the ListOps results in LRA exhibit signiﬁcant\ndivergence without much explanation. And for Pathﬁnder,\nwe are unable to reproduce the baseline results with the\ngiven training conﬁgurations from LRA.\nA.1 Text Classiﬁcation\nThe Text Classiﬁcation task, as introduced in LRA (Tay\net al., 2020b), is a binary classiﬁcation that uses real-world\ndata to benchmark the ability of the models to deal with com-\npositionality. IMDb review (Maas et al., 2011) is selected\nas the dataset, which is a common choice for document\nclassiﬁcation. Moreover, to make the problem more chal-\nlenging, this task takes a byte-level setup instead of the\nnormal character-level setup for language modeling. There-\nfore, the model needs to learn from the unsegmented data\nand make compositional decisions.\nFor model conﬁguration, we use the original hyperparame-\nters given in the LRA repository 1. Speciﬁcally, the baseline\ntransformer consists of 4 attention layers, each with 4 heads.\nThe hidden dimension size is 256 and the positional FFN\nlayer has a dimension size of 1024. The learning rate is 0.05\nwith a weight decay of 0.1. Finally, the baseline model is\ntrained for 20Ksteps where the ﬁrst 8Kare warmup steps\nand the batch size is 32.\nWhen compared with the dense baseline in Figure 2 of the\nfull paper, the DSA-x% models are obtained from ﬁne-\ntuning the dense model for 5K steps with different levels\nof sparsity constraints. During ﬁne-tuning, parameters from\nboth original model and the predictor are updated simultane-\nously using the combination of cross-entropy loss and MSE\nloss. The weight factor λof the MSE loss is 0.01 and the\nlearning rate is uniformly set as 0.0002.\nWhen compared with other efﬁcient transformers as shown\nin Table 1 of the full paper, we directly train the DSA pre-\ndiction path from scratch. The overall training step is still\n20K, but we use the ﬁrst 15K to train the original model\nand freeze the predictor module. Therefore, the ﬁrst 15K\nsteps are the same as training a dense baseline. After this,\nwe jointly optimize the model and the predictor module\nduring the last 5Ksteps with the same MSE loss factor and\nlearning rate as above.\nFinally, to limit the training cost, we set the sequence length\nto be 2000 for the baseline comparison and sensitivity study,\n1https://github.com/google-research/\nlong-range-arena\nwhile only set the length to be 4000 when comparing with\nother models.\nA.2 Document Retrieval\nDocument Retrieval is a binary classiﬁcation task that serves\nas a test to evaluate how well a model compresses long se-\nquences into representations for similarity-based matching.\nThis task uses ACL Anthology Network (Radev et al., 2009)\nand aims to identify if two papers have a citation link. Simi-\nlar to Text Classiﬁcation, byte-level setup is used to increase\nthe difﬁculty of the problem.\nWe use a uniform sequence length of 4000 in this task. The\nbaseline transformer consists of 4 attention layers. Each\nattention layer has 4 heads, 128 hidden dimensions, and 512\nFFN dimensions. The learning rate is 0.05 with a weight\ndecay of 0.1. The model is trained for 5Ksteps with Adam\noptimizer and a batch size of 32. Similar to the strategy in\nthe Text Classiﬁcation task, we use ﬁne-tuning for baseline\ncomparison and training-from-scratch for cross model com-\nparison. The 5K steps are equally divided into 2.5K for\ndense training and 2.5K for joint training in the training-\nfrom-scratch experiment. When jointly optimizing all the\nparameters, the weight factor λof the MSE loss is 0.01 and\nthe learning rate is 0.0002.\nA.3 Image Classiﬁcation\nThe ﬁnal task we include in our evaluation is image classiﬁ-\ncation using CIFAR-10 (Krizhevsky, 2012). Each 32 ×32\ninput image is ﬂattened as a sequence of pixels. Therefore,\nthe sequence length of this task is 1024. The input images\nare mapped to a single gray-scale channel where each pixel\nis represented with an 8-bit integer value. Following the\ngiven settings, the baseline transformer model contains one\nattention layer with 8 heads, 64 query/key/value hidden\ndimensions, and 128 FFN dimensions.\nThere are in total 45,000 training samples and 15,000 val-\nidation samples. We train the model for 200 epochs with\na learning rate of 0.0005 and a batch size of 128. Same\nas above, we use ﬁnetuning for baseline comparison and\ntraining-from-scratch for cross model comparison. The 200\nsteps are divided into 150 for dense training and 50 for joint\ntraining in the training-from-scratch experiment. When\njointly optimizing all the parameters, the weight factor λof\nthe MSE loss is 0.01 and the learning rate is 0.0002.",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.7768150568008423
    },
    {
      "name": "Exploit",
      "score": 0.7723903656005859
    },
    {
      "name": "Computer science",
      "score": 0.7694595456123352
    },
    {
      "name": "Bottleneck",
      "score": 0.7602420449256897
    },
    {
      "name": "Computation",
      "score": 0.6561527848243713
    },
    {
      "name": "Transformer",
      "score": 0.6482810974121094
    },
    {
      "name": "Software deployment",
      "score": 0.5326991081237793
    },
    {
      "name": "Computer engineering",
      "score": 0.5212993621826172
    },
    {
      "name": "Quadratic equation",
      "score": 0.4631485342979431
    },
    {
      "name": "Suite",
      "score": 0.46053269505500793
    },
    {
      "name": "Parallel computing",
      "score": 0.33409449458122253
    },
    {
      "name": "Algorithm",
      "score": 0.23505884408950806
    },
    {
      "name": "Embedded system",
      "score": 0.19865167140960693
    },
    {
      "name": "Software engineering",
      "score": 0.12169438600540161
    },
    {
      "name": "Voltage",
      "score": 0.09469667077064514
    },
    {
      "name": "Engineering",
      "score": 0.09294465184211731
    },
    {
      "name": "Computer security",
      "score": 0.08054548501968384
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 12
}