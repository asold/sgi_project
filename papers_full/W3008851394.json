{
  "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",
  "url": "https://openalex.org/W3008851394",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227371052",
      "name": "Li, Zhuohan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2913157063",
      "name": "Wallace, Eric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120683661",
      "name": "Shen, Sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2915472489",
      "name": "Lin, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743083855",
      "name": "Keutzer, Kurt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2615303194",
      "name": "Klein, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222709620",
      "name": "Gonzalez, Joseph E.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963684275",
    "https://openalex.org/W2963376662",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963982496",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2963643655",
    "https://openalex.org/W2952865063",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3035180000",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W2182361439",
    "https://openalex.org/W2996331410",
    "https://openalex.org/W2962965870",
    "https://openalex.org/W3037639655",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2986300872",
    "https://openalex.org/W2964233199",
    "https://openalex.org/W2964161337",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2953212265",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2947149462",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2964108773",
    "https://openalex.org/W2964307104",
    "https://openalex.org/W133229983",
    "https://openalex.org/W3035304835",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W3034733718",
    "https://openalex.org/W3137695714",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2963097630",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2903697572",
    "https://openalex.org/W2770298516",
    "https://openalex.org/W2948954216",
    "https://openalex.org/W2945667196",
    "https://openalex.org/W2962776038",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2963236897",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W2907127169",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2982041622"
  ],
  "abstract": "Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.",
  "full_text": "Train Large, Then Compress:\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\nZhuohan Li * 1 Eric Wallace* 1 Sheng Shen * 1 Kevin Lin * 1\nKurt Keutzer 1 Dan Klein 1 Joseph E. Gonzalez 1\nAbstract\nSince hardware resources are limited, the ob-\njective of training deep learning models is typ-\nically to maximize accuracy subject to the time\nand memory constraints of training and inference.\nWe study the impact of model size in this set-\nting, focusing on Transformer models for NLP\ntasks that are limited by compute: self-supervised\npretraining and high-resource machine transla-\ntion. We ﬁrst show that even though smaller\nTransformer models execute faster per iteration,\nwider and deeper models converge in signiﬁcantly\nfewer steps. Moreover, this acceleration in conver-\ngence typically outpaces the additional computa-\ntional overhead of using larger models. Therefore,\nthe most compute-efﬁcient training strategy is to\ncounterintuitively train extremely large models\nbut stop after a small number of iterations.\nThis leads to an apparent trade-off between the\ntraining efﬁciency of large Transformer models\nand the inference efﬁciency of small Transformer\nmodels. However, we show that large models\nare more robust to compression techniques such\nas quantization and pruning than small models.\nConsequently, one can get the best of both worlds:\nheavily compressed, large models achieve higher\naccuracy than lightly compressed, small models.\n1. Introduction\nIn the current deep learning paradigm, using more compute\n(e.g., increasing model size, dataset size, or training steps)\ntypically leads to higher model accuracy (Brock et al., 2019;\nRaffel et al., 2019). This phenomenon is exacerbated by\nthe recent success of self-supervised pretraining (Devlin\n*Equal contribution 1UC Berkeley. Correspondence to: Zhuo-\nhan Li <zhuohan@cs.berkeley.edu>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\nTrain Small\nModel\nOptimal\nStop Training \nWhen Converged\nLightly\nCompress\nTrain Large\nModel\nStop Training \nEarly\nHeavily \nCompress\nCommon \nPractice\nFigure 1.Under the usual presumption that models are trained to\nconvergence, only small models that are fast-to-execute are feasible\nin resource-constrained settings. Our work shows that the most\ncompute-efﬁcient training scheme is instead to train very large\nmodels, stop them well short of convergence, and then heavily\ncompress them to meet test-time constraints.\net al., 2019; Hnaff et al., 2019), which allows training to\nscale to massive amounts of unlabeled data and very large\nneural models. Consequently, computational resources are\nincreasingly the critical constraint on improving model ac-\ncuracy. This constraint causes the (often implicit) goal of\nmodel training to be maximizing compute efﬁciency: how\nto achieve the highest model accuracy given a ﬁxed amount\nof hardware and training time.\nMaximizing compute efﬁciency requires rethinking com-\nmon assumptions about model training. In particular, there\nis typically an implicit assumption that models must be\ntrained until convergence, which makes larger models ap-\npear less viable for limited compute budgets. We challenge\nthis assumption by demonstrating the opportunity to in-\ncrease model size at the cost of convergence. Concretely,\nwe show that the fastest way to train Transformer mod-\nels (Vaswani et al., 2017) is to substantially increase model\nsize but stop training very early.\nIn our experiments, we vary the width and depth of Trans-\nformer models and evaluate their training time and accu-\nracy on self-supervised pretraining (ROBERTA (Liu et al.,\n2019b) trained on Wikipedia and BookCorpus) and machine\ntranslation (WMT14 English→French). For these tasks, we\nﬁrst show that larger models converge to lower validation\nerror in fewer gradient updates than smaller models (Sec-\ntion 3). Moreover, this increase in convergence outpaces the\nadditional computational overhead of using larger models—\nthe most compute-efﬁcient models are extremely large and\nstopped well short of convergence (e.g., Figure 2, left). We\narXiv:2002.11794v2  [cs.CL]  23 Jun 2020\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n0 250000 500000 750000 1000000\nWall Clock (Seconds)\n4\n6\n8\n10MLM Validation Perplexity\nModel Depth\n3 Layers\n6 Layers\n12 Layers\n18 Layers\n24 Layers\nEffect of RoBERTa Depth on Training\n(a)\n0 50 100 150 200\nNumber of Parameters (Millions)\n0.75\n0.80\n0.85MNLI Validation Accuracy\n45%\n60%\n75%\n90%\n0%\n15%30%\n0%15%30%45%\n60%\n75%\n90%\n0%15%30%45%\n60%\n75%\n90%\n0%15%30%\n45%\n60%\n75%\n90%\n0%15%30%\n45%60%75%\n90%\nOriginal Size\na 3 Layers\na 6 Layers\na 12 Layers\na 18 Layers\na 24 Layers\nEffect of RoBERTa Depth on Pruning (b)\nFigure 2.Increasing Transformer model size results in lower validation error as a function of wall-clock time and better test-time accuracy\nfor a given inference budget. (a) demonstrates the training speedup for ROBERTA models of different sizes on the masked language\nmodeling pretraining task. In (b), we take ROBERTA checkpoints that have been pretrained for the same amount of wall-clock time and\nﬁnetune them on a downstream dataset (MNLI). We then iteratively prune model weights to zero and ﬁnd that the best models for a given\ntest-time memory budget are ones which are trained large and then heavily compressed.\nalso show that this acceleration in wall-clock convergence\nis largely a function of parameter count and only weakly\ninﬂuenced by model width, depth, and batch size.\nAlthough larger models train faster, they also increase the\ncomputational and memory requirements of inference. This\nincreased cost is especially problematic in real-world appli-\ncations, where the cost of inference dominates the cost of\ntraining (Jouppi et al., 2017; Crankshaw et al., 2017; Metz,\n2017). However, we show that forROBERTA, this apparent\ntrade-off can be reconciled with compression: large models\nare considerably more robust to compression as compared to\nsmall models (Section 4). Thus, large, heavily compressed\nmodels outperform small, lightly compressed models using\ncomparable inference costs (e.g., Figure 2, right).\nWe ﬁnally analyze when and why large models train fast\nand compress well (Section 5). We show that the optimal\nmodel size is closely linked to the dataset size. In particular,\nlarge models perform favorably in big data settings where\noverﬁtting is a limited concern. We then analyze why larger\nmodels are more compressible by measuring the difference\nin weights when using quantized or sparse weight matrices.\nThis error decreases as model size increases, i.e., greater\noverparameterization leads to easy-to-compress weights.\n2. Experimental Setup\n2.1. Tasks, Models, and Datasets\nWe train state-of-the-art models for two NLP tasks: self-\nsupervised pretraining using masked language modeling and\nhigh-resource machine translation. We chose these tasks\nbecause accuracy continues to improve as models are made\nlarger (Shazeer et al., 2018), trained for more steps (Liu\net al., 2019b), and trained using larger batches (Raffel et al.,\n2019). Thus, a critical factor in improving accuracy for these\ntasks is to maximize the compute efﬁciency of training.\nSelf-supervised Pretraining (MLM) We closely follow\nthe pretraining setup and model from ROBERTA (Liu et al.,\n2019b) with a few minor exceptions. We move the model’s\nlayer normalization layers (Ba et al., 2016) to the input\nof every sub-layer (often called pre-norm). This slightly\nimproves results and stabilizes training (Wang et al., 2019b).\nWe also use an input sequence length of 128 and a batch\nsize of 8192, unless otherwise noted. For ROBERTA, we\nvary the depth in {3, 6, 12, 18, 24}, and the hidden size in\n{256, 512, 768, 1024, 1536}.\nThe dataset for pretraining ROBERTA is not publicly avail-\nable. We instead follow BERT (Devlin et al., 2019) and con-\ncatenate the BookCorpus (Zhu et al., 2015) and a Wikipedia\ndump to use for training. Since the BookCorpus is no longer\npublicly available, we follow Devlin et al. (2019) and crawl\nhttp://smashwords.com. Our ﬁnal dataset is roughly 3.4\nbillion words in total. We hold out a random 0.5% of the\ndata for validation and report the masked language model-\ning (MLM) perplexity on this data. We also evaluate the\nmodel by ﬁnetuning on MNLI (Williams et al., 2018) and\nSST-2 (Socher et al., 2013). We found the variance in accu-\nracy for these two tasks to be lower than the other GLUE\ntasks (Wang et al., 2019a).\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\nMachine Translation For machine translation (MT) we\ntrain the standard Transformer architecture and hyperpa-\nrameters on the WMT14 English →French dataset. We\nuse the standard dataset splits: 36M sentences for train-\ning, newstest2013 for validation, and newstest2014 for\ntesting. We follow standard practice and report tokenized\ncase-sensitive BLEU (Papineni et al., 2002) with compound\nsplitting (Vaswani et al., 2017). We vary the model depth in\n{2, 6, 8}and hidden size in {128, 256, 512, 1024, 2048}.\n2.2. Evaluation Metrics: FLOPs and Wall-Clock Time\nRecent work on resource-constrained training uses the total\nnumber of training steps (Li et al., 2020) or the total num-\nber of training FLOPs (Schwartz et al., 2019; Clark et al.,\n2020) as the main evaluation metric. These metrics do not\nadequately capture the true training time. In particular, re-\nporting gradient steps does not account for the cost of using\nbigger batches or models. Moreover, although reporting\nFLOPs is useful for comparison as it is hardware-agnostic,\nit neglects the fact that parallel operations are signiﬁcantly\ncheaper than sequential operations on modern hardware.\nWe instead directly report wall-clock time as our main eval-\nuation metric.1 Since the runtime varies across machines\n(the hardware setups are different, the jobs are not isolated,\netc.), we use a single machine to benchmark the time per\ngradient step for each model size. In particular, we train\nmodels and wait for the time per gradient step to stabi-\nlize, and then we use the average time over 100 steps to\ncalculate the training duration. We conduct the timing on\none NVIDIA 16GB V100 GPU and use gradient accumu-\nlation to ﬁt larger models and batches. In order to be fair\nto smaller models, we increase the batch size to the largest\nsize that ﬁts in memory. This means that smaller models\nuse fewer gradient accumulation steps and thus take less\ntime per gradient step (which we conﬁrmed empirically).\nWe use Tensor2Tensor (Vaswani et al., 2018) for MT and\nfairseq (Ott et al., 2019) for RoBERTa. We train using a mix\nof v3-8 TPUs and 8xV100 GPUs for both tasks.\n3. Larger Models Train Faster\nWider and deeper Transformer models are more sample-\nefﬁcient than small models: they reach the same level of\nperformance using fewer gradient steps (Figures 3–5). More-\nover, this increase in convergence outpaces the additional\ncomputational overhead from increasing model size, even\nthough we need to use more steps of gradient accumulation.\nConsequently, after adjusting for wall-clock time, the larger\nmodels are faster to train than smaller models (Figures 4–5).\n1We also report selected learning curves as a function of FLOPs\nin Appendix A.1. These curves show that our conclusion that larger\nmodels are faster to train is not speciﬁc to our hardware setup.\n0 40000 80000 120000\nNumber of Gradient Steps\n4\n6\n8\n10Validation MLM Perplexity\nModel Depth\n3 Layers\n6 Layers\n12 Layers\n18 Layers\n24 Layers\nEffect of RoBERTa Depth\nFigure 3.Deeper ROBERTA models converge faster than shallow\nmodels with respect to the gradient steps (wall-clock time shown\nin Figure 2, left).\nIncrease Model Width and Sometimes Depth For the\nmasked language modeling task, the validation perplexity\nweakly depends on the shape of the model. Instead, the\ntotal number of model parameters is the key determiner of\nthe convergence rate. Thus, increasing either the width or\nthe depth is effective at accelerating model training. On the\nother hand, the preferred way to scale models for MT is\nto increase their width as wider models usually outperform\ndeep models in ﬁnal performance (Vaswani et al., 2017;\nShazeer et al., 2018).\nIncrease Model Size, Not Batch Size Another factor that\naffects the training efﬁciency is the batch size. In particu-\nlar, there is a trade-off between using fast-to-execute small\nbatches and slow-but-accurate large batches. We study the\neffect of scaling batch size because it provides an alternative\nto scaling model size. In particular, what if we use gradi-\nent accumulation to increase the batch size rather than the\nmodel size? We vary the batch size for the 12 layer, 768H\nmodel and increase the learning rate as is common prac-\ntice (Goyal et al., 2017; Liu et al., 2019b). We report the\nbest found learning rate values in Table 1 in Appendix A.\nWe show the training curves in Figure 13 in Appendix A.\nBigger batch sizes cause the model to converge in fewer\nsteps. However, when adjusting for wall-clock time, in-\ncreasing the batch size beyond a certain point only provides\nmarginal improvements.2 In particular, varying the batch\nsize has little impact when training with a batch size in the\n2Note that our timing is done by accumulating gradients on a\nsingle GPU machine. For multi-GPU setups, the cost of accumu-\nlating gradients is lower as it naturally helps to balance out uneven\nruntimes across workers (Ott et al., 2018). In this setup, the wall-\nclock improvements from increasing batch sizes by accumulating\ngradients may be slightly larger.\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n0 40000 80000 120000\nNumber of Gradient Steps\n5\n7.5\n10Validation MLM Perplexity\nHidden Size\n256H\n512H\n768H\n1024H\n1536H\nEffect of RoBERTa Hidden Size\n0 250000 500000 750000 1000000\nWall Clock (Seconds)\n5\n7.5\n10Validation MLM Perplexity\nHidden Size\n256H\n512H\n768H\n1024H\n1536H\nEffect of RoBERTa Hidden Size\nFigure 4.Wider models converge faster than narrower models as function of both gradient steps (left plot) and wall-clock time (right plot).\nrange from 2048–16384. This aligns with the ﬁndings of\nMcCandlish et al. (2018): training efﬁciency is maximized\nwhen models are trained near some critical batch size.\nAn additional downside of increasing the batch size is that\nit requires simultaneously tuning the learning rate. On the\nother hand, scaling model size provides improvements in\ntraining efﬁciency without adjusting any hyperparameters.\nOverall, our results show that one should increase the batch\nsize (and learning rate) until the critical batch size region is\nreached and then to focus on increasing model size.\nLarger Models Are Not Harder to Finetune Although\nthe larger models minimize validation MLM perplexity\nfaster, one concern is that they may not minimize down-\nstream task error faster. For instance, larger models may\noverﬁt on small downstream datasets. We investigate this by\ntraining ROBERTA models of different sizes and stopping\nthem when they reach the same MLM perplexity (the larger\nmodels have been trained for less wall-clock time). We\nthen ﬁnetune each model using the ROBERTA ﬁnetuning\nhyperparameters (Liu et al., 2019b) on MNLI and SST-2.\nWe report the model accuracies in Table 2 in Appendix B.\nAll models reach comparable accuracies (in fact, the larger\nmodels typically outperform the smaller ones), which shows\nthat larger models are not more difﬁcult to ﬁnetune.\nReturns Diminish As Size Increases For both RoBERTa\nand MT, the largest models have reached the point where\nthey stop improving convergence with respect to wall-clock\ntime. For example, the largest model for MT (6L, 2048H)\nstarts to converge slower with respect to wall-clock time than\nthe second-largest model (6L, 1024H). These diminishing\nreturns occur because (1) the per-step convergence improve-\nments from using larger models decreases as the model gets\nlarger and (2) the computational overhead increases as our\nhardware becomes increasingly compute-bound. We further\nanalyze when and why returns diminish in Section 5.\n4. Larger Models Compress Better\nAlthough the most compute-efﬁcient training scheme is to\nuse larger models, this results in models which are lessinfer-\nence efﬁcient. Here, we demonstrate how to get the best of\nboth worlds. In particular, we show that since large models\nare more compressible than small models, they can outper-\nform small models while using similar inference costs.\n4.1. Compression Methodology and Evaluation\nCompression Methods Model compression methods re-\nduce the inference costs of trained models. For example,\nmodel compression can reduce inference latency to enable\nreal-time applications like simultaneous MT (See et al.,\n2016) or reduce memory usage to save energy for mobile de-\nvices (Han et al., 2016). We focus on compression methods\nwhich are fast to perform—methods which require signif-\nicant amounts of compute will negate the speedup from\nusing larger models.3 In particular, we consider two com-\npression techniques: quantization (Section 4.2) and pruning\n(Section 4.3), as well as their combination.4 Quantization\nstores model weights in low precision formats to (1) accel-\nerate operations when using hardware with reduced preci-\nsion support and (2) reduce overall memory footprint (Han\net al., 2016; Dong et al., 2019). Pruning sets neural network\nweights to zero to (1) remove operations and (2) reduce the\nmemory footprint when models are stored in sparse matrix\nformats (LeCun et al., 1990; Han et al., 2015). We apply\nboth quantization and pruning post-hoc to the ﬁnetuned\nmodels to limit the additional computational overhead.\n3For example, we avoid using model distillation methods be-\ncause they can add a signiﬁcant computational overhead (Sanh\net al., 2019; Turc et al., 2019) or cause a signiﬁcant degradation in\naccuracy (Liu et al., 2019a; Sun et al., 2019).\n4We also experiment with parameter sharing (Lan et al., 2020;\nDehghani et al., 2019)—tying the weights of the Transformer lay-\ners together—and ﬁnd that it slows convergence (see Appendix C).\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n0 100000 200000 300000\nNumber of Gradient Steps\n0.20\n0.25\n0.30\n0.35\n0.40Validation BLEU Model Size\n128H, 2L\n256H, 2L\n512H, 6L\n1024H, 6L\n2048H, 8L\nEffect of MT Model Size\n0 100000 200000 300000 400000\nWall Clock (Seconds)\n0.20\n0.25\n0.30\n0.35\n0.40Validation BLEU Model Size\n128H, 2L\n256H, 2L\n512H, 6L\n1024H, 6L\n2048H, 8L\nEffect of MT Model Size\nFigure 5.BLEU Scores on the English→French validation set (newstest2013) using models of different sizes. Larger models typically\nconverge faster as a function of both iterations (left plot) and wall-clock time (right plot). When models become too large (2048H, 6L),\nthey converge faster per iteration but their overhead on our limited hardware negates their convergence improvements.\nFinetuning Setup and Compression Evaluation We fo-\ncus on compressing the ﬁnetuned ROBERTA models as a\ncase study. We train models of different sizes for 1,000,000\nseconds,5 ﬁnetune them on MNLI/SST-2, and then apply\nquantization/pruning. For evaluation, even though pruning\nand quantization will improve inference latency/throughput,\nquantifying these improvements is challenging because they\nare highly hardware-dependent. Instead, we follow past\nwork and report the memory needed to store the model\nparameters (Thakker et al., 2019; Shen et al., 2020).\n4.2. Larger Models Are More Robust to Quantization\nWe quantize every parameter, including the embedding ma-\ntrix, but keep the model activations at full precision. We use\nﬂoating point precisions in {4, 6, 8, 32}bits (using lower\nthan 4-bits resulted in severe accuracy loss). We apply quan-\ntization post-hoc which adds no additional time.\nWe quantize uniformly: the range of ﬂoats is equally split\nand represented by unsigned integers in {0, . . . ,2k −1},\nwhere k is the precision. We accomplish this by quantizing\nthe weights W as:\nW′= Clamp(W, q0, q2k−1),\nWI = ⌊W′−q0\n∆ ⌉, where ∆ = q2k−1 −q0\n2k −1 ,\nQuantize(W) = ∆WI + q0,\nwhere Clamp() clamps all elements to the min/max range,\nWI is a set of integer indices, ⌊·⌉is the round operator, ∆\nis the distance between two adjacent quantized points, and\n[q0, q2k−1] indicates the quantization range.\n5We expect similar conclusions to hold for other budgets.\nResults The quantization results for MNLI are shown on\nthe left of Figure 6 (SST-2 results are in Appendix D). We\nplot each model’s accuracy at different quantization levels\nas a function of its total memory usage. The larger models\nare more robust to quantization than the smaller models (the\naccuracy drop is smaller when the precision is reduced).\nHence, the models which are trained using large parame-\nter counts and then heavily quantized achieve the highest\naccuracy for almost all memory budgets.\n4.3. Larger Models Are More Robust to Pruning\nWe use iterative magnitude pruning (Str¨om, 1997; Han et al.,\n2016): we iteratively zero out the smallest magnitude param-\neters and continue ﬁnetuning the model on the downstream\ntask to recover lost accuracy.\nConcretely, we consider models with sparsity levels of 15%,\n30%, 45%, 60%, 75%, and 90%. We ﬁrst ﬁnd the 15% of\nweights with the smallest magnitude and set them to zero.6\nWe then ﬁnetune the model on the downstream task until\nit reaches within 99.5% of its original validation accuracy\nor until we reach one training epoch. We then repeat this\nprocess—we prune another 15% of the smallest magnitude\nweights and ﬁnetune—stopping when we reach the desired\nsparsity level. The additional training overhead from this it-\nerative process is small because the model typically recovers\nits accuracy in signiﬁcantly less than one epoch (sometimes\nit does not require any retraining to maintain 99.5%). For\nexample, pruning to 45% can be done with one or two addi-\ntional epochs of ﬁnetuning on MNLI.\n6It also may be possible to remove entire attention heads in\naddition to zeroing out weights (Michel et al., 2019; V oita et al.,\n2019). This may further improve our compression results.\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n0 500 1000 1500\nMemory Usage (MB)\n0.65\n0.70\n0.75\n0.80\n0.85MNLI Validation Accuracy\n32b8b\n6b\n32b8b\n32b8b6b\n4b\n32b\n32b8b6b\nOriginal Size\na 3 Layers, 768H\na 6 Layers, 768H\na 12 Layers, 768H\na 18 Layers, 768H\na 24 Layers, 768H\na 12 Layers, 256H\na 12 Layers, 512H\na 12 Layers, 1024H\na 12 Layers, 1536H\nRoBERTa Quantization\n0 100 200 300\nNumber of Parameters (Millions)\n0.70\n0.75\n0.80\n0.85MNLI Validation Accuracy\n0%45%\n0%30%\n45%   \n0%15%30%45%60%75%\n90%\n60%     \n75%     \n90%     \n60%    \n75%    \n90%    \n0%\n15%30%\nOriginal Size\na 3 Layers, 768H\na 6 Layers, 768H\na 12 Layers, 768H\na 18 Layers, 768H\na 24 Layers, 768H\na 12 Layers, 256H\na 12 Layers, 512H\na 12 Layers, 1024H\na 12 Layers, 1536H\nRoBERTa Pruning\nFigure 6.We ﬁrst pretrain ROBERTA models of different sizes for the same total wall-clock time (larger models are trained for fewer\nsteps). We then ﬁnetune each model on MNLI and compress them using quantization (left) and pruning (right). For most budgets (x-axis),\nthe highest accuracy models are the ones which are trained large and then heavily compressed. The labels above each point indicate the\ncompression amount (e.g., 4-bit quantization or 45% sparsity); we omit cluttered labels. SST-2 results are shown in Appendix D.\nResults The pruning results for MNLI are shown in the\nright of Figure 6. We report the model’s accuracy as a\nfunction of the total number of nonzero parameters.7 The\nlarger models can be pruned more than the smaller models\nwithout signiﬁcantly hurting accuracy. Consequently, the\nlarge, heavily pruned models provide the best accuracy-\nefﬁciency trade-off. We ﬁnd that deep networks are more\nrobust to pruning than wider networks, e.g., the 24 Layer,\n768H model outperforms the 12 Layer, 1536H model at\nmost test budgets.\nCombining Quantization and Pruning Results Pruning\nand quantization are complementary techniques for com-\npressing Transformer models. We ﬁrst prune models to\nvarious sparsity levels (e.g., 15%, 30%, etc.) and then apply\nvarying amounts of quantization (e.g., 8-bit, 4-bit, etc.) to\neach model. In Figure 7 we plot combinations of pruning\nand quantization that lie at or near the Pareto frontier. Large\nmodels that are heavily compressed still provide the best\ntrade-off between accuracy and efﬁciency when leveraging\nboth pruning and quantization. A particularly strong com-\npression method is to prune 30-40% of the weights and then\nquantize the model to 6-8 bits.\n4.4. Convergence Does Not Affect Compressibility\nAlthough larger Transformer models are more compress-\nible, there is a confounding factor that our larger models\nare also less converged on the pretraining task. Is it the\nlarger model size or the lack of convergence that causes the\nenhanced compressibility? We investigate this by ﬁnetun-\n7Since the reduction in memory from storing sparse matrices is\nhighly dependent on the data structure used, we follow past work\nand report the number of nonzero model parameters (Luo et al.,\n2017; Li et al., 2017).\ning ROBERTA models starting from different pretraining\ncheckpoints (e.g., 3 epochs, 6 epochs, etc.) on MNLI. We\nthen quantize the models to 4-bits.\nFigure 8 shows the results. Quantization is hardly affected\nby pretraining convergence—the drop in accuracy between\nthe full precision and the 4-bit precision MNLI models is\ncomparable as the pretrained model becomes more con-\nverged. Instead, the factor that determines compressibility\nis model size—the drop in accuracy is very large when\ncompressing smaller models and vice versa.\n5. When and Why Are Larger Models Better?\nThis section presents results and discussion on why larger\nTransformer models train faster and compress better.\n5.1. Better Sample Efﬁciency With Larger Models\nFor larger models to train faster, they must converge faster\n(w.r.t. test error) per iteration. While there is a robust\nliterature studying why larger models achieve better ﬁnal\ntest accuracy,8 there is considerably less work exploring\nif and why larger models converge faster. One initial step\nin this direction is Arora et al. (2018a), who show that for\ndeep linear neural networks, increasing depth can promote\nmovement along directions already taken by the optimizer.\n8Chieﬂy, this work seeks to reconcile the conﬂict between\nmodern deep learning practice and the classical bias-variance trade-\noff. For instance, it studies forms of implicit regularization (Zhang\net al., 2017; Belkin et al., 2018), characterizes the expressivity of\ndeep models (Raghu et al., 2017; Lu et al., 2017), and bounds the\nneural network generalization error (Du et al., 2019; Arora et al.,\n2018b).\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n0 50 100 150\nBits x Parameter Count (Bits x Millions)\n0.70\n0.75\n0.80\n0.85MNLI Validation Accuracy\nOriginal Size\n3 Layers, 768H\n6 Layers, 768H\n12 Layers, 768H\n18 Layers, 768H\n24 Layers, 768H\n12 Layers, 256H\n12 Layers, 512H\n12 Layers, 1024H\n12 Layers, 1536H\nRoBERTa Quantization + Pruning\nFigure 7.We combine pruning and quantization and ﬁnd their gains\nto be complementary. The models which are trained large and then\ncompressed are the best performing for each test-time budget.\nFast Minimization and the Role of Overﬁtting One em-\npirical reason for the acceleration in convergence is that\nlarger Transformer models minimize the training error faster.\nAnd, since the generalization gap is small for our tasks due\nto very large training sets, the larger models also converge\nfaster w.r.t test error. In fact, the challenge in the MLM task\nis not overﬁtting, but instead, it is ﬁtting the data—even 8\nbillion parameter models do not overﬁt to large pretraining\ncorpora (Shoeybi et al., 2019).\nWhen overﬁtting is a concern, larger models start to con-\nverge slower (w.r.t test error). We demonstrate this by ran-\ndomly subsampling our pretraining dataset to 5% and 1%\nof its original size and training ROBERTA models of var-\nious sizes. When subsampling the data to 5% (top row of\nFigure 14 in Appendix A), the largest models do not im-\nprove on the training time of the smaller models (e.g., 12\nlayer ROBERTA trains just as fast as a 24 layerROBERTA).\nMoreover, when the data is subsampled to 1% (bottom row\nof Figure 14), the largest models are worse in terms of\nperplexity due to overﬁtting. Thus, although our main con-\nclusion that increasing model size accelerates convergence\nstill holds for the smaller models (e.g., the 12 layer model\noutperforms the 3 layer one), overﬁtting causes it to break\ndown for the largest models.\n5.2. Manageable Compute Costs for Large Models\nFor larger models to train faster with respect to wall-clock\ntime, their convergence improvements must not be negated\nby their slowdown in per-iteration time. Fortunately, par-\nallel hardware (e.g., GPUs, TPUs) is usually not compute\nbound when training deep learning models. Instead, mem-\nory storage/movement is the limiting factor in image classi-\nﬁcation (Gomez et al., 2017), semantic segmentation (Chen\n30000 60000 90000\nNumber of Pretraining Gradient Steps\n0.72\n0.76\n0.80\n0.84MNLI Validation Accuracy\nModel Size\n24 Layers, 32-bit\n24 Layers, 4-bit\n12 Layers, 32-bit\n12 Layers, 4-bit\n6 Layers, 32-bit\n6 Layers, 4-bit\nEffect of Pretraining Convergence on Quantization\nFigure 8.We disentangle whether model size or pretraining con-\nvergence causes the enhanced compressibility of larger models.\nWe ﬁnetune ROBERTA models starting from different pretrain-\ning checkpoints on MNLI. We then quantize the models to 4-bits.\nQuantization is hardly affected by convergence—the drop in MNLI\naccuracy due to quantization is comparable as the pretrained model\nbecomes more converged. Instead, the factor that determines com-\npressibility is model size—the drop in accuracy is very large when\ncompressing smaller models and vice versa.\net al., 2017), language modeling (Kitaev et al., 2020), and\nother tasks (Jain et al., 2020). Thus, larger models will\nmore fully utilize the available compute, causing their slow-\ndown to be sublinear. Moreover, when larger models cause\nhardware to run out of memory, gradient accumulation can\ntrade-off memory for compute while still preserving the\ngains of large models, as shown in our experiments.\n5.3. Smaller Compression Error for Larger Models\nLarge transformer models are more compressible than small\ntransformer models.9 Here, we present initial experiments\nto better understand why this occurs.\nQuantization Error is Smaller for Larger Models We\nﬁrst measure the quantization error—the difference between\nthe full-precision and low-precision weights—for the 4-bit\nROBERTA models. On the left of Figure 9, we plot this\nvalue for models of varying depths (6, 12, and 24 layers)\naveraged across different Transformer modules (e.g., in-\nprojection matrix of the self-attention). The mean and vari-\nance of the quantization error are smaller for deeper models.\nPruning Error is Smaller for Larger Models Similarly,\nwe measure the pruning error—the difference between the\n9Similar ﬁndings hold for large but sparse audio synthesis\nmodels (Kalchbrenner et al., 2018) and convolutional models for\ncomputer vision (Zhu & Gupta, 2018; Elsen et al., 2019; Evci et al.,\n2020; Kusupati et al., 2020).\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\nSelf-attention \n In Projection\nSelf-attention \n Out Projection\nFeed-forward \n In Projection\nFeed-forward \n Out Projection\n0.5\n1.0\n1.5\n2.0\n2.5Average Floating Point Difference\nRoBERTa Quantization Error\n6 Layers\n12 Layers\n24 Layers\nSelf-attention \n In Projection\nSelf-attention \n Out Projection\nFeed-forward \n In Projection\nFeed-forward \n Out Projection\n0.5\n1.0\n1.5\n2.0\n2.5Average Floating Point Difference\nRoBERTa Pruning Error\n6 Layers\n12 Layers\n24 Layers\nFigure 9.We ﬁnetune ROBERTA models of different sizes (6 layers, 12 layers, and 24 layers) on MNLI. We then quantize models to 4-bits\nor prune models to 60% sparsity. We plot the difference between the weights of the original and the quantized/pruned models averaged\nacross different modules in the Transformer. The mean and variance of the weight difference after quantization (left) is consistently lower\nfor the deeper models compared to the shallower models. The same holds for the difference after pruning (right). This shows that the\nlarger model’s weights are naturally easier to approximate with low-precision / sparse matrices than smaller models.\noriginal weights and the sparse weights—for the 60% sparse\nROBERTA models. The mean and variance of the pruning\nerror are smaller for deeper models (Figure 9, right).\nThese two results show that the larger model’s weights are\nmore easily approximated by low-precision or sparse matri-\nces. Interestingly, this phenomenon naturally occurs without\ndirectly optimizing for it; an area for future work is to study\nwhy these weight patterns emerge in larger models.\nConnection to the Lottery Ticket Hypothesis Our com-\npression ﬁndings have deep connections to recent conjec-\ntures such as the lottery ticket hypothesis (Frankle & Carbin,\n2019). The lottery ticket hypothesis argues that larger mod-\nels are preferable as they have a higher chance of ﬁnding a\nlucky initialization in one of their subnetworks. Our work\nshows that, for certain accuracies, as models become in-\ncreasingly large, they contain increasingly small subnet-\nworks which achieve that accuracy.\n6. Related Work\nImproving Training Speed and Efﬁciency There is a\nlarge body of work on accelerating model training, tradi-\ntionally accomplished via improved optimizers (Nesterov,\n1983; Kingma & Ba, 2015). More recent work improves\ntraining efﬁciency by modifying loss functions (Clark et al.,\n2020), model structures/sparsities (Louizos et al., 2018;\nGong et al., 2019; Tan & Le, 2019), backpropagation stor-\nage requirements (Gruslys et al., 2016), or learning rate\nschedules (Loshchilov & Hutter, 2017; Li et al., 2020). We\nstudy the impact of model size, which is largely orthogonal\nto these other training efﬁciency improvements.\nScaling Model TrainingAnother line of work scales model\ntraining to large amounts of distributed hardware and ad-\ndresses the associated systems and machine learning chal-\nlenges (Goyal et al., 2017; Ott et al., 2018; You et al., 2020).\nOur work instead looks to choose the optimal model size\nfor a ﬁxed (small) hardware budget. Future work can study\nwhether our conclusion that large models are more compute-\nefﬁcient also holds in this highly-distributed setting, where\nthe “budget” is extremely large.\nHyperparameter Tuning and AutoML In our work, we\nhave an initial setting for the hyperparameters and optimize\nthe model size. However, good initial models and hyper-\nparameters are unknown when approaching new problems.\nFor these cases, the optimal training strategy must consider\nthe cost of experimenting with different architectures and\nhyperparameters; future work can study the effect of model\nsize in this setting. More generally, our ﬁndings may impact\nthe design of automated methods for solving/optimizing\nmachine learning problems (Feurer et al., 2015; Zoph & Le,\n2017; Jaderberg et al., 2017). In particular, the compute-\nefﬁciency of these methods may improve by following our\ntrain large, then compress methodology.\nTraining Efﬁciency of Large Models Recent and concur-\nrent work also considers the impact of model size on the\ncompute efﬁciency of training. Raffel et al. (2019) show that\ntraining a 4x larger Transformer model is a good usage of 4x\nmore compute. Ardalani et al. (2019) show that larger RNN\nmodels take fewer gradient iterations to converge but do not\nconsider that larger models are faster when adjusting for\nwall-clock time. In concurrent work, Kaplan et al. (2020)\nstudy the impact of model size on the training efﬁciency of\nTransformer language models. They make similar conclu-\nsions that large, undertrained models are superior to small,\nwell-trained models. Our work differs in that we study ma-\nchine translation and the impact of training large models on\ndownstream tasks (model ﬁnetuning and compression).\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n7. Conclusion and Future Work\nWe studied the impact of Transformer model size on the\nefﬁciency of training and inference. We show that increasing\nmodel width and depth accelerates convergence in terms of\nboth gradient steps and wall-clock time. Moreover, even\nthough large models appear less efﬁcient during inference,\nwe demonstrate that they are more robust to compression.\nTherefore, we conclude that the best strategy for resource-\nconstrained training is totrain large models and then heavily\ncompress them.\nIn the future, we will examine these conclusions on more\ndomains such as computer vision. Moreover, we look to\nanswer the questions that are raised by our results: why\ndo larger transformer models train fast and compress well,\nhow does model size impact overﬁtting and hyperparameter\ntuning, and more generally, what other common design deci-\nsions should be rethought in the compute-efﬁcient setting?\nAcknowledgements\nThis research was supported by the Berkeley RISE Lab.\nWe would like to thank the Google Cloud TPU team for\ntheir hardware support. We are also grateful to Shi Feng,\nYang Liu, Suchin Gururangan, Nelson Liu, the members of\nBerkeley NLP, and the members of the Berkeley RISE Lab\nfor their valuable feedback.\nReferences\nArdalani, N., Hestness, J., and Diamos, G. Empirically char-\nacterizing overparameterization impact on convergence.\nOpenReview: S1lPShAqFm, 2019.\nArora, S., Cohen, N., and Hazan, E. On the optimization of\ndeep networks: Implicit acceleration by overparameteri-\nzation. In ICML, 2018a.\nArora, S., Ge, R., Neyshabur, B., and Zhang, Y . Stronger\ngeneralization bounds for deep nets via a compression\napproach. In ICML, 2018b.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\nIn NeurIPS, 2016.\nBelkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling\nmodern machine learning and the bias-variance trade-off.\nIn PNAS, 2018.\nBrock, A., Donahue, J., and Simonyan, K. Large scale GAN\ntraining for high ﬁdelity natural image synthesis. InICLR,\n2019.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and\nYuille, A. L. DeepLab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and\nfully connected CRFs. In TPAMI, 2017.\nClark, K., Luong, M.-T., Le, Q. V ., and Manning, C. D.\nELECTRA: Pre-training text encoders as discriminators\nrather than generators. In ICLR, 2020.\nCrankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gon-\nzalez, J. E., and Stoica, I. Clipper: A low-latency online\nprediction serving system. In NSDI, 2017.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, Ł. Universal transformers. In ICLR, 2019.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In NAACL, 2019.\nDong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and\nKeutzer, K. HAWQ: Hessian aware quantization of neural\nnetworks with mixed-precision. In ICCV, 2019.\nDu, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient\ndescent provably optimizes over-parameterized neural\nnetworks. In ICLR, 2019.\nElsen, E., Dukhan, M., Gale, T., and Simonyan, K. Fast\nsparse convnets. arXiv preprint arXiv:1911.09723, 2019.\nEvci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E.\nRigging the lottery: Making all tickets winners. In ICML,\n2020.\nFeurer, M., Klein, A., Eggensperger, K., Springenberg, J.,\nBlum, M., and Hutter, F. Efﬁcient and robust automated\nmachine learning. In NeurIPS, 2015.\nFrankle, J. and Carbin, M. The lottery ticket hypothesis:\nFinding sparse, trainable neural networks. In ICLR, 2019.\nGomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The\nreversible residual network: Backpropagation without\nstoring activations. In NeurIPS, 2017.\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\nEfﬁcient training of BERT by progressively stacking. In\nICML, 2019.\nGoyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He,\nK. Accurate, large minibatch SGD: Training ImageNet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nGruslys, A., Munos, R., Danihelka, I., Lanctot, M., and\nGraves, A. Memory-efﬁcient backpropagation through\ntime. In NeurIPS, 2016.\nHan, S., Pool, J., Tran, J., and Dally, W. Learning both\nweights and connections for efﬁcient neural network. In\nNeurIPS, 2015.\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\nHan, S., Mao, H., and Dally, W. J. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding. In ICLR, 2016.\nHnaff, O. J., Srinivas, A., Fauw, J. D., Razavi, A., Doer-\nsch, C., Eslami, S. M. A., and van den Oord, A. Data-\nefﬁcient image recognition with contrastive predictive\ncoding. arXiv preprint arXiv:1905.09272, 2019.\nJaderberg, M., Dalibard, V ., Osindero, S., Czarnecki, W. M.,\nDonahue, J., Razavi, A., Vinyals, O., Green, T., Dunning,\nI., Simonyan, K., et al. Population based training of\nneural networks. arXiv preprint arXiv:1711.09846, 2017.\nJain, P., Jain, A., Nrusimha, A., Gholami, A., Abbeel, P.,\nKeutzer, K., Stoica, I., and Gonzalez, J. E. Checkmate:\nBreaking the memory wall with optimal tensor remateri-\nalization. In MLSys, 2020.\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,\nG., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,\nA., et al. In-datacenter performance analysis of a tensor\nprocessing unit. In ISCA, 2017.\nKalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,\nCasagrande, N., Lockhart, E., Stimberg, F., Oord, A.\nv. d., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural\naudio synthesis. In ICML, 2018.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In ICLR, 2015.\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer: The\nefﬁcient transformer. In ICLR, 2020.\nKusupati, A., Ramanujan, V ., Somani, R., Wortsman, M.,\nJain, P., Kakade, S., and Farhadi, A. Soft threshold weight\nreparameterization for learnable sparsity. In ICML, 2020.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\nand Soricut, R. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In ICLR, 2020.\nLeCun, Y ., Denker, J. S., and Solla, S. A. Optimal brain\ndamage. In NeurIPS, 1990.\nLi, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H. P.\nPruning ﬁlters for efﬁcient convnets. In ICLR, 2017.\nLi, M., Yumer, E., and Ramanan, D. Budgeted training:\nRethinking deep neural network training under resource\nconstraints. In ICLR, 2020.\nLiu, L., Wang, H., Lin, J., Socher, R., and Xiong, C. Atten-\ntive student meets multi-task teacher: Improved knowl-\nedge distillation for pretrained models. arXiv preprint\narXiv:1911.03588, 2019a.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692, 2019b.\nLoshchilov, I. and Hutter, F. SGDR: Stochastic gradient\ndescent with warm restarts. In ICLR, 2017.\nLouizos, C., Welling, M., and Kingma, D. P. Learning\nsparse neural networks through L0 regularization. In\nICLR, 2018.\nLu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres-\nsive power of neural networks: A view from the width.\nIn NeurIPS, 2017.\nLuo, J.-H., Wu, J., and Lin, W. ThiNet: A ﬁlter level pruning\nmethod for deep neural network compression. In ICCV,\n2017.\nMcCandlish, S., Kaplan, J., Amodei, D., and Team, O. D.\nAn empirical model of large-batch training. arXiv\npreprint arXiv:1812.06162, 2018.\nMetz, C. Building an AI chip saved Google from building a\ndozen new data centers. Wired, 2017.\nMichel, P., Levy, O., and Neubig, G. Are sixteen heads\nreally better than one? In NeurIPS, 2019.\nNesterov, Y . A method of solving a convex programming\nproblem with convergence rate O(1/k2). In Soviet Math-\nematics Doklady, 1983.\nOtt, M., Edunov, S., Grangier, D., and Auli, M. Scaling\nneural machine translation. In WMT, 2018.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N.,\nGrangier, D., and Auli, M. Fairseq: A fast, extensible\ntoolkit for sequence modeling. In NAACL Demo, 2019.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. BLEU:\na method for automatic evaluation of machine translation.\nIn ACL, 2002.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nRaghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Dick-\nstein, J. S. On the expressive power of deep neural net-\nworks. In ICML, 2017.\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\nSanh, V ., Debut, L., Chaumond, J., and Wolf, T. Distilbert,\na distilled version of BERT: smaller, faster, cheaper and\nlighter. In NeurIPS EMC 2 Workshop, 2019.\nSchwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. Green\nAI. arXiv preprint arXiv:1907.10597, 2019.\nSee, A., Luong, M.-T., and Manning, C. D. Compression\nof neural machine translation models via pruning. In\nCoNLL, 2016.\nShazeer, N., Cheng, Y ., Parmar, N., Tran, D., Vaswani,\nA., Koanantakool, P., Hawkins, P., Lee, H., Hong, M.,\nYoung, C., et al. Mesh-TensorFlow: Deep learning for\nsupercomputers. In NeurIPS, 2018.\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A.,\nMahoney, M. W., and Keutzer, K. Q-BERT: Hessian\nbased ultra low precision quantization of BERT. In AAAI,\n2020.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,\nand Catanzaro, B. Megatron-LM: Training multi-billion\nparameter language models using GPU model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nEMNLP, 2013.\nStr¨om, N. Sparse connection and pruning in large dynamic\nartiﬁcial neural networks. In EUROSPEECH, 1997.\nSun, S., Cheng, Y ., Gan, Z., and Liu, J. Patient knowledge\ndistillation for BERT model compression. In EMNLP,\n2019.\nTan, M. and Le, Q. V . EfﬁcientNet: Rethinking model scal-\ning for convolutional neural networks. In ICML, 2019.\nThakker, U., Beu, J., Gope, D., Zhou, C., Fedorov, I., Dasika,\nG., and Mattina, M. Compressing RNNs for IOT de-\nvices by 15-38x using kronecker products. arXiv preprint\narXiv:1906.02876, 2019.\nTurc, I., Chang, M.-W., Lee, K., and Toutanova, K. Well-\nread students learn better: The impact of student ini-\ntialization on knowledge distillation. arXiv preprint\narXiv:1908.08962, 2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention\nis all you need. In NeurIPS, 2017.\nVaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez,\nA. N., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner,\nN., Parmar, N., et al. Tensor2Tensor for neural machine\ntranslation. In AMTA, 2018.\nV oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.\nAnalyzing multi-head self-attention: Specialized heads\ndo the heavy lifting, the rest can be pruned. In ACL, 2019.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. In\nICLR, 2019a.\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F.,\nand Chao, L. S. Learning deep transformer models for\nmachine translation. In ACL, 2019b.\nWilliams, A., Nangia, N., and Bowman, S. R. A broad-\ncoverage challenge corpus for sentence understanding\nthrough inference. In NAACL, 2018.\nYou, Y ., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli,\nS., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-\nJ. Large batch optimization for deep learning: Training\nBERT in 76 minutes. In ICLR, 2020.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.\nUnderstanding deep learning requires rethinking general-\nization. In ICLR, 2017.\nZhu, M. and Gupta, S. To prune, or not to prune: exploring\nthe efﬁcacy of pruning for model compression. In ICLR\nWorkshop Track, 2018.\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. In CVPR, 2015.\nZoph, B. and Le, Q. V . Neural architecture search with\nreinforcement learning. In ICLR, 2017.\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\nA. Additional Training Curves\nA.1. Training Cost Using FLOPs\nIn Figure 10, we plot selected learning curves from the\nmain text as a function of FLOPs rather than seconds. We\ncompute FLOPs using the code provided by Clark et al.\n(2020).\nA.2. The Impact of Batch Size\nFigure 13 shows the learning curves associated with differ-\nent batch sizes. Table 1 shows the learning rates associated\nwith each batch size. We use the hyperparameters from Liu\net al. (2019b) as a starting point and then lightly tune them.\nBatch Size Learning Rate\n256 .0002\n2048 .001\n4096 .00125\n8192 .0015\n16384 .001875\nTable 1.The learning rate for each batch size in Figure 13.\nA.3. The Impact of Dataset Size\nFigure 14 shows the learning curves for models trained\nusing 5% and 1% of the training data.\nB. Finetuning Models of Different Sizes\nTable 2 shows that models with more parameters are not\nharder to ﬁnetune.\nModel Perplexity MNLI SST-2\n12-layer, 768H 4.3 84.3 93.0\n18-layer, 768H 4.1 85.4 92.6\n24-layer, 768H 4.0 85.2 93.1\n12-layer, 768H 4.3 84.3 93.0\n12-layer, 1024H 3.9 85.5 93.2\n12-layer, 1536H 4.3 85.1 93.8\nTable 2.We train ROBERTA models of different sizes and stop\nthem at roughly the same pretraining perplexity (the bigger models\nare trained for less wall-clock time). We then ﬁnetune each model\non MNLI and SST-2. All models reach comparable accuracies (in\nfact, the big models often outperform small ones), which shows\nthat larger models are not harder to ﬁnetune.\nC. Negative Results: Layer Sharing\nSharing weights across transformer layers can provide a\nsmall or negligible degradation in ﬁnal performance (Lan\net al., 2020; Dehghani et al., 2019) while providing a re-\nduction in memory consumption. In addition, models with\nshared layers are slightly faster to execute because they\nrequire less memory movement and reduced inter-device\ncommunication. Similar to Lan et al. (2020), we experi-\nment with two types of layer sharing: sharing all layers and\nsharing only the attention layers.\nSharing layers reduces the maximum memory requirements,\nespecially for small batch sizes. For example, sharing all\nthe layers of a ROBERTA model with batch size 32 re-\nduces total memory usage by 41%. However, both forms of\nsharing lead to slower training convergence and thus worse\nperformance in the resource-constrained setting (Figure 11).\nConsequently, we do not recommend sharing layers for\ncompute-efﬁcient training or inference of transformers.\n0 20000 40000 60000 80000\nNumber of Gradient Steps\n4\n6\n8\n10Validation MLM Perplexity\nSharing Type\nAttention Shared\nNo Sharing\nEffect of RoBERTa Layer Sharing\nFigure 11.Sharing attention layers reduces the maximum memory\nconsumption of ROBERTA but causes slower convergence and\nworse ﬁnal accuracy.\nD. Compression Results for SST-2\nWe follow Liu et al. (2019b) and report results on SST-\n2 (Socher et al., 2013) in addition to MNLI. Since the\nSST-2 dataset is smaller than MNLI it requires a more\nsigniﬁcant tuning of the ﬁnetuning hyperparameters. We\ntune the batch size in {16, 32, 64}, the learning rate in\n{5e−4, 3e−4, 1e−4}, the seed which controls the classiﬁer\ninitialization and training data shufﬂing in {100, 300, 500},\nand the dropout in {0.1, 0.2, 0.3}. We choose the best value\nusing the validation set for each model size. We then per-\nform quantization, pruning, and quantization and pruning\non all ﬁnetuned models. Similar to MNLI, the bigger mod-\nels provide the highest accuracy for a given test budget\n(Figure 12).\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n0 20 40 60\nexaFLOPS\n4\n6\n8\n10Validation MLM Perplexity\nModel Depth\n3 Layers\n6 Layers\n12 Layers\n18 Layers\n24 Layers\nEffect of RoBERTa Depth\n0 20 40 60\nexaFLOPS\n5\n7.5\n10Validation MLM Perplexity\nModel Depth\n256H\n512H\n768H\n1024H\n1536H\nEffect of RoBERTa Hidden Size\n0 20 40 60\nexaFLOPs\n4\n5\n6\n7\n8\n9Validation MLM Perplexity\nBatch Size\n256\n2048\n4096\n8192\n16384\nEffect of RoBERTa Batch Size\nFigure 10.Floating Point Operations. We show Figures 2, 4, and 13 in terms of exaFLOPs instead of wall-clock time. Bigger models\nachieve better results than smaller models using the same number of ﬂoating point operations.\n0 500 1000 1500\nMemory Usage (MB)\n0.88\n0.90\n0.92\n0.94SST-2 Validation Accuracy\nOriginal Size\n3 Layers, 768H\n6 Layers, 768H\n12 Layers, 768H\n18 Layers, 768H\n24 Layers, 768H\n12 Layers, 512H\n12 Layers, 1024H\n12 Layers, 1536H\nRoBERTa Quantization\n0 100 200 300\nNumber of Parameters (Millions)\n0.850\n0.875\n0.900\n0.925SST-2 Validation Accuracy\nOriginal Size\n3 Layers, 768H\n6 Layers, 768H\n12 Layers, 768H\n18 Layers, 768H\n24 Layers, 768H\n12 Layers, 512H\n12 Layers, 1024H\n12 Layers, 1536H\nRoBERTa Pruning\n0 50 100\nBits x Parameter Count (Bits x Millions)\n0.850\n0.875\n0.900\n0.925SST-2 Validation Accuracy\nOriginal Size\n3 Layers, 768H\n6 Layers, 768H\n12 Layers, 768H\n18 Layers, 768H\n24 Layers, 768H\n12 Layers, 512H\n12 Layers, 1024H\n12 Layers, 1536H\nRoBERTa Quantization + Pruning\nFigure 12.Compression for SST-2. For most budgets (x-axis), the highest accuracy SST-2 models are the ones which are trained large and\nthen heavily compressed. We show results for quantization (left), pruning (center), and quantization and pruning (right).\n0 40000 80000 120000\nNumber of Gradient Steps\n4\n5\n6\n7\n8\n9Validation MLM Perplexity\nBatch Size\n256\n2048\n4096\n8192\n16384\nEffect of RoBERTa Batch Size\n0 500000 1000000 1500000\nWall Clock (Seconds)\n4\n5\n6\n7\n8\n9Validation MLM Perplexity\nBatch Size\n256\n2048\n4096\n8192\n16384\nEffect of RoBERTa Batch Size\nFigure 13.Increasing the batch size and the associated learning rate accelerates convergence in terms of gradient steps. However,\nincreasing the batch size beyond 2048 provides only marginal improvements with respect to wall-clock time. Note that the wall-clock\ntime includes the cost of accumulating gradients on a single machine (see Section 2.2). In other words, beyond a certain point increasing\nthe batch size only provides speedups when additional hardware is available. The 256 batch size result is far to the right in the left plot.\nRethinking Model Size for Efﬁcient Training and Inference of Transformers\n0 25000 50000 75000 100000\nNumber of Gradient Steps\n7.5\n10\n12.5\n15\nValidation MLM Perplexity\nModel Size\n256H, 12L\n768H, 3L\n768H, 12L\n768H, 24L\nEffect of RoBERTa Model Size with 5% Data\n0 100000 200000 300000 400000 500000\nWall Clock (Seconds)\n7.5\n10\n12.5\n15\nValidation MLM Perplexity\nModel Size\n256H, 12L\n768H, 3L\n768H, 12L\n768H, 24L\nEffect of RoBERTa Model Size with 5% Data\n0 10000 20000 30000 40000\nNumber of Gradient Steps\n10\n15\n20\n25Validation MLM Perplexity\nModel Size\n256H, 12L\n768H, 3L\n768H, 12L\n768H, 24L\nEffect of RoBERTa Model Size with 1% Data\n0 100000 200000 300000\nWall Clock (Seconds)\n10\n15\n20\n25Validation MLM Perplexity\nModel Size\n256H, 12L\n768H, 3L\n768H, 12L\n768H, 24L\nEffect of RoBERTa Model Size with 1% Data\nFigure 14.Effect of Smaller Datasets. In our experiments on the full dataset (see main text), the largest models we trained are always faster\nin terms of wall-clock time. However, when subsampling the data to 5% (top row), the biggest models do not improve on the speed of the\nsmaller models (e.g., compare 24 Layer ROBERTA and 12 Layer ROBERTA). When the data is subsampled to 1% (bottom row), the\nbigger models are worse in terms of perplexity due to overﬁtting. This illustrates that the optimal model size depends on the dataset size.",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.7831333875656128
    },
    {
      "name": "Transformer",
      "score": 0.689011812210083
    },
    {
      "name": "Computer science",
      "score": 0.5165380835533142
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4519514739513397
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39210042357444763
    },
    {
      "name": "Machine learning",
      "score": 0.38073334097862244
    },
    {
      "name": "Engineering",
      "score": 0.24642160534858704
    },
    {
      "name": "Electrical engineering",
      "score": 0.13552376627922058
    },
    {
      "name": "Geography",
      "score": 0.08296853303909302
    },
    {
      "name": "Voltage",
      "score": 0.06689533591270447
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 51
}