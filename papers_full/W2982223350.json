{
    "title": "Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders",
    "url": "https://openalex.org/W2982223350",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4221541405",
            "name": "Liu, Andy T.",
            "affiliations": [
                "National Taiwan University"
            ]
        },
        {
            "id": "https://openalex.org/A2223365993",
            "name": "Yang Shu-wen",
            "affiliations": [
                "National Taiwan University"
            ]
        },
        {
            "id": null,
            "name": "Chi, Po-Han",
            "affiliations": [
                "National Taiwan University"
            ]
        },
        {
            "id": "https://openalex.org/A4224974287",
            "name": "Hsu, Po-chun",
            "affiliations": [
                "National Taiwan University"
            ]
        },
        {
            "id": "https://openalex.org/A4221356874",
            "name": "Lee, Hung-Yi",
            "affiliations": [
                "National Taiwan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2947445680",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W2964089206",
        "https://openalex.org/W2972451902",
        "https://openalex.org/W6737778391",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W6768021236",
        "https://openalex.org/W2963571336",
        "https://openalex.org/W2963425185",
        "https://openalex.org/W2972943112",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2979476256",
        "https://openalex.org/W2973049979",
        "https://openalex.org/W3100270690",
        "https://openalex.org/W2998649947",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2747874407",
        "https://openalex.org/W6674330103",
        "https://openalex.org/W2883409523",
        "https://openalex.org/W2336585117",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2996383576",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3125709657",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W2943493972",
        "https://openalex.org/W179875071",
        "https://openalex.org/W4297808394"
    ],
    "abstract": "We present Mockingjay as a new speech representation learning approach, where\\nbidirectional Transformer encoders are pre-trained on a large amount of\\nunlabeled speech. Previous speech representation methods learn through\\nconditioning on past frames and predicting information about future frames.\\nWhereas Mockingjay is designed to predict the current frame through jointly\\nconditioning on both past and future contexts. The Mockingjay representation\\nimproves performance for a wide range of downstream tasks, including phoneme\\nclassification, speaker recognition, and sentiment classification on spoken\\ncontent, while outperforming other approaches. Mockingjay is empirically\\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\\nfurther improve performance dramatically. In a low resource setting with only\\n0.1% of labeled data, we outperform the result of Mel-features that uses all\\n100% labeled data.\\n",
    "full_text": "MOCKINGJAY: UNSUPERVISED SPEECH REPRESENTATION LEARNING WITH DEEP\nBIDIRECTIONAL TRANSFORMER ENCODERS\nAndy T. Liu Shu-wen Yang Po-Han Chi Po-chun Hsu Hung-yi Lee\nNational Taiwan University\nCollege of Electrical Engineering and Computer Science\n{r07942089, r08944041, r08942074, r07942095, hungyilee}@ntu.edu.tw\nABSTRACT\nWe present Mockingjay as a new speech representation learn-\ning approach, where bidirectional Transformer encoders are\npre-trained on a large amount of unlabeled speech. Previ-\nous speech representation methods learn through condition-\ning on past frames and predicting information about future\nframes. Whereas Mockingjay is designed to predict the cur-\nrent frame through jointly conditioning on both past and fu-\nture contexts. The Mockingjay representation improves per-\nformance for a wide range of downstream tasks, including\nphoneme classiﬁcation, speaker recognition, and sentiment\nclassiﬁcation on spoken content, while outperforming other\napproaches. Mockingjay is empirically powerful and can be\nﬁne-tuned with downstream models, with only 2 epochs we\nfurther improve performance dramatically. In a low resource\nsetting with only 0.1% of labeled data, we outperform the re-\nsult of Mel-features that uses all 100% labeled data.\nIndex Terms— speech representation learning, unsuper-\nvised training, transformer encoders, low resource\n1. INTRODUCTION\nThe goal of speech representation learning is to ﬁnd a trans-\nform from speech that makes high-level information more\naccessible to SLP (Speech and Language Processing) down-\nstream tasks, as speech signal possess a rich set of acoustic\nand linguistic content, including phonemes, words, semantic\nmeanings, tone, speaker characteristics, and even sentiment\ninformation. In this paper, we propose Mockingjay to learn\nspeech representations through unsupervised training with-\nout the use of any labels. We use multi-layer transformer\nencoders and multi-head self-attention [1] to achieve bidirec-\ntional encoding; this framework allows our model to consider\npast and future contexts at the same time. To achieve unsu-\npervised pre-training for speech representations, Mockingjay\nlearns under the proposed Masked Acoustic Model (MAM)\ntask. During training, masked frames are given, and the\nmodel learns to reconstruct and predict the original frames.\nHence we gave the name Mockingjay, a bird that mimics\nsound. The proposed framework is illustrated in Figure 1.\nFig. 1. The proposed Masked Acoustic Model pre-training\ntask, 15% of input the frames are masked to zero at random.\n1.1. Related work\nUnsupervised speech representation learning [2, 3, 4, 5, 6, 7,\n8, 9, 10] is effective in extracting high-level properties from\nspeech. SLP downstream tasks can be improved through\nspeech representations because surface features such as log\nMel-spectrograms or waveform can poorly reveal the abun-\ndant information within speech.\nContrastive Predictive Coding (CPC) [5] and wav2vec [7]\nuse a multi-layer CNN to encode past context, representa-\ntions are learned by predicting the future in latent space un-\nder a contrastive binary classiﬁcation task. Autoregressive\nPredictive Coding (APC) [6] uses autoregressive models to\nencode temporal information of past acoustic sequence; the\nmodel predicts future frames like an RNN-based language\nmodel [11], optimized with reconstruction loss. Unidirec-\ntional models are commonly used in the previous approaches\n[2, 3, 4, 5, 6, 7]. However, this constraint on model architec-\ntures limits the potential of speech representation learning.\nThe recently proposed vq-wav2vec [8] approach attempts\nto apply the well-performing Natural Language Processing\n(NLP) algorithm BERT [12] on continuous speech. Input\nspeech is discretized to a K-way quantized embedding space,\nso continuous speech could act like discrete units similar to\nword tokens in NLP tasks. In vq-wav2vec [8], an exhaustive\narXiv:1910.12638v2  [eess.AS]  2 Feb 2020\nFig. 2. The proposed Mockingjay training framework.\ntwo-stage training pipeline with massive computing resources\nare required to adapt speech to NLP algorithm, as the quan-\ntization process is against the continuous nature of speech.\nUnlike [8] that adapts speech to BERT [12] through quantiza-\ntion, the proposed approach can be seen as a modiﬁed version\nof BERT [12] for direct application on continuous speech.\n1.2. Proposed Method\nUnlike previous left-to-right unidirectional approaches that\nonly consider past sequences to predict information about fu-\nture frames, the proposed method allows us to train a bidi-\nrectional speech representation model, alleviating the unidi-\nrectionality constraint of previous methods. As a result, the\nMockingjay model obtains substantial improvements in sev-\neral SLP tasks. Moreover, as previous approaches restrict the\npower of the pre-trained models to representation extraction\nonly [5, 6, 7, 8], the proposed method is robust as it can be\nﬁne-tuned easily on downstream tasks. We show that ﬁne-\ntuning for 2 epochs easily acquires signiﬁcant improvement.\nThe proposed approach outperforms other representa-\ntions and features. When compared to the commonly used\nlog Mel-features, we outperformed it by 35.2% (absolute\nimprovement) for phoneme classiﬁcation accuracy, 28.0%\n(absolute improvement) for speaker recognition accuracy,\nand 6.4% (absolute improvement) for sentiment discrimi-\nnation accuracy on a spoken content dataset unseen during\npre-train. We also experiment in low resource settings to\nshow that Mockingjay is capable of improving supervised\ntraining in real-life low-resource scenarios. With only 0.36\nhours (0.1%) of transcribed speech, the proposed approach\noutperforms Mel-features with 360 hours (100%) of labels.\n2. MOCKINGJAY\nIn this section, we ﬁrst introduce model architecture and its\ndesigns, secondly we explain the proposed unsupervised con-\ntext prediction task, and ﬁnally we explain how the proposed\nmodel is used with downstream task models.\n2.1. Model Architecture\nWe use a multi-layer Transformer encoder with multi-head\nself-attention for left-and-right bidirectional encoding, this ar-\nchitecture is illustrated in Figure 2. Each encoder layer has\ntwo sub-layers, the ﬁrst is a multi-head self-attention network,\nand the second is a feed-forward layer, each sub-layer has\na residual connection followed by layer normalization [13],\nbased on the design described in [1]. All encoder layers in\nthe model, as well as the sub-layers, produce outputs of iden-\ntical dimensions denoted as Hdim. In Figure 2, we denote the\nfeed-forward size as Fdim, the number of self-attention heads\nas Anum, and the total of Transformer layers as Lnum. The\nMockingjay representations can be extracted from the Trans-\nformer encoders’ hidden state and labeled asHidden, we ex-\nplain how they are used as representations in Section 2.3.\nSince Transformer encoders contain no recurrence and\nconvolution, we use positional encoding to make our model\naware of the input sequence order [1]. As direct addition of\nacoustic features to positional encoding may lead to poten-\ntial training failure [14], the input frames are ﬁrst projected\nlinearly to the hidden dimension of Hdim before adding with\npositional encoding [15]. We use sinusoidal positional encod-\ning instead of learnable positional embeddings [16] because\nacoustic features can be arbitrarily long with high variance\n[15]. We apply downsampling on input features to adapt our\nmodel to long sequences. To reduce the length of frames by a\nfactor of Rfactor , we use the reshape technique from [14, 15]\nby stacking Rfactor consecutive frames into one step.\n2.2. Masked Acoustic Modeling\nWe propose the Masked Acoustic Modeling task, where we\nrandomly select 15% of the input frames, and the model pre-\ndicts the selected frames based on its left and right context,\nas illustrated in Figure 1. During training, we add a predic-\ntion head consists of two layers of feed-forward network with\nlayer-normalization, using the last encoder layer as it’s input.\nWe use L1 Loss to minimize reconstruction error between\nprediction and ground-truth frames on the selected 15%. The\nprediction head is not used once the model is trained.\nDuring training, for the selected 15% of frames, 1) we\nmask it all to zero 80% of the time, 2) replace all with a\nrandom frame 10% of the time, and 3) leave the frames un-\ntouched 10% of the time. 1 We introduce this sub-random\n1This process is similar to the Cloze task [17] and the Masked Language\nModel task from BERT [12], but we mask frames of speech to zero instead\nof using the MASK token.\nprocess instead of always masking the frames to alleviate the\nmismatch between training and inference, as masked frames\ndo not appear during inference time. Note that in contrast\nto BERT [12], where the sub-random process is performed\ntoken-wise on the i-th chosen token, our sub-random process\nis performed utterance-wise. In other words, our model may\nreceive inputs as ground-truth frames for 3) 10% of the time,\nrather with some of the inputs always augmented as in [12].\nTo avoid the model exploiting local smoothness of acous-\ntic frames, we propose additional consecutive masking where\nwe mask consecutive frames Cnum to zero. The model is\nrequired to infer on global structure rather than local infor-\nmation. We also use dynamic masking [18] where masking\npatterns are sampled from an uniform distribution every time\nwe feed a sequence to the model, unlike the static mask em-\nployed in [12] where masking is performed during data pre-\nprocessing. We only use a single context prediction task to\ntrain our representation model, as suggested by [18]. Unlike\nBERT [12] and ALBERT [19] that needs two tasks to train\ntheir language model. In our preliminary experiments, we\nfound that the sentence prediction task used in [12, 19] is not\nhelpful, as additional tasks can potentially harm training be-\nhavior. We do not include details due to space limitations.\n2.3. Incorporating with Downstream Tasks\nMockingjay representations are essentially the Transformer\nencoder hidden states. There are many ways to incorporate\nlearned representations to downstream tasks. In this work, we\nmainly extract representations from the last layer. However,\nwe also expose the deep internals of Mockingjay to down-\nstream models, where we use a mixture of representations\nfrom all layers, similar to the ELMO [20] approach. In other\nwords, we use a learnable weighted sum to integrate hidden\nstates from all layers. Last but not least, a pre-trained Mock-\ningjay model can be ﬁne-tuned with downstream models to\ncreate improving results, we update the pre-trained Mocking-\njay together with random initialized downstream task models.\n3. IMPLEMENTATION\nIn this work, we use two types of features as our model’s\noutput reconstruction target: the Mel-scale spectrogram and\nthe linear-scale spectrogram. As Mel-scale spectrogram is\na more concise acoustic feature when compared to linear-\nscale spectrogram, we propose two model settings:BASE and\nLARGE. Both of these models take Mel-features as input, and\ntransform input Mel-features into high-level representations.\nThey use the same hidden dimension size ofHdim=768, feed-\nforward size ofFdim=3072, and attention heads ofAnum=12,\nwith the exception of layer number Lnum, downsample fac-\ntor Rfactor , and consecutive masking number Cnum, the dif-\nferences in model settings are listed in Table 1. We further\nanalyze their differences in our experiment section.\nTable 1. The proposed BASE and LARGE model settings\nModel BASE LARGE\nTarget Mel Linear\nLnum 3 12\nRfactor 1 3\nCnum 7 3\nparameters 21.4M 85.4M\nThe proposed Mockingjay models are pre-trained on\nthe LibriSpeech [21] corpus train-clean-360 subset. We use\nAdam [22] where learning rate is warmed up over the ﬁrst 7%\nof 500k total training steps to a peak value of 4e-4 and then\nlinearly decayed. A dropout [23] of 0.1 is applied on all layers\nand attention weights. For downstream task ﬁne-tuning, most\nof the hyperparameters are the same as in pre-training, with\nthe exception of a learning rate of 4e-3, and the number of\ntraining epochs is set to 2 (which is approximately 50k steps).\nWe train with a batch size of 6 using a single 1080Ti GPU.\nWe provide pre-trained models with our implementation, they\nare publicly available for reproducibility2.\n4. EXPERIMENT\nFollowing previous works [2, 3, 4, 5, 6, 7, 8], we evaluate\ndifferent features and representations on downstream tasks,\nincluding: phoneme classiﬁcation, speaker recognition, and\nsentiment classiﬁcation on spoken content. For a fair com-\nparison, each downstream task uses an identical model archi-\ntecture and hyperparameters despite different input features.\nWe report results from 5 of our settings: 1) BASE and\n2) LARGE where Mockingjay representations are extracted\nfrom the last encoder layer, 3) the BASE-FT2 where we ﬁne-\ntune BASE with random initialized downstream models for 2\nepochs, and 4) the BASE-FT500 where we ﬁne-tune for 500k\nsteps, and ﬁnally 5) the LARGE-WS where we incorporate\nhidden states from all encoder layers of the LARGE model\nthrough a learnable weighted sum. We did not ﬁne-tune the\nLARGE model, as it is meant for extracting representations.\nEmpirically we found that even with supervised training, a\nrandom initialized Mockingjay model followed by any down-\nstream model is hard to be trained from scratch. This shows\nthat the proposed pre-training is essentially indispensable.\n4.1. Comparing with other representations\nThe proposed approaches are mainly compared with APC [6]\nrepresentations, as they also experiment on phone classiﬁca-\ntion and speaker veriﬁcation. As reported in [6], the APC\napproach outperformed CPC representations [5, 7, 9] in both\ntwo tasks, which makes APC suitable as a strong baseline.\nAPC uses an unidirectional autoregressive model. We com-\npare the proposed approach with APC to show that our bidi-\nrectional approach has advantages in speech representation\n2https://github.com/andi611/Mockingjay-Speech-Representation\nFig. 3. Comparing representations with phone classiﬁcation\naccuracy across different amount of transcribed data.\nlearning. For fair comparison, we pre-train APC using their\nofﬁcial implementations with the reported ideal parameters\nand settings, but expand the model’s hidden size toHdim=768\nto match ours. We also report results on 160-dimensional log\nMel-features, which helps evaluate the accessibility of speech\ninformation from regular acoustic features.\n4.2. Phoneme Classiﬁcation\nTo measure the accessibility of phonetic information, we train\nlinear phone classiﬁers using Mel-features, APC and Mock-\ningjay representations from the LibriSpeech train-clean-360\nsubset. We obtain force-aligned phoneme sequences with the\nMontreal Forced Aligner [24], where there are 72 possible\nphone classes. Testing results on the LibriSpeech test-clean\nsubset are presented in Figure 3. In the case where all 360\nhours of labels are used to train the classiﬁer, BASE and\nLARGE representations increase 11.8% and 15.2% accuracy\nfrom Mel-features. The BASE-FT2 model outperforms all\nrepresentations after 2 epochs of ﬁne-tuning, with 10.2% and\n35.2% absolute improvement over APC and Mel-features, re-\nspectively. We observe that ﬁne-tuning for 2 epochs is enough\nto reveal our method’s potential as there is only a small gap\n(3.9%) between BASE-FT2 and BASE-FT500. Furthermore,\nLARGE-WS improves over LARGE, just as we expected.\nTo demonstrate how pre-training on speech can improve\nsupervised training in resource constrained scenarios where\nhuman labels are short, we train the classiﬁer with reduced\namount of training data. The performance variation of differ-\nent methods are plotted in Figure 3, where we measure over\nvarious intervals of constrained training data to observe per-\nformance drop. Both BASE and LARGE increase accuracy\nover Mel-features across various amount of transcribed data.\nWhereas the APC approach performs well on the full resource\nbut fails to generalize for limited amount of labeled data. In\nthe case where there are only 0.36 hours of data available,\nwe improve accuracy by 22.7% (an absolute improvement\nfrom Mel-features). Note that with only 0.36 hours (0.1%) of\nlabels available, BASE-FT2 (57.9% acc) even outperformed\nTable 2. Comparing different methods with different tasks.\nMethods Speaker (acc) Sentiment (acc)\nMel-Features 70.06 64.63\nAPC 85.88 65.97\nBase 94.54 67.38\nBaseFT2 98.05 68.45\nLarge 96.26 70.07\nLargeWS 96.40 71.05\nMel-features (49.1% acc) that uses all 360 hours (100%) of\nlabeled data. We conclude that pre-training Mockingjay on\nspeech substantially improves the performance on supervised\ntask that requires human annotations.\n4.3. Speaker Recognition\nTo demonstrate that the proposed approach performs con-\nstantly for all SLP downstream tasks, we report speaker\nrecognition results on the LibriSpeech 100 hour selected\nsubset, where train/test split is performed randomly with a\n9:1 ratio, and there are 63 possible speakers. We trained a\nsimple one-layer RNN classiﬁer for speaker recognition us-\ning different representations, results are listed in Table 2 for\ncomparison. The proposed BASE and LARGE representa-\ntions outperformed both APC and Mel-Features. BASE-FT2\nfurther improves upon BASE while achieving the highest\naccuracy, whereas LARGE-WS also outperforms LARGE.\n4.4. Sentiment Classiﬁcation on Spoken Content\nTo demonstrate domain invariant transferability of the pro-\nposed representation across different datasets, the Mocking-\njay model is pre-trained on LibriSpeech and applied on the\nMOSEI [25] dataset. We also use a simple one-layer RNN\nclassiﬁer, where the model is trained to extract linguistic\nmeanings from speech and discriminates between sentiments.\nThe results listed in Table 2 lead to an identical conclusion as\nin the speaker recognition task discussed above. Except that\nin the case of sentiment classiﬁcation, LARGE-WS achieved\nthe highest score without the need of ﬁne-tuning, demon-\nstrating that a deeper model has great potential for extracting\ngeneral speech representations. To conclude this section, we\nclaim that the proposed representations are general and can\nbe used on datasets with various unseen domains.\n5. CONCLUSION\nThe proposed representation contains a variety of knowledge,\nincluding but not limited to phonetic, speaker, and sentiment\ninformation. We improve performance for a wide range of\ndownstream tasks, and show promising results in low re-\nsource settings, as the learned speech representations are\nrobust and can be transferred to different tasks across differ-\nent datasets. In future work, we will investigate and deploy\nMockingjay representations on more downstream SLP tasks,\nincluding ASR, voice conversion, and speech translation.\n6. REFERENCES\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin, “Attention is all you need,”\n2017.\n[2] Jan Chorowski, Ron J. Weiss, Samy Bengio, and Aaron\nvan den Oord, “Unsupervised speech representation\nlearning using wavenet autoencoders,” IEEE/ACM\nTransactions on Audio, Speech, and Language Process-\ning, vol. 27, no. 12, pp. 2041–2053, Dec 2019.\n[3] Yu-An Chung and James Glass, “Speech2vec: A\nsequence-to-sequence framework for learning word em-\nbeddings from speech,” Interspeech 2018, Sep 2018.\n[4] Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, Hung-\nYi Lee, and Lin-Shan Lee, “Audio word2vec: Unsuper-\nvised learning of audio segment representations using\nsequence-to-sequence autoencoder,” 2016.\n[5] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-\nresentation learning with contrastive predictive coding,”\n2018.\n[6] Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James\nGlass, “An unsupervised autoregressive model for\nspeech representation learning,” Interspeech, Sep 2019.\n[7] Steffen Schneider, Alexei Baevski, Ronan Collobert,\nand Michael Auli, “wav2vec: Unsupervised pre-training\nfor speech recognition,” Interspeech, Sep 2019.\n[8] Anonymous authors, “vq-wav2vec: Self-supervised\nlearning of discrete speech representations,” in ICLR\n2020 Conference Blind Submission, 2020.\n[9] Anonymous authors, “Unsupervised learning of efﬁ-\ncient and robust speech representations,” in ICLR 2020\nConference Blind Submission, 2020.\n[10] Andy T. Liu, Po chun Hsu, and Hung yi Lee, “Unsuper-\nvised end-to-end learning of discrete linguistic units for\nvoice conversion,” 2019.\n[11] Tom ´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur, “Recurrent neural\nnetwork based language model,” in Eleventh annual\nconference of the international speech communication\nassociation, 2010.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding,” 2018.\n[13] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton, “Layer normalization,” 2016.\n[14] Matthias Sperber, Jan Niehues, Graham Neubig, Sebas-\ntian St¨uker, and Alex Waibel, “Self-attentional acoustic\nmodels,” Interspeech 2018, Sep 2018.\n[15] Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues,\nMarkus Muller, and Alex Waibel, “Very deep self-\nattention networks for end-to-end speech recognition,”\narXiv preprint arXiv:1904.13377, 2019.\n[16] Jonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin, “Convolutional sequence\nto sequence learning,” 2017.\n[17] Wilson L Taylor, ““cloze procedure”: A new tool for\nmeasuring readability,”Journalism Bulletin, vol. 30, no.\n4, pp. 415–433, 1953.\n[18] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” 2019.\n[19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut, “Al-\nbert: A lite bert for self-supervised learning of language\nrepresentations,” 2019.\n[20] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer, “Deep contextualized word representa-\ntions,” 2018.\n[21] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur,\n“Librispeech: An asr corpus based on public domain\naudio books,” in 2015 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\nApril 2015, pp. 5206–5210.\n[22] Diederik P. Kingma and Jimmy Ba, “Adam: A method\nfor stochastic optimization,” 2014.\n[23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov, “Dropout:\nA simple way to prevent neural networks from overﬁt-\nting,” Journal of Machine Learning Research, vol. 15,\npp. 1929–1958, 2014.\n[24] Michael McAuliffe, Michaela Socolof, Sarah Mihuc,\nMichael Wagner, and Morgan Sonderegger, “Montreal\nforced aligner: Trainable text-speech alignment using\nkaldi.,” 2017.\n[25] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-\nria, Erik Cambria, and Louis-Philippe Morency, “Mul-\ntimodal language analysis in the wild: CMU-MOSEI\ndataset and interpretable dynamic fusion graph,” inPro-\nceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\nMelbourne, Australia, July 2018, pp. 2236–2246."
}