{
    "title": "Deep Transformers with Latent Depth",
    "url": "https://openalex.org/W3089072946",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1968637396",
            "name": "Li Xian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281520664",
            "name": "Stickland, Asa Cooper",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2354260411",
            "name": "Tang, Yuqing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2178735079",
            "name": "Kong Xiang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3016545553",
        "https://openalex.org/W2978409868",
        "https://openalex.org/W2798081680",
        "https://openalex.org/W2895899670",
        "https://openalex.org/W2907121943",
        "https://openalex.org/W2958953787",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963460174",
        "https://openalex.org/W2970925270",
        "https://openalex.org/W2948384082",
        "https://openalex.org/W2798362442",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2949454572",
        "https://openalex.org/W3010768098",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2806311723",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W2971137988",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2964088238",
        "https://openalex.org/W2911300548",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2927589347",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2767175863",
        "https://openalex.org/W3021993108",
        "https://openalex.org/W2962944050",
        "https://openalex.org/W3000779003",
        "https://openalex.org/W3122317902",
        "https://openalex.org/W2963430933",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W3017454464",
        "https://openalex.org/W2970290486",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W2933138175"
    ],
    "abstract": "The Transformer model has achieved state-of-the-art performance in many sequence modeling tasks. However, how to leverage model capacity with large or variable depths is still an open challenge. We present a probabilistic framework to automatically learn which layer(s) to use by learning the posterior distributions of layer selection. As an extension of this framework, we propose a novel method to train one shared Transformer network for multilingual machine translation with different layer selection posteriors for each language pair. The proposed method alleviates the vanishing gradient issue and enables stable training of deep Transformers (e.g. 100 layers). We evaluate on WMT English-German machine translation and masked language modeling tasks, where our method outperforms existing approaches for training deeper Transformers. Experiments on multilingual machine translation demonstrate that this approach can effectively leverage increased model capacity and bring universal improvement for both many-to-one and one-to-many translation with diverse language pairs.",
    "full_text": "Deep Transformers with Latent Depth\nXian Li1, Asa Cooper Stickland2, Yuqing Tang1, and Xiang Kong1\n1Facebook AI\n{xianl, yuqtang, xiangk}@fb.com\n2University of Edinburgh\n{a.cooper.stickland}@ed.ac.uk\nAbstract\nThe Transformer model has achieved state-of-the-art performance in many se-\nquence modeling tasks. However, how to leverage model capacity with large or\nvariable depths is still an open challenge. We present a probabilistic framework\nto automatically learn which layer(s) to use by learning the posterior distributions\nof layer selection. As an extension of this framework, we propose a novel method\nto train one shared Transformer network for multilingual machine translation\nwith different layer selection posteriors for each language pair. The proposed\nmethod alleviates the vanishing gradient issue and enables stable training of deep\nTransformers (e.g. 100 layers). We evaluate on WMT English-German machine\ntranslation and masked language modeling tasks, where our method outperforms\nexisting approaches for training deeper Transformers. Experiments on multilin-\ngual machine translation demonstrate that this approach can effectively leverage\nincreased model capacity and bring universal improvement for both many-to-one\nand one-to-many translation with diverse language pairs.\n1 Introduction\nThe Transformer model has achieved the state-of-the-art performance on various natural language\npreprocessing (NLP) tasks, originally in neural machine translation [ 30], and recently in massive\nmultilingual machine translation [3, 37], crosslingual pretraining [8, 17], and many other tasks. There\nhas been a growing interest in increasing the model capacity of Transformers, which demonstrates\nimproved performance on various sequence modeling and generation tasks [35, 24, 1].\nTraining Transformers with increased or variable depth is still an open problem. Depending on the\nposition of layer norm sub-layer, backpropagating gradients through multiple layers may suffer from\ngradient vanishing [19, 31, 5]. In addition, performance does not always improve by simply stacking\nup layers [6, 31]. When used for multilingual or multi-task pretraining, such as multilingual machine\ntranslation, crosslingual language modeling, etc., the simplicity of using one shared Transformer\nnetwork for all languages (and tasks) is appealing. However, how to share model capacity among\nlanguages (and tasks) so as to facilitate positive transfer while mitigating negative transfer has not\nbeen well explored.\nIn this work, we present a novel approach to train deep Transformers, in which the layers to be used\n(and shared) and the effective depth are not static, but learnt based on the underlying task. Concretely,\nwe model the decision to use each layer as a latent variable, whose distribution is jointly learnt with\nthe rest of the Transformer parameters. At training time we approximate the discrete choice with\na Gumbel-Softmax [14] distribution. The ‘soft weights’ sampled from this distribution also act as\ngradient normalization for each layer, and this allows us to train very deep Transformers (up to\n100 layers) without using regular layer normalization layers. At inference time, the learnt discrete\nchoice can be used to directly derive a compact model by pruning layers with low probability, but we\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2009.13102v2  [cs.CL]  16 Oct 2020\nLayer\tselection\nsamples\t\nInference\nnetwork\t\n...\n * *\n * *: skip : select\nSelf Attention\nEncoder Attention\nFFN\nLanguage\t1 \nLanguage\t2 \n...\nLanguage\tN \nInference\twith\tlearnt\tsub\tnetworksTrain\tone\tmodel\nLanguage\t1 ... Language\tN Language\t2 \nMultilingual\nFigure 1: We learn the posterior distribution qφ to “select\" or “skip\" each layer in Transformers. In\nmultilingual setting, each language learns their own “views\" of latent layers in a shared Transformer.\nhave the choice of leaving the learned layer selection probabilities as soft weights. By evaluating\non WMT’16 English-German machine translation (MT) and masked language modeling (MLM)\ntasks (similar to the XLM-R model [8]), we show that we can successfully train deeper Transformer\n(64-layer encoder/decoder model for MT, and 96-layer encoder for MLM) and outperform existing\napproaches in terms of quality and training stability.\nWe show this approach can be extended to learn task-speciﬁc sub-networks by learning different\nlayer selection probabilities for each language pair in multilingual machine translation. This result\ncontributes to the growing interest of learning efﬁcient architectures for multi-task and transfer\nlearning in natural language understanding and generation [28, 12, 7].\nThe main contributions of this paper are as follows. We present a probabilistic framework to learn\nwhich layers to select in the Transformer architecture. Based on this framework, we propose a novel\nmethod to train one shared Transformer network for multilingual machine translation with different\nlayer selection probabilities for each language pair. The proposed method alleviates the vanishing\ngradient issue and enables stable training of deep Transformers. We conduct experiments on several\ntasks to evaluate the proposed approach: WMT’16 English-German machine translation, masked\nlanguage modeling, and multilingual many-to-one as well as one-to-many machine translation with\ndiverse languages.\n2 Method\nBackground In this section, we brieﬂy describe the standard Transformer layer architecture [30].\nFor a hidden state xl of a single token at layer l, each Transformer layer is a function Fl(xl) that\ntransforms its input xl by sequentially applying several sub-layers. The sub-layer is as follows:\nxl+1 = xl + SubLayerl(Norm(xl)), (1)\nwhere SubLayerl(·) is either a Self Attention module, an Encoder Attention module (for a Trans-\nformer decoder in a sequence-to-sequence model), or a feed-forward network (FFN) module, and\nNorm(·) is a normalisation layer, usually layer-norm [ 4]. This is the ‘pre-norm’ setting which is\nnow widely used [19], as opposed to ‘post-norm’ in which caseNorm(·) would be applied after the\nresidual connection: xl+1 = Norm(xl + SubLayerl(xl)).\n2\n2.1 Latent Layer Selection\nFor each Transformer layer l, we treat the selection of all sub-layers in non-residual block Fl(x) as a\nlatent variable zl from a parameterizable distribution p(z),\nxl+1 = xl + zl ×Fl(xl), zl ∼p(z; l) (2)\nwhere the standard Transformer [30] is a special case with zl = 1 for l= 0,...,L −1, where Lis the\ndepth of the network, i.e. total number of layers.\nFor the sequence generation taskp(y|x) parameterized by a Transformer network with the remaining\nstandard parameters Θ, we assume the following generative process:\ny∼p(y|x; θ,z), p(y|x) =\n∫\nz\np(y|x; Θ,z)p(Θ,z) dΘdz (3)\nParameterization and inference ofz. We model zl as discrete latent variable from a Bernoulli\ndistribution with zl ∼B(π; l), π ∈[0,1] indicating select or skip the non-residual block Fl(x) in\nlayer l, and samples from one layer are independent from other layers. This modeling choice allows\nus to prune layers which reduces inference cost and may regularize training.\nMarginalizing over zbecomes intractable when lgrows large. Therefore, we use variational inference\nas a more general optimization solution. Speciﬁcally, we instead maximize the evidence lower bound\n(ELBO) of Eq. 3\nlog p(y|x) ≥Eqφ(z)[log pθ(y|x,z)] −DKL(qφ(z) ∥p(z)) (4)\nWe point out that although we could treat the rest of the network parameters Θ as latent variables\ntoo and model the joint distribution of p(θ,z), which could be optimized using Coupled Variational\nBayes (CVB) and optimization embedding as demonstrated in [27] for neural architecture search, in\npractice we found a simpler optimization procedure (Algorithm 2) to learn both θand zjointly from\nscratch.\nWe use the Gumbel-Softmax reparameterization [14] to sample from the approximate posterior qφ(z)\nwhich makes the model end-to-end differentiable while learning (approximately) discrete policies\nwithout resorting to policy gradients. To allow both “soft weighting\" and “hard selection\" of layers,\neach of which has the appealing property of achieving model pruning while training with larger\nmodel capacity, we generate soft samples of zduring training and draw hard samples for pruning at\ninference time if qφ(z) becomes (close to) discrete. We directly learn the logits parameter αl for each\nlayer l:\nzi\nl(αl) = exp((αl(i) + ϵ(i))/τ)∑\ni∈{0,1}exp((αl(i) + ϵ(i))/τ) , ϵ∼G(0,1) (5)\nwhere G(0,1) is the Gumbel distribution, and τ is a temperature hyperparameter which increases\nthe discreteness of samples when τ →0. For p(z) we can use the conjugate prior Beta(a,b) which\nallows us to express different preferences of z, such as a= b= 1 for an uniform prior, a>b to bias\ntowards layer selection and a<b to favor skipping layers.\nGradient scaling. Next we analyze the impact of latent layers on gradient backpropagation during\ntraining in the pre-norm setting. In Eq. 6, we can see that given the forward pass loss L, the gradient\naccumulation from higher layers ml<m<L is now weighted by the their corresponding latent samples\nzm, which acts as gradient scaling. In Section 3 we show that with such gradient normalization we\ncan train deeper Transformers without using layer normalisation.\n∂L\n∂xl\n= ∂L\n∂xL\n×(1 +\nL−1∑\nm=l\nzm\n∂Fm(xm)\n∂xl\n) (6)\n2.2 Multilingual Latent Layers\nIt is sometimes convenient to share a Transformer network across multiple languages, enabling\ncrosslingual transfer, with recent success in multilingual machine translation and multilingual pre-\ntraining (e.g. multilingual BERT and BART) [ 3, 8, 20, 17]. Current approaches share a vanilla\n(usually 12-layer) Transformer across all languages.\n3\nTo explore the potential of latent layers for a multilingual Transformer, we let each language learn its\nown layer utilization given a single Transformer network Θ shared among N languages by learning\nits own posterior inference network q(n)\nφ of {αl}. We acknowledge that an alternative is to learn a\nshared inference network qφ(n) which takes language nas input. The latter may enable learning\ncommonalities across languages but at the cost of extra parameters, including a non-trivial N ×d\nparameters for language embeddings. Therefore, we chose the former approach and leave the latter\n(and the comparison) for future work. With this modeling choice, we can still encourage layer-sharing\nacross languages by using the aggregated posterior across languages ˜q(z) as the prior in the DKL\nterm:\nDKL(qφ(z) ∥˜q(z)) = Eqφ(z)[log qφ(z)\n˜q(z) ] , ˜q(z) = 1\nN\nN∑\nn=1\nqφ(z|x(n),y(n),ˆθ) (7)\nLatent Layers with Targeted DepthTo deploy Transformers in the real world, we would like to\nhave lower computational cost at inference time. Within a Transformer layer, some computation is\nparallel, such as multi-head attention, but the time and space complexity at inference time grows\nlinearly with the number of layers. Therefore, pruning layers at test time directly reduces inference\ncost. Our approach can be extended to perform model pruning, encouraging the model to achieve a\ntarget depth Kby adding an extra loss LK = ∥∑L−1\nl=0 ul −K∥2 where ul refers to the “utilization\"\nof layer l. ul can be approximated by samples of the latent variables zl and for the multilingual case\nul = ∑N\nn=1 z(n)\nl /N.\nThe general loss for training a Transformer with latent depth Kis\nLLL = Eqφ(z)[−log pθ(y|x,z)] + βDKL(qφ(z) ∥p(z))\n  \nLELBO\n+λLK (8)\nTo learn Θ and qφ jointly from scratch, we use an two-level optimization procedure described in\nAlgorithm 2. This training strategy is inspired by the Generalized Inner Loop Meta-learning [10]. We\nprovide a more detailed explanation of this training procedure in Appendix B.1.\n3 Experimental Settings\nAlgorithm 1 Training with Latent Layers\n1: Initialize Θ, qφ.\n2: for t=1, ..., T do\n3: for i=1, ..., I do\n4: Sample a mini-batch (x,y) ∼D.\n5: Sample zl=0,...,L−1 with Eq. 5\n6: Compute ˆLLL((x,y); Θi−1,qt−1\nφ )\nwith Eq. 8\n7: Update Θi = Θi−1 −η∇Θi−1\nˆLLL\n8: Update qt\nφ = qt−1\nφ −η∇qt−1\nφ\nˆLLL\nWe ﬁrst evaluate on the standard WMT English-\nGerman translation task and a masked language\nmodeling task to demonstrate the effectiveness\nof the proposed approach at enabling train-\ning deeper Transformers and whether this in-\ncreased depth improves model performance. We\nthen evaluate multilingual latent layers (see sec-\ntion 2.2) on multilingual machine translation.\nBilingual Machine Translation. We use the\nsame preprocessed WMT’16 English-German\nsentence pairs as is used in [ 30, 31]. To make\ncomparison more clear and fair, we evaluate on\nthe last model checkpoint instead of ensembles\nfrom averaging the last 5 checkpoints. We use beam size 5 and length penalty 1.0 in decoding and\nreport corpus-level BLEU with sacreBLEU [22].\nCrosslingual Masked Language Modelling. We test our method on a scaled-down version of\nXLM-R [8], intending to show the promise of our method, but not obtain state-of-the-art results on\ndownstream tasks. In particular we use as training data the Wikipedia text of the 25 languages used\nin the mBART [17] model, and evaluate using perplexity on a held out dataset consisting of 5000\nsentences in each language (sampled randomly from each Wikipedia text).\nMultilingual Machine Translation. We evaluate the proposed approach on multilingual machine\ntranslation using the 58-language TED corpus [23]. To study its performance independent of task\n4\n(a) Gradient norms of encoder and decoder in standard Transformer.\n(b) Improvement of decoder’s gradient norm using latent layers.\nFigure 2: Comparing gradient norms of baseline (a) and using latent layers (b).\nsimilarity and difﬁculty, we evaluate on both related (four low resource languages and four high\nresource languages from the same language family) and diverse (four low resource languages and\nfour high resource ones without shared linguistic properties) settings as is used in [ 32]. Dataset\ndescriptions and statistics are summarized in the Appendix C.1. For each set of languages, we\nevaluate both many-to-one (M2O), i.e. translating all languages to English, and one-to-many (O2M),\ntranslating English to each of the target languages, which is a more difﬁcult task given the diversity\nof target-side languages.\nBaselines. We compare to the standard Transformer with static depth on machine translation task\nand “wide\" model, e.g. Transformer-big architecture in [30] which increases the hidden (and FFN)\ndimension and has been a common approach to leverage large model capacity without encountering\nthe optimization challenges of training a deeper model.\nWe also compare to recent approaches to training deeper Transformers:\n• Random Layer drop. For deeper models where the static depth baselines diverged, we apply\nthe random LayerDrop described in [9] which trains a shallower model by skipping layers.\n• Dynamic linear combination of layers (DLCL). This is a recently proposed approach\nto address vanishing gradient by applying dense connections between layer which was\ndemonstrated effective for machine translation[31].\n• ReZero[5]. This is similar to our method in that both methods learn to weigh each layer.\nThe key difference is that ReZero learns (unconstrained) weighting parameters. In our\nexperiments, we found ReZero suffers from gradient exploding and training loss diverged.\n4 Results\n4.1 Addressing vanishing gradient\nFigure 3: Comparing learning curves, training and vali-\ndation per-token negative loglikelihood (NLL) loss, of\nbaseline models (static depth) and the proposed method\nwhen training deeper model (decoder).\nFirst, we empirically show that with static depth,\ngradient vanishing happens at bottom layers of\ndecoder Figure 2a. The effect of training with\nlatent layers using the proposed approach is il-\nlustrated in Figure 2b, which shows that gradient\nnorms for bottom layers in the decoder are in-\ncreased.\nNext, we compared the learning curves when\ntraining deeper models. As is shown in Figure\n5\n3 (evaluated on multilingual translation task O2M-Diverse dataset), the baseline model with static\ndepth diverged for a 24-layer decoder, while using the latent layers ((LL-D) approach we could train\nboth 24-layer and 100-layer decoder successfully. We further compared the 100-layer model with a\nwider model (Transformer-big), and found that besides stable training, deep latent layer models are\nless prone to overﬁtting (i.e. they achieve lower validation loss, with a smaller gap between train and\nvalidation losses) despite having more parameters.\n4.2 En-De Machine Translation\nIn Table 1 we evaluate on training deeper Transformers and examine the impact of\nModel Params NLL valid ↓ BLEUvalid ↑ BLEUtest ↑\nTransformer-Big 246M 2.081 28.7 28.1\nDLCL, 36/36 224M 2.128 28.5 27.7\nDLCL, 48/48 224M 2.090 28.8 28.1\nLL-D, 12/24 135M 2.179 28.1 ±0.08 27.2 ±0.04\nLL-D,12/48 211M 2.128 28.1 ±0.00 27.3 ±0.04\nLL-Both, 36/36 224M 2.147 28.4 ±0.07 28.1 ±0.07\nLL-Both, 48/48 287M 2.078 28.7 ±0.10 28.7±0.09\nLL-Both, 64/64 371M 2.069 28.5±0.07 28.4 ±0.08\nTable 1: Performance on WMT’16 En-De. For BLEU scores evaluation, we\nprovide standard errors from 5 runs with different seeds.\nlatent layers in decoder (LL-\nD) and both encoder and\ndecoder (LL-Both) respec-\ntively. Compared to ex-\nisting methods for training\ndeeper Transformers such\nas using dense residual con-\nnections (DLCL), our ap-\nproach can leverage larger\nmodel capacity from in-\ncreased depth and achieved\nimproved generalization.\n4.3 Masked Language Modeling\nModel Params Perplexity ↓\nStatic depth 24 202M 2.91\nLL, 24 202M 2.82\nStatic 48 372M 2.60\nLL, 48 372M 2.71\nStatic 96 712M Diverged\n+ layer-drop 712M Diverged\nLL, 96 712M 2.66\nTable 2: Perplexity on held-out data for\ncrosslingual masked language modeling.\nLatent layers (LL) is also shown to be effective for training\ndeeper encoder without divergence (see Table 2). For 24\nand 48 layer encoders, we observed stable training with 2x\nlearning rate and achieved better performance for 24 layers.\nHowever the result of scaling up to 96 layers was slightly\nworse performance than a vanilla 48 layer model. This\nshows the promise of the method for stabilising training\nat increased depth, however we did not attempt to scale up\nour data to match our larger model capacity.\n4.4 Multilingual Translation\nWe separately test the impact of applying latent layers in the decoder (LL-D), encoder (LL-E) and\nboth (LL-Both).\nModel Params Avg. aze bel ces glg por rus slk tur\n6/6 63.6M 19.65 5.4 9.1 21.9 22.4 38.6 19.4 24.6 15.8\n6/6, wide 190M 20.33 5.7 9.7 22.4 23.1 40.3 20.6 24.1 16.8\n12/12 95.1M 20.48 5.6 10.3 23.1 22.8 39.7 20.1 25.1 17.1\n12/24 133M NA - - - - - - - -\n24/24 158M NA - - - - - - - -\n+layer drop 158M 11.16 3.3 7.5 11.6 14.4 23.4 10.4 12.9 5.8\nLL-D, 12/24 133M 20.83 5 10.2 23.4 24.3 40.3 21 24.8 17.6\nLL-D, 24/24 158M 20.84 5.3 10.6 23.4 23.7 40.7 20.9 24.8 17.5\nTable 3: BLEU scores for one-to-many multilingual translation on related languages. “NA\" means\ntraining diverged.\nLatent layers in decoder.To evaluate the impact of increased decoder depth, we tested on one-to-\nmany (O2M) multilingual translation. In Table 3 we show performance on the “Related\" languages\nsetting. Baseline models began to diverge when decoder depth increases to L= 24, and applying\nrandom LayerDrop did not help. Latent layers allows us to train the same depth successfully, and we\nobserve improvement in translation quality for both language pairs as well as overall quality shown\nby the average BLEU score. In Table 4, we evaluate the impact of deeper decoder with latent layers\n6\nin the O2M-Diverse setting. This is a more challenging task than O2M-Related since decoder needs\nto handle more diversiﬁed syntax and input tokens.\nModel Avg. bos mar hin mkd ell bul fra kor\n6/6 22.12 12.6 11.1 14.6 22.7 29.8 31.8 37.3 17.1\n6/6, wide 23.51 12.7 11.3 13.9 23.8 32.5 34.8 40.6 18.5\n12/12 23.34 13.1 11.1 13.6 22.5 32.7 34.7 40.4 18.6\n12/24 NA - - - - - - - -\n24/24 NA - - - - - - - -\n+layer drop 22.06 13.0 10.0 12.2 21.5 30.7 33.0 38.5 17.6\nLL-D, 12/24 23.70 13.4 10.7 14.1 22.8 33.1 35.1 41.1 19.3\nLL-D, 12/100 24.16 13.5 10.6 13.8 24.1 32.7 38.2 41.3 19.1\nLL-D, 24/24 24.46 15.5 11.4 14.6 24.4 33.5 35.5 41.5 19.3\nTable 4: BLEU scores for one-to-many multilingual translation on diverse languages.\nLatent layers in encoder, decoder, and both.We use the many-to-one multilingual translation\ntask to verify the pattern observed above, and test the effect of increased depth in encoder. Results\nare summarized in Table 5. Similar to O2M, standard Transformer begins to diverge when decoder\ndepth increase over 24 while applying latent layers enable successful training and yields improved\ntranslation quality.\nModel Avg. bos mar hin mkd ell bul fra kor\n6/6 25.95 20.7 8.6 19.2 30.0 36.3 36.9 38.4 17.5\n12/12 27.73 22.5 9.4 20.1 31.6 38 39.6 40.8 19.9\n24/12 27.86 23.7 9.7 21.6 31.2 37.6 39.3 40.0 19.8\n24/24 NA - - - - - - - -\n+layer drop 26.7 21.3 9 19.2 29.2 37.5 38.8 39.9 18.7\nLL-E, 36/12 27.98 24.2 10.2 21.9 32 37.3 38.8 39.3 20.1\nLL-D, 12/24 27.63 22.4 9.3 20.2 30.8 38.2 39.7 40.5 19.9\nLL-D, 12/36 27.89 22.3 9.5 21.1 30.7 38.2 40.2 41.2 19.9\nLL-D, 24/24 28.43 23.6 10.0 21.9 31.7 38.4 40.3 41.2 20.4\nLL-Both, 24/24 28.56 23.5 10.3 22.3 32.8 38.3 40 40.8 20.5\nTable 5: BLEU scores of models with increased depth in the encoder and decoder for many-to-one on diverse\nlanguages.\nFigure 4: Quality improvement (over static depths 12/12) by allocating\nincreased capacity to all-encoder (36/12), all-decoder (12/36), and even\nallocation (24/24).\nBy applying latent layers\nto encoder only (LL-E)\nwe found increased depth\n(36/12) improves low re-\nsource languages (e.g. bos\nand hin) over the baseline\n(12/12). However, deeper\ndecoder (12/36) or even al-\nlocation of depth (24/24)\nbrings consistent gains as is\nshown in Fig 4.\n5 Analysis\nIn this section, we analyze the effect of several modeling choices and understand their contribution to\nthe results.\nEffect of Priors In Figure 5 we illustrate the difference between using aggregated posterior ˜q(z)\nversus a uniform prior Beta(1,1) in computing the DKL loss.\nCompared to the uniform prior, using the aggregated posterior as prior discretizes layer utilization, that\nis, the model is incentivised to make layer selections consistent across all languages, i.e. facilitating\nparameter sharing. Interestingly, the learnt “sharing\" pattern by using ˜q(z) as prior is consistent with\n7\nFigure 5: Layer selection samples zl at epoch 1 from different priors used for DKL.\nFigure 6: Visualization of layer utilization ul during training using the M2O Diverse dataset.\nheuristics such as dropping every other layer for pruning which was empirically found effective [9].\nHowever, training with such a prior in the beginning can lead to “posterior collapse”, which is a\nwell-known challenge found in training variational autoencoders. After applying “KL annealing”\n(annealing the DKL coefﬁcient β), we can see that layer selection samples are more continuous with\na curriculum to use the bottom layers ﬁrst.\nEL Avg. valid BLEU\nβ = 0 10.25 28.50\nβ = 1 11.25 28.53\nβ = 10 12.125 28.23\nTable 6: Impact of the KL coefﬁcient βon\nnetwork effective depth (EL) and translation\nquality, evaluated on M2O-Diverse.\nEffect ofβ. In order to understand how the DKL loss\nterm affects layer selection policies and samples through-\nout training, we vary the DKL coefﬁcient β ∈{0,1,10}.\nFirst, we examine layer utilization ul, e.g. whether “hot\nlayers\" (ul →1) and “cold layers\" (ul →0) change over\ntime. As is shown in Figure 6, without theDKL term, layer\nutilization stays constant for most of the layers, especially\nseveral top layers whose parameters were rarely updated.\nBy increasing the contribution from the DKL to the total\nloss, layer selections are more evenly spread out across languages, i.e. ul becomes more uniform.\nThis is also reﬂected in Table 6 where the “effective depth\"EL increases with β.\n5.1 Ablation Studies\nIn this section, we provide ablation experiments to understand how different loss terms contribute to\nthe results. Table 7 compares the effect on translation quality from different loss terms in Eq. 8. We\ncan see that optimizing the LELBO loss brings the most quality gains, and LK loss adds additional\nimprovement by acting as a regularization.\nModel Avg. bos mar hin mkd ell bul fra kor\nLL-D, 24/24 24.46 15.5 11.4 14.6 24.4 33.5 35.5 41.5 19.3\n- LK 24.28 14.5 10.8 14.3 25 33.4 35.4 41.6 19.2\n- DKL 23.89 13.7 11.0 14.7 24.2 32.9 35.0 40.9 18.9\n- both 23.75 13.4 10.9 14.2 23.6 33.2 35.2 40.7 18.8\nTable 7: Effects from different terms in LLL evaluated on the O2M-Diverse dataset.\n5.2 Latent depth vs. static depth\nWe compare a deeper model with latent effective depthE[L] to models with the same depth trained\nfrom scratch.\n8\nModel BLEU valid ↑ BLEUtest ↑\nLatent depth, L= 24, E[L] = 12 28.6±0.07 27.88 ±0.04\nStatic depth, L= 12 27.2 26.5\nTable 8: Comparing a 24 latent layers model with effective depthE[L] =\n12 with a 12-layer static depth model trained from scratch, evaluated on\nWMT’16 En-De.\nAs is observed in both bilingual\n(Table 8) and multilingual (Ta-\nble 9) machine translation tasks,\ntraining a deeper model with la-\ntent depth outperforms standard\nTransformer with the same num-\nber of effective layers but trained\nwith static depth.\nModel Avg. bos mar hin mkd ell bul fra kor\nLatent depth, E[L] = 14.5 28.43 23.6 10.0 21.9 31.7 38.4 40.3 41.2 20.4\nStatic depth, L= 15 27.9 23.9 10.3 21.5 31.4 37.5 38.9 39.8 19.9\nTable 9: Comparing a 24 latent layers model with effective depth E[L] = 14.5 with a 15-layer static\ndepth model trained from scratch, evaluated on M2O-Diverse dataset.\n6 Related Work\nThe Transformer model [30] has achieved state-of-the-art performance on various natural language\nprocessing (NLP) tasks. Theoretical results suggest networks often have an expressive power that\nscales exponentially in depth instead of width [21], and recent work [36, 1, 23, 32] ﬁnds that deeper\nTransformers improve performance on various generation tasks. However, deeper Transformer\nmodels also face the gradient vanishing/exploding problem leading to unstable training [6, 31]. In\norder to mitigate this issue, Huang et al. (2016) [13] drop a subset of layers during the training, and\nbypass them with the identity function. Zhang et al. (2019) [38] propose an initialization method to\nscale gradients at the beginning of training to prevent exploding or vanishing gradient. Bachlechner\net al. (2020) [ 5] initialize an arbitrary layer as the identity map, using a single additional learned\nparameter per layer to dynamically facilitates well-behaved gradients and arbitrarily deep signal\npropagation. Fan et al. (2019) [9] introduce a form of structured dropout, LayerDrop, which has a\nregularization effect during training and allows for efﬁcient pruning at inference time. Concurrent\nwork which shown improvement on NMT task by increasing model depth includes Zhang et al.\n(2020) [37] and Wei et al. (2020) [33].\nExploring dynamic model architecture beyond hard parameter sharing has received growing interest.\nIn multi-task learning, Multi-Task Attention Network (MTAN) [ 16], routing network [ 25] and\nbranched network [ 29] enables soft parameter sharing by learning a dynamic sub-network for a\ngiven task. One concurrent work “GShard\" [ 15] also demonstrate deeper model with conditional\ncomputation brings consistent quality improvement for multilingual translation. More work on\nlearning an adaptive sub-network includes BlockDrop [34] which learns dynamic inference paths\nper instance, and SpotTune [ 11] which learns which layers to ﬁnetune or freeze to improve transfer\nlearning from a pretrained model.\n7 Conclusion\nWe proposed a novel method to enable training deep Transformers, which learns the effective network\ndepth, by modelling the choice to use each layer as a latent variable. Experiments on machine\ntranslation and masked language modeling demonstrate that this approach is effective in leveraging\nincreased model capacity and achieves improved quality. We also presented a variant of this method in\na multilingual setting where each language can learn its own sub-network with controllable parameter\nsharing. This approach can be extended to use a shared Transformer for multi-task learning in NLP\ntasks, and offers insight into which layers are important for which tasks.\nBroader Impact\nThis work proposes a new method to leverage a model with increased depth during training, while\nlearning a compact sub-work with reduced depth which can be used for deployment in real-world\napplications where Transformers have achieved state-of-the-art quality such as machine translation\nsystems, dialog and assistant applications, etc, as reducing the number of layers especially in\n9\ndecoder (often autoregressive) can have direct impact on reducing inference-time latency, memory\nconsumption, etc. However scaling up the number of layers adds to energy cost of training, even if\nwe can prune at inference time.\nWe hope our research on multilingual NLP will contribute to the effort of improving the standard\nof NLP tools for low-resource languages. However we only test our machine translation systems\non to-English or from-English tasks, leaving out translation from non-English languages to other\nnon-English languages entirely.\nReferences\n[1] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot.\narXiv preprint arXiv:2001.09977, 2020.\n[2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. arXiv preprint\narXiv:1810.09502, 2018.\n[3] Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine\ntranslation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019.\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. 2016.\n[5] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W Cottrell, and Julian\nMcAuley. Rezero is all you need: Fast convergence at large depth. arXiv preprint arXiv:2003.04887, 2020.\n[6] Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. Training deeper neural machine\ntranslation models with transparent attention. arXiv preprint arXiv:1808.07561, 2018.\n[7] Ankur Bapna and Orhan Firat. Simple, scalable adaptation for neural machine translation. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1538–1548, Hong Kong,\nChina, November 2019. Association for Computational Linguistics.\n[8] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual\nrepresentation learning at scale. arXiv preprint arXiv:1911.02116, 2019.\n[9] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured\ndropout. arXiv preprint arXiv:1909.11556, 2019.\n[10] Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska Meier,\nDouwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-learning. arXiv\npreprint arXiv:1910.01727, 2019.\n[11] Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris. Spottune:\ntransfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 4805–4814, 2019.\n[12] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. In\nProceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of\nMachine Learning Research, pages 2790–2799, Long Beach, California, USA, 09–15 Jun 2019. PMLR.\n[13] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In European conference on computer vision, pages 646–661. Springer, 2016.\n[14] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144, 2016.\n[15] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\nand automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n[16] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1871–1880,\n2019.\n[17] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint\narXiv:2001.08210, 2020.\n10\n[18] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[19] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. arXiv\npreprint arXiv:1806.00187, 2018.\n[20] Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? arXiv preprint\narXiv:1906.01502, 2019.\n[21] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential\nexpressivity in deep neural networks through transient chaos. InAdvances in neural information processing\nsystems, pages 3360–3368, 2016.\n[22] Matt Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018.\n[23] Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Janani Padmanabhan, and Graham Neubig.\nWhen and why are pre-trained word embeddings useful for neural machine translation? arXiv preprint\narXiv:1804.06323, 2018.\n[24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[25] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of\nnon-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239, 2017.\n[26] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909, 2015.\n[27] Albert Shaw, Wei Wei, Weiyang Liu, Le Song, and Bo Dai. Meta architecture search. In Advances in\nNeural Information Processing Systems, pages 11225–11235, 2019.\n[28] Asa Cooper Stickland and Iain Murray. BERT and PALs: Projected attention layers for efﬁcient adaptation\nin multi-task learning. In Proceedings of the 36th International Conference on Machine Learning,\nvolume 97 of Proceedings of Machine Learning Research, pages 5986–5995, Long Beach, California,\nUSA, 09–15 Jun 2019. PMLR.\n[29] Simon Vandenhende, Stamatios Georgoulis, Bert De Brabandere, and Luc Van Gool. Branched multi-task\nnetworks: deciding what layers to share. arXiv preprint arXiv:1904.02920, 2019.\n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\nsystems, pages 5998–6008, 2017.\n[31] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning\ndeep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 1810–1822, 2019.\n[32] Xinyi Wang, Yulia Tsvetkov, and Graham Neubig. Balancing training for multilingual neural machine\ntranslation. arXiv preprint arXiv:2004.06748, 2020.\n[33] Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng, and Weihua Luo. Multiscale collaborative\ndeep models for neural machine translation. arXiv preprint arXiv:2004.14021, 2020.\n[34] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman, and\nRogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 8817–8826, 2018.\n[35] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in neural information\nprocessing systems, pages 5754–5764, 2019.\n[36] Biao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled initialization\nand merged attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\npages 897–908, 2019.\n[37] Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. Improving massively multilingual neural\nmachine translation and zero-shot translation. arXiv preprint arXiv:2004.11867, 2020.\n[38] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without\nnormalization. arXiv preprint arXiv:1901.09321, 2019.\n11\nA Gradient analysis\nWe provide a detailed derivation of Eq. 6. The gradient backpropagated to layer l, ∂L\n∂xl\n, can be computed by\napplying the chain rule:\n∂L\n∂xl\n= ∂L\n∂xL\n×∂xL\n∂xl\n(9)\n(10)\nTo compute ∂xL\n∂xl\n, we ﬁrst apply Eq. 2 recursively to expand xL as:\nxL = xL−1 + zL−1 ×FL−1(xL−1) (11)\n= xL−2 + zL−2 ×FL−2(xL−2) + zL−1 ×FL−1(xL−1) (12)\n= xl +\nL−1∑\nm=l\nzmFm(xm) (13)\n∂xL\n∂xl\n= 1 +\nL−1∑\nm=l\nzm\n∂Fm(xm)\n∂xl\n(14)\n∂L\n∂xl\n= ∂L\n∂xL\n×(1 +\nL−1∑\nm=l\nzm\n∂Fm(xm)\n∂xl\n) (15)\nB Training Details\nB.1 Training procedure\nThe proposed training procedure is motivated by the Generalized Inner Loop Meta-learning [10] although we\nuse ﬁrst-order gradient as approximation. Speciﬁcally, we treat qφ as “meta parameters\" and the rest of the\nTransformer parameters Θ as “task-speciﬁc\" parameters. A key difference is that in our case there is only one\ntask and the support set and target set are from the same distribution. At a high-level, we learnΘ in an inner-loop\nwhile updating qφ from the unrolled gradient steps. Such nested optimization is computationally expensive as the\ngraph for multiple steps needs to be stored in memory, and training was found to be unstable due to challenges\nin backpropagating second-order gradients through multiple steps [2]. We adopt a multi-step loss approximation\nusing ﬁrst-order gradients only as is shown to be effective in [2]. Speciﬁcally, in each outer loop we take the\nlatest parameters of qt−1\nφ , and perform Iinner loop steps. The gradients from each inner loop loss ˆLare directly\nbackpropagated to Θ, and the last step’s gradient are used to updateqφ, which is a special case of multi-step loss\nannealing where ωI−1 = 1, ωj = 0 for j <I−1.\nAlgorithm 2 Training with latent layers in multilingual setting\n1: Input: training examples from N languages {Dn}N\nn=1; total number of training steps T; inner\nloop update frequency I\n2: Initialize Θ, q0\nφ = {α0\nl}; t= 0.\n3: for t=1, ..., T do\n4: for i=1, ..., I do\n5: for n = 1, ..., N do\n6: Sample a mini-batch (x,y) ∼Dn.\n7: Compute zl=0,...,L−1 all at once following Eq. 5 with samples ϵl ∼G\n8: Compute loss ˆLLL((x,y); Θi−1,qt−1\nφ ) with Eq. 8\n9: Update Θi = Θi−1 −η∇Θi−1\nˆLLL\n10: Update qt\nφ = qt−1\nφ −η∇qt−1\nφ\nˆLLL\nB.2 Training stability.\nWe examine the stability of our training procedure, e.g. whether training is sensitive\nto the choice of inner loop frequencies. Figure 7 plots the gradient norms of using\nI ∈ {1,2,5,10}, and the impact on translation performance is summarized in Table 10.\n12\nFigure 7: Comparison of gradient norms using different inner\nloop iterations I to verify training stability is not sensitive to\nthe choice of I.\nC Experiments\nImplementation Details\nC.1 Dataset description\nFor WMT’16 English-German experi-\nment, we used the same preprocessed\ndata provided by [ 31] 1, including the\nsame validation (neewsteest2013) and test\n(neewsteest2014) splits. The data vol-\nume for train, validation and test splits\nare 4500966, 3000, 3003 sentence pairs\nrespectively. The data was tokenized and\nnumberized with a joint BPE (byte pair en-\ncoding) [26] vocabulary with 32k merge\noperations.\nFor multilingual translation experiments, we use the same preprocessed data2 provided by [32] using the same\ntrain, valid, and test split as in [23]. The data volumes for related and diverse language groups are summarized\nin Table 12.\nFor crosslingual language modelling we used data from Wikipedia from the 25 languages used in the mBART [17]\nmodel, using the same data collection and preprocessing as [8]. We list the languages used and corresponding\nWikipedia corpus size in Table 11. A random sample of 5000 sentences from each of the languages was used as\nheld-out data to compare models.\nAvg. bos mar hin mkd ell bul fra kor\nI = 1 28.28 23.0 14.1 19.2 31.6 37.2 39.4 40.8 20.9\nI = 2 28.49 23.4 15.1 19 32.1 37.2 39.4 40.8 20.9\nI = 5 28.24 23.4 14.5 18.6 32.1 37.2 39.1 40.2 20.8\nI = 10 28.25 23.8 14.3 19 314 37 39.4 40.5 20.6\nTable 10: BLEU scores on validation set to assess the impact of the inner loop frequencyIon training\nstability and model performance, evaluated on the M2O-Diverse dataset.\nC.2 Models and hyperparameters\nBoth baselines and proposed models are implemented using Transformer models in fairseq [18]. For baseline\nmodels, we use the pre-norm setting which provides a stronger baseline since it was shown to more effective for\ntraining deeper Transformer models than post-norm[19, 31]. Therefore, the comparison with baseline can focus\non evaluate the difference made from using latent layers. We use per-token negative loglikelihood (NLL) loss on\nthe validation set to choose the loss coefﬁcients for βand λ.\nWMT’16 English-German. All models were trained for 75 epochs and evaluating on the last checkpoint.\nFor Transformer-big, we use the standard model architecture as is described in [30]: d= 1024 for embedding\nand hidden dimension, and d= 4096 for FFN dimension, 6-layer encoder and decoder, 0.3 dropout (0.1 after\nattention sub-layer and ReLU activation). Model was trained with 8192 token per GPU and 32 GPUs, learning\nrate 7e-4 and 8000 warm-up updates with Adam optimizer. For deeper models, i.e. both DLCL (baseline) and\nlatent layers (LL, the proposed approach), since the depth is increased we reduce the model width by using\nd= 512 for embedding and hidden dimension, and d= 1024 for FFN dimension, and 4 attention heads. Also,\nwe found for deeper models we were able to use almost 2×learning rate (1.5e-3). We use β = 1 and λ= 0.1\nfor latent layers models.\nCrosslingual Masked Language Modelling.We use the XLM-RBase architecture of [8], which has a\nhidden dimension of 768, but we explore increasing the number of layers, considering 24, 48 and 96 layer\n1The authors of [ 31] provided the downloadable data at https://drive.google.com/uc?export=\ndownload&id=0B_bZck-ksdkpM25jRUN2X2UxMm8\n2The authors of [ 32] provided the downloadable data at https://drive.google.com/file/d/\n1xNlfgLK55SbNocQh7YpDcFUYymfVNEii/view?usp=sharing\n13\nCode Language Sentences (M)\nEn English 41.9\nRu Russian 12.0\nVi Vietnamese 3.7\nJa Japanese 1.7\nDe German 16.7\nRo Romanian 1.8\nFr French 14.8\nFi Finnish 2.4\nKo Korean 2.1\nEs Spanish 10.9\nZh Chinese (Sim) 5.2\nIt Italian 9.7\nNl Dutch 7.7\nAr Arabic 3.2\nTr Turkish 1.8\nHi Hindi 0.6\nCs Czech 2.7\nLt Lithuanian 0.9\nLv Latvian 0.45\nKk Kazakh 1.0\nEt Estonian 2.2\nNe Nepali 0.1\nSi Sinhala 0.1\nGu Gujarati 0.1\nMy Burmese 0.4\nTable 11: A list of the 25 languages and corresponding Wikipedia corpus size (in millions of\nsentences) used for crosslingual masked language modelling.\nRelated Diverse\naze bel glg slk ces por rus tur bos mar hin mkd ell bul fra kor\ntrain (K) 5.94 4.51 10 61.5 103 195 208 182 5.64 9.84 18.79 25.33 134 174 192 205\nvalid 671 248 682 2271 3462 4035 4814 4045 474 767 854 640 3344 4082 4320 4441\ntest 903 664 1007 2445 3831 4855 5483 5029 463 1090 1243 438 4433 5060 4866 5637\nTable 12: Data statistics (number of sentence pairs or thousands of sentence pairs for training data)\nfor languages used in multilingual translation experiments.\nmodels. We learn a Sentencepiece vocabulary of size 40k on the training data. We evaluate the models after\n100k updates (as opposed to [8] who train for 1.5 million updates) with a per-GPU batch size of 8192 tokens\nand 32 GPUs. Note we do not use language-aware latent variables despite the multilingual training data. We\nuse the Adam optimizer with learning rate of either 5e-4, 2.5e-4 (24 or 48 layers) or 1.25e-4 (96 layers) and\nlinear warmup followed by polynomial decay with either 5000 (24 or 48 layers) or 15000 (96 layers) warmup\nsteps. For our static model with 96 layers we further tried increasing warmup to 30000 steps and decreasing the\nlearning rate to 1.5625e-5 but this did not help with training loss divergence issues. When using LayerDrop\nwe use 50% dropout probability. We re-use all other hyperparameters from XLM-R [ 8] (i.e. token masking\nprobability etc.).\nMultilingual Machine Translation. For multilingual experiments, we use a single Transformer network\nshared across all languages for both the encoder and decoder, with the same model size as used in [32]. We use\nthe same optimization hyperparameters (learning rate, warm up schedule, etc) as used in WMT English-German\nexperiments except that the batch size is 4096 tokens per-language and we train the model for 14k updates, and\nevaluated on the last checkpoint. Similarly, we use beam search with beam size 5 and length penalty 1.0 for\ndecoding.\nD Visualizations\nWe provide visualizations of the layer selection samples zl to further illustrate modeling choices around LK and\npriors.\n14\nEffect ofLK. First, we show that adding the auxiliary loss LK discretizes the samples and achieve the\npruning purpose by enforcing sparsity of the resulting model. In Figure 8, we visualized samples throughout\ntraining using the WMT’16 English-German dataset. Since decoder depth directly contributes to latency at\ninference time, we only apply LK with K = 12 to latent layers training in decoder and not in encoder. We could\nsee that samples zl in decoder becomes discrete throughout training while samples in encoder stay continuous.\nEncoder Decoder\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34\n0.0\n0.5\n1.0\nFigure 8: Layer selection samples throughout training evaluated on the WMT’16 English-German\ndataset. Rows correspond to samples from encoder and decoder with 36 latent layers at epoch 2, 6,\n25, 50, and 100 respectively. LK (K = 12) was applied to decoder only and not encoder to contrast\nthe discretizing and pruning effect.\nEffect of priors. In Section 5 we showed the difference between using an uniform prior Beta(1,1) and\naggregated posterior ˜q(z) in the early stage of training. In Figure 9, we further compared the resulting samples\nused at inference time, where we can see that using aggregated posterior ˜q(z) leads to more consistent sampling\nbehavior for each layer (either “all select\" or “all skip\") across languages and thus obtain increased sparsity\nand a more compact model. We used the O2M-Related language group for evaluation, where we could observe\nqualitatively common layer selection patterns for languages of the same language family, e.g. aze (Azerbaijani)\nand tur (Turkish), bel (Belorussian) and rus (Russian), glg (Galician) and por (Portuguese), slk (Slovak) and ces\n(Czech). We leave a systematic study of layer selection and linguistic similarity to future work.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\naze\ntur\nbel\nrus\nglg\npor\nslk\nces\nDKL(q(z)||Beta(1,1))\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\nDKL(q(z)||q(z))\n0.0\n0.5\n1.0\nFigure 9: Layer selection samples zl at inference time trained with uniform prior (left) and aggregated\nposterior ˜q(z) (right) in DKL. Compared to the uniform prior, using aggregated posterior is more\neffective for “pruning\" by encouraging consistent “select\" and “skip\" across languages. For example,\nlayer 0, 2, 6, and 23 can be complete pruned for all languages besides language-speciﬁc pruning (e.g.\nfor each language/row, layers corresponding to lighter cells could be pruned to derive a sub-network\nfor the given language). This property is appealing for deploying one shared multilingual model for\nall languages.\n15"
}