{
  "title": "How understanding large language models can inform the use of ChatGPT in physics education",
  "url": "https://openalex.org/W4389574762",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092926042",
      "name": "Giulia Polverini",
      "affiliations": [
        "Uppsala University"
      ]
    },
    {
      "id": "https://openalex.org/A1997897923",
      "name": "Bor Gregorcic",
      "affiliations": [
        "Uppsala University"
      ]
    },
    {
      "id": "https://openalex.org/A5092926042",
      "name": "Giulia Polverini",
      "affiliations": [
        "Uppsala University"
      ]
    },
    {
      "id": "https://openalex.org/A1997897923",
      "name": "Bor Gregorcic",
      "affiliations": [
        "Uppsala University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6803096969",
    "https://openalex.org/W6791376898",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W6849422333",
    "https://openalex.org/W4362723020",
    "https://openalex.org/W6850668563",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W6849710474",
    "https://openalex.org/W6850936240",
    "https://openalex.org/W4360846784",
    "https://openalex.org/W4381895526",
    "https://openalex.org/W4377855098",
    "https://openalex.org/W1985858077",
    "https://openalex.org/W6850305593",
    "https://openalex.org/W6851148351",
    "https://openalex.org/W4362551436",
    "https://openalex.org/W4391575364",
    "https://openalex.org/W6853958801",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W6854515434",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4391215636",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W6776231494",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W4366420437",
    "https://openalex.org/W6854561919",
    "https://openalex.org/W6855664720",
    "https://openalex.org/W4387821331",
    "https://openalex.org/W4387624562",
    "https://openalex.org/W6856009283",
    "https://openalex.org/W4385852384",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W4392669753",
    "https://openalex.org/W6852331649",
    "https://openalex.org/W4391836239",
    "https://openalex.org/W6874908499",
    "https://openalex.org/W2157615591",
    "https://openalex.org/W2115886036",
    "https://openalex.org/W4384008291",
    "https://openalex.org/W4313582009",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4384834621",
    "https://openalex.org/W4399203759",
    "https://openalex.org/W6850202480",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W6851841283",
    "https://openalex.org/W6839469050",
    "https://openalex.org/W6852353377",
    "https://openalex.org/W6849413121",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W6638714428",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W6810162553",
    "https://openalex.org/W6852712537",
    "https://openalex.org/W6853465110",
    "https://openalex.org/W6850401776",
    "https://openalex.org/W6852908079",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4323665984",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4381982883",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4378194664",
    "https://openalex.org/W4323313947",
    "https://openalex.org/W4323536358",
    "https://openalex.org/W4362581834",
    "https://openalex.org/W4389437528",
    "https://openalex.org/W4360980141",
    "https://openalex.org/W4323359867",
    "https://openalex.org/W3202285719",
    "https://openalex.org/W4366377796",
    "https://openalex.org/W4399390300",
    "https://openalex.org/W4381805978",
    "https://openalex.org/W4364377381",
    "https://openalex.org/W4361204578",
    "https://openalex.org/W6921321814",
    "https://openalex.org/W4362597819",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4318899036",
    "https://openalex.org/W4382492148",
    "https://openalex.org/W4385262268",
    "https://openalex.org/W4366249120",
    "https://openalex.org/W1843246605",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4384808216",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4366459745",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4376653732",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W4361865432",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4404782964",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4404782209",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4323066793",
    "https://openalex.org/W4378473736",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W4386839859",
    "https://openalex.org/W4386302153",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "Abstract The paper aims to fulfil three main functions: (1) to serve as an introduction for the physics education community to the functioning of large language models (LLMs), (2) to present a series of illustrative examples demonstrating how prompt-engineering techniques can impact LLMs performance on conceptual physics tasks and (3) to discuss potential implications of the understanding of LLMs and prompt engineering for physics teaching and learning. We first summarise existing research on the performance of a popular LLM-based chatbot (ChatGPT) on physics tasks. We then give a basic account of how LLMs work, illustrate essential features of their functioning, and discuss their strengths and limitations. Equipped with this knowledge, we discuss some challenges with generating useful output with ChatGPT-4 in the context of introductory physics, paying special attention to conceptual questions and problems. We then provide a condensed overview of relevant literature on prompt engineering and demonstrate through illustrative examples how selected prompt-engineering techniques can be employed to improve ChatGPT-4 ’s output on conceptual introductory physics problems. Qualitatively studying these examples provides additional insights into ChatGPT’s functioning and its utility in physics problem-solving. Finally, we consider how insights from the paper can inform the use of LLMs in the teaching and learning of physics.",
  "full_text": "European Journal of\nPhysics\n       \nPAPER • OPEN ACCESS\nHow understanding large language models can\ninform the use of ChatGPT in physics education\nTo cite this article: Giulia Polverini and Bor Gregorcic 2024 Eur. J. Phys. 45 025701\n \nView the article online for updates and enhancements.\nYou may also like\nArtificial intelligence and undergraduate\nphysics education\nJoseph J Trout and Lauren Winterbottom\n-\nChatGPT as a tool for honing teachers’\nSocratic dialogue skills\nBor Gregorcic, Giulia Polverini and\nAndreja Sarlah\n-\nExploring the role of large language\nmodels in radiation emergency response\nAnirudh Chandra and Abinash\nChakraborty\n-\nThis content was downloaded from IP address 91.100.61.210 on 05/11/2025 at 13:46\nHow understanding large language models\ncan inform the use of ChatGPT in physics\neducation\nGiulia Polverini and Bor Gregorcic\nDepartment of Physics and Astronomy, Uppsala University, Box 516, SE-75120\nUppsala, Sweden\nE-mail: bor.gregorcic@physics.uu.se\nReceived 21 September 2023, revised 1 November 2023\nAccepted for publication 11 December 2023\nPublished 29 January 2024\nAbstract\nThe paper aims to fulﬁl three main functions:(1) to serve as an introduction\nfor the physics education community to the functioning of large language\nmodels (LLMs), (2) to present a series of illustrative examples demonstrating\nhow prompt-engineering techniques can impact LLMs performance on con-\nceptual physics tasks and(3) to discuss potential implications of the under-\nstanding of LLMs and prompt engineering for physics teaching and learning.\nWe ﬁrst summarise existing research on the performance of a popular LLM-\nbased chatbot (ChatGPT) on physics tasks. We then give a basic account of\nhow LLMs work, illustrate essential features of their functioning, and discuss\ntheir strengths and limitations. Equipped with this knowledge, we discuss\nsome challenges with generating useful output withChatGPT-4 in the context\nof introductory physics, paying special attention to conceptual questions and\nproblems. We then provide a condensed overview of relevant literature on\nprompt engineering and demonstrate through illustrative examples how\nselected prompt-engineering techniques can be employed to improve\nChatGPT-4’s output on conceptual introductory physics problems. Qualita-\ntively studying these examples provides additional insights into ChatGPT’s\nfunctioning and its utility in physics problem-solving. Finally, we consider\nhow insights from the paper can inform the use of LLMs in the teaching and\nlearning of physics.\nEuropean Journal of Physics\nEur. J. Phys.45 (2024) 025701 (35pp) https://doi.org/10.1088/1361-6404/ad1420\nOriginal content from this work may be used under the terms of theCreative Commons\nAttribution 4.0 licence. Any further distribution of this work must maintain attribution to the\nauthor(s) and the title of the work, journal citation and DOI.\n© 2024 The Author(s). Published on behalf of the European Physical Society by IOP Publishing Ltd 1\nKeywords: physics education, large language models, prompt engineering,\nGPT-4, ChatGPT, conceptual physics tasks, artiﬁcial intelligence in education\n1. Introduction\nIn late 2022, the public release of a large language model-based chatbot, OpenAI’s ChatGPT,\nintroduced a new type of entity into the educational landscape, showing the potential for\nsigniﬁcant disruptions of educational processes[1]. Recent reports from Sweden, for exam-\nple, show that around half of high-school students have already used ChatGPT for‘cheating’\non school assignments[2] and similarﬁndings are emerging from elsewhere[3]. As we come\nto terms with large language models(LLMs) already being part of the teaching and learning\necosystem in different subjects, including physics, it is important for the community of\nphysics educators to build some understanding of their ways of functioning and what they are\ncapable of. In response to the emerging need for a better understanding of LLMs[1], the aim\nof this paper is to present some rudimentary concepts concerning LLMs’ functioning on a\nlevel that can be understood by physics educators and physics education researchers without\nin-depth knowledge of machine learning. By acquiring some rudimentary understanding of\nLLMs’ functioning, physics educators can better understand these models’ inherent strengths\nand limitations and leverage this understanding to inform their practice.\nIn section2, we summarise the still relatively sparse but quickly growing body of research\non the performance of LLMs on physics tasks, as well as provide a preview of our own\nﬁndings on the topic. In section3,w eﬁrst give a simpliﬁed technical overview of how LLMs\nfunction, leading up to an unpacking of their limitations and the discussion of key differences\nbetween LLMs and humans. These insights can help us build a better understanding of the\nﬁndings reported in the literature on LLMs’ performance in physics and more broadly.\nInsights from section3 also lay the ground for section4, where we demonstrate how our\ninteraction with ChatGPT-4 can be optimised for its performance in physics problem-solving.\nThrough a series of illustrative examples, we showcase two promising approaches to LLM\nprompting and discuss how insights from the process of designing prompts(also referred to as\nprompt engineering) can contribute to scholarly discussion in PER and adjacentﬁelds of\nstudy. Section5 offers a broader discussion on the educational implications of understanding\nLLMs’ functionality.\n2. Existing research on LLM performance on physics tasks\nEven before the public release of the now well-known LLM-powered chatbot ChatGPT,\nLLMs were studied in different contexts and were found to be well-performing in natural\nlanguage tasks, including common-sense reasoning, question-answering, and summarisation\n[4, 5]. They were also found to be struggling with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems[6, 7], and were likely to\nmake up false facts\n[8]. Since the release of ChatGPT, thisﬁeld of research has signiﬁcantly\nexpanded, with scholars in differentﬁelds studying ChatGPT’s performance in their subject\nareas [9–12]. Scholars generally ﬁnd the progress in the development of the technology\nimpressive, but also point out its limited abilities in different areas.\nToday, GPT-4 is considered the state-of-the-art LLM, outperforming its predecessor, GPT-\n3.5, on most academic and professional exams[13–15]. Bubeck et al [16] provide an early\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n2\nbut accurate demonstration of how GPT-4 can solve novel complex tasks on an almost\nhuman-like level of performance. However, its level of performance in mathematics has been\nshown to be quite far from that of experts and it can still fail in some elementary mathematical\nand common-sense reasoning tasks[13, 16], when not augmented by external plugins.\nThe PER community has also been quick to start exploring how ChatGPT performs on\ndifferent physics-related tasks. Gregorcic and Pendrill[17] presented a case study in which\nChatGPT-3.5’s answer to a simple problem about mechanics contains wrong and often\ncontradictory physics reasoning. Drawing on this work, dos Santos[18] proposed the same\nquestion to several LLM-powered chatbots (ChatGPT-4, Bing, and Bard ), carefully\ndescribing the quality of all the outputs. He found that ChatGPT-4 outperforms the other\nchatbots (including its previous version) by providing a both correct and detailed answer.\nMore importantly, it does not present any inconsistency in its arguments. This work is\ninteresting because it highlights some important differences between typical human reasoning\nand the chatbot’s output. In order to observe these differences, however, one needs to ask the\nchatbot to answer an open question or to explain how it came to its conclusion.\nOn the other hand, looking at the overall performance of ChatGPT on a multiple-choice\nquestionnaire, it becomes almost impossible to distinguish between a human and an LLM.\nKortemeyer [19] tested ChatGPT-3.5 performance on the Force Concept Inventory(FCI)\n[20], where it scored comparably to beginner students(a score of 50%–60%). Theseﬁndings\nare corroborated by West’s [21] study. In a successive paper, West[22] highlighted the\nimprovements in performance on FCI that characterise the upgrade from ChatGPT-3.5 to\nChatGPT-4. He found that ChatGPT-4 typically scores between 90% and 100% correct on the\nFCI, which is comparable to the performance of physics experts. West[22] also found that the\nchatbot’s responses on the FCI became more stable. While ChatGPT-3.5 typically provided\nsigniﬁcantly different responses on consecutive attempts, ChatGPT-4 has displayed a zero\nmedian amount of variation in its responses.\nGoing beyond multiple-choice questionnaires, Yeadon et al [23] demonstrated that\nChatGPT-3.5 would achieveﬁrst-class grades on short-form physics essays in a university\nphysics course about the history and philosophy of physics. Kortemeyer[19] challenged\nChatGPT-3.5 on different kinds of assessment problems taken from an introductory university\nphysics course. The chatbot performance varied from 46% to 55% of correctness on exams\nand homework, to 90%–93% in programming exercises and clicker questions. A recent study\nby Yeadon and Hardy[\n24] assessed ChatGPT-3.5’s ability to answer physics exam questions\nacross three different academic levels, from secondary school to introductory university\ncourses. They found that at the introductory university level, ChatGPT-3.5 correctly solved\nbetween 30 and 40% of problems.\nWhile the performance of ChatGPT-3.5 and -4 on some of the above-mentioned tasks has\nbeen reported to be very good(especially on FCI, essay writing, and programming tasks),\neven its most recent version(ChatGPT-4) was found to perform quite unreliably on a diverse\nset of physics examinations(i.e. Foundations of Physics 1 and 2, Theoretical Astrophysics,\netc) [25]. These ﬁndings suggest that the relatively unimpressive performance that Korte-\nmeyer [19] and Yeadon and colleagues[24, 25] found with ChatGPT-3.5 remains an issue\nwith ChatGPT-4. Notable exceptions here are programming tasks, on which ChatGPT-3.5\nalready performed very well, and FCI tasks, on which ChatGPT-4 showed signi ﬁcant\nimprovement [22]. Based on theseﬁndings, Yeadon and Halliday[25] argue that in most\ncases, only the most poorly performing students would beneﬁt from their exams being solved\nby ChatGPT-3.5 or -4. They conclude that the chatbot does not yet pose a signiﬁcant threat to\nthe ﬁdelity of physics assessment exams similar to those used in their study.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n3\nIn our own investigation, we did not opt for FCI tasks, since they have already been\nstudied, and there is a signiﬁcant risk that they were also present extensively in the latest\nmodel’s training data. Most of the tasks that we used in our investigations presented in this\npaper were original, inspired by end-of-chapter problems from Etkina, Planinsic and Van\nHeuvelen’s College Physics: Explore and apply textbook [26]. The tasks we used were\nconceptually oriented, with little to no computation required, but often requiring some simple\nsymbolic mathematical reasoning to solve.\nThroughout sections3 and 4, we present excerpts from our dialogue with ChatGPT-4 that\nadd to the body of research on ChatGPT’s performance on physics tasks, further supporting\nthe ﬁnding that it is still quite unreliable in solving them. Based on theseﬁndings, we could\nindeed be led to conclude that even the most state-of-the-art LLM-based chatbot(ChatGPT-4)\nis not up to the task of reliably solving conceptual physics problems. However, as we\nillustrate in section 4, an LLM’s performance (reliability and correctness) on conceptual\nphysics tasks can be signiﬁcantly improved by applying a relatively basic understanding of\nhow LLMs function and using simple but effective prompt-engineering techniques. We\nwould like to note that the examples presented in this paper were generated in May 2023.\nBecause of the rapid pace of development in theﬁeld and constant improvement made to\nChatGPT-4, it is possible that the same prompts will not result in similar outputs now.\nTherefore, the primary role of the dialogue excerpts in the paper is not to serve as data for a\ncomprehensive assessment of ChatGPT-4’s performance or to predict its future performance\non conceptual physics tasks. Instead, we use the examples to support our discussion of(1)\nhow LLMs function and(2) how prompt engineering can improve LLMs’ performance on\nconceptual physics tasks.\n3. How LLMs work\n3.1. A simplified overview of the fundamental principles of LLM functioning\nIn 2022, we have witnessed the public release and mass uptake of several modern artiﬁcial\nintelligence programs utilising machine learning to process large amounts of data, including\nimages, audio recordings and text. Furthermore, in addition to the ability to process data,\nthese programs can generate outputs that resemble the data that they were fed. For this reason,\nthey are often referred to as generative AI. One subset of these so-called generative AI\nprograms is LLMs. LLMs were designed to process and generate natural language but have\nalso proven helpful for processing and generating programming languages and even symbolic\nlanguages, such as mathematics. OpenAI’s GPT series and its implementation in the chatbot\napplication ChatGPT is the most prominent example of a broadly accessible LLM application.\nThe GPT acronym stands forgenerative pre-trained transformer[27] and is nowadays often\nused as a synonym for LLMs. To set the stage for a rudimentary explanation of how LLMs\nwork, we brieﬂy unpack the GPT acronym. For an explanation of the difference between GPT\nand ChatGPT, see the following section3.2.\n Generative Early language models were mostly designed to encode input. The LLMs that\nhave become known to the public in the last year also have the ability to generate text as\noutput. This ability is ostensibly what made them interesting to the broader public.\nOpenAI’s GPT-3 and GPT-4, for example, are both generative models that output text\nbased on their training and a user-provided textual prompt.\n Pre-trained An LLM such as GPT is essentially a neural network with a large number\n(counted in billions) of learning parameters. Training an LLM involves using machine\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n4\nlearning algorithms to process and encode a large corpus of text. The training process\ntypically requires a lot of computing resources and results in a pre-trained neural\nnetwork, which is not changed further as the model is used by the end user. Two crucial\nfactors for the success and broad applicability of LLMs, such as the GPT series, are their\nefﬁcient architecture and a sufﬁcient quantity and quality1 of the training data[28]. While\nOpenAI did not openly declare what textual datasets they used to train GPT-4, their size is\nspeculated to be roughly comparable to that of the entire internet.\n Transformer Before 2018, LLM training was difﬁcult to do at large scales because of the\ninefﬁciency of the machine learning approaches and the need for manually processing\ndata before it could be fed into the pre-training process. In 2018, the transformer\narchitecture signiﬁcantly simpliﬁed the pre-training process by making it possible for\nLLMs to be trained on essentially unprocessed text, dramatically decreasing the difﬁculty\nof model training on truly large datasets. The transformer architecture was a result of\ndevelopmental work at Google for the purposes of machine translation. It makes use of\nspecial mechanisms (‘attention’ and ‘self-attention’) which allow the model to\nsuccessfully capture complex relations between words in a given input. For a more\ntechnical discussion of the transformer architecture, see[29].\nThe LLM training process is done by mechanisms of deep learning, where a computer\nprogram automatically processes a large amount of language data and encodes its regularities\ninto a large neural network. Using some additional steps, this neutral network can also be\nused to generate natural language output. A familiar tool that illustrates the way LLMs\ngenerate text is that of an auto-complete function used in popular online e-mail services.\nGiven some initial input(such as the beginning of a sentence), the model produces the output\nbased on what is essentially an advanced form of statistical inference. The inference is done\nby running a predictive algorithm based on the pre-trained neural network and the speciﬁc\ntextual input given to it on the spot, also referred to as theprompt. If we were to provide a\nsomewhat simpliﬁed account of the way LLMs generate output, we could say that the LLM\nalways answers the question:‘According to your model of the statistics of human language,\nwhat words are likely to come next?’ [30 p. 2].\nAs a consequence of the functioning principles of LLMs, the output is always produced on\nthe spot as a statistical inference of the most likely words to follow the prompt. Thus, it does\nnot directly reproduce parts of the data it was trained on, even if some sequences of words can\nappear in the training data with such regularity that the model is likely to reproduce them\nverbatim (e.g. standard formulations of Newton’s laws). Still, it is essential to note that this is\nnot because the model has directly copied its output from the learning dataset but because it is\nstatistically a very likely sequence of words. This has several interesting consequences. For\nexample, the LLM’s output tends to be of high quality in terms of syntax and richness of\nvocabulary. On the other hand, the same process of statistical inference can also produce\nsequences of words which are in their content untrue when checked against real-world\ninformation. Such statements are often referred to as‘hallucinations in natural language\ngeneration’ [31] and represent an important challenge to LLMs’ usage in different contexts.\nWe further address this issue in section3.3.2.\nTo illustrate the working principle of an LLM, we present two example texts generated on\nOpenAI’\ns GPT playground (https://platform.openai.com/playground)— a developer appli-\ncation that gives end users access to their GPT-3‘auto-complete’ model, also referred to as\n1 With extremely large models such as GPT-4 it is less clear if data quality plays as important of a role as it did in\nsmaller models.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n5\n‘the bare-bones’ model, and allows them to modify some of its settings. GPT playground also\nallows end users to display a ranked list of words that the model deemed to be the most likely\nchoice for continuing the output sentence(see example below). The words that we gave to\nGPT-3 as input from which to continue the text were:‘In physics, force is’ and ‘In law, force\nis’, respectively. Figure 1 shows both outputs and the calculated probabilities for theﬁrst\nnoun in the continuation of the sentence generated by GPT. If the LLM is set to always\nchoose the most likely word from the list shown inﬁgure 1, the output becomes deterministic,\nalways resulting in the same string of words. In LLM applications this is not always desired,\nso some randomness is often intentionally introduced. The degree of randomness is controlled\nby the ‘temperature’ setting. A high temperature setting will result in more diverse outputs,\nwith the probability of a word being selected depending both on the‘temperature’ and a\nword’s statistical weight(percentage value displayed next to it in the list shown inﬁgure 1).\nNote that‘temperature’ can be controlled by the user in OpenAI playground or in ChatGPT’s\napplication programming interface intended for app developers, but not in ChatGPT, where it\nis set to a non-zero value.\nWe can see from the two examples provided inﬁgure 1 that GPT's output is context-\nsensitive [32]. The given‘deﬁnitions’ of force do not bleed into each other but are provided\nindependently in their respective domains (physics and law).\n2 The model responds to a\ndiscipline-speciﬁc prompt by focusing on a speci ﬁc disciplinary domain. This will be\nimportant for considerations that we bring up later in the paper.\n3.2. From LLMs to chatbots\nMost readers will likely be familiar with ChatGPT, which, as we brieﬂy allude to in the\nprevious section, is not quite the same as the GPT models. If ChatGPT were a vehicle, then\nGPT would be its engine. When an LLM(GPT) is used for a chatbot application(ChatGPT),\nadditional steps need to be taken to provide the users with a good interaction experience. A\nbare-bones ‘advanced auto-complete algorithm’ model, like GPT, does not yet automatically\nallow for a dialogue-like interaction\n3 or following the user’s instructions. As an advanced\nauto-complete algorithm, an LLM is more likely to continue the instruction than execute what\nthe instructions say. There is a similar problem with answering user questions. Shanahan[30]\nexplains that because answers to questions rarely directly follow the question itself in most\ndata sources, it is unlikely that‘auto-completing’ the text following the question mark would\nprovide the answer to the question asked. One technique to make it behave like a conversation\npartner and actually answer the question is embedding a user-posed question into the format\nof a script of a dialogue between two actors and giving the LLM the task of continuing the\ndialogue by writing the line of one of the actors.\n4 For an illustration of the use of this trick, see\nsection 4.3.3.\nIn addition to such prompting tricks, going from GPT to ChatGPT involved additional pre-\ntraining steps. Following the initial phase of automatised machine learning on large datasets,\nadditional training was done in the form of what is referred to asreinforcement learning from\nhuman and AI feedback.Reinforcement learning is a process of teaching an AI program the\ndesired behaviour by establishing a reward-seeking system that aims to achieve a given\nobjective. In the case of ChatGPT, this involved humans manually ranking a large number of\nthe model’s responses by their desirability, feeding that information into a machine learning\n2 This context sensitivity ensured through the mechanism of self-attention[29], is an important feature of the\ntransformer architecture and signiﬁcantly contributed to its broad uptake.\n3 The LLM itself includes just the model architecture and pre-trained learning parameters.\n4 This ‘trick’ can provide aﬁrst glimpse into the craft of prompt engineering, discussed in section4.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n6\nFigure 1. GPT playground gives end users access to OpenAI's GPT bare-bones model (not\nChatGPT), allowing for experimentation and displaying probabilities of words that follow the\npreceding words. Note that in the physics context the word ‘interaction’ has a much larger\nstatistical weight than the others. This means that even with non-zero‘temperature’ it will be the\nmost likely choice. On the other hand, in the context of law, the word‘power’ is not equally\ndominating. This will result in a more diverse output at the same‘temperature’.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n7\nalgorithm, and using it toﬁne-tune the model for use as a chatbot. ChatGPT is therefore an\napplication built upon GPT. Creating it involved additional training steps, ensuring that it\nbehaves like a human conversational partner.\nIn this paper, we use the shorthands ChatGPT-3.5 and ChatGPT-4 to refer to ChatGPT\nusing GPT-3.5 or GPT-4 as its underlying model, respectively. At the time of writing,\nChaGPT-3.5 is available as a cost-free service in most western countries, while ChatGPT-4\nrequires a paid subscription of around 20 USD per month.\n3.3. Limitations of LLMs\nFrom the previous sections, we can already see that the process through which LLMs predict\nand generate word sequences does not involve any form of human-like understanding,\nknowing, thinking or reasoning. A bare-bones LLM such as GPT has no perception of the\ndifﬁculty of the task we are asking it to solve, nor if the task is meaningful or reasonable. It\nmerely ‘produces the text that is a statistically goodﬁt, given the starting text’ [33 p. 684].\nThere are real downsides to such models, which are in turn passed down to chatbots. Borji\n[34] lists eleven categories of possible failures of ChatGPT, including errors in reasoning,\nmath, coding, and factual errors, as well as the display of bias, all carrying potential risks and\nsocietal implications. Increasing the amount of learning data used to train the LLM can\npositively inﬂuence the correctness of the outputs, but it seems to do this better in some\ndomains compared to others. For example, Lo [35] shows how ChatGPT achieves out-\nstanding performance in economics and programming tasks while it performs less well in law\nand medical education. Below, we take a closer look of(1) LLMs’ bias, (2) their tendency to\ngenerate false information,(3) the prospect of the use of external plugins to remedy some of\nLLMs’ limitations, and (4) LLMs’ challenges with coherent argumentation, particularly in\nphysics contexts.\n3.3.1. Bias. The training dataset on which LLMs are trained often contains different forms\nof bias. This is not surprising, considering that the data consists of different sources, including\nthe internet, and it is often too large to be actively supervised to eliminate even overtly biased\nor hateful content. In the training process, the LLM acquires the bias similarly to any other\nstatistical correlation in the training data. Bias is then reﬂected, for example, by LLMs’\ntendencies to relate certain traits to speciﬁc groups of people, often in a stereotypical way.\nResearchers have reported that LLMs exhibit bias regarding gender[36, 37], race [38, 39],\nreligion and caste [40], as well as political views[41]. It is noteworthy that these biases\npresent an important challenge for LLMs’ use in education. Learners using LLMs without\nbeing aware and critically reﬂecting on their biases may result in their further propagation.\nUnderstanding, measuring and mitigating bias in LLMs are challenging tasks, requiring in-\ndepth understanding of the different phases of LLM training[42].\n3.3.2. Factually inaccurate information. Another commonly discussed limitation of LLMs is\nthat they can provide factually inaccurate information. As mentioned earlier, LLMs are not\ndatabase retrieval tools[43]. Their mapping of the training data is‘compressed’ in a lossy\nmanner [44], meaning that the model cannot retrieve the data in its original form, and its\noutput is always the result of on-the-spot text generation based on statistical inference\n5 [45].\nEven when the training data is factually correct, LLM-generated output can contain factually\n5 This is also why LLM-generated text often goes undetected though plagiarism detection software, which looks for\ndirect matches of a piece of text against large databases.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n8\nincorrect statements. Such behaviour is often referred to asAI hallucination [31]. Factually\nincorrect or made-up statements can be particularly problematic in ﬁelds such as law,\nmedicine or history, but also elsewhere. Yet, the so-called hallucinations can also play a\nproductive role in creative processes, where novel ideas not previously present in the training\ndata are actually desired.\nIn physics contexts, and especially in the context of learning introductory physics,\nconfabulated or untrue statements tend to manifest themselves somewhat differently, as retrieval\nof large amounts of factual information is not typically central to physics tasks. Mathematically\nincorrect statements or mistakes in logic, on the other hand, are more relevant for physics\napplications(and arguably harder to spot than, for example, a wrong value of an otherwise well-\nknown physics constant). For example, researchers have shown that even basic operations, such\nas multiplication,can present a signiﬁcant challenge to GPT-3[46]. This is a direct consequence\nof the fact that LLMs are not actually performing calculations but are instead producing an answer\nusing statistical inference from its learning data. Such‘arithmetic confabulations’ nicely illustrate\nthe limitations of the LLM functioning. Even if the data contains many correct examples of\nnumbers being multiplied and the result of themultiplication, the model cannot extract and\nimplement a general rule for multiplying arbitrary pairs of numbers and on its own does not have\nthe capacity to judge the correctness of the answer, nor can it relate critically to its own capability\nto multiply numbers. Indeed, OpenAI’s state-of-the-art application, ChatGPT-4, as of summer\n2023, still cannot reliably multiply four-digit numbers and regularly returns incorrect results when\nused in‘default mode’, which relies on the underlying LLM as the source of information for\ngenerating responses.\n3.3.3. Plugins. In summer 2023, OpenAI introduced a beta feature called‘plugin mode’,i n\naddition to the‘default’ mode, for subscription-paying users of ChatGPT-4. A plugin is an\nexternal module, which is typically tasked with operations that the LLM cannot perform\nadequately. Plugins, such as Wolfram represent an attempt at improving ChatGPT ’s\nmathematical abilities[47]. Other kinds of plugins have also become available for enhancing\nChatGPT-4’s performance.6 However, for this approach to work well, the chatbot would need\nto reliably make autonomous decisions about when to outsource tasks to external modules.\nAchieving this remains a challenge for LLM and chatbot developers. However, when such a\ndecision can conﬁdently be made by the user, they can explicitly instruct the chatbot to\noutsource certain operations. For example, a user can enable the Wolfram plugin[47] and\ninstruct the chatbot explicitly to use it in solving a computational task. This way, plugins can,\nwhen in the hands of experienced users, signiﬁcantly enhance the usability of ChatGPT-4 for\ntasks that extend beyond what LLMs can reliably do on their own. Another example of the\nplugin approach to the enhancement of ChatGPT’s computational abilities is the introduction\nof a beta feature,‘Advanced data analysis mode’ (previously known as the Code interpreter\nplugin), made available to subscription-paying users of ChatGPT in late summer 2023. It\nintroduces a code-compiler within the chat window, allowing ChatGPT-4 to not only write,\nbut also execute Python code. This feature signiﬁcantly improves the chatbot’s ability to\nperform calculations and other tasks that have proven to be challenging to LLMs without\nplugins, also referred to as ChatGPT-4’s ‘default mode’. While plugins are a relatively recent\ndevelopment on the chatbot landscape, in the future they will likely play an important role in\nhow LLM-based tools are used in a wide range of contexts, including physics and physics\neducation.\n6 As of 1 November, 2023, more than 1000 different plugins are available to users of ChatGPT-4.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n9\nFor generating examples in this paper, we used ChatGPT-4 in its‘default mode’,w h i c hd o e s\nnot make use of plugins. We did this for two main reasons. First, this paper focuses predominantly\non tasks that do not involve computation. Second, we believe that having a basic understanding of\nLLMs functioning without plugins is necessary for the productive use of plugins down the line.\nHowever, a systematic and in-depth discussion of plugins falls beyond the scope of this paper.\n3.3.4. Problematic argumentation. Another challenge for the use of LLMs in the domain of\nphysics, is that they often produce incoherent or inconsistent chains of argumentation. For\nexample, in introductory classical mechanics,the challenge rarely lies in being able to merely\nrecite known conceptual and mathematical relations that describe physics models, or to perform\ncalculations. In fact, the relations and fundamental principles are so few and consistently\nformulated in the existing texts that LLMs typically have no issue producing them without\nmistakes. The real challenge lies instead in the consistent application of basic principles toﬁnd\nramiﬁcations of physics models and generate predictions about the physical world[48]. Recalling\na solution to a problem is rarely the point of learning physics[49]. On the contrary, it is how one\narrives at the solution that matters. This typically requires using both mathematical and physics\nconcepts. Physics thus presents a particularly interesting context for exploring LLMs’ abilities to\nproduce coherent conceptual and mathematical argumentation.\nBelow, we provide an illustrative pair of examples demonstrating how ChatGPT-4 deals with\na conceptual introductory physics question that does not involve any numerical calculation. The\nexamples showcase that ChatGPT-4 is still quite unreliable at answering such questions.\nIf two bodies with different mass have the same kinetic energy, which one has the\nlargest momentum?\nWe regenerated the response in ChatGPT-4 eight times. We got the correct answer that the\nbody with the larger mass has the larger momentum in four out of the eight responses. Below,\nwe ﬁrst present an example of a response with a correct conclusion, even though a more\nthorough explanation would be desirable, especially towards the last part of the response.\nThe kinetic energy(K) of a body is given by the formula:\nK = 1/2 × m × v\n2\nwhere:\nm is the mass of the body, and\nv is the velocity of the body.\nThe momentum (p) of a body is given by the formula:\np = m × v\nwhere again:\nm is the mass of the body, and\nv is the velocity of the body.\nIf two bodies have the same kinetic energy but different masses, it means that the body\nwith the smaller mass must be moving with a higher velocity to make up for the smaller\nmass in the kinetic energy formula.\nHowever, when you look at the momentum formula, it becomes clear that even though the\nsmaller mass body is moving faster, it might not have more momentum than the larger\nmass body. This is because momentum is directly proportional to both mass and velocity.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n10\nTherefore, for two bodies with different masses but the same kinetic energy, the body\nwith the larger mass would have the larger momentum.\nWe regenerated the response to this task through the ChatGPT-4 end-user interface by\nstarting a new chat each time. For four out of eight responses, we got an incorrect conclusion\nthat the object with the smaller mass has larger momentum. Below, we now also present one\nexample of an incorrect answer:\nKinetic energy(KE) and momentum(p) are related to the mass(m) and velocity(v) of an\nobject as follows:\nKE = 1/2 × m × v\n2\np = m × v\nIf two bodies have the same kinetic energy, but different masses, we can analyze the\nsituation using the above formulas.\nLet's call the two objects A and B, with mass mA and mB(mA > mB) and velocity vA\nand vB respectively.\nSince the kinetic energy of both bodies is equal:\n1/2 × mA × vA2 = 1/2 × mB × vB2\nSolving for vA and vB from here:\nvA = sqrt((2 × KE)/mA)\nvB = sqrt((2 × KE)/mB)\nAnd since mA> mB, it follows that vA< vB.\nThe momentum of the two bodies would be:\npA = mA × vA\npB = mB × vB\nGiven vA< vB and mA> mB, we cannot directly determine which momentum is greater\nwithout knowing the speciﬁc values of mA and mB. However, because momentum is\nlinearly proportional to velocity while kinetic energy is proportional to the square of\nvelocity, the object with the lower mass(mB) will have a higher increase in velocity than\nthe decrease caused by its lower mass. Therefore, the object with less mass(and higher\nvelocity) will have a greater momentum when they have the same kinetic energy.\nFor this particular question, ChatGPT-4 returned a combination of mostly correct\nargumentation and a correctﬁnal response only for half of the attempts. Importantly, our\nsimple investigation does not provide us with a deﬁnitive measure of the rate of correctness of\nChatGPT-4's responses to physics questions but rather allows us to illustrate our own and\nothers' ﬁndings that the chatbot is unreliable and inconsistent even on basic conceptual\nproblems. This unreliability and inconsistency in producing well-argued and correct\nresponses, combined with what looks like an authoritative, plausible-sounding and conﬁdent\ntone, presents an important limitation for the use of ChatGPT-4 in physics education\n[17, 24, 50], especially if students wish to use it as a tutor. We will further discuss some\npossible ways of using LLMs in education in section5.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n11\n3.4. LLMs versus humans\nFrom the brief overview of LLMs’ functioning provided in this paper, we can already see at\nleast two important ways of how their way of working fundamentally differs from that of\nhumans.\nFirst, humans need far less linguistic input information to be able to produce text which is\ncomparable to even the most state-of-the-art LLM. To produce coherent and plausible-\nsounding output, LLMs need to be trained on truly huge datasets, which a single person could\nnot possibly consume in a lifetime or even multiple lifetimes.\nSecond, in addition to written language, human discourse is inﬂuenced by other inputs.\nThis is because people inhabit a world which is itself a source of non-linguistic information\n(e.g. visual, auditory, haptic). In addition, human learning is heavily inﬂuenced by implicit\nand explicit social rules and norms. In contrast to humans, LLMs lack non-linguistic real-\nworld experience. This is one of the reasons an LLM’s output can sometimes be incorrect in\nways that feel uncanny to a human reader. We have, for example, noticed that its answers to\nproblems on the topic of kinematics or dynamics(with a strong everyday coupling) some-\ntimes contain mistakes that a human would be very unlikely to make, such as that an object\nthrown vertically continues travelling upwards even after it has reached the highest point on\nits trajectory [17].\nOn the other hand, LLMs can also produce incorrect statements that mirror learners’ difﬁ-\nculties [17, 51]. Talanquer even suggested that this correspondence may reveal some underlying\nsimilarities in the process of the emergence of students’ and LLMs’ difﬁculties [51].\nDespite our understanding of the differences between how LLMs and humans function,\nanthropomorphising is relatively widespread in the discourse about LLMs, even in the\nresearch literature on the topic. Humans, including researchers, often explicitly or implicitly\nattribute to LLMs abilities like‘thinking’, ‘reasoning’, ‘understanding’, and ‘knowing’ [30]\nand, as we mentioned before,‘hallucinating’\n7 [31]. This is understandable since state-of-the-\nart LLM-based chatbots have been intentionally designed andﬁne-tuned to be human-like in\ntheir manifest behaviour.\nThere are, however, important pitfalls of anthropomorphising LLMs. Here, we only brieﬂy\ntouch on two general ways it can be problematic: technical and affective. From the technical\npoint of view, adherence to the interaction practices used in social situations between people can\nrepresent a barrier to using LLMs efﬁciently [53]. To make the best possible use of LLMs, they\nneed to be prompted in speciﬁc ways by providing several categories of information related to\nthe particular domain of the question at hand and other speciﬁcations that we usually would take\nfor granted in a conversation with a person. This topic will be further explored in section4,\ndealing with prompting techniques. Emotion-wise, important considerations need to be done\nwhen we allow ourselves or our students to spend time interacting with an LLM-based chatbot,\nespecially if this involves long and intensive periods of interaction. One such consideration is\nhow interacting with chatbots makes us feel and how it impacts our self-perception as learners\nand/or experts[54]. This topic will be further discussed in section5.\nUnderstanding the differences between humans and LLMs in terms of their functioning\nrepresents an important resource for using LLMs as tools. To make the best use of LLMs and\nhave realistic expectations of what they can offer, weﬁnd it useful to keep in mind and\nrepeatedly remind ourselves how LLMs differ from humans, even if, and especially because,\nthey are designed to convince us they are human(namely, by emulating human discourse).\n7 The term hallucination has been suggested to be misleading, because it implies that there is a perceptual\ncomponent to LLMs' acting. Alternatives to the term such as simply‘making things up’ or even‘bullshitting’ [52]\nhave been argued to be more representative of the mechanism of the production of these statements.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n12\nThis does not mean that all our intuitions about interpersonal communication are useless for\ninteracting with LLMs, but rather that we should be prepared to be selective and critical of the\nuse of these intuitions when interacting with LLMs.\n4. Improving the quality of LLM responses\n4.1. Introduction to prompt engineering\nA prompt is a natural language instruction given to an LLM to facilitate its generation of an\noutput. Prompting can thus be seen as a form of programming used to instruct an LLM[55].\nWhen dealing with a chatbot like ChatGPT speciﬁcally, a user prompt is a query that the user\nwrites for the chatbot using the chat interface. How a prompt is formulated highly impacts the\noutput of the model[56]. Therefore, learning to formulate appropriate prompts is crucial for\nthe productive use of LLMs and LLM-based chatbots in any context, including physics.\nThe craft of formulating effective prompts is often referred to asprompt engineering.\nPrompt engineering is a highly empiricalﬁeld. It is not uncommon to see an LLM fail a given\ntask but signiﬁcantly improve its performance once given a properly reﬁned prompt[57]. So,\nit has been argued that an LLM’s failure is not necessarily evidence of the lack of an LLM’s\ncapacity to solve similar problems. It could just mean that we have not yet learned how to\nformulate the prompt in a good way[58].\nDesigning an effective prompt is a non-trivial task, requiring at least some understanding\nof how LLMs work. Zamﬁrescu-Pereira et al[53] observed how end users with zero or very\nlimited familiarity with LLMs struggle with generating prompts, evaluating prompt effec-\ntiveness, and explaining the effects their prompts had on the LLM’s outputs. Without any in-\ndepth understanding of LLM mechanics or dedicated instruction on prompt engineering, the\nparticipants in the study tended to approach the formulation of inputs as if they were dealing\nwith a human. The authors[53] suggest that social experiences and expectations inﬂuence the\nresearch subjects’ attempts at prompting in a way that reduces the quality of the obtained\nLLM responses. This supports our argument that understanding LLMs functioning is useful if\nwe are to make good use of them. As discussed above, an important point of departure for\nprompt engineering is the understanding that we are not dealing with a human. Furthermore,\nit is important to understand LLMs’ inherent strengths and weaknesses. For example, LLMs\nare particularly well suited for tasks such as text summarization and paraphrasing but much\nworse at algebraic tasks and planning solutions to multiple-step problems, even those often\nperceived as trivial for humans[59]. With just this information in mind, we can already\nexpect that solving physics problems will likely present a challenge for LLMs. In our case,\nthis expectation stems not only from consulting others’ research but also from ﬁrst-hand\nexperience interacting with LLMs and asking them conceptual physical questions (see\nexamples presented in this paper).\nHowever, we will show that with some relatively simple prompt engineering techniques,\none can signiﬁcantly improve the performance of ChatGPT-4 on such physics tasks. By\nshowing some illustrative examples, we aim to demonstrate how the understanding of LLMs’\nfunctioning and applying this knowledge through prompt engineering can improve the cor-\nrectness and reliability of ChatGPT-4’s responses on introductory physics tasks.\nHere, we would also like to issue a word of caution. Due to the probabilistic mechanics of\nLLMs, it is unlikely that using the same prompts will produce the same outputs every time.\nThe exact reproducibility of the examples shown in this paper should not be taken as the only\ncriterion for the validity of our arguments. The tool we have used to generate the responses is\nstill receiving updates, and its performance on any particular task may have improved since\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n13\nwe generated the responses. Our aim is not to provide deﬁnitive prompts to help solve\nconcrete examples of tasks but to raise awareness within the PER community of how LLMs\nwork and demonstrate through illustrative examples how this knowledge can be put to use\nthrough different prompting approaches. We expect that this knowledge will be useful for\nsome time to come, regardless of how well speciﬁc iterations of LLM-based tools perform on\nthe concrete examples selected for this publication.\n4.2. Providing the context\nIn ﬁgure 1, we have already illustrated how the bare-bones GPT model acts as an advanced\n‘auto-complete’ tool, which continues the started sentence in a way that is statistically most\nlikely, based on the regularities in the training data, and, importantly, the speciﬁc text given to\nit in the prompt. The prompt serves as a point of departure from which the LLM extrapolates\nand generates further text based on its pre-trained neural network. What makes GPT and\nsimilar modern LLMs powerful is their ability to direct and narrow their‘attention’ to a\nsubspace of possible meanings. Figure1 demonstrates how GPT generates domain-speciﬁc\nexplanations of the term force, not confounding the meanings of‘force’ across the domains of\nphysics and law.\nTaking GPT’s context sensitivity and context-responsiveness into account is essential\nwhen crafting appropriate prompts. If we want to improve the model’s performance in a given\ndomain, we need to prompt it in ways that will elicit the relevant context for the response.\nHere, we provide an illustrative example of the impact of providing such context.\nTaking inspiration from a video clip of a Nascar race ([60], https://youtu.be/\nsOY9p5gFa5Q), we asked ChatGPT-4 to explain how a racer could gain an advantage in\nthe last lap of the race by grinding the car against the outer fence of the racetrack. The trick\nused by the driver can be explained at a basic level with the application of Newton's second\nlaw in the context of circular motion. The driver took advantage of the outer fence to provide\nan additional force pointing towards the centre of the turn, allowing him to make the turn at a\nmuch higher speed than the other drivers, who were relying only on frictional forces between\nthe tires and the road to keep them‘in the turn’.\nWe ﬁrst prompted ChatGPT-4 with the following question and got the response that\nfollows:\nA Nascar racer won the race in the last lap by grinding against the outer fence of the\nracetrack. Why did this trick work?\nYour question implies a situation where a NASCAR racer won a race by driving his car\nagainst the outer fence or wall of the track on theﬁnal lap. This scenario is interesting, but\nin reality, deliberately‘grinding‘ against the outer fence of a racetrack is not typically a\nsuccessful strategy, due to several reasons:\n1. Damage to the car: scraping against the fence can cause damage to the vehicle,\naffecting its aerodynamics, tire condition, and overall performance. Signiﬁcant damage\ncould even lead to a tire puncture or mechanical failure.\n2. Loss of speed: friction between the car and the wall would typically cause a reduction in\nspeed. In general, racers try to maintain a clean line to maximize their speed.\nHowever, there are situations where close wall proximity might be advantageous:\n1. Late apexing: if the driver took a‘late apex’ line, where they approach the corner wide,\nhit the apex late, and then exit the corner wide, it can sometimes allow for a faster exit\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n14\nspeed, which could be beneﬁcial on the ﬁnal lap if the straightaway follows. This\ntechnique may give the appearance of‘grinding’ against the outer fence.\n2. Drafting and slingshot maneuver: if the move against the outer wall was to escape from\nthe turbulence created by the car in front, or to line up for a‘slingshot‘ pass down the\nfollowing straight, it could possibly make sense. However, this still typically involves\navoiding contact with the wall.\nIn NASCAR races, it is common to see cars running very close to the wall, especially at\nhigh-speed tracks. This is because the banking(the incline of the track from the inside to\nthe outside) can provide additional grip, and the higher line can allow drivers to maintain\nhigher speeds. But actually making contact with the wall is generally detrimental to\nperformance.\nLastly, it is worth considering the possibility that the story might be somewhat embellished\nor misinterpreted. Motorsport, like any other sport, sometimes has its share of‘ﬁshing\nstories’ where the reality might be a bit less dramatic than the tale being told.\nThe chatbot did not provide physics argumentation for the success of the driver’s trick. In\nfact, it did not really answer the question. It did demonstrate some knowledge of the Nascar\nracing domain(although it is hard for us to judge the quality or reasonableness of the response\nin this context without being experts on the topic). It used technical terms and explained what\nappear to be typical approaches and tactics used by drivers in certain situations. Throughout\nthe response, it maintained what looks like a sceptical tone, and in the conclusion even\nsuggested that the story might have been manipulated or exaggerated.\nIf we are equipped with some basic understanding of LLM functioning, it is unsurprising\nthat the response isﬁrmly rooted in Nascar racing jargon. To the LLM, there is very little in\nthe prompt that would cue an introductory-mechanics approach to answering the question.\nOther researchers have also reported that ChatGPT can have trouble answering simple\nphysics problems when they are not contextualised explicitly as physics problems[22].\n4.2.1. Specifying the domain. The ﬁrst technique for improving the response is adjusting the\nprompt by specifying the domain on which the response should draw. Specifying the domain\nhelps the LLM narrow down the context from which it draws inferences when generating the\nresponse. In our case, the aim is to get ChatGPT to not focus on Nascar as the primary domain\nof information but rather contextualise the answer in physics language. We therefore asked\nthe chatbot to explain the situation from a perspective of forces in circular motion, resulting in\nthe following reﬁned prompt and response:\nA Nascar racer won the race in the last lap by grinding against the outer fence of the\nracetrack. Why did this trick work? Explain it from the perspective of forces in circular\nmotion.\nThis maneuver appears counterintuitive because generally, you would want to minimize\nfriction and contact with anything that could slow you down. However, in the context of\ncircular motion and NASCAR racing, this can actually be a smart tactic due to the\nprinciples of physics.\nThere are a few factors that could explain why grinding against the outer fence during\nthe last lap might help a racer win:\n1. Centripetal force: when a car is moving in a circular path, like on a racetrack, there\nneeds to be a force that is directed towards the center of the circle to keep the car\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n15\nmoving along that circular path. This is known as centripetal force. In the case of a\nNASCAR race, the friction between the tires and the track usually provides this force.\nHowever, by grinding against the fence, the driver may have been able to use the\nadditional friction between the car and the fence to create an additional centripetal\nforce. This could potentially allow the car to maintain a higher speed without skidding\noutwards due to centrifugal force(the apparent force that draws a rotating body away\nfrom the center of rotation).\n2. Banking of the turns: NASCAR tracks are typically banked to help the cars make the\nturns at high speeds. The banking allows the normal force exerted by the track(which\nis perpendicular to the surface of the track) to provide a component of the centripetal\nforce required for circular motion. If the racer is at the top of the banking(near the\nfence), the angle of banking is steeper, and hence the component of the normal force\nproviding the centripetal force is larger. Therefore, he can maintain a higher speed.\n3. Drafting and aerodynamics: NASCAR drivers often use a technique known as\n‘drafting’ where they align their cars with another car in front to reduce wind\nresistance. The leading car blocks the air for the following car, reducing the following\ncar’s aerodynamic drag and allowing it to save energy and increase speed. If the\ndriver was following closely behind another car while grinding the fence, it could have\ngiven them an aerodynamic advantage.\nThis is a simpliﬁed explanation and the real situation can be more complex, taking into\naccount factors such as the condition of the tires, the car’s handling characteristics, the\npower output of the engine, and the driver 's skill and experience. But from the\nperspective of forces in circular motion, these could be potential reasons why grinding\nagainst the fence could have given the racer an advantage.\nThe response is now more clearly focused on physics argumentation, providing a few\npossible physics explanations for why the‘grinding’ trick may have worked. Note that in this\ncase, the response actually addresses the question instead of arguing that the car might have\nonly appeared to be grinding against the fence. However, only theﬁrst bullet addresses the\nphenomenon directly and suggests incorrectly that friction between the wall and the car\ncontributes to the ‘centripetal’ force, as well as mentions centrifugal force, which is not\nrequired to explain the phenomenon. We can say that the new prompt provides a step in the\nright direction, but the response is still not particularly high quality from a physics\nperspective.\nIt is worth noting that the appropriate domain can be cued explicitly, as demonstrated\nabove, or implicitly, by formulating the problem in terminology closely associated with a\nparticular domain. For example, end-of-chapter problems in textbooks often contain phrases\nsuch as‘consider a point mass’ or ‘determine the coefﬁcient of friction’, which typically cue\nthe LLM to answer the question using language from the physics disciplinary domain without\nneeding an explicit prompt to do so. However, when faced with everyday situations, such\nreformulation of problems into physics language already involves a fair amount of physics\nmodelling.\n4.2.2. Specifying how to act. Another well-documented prompting technique [61] is to ask\nthe chatbot to behave like an expert in a givenﬁeld of interest. This tactic can be seen as a\nvariant of the‘specifying-the-context’ tactic. The previous prompt can be further tweaked by\nspecifying how the chatbot should act. In our case, to get a better physics explanation, we\nasked the chatbot to act like a physics teacher. The updated prompt was:\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n16\nA Nascar racer won the race in the last lap by grinding against the outer fence\nof the racetrack. Why did this trick work? Explain it like a physics teacher\nwould from the perspective of forces in circular motion.\nSure, I’d be happy to explain this from a physics perspective!\nWhen a car is moving in a circular path, as it does around a NASCAR track, it\nexperiences a force known as centripetal force. This force is directed towards\nthe center of the path, and it’s required to keep the car moving in a circular\ntrajectory. Without this force, the car would simply continue in a straight line\naccording to Newton’s ﬁrst law of motion(the law of inertia).\nThe source of the centripetal force in this case comes from the friction between\nthe tires of the car and the surface of the racetrack. However, as the car\nspeeds up, more and more friction is required to supply the necessary cen-\ntripetal force. Beyond a certain speed, the tires cannot supply enough static\nfriction and they start to slide—a situation known as understeer or oversteer.\nIn either case, control over the vehicle is lost.\nNow, when the racer moved his car against the outer fence, what he essen-\ntially did was introduce an additional force to help supply the required cen-\ntripetal force. This force came from the contact of the car with the fence. By\npushing against the fence, the car could maintain its circular path at a higher\nspeed than if it relied on tire-road friction alone. This allowed the driver to\nmaintain a higher speed through the turn without losing control, enabling him\nto overtake competitors.\nHowever, it is important to note that this is not an optimal way of completing a\nrace. Grinding against the fence would cause signiﬁcant damage to the car\nand could even lead to a dangerous situation. It is a tactic of desperation, and\nit only worked here because it was theﬁnal lap—the car just needed to hold\ntogether for a little longer. But, from a physics point of view, it makes sense -\nby utilizing the fence as an additional source of centripetal force, the car could\ntake the curve at a higher speed than would be possible relying solely on the\nfriction between the tires and the track.\nThis time, the answer is correct and quite well-formulated from a physics and physics\nteaching perspective. At the same time, it still contains a healthy amount of scepticism\ntowards the reasonableness and safety of the described racing tactics. We can say that it\nappropriately grounded its response in both physics and racing contexts.\n4.2.3. Comments on providing the context. LLMs ’ context sensitivity and responsiveness\ncan be seen as both a strength and a weakness. The weakness aspect becomes apparent when\nthe prompt contains superﬂuous information. Shiet al[62] show how superﬂuous details can\ndistract an LLM and degrade the quality of its responses. It can be difﬁcult for an LLM to\ndetermine which information is crucial for the task. This can also be observed in physics\nexamples. We experimented with providing ChatGPT-4 with problems concerning an incline\nbut instead of using a generic‘mass’ for the sliding object sliding down the incline, we\nintroduced everyday objects, such as a shoe or aCornu aspersum (the Latin name for the\ncommon garden snail). We observed that the chatbot’s responses were much less‘to the\npoint’ in these cases (from a physics point of view) and contained many non-physics\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n17\nreferences (zoological references in the case of the snail). This resembles how the Nascar\ncontext in the above-presented example signiﬁcantly inﬂuenced the content of the response.\nIn another example, we saw that specifying in the problem text that the setup was a\n‘modiﬁed Atwood machine’ (where one of the blocks is placed on a horizontal frictionless\nsurface, and the other hangs vertically)8 resulted in ChatGPT-4 producing a mathematical\nexpression corresponding to an ordinary Atwood machine (where both blocks hang\nvertically). Given how LLMs function, this is not particularly surprising. Statistically, it is\nmore likely that a problem containing the phrase ‘Atwood machine’ would have in its\nsolution the expression for an ordinary Atwood machine, even if the problem actually deals\nwith a ‘modiﬁed’ Atwood machine.\nOn the other hand, with some basic knowledge of how LLMs work and the use of simple\nprompt engineering techniques, context sensitivity and responsiveness can be productively\nharnessed to generate output which is well-focused in terms of both content(by specifying the\ncontext)\n9 and style(by specifying how to act). We have also illustrated in this subsection that\nthese prompting techniques can, besides generating the output in more contextually\nappropriate language, also improve the correctness of the output content from a physics\nperspective.\nIt is once again important to be aware that LLMs are not human and thus often require\ndifferent approaches to prompting. While context is often made available to humans in\ndifferent ways, including non-linguistic cues like being physically present in a physics class\nor inhabiting speciﬁc social roles, such as a student or a physics teacher, LLMs often need to\nbe prompted explicitly to draw on speciﬁc contexts or to imitate speciﬁc types of discourse.\nWe need to remind ourselves that prompts that would be seen as unnecessary or even rude\nwhen interacting with people, such as high repetitiveness [53], may present a powerful\nprompting technique when interacting with LLMs.\nIn addition to using prompts to direct the LLM to draw on speciﬁc domains of knowledge\nfrom its training data, they can sometimes also be used to provide the LLM with new\ninformation not included in the initial training. The extent of such new information can vary\nsigniﬁcantly. It can range from introducing an important fact that the LLM should consider\nwhen formulating the response to providing it with a whole scientiﬁc article or even a book(if\nthe LLM’s context window is large enough). A common term for using the prompt to‘teach’\nan LLM new information is called in-context learning(ICL).\nIn the following section, we will brieﬂy explain the history and early uses of ICL in the\nLLM research literature and how it relates to another important family of prompting\napproaches— Chain-of-Thought prompting.\n4.3. The chain-of-thought approaches\nPhysics problem-solving (real-world or end-of-textbook) is typically a multi-step process,\noften involving running mental models of physical phenomena, translating between the\nphysical world, mental models and mathematical formalisms[48]. One typical goal of physics\ninstruction is for students to learn to reason systematically and build robust arguments from\nbasic physics principles. By having some understanding of how LLMs work, we can see that\ntheir functioning is almost the opposite of what we want our students to do. While LLMs can\ngive the illusion of reasoning by imitating disciplinary discourse, they do not‘think’ before\n8 The problem was taken from[26 p. 110].\n9 Another related technique to strengthen the grounding in a speciﬁc context is to write a prompt where essential\ninformation is repeated multiple times. Zamﬁrescu-Pereira et al [53] consider this technique a useful but under-\ninvestigated technique in prompt engineering.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n18\nthey ‘talk’ [30]. It is, in fact, quite remarkable that LLMs can produce what on the outside\nappears like logically coherent arguments[63]. In computational linguistics, this ability is\noften referred to as the ability to‘reason’ despite this obviously being a misnomer.10 In\ncomputational linguistics, anthropomorphising terms like‘reasoning’ and ‘thought’ are often\nused to refer to the abilities of LLMs, which we would typically ascribe to people displaying\nsimilar behaviours. In the remainder of this section, we also use the terms‘reasoning’ and\n‘thought’ in a way that reﬂects their use in the existing literature on LLMs. However, we wish\nto stress once again that the process behind this apparent reasoning is very dissimilar to\nhuman reasoning and would be better referred to as‘argumentation’ to avoid the often-\nmisleading anthropomorphising of LLMs.\nChain-of-thought (CoT) approaches to prompting are a family of prompting strategies to\nimprove the performance of LLMs on complex reasoning tasks, such as arithmetic reasoning,\ncommon-sense reasoning, and symbolic reasoning[57, 64]. CoT prompting engages the LLM\nin step-by-step‘reasoning’ and has been demonstrated to lead to better output than when an\nLLM omits such intermediate steps in its response[64].\nThe possibility to ‘think out loud’ is crucial for LLMs’ performance since they cannot\n‘reason’ other than through generating text. By literally spelling out the reasoning, every step\nof the process of solving a task is effectively appended to the prompt, and the‘auto-complete\ninstinct’ of the LLM allows it to build a chain of argumentation, which can exhibit continuity\nand logical coherence.\n11 As we will demonstrate below, CoT prompting holds the potential to\nimprove LLMs’ likelihood of correct‘reasoning’ and quality of the‘reasoning’ itself in the\ndomain of physics. First, we provide some background that will allow for a better under-\nstanding of how and why CoT prompting is such a powerful approach to improving the\nquality of LLM output. We begin by reviewing the evolution of the CoT approach from the\nso-called ‘ICL’ approaches to prompting.\n4.3.1. From ICL to the CoT strategies. Historically, the term ICL was used to describe a\nfamily of approaches aimed at improving the performance of LLMs. ICL takes advantage of\nthe capacity of LLMs to learn from the prompt itself instead of just its training data[4]. ICL\npresents an important and less resource-intensive approach to teaching LLMs, as it does not\nrequire revisiting the resource-expensive pre-training procedure. One important limitation is\nthe length of the prompt, which can only be as long as the particular LLM’s context window\n(roughly analogous to the LLM’s working memory). In the case of the more recent GPT\nversions, the maximum context length is quite limited: GPT-3.5 allows for about 3000 words,\nand GPT-4 allows for about 6000[67]. New developments in LLM design, however, may\nsoon allow for a signiﬁcant expansion of prompt length, which also expands the possibilities\nfor ICL. One possible use case in physics would be feeding the LLM a physics textbook to\nfacilitate successful end-of-chapter problem-solving using a particular approach explained in\nthat book. However, this has yet to be tested in practice. For a comprehensive review of ICL\napproaches, see [68].\nA common approach to ICL is to begin the prompt by providing one or more examples of\ntasks accompanied by solutions, to create a demonstration context, and then providing the\ntask to be answered by the LLM. In this way, the LLM can learn from analogy directly from\n10 Granted, the computational linguistic community is likely less concerned with the epistemological considerations\nof physics learning than the PER community.\n11 Cognitive linguists have compared this step-by-step approach to solving problems to‘thinking slow’, by analogy\nto the dual-process theories of human cognition[65, 66].\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n19\nthe prompt by means of imitating it. Figure2 shows an example of building the prompt with a\nseries of input/output pairs.\nICL approaches can work even with few examples and without detailed task\nspeciﬁcations. Once again, this approach leverages the LLM's capability to produce a\nplausible continuation of the sequence presented in the prompt. This type of short example-\nbased ICL works well with simple tasks. However, providing examples for more complex\ntasks becomes more challenging.\n4.3.2. CoT prompting. More recently developed LLMs like GPT-3.5 and GPT-4 present\nemergent ‘reasoning’ abilities, an unforeseen side effect of the higher number of learning\nparameters they are trained on[63]. This means that they are capable of producing arguments\nthat resemble human reasoning on a variety of tasks. This skill is crucial for introducing\nLLMs intoﬁelds such as physics, which often require advanced argumentation and reasoning\ncapabilities. It turns out that the ability of LLMs to exhibit reasoning can be exploited by\nproviding the LLM with more articulated demonstration examples as part of ICL. Because of\nthe complexity of the tasks, the provided pattern can no longer be simple input/output pairs.\nThe examples need to showcase desired forms of reasoning. This brings us to the CoT family\nof approaches to prompting. CoT strategies have been developed building on the existing idea\nof ICL and have been demonstrated to improve the performance of LLMs[64]. In addition to\nproviding input/output pairs as a demonstration context, CoT prompting incorporates a series\nof intermediate reasoning steps into the prompt. A complex task can often be approached by\ndecomposing it into sub-tasks and then solving these smaller parts one by one. This is also\ntrue for many physics problems.\nIn the following subsections, we elaborate on the use of CoT prompting. CoT prompting\ncan be integrated into ICL approaches by providing the LLM with a few examples of how to\nreason (Few-Shot CoT prompting) or by explicitly instructing it to reason step-by-step\nwithout providing any concrete example of a solution in the prompt (Zero-Shot CoT\nprompting). Moreover, recent and ongoing research is providing further prompting strategies,\nexpanding the CoT family of approaches(Self-Consistency [69], Tree-of-Thought [70, 71]),\nincluding approaches involving a recursive dialogue between the end user and the LLM[72]\nor even between LLMs[73].\nWhile these approaches have shown great promise in improving the quality of LLM\noutput, it is important to recall that the probabilistic nature of LLMs means that these\nimprovements are typically achieved statistically over a large number of produced outputs\n[64], and individual outputs may vary in quality.\nFigure 2.Illustration of an ICL prompt. The prompt begins by providing input/output\npairs as the demonstration context(assigning SI-unit symbols to physical quantities).\nThe demonstration context is followed by a task, left open for the LLM to complete.\nThe generated output is shown on the right.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n20\n4.3.3. Few-shot CoT prompting. Few-Shot Learning is the direct evolution of ICL for more\ncomplex reasoning tasks, which incorporates the CoT reasoning steps into the input/output\ndemonstration provided as learning context in the initial prompt. This approach can be\nsuccessful even with just one example: in this case, we refer to the strategy as one-shot\nlearning. Figure 3 shows an example of its functioning.\nIn this way, the reasoning proposed in the prompt becomes crucial since the model will\nbase its response on it. However, providing isomorphic examples with correct and well-\nstructured solutions is not necessarily a straightforward task. In physics, being able to identify\nthe domain and structure of a problem and providing a similar problem with a good solution\nas a learning demonstration for the LLM requires a good knowledge and understanding of the\ntopic. This is likely to represent a major difﬁculty, especially for students in their early phase\nof learning. Consequently, this strategy is not easily applicable for novices or inﬁelds where\nthe users lack previous experience.\n4.3.4. Zero-shot CoT prompting. Zero-Shot CoT is another strategy belonging to the family\nof CoT approaches, which prompts the LLM with an explicit request to write out its\nreasoning. This method is more straightforward to implement than the Few- or One-Shot CoT\napproach since it does not require examples of reasoning as part of the prompt. The core idea\nis instead to stimulate the LLM to engage in step-by-step‘reasoning’ by simply asking it to do\nit, as illustrated inﬁgure 4. This approach leverages the capability of the LLM to imitate the\nhuman form of step-by-step reasoning, which it had encountered in its training data. Encoded\npatterns of step-by-step reasoning in the pre-training data thus serve as a general resource that\ncan be employed in concrete tasks where explicit reasoning is prompted for. In this sense,\nCoT prompting essentially provides further context to the LLM. In this case, the context is\nrelated to the general form of the response. The addition of a CoT prompt effectively makes\nthe LLM take a different path through its training data, one that more closely resembles\ncareful deliberation in humans.\nFigure 3. Example of a Few-Shot-ICL prompt (left) versus One-shot-CoT prompt\n(right). Reproduced with permission from[74].\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n21\nThis approach turns out to be successful for its simplicity and high effectiveness[57].I n\nfact, using it does not require any knowledge of the topic nor particular prompt engineering\nstrategies. Research shows that Zero-Shot CoT prompting tends to improve the likeliness of\ncorrect responses and the quality of reasoning compared to non-CoT prompts[64].\nWe have tested the Zero-Shot CoT approach on several tasks, including the following\nfree-response task, already presented at the beginning of the paper:\nIf two bodies with different mass have the same kinetic energy, which one has the\nlargest momentum?\nWith the original form of this prompt, ChatGPT-4 answered incorrectly four out of eight\ntimes upon regenerating the response. Moreover, even the responses containing the correct\nﬁnal answer mostly contained incomplete or partly faulty argumentation. However, upon\napplying a Zero-Shot CoT prompt, the results improved signiﬁcantly. Kojima et al [57]\nproposed the ‘Let us think step-by-step’ line as a template to cue CoT reasoning. Other\npossibilities, such as‘Explain your reasoning’, have also been explored[57]. We formulated\nour Zero-Shot CoT query as follows:\nIf two bodies with different mass have the same kinetic energy, which one has the\nlargest momentum? Provide your reasoningﬁrst and only then provide the answer.\nThe rate of the correctness of responses increased to seven out of eight. The provided\nreasoning also became more detailed and logically well-structured. Here, we offer a\nrepresentative example of a response to the prompt with a Zero-Shot CoT sufﬁx underlined\nabove:\nSure, let us break it down.\nThe kinetic energy(K.E) of a body is given by the formula:\nK.E = 1/2m v2\nwhere m is mass and v is velocity.\nThe momentum (p) of a body is given by the formula:\nFigure 4. Example of a Zero-Shot prompt without CoT(left) and with CoT (right).\nReproduced with permission from[57].\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n22\np = mv\nwhere m is mass and v is velocity.\nGiven that two bodies with different masses have the same kinetic energy, this implies\nthat the body with the smaller mass must have a larger velocity to achieve the same\nkinetic energy.\nLooking at the momentum formula, it is clear that momentum is directly proportional to\nboth mass and velocity. Given two bodies with the same kinetic energy but different\nmasses, the body with the smaller mass must have a greater velocity to compensate and\nachieve the same kinetic energy.\nGiven the relationship between mass, velocity, and momentum, the body with the larger\nvelocity (i.e. the one with the smaller mass) would have a larger momentum if the\nincrease in velocity is enough to offset the decrease in mass.\nSo, let’s use some algebra to derive a direct relationship between the kinetic energy and\nmomentum. From the formula of kinetic energy:\nv\n2 = 2K.E/m\nSubstituting v from the formula of momentum:\np\n2 = 2mK.E\nTherefore, momentum squared is directly proportional to the mass. This means that the\nobject with the greater mass, assuming they both have the same kinetic energy, has the\ngreater momentum.\nTo summarize, the body with the larger mass will have the larger momentum if two\nbodies with different masses have the same kinetic energy.\nBelow, we provide a pair of examples that demonstrate an important feature and\nadvantage of CoT prompting. CoT prompting shapes the LLM’s output by inﬂuencing its\nreasoning path, making it more likely to follow a consistent chain of reasoning from\nbeginning to end. Weﬁrst present an example of a prompt without a CoT sufﬁx and analyse\nhow the answer went awry.\nYou hold a block on a horizontal, frictionless surface. It is connected by a string that\npasses over a pulley to a vertically hanging block. What is the magnitude of the\nacceleration of the hanging object after you release the block on the horizontal\nsurface? (a) Less than g(b) More than g(c) Equal to g(d) Any of the answers a–c,\ndepending on the masses of the blocks.\n(d) Any of the answers a– c, depending on the masses of the blocks.\nThe acceleration of the hanging object depends on the ratio of the masses of the two\nblocks. To understand this, we can analyze the situation using Newton’s second law\n(F = ma) and considering the tension in the string and the gravitational force acting on\nthe hanging block.\nLet m1 be the mass of the block on the horizontal surface, and m2 be the mass of the\nhanging block. When the blocks are released, the tension in the string(T) and the\ngravitational force (m2 × g) act on the hanging block. We can set up two equations\nbased on Newton’s second law:\nFor the horizontal block(m1):\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n23\nT = m1 × a\nFor the hanging block(m2):\nm2 × g −T = m2 × a\nWe can solve these equations simultaneously toﬁnd the acceleration(a):\nm2 × g−m1 × a = m2 × a\na = (m2 × g)/(m1 + m2)\nFrom this equation, we can see that the acceleration of the hanging block depends on the\nmasses of both blocks:\n(a) If m1 > m2, then a< g.\n(b) If m1 < m2, then a> g.\n(c) If m1 = m2, then a= g.\nSo, the correct answer is(d) Any of the answers a– c, depending on the masses of the\nblocks.\nWe can see that ChatGPT-4ﬁrst answers the question by selecting an answer(wrong\nanswer), then proceeds to give a fairly well-argued analysis of the situation, including the\nderivation of the correct algebraic expression for the acceleration of the system. However, its\ninitial selection of the answer(d) inﬂuences theﬁnal steps of the analysis, where it determines\nthe values of acceleration for different values of the two masses. It appears as if the chatbot is\njustifying the initial answer(d) using this analysis, even if that means making, what are from a\nphysics perspective, unjustiﬁed conclusions. We have seen this pattern reoccur also upon\nrepeated generation of the response. If we understand the rudimentary principles of LLMs’\noperation, this behaviour should not surprise us. By generating each new word, the LLM\neffectively extends its prompt one word at a time. The generated words get added to the\nprompt and form a new context from which the subsequent word gets generated. Because the\nLLM is context-sensitive, it tends to build on the previously generated text. Because it does\nnot have a mechanism for checking the correctness of the previously generated text, this can\nresult in situations such as the one in the example above, where incorrect and correct\nstatements and arguments are combined in what can appear weird or uncanny reasoning.\nHowever, with the addition of a simple sentence at the end of the prompt, we can take\nadvantage of the CoT approach:\nYou hold a block on a horizontal, frictionless surface. It is connected by a string that\npasses over a pulley to a vertically hanging block. What is the magnitude of the\nacceleration of the hanging object after you release the block on the horizontal\nsurface? (a) Less than g(b) More than g(c) Equal to g(d) Any of the answers a–c,\ndepending on the masses of the blocks. Explain the reasoningﬁrst and only then\nprovide the answer.\nLet us consider the two blocks: block A is on the horizontal, frictionless surface, and\nblock B is hanging vertically. The mass of block A is m_A, and the mass of block B is\nm_B. The gravitational acceleration is g.\nWhen you release the block on the horizontal surface, there are two forces acting on the\nsystem: the gravitational force acting on block B(F_gravity = m_B × g) and the tension\nin the string(T). Block A is only affected by the tension in the string, while block B is\naffected by both the tension in the string and the gravitational force.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n24\nTo analyze the system, we’ll use Newton’s second law of motion(F = m × a) for both\nblocks:\nFor block A(horizontal direction):\nT = m_A × a\nFor block B(vertical direction):\nm_B × g −T = m_B × a\nSubstituting the expression for T from theﬁrst equation into the second equation:\nm_B × g−m_A × a = m_B × a\nNow, we solve for the acceleration a:\na = (m_B × g)/(m_A + m_B)\nAs you can see from the equation, the acceleration of the hanging object(block B)\ndepends on the masses of both blocks(m_A and m_B). Since the denominator(m_A +\nm_B) is always larger than m_B, the acceleration a will always be less than g.\nSo, the magnitude of the acceleration of the hanging object after you release the block on\nthe horizontal surface is:\n(a) Less than g\nBy avoiding making a fast conclusion at the beginning and effectively pushing theﬁnal\njudgement to the end of the response, the chatbot is able to arrive at the correct answer. It is\nimportant to note that regardless of which CoT-based prompting approach one might decide\nto use(Few-, One-, or Zero-Shot) the assessment of the correctness of the output remains in\nthe hands of the end user.\n4.3.5. Derivatives of the CoT prompting approach . The CoT family of approaches to\nprompting has received much attention in recent months. It has been found to be so successful\nthat it is reasonable to expect that LLM-based applications will use it— and with high\nprobability are already using it—‘ behind the scenes’ to improve the performance of their\nproducts. For example, this can already be seen when using‘prompt perfecting’ tools, which\nutilise AI to improve user-generated prompts. We observed one such tool, a ChatGPT plugin,\nappend a Zero-Shot CoT prompt to our physics question when we asked it to improve our\nprompt. As of September 2023, ChatGPT-4 has increasingly been responding to different\ntasks in ways that resemble CoT without being explicitly prompted by the end user. We\nspeculate that this may be the consequence of the results of research on prompt engineering\nbeing applied on a general level in the form of hidden pre-prompts, which are not visible to\nthe end users\n12 [75].\nThere are ongoing research and development efforts in the LLMs researchﬁelds that aim\nto develop even more advanced prompting strategies. Some are building on the CoT approach\nand can be seen as its derivatives. These derivative techniques aim to improve the LLMs’\noutput quality by allowing the LLM to critically revisit its output. While an LLM cannot\ncritically assess its own reasoning while generating it, it can do this with its past outputs. This\ncan be done by having an LLMﬁrst generate some output and then include it in a new prompt\nasking for some form of critical revision.\nIn one strategy, referred to asself-consistency (SC) [69], the LLM isﬁrst prompted to\ngenerate several answers using CoT prompting and then asked to assess them before deciding\n12 However, because this is OpenAI’s proprietary information, we are not able to verify our suspicion.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n25\non theﬁnal response. The idea behind this strategy is that the reasoning path underpinning the\nsolution of a problem is not necessarily unique. Indeed, in physics, one can often approach a\nproblem in different ways, producing different valid paths to an answer.\n13 It could also\nhappen that one path is better than the others because of its strength and consistency. The best\nresponse can be chosen by following a predetermined set of criteria or by deciding on theﬁnal\nanswer by looking at the distribution of the answers generated through repeated prompting\nand selecting the most common one, assuming that correct answers are more likely to appear.\nThis approach is more likely to work with simple problems, e.g. in arithmetic, and has yet to\nbe tested in physics contexts and it is unclear if the SC’s ‘democratic’ approach can lead to\nmore reliable and correct responses to physics tasks.\nAnother emerging strategy building on the CoT approach is known asTree-of-Thought\n(ToT) [70, 71]. It works similarly to SC, but instead of self-evaluating a reasoning path in its\nentirety, it evaluates every step(every ‘thought’) in a longer argument. The main idea is that\nwhile generating text, the LLM can commit mistakes, because of its way of functioning\n(namely, not being able to stop and revise the output it is currently generating). However,\nwhen the model is given a text to evaluate, it is able to consider it as a whole and potentially\nrevise it. In this way, the LLM can re-generate a new path(or several new paths) at each\nreasoning step, resulting in a‘tree-of-thoughts’, offering many possible reasoning chains.\n4.3.6. Dialogue. A particularly promising approach to using the above discussed CoT-based\nprompting techniques in physics education is integrating them into human-LLM dialogue.\nThere are two main advantages of doing this. Theﬁrst advantage is that it does not require\nspecialised technical programming skills and can be done by simply‘chatting’ to LLMs.\nSecond, it can provide meaningful opportunities for eliciting reasoning about physics in\nstudents, or opportunities for physics teachers to hone their skills of giving helpful feedback\nto others’ reasoning. In this way, some of our skills and intuitions around social interactions\ncan be used productively for interacting with an LLM.\nIn previous research, one of the authors of this paper, together with a colleague, engaged\nin a Socratic-style dialogue with theﬁrst released version of ChatGPT in December 2022[17]\nand found that pointing out inconsistencies in the chatbot’s reasoning did not result in it\nimproving its reasoning. This sort of behaviour of the chatbot the potential to cause feelings\nof frustration with the users, as has also been noted by Kortemeyer[19]. On the other hand,\nmore recent models, such as ChatGPT-3.5 and -4, now project a‘humbler’ persona, one that\nis able to and often admits its mistakes or unclarities when they are pointed out or questioned.\nThis lays a better ground for implementing dialogue-based prompting approaches[72] in\nphysics contexts. The combination of these dialogue-based approaches with providing ample\ncontext and CoT prompting strikes us as an especially promising direction for future research\nefforts in PER.\nTo illustrate the possibilities of dialogue-based approaches to LLM prompting, we\nprovide a concrete example of using a dialogue version of the SC approach to solve a physics\ntask. The question is:\nThere is a shoe that weighs 500 grams, resting on a table tilted at an angle of 30\ndegrees. What is the friction force between the table and the shoe?\nThe problem describes a classical physics situation of an inclined plane and an object in\nstatic equilibrium on it. By employing Newton’s laws for a body in static equilibrium, we can\n13 It is important to remember though the possibility that one reaches the correct answer even when the reasoning\npath followed is incorrect.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n26\nﬁnd the expression for the friction force, even without knowing the coefﬁcient of friction.\nHowever, ChatGPT-4 frequently provided the following reasoning:\nTo calculate the friction force between the table and the shoe, weﬁrst need to determine\nthe normal force and then use the friction coefﬁcient. However, you have not provided\nthe friction coefﬁcient (usually denoted byμ) between the shoe and the table, so I will\nexplain the general approach to solving the problem.\n1. Calculate the gravitational force acting on the shoe:\nF_gravity = m × g\nwhere m is the mass of the shoe(0.5 kg, given that the shoe weighs 500 grams) and\ng is the acceleration due to gravity(approximately 9.81 m s−2).\n2. Calculate the normal force acting on the shoe:\nF_normal = F_gravity × cos(angle)\nwhere angle is the angle of the table(30 degrees).\n3. Calculate the friction force:\nF_friction = μ × F_normal\nwhere μ is the friction coefﬁcient.\nWithout the friction coefﬁcient, we cannot provide a speciﬁc value for the friction force.\nIf you can provide the friction coefﬁcient, I can help you complete the calculation.\nThis reasoning path cannot be considered completely incorrect. However, it does not lead\nto an answer since it requires the knowledge of a parameter(the friction coefﬁcient), which is\nnot provided by the text. Our guess is that the formula with the normal force and the friction\ncoefﬁcient is statistically likely to appear together with‘friction force’ in the training data.\nOur goal was to make it‘consider’ the possibility that there could be another way to\napproach the problem. We thus asked an additional question, which did not give any speciﬁc\nhints:\nIsn’t there another way to do it?\nI apologize for any confusion. Yes, there is an alternate approach toﬁnd the friction\nforce by considering the component of gravitational force acting parallel to the inclined\nsurface of the table. Here’s the step-by-step process:\n1. Calculate the gravitational force acting on the shoe:\nF_gravity = m × g\nwhere m is the mass of the shoe(0.5 kg, given that the shoe weighs 500 grams) and\ng is the acceleration due to gravity(approximately 9.81 m s−2).\n2. Calculate the component of gravitational force acting parallel to the inclined\nsurface:\nF_parallel = F_gravity × sin(angle)\nwhere angle is the angle of the table(30 degrees).\n3. Since the shoe is at rest, the friction force acting on it must be equal and opposite to\nthe component of gravitational force acting parallel to the inclined surface:\nF_friction = F_parallel\nNow, let us do the calculations:\n1. F_gravity = 0.5 kg× 9.81 m s−2 = 4.905 N\n2. F_parallel = 4.905 N × sin(30 degrees) = 4.905 N × 0.5 = 2.4525 N\n3. F_friction = F_parallel = 2.4525 N\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n27\nSo, the friction force between the table and the shoe is approximately 2.4525 N.\nThe question‘Isn’t there another way to do it?’ was enough to get the desired answer.\nThis is a short and simple example of a dialogue with the chatbot, demonstrating the potential\nof dialogue-based SC-prompting for better performance.\nHowever, we must also consider the possible limitations of using similar dialogue-based\napproaches. First, it is not self-evident that a chatbot would accept criticism or respond\nfruitfully to a follow-up question. Gregorcic and Pendrill[17] have reported that attempting to\nengage in Socratic dialogue with ChatGPT-3.5 in December 2022 was frustrating because the\nchatbot would not accept that it may have been wrong. Thus, it would have been harder to use\nit in an educational context for students. However, ChatGPT-4 is more likely to project a\n‘humbler’ attitude and accept the possibility that it may have gotten something wrong.\nStill, the type of interaction shown in the above example requires the end user to have a\nspeciﬁc mindset, which may not come naturally to physics learners. When a teacher provides\na solution or answer to a physics task, the student typically does not ask,‘Is not there another\nway you could solve this?’ Such a question would imply that the student has reason to doubt\nthat the teacher has provided an exhaustive treatment of the problem. We believe that such an\nattitude is uncommon among physics learners. It is therefore crucial to consider what social\nnorms and roles students might bring into their interactions with LLM-based chatbots and\nwhen these unspoken norms may actually hamper their productive use[53].\n4.3.7. Comments on CoT and dialogue approaches to prompting. The step-by-step answers\nelicited by CoT prompting can be seen as particularly valuable in learning physics since they\nmore explicitly delineate the path to a solution and consequently tend to arrive at better-\nargued conclusions. Still, it is important to keep in mind that even though the quality of\nresponses tends to increase when using CoT prompting, there is no guarantee that all parts of\nthe chain of arguments, or theﬁnal solution, will always be correct from a physics perspective\n(as seen in examples). However, with the path to the solution being available for scrutiny,\npotential mistakes or unclear steps can be interrogated. This can be done by the LLM itself,\nthrough SC and ToT strategies, or by humans in dialogue with the LLM. In any case, having\nthe reasoning process or the chains of‘thought’ available for scrutiny is helpful and necessary\nif we want to use LLMs productively in the process of learning and teaching physics.\nFurthermore, applying the understanding of LLMs and prompt-engineering skills in the\ncontext of dialogic interactions with LLMs should help us use them more productively.\n5. Implications for instruction\nIn this section, weﬁrst summarise common topics in the existing literature on the instructional\nimplications of LLMs. We only brieﬂy overview general education literature and look more\nclosely at physics-speciﬁc publications. We then propose how our own work presented in this\npaper relates to those considerations and what other implications we can see stemming\nfrom it.\nThe topic of artiﬁcial intelligence in education has been an establishedﬁeld of research.\nPublications in theﬁeld have explored the impact of AI on curriculum design and how it\nmight impact the processes of teaching and learning[76]. However, since the public release of\nChatGPT, there has been an escalation in the sense of urgency to investigate the possible\nimplication of LLM-based tools for education[1]. In the months following the release of\nChatGPT, scholars in different subﬁelds of education research began reﬂecting on the\nopportunities and challenges arising from LLMs’ application in education [77–81]. Some\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n28\nrecurring ideas in this literature include the impact of LLMs on assessment practices on one\nside [52, 82, 83], as well as LLMs’ potential to enable interactive and personalised learning\nassistance for students[84, 85], and lesson planning and assessment assistance for teachers\n[84–88].\nResearchers in PER point out that students having access to LLMs would likely require a\nrevision of the typical tools for assessing and evaluating students’ output. Yeadonet al[23]\ndemonstrated that ChatGPT-3.5 could achieveﬁrst-class performance on short-form physics\nessays in a university physics module on the history and philosophy of physics. They suggest\nthat this holds serious potential to undermine the usefulness of at-home essay writing as an\nassessment tool. Kortemeyer’s [19] ﬁndings suggest that ChatGPT-3.5 already holds the\npotential to signiﬁcantly impact the use of programming in physics learning and assessment.\nWhile research has in some cases been clearly sceptical towards ChatGPT’s usefulness for\n‘cheating’ [17], with further improvements in LLMs’ development and the evolution of the\nend users’ skills to prompt them effectively, we could soon see a more substantial impact on\nhow physics teachers approach assessment and evaluation of physics knowledge and skills.\nImportant questions include what physics-related skills students still need to master and how\nwe should assess their mastery.\nRegarding students’ use of LLM-based chatbots, recent research points to some crucial\nissues regarding how students perceive their trustworthiness. Shoufan[89] explored senior\nengineering students’ perceptions of ChatGPT-3.5, highlighting their admiration of its cap-\nabilities but also the need for a good background knowledge to assess its answers. As both\nGregorcic and Pendrill [17] and dos Santos [18] pointed out, the linguistic proﬁciency of\nLLM-based tools can create an illusion of competence, especially when the end users lack an\nin-depth understanding of the produced content. Even more recently, Kruppet al [90] have\ndemonstrated that students who used ChatGPT as a tool in solving physics tasks often used its\noutput uncritically, even in topics that they had signiﬁcant experience with. In another study\nfocusing on physics students’ evaluation of ChatGPT performance, Dahlkemperet al [91]\nasked students to analyse ChatGPT-3.5-generated answers to a set of questions of progressing\ndifﬁculty in three areas of physics: rolling motion, waves, andﬂuid dynamics. They were\nasked to evaluate the answers in terms of their linguistic quality and scientiﬁc accuracy.\nStudents were found to be less able to evaluate the scientiﬁc accuracy of answers on the topic\nwith which they were less familiar— they had trouble distinguishing between a correct,\nexpert-generated answer and an incorrect ChatGPT-generated answer. These research efforts\nsuggest that a real drawback for learners might be an overreliance on AI-based tools, with a\nconsequent risk of a reduction in critical thinking and problem-solving skills.\nOn the other hand, we can already see theﬁrst attempts of meaningfully integrating\nChatGPT into physics learning activities to mitigate the challenges discussed above. Bit-\nzenbauer [92] proposed using the chatbot in high-school classrooms to foster students’ critical\nthinking skills by asking students to use ChatGPT to generate a text about photons, evaluate\nits consistency and correctness, and revise it using additional sources. Gregorcic and Pendrill\n[17] have also suggested that ChatGPT’s output could be used in the training of critical\nthinking in teacher education.\nAgainst the background of existing literature on the educational implications of LLMs in\nphysics and our own exploration of LLMs’ functioning presented in this paper, we propose\nthree possible roles\n14 that LLMs could play in the process of physics teaching and learning:\n(1) LLMs as physics tutors,(2) LLMs as models of physics students,(3) LLMs as peers.\n14 The roles proposed here resemble those proposed by Taylor[93] in his seminal work on the use of computers in\neducation.\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n29\n1. We have shown that without careful and deliberate attention to prompt engineering, even\nthe state-of-the-art LLM-based tools are still deeply unreliable in solving relatively basic\nphysics problems. We speculate that signiﬁcant efforts will need to be made to engineer\n(both through advanced prompting techniques but potentially also through modelﬁne-\ntuning and selective training data) LLM-based chatbots that can serve as reliable tutors\nfor physics. It is hard to predict when this could happen, but given the current pace of\ndevelopment, it is likely that some form of AI-based physics tutoring will materialise in\nthe next couple of years. How successful it will be remains to be seen. As things stand\nnow, we would argue strongly against relying on the current versions of ChatGPT or any\nother LLM-based chatbots for tutoring purposes.\n2. There is some potential to use chatbots, such as ChatGPT, for generating content to be\nevaluated by humans. This is especially interesting in the context of physics teacher\ntraining, as LLMs have been found sometimes to produce statements that resemble\nstudent difﬁculties [17]. On the other hand, LLM can also produce uncanny statements,\nwhich are very atypical of real students’ reasoning. This presents a challenge for using\nLLM-based chatbots as models of students, on which future and existing teachers can\ntrain to better respond to student difﬁculties. Additional limitations for such use come\nfrom the LLMs' enormous training database, which makes them behave as very conﬁdent\nand knowledgeable on a wide variety of topics— something that can seldom be seen in\nhuman students. However, interaction with LLM-based chatbots still holds the potential\nto be a training tool for the critical assessment of ideas. As Gregorcic and Pendrill[17]\nhave suggested, because LLMs’ responses are so well-formulated in terms of grammar\nand style, this requires the critical reader to‘really engage with the substance of their\ncontent, instead of focusing on grammatical or stylistic issues’ [p8 ].\n3. Perhaps the most promising way of using LLMs at this point in time is also one that has\nthe potential to mitigate some of the main concerns regarding students’ tendency to\nuncritically accept their output. If an LLM-based chatbot plays the role of a collaborative\npeer instead of an authority(e.g. a teacher or expert in a givenﬁeld), this sets up a more\nproductive framing for the critical interpretation of its output. At the same time, such\nframing also allows students to take advantage of the often-fruitful outputs that these\ntools can provide. Framing the chatbot as a peer would involve students taking a sceptical\nstance towards its outputs and employing independent means of evaluating their\ncorrectness and usefulness. With recent versions of ChatGPT displaying a‘humbler’\ncharacter, willing to accept criticism and revise its responses(e.g. the example provided\nin IV.C.6), compared to previous versions [17], this opens the door to dialogic\napproaches to physics problem-solving, a form of collaboration between AI and humans,\nwhere LLM-based chatbots can be used as‘objects to think with’ [81]. As we have\ndemonstrated in this paper, dialogue-based approaches, combined with some basic\nunderstanding of LLMs’ functioning and prompting techniques, hold the potential to\nbecome a useful tool for physics problem-solving.\nWhile tools such as LLM-based chatbots in principle hold the potential to be useful by\nallowing students to learn more independently, provide them with personalized and real-time\nresponses, and even help reducing teacher work-load[84, 94], they can also be detrimental to\nlearning when used in unproductive ways. Their use involves potential threats to the students’\ndevelopment of deep understanding of subject content and higher-order thinking skills,\nacademic integrity and un-biased world views[84, 94].\nWhile the potential beneﬁts and risks are not limited to any speciﬁc group, studies point\nout that students from underserved communities can potentially be more affected, both\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n30\npositively and negatively, by the use of this technology in education[80, 95]. One major risk\nis the potential widening of the digital divide between those with access to LLM-based\nservices and to the know-how of using them, and those who do not have access to them or are\nnot taught how to use them productively. Furthermore, because LLMs are mostly trained on\nlarge languages, with English being the most represented, non-speakers of these languages are\nalso in a disadvantaged position[96, 97].\nLastly, paying attention and noticing LLMs’ biases and stopping the propagation of ste-\nreotypes related to who is or can be a physicist is something we should pay special attention\nto in our efforts to make physics a more inclusive discipline[98].\n6. Conclusion\nThis paper aims to provide physics educators with some rudimentary insights into the\noperation of LLMs. We summarise existing research on the most commonly used LLM-based\nchatbot’s (ChatGPT) performance on physics tasks, pinpoint its strengths and weaknesses,\nand explain in simpliﬁed terms how it functions. We then look at how selected prompting\napproaches can improve the performance of ChatGPT-4 on introductory physics tasks,\nfocusing on conceptual tasks requiring little to no calculation. We provide several illustrative\nphysics examples, through which we qualitatively explore the effects of different prompting\ntechniques and, by doing this, glean additional insights into the functioning principles of\nLLMs. Finally, we provide a summary of existing literature on the educational implications of\nLLMs and supplement it with our own insights and reﬂections developed through the process\nof writing this paper. We hope that the paper can serve as a resource for those in the physics\neducation community who wish to develop a better understanding of how these AI-based\ntools work and how they can be used in physics teaching and learning. We are also hopeful\nthat the paper can inform and facilitate further research at the cross-section of computational\nlinguistics and physics education research.\nData availability statement\nAll data that support the ﬁndings of this study are included within the article(and any\nsupplementary ﬁles).\nORCID iDs\nGiulia Polverini\nhttps://orcid.org/0000-0001-9280-4329\nBor Gregorcic https://orcid.org/0000-0002-9185-628X\nReferences\n[1] Tate T P, Doroudi S, Ritchie D, Xu Y and Uci M W 2022Educational Research and AI-Generated\nWriting: Confronting the Coming Tsunami(https://doi.org/10.35542/osf.io/4mec3)\n[2] Ungdomsbarometern 2022 Back2School 2023 (https://ungdomsbarometern.se/rapportslapp-\nback2school-2023/?fbclid=IwAR0qrPf9sYyiFzNy0c-f-\nlZ9APu8sSJPm3b9cOfGZuUfkg95DSCda9-Ob_w)\n[3] Welding L Half of College Students Say Using AI on Schoolwork Is Cheating or Plagiarism\n(https://bestcolleges.com/research/college-students-ai-tools-survey/)\n[4] Brown T Bet al 2005 Language models are few-shot learners arXiv:2005.14165\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n31\n[5] Rae J W et al Scaling language models: methods, analysis and insights from training gopher\narXiv:2112.11446\n[6] Cobbe K et al Training veriﬁers to solve math word problems arXiv:2110.14168\n[7] Hendrycks D, Burns C, Kadavath S, Arora A, Basart S, Tang E, Song D and Steinhardt J\nMeasuring mathematical problem solving with the MATH dataset arXiv:2103.03874\n[8] Lewis P et al 2005 Retrieval-augmented generation for knowledge-intensive NLP tasks\narXiv:2005.11401\n[9] Choi J H, Hickman K E, Monahan A and Schwarcz D B 2022 ChatGPT goes to law school\nJ. Legal Educ.71 387 (https://jle.aals.org/cgi/viewcontent.cgi?article=1763&context=home)\n[10] Geerling W, Mateer G D, Wooten J and Damodaran N 2023 ChatGPT has aced the test of\nunderstanding in college economics: now what?Am. Econ. 68 233–45\n[11] Nori H, King N, McKinney S M, Carignan D and Horvitz E Capabilities of GPT-4 on medical\nchallenge problems arXiv:2303.13375\n[12] Sallam M 2023 ChatGPT utility in healthcare education, research, and practice: systematic review\non the promising perspectives and valid concernsHealthcare (Basel) 11 887\n[13] Frieder S, Pinchetti L, Grifﬁths R-R, Salvatori T, Lukasiewicz T, Petersen P C, Chevalier A and\nBerner J Mathematical capabilities of ChatGPT arXiv:2301.13867\n[14] Newton P M and Xiromeriti MChatGPT Performance on MCQ Exams in Higher Education. A\nPragmatic Scoping Review(https://doi.org/10.35542/osf.io/sytu3)( https://osf.io/preprints/\nedarxiv/sytu3)\n[15] OpenAI GPT-4 technical report arXiv:2303.08774\n[16] Bubeck Set alSparks of artiﬁcial general intelligence: early experiments with GPT-4 arXiv:2303.\n12712\n[17] Gregorcic B and Pendrill A-M 2023 ChatGPT and the frustrated socratesPhys. Educ.58 035021\n[18] Dos Santos R P 2023 Enhancing physics learning with ChatGPT, bing chat, and bard as agents-to-\nthink-with: a comparative case study arXiv:2306.00724\n[19] Kortemeyer G 2023 Could an artiﬁcial-intelligence agent pass an introductory physics course?\nPhys. Rev. Phys. Educ. Res.19 010132\n[20] Hestenes D, Wells M and Swackhamer G 1992 Force concept inventoryPhys. Teach.30 141–58\n[21] West C G AI and the FCI: Can ChatGPT project an understanding of introductory physics?\narXiv:2303.01067\n[22] West C G Advances in apparent conceptual physics reasoning in GPT-4 arXiv:2303.17012\n[23] Yeadon W, Inyang O-O, Mizouri A, Peach A and Testrow C P 2023 The death of the short-form\nphysics essay in the coming AI revolutionPhys. Educ. 58 035027\n[24] Yeadon W and Hardy T The impact of AI in physics education: a comprehensive review from\nGCSE to university levels arXiv:2309.05163\n[25] Yeadon W and Halliday D P Exploring Durham university physics exams with large language\nmodels arXiv:2306.15609\n[26] Etkina E, Planinsic G and Van Heuvelen A 2019College Physics: Explore and Apply(Pearson)\n2nd edn\n[27] Ouyang Let alTraining language models to follow instructions with human feedback arXiv:2203.\n02155\n[28] Wang Y, Zhong W, Li L, Mi F, Zeng X, Huang W, Shang L, Jiang X and Liu Q Aligning large\nlanguage models with human: a survey arXiv:2307.12966\n[29] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A N, Kaiser L and Polosukhin I\n2017 Attention is all you need Advances in Neural Information Processing Systems\n(Curran Associates, Inc.) Vol. 30 (https://proceedings.neurips.cc/paper_ﬁles/paper/2017/\nﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n[30] Shanahan M Talking about large language models arXiv:2212.03551\n[31] Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, Ishii E, Bang Y J, Madotto A and Fung P 2023 Survey\nof hallucination in natural language generationACM Comput. Surv.55 1–38\n[32] Petroni F, Lewis P, Piktus A, Rocktäschel T, Wu Y, Miller A H and Riedel S 2005 How context\naffects language models’ factual predictions arXiv:2005.04611\n[33] Floridi L and Chiriatti M 2020 GPT-3: Its nature, scope, limits, and consequencesMinds Mach.30\n681–94\n[34] Borji A A categorical archive of ChatGPT failures arXiv:2302.03494\n[35] Lo C K 2023 What is the impact of ChatGPT on education? A rapid review of the literatureEduc.\nSci. 13 410\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n32\n[36] Thakur V Unveiling gender bias in terms of profession across LLMs: analyzing and addressing\nsociological implications arXiv:2307.09162\n[37] Kotek H, Dockum R and Sun D Q 2023 Gender bias and stereotypes in large language models\nProc. of The ACM Collective Intelligence Conf.pp 12–24\n[38] Omiye J A, Lester J C, Spichak S, Rotemberg V and Daneshjou R 2023 Large language models\npropagate race-based medicinenpj Digit. Med.6 195\n[39] Fang X, Che S, Mao M, Zhang H, Zhao M and Zhao X Bias of AI-generated content: an\nexamination of news produced by large language models arXiv:2309.09825\n[40] Khandelwal K, Tonneau M, Bean A M, Kirk H R and Hale S A Casteist but not racist?\nQuantifying disparities in large language model bias between India and the west arXiv:2309.\n08573\n[41] Motoki F, Pinho Neto V and Rodrigues V 2023 More human than human: measuring ChatGPT\npolitical bias Public Choice (https://doi.org/10.1007/s11127-023-01097-2)\n[42] Gallegos I O, Rossi R A, Barrow J, Tanjim M M, Kim S, Dernoncourt F, Yu T, Zhang R and\nAhmed N K Bias and fairness in large language models: a survey arXiv:2309.00770\n[43] Bang Y et al A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning,\nhallucination, and interactivity arXiv:2302.04023\n[44] Chiang T ChatGPT Is a Blurry JPEG of the Web (https://newyorker.com/tech/annals-of-\ntechnology/chatgpt-is-a-blurry-jpeg-of-the-web)\n[45] Shen X, Chen Z, Backes M and Zhang Y In ChatGPT we trust? measuring and characterizing the\nreliability of ChatGPT arXiv:2304.08979\n[46] Davis E Mathematics, word problems, common sense, and artiﬁcial intelligence arXiv:2301.09723\n[47] Wolfram S 2023 ChatGPT Gets Its 'Wolfram Superpowers'!Stephen Wolfram Writings(https://\nwritings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers)\n[48] Hestenes D 1992 Modeling games in the newtonian worldAm. J. Phys.60 732–48\n[49] Redish E F 1994 Implications of cognitive studies for teaching physicsAm. J. Phys.62 796–803\n[50] Goodwins R ChatGPT Has Mastered the Conﬁdence Trick, and That’s a Terrible Look for AI\n(https://theregister.com/2022/12/12/chatgpt_has_mastered_the_conﬁdence/)\n[51] Talanquer V 2023 Interview with the chatbot: how does it reason?J. Chem. Educ.100 2821–4\n[52] Rudolph J, Tan S and Tan S 2023 ChatGPT: bullshit spewer or the end of traditional assessments\nin higher education?J. Appl. Learn. Teach.6\n[53] Zamﬁrescu-Pereira J D, Wong R Y, Hartmann B and Yang Q 2023 Why Johnny cannot Prompt:\nhow non-AI experts try(and Fail) to design LLM promptsProc. of the 2023 CHI Conf. on\nHuman Factors in Computing Systems(ACM, Hamburg Germany, 2023), pp 1–21\n[54] Salah M, Alhalbusi H, Ismail M M and Abdelfattah F 2023 Chatting with ChatGPT: decoding the\nmind of chatbot users and unveiling the intricate connections between user perception, trust and\nstereotype perception on self-esteem and psychological well-beingCurr. Psychol. (https://doi.\norg/10.1007/\ns12144-023-04989-0)\n[55] White J, Hays S, Fu Q, Spencer-Smith J and Schmidt D C ChatGPT prompt patterns for improving\ncode quality, refactoring, requirements elicitation, and software design arXiv:2303.07839\n[56] White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, Elnashar A, Spencer-Smith J and\nSchmidt D C A prompt pattern catalog to enhance prompt engineering with ChatGPT\narXiv:2302.11382\n[57] Kojima T, Gu S S, Reid M, Matsuo Y and Iwasawa Y Large language models are zero-shot\nreasoners arXiv:2205.11916\n[58] Bowman S R Eight things to know about large language models arXiv:2304.00612\n[59] Valmeekam K, Olmo A, Sreedharan S and Kambhampati S PlanBench: an extensible benchmark\nfor evaluating large language models on planning and reasoning about change arXiv:2206.\n10498\n[60] Edwards B NASCAR driver stuns racing world with a move learned from Nintendo GameCube\n(https://arstechnica.com/gaming/2022/11/nascar-driver-stuns-racing-world-with-a-move-\nlearned-from-nintendo-gamecube/)\n[61] Xu B, Yang A, Lin J, Wang Q, Zhou C, Zhang Y and Mao Z ExpertPrompting: instructing large\nlanguage models to be distinguished experts arXiv:2305.14688\n[62] Shi F, Chen X, Misra K, Scales N, Dohan D, Chi E, Schärli N and Zhou D Large language models\ncan be easily distracted by irrelevant context arXiv:2302.00093\n[63] Wei Jet al2022 Emergent abilities of large language modelsTransactions on Machine Learning\nResearch pp. 2835–8856\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n33\n[64] Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi E, Le Q and Zhou D Chain-of-\nthought prompting elicits reasoning in large language models arXiv:2201.11903\n[65] Kahneman D and Frederick S 2002 Representativeness revisited: attribute substitution in intuitive\njudgment Heuristics and Biases ed T Gilovich, D Grifﬁn and D Kahneman (Cambridge\nUniversity Press) 1st ednpp 49–81\n[66] Bellini-Leite S CAnalytic Thinking (Type 2 or‘System 2’) for Large Language Models: Using\nPsychology to Address Hallucination and Reliability Issues (https://doi.org/10.1177/\n10597123231206604)\n[67] OpenAI Models, (https://platform.openai.com/docs/models/overview)\n[68] Dong Q, Li L, Dai D, Zheng C, Wu Z, Chang B, Sun X, Xu J, Li L and Sui Z A survey on in-\ncontext learning arXiv:2301.00234\n[69] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, Chowdhery A and Zhou D Self-\nconsistency improves chain of thought reasoning in language models arXiv:2203.11171\n[70] Long J Large language model guided tree-of-thought arXiv:2305.08291\n[71] Yao S, Yu D, Zhao J, Shafran I, Grifﬁths T L, Cao Y and Narasimhan K Tree of thoughts:\ndeliberate problem solving with large language models arXiv:2305.10601\n[72] Chang E Y 2023 Prompting large language models with the socratic methodIEEE 13th Annual\nComputing and Communication Workshop and Conf.(CCWC)( 2023)( IEEE) pp 0351–60\n[73] Liang T, He Z, Jiao W, Wang X, Wang Y, Wang R, Yang Y, Tu Z and Shi S Encouraging\ndivergent thinking in large language models through multi-agent debate arXiv:2305.19118\n[74] Zhao W Xet al A survey of large language models arXiv:2303.18223\n[75] Min B, Ross H, Sulem E, Veyseh A P B, Nguyen T H, Sainz O, Agirre E, Heintz I and Roth D\n2023 Recent advances in natural language processing via large pre-trained language models: a\nsurvey ACM Comput. Surv.56 1–40\n[76] Holmes W, Bialik M and Fadel C 2019Promises and Implications for Teaching and Learning., in\nArtiﬁcial Intelligence in Education(Center for Curriculum Redesign)\n[77] Chinonso O E, Theresa A M-E and Aduke T C 2023 ChatGPT for teaching, learning and research:\nprospects and challengesGlob. Acad. J. Humanit. Soc. Sci.5 33–40\n[78] Kasneci E et al 2023 ChatGPT for good? On opportunities and challenges of large language\nmodels for educationLearn. Individ. Differ.103 102274\n[79] Md M, Rahman and Watanobe Y 2023 ChatGPT for education and research: opportunities,\nthreats, and strategiesAppl. Sci. 13 5783\n[80] Ray P P 2023 ChatGPT: a comprehensive review on background, applications, key challenges,\nbias, ethics, limitations and future scopeInternet Things Cyber-Phys. Syst.3 121–54\n[81] Vasconcelos M A R and Dos Santos R P 2023 Enhancing STEM learning with ChatGPT and bing\nchat as objects to think with: a case studyEurasia J. Math. Sci. Technol. Educ.19 em2296\n[82] AlAfnan M A, Dishari S, Jovic M and Lomidze K 2023 ChatGPT as an educational tool:\nopportunities, challenges, and recommendations for communication, business writing, and\ncomposition courses J. Artif. Intell. Technol.3 60–8\n[83] Crawford J, Cowling M and Allen K 2023 Leadership is needed for ethical ChatGPT: Character,\nassessment, and learning using artiﬁcial intelligence (AI) J. Univ. Teach. Learn. Pract.20\n[84] Adiguzel T, Kaya M H and Cansu F K 2023 Revolutionizing education with ai: exploring the\ntransformative potential of chatGPTContemp. Educ. Technol.15 ep429\n[85] Baidoo-Anu D and Owusu Ansah L 2023 Education in the era of generative artiﬁcial intelligence\n(AI): understanding the potential beneﬁts of ChatGPT in promoting teaching and learningJ. AI\n7 52–62\n[86] Cooper G 2023 Examining science education in ChatGPT: an exploratory study of generative\nartiﬁcial intelligence J. Sci. Educ. Technol.32 444–52\n[87] Halaweh M 2023 ChatGPT in education: Strategies for responsible implementationContemp.\nEduc. Technol. 15 ep421\n[88] Okonkwo C W and Ade-Ibijola A 2021 Chatbots applications in education: a systematic review\nComput. Educ. Artif. Intell.2 100033\n[89] Shoufan A 2023 Exploring students’ perceptions of ChatGPT: thematic analysis and follow-up\nsurvey IEEE Access 11 38805–18\n[90] Krupp L, Steinert S, Kiefer-Emmanouilidis M, Avila K E, Lukowicz P, Kuhn J, Küchemann S and\nKarolus J Unreﬂected acceptance— investigating the negative consequences of ChatGPT-\nassisted problem solving in physics education arXiv:2309.03087\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n34\n[91] Dahlkemper M N, Lahme S Z and Klein P 2023 How do physics students evaluate artiﬁcial\nintelligence responses on comprehension questions? a study on the perceived scientiﬁc accuracy\nand linguistic quality of ChatGPTPhys. Rev. Phys. Educ. Res.19 010142\n[92] Bitzenbauer P 2023 ChatGPT in physics education: a pilot study on easy-to-implement activities\nContemp. Educ. Technol.15 ep430\n[93] Taylor R 1980The Computer in the School: Tutor, Tool, Tutee(Teachers College Press)\n[94] Farrokhnia M, Banihashem S K, Noroozi O and Wals A 2023 A SWOT analysis of ChatGPT:\nimplications for educational practice and researchInnov. Educ. Teach. Int.613101 1–15\n[95] Mhlanga D 2023ChatGPT in Education: Exploring Opportunities for Emerging Economies to\nImprove Education with ChatGPT(https://doi.org/10.2139/ssrn.4355758)\n[96] Dave P ChatGPT Is cutting non-english languages out of the AI revolution(https://wired.com/\nstory/chatgpt-non-english-languages-ai-revolution/)\n[97] Nguyen S and Anderson C 2023 Do all minority languages look the same to GPT-3? linguistic\n(Mis)information in a large language modelProc. Soc. Comput. Linguist.6 400–2\n[98] APS Statement on Diversity in Physics(https://aps.org/policy/statements/08_2.cfm)\nEur. J. Phys.45 (2024) 025701 G Polverini and B Gregorcic\n35",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.671273410320282
    },
    {
      "name": "Physics",
      "score": 0.5530567169189453
    },
    {
      "name": "Physics education",
      "score": 0.4433940052986145
    },
    {
      "name": "Engineering ethics",
      "score": 0.43362894654273987
    },
    {
      "name": "Management science",
      "score": 0.3803284466266632
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34200090169906616
    },
    {
      "name": "Mathematics education",
      "score": 0.3413969874382019
    },
    {
      "name": "Computer science",
      "score": 0.21429243683815002
    },
    {
      "name": "Psychology",
      "score": 0.1582021415233612
    },
    {
      "name": "Engineering",
      "score": 0.11634010076522827
    },
    {
      "name": "Quantum mechanics",
      "score": 0.10704231262207031
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I123387679",
      "name": "Uppsala University",
      "country": "SE"
    }
  ]
}