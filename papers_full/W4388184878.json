{
  "title": "Neural Retrievers are Biased Towards LLM-Generated Content",
  "url": "https://openalex.org/W4388184878",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5075518954",
      "name": "Sunhao Dai",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5102010069",
      "name": "Yuqi Zhou",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5004759804",
      "name": "Liang Pang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5102612071",
      "name": "Weihao Liu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5103895382",
      "name": "Xiaolin Hu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5028071674",
      "name": "Yong Liu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5100320847",
      "name": "Xiao Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5002333577",
      "name": "Gang Wang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5020766468",
      "name": "Jun Xu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Renmin University of China",
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2098502158",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4379256134",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W3137305332",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4389523771",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4366559971",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4361865428",
    "https://openalex.org/W4390430574",
    "https://openalex.org/W4385889719",
    "https://openalex.org/W4246927951",
    "https://openalex.org/W4399205394",
    "https://openalex.org/W4386874846",
    "https://openalex.org/W4312943126",
    "https://openalex.org/W3166441238",
    "https://openalex.org/W4385570537",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W4368755500",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4388067858",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W8870360",
    "https://openalex.org/W4385571551",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3090556797",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4385573586",
    "https://openalex.org/W4386908184",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W3166731436",
    "https://openalex.org/W4318719246",
    "https://openalex.org/W3007780968",
    "https://openalex.org/W4387165318",
    "https://openalex.org/W3154670582",
    "https://openalex.org/W4385638369",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2008037086",
    "https://openalex.org/W4389519982",
    "https://openalex.org/W4383472770",
    "https://openalex.org/W4389921502",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4365601405",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288365749",
    "https://openalex.org/W4389520758",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4389156994",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4382361534",
    "https://openalex.org/W4386875656",
    "https://openalex.org/W4300415226",
    "https://openalex.org/W4385571377",
    "https://openalex.org/W4253083841",
    "https://openalex.org/W4213009331",
    "https://openalex.org/W4323706279",
    "https://openalex.org/W2922386288",
    "https://openalex.org/W4387075795",
    "https://openalex.org/W4323652488",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W4206827264",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the \\textbf{source bias}. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias.",
  "full_text": "Neural Retrievers are Biased Towards LLM-Generated Content\nSunhao Dai\nYuqi Zhou\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{sunhaodai,yuqizhou}@ruc.edu.cn\nLiang Pang\nCAS Key Laboratory of AI Safety\nInstitute of Computing Technology\nChinese Academy of Sciences\nBeijing, China\npangliang@ict.ac.cn\nWeihao Liu\nXiaolin Hu\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{weihaoliu,xiaolinhu}@ruc.edu.cn\nYong Liu\nXiao Zhang\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{liuyonggsai,zhangx89}@ruc.edu.cn\nGang Wang\nHuawei Noahâ€™s Ark Lab\nShenzhen, China\nwanggang110@huawei.com\nJun Xuâˆ—\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\njunxu@ruc.edu.cn\nABSTRACT\nRecently, the emergence of large language models (LLMs) has revo-\nlutionized the paradigm of information retrieval (IR) applications,\nespecially in web search, by generating vast amounts of human-\nlike texts on the Internet. As a result, IR systems in the LLM era\nare facing a new challenge: the indexed documents are now not\nonly written by human beings but also automatically generated by\nthe LLMs. How these LLM-generated documents influence the IR\nsystems is a pressing and still unexplored question. In this work,\nwe conduct a quantitative evaluation of IR models in scenarios\nwhere both human-written and LLM-generated texts are involved.\nSurprisingly, our findings indicate that neural retrieval models tend\nto rank LLM-generated documents higher. We refer to this cat-\negory of biases in neural retrievers towards the LLM-generated\ncontent as the source bias. Moreover, we discover that this bias is\nnot confined to the first-stage neural retrievers, but extends to the\nsecond-stage neural re-rankers. Then, in-depth analyses from the\nperspective of text compression indicate that LLM-generated texts\nexhibit more focused semantics with less noise, making it easier for\nneural retrieval models to semantic match. To mitigate the source\nbias, we also propose a plug-and-play debiased constraint for the op-\ntimization objective, and experimental results show its effectiveness.\nFinally, we discuss the potential severe concerns stemming from the\nobserved source bias and hope our findings can serve as a critical\nwake-up call to the IR community and beyond. To facilitate future\nexplorations of IR in the LLM era, the constructed two new bench-\nmarks are available at https://github.com/KID-22/Source-Bias.\nâˆ—Jun Xu is the corresponding author. Work partially done at Engineering Research Cen-\nter of Next-Generation Intelligent Search and Recommendation, Ministry of Education.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671882\nCCS CONCEPTS\nâ€¢ Information systems â†’Information retrieval.\nKEYWORDS\nSource Bias, Information Retrieval, LLM-Generated Texts, Artificial\nIntelligence Generated Content\nACM Reference Format:\nSunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu,\nXiao Zhang, Gang Wang and Jun Xu. 2024. Neural Retrievers are Biased\nTowards LLM-Generated Content. In Proceedings of the 30th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining (KDD â€™24), August\n25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 13 pages. https:\n//doi.org/10.1145/3637528.3671882\n1 INTRODUCTION\nWith the advent of large language models (LLMs), exemplified by\nChatGPT, the field of artificial intelligence generated content (AIGC)\nhas surged to new heights of prosperity [12, 65]. LLMs have demon-\nstrated their remarkable capabilities in automatically generating\nhuman-like text at scale, resulting in the Internet being inundated\nwith an unprecedented volume of AIGC content [47, 64]. This influx\nof LLM-generated content has fundamentally reshaped the digital\necosystem, challenging conventional paradigms of content creation,\ndissemination, and information access on the Internet [2, 75].\nMeanwhile, information retrieval (IR) systems have become in-\ndispensable for navigating and accessing the Internetâ€™s vast infor-\nmation landscape [ 36, 45]. As illustrated in Figure 1, in the era\npreceding the widespread emergence of LLMs, IR systems focused\non retrieving documents solely from the human-written corpus in\nresponse to usersâ€™ queries [33, 34, 68]. However, the proliferation of\nAIGC driven by LLMs has expanded the corpus of IR systems to in-\nclude both human-written and LLM-generated texts. Consequently,\nthis paradigm shift raises a fundamental research question: What\nis the impact of the proliferation of generated content on IR\nsystems? We aim to explore whether existing retrieval models tend\nto prioritize LLM-generated text over human-written text, even\nwhen both convey similar semantic information. If this holds, LLMs\nmay dominate information access, particularly as their generated\ncontent is rapidly growing on the Internet [25].\narXiv:2310.20501v3  [cs.IR]  31 Jul 2024\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sunhao Dai et al.\nIR \nSystem\nIR \nSystem\nCorpus\nWrite\nGenerate\nWrite\nOnline Serving\nExpose & Interact\nOffline Training\nOnline Serving\nOffline Training\nExpose & Interact\nCorpus\nQuery\nQuery\n(a) IR in the Pre-LLM Era\nIR \nSystem\nIR \nSystem\nCorpus\nWrite\nGenerate\nWrite\nOnline Serving\nExpose & Interact\nOffline Training\nOnline Serving\nOffline Training\nExpose & Interact\nCorpus\nQuery\nQuery\n(b) IR in the LLM Era\nFigure 1: The overview evolution of IR paradigm from the\nPre-LLM era to the LLM era.\nTo approach the fundamental research question, we decompose\nit into four specific research questions. The first question is RQ1:\nHow to construct an environment to evaluate IR models in\nthe LLM era? Given the lack of public retrieval benchmarks encom-\npassing both human-written and LLM-generated texts, we propose\nan innovative and practical method to create such a realistic evalu-\nation environment without the need of costly human annotation.\nSpecifically, we leverage the original human-written texts as the\ninstruction conditions to prompt LLMs to generate rewritten text\ncopies while preserving the same semantic meaning. In this way, we\ncan confidently assign the same relevancy labels to LLM-generated\ndata according to the original labels. Extensive empirical analysis\nvalidates the quality of our constructed environment, demonstrat-\ning its effectiveness in mirroring real-world IR scenarios in the LLM\nera. As a result, we introduce two new benchmarks, SciFact+AIGC\nand NQ320K+AIGC, tailored for IR research in the LLM era.\nWith the constructed environment, we further exploreRQ2: Are\nretrieval models biased towards LLM-generated content? We\nconduct comprehensive experiments with various representative\nretrieval models, ranging from traditional lexical models to modern\nneural models based on pretrained language models (PLMs) [22, 23,\n72, 73]. Surprisingly, we uncover that neural retrievers are biased\ntowards LLM-generated texts, i.e., tend to rank LLM-generated\ntexts in higher positions. We refer to this as source bias, as the\nneural retrievers favor content from specific sources (i.e., LLM-\ngenerated content). Further experiments indicate that the source\nbias not only extends to the second-stage neural re-rankers from the\nfirst-stage retrieval but also manifests more severely. These findings\ncorroborate the prevalence of source bias in neural retrieval models.\nThen, what we are curious about is RQ3: Why are neural re-\ntrieval models biased towards LLM-generated texts? Inspired\nby the recent studies positing LLMs as lossless compressors [17],\nwe analyze the cause of source bias from the viewpoint of text\ncompression. Our analysis of singular values [31] in different cor-\npora reveals that LLM-generated texts exhibit more focused seman-\ntics with minimal noise, enhancing their suitability for semantic\nmatching. Furthermore, our in-depth perplexity analysis shows that\nLLM-generated texts consistently achieve lower perplexity scores,\nwhich indicates a higher degree of comprehensibility and confi-\ndence from the PLMâ€™s perspective. These observations collectively\nsuggest that LLM-generated texts are more readily understandable\nto PLM-based neural retrievers, thereby resulting in source bias.\nFinally, we try to answerRQ4: How to mitigate source bias in\nneural retrieval models? To tackle this, we propose an intuitive\nyet effective debiased constraint. This constraint is designed to\npenalize biased samples during the optimization process, thereby\nshifting the focus of retrieval models from exploiting inherent short-\ncuts to emphasizing semantic relevance. Besides, our debiased con-\nstraint is model-agnostic and can be plugged and played to the\nranking optimization objectives of various neural retrieval models.\nFurthermore, it offers the capability to control the degree of bias\nremoval, offering the flexibility to balance the treatment between\nthe two sources of content based on specific requirements and\nenvironmental considerations.\nLast but not least, we discuss the potential emerging concerns\nstemming from source bias, highlighting the risk of human-written\ncontent being gradually inaccessible, especially due to the rapidly\nincreasing LLM-generated content on the Internet [8, 25]. Further-\nmore, source bias could be maliciously exploited to manipulate\nalgorithms and potentially amplify the spread of misinformation,\nposing a threat to online security. In light of these pressing issues,\nwe hope that our findings serve as a resounding wake-up call to all\nstakeholders involved in IR systems and beyond.\nIn summary, the contributions of this paper are as follows:\n(1) We introduce a more realistic paradigm of IR systems consid-\nering the growing prosperity of AIGC, where the retrieval corpus\nconsists of both human-written and LLM-generated texts. We then\nuncover a new inherent bias in both neural retrieves and re-rankers\npreferring LLM-generated content, termed as source bias.\n(2) We provide an in-depth analysis and insights of source bias\nfrom a text compression perspective, which indicates that LLM-\ngenerated texts maintain more focused semantics with minimal\nnoise and are more readily comprehensible for neural retrievers.\n(3) We propose a debiased constraint to penalize the biased sam-\nples during optimization, and experimental results demonstrate its\neffectiveness in mitigating source bias in different degrees.\n(4) We also provide two new benchmarks, SciFact+AIGC and\nNQ320K+AIGC, which contain both high-quality human-written\nand various LLM-generated corpus and corresponding relevancy\nlabels. We believe these two benchmarks can serve as valuable\nresources for facilitating future research of IR in the LLM era.\n2 RQ1: ENVIRONMENT CONSTRUCTION\nWith the increasing usage of LLMs in generating texts (e.g., para-\nphrasing or rewriting), the corpus of IR systems includes both\nhuman-written and LLM-generated texts nowadays. Constructing\nan IR dataset in the LLM era typically involves two steps: collecting\nboth human-written and LLM-generated corpora and then employ-\ning human evaluators to annotate relevancy labels for each query-\ndocument pair. Given that LLM-generated content is currently\nunidentifiable [42] and the significant cost of human annotation,\nNeural Retrievers are Biased Towards LLM-Generated Content KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nHuman-Written Corpus\nIR Models Retrieval\nRetrieved Documents\nPrompt\nGenerate\nLLM-Generated Corpus\nâ€¦\nâ€¦\n1 2 3 4\n1 2 3 4\n1\n1\n3\n2\n3\n2\nQueryInstruction Prompt \nPlease rewrite the following text: {{Human-Written Text}}\nHuman-Written Text \nAllele, also called allelomorph, any one of two or more genes that may \noccur alternatively at a given site (locus) on a chromosome. Alleles may \noccur in pairs, or there may be multiple alleles affecting the expression \n(phenotype) of a particular trait.\nAllele, also known as an allelomorph, refers to any of the two or more genes \nthat can exist alternatively at a specific location (locus) on a chromosome. \nThese alleles can exist in pairs, or there can be multiple alleles that influence \nthe expression (phenotype) of a specific trait.\nLLM-Generated Text \nFigure 2: The overall paradigm of the proposed evaluation framework for IR in the LLM era.\nwe introduce a natural and practical framework for quantitatively\nevaluating retrieval models in the LLM era, as shown in Figure 2.\nTo better align with real-world scenarios, the evaluation environ-\nments should meet the following three essential criteria. Firstly,\nit is imperative to distinguish between human-written and LLM-\ngenerated texts within the corpus. Secondly, we need access to\nrelevancy labels for LLM-generated data in response to queries.\nThirdly, each human-written text should better have a correspond-\ning LLM-generated counterpart with the same semantics, ensuring\nthe most effective and fair evaluation.\n2.1 Notation\nFormally, in the Pre-LLM era, given a query ğ‘ âˆˆQ where Qis the\nset of all queries, the traditional IR system aims to retrieve a list\nof top-ğ¾ relevant documents {ğ‘‘(1),ğ‘‘(2),...,ğ‘‘ (ğ¾)}from a corpus\nCğ» = {ğ‘‘ğ»\n1 ,ğ‘‘ğ»\n2 ,...ğ‘‘ ğ»\nğ‘}which consists of ğ‘ human-written docu-\nments. However, in the era of LLMs, there is also LLM-generated\ntext in the corpus. To evaluate the IR models in the LLM era, we also\ncreate an additional LLM-generated corpus Cğº = {ğ‘‘ğº\n1 ,ğ‘‘ğº\n2 ,...,ğ‘‘ ğº\nğ‘}\nwhere each document is generated by a LLM, e.g.,ğ‘‘ğº\n1 can be created\nby prompting ChatGPT to rewrite ğ‘‘ğ»\n1 while preserving its original\nsemantics information. Consequently, given a query ğ‘, the objec-\ntive of a retriever in the LLM era is to return the top- ğ¾ relevant\ndocuments from the mixed corpus C= Cğ» Ã Cğº.\n2.2 Constructing IR Datasets in the LLM Era\nIn this section, we prompt LLMs to rewrite human-written corpus\nto build two new standard retrieval datasets: SciFact+AIGC and\nNQ320K+AIGC. These two new datasets can serve as valuable\nresources to facilitate future research of IR in the LLM era.\n2.2.1 Human-Written Corpus. We first choose two widely used\nretrieval datasets written by humans in the Pre-LLM era as the\nseed data: SciFact and NQ320K. SciFact 1 [57] dataset aims to re-\ntrieve evidence from the research literature containing scientific\npaper abstracts for fact-checking. NQ320K 2 [32] is based on the\nNatural Questions (NQ) dataset from Google, where the documents\nare gathered from Wikipedia pages, and the queries are natural\nlanguage questions. Following the practice in BEIR benchmark [52],\n1https://allenai.org/data/scifact\n2https://ai.google.com/research/NaturalQuestions\nwe process these two datasets in a standard format: corpus Cğ»,\nqueries Q, and relevancy labels Rğ» = {(ğ‘ğ‘š,ğ‘‘ğ»ğ‘š,ğ‘Ÿğ‘š)}ğ‘€\nğ‘š=1, where\nğ‘€ is the number of labeled query-document pairs in the dataset.\n2.2.2 LLM-Generated Corpus. For the LLM-generated corpus, we\nrepurpose the original human-written corpus as our seed data and\ninstruct LLMs to rewrite each given text from the human-written\ncorpus. As the written text generated by LLM carries almost the\nsame semantic information as the original human-written text, we\ncan assign the same relevancy labels to new <query, LLM-generated\ndocument> pairs as those assigned to the original labeled <query,\nhuman-written document> pairs.\nOur instruction is straightforward: â€œPlease rewrite the following\ntext: {{human-written text}} â€, as illustrated in the left part of Figure 2.\nThis straightforward instruction enables LLMs to generate text\nwithout too many constraints while maintaining semantic equiva-\nlence to the original human-written text. Specifically, we choose\nLlama2 [54] and ChatGPT to rewrite each seed human-written cor-\npus, as Llama2 and ChatGPT are both the most widely-used and\nnearly the state-of-the-art open-sourced and closed-source LLM,\nrespectively. We only generate texts with ChatGPT corresponding\nto the texts in SciFact dataset, mainly due to the significant cost\ninvolved in processing the larger NQ320K dataset.\nFor the LLM-generated corpus, we conduct post-processing to re-\nmove unrelated parts of the original response from LLM like â€œSure,\nhereâ€™s a possible rewrite of the text:â€. As a result, we can obtain two\ncorresponding LLM-generated corpora with SciFact and NQ320K\nas seed data. After that, we extend the original labels of query\nand human-written text Rğ» = {(ğ‘ğ‘š,ğ‘‘ğ»ğ‘š,ğ‘Ÿğ‘š)}ğ‘€\nğ‘š=1 to get the cor-\nresponding label of LLM-generated text Rğº = {(ğ‘ğ‘š,ğ‘‘ğºğ‘š,ğ‘Ÿğ‘š)}ğ‘€\nğ‘š=1.\nWe will validate the quality of the datasets in the following sec-\ntion. Combining each original human-written corpus Cğ» with its\ncorresponding LLM-generated corpus Cğº, original queries Q, and\nlabels Rğ» ÃRğº, we can create two new datasets, denoted as Sci-\nFact+AIGC and NQ320K+AIGC. Table 1 summarizes the statistics\nof the proposed two datasets.\n2.3 Statistics and Quality Validation of Datasets\nTake the Llama2-generated data as an example, we conduct the\nstatistics and quality validation of the constructed datasets. The\nanalysis of ChatGPT-generated datasets shows similar observations\nand conclusions and is omitted due to the page limitation.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sunhao Dai et al.\nTable 1: Statistics of the constructed two datasets. Avg. Doc / Query means the average number of relevant documents per query.\nDataset # Test Queries # Avg. Query Length Human-Written Corpus Llama2-Generated Corpus ; ChatGPT-Generated Corpus\n# Corpus Avg. Doc Length Avg. Doc / Query # Corpus Avg. Doc Length Avg. Doc / Query\nSciFact+AIGC 300 12.38 5,183 201.81 1.1 5,183 ; 5,183 192.66 ; 203.57 1.1 ; 1.1\nNQ320K+AIGC 7,830 9.24 109,739 199.79 1.0 109,739 ; â€“ 174.49 ; â€“ 1.0 ; â€“\nTable 2: Performance comparison of retrieval models on the sole human-written or Llama2-generated corpus on SciFact+AIGC\nand NQ320K+AIGC datasets. For brevity, we omit the percent sign â€˜ %â€™ of ranking metrics in subsequent tables and figures.\nModel Model Corpus SciFact+AIGC NQ320K+AIGC\nType NDCG@1 NDCG@3 NDCG@5 MAP@1 MAP@3 MAP@5 NDCG@1 NDCG@3 NDCG@5 MAP@1 MAP@3 MAP@5\nLexical\nTF-IDF Human-Written 42.0 49.5 52.7 40.7 47.1 49.0 12.2 15.8 16.8 12.2 14.9 15.5\nLLM-Generated 43.0 49.8 52.6 40.8 47.5 49.2 9.4 12.6 13.9 9.4 11.8 12.5\nBM25 Human-Written 46.0 54.2 56.3 43.8 51.5 52.8 12.9 16.3 17.6 12.9 15.5 16.2\nLLM-Generated 46.3 53.6 55.3 44.1 51.1 52.2 11.9 15.3 16.5 11.9 14.5 15.1\nNeural\nANCE Human-Written 38.7 44.3 46.5 36.3 41.9 43.3 50.6 60.0 62.2 50.6 57.7 58.9\nLLM-Generated 41.0 46.0 48.2 37.8 43.5 45.0 49.3 58.8 61.2 49.3 56.5 57.8\nBERM Human-Written 37.0 42.1 44.2 34.7 39.7 41.3 49.2 58.3 60.4 49.2 56.1 57.3\nLLM-Generated 40.7 44.5 46.2 37.7 42.3 43.5 48.4 57.5 59.8 48.4 55.3 56.5\nTAS-B Human-Written 52.7 58.1 60.2 49.9 55.6 57.2 53.4 63.0 65.4 53.4 60.7 62.0\nLLM-Generated 50.7 57.0 58.9 48.0 54.6 55.9 51.9 62.3 64.7 51.9 59.8 61.1\nContrieverHuman-Written 54.0 61.8 63.2 51.4 58.9 60.0 58.2 68.4 70.3 58.2 65.9 67.0\nLLM-Generated 55.7 62.0 64.8 52.9 59.5 61.5 57.1 67.5 69.8 57.1 64.9 66.2\n0.0 0.2 0.4 0.6 0.8 1.0\nJaccard Similarity\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10Probability\nSciFact+AIGC\nNQ320K+AIGC\n(a) Jaccard Similarity\n0.0 0.2 0.4 0.6 0.8 1.0\nOverlap\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10Probability\nSciFact+AIGC\nNQ320K+AIGC (b) Overlap\nFigure 3: Distribution of term Jaccard similarity and overlap\nbetween Llama2-generated and human-written corpora.\n2.3.1 Term-based Statistics and Analysis. We first analyze the term-\nbased similarity between the LLM-generated corpus and the human-\nwritten corpus. Specifically, we compute the Jaccard similarity\n( |ğ‘‘ğº Ã‘ğ‘‘ğ»|\n|ğ‘‘ğº Ãğ‘‘ğ»|) and the overlap (|ğ‘‘ğº Ã‘ğ‘‘ğ»|\n|ğ‘‘ğ»| ) between each LLM-generated\ndocument and orginal human-written document. As shown in Fig-\nure 3, both the Jaccard similarity and overlap distributions ex-\nhibit normal distribution, with peaks at about 0.6 and 0.8 for Sci-\nFact+AIGC, and about 0.4 and 0.6 for NQ320K+AIGC, respectively.\nThese observations suggest that while there is a considerable over-\nlap of terms between the LLM-generated text and the original\nhuman-written text, there are also distinct differences, especially\nnoticeable in the NQ320K+AIGC dataset.\n2.3.2 Semantic-based Statistics and Analysis. For the LLM-generated\ntexts, a pivotal consideration is whether they faithfully preserve\nthe underlying semantics of the corresponding human-written cor-\npus. If they indeed do so, we then can confidently assign them the\nsame relevancy labels as the labels of their corresponding original\nhuman-written texts given each query.\nTo assess this, we first leverage the OpenAI embedding model3\nto acquire semantic embeddings for both the LLM-generated and\n3text-embedding-ada-002:https://platform.openai.com/docs/guides/embeddings\n-75 -50 -25 0 25 50 75\n-60\n-40\n-20\n0\n20\n40\n60\nHuman-written\nLlama2-generated\n(a) SciFact+AIGC\n-100 -50 0 50 100\n-60\n-40\n-20\n0\n20\n40\n60\nHuman-written\nLlama2-generated (b) NQ320K+AIGC\nFigure 4: Semantic embedding visualization of different cor-\npora on SciFact+AIGC and NQ320K+AIGC datasets.\n0.90 0.92 0.94 0.96 0.98 1.00\nCosine Similarity\n0.00\n0.06\n0.12\n0.18Probability\nSciFact+AIGC\nNQ320K+AIGC\nFigure 5: Distribution of cosine similarity of semantic embed-\nding between Llama2-generated and human-written corpora.\nhuman-written texts. Subsequently, we visualize these embeddings\nthrough T-SNE [55] in Figure 4. We observe a strikingly close over-\nlap between the Llama2-generated corpus and the human-written\ncorpus in the latent space. This observation strongly suggests that\nthese LLM-generated corpora adeptly preserve the original seman-\ntics. Moreover, we delve into the cosine similarity of semantic em-\nbeddings between the LLM-generated text and their corresponding\nhuman-written counterparts. The results, as shown in Figure 5, also\nindicate a high degree of similarity, with most values exceeding 0.95,\naffirming the faithful preservation of semantics in LLM-generated\ntext. Hence, for each query-document pair (ğ‘,ğ‘‘ğº), we can confi-\ndently assign the relevancy label ğ‘Ÿ to be the same as that of (ğ‘,ğ‘‘ğ»).\nNeural Retrievers are Biased Towards LLM-Generated Content KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nTable 3: Verification of semantics and text quality with hu-\nman evaluation. The numbers in parentheses represent the\nproportion agreed upon by all three human annotators for\neach option.\nSciFact+AIGC NQ320K+AIGC\nWhich document is more relevant to the given query?\nHuman LLM Equal Human LLM Equal\n0.0%(0.0%) 0.0%(0.0%) 100.0%(82.0%) 2.0%(0.0%) 0.0%(0.0%) 98.0%(81.6%)\nWhich document exhibits higher quality by considering the following aspects:\nlinguistic fluency, logical coherence, and information density?\nHuman LLM Equal Human LLM Equal\n8.0%(0.0%) 6.0%(0.0%) 86.0%(46.5%) 4.0%(0.0%) 6.0%(0.0%) 90.0%(60.%)\n2.3.3 Retrieval Performance Evaluation. To further validate the\naccuracy of the relevancy label assignments, we conduct an eval-\nuation of retrieval models on the human-written corpus and the\nLLM-generated corpus, respectively. The following representative\nretrieval models are adopted in the experiments: (1) Lexical Re-\ntrieval Models: TF-IDF [46] and BM25 [41] and (2) Neural Retrieval\nModels: ANCE [67], BERM [70], TAS-B [26], Contriever [28].\nThe results on each sole source corpus on the proposed two new\nbenchmarks are presented in Table 2. It is evident that all retrieval\nmodels exhibit no significant performance discrepancies in terms\nof various ranking metrics between the human-written and LLM-\ngenerated corpora across all datasets. This observation reinforces\nthe confidence in the quality of our newly constructed datasets.\n2.3.4 Human Evaluation. Note that in our constructed datasets,\nLLMs were instructed to rewrite human-written texts based solely\non the original human-written text, without any query-related in-\nput, thereby preventing the additional query-specific information\nduring rewriting . Moreover, to further verify this, we conduct a hu-\nman evaluation. Specifically, we randomly select 50 <query, human-\nwritten document, LLM-generated document> triples from each\ndataset. The human annotators, comprising the authors and their\nhighly educated colleagues, are asked to determine which docu-\nment is more semantically relevant to the given query. The options\nare â€œHumanâ€, â€œLLMâ€, or â€œEqualâ€. During the evaluation, annota-\ntors are unaware of the source of each document. Each triple is\nlabeled at least by three different annotators, with the majority\nvote determining the final label. The results in Table 3, confirm that\nboth sources of texts have almost the same semantic relevance to\nthe given queries, which guarantees the fairness of our following\nexploration of source bias.\nAdditionally, we also conduct further human evaluations specifi-\ncally focused on text quality. The human annotators are asked to\ndetermine â€œWhich document exhibits higher quality by consider-\ning the following aspects: linguistic fluency, logical coherence, and\ninformation density?â€ The notation process is the same as above,\nand the results are summarized in Table 3. The results indicate no\nsignificant distinction between LLM-generated and human-written\ncontent on text quality, demonstrating consistency across both\nsources. In fact, we also analyze the data cases and find that LLMs\ntypically alter only parts of the vocabulary, leading to minor stylis-\ntic differences without impacting the core content, which can be\nfurther verified with these human evaluations.\n3 RQ2: UNCOVERING SOURCE BIAS\nIn this section, we conduct extensive experiments on the con-\nstructed datasets to explore the source bias from various aspects.\nWith the constructed simulated environment, we first introduce\nthe evaluation metrics to quantify the severity of source bias. We\nthen conduct experiments with different retrieval models on both\nthe first-stage retrieval and the second-stage re-ranking.\n3.1 Evaluation Metrics for Source Bias\nTo quantitatively explore source bias, we calculate ranking metrics,\ntargeting separately either human-written or LLM-generated cor-\npus. Specifically, for each query, an IR model produces a ranking\nlist that comprises documents from mixed corpora. We then calcu-\nlate top-ğ¾ Normalized Discounted Cumulative Gain (NDCG@ğ¾)\nand Mean Average Precision (MAP@ğ¾), for ğ¾ âˆˆ{1,3,5}, indepen-\ndently for each corpus source. When assessing one corpus (e.g.,\nhuman-written), documents from the other (e.g., LLM-generated)\nare treated as non-relevant, though the original mixed-source rank-\ning order is maintained. This approach allows us to independently\nassess the performance of IR models on each corpus source.\nTo better normalize the difference among different benchmarks,\nwe also introduce the relative percentage difference as follows:\nRelative Î” =\nMetricHuman-written âˆ’MetricLLM-generated\n1\n2 (MetricHuman-written +MetricLLM-generated)\nÃ—100%,\nwhere the Metric can be NDCG@ğ¾ and MAP@ğ¾. Note that Rela-\ntive Î” > 0 means retrieval models rank human-written texts higher,\nand Relative Î” < 0 indicates LLM-generated texts are ranked higher.\nThe greater the absolute value of RelativeÎ”, the greater the ranking\nperformance difference between two sourced content.\n3.2 Bias in Neural Retrieval Models\nIn our assessment of various retrieval models on SciFact+AIGC\nand NQ320K+AIGC datasets, we observe distinct phenomena when\nevaluating against human-written and LLM-generated corpora, as\nreported in Table 4. Our key findings are as follows:\nLexical models prefer human-written texts. Lexical models\nlike TF-IDF and BM25 show a tendency to favor human-written\ntexts over LLM-generated texts across most ranking metrics in\nboth datasets. A plausible explanation for this phenomenon lies in\nthe term-based distinctions between text generated by LLMs and\nhuman-written content, as evident in Figure 3. Additionally, the\nqueries are crafted by humans and thus exhibit a style more closely\naligned with human-written text.\nNeural retrievers are biased towards LLM-generated texts.\nNeural models, which rely on semantic matching with PLMs, demon-\nstrate a pronounced preference for LLM-generated texts, often per-\nforming over 30% better on these compared to human-written texts.\nThese findings suggest an inherent bias in neural retrievers towards\nLLM-generated text, which we named the source bias. This source\nbias may stem from PLMs-based neural retrievers and LLMs sharing\nsimilar Transformer-based architectures [56] and pretraining ap-\nproaches, leading to potential exploitation of semantic shortcuts in\nLLM-generated text during semantic matching. Additionally, LLMs\nseem to semantically compress information in a manner that makes\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sunhao Dai et al.\nTable 4: Performance comparison of retrieval models for mixed human-written and Llama2-generated corpora on SciFact+AIGC\nand NQ320K+AIGC dataset. The numbers indicate that retrieval models rank human-written documents in higher positions\nthan LLM-generated documents (i.e., Relative Î” > 0%). Conversely, the numbers mean retrieval models rank LLM-generated\ndocuments in higher positions than human-written documents (i.e., Relative Î” â‰¤0%). The intensity of the color reflects the\nextent of the difference. In the subsequent tables, we will continue with this color scheme.\nModel Model Target Corpus SciFact+AIGC NQ320K+AIGC\nType NDCG@1 NDCG@3 NDCG@5 MAP@1 MAP@3 MAP@5 NDCG@1 NDCG@3 NDCG@5 MAP@1 MAP@3 MAP@5\nLexical\nTF-IDF\nHuman-Written 22.0 36.9 39.7 21.2 33.0 34.7 7.1 11.0 12.3 7.1 10.0 10.8\nLLM-Generated 17.0 33.8 37.2 16.2 29.5 31.5 3.4 8.1 9.4 3.4 7.0 7.7\nRelativeÎ” 25.6 8.8 6.5 26.7 11.2 9.7 70.5 30.4 26.7 70.5 35.3 33.5\nBM25\nHuman-Written 26.7 40.3 44.4 25.7 36.7 39.1 7.2 11.6 12.9 7.2 10.6 11.3\nLLM-Generated 21.0 38.8 41.5 19.6 34.3 35.9 6.1 10.9 11.9 6.1 9.7 10.3\nRelativeÎ” 23.9 3.8 6.8 26.9 6.8 8.5 16.5 6.2 8.1 16.5 8.9 9.3\nNeural\nANCE\nHuman-Written 15.3 30.1 32.7 14.2 26.2 27.7 22.2 41.2 44.6 22.2 36.9 38.8\nLLM-Generated 24.7 35.8 37.7 23.3 32.4 33.6 29.1 45.9 49.0 29.1 42.0 43.8\nRelativeÎ” -47.0 -17.3 -14.2 -48.5 -21.2 -19.2 -26.9 -10.8 -9.4 -26.9 -12.9 -12.1\nBERM\nHuman-Written 16.3 30.2 31.8 15.7 26.5 27.5 18.6 37.5 40.7 18.6 33.1 34.9\nLLM-Generated 23.7 34.1 36.4 21.7 30.8 32.2 31.6 47.0 50.0 31.6 43.5 45.1\nRelativeÎ” -37.0 -12.1 -13.5 -32.1 -15.0 -15.7 -51.8 -22.5 -20.5 -51.8 -27.2 -25.5\nTAS-B\nHuman-Written 20.0 40.2 43.1 19.5 35.2 36.9 25.7 45.4 48.8 25.7 40.9 42.8\nLLM-Generated 31.7 44.8 47.5 29.7 41.1 42.7 27.6 46.5 50.0 27.6 42.2 44.2\nRelativeÎ” -45.3 -10.8 -9.7 -41.5 -15.5 -14.6 -7.1 -2.4 -2.4 -7.1 -3.1 -3.2\nContriever\nHuman-Written 24.0 43.7 47.8 23.3 38.8 41.2 25.9 48.5 51.9 25.9 43.3 45.3\nLLM-Generated 31.0 47.8 50.5 29.6 43.2 44.8 32.5 51.9 55.4 32.5 47.5 49.4\nRelativeÎ” -25.5 -9.0 -5.5 -23.8 -10.7 -8.4 -22.6 -6.8 -6.5 -22.6 -9.3 -8.7\nTable 5: Performance comparison of different neural re-\ntrieval models for mixed human-written and ChatGPT-\ngenerated corpora on SciFact+AIGC dataset.\nModel Target Corpus NDCG@1 NDCG@3 NDCG@5 MAP@1 MAP@3 MAP@5\nTF-IDFHuman-Written 22.7 36.5 39.5 22.0 32.8 34.6LLM-Generated 16.7 34.9 37.1 16.0 30.2 31.4RelativeÎ” 30.5 4.5 6.3 31.6 8.3 9.7\nBM25 Human-Written 24.3 38.5 42.7 23.7 34.8 37.3LLM-Generated 24.3 40.2 42.7 23.1 35.8 37.3RelativeÎ” 0.0 -4.3 0.0 2.6 -2.8 0.0\nANCE Human-Written 18.0 30.8 33.8 16.5 27.2 29.0LLM-Generated 24.7 35.6 37.4 24.0 32.7 33.7RelativeÎ” -31.4 -14.5 -10.1 -37.0 -18.4 -15.0\nBERM Human-Written 16.3 29.9 32.3 14.8 26.0 27.4LLM-Generated 22.7 32.5 35.3 21.9 29.7 31.4RelativeÎ” -32.8 -8.3 -8.9 -38.7 -13.3 -13.6\nTAS-B Human-Written 23.0 41.5 44.4 22.2 36.9 38.6LLM-Generated 28.7 45.5 46.7 27.2 40.9 41.6RelativeÎ” -22.1 -9.2 -5.0 -20.2 -10.3 -7.5\nContrieverHuman-Written 24.0 44.0 47.2 23.3 39.1 41.0LLM-Generated 33.0 48.3 50.6 31.3 44.0 45.4RelativeÎ” -31.6 -9.3 -7.0 -29.3 -11.8 -10.2\nit more comprehensible to neural models. A deeper exploration\ninto the causes of source bias is presented in the following section.\nTo strengthen our conclusion thatsource bias is not limited to\nany specific LLM, we extend our investigation to include ChatGPT,\nanother widely adopted and nearly state-of-the-art LLM. We em-\nploy ChatGPT to generate a corpus using the same prompts as those\nutilized with Llama2 in the above experiments. Subsequently, in Ta-\nble 5, we report the evaluation results on the SciFact+AIGC dataset,\nwhich contains both human-written and ChatGPT-generated texts.\nOnce again, the results clearly indicate a bias within neural retrieval\nmodels, favoring the corpus generated by ChatGPT across all rank-\ning metrics. This observation provides additional substantiation of\nthe presence of source bias within these neural retrieval models.\nFurthermore, we also explore the popular InstructGPT-prompts\nGitHub Repository, which includes several common prompts for\nTable 6: Bias evaluation of re-ranking models on Sci-\nFact+AIGC dataset. The re-ranking methods rerank the top-\n100 retrieved hits from a first-stage BM25 model.\nMetrics Target Corpus Llama2-generated ChatGPT-generated\nBM25 +MiniLM +monoT5 BM25 +MiniLM +monoT5\nNDCG@1\nHuman-Written 26.7 21.3 19.7 24.3 18.3 21.3\nLLM-Generated 21.0 32.7 39.7 24.3 35.7 39.3\nRelativeÎ” 23.9 -42.2 -67.3 0.0 -64.4 -59.4\nNDCG@3\nHuman-Written 40.3 42.8 45.9 38.5 41.4 46.4\nLLM-Generated 38.8 47.8 52.9 40.2 50.1 54.2\nRelativeÎ” 3.8 -11.0 -14.2 -4.3 -19.0 -15.5\nNDCG@5\nHuman-Written 44.4 46.9 49.0 42.7 45.6 48.9\nLLM-Generated 41.5 50.2 54.7 42.7 53.0 56.1\nRelativeÎ” 6.8 -6.8 -11.0 0.0 -15.0 -13.7\nMAP@1\nHuman-Written 25.7 20.8 18.9 23.7 17.9 20.5\nLLM-Generated 19.6 30.8 37.8 23.1 33.8 37.8\nRelativeÎ” 26.9 -38.8 -66.7 2.6 -61.5 -59.3\nMAP@3\nHuman-Written 36.7 37.5 39.7 34.8 35.8 40.3\nLLM-Generated 34.3 43.6 48.9 35.8 45.9 50.0\nRelativeÎ” 6.8 -15.0 -20.8 -2.8 -24.7 -21.5\nMAP@5\nHuman-Written 39.1 40.0 41.6 37.3 38.3 41.7\nLLM-Generated 35.9 45.0 50.1 37.3 47.6 51.4\nRelativeÎ” 8.5 -11.8 -18.5 0.0 -21.7 -20.8\nrephrasing passages4. The experimental results in Appendix B show\nvarying degrees of source bias, indicating that common prompts\ncan easily trigger source bias with LLM-generated content. These\nfindings highlight the notable presence of source bias in neural\nretrieval models towards LLM-generated content.\n3.3 Bias in Re-Ranking Stage\nIn a typical IR system, there are two primary stages of document\nfiltering. The first stage involves a retriever, responsible for doc-\nument recall, while the second stage employs a re-ranker, which\nfine-tunes the ordering of documents within the initially retrieved\nset. While we have revealed the presence of the source bias in the\nfirst stage, a natural pivotal research question remains: does this\n4https://github.com/kevinamiri/Instructgpt-prompts\nNeural Retrievers are Biased Towards LLM-Generated Content KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\n0 200 400 600 800 1000 1200 1400\nIndex (High SV âŸ·  Low SV)\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5\nRelative SV (%)\nSV(LLM) âˆ’ SV(Human)\nSV(Human)\nHuman\nLlama2\nChatGPT\nFigure 6: Comparision of the relative singular value (SV) of\nthe different corpus after SVD. The singular values are sorted\nin descending order from left to right.\nbias also manifest in the re-ranking stage? To delve into this, we\nselect two representative and state-or-the-art re-ranking models:\nMiniLM [62] and monoT5 [37] to rerank the top-100 document\nlist retrieved by a first-stage BM25 model.\nThe results on the SciFact+AIGC dataset with Llama-generated\ncorpus and ChatGPT-generated corpus are presented in Table 6.\nFrom the results, while even the first-stage retrievers (BM25) may\nexhibit a preference for human-written content, the second-stage\nre-rankers once again demonstrate a bias in favor of LLM-generated\ncontent. Remarkably, the bias in re-ranking models appears to be\nmore severe, as evidenced by the relative percentage difference of\nâˆ’67.3% and âˆ’59.4% in NDCG@1 for monoT5, respectively. These\nfindings further confirm the pervasiveness of source bias in neural\nranking models that rely on PLMs, regardless of the first retrieval\nstage or second re-ranking stage.\n4 RQ3: THE CAUSE OF SOURCE BIAS\nIn this section, we delve deeper into why neural retrieval mod-\nels exhibit source bias. Our objective is to determine whether the\nLLM-generated texts, characterized by reduced noise and more con-\ncentrated semantic topics, are inherently easier for neural retrieval\nmodels to semantically match. We conduct a series of analyses from\nthe perspective of text compression and provide valuable insights.\n4.1 Viewpoint from Text Compression\nWe first explore the cause of source bias from a compression per-\nspective, drawing inspiration from recent studies that suggest LLMs\nare lossless compressors [17]. We hypothesize that LLMs efficiently\nfocus on essential information, minimizing noise during generation,\nin contrast to human-written texts, which may include more di-\nverse topics and incidental noise. To verify this, we employ Singular\nValue Decomposition (SVD) [31] to compare topic concentration\nand noise in human-written and LLM-generated texts. The dimen-\nsion of the SVD corresponds to the maximum number of topics, and\nthe singular value associated with each topic represents its strength.\nHigh singular values predominantly capture primary topic infor-\nmation, whereas low singular values indicate noise.\nSpecifically, we utilize the OpenAI embedding model to obtain\nembedding matrices for each corpus in the SciFact+AIGC dataset\nand then conduct SVD. The resulting singular values are arranged\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nPPL\n0.00\n0.02\n0.04\n0.06\n0.08Probability\nHuman\nLlama2\nChatGPT\nFigure 7: Comparision of the PPL of the different corpus.\nin descending order, and their comparison to the human-written\ncorpus is visualized in Figure 6. As we can see, LLM-generated\ntexts exhibit larger singular values at the top large singular values,\nwhile smaller singular values at the tail small singular values. This\nobservation suggests that LLM-generated texts tend to have more\nfocused semantics with less noise, rendering them more suitable\nfor precise semantic matching. In contrast, human-written texts\noften contain a wider range of latent topics and higher levels of\nnoise, making them harder for neural retrievers to understand. As\na result, this difference in semantic concentration may contribute\nto the observed source bias in neural retrievers.\n4.2 Further Analysis from Perplexity\nConsidering that most modern neural retrievers are grounded on\nPLMs [23, 72, 73], such as BERT [19], Roberta [35], and T5 [40], we\nanalyze the perplexity of PLMs to further support the conclusion\nabove from the viewpoint of compression that LLM-generated texts\ncan be better understood by PLMs. Perplexity is an important metric\nfor evaluating how well a language model can understand a given\ntext [6, 59]. For a specific language model (LM) and a documentğ‘‘ =\n(ğ‘‘0,ğ‘‘1,Â·Â·Â· ,ğ‘‘ğ‘†), the log perplexity is defined as the exponentiated\naverage negative log-likelihood of each token in the tokenized\nsequence of ğ‘‘5:\nPPL(ğ‘‘)= âˆ’1\nğ‘†\n ğ‘†âˆ‘ï¸\nğ‘ =1\nlog ğ‘ƒLM (ğ‘‘ğ‘ |context)\n!\n,\nwhere ğ‘† is the token length of text ğ‘‘ and ğ‘ƒLM (ğ‘‘ğ‘ )is the predicted\nlikelihood of the ğ‘ -th token conditioned on the context. Lower\nperplexity suggests more confidence and understanding of LM for\ntext patterns, while higher perplexity implies greater uncertainty\nin predictions, often arising from complex or unpredictable text\npatterns.\nUsing the most widely-used LM, BERT [19], as an example, we\nemploy it to calculate the PPL for different corpus. As BERT is\nnot an autoregressive LM, we follow standard practices [ 58, 63]\nto calculate the likelihood of each token conditioned on the other\ntokens, i.e.,\nğ‘ƒLM (ğ‘‘ğ‘ |context):= ğ‘ƒBERT (ğ‘‘ğ‘ |ğ‘‘â‰¤ğ‘†\\{ğ‘ }).\nThe distribution of perplexity for different corpus in the SciFact+AIGC\ndataset is shown in Figure 7. Notably, LLM-generated texts consis-\ntently exhibit significantly lower perplexity, indicating enhanced\ncomprehensibility and higher confidence from BERTâ€™s perspective.\nConsequently, PLMs-based neural retrievers can more effectively\nmodel the semantics of LLM-generated texts, leading to the ob-\nserved source bias in favor of LLM-generated texts.\n5For simplicity, we denote the log perplexity as PPL.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sunhao Dai et al.\nw/o debias   1e-4 5e-4 1e-3 5e-3 1e-2\nÎ±\n0\n10\n20\n30NDCG@1 (%)\nw/o debias   1e-4 5e-4 1e-3 5e-3 1e-2\nÎ±\n0\n10\n20\n30NDCG@1 (%)\nANCE BERM\n-47.0% -47.0%\n14.9%\n76.5%\n113.0%\n134.7%\n-37.0% -36.4%\n-4.8%\n29.8%\n91.9%\n79.1%\nTarget on human Target on LLM Relative Î”\n(a) Results on mixed human-written and Llama2-generated corpora\nw/o debias   1e-4 5e-4 1e-3 5e-3 1e-2\nÎ±\n0\n10\n20\n30NDCG@1 (%)\nw/o debias   1e-4 5e-4 1e-3 5e-3 1e-2\nÎ±\n0\n10\n20\n30NDCG@1 (%)\nANCE BERM\n-31.4%\n-11.4%\n48.1%\n73.8%\n129.5% 134.7%\n-32.8%\n-10.9%\n17.6%\n47.6%\n92.7% 87.1%\nTarget on human Target on LLM Relative Î”\n(b) Results on mixed human-written and ChatGPT-generated corpora\nFigure 8: Performance comparison (NDCG@ 1) of neural\nmodels on SciFact+AIGC dataset with different debiased co-\nefficient ğ›¼. The grey dashed line represents Relative Î” = 0.\nIn Appendix C, we also provide a theoretical analysis to illustrate\nand verify the observation in Figure 7 that LLM-generated texts\nhave a smaller perplexity than human-written texts.\n5 RQ4: MITIGATING SOURCE BIAS\nIn this section, we propose a simple but effective approach to miti-\ngate source bias by introducing a debiased constraint to the opti-\nmization objective. In this way, we can force the neural IR models\nto focus on modeling semantic relevance rather than the inherent\nsemantic shortcut of the LLM-generated content.\n5.1 Our Method: A Debiased Constraint\nOur earlier findings of source bias indicate that neural retrievers\ntend to rank LLM-generated documents in higher positions. Thus,\nthe motivation of our debiased method is straightforward, which is\nto force the retrieval models to focus on modeling the semantic rele-\nvance and not assign higher predicted relevance scores to the LLM-\ngenerated documents. Specifically, following the practice in Sec-\ntion 2.2, we first generate the corresponding LLM-generated corpus\nCğº for the original human-written training corpusCğ». In this way,\nwe can get the new paired training data D= {(ğ‘ğ‘š,ğ‘‘ğ»ğ‘š,ğ‘‘ğºğ‘š)}ğ‘€\nğ‘š=1,\nwhere each element (ğ‘ğ‘š,ğ‘‘ğ»ğ‘š,ğ‘‘ğºğ‘š)is a <query, human-written docu-\nment, LLM-generated document> triplet. ğ‘‘ğ»ğ‘š and ğ‘‘ğºğ‘š are the corre-\nsponding human-written and LLM-generated relevant documents\nfor the query ğ‘, respectively. Then we introduce the debiased con-\nstraint, which can be defined as\nLdebias =\nâˆ‘ï¸\n(ğ‘ğ‘š,ğ‘‘ğ»ğ‘š,ğ‘‘ğºğ‘š)âˆˆD\nmax{0,Ë†ğ‘Ÿ(ğ‘,ğ‘‘ğº; Î˜)âˆ’ Ë†ğ‘Ÿ(ğ‘,ğ‘‘ğ»; Î˜)} (1)\nw/o debias   1e-4 5e-4 1e-3 5e-3 1e-2\nÎ±\n36.0\n38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0NDCG (%)\nNDCG@1\nNDCG@3\nNDCG@5\n(a) ANCE\nw/o debias   1e-4 5e-4 1e-3 5e-3 1e-2\nÎ±\n36.0\n38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0NDCG (%)\nNDCG@1\nNDCG@3\nNDCG@5 (b) BERM\nFigure 9: Performance comparison of neural retrievers\non only human-written SciFact dataset with different co-\nefficient ğ›¼ in our proposed debiased method.\nwhere Ë†ğ‘Ÿ(ğ‘,ğ‘‘ğº; Î˜)and Ë†ğ‘Ÿ(ğ‘,ğ‘‘ğ»; Î˜)are the predicted relevance scores\nof (ğ‘,ğ‘‘ğº)and (ğ‘,ğ‘‘ğ»)by the retrieval models with parameters Î˜,\nrespectively. This constraint can penalize biased samples when the\npredicted relevance score of (ğ‘,ğ‘‘ğº)is greater than that of (ğ‘,ğ‘‘ğ»).\nBased on the debiased constraint defined in (1), we can define\nthe final loss for training an unbiased neural retriever:\nL= Lrank +ğ›¼Ldebias (2)\nwhere the Lrank can be any common-used loss for the ranking task,\ne.g., contrastive loss or regression loss [ 22, 23, 73]. And ğ›¼ is the\ndebiased co-efficient that can balance the ranking performance and\nthe degree of the source bias. The larger ğ›¼ indicates the greater\npenalty on the biased samples, leading to the retriever being more\nlikely to rank the human-written texts in higher positions.\n5.2 Results and Analysis\nTo evaluate the effectiveness of our proposed debiased method,\nwe equip the debiased constraint defined in Eq. (1) to two rep-\nresentative neural retrievers: ANCE [ 67] and BERM [ 70]. In the\nexperiments, we vary the debiased co-efficientğ›¼within the range of\n{1ğ‘’-4,5ğ‘’-4,1ğ‘’-3,5ğ‘’-3,1ğ‘’-2}. The original retrieval models learned\nwithout the debiased constraint are denoted as â€œw/o debiasâ€. The\nresults on the SciFact+AIGC dataset are presented in Figure 8.\nAs we can see, as the debiased co-efficient ğ›¼ increases, the Rela-\ntive Î” gradually shifts from negative to positive across almost all\nmetrics and mixed datasets. This trend indicates that the neural\nretrieval models can rank human-written text higher than LLM-\ngenerated text with large ğ›¼. This can be attributed to the inclusion\nof our debiased constraint into the learning objective, which can\npenalize the biased samples and compel the retrieval models not to\nassign higher predicted relevance scores to LLM-generated content.\nMoreover, as shown in Figure 9, our method not only maintains the\nretrieval performance on the sole human-written corpus but also\nprovides improvements, especially with BERM as the backbone.\nThis improvement is likely due to the inclusion of LLM-generated\nsamples, which might enhance the modelâ€™s ability to discern rele-\nvance among similar documents.\nIn summary, these empirical results have demonstrated the ef-\nficacy of our proposed debiased method in mitigating source bias\nto different extents by adjusting the debiased coefficient ğ›¼. This\nflexibility allows for customizing debiasing mechanisms to meet di-\nverse perspectives and demands. Notably, the decision to maintain\nNeural Retrievers are Biased Towards LLM-Generated Content KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nequality between the two content sources or favor human-written\ncontent can be tailored based on specific requirements and envi-\nronmental considerations. For example, users may not mind the\ncontentâ€™s source if it is of high quality and fulfills their informa-\ntional needs. However, bias becomes a significant issue when we\naim to credit content providers and encourage more creation, im-\npacting the sustainability of the content creation ecosystem. The\noptimal strategy for enhancing the sustainable development of the\nIR ecosystem remains an open question for further exploration.\n6 DISCUSSION: SOUNDING THE ALARM\nThrough a rigorous series of experiments and thorough analysis,\nwe have identified that neural retrieval models demonstrate clear\npreferences for LLM-generated texts, referred to as source bias. This\nbias, with the burgeoning proliferation of LLMs and AIGC, may\nraise significant concerns for a variety of aspects.\nFirst, the presence of source bias poses a significant risk of\ngradually rendering human-written content less accessible, po-\ntentially causing a disruption in the content ecosystem. More se-\nverely, the concern is escalating with the growing prevalence of\nLLM-generated content online[8, 25]. Second, there is the risk that\nsource bias may amplify the spread of misinformation, especially\nconsidering the potential of LLMs to generate deceptive content,\nwhether intentionally or not [5, 13, 39, 49]. Third, source bias may\nbe maliciously exploited to attack against neural retrieval models\nwithin todayâ€™s search engines, creating a precarious vulnerability\nthat could be weaponized by malicious actors, reminiscent of earlier\nweb spam link attacks against PageRank [24].\nAs discussed above, since LLMs can be readily instructed to\ngenerate texts at scale, source bias presents potential tangible and\nserious threats to the ecosystem of web content, public trust, and on-\nline safety. We hope this discussion will sound the alarm regarding\nthe risks posed by source bias in the LLM era.\n7 RELATED WORK\nLarge Language Models for IR. The emergence of large language\nmodels (LLMs) [ 64, 71, 74] has ushered in a transformative era\nacross various research domains, such as natural language process-\ning (NLP) [7, 11], education [21, 38], recommender systems [15, 20],\nfinance [27, 66], and medicine [3, 53]. In the field of IR, much ef-\nfort has also been made to utilize the remarkable knowledge and\ncapabilities of LLMs to enhance IR systems [ 2, 75]. In the indus-\ntry community, an exemplary successful application is New Bing6,\nwhich is an LLM-powered search assistant that adeptly extracts\ninformation from various web pages and delivers concise sum-\nmarized responses to user queries, thereby improving the search\nexperience. In the research community, there has been a proactive\nexploration of integrating LLMs into the IR components, including\nquery rewriters [48, 60], retrievers [16, 69], re-rankers [14, 50], and\nreaders [29, 43]. For a more comprehensive overview of the recent\nadvancements in LLMs for IR, please refer to the recent survey [75].\nArtificial Intelligence Generated Content. Artificial Intelligence\nGenerated Content (AIGC) is a rapidly advancing field that involves\nthe creation of content using advanced Generative AI (GAI) [1, 12,\n6https://www.bing.com/new\n65]. Unlike traditional content crafted by humans, AIGC can be gen-\nerated at scale and in considerably less time [25, 47]. Recently, the\ndevelopment of LLMs and other GAI models has greatly improved\nthe quality of AIGC content than before. For instance, LLMs such\nas ChatGPT have shown impressive abilities in generating human-\nlike content [12, 65]. The DALL-E-3 [9], another state-of-the-art\ntext-to-image generation system, can follow user instructions to\nproduce high-quality images. Nevertheless, as AIGC becomes more\nprevalent across myriad domains, ethical concerns, and potential\nrisks come into sharper focus [51, 61]. In fact, inevitably, the GAI\nmodels may generate content with bias and discrimination as the\nlarge training data always contain bias and toxicity [8, 18, 76]. Fur-\nthermore, researchers have found that LLMs can be manipulated\ninto generating increasingly deceptive misinformation, posing chal-\nlenges to online safety [13, 30, 49]. In addition, some recent studies\nindicate that training GAI models with synthetic data could result in\nthe collapse of the next-generation models [4, 10, 44]. Thus, AIGC\nis a double-edged sword that requires cautious handling.\n8 CONCLUSION AND FUTURE WORK\nIn this paper, we provide a preliminary analysis of the impact of the\nproliferation of generated content on IR systems, which is a pressing\nand emerging problem in the LLM era. We first introduce two\nnew benchmarks, SciFact+AIGC and NQ320K+AIGC, and build an\nenvironment for evaluating IR models in scenarios where the corpus\ncomprises both human-written and LLM-generated texts. Through\nextensive experiments within this environment, we uncover an\nunexpected bias of neural retrieval models favoring LLM-generated\ntext. Moreover, we provide an in-depth analysis of this bias from the\nperspective of text compression. We also introduce a plug-and-play\ndebiased strategy, which shows the potential to mitigate the source\nbias to different degrees. Finally, we discuss the crucial concerns\nand potential risks of this bias to the whole web ecosystem.\nOur study offers valuable insights into several promising direc-\ntions for future research, including exploring source bias in other\ninformation systems (e.g., recommender systems and advertising\nsystems) and examining source bias in neural models towards AIGC\ndata across multiple data modalities, not limited to text. Moreover,\nuncovering the root cause of the source bias and thus further miti-\ngating it are difficult but crucial research directions.\nACKNOWLEDGMENTS\nThis work was funded by the National Key R&D Program of China\n(2023YFA1008704), the National Natural Science Foundation of\nChina (No. 62377044, 62276248, 62376275, 62076234), Beijing Nat-\nural Science Foundation (No. 4222029), Beijing Key Laboratory of\nBig Data Management and Analysis Methods, Major Innovation\n& Planning Interdisciplinary Platform for the â€œDouble-First Classâ€\nInitiative, PCC@RUC, funds for building world-class universities\n(disciplines) of Renmin University of China, and the Youth Innova-\ntion Promotion Association CAS under Grants No.2023111. This\nwork was supported by the Fundamental Research Funds for the\nCentral Universities, and the Research Funds of Renmin University\nof China (RUC24QSDL013). We thank all the anonymous reviewers\nfor their positive and insightful comments.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sunhao Dai et al.\nREFERENCES\n[1] Jorge Agnese, Jonathan Herrera, Haicheng Tao, and Xingquan Zhu. 2020. A\nsurvey and taxonomy of adversarial neural networks for text-to-image synthesis.\nWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 10, 4 (2020),\ne1345.\n[2] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong\nCheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval\nMeets Large Language Models: A Strategic Report from Chinese IR Community.\nAI Open 4 (2023), 80â€“90.\n[3] Ian L Alberts, Lorenzo Mercolli, Thomas Pyka, George Prenosil, Kuangyu Shi,\nAxel Rominger, and Ali Afshar-Oromieh. 2023. Large language models (LLM)\nand ChatGPT: what will the impact on nuclear medicine be? European journal of\nnuclear medicine and molecular imaging 50, 6 (2023), 1549â€“1552.\n[4] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Hu-\nmayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G Baraniuk.\n2023. Self-consuming generative models go mad. arXiv preprint arXiv:2307.01850\n(2023).\n[5] Kevin Aslett, Zeve Sanderson, William Godel, Nathaniel Persily, Jonathan Nagler,\nand Joshua A Tucker. 2023. Online searches to evaluate misinformation can\nincrease its perceived veracity. Nature (2023), 1â€“9.\n[6] Leif Azzopardi, Mark Girolami, and Keith Van Risjbergen. 2003. Investigating the\nrelationship between language model perplexity and IR precision-recall measures.\nIn Proceedings of the 26th annual international ACM SIGIR conference on Research\nand development in informaion retrieval . 369â€“370.\n[7] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan\nWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask,\nmultilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and\ninteractivity. arXiv preprint arXiv:2302.04023 (2023).\n[8] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yu-\nval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Had-\nfield, et al. 2023. Managing ai risks in an era of rapid progress. arXiv preprint\narXiv:2310.17688 (2023).\n[9] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long\nOuyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhari-\nwal, Casey Chu, and Yunxin Jiao. 2023. Improving Image Generation with Better\nCaptions. (2023).\n[10] Martin Briesch, Dominik Sobania, and Franz Rothlauf. 2023. Large Language\nModels Suffer From Their Own Output: An Analysis of the Self-Consuming\nTraining Loop. arXiv preprint arXiv:2311.16822 (2023).\n[11] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712 (2023).\n[12] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, and Lichao\nSun. 2023. A comprehensive survey of ai-generated content (aigc): A history of\ngenerative ai from gan to chatgpt. arXiv preprint arXiv:2303.04226 (2023).\n[13] Canyu Chen and Kai Shu. 2023. Can LLM-Generated Misinformation Be Detected?\narXiv preprint arXiv:2309.13788 (2023).\n[14] Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, and Jong C Park. 2023. Discrete\nPrompt Optimization via Constrained Generation for Zero-shot Re-ranker. arXiv\npreprint arXiv:2305.13729 (2023).\n[15] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongx-\niang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPTâ€™s Capabilities\nin Recommender Systems. In Proceedings of the 17th ACM Conference on Recom-\nmender Systems .\n[16] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\nKelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot\ndense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).\n[17] GrÃ©goire DelÃ©tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim\nGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew\nAitchison, Laurent Orseau, et al. 2023. Language modeling is compression. arXiv\npreprint arXiv:2309.10668 (2023).\n[18] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and\nKarthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned\nlanguage models. arXiv preprint arXiv:2304.05335 (2023).\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies . 4171â€“4186.\n[20] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang\nTang, and Qing Li. 2023. Recommender systems in the era of large language\nmodels (llms). arXiv preprint arXiv:2307.02046 (2023).\n[21] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,\nJianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts?\nComparison Corpus, Evaluation, and Detection. arXiv preprint arXiv:2301.07597\n(2023).\n[22] Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng.\n2022. Semantic models for the first-stage retrieval: A comprehensive review.\nACM Transactions on Information Systems (TOIS) 40, 4 (2022), 1â€“42.\n[23] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen\nWu, W Bruce Croft, and Xueqi Cheng. 2020. A deep look into neural ranking\nmodels for information retrieval. Information Processing & Management 57, 6\n(2020), 102067.\n[24] ZoltÃ¡n GyÃ¶ngyi, Hector Garcia-Molina, and Jan Pedersen. 2004. Combating web\nspam with trustrank. In Proceedings of the Thirtieth international conference on\nVery large data bases-Volume 30 . 576â€“587.\n[25] Hans WA Hanley and Zakir Durumeric. 2023. Machine-Made Media: Moni-\ntoring the Mobilization of Machine-Generated Articles on Misinformation and\nMainstream News Websites. arXiv preprint arXiv:2305.09820 (2023).\n[26] Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan\nHanbury. 2021. Efficiently teaching an effective dense retriever with balanced\ntopic aware sampling. In Proceedings of the 44th International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval . 113â€“122.\n[27] Allen H Huang, Hui Wang, and Yi Yang. 2023. FinBERT: A large language model\nfor extracting information from financial text.Contemporary Accounting Research\n40, 2 (2023), 806â€“841.\n[28] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118\n(2021).\n[29] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,\nTimo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Few-shot learning with retrieval augmented language models.arXiv\npreprint arXiv:2208.03299 (2022).\n[30] Bohan Jiang, Zhen Tan, Ayushi Nirmal, and Huan Liu. 2023. Disinformation Detec-\ntion: An Evolving Challenge in the Age of LLMs. arXiv preprint arXiv:2309.15847\n(2023).\n[31] Virginia Klema and Alan Laub. 1980. The singular value decomposition: Its\ncomputation and some applications. IEEE Transactions on automatic control 25, 2\n(1980), 164â€“176.\n[32] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton\nLee, et al. 2019. Natural questions: a benchmark for question answering research.\nTransactions of the Association for Computational Linguistics 7 (2019), 453â€“466.\n[33] Hang Li. 2022. Learning to rank for information retrieval and natural language\nprocessing. Springer Nature.\n[34] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations\nand TrendsÂ® in Information Retrieval 3, 3 (2009), 225â€“331.\n[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[36] Christopher D Manning. 2009.An introduction to information retrieval . Cambridge\nuniversity press.\n[37] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a\npretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020).\n[38] Desnes Nunes, Ricardo Primi, Ramon Pires, Roberto de Alencar Lotufo, and\nRodrigo Nogueira. 2023. Evaluating GPT-3.5 and GPT-4 Models on Brazilian\nUniversity Admission Exams. ArXiv abs/2303.17003 (2023).\n[39] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and\nWilliam Yang Wang. 2023. On the Risk of Misinformation Pollution with Large\nLanguage Models. arXiv preprint arXiv:2305.13661 (2023).\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research 21, 1 (2020), 5485â€“5551.\n[41] Stephen Robertson, Hugo Zaragoza, et al . 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends Â® in Information Retrieval\n3, 4 (2009), 333â€“389.\n[42] Vinu Sankar Sadasivan, Aounon Kumar, S. Balasubramanian, Wenxiao Wang,\nand Soheil Feizi. 2023. Can AI-Generated Text be Reliably Detected? ArXiv\nabs/2303.11156 (2023). https://api.semanticscholar.org/CorpusID:257631570\n[43] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models. arXiv preprint arXiv:2301.12652 (2023).\n[44] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and\nRoss Anderson. 2023. Model dementia: Generated data makes models forget.\narXiv e-prints (2023), arXivâ€“2305.\n[45] Amit Singhal et al. 2001. Modern information retrieval: A brief overview. IEEE\nData Eng. Bull. 24, 4 (2001), 35â€“43.\n[46] Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its\napplication in retrieval. Journal of documentation 28, 1 (1972), 11â€“21.\n[47] Giovanni Spitale, Nikola Biller-Andorno, and Federico Germani. 2023. AI model\nGPT-3 (dis) informs us better than humans. arXiv preprint arXiv:2301.11924\n(2023).\nNeural Retrievers are Biased Towards LLM-Generated Content KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\n[48] Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lingrui Liao, Luca Bertelli,\nand Michael Bendersky. 2022. QUILL: Query Intent with Large Language Models\nusing Retrieval Augmentation and Multi-stage Distillation. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing: Industry\nTrack. 492â€“501.\n[49] Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, and Preslav Nakov. 2023.\nFake News Detectors are Biased against Texts Generated by Large Language\nModels. arXiv preprint arxiv:2309.08674 (2023).\n[50] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun\nRen. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as\nRe-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).\n[51] Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-\nPhilippe Morency. 2023. Language Models Get a Gender Makeover: Mitigating\nGender Bias with Few-Shot Data Interventions. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics . 340â€“351.\n[52] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna\nGurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of\nInformation Retrieval Models. In Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track .\n[53] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura\nGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models\nin medicine. Nature medicine 29, 8 (2023), 1930â€“1940.\n[54] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[55] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research 9, 11 (2008).\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[57] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen,\nArman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific\nclaims. arXiv preprint arXiv:2004.14974 (2020).\n[58] Alex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak:\nBERT as a Markov random field language model. arXiv preprint arXiv:1902.04094\n(2019).\n[59] Chenguang Wang, Mu Li, and Alexander J Smola. 2019. Language models with\ntransformers. arXiv preprint arXiv:1904.09408 (2019).\n[60] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with\nLarge Language Models. arXiv preprint arXiv:2303.07678 (2023).\n[61] Tao Wang, Yushu Zhang, Shuren Qi, Ruoyu Zhao, Zhihua Xia, and Jian Weng.\n2023. Security and privacy on generative data in aigc: A survey. arXiv preprint\narXiv:2309.09435 (2023).\n[62] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\n2020. Minilm: Deep self-attention distillation for task-agnostic compression of\npre-trained transformers. Advances in Neural Information Processing Systems 33\n(2020), 5776â€“5788.\n[63] Xiting Wang, Xinwei Gu, Jie Cao, Zihua Zhao, Yulan Yan, Bhuvan Middha, and\nXing Xie. 2021. Reinforcing pretrained models for generating attractive text\nadvertisements. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining . 3697â€“3707.\n[64] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.arXiv preprint arXiv:2206.07682\n(2022).\n[65] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Hong Lin. 2023.\nAi-generated content (aigc): A survey. arXiv preprint arXiv:2304.06632 (2023).\n[66] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebas-\ntian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.\n2023. Bloomberggpt: A large language model for finance. arXiv preprint\narXiv:2303.17564 (2023).\n[67] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-\ntive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\n(2020).\n[68] Jun Xu and Hang Li. 2007. Adarank: a boosting algorithm for information\nretrieval. In Proceedings of the 30th annual international ACM SIGIR conference on\nResearch and development in information retrieval . 391â€“398.\n[69] Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2022. Match-Prompt:\nImproving Multi-task Generalization Ability for Neural Text Matching via Prompt\nLearning. In Proceedings of the 31st ACM International Conference on Information\n& Knowledge Management . 2290â€“2300.\n[70] Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2023. BERM: Training\nthe Balanced and Extractable Representation for Matching to Improve General-\nization Ability of Dense Retrieval. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics . 6620â€“6635.\n[71] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming\nJiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A\nsurvey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 (2023).\n[72] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained transformers\nfor text ranking: BERT and beyond. In Proceedings of the 14th ACM International\nConference on web search and data mining . 1154â€“1156.\n[73] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2023. Dense Text\nRetrieval based on Pretrained Language Models: A Survey. ACM Trans. Inf. Syst.\n(dec 2023).\n[74] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n[75] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-\nlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for\ninformation retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).\n[76] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Ex-\nploring ai ethics of chatgpt: A diagnostic analysis.arXiv preprint arXiv:2301.12867\n(2023).\nA EXAMPLE OF EVALUATION METRICS\nğ‘‘1\nğ» ğ‘‘2\nğ» ğ‘‘3\nğ»ğ‘‘2\nğº ğ‘‘3\nğºğ‘‘1\nğºFor Human-Written Corpus:\nàµ—1 log2(3+1)\nàµ—1 log2(1+1)\n= 1\n2\nàµ—1 log2(3+1)\nàµ—1 log2(1+1)\n= 1\n20\nNDCG@K\n0\nK=1 K=3 K=5\nMAP@K\nK=1 K=3 K=5\nğ‘‘2\nğ» ğ‘‘3\nğ»ğ‘‘2\nğº ğ‘‘3\nğºğ‘‘1\nğº ğ‘‘1\nğ»For LLM-Generated Corpus:\nNDCG@K\nK=1 K=3 K=5\nMAP@K\nK=1 K=3 K=5\nàµ—1 log2(1+1)\nàµ—1 log2(1+1)\n= 1\nàµ—1 log2(1+1)\nàµ—1 log2(1+1)\n= 1\nàµ—1 log2(1+1)\nàµ—1 log2(1+1)\n= 1 1\n1âˆ— àµ—1 3\n1 = 1\n3\n1âˆ—1\n1 = 1\n1âˆ— àµ—1 3\n1 = 1\n3\n1âˆ—1\n1 = 1\nFigure 10: A toy example to illustrate how ranking metrics\nare calculated for each target corpus. In this toy example,\ngiven a query, the top 6 documents are retrieved and the\nrank is from left to right in descending order. Two relevant\ndocuments (i.e., ğ‘‘ğº\n1 and ğ‘‘ğ»\n1 ) are highlighted with red boxes.\nIn this section, we provide a toy example for illustrating the\ncalculation of the evaluation metrics for exploring source bias,\nas depicted in Figure 10. Specifically, for each query, an IR model\nproduces a ranking list that comprises documents from both human-\nwritten and LLM-generated corpora. We then calculate ranking\nmetrics separately for human-written and LLM-generated texts,\ndepending on the target data source. When computing metrics for\none target (e.g., human-written corpusCğ»), the data corresponding\nto the other side (e.g., LLM-generated corpusCğº) is disregarded (i.e.,\nmask all the positive labelğ‘Ÿ âˆˆRğº as negative), but the rank of each\ndocument is still based on the original ranking list that incorporates\na mixture of both types of text. For instance, in this toy example,\nwhen targeting the human-written corpus, the relevant document\nğ‘‘ğº\n1 generated by LLM is treated as a negative sample. And when\ncalculating the ranking metrics, we only consider the rank of the\npositive human-written documents. When the target corpus is LLM-\ngenerated, we adopt the same principle, i.e., only take the rank of\nthe positive LLM-generated documents into account for calculating\nthe ranking metrics.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sunhao Dai et al.\nB MORE EXPERIMENTAL RESULTS\nIn Table 7, we provide the results for source bias with more common\nprompts sourced from InstructGPT-prompts Github Repository 7.\nThese results indicate that common prompts can easily trigger\nsource bias with LLM-generated content.\nTable 7: Overall source bias in neural retrievers w.r.t. Rel-\native Î” (NDCG@1) on SciFact+AIGC with mixed human-\nwritten and Llama2-generated corpora generated from dif-\nferent common rephrasing prompts.\nPrompt ANCE BERM TAS-B ContrieverRewrite the text below in your own words: -7.0 -22.8 -11.2 -27.6Paraphrase the provided text while maintaining its meaning: -26.0 -55.3 -24.1 -13.6Summarize the following passage in a concise manner: -1.4 -43.3 -34.0 -32.4Simplify the given passage while keeping the main ideas: -29.0 -21.7 -22.5 -40.9Rephrase the given text using alternative expressions: -25.3 -34.7 -61.9 -18.4Condense the following passage to focus on key points: -19.0 -24.8 -22.8 -16.4Briefly restate the provided text without losing its essence: -29.6 -50.7 -41.3 -29.8Reword the passage below to make it more succinct: -40.6 -71.1 -54.7 -34.0Express the following text in a different way while keeping its intent: -50.7 -39.8 -34.9 0.0\nC THEORETICAL ANALYSIS AND INSIGHTS\nIn Figure 7, we have compared the PPL for different corpus using\nthe BERT model. In this section, we aim to further provide some\ntheoretical insights into the above observations that LLM-generated\ntexts have a smaller perplexity than human-written texts.\nWithout loss of generality, we define the PPL in an autoregressive\nmanner. Let ğ‘‘ğ» denote a document written by humans, and ğ‘‘ğº a\ndocument generated by an LLM conditioned on ğ‘‘ğ».For a given\ndocument ğ‘‘ and BERT model B, PPL is calculated as\nPPL(ğ‘‘,B)= âˆ’1\nğ‘†\nğ‘†âˆ‘ï¸\nğ‘ =1\nlog ğ‘ƒBERT (ğ‘‘ğ‘ |ğ‘‘<ğ‘ ).\nSimilarly, we use PPL(ğ‘‘,H)to represent the PPL of document ğ‘‘\nwhen evaluated by humans. The PPL of ğ‘‘ğº conditioned on ğ‘‘ğ» is\ndenoted as\nPPL(ğ‘‘ğº |ğ‘‘ğ»,B)= âˆ’1\nğ‘†\nğ‘†âˆ‘ï¸\nğ‘ =1\nlog ğ‘ƒBERT (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»).\nWhen evaluated by humans, we use PPL(ğ‘‘ğº |ğ‘‘ğ»,H)to represent\nthe PPL of ğ‘‘ğº conditioned on ğ‘‘ğ».\nIn the theorem below, we introduce three assumptions: Seman-\ntic Superiority, Conditional Redundancy, and Bounded Perplexity,\nto theoretically establish the sufficient conditions under which\nPPL(ğ‘‘ğº,B)â‰¤ PPL(ğ‘‘ğ»,B)holds. Semantic Superiority suggests\nthat the perplexity of human-written texts, when evaluated by\nhumans, is lower than when evaluated by BERT. Conditional Re-\ndundancy implies that the perplexity ofğ‘‘ğº, given ğ‘‘ğ», is lower than\nthe perplexity of ğ‘‘ğ» when evaluated directly. This is intuitively\ntrue when the information added in generating from ğ‘‘ğ» to ğ‘‘ğº\ndoesnâ€™t exceed the original information in ğ‘‘ğ». Bounded perplex-\nity assumes that there exists an upper bound ğœ– on the increase in\nperplexity when evaluating ğ‘‘ğºdirectly, compared to evaluating ğ‘‘ğº\nconditioned on ğ‘‘ğ». Then we have the following theorem:\nTheorem C.1. Given the following conditions:\n7https://github.com/kevinamiri/Instructgpt-prompts#rephrase-a-passage\nâ€¢Semantic Superiority: human beings outperform BERT in under-\nstanding human-written texts, i.e.,\nPPL(ğ‘‘ğ»,B)âˆ’ PPL(ğ‘‘ğ»,H)â‰¥ 0.\nâ€¢Conditional Redundancy: generating ğ‘‘ğº from ğ‘‘ğ» adds less per-\nplexity than ğ‘‘ğ» itself, i.e.,\nPPL(ğ‘‘ğ»,H)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,H)â‰¥ 0.\nâ€¢Bounded Perplexity: there exists a bounded non-negative difference\nğœ– in BERTâ€™s perplexity for ğ‘‘ğº with or without ğ‘‘ğ»,i.e.,\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,B)â‰¤ ğœ–.\nIf LLM aligns more closely with BERT than with humans when pre-\ndicting ğ‘‘ğº given ğ‘‘ğ», such that for any ğ‘  âˆˆ[ğ‘†],\nğ·KL\n\u0010\nğ‘ƒLLM (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)âˆ¥ğ‘ƒBERT (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)\n\u0011\n+ğœ–\nâ‰¤ğ·KL\n\u0010\nğ‘ƒLLM (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)âˆ¥ğ‘ƒHuman (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)\n\u0011\n,\n(3)\nit follows that\nEğ‘ƒLLM (ğ‘‘ğº|ğ‘‘ğ»)\nh\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,B)\ni\nâ‰¤0.\nIn Theorem C.1, the KL divergence is used to compare the distri-\nbutions of the document ğ‘‘ğº conditioned on ğ‘‘ğ» according to the\nLLM, BERT model, and humans. It is worth emphasizing that in-\nequation (3) is not the assumption on the understanding capabilities\nof BERT, LLM, and humans. Instead, this inequation assumes that\nwhen predicting ğ‘‘ğº given ğ‘‘ğ»,the predictions by LLM are more\nclosely aligned with those of BERT.\nWe demonstrate that, when inequation (3) is satisfied, the per-\nplexity (evaluated by PLMs such as BERT) of ğ‘‘ğº is lower than\nthat of ğ‘‘ğ». Weâ€™d like to emphasize that it is reasonable to expect\nthat inequation (3) holds true because both LLM and BERT are\nTransformer-based models that use similar pretraining paradigms.\nThe commonality in model structure and learning paradigms may\nlead to similar inherent biases in text prediction, making their pre-\ndictions more aligned with each other.\nThe proof for Theorem C.1 is provided as follows:\nProof. We start by introducing the term PPL(ğ‘‘ğ»,H):\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,B)\n= PPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,H)+ PPL(ğ‘‘ğ»,H)âˆ’ PPL(ğ‘‘ğ»,B)\nâ‰¤PPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,H),\nwhere the last step follows from the Semantic Superiority condition.\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,B)â‰¤ PPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,H)\n= PPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,G)\n+PPL(ğ‘‘ğº |ğ‘‘ğ»,G)âˆ’ PPL(ğ‘‘ğ»,H).\nNext, we provide upper bounds ofPPL(ğ‘‘ğº |ğ‘‘ğ»,G)âˆ’PPL(ğ‘‘ğ»,H):\nPPL(ğ‘‘ğº |ğ‘‘ğ»,G)âˆ’ PPL(ğ‘‘ğ»,H)\n= PPL(ğ‘‘ğº |ğ‘‘ğ»,G)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,H)\n+PPL(ğ‘‘ğº |ğ‘‘ğ»,H)âˆ’ PPL(ğ‘‘ğ»,H)\nâ‰¤PPL(ğ‘‘ğº |ğ‘‘ğ»,G)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,H),\nNeural Retrievers are Biased Towards LLM-Generated Content KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nwhere the inequality follows from the Conditional Redundancy.\nTaking expectation on PPL(ğ‘‘ğº |ğ‘‘ğ»,G)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,H):\nâˆ’ğ‘†Eğ‘ƒLLM (ğ‘‘ğº|ğ‘‘ğ»)\nh\nPPL(ğ‘‘ğº |ğ‘‘ğ»,G)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,H)\ni\n=\nğ‘†âˆ‘ï¸\nğ‘ =1\nEğ‘ƒLLM (ğ‘‘ğº|ğ‘‘ğ»)log ğ‘ƒLLM (ğ‘‘ğºğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)\nğ‘ƒHuman (ğ‘‘ğºğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)\n=\nğ‘†âˆ‘ï¸\nğ‘ =1\nEğ‘ƒLLM (ğ‘‘ğº\n<ğ‘ |ğ‘‘ğ»)Eğ‘ƒLLM (ğ‘‘ğºğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)log ğ‘ƒLLM (ğ‘‘ğºğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)\nğ‘ƒHuman (ğ‘‘ğºğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)\n=\nğ‘†âˆ‘ï¸\nğ‘ =1\nEğ‘ƒLLM (ğ‘‘ğº\n<ğ‘ |ğ‘‘ğ»)ğ·KL (ğ‘ƒLLM (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)âˆ¥ğ‘ƒHuman (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»))\n|                                                         {z                                                         }\nğ·KL (ğ‘ƒLLM âˆ¥ğ‘ƒHuman )\n.\nSimilarly, PPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,G)can be rewritten as:\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,G)\n= PPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,B)\n+PPL(ğ‘‘ğº |ğ‘‘ğ»,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,G).\nTaking expectation on PPL(ğ‘‘ğº |ğ‘‘ğ»,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,G):\nğ‘†Eğ‘ƒLLM (ğ‘‘ğº|ğ‘‘ğ»)\nh\nPPL(ğ‘‘ğº |ğ‘‘ğ»,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,G)\ni\n=\nğ‘†âˆ‘ï¸\nğ‘ =1\nEğ‘ƒLLM (ğ‘‘ğ¿\n<ğ‘ |ğ‘‘ğ»)ğ·KL (ğ‘ƒLLM (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»)âˆ¥ğ‘ƒBERT (ğ‘‘ğº\nğ‘  |ğ‘‘ğº\n<ğ‘ ,ğ‘‘ğ»))\n|                                                       {z                                                       }\nğ·KL (ğ‘ƒLLM âˆ¥ğ‘ƒBERT )\n.\nThus,\nEğ‘ƒLLM (ğ‘‘ğº|ğ‘‘ğ»)\nh\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,B)\ni\nâ‰¤Eğ‘ƒLLM (ğ‘‘ğº|ğ‘‘ğ»)\nh\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,B)\ni\n+1\nğ‘†\nğ‘†âˆ‘ï¸\nğ‘ =1\nEğ‘ƒLLM (ğ‘‘ğº\n<ğ‘ |ğ‘‘ğ»)(ğ·KL (ğ‘ƒLLM âˆ¥ğ‘ƒBERT)âˆ’ğ·KL (ğ‘ƒLLM âˆ¥ğ‘ƒHuman)).\nThe final results can be derived by considering the assumptions:\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğº |ğ‘‘ğ»,B)â‰¤ ğœ–\nğ·KL (ğ‘ƒLLM âˆ¥ğ‘ƒBERT)âˆ’ğ·KL (ğ‘ƒLLM âˆ¥ğ‘ƒHuman)â‰¤âˆ’ ğœ–.\nFrom these, it follows that:\nEğ‘ƒLLM (ğ‘‘ğº|ğ‘‘ğ»)\nh\nPPL(ğ‘‘ğº,B)âˆ’ PPL(ğ‘‘ğ»,B)\ni\nâ‰¤ğœ–âˆ’ğœ– = 0.\nâ–¡",
  "topic": "Constraint (computer-aided design)",
  "concepts": [
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.6006116271018982
    },
    {
      "name": "Computer science",
      "score": 0.5781707167625427
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5567521452903748
    },
    {
      "name": "Artificial neural network",
      "score": 0.5178287625312805
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.49416568875312805
    },
    {
      "name": "The Internet",
      "score": 0.4787156581878662
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.43412482738494873
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42459675669670105
    },
    {
      "name": "Natural language processing",
      "score": 0.40381479263305664
    },
    {
      "name": "Information retrieval",
      "score": 0.37985074520111084
    },
    {
      "name": "Cognitive psychology",
      "score": 0.34670954942703247
    },
    {
      "name": "Psychology",
      "score": 0.28872162103652954
    },
    {
      "name": "World Wide Web",
      "score": 0.24472489953041077
    },
    {
      "name": "Engineering",
      "score": 0.10531795024871826
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}