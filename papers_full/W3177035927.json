{
  "title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment",
  "url": "https://openalex.org/W3177035927",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2969024387",
      "name": "Zewen Chi",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Beijing Microelectronics Technology Institute",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Microsoft (United States)",
        "Beijing Microelectronics Technology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2100574101",
      "name": "Bo Zheng",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Beijing Microelectronics Technology Institute",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2612910427",
      "name": "Shaohan Huang",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Microsoft (United States)",
        "Beijing Microelectronics Technology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4208379621",
      "name": "Xian-Ling Mao",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Microsoft (United States)",
        "Beijing Microelectronics Technology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2101832271",
      "name": "Heyan, Huang",
      "affiliations": [
        "Beijing Microelectronics Technology Institute",
        "Microsoft Research Asia (China)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Microsoft (United States)",
        "Beijing Microelectronics Technology Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W3103942011",
    "https://openalex.org/W3039256091",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2150066400",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2538358357",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W4298170715",
    "https://openalex.org/W3100069613",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W4206471589",
    "https://openalex.org/W2960374072",
    "https://openalex.org/W2963877297",
    "https://openalex.org/W4301187301",
    "https://openalex.org/W2964174820",
    "https://openalex.org/W2952682849",
    "https://openalex.org/W3097879195",
    "https://openalex.org/W4288284086",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W3105813095",
    "https://openalex.org/W3156665996",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W3105492289",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W3118106810",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W3186903869",
    "https://openalex.org/W3169425228",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W2996822578"
  ],
  "abstract": "Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan Huang, Furu Wei. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3418–3430\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3418\nImproving Pretrained Cross-Lingual Language Models via\nSelf-Labeled Word Alignment\nZewen Chi†∗, Li Dong ‡, Bo Zheng ‡∗, Shaohan Huang ‡\nXian-Ling Mao†, Heyan Huang †, Furu Wei ‡\n†Beijing Institute of Technology\n‡Microsoft Research\n{czw,maoxl,hhy63}@bit.edu.cn\n{lidong1,v-zhebo,shaohanh,fuwei}@microsoft.com\nAbstract\nThe cross-lingual language models are typi-\ncally pretrained with masked language model-\ning on multilingual text or parallel sentences.\nIn this paper, we introduce denoising word\nalignment as a new cross-lingual pre-training\ntask. Speciﬁcally, the model ﬁrst self-labels\nword alignments for parallel sentences. Then\nwe randomly mask tokens in a bitext pair.\nGiven a masked token, the model uses a\npointer network to predict the aligned token\nin the other language. We alternately per-\nform the above two steps in an expectation-\nmaximization manner. Experimental results\nshow that our method improves cross-lingual\ntransferability on various datasets, especially\non the token-level tasks, such as question an-\nswering, and structured prediction. More-\nover, the model can serve as a pretrained\nword aligner, which achieves reasonably low\nerror rates on the alignment benchmarks. The\ncode and pretrained parameters are available at\ngithub.com/CZWin32768/XLM-Align.\n1 Introduction\nDespite the current advances in NLP, most applica-\ntions and resources are still English-centric, mak-\ning non-English users hard to access. Therefore, it\nis essential to build cross-lingual transferable mod-\nels that can learn from the training data in high-\nresource languages and generalize on low-resource\nlanguages. Recently, pretrained cross-lingual lan-\nguage models have shown their effectiveness for\ncross-lingual transfer. By pre-training on monolin-\ngual text and parallel sentences, the models provide\nsigniﬁcant improvements on a wide range of cross-\nlingual end tasks (Conneau and Lample, 2019; Con-\nneau et al., 2020; Liu et al., 2020; Chi et al., 2021b).\nCross-lingual language model pre-training is typ-\nically achieved by learning various pretext tasks on\n∗Contribution during internship at Microsoft Research.\nmonolingual and parallel corpora. By simply learn-\ning masked language modeling (MLM; Devlin et al.\n2019) on monolingual text of multiple languages,\nthe models surprisingly achieve competitive results\non cross-lingual tasks (Wu and Dredze, 2019; K\net al., 2020). Besides, several pretext tasks are\nproposed to utilize parallel corpora to learn better\nsentence-level cross-lingual representations (Con-\nneau and Lample, 2019; Chi et al., 2021b; Hu et al.,\n2020a). For example, the translation language mod-\neling (TLM; Conneau and Lample 2019) task per-\nforms MLM on the concatenated parallel sentences,\nwhich implicitly enhances cross-lingual transfer-\nability. However, most pretext tasks either learn\nalignment at the sentence level or implicitly en-\ncourage cross-lingual alignment, leaving explicit\nﬁne-grained alignment task not fully explored.\nIn this paper, we introduce a new cross-lingual\npre-training task, named as denoising word align-\nment. Rather than relying on external word aligners\ntrained on parallel corpora (Cao et al., 2020; Zhao\net al., 2020; Wu and Dredze, 2020), we utilize\nself-labeled alignments in our task. During pre-\ntraining, we alternately self-label word alignments\nand conduct the denoising word alignment task in\nan expectation-maximization manner. Speciﬁcally,\nthe model ﬁrst self-labels word alignments for a\ntranslation pair. Then we randomly mask tokens\nin the bitext sentence, which is used as the per-\nturbed input for denosing word alignment. For\neach masked token, the model learns a pointer net-\nwork to predict the self-labeled alignments in the\nother language. We repeat the above two steps to\niteratively boost the bitext alignment knowledge\nfor cross-lingual pre-training.\nWe conduct extensive experiments on a wide\nrange of cross-lingual understanding tasks. Experi-\nmental results show that our model outperforms the\nbaseline models on various datasets, particularly\non the token-level tasks such as question answer-\n3419\ning and structured prediction. Moreover, our model\ncan also serve as a multilingual word aligner, which\nachieves reasonable low error rates on the bitext\nalignment benchmarks.\nOur contributions are summarized as follows:\n• We present a cross-lingual pre-training\nparadigm that alternately self-labels and pre-\ndicts word alignments.\n• We introduce a pre-training task, denoising\nword alignment, which predicts word align-\nments from perturbed translation pairs.\n• We propose a word alignment algorithm that\nformulates the word alignment problem as\noptimal transport.\n• We demonstrate that our explicit alignment\nobjective is effective for cross-lingual transfer.\n2 Related Work\nCross-lingual LM pre-training Pretrained with\nmasked language modeling (MLM; Devlin et al.\n2019) on monolingual text, multilingual BERT\n(mBERT; Devlin et al. 2019) and XLM-R (Con-\nneau et al., 2020) produce promising results\non cross-lingual transfer benchmarks (Hu et al.,\n2020b). mT5 (Xue et al., 2020) learns a multilin-\ngual version of T5 (Raffel et al., 2020) with text-to-\ntext tasks. In addition to monolingual text, several\nmethods utilize parallel corpora to improve cross-\nlingual transferability. XLM (Conneau and Lample,\n2019) presents the translation language modeling\n(TLM) task that performs MLM on concatenated\ntranslation pairs. ALM (Yang et al., 2020) intro-\nduces code-switched sequences into cross-lingual\nLM pre-training. Unicoder (Huang et al., 2019) em-\nploys three cross-lingual tasks to learn mappings\namong languages. From an information-theoretic\nperspective, InfoXLM (Chi et al., 2021b) proposes\nthe cross-lingual contrastive learning task to align\nsentence-level representations. Additionally, AM-\nBER (Hu et al., 2020a) introduces an alignment\nobjective that minimizes the distance between the\nforward and backward attention matrices. More re-\ncently, Ernie-M (Ouyang et al., 2020) presents the\nback-translation masked language modeling task\nthat generates pseudo parallel sentence pairs for\nlearning TLM, which provides better utilization\nof monolingual corpus. VECO (Luo et al., 2020)\npretrains a uniﬁed cross-lingual language model\nfor both NLU and NLG. mT6 (Chi et al., 2021a)\nimproves the multilingual text-to-text transformer\nwith translation pairs.\nNotably, Word-aligned BERT models (Cao et al.,\n2020; Zhao et al., 2020) ﬁnetune mBERT by an\nexplicit alignment objective that minimizes the dis-\ntance between aligned tokens. Wu and Dredze\n(2020) exploit contrastive learning to improve the\nexplicit alignment objectives. However, Wu and\nDredze (2020) show that these explicit alignment\nobjectives do not improve cross-lingual representa-\ntions under a more extensive evaluation. Moreover,\nthese models are restricted to stay close to their\noriginal pretrained values, which is not applicable\nfor large-scale pre-training. On the contrary, we\ndemonstrate that employing our explicit alignment\nobjective in large-scale pre-training can provide\nconsistent improvements over baseline models.\nWord alignment The IBM models (Brown et al.,\n1993) are statistical models for modeling the trans-\nlation process that can extract word alignments be-\ntween sentence pairs. A large number of word\nalignment models are based on the IBM mod-\nels (Och and Ney, 2003; Mermer and Sara c ¸lar,\n2011; Dyer et al., 2013; ¨Ostling and Tiedemann,\n2016). Recent studies have shown that word align-\nments can be extracted from neural machine trans-\nlation models (Ghader and Monz, 2017; Koehn and\nKnowles, 2017; Li et al., 2019) or from pretrained\ncross-lingual LMs (Jalili Sabet et al., 2020; Nagata\net al., 2020).\n3 Method\nFigure 1 illustrates an overview of our method\nfor pre-training our cross-lingual LM, which is\ncalled XLM-A LIGN . XLM-A LIGN is pretrained\nin an expectation-maximization manner with two\nalternating steps, which are word alignment self-\nlabeling and denoising word alignment. We ﬁrst\nformulate word alignment as an optimal transport\nproblem, and self-label word alignments of the in-\nput translation pair on-the-ﬂy. Then, we update the\nmodel parameters with the denoising word align-\nment task, where the model uses a pointer net-\nwork (Vinyals et al., 2015) to predict the aligned\ntokens from the perturbed translation pair.\n3.1 Word Alignment Self-Labeling\nThe goal of word alignment self-labeling is\nto estimate the word alignments of the input\ntranslation pair on-the-ﬂy, given the current\nXLM-A LIGN model. Given a source sentence\n3420\n(a) Word alignment self-labeling\nXLM-Align Encoder你好世界。\nHelloworld.\n你好<-> Hello世界<->world。<-> .   \nSelf-Labeled Word Alignments\nXLM-Align Encoder\nAlignment Probability\n你好[M]。[M]world.\n你好世界。\nHelloworld.\nAlignment as Optimal Transport\nTranslationPair Noisy Translation Pair (Random Masks)\nPointer Network\nquery keys\nDWA Loss\n(b) Denoising word alignment\nFigure 1: An overview of our method. XLM-A LIGN is pretrained in an expectation-maximization manner with\ntwo alternating steps. (a) Word alignment self-labeling: we formulate word alignment as an optimal transport\nproblem, and self-labels word alignments of the input translation pair on-the-ﬂy; (b) Denoising word alignment:\nwe update the model parameters with the denoising word alignment task, where the model uses a pointer network\nto predict the aligned tokens from the perturbed translation pair.\nS = s1 ...s i...s n and a target sentence T =\nt1 ...t j ...t m, we model the word alignment be-\ntween S and T as a doubly stochastic matrix\nA ∈Rn×m\n+ such that the rows and the columns\nall sum to 1, where Aij stands for the probability\nof the alignment between si and tj. The rows and\nthe columns of A represent probability distribu-\ntions of the forward alignment and the backward\nalignment, respectively. To measure the similarity\nbetween two tokens from Sand T, we deﬁne a\nmetric function fsim by using cross-lingual repre-\nsentations produced by XLM-A LIGN :\nfsim(si,tj) =−log max(ϵ,h⊤\ni hj) (1)\nwhere ϵis a constant to avoid negative values in the\nlog function, and hi is the hidden vector of the i-th\ntoken by encoding the concatenated sequence of S\nand T with XLM-A LIGN . Empirically, the metric\nfunction produces a high similarity score if the two\ninput tokens are semantically similar.\nThe word alignment problem is formulated as\nﬁnding Athat maximizes the sentence similarity\nbetween Sand T:\nmax\nA\nn∑\ni=1\nm∑\nj=1\nAijfsim(si,tj) (2)\nWe can ﬁnd that Eq. (2) is identical to the regular-\nized optimal transport problem (Peyr´e et al., 2019),\nif we add an entropic regularization to A:\nmax\nA\nn∑\ni=1\nm∑\nj=1\nAijfsim(si,tj) −µAij log Aij (3)\nEq. (3) has a unique solution A∗such that\nA∗= diag(u)Kdiag(v) (4)\nKij = efsim(si,tj )/µ (5)\nwhere u∈Rn\n+,v∈Rm\n+ ,K ∈Rn×m\n+ . According\nto Sinkhorn’s algorithm (Peyr´e et al., 2019), the\nvariables uand vcan be calculated by the follow-\ning iterations:\nut+1 = 1n\nKvt, vt+1 = 1m\nK⊤ut+1 (6)\nwhere vt can be initialized by vt=0 = 1m.\nWith the solved stochastic matrix A∗, we can\nproduce the forward word alignments − →Aby apply-\ning argmax over rows:\n− →A= {(i,j) |j = arg max\nk\nA∗\nik} (7)\nSimilarly, the backward word alignments← −Acan be\ncomputed by applying argmax over columns. To\nobtain high-precision alignment labels, we adopt an\niterative alignment ﬁltering operation. We initialize\n3421\nthe alignment labels Aas ∅. In each iteration, we\nfollow the procedure of Itermax (Jalili Sabet et al.,\n2020) that ﬁrst computes − →Aand ← −Aby Eq. (7).\nThen, the alignment labels are updated by:\nA←A∪ (− →A∩ ← −A) (8)\nFinally, A∗is updated by:\nA∗\nij ←\n\n\n\n0, (i,j) ∈A\nαA∗\nij, ∃k (i,k) ∈A∨ (k,j) ∈A\nA∗\nij, others\n(9)\nwhere αis a discount factor. After several itera-\ntions, we obtain the ﬁnal self-labeled word align-\nments A.\n3.2 Denoising Word Alignment\nAfter self-labeling word alignments, we update the\nmodel parameters with the denoising word align-\nment (DWA) task. The goal of DWA is to predict\nthe word alignments from the perturbed version of\nthe input translation pair.\nConsider the perturbed version of the input trans-\nlation pair (S∗,T∗) constructed by randomly re-\nplacing the tokens with masks. We ﬁrst encode\nthe translation pair into hidden vectors h∗with the\nXLM-A LIGN encoder:\nh∗\n1 ... h∗\nn+m = encoder([S∗,T∗]) (10)\nwhere [S∗,T∗] is the concatenated sequence of S∗\nand T∗with the length of n+ m. Then, we build\na pointer network upon the XLM-A LIGN encoder\nthat predicts the word alignments. Speciﬁcally, for\nthe i-th source token, we useh∗\ni as the query vector\nand h∗\nn+1, ... , h∗\nn+m as the key vectors. Given the\nquery and key vectors, the forward alignment prob-\nability ai is computed by the scaled dot-product\nattention (Vaswani et al., 2017):\nai = softmax(q⊤\ni K√dh\n) (11)\nqi = linear(h∗\ni) (12)\nK = linear([h∗\nn+1 ... h∗\nn+m]) (13)\nwhere dh is the dimension of the hidden vectors.\nSimilarly, the backward alignment probability can\nbe computed by above equations if we use target to-\nkens as the query vectors and h∗\n1 ... h∗\nn as key vec-\ntors. Notice that we only consider the self-labeled\nand masked positions as queries. Formally, we\nuse the following query positions in the pointer\nnetwork:\nP= {i|(i,·) ∈A∨ (·,i) ∈A}∩M (14)\nwhere Mis the set of masked positions. The\ntraining objective is to minimize the cross-entropy\nbetween the alignment probabilities and the self-\nlabeled word alignments:\nLDW A=\n∑\ni∈P\nCE(ai,A(i)) (15)\nwhere CE(·,·) stands for the cross-entropy loss,\nand A(i) is the self-labeled aligned position of the\ni-th token.\nAlgorithm 1 Pre-training XLM-A LIGN\nInput: Multilingual corpus Dm, parallel corpus\nDp, learning rate τ\nOutput: XLM-A LIGN parameters θ\n1: Initialize θwith cold-start pre-training\n2: while not converged do\n3: X∼D m, (S,T) ∼Dp\n4: A← fself-labeling(S,T; θ)\n5: g ← ∇θLMLM(X) +∇θLTLM(S,T) +\n∇θLDW A(S,T,A)\n6: θ←θ−τg\n3.3 Pre-training XLM-A LIGN\nWe illustrate the pre-training procedure of XLM-\nALIGN in Algorithm 1. In addition to DWA, we\nalso include MLM and TLM for pre-trainingXLM-\nALIGN , which implicitly encourage the cross-\nlingual alignment. The overall loss function is\ndeﬁned as:\nLMLM(X) +LTLM(S,T) +LDW A(S,T,A)\nIn each iteration, we ﬁrst sample monolingual text\nX, and parallel text (S,T). Then, we self-label\nword alignments and update the model parameters\nby learning pretext tasks. Notice that the model pa-\nrameters are initialized by a cold-start pre-training\nto avoid producing low-quality alignment labels.\nThe cold-start pre-training can be accomplished by\nusing a pretrained LM as the model initialization.\n4 Experiments\n4.1 Pre-training\nFollowing previous cross-lingual pretrained mod-\nels (Conneau and Lample, 2019; Conneau et al.,\n3422\nModel Structured Prediction Question Answering Sentence Classiﬁcation AvgPOS NER XQuAD MLQA TyDiQA XNLI PAWS-X\nMetrics F1 F1 F1 / EM F1 / EM F1 / EM Acc. Acc.\nMBERT* 70.3 62.2 64.5 / 49.4 61.4 / 44.2 59.7 / 43.9 65.4 81.9 63.1\nXLM* 70.1 61.2 59.8 / 44.3 48.5 / 32.6 43.6 / 29.1 69.1 80.9 58.6\nMT5base - 56.6 67.0 / 49.0 64.6 / 45.0 58.1 / 42.8 75.4 87.4 -\nXLM-Rbase 75.6 61.8 71.9 / 56.4 65.1 / 47.2 55.4 / 38.3 75.0 84.9 66.4\nXLM-ALIGN 76.0 63.7 74.7 / 59.0 68.1 / 49.8 62.1 / 44.8 76.2 86.8 68.9\nTable 1: Evaluation results on XTREME structured prediction, question answering, and sentence classiﬁcation\ntasks. We adopt the cross-lingual transfer setting, where models are only ﬁne-tuned on the English training data\nbut evaluated on all target languages. Results with “*” are taken from (Hu et al., 2020b). Results of XLM-ALIGN\nand XLM-Rbase are averaged over ﬁve runs.\n2020; Chi et al., 2021b), we use raw sentences\nfrom the Wikipedia dump and CCNet (Wenzek\net al., 2019) for MLM, including 94 languages.\nFor TLM and DWA, we use parallel corpora\nfrom MultiUN (Ziemski et al., 2016), IIT Bom-\nbay (Kunchukuttan et al., 2018), OPUS (Tiede-\nmann, 2012), and WikiMatrix (Schwenk et al.,\n2019), including 14 English-centric language pairs.\nWe pretrain a Transformer with 12 layers and\nthe hidden size of 768, where the parameters are\ninitialized with XLM-R (Conneau et al., 2020).\nThe model is optimized with the Adam opti-\nmizer (Kingma and Ba, 2015) for 150K steps with\nbatch size of 2,048. Notice that TLM and DWA\nshare the same forward procedure for encoding the\nperturbed sentence pair. The pre-training of XLM-\nALIGN takes about six days with two Nvidia DGX-\n2 stations. More details of the training data and the\nhyperparameters are in supplementary document.\n4.2 XTREME Benchmark\nXTREME is a multilingual benchmark for eval-\nuating cross-lingual generalization. We evaluate\nour model on 7 cross-lingual downstream tasks in-\ncluded by XTREME, which can be grouped into\n3 categories: (1) Structured prediction: part-of-\nspeech tagging on the Universal Dependencies\nv2.5 (Zeman et al., 2019), and named entity recog-\nnition on the WikiAnn (Pan et al., 2017; Rahimi\net al., 2019) dataset; (2) Question answering: cross-\nlingual question answering on MLQA (Lewis et al.,\n2020) and XQuAD (Artetxe et al., 2020), and gold\npassage of typologically diverse question answer-\ning (TyDiQA-GoldP; Clark et al. 2020); (3) Sen-\ntence classiﬁcation: cross-lingual natural language\ninference (XNLI; Conneau et al. 2018), and cross-\nlingual paraphrase adversaries from word scram-\nbling (PAWS-X; Yang et al. 2019).\nBaselines We use the following pretrained cross-\nlingual LMs as baselines. (1) Multilingual BERT\n(MBERT; Devlin et al. 2019) is pretrained with\nmasked language modeling (MLM) and next sen-\ntence prediction on Wikipedia of 104 languages;\n(2) XLM (Conneau and Lample, 2019) is jointly\npretrained with MLM on 100 languages and trans-\nlation language modeling (TLM) on 14 language\npairs; (3) MT5 (Xue et al., 2020) is the multilingual\nversion of T5 pretrained with text-to-text tasks;\n(4) XLM-R (Conneau et al., 2020) is pretrained\nwith MLM on large-scale CC-100 dataset with long\ntraining steps.\nFine-tuning Following Hu et al. (2020b), we\nadopt the zero-shot transfer setting for evaluation,\nwhere the models are only ﬁne-tuned on English\ntraining data but evaluated on all target languages.\nBesides, we only use one model for evaluation on\nall target languages, rather than selecting different\nmodels for each language. The detailed ﬁne-tuning\nhyperparameters can be found in supplementary\ndocument.\nResults In Table 1, we present the evaluation re-\nsults on XTREME structured prediction, question\nanswering, and sentence classiﬁcation tasks. It can\nbe observed that our XLM-A LIGN obtains the best\naverage score over all the baseline models, improv-\ning the previous score from 66.4 to 68.9. It demon-\nstrates that our model learns more transferable rep-\nresentations for the cross-lingual tasks, which is\nbeneﬁcial for building more accessible multilin-\ngual NLP applications. It is worth mentioning that\nour method brings noticeable improvements on the\nquestion answering and the structured prediction\ntasks. Compared with XLM-Rbase, XLM-A LIGN\nprovides 6.7% and 1.9% F1 improvements on Ty-\nDiQA and NER. The improvements show that the\n3423\nAlignment Method Pretrained Alignment Error Rate ↓ AvgModel en-de en-fr en-hi en-ro\nfast align(Dyer et al., 2013) - 32.14 19.46 59.90 - -\nSimAlign- Argmax (Jalili Sabet et al., 2020) XLM-R 19. 7. 39. 29. 24.\nSimAlign- Itermax (Jalili Sabet et al., 2020) XLM-R 20. 9. 39. 28. 24.\nSimAlign- Itermax (reimplementation) XLM-R 20.15 10.05 38.72 27.41 24.08\nOurs - Optimal Transport (Section 3.1) XLM-R 17.74 7.54 37.79 27.49 22.64\nSimAlign(reimplementation) XLM-A LIGN 18.93 10.33 33.84 27.09 22.55\nOurs - Optimal Transport (Section 3.1) XLM-A LIGN 16.63 6.61 33.98 26.97 21.05\nTable 2: Evaluation results for word alignment on four English-centric language pairs. We report the alignment er-\nror rate scores (lower is better). For bothSimAlign (Jalili Sabet et al., 2020) and our optimal-transport alignment\nmethod, we use the hidden vectors from the 8-th layer produced by XLM-Rbase or XLM-A LIGN . “(reimplementa-\ntion)” is our reimplementation of SimAlign-Itermax.\n3 4 5 6 7 8 9 10 11 12\nLayer\n15\n20\n25\n30\n35Alignment Error RateXLM-R\nXLM-Align\nFigure 2: Evaluation results on word alignment across\ndifferent layers. We illustrate the averaged AER scores\non the test sets of four language pairs. The results of the\nﬁrst two layers are not included due to the high AER.\npretrained XLM-A LIGN beneﬁts from the explicit\nword alignment objective, particularly on the struc-\ntured prediction and question answering tasks that\nrequire token-level cross-lingual transfer. In terms\nof sentence classiﬁcation tasks, XLM-A LIGN also\nconsistently outperforms XLM-Rbase.\n4.3 Word Alignment\nWord alignment is the task of ﬁnding corresponding\nword pairs in a parallel sentence. We conduct eval-\nuations with golden alignments of four language\npairs from EuroParl1, WPT20032, and WPT20053,\ncontaining 1,244 annotated sentence pairs in total.\nWe use alignment error rate (AER; Och and Ney\n1www-i6.informatik.rwth-aachen.de/\ngoldAlignment/\n2web.eecs.umich.edu/˜mihalcea/wpt/\n3web.eecs.umich.edu/˜mihalcea/wpt05/\n2003) as the evaluation metrics.\nResults We ﬁrst explore whether our word align-\nment self-labeling method is effective for generat-\ning high-quality alignment labels. Thus, we com-\npare our method with (1) fast align (Dyer\net al., 2013), a widely-used implementation\nof IBM Model 2 (Och and Ney, 2003); (2)\nSimAlign (Jalili Sabet et al., 2020), state-of-the-\nart unsupervised word alignment method. For a fair\ncomparison, we use the same pretrained LM and\nhidden layer as in SimAlign to produce sentence\nrepresentations. In speciﬁc, we take the hidden\nvectors from the 8-th layer of XLM-Rbase or XLM-\nALIGN , and obtain the alignments following the\nprocedure as described in Section 3.1. Since the\nproduced alignments are subword-level, we con-\nvert the alignments into word-level by the following\nrule that “if two subwords are aligned, the words\nthey belong to are also aligned”.\nAs shown in Table 2, we report the AER\nscores on the four language pairs. It can be ob-\nserved that our optimal-transport method outper-\nforms fast align and SimAlign, demonstrat-\ning that our method can produce high-quality align-\nment labels, which is helpful for the DWA task.\nMoreover, our method consistently outperforms\nSimAlign when using hidden vectors from both\nXLM-Rbase and XLM-A LIGN .\nThen, we compare our XLM-A LIGN with XLM-\nRbase on the word alignment task. Empirically, a\nlower AER indicates that the model learns bet-\nter cross-lingual representations. From Table 2,\nXLM-A LIGN obtains the best AER results over\nall the four language pairs, reducing the aver-\naged AER from 22.64 to 21.05. Besides, un-\n3424\nModels XNLI POS NER MLQA Avg\nXLM-R* 74.6 75.7 61.6 65.7 69.4\nXLM-ALIGN 75.2 75.6 62.6 66.7 70.0\n−DW A 75.1 75.2 62.0 65.8 69.5\n−TLM 74.4 76.0 60.4 66.0 69.2\nTable 3: Ablation studies on the components of XLM-\nALIGN . XLM-R* stands for continue-training XLM-\nRbase with MLM for fair comparisons. Results are aver-\naged over ﬁve runs.\nder both SimAlign and our optimal-transport\nmethod, XLM-A LIGN provides consistent reduc-\ntion of AER, demonstrating the effectiveness of\nour method for learning ﬁne-grained cross-lingual\nrepresentations.\nWe also compareXLM-A LIGN with XLM-Rbase\nusing the hidden vectors from the 3-th layer to the\n12-th layer. We illustrate the averaged AER scores\nin Figure 2. Notice that the results on the ﬁrst two\nlayers are not presented in the ﬁgure because of the\nhigh AER. It can be observed that XLM-A LIGN\nconsistently improves the results over XLM-Rbase\nacross these layers. Moreover, it shows a parabolic\ntrend across the layers of XLM-R base, which is\nconsistent with the results in (Jalili Sabet et al.,\n2020). In contrast to XLM-R base, XLM-A LIGN\nalleviates this trend and greatly reduces AER in the\nlast few layers. We believe this property of XLM-\nALIGN brings better cross-lingual transferability\non the end tasks.\n5 Analysis\nIn this section, we conduct comprehensive ablation\nstudies for a better understanding of our XLM-\nALIGN . To reduce the computational cost, we re-\nduce the batch size to 256, and pretrain models\nwith 50K steps in the following experiments.\n5.1 Ablation Studies\nWe perform ablation studies to understand the com-\nponents of XLM-A LIGN , by removing the de-\nnoising word alignment loss ( −DWA), the TLM\nloss (−TLM), or removing both (XLM-R*), which\nis identical to continue-training XLM-R base with\nMLM. We evaluate the models on XNLI, POS,\nNER, and MLQA, and present the results in Ta-\nble 3. Comparing −TLM with −DWA, we ﬁnd\nthat DWA is more effective for POS and MLQA,\nwhile TLM performs better on XNLI and NER.\nComparing −TLM with XLM-R*, it shows that\ndirectly learning DWA slightly harms the perfor-\nLayer XNLI POS NER MLQA Avg\nLayer-8 75.1 75.3 61.9 66.7 69.8\nLayer-10 75.2 75.6 62.6 66.7 70.0\nLayer-12 75.2 75.8 62.3 67.0 70.1\nTable 4: Results of XLM-A LIGN with different lay-\ners used for word alignment self-labeling during pre-\ntraining. Results are averaged over ﬁve runs.\nLayer XNLI POS NER MLQA Avg\nLayer-8 75.4 75.3 61.7 66.2 69.7\nLayer-10 75.1 75.6 62.5 66.3 69.9\nLayer-12 75.2 75.8 62.3 67.0 70.1\nTable 5: Results of XLM-A LIGN with different layers\nused for denoising word alignment during pre-training.\nResults are averaged over ﬁve runs.\nmance. However, jointly learning DW A with TLM\nprovides remarkable improvements over −DWA,\nespecially on the question answering and the struc-\nture prediction tasks that requires token-level cross-\nlingual transfer. This indicates that TLM poten-\ntially improves the quality of self-labeled word\nalignments, making DW A more effective for cross-\nlingual transfer.\n5.2 Word Alignment Self-Labeling Layer\nIt has been shown that the word alignment perfor-\nmance has a parabolic trend across the layers of\nmBERT and XLM-R (Jalili Sabet et al., 2020). It\nindicates that the middle layers produce higher-\nquality word alignments than the bottom and the\ntop layers. To explore which layer produces bet-\nter alignment labels for pre-training, we pretrain\nthree variants of XLM-A LIGN , where we use the\nhidden vectors from three different layers for word\nalignment self-labeling. We use the 8-th, 10-th,\nand 12-th layers for word alignment self-labeling\nduring the pre-training. We present the evaluation\nresults in Table 4. Surprisingly, although Layer-\n8 produces higher-quality alignment labels at the\nbeginning of the pre-training, using the alignment\nlabels from the 12-th layer learns a more trans-\nferable XLM-A LIGN model for cross-lingual end\ntasks.\n5.3 Denoising Word Alignment Layer\nBeyond the self-labeling layer, we also investigate\nwhich layer is better for learning the denoising\nword alignment task. Recent studies have shown\n3425\nFiltering XNLI POS NER MLQA Avg\nEnable 75.2 75.6 62.6 66.7 70.0\nDisable 74.2 75.3 61.6 65.3 69.1\nTable 6: Effects of alignment ﬁltering in word align-\nment self-labeling. Results are averaged over ﬁve runs.\nthat it is beneﬁcial to learn sentence-level cross-\nlingual alignment at a middle layer (Chi et al.,\n2021b). Therefore, we pretrain XLM-A LIGN mod-\nels by using three different layers for DWA, that\nis, using the hidden vectors of middle layers as the\ninput of the pointer network. We compare the eval-\nuation results of the three models in Table 5. It can\nbe found that learning DWA at Layer-8 improves\nXNLI while learning DWA at higher layers pro-\nduces better performance on the other three tasks.\nIt suggests that, compared with sentence-level pre-\ntext tasks that prefers middle layers, the DW A task\nshould be applied at top layers.\n5.4 Effects of Alignment Filtering\nAlthough our self-labeling method produces high-\nquality alignment labels, the alignment ﬁltering\noperation can potentially make some of the tokens\nunaligned, which reduces the example efﬁciency.\nThus, we explore whether the alignment ﬁltering is\nbeneﬁcial for pre-training XLM-A LIGN . To this\nend, we pretrain an XLM-A LIGN model without\nalignment ﬁltering. In speciﬁc, we use the union set\nof the forward and backward alignments as the self-\nlabeled alignments so that all tokens are aligned at\nleast once. The forward and backward alignments\nare obtained by applying the argmax function over\nrows and columns of A∗, respectively. Empiri-\ncally, the alignment ﬁltering operation generates\nhigh-precision yet fewer labels, while removing\nthe ﬁltering promises more labels but introduces\nlow-conﬁdent labels. In Table 6, we compare the\nresults of the models with or without alignment\nﬁltering. It can be observed that the alignment ﬁl-\ntering operation improves the performance on the\nend tasks. This demonstrates that it is necessary to\nuse high-precision labels for learning the denoising\nword alignment task. On the contrary, using per-\nturbed alignment labels in pre-training harms the\nperformance on the end tasks.\n5.5 Effects of DW A Query Positions\nIn the denoising word alignment task, we always\nuse the hidden vectors of the masked positions\nPosition XNLI POS NER MLQA Avg\nmasked 75.2 75.6 62.6 66.7 70.0\nunmasked 75.5 75.5 62.0 66.5 69.8\nall-aligned 75.3 75.9 61.6 66.7 69.9\nno-query 75.1 75.2 62.0 65.8 69.5\nTable 7: Effects of the query positions in the pointer\nnetwork for denoising word alignment. Results are av-\neraged over ﬁve runs.\nas the query vectors in the pointer network. To\nexplore the impact of the DWA query positions,\nwe compare three different query positions in Ta-\nble 7: (1) masked: only using the masked tokens\nas queries; (2) unmasked: randomly using 15% of\nthe unmasked tokens as queries; (3) all-aligned:\nfor each self-labeled aligned pair, randomly using\none of the two tokens as a query. Also, we in-\nclude the no-query baseline that does not use any\nqueries, which is identical to removing DWA. It\ncan be observed that using all the three query posi-\ntions improves the performance over the no-query\nbaseline. Moreover, using the masked positions as\nqueries achieves better results than the other two\npositions, demonstrating the effectiveness of the\nmasked query positions.\n6 Discussion\nIn this paper, we introduce denoising word align-\nment as a new cross-lingual pre-training task. By\nalternately self-labeling and predicting word align-\nments, our XLM-A LIGN model learns transferable\ncross-lingual representations. Experimental results\nshow that our method improves the cross-lingual\ntransferability on a wide range of tasks, particularly\non the token-level tasks such as question answering\nand structured prediction.\nDespite the effectiveness for learning cross-\nlingual transferable representations, our method\nalso has the limitation that requires a cold-start\npre-training to prevent the model from producing\nlow-quality alignment labels. In our experiments,\nwe also try to pretrain XLM-A LIGN from scratch,\ni.e., without cold-start pre-training. However, the\nDW A task does not work very well due to the low-\nquality of self-labeled alignments. Thus, we recom-\nmend continue-training XLM-A LIGN on the basis\nof other pretrained cross-lingual language models.\nFor future work, we would like to research on re-\nmoving this restriction so that the model can learn\nword alignments from scratch.\n3426\n7 Ethical Considerations\nDespite the current advances in NLP, most NLP re-\nsearch works and applications are English-centric,\nmaking none-English users hard to access to NLP-\nrelated services. Our method aims to pretrain\ncross-lingual language models that transfer super-\nvision signals from high-resource languages to low-\nresource languages, which makes the NLP services\nand applications more accessible for low-resource-\nlanguage speakers. Furthermore, our method can\nbuild multilingual models that serve on different\nlanguages at the same time, reducing the computa-\ntional resources for building multilingual models\nseparately for each language.\nAcknowledgements\nHeyan Huang is the corresponding author. The\nwork is supported by National Key R&D Plan\n(No. 2018YFB1005100), National Natural Science\nFoundation of China (No. 61751201, 61602197\nand 61772076), Natural Science Fund of Beijing\n(No. Z181100008918002), and the funds of Bei-\njing Advanced Innovation Center for Language Re-\nsources (No. TYZ19005).\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–\n311.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In International Conference on Learning Rep-\nresentations.\nZewen Chi, Li Dong, Shuming Ma, Shaohan Huang\nXian-Ling Mao, Heyan Huang, and Furu Wei.\n2021a. mt6: Multilingual pretrained text-to-text\ntransformer with translation pairs. arXiv preprint\narXiv:2104.08692.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-\nsham Singhal, Wenhui Wang, Xia Song, Xian-Ling\nMao, Heyan Huang, and Ming Zhou. 2021b. In-\nfoXLM: An information-theoretic framework for\ncross-lingual language model pre-training. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3576–3588, Online. Association for Computational\nLinguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057–7067. Curran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nChris Dyer, Victor Chahuneau, and Noah A Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of ibm model 2. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–648.\nHamidreza Ghader and Christof Monz. 2017. What\ndoes attention in neural machine translation pay at-\ntention to? In Proceedings of the Eighth Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 30–39,\nTaipei, Taiwan. Asian Federation of Natural Lan-\nguage Processing.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant, and Graham Neubig. 2020a. Explicit align-\nment objectives for multilingual bidirectional en-\ncoders. arXiv preprint arXiv:2010.07972.\n3427\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020b. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. arXiv preprint arXiv:2003.11080.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing, pages 2485–2494, Hong Kong, China.\nAssociation for Computational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, Franc ¸ois Yvon,\nand Hinrich Sch ¨utze. 2020. SimAlign: High qual-\nity word alignments without parallel training data us-\ning static and contextualized embeddings. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1627–1643, Online. As-\nsociation for Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study. In International Con-\nference on Learning Representations.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, San\nDiego, CA.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation, Miyazaki, Japan. European Language\nResources Association.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1293–1303.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. arXiv\npreprint arXiv:2001.08210.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi,\nSongfang Huang, Fei Huang, and Luo Si. 2020.\nVeco: Variable encoder-decoder pre-training for\ncross-lingual understanding and generation. arXiv\npreprint arXiv:2010.16046.\nCos ¸kun Mermer and Murat Sarac ¸lar. 2011. Bayesian\nword alignment for statistical machine translation.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 182–187.\nMasaaki Nagata, Katsuki Chousa, and Masaaki\nNishino. 2020. A supervised word alignment\nmethod based on cross-language span prediction us-\ning multilingual BERT. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 555–565, Online.\nAssociation for Computational Linguistics.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational linguistics, 29(1):19–51.\nRobert ¨Ostling and J ¨org Tiedemann. 2016. Efﬁcient\nword alignment with markov chain monte carlo.\nThe Prague Bulletin of Mathematical Linguistics ,\n106(1):125–146.\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie-\nm: Enhanced multilingual representation by align-\ning cross-lingual semantics with monolingual cor-\npora. arXiv preprint arXiv:2012.15674.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nGabriel Peyr ´e, Marco Cuturi, et al. 2019. Computa-\ntional optimal transport: With applications to data\nscience. Foundations and Trends® in Machine\nLearning, 11(5-6):355–607.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm ´an. 2019. Wiki-\nMatrix: Mining 135M parallel sentences in 1620\nlanguage pairs from wikipedia. arXiv preprint\narXiv:1907.05791.\n3428\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation, pages 2214–2218, Istanbul, Turkey. Eu-\nropean Language Resources Association.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998–6008. Curran As-\nsociates, Inc.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural\nInformation Processing Systems , volume 28, pages\n2692–2700.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzman, Ar-\nmand Joulin, and Edouard Grave. 2019. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. arXiv preprint arXiv:1911.00359.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, pages 833–844, Hong Kong,\nChina. Association for Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Do explicit align-\nments robustly improve multilingual encoders? In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4471–4482, Online. Association for Computa-\ntional Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi\nWu, Zhoujun Li, and Ming Zhou. 2020. Alternating\nlanguage modeling for cross-lingual pre-training. In\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual ad-\nversarial dataset for paraphrase identiﬁcation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3687–\n3692, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams,\nand et al. 2019. Universal dependencies 2.5.\nLINDAT/CLARIAH-CZ digital library at the Insti-\ntute of Formal and Applied Linguistics (´UFAL), Fac-\nulty of Mathematics and Physics, Charles Univer-\nsity.\nWei Zhao, Steffen Eger, Johannes Bjerva, and Is-\nabelle Augenstein. 2020. Inducing language-\nagnostic multilingual representations. arXiv\npreprint arXiv:2008.09112.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel corpus\nv1. 0. In LREC, pages 3530–3534.\nA Pre-Training Data\nWe use raw sentences from the Wikipedia dump\nand CCNet4 as monolingual corpora. The CCNet\ncorpus we use is reconstructed following (Conneau\net al., 2020) to reproduce the CC-100 corpus. The\nresulting corpus contains 94 languages. Table 8\nand Table 9 report the language codes and data\nsize of CCNet and Wikipedia dump. Notice that\nseveral languages share the same ISO language\ncodes, e.g., zh represents both Simpliﬁed Chinese\nand Traditional Chinese. Besides, Table 10 shows\nthe statistics of our parallel corpora.\nCode Size (GB) Code Size (GB) Code Size (GB)\naf 0.2 hr 1.4 pa 0.8\nam 0.4 hu 9.5 pl 28.6\nar 16.1 hy 0.7 ps 0.4\nas 0.1 id 17.2 pt 39.4\naz 0.8 is 0.5 ro 11.0\nba 0.2 it 47.2 ru 253.3\nbe 0.5 ja 86.8 sa 0.2\nbg 7.0 ka 1.0 sd 0.2\nbn 5.5 kk 0.6 si 1.3\nca 3.0 km 0.2 sk 13.6\nckb 0.6 kn 0.3 sl 6.2\ncs 14.9 ko 40.0 sq 3.0\ncy 0.4 ky 0.5 sr 7.2\nda 6.9 la 0.3 sv 60.4\nde 99.0 lo 0.2 sw 0.3\nel 13.1 lt 2.3 ta 7.9\nen 731.6 lv 1.3 te 2.3\neo 0.5 mk 0.6 tg 0.7\nes 85.6 ml 1.3 th 33.0\net 1.4 mn 0.4 tl 1.2\neu 1.0 mr 0.5 tr 56.4\nfa 19.0 ms 0.7 tt 0.6\nﬁ 5.9 mt 0.2 ug 0.2\nfr 89.9 my 0.4 uk 13.4\nga 0.2 ne 0.6 ur 3.0\ngl 1.5 nl 25.9 uz 0.1\ngu 0.3 nn 0.4 vi 74.5\nhe 4.4 no 5.5 yi 0.3\nhi 5.0 or 0.3 zh 96.8\nTable 8: The statistics of CCNet used for pre-training.\nB Hyperparameters for Pre-Training\nAs shown in Table 11, we present the hyperpa-\nrameters for pre-training XLM-A LIGN . We use\nthe same vocabulary with XLM-R (Conneau et al.,\n2020).\n4https://github.com/facebookresearch/\ncc_net\n3429\nCode Size (GB) Code Size (GB) Code Size (GB)\naf 0.12 hr 0.28 pa 0.10\nam 0.01 hu 0.80 pl 1.55\nar 1.29 hy 0.60 ps 0.04\nas 0.04 id 0.52 pt 1.50\naz 0.24 is 0.05 ro 0.42\nba 0.13 it 2.70 ru 5.63\nbe 0.31 ja 2.65 sa 0.04\nbg 0.62 ka 0.37 sd 0.02\nbn 0.41 kk 0.29 si 0.09\nca 1.10 km 0.12 sk 0.21\nckb 0.00 kn 0.25 sl 0.21\ncs 0.81 ko 0.56 sq 0.11\ncy 0.06 ky 0.10 sr 0.74\nda 0.33 la 0.05 sv 1.70\nde 5.43 lo 0.01 sw 0.03\nel 0.73 lt 0.19 ta 0.46\nen 12.58 lv 0.12 te 0.45\neo 0.25 mk 0.34 tg 0.04\nes 3.38 ml 0.28 th 0.52\net 0.23 mn 0.05 tl 0.04\neu 0.24 mr 0.10 tr 0.43\nfa 0.66 ms 0.20 tt 0.09\nﬁ 0.68 mt 0.01 ug 0.03\nfr 4.00 my 0.15 uk 2.43\nga 0.03 ne 0.06 ur 0.13\ngl 0.27 nl 1.38 uz 0.06\ngu 0.09 nn 0.13 vi 0.76\nhe 1.11 no 0.54 yi 0.02\nhi 0.38 or 0.04 zh 1.08\nTable 9: The statistics of Wikipedia dump used for pre-\ntraining.\nISO Code Size (GB) ISO Code Size (GB)\nen-ar 5.88 en-ru 7.72\nen-bg 0.49 en-sw 0.06\nen-de 4.21 en-th 0.47\nen-el 2.28 en-tr 0.34\nen-es 7.09 en-ur 0.39\nen-fr 7.63 en-vi 0.86\nen-hi 0.62 en-zh 4.02\nTable 10: Parallel data used for pre-training.\nC Hyperparameters for Fine-Tuning\nIn Table 12, we present the hyperparameters for\nﬁne-tuning XLM-Rbase and XLM-A LIGN on the\nXTREME end tasks. For each task, the hyperpa-\nrameters are searched on the joint validation set of\nall languages.\nD Detailed Results on XTREME\nWe present the detailed results of XLM-A LIGN on\nXTREME in Table 13-19.\nHyperparameters Value\nLayers 12\nHidden size 768\nFFN inner hidden size 3,072\nAttention heads 12\nTraining steps 150K\nBatch size 2,048\nAdam ϵ 1e-6\nAdam β (0.9, 0.98)\nLearning rate 2e-4\nLearning rate schedule Linear\nWarmup steps 10,000\nGradient clipping 1.0\nWeight decay 0.01\nSelf-labeling layer 10\nEntropic regularization µ 1.0\nSinkhorn iterations 2\nAlignment ﬁltering iterations 2\nAlignment ﬁltering α 0.9\nTable 11: Hyperparameters used for pre-training XLM-\nALIGN .\n3430\nPOS NER XQuAD MLQA TyDiQA XNLI PAWS-X\nBatch size {8,16,32} 8 32 32 32 32 32\nLearning rate {1,2,3}e-5 {5,...,9}e-6 {2,3,4}e-5 {2,3,4}e-5 {2,3}e-5 {5,...,8}e-6 {1,2}e-5\nLR schedule Linear Linear Linear Linear Linear Linear Linear\nWarmup 10% 10% 10% 10% 10% 12,500 steps 10%\nWeight decay 0 0 0 0 0 0 0\nEpochs 10 10 4 {2,3,4} { 5,10,15,20} 10 10\nTable 12: Hyperparameters used for ﬁne-tuning XLM-Rbase and XLM-A LIGN on the XTREME end tasks.\nModel af ar bg de el en es et eu fa ﬁ fr he hi hu id it\nXLM-ALIGN 88.5 69.1 88.8 88.8 85.8 95.9 88.5 84.9 68.3 70.9 84.8 88.1 79.6 71.6 83.3 72.3 89.4\nModel ja kk ko mr nl pt ru ta te th tl tr ur vi yo zh Avg\nXLM-ALIGN 51.1 75.3 53.8 80.3 89.3 87.6 88.9 62.3 85.9 60.2 90.1 74.8 63.3 55.9 24.2 67.9 76.0\nTable 13: Results on part-of-speech tagging.\nModel ar he vi id jv ms tl eu ml ta te af nl en de el bn hi mr ur\nXLM-ALIGN 57.7 54.3 72.5 49.7 56.9 68.3 72.0 53.1 68.6 58.0 54.6 76.3 82.1 84.2 77.9 76.4 73.1 69.2 64.9 65.8\nModel fa fr it pt es bg ru ja ka ko th sw yo my zh kk tr et ﬁ hu Avg\nXLM-ALIGN 53.2 79.0 79.4 78.8 73.8 78.9 66.2 23.0 70.6 56.6 2.2 69.3 43.8 56.5 28.3 49.2 77.5 73.3 77.0 77.0 63.7\nTable 14: Results on WikiAnn named entity recognition.\nModel en es de el ru tr ar vi th zh hi Avg\nXLM-ALIGN 85.7 / 74.6 70.3 / 52.5 76.6 / 60.3 75.5 / 56.8 79.4 / 60.8 71.8 / 54.7 75.4 / 59.4 72.1 / 61.0 70.9 / 55.5 76.7 / 56.9 67.3 / 56.8 74.7 / 59.0\nTable 15: Results on XQuAD question answering.\nModel en es de ar hi vi zh Avg\nXLM-ALIGN 81.5 / 68.3 70.3 / 52.2 64.5 / 49.8 60.7 / 41.2 65.2 / 47.5 69.8 / 48.9 64.4 / 40.4 68.1 / 49.8\nTable 16: Results on MLQA question answering.\nModel en ar bn ﬁ id ko ru sw te Avg\nXLM-ALIGN 69.4 / 56.2 68.7 / 49.4 56.0 / 38.9 64.2 / 47.2 73.9 / 57.9 53.0 / 40.4 62.3 / 38.0 60.1 / 42.8 51.0 / 31.9 62.1 / 44.8\nTable 17: Results on TyDiQA question answering.\nModel en fr es de el bg ru tr ar vi th zh hi sw ur Avg\nXLM-ALIGN 86.7 80.6 81.0 78.8 77.4 78.8 77.4 75.2 73.9 76.9 73.8 77.0 71.9 67.1 66.6 76.2\nTable 18: Results on XNLI natural language inference.\nModel en fr de es ja ko zh Avg\nXLM-A LIGN 95.1 89.3 90.5 90.7 79.1 79.5 83.2 86.8\nTable 19: Results on PAWS-X cross-lingual paraphrase adversaries.",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.6979522109031677
    },
    {
      "name": "Computer science",
      "score": 0.6927255988121033
    },
    {
      "name": "Artificial intelligence",
      "score": 0.569407045841217
    },
    {
      "name": "Word (group theory)",
      "score": 0.5298713445663452
    },
    {
      "name": "Linguistics",
      "score": 0.5117899775505066
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.48206406831741333
    },
    {
      "name": "Computational linguistics",
      "score": 0.4336888790130615
    },
    {
      "name": "Philosophy",
      "score": 0.11027634143829346
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}