{
  "title": "Probing for Incremental Parse States in Autoregressive Language Models",
  "url": "https://openalex.org/W4385573516",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2941804323",
      "name": "Tiwalayo Eisape",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A5063044780",
      "name": "Vineet Gangireddy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982032445",
      "name": "Roger Lévy",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2124530043",
      "name": "Yoon Kim",
      "affiliations": [
        "Harvard University Press"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3103536442",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W3162404768",
    "https://openalex.org/W3114409145",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W3174281149",
    "https://openalex.org/W4237694247",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W3173798466",
    "https://openalex.org/W3202070718",
    "https://openalex.org/W4212828284",
    "https://openalex.org/W2506931122",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2962733492",
    "https://openalex.org/W3176899693",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W4255690937",
    "https://openalex.org/W3199610709",
    "https://openalex.org/W3042795397",
    "https://openalex.org/W2962941914",
    "https://openalex.org/W4385573279",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W3210923133",
    "https://openalex.org/W2030904529",
    "https://openalex.org/W3033254023",
    "https://openalex.org/W3176175390",
    "https://openalex.org/W2918996109",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4287854901",
    "https://openalex.org/W3176357828",
    "https://openalex.org/W2962935430",
    "https://openalex.org/W3133995764"
  ],
  "abstract": "Next-word predictions from autoregressive neural language models show remarkable sensitivity to syntax. This work evaluates the extent to which this behavior arises as a result of a learned ability to maintain implicit representations of incremental syntactic structures. We extend work in syntactic probing to the incremental setting and present several probes for extracting incomplete syntactic structure (operationalized through parse states from a stack-based parser) from autoregressive language models. We find that our probes can be used to predict model preferences on ambiguous sentence prefixes and causally intervene on model representations and steer model behavior. This suggests implicit incremental syntactic inferences underlie next-word predictions in autoregressive neural language models.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2801–2813\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nProbing for Incremental Parse States in Autoregressive Language Models\nTiwalayo Eisape1 Vineet Gangireddy2 Roger P. Levy1 Yoon Kim1\nMIT1, Harvard University2\n{eisape,rplevy,yoonkim}@mit.edu\nvineetgangireddy@college.harvard.edu\nAbstract\nNext-word predictions from autoregressive neu-\nral language models show remarkable sensi-\ntivity to syntax. This work evaluates the ex-\ntent to which this behavior arises as a result\nof a learned ability to maintain implicit repre-\nsentations of incremental syntactic structures.\nWe extend work in syntactic probing to the in-\ncremental setting and present several probes\nfor extracting incomplete syntactic structure\n(operationalized through parse states from a\nstack-based parser) from autoregressive lan-\nguage models. We find that our probes can\nbe used to predict model preferences on am-\nbiguous sentence prefixes and causally inter-\nvene on model representations and steer model\nbehavior. This suggests implicit incremental\nsyntactic inferences underlie next-word predic-\ntions in autoregressive neural language models.\n1 Introduction\nThe behavior of large-scale autoregressive neural\nlanguage models (ALMs) appears to demonstrate\nimpressive command of syntax (Wilcox et al., 2019;\nHu et al., 2020; Futrell et al., 2019; Wilcox et al.,\n2021; Arehalli and Linzen, 2020; Warstadt and\nBowman, 2020). To what extent can we attribute\nthis behavior to a model’s maintaining and updating\nrepresentations of incremental syntactic structures?\nInterpretability work on bidirectional masked\nlanguage models suggests that neural models of\nlanguage may learn to encode syntactic structure\nthrough the geometry of their word embedding\nspace. For example, Hewitt and Manning (2019)\ndemonstrate that the dependency parse tree of a\nsentence can be decoded by finding the minimum\nspanning tree on pairwise syntactic distances re-\ngressed from linearly transformed contextualized\nword embeddings.\nUnlike the models considered in Hewitt and\nManning (2019), ALMs are unidirectional, and the\nCode and materials: https://github.com/\neisape/incremental_parse_probe\ndecision problem of incrementally deriving global\nsyntactic structures has several nuances not present\nin the bidirectional setting. Sentence prefixes (e.g.\n“I watched her duck ...”) can be ambiguous in their\nintended meaning in ways that are disambiguated\nby their suffixes (e.g. “quack” vs. “under the ta-\nble”). Thus, incremental processors must maintain\na belief state of parses that can be flexibly updated\non the basis of future input. Insofar as language\nin the real world (e.g., speech, text, sign) is pro-\ncessed sequentially, incremental disambiguation of\nstructure and meaning is a task humans solve in ev-\neryday cognition, seemingly effortlessly (Jurafsky,\n1996; Hale, 2001; Levy, 2008).\nALMs have been shown to recapitulate crucial\nfeatures of human sentence processing (Futrell\net al., 2019) and their representations moreover\nhave been shown to align with language processing\nin the brain better than their bidirectional counter-\nparts (Schrimpf et al., 2021; Caucheteux and King,\n2022). We hypothesize that ALMs learn and main-\ntain correlates of incremental syntactic structures\nwhich play an important role in mediating model\nbehavior. We investigate this hypothesis through\nthe lens of counterfactual probing, i.e., by learning\nclassifiers over hidden states of pretrained ALMs\nto predict linguistic properties, and then using the\nprobes to intervene on model representations.\nWe present a suite of probing architectures\nfor decoding belief states of incremental struc-\ntures from pretrained Transformer ALMs given\nthe model’s hidden state, each of which embodies\na different hypothesis for how incremental parse\nstates might be encoded. We validate (and adju-\ndicate between) our probes by using them to pre-\ndict and control model behavior. Our results sug-\ngest that ALMs, through pretraining at scale, learn\nstack-like representations of syntax and use them\nin interpretable and controllable ways.\n2801\n1\nFigure 1: (Left) Results of applying the structural probe of Hewitt and Manning (2019) to GPT2. Displayed are the\nperformances of the distance as quantified in both unlabeled undirected attachment score (UUAS) and Spearman\ncorrelation (Dspr.) across layers. Layer 0 denotes uncontextualized embeddings. The best performing BERT and\nELMo layers as reported in Hewitt and Manning (2019) are shown as horizontal lines. (Right) Dependency graphs\nelicited from GPT2 small for the sentences in Figure 2 via Hewitt and Manning (2019)’s structural probe before\n(left) and after (right) disambiguation, and then decoding with minimum spanning tree. Visualization is done\nby projecting the predicted pairwise distances into two dimensions via PCA. In the zero-complement condition\n(bottom), the “went” embedding is emitted between an existent dependency resulting in a new parse, effectively\n‘deleting’ the arc in red.\n2 Motivating Study\nPrior work has developed methods for extracting\nsyntactic parses from neural language models given\nglobal context. In particular, Hewitt and Manning\n(2019) learn a distance function parameterized by\na linear transformation (B) of the word embedding\nspace of BERT,\nδB\n(\nhi,hj\n)2 =\n(\nB\n(\nhi −hj\n))T (\nB\n(\nhi −hj\n))\n,\nsuch that this distance approximates the distance of\nwords wi,wj in a dependency tree. After learning\nBvia regression, i.e.,\nmin\nB\n∑\nℓ\n1\n|sℓ|2\n∑\ni,j\n⏐⏐⏐⏐δTℓ\n(\nwℓ\ni,wℓ\nj\n)\n−δB\n(\nhℓ\ni,hℓ\nj\n)2\n⏐⏐⏐⏐,\n(here ℓ indexes the sentences in the training set,\n|sℓ|is the sentence length, and δTℓ(wi,wj) is the\ntree distance between wi and wj), the authors find\nthat the minimum spanning tree obtained from\nδB(·,·) well-approximates the gold dependency\ntree in many cases, indicating that the geometry of\ncontextualized word embeddings captures aspects\nof syntax.\nIn an initial study, we run Hewitt and Manning\n(2019)’s structural probe on the hidden states of\nGPT2 (Radford et al., 2019), a unidirectional ALM,\nand find that GPT2’s hidden states recover gold tree\nstructures almost as well as similarly-sized bidirec-\ntional models (Figure 1). For example GPT2-XL\nFigure 2: NP/Z ambiguous sentences and their parses.\nmatches BERT-base in correlation with syntactic\ndistances (Dspr.; .85 for both models). We further\nobserve that this probe can recover the correct struc-\ntures even on sentences with ambiguous prefixes,\nsuch as the ‘NP/Z’ ambiguity shown in Figure 2.\nIn this ambiguity, the shared prefix (“even though\nthe band left the party”) is consistent with at least\ntwo interpretations: one in which the underlined\nspan (“the party”) is a direct object noun phrase\n(NP) of the verb “left”, and one in which the under-\nlined content is the subject of a new noun phrase\nand “left” has a zero (Z) complement. This is a\nwell-studied phenomenon in psycholinguistics, and\nhumans make rapid and accurate inferences in this\nsetting (Bever, 1970).\nIt is not at all obvious how a strictly left-to-right\nmodel, whose representations of the words in con-\ntext are static (i.e., not affected by future words),\ncan maintain a prefix representation that are dy-\nnamic enough to encode global syntactic distances\nwith high accuracy. We visualize the evolution of\npairwise syntactic distances for the two sentences\nin Figure 2 along with the minimum spanning trees\n2802\nin Figure 1 (right). We observe that parses probed\nfrom sentence prefixes up until the disambiguators\nare consistent with the NP-complement parse (i.e.,\nwhere “party” is attached to “left”), which is in line\nwith previous work that analyzes GPT2’s behavior\non the NP/Z ambiguity (Futrell et al., 2019). Af-\nter observing “went,” GPT2 emits a representation\nwhose distance is interleaved between “left” and\n“party” (i.e., δB(hparty,hwent) < δB(hparty,hleft)),\nthus altering the minimum spanning tree to be con-\nsistent with the zero-complement parse.\nThis analysis provides an initial hypothesis for\nhow incremental parsing arises in ALMs from\nthe perspective of syntactic distance—ALMs emit\nword representations that maintain syntactic uncer-\ntainty that can be exploited by later emissions, ef-\nfectively editing inferred dependencies and bypass-\ning the need to re-position past emissions. How-\never, this analysis also reveals several limitations\nthat preclude it as a viable model of incremen-\ntal parsing in ALMs: (1) this method assumes a\nspanning-tree and therefore cannot represent parses\nwith open nodes, and (2) GPT2’s behavior is consis-\ntent with a probabilistic parallel parser as it seems\nto entertain both possible parses until seeing the\ndisambiguating words, but such a pure distance-\nbased probe is not inherently probabilistic.1 In the\nfollowing section, we develop several parameter-\nizations of a probabilistic incremental parser that\ncan represent incomplete syntactic structures.\n3 An Incremental-Parse Probe for\nAutoregressive Language Models\nWe argue that the stack representation of shift-\nreduce parsers provides a natural way to represent\nincomplete tree structures. In this paper, we work\nwith the generative arc-standard dependency for-\nmalism (Nivre, 2004), which maintains a stack of\ngenerated subtrees S = [s1,s2,s3,...] (where the\nroot of subtree si is a word), and makes one of the\nfollowing actions at each time step:\nLEFT-ARC: pop the top two nodes, create a\nnew subtree by adding an arc s1 →s2, and\npush the new subtree onto the stack,\nRIGHT-ARC: pop the top two nodes, create\na new subtree by adding an arc s2 →s1, and\npush the new subtree onto the stack,\nGEN: generate the next token.\n1Though it is possible to derive a probabilistic parser by\ninterpreting the summed distances as the energy of a globally\nnormalized model.\nGeneration starts with only ROOT (an embedding\nlearned independently for each probe) on the stack,\nS = [ROOT], and terminates when only ROOT is\nleft on the stack and the period token has been gen-\nerated. Note that an action sequence a fully spec-\nifies a projective dependency structure. 2 Hence,\nletting a≤n(t) be the sequence of actions up to (and\nincluding) the generation of wt, we use a≤n(t) to\nrepresent the incremental (i.e., incomplete) tree\nstructure state after emittingwt.3 We design several\nprobes that place distributions over action trajecto-\nries - P\n(\na≤n(wt) |w<t\n)\ngiven ALM embeddings\nh<t.\n3.1 Probe architectures\nWe explore three probing architectures for param-\neterizing the action probabilities, each of which\nembodies a different hypothesis for how incremen-\ntal parse states might be represented in ALMs.\nGeometric Action Probe (GAP).Our first archi-\ntecture leverages the geometry of the embeddings\nand links the syntactic distances and depths derived\nfrom Hewitt and Manning (2019) to action proba-\nbilities (Figure 3, left). The action probabilities are\nparameterized as below (where for brevity we omit\nthe conditioning variables w<t and the previously\ngenerated actions):\nP(GEN) =σ\n(δB(hs1 ,hs2 ) −τ\nβ\n)\n,\nP(LEFT-ARC) = (1−P(GEN)) ×\nσ\n(∥hs1 ∥B −∥hs2 ∥B\nβ\n)\n,\nP(RIGHT-ARC) = (1−P(GEN)) ×\nσ\n(∥hs2 ∥B −∥hs1 ∥B\nβ\n)\n,\nwhere τ is a threshold parameter, β is a temper-\nature term, σ(·) is the sigmoid to turn distances\ninto probabilities, and hsi is the ALM’s representa-\ntion for the root word of subtree si. Note that the\nprobability of GEN increases as the predicted syn-\ntactic distance between (the root words of) s1 and\ns2 increases, which intuitively captures the notion\nthat there is less likely to be a link between s1 and\ns2 if their predicted distance is large. The action\n2Because ArcStandard can only recognize projective de-\npendency trees, we exclude non-projective structures from\ntraining and evaluation in the sections to follow.\n3We omit the generation of the next word (from GPT2’s\nnext word distribution) after predicting GEN, as we assume\nthat sentences are given for probing purposes.\n2803\n1Threshold\nvet\nLEFT -ARC\nthethe\n bit \ndog\nPRIGHT -ARC\nP =1=1\nPGEN\n≈1\ndog bit the vetthe\nGPT2\nprobe\nprobe\nLEFT -ARC\nRIGHT -ARC\nh\nd\nhbit\nhth\nhve\nhth\nGPT2\nprobeattention\nRIGHT -\nrecurrence\nτ\ndog bit the vetthe\nGPT2\nprobe\nprobe\nLEFT -ARC\nRIGHT -ARCattention\nGeometric Action Probe\nMLP Action Probe No-Stack Action Probe\nFigure 3: Schematics of each of the incremental parse probes parsing the sentence “the dog bit the vet”. (Top)\nIncremental parse states (i.e., stacks of subtrees rooted by head words) are shown between the action transitions\n(LEFT-ARC, RIGHT-ARC) that connect the parse states. (Dotted) Visualizations of how each probe decides on\nthe next actions. (Left) Geometric action probe (GAP) links action probabilities with its learned distance function\nδB. The distance from the top node on the stack (“vet” and “the” in the LEFT-ARC case) affects whether an arc is\nchosen. If the distance between the top two stack nodes is above a threshold τ, then then we GEN with increasing\nprobability. Relative depth (blue = shallow relative to “vet”, green = deeper) predicts arc directionality probability.\n(Center) MLP Action probe (MAP) directly classifies each action by running the contextualized word embeddings\nfor the top two nodes on the stack through an MLP. (Right) No-Stack Action Probe (NAP) classifies by attending\nover the entire prefix without maintaining an explicit stack.\nprobabilities for LEFT-ARC and RIGHT-ARC are\nbased on comparing the predicted depths, which\nwe define to be the norm of the word embedding\nafter a linear transformation as in Hewitt and Man-\nning (2019). For example, the probability of the arc\ns1 →s2 increases as s2 is predicted to be further\naway from the root than s1 (i.e., relatively deeper\nnodes are the dependants of shallower nodes; see\nFigure 3). We initialize B by pretraining on the\ndistance and depth regression task from Hewitt and\nManning (2019). Because we also use a linear\nprojection our probe can also be interpreted as a\nlearned distance function on word representations.\nMLP Action Probe (MAP).The geometric ac-\ntion probe makes strong assumptions about the\nunderlying geometry of the representation space,\ni.e., that syntactic distances and depths are well-\ncaptured by linear transformations of the ALM’s\nrepresentations and that these measures can more-\nover be monotonically transformed to action prob-\nabilities via a sigmoid link function. This next\nvariant relaxes this assumption and replaces the\ndistance and depth-based linking function in GAP\nwith a learned multilayer perceptron,\nP(a) ∝exp\n(\ne⊤\naMLP ([hs1 ,hs2 ]) +ba\n)\nwhere a∈{GEN,LEFT-ARC,RIGHT-ARC}. As\nin the geometric version, we still make use of an\nexplicit stack, but this variant allows for an arbi-\ntrary link function from features of the model’s\nrepresentations to action probabilities.\nNo-Stack Action Probe (NAP).Our final variant\nrelaxes assumptions about both the linking func-\ntion and the existence of an explicit stack. This\napproach, which is closely related to Qian et al.\n(2021), simply predicts the sequence of actions\nbetween two words wt and wt+1 using the action\nhistory and the hidden representations h<t,\nvj = Action-LSTM(vj−1,aj),\n˜h = Attention(h<t,vj),\nP(a) ∝exp\n(\ne⊤\naMLP\n(\n[˜h,vj]\n)\n+ ba\n)\n.\nHere the action LSTM’s hidden state vj is used to\nattend over the previous word representations h<t\nto obtain a context vector ˜h, which is combined\nwith the hidden state to produce a distribution over\nthe following action.\nWe train each of these architectures 4 on the\nground truth action trajectories in the Penn Tree-\nbank (PTB) (Marcus et al., 1993) and use gpt2\nand gpt2-xl model checkpoints from Hugging-\nFace (Wolf et al., 2019) as our ALM text encoders.\nAll hyperparameters are reported in Appendix A.1.\n4 Experiments\nWe evaluate each of our probes on their parsing\nperformance over the PTB test split as well as their\nability to predict and control model behavior.\n4Due to resource constraints we only train NAP on GPT-2\nsmall.\n2804\nAlgorithm 1:Probe-Based Word-Synchronous Beam Search\nInput: w : Words in sentence.\nPprobe: Incremental parse probe.\nkaction: Number of action n-grams to consider between words.\nkword: Max number of parses to consider for each word.\nkout: Number of parses to output.\nOutput: Bout: beam of terminal action sequences\n1 B, Bout ←[[ROOT]], [ ]\n2 while |Bout|≤ kout do\n3 for n ∈[1, ...,|w|] do\n4 Bword ←[ ]\n5 h<n ←GPT2 (w<n) // get hidden states from GPT2\n6 while B not empty do\n7 a ←pop (B)\n8 {a1, a2, ...,akaction }← BeamSearch (Pprobe, a, h<n, kaction) // search until GEN or termination\n9 for anew ∈{a1, a2, ...,akaction }do\n10 if n = |w|: push (Bout, a.append (anew))\n11 else : push\n(\nBword, a.append (anew)\n)\n12 B ←top-k\n(\nBword, kword\n)\n// synchronize beam at word-level\n13 return Bout // return beam with complete parse trees\nAlgorithm 1: Probe-based word-synchronous beam search. We use beam search (line 8) as a subroutine to extend\naction sequences in the beam with action n-gram continuations. After the generation of each word the beam is\npruned to the top-kword best parses (line 12).\n4.1 Parsing Performance\nFor parsing performance, we evaluate with ac-\ntion perplexity (PPL) and unlabeled attachment\nscore on the PTB test set. For the latter, we ex-\ntend the word-synchronous beam search algorithm\nfor decoding from generative neural parsers (Stern\net al., 2017) to the incremental probe setting. This\nprobe-based word-synchronous beam search algo-\nrithm uses an incremental parse probe to decode\naction sequences between word emissions with\nbeam search. Because the word emissions from the\nALMs we consider are contextualized, this quan-\ntity implicitly conditions on the previous action\nsequence including all of the words generated in\nthe prefix so far 5 thus allowing us to predict the\nlikelihood:\n∏\na∈π\\w\nP(a) (where π\\w are the actions\nin any parse πexcluding the likelihoods of words\nconditioned on actions).\nThis algorithm is an interesting testbed for\nALMs because it relies only on the syntactic dis-\nambiguation implicit in the ALMs hidden states\nto update its beam of parses (compared to models\nsuch as RNNGs (Dyer et al., 2016) whose next-\nword distribution is explicitly conditioned on the\nstack states). This decoding scheme is shown in\nAlgorithm 1 where kaction is the the number of ac-\n5That is, even though our incremental-parse probes only\npredict GEN, action likelihoods in our probes are conditioned\non GEN(w) for words in the prefix: w ∈w≤t.\ntion n-grams to consider between model emissions,\nand kword is number of parses to consider at word\nboundaries. During decoding we only consider the\nvalid set of actions for a given parse state given the\nrules of ArcStandard.\nThe results of these experiments are shown in\nFigure 4, where we set kaction = kword = 10.\nMAP outperforms GAP as expected (since it makes\nweaker assumptions). Interestingly, while NAP out-\nperforms the other probes in terms of PPL, when\nevaluated as an incremental parser, it has worse\nUAS than MAP. This is significant because while\nMAP makes explicit use of a stack (i.e., the em-\nbeddings it uses to make decisions are entirely de-\ntermined by the stack), NAP does not and instead\nuses the attention distribution over the prefix at\neach time step to implicitly encode the stack. We\ntake these results as an important data point in adju-\ndicating between representational hypotheses of in-\ncremental syntax in ALMs. Under the assumption\nthat high-performance ALMs are adept incremen-\ntal parsers—as has been previously demonstrated\n(Marvin and Linzen, 2018; Hu et al., 2020)—the\nhigh performance of MAP suggests our best cur-\nrent model of syntactic parsing in ALMs may be\nstack-based but not necessarily geometry-based.\nProbe performance to an extent implies the ex-\nistence of implicit representaitons of syntax with\nALMs. However, it does not necessarily imply that\nthese structures mediate model behavior. In the\n2805\nFigure 4: Action perplexity (left) and UAS (right) for each of the incremental-parse probes.\nfollowing section, we conduct experiments to see\nwhether we can predict and control model behavior\nwith our probes.\n4.2 Probing Incremental Disambiguation\nWe evaluate our probes to predict model behavior.\nFor this purpose, we extend the dataset of NP/Z\nambiguous sentences of Futrell et al. (2019) by\naugmenting it to include continuations that disam-\nbiguate toward the Zero- and NP-complement inter-\npretations (Figure 2 shows an example data point).\nWe also include continuations that are consistent\nwith both parses (“Even though the band left the\nparty that was raging [...]”) or neither parse (we\nuse the period token, which is ungrammatical in ei-\nther case, i.e., “Even though the band left the party\n.”) for a more complete comparison of the effects\nof disambiguation. Concretely, the likelihoods of\n‘Both’ and ‘Neither’ continuations are expected to\nincrease and stay the same, respectively, regardless\nof the direction of disambiguation.\nPredicting Behavior. First, we replicate Futrell\net al. (2019)’s result showing that ALMs are sen-\nsitive to verb transitivity and use it to guide next-\nword predictions, resulting in incremental parser-\nlike behavior. Specifically, we show that changing\nthe verb in NP/Z-ambiguous prefixes from intran-\nsitive (unambiguously Z-complement favoring) to\ntransitive (ambiguous), e.g.,:\n(1) Even though the band left [ambiguous],\nEven though the band performed [unam-\nbiguous],\ncauses ALMs to prefer the continuation consis-\ntent with the zero-complement parse. Following\nFutrell et al. (2019) we use the ALM’s surprisal\nover the words in the continuation 6 to quantify\n6Note that as the sentence is not fully disambiguated until\nthe period is reached. We include the period token in our\nsurprisal estimates.\nthe ALM’s expectations, i.e., S(“went on.”) =\n−log Pmodel(“went on.”|prefix). In Figure 5 we\nshow the change (difference) in surprisal from the\nunambiguous to the ambiguous case and observe\nthat GPT2 and GPT2-XL both find the Z continu-\nation more surprising in the presence of a transi-\ntive verb (Figure 5, right). Somewhat surprisingly,\nthe surprisal of the NP continuation is unchanged\nby verb transitivity. As expected, the surprisal of\nthe ‘Both’ and ‘Neither’ conditions are largely un-\nchanged between conditions.\nWe find that our probes can predict model be-\nhavior in this setting. We target the first action that\ndifferentiates the NP and Z parses, which corre-\nsponds to the decision to place and arc from the\nverb to the head of the noun phrase (Figure 2) and\nplot the difference in surprisal of this decision from\nthe intransitive condition to the transitive condition\n(Figure 5, left). This probe illustrates a preference\nfor the Z-complement parse in the presence of an\nintransitive verb which provides representational\nevidence for incremental parse disambiguation on\nthe part of the model.\nIn a further study, we present each of the probes\nwith the two NP/Z parses in two conditions: 1)\nwhere the sentence suffix was consistent with the\nparse being probed, and 2) where the sentence suf-\nfix was consistent with the other parse. This can be\nseen as a probabilistic version of Figure 1 (right).\nWe plot the difference in negative log-likelihoods\nfrom condition 2 to condition 1 in Figure 6. As\nexpected, we find each of the probes assign higher\nnegative log likelihood (i.e., lower likelihood) to\nparses when matched with incongruent suffixes.\nThis effect is especially pronounced for the MAP\nprobe.\nEditing Parse States. Finally, we use counter-\nfactual analysis (Tucker et al., 2021, 2022) at the\nlevel of ALM hidden states to probe the extent\nto which our probes capture essential information\n2806\n1\nFigure 5: Disambiguation as evaluated by MAP and\nmodel behavior. (Right) Difference in GPT2 surprisal\nover the continuations in our corpus from the unam-\nbiguous condition to the ambiguous condition (higher\nmeans the model is more surprised in the unambiguous\ncondition). (Left) Difference in (MAP) probe surprisal\nover the disambiguating parse action. Error bars show\n95% confidence intervals across our corpus.\nabout incremental parses that mediate model behav-\nior. Our approach generates counterfactual ALM\nhidden states by propagating the loss of a par-\nticular parse state ( a) from our probe output to\nmodel embeddings. Specifically, we iteratively\nupdate GPT2 hidden states with the following:\nˆh = h + ϵ∇hPprobe (a |h), for a small step size ϵ\n(see Appendix 7 for further details). Crucially, we\nperturb towards incremental, incomplete structures\nbefore a word is generated, rather than complete\nstructures.\nWe conjecture that if our probes pick up on es-\nsential syntactic information in model embeddings,\nthey should be able to generate model embed-\ndings that produce the expected effect on model be-\nhaviour (e.g., perturbing toward the Z-complement\nparse should make the Z-complement continuation\nmore likely). We generate two sets of counterfac-\ntual embeddings (one for each syntactic interpreta-\ntion; Figure 2) for each sentence in our modified\nNP/Z data set. We do this for each layer of the\nmodels considered for each of our probes and eval-\nuate the effect of these treatments by generating\nsurprisal estimates from each of the counterfactual\nembeddings.\nThe results of our analysis are shown in Figure 7.\nWe find that counterfactuals generated with MAP\nFigure 6: Effect of sentence continuations on parse\nlikelihood. For each of the incremental parse probes,\nwe produce the negative log likelihood for each parse\ngiven 1) congruent suffix words and 2) incongruent suf-\nfix words (larger is better) and show each probe assigns\nhigher negative log-likelihood to parses when matched\nwith an incongruent suffix (lines above origin in all\ncases).\nproduce the predicted effect on both GPT2 and\nGPT2-XL for the Z- and NP-congruent continua-\ntions (but not for the ‘Both’ or ‘Neither’ conditions;\nAppendix A.2). This effect is significant across sev-\neral layers of both models and is most significant\nat layer 2 of GPT2 small, where Z-complement-\ncongruent continuations increase in likelihood by\n∼8 fold (3 nats) on average after model interven-\ntion. We find that counterfactuals generated with\nthe other probes do not mediate next-word predic-\ntions in the expected way (Appendix A.2).\n5 Related Work\nOur work is related to the growing literature quan-\ntifying the syntactic sensitivity of neural models of\nlanguage at behavioral (Linzen and Baroni, 2021;\nAina and Linzen, 2021; Lakretz et al., 2021) and\nrepresentational (Pimentel et al., 2020; Hewitt and\n2807\n1\nFigure 7: Results of counterfactual syntactic perturbations. The y-axis shows the surprisal difference over the\nsentence continuations from unperturbed GPT2 to GPT2 perturbed with counterfactual embeddings. We see\ndifferences in the expected directions for the Z-complement and NP-complement (i.e., red above blue in the plots\non the left and blue above red in the rightward plots) completions for MAP across model. Error bars show 95%\nconfidence intervals across our corpus.\nManning, 2019; Manning et al., 2020; Müller-\nEberstein et al., 2022) levels. Of particular note\namong these studies is the dependency-arc labeling\ntask introduced in Tenney et al. (2019) which bears\nresemblance to our MAP architecture. Our work\nextends this literature by developing several new\ntypes of syntactic probes in the incremental setting.\nSimilar to our work but in semantics, Li et al.\n(2021) use probes to interpret text-encoders as\nmaintaining an evolving semantic information state\nwhile interacting with a text-based game. Relatedly,\nour work is among others that attempt to control\nneural models of language for counterfactual anal-\nysis or controllable AI more generally (De Cao\net al., 2021; Ravfogel et al., 2021; Elazar et al.,\n2020; Meng et al., 2022; Dathathri et al., 2019).\nLastly, we expect our work to be of consequence\nto computational psycholinguistics, where neural\nlanguage models and incremental parsers have been\nhistorically prominent as candidate cognitive mod-\nels (Hale et al., 2018; Hale, 2001; Wilcox et al.,\n2020; Levy, 2008; Jurafsky, 1996; Eisape et al.,\n2020). This work draws representational parallels\nbetween these two models.\n6 Conclusion\nMotivated by recent work showing human-like in-\ncremental parsing behaviour in ALMs, this work\nextends structural probing to the incremental set-\nting. We find that autoregressive language models\ncan perform on par with bidirectional models in\nterms of global parsing metrics despite the fact that\nan ALM’s representation cannot condition on fu-\nture words. We present several hypotheses of the\nrepresentation of incremental structure in ALMs,\ninstantiate them in probe architectures, and eval-\nuate their explanatory power across a wide range\nof experiment types: parsing performance, predict-\ning model behavior, and counterfactual analyses.\nCumulatively, MAP performs the best, which is\nsignificant because this architecture incorporates\nstrong constraints based on linguistic theory (i.e.,\nstack-based representations) while still being rela-\ntive agnostic (compared to the geometric probe) to\nthe details of how parse probabilities might be en-\ncoded. This suggests that despite a lack of explicit\nfeedback, language models not only rediscover lin-\nguistic structure through pretraining but learn to\nmake ‘inferences’ over this structure in real-time\ncomprehension.\n7 Limitations\nThe methods presented here have several limita-\ntions. Principally, the question of what consti-\ntutes a probe remains a potential confound (He-\nwitt and Liang, 2019; Belinkov, 2021). While we\nshow several effects that suggest our probes iden-\ntify meaningful syntactic information, including\ncounterfactual perturbations, our counterfactual ef-\nfects, though significant for many of the layers\nof the models considered, are small in absolute\nterms. Our approach to mitigating these has been\nto apply a varied set of analyses that, in aggregate,\nadjudicate between the hypotheses we present here.\nFinally, our probes rely on a particular oracle—it\nis possible that action sets from other oracles (e.g.\n2808\nArcEager) are better-suited towards representing\nincremental syntactic structures than ArcStandard.\n8 Ethical and Broader Impacts\nLanguage models are increasingly used for tasks\nbeyond language that assume the ability of the lan-\nguage model to structure their input and make ‘de-\ncisions’ in real-time. Our study targets this ability\nin a basic linguistic phenomenon and thus has the\npotential to be useful for interpretability and align-\nment. While the model control aspects of our study\npose some risks in that it may enable malicious\nparties to generate harmful content with language\nmodels, the authors believe the benefits in terms of\ninterpretability reasonably balance these.\nAcknowledgments\nWe would like to thank Peng Qian, Mycal Tucker,\nJacob Andreas, Josh Tenenbaum, and Ted Gib-\nson for helpful discussions. TE acknowledges\nsupport from the GEM consortium and the Na-\ntional Science Foundation Graduate Research Fel-\nlowship under Grant No. 1745302. RPL acknowl-\nedges support from NSF award BCS-2121074, NIH\naward U01-NS121471, and a Newton Brain Sci-\nence Award. YK acknowledges support from MIT-\nIBM Watson AI lab. Lastly, our codebase is built, in\npart, on code from open source projects for prepos-\nsessing and parsing PTB (Qi and Manning, 2017;\nHewitt and Manning, 2019; Noji and Oseki, 2021)\nas well deploying transformer language models\n(Paszke et al.; Wolf et al., 2019).\nReferences\nLaura Aina and Tal Linzen. 2021. The language model\nunderstood the prompt was ambiguous: Probing syn-\ntactic uncertainty through generation.\nSuhas Arehalli and Tal Linzen. 2020. Neural language\nmodels capture some, but not all, agreement attrac-\ntion effects.\nYonatan Belinkov. 2021. Probing classifiers: Promises,\nshortcomings, and advances.\nThomas G Bever. 1970. The cognitive basis for linguis-\ntic structures. In Cognition and the development of\nlanguage. Wiley.\nCharlotte Caucheteux and Jean-Rémi King. 2022.\nBrains and algorithms partially converge in natu-\nral language processing. Communications Biology,\n5(1):1–10.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language models:\nA simple approach to controlled text generation.\nNicola De Cao, Leon Schmid, Dieuwke Hupkes, and\nIvan Titov. 2021. Sparse interventions in language\nmodels with differentiable masking.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars.\nTiwalayo Eisape, Noga Zaslavsky, and Roger Levy.\n2020. Cloze distillation: Improving neural language\nmodels with human Next-Word prediction. In Pro-\nceedings of the 24th Conference on Computational\nNatural Language Learning, pages 609–619, Online.\nAssociation for Computational Linguistics.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2020. Amnesic probing: Behavioral ex-\nplanation with amnesic counterfactuals.\nWilliam Falcon et al. 2019. Pytorch lightning. GitHub.\nNote: https://github. com/PyTorchLightning/pytorch-\nlightning, 3(6).\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic subjects:\nRepresentations of syntactic state. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 32–42, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Second Meeting of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro, and\nJonathan R Brennan. 2018. Finding syntax in hu-\nman encephalography with beam search.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks.\nJohn Hewitt and Christopher D Manning. 2019. A struc-\ntural probe for finding syntax in word representations.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4129–4138.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger P Levy. 2020. A systematic assessment of\nsyntactic generalization in neural language models.\nDaniel Jurafsky. 1996. A probabilistic model of lexical\nand syntactic access and disambiguation. Cogn. Sci.,\n20(2):137–194.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\n2809\nYair Lakretz, Théo Desbordes, Dieuwke Hupkes, and\nStanislas Dehaene. 2021. Causal transformers per-\nform below chance on recursive nested constructions,\nunlike humans.\nRoger Levy. 2008. Expectation-based syntactic compre-\nhension. Cognition, 106(3):1126–1177.\nBelinda Z Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models.\nTal Linzen and Marco Baroni. 2021. Syntactic structure\nfrom deep learning. Annu. Rev. Linguist., 7(1):195–\n212.\nChristopher D Manning, Kevin Clark, John Hewitt, Ur-\nvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artificial neural networks\ntrained by self-supervision. Proc. Natl. Acad. Sci. U.\nS. A.\nMitchell P Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Comput.\nLinguist., 19(2):313–330.\nRebecca Marvin and Tal Linzen. 2018. Targeted syntac-\ntic evaluation of language models.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in GPT.\nMax Müller-Eberstein, Rob van der Goot, and Barbara\nPlank. 2022. Probing for labeled dependency trees.\nJoakim Nivre. 2004. Incrementality in deterministic\ndependency parsing. pages 50–57.\nHiroshi Noji and Yohei Oseki. 2021. Effective batching\nfor recurrent neural network grammars.\nAdam Paszke, Sam Gross, and Soumith Chintala. Auto-\nmatic differentiation in PyTorch.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-Theoretic probing for linguistic\nstructure.\nPeng Qi and Christopher D Manning. 2017. Arc-swift:\nA novel transition system for dependency parsing.\nPeng Qian, Tahira Naseem, Roger Levy, and Ramón\nFernandez Astudillo. 2021. Structural guidance for\ntransformer language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3735–3745, Online. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nShauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav\nGoldberg. 2021. Counterfactual interventions reveal\nthe causal effect of relative clause representations on\nagreement prediction.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Ca-\nrina Kauf, Eghbal A Hosseini, Nancy Kanwisher,\nJoshua B Tenenbaum, and Evelina Fedorenko. 2021.\nThe neural architecture of language: Integrative mod-\neling converges on predictive processing. Proc. Natl.\nAcad. Sci. U. S. A., 118(45).\nMitchell Stern, Daniel Fried, and Dan Klein. 2017. Ef-\nfective inference for generative neural parsing. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1695–1700, Copenhagen, Denmark. Association for\nComputational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, and\nEllie Pavlick. 2019. What do you learn from context?\nprobing for sentence structure in contextualized word\nrepresentations.\nMycal Tucker, Tiwalayo Eisape, Peng Qian, Roger Levy,\nand Julie Shah. 2022. When does syntax mediate\nneural language model performance? evidence from\ndropout probes.\nMycal Tucker, Peng Qian, and Roger Levy. 2021. What\nif this modified that? syntactic interventions with\ncounterfactual embeddings. In Findings of the Asso-\nciation for Computational Linguistics: ACL-IJCNLP\n2021, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\nAlex Warstadt and Samuel R Bowman. 2020. Can neu-\nral networks acquire a structural bias from raw lin-\nguistic data?\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019. Structural super-\nvision improves learning of Non-Local grammatical\ndependencies.\nEthan Wilcox, Pranali Vani, and Roger P Levy. 2021.\nA targeted assessment of incremental processing in\nneural LanguageModels and humans. arXiv e-prints,\npage arXiv:2106.03232.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger Levy. 2020. On the predictive power\nof neural language models for human Real-Time com-\nprehension behavior.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M Rush. 2019. Hugging-\nFace’s transformers: State-of-the-art natural language\nprocessing.\n2810\nA Appendix\nA.1 Training\nOur probes are built in PyTorch (Paszke et al.).\nAll probes were trained with Adam (Kingma and\nBa, 2014) with a learning rate of 10−3 except for\nthe GAP, which used a learning rate of10−5 after\npretraining on the regression task of Hewitt and\nManning (2019). For multilayer architectures, a\ndropout rate of 0.2 was used between layers (3 lay-\ners), and hidden state size was fixed to be the same\nsize as the model emission. The attentive probe\nwas implemented using a GRU of 200 hidden units,\nbiaffine attention, and a 3 layer MLP read-out net-\nwork. All architectures were optimized in PyTorch\nlightning (Falcon et al., 2019). Each model trained\nfor at most 40 each but used early stopping with a\nplateau tolerance of 3.\nA.2 Counterfactual details\nWe find training our probes with a high dropout\nrate (Tucker et al., 2022) improves counterfactual\nresults. Thus, we train separate checkpoints for\nevaluation (0 dropout on model embeddings) and\ncounterfactual generation (0.4 dropout). We found\nMAP achieved the largest counterfactual effect size\nin the predicted direction when evaluating NP- and\nZ-parse consistent completions. It had less inter-\npretable effects on the ‘Both’ and ’Neither’ condi-\ntions. We include the full range of experiments in\nFigures 8 and 9.\n2811\nFigure 8: Extended results of counterfactual analyses.\n2812\nFigure 9: Extended results of counterfactual analyses\n2813",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8308727741241455
    },
    {
      "name": "Parsing",
      "score": 0.7678834795951843
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6597325801849365
    },
    {
      "name": "Autoregressive model",
      "score": 0.6547358632087708
    },
    {
      "name": "Language model",
      "score": 0.6414989233016968
    },
    {
      "name": "Natural language processing",
      "score": 0.636702835559845
    },
    {
      "name": "Syntax",
      "score": 0.6158382892608643
    },
    {
      "name": "Sentence",
      "score": 0.5317778587341309
    },
    {
      "name": "Word (group theory)",
      "score": 0.42984139919281006
    },
    {
      "name": "Linguistics",
      "score": 0.2118825912475586
    },
    {
      "name": "Econometrics",
      "score": 0.09422034025192261
    },
    {
      "name": "Mathematics",
      "score": 0.08836174011230469
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    }
  ]
}