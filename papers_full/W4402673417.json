{
  "title": "Transformer-Based Model Predictive Control: Trajectory Optimization via Sequence Modeling",
  "url": "https://openalex.org/W4402673417",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2986161080",
      "name": "Celestini Davide",
      "affiliations": [
        "Polytechnic University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A4223103734",
      "name": "Gammelli, Daniele",
      "affiliations": [
        "Vaughn College of Aeronautics and Technology",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4297694928",
      "name": "Guffanti, Tommaso",
      "affiliations": [
        "Vaughn College of Aeronautics and Technology",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4221637350",
      "name": "D'Amico, Simone",
      "affiliations": [
        "Vaughn College of Aeronautics and Technology",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2746634172",
      "name": "Capello Elisa",
      "affiliations": [
        "Polytechnic University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A2755084559",
      "name": "Pavone, Marco",
      "affiliations": [
        "Stanford University",
        "Vaughn College of Aeronautics and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4396875795",
    "https://openalex.org/W3208343212",
    "https://openalex.org/W2162991084",
    "https://openalex.org/W2772821814",
    "https://openalex.org/W2016211524",
    "https://openalex.org/W2162218551",
    "https://openalex.org/W6682849425",
    "https://openalex.org/W6747993910",
    "https://openalex.org/W6791168167",
    "https://openalex.org/W2963439114",
    "https://openalex.org/W6759886010",
    "https://openalex.org/W4383109148",
    "https://openalex.org/W6796289742",
    "https://openalex.org/W6804244202",
    "https://openalex.org/W4385403811",
    "https://openalex.org/W3080523054",
    "https://openalex.org/W3119053045",
    "https://openalex.org/W3208705304",
    "https://openalex.org/W6754302822",
    "https://openalex.org/W6767977373",
    "https://openalex.org/W6784114448",
    "https://openalex.org/W6640174482",
    "https://openalex.org/W3109830187",
    "https://openalex.org/W2595023674",
    "https://openalex.org/W3107452320",
    "https://openalex.org/W2155007355"
  ],
  "abstract": "Model predictive control (MPC) has established itself as the primary methodology for constrained control, enabling general-purpose robot autonomy in diverse real-world scenarios. However, for most problems of interest, MPC relies on the recursive solution of highly non-convex trajectory optimization problems, leading to high computational complexity and strong dependency on initialization. In this work, we present a unified framework to combine the main strengths of optimization-based and learning-based methods for MPC. Our approach entails embedding high-capacity, transformer-based neural network models within the optimization process for trajectory generation, whereby the transformer provides a near-optimal initial guess, or target plan, to a non-convex optimization problem. Our experiments, performed in simulation and the real world onboard a free flyer platform, demonstrate the capabilities of our framework to improve MPC convergence and runtime. Compared to purely optimization-based approaches, results show that our approach can improve trajectory generation performance by up to 75%, reduce the number of solver iterations by up to 45%, and improve overall MPC runtime by 7x without loss in performance.",
  "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024 1\nTransformer-based Model Predictive Control:\nTrajectory Optimization via Sequence Modeling\nDavide Celestini1∗, Daniele Gammelli2∗, Tommaso Guffanti2, Simone D’Amico2,\nElisa Capello1, and Marco Pavone2\nAbstract—Model predictive control (MPC) has established it-\nself as the primary methodology for constrained control, enabling\ngeneral-purpose robot autonomy in diverse real-world scenarios.\nHowever, for most problems of interest, MPC relies on the recursive\nsolution of highly non-convex trajectory optimization problems,\nleading to high computational complexity and strong dependency\non initialization. In this work, we present a unified framework to\ncombine the main strengths of optimization-based and learning-\nbased methods for MPC. Our approach entails embedding high-\ncapacity, transformer-based neural network models within the op-\ntimization process for trajectory generation, whereby the trans-\nformer provides a near-optimal initial guess, or target plan, to a\nnon-convex optimization problem. Our experiments, performed in\nsimulation and the real world onboard a free flyer platform, demon-\nstrate the capabilities of our framework to improve MPC conver-\ngence and runtime. Compared to purely optimization-based ap-\nproaches, results show that our approach can improve trajectory\ngeneration performance by up to 75%, reduce the number of solver\niterations by up to 45%, and improve overall MPC runtime by 7x\nwithout loss in performance.\nProject website and code: https://transformermpc.github.io\nIndex Terms—Optimization and Optimal Control, Deep Learn-\ning Methods, Machine Learning for Robot Control\nI. I NTRODUCTION\nT\nRAJECTORY generation is crucial to achieving reliable\nrobot autonomy, endowing autonomous systems with the\ncapability to compute a state and control trajectory that simul-\ntaneously satisfies constraints and optimizes mission objectives.\nAs a result, trajectory generation problems have been formulated\nin many practical areas, including space and aerial vehicles [2],\n[3], robot motion planning [4], chemical processes [5], and more.\nCrucially, the ability to solve the trajectory generation problem in\nreal time is pivotal to safely operate within real-world scenarios,\nallowing the autonomous system to rapidly recompute an opti-\nmal plan based on the most recent information.\nManuscript received: May, 16, 2024; Revised August, 4, 2024; Accepted\nSeptember, 3, 2024. This letter was recommended for publication by Editor\nJ. Kober upon evaluation of the reviewers’ comments. This work is supported\nby Blue Origin (SPO #299266) as Associate Member and Co-Founder of the\nStanford’s Center of AEroSpace Autonomy Research (CAESAR) and the NASA\nUniversity Leadership Initiative (grant#80NSSC20M0163). This article solely\nreflects the opinions and conclusions of its authors and not any Blue Origin or\nNASA entity.(Corresponding author: Davide Celestini.)\n∗ Equal contribution.\n1 Davide Celestini and Elisa Capello are with the Department of Mechanical\nand Aerospace Engineering, Politecnico di Torino, 10129 Torino, Italy. (e-mail :\ndavide.celestini@polito.it, elisa.capello@polito.it)\n2 Daniele Gammelli, Tommaso Guffanti, Simone D’Amico and Marco Pavone\nare with the Department of Aeronautics and Astronautics, Stanford University,\n94305, USA. (e-mail : gammelli@stanford.edu, tommaso@stanford.edu, dami-\ncos@stanford.edu, pavone@stanford.edu)\nDigital Object Identifier (DOI): 10.1109/LRA.2024.3466069.\nMotivated by its widespread applications, a collection of\nhighly effective solution strategies exist for the trajectory gen-\neration problem. For example, numerical optimization provides\na systematic mathematical framework to specify mission objec-\ntives as costs or rewards and enforce state and control specifica-\ntions via constraints [6]. However, for most problems of interest,\nthe trajectory optimization problem is almost always nonconvex,\nleading to high computational complexity, strong dependency\non initialization, and a lack of guarantees of either obtaining a\nsolution or certifying that a solution does not exist [7]. The above\nlimitations are further exacerbated in the case of model predic-\ntive control (MPC) formulations, where the trajectory generation\nproblem needs to be solved repeatedly and in real-time as the\nmission evolves, enforcing strong requirements on computation\ntime. Moreover, MPC formulations typically require significant\nmanual trial-and-error in defining ad-hoc terminal constraints\nand cost terms to, e.g., achieve recursive feasibility or exhibit\nshort-horizon behavior that aligns with the full trajectory gener-\nation problem.\nBeyond methods based on numerical optimization, recent ad-\nvances in machine learning (ML) have motivated the applica-\ntion of learning-based methods to the trajectory generation prob-\nlem [8]. ML approaches are typically highly computationally\nefficient and can be optimized for (potentially nonconvex) per-\nformance metrics from high-dimensional data (e.g., images).\nHowever, learning-based methods are often sensitive to dis-\ntribution shifts in unpredictable ways, whereas optimization-\nbased approaches are more readily characterized both in terms\nof robustness and out-of-distribution behavior. Additionally, ML\nmethods often perform worse on these problems due to their\nhigh-dimensional action space, which increases variance in, e.g.,\npolicy-gradient algorithms [9], [10]. As a result, real-world de-\nployment of learning-based methods has so far been limited\nwithin safety-critical applications.\nIn this work, we propose a framework to exploit the specific\nstrengths of optimization-based and learning-based methods for\ntrajectory generation, specifically tailored for MPC formulations\n(Fig. 1). By extending the framework introduced in [1], we pro-\npose a pre-train-plus-fine-tuning strategy to train a transformer\nto generate near-optimal state and control sequences and show\nhow this allows to (i) warm-start the optimization with a near-\noptimal initial guess, leading to improved performance and faster\nconvergence, and (ii) provide long-horizon guidance to short-\nhorizon problems in MPC formulations, avoiding the need for\nexpensive tuning of cost terms or constraints within the opti-\nmization process. Crucially, we show how the proposed fine-\ntuning scheme results in substantially improved robustness to\ndistribution shifts caused by closed-loop execution and how the\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current\nor future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works,\nfor resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\narXiv:2410.23916v1  [cs.RO]  31 Oct 2024\n2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\nFig. 1: We propose a framework to combine high-capacity sequence models and optimization for MPC. The core idea is to train a transformer model\nto generate near-optimal trajectories (top), which can be used to warm-start optimal control problems (OCPs) at inference (bottom). We leverage\nmethods introduced in [1] as a pre-training step, whereby a transformer is trained on pre-collected (open-loop) trajectory data (top left) to provide\nan initial guess for the full OCP (bottom left). For effective MPC, we fine-tune the model (top right) through closed-loop corrections (red) and use\nit to provide both an initial guess for the OCP and a target state to approximate future cost within the short-sighted problem (bottom right).\ninjection of learning-based guidance within MPC formulations\ndrastically decreases the loss in performance due to the reduction\nof the planning horizon, enabling the solution of substantially\nsmaller optimization problems without sacrificing performance.\nThe contributions of this paper are threefold:\n• We present a framework to combine the strengths of of-\nfline learning andonline optimization for efficient trajectory\ngeneration within MPC formulations.\n• We investigate design and learning strategies within our\nframework, assessing the impact of fine-tuning on MPC\nexecution and the benefits of learned terminal cost defini-\ntions to mitigate the inherent myopia inshort-horizon MPC\nformulations.\n• Through experiments performed in simulation (i.e., space-\ncraft rendezvous and quadrotor control) and real-world\nrobotic platforms (i.e., a free flyer testbed), whereby we\nassume perfect state and scene estimation, we show how our\nframework substantially improves the performance of off-\nthe-shelf trajectory optimization methods in terms of cost,\nruntime, and convergence rates, demonstrating its applica-\nbility within real-world scenarios characterized by nonlin-\near dynamics and constraints.\nII. R ELATED WORK\nOur work is closely related to previous approaches that com-\nbine learning and optimization for control [11]–[13], exploit\nhigh-capacity neural networks for control [14]–[16], and meth-\nods for warm-starting nonlinear optimization problems [1],\n[17]–[19], providing a way to train general-purpose trajectory\ngeneration models that guide the solution of inner optimization\nproblems.\nNumerous strategies have been developed for learning to con-\ntrol via formulations that leverage optimization-based planning\nas an inner component. For instance, prior work focuses on de-\nveloping hierarchical formulations, where a high-level (learning-\nbased) module generates a waypoint-like representation for a\nlow-level planner, e.g., based on sampling-based search [11], tra-\njectory optimization [13], or model-based planning [12]. Within\nthis context, the learning-based component is typically trained\nthrough either (online) reinforcement learning or imitation of\noracle guidance algorithms. An alternate strategy consists of\ndirectly optimizing through an inner controller. A large body\nof work has focused on exploiting exact solutions to the gra-\ndient of (convex) optimization problems at fixed points [20],\n[21], allowing the inner optimization to be used as a generic\ncomponent in a differentiable computation graph (e.g., a neu-\nral network). On the contrary, our approach targets non-convex\nproblems and leverages the output of a learning-based module as\ninput to the unmodified trajectory optimization problems. This\nsimplifies non-convex trajectory planning, avoiding the potential\nmisalignment caused by intermediate problem representations\nthat do not necessarily correlate with the downstream task (e.g.,\nfixed-point iteration in value function learning).\nOur method is closely related to recent work that leverages\nhigh-capacity generative models for control. For example, prior\nwork has shown how transformers [14], [15] and diffusion mod-\nels [16] trained via supervised learning on pre-collected trajec-\ntory data are amenable to both model-free feedback control [14]\nor (discrete) model-based planning [15]. These methods have\ntwo main drawbacks: (i) they typically ignore non-trivial state-\ndependent constraints, a setting that is both extremely common\nin practice and challenging for purely learning-based methods\nand (ii) they do not exploit all the information available to system\ndesigners, which typically includes approximate knowledge of\nthe system dynamics. As in [1], our method alleviates both of\nthese shortcomings by (i) leveraging online trajectory optimiza-\ntion to enforce non-trivial constraint satisfaction, and (ii) taking\na model-based view to transformer-based trajectory generation\nand leveraging readily available approximations of the system\ndynamics to improve autoregressive generation.\nLastly, prior work has explored the idea of using ML to warm-\nstart the solution of optimization problems. While the concept\nof warm-starting optimization solvers is appealing in princi-\nple, current approaches are typically (i) limited by the choice\nof representation for the output of the ML model (e.g., a fixed\ndegree polynomial) [17], [18], (ii) restricted to the open-loop\nplanning setting [1], whereby the impact of distribution shifts due\nto closed-loop execution [22] (e.g., model mismatch, stochas-\nticity, etc.) is often overlooked, and (iii) confined to linear con-\nCELESTINI et al.: TRANSFORMER-BASED MODEL PREDICTIVE CONTROL: TRAJECTORY OPTIMIZATION VIA SEQUENCE MODELING 3\nstraints [19]. To address these limitations, we present an extended\nand revised version of [1], whereby we (i) build on prior work\nand use transformer network for effective trajectory modeling,\n(ii) introduce a pre-train-plus-fine-tuning learning strategy to\nmaximize closed-loop performance, (iii) develop an MPC for-\nmulation that enables the transformer to provide long-horizon\nguidance to short-horizon problems, and (iv) augment the tra-\njectory representation and the transformer architecture to handle\nconditioning on a target state. Crucially, we take full advantage of\nthe transformer’s autoregressive generation capabilities within\nreceding horizon control formulations, enabling the same model\nto be used with varying planning horizon specifications.\nIII. P ROBLEM STATEMENT\nLet us consider the time-discrete optimal control problem\n(OCP):\nminimize\nx(ti), u(ti)\nJ =\nNX\ni=1\nJ (x(ti), u(ti)) (1a)\nsubject to x(ti+1) = f (x(ti), u(ti)) ∀i ∈ [1, N] , (1b)\nx(ti) ∈ Xti, u(ti) ∈ Uti ∀i ∈ [1, N] , (1c)\nwhere x(ti) ∈ Rnx and u(ti) ∈ Rnu are respectively the\nnx-dimensional state and nu-dimensional control vectors, J :\nRnx+nu → R defines the running cost, f : Rnx+nu → Rnx\nrepresents the system dynamics,Xti and Uti are generic state and\ncontrol constraint sets, andN ∈ Ndefines the number of discrete\ntime instants ti over the full OCP horizonT.\nIn particular, we consider a receding-horizon reformulation of\nProblem (1), whereby we recursively solve an OCP characterized\nby a moving (and typically shorter) horizonH = [h, h+ H] ⊆\n[1, N], with h being the moving initial timestep andH ∈ (0, N]\nthe length of the horizon. To ensure desirable long-horizon be-\nhavior from the solution of a series of short-horizon problems,\nMPC formulations typically require the definition of a terminal\ncost JT or constraint set Xh+H. Specifically, we will consider\ncost functions of the form:\nJ = JT (x(th+H)) +\nh+H−1X\ni=h\nJ (x(ti), u(ti)) , (2)\nwhere the running cost J is evaluated exclusively over H and\nJT : Rnx → R is the terminal cost function.\nIn this work, we explore approaches to address Problem (2)\nby incorporating high-capacity neural network models within the\noptimization process for trajectory generation, explicitly tailored\nfor closed-loop performance.\nIV. T RAJECTORY OPTIMIZATION VIA SEQUENCE\nMODELING\nWe consider a strategy for trajectory generation whereby state\nand control sequences X = (x1, ...,xN), U = (u1, ...,uN) are\nobtained through the composition of two components,\n\u0010\nˆX, ˆU\n\u0011\n∼ pθ(X, U |σ0) (3)\n(X, U) = Opt\n\u0010\nx(t1), ˆX, ˆU\n\u0011\n, (4)\nwhere pθ(·) denotes the conditional probability distribution over\ntrajectories (given an initial condition σ0) learned by a trans-\nformer model with parametersθ, Opt denotes the trajectory op-\ntimization problem, and ˆX, ˆU denote the predicted (state and\ncontrol) trajectories that are taken as an input to the optimization\nproblem. The initial condition σ0 is used to inform trajectory\ngeneration in the form of, e.g., an initial state x(t1), a target\nstate x(tN ) to be reached by the end of the trajectory, or other\nperformance-related parameters. In this section, we will first dive\ninto the details of the trajectory representation for sequence mod-\neling. We will then discuss both open-loop pre-training and MPC\nfine-tuning formulations of our approach, together with specific\ninference algorithms for open-loop planning and MPC.\nA. Trajectory Representation\nAt the core of our approach is the treatment of trajectory data\nas a sequence to be modeled by a transformer model [1]. Specif-\nically, given a pre-collected dataset of trajectories of the form\nτraw = (x1, u1, r1, . . . ,xN , uN , rN ) , where xi and ui denote\nthe state and control at timeti, and ri = −J(x(ti), u(ti)) is the\ninstantaneous reward, or negative cost, we define the following\ntrajectory representation:\nτ = (T1, R1, C1, x1, u1, . . . ,TN , RN , CN , xN , uN ) , (5)\nwhere Ti ∈ Rnx, Ri ∈ Rand Ci ∈ N+ represent a set of perfor-\nmance parameters that allow for effective trajectory generation.\nIn particular, we defineTi as thetarget state, i.e., the state we wish\nto reach by the end of the trajectory, that could be either constant\nduring the entire trajectory or time-dependent. We further define\nRi and Ci as the reward-to-go and constraint-violation-budget\nevaluated atti, respectively. Formally, as in [1], we defineRi and\nCi to express future optimality and feasibility of the trajectory as:\nR(ti)=\nNX\nj=i\nrj, C (ti)=\nNX\nj=i\nCj, Cj =\n(\n1, if∃xj,uj /∈Xj, Uj\n0, otherwise.\n(6)\nThis definition of the performance parameters is appealing for\ntwo reasons: (i) during training, the performance parameters are\neasily derivable from raw trajectory data by applying (6) forRi\nand Ci, and by setting Ti = xN , (ii) at inference, according to\n(3), it allows to condition the generation of predicted state and\ncontrol trajectories ˆX and ˆU through user-defined initial con-\nditions σ0. Namely, given user-defined σ0 = ( T1, R1, C1, x1),\na successfully trained transformer should be able to generate a\ntrajectory ( ˆX, ˆU) starting from x(t1) that achieves a reward of\nR1, a constraint violation ofC1, and terminates in stateT1.\nB. Open-loop Pre-training\nIn what follows, we introduce the pre-training strategy used to\nobtain a transformer model capable of generating near-optimal\ntrajectories ˆX = ( ˆx1, ...,ˆxN ) , ˆU = ( ˆu1, ...,ˆuN ) in an open-\nloop setting.\nDataset generation. The first step entails generating a dataset\nfor effective transformer training. To do so, we generate ND\ntrajectories by repeatedly solving Problem (1) with randomized\ninitial conditions and target states, and then re-arranging the raw\ntrajectories according to (5). We construct the datasets to include\nboth solutions to Problem (1) as well as to its relaxations. We\nobserve that diversity in the available trajectories is crucial to\nenable the transformer to learn the effect of the performance\nmetrics; for example, solutions to relaxations of Problem (1) will\ntypically yield trajectories with low cost and non-zero constraint\nviolation (i.e., highRand C > 0), while direct solutions to Prob-\nlem (1) will be characterized by higher cost and zero constraint\nviolation ( i.e., lower R and C = 0 ), allowing the transformer\n4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\nto learn a broad range of behaviors from the specification of the\nperformance parameters.\nTraining. We train the transformer with the standard teacher-\nforcing procedure used to train sequence models. Specifically,\ndenoting the L2-norm as||·|| 2, we optimize the following mean-\nsquared-error (MSE) loss function:\nL(τ) =\nNDX\nn=1\nNX\ni=1\n\u0010\n∥x(n)\ni − ˆx(n)\ni ∥2\n2 + ∥u(n)\ni − ˆu(n)\ni ∥2\n2\n\u0011\n, (7)\nwhere n denotes the n-th trajectory sample from the dataset,\nˆx(n)\ni ∼ pθ(x(n)\ni | τ(n)\n<ti), ˆu(n)\ni ∼ pθ(u(n)\ni | τ(n)\n<ti, x(n)\ni ) are the one-\nstep predictions for both state and control vectors, and where we\nuse τ<ti to denote a trajectory spanning timestepst ∈ [t1, ti−1].\nInference. Once trained, the transformer can be used to\nautoregressively generate an open-loop trajectory, i.e.,\n( ˆX, ˆU) ∼ pθ(X, U |σ0), from a given initial condition\nσ0 = (T1, R1, C1, x1) that encodes the initial state, target state,\nand desired performance metrics. Given σ0, the autoregressive\ngeneration process is defined as follows: (i) the transformer\ngenerates a controlu1, (ii) building on the findings of [1], we take\na model-based view on autoregressive generation, whereby the\nnext state x2 is generated according to a (known) approximate\ndynamics model ˆf(x, u)—a reasonable assumption across\nthe problem settings we investigate—, (iii) the performance\nmetrics are updated by decreasing the reward-to-go R1 and\nconstraint-violation-budget C1 by the instantaneous reward\nr1 and constraint violation C(t1), respectively, (iv) the target\nstate T1 is either kept constant or updated to a new target state,\nand (v) the previous four steps are repeated until the horizon\nis reached. To specify the performance parameters, we select\nR1 as a (negative) quantifiable lower bound of the optimal cost\nand C1 = 0 , incentivizing the generation of near-optimal and\nfeasible trajectories.\nC. Long-horizon Guidance for Model Predictive Control\nThe core of our approach relies on leveraging the recursive\ncomputation within sequence models for effective model pre-\ndictive control. Specifically, given a planning horizon H, we\nfine-tune a transformer to generate trajectories ˆXh:h+H =\n(ˆxh, . . . ,ˆxh+H), ˆUh:h+H = (ˆuh, . . . ,ˆuh+H). In our formula-\ntion, the generated trajectories are used to provide a warm-start to\nthe trajectory optimization problem, as in the open-loop planning\nscenario, but also, crucially, to specify an effective terminal cost\nJT. As a result, we use the transformer to (i) improve runtime\nby enabling the solution of smaller optimization problems and\n(ii) define learned terminal cost terms and avoid expensive cost-\ntuning strategies.\nClosed-loop fine-tuning.Rather than naively applying the trans-\nformer trained on open-loop data to the MPC setting, we devise\na fine-tuning scheme to achieve effective closed-loop behavior.\nIt is well-known that methods based on imitation learning are\nexposed to severe error compounding when applied in closed-\nloop [22]. The main reason behind this error compounding is\nthe covariate shift problem, i.e., the fact that the actual perfor-\nmance of the learned policy depends on its own state distribution,\nwhereas training performance is only affected by the expert’s\nstate distribution (i.e., the policy used to generate the data). To\naddress this problem, we resort to iterative algorithms, namely\nDAGGER [23], in which the current policy interacts with the\ntraining environment to collect trajectory data and define a new\nAlgorithm 1: Fine-tuning strategy for MPC\nInput: Pre-trained pθ0 , Expert π∗, Open-loop dataDol\n1 Initialize Dft ← {Dol};\n2 for i = 0 to dagger iterationsdo\n3 for num trajectoriesdo\n4 Select exploration policyπt\nexp = {pθi, π∗}, ∀t;\n5 Sample planning horizonH ∈ (0, N];\n6 Get dataset Di = {x, π∗(x)} of states visited byπexp\nand controls given byπ∗;\n7 Aggregate dataset Dft ← Dft ∪ Di;\n8 Update model pθi+1 ← Train (pθi, Dft);\ndataset for training based on expert corrections. In our frame-\nwork, we fine-tune the transformer in simulation according to the\nsame approximate modelˆf(x, u)used for the pre-training, hence\nthe state distribution shift arises uniquely from the differences\nbetween open-loop and closed-loop cost functions—see (1a) vs\n(2). Our tailored DAGGER algorithm is defined according to three\nmain design choices (Alg. 1). First, to incentivize the transformer\nto learn optimal solutions for the full-horizon OCP, we define the\nexpert policy as the solution to the full horizon Problem (1). Sec-\nond, to promote successful generalization across various plan-\nning horizons H, we randomly sample the exploration policy\nparameter H from a discrete set that uniformly spans short to full-\nhorizon problem formulations. Third, as is usual for DA GGER\nimplementations, we alternate between two exploration policies:\nthe transformer from the last dagger iteration and the expert\npolicy. Note that using DAGGER from scratch would decrease the\nefficiency in the exploration and increase the risk of getting stuck\nin locally optimal policies.\nInference.After fine-tuning, we use the transformer to generate a\ntrajectory ( ˆXh:h+H, ˆUh:h+H) ∼ pθ(Xh:h+H, Uh:h+H | τ<h)\nfollowing the same five-step process defined in the open-loop\nscenario, whereby the autoregressive generation is executed for\nH steps, rather than for the full OCP horizonN. We further use\nthe generated trajectory to (i) warm-start the short-horizon OCP,\nand (ii) define a cost function of the form:\nJ = JT (x(th+H), ˆx(th+H)) +\nh+HX\ni=h\nJ (x(ti), u(ti)) , (8)\nwhere the function JT(xh+H, ˆxh+H) is defined as a distance\nmetric that penalizes deviation from the predicted terminal\nstate ˆxh+H. As a consequence of this formulation, we use the\ntransformer—trained to generate near-optimal trajectories for\nthe full OCP—to incentivize solutions that better align with the\nlong-horizon problem.\nV. E XPERIMENTS\nIn this section, we demonstrate the performance of our frame-\nwork on three trajectory optimization problems: two in simula-\ntion (i.e., spacecraft rendezvous and quadrotor control scenar-\nios) and one real-world robotic platform (i.e., afree flyertestbed).\nWe consider three instantiations of Problem (1) based on the\nfollowing general formulation:\nminimizexi, ui\nNX\ni=1\n∥ui∥q\np (9a)\nsubject to xi+1 = ˆf (xi, ui) ∀i ∈ [1, N], (9b)\nΓ (xi) ≥ 0 ∀i ∈ [1, N] , (9c)\nx1 = xstart, xN+1 = xgoal. (9d)\nCELESTINI et al.: TRANSFORMER-BASED MODEL PREDICTIVE CONTROL: TRAJECTORY OPTIMIZATION VIA SEQUENCE MODELING 5\nFig. 2: Visualization of the three scenarios considered in this work. For the tasks of (a) spacecraft rendezvous, (b) quadrotor and (c) free flyer control,\nwe show three example trajectories obtained by warm-starting the SCP through a relaxation of the full OCP (blue) or through our proposed approach\n(cyan). Keep-out-zones and obstacles are denoted by the red shaded areas.\nwhere the objective function in (9a) expresses the minimization\nof the control effort (with p, qdenoting application-dependent\nparameters), (9b) represents the (potentially nonlinear) approxi-\nmate system dynamics, (9c) defines the obstacle avoidance con-\nstraints through a nonlinear distance functionΓ :Rm×nx →Rm\nwith respect tom ∈ [1, M] non-convex keep-out-zones, and (9d)\nrepresents the initial and terminal state constraints.\nExperimental design. While the specific formulations of the\ntrajectory optimization problem will necessarily depend on the\nindividual application, we design our experimental setup to fol-\nlow some common desiderata.\nFirst, we care to isolate the benefits of (i) the initial guess (in\nboth open-loop and MPC settings) and (ii) learning the terminal\ncost for MPC; thus, we keep both the formulation and the so-\nlution algorithm for the OCP fixed and only evaluate the effect\nof different initializations. In particular, we resort to sequential\nconvex programming (SCP) methods for the solution of the tra-\njectory optimization problem and compare different approaches\nthat provide an initial guess for the sequential optimizer (or, for\nMPC, also a specification of the terminal cost). Second, in the\nopen-loop setting, we always compare our approach, identified\nas Transformers for Trajectory Optimization (TTO), with the fol-\nlowing classes of methods: (i) a quantifiable cost Lower Bound\n(LB), characterized by the solution of a relaxation (REL) to the\nfull-horizon Problem (9) which ignores obstacle avoidance con-\nstraints from (9c) and thus represents a (potentially infeasible)\ncost lower bound to the full problem and (ii) a state-of-the-art\nSCP approach where the initial guess to Problem (9) is provided\nby the solution to the REL relaxation described in (i) [17], [24].\nThird, in the model predictive control setting, we always consider\nterminal cost terms of the formJT(xh+H, ¯x) = ∥xh+H − ¯x∥P ,\nwhere ∥·∥P denotes the weighted norm with respect to a diagonal\nmatrix P and compare different choices for ¯x and P. Lastly, as\nintroduced in Sec. IV-B, to obtain diverse trajectories for offline\ntraining we solve Nd = 400 , 000 instances of the trajectory\noptimization problem, whereby Nd\n2 are solutions to the full non-\nconvex Problem (9), andNd\n2 to its REL relaxation.\nIn our experiments, we care about three main performance\nmetrics: cost, speed of optimization (expressed in terms of\nthe number of SCP iterations), and overall runtime. As in [1],\nthroughout our experiments, we assume knowledge of the fixed\noperational environment and perfect state estimation at all times.\nWe further assume that the target state is reachable within N\ndiscrete time instants.\nTransformer architecture. Our model is a transformer archi-\ntecture designed to account for continuous inputs and outputs.\nGiven an input sequence and a pre-defined maximum con-\ntext length K, our model takes as input the last 5K sequence\nelements, one for each modality: target state, reward-to-go,\nconstraint-violation-budget, state, and control. The sequence el-\nements are projected through a modality-specific linear transfor-\nmation, obtaining a sequence of5K embeddings. As in [15], we\nfurther encode an embedding for each timestep in the sequence\nand add it to each element embedding. The resulting sequence of\nembeddings is processed by a causal GPT model consisting of\nsix layers and six self-attention heads. Lastly, the GPT architec-\nture autoregressively generates a sequence of latent embeddings\nwhich are projected through modality-specific decoders to the\npredicted states and controls.\nA. Problem Description\nIn this section, we describe our experimental scenarios, the\ndefinition of cost, state, and control variables, and the specific\nimplementation of constraints in (9b-9d).\nSpacecraft rendezvous and proximity operations. We con-\nsider the task of performing an autonomous rendezvous, prox-\nimity operation, and docking (RPOD) transfer, in which a ser-\nvicer spacecraft equipped with impulsive thrusters approaches\nand docks with a target spacecraft. The relative motion of the\nservicer with respect to the target is described using the quasi-\nnonsingular Relative Orbital Elements (ROE) formulation, i.e.\nx := δœ ∈ R6, [1], [25]. Cartesian coordinates in the Radial\nTangential Normal (RTN) frame can be obtained through a time-\nvarying linear mapping [1] and employed for imposing geomet-\nric constraints.\nProblem (9) is detailed as follows: (i) in (9a), we selectp = 2\nand q = 1 , to account for impulsive thrusters aligned with\nthe desired control input; (ii) in (9d), xstart is defined by a\nrandom passively-safe relative orbit with respect to the target,\nwhile xgoal is selected among two possible docking ports lo-\ncated on the T axis, as shown in Fig. 2a; (iii) in (9b), δœi+1 =\nΦi (∆t) (δœi + Biui) is used to enforce dynamics constraints,\nwith Φi(∆t) ∈ R6×6 and Bi ∈ R6×3 being the linear time-\nvarying state transition and control input matrices [1], ∆t the\ntime discretization and ui = ∆ vi ∈ R3 the impulsive delta-\nvelocity applied by the servicer; (iv) in (9c), we consider an\nelliptical keep-out-zone (KOZ) centered at the target, limiting\nthe time index i to [1, Nwp]. We further consider two additional\ndomain-specific constraints: (v) a zero relative velocity pre-\ndocking waypoint to be reached ati = Nwp; (vi) a second-order\ncone constraint enforcing the service spacecraft to approach the\ndocking port inside a cone with aperture angle γcone for Nwp ≤\n6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\nFig. 3: Free flyer testbed and real-world execution of the FT-TTO tra-\njectory in Fig. 2c. Transparency identifies the time progression from the\nstart (faded) to the end (opaque) of the trajectory.\ni ≤ N, with axisncone ∈ R3 and vertexδrcone ∈ R3 determined\naccordingly to the selected docking port.\nQuadrotor control. We consider the task of autonomous\nquadrotor flight, in which a quadrotor robot has to reach a target\nstate while navigating an obstacle field, Fig. 2b. We use a Carte-\nsian reference frame Oxyz, with the z-axis pointing upward, to\ndefine x := (r, v) ∈ R6, u := T ∈ R3, with r, v, T ∈ R3 being\nthe quadrotor’s position, velocity and thrust vectors, respectively.\nWe define the following elements of Problem (9): (i) in (9a),\nwe select p = q = 2 to adhere to the continuous nature of the\ncontrol input; (ii) in (9d), we sample xstart and xgoal from pre-\ndetermined start and goal regions; (iii) in (9b), system dynamics\nare given by the nonlinear model ri+1 = ri + vi∆t, vi+1 =\nvi + 1\nm\n\u0000\n−βdrag∥vi∥2\n2 + ui\n\u0001\n∆t, withm and βdrag being the mass\nand the drag coefficient of the drone, respectively; (iv) in (9c), we\nconsider multiple spherical KOZs distributed between the start\nand goal regions.\nFree flyer testbed. Lastly, we consider a real-world free flyer\nplatform in which a floating robot—the free flyer—can move\nover a granite table in absence of friction, simulating space\nflight. We consider the task of controlling the free flyer, equipped\nwith eight ON-OFF thrusters allowing bi-dimensional roto-\ntranslational motion, to achieve a target state by traversing an\nobstacle field. The simulation scenario and the real-world testbed\nare reported in Fig. 2c and 3, respectively. We use a global\nCartesian frame Oxy to define x := ( r, ψ,v, ω) ∈ R6, u :=\nRGB(ψ)Λ∆V ∈ R3, where r, v ∈ R2 and ψ, ω ∈ R are\nrespectively the position, velocity, heading angle and angular rate\nof the free flyer, ∆V ∈ R8 consists of the impulsive delta-\nvelocity applied by each thruster, Λ ∈ R3×8 is the thrusters’\nconfiguration matrix and RGB ∈ R3×3 is the rotation matrix\nfrom the body reference frame of the free flyer toOxy.\nProblem (9) is specified as follows: (i) in (9a), we select\np = q = 1 as the ON-OFF thruster are not aligned with the\ncontrol input; (ii) in (9d), xstart and xgoal are sampled from\npre-determined start and goal regions; (iii) in (9b), the dynam-\nics are propagated using the impulsive model xi+1 = xi +\ndiag ([∆t, ∆t, ∆t, 1, 1, 1]) ui; (iv) in (9c), we consider multi-\nple spherical KOZs distributed between the start and goal re-\ngions, comprising the obstacles’ and the free flyer’s radii. We\nfurther consider two domain-specific constraints: (v) nonlinear\nactuation limits defined by 0 ≤ Λ−1R−1\nGBu ≤ ∆Vmax, with\n∆Vmax = T∆t\nm , where T and m denote the thrust level of the\nthrusters and the mass of the free flyer, respectively; (vi) state\nbounds, i.e. x ∈ Xtable, with Xtable defining the table region.\nFig. 4: Percentage improvement in terms of cost suboptimality (top) and\nnumber of SCP iterations (bottom) with respect to REL achieved by\nwarm-starting the SCP with FT-TTO and TTO. Each bar represents the\nimprovement averaged over non-convexity factors greater or equal to the\ncorresponding x-axis value.\nB. Open-loop Planning\nAs a first experiment, we aim to solve the trajectory generation\nproblem in an open-loop setting. As shown in Fig. 2, this results\nin a single optimal trajectory that can be robustly tracked by a\ndownstream control system. Specifically, as a form of compar-\nison, we warm-start the SCP using trajectories generated by (i)\nsolving the REL relaxation of Problem (9) [17], [24] (ii) the\ntransformer before fine-tuning (TTO), and (iii) the transformer\nafter fine-tuning (FT-TTO). To initialize the generation process,\nwe set the performance parameterR1 to the negative cost of the\nREL solution and C1 = 0 . For each scenario, we evaluate tra-\njectory generation performance across 40,000 random problem\nspecifications generated as in Sec. V-A and ordered according to\na non-convexity factorcomputed as Cj\n1\nCmax\n, whereCj\n1 is the number\nof KOZ constraint violations observed in the REL solution to\neach problem specificationj ∈ [1, 40,000] and Cmax =maxj Cj\n1\nis the maximum constraint violation observed in the scenario. In\npractice, we use the non-convexity factor as a direct metric of the\ndifficulty of the considered scenario, whereby scenarios with a\nfactor of 0 represent OCPs for which there exists a convex solu-\ntion and hence the solution of REL is optimal for the full problem;\nwhereas scenarios with a factor of 1 represent OCPs for which\nthe closest convex relaxation is highly infeasible and where a\nbetter selection of the warm-start can have a larger impact on the\nsolution to the non-convex problem.\nResults in Fig. 4 confirm the findings of [1] and show a clear\nadvantage in warm-starting the non-convex problem with TTO,\nwhich strongly outperforms initial guesses based on problem\nrelaxations, both in terms of cost and number of SCP iterations.\nAdditionally, Fig. 4 clearly highlights the benefits of the pro-\nposed fine-tuning strategy on open-loop performance. In partic-\nular, Fig. 4 (top) shows how FT-TTO is able to reduce the sub-\noptimality gap with respect to the lower bound when compared\nto the model before fine-tuning: FT-TTO achieves approximately\na 2x cost improvement compared to TTO in the case of space-\ncraft RPOD (peak value75%) and quadrotor control (peak value\n20%), while resulting in similar cost performance in the free flyer\ntestbed simulation. Furthermore, in all the scenarios considered,\nwarm-starting with transformers consistently reduces the num-\nber of SCP iterations when compared to initial guesses based on\nREL. Specifically, Fig. 4 (bottom) shows how both TTO and FT-\nTTO achieve at least a 10% reduction in SCP iterations across\nall scenarios and non-convexity levels, achieving up to40% im-\nCELESTINI et al.: TRANSFORMER-BASED MODEL PREDICTIVE CONTROL: TRAJECTORY OPTIMIZATION VIA SEQUENCE MODELING 7\nFig. 5: Average normalized cost increment with respect to the problem\nlower bound.\nFig. 6: Average maximum runtime per single MPC iteration. The run-\ntime is defined as the total computation time needed to generate the\nwarm-start and solve the OCP for the time horizon of interest.\nprovement for high values of the non-convexity factor. Crucially,\nthe substantial cost reduction and the comparable results in SCP\niterations demonstrate that, from an open-loop perspective, the\nfine-tuning of FT-TTO does not cause any degradation in perfor-\nmance, improving open-loop behavior.\nC. Model Predictive Control\nThe second part of our experiments focuses on solving the\ntrajectory generation problem within the MPC scheme. We com-\npare two state-of-the-art approaches with our transformer-based\nframework for the generation of the warm-start and the defi-\nnition of the terminal cost JT(xh+H, ¯x) = ∥xh+H − ¯x∥2:\n(i) REL-MPC solves the full-horizon REL problem and defines\n¯x as the state obtained by the REL solution at time th+H, i.e.\n¯x := x(REL)\nh+H [24], (ii) DIST-MPC defines JT to encode the\ndistance from the goal, i.e.¯x := xgoal, and uses the REL solution\nto this short-horizon problem as warm-start [26], (iii) TTO-MPC\nand (iv) FT-TTO-MPC employ the transformer models before\nand after DAGGER fine-tuning, respectively, to generate a short-\nhorizon warm-start and define¯x := ˆxh+H.\nFor each of the three simulation scenarios, we evaluate all\nmethods on 500 problem instances uniformly distributed across\nthe non-convexity factor. Each trajectory is discretized in 100\ntimesteps and solved according to ten different planning hori-\nzons, ranging from 10 to 100 in increments of 10. To facili-\ntate comparison across tasks, we normalize costs to the range\nbetween 0 and 1 by computing the normalized cost increment\nas cost - lowerbound\nmaxcost - lowerbound. A normalized cost increment of 0 cor-\nresponds to a trajectory that matches the performance of the\nlower bound. A normalized cost increment of 1 corresponds to\na trajectory that matches the worst performance observed across\nall planning horizons. Finally, the trajectory generation process\nof transformer models is initialized setting the parameters R1\nand C1 as in Sec. V-B. Aggregate results in Fig. 5 and 6 show\nhow FT-TTO-MPC is able to substantially outperform all other\nFig. 7: Qualitative representation of the experimental results obtained\nemploying FT-TTO (cyan) and REL (blue) in the open-loop (dashed)\nand MPC (solid) settings. Videos: https://transformermpc.github.io.\napproaches both in terms of cost and runtime1. In particular, Fig.\n5 highlights a few key take-aways. First, due to the short-sighted\ndefinition of the terminal cost, DIST-MPC exhibits the biggest\nperformance degradation, showcasing the limits of heuristically\ndefined terminal costs. Second, TTO-MPC (trained solely open-\nloop data) is highly affected by the covariate shift problem, which\nseverely impacts its closed-loop performance causing it to per-\nform worse than REL-MPC. Lastly, FT-TTO-MPC showcases\nthe importance of fine-tuning transformer models for MPC, re-\nsulting in a significative advantage over all the other methods.\nIn particular, the cost induced by FT-TTO-MPC remains stable\nacross different planning horizons, achieving results with H =\n10 that are on par with other methods withH = 30 and allowing\nfor the solution of substantially smaller OCPs.\nResults in Fig. 6 further highlight the runtime benefits of FT-\nTTO-MPC, which outperforms other non-learning-based meth-\nods across all planning horizons. Crucially, by evaluating run-\ntime results in the context of their cost performance from Fig. 5,\nsome relevant findings emerge. In the context of REL-MPC,\nplanning with H = 30 is necessary to achieve performance\ncomparable to the one obtained by FT-TTO-MPC withH = 10.\nComparing the corresponding computational times, we see how\nthe long-term guidance introduced by FT-TTO leads to approxi-\nmately a 7x reduction in runtime for the same performance.\nD. Real-world Experiments\nFinally, we perform experiments evaluating the real-world ef-\nfectiveness of our approach through the free flyer testbed de-\nscribed in Sec. V-A. Specifically, we use the cumulative firing\ntime of the eight thrusters of the free flyer as a performance metric\nto compare FT-TTO and REL. In the open-loop setting, we use\na PID controller to track the solution obtained by warm-starting\nthe SCP with FT-TTO and REL, respectively. In the MPC frame-\nwork, we directly control the free flyer using FT-TTO-MPC and\nREL-MPC with a planning horizonH = 10.\nFig. 7 and Tab. I show qualitative and quantitative results\nfor the free flyer experiments, focusing on highly non-convex\nOCPs. In the open-loop setting (O-L), FT-TTO facilitates the\nconvergence to a solution with a reduced number of sharp turns\n(Fig. 7)—and thus, propellant consumption (Tab. I)—clearly\noutperforming REL. In the MPC framework, FT-TTO-MPC\nleads to a closed-loop execution that closely mimics the open-\nloop solution, limiting the increase in propellant consumption\nto approximately 20%, i.e., a value in line with the numerical\n1Computation times are from a Linux system equipped with a 4.20GHz pro-\ncessor and 128GB RAM, and a NVIDIA RTX 4090 24GB GPU.\n8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\nTABLE I: Quantitative results of experimental tests shown in Fig. 7.\nMethod Target acquisition Frequency Total firing\ntime [s] [ Hz] time [s]\nO-L REL 40.0 - 99.70\nFT-TTO 40.0 - 75.78\nMPC REL-MPC 80.0 1.25 83.76\nFT-TTO-MPC 40.0 2.5 91.34\nanalysis in simulation. On the other hand, REL-MPC results\nin a terminal cost that aggressively steers the free flyer toward\nthe target, leading to significant deviations from the open-loop\nsolution and abrupt turns in the proximity of the obstacles. Fur-\nthermore, in our experiments, it was necessary to double the\ntarget acquisition time and halve the control frequency to have\nREL-MPC successfully reach the target while accounting for\nhigher computation times and preventing collisions—a scenario\nwe encountered consistently during experimentation with REL-\nMPC. As a result, the increased target acquisition times lead to\nan overall lower firing time, as shown in Tab. I. Crucially, FT-\nTTO-MPC was the only MPC algorithm able to be deployed\nwithin real-world scenarios that did not require loosened time\nconstraints.\nVI. C ONCLUSIONS\nDespite its potential, the application of learning-based meth-\nods for the purpose of trajectory optimization has so far been\nlimited by the lack of safety guarantees and characterization of\nout-of-distribution behavior. At the same time, methods relying\non numerical optimization typically lead to high computational\ncomplexity and strong dependency on initialization. In this work,\nwe address these shortcomings by proposing a framework to\nleverage the flexibility of high-capacity neural network models\nwhile providing the safety guarantees and computational effi-\nciency needed for real-world operations. We do so by defin-\ning a structure where a transformer model—trained to generate\nnear-optimal trajectories—is used to provide an initial guess and\na predicted target state as long-horizon guidance within MPC\nschemes. This decomposition, coupled with a pre-training-plus-\nfine-tuning learning strategy, results in substantially improved\nperformance, runtime, and convergence rates for both open-loop\nplanning and model predictive control formulations.\nIn future works, we plan to extend our formulation to handle\nmulti-task stochastic optimization problems employing gener-\nalizable scene and input representations. Specifically, current\nlimitations, such as unmodeled and out-of-distribution dynam-\nics, will be addressed through tailored learning strategies (e.g.\ndata augmentation and meta-learning of dynamics models) and\nstructured treatment of uncertainties. Moreover, while we have\ninvestigated the impact of fine-tuning based on (iterative) super-\nvised learning in this work, the exploration of other fine-tuning\nstrategies (e.g., based on reinforcement learning) is a highly\npromising direction that warrants future work. More generally,\nwe believe this research opens several promising directions to\nleverage the power of learning-based methods for trajectory op-\ntimization within safety-critical robotic applications.\nREFERENCES\n[1] T. Guffanti, D. Gammelli, S. D’Amico, and M. Pavone, “Transformers for\ntrajectory optimization with application to spacecraft rendezvous,” inIEEE\nAerospace Conference, 2024.\n[2] J. Alonso-Mora, S. Samaranayake, A. Wallar, E. Frazzoli, and D. Rus,\n“Advances in trajectory optimization for space vehicle control,” Annual\nReviews in Control, vol. 52, 2021.\n[3] D. Mellinger and V . Kumar, “Minimum snap trajectory generation and\ncontrol for quadrotors,” inProc. IEEE Conf. on Robotics and Automation,\n2011.\n[4] M. Mohanan and A. Salgoankar, “A survey of robotic motion planning\nin dynamic environments,” Robotics and Autonomous Systems, vol. 100,\n2018.\n[5] M. Morari and J. H. Lee, “Model predictive control: past, present and\nfuture,” Computers & Chemical Engineering, vol. 23, 1999.\n[6] J. T. Betts, “Survey of numerical methods for trajectory optimization,”\nAIAA Journal of Guidance, Control, and Dynamics, vol. 21, 1998.\n[7] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge Univ.\nPress, 2009.\n[8] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of\ndeep visuomotor policies,”Journal of Machine Learning Research, vol. 17,\nno. 1, pp. 1–40, 2016.\n[9] C. Wu, A. Rajeswaran, Y . Duan, V . Kumar, A. Bayen, S. Kakade, I. Mor-\ndatch, and P. Abbeel, “Variance reduction for policy gradient with action-\ndependent factorized baselines,” inInt. Conf. on Learning Representations,\n2018.\n[10] J. Zhang, C. Ni, Z. Yu, C. Szepesvari, and M. Wang, “On the convergence\nand sample efficiency of variance-reduced policy gradient method,” in\nConf. on Neural Information Processing Systems, 2021.\n[11] B. Ichter, J. Harrison, and M. Pavone, “Learning sampling distributions for\nrobot motion planning,” inProc. IEEE Conf. on Robotics and Automation,\n2018.\n[12] S. Bansal, V . Tolani, S. Gupta, J. Malik, and C. Tomlin, “Combining opti-\nmal control and learning for visual navigation in novel environments,” in\nConf. on Robot Learning, 2020.\n[13] T. Lew, S. Singh, M. Prats, J. Bingham, J. Weiszet al., “Robotic table wip-\ning via reinforcement learning and whole-body trajectory optimization,” in\nProc. IEEE Conf. on Robotics and Automation, 2023.\n[14] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel,\nA. Srinivas, and I. Mordatch, “Decision transformer: Reinforcement learn-\ning via sequence modeling,” in Conf. on Neural Information Processing\nSystems, 2021.\n[15] M. Janner, Q. Li, and S. Levine, “Offline reinforcement learning as one big\nsequence modeling problem,” in Conf. on Neural Information Processing\nSystems, 2021.\n[16] C. Chi, S. Feng, Y . Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song,\n“Diffusion policy: Visuomotor policy learning via action diffusion,” in\nRobotics: Science and Systems, 2023.\n[17] S. Banerjee, T. Lew, R. Bonalli, A. Alfaadhel, I. A. Alomar, H. M. Shageer,\nand M. Pavone, “Learning-based warm-starting for fast sequential convex\nprogramming and trajectory optimization,” in IEEE Aerospace Confer-\nence, 2020.\n[18] A. Cauligi, P. Culbertson, B. Stellato, D. Bertsimas, M. Schwager, and\nM. Pavone, “Learning mixed-integer convex optimization strategies for\nrobot planning and control,” inProc. IEEE Conf. on Decision and Control,\n2020.\n[19] S. W. Chen, T. Wang, N. Atanasov, V . Kumar, and M. Morari, “Large\nscale model predictive control with neural networks and primal active sets,”\nAutomatica, vol. 135, p. 109947, 2022.\n[20] B. Amos, I. D. J. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter, “Differ-\nentiable MPC for end-to-end planning and control,” in Conf. on Neural\nInformation Processing Systems, 2018.\n[21] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter,\n“Differentiable convex optimization layers,” inConf. on Neural Informa-\ntion Processing Systems, 2019.\n[22] N. Rajaraman, L. F. Yang, J. Jiao, and K. Ramchandran, “Toward the\nfundamental limits of imitation learning,” inConf. on Neural Information\nProcessing Systems, 2020.\n[23] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning and\nstructured prediction to no-regret online learning,” in Proc. Int. Conf. on\nArtificial Intelligence and Statistics, 2011.\n[24] G. Alcan and V . Kyrki, “Differential dynamic programming with nonlinear\nsafety constraints under system uncertainties,”IEEE Robotics and Automa-\ntion Letters, vol. 7, no. 2, pp. 1760–1767, 2022.\n[25] A. W. Koenig, T. Guffanti, and S. D’Amico, “New state transition matrices\nfor spacecraft relative motion in perturbed orbits,” Journal of Guidance,\nControl, and Dynamics, vol. 40, no. 7, pp. 1749–1768, 2017.\n[26] H. Nguyen, M. Kamel, K. Alexis, and R. Siegwart, “Model predictive\ncontrol for micro aerial vehicles: A survey,” in 2021 European Control\nConference (ECC), 2021, pp. 1556–1563.",
  "topic": "Model predictive control",
  "concepts": [
    {
      "name": "Model predictive control",
      "score": 0.7122558355331421
    },
    {
      "name": "Transformer",
      "score": 0.5332567095756531
    },
    {
      "name": "Trajectory",
      "score": 0.5144586563110352
    },
    {
      "name": "Computer science",
      "score": 0.48315709829330444
    },
    {
      "name": "Control theory (sociology)",
      "score": 0.44433942437171936
    },
    {
      "name": "Trajectory optimization",
      "score": 0.42673003673553467
    },
    {
      "name": "Engineering",
      "score": 0.2635727524757385
    },
    {
      "name": "Control (management)",
      "score": 0.2566853165626526
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1673678457736969
    },
    {
      "name": "Voltage",
      "score": 0.11332365870475769
    },
    {
      "name": "Physics",
      "score": 0.09851887822151184
    },
    {
      "name": "Electrical engineering",
      "score": 0.06208494305610657
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177477856",
      "name": "Polytechnic University of Turin",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I13805885",
      "name": "Vaughn College of Aeronautics and Technology",
      "country": "US"
    }
  ],
  "cited_by": 11
}