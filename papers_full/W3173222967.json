{
    "title": "On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers",
    "url": "https://openalex.org/W3173222967",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5077889745",
            "name": "Tianchu Ji",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A5057502103",
            "name": "Shraddhan Jain",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A5010051312",
            "name": "Michael Ferdman",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A5061612911",
            "name": "Peter Milder",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A5046253607",
            "name": "H. Andrew Schwartz",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A5101768349",
            "name": "Niranjan Balasubramanian",
            "affiliations": [
                "Stony Brook University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3038035611",
        "https://openalex.org/W3098576111",
        "https://openalex.org/W3015233032",
        "https://openalex.org/W4288337707",
        "https://openalex.org/W3034487470",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W2970777192",
        "https://openalex.org/W2977944219",
        "https://openalex.org/W3114304470",
        "https://openalex.org/W3153147196",
        "https://openalex.org/W3104263050",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W3100985894",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W2516141709",
        "https://openalex.org/W3105238007",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4251575795",
        "https://openalex.org/W2799051177",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W4287667694",
        "https://openalex.org/W3170233084",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W3127151792",
        "https://openalex.org/W2985671716",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2947946877",
        "https://openalex.org/W2972498556",
        "https://openalex.org/W2995446988",
        "https://openalex.org/W3098873988",
        "https://openalex.org/W2950784811"
    ],
    "abstract": "How much information do NLP tasks really need from a transformer's attention mechanism at application-time (inference)?From recent work, we know that there is sparsity in transformers and that the floating-points within its computation can be discretized to fewer values with minimal loss to task accuracies.However, this requires retraining or even creating entirely new models, both of which can be expensive and carbon-emitting.Focused on optimizations that do not require training, we systematically study the full range of typical attention values necessary.This informs the design of an inference-time quantization technique using both pruning and logscaled mapping which produces only a few (e.g. 2 3 ) unique values.Over the tasks of question answering and sentiment analysis, we find nearly 80% of attention values can be pruned to zeros with minimal (< 1.0%) relative loss in accuracy.We use this pruning technique in conjunction with quantizing the attention values to only a 3-bit format, without retraining, resulting in only a 0.8% accuracy reduction on question answering with fine-tuned RoBERTa.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4147–4157\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4147\nOn the Distribution, Sparsity, and Inference-time Quantization of\nAttention Values in Transformers\nTianchu Ji1, Shraddhan Jain2, Michael Ferdman2,\nPeter Milder1, H. Andrew Schwartz2, and Niranjan Balasubramanian2\n1,2Stony Brook University\n1{tianchu.ji, peter.milder}@stonybrook.edu\n2{shrjain, mferdman, has, niranjan}@cs.stonybrook.edu\nAbstract\nHow much information do NLP tasks really\nneed from a transformer’s attention mecha-\nnism at application-time (inference)? From\nrecent work, we know that there is sparsity\nin transformers and that the ﬂoating-points\nwithin its computation can be discretized to\nfewer values with minimal loss to task accu-\nracies. However, this requires retraining or\neven creating entirely new models, both of\nwhich can be expensive and carbon-emitting.\nFocused on optimizations that do not require\ntraining, we systematically study the full range\nof typical attention values necessary. This in-\nforms the design of an inference-time quanti-\nzation technique using both pruning and log-\nscaled mapping which produces only a few\n(e.g. 23) unique values. Over the tasks of ques-\ntion answering and sentiment analysis, we ﬁnd\nnearly 80% of attention values can be pruned\nto zeros with minimal ( < 1.0%) relative loss\nin accuracy. We use this pruning technique in\nconjunction with quantizing the attention val-\nues to only a 3-bit format, without retraining,\nresulting in only a 0.8% accuracy reduction on\nquestion answering with ﬁne-tuned RoBERTa.\n1 Introduction\nWhile the verdict is still out on which large lan-\nguage model will prove best, at this point in time,\nall contenders rely on multi-headed attention over\nmultiple layers. Many have investigated whether\nattention (the output of the softmax, α) itself is\nqualitatively sensible (e.g., correlating with lin-\nguistic aspects) (Vig and Belinkov, 2019; Clark\net al., 2019; V oita et al., 2018, 2019; Kovaleva\net al., 2019; Rogers et al., 2020) or how useful it\nis for interpreting models (Jain and Wallace, 2019;\nWiegreffe and Pinter, 2019; Brunner et al., 2020;\nRogers et al., 2020). Others have focused on in-\nducing sparsity in the attention: whether some of\nthe structural components (the softmax function,\nattention heads and layers) introduce attention spar-\nsity (Correia et al., 2019; Michel et al., 2019; V oita\net al., 2019; Sajjad et al., 2020), if the model tends\nto focus on a small amount of tokens (Clark et al.,\n2019; Ramsauer et al., 2020), and the interpretabil-\nity of such sparsity (Chen et al., 2020; Rogers et al.,\n2020). Yet, little is known about our ability to\ninduce sparsity or reduce its values at application-\ntime, and what role the inherent sparsity could play\nin building inference-time efﬁcient transformers.\nThis work focuses on a systematic study of\nthe quantitative distribution of the attention values\nacross the layers and heads as well as the potential\nfor reducing the information content of attention\nvalues during inference at application-time1. We\nconsider two popular pretrained transformer mod-\nels: BERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019) over tasks of Masked Language Mod-\neling as well as question answering and sentiment\nanalysis. We explore the attention distributions on\nthe different models and tasks, and quantitatively\nproﬁle the sparse attention that commonly exists\nin the transformer model. Motivated by the high\nlevels of inherent sparsity in these distributions,\nwe design a pruning and quantization technique\nand test the limits of information necessary from\nattention.\nWe ﬁnd that most attention values can be pruned\n(i.e. set to zero) and the remaining non-zero values\ncan be mapped to a small number of discrete-levels\n(i.e. unique values) without any signiﬁcant impact\non accuracy. Approximately 80% of the values\ncan be set to zero without signiﬁcant impact on\nthe accuracy for QA and sentiment analysis tasks.\nFurther, when we add quantization utilizing a log-\nscaling, we ﬁnd a 3-bit discrete representation is\nsufﬁcient to achieve accuracy within 1% of using\nthe full ﬂoating points of the original model.\n1Our analyzing code and data are available at\nhttps://github.com/StonyBrookNLP/spiqa\n4148\n2 Method\nTo analyze attention distribution we ﬁrst plot his-\ntograms of attention values for BERT (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019) models. We\nalso compute a sparsity distribution using the pro-\nportion of the attention values smaller than a given\nthreshold. For attention pruning, we ﬁnd attention\nvalues that are below a speciﬁed threshold and re-\nplace them with zero. We experiment with different\nthresholds. For quantization to k-bits we map the\ncontinuous attention values to one of 2k real val-\nues2. We use two methods: (i) Linear - Bin the\nattention values to 2k quantiles and set the mid-\npoint of each as the quantized value. (ii) Log - Bin\nthe log transformed attention values and pick the\nmid-point of each on the log scale as the quantized\nvalue. The quantization methods are explained in\ndetail in Appendix E.\nWe apply these inference-time (i.e. no training)\ntechniques on three tasks: masked language mod-\neling, question answering and sentiment analysis.\nFor QA we used BERT 3 and RoBERTa4 models\nﬁne-tuned on SQuAD v1.1 (Rajpurkar et al., 2016).\nFor sentiment analysis we used RoBERTa 5 ﬁne-\ntuned on the SST-2 dataset (Socher et al., 2013).\nFor both these tasks we report accuracy on the\ncorresponding development sets. For the Masked\nLanguage Modeling (MLM) task we report pseudo-\nperplexity (Salazar et al., 2020) computed on the\nHuggingface Wikipedia dataset6.\n3 Evaluation\nAttention distribution and sparsity. A thor-\nough quantitative analysis on the attention distri-\nbution could help build efﬁcient transformers by\nproviding useful information, such as the degree\nof sparsity and the range of the attention values.\nWe plot the histogram of each token’s attention to\nall the others (αi) and provide three examples of\nthe heads in Figure 1 to investigate the density of\nthe attention values, how differently the tokens at-\ntend to others in the same attention head, and how\nsparse a token/head/layer’s attention can be. We\nﬁnd that, for most of the heads, attention forms\na lognormal-like distribution similar to Figure 1a.\n2Note here we use full precision ﬂoating point rather than\na k-bit value since our main goal is to see how many discrete\nlevels of attention is needed.\n3http://huggingface.co/csarron/bert-base-uncased-squad-v1\n4http://huggingface.co/csarron/roberta-base-squad-v1\n5http://huggingface.co/textattack/roberta-base-SST-2\n6https://huggingface.co/datasets/wikipedia\nOn some heads, some of the attention for query to-\nken (αi) have more tiny attention values (αij ) and\ninduce more sparsity than others (like in Figure 1c).\nWe also observe entire heads with high sparsity, in\nwhich nearly all tokens only slightly attend to oth-\ners (like in Figure 1b). Our observation conﬁrms\nthe existence of sparsity in the attention heads.\nA key motivation for us is to quantitatively char-\nacterize sparsity, especially in terms of how much\npotential there is in reducing the information con-\ntent in attention values. To this end, we speciﬁcally\nmeasure the proportion of small attention values\nby counting the number of αij that sum up to 0.5\nin each αi. This indicates that most heads focus\nstrongly on fewer than 10 tokens on average (de-\ntails in Appendix A), leading to notable sparsity\nand suggesting large potential for conveying the\nsame information as continuous attention values\nusing fewer discrete levels.\nBeyond these, we occasionally observe outlier\nattention histograms (like the outliers between\n[10−4,10−1] in Figure 1b). We also found notice-\nable differences on the attention histograms from\nlayer to layer. These ﬁndings are related to the\nworks on the syntactic heads/special tokens (V oita\net al., 2019; Kovaleva et al., 2019; V oita et al., 2018;\nClark et al., 2019; Rogers et al., 2020)) and the dif-\nferences of the layers/heads (Correia et al., 2019;\nClark et al., 2019). We discuss how our ﬁndings\nrelate to them in Appendices B and C.\nLimited effect of near-zero attention values dur-\ning inference. The inherent sparsity we observed\nmotivates us to explore the sparsity of attention at\ninference-time—how much attention can be pruned\nduring inference, without impacting the model ac-\ncuracy? By setting up a series of pruning thresh-\nolds, we clamp different proportions of the atten-\ntion to zero and examine how attention sparsity\naffects the accuracy, on both pretrained and ﬁne-\ntuned models. The results shown in Figure 2 in-\ndicate that the sparsity can grow above 80% with\nonly a 0.1%–1.3% drop in accuracy. Speciﬁcally,\nthe pretrained BERT model achieves 99.9% of\nthe original performance with 87% of the sparsity\non Masked Language Modeling. By comparing\nRoBERTa’s accuracy on different tasks, we ﬁnd\nthat sentiment analysis suffers more from increased\nsparsity, suggesting that different models are dif-\nferentially sensitive to the induced sparsity. Our\nresults quantitatively show how much sparsity can\nbe induced in all the attention values without losing\n4149\n(a) Layer 1 Head 4\n (b) Layer 2 Head 3\n (c) Layer 12 Head 11\nFigure 1: Normalized histograms (in blue) and cumulative histograms (in red) for every token’s attention to others\n(αi) at different heads in the pretrained RoBERTa model, starting from 10−8. The histograms show different\npatterns of attention distribution. E.g., in (b) many tokens’ attention form an evenly distributed histogram from\n10−8 to 1, and most of the αi have 80%–100% of all the attention values ( αij ) ≤10−8. This indicates a higher\nlevel of sparsity compared to (a) and (c). The “sparsity distribution” bar on the right shows the density of αi to\neach level of sparsity. E.g., the red cell with “0.96” between 0.9–1.0 in (b) means 96% of all αi have sparsity\nbetween 90%–100%, whereas the sparsity is the proportion of αij in αi that are less than 10−8.\n0.00 0.25 0.50 0.75 1.00\nsparsity\n0\n20\n40\n60\n80\n100EM score\nQA\nRoBERTa SQuAD\nBERT SQuAD\n0.00 0.25 0.50 0.75 1.00\nsparsity\n0\n20\n40\n60\n80\n100accuracy\nSA\nRoBERTa SST-2\n0.00 0.25 0.50 0.75 1.00\nsparsity\n0\n5\n10\n15\n20\n25 pseudo-perplexity\nMLM\nRoBERTa MLM\nBERT MLM\nFigure 2: Exact Match score (for QA), Accuracy (for SA) and pseudo-perplexity (for MLM) under different levels\nof sparsity that we induce, showing that on these models and tasks ∼80% of the sparsity can be induced with\nlimited performance drop. X-axis values denotes the induced sparsity levels measured as the proportion of the\nattention values less than a speciﬁed threshold.\naccuracy, suggesting that one can expect to prune\nup to 80% of the attention values without retrain-\ning.\nQuantizing pruned attention. Quantization is\noften used to compress transformer models for\nhigher computational and memory efﬁciency. Re-\ncently Prato et al. (2020) showed that for machine\ntranslation, attention values in transformers can\nbe quantized with only a small impact on accu-\nracy. While their results suggest that full precision\nattention values may not be necessary for high ac-\ncuracy, it is unclear if one can retain the accuracy in\ninference-time quantization in general settings i.e.,\nwithout retraining. Bhandare et al. (2019); Shen\net al. (2020); Prato et al. (2020) have proved the im-\nportance of meticulously selecting the range of the\nquantization when pursuing higher accuracy. Intu-\nitively, pruning the tiny attention values will lead\nto a narrower quantization range with more precise\nvalue representatives. For example, if allα< 10−3\nare pruned before 3-bit quantization, all numbers\nwe need to quantize will land in [10−3,1] rather\nthan [0,1], with the 8 quantiles of the quantization\nlocated more densely; this forms a higher resolu-\ntion within the quantization range compared to the\nnon-pruned version. Since we observed that prun-\ning most of the attention values during inference\nhas minimal effect on the accuracy when removing\nonly the tiny attention values ( α <10−3 in our\ncase), we hypothesize that properly pruning atten-\ntion values will help increase the accuracy of the\nquantized model.\nTo verify the pruning hypothesis, we selected\ntwo quantization methods: linear scale quantiza-\ntion and logarithmic scale quantization (details in\n4150\n16\n0\n20\n40\n60\n80\nRoBERTa\nBERT\n123456789\nRoBERTa\nBERT\n#bits\nEM score\nRoBERTa-linear\nRoBERTa-linear-pruned\nRoBERTa-log\nRoBERTa-log-pruned\nBERT-linear\nBERT-linear-pruned\nBERT-log\nBERT-log-pruned\nRoBERTa-boolean\nBERT-boolean\n(a) EM scores of the models with differ-\nently quantized attention\n10 6\n 10 3\n 100\npruning threshold\n50\n60\n70\n80\n90Accuracy\noriginal\nRoBERTa-SST\n10 6\n 10 3\n 100\npruning threshold\n0\n20\n40\n60\n80EM score\noriginal\nRoBERTa-SQuAD\n10 6\n 10 3\n 100\npruning threshold\n102\n104\n106\n108\npseudo-perplexity\noriginal\nRoBERTa-MLM\n(b) performance with different pruning thresholds for 2-bit log quantization\nFigure 3: Performance of the quantized models with/without attention pruning, showing that the attention can be\neffectively quantized to as low as 3 bits with certain pruning thresholds. (a) Exact Match scores for the QA with\ndifferent quantization methods on ﬁne-tuned BERT and RoBERTa. “Boolean” quantization is provided as the ex-\ntreme case of quantization to a single bit. The pruning has only negligible effect on the linear scale quantization so\nthat “*-linear” and “*-linear-pruned” curves are highly overlapped. (b) Accuracy of the ﬁne-tuned RoBERTa mod-\nels with 2-bit quantized attention for QA, SA and MLM respectively. The attention is pruned before quantization\nby using different thresholds (shown on the x-axis). In all the ﬁgures, the original model’s performance scores are\nmarked with black dashed lines.\nAppendix E), quantized only the transformers’ at-\ntention with various number of bits, and measured\nthe accuracy of the models. Then we repeated the\nexperiment but pruning α< 10−3 (which creates\n∼80% sparsity with limited accuracy drop in our\nsparsity experiment) before quantizing the atten-\ntion.\nWe evaluate the models on different tasks to com-\npare how pruning the attention affects the accuracy\nwhen quantizing. Results in Figure 3a show that for\nboth BERT and RoBERTa models, log quantization\nis greatly improved after pruning, especially with\nthe 3-bit and 2-bit quantization. Notably, the 3-bit\nlog quantization with pruning only loses 0.8% and\n1.5% of the original accuracy for the RoBERTa\nand BERT, respectively. Contrarily, the pruning\nhas very limited effect on the linear quantization\nbecause the selected pruning threshold results only\nin a negligible change to the effective quantiza-\ntion range. (Details are provided in Appendix F.)\nWe also repeated the experiment on other tasks\nand found 2-bit log quantization with pruning only\nloses 0.7% accuracy on RoBERTa ﬁne-tuned for\nsentiment analysis. (Full results are provided in\nAppendix D.)\nWe further experimented with different pruning\nthresholds (Figure 3b) and observed that pruning\nα< 10−2 gives the best performance; the thresh-\nold can undermine model accuracy if it is either too\nlarge (>10−2) or too small (<10−3).\nOur results prove that pruning the sparse atten-\ntion values helps recover model accuracy with log-\nscale quantization methods, without any retrain-\ning or ﬁne-tuning. With attention pruning, a trans-\nformer can retain a comparable amount of accuracy\neven with a simple, low-precision quantized atten-\ntion (in our case, a 3-bit log quantization).\nDiscussion. Sparsifying the attention can help re-\nduce both the computation and memory cost of\nself-attention during inference. Our experiments\nabove demonstrate that it is possible to prune ap-\nproximately 80% of attention values while quan-\ntizing them to a 3-bit representation. Specialized\nhardware (FPGA and ASIC) can be designed to ef-\nﬁciently operate on highly quantized datatypes and\nto “skip” the zeros to accelerate deep learning infer-\nence, such as Albericio et al. (2016) (which targets\nCNNs). Our results show that such an accelerator\ncould effectively reduce the arithmetic cost of com-\nputing attention matrices by 80% and reduce the\nmemory footprint of the attention matrices by up to\n96% (compounding the effect of sparse representa-\ntion and quantization). Although attention matrices\nare not occupying a huge amount of storage, these\nmemory savings can potentially greatly increase\nthe efﬁciency of a specialized hardware accelera-\ntor by reducing its on-chip SRAM usage and/or\nits memory bandwidth requirement. Further, the\ncomputational savings can help reduce the latency.\nLastly, it is important to note that the beneﬁts of\nattention sparsity may extend much further than\njust computing attention values themselves; other\ncomputations in the transformer network can also\n4151\nbeneﬁt from leveraging the high degree of sparsity\nwithout retraining/ﬁne-tuning, potentially yielding\nlarger beneﬁts. Future work will investigate the\ncomputational beneﬁts of utilizing attention spar-\nsity and the design of customized hardware accel-\nerators to efﬁciently do so.\n4 Related Work\nAttention distribution. Many have abstractly\nstudied the attention distribution from different\naspects (Clark et al., 2019; Pascual et al., 2021;\nRamsauer et al., 2020; Correia et al., 2019), but\nnone speciﬁcally have shown the histogram of the\nαi directly, nor did they investigate the sparse at-\ntention values quantitatively. Correia et al. (2019)\nindicated that not all of the sparsity in attention\nwas caused by the softmax, and it remained unclear\nwhether such sparsity affected accuracy (which is\ninspected in this paper).\nPruning. V oita et al. (2019); Sajjad et al. (2020);\nMichel et al. (2019); Kovaleva et al. (2019) pruned\none or more heads/layers resulting in comparable\nor higher model accuracy, either with or without\nﬁne-tuning. These approaches assume that some\nheads/layers interpret the information redundantly,\nwhich is not always true (Brunner et al., 2020;\nRogers et al., 2020). In contrast, our work focuses\non a more general method of inducing attention\nsparsity without operating at layer/head granular-\nity.\nQuantization. Bhandare et al. (2019); Shen et al.\n(2020); Prato et al. (2020) have shown beneﬁts\nfrom selecting the quantization range, which mo-\ntivates us to prune the attention before quantiza-\ntion (Section 3). Kim et al. (2021); Zafrir et al.\n(2019); Prato et al. (2020) required re-training\nwhile ours does not. Zhang et al. (2020); Bai et al.\n(2020); Zadeh et al. (2020) focused on quantizing\nthe weights rather than the attention values, which\nis out of our scope.\nSparse transformers and attention visualiza-\ntion Parmar et al. (2018); Child et al. (2019);\nHo et al. (2019); Beltagy et al. (2020); Ainslie et al.\n(2020); Li and Chan (2019); Tay et al. (2020) have\nproposed/summarized various kinds of efﬁcient\ntransformers utilizing induced attention sparsity.\nHowever, none of them quantitatively analyzed the\nstatistical distribution and the tiny values of the at-\ntention. Vig (2019); Hoover et al. (2020) proposed\ninstance-level attention visualization tools. These\nare complementary to our quantitative visualization\nof the distributions of all attention values.\n5 Conclusion\nWe demonstrated that pruning near-zero values and\nlarge reductions in the number of bits needed for at-\ntention, even at application time without retraining\nor ﬁne-tuning, is possible with little loss of accu-\nracy. This suggests attention plays a very coarse\nrole in model accuracy at inference-time, yielding\nopportunities to run transformers more efﬁciently\nover applications. While quantization during train-\ning had previously shown promise (down to three\nbits, for most weights of the transformer), we ob-\nserved the same reduction potential on attention\nvalues at application-time, allowing their represen-\ntation to be reduced down to three bits (or even two\nfor sentiment) with little effort (e.g., without re-\ntraining or using a dynamic quantization range).\nThis shows it is feasible to implement efﬁcient\ntransformers by leveraging heavily sparse and quan-\ntized attention values, suggesting the possibility\nof building specialized hardware (e.g., FPGA and\nASIC accelerators) to optimize the transformer’s\nevaluation on-the-ﬂy.\nAcknowledgments\nWe would like to express our appreciation to\nAdithya V . Ganesan who assisted with our experi-\nments.\nThis material is based upon work supported by\nthe National Science Foundation under Grant Nos.\n2007362 and 1918225. The experiments were con-\nducted with equipment purchased through NSF\nGrant No. OAC-1919752.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding Long and Structured Inputs\nin Transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284, Online. Asso-\nciation for Computational Linguistics.\nJ. Albericio, P. Judd, T. Hetherington, T. Aamodt,\nN. E. Jerger, and A. Moshovos. 2016. Cn-\nvlutin: Ineffectual-Neuron-Free Deep Neural Net-\nwork Computing. In 2016 ACM/IEEE 43rd Annual\nInternational Symposium on Computer Architecture\n(ISCA), pages 1–13. ISSN: 1063-6897.\n4152\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin,\nXin Jiang, Qun Liu, Michael Lyu, and Irwin King.\n2020. BinaryBERT: Pushing the Limit of BERT\nQuantization. arXiv:2012.15701 [cs] . ArXiv:\n2012.15701.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The Long-Document Transformer.\narXiv:2004.05150 [cs]. ArXiv: 2004.05150.\nAishwarya Bhandare, Vamsi Sripathi, Deepthi\nKarkada, Vivek Menon, Sun Choi, Kushal Datta,\nand Vikram Saletore. 2019. Efﬁcient 8-Bit Quan-\ntization of Transformer Neural Machine Language\nTranslation Model. arXiv:1906.00532 [cs]. ArXiv:\n1906.00532.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\nhofer. 2020. On Identiﬁability in Transformers. In\nInternational Conference on Learning Representa-\ntions.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The Lottery Ticket Hypothesis for\nPre-trained BERT Networks. In Advances in Neural\nInformation Processing Systems , volume 33, pages\n15834–15846. Curran Associates, Inc.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating Long Sequences with\nSparse Transformers. arXiv:1904.10509 [cs, stat] .\nArXiv: 1904.10509 version: 1.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What Does BERT\nLook at? An Analysis of BERT’s Attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nGonc ¸alo M. Correia, Vlad Niculae, and Andr ´e F. T.\nMartins. 2019. Adaptively Sparse Transformers. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2174–\n2184, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn,\nand Tim Salimans. 2019. Axial Attention in Multi-\ndimensional Transformers. arXiv:1912.12180 [cs].\nArXiv: 1912.12180.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2020. exBERT: A Visual Analysis Tool\nto Explore Learned Representations in Transformer\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 187–196, Online. As-\nsociation for Computational Linguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W.\nMahoney, and Kurt Keutzer. 2021. I-BERT: Integer-\nonly BERT Quantization. arXiv:2101.01321 [cs] .\nArXiv: 2101.01321.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the Dark Secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nLala Li and William Chan. 2019. Big bidirectional in-\nsertion representations for documents. In Proceed-\nings of the 3rd Workshop on Neural Generation and\nTranslation, pages 194–198, Hong Kong. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre Sixteen Heads Really Better than One? In Ad-\nvances in Neural Information Processing Systems ,\nvolume 32, pages 14014–14024. Curran Associates,\nInc.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image Transformer. In Proceed-\nings of the 35th International Conference on Ma-\nchine Learning, volume 80 of Proceedings of Ma-\nchine Learning Research, pages 4055–4064, Stock-\nholmsm¨assan, Stockholm Sweden. PMLR.\nDamian Pascual, Gino Brunner, and Roger Watten-\nhofer. 2021. Telling BERT’s full story: from Local\nAttention to Global Aggregation. arXiv:2004.05916\n[cs]. ArXiv: 2004.05916.\nGabriele Prato, Ella Charlaix, and Mehdi Reza-\ngholizadeh. 2020. Fully Quantized Transformer for\nMachine Translation. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\n4153\npages 1–14, Online. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2383–2392,\nAustin, Texas. Association for Computational Lin-\nguistics.\nHubert Ramsauer, Bernhard Sch ¨aﬂ, Johannes Lehner,\nPhilipp Seidl, Michael Widrich, Thomas Adler,\nLukas Gruber, Markus Holzleitner, Milena Pavlovi´c,\nGeir Kjetil Sandve, Victor Greiff, David Kreil,\nMichael Kopp, G¨unter Klambauer, Johannes Brand-\nstetter, and Sepp Hochreiter. 2020. Hopﬁeld Net-\nworks is All You Need. arXiv:2008.02217 [cs, stat].\nArXiv: 2008.02217.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the As-\nsociation for Computational Linguistics , 8(0):842–\n866. Number: 0.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and\nPreslav Nakov. 2020. Poor Man’s BERT: Smaller\nand Faster Transformer Models. arXiv:2004.03844\n[cs]. ArXiv: 2004.03844.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and\nKatrin Kirchhoff. 2020. Masked Language Model\nScoring. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2699–2712, Online. Association for Computa-\ntional Linguistics.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W. Mahoney, and\nKurt Keutzer. 2020. Q-BERT: Hessian Based Ultra\nLow Precision Quantization of BERT. Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\n34(05):8815–8821. Number: 05.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient Transformers: A Survey.\narXiv:2009.06732 [cs] . ArXiv: 2009.06732 ver-\nsion: 1.\nJesse Vig. 2019. A Multiscale Visualization of At-\ntention in the Transformer Model. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations,\npages 37–42, Florence, Italy. Association for Com-\nputational Linguistics.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing the\nStructure of Attention in a Transformer Language\nModel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76, Florence, Italy. As-\nsociation for Computational Linguistics.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-Aware Neural Machine Trans-\nlation Learns Anaphora Resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing Multi-Head\nSelf-Attention: Specialized Heads Do the Heavy\nLifting, the Rest Can Be Pruned. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention\nis not not Explanation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP) , pages 11–20, Hong Kong,\nChina. Association for Computational Linguistics.\nJohn Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Dur-\nrani, Fahim Dalvi, and James Glass. 2020. Simi-\nlarity Analysis of Contextual Word Representation\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4638–4655, Online. Association for Computa-\ntional Linguistics.\nA. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos.\n2020. GOBO: Quantizing Attention-Based NLP\nModels for Low Latency and Energy Efﬁcient Infer-\nence. In 2020 53rd Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO) , pages\n811–824.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8Bit BERT.\narXiv:1910.06188 [cs]. ArXiv: 1910.06188.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternary-\nBERT: Distillation-aware Ultra-low Bit BERT. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 509–521, Online. Association for Computa-\ntional Linguistics.\n4154\nA Consistency of Inducing Sparsity in\nthe Attention\nBecause the softmax function normalizes its in-\nput into a probability distribution that sums to 1\nand larger values are projected to larger probabili-\nties, when highly focused tokens with close-to-one\nprobability appear in the attention, they must be\naccompanied by a large number of near-zero atten-\ntion values like in Figure 1b. Thus, the number of\nclose-to-one attention values not only represents\nhow many tokens are strongly attended, but also\nwhether αi has many near-zero attention values.\nTo quantitatively evaluate the proportion of these\ntiny attention values, we computed the number of\nthe largest values in each αi that sum to 0.5, visu-\nalizing their mean and standard deviation in Fig-\nure 4. On both pretrained RoBERTa and SQuAD-\nﬁne-tuned RoBERTa, we observed that most of the\nheads require on average fewer than ten attention\nvalues to sum up to 0.5, meaning that most heads\nfocus strongly on fewer than ten tokens on average,\nleading to notable sparsity. We observe that seven\nof twelve heads in the ﬁrst layers of both models\nhave a larger average number (>10) of such major\ntokens. For deeper layers, the average number of\nmajor tokens decreases. Finally, in the last two lay-\ners, we again see an increasing trend in the average\nnumber of major tokens. This indicates that middle\nlayers commonly focus on only a small number of\ntokens, making these layers rich in sparsity. This\nconﬁrms the “sparse deeper layers” identiﬁed by\nCorreia et al. (2019); Clark et al. (2019) and fur-\nther proves the existence of heavily focused tokens.\nIt implies the large potential of inducing sparsity\nin the transformers and motivates us to explore\nhow these sparse attention values contribute to the\nmodel accuracy. We also examined the BERT pre-\ntrained model and SQuAD-ﬁne-tuned model, and\nwe found behavior similar to RoBERTa. Figure 4\nshows the average of major tokens in the pretrained\nBERT and SQuAD-ﬁne-tuned BERT.\nB Dispersion of Attention Histograms\nComparing the attention histograms in the lower\nlayers and the higher layers in RoBERTa (exam-\nples shown in Figure 5a and 5b respectively), we\nfound that the higher layers have more cumulative\nhistograms “dispersed” along the x-axis. Together\nwith the increasing variance of the number of ma-\njor tokens in the last two layers shown in Figure 4,\nsuch a distribution pattern evidently expresses the\ngreatly dissimilar sparsity among all the αi in the\nhead. As a quantitative analysis, we deﬁne the\ndispersion of the αi distribution in a head as the\nstandard deviation of the index of the cumulative\nhistogram bin reaching 0.5. The dispersion ex-\npresses the dissimilarity of the αi histogram. Note\nthat this is different from the standard deviation\nshown in Figure 4, as the dispersion is measuring\nthe histograms of the attention, but not the attention\nvalues themselves.\nWe measure the dispersion at each head along\nthe layers for both pretrained and ﬁne-tuned\nRoBERTa models. Figure 5c illustrates the changes\nin dispersion along the layers in the RoBERTa\nmodels. In pretrained RoBERTa and its SQuAD-\nﬁne-tuned version, the deep layers generally have\nhigher dispersion. The difference between these\ntwo models is mainly in layer 11, where the pre-\ntrained model has a dispersion drop. RoBERTa\nﬁne-tuned for SST-2 does not show this trend. On\nthe BERT models, dispersion rarely increases along\nthe layers (shown in Figure 5d). The last layers\nhave been proved to be task-speciﬁc (Wu et al.,\n2020; Rogers et al., 2020), and their attention can\nlargely change after ﬁne-tuning (Kovaleva et al.,\n2019). This potentially explains why we observed\ndifferent dispersion behavior on different tasks, but\nneeds further investigation.\nC Heads with Outlier Attention\nDistribution\nOn some heads, a small portion of the tokens forms\nan attention histogram cluster separate from the\nmajority, clearly showing a dissimilarity between\nthese two types of distributions. For example, in\nFigure 1b, we observe a small number of tokens\nclustered on the right of the majority, between\n[10−4,10−2]. Here we list all the heads with such\npattern:\n• Pretrained RoBERTa: Layer 1: head 8, head\n10, head 12; Layer 2: head 3, head 5, head\n10; Layer 3: head 2, head 10; Layer 4: head\n4, head 9; Layer 5: head 2, head 7, head 10;\nLayer 6: head 5, head 11, head 12; Layer 7:\nhead 3; Layer 8: head 7\n• Pretrained BERT: Layer 3: head 10; Layer 5:\nhead 5\nWe found that on these heads, the functional\nwords/tokens and punctuation exhibit distributions\nthat are signiﬁcantly different from other tokens.\nFor example, tokens such as <s>, </s>, and,\n4155\nlayer 1 layer 2 layer 3 layer 4 layer 5 layer 6 layer 7 layer 8 layer 9 layer 10 layer 11 layer 12\n0\n20\n40\n60\n80\naverage #tokens \nto majority\nSQuAD RoBERTa\nPretrained RoBERTa\n(a) RoBERTa\nlayer 1 layer 2 layer 3 layer 4 layer 5 layer 6 layer 7 layer 8 layer 9 layer 10 layer 11 layer 12\n0\n20\n40\n60\n80\naverage #tokens \nto majority\nSQuAD BERT\nPretrained BERT\n(b) BERT\nFigure 4: Mean and standard deviation of the number of tokens’ attentions needed to cover a majority (i.e. sum\nto 0.5) of attention densities in both pretrained and SQuAD-ﬁne-tuned RoBERTa/BERT models. Different layers\nare distinguished by different colors. In each layer the error bar represents the mean and std of head 1, head 2, ... ,\nhead 12 from the left to the right respectively.\n(a) Layer 1 head 1\n (b) Layer 12 head 1\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n2\n3\n4\n5\n6\n7\naverage diversity \nof all heads\nRoBERTa\nRoBERTa-SQuAD\nRoBERTa-SST2\n(c) Average dispersion of attention per layer in\nRoBERTa\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0\n1\n2\n3\n4\n5\naverage diversity \nof all heads\nBERT\nBERT-SQuAD\n(d) Average dispersion of attention per layer in\nBERT\nFigure 5: Attention distribution dispersion in different layers. Pretrained RoBERTa has more spread attention\ndistributions in layer 12 than in layer 1. In (c), the pretrained and SQuAD-ﬁne-tuned RoBERTa models exhibit\nincreasing dispersion in deeper layers, while RoBERTa ﬁne-tuned for SST-2 does not show such a trend.\n: and . are outliers in the pretrained RoBERTa\nmodel and [SEP] and [CLS] are outliers in the\npretrained BERT model. We also noticed these\ntokens’ attention histograms could gather together\nlike the majority of the tokens do, to form either\na less sparse histogram cluster or more sparse his-\ntogram cluster, implying that on some heads, the\nfunctional words/tokens must be treated differently\n4156\n(a) Layer 2 head 3, <s>, le, and, : and </s>\nform a weak, less sparse cluster.\n(b) Layer 4 head 4, <s> and . form a weak, more\nsparse cluster\nFigure 6: A small portion of the tokens cluster outside\nof the majority of the attention’s cumulative histogram\nin RoBERTa. Such tokens are noted in different colors\nwith their token strings ( <s> and </s> are the “start\nof instance” and “end of instance” tokens, respectively),\nwhile other tokens are in black as dashed lines.\nfrom the other tokens when exploring efﬁciency by\nutilizing sparsity. In Figure 6, we illustrate the at-\ntention histogram of such tokens. Our observation\nconﬁrms that the special tokens and punctuation\ncan be heavily attended (V oita et al., 2018; Clark\net al., 2019; Kovaleva et al., 2019; Rogers et al.,\n2020). As a complement, we observed that it does\nnot necessarily mean that the special tokens’ at-\ntention are always more sparse than other tokens’\nattention.\nD Quantization with Pruned Attention\nfor SA and MLM\nWe provide the performance of different quantiza-\ntion methods with and without attention pruning\non the BERT and RoBERTa models tested on SA\nand MLM in Figure 7.\n16\n50\n60\n70\n80\n90\noriginal\n2468\noriginal\n#bits\naccuracy\nRoBERTa-linear\nRoBERTa-linear-pruned\nRoBERTa-log\nRoBERTa-log-pruned\nRoBERTa-boolean\n(a) sentiment analysis\n16\n101\n102\n103\n104\n105\noriginal\nBERT\n2468\noriginal\nBERT\n#bits\npseudo-perplexity\nRoBERTa-linear\nRoBERTa-linear-pruned\nRoBERTa-log\nRoBERTa-log-pruned\nBERT-linear\nBERT-linear-pruned\nBERT-log\nBERT-log-pruned\n(b) masked language modeling\nFigure 7: Performance of the quantized models\nwith and without pruning in advance for BERT and\nRoBERTa models on SA and MLM tasks.\nE Quantization Methods and Their\nEffectiveness\nQuantization methods. In Section 3, we imple-\nmented two different quantization methods. Algo-\nrithms 1 and 2 list their pseudo code.\nQuantization and attention distributionBhan-\ndare et al. (2019) suggested analyzing the distri-\nbution to improve the quantization-effort-intensive\nfunctions like softmax (which generates the atten-\ntion values). Based on this, we assume that the\ntransformer model will perform better if its quan-\ntized attention values are distributed similarly to\nthe unquantized distribution. By measuring the\naverage Jensen-Shannon divergence between the\noriginal αi histogram and its quantized version, we\nfound that the logarithmic quantization has lower\ndivergence from the original attention distribution\ncompared to the linear quantization (see Table 1).\nWhile in our quantization experiment, the loga-\nrithmic quantization indeed achieves higher perfor-\n4157\nAlgorithm 1:Linear quantization\ninput : att←attention values;\nk←number of bits used for quantization;\nt←pruning threshold\noutput :res←quantized attention values\nquantile size = (1 −t)/2k;\nset quantized value as middle point of quantile:\nquantile size/2;\nres=ﬂoor(att / quantile size) * quantile size +\nquantized value + t;\nset attention values less than quantile size+t as zeros;\nAlgorithm 2:Log quantization\ninput : att←attention values;\nk←number of bits used for quantization;\nt←pruning threshold\noutput :res←quantized attention values\nwhen not pruning att, choosing a small value 10−10\nfor t;\nif pruning att then\nquantile size = (0 −log(t))/(2k −1);\nelse\nquantile size = (0 −log(t))/(2k)\nset quantized value as middle point of quantile:\nquantile size/2;\ncompute exponent of res: exp res=ﬂoor((log(att) −\nlog(t))/quantile size)*quantile size+quantized value+t;\nres=power(2, exp res);\nset values less than the ﬁrst quantile boundry in the\nres as zeros;\nmance than the linear quantization on most num-\nbers of bits. This result indicates that selecting\nthe quantization method with less divergence from\nthe original attention distribution could improve\nthe performance. However, the lower divergence\nbetween the quantized and original attention dis-\ntribution does not necessarily relate to the model\nperformance once we introduce pruning. In Ta-\nble 1, even though the histogram’s divergence of\nthe pruned log quantization is higher than the un-\npruned one, pruning still helps get better results.\nWe hypothesize that the pruning enlarged the dis-\nsimilarity between the attention histograms, but\nsuch a change did not affect the accuracy since it\nonly happened to the near-zero attention values.\nF Limited Accuracy Change on the\nLinear Quantization with/without\nPruning\nIn Figure 3a we observed similar performance of\nthe linear quantized attention models before and\nafter pruning. It is worth noting that the pruning\nthreshold we selected, α< 10−3, is already a tiny\nvalue on the linear scale with respect to the range\nquantization method pruned un-pruned\nlinear 0.67 0.67\nlog 0.58 0.55\nTable 1: Average Jensen-Shannon divergence between\nthe histogram of original αi and its 3-bit quantized val-\nues, evaluated on 100 samples from SQuAD Dev-1.1.\nLog quantization, which has lower divergence from the\noriginal attention distribution, retains more accuracy\nfrom the original model.\nof the attention values [0,1]. As a result, pruning\nwill not signiﬁcantly narrow the quantization range,\nas it does for the log-scale quantization. Thus the\nlinear quantization has nearly the same effective\nquantized range with or without pruning, making it\nnearly impossible for the pruned linear quantized\nmodel to outperform the un-pruned one. This can\nbe veriﬁed by the fact that the Jensen-Shannon\nDivergence of the linear quantized attention and\nthe original attention’s histogram are the same with\nor without pruning in Table 1.\nG Experiment reproducibility\nAll evaluation is done on a server with the follow-\ning speciﬁcations:\n• CPU: Intel(R) Xeon(R) Silver 4216, 64 cores\n• GPU: Quadro RTX 8000\n• RAM: 377GB\nThe average runtime of the model inferences\nthrough the entire dataset is ∼4 hours, for differ-\nent tasks. All datasets used in our experiment are\nbased on English. The SQuAD tests are evaluated\non 10570 sentences from the SQuAD Dev-v1.1\ndataset. The SST2 tests are evaluated on 872 in-\nstances from the GLUE validation dataset. The\nMasked Language Modeling tests are evaluated\non 480 paragraphs from the wikipedia training set,\neach having one random, unrepeated token masked\nfor 15–25 iterations."
}