{
  "title": "Tensor network language model",
  "url": "https://openalex.org/W2766674064",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225582752",
      "name": "Pestun, Vasily",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225587899",
      "name": "Vlassopoulos, Yiannis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2079216472",
    "https://openalex.org/W1480966681",
    "https://openalex.org/W2002855434",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2016407890",
    "https://openalex.org/W2088527461",
    "https://openalex.org/W2246296081",
    "https://openalex.org/W2107674601",
    "https://openalex.org/W3103690389",
    "https://openalex.org/W2019588402",
    "https://openalex.org/W1998236996",
    "https://openalex.org/W1498437823",
    "https://openalex.org/W2684886669",
    "https://openalex.org/W895719331",
    "https://openalex.org/W2087148916",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W3103758373",
    "https://openalex.org/W2118564312",
    "https://openalex.org/W3100056737",
    "https://openalex.org/W3105432754",
    "https://openalex.org/W3100624102",
    "https://openalex.org/W1832989093",
    "https://openalex.org/W1889142700",
    "https://openalex.org/W3099321628",
    "https://openalex.org/W1890000358",
    "https://openalex.org/W3106268039",
    "https://openalex.org/W3207342693",
    "https://openalex.org/W2319680621",
    "https://openalex.org/W2040940389",
    "https://openalex.org/W2037768897",
    "https://openalex.org/W2036463035",
    "https://openalex.org/W2052810501",
    "https://openalex.org/W2152471738",
    "https://openalex.org/W2418689459",
    "https://openalex.org/W2530117613",
    "https://openalex.org/W2090558193"
  ],
  "abstract": "We propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. The model is based on directed acyclic graph decorated by multi-linear tensor maps in the vertices and vector spaces in the edges, called tensor network. Such tensor networks have been previously employed for effective numerical computation of the renormalization group flow on the space of effective quantum field theories and lattice models of statistical mechanics. We provide explicit algebro-geometric analysis of the parameter moduli space for tree graphs, discuss model properties and applications such as statistical translation.",
  "full_text": "TENSOR NETWORK LANGUAGE MODEL\nVASILY PESTUN AND YIANNIS VLASSOPOULOS\nAbstract. We propose a new statistical model suitable for machine learning of systems\nwith long distance correlations such as natural languages. The model is based on directed\nacylic graph decorated by multi-linear tensor maps in the vertices and vector spaces in\nthe edges, called tensor network. Such tensor networks have been previously employed for\neﬀective numerical computation of the renormalization group ﬂow on the space of eﬀective\nquantum ﬁeld theories and lattice models of statistical mechanics. We provide explicit\nalgebro-geometric analysis of the parameter moduli space for tree graphs, discuss model\nproperties and applications such as statistical translation.\nContents\n1. Introduction 2\n1.1. Previous work 4\n1.2. Acknowledgements 5\n2. Quantum statistical models 5\n2.1. Quantum states 5\n2.2. Pure state statistical model 6\n2.3. Learning the model 7\n3. Isometric tensor network model 8\n3.1. Isometric maps 8\n3.2. Multi-linear isometric maps and tensor vertices 9\n3.3. Directed multigraphs 10\n3.4. Directed acyclic graphs and props 11\n3.5. Isometric pure state tensor network model 12\n3.6. Slicing and layers 13\n4. Geometry of the moduli space 14\n4.1. Tree ﬂag variety 14\n5. Learning the network and sampling 16\n5.1. Learning 16\n5.2. Sampling 16\n6. Discussion 17\n6.1. Supervised model and classiﬁcation tasks 17\n6.2. Translation of natural languages 17\n6.3. Network architecture 18\n6.4. Testing the model 18\nReferences 18\nInstitut des Hautes ´Etudes Scientifiques (IH ´ES), Bures-sur-Yvette, France\nE-mail addresses: {pestun,yvlassop}@ihes.fr .\n1\narXiv:1710.10248v2  [cs.CL]  30 Oct 2017\n2 TENSOR NETWORK LANGUAGE MODEL\n1. Introduction\nIt must be recognized that the notion\nprobability of a sentence is an entirely\nuseless one, under any known\ninterpretation of this term.\nNoam Chomsky, 1969\nIn natural language processing, unsupervised statistical learning of a language aims to\nconstruct an eﬃcient approximation to the probability measure on the set of expressions in\nthe language learned from a sampling data set.\nCurrently, neural network models have proved to be the most eﬃcient. A particular\nsuccess is attributed to models which construct distributed word representations [1–3], that\nis a function v : W ÑV from the set of words W in a language to a real vector space V of\na certain dimension depending on a particular model.\nImpressive results for constructing such a function v, also called continuous vector repre-\nsentation, have been achieved in [4, 5] (see also [6] for earlier construction of neural network\nlanguage model and [7] for continuous space word representations). A model in [5] constructs\nvector representation vpwqof a word wPW by training a predictor of words within a certain\nrange of win the training sample of language. Curiously, the function v constructed in [4, 5]\nwas found to satisfy interesting semantic and syntactic linear relations in English language\nsuch as\nvpapplesq´ vpappleq» vpcarsq´ vpcarq» vpfamiliesq´ vpfamilyq\nencoding the syntactic concept of grammatical number,\nvpSpainq´ vpMadridq» vpFranceq´ vpParisq» vpItalyq´ vpRomeq\nencoding the semantic concept of capital of a country,\nvpkingq´ vpqueenq» vpmanq´ vpwomanq» vpuncleq´ vpauntq\nencoding the semantic concept of gender.\nThe diﬃculties in the statistical modeling of language are due to the long range correlation.\nIn [8] mutual information1 Iplqbetween two characters in the text was measured as a function\nof the distance l between positions of the characters in two samples of English Literature\n(“Moby Dick” by Melville and Grimm’s Tales), and it was found that Iplq in the range\n25 ălă1000 is well approximated by the power law\nIplq“ c1l´α `c2 (1.1)\nwith critical exponent α»0.37. Further measurements in [9] showed that the long distance\ncorrelation are due to the structures beyond the sentence level. In [10] it was proposed to\nexplain the long range correlations in the text by a hierarchy of levels in language which\nreminds the hierachical structure/renormalization group ﬂow in the physical theories. The\nanalysis in [11] conﬁrmed long range correlations in the sequence of integers constructed\nfrom the sequence of words in the text, where each word is mapped to a positive integer\nequal to the rank of this word in the sorted list of individual word frequencies. Criticality of\nlanguage is not surprising with abundance of critical phenomena in biology [12].\n1deﬁned in terms of relative entropy\nTENSOR NETWORK LANGUAGE MODEL 3\nMoreover, in [13] it was shown that a language described by a formal probabilistic regular\ngrammar necessarily has mutual entropy function Iplq with exponentially fast decay with\nrespect to the distance l\nIplq» cexpp´mlq (1.2)\nwhere m is the inverse correlation length, or the mass gap in the physics terminology. A\nformal probabilistic regular grammar is almost equivalent to a probabilistic ﬁnite automaton,\nhidden Markov model chain or matrix product state called also tensor train decomposition\nin machine learning (see [14, 15] for more precise statements). These models can be thought\nas tensor networks based on a graph with topology of 1-dimensional chain constructed from\nthree-valent vertices whose output edges correspond to the observables. In the context of\nlanguage a matrix product state model is considered in [16]. The absence of mass gap\n(power law correlation function, criticality, conformal structure, scale invariance) observed\nin the natural language (1.1) implies that 1-dimensional tensor chain models (e.g. hidden\nMarkov model or similar) fail to reproduce correctly the basic statistics of natural language:\nthese models are mass-gapped with exponential decay (1.2) while a natural language is at\ncriticality with the power law decay (1.1). This observation of [13] explains why hidden\nMarkov models or similar do not describe very well natural languages.\nOn the other hand, [13] also showed that a language described by a formal probabilistic\ncontext free grammar (probabilistic regular tree grammar) has mutual entropy function\nfunction Iplq of a power law type. A probabilistic regular tree grammar is modelled by a\ntensor network based on a graph with topology of a tree in which leafs correspond to an\nobservable, see also [17]. The hierarchy tree structure, for example in the case of a binary\ntree, means that from the state vectors of two words we construct the state vector of the\nsentence of two words they form. From state vectors of sentences of two words we construct\nthe state vector of the sentence of four words they form and so on.\nIn physics this corresponds to the idea of iteratively coarse graining a system of many\nlocally interacting variables and the resulting renormalization group ﬂow on the space of\ntheories developed by Kadanoﬀ [18], Wilson [19–21], Fischer [22] and many others (see\ne.g. review [23]). Further in [24, 25] density matrix renormalization algorithm was suggested\nwhich turned out to be quite eﬃcient for numerical solutions of quantum 1-D lattice systems.\nAlgorithms implementing Kadanoﬀ-Wilson-White real space renormalization group on tensor\ntree networks have been developed in [26, 27]. See [28] for a recent survey of tensor network\nmodels.\nHowever, for critical systems the performance of bare tree tensor networks models was\nlimited due to eﬀects of remaining long range entanglement. To handle this entanglement\nthe dimensions of Hilbert spaces in the layers have to grow substantially when moving up to\nthe higher layers in which nodes represent compound objects.\nIn [29] Vidal modiﬁed bare tensor tree network by introducing disentangling operators\nbetween neighbor blocks before applying each step of Kadanoﬀ-Wilson-White [21, 25] density\nmatrix renormalization group projection operation, see Fig. 1. Such tensor tree interlaced\nwith disentaglers is called Multiscale Entanglement Renormalization Ansatz, and it has been\nsuccessfully applied to study numerically many critical systems with impressive precision,\nsee e.g. review in [30]. Intuitively, p-valent tree tensor networks could be thought as discrete\nmodels of AdS/CFT correspondence (holography) [31–33], between the AdS side which is\na discrete gravity theory in discrete hyperbolic geometry represented by p-valent trees, and\n4 TENSOR NETWORK LANGUAGE MODEL\nV 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1\nV 4\nV 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1\nV 4\nV 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1 ⌦ V 1\nV 4\nFigure 1. On the left: bare tree tensor network. On the right: MERA\ndecorated tensor tree network is a map V4 Ñ Vb8\n1 obtained as composition\nof tensors at vertices, where V4 is the vector space associated to the single\nincoming arrow at the top, and Vb8\n1 is the vector space associated to the 8\noutgoing arrows at the bottom. Tensors of type (1,2) realizing maps\nVk`1 ÑVk bVk\nare displayed by triangles, and disentagling tensors of type (2,2) realizing maps\nVk bVk ÑVk bVk\nby squares. The direction of arrows is opposite to the renormalization group\nﬂow. The picture displays base layer of length 8 and the binary tree. In the\nconformal limit the length n of base layer goes to inﬁnity, and the number of\nlayers scales as log n.\nthe CFT side which is a critical (conformal) theory that lives on the boundary of the tree,\nsee e.g.[34].\nMotivated by\n(1) criticality of language [8–12]\n(2) emergence of eﬀective vector structure on the space of linguistic syntactic and se-\nmantic concepts [1, 2, 4, 5, 35]\n(3) real space matrix density renormalization group or discretized holographic AdS/CFT\ncorrespondence realized by tensor trees [18, 19, 25, 26, 29]\nin this paper we propose to use quantum MERA-like tensor networks for a statistical model\nof language or other data sets with observed critical phenomena and long range power law\ntype correlation functions.\n1.1. Previous work. Recurrent neural networks have been shown to have long range cor-\nrelation [13, 36]. The connection between deep learning architectures and renormalization\ngroup has been pointed out in several recent works [37–39]. Moreover, in [40, 41] it was\nshown that an arithmetic circuit deep convolutional network is a particular case of a tree\ntensor network. Linear matrix product states and density matrix renormalization group [42]\nTENSOR NETWORK LANGUAGE MODEL 5\nin the context of image recognition have been analyzed in [43]. On the other hand, deep\nneural networks in [44] have been used to compute correlation functions in the Ising model,\nand [45] deep neural networks have been used to learn a wave-function of a quantum system.\nIn [46] it was suggest to represent topological states with long-range quantum entanglement\nby a neural network, and in [47] it was suggested how to accelerat Monte Carlo statistical\nsimulations with deep neural network. In [48] some analysis has been put for equivalence of\ncertain restricted Boltzmann machines and tensor network states.\nWhile the present manuscript was in preparation we noticed works [17, 49, 50] which\ncontain partial overlap with presented constructions.\n1.2. Acknowledgements. We would like to thank Maxim Kontsevich and John Terilla\nfor useful discussions. The research of V.P. on this project has received funding from the\nEuropean Research Council (ERC) under the European Union’s Horizon 2020 research and\ninnovation program (QUASIFT grant agreement 677368), and research of Y.V. received\nfunding from Simons Foundation Award 385577.\n2. Quantum statistical models\n2.1. Quantum states. Let W be the set of symbols from which a language is constructed.\nFor example, W could be a set of words, a set of syllables, a set of ASCII characters, a set\nt0,1uin the binary representation, a set of musical characters, a set of tokens in computer\nprogramming language, a set of DNA pairs and so on. By w “|W|we denote the number\nof symbols in the set W.\nLet W˚ be the set of sequences of symbols in W, including the empty sequence\nW˚ “>nPZě0Wˆn (2.1)\nwhere Wˆn “ W ˆW¨¨¨ˆ Wlooooooooomooooooooon\nn\n. An element of the set S “ Wn is a sequence of length n\nconsisting of symbols in W.\nLet W “CW »Cw be a vector space over the ﬁeld of complex numbers generated by W.\nElements of W are formal linear combinations of symbols in W with complex coeﬃcients.\nUsing Dirac bra-ket notations one can denote an element ψPW as\nÿ\nwPW\nψw|wy (2.2)\nwhere |wyis a basis element in W labelled by a symbol wPW and ψw is a complex number.2\nAn element in the vector space W is called state.\nFor example, if the set of symbols W is a set of English words, a state ψ PW could be\nequal to\nψ“1 `\n?\n3i\n4 |mountainy`\n?\n3\n2 |hilly (2.3)\nWe equip W with Hermitian metric x,y, that is a positive deﬁnite sesquilinear form, also\ncalled inner product\nx,y: W bW ÑC (2.4)\n2We use the standard conventions to label coeﬃcients of controvariant vectors |ψy PW by upper indices\nand coeﬃcients of covariant vectors xψ| PW_ by lower indices\n6 TENSOR NETWORK LANGUAGE MODEL\nwhere W denotes a vector space complex conjugate to W in such a way that W is the\nstandard basis\nxw|w1y“ δw\nw1, w,w 1 PW (2.5)\nwhere\nδw\nw1 “\n#\n1,w “w1\n0,w ‰w1\nis the Kronecker symbol. In other words, W is a ﬁnite-dimensional Hilbert space with\northonormal basis W. For every state |ψyP W there is an adjoint state xψ|P W_ induced\nby the Hermitian metric x,y.\nFor example, the norm of a state |ψy“ ř\nwψw|wyis\nxψ|ψy“\nÿ\nwPW\n¯ψwψw (2.6)\nwhere ψw denotes complex conjugation of a complex number ψw.\nTo a length n sequence of symbols s “pw1,...,w nqP Wˆn we associate a basis element\n|sy“| w1w2 ...w nyin the vector space\nWbn “W bW¨¨¨b Wloooooooomoooooooon\nn\n(2.7)\nA generic state Ψ PWbn is a linear combination of basis elements with complex coeﬃcients\nΨ “\nÿ\nsPWn\nΨs|sy (2.8)\nAn operator os : Wbn ÑWbn deﬁned as\nos “|syxs| (2.9)\nis the projection operator on the basis element |sy. In particular,\nxΨosΨy“x Ψ|syxs|Ψy“|x s|Ψy|2 “ΨsΨs (2.10)\nis the absolute value square of the coeﬃcient Ψ s, thus it is a real non-negative number. A\nstate Ψ is called normalized if it has unit norm\nxΨ|Ψy“ 1 (2.11)\nFor a normalized state Ψ it holds that ÿ\nsPWˆn\nxΨosΨy“ 1 (2.12)\n2.2. Pure state statistical model. A statistical model on the set Wˆn is a family of\nprobability distributions µ on Wˆn ﬁbered over the base space of parameters U. That is,\nfor each parameter uPU there is a positive real valued function\nµu : Wˆn ÑRě0, u PU (2.13)\nsuch that ÿ\nsPWˆn\nµupsq“ 1, u PU (2.14)\nThe base space U of parameters can be thought as moduli space of distributions in a given\nstatistical model.\nTENSOR NETWORK LANGUAGE MODEL 7\nA quantum (pure state) statistical model on Wˆn with the space of parameters U is a\nfamily of normalized states Ψ PWbn ﬁbered over the base space U. That is for each point\nu in the space of parameters U there is a normalized state Ψ u PWbn:\nxΨu|Ψuy“ 1. (2.15)\nA quantum statistical model Ψ induces classical statistical model µ by the Born rule\nµpsq:“xΨosΨy (2.16)\nIndeed, µpsqis a real non-negative number, and the normalization (2.14) follows from (2.15).\nWe remark that in quantum physics with a Hilbert space of statesH, a U-family of normed\nstates Ψu PH,xΨu|Ψuy“ 1 is often called a wave-function ansatz. A typical problem posed\nin quantum physics is to ﬁnd a ground state of a quantum system, that is an eigenstate with\nthe lowest eigenvalue of a positive deﬁnite Hermitian operator H (Hamiltonian) acting on a\nHilbert space H. Often the exact solution of this problem is not possible, and one resolves\nto approximate methods. A wave-function ansatz is such an approximate method. While\nthe exact ground state problem is equivalent to the minimization problem\nmin\n|ΨyPH,xΨΨy“1\nxΨ|H|Ψy (2.17)\nover the space of all states in H with unitary norm, an approximate solution by an ansatz\nsearches the minimum over, usually, much smaller subset of states tΨu|uPUu\nmin\nuPU\nxΨu|H|Ψuy (2.18)\nSimilarly, using the Born rule induction (2.16) of a statistical model µ from a pure state\nmodel Ψ, we can think about a family of quantum states tΨu,u PUuas a particular case of\nclassical statistical model.\n2.3. Learning the model. Given a statistical model µu on a discrete set S, a statistical\nlearning of a model is to ﬁnd an optimal value u˚ of parameters in U, which means that µu˚\napproximates best an observed distribution ˚µ in a sample of data points in S. Formally, a\nsample of data is a multiset\nS “pS,m : S ÑZě0q\nbased on the set S, that is a set of pairs ps,mpsqqwhere s PS is an observed data point,\nand non-negative integer mpsqP Zě0 is the multiplicity of point s in the sample.\nAn observed distribution ˚µS associated to a sample S is a normalized frequency function\nS ÑRě0 deﬁned by\n˚µSpsq“ mpsq\n|S| (2.19)\nwhere |S|:“ř\nsPS mpsq:“ř\nsPS 1 is the cardinality of the sample S. We use conventions\nwhere ř\nsPS yields sPS with multiplicity mpsq.\nA distance between between two probability distributions ˚ µ and µu can be deﬁned as\nKullback-Leibler (KL) divergence\nDp˚µ||µuq“\nÿ\nsPS\n˚µpsqlog ˚µpsq\nµupsq (2.20)\nThis deﬁnition of distance between two probability distributions satisﬁes certain natural\naxioms in the information theory that also lead to the standard deﬁnition of the entropy of\na distribution.\n8 TENSOR NETWORK LANGUAGE MODEL\nHence, a standard method of learning a statistical model µu from a sample of data points\nS is minimization of the KL divergence between probablity distribution µu and the observed\ndistribution ˚µ, so that the optimal value of the parameters u is\nu˚ “arg min\nuPU\nDp˚µS||µuq (2.21)\nMinimization of KL divergence bewteen ˚ µ and µu is equivalent to maximization of log-\nlikelihood ř\nsPS mpsqlog µupsq, that is\nu˚ “arg max\nuPU\nÿ\nsPS\nlog µupsq (2.22)\nusing the multiset yield notation sPS.\nIn particular, for a quantum pure state statistical model Ψ u and data sample S, the\noptimal value of u˚ PU is\nu˚ “arg max\nuPU\nÿ\nsPS\nplog Ψupsq` log Ψupsqq (2.23)\n3. Isometric tensor network model\nNow we consider a particular pure state statistical model Ψu on the Hilbert spaceH “Wbn\ncalled isometric (or unitary) tensor network model.\n3.1. Isometric maps. Let V,W be any Hermitian vector spaces, then a map\nu: V ÑW (3.1)\nis called isometric embedding (isometry for short) if the Hermitian metric on V is equal\nto the pullback by u of Hermtian metric on W. In other words, for any v,v1 P V and\nw“uv,w1 “uv1 it holds that\nxw|w1yW “xv|v1yV (3.2)\nSince Hermitian metric is non-degenerate\nrank u“dim V (3.3)\nand isometric embedding exists only if dim W ědim V.\nAn equivalent deﬁnition of isometric embedding of Hermitian spaces u: V ÑW is that\nu˚u“1 V (3.4)\nwhere u˚ : W ÑV is the adjoint map and 1 V is the identity map on V. With respect to a\nbasis on V and a basis on W, the isometric property of a map uw\nv means\ng¯vv1 “¯u¯w\n¯vuw1\nv1 g¯ww1 (3.5)\nwhere g¯vv1 and g¯ww1 are components of Hermitian metric on V and W.\nThe deﬁnition (3.4) also implies that the operator uu˚ : W ÑW is a projection, since\npuu˚q2 “uu˚uu˚ “uu˚ (3.6)\nWe can think about morphism u˚ : W ÑV as a projection on the image of u in W.\nA unitary transformation can be deﬁned as an isometry V Ñ V. The set of unitary\ntransformations of V forms a group called unitary group Upvq. In this sense an isometry\nV ÑW is a generalization of the notion of unitary transformation.\nTENSOR NETWORK LANGUAGE MODEL 9\nIn general, the space UV,W of isometries V ÑW is not a group, but a homogeneous space\nisomorphic to the quotient\nUV,W “ Upwq\nUpw ´vq, dimR UV,W “2wv ´v2 (3.7)\nFor example, a normalized state ψ P W can be canonically identiﬁed with an isometry\n˜ψ: C ÑW by taking the image of 1 PC\nψ“ ˜ψp1q (3.8)\nand, indeed, the space of isometric embeddings of C to W is a unit sphere S2w´1 isomorphic\nto the sphere of normalized states\nUC,W “ Upwq\nUpw ´1q“S2w´1 (3.9)\nwhere\nS2w´1 “tψPW | xψψy“ 1u (3.10)\nAlso we deﬁne the reduced space ˜UV,W of isometries from V ÑW that is obtained from\nUV,W by reduction by the unitary group Upvqof automorphisms of the Hermitian space V.\nFor example, if V “C, then\n˜UV,W “ Upwq\nUp1qUpw ´1q“Pw´1 (3.11)\nwhich is a familiar statement from quantum physics that the space of normalized states inW\nup to equivalence by phase rotation forms the complex projective space Pw´1. In generic\ncase\n˜UV,W “ Upwq\nUpvqUpw ´vq“Grv,w, dimC ˜UV,W “vw (3.12)\nwhere Grw,w denotes a Grassmanian of complex v-planes in w-dimensional complex vector\nspace.\nThe reduced space ˜UV,W is an example of moduli spaces of isometric tree tensor networks\nwhich we discuss in more generality in section 4.\n3.2. Multi-linear isometric maps and tensor vertices. For vector spacesV1,V2,..., Vq\nand vector spaces W1,W2,..., Wp, a linear map u of type pp,qqis morphism\nu: Vbq ÑWbp (3.13)\nwhere Vbq “V1 bV2 b¨¨¨b Vq, and Wbp “W1 bW2 b¨¨¨b Wp.\nLet Wi and Vj be the bases of vector spaces Wi and Vj for iPr1,psand j Pr1,qs. Then\ntensor uw\nv of type pp,qqis the table of components of urepresented as a pp`qq-dimensional\narray table with p upper indices and q lower indices\npuw\nvq“p uw1...wp\nv1...vq | wPW1 ˆ¨¨¨ˆ Wp, v PV1 ˆ¨¨¨ˆ Vqq (3.14)\nA Hermitian metric on pWiqiPr1,ps and on pVjqjPr1,qs induces Hermitian metric on Wbp and\nVbq, and then a tensor uw\nv is called an isometry if the map (3.13) is an isometry from Vbq\nto Wbp with respect to the induced Hermitian metric. In the orthonormal basis this meansÿ\nwPW1ˆ¨¨¨ˆWp\n¯u˜v\nwuw\nv “δ˜v\nv, v, ˜vPV1 ˆ¨¨¨ˆ Vq (3.15)\nGraphically we display tensor vertex (3.13) as in ﬁgure 2.\n10 TENSOR NETWORK LANGUAGE MODEL\nV 1 ⌦ V 2 ⌦ V 3\nW 1 ⌦ W 1 ⌦ W 1 ⌦ W 1\nFigure 2. A tensor vertex u: Vb3 ÑWb4\n3.3. Directed multigraphs. Let γ be a directed multigraph in which edges have their\nidentity (sometimes called quiver). More formally, a directed multigraphγis pVert,Edge,s,t q\nwhere Vert is a set of vertices, Edge is a set of edges, s: Edge ÑVert is a source map which\nassociates to each edge its source vertex, andt: Edge ÑVert is a target map which associates\nto each edge its target vertex.\nA tensor network Uγ (without inputs and outputs) is a decoration of a quiver γby a vector\nspace Ve at each edge ePEdge, and linear map ui : bePt´1piqVe ÑbePs´1piqVe at each vertex\niPVert.\nNotice, that unlike the theory of quiver representations, in which vertices are decorated\nby vector spaces, and the edges by maps between vector spaces, in a tensor network edges of\na quiver are decorated by vector spaces, and the vertices are decorated by multi-linear maps\nfrom the tensor product of vector spaces of incoming edges to the tensor product of vector\nspace of outgoing edges. A vertex i decorated by a linear map ui is called tensor vertex.\nTo deﬁne an open tensor networ, or tensor network with a boundary we add a set of\nexternal incoming edges and a set of external outgoing edges. Formally, a quiver γ with a\nboundary is pVert,Edge,In,Out,s,t qwhere Vert is a set of vertices, Edge is a set of internal\nedges, In is a set of incoming edges, Out is a set of outgoing edges, s: Edge \\Out ÑVert is\na source map which associates to each edge its source vertex, and t: Edge \\In ÑVert is a\ntarget map which associates to each edge its target vertex.\nAn open tensor network Uγ “pγ,pupiqqiPVertqis a quiver γ, possibly with a boundary, in\nwhich each edge e P Edge \\In \\Out is decorated by a vector space Ve, and each vertex\niPVert is decorated by a multi-linear map\nupiq : bePt´1piqVe ÑbePs´1piqVe (3.16)\nAn open tensor network pγ,puiqiPVertqdeﬁnes a multi-linear map uγ called evaluation from\nthe tensor product of vector spaces in the In edges to the tensor product of vector space in\nthe Out edges by composition of maps ui\nuγ :\nâ\nePIn\nVe Ñ\nâ\nePOut\nVe (3.17)\nIf the boundary is empty, i.e. tensor network is closed, then uγ is a number.\nTENSOR NETWORK LANGUAGE MODEL 11\nFigure 3. Directed acyclic graph\nIn components, evaluation map uγ is obtained by summing over all pairs of upper and\nlower indices in the product of tensor vertices\nuγ “\nÿ\nvPŚ\nePEdge Ve\nź\niPVert\npuiqvps´1piqq\nvpt´1piqq (3.18)\nWe remark that a tensor networkpγ,Uγqcould be recognized as a Feynman diagram of the\ndirected graph γ for a 0-dimensional ﬁeld theory with a pair of ﬁelds pφe,˜φeqwith φe PVe\nand ˜φe PV_\ne for each edge ePEdge with kinetic term x˜φe,φeyand interaction tensor vertices\n(3.16)\nx\nź\nePt´1piq\n˜φ,ui\nź\nePs´1piq\nφey (3.19)\nAlso, graphical representation of contraction of tensor indices corresponding to the compo-\nsition of multi-linear maps in vertices is known as Penrose graphical notation.\n3.4. Directed acyclic graphs and props. If we assume that directed multigraph γ is\nacyclic (see Figure 3), i.e. γ does not contain any directed cycles, then mathematical struc-\nture that associates to γ a tensor network is called a colored prop [51–59]. A prop is gen-\neralization of the notion of operad. While operad takes several inputs and returns a single\noutput, a prop takes an element of tensor product of several inputs and returns an element\nin the tensor product of several outputs. A tensor network for acyclic directed graph γ is an\nobject in the endomorphism prop of a set of objects in some symmetric monoidal category\nC. In the above deﬁnition (3.16) (3.18) the category C pis a category of ﬁnite-dimensional\nvector spaces over complex numbers with linear morphisms.\nWe remark that if directed multigraphγcontains directed cycles, the corresponding tensor\nnetwork involves trace operation. Formally this structure is encoded in the notion of colored\nwheeled prop. In this situation C could be any symmetric monoidal compact closed category\nso that there is the trace operation in C. Directed cycles in γ correspond to the trace map\nand are called ‘wheels’ in the context of ‘wheeled prop’. A category of ﬁnite-dimensional\nvector spaces with linear morphisms has trace map, and thus it is suitable to build tensor\nnetwork (3.16)(3.18) on arbitrary directed graph.\nIn the context of this paper we are interested in a particular case of tensor networks called\nisometric tensor network.\n12 TENSOR NETWORK LANGUAGE MODEL\nL1\nL2\nL3\nL4\nL5\nFigure 4. Pure state isometric tensor network model\nAn isometric tensor networkpγ,Uγqis a tensor network built on directed acyclic multigraph\nγ in which edges are decorated by Hermitian vector spaces and all multi-linear maps in\nvertices are isometric embeddings. Hermitian vector spaces with isometric linear maps form\na category, since composition of isometries f : V1 Ñ V2 and g : V2 Ñ V3 is an isometry\ng˝f : V1 ÑV3, and this category is symmetric monoidal with the standard tensor product\nof vector spaces. Abstractly, an isometric tensor network can be thought as an object in the\nendomorphism prop of a set of objects from the category of Hermitian vector spaces with\nisometric morphisms. Concretely, this means that if γ is a directed acyclic multigraph and\nall tensor vertices ui are isometries, the evaluation map uγ (3.18) is also an isometry.\n3.5. Isometric pure state tensor network model. Given a Hilbert space Hn “W1 b\nW2 ¨¨¨b Wn, whose basis is a set Wn of length n sequences of symbols in W, a pure state\nisometric tensor network model pγ,Uγqfor a state\nΨ PW1 bW2 ¨¨¨b Wn\nis an isometric tensor network pγ,Uγq: C ÑH with a single input vector space C and output\nvector space Hn “W1 bW2 ¨¨¨b Wn built on n output edges, see\nThe state Ψ equals to the tensor network morphism uγ evaluated on 1 PC\nΨ “uγ1 (3.20)\nNotice, that in general, a directed acyclic graph γ underlying a tensor network is not a\ntree, i.e. there could be multiple directed paths from a node ito a node j (and ‘acyclic’ here\nmeans that directed cycles are not allowed). In particular, MERA-like graph [30, 60] (see\nﬁgure 1) is directed acyclic but not a tree.\nHowever, in a particular case of isometric tree tensor network pγ,Uγq, see Figure 5, it is\ncomputationally easier to evaluate an amplitude of a sequence s\nxs|uγ1y“ x1u˚\nγsy (3.21)\nsince pulling back the state xs|from the lower layer to the top vertex always keeps it in the\nform of tensor product of states on the intermediate edges. However, because of the observed\ncriticality of language the dimension of vector spaces at the top layers needs to grow for a\nsensible model.\nA MERA-like graph with disentagling intermediate vertices V bV Ñ V bV is not a\ntree, and therefore algorithms of the amplitude evaluation are computationally more costly.\nHowever, it is feasible that the dimensions of the vector spaces in the edges at the higher\nTENSOR NETWORK LANGUAGE MODEL 13\nL1\nL2\nL3\nL4\nFigure 5. Pure state tree isometric tensor network model\nlayers of MERA graph do not have to grow as fast [28], and MERA-like tensor model will\nturn out to be computationally more eﬀective.\n3.6. Slicing and layers. In any case, a directed acyclic graph can be always sliced into\nlayers l (see Figures 4 and 5) such that each layer contains vertices which are evaluated in\nparallel by tensor product. Denoting by rlsa set of vertices in layer lwe write urls “biPrlsupiq\nfor a morphism in the layer l, and then the evaluation map of an isometric tensor network\nis a sequential composition of maps starting at the source (or higher) layer ‘L’ with input C\nand ﬁnishing at the target (or lower) layer 1 with output H.\nuγ,r1Ls “ur12s˝¨¨¨˝ urL´1,Ls (3.22)\nThe adjoint map u˚\nγ : H ÑC is evaluated in the reverse order\nu˚\nγ,rL1s “u˚\nrL,L´1s...u ˚\nr21s (3.23)\nwhere now the morphisms u˚\nrls are not isometries in general, but projections if we identify\nthe source of urls with its image in the target space.\nIn particular, the projection operators u˚\nrls do not preserve inner product. Consequently,\nby the analogy with renormalization group ﬂow, we expect that orthogonal commuting\noperators of projections on basis states at the bottom layer (UV scale in renormalization\ngroup/holography terminology), such as projection operators or1s\nor1s “o|largeyb|hilly, o 1\nr1s “o|smallyb|mountainy\nare projected by the linear map to the similar operators ol »o1\nl operating at the middle layer\nl\norls “u˚\nrlLsor1surLls, o 1\nrls “u˚\nrlLso1\nr1surLls (3.24)\nIn the opposite direction, from intermediate higher layer l to the bottom layer 1, under\nthe map\nno2\nr1s “ur1lsorlsu˚\nrl1s »ur1lso1\nrlsu˚\nrl1s, (3.25)\nand in the context of the example we expect to see an operator of the form\no2\nr1s “oc1|largeyb|hilly`c2|smallyb|mountainy (3.26)\nwhere |c1|2,|c2|2 are context-free probabilities of expressing similar concept with diﬀerent\nwords.\n14 TENSOR NETWORK LANGUAGE MODEL\nIn other words, the context free choice between the UV layer expressions |largeyb|hillyvs\n|smallyb|mountainyis irrelevant at a higher level which operates within a Hilbert space Hrls\nof higher level concepts.\nNotice that the renormalization group ﬂow preserves expectation values of relevant oper-\nators, in other words, if we deﬁne a state |ψyl at the intermediate level l as\n|ψyl “ulL|1y (3.27)\nwhere\nuγ,rlLs “url,l`1s˝¨¨¨˝ urL´1,Ls (3.28)\nthen\nxψolψyl “xψo1ψy1 (3.29)\nwhere ol is an image under renormalization group ﬂow of the operator o1 operating at the\nbase layer ‘1’.\nTherefore, the expectation value of o2\nr1s “ oc1|largeyb|hilly`c2|smallyb|mountainy is approximately\nequal to the expectation value of orls or o1\nrls.\n4. Geometry of the moduli space\nLet pγ,Uγqbe an isometric tensor network built on a directed acyclic graph γ. We deﬁne\nautomorphism group (gauge group)\nAutpγ,Uγq“\nź\nePEdge\\In\nUpVeq (4.1)\nto be the group of unitary transformations which act on all incoming and internal edges\npreserving their Hermitian metric, so that tensor vertex upiq transform as\nupiq ÞÑ\n¨\n˝ ź\nePs´1piq\nue\n˛\n‚upiq\n¨\n˝ ź\nePt´1piq\nu´1\ne\n˛\n‚ (4.2)\nunder the action of automorphism group element pueqePEdge\\In,p1 eqePOut.\nThe moduli space of an isometric tensor network is deﬁned as a quotient\nUγ “‘iPVertIsompbePt´1piqVe,bePs´1piqVeq {\nź\nePEdge\\In\nUpVeq (4.3)\nwhere IsompV,W qdenotes the space of isometric maps from Hermitian space V to Hermitian\nspace W.\n4.1. Tree ﬂag variety. If γ is a directed tree graph, the moduli space Uγ has a particular\nsimple algebro-geometric description. First notice, that the constraint that a mapu: V ÑW\nis an isometry between Hermitian spaces V and W\nu˚u“1 V (4.4)\nis a symplectic moment map for UpWq group action on the Kahler space of all maps\nHompV,Wq. By geometric invariant theory, the quotient of the level subset of the con-\nstraint by the action of compact group is isomorphic to the quotient of the (stable locus) of\nthe whole set by the complexiﬁed group\nUu:VÑW “tuPHompV,Wq | u˚u“1 wu{UpVq» HompV,Wqstab{GLpVq (4.5)\nTENSOR NETWORK LANGUAGE MODEL 15\nV 1\nV 2\nV 3\nV 4\nV 5\nFigure 6. A small tree tensor network with moduli space Uγ isomorphic to a\ntree ﬂag variety given by a ﬁbration with Grassmanian ﬁbers Gr pv5,V3 b V4q\nover the base Gr pv3,V1qˆ Grpv4,V2q, where V3, V4 are tautological vector\nbundles over the base.\nIn the present case the stable locus Hom pV,Wqstab is the space of injective maps, and we\nobtain\nUu:VÑW »GrvpWq (4.6)\nwhere Gr vpWq denotes Grassmanian of v-dimensional planes in the vector space W, with\ndimC GrvpWq“ vw ´v2 “pv ´wqv, c.f. (3.12).\nNow, if γ is a directed tree with a single input, then each vertex upiq has a single incoming\ninput edge. Therefore, for a tree tensor network, the set of isometric constraints on the maps\nupiq in each vertex i (4.4) is a level set of symplectic moment map of the action of the full\nautomorphism group (4.1). Consequently,\nUγtree “‘iPVHompVt´1piq,bePs´1piqVeqstab{\nź\niPVert\nGLpVt´1piqq (4.7)\nIn the simplest example, when γ is chain quiver in which each vertex has a single input\nand a single output the moduli space Uγ is explicitly generalized ﬂag variety, i.e. the moduli\nspace of ﬂags\nVin »VL ĂVL´1 Ă¨¨¨Ă V2 ĂV1 »Vout (4.8)\nwhich can be thought as Gr pvL,vL´1q ﬁbered over Gr pvL´1,vL´2q ﬁbered over .... ﬁbered\nover Grpv2,V1q.\nIf γ is a generic directed tree, the moduli space Uγ can be thought as a generalization of\nﬂag variety for linear quiver, and could be called tree ﬂag variety.\nFor the illustration, consider a tree isometric tensor network γ displayed on Figure 6\nIn this case the moduli space Uγ is a ﬁbration over the base Gr pv3,V1qˆ Grpv4,V2qwith\nthe ﬁbers Gr pv5,V3 b V4q where V3, V4 denote tautological vector bundles over Gr pv3,V1q\nand Grpv4,V2q.\nFor general tree isometric tensor network γ, the moduli space Uγ is projective algebraic\nmanifold with has explicit description of a tower of ﬁbrations, where the ﬁber at level l is a\n16 TENSOR NETWORK LANGUAGE MODEL\nproduct of Grassmanians, and then the structure of ﬁbration of products of Grassmanians\nat level l`1 uses external tensor product of the tautological vector bundles of Grassmanians\nat level l according to the combinatorics of tree vertices.\nHence, we deduce that for an isometric tree tensor network the parameter moduli space\nUγ is a smooth algebraic projective Kahler variety of complex dimension\ndimC Uγ “\nÿ\niPVert\n¨\n˝vt´1piq\nź\nePs´1piq\nve ´v2\nt´1piq\n˛\n‚ (4.9)\nOne can expect that the explicit algebro-geometric structure of the moduli space might\nbe useful for optimization algorithms that minimize the target function (2.23).\n5. Learning the network and sampling\n5.1. Learning. For an isometric tensor network pγ,Uγq : C Ñ Hn, where Hn “ Wbn is\nthe Hilbert space based on length n sequences, and a training sample S of strings sPWˆn,\nthe eﬀective free energy function Fpuq that needs to be minimized over the moduli space\nof parameters u P Uγ, is equivalent to the KL divergence between observed probability\ndistribution and the model probability distribution (2.23) and is given by\nFpu|Sq“´\nÿ\nsPS\n2 Re logxs|uγ|1y (5.1)\nwhere x,yis the standard Hermitian metric on Hn, and |syis a basis element in Hn labelled\nby a sequence s PWˆn. The summation over s PS takes each element from the training\nmultiset S with its multiplicity.\nSince the objective function Fpuq is additive over the training sample S, a particular\neﬀective approximate algorithm to minimize Fpuqwith a large sample S is a term-wise local\ngradient descent (called sometimes stochastic gradient descent). This algorithm constructs\na ﬂow on the moduli space Uγ, where each step of the ﬂow for a limited time (called learning\nrate) follows the gradient ﬂow associated to a single term sin the objective function, so that\nevolution of u for a step s is\nBtu“´∇Fpu|sq, t Pr0,ηs (5.2)\nwhere Fpu|sqis\nFpu|sq“´ 2 Re logxs|uγ|1y (5.3)\nOther learning methods based on the recursive singular value decomposition of eﬀecive\ndensity matrices developed in tensor networks applied to many body quantum systems [29,\n30, 60, 61] might turn out also to be eﬀective.\n5.2. Sampling. To sample from a probability distribution of learned isometric tensor net-\nwork a standard recursive procedure can be used.\nNamely, a sequence sPWn is sampled recursively according to the following algorithm.\nThe k-th element of sis recursively sampled from the probability distribution of the k-th\nelement conditioned on the previously sampled k´1 elements of s, that is\nprobpsk|s1 ...s k´1q“ xΨos1...sk Ψy\nxΨos1...sk´1Ψy (5.4)\nTENSOR NETWORK LANGUAGE MODEL 17\nwhere os1...sk denotes the projection operator on a length n sequence with ﬁrst k elements\nﬁxed to be s1 ...s k, i.e.\nos1...sk “os1 b¨¨¨b osk b1 k`1 b¨¨¨b 1 n (5.5)\nSee also [62] addressing sampling in the context of tensor networks.\nFor tree tensor networks, the state ψs “xs|uγ|1ycan be evaluated particularly eﬀectively\nby recursive composition over a leaf of the tree and deleting that leaf. The local term gradient\n(5.1) is also eﬃciently evaluated because of the product structure of the evaluation morphism\n(3.18). Namely, the gradient components for the moduli of parameters in vertexiis computed\nas pulling the tangent bundle for local variation upiq in (3.18) along the composition map in\n(3.18) of all remaining vertices (the pullback or ‘chain rule’ for diﬀerential of composition\nof functions is sometimes referred as ‘back propagation’ in the context of neural network\noptimizations).\n6. Discussion\n6.1. Supervised model and classiﬁcation tasks. A ‘supervised’ version of algorithm is\nalso possible, where a ‘supervised’ label is a relevant operator which survives at the higher\n(IR) levels of the network, such as the general topic of the input text or other general feature\nrelevant operators. We simply add another leaf input to the network at higher level decorated\nby a vector space whose basis is the set of higher level labels.\n6.2. Translation of natural languages. It is expected that various human natural lan-\nguages are in the same critical universality classL at the suﬃciently high level of the network\n(i.e. at the suﬃciently IR scale of the renormalization group ﬂow).\nThen a translation engine from language L1 to language L2 can be constructed by con-\nnecting by unitary transformation S21 at suﬃciently high layers L1 and L2 of two isometric\ntensor networks describing language L1 and language L2\nL2 L1\nL2 L1\nu2\nS21\nu1 (6.1)\nA sequence |sy1 in language L1 is translated to a state |ψy2 in the Hilbert space HL2 of\nlanguage L2 equal to\n|ψy2 “u2S21u˚\n1|sy1 (6.2)\nThe state ψ PHL2, in general, is a not basis element corresponding to a single sequence\nin L2, but rather a linear combination\n|ψy2 “\nÿ\nsPWˆn\n2\nψs|sy2 (6.3)\nwith complex coeﬃcients ψs. If the state |ψy2 is sampled, a basis sequence |sy2 from language\nL2 is generated with probability |ψs|2. As expected, the translation (6.1) is not isomorphism\nbetween languages in the base layer. However, one can conjecture approximate isomorphism,\nor a single universality class of all human languages at some deeper scale of renormalization\ngroup ﬂow L1 » L2 that could be called the scale of ‘meaning’ or ‘thought’. The renor-\nmalization group ﬂow u˚\n1 from the base layer L1 to the ‘meaning’ layer L1 is many to one,\n18 TENSOR NETWORK LANGUAGE MODEL\nprojecting diﬀerent phrases in L1 with equivalent meaning to the same state in HL1. The\nmap u2 from the scale of meaning L2 to the base layer of language L2 is an ordinary isometry\nmap between vector spaces, however, in general, the expansion of the image state |ψy2 over\nthe basis in L2 contains many phrases |sy2 weighted with probability amplitudes. Each of\nthese phrase is a possible translation with corresponding probability |ψs|2.\n6.3. Network architecture. In this note, for simplicity we have assumed a certain ﬁxed\ntopology of an isometric tensor network. However, we expect that there is a natural gener-\nalization of the construction in which the topology of the underlying graph of the model is\nnot ﬁxed, but arbitrary, so that the amplitudes ψs are computed in the spirit of Feynman\ndiagrams where diﬀerent graph topologies appear.\n6.4. Testing the model. The presented construction is theoretical. It would be very in-\nteresting to implement the suggested models and study its performance on various types of\nlanguages that display critical properties (human natural languages, DNA sequences, musical\nscores, etc).\nReferences\n[1] G. E. Hinton, J. L. Mcclelland, and D. E. Rumelhart, “Distributed representations,\nparallel distributed processing: explorations in the microstructure of cognition, vol. 1:\nfoundations,”. 2, 4\n[2] D. E. Rumelhart, G. E. Hinton, R. J. Williams, et. al., “Learning representations by\nback-propagating errors,” Cognitive modeling5 (1988), no. 3 1. 2, 4\n[3] J. L. Elman, “Distributed representations, simple recurrent networks, and grammatical\nstructure,” Machine learning7 (1991), no. 2-3 195–225. 2\n[4] W. Y. T. Mikolov and G. Zweig, “Linguistic regularities in continuous space word\nrepresentation,” Microsoft Research. 2, 4\n[5] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Eﬃcient estimation of word\nrepresentations in vector Space,” ArXiv e-prints(Jan., 2013) 1301.3781. 2, 4\n[6] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language\nmodel,” Journal of machine learning research3 (2003), no. Feb 1137–1155. 2\n[7] H. Schwenk, “Continuous space language models,” Computer Speech & Language21\n(2007), no. 3 492–518. 2\n[8] W. Ebeling and T. P¨ oschel, “Entropy and long-range correlations in literary English,”\nEPL (Europhysics Letters)26 (May, 1994) 241–246, cond-mat/0204108. 2, 4\n[9] W. Ebeling and A. Neiman, “Long-range correlations between letters and sentences in\ntexts,” Physica A: Statistical Mechanics and its Applications215 (1995), no. 3\n233–241. 2, 4\n[10] E. G. Altmann, G. Cristadoro, and M. D. Esposti, “On the origin of long-range\ncorrelations in texts,” Proceedings of the National Academy of Sciences109 (2012),\nno. 29 11582–11587, http://www.pnas.org/content/109/29/11582.full.pdf. 2, 4\n[11] M. A. Montemurro and P. A. Pury, “Long-range fractal correlations in literary\ncorpora,” eprint arXiv:cond-mat/0201139(Jan., 2002) cond-mat/0201139. 2, 4\n[12] T. Mora and W. Bialek, “Are biological systems poised at criticality?,” Journal of\nStatistical Physics144 (July, 2011) 268–302, 1012.2242. 2, 4\n[13] H. Lin and M. Tegmark, “Criticality in formal languages and statistical physics,”\nEntropy 19 (June, 2017) 299, 1606.06737. 3, 4\nTENSOR NETWORK LANGUAGE MODEL 19\n[14] E. Vidal, F. Thollard, C. De La Higuera, F. Casacuberta, and R. C. Carrasco,\n“Probabilistic ﬁnite-state machines-part I,” IEEE transactions on pattern analysis and\nmachine intelligence 27 (2005), no. 7 1013–1025. 3\n[15] E. Vidal, F. Thollard, C. De La Higuera, F. Casacuberta, and R. C. Carrasco,\n“Probabilistic ﬁnite-state machines-part II,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence27 (2005), no. 7 1026–1039. 3\n[16] V. Pestun, J. Terilla, and Y. Vlassopoulos, “Language as a matrix product state,” in\npreparation (2017). 3\n[17] A. J. Gallego and R. Orus, “The physical structure of grammatical correlations:\nequivalences, formalizations and consequences,” ArXiv e-prints(Aug., 2017)\n1708.01525. 3, 5\n[18] L. P. Kadanoﬀ, “Scaling laws for Ising models near T(c),” Physics 2 (1966) 263–272.\n3, 4\n[19] K. G. Wilson, “Renormalization group and critical phenomena. I. Renormalization\ngroup and the Kadanoﬀ scaling picture,” Phys. Rev. B4 (Nov, 1971) 3174–3183. 3, 4\n[20] K. G. Wilson, “Renormalization group and critical phenomena. II. Phase-space cell\nanalysis of critical behavior,” Phys. Rev. B4 (Nov, 1971) 3184–3205. 3\n[21] K. G. Wilson, “The renormalization group: critical phenomena and the Kondo\nproblem,” Rev. Mod. Phys.47 (Oct, 1975) 773–840. 3\n[22] M. E. Fisher, Scaling, universality and renormalization group theory, pp. 1–139.\nSpringer Berlin Heidelberg, Berlin, Heidelberg, 1983. 3\n[23] M. E. Fisher, “Renormalization group theory: its basis and formulation in statistical\nphysics,” Rev. Mod. Phys.70 (Apr, 1998) 653–681. 3\n[24] S. R. White and R. M. Noack, “Real-space quantum renormalization groups,” Phys.\nRev. Lett.68 (Jun, 1992) 3487–3490. 3\n[25] S. R. White, “Density-matrix algorithms for quantum renormalization groups,” Phys.\nRev. B48 (Oct, 1993) 10345–10356. 3, 4\n[26] Y.-Y. Shi, L.-M. Duan, and G. Vidal, “Classical simulation of quantum many-body\nsystems with a tree tensor network,” Phys. Rev. A74 (Aug, 2006) 022320. 3, 4\n[27] L. Tagliacozzo, G. Evenbly, and G. Vidal, “Simulation of two-dimensional quantum\nsystems using a tree tensor network that exploits the entropic area law,” 0903.5017. 3\n[28] P. Silvi, F. Tschirsich, M. Gerster, J. Jnemann, D. Jaschke, M. Rizzi, and\nS. Montangero, “The Tensor Networks Anthology: Simulation techniques for\nmany-body quantum lattice systems,” 1710.03733. 3, 13\n[29] G. Vidal, “Class of Quantum Many-Body States That Can Be Eﬃciently Simulated,”\nPhysical Review Letters101 (Sept., 2008) 110501, quant-ph/0610099. 3, 4, 16\n[30] G. Vidal, “Entanglement Renormalization: an introduction,” ArXiv e-prints(Dec.,\n2009) 0912.1651. 3, 12, 16\n[31] J. M. Maldacena, “The large N limit of superconformal ﬁeld theories and\nsupergravity,” Adv. Theor. Math. Phys.2 (1998) 231–252, hep-th/9711200. 3\n[32] S. S. Gubser, I. R. Klebanov, and A. M. Polyakov, “Gauge theory correlators from\nnon-critical string theory,” Phys. Lett.B428 (1998) 105–114, hep-th/9802109. 3\n[33] E. Witten, “Anti-de Sitter space and holography,” Adv. Theor. Math. Phys.2 (1998)\n253–291, hep-th/9802150. 3\n[34] S. S. Gubser, J. Knaute, S. Parikh, A. Samberg, and P. Witaszczyk, “ p-adic\nAdS/CFT,” Commun. Math. Phys.352 (2017), no. 3 1019–1059, 1605.01061. 4\n20 TENSOR NETWORK LANGUAGE MODEL\n[35] J. L. Elman, “Finding structure in time,” Cognitive science14 (1990), no. 2 179–211. 4\n[36] A. Graves, “Generating Sequences With Recurrent Neural Networks,” ArXiv e-prints\n(Aug., 2013) 1308.0850. 4\n[37] C. B´ eny, “Deep learning and the renormalization group,”ArXiv e-prints(Jan., 2013)\n1301.3124. 4\n[38] P. Mehta and D. J. Schwab, “An exact mapping between the Variational\nRenormalization Group and Deep Learning,” ArXiv e-prints(Oct., 2014) 1410.3831. 4\n[39] H. W. Lin, M. Tegmark, and D. Rolnick, “Why Does Deep and Cheap Learning Work\nSo Well?,” Journal of Statistical Physics168 (Sept., 2017) 1223–1247, 1608.08225. 4\n[40] N. Cohen, O. Sharir, and A. Shashua, “On the Expressive Power of Deep Learning: A\nTensor Analysis,” ArXiv e-prints(Sept., 2015) 1509.05009. 4\n[41] Y. Levine, D. Yakira, N. Cohen, and A. Shashua, “Deep Learning and Quantum\nEntanglement: Fundamental Connections with Implications to Network Design,”\nArXiv e-prints(Apr., 2017) 1704.01552. 4\n[42] S. R. White, “Density matrix formulation for quantum renormalization groups,” Phys.\nRev. Lett.69 (Nov, 1992) 2863–2866. 4\n[43] E. Miles Stoudenmire and D. J. Schwab, “Supervised Learning with Quantum-Inspired\nTensor Networks,” ArXiv e-prints(May, 2016) 1605.05775. 5\n[44] G. Torlai and R. G. Melko, “Learning thermodynamics with Boltzmann machines,”\nPhysical Review B94 (2016), no. 16 165134. 5\n[45] G. Carleo and M. Troyer, “Solving the quantum many-body problem with artiﬁcial\nneural networks,” Science 355 (Feb., 2017) 602–606, 1606.02318. 5\n[46] D.-L. Deng, X. Li, and S. Das Sarma, “Exact Machine Learning Topological States,”\nArXiv e-prints(Sept., 2016) 1609.09060. 5\n[47] L. Huang and L. Wang, “Accelerated Monte Carlo simulations with restricted\nBoltzmann machines,” Physical Review B95 (2017), no. 3 035105. 5\n[48] J. Chen, S. Cheng, H. Xie, L. Wang, and T. Xiang, “On the Equivalence of Restricted\nBoltzmann Machines and Tensor Network States,” ArXiv e-prints(Jan., 2017)\n1701.04831. 5\n[49] Z.-Y. Han, J. Wang, H. Fan, L. Wang, and P. Zhang, “Unsupervised Generative\nModeling Using Matrix Product States,” ArXiv e-prints(Sept., 2017) 1709.01662. 5\n[50] D. Liu, S.-J. Ran, P. Wittek, C. Peng, R. Bl´ azquez Garc´ ıa, G. Su, and M. Lewenstein,\n“Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum\nInformation Theoretic Perspective on Deep Architectures,” ArXiv e-prints(Oct., 2017)\n1710.04833. 5\n[51] F. W. Lawvere, “Functorial semantics of algebraic theories,” Proceedings of the\nNational Academy of Sciences of the United States of America50 (1963), no. 5\n869–872. 11\n[52] S. Mac Lane, “Categorical algebra,” Bull. Amer. Math. Soc.71 (1965) 40–106. 11\n[53] A. Joyal and R. Street, “The geometry of tensor calculus. I,” Adv. Math.88 (1991),\nno. 1 55–112. 11\n[54] D. Yau, “Higher dimensional algebras via colored PROPs,” ArXiv e-prints(Sept.,\n2008) 0809.2161. 11\n[55] M. Markl, S. Merkulov, and S. Shadrin, “Wheeled PROPs, graph complexes and the\nmaster equation,” J. Pure Appl. Algebra213 (2009), no. 4 496–535. 11\nTENSOR NETWORK LANGUAGE MODEL 21\n[56] P. Hackney and M. Robertson, “On the category of props,” Appl. Categ. Structures23\n(2015), no. 4 543–573. 11\n[57] D. Yau and M. W. Johnson, A foundation for PROPs, algebras, and modules, vol. 203\nof Mathematical Surveys and Monographs. American Mathematical Society,\nProvidence, RI, 2015. 11\n[58] J. C. Baez, B. Coya, and F. Rebro, “Props in Network Theory,” ArXiv e-prints(July,\n2017) 1707.08321. 11\n[59] S. Yalin, “Function spaces and classifying spaces of algebras over a prop,” Algebr.\nGeom. Topol.16 (2016), no. 5 2715–2749. 11\n[60] G. Evenbly and G. Vidal, “Tensor Network States and Geometry,” Journal of\nStatistical Physics145 (Nov., 2011) 891–918, 1106.1082. 12, 16\n[61] J. Y. Lee and O. Landon-Cardinal, “Practical variational tomography for critical\none-dimensional systems,” Phys. Rev. A91 (Jun, 2015) 062128. 16\n[62] A. J. Ferris and G. Vidal, “Perfect sampling with unitary tensor networks,” Physical\nReviews B85 (Apr., 2012) 165146, 1201.3974. 17",
  "topic": "Vector space",
  "concepts": [
    {
      "name": "Vector space",
      "score": 0.5640034675598145
    },
    {
      "name": "Tensor (intrinsic definition)",
      "score": 0.5614310503005981
    },
    {
      "name": "Tensor contraction",
      "score": 0.5219511389732361
    },
    {
      "name": "Statistical mechanics",
      "score": 0.5157806873321533
    },
    {
      "name": "Computation",
      "score": 0.47385647892951965
    },
    {
      "name": "Tensor field",
      "score": 0.47160229086875916
    },
    {
      "name": "Mathematics",
      "score": 0.44824984669685364
    },
    {
      "name": "Machine translation",
      "score": 0.4451276659965515
    },
    {
      "name": "Computer science",
      "score": 0.40150952339172363
    },
    {
      "name": "Tensor product",
      "score": 0.38792693614959717
    },
    {
      "name": "Statistical physics",
      "score": 0.3477778434753418
    },
    {
      "name": "Pure mathematics",
      "score": 0.25856125354766846
    },
    {
      "name": "Exact solutions in general relativity",
      "score": 0.24159088730812073
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19386443495750427
    },
    {
      "name": "Algorithm",
      "score": 0.1921556293964386
    },
    {
      "name": "Physics",
      "score": 0.16487646102905273
    },
    {
      "name": "Mathematical analysis",
      "score": 0.1616862416267395
    }
  ],
  "institutions": []
}