{
  "title": "Transformer in Convolutional Neural Networks",
  "url": "https://openalex.org/W3172345956",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1977837230",
      "name": "Yun Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2315723189",
      "name": "Guolei Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095781328",
      "name": "Yu Qiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104362018",
      "name": "Le Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2310006641",
      "name": "Ajad Chhatkuli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103920730",
      "name": "Luc Van Gool",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2560622558",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2125188192",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2798489385",
    "https://openalex.org/W2795685215",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3083550439",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W2065391104",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2963843116",
    "https://openalex.org/W2962802300",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2962834855",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3133696297"
  ],
  "abstract": "We tackle the low-efficiency flaw of vision transformer caused by the high computational/space complexity in Multi-Head Self-Attention (MHSA). To this end, we propose the Hierarchical MHSA (H-MHSA), whose representation is computed in a hierarchical manner. Specifically, our H-MHSA first learns feature relationships within small grids by viewing image patches as tokens. Then, small grids are merged into larger ones, within which feature relationship is learned by viewing each small grid at the preceding step as a token. This process is iterated to gradually reduce the number of tokens. The H-MHSA module is readily pluggable into any CNN architectures and amenable to training via backpropagation. We call this new backbone TransCNN, and it essentially inherits the advantages of both transformer and CNN. Experiments demonstrate that TransCNN achieves state-of-the-art accuracy for image recognition. Code and pretrained models are available at this https URL. This technical report will keep updating by adding more experiments.",
  "full_text": "Springer Nature 2021 LATEX template\nVision Transformers with Hierarchical Attention\nYun Liu1, Yu-Huan Wu2, Guolei Sun3†, Le Zhang4†, Ajad Chhatkuli3 and Luc Van\nGool3\n1Institute for Infocomm Research (I2R), A*STAR, Singapore 138632.\n2Institute of High Performance Computing (IHPC), A*STAR, Singapore 138632.\n3Computer Vision Lab, ETH Z¨ urich, Z¨ urich 8092, Switzerland.\n4School of Information and Communication Engineering, UESTC, Chengdu 611731, China.\n†Corresponding authors.\nAbstract\nThis paper tackles the high computational/space complexity associated with Multi-Head Self-\nAttention (MHSA) in vanilla vision transformers. To this end, we propose Hierarchical MHSA\n(H-MHSA), a novel approach that computes self-attention in a hierarchical fashion. Specifically,\nwe first divide the input image into patches as commonly done, and each patch is viewed as a\ntoken. Then, the proposed H-MHSA learns token relationships within local patches, serving as local\nrelationship modeling. Then, the small patches are merged into larger ones, and H-MHSA mod-\nels the global dependencies for the small number of the merged tokens. At last, the local and\nglobal attentive features are aggregated to obtain features with powerful representation capacity.\nSince we only calculate attention for a limited number of tokens at each step, the computa-\ntional load is reduced dramatically. Hence, H-MHSA can efficiently model global relationships\namong tokens without sacrificing fine-grained information. With the H-MHSA module incorpo-\nrated, we build a family of Hierarchical-Attention-based Transformer Networks, namely HAT-Net.\nTo demonstrate the superiority of HAT-Net in scene understanding, we conduct extensive exper-\niments on fundamental vision tasks, including image classification, semantic segmentation, object\ndetection, and instance segmentation. Therefore, HAT-Net provides a new perspective for vision\ntransformers. Code and pretrained models are available at https://github.com/yun-liu/HAT-Net.\nKeywords: Vision transformer, hierarchical attention, global attention, local attention, scene understanding\n1 Introduction\nIn the last decade, convolutional neural networks\n(CNNs) have been the go-to architecture in com-\nputer vision, owing to their powerful capability\nin learning representations from images/videos\n[1–12]. Meanwhile, in another field of natural\nlanguage processing (NLP), the transformer archi-\ntecture [13] has been the de-facto standard to\nhandle long-range dependencies [14, 15]. Trans-\nformers rely heavily on self-attention to model\nglobal relationships of sequence data. Although\nglobal modeling is also essential for vision tasks,\nthe 2D/3D structures of vision data make it\nless straightforward to apply transformers therein.\nThis predicament was recently broken by Dosovit-\nskiy et al. [16], by applying a pure transformer to\nsequences of image patches.\nMotivated by [16], a large amount of litera-\nture on vision transformers has emerged to resolve\nthe problems caused by the domain gap between\n1\narXiv:2106.03180v5  [cs.CV]  26 Mar 2024\nSpringer Nature 2021 LATEX template\n2 HAT\ncomputer vision and NLP [17–21]. From our point\nof view, one major problem of vision transform-\ners is that the sequence length of image patches\nis much longer than that of tokens (words) in\nan NLP application, thus leading to high com-\nputational/space complexity when computing the\nMulti-Head Self-Attention (MHSA). Some efforts\nhave been dedicated to resolving this problem.\nToMe [22] improves the throughput of existing\nViT models [16] by systematically merging sim-\nilar tokens through the utilization of a general\nand light-weight matching algorithm. PVT [19]\nand MViT [21] downsample the feature to com-\npute attention in a reduced length of tokens but\nat the cost of losing fine-grained details. Swin\nTransformer [18] computes attention within small\nwindows to model local relationships, and it grad-\nually enlarges the receptive field by shifting win-\ndows and stacking more layers. From this point\nof view, Swin Transformer [18] may still be sub-\noptimal because it works in a similar manner to\nCNNs and needs many layers to model long-range\ndependencies [16].\nBuilding upon the discussed strengths of\ndownsampling-based transformers [19, 21] and\nwindow-based transformers [18], each with its\ndistinctive merits, we aim to harness their comple-\nmentary advantages. Downsampling-based trans-\nformers excel at directly modeling global depen-\ndencies but may sacrifice fine-grained details,\nwhile window-based transformers effectively cap-\nture local dependencies but may fall short in\nglobal dependency modeling. As widely accepted,\nboth global and local information is essential for\nvisual scene understanding. Motivated by this\ninsight, our approach seeks to amalgamate the\nstrengths of both paradigms, enabling the direct\nmodeling of both global and local dependencies.\nTo achieve this, we introduce the Hierarchi-\ncal Multi-Head Self-Attention (H-MHSA) ,\na novel mechanism that enhances the flexibil-\nity and efficiency of self-attention computation\nin transformers. Our methodology begins by seg-\nmenting an image into patches, treating each\npatch akin to a token [16]. Rather than computing\nattention across all patches, we further organize\nthese patches into small grids, performing atten-\ntion computation within each grid. This step is\ninstrumental in capturing local relationships and\ngenerating more discriminative local representa-\ntions. Subsequently, we amalgamate these smaller\npatches into larger ones and treat the merged\npatches as new tokens, resulting in a substantial\nreduction in their number. This enables the direct\nmodeling of global dependencies by calculating\nself-attention for the new tokens. Ultimately, the\nattentive features from both local and global hier-\narchies are aggregated to yield potent features\nwith rich granularities. Notably, as the attention\ncalculation at each step is confined to a small\nnumber of tokens, our hierarchical strategy mit-\nigates the computational and space complexity\nof vanilla transformers. Empirical observations\nunderscore the efficacy of this hierarchical self-\nattention mechanism, revealing improved general-\nization results in our experiments.\nBy simply incorporating H-MHSA, we build\na family of Hierarchical-Attention-based\nTransformer Networks (HAT-Net). To eval-\nuate the efficacy of HAT-Net in scene understand-\ning, we experiment HAT-Net for fundamental\nvision tasks, including image classification, seman-\ntic segmentation, object detection, and instance\nsegmentation. Experimental results demonstrate\nthat HAT-Net performs favorably against previ-\nous backbone networks. Note that H-MHSA is\nbased on a very simple and intuitive idea, so H-\nMHSA is expected to provide a new perspective\nfor the future design of vision transformers.\n2 Related Work\nConvolutional neural networks. More than\ntwo decades ago, LeCun et al. [23] built the first\ndeep CNN, i.e., LeNet, for document recognition.\nAbout ten years ago, AlexNet [1] introduced pool-\ning layers into CNNs and pushed forward the state\nof the art of ImageNet classification [24] signifi-\ncantly. Since then, CNNs have become the de-facto\nstandard of computer vision owing to its pow-\nerful ability in representation learning. Brilliant\nachievements have been seen in this direction.\nVGGNet [2] investigates networks of increasing\ndepth using small (3 × 3) convolution filters.\nResNet [3] manages to build very deep networks by\nresolving the gradient vanishing/exploding prob-\nlem with residual connections [25]. GoogLeNet\n[26] presents the inception architecture [27, 28]\nusing multiple branches with different convolu-\ntion kernels. ResNeXt [29] improves ResNet [3]\nby replacing the 3 × 3 convolution in the bot-\ntleneck with a grouped convolution. DenseNets\nSpringer Nature 2021 LATEX template\nHAT 3\n[30] present dense connections, i.e., using the fea-\nture maps of all preceding layers as inputs for\neach layer. MobileNets [31, 32] decompose the\ntraditional convolution into a pointwise convolu-\ntion and a depthwise separable convolution for\nacceleration, and an inverted bottleneck is pro-\nposed for ensuring accuracy. ShuffleNets [33, 34]\nfurther decompose the pointwise convolution into\npointwise group convolution and channel shuffle\nto reduce computational cost. MansNet [35] pro-\nposes an automated mobile neural architecture\nsearch approach to search for a model with a good\ntrade-off between accuracy and latency. Efficient-\nNet [36] introduces a scaling method to uniformly\nscale depth/width/resolution dimensions of the\narchitecture searched by MansNet [35]. The above\nadvanced techniques are the engines driving the\ndevelopment of computer vision in the last decade.\nThis paper aims at improving feature representa-\ntion learning by designing new transformers.\nSelf-attention mechanism. Inspired by the\nhuman visual system, the self-attention mecha-\nnism is usually adopted to enhance essential infor-\nmation and suppress noisy information. STN [37]\npresents the spatial attention mechanism through\nlearning an appropriate spatial transformation for\neach input. Chen et al. [38] proposed the channel\nattention model and achieved promising results\non the image captioning task. Wang et al. [39]\nexplored self-attention in well-known residual net-\nworks [3]. SENet [40] applies channel attention\nto backbone network design and boosts the accu-\nracy of ImageNet classification [24]. CBAM [41]\nsequentially applies channel and spatial attention\nfor adaptive feature refinement in deep networks.\nBAM [42] produces a 3D attention map by com-\nbining channel and spatial attention. SK-Net [43]\nuses channel attention to selectively fuse multi-\nple branches with different kernel sizes. Non-local\nnetwork [44] presents non-local attention for cap-\nturing long-range dependencies. ResNeSt [45] is\na milestone in this direction. It applies channel\nattention on different network branches to capture\ncross-feature interactions and learn diverse repre-\nsentations. Our work shares some similarities with\nthese works by applying self-attention for learn-\ning feature representations. The difference is that\nwe propose H-MHSA to learn global relationships\nrather than a simple feature recalibration using\nspatial or channel attention in these works.\nVision transformer. Transformer [13] entirely\nrelies on self-attention to handle long-range depen-\ndencies of sequence data. It was first proposed\nfor NLP tasks [14, 15]. In order to apply trans-\nformers on image data, Dosovitskiy et al. [16]\nsplit an image into patches and treated them\nas tokens. Then, a pure transformer [13] can\nbe adopted. Such a vision transformer (ViT)\nattains competitive accuracy for ImageNet clas-\nsification [24]. More recently, lots of efforts have\nbeen dedicated to improving ViT. T2T-ViT [46]\nproposes to split an image into tokens of overlap-\nping patches so as to represent local structures\nby surrounding tokens. CaiT [47] builds a deeper\ntransformer network by introducing a per-channel\nweighting and specific class attention. DeepViT\n[48] proposes Re-attention to re-generate atten-\ntion maps to increase their diversity at different\nlayers. DeiT [49] presents a knowledge distilla-\ntion strategy for improving the training of ViT\n[16]. Srinivas et al. [50] tried to add the bottle-\nneck structure to vision transformers. Some works\nbuild pyramid transformer networks to generate\nmulti-scale features [17–21]. PVT [19] adopts con-\nvolution operation to downsample the feature map\nin order to reduce the sequence length in MHSA,\nthus reducing the computational load. Similar to\nPVT [19], MViT [21] utilizes pooling to com-\npute attention on a reduced sequence length. Swin\nTransformer [18] computes attention within small\nwindows and shifts windows to gradually enlarge\nthe receptive field. CoaT [20] computes attention\nin the channel dimension rather than in the tradi-\ntional spatial dimension. ToMe [22] enhances the\nthroughput of existing ViT models [16] without\nrequiring retraining, which is achieved by grad-\nually combining similar tokens in a transformer\nusing a matching algorithm. In this paper, we\nintroduce a novel design to reduce the computa-\ntional complexity of MHSA and learn both the\nglobal and local relationship modeling through\nvision transformers.\nVision MLP networks. While CNNs and\nvision transformers have been widely adopted for\ncomputer vision tasks, Tolstikhin et al. [51] chal-\nlenged the necessity of convolutions and attention\nmechanisms. They introduced the MLP-Mixer\narchitecture, which relies solely on multi-layer per-\nceptrons (MLPs). MLP-Mixer incorporates two\ntypes of layers: one applies MLPs independently\nSpringer Nature 2021 LATEX template\n4 HAT\nto image patches, facilitating the mixing of per-\nlocation features, and the other applies MLPs\nacross patches, enabling the mixing of spatial\ninformation. Despite lacking convolutions and\nattention, MLP-Mixer demonstrated competitive\nperformance in image classification compared to\nstate-of-the-art models. Liu et al. [52] introduced\ngMLP, an MLP-based model with gating, show-\ncasing its comparable performance to transformers\nin crucial language and vision applications. In con-\ntrast to other MLP-like models that encode spatial\ninformation along flattened spatial dimensions,\nVision Permutator [53] uniquely encodes feature\nrepresentations along height and width dimen-\nsions using linear projections. Wang et al. [54]\nproposed a novel positional spatial gating unit,\nleveraging classical relative positional encoding to\nefficiently capture cross-token relations for token\nmixing. Despite these advancements, the perfor-\nmance of vision MLP networks still lags behind\nthat of vision transformers. In this paper, we focus\non the design of a new vision transformer network.\n3 Methodology\nIn this section, we first provide a brief review\nof vision transformers [16] in Sec. 3.1. Then, we\npresent the proposed H-MHSA and analyze its\ncomputational complexity in Sec. 3.2. Finally, we\ndescribe the configuration details of the proposed\nHAT-Net in Sec. 3.3.\n3.1 Review of Vision Transformers\nTransformer [13, 16] heavily relies on MHSA to\nmodel long-range relationships. Suppose X ∈\nRH×W×C denotes the input, where H, W, and C\nare the height, width, and the feature dimension,\nrespectively. We reshape X and define the query\nQ, key K, value V as\nX ∈ RH×W×C → X ∈ R(H×W)×C,\nQ = XWq, K = XWk, V = XWv,\n(1)\nwhere Wq ∈ RC×C, Wk ∈ RC×C, and Wv ∈\nRC×C are the trainable weight matrices of linear\ntransformations. With a mild assumption that the\ninput and output have the same dimension, the\ntraditional MHSA can be formulated as\nA = Softmax(QKT/\n√\nd)V, (2)\nin which\n√\nd means an approximate normalization,\nand the Softmax function is applied to the rows\nof the matrix. Note that we omit the concept of\nmultiple heads here for simplicity. In Eq. (2), the\nmatrix product of QKT first computes the simi-\nlarity between each pair of tokens. Each new token\nis then derived over the combination of all tokens\naccording to the similarity. After the computation\nof MHSA, a residual connection is further added\nto ease the optimization, like\nX ∈ R(H×W)×C → X ∈ RH×W×C,\nA′ = AWp + X, (3)\nin which Wp ∈ RC×C is a trainable weight\nmatrix for feature projection. At last, a multi-\nlayer perceptron (MLP) is adopted to enhance the\nrepresentation, like\nY = MLP(A′) + A′, (4)\nwhere Y denotes the output of a transformer\nblock.\nIt is easy to infer that the computational\ncomplexity of MHSA (Eq. (2)) is\nΩ(MHSA) = 3HW C2 + 2H2W2C. (5)\nSimilarly, the space complexity (memory con-\nsumption) also includes the term of O(H2W2).\nAs commonly known, O(H2W2) could become\nvery large for high-resolution inputs. This limits\nthe applicability of transformers for vision tasks.\nMotivated by this, we aim at improving MHSA to\nreduce such complexity and maintain the capacity\nof global relationship modeling without the risk of\nsacrificing performances.\n3.2 Hierarchical Multi-Head\nSelf-Attention\nIn this section, we present an approach to alleviate\nthe computational and space demands associ-\nated with Eq. (2) through the utilization of our\nproposed H-MHSA mechanism. Rather than com-\nputing attention over the entire input, we adopt a\nhierarchical strategy, allowing each step to process\nonly a limited number of tokens.\nThe initial step concentrates on local attention\ncomputation. Assuming the input feature map is\nSpringer Nature 2021 LATEX template\nHAT 5\nConv Block\nDownsample\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nGAP\nFC\nGAP\nFC\nHW\n44\nHW\n1L\n88\nHW\n2L\n16 16\nHW\n32 32\nHW\n3L\n4L\nDownsample\nDownsample\nFig. 1 Illustration of the proposed HAT-Net. GAP: global average pooling; FC: fully-connected layer. ×Li means\nthat the transformer block is repeated for Li times. H and W denote the height and width of the input image, respectively.\nTable 1 Network configurations of HAT-Net. The settings of building blocks are shown in brackets, with the number\nof blocks stacked. For the first stage, each convolution has C channels and a stride of S. For the other four stages, each\nMLP uses a K × K DW-Conv and an expansion ratio of E. Note that we omit the downsampling operation after the t-th\nstage (t = {2, 3, 4}) for simplicity. “#Param” refers to the number of parameters.\nStage Input Size Operator HAT-Net-Tiny HAT-Net-Small HAT-Net-Medium HAT-Net-Large\n1 224×224 3×3 conv. C= 16,S= 2\nC= 48,S= 2\nC= 16,S= 2\nC= 64,S= 2\nC= 16,S= 2\nC= 64,S= 2\nC= 16,S= 2\nC= 64,S= 2\n2 56×56 H-MHSA\nMLP\n\n\nC= 48\nK= 3\nE= 8\n\n×2\n\n\nC= 64\nK= 3\nE= 8\n\n×2\n\n\nC= 64\nK= 5\nE= 8\n\n×3\n\n\nC= 64\nK= 3\nE= 8\n\n×3\n3 28×28 H-MHSA\nMLP\n\n\nC= 96\nK= 3\nE= 8\n\n×2\n\n\nC= 128\nK= 3\nE= 8\n\n×3\n\n\nC= 128\nK= 3\nE= 8\n\n×6\n\n\nC= 128\nK= 3\nE= 8\n\n×8\n4 14×14 H-MHSA\nMLP\n\n\nC= 240\nK= 3\nE= 4\n\n×6\n\n\nC= 320\nK= 3\nE= 4\n\n×8\n\n\nC= 320\nK= 5\nE= 4\n\n×18\n\n\nC= 320\nK= 3\nE= 4\n\n×27\n5 7×7 H-MHSA\nMLP\n\n\nC= 384\nK= 3\nE= 4\n\n×3\n\n\nC= 512\nK= 3\nE= 4\n\n×3\n\n\nC= 512\nK= 3\nE= 4\n\n×3\n\n\nC= 640\nK= 3\nE= 4\n\n×3\n1×1 - Global Average Pooling, 1000-d FC, Softmax\n#Param 12.7M 25.7M 42.9M 63.1M\ndenoted as X ∈ RH×W×C, we partition the fea-\nture map into small grids of size G1 × G1 and\nreshape it as follows:\nX ∈ RH×W×C → X1 ∈ R( H\nG1 ×G1)×( W\nG1 ×G1)×C\n→ X1 ∈ R( H\nG1 × W\nG1 )×(G1×G1)×C.\n(6)\nThe query, key, and value are then calculated by\nQ1 = X1Wq\n1, K1 = X1Wk\n1 , V1 = X1Wv\n1,\n(7)\nwhere Wq\n1, Wk\n1 , Wv\n1 ∈ RC×C are trainable weight\nmatrices. Subsequently, Eq. (2) is applied to gener-\nate the local attentive featureA1. To ease network\noptimization, we reshape A1 back to the shape of\nX through\nA1 ∈ R( H\nG1 × W\nG1 )×(G1×G1)×C\n→A1 ∈ R( H\nG1 ×G1)×( W\nG1 ×G1)×C\n→A1 ∈ RH×W×C,\n(8)\nand incorporate a residual connection:\nA1 = A1 + X. (9)\nAs the local attentive feature A1 is computed\nwithin each small G1 × G1 grid, a substantial\nreduction in computational and space complexity\nis achieved.\nSpringer Nature 2021 LATEX template\n6 HAT\nThe second step focuses on global attention\ncalculation. Here, we downsample A1 by a fac-\ntor of G2 during the computation of key and\nvalue matrices. This downsampling enables effi-\ncient global attention calculation, treating each\nG2 × G2 grid as a token. This process can be\nexpressed as\nbA1 = AvePoolG2 (A1), (10)\nwhere AvePoolG2 (·) denotes downsampling a fea-\nture map by G2 times using average pooling with\nboth the kernel size and stride set to G2. Con-\nsequently, we have bA1 ∈ R\nH\nG2 × W\nG2 ×C. We then\nreshape A1 and bA1 as follows:\nA1 ∈ RH×W×C → A1 ∈ R(H×W)×C,\nbA1 ∈ R\nH\nG2 × W\nG2 ×C → bA1 ∈ R( H\nG2 × W\nG2 )×C.\n(11)\nFollowing this, we compute the query, key, and\nvalue as\nQ2 = A1Wq\n2, K2 = bA1Wk\n2 , V2 = bA1Wv\n2,\n(12)\nwhere Wq\n2, Wk\n2 , Wv\n2 ∈ RC×C are trainable weight\nmatrices. It is easy to derive that we have\nQ2 ∈ R(H×W)×C, K2 ∈ R( H\nG2 × W\nG2 )×C, and\nV2 ∈ R( H\nG2 × W\nG2 )×C. Subsequently, Eq. (2) is\ncalled to obtain the global attentive feature A2 ∈\nR(H×W)×C, followed by a reshaping operation:\nA2 ∈ R(H×W)×C → A2 ∈ RH×W×C. (13)\nThe final output of H-MHSA is given by\nH-MHSA(X) = (A1 + A2)Wp + X, (14)\nwhere Wp has the same meaning as in Eq. (3). In\nthis way, H-MHSA effectively models both local\nand global relationships, akin to vanilla MHSA.\nThe computational complexity of H-MHSA\ncan be expressed as\nΩ(H-MHSA) = HWC (4C+2G2\n1)+2 HW\nG2\n2\nC(C+HW ).\n(15)\nCompared to Eq. (5), this represents a reduction\nin computational complexity from O(H2W2) to\nO(HW G2\n1 + H2W2\nG2\n2\n). The same conclusion can be\neasily derived for space complexity.\nWe continue by comparing H-MHSA with\nexisting vision transformers, highlighting dis-\ntinctive features. Swin Transformer [18] focuses\non modeling local relationships, progressively\nexpanding the receptive field through shifted win-\ndows and additional layers. Conversely, PVT\n[19] prioritizes global relationships through down-\nsampling key and value matrices but overlooks\nlocal information. In contrast, our proposed H-\nMHSA excels by concurrently capturing both local\nand global relationships. While Swin Transformer\nemploys a fixed window size ( i.e., a fixed-size\nbias matrix), and PVT uses a constant downsam-\npling ratio (i.e., a convolution with the kernel size\nequal to the stride), these approaches necessitate\nretraining on the ImageNet dataset [24] for any re-\nparameterization. In contrast, the parameter-free\nnature of G1 and G2 in H-MHSA allows flexible\nconfiguration adjustments for downstream vision\ntasks without the need for retraining on ImageNet.\nIn computer vision, achieving a comprehen-\nsive understanding of scenes relies on the simul-\ntaneous consideration of both global and local\ninformation. Within the framework of our pro-\nposed H-MHSA, global self-attention calculation\n(Eq. (10)-(13)) is instrumental in establishing the\nfoundation for scene interpretation, enabling the\nrecognition of overarching patterns and aiding\nin high-level decision-making processes. Concur-\nrently, local self-attention calculation (Eq. (6)-(9))\nis crucial for refining the understanding of individ-\nual components within the larger context, facili-\ntating more detailed and nuanced scene analysis.\nH-MHSA excels in striking the delicate balance\nbetween global and local information, thereby\nfacilitating a nuanced and accurate comprehension\nof diverse scenes. In essence, the seamless inte-\ngration of global and local self-attention within\nthe H-MHSA framework empowers transformers\nto navigate the intricacies of scene understanding,\nfacilitating context-aware decision-making.\n3.3 Network Architecture\nThis part introduces the network architecture\nof HAT-Net. We follow the common practice in\nCNNs to use a global average pooling layer and a\nfully connected layer to predict image classes [18].\nThis is different from existing transformers which\nrely on another 1D class token to make predictions\n[16, 17, 19–21, 46–49, 55–57]. We also observe that\nSpringer Nature 2021 LATEX template\nHAT 7\nexisting transformers [16–21, 46–49] usually adopt\nthe GELU function [58] for nonlinear activation.\nHowever, GELU is memory-hungry during net-\nwork training. We empirically found that the SiLU\nfunction [59], originally coined in [58], performs\non-par with GELU and is more memory-friendly.\nHence, HAT-Net uses SiLU [59] for nonlinear acti-\nvation. Besides, we add a depthwise separable\nconvolution (DW-Conv) [31] inside the MLP as\nwidely done.\nThe overall architecture of HAT-Net is illus-\ntrated in Fig. 1. At the beginning of HAT-Net,\ninstead of flattening image patches [16], we apply\ntwo sequential vanilla 3 × 3 convolutions, each of\nwhich has a stride of 2, to downsample the input\nimage into 1 /4 scale. Then, we stack H-MHSA\nand MLP alternatively, which can be divided\ninto four stages with pyramid feature scales of\n1/4, 1 /8, 1 /16, and 1 /32, respectively. For fea-\nture downsampling at the end of each stage, a\nvanilla 3×3 convolution with a stride of 2 is used.\nThe configuration details of HAT-Net are sum-\nmarized in Table 1. We provide four versions of\nHAT-Net: HAT-Net-Tiny, HAT-Net-Small, HAT-\nNet-Medium, and HAT-Net-Large, whose number\nof parameters is similar to ResNet18, ResNet50,\nResNet101, and ResNet152 [3], respectively. We\nonly adopt simple parameter settings without\ncareful tuning to demonstrate the effectiveness\nand generality of HAT-Net. The dimension of each\nhead in the multi-head setting is set to 48 for\nHAT-Net-Tiny and 64 for other versions.\nTo enhance the applicability of HAT-Net\nacross diverse vision tasks, we present guidelines\nfor configuring the parameter-free G1 and G2.\nWhile established models like Swin Transformer\n[18] adhere to a fixed window size of 7, and PVT\n[19] employs a set of constant downsampling ratios\n8, 4, 2 for the t-th stage ( t = 2, 3, 4), we advocate\nfor certain adjustments. Practically, we find that\na window size of 8 is more pragmatic than 7, given\nthat input resolutions often align with multiples\nof 8. Moreover, augmenting downsampling ratios\nserves to mitigate computational complexity. Con-\nsequently, for image classification on the ImageNet\ndataset [24], where the standard input resolution\nis 224 × 224 pixels, we designate G1 = 8, 7, 7 and\nG2 = 8, 4, 2 for the t-th stage ( t = 2, 3, 4). Here,\na window size of 7 is necessitated by the cho-\nsen resolution, and small downsampling rates are\nin line with the approach taken by PVT [19]. In\nscenarios involving downstream tasks like seman-\ntic segmentation, object detection, and instance\nsegmentation, where input resolutions tend to be\nlarger, we opt for G1 = 8, 8, 8 for convenience and\nG2 = 16 , 8, 4 to curtail computational expenses.\nFor a comprehensive analysis of the impact of dif-\nferent G1 and G2 settings, we conduct an ablation\nstudy in Sec. 4.4.\n4 Experiments\nTo show the superiority of HAT-Net in feature rep-\nresentation learning, this section evaluates HAT-\nNet for image classification, semantic segmenta-\ntion, object detection and instance segmentation.\n4.1 Image Classification\nExperimental setup. The ImageNet dataset\n[24] consists of 1.28M training images and 50K\nvalidation images from 1000 categories. We adopt\nthe training set to train our networks and the vali-\ndation set to test the performance. We implement\nHAT-Net using the popular PyTorch framework\n[60]. For a fair comparison, we follow the same\ntraining protocol as DeiT [49], which is the stan-\ndard protocol for training transformer networks\nnowadays. Specifically, the input images are ran-\ndomly cropped to 224 × 224 pixels, followed by\nrandom horizontal flipping andmixup [61] for data\naugmentation. Label smoothing [27] is used to\navoid overfitting. The AdamW optimizer [62] is\nadopted with the momentum of 0.9, the weight\ndecay of 0.05, and a mini-batch size of 128 per\nGPU by default. The initial learning rate is set to\n1e-3, which decreases following the cosine learning\nrate schedule [63]. The training process lasts for\n300 epochs on eight NVIDIA Tesla V100 GPUs.\nNote that for ablation studies, we utilize a mini-\nbatch size of 64 and 100 training epochs to save\ntraining time. Moreover, we set G1 = {8, 7, 7} and\nG2 = {8, 4, 2} for t-th stage (t = {2, 3, 4}), respec-\ntively. The fifth stage can be processed directly\nusing the vanilla MHSA mechanism. For model\nevaluation, we apply a center crop of 224 × 224\npixels on validation images to evaluate the recog-\nnition accuracy. We report the top-1 classification\naccuracy on the ImageNet validation set [24] as\nwell as the number of parameters and the number\nof FLOPs for each model.\nSpringer Nature 2021 LATEX template\n8 HAT\nTable 2 Comparison to state-of-the-art methods\non the ImageNet validation set [24]. “*” indicates the\nperformance of a method using the default training set-\nting in the original paper. “#Param” and “#FLOPs” refer\nto the number of parameters and the number of FLOPs,\nrespectively. “†” marks models that use the input size of\n384×384; otherwise, models use the input size of 224×224.\nArch. Models #Param#FLOPsTop-1 Acc.\nCNN ResNet18* [3] 11.7M 1.8G 69.8\nResNet18 [3] 11.7M 1.8G 68.5\nTrans\nDeiT-Ti/16 [49] 5.7M 1.3G 72.2\nPVT-Tiny [19] 13.2M 1.9G 75.1\nPVTv2-B1 [64] 13.1M 2.1G 78.7\nHAT-Net-Tiny 12.7M 2.0G 79.8\nCNN\nResNet50* [3] 25.6M 4.1G 76.1\nResNet50 [3] 25.6M 4.1G 78.5\nResNeXt50-32x4d* [29] 25.0M 4.3G 77.6\nResNeXt50-32x4d [29] 25.0M 4.3G 79.5\nRegNetY-4G [65] 20.6M 4.0G 80.0\nResNeSt-50 [45] 27.5M 5.4G 81.1\nToMe-ViT-S/16 [22] 22.1M 2.7G 79.4\nTrans\nDeiT-S/16 [49] 22.1M 4.6G 79.8\nT2T-ViTt-14 [46] 21.5M 5.2G 80.7\nTNT-S [55] 23.8M 5.2G 81.3\nCvT-13 [66] 20.0M 4.5G 81.6\nPVT-Small [19] 24.5M 3.8G 79.8\nPVTv2-B2 [64] 25.4M 4.0G 82.0\nSwin-T [18] 28.3M 4.5G 81.3\nTwins-SVT-S [67] 24.0M 2.8G 81.7\nHAT-Net-Small 25.7M 4.3G 82.6\nCNN\nResNet101* [3] 44.7M 7.9G 77.4\nResNet101 [3] 44.7M 7.9G 79.8\nResNeXt101-32x4d* [29]44.2M 8.0G 78.8\nResNeXt101-32x4d [29] 44.2M 8.0G 80.6\nRegNetY-8G [65] 39.2M 8.0G 81.7\nResNeSt-101 [45] 48.3M 10.3G 83.0\nTrans\nT2T-ViTt-19 [46] 39.2M 8.4G 81.4\nCvT-21 [66] 31.5M 7.1G 82.5\nMViT-B-16 [21] 37.0M 7.8G 82.5\nPVT-Medium [19] 44.2M 6.7G 81.2\nPVTv2-B3 [64] 45.2M 6.9G 83.2\nSwin-S [18] 49.6M 8.7G 83.0\nHAT-Net-Medium 42.9M 8.3G 84.0\nCNN\nResNet152* [3] 60.2M 11.6G 78.3\nResNeXt101-64x4d* [29]83.5M 15.6G 79.6\nResNeXt101-64x4d [29] 83.5M 15.6G 81.5\nTrans\nViT-B/16† [16] 86.6M 55.4G 77.9\nViT-L/16† [16] 304.3M 190.7G 76.5\nToMe-ViT-L/16 [22] 304.3M 22.3G 84.2\nDeiT-B/16 [49] 86.6M 17.6G 81.8\nMViT-B-24 [21] 53.5M 10.9G 83.1\nTNT-B [55] 65.6M 14.1G 82.8\nPVT-Large [19] 61.4M 9.8G 81.7\nPVTv2-B4 [64] 62.6M 10.1G 83.6\nSwin-B [18] 87.8M 15.4G 83.3\nTwins-SVT-B [67] 56.0M 8.3G 83.2\nHAT-Net-Large 63.1M 11.5G 84.2\nExperimental results. We compare HAT-\nNet with state-of-the-art network architectures,\nincluding CNN-based ones like ResNet [3],\nTable 3 Experimental results on the ADE20K val-\nidation dataset [68] for semantic segmentation. We\nreplace the backbone of Semantic FPN [69] with various\nnetwork architectures. The number of FLOPs is calculated\nwith the input size of 512 × 512.\nBackbone\nSemantic FPN [69]\n#Param (M)↓ FLOPs (G)↓ mIoU (%)↑\nResNet-18 [3] 15.5 31.9 32.9\nPVT-Tiny [19] 17.0 32.1 35.7\nPVTv2-B1 [64] 17.8 33.1 41.5\nHAT-Net-Tiny 15.9 33.2 43.6\nResNet-50 [3] 28.5 45.4 36.7\nPVT-Small [19] 28.2 42.9 39.8\nSwin-T [18] 31.9 46 41.5\nTwins-SVT-S [67] 28.3 37 43.2\nPVTv2-B2 [64] 29.1 44.1 46.1\nHAT-Net-Small 29.5 49.6 46.6\nResNet-101 [3] 47.5 64.8 38.8\nResNeXt-101-32x4d [29] 47.1 64.6 39.7\nPVT-Medium [19] 48.0 59.4 41.6\nSwin-S [18] 53.2 70 45.2\nTwins-SVT-B [67] 60.4 67 45.3\nPVTv2-B3 [64] 49.0 60.7 47.3\nHAT-Net-Medium 46.7 74.7 49.3\nResNeXt-101-64x4d [29] 86.4 104.2 40.2\nPVT-Large [19] 65.1 78.0 42.1\nSwin-B [18] 91.2 107 46.0\nTwins-SVT-L [67] 102 103.7 46.7\nPVTv2-B4 [64] 66.3 79.6 48.6\nHAT-Net-Large 66.8 96.4 49.5\nResNeXt [29], RegNetY [65], ResNeSt [45], and\ntransformer-based ones like ViT [16], DeiT [49],\nT2T-ViT [46], TNT [55], CvT [66], MViT [21],\nPVT [19], Swin Transformer [18], Twins [67],\nToMe [22]. The results are summarized in Table 2.\nWe can observe that HAT-Net achieves state-\nof-the-art performance. Specifically, with similar\nnumbers of parameters and FLOPs, HAT-Net-\nTiny, HAT-Net-Small, HAT-Net-Medium, and\nHAT-Net-Large outperforms the second best\nresults by 1.1%, 0.6%, 0.8%, and 0.6% in terms of\nthe top-1 accuracy, respectively. Since the perfor-\nmance for image classification implies the ability\nof a network for learning feature representations,\nthe above comparison suggests that the proposed\nHAT-Net has great potential for generic scene\nunderstanding.\n4.2 Semantic Segmentation\nExperimental setup. We continue by apply-\ning HAT-Net to a fundamental downstream vision\ntask, semantic segmentation, which aims at pre-\ndicting a class label for each pixel in an image.\nWe follow [19, 67] to replace the backbone of the\nwell-known segmentor, Semantic FPN [69], with\nSpringer Nature 2021 LATEX template\nHAT 9\nTable 4 Object detection results with RetinaNet [70] and instance segmentation results with Mask R-CNN\n[5] on the MS-COCO val2017 set [71]. “R” and “X” represent ResNet [3] and ResNeXt [29], respectively. The number\nof FLOPs is computed with the input size of 800 × 1280.\nBackbone\nObject Detection Instance Segmentation\n#Param\n(M)↓\n#FLOPs\n(G)↓\nRetinaNet [70] #Param\n(M)↓\n#FLOPs\n(G)↓\nMask R-CNN [5]\nAP AP50 AP75 APS APM APL APb APb50 APb75 APm APm50 APm75\nR-18 [3] 21.3 190 31.8 49.6 33.6 16.3 34.3 43.2 31.2 209 34.0 54.0 36.7 31.2 51.0 32.7\nViL-Tiny [72] 16.6 204 40.8 61.3 43.6 26.7 44.9 53.6 26.9 223 41.4 63.5 45.0 38.1 60.3 40.8\nPVT-Tiny [19] 23.0 205 36.7 56.9 38.9 22.6 38.8 50.0 32.9 223 36.7 59.2 39.3 35.1 56.7 37.3\nHAT-Net-Tiny 21.6 212 42.5 63.3 45.8 26.9 46.1 56.6 31.8 231 43.1 65.4 47.4 39.7 62.5 42.4\nR-50 [3] 37.7 239 36.3 55.3 38.6 19.3 40.0 48.8 44.2 260 38.0 58.6 41.4 34.4 55.1 36.7\nPVT-Small [19] 34.2 261 40.4 61.3 43.0 25.0 42.9 55.7 44.1 280 40.4 62.9 43.8 37.8 60.1 40.3\nSwin-T [18] 38.5 248 41.5 62.1 44.2 25.1 44.9 55.5 47.8 264 42.2 64.6 46.2 39.1 61.6 42.0\nViL-Small [72] 35.7 292 44.2 65.2 47.6 28.8 48.0 57.8 45.0 310 44.9 67.1 49.3 41.0 64,2 44.1\nTwins-SVT-S [67]34.3 236 43.0 64.2 46.3 28.0 46.4 57.5 44.0 254 43.4 66.0 47.3 40.3 63.2 43.4\nHAT-Net-Small 35.5 286 44.8 65.8 48.1 28.8 48.6 59.5 45.4 303 45.2 67.6 49.9 41.6 64.6 44.7\nR-101 [3] 56.7 315 38.5 57.8 41.2 21.4 42.6 51.1 63.2 336 40.4 61.1 44.2 36.4 57.7 38.8\nX-101-32x4d [29] 56.4 319 39.9 59.6 42.7 22.3 44.2 52.5 62.8 340 41.9 62.5 45.9 37.5 59.4 40.2\nPVT-Medium [19]53.9 349 41.9 63.1 44.3 25.0 44.9 57.6 63.9 367 42.0 64.4 45.6 39.0 61.6 42.1\nSwin-S [18] 59.8 336 44.5 65.7 47.5 27.4 48.0 59.9 69.1 354 44.8 66.6 48.9 40.9 63.4 44.2\nHAT-Net-Medium52.7 405 45.9 66.9 49.2 29.7 50.0 61.6 62.6 424 47.0 69.0 51.5 42.7 66.0 46.0\nX-101-64x4d [29] 95.5 473 41.0 60.9 44.0 23.9 45.2 54.0 101.9 493 42.8 63.8 47.3 38.4 60.6 41.3\nPVT-Large [19] 71.1 450 42.6 63.7 45.4 25.8 46.0 58.4 81.0 469 42.9 65.0 46.6 39.5 61.9 42.5\nTwins-SVT-B [67]67.0 376 45.3 66.7 48.1 28.5 48.9 60.6 76.3 395 45.2 67.6 49.3 41.5 64.5 44.8\nHAT-Net-Large 73.1 519 46.3 67.2 49.6 30.0 50.6 62.4 82.7 537 47.4 69.3 52.1 43.1 66.5 46.6\nHAT-Net or other backbone networks for a fair\ncomparison. Experiments are conducted on the\nchallenging ADE20K dataset [68]. This dataset\nhas 20000 training images, 2000 validation images,\nand 3302 testing images. We train Semantic FPN\n[69] using the training set and evaluate on the\nvalidation set. The training optimizer is AdamW\n[62] with weight decay of 1 e-4. We apply the poly\nlearning rate schedule with γ = 0.9 and the ini-\ntial learning rate of 1 e-4. During training, the\nbatch size is 16, and each image has a resolution\nof 512 × 512 through resizing and cropping. Dur-\ning testing, each image is resized to the shorter\nside of 512 pixels, without multi-scale testing or\nflipping. We adopt the well-known MMSegmenta-\ntion toolbox [73] for the above experiments. We\nset G1 = {8, 8, 8} and G2 = {16, 8, 4} for the t-th\nstage (t = {2, 3, 4}), respectively.\nExperimental results. The results are\ndepicted in Table 3. We compare with typical\nCNN networks, i.e., ResNets [3] and ResNeXts\n[29], and transformer networks, i.e., Swin Trans-\nformer [18], PVT [19], PVTv2 [64] and Twins-SVT\n[67]. As can be observed, the proposed HAT-\nNet achieves significantly better performance than\nprevious competitors. Specifically, HAT-Net-Tiny,\nHAT-Net-Small, HAT-Net-Medium, and HAT-\nNet-Large attain 1.9%, 0.4%, 1.9%, and 0.7%\nhigher mIoU than the second better results with\nsimilar number of parameters and FLOPs. This\ndemonstrates the superiority of HAT-Net in learn-\ning effective feature representations for dense\nprediction tasks.\n4.3 Object Detection and Instance\nSegmentation\nExperimental setup. Since object detection\nand instance segmentation are also fundamental\ndownstream vision tasks, we apply HAT-Net to\nboth tasks for further evaluating its effectiveness.\nSpecifically, we utilize two well-known detectors,\ni.e., RetinaNet [70] for object detection and Mask\nR-CNN [5] for instance segmentation. HAT-Net\nis compared to some well-known CNN and trans-\nformer networks by only replacing the backbone\nof the above two detectors. Experiments are con-\nducted on the large-scale MS-COCO dataset [71]\nby training on the train2017 set (∼118K images)\nand evaluating on the val2017 set (5K images).\nSpringer Nature 2021 LATEX template\n10 HAT\nTable 5 Ablation studies for the hierarchical atten-\ntion in HAT-Net.The configuration of HAT-Net-Small is\nadopted for all experiments. “✔” indicates that we replace\nthe window attention [18] with the hierarchical attention\nat the i-th stage. “Top-1 Acc” is the top-1 accuracy on the\nImageNet validation dataset [24]. “mIoU” is the mean IoU\nfor semantic segmentation on the ADE20K dataset [68].\nDesign #Stage Top-1 Acc mIoU2 3 4\n1 78.2 42.1\n2 ✔ 78.2 42.4\n3 ✔ ✔ 78.4 42.5\n4 ✔ ✔ ✔ 79.3 43.4\nWe adopt MMDection toolbox [74] for experi-\nments and follow the experimental settings of\nPVT [19] for a fair comparison. During train-\ning, we initialize the backbone weights with the\nImageNet-pretrained models. The detectors are\nfine-tuned using the AdamW optimizer [62] with\nan initial learning rate of 1 e-4 that is decreased\nby 10 times after the 8 th and 11th epochs, respec-\ntively. The whole training lasts for 12 epochs with\na batch size of 16. Each image is resized to a\nshorter side of 800 pixels, but the longer side is\nnot allowed to exceed 1333 pixels. We set G1 =\n{8, 8, 8} and G2 = {16, 8, 4} for the t-th stage\n(t = {2, 3, 4}), respectively.\nExperimental results. The results are dis-\nplayed in Table 4. As can be seen, HAT-Net\nsubstantially improves the accuracy over other\nnetwork architectures with a similar number of\nparameters. Twins-SVT [67] combines the advan-\ntages of PVT [19] and Swin Transformer [18] by\nalternatively stacking their basic blocks. When\nRetinaNet [70] is adopted as the detector, HAT-\nNet-Small attains 1.8%, 1.6% and 1.8% higher\nresults than Twins-SVT-S [67] in terms of AP,\nAP50 and AP 75, respectively. Correspondingly,\nHAT-Net-Large gets 1.0%, 0.5% and 1.5% higher\nresults than Twins-SVT-B [67]. With Mask R-\nCNN [5] as the detector, HAT-Net-Large achieves\n2.2%, 1.7% and 2.8% higher results than Twins-\nSVT-B [67] in terms of bounding box metrics\nAPb, AP b\n50 and AP b\n75, respectively. HAT-Net-\nLarge achieves 1.6%, 2.0% and 1.8% higher results\nthan Twins-SVT-B [67] in terms of mask metrics\nAPm, APm\n50 and APm\n75, respectively. Such signifi-\ncant improvement in object detection and instance\nsegmentation shows the superiority of HAT-Net in\nlearning effective feature representations.\nTable 6 Ablation studies for the settings of G1\nand G2 in HAT-Net. The performance assessment is\nconducted using Mask R-CNN [5] with HAT-Net-Small as\nthe backbone. Evaluation results are reported on the MS-\nCOCO val2017 dataset [71].\nSettings #FLOPs\n(G)↓\nMask R-CNN [5]\nG1 G2 APb APb50 APb75 APm APm50 APm75\n{8,8,8} {12,4,2} 338 45.7 67.8 50.4 41.7 64.8 44.7\n{8,8,8} {12,6,3} 313 45.3 67.6 49.7 41.6 64.8 44.9\n{8,8,8} {16,8,4} 303 45.2 67.6 49.9 41.6 64.6 44.7\n{8,8,8} {32,16,8} 291 44.6 66.8 49.1 41.0 63.8 44.3\n{16,16,8} {16,8,4} 309 45.1 67.5 49.5 41.3 64.5 44.4\n{4,4,4} {16,8,4} 300 45.1 67.1 49.5 41.3 64.3 44.5\n4.4 Ablation Studies\nIn this part, we evaluate various design choices of\nthe proposed HAT-Net. As discussed above, we\nonly train all ablation models for 100 epochs to\nsave training time. The batch size and learning\nrate are also reduced by half accordingly. HAT-\nNet-Small is adopted for these ablation studies.\nEffect of the proposed H-MHSA. Starting\nfrom the window attention [18] based transformer\nnetwork, we gradually replace the window atten-\ntion with our proposed H-MHSA at different\nstages. The results are summarized in Table 5.\nSince the feature map at the fifth stage is small\nenough for directly computing MHSA, the fifth\nstage is excluded from Table 5. Note that the first\nstage of HAT-Net only consists of convolutions so\nthat it is also excluded. From Table 5, we can\nobserve that the performance for both image clas-\nsification and semantic segmentation is improved\nwhen more stages adopt H-MHSA. This verifies\nthe effectiveness of the proposed H-MHSA in fea-\nture presentation learning. It is interesting to find\nthat the usage of H-MHSA at the fourth stage\nleads to more significant improvement than other\nstages. Intuitively, the fourth stage has the most\ntransformer blocks, so the changes at this stage\nwould lead to more significant effects.\nA pure transformer version of HAT-Net\nvs. PVT [19]. When we remove all depth-\nwise separable convolutions from HAT-Net and\ntrain the resulting transformer network for 100\nepochs, it achieves 77.7% top-1 accuracy on the\nImageNet validation set [24]. In contrast, the well-\nknown transformer network, PVT [19], attains\n75.8% top-1 accuracy under the same condition.\nSpringer Nature 2021 LATEX template\nHAT 11\nThis suggests that our proposed H-MHSA is very\neffective in feature representation learning.\nSiLU [59] vs. GELU [58]. We use SiLU\nfunction [59] for nonlinearization rather than the\nwidely-used GELU function [58] in transformers\n[13, 16]. Here, we evaluate the effect of this choice.\nHAT-Net with SiLU [59] attains 82.6% top-1 accu-\nracy on the ImageNet validation set [24] when\ntrained for 300 epochs. HAT-Net with GELU [58]\ngets 82.7% top-1 accuracy, slightly higher than\nSiLU [59]. However, HAT-Net with GELU [58]\nonly obtains 45.7% mIoU on the ADE20K dataset,\n0.8% lower than HAT-Net with SiLU. When using\na batch size of 128 per GPU, HAT-Net with\nSiLU [59] occupies 20.2GB GPU memory during\ntraining, while HAT-Net with GELU [58] occu-\npies 23.8GB GPU memory. Hence, HAT-Net with\nSiLU [59] can achieve slightly better performance\nwith less GPU memory consumption.\nSettings of G1 and G2. In HAT-Net, the\nparameters G1 and G2 play pivotal roles, con-\ntrolling grid sizes for local attention calculation\nand downsampling rates for global attention cal-\nculation, respectively. In this evaluation, we assess\nthe model’s performance under various configura-\ntions of G1 and G2. By default, for tasks such\nas object detection and instance segmentation,\nwe employ G1 = 8 , 8, 8 and G2 = 16 , 8, 4 for\nthe t-th stage ( t = 2 , 3, 4), respectively. Sub-\nsequently, we systematically vary G1 and G2,\nevaluating the performance of Mask R-CNN [5]\nwith HAT-Net-Small as the backbone. The evalua-\ntion results, conducted on the MS-COCOval2017\ndataset [71], are presented in Table 6. The findings\nindicate that HAT-Net demonstrates robustness\nacross different G1 and G2 settings. Notably, alter-\ning G1 from its default 8 , 8, 8 configuration has\na marginal impact on performance, resulting in\nslight performance reduction. Similarly, adjust-\ning the values of G2 yields a trade-off: decreasing\nvalues enhances performance at the expense of\nincreased computational cost, while increasing val-\nues reduces computational cost at the cost of\nslightly degraded performance. Our default choice\nof G1 = 8, 8, 8 and G2 = 16, 8, 4 strikes a favorable\nbalance between accuracy and efficiency, offering\na practical configuration for general use.\n5 Conclusion\nThis paper addresses the inefficiency inherent\nin vanilla vision transformers due to the ele-\nvated computational and space complexity asso-\nciated with MHSA. In response to this challenge,\nwe introduce a novel hierarchical framework for\nMHSA computation, denoted as H-MHSA, aim-\ning to alleviate the computational and space\ndemands. Compared to existing approaches in this\ndomain, such as PVT [19] and Swin Transformer\n[18], H-MHSA distinguishes itself by directly cap-\nturing both global dependencies and local rela-\ntionships. Integrating the proposed H-MHSA, we\nformulate the HAT-Net family, showcasing its\nprowess through comprehensive experiments span-\nning image classification, semantic segmentation,\nobject detection, and instance segmentation. Our\nresults affirm the efficacy and untapped potential\nof HAT-Net in advancing representation learning.\nApplications of HAT-Net. The versatility of\nHAT-Net extends its utility across diverse real-\nworld scenarios and downstream vision tasks. As\na robust backbone network for feature extrac-\ntion, HAT-Net seamlessly integrates with existing\nprediction heads and decoder networks, enabling\nproficient execution of various scene understand-\ning tasks. Furthermore, HAT-Net’s adaptability\nto different input resolutions and computational\nresource constraints is facilitated by the flexible\nadjustment of parameters, specifically G1 and G2.\nUsers can tailor HAT-Net to their specific require-\nments, selecting from different HAT-Net versions\nto align with their objectives.\nIn conclusion, HAT-Net not only presents a\npragmatic solution to the limitations of vanilla\nvision transformers but also opens avenues for\ninnovation in the future design of such architec-\ntures. The simplicity of the proposed H-MHSA\nunderscores its potential as a transformative ele-\nment in the evolving landscape of vision trans-\nformer development.\nReferences\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hin-\nton, “ImageNet classification with deep con-\nvolutional neural networks,” in Adv. Neural\nInform. Process. Syst., 2012, pp. 1097–1105.\nSpringer Nature 2021 LATEX template\n12 HAT\n[2] K. Simonyan and A. Zisserman, “Very deep\nconvolutional networks for large-scale image\nrecognition,” in Int. Conf. Learn. Represent.,\n2015.\n[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep\nresidual learning for image recognition,” in\nIEEE Conf. Comput. Vis. Pattern Recog.,\n2016, pp. 770–778.\n[4] S. Ren, K. He, R. Girshick, and J. Sun,\n“Faster R-CNN: Towards real-time object\ndetection with region proposal networks,”\nIEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 39, no. 6, pp. 1137–1149, 2016.\n[5] K. He, G. Gkioxari, P. Doll´ ar, and R. Gir-\nshick, “Mask R-CNN,” in Int. Conf. Comput.\nVis., 2017, pp. 2961–2969.\n[6] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia,\n“Pyramid scene parsing network,” in IEEE\nConf. Comput. Vis. Pattern Recog., 2017, pp.\n2881–2890.\n[7] Y. Liu, M.-M. Cheng, X. Hu, J.-W. Bian,\nL. Zhang, X. Bai, and J. Tang, “Richer con-\nvolutional features for edge detection,” IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 41,\nno. 8, pp. 1939–1946, 2019.\n[8] Y. Liu, M.-M. Cheng, D.-P. Fan, L. Zhang,\nJ.-W. Bian, and D. Tao, “Semantic edge\ndetection with diverse deep supervision,” Int.\nJ. Comput. Vis., vol. 130, no. 1, pp. 179–198,\n2022.\n[9] Y. Liu, M.-M. Cheng, X.-Y. Zhang, G.-Y.\nNie, and M. Wang, “DNA: Deeply super-\nvised nonlinear aggregation for salient object\ndetection,” IEEE Trans. Cybernetics, vol. 52,\nno. 7, pp. 6131–6142, 2021.\n[10] Y. Liu, Y.-C. Gu, X.-Y. Zhang, W. Wang,\nand M.-M. Cheng, “Lightweight salient\nobject detection via hierarchical visual per-\nception learning,” IEEE Trans. Cybernetics,\nvol. 51, no. 9, pp. 4439–4449, 2020.\n[11] Y. Liu, Y.-H. Wu, Y. Ban, H. Wang, and\nM.-M. Cheng, “Rethinking computer-aided\ntuberculosis diagnosis,” in IEEE Conf. Com-\nput. Vis. Pattern Recog., 2020, pp. 2646–\n2655.\n[12] Y. Liu, Y.-H. Wu, P. Wen, Y. Shi, Y. Qiu,\nand M.-M. Cheng, “Leveraging instance-\n, image-and dataset-level information for\nweakly supervised instance segmentation,”\nIEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 44, no. 3, pp. 1415–1428, 2020.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin, “Attention is all you need,” in\nAdv. Neural Inform. Process. Syst., 2017, pp.\n6000–6010.\n[14] J. Devlin, M.-W. Chang, K. Lee, and\nK. Toutanova, “BERT: Pre-training of\ndeep bidirectional transformers for language\nunderstanding,” in NAACL-HLT, 2019, pp.\n4171–4186.\n[15] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell,\nQ. Le, and R. Salakhutdinov, “Transformer-\nXL: Attentive language models beyond a\nfixed-length context,” in ACL, 2019, pp.\n2978–2988.\n[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov,\nD. Weissenborn, X. Zhai, T. Unterthiner,\nM. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, and N. Houlsby, “An\nimage is worth 16x16 words: Transformers\nfor image recognition at scale,” in Int. Conf.\nLearn. Represent., 2021.\n[17] B. Heo, S. Yun, D. Han, S. Chun, J. Choe,\nand S. J. Oh, “Rethinking spatial dimensions\nof vision transformers,” inInt. Conf. Comput.\nVis., 2021, pp. 11 936–11 945.\n[18] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei,\nZ. Zhang, S. Lin, and B. Guo, “Swin trans-\nformer: Hierarchical vision transformer using\nshifted windows,” in Int. Conf. Comput. Vis.,\n2021, pp. 10 012–10 022.\n[19] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song,\nD. Liang, T. Lu, P. Luo, and L. Shao, “Pyra-\nmid vision transformer: A versatile backbone\nfor dense prediction without convolutions,” in\nSpringer Nature 2021 LATEX template\nHAT 13\nInt. Conf. Comput. Vis., 2021, pp. 568–578.\n[20] W. Xu, Y. Xu, T. Chang, and Z. Tu, “Co-\nScale conv-attentional image transformers,”\nin Int. Conf. Comput. Vis., 2021, pp. 9981–\n9990.\n[21] H. Fan, B. Xiong, K. Mangalam, Y. Li,\nZ. Yan, J. Malik, and C. Feichtenhofer, “Mul-\ntiscale vision transformers,” in Int. Conf.\nComput. Vis., 2021, pp. 6824–6835.\n[22] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang,\nC. Feichtenhofer, and J. Hoffman, “Token\nmerging: Your ViT but faster,” in Int. Conf.\nLearn. Represent., 2023.\n[23] Y. LeCun, L. Bottou, Y. Bengio, and\nP. Haffner, “Gradient-based learning applied\nto document recognition,” Proceedings of the\nIEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n[24] O. Russakovsky, J. Deng, H. Su, J. Krause,\nS. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein et al., “ImageNet\nlarge scale visual recognition challenge,” Int.\nJ. Comput. Vis., vol. 115, no. 3, pp. 211–252,\n2015.\n[25] R. K. Srivastava, K. Greff, and J. Schmid-\nhuber, “Highway networks,” arXiv preprint\narXiv:1505.00387, 2015.\n[26] C. Szegedy, W. Liu, Y. Jia, P. Sermanet,\nS. Reed, D. Anguelov, D. Erhan, V. Van-\nhoucke, and A. Rabinovich, “Going deeper\nwith convolutions,” in IEEE Conf. Comput.\nVis. Pattern Recog., 2015, pp. 1–9.\n[27] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens,\nand Z. Wojna, “Rethinking the inception\narchitecture for computer vision,” in IEEE\nConf. Comput. Vis. Pattern Recog., 2016, pp.\n2818–2826.\n[28] C. Szegedy, S. Ioffe, V. Vanhoucke, and\nA. Alemi, “Inception-v4, Inception-ResNet\nand the impact of residual connections on\nlearning,” in AAAI Conf. Artif. Intell., 2017,\npp. 4278–4284.\n[29] S. Xie, R. Girshick, P. Doll´ ar, Z. Tu, and\nK. He, “Aggregated residual transformations\nfor deep neural networks,” in IEEE Conf.\nComput. Vis. Pattern Recog., 2017, pp. 1492–\n1500.\n[30] G. Huang, Z. Liu, L. Van Der Maaten, and\nK. Q. Weinberger, “Densely connected convo-\nlutional networks,” in IEEE Conf. Comput.\nVis. Pattern Recog., 2017, pp. 4700–4708.\n[31] A. G. Howard, M. Zhu, B. Chen,\nD. Kalenichenko, W. Wang, T. Weyand,\nM. Andreetto, and H. Adam, “MobileNets:\nEfficient convolutional neural networks for\nmobile vision applications,” arXiv preprint\narXiv:1704.04861, 2017.\n[32] M. Sandler, A. Howard, M. Zhu, A. Zhmogi-\nnov, and L.-C. Chen, “MobileNetV2: Inverted\nresiduals and linear bottlenecks,” in IEEE\nConf. Comput. Vis. Pattern Recog., 2018, pp.\n4510–4520.\n[33] X. Zhang, X. Zhou, M. Lin, and J. Sun,\n“ShuffleNet: An extremely efficient convolu-\ntional neural network for mobile devices,”\nin IEEE Conf. Comput. Vis. Pattern Recog.,\n2018, pp. 6848–6856.\n[34] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun,\n“ShuffleNet V2: Practical guidelines for effi-\ncient CNN architecture design,” inEur. Conf.\nComput. Vis., 2018, pp. 116–131.\n[35] M. Tan, B. Chen, R. Pang, V. Vasude-\nvan, M. Sandler, A. Howard, and Q. V.\nLe, “MnasNet: Platform-aware neural archi-\ntecture search for mobile,” in IEEE Conf.\nComput. Vis. Pattern Recog., 2019, pp. 2820–\n2828.\n[36] M. Tan and Q. Le, “EfficientNet: Rethinking\nmodel scaling for convolutional neural net-\nworks,” in Int. Conf. Mach. Learn., 2019, pp.\n6105–6114.\n[37] M. Jaderberg, K. Simonyan, A. Zisserman,\nand K. Kavukcuoglu, “Spatial transformer\nnetworks,” in Adv. Neural Inform. Process.\nSyst., 2015, pp. 2017–2025.\nSpringer Nature 2021 LATEX template\n14 HAT\n[38] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao,\nW. Liu, and T.-S. Chua, “SCA-CNN: Spatial\nand channel-wise attention in convolutional\nnetworks for image captioning,” in IEEE\nConf. Comput. Vis. Pattern Recog., 2017, pp.\n5659–5667.\n[39] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li,\nH. Zhang, X. Wang, and X. Tang, “Residual\nattention network for image classification,”\nin IEEE Conf. Comput. Vis. Pattern Recog.,\n2017, pp. 3156–3164.\n[40] J. Hu, L. Shen, S. Albanie, G. Sun, and\nE. Wu, “Squeeze-and-Excitation networks,”\nIEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 42, no. 8, pp. 2011–2023, 2020.\n[41] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon,\n“CBAM: Convolutional block attention mod-\nule,” in Eur. Conf. Comput. Vis., 2018, pp.\n3–19.\n[42] J. Park, S. Woo, J.-Y. Lee, and I. S. Kweon,\n“BAM: Bottleneck attention module,” in\nBrit. Mach. Vis. Conf., 2018, p. 147.\n[43] X. Li, W. Wang, X. Hu, and J. Yang, “Selec-\ntive kernel networks,” in IEEE Conf. Com-\nput. Vis. Pattern Recog., 2019, pp. 510–519.\n[44] X. Wang, R. Girshick, A. Gupta, and K. He,\n“Non-local neural networks,” in IEEE Conf.\nComput. Vis. Pattern Recog., 2018, pp. 7794–\n7803.\n[45] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin,\nZ. Zhang, Y. Sun, T. He, J. Mueller, R. Man-\nmatha et al., “ResNeSt: Split-attention net-\nworks,” in IEEE Conf. Comput. Vis. Pattern\nRecog., 2022, pp. 2736–2746.\n[46] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi,\nZ.-H. Jiang, F. E. Tay, J. Feng, and S. Yan,\n“Tokens-to-token ViT: Training vision trans-\nformers from scratch on ImageNet,” in Int.\nConf. Comput. Vis., 2021, pp. 558–567.\n[47] H. Touvron, M. Cord, A. Sablayrolles,\nG. Synnaeve, and H. J´ egou, “Going deeper\nwith image transformers,” in Int. Conf. Com-\nput. Vis., 2021, pp. 32–42.\n[48] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian,\nQ. Hou, and J. Feng, “DeepViT: Towards\ndeeper vision transformer,” arXiv preprint\narXiv:2103.11886, 2021.\n[49] H. Touvron, M. Cord, M. Douze, F. Massa,\nA. Sablayrolles, and H. J´ egou, “Training\ndata-efficient image transformers & distilla-\ntion through attention,” in Int. Conf. Mach.\nLearn., 2021, pp. 10 347–10 357.\n[50] A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens,\nP. Abbeel, and A. Vaswani, “Bottleneck\ntransformers for visual recognition,” in IEEE\nConf. Comput. Vis. Pattern Recog., 2021, pp.\n16 519–16 529.\n[51] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov,\nL. Beyer, X. Zhai, T. Unterthiner, J. Yung,\nA. Steiner, D. Keysers, J. Uszkoreit et al.,\n“Mlp-Mixer: An all-MLP architecture for\nvision,” Adv. Neural Inform. Process. Syst.,\npp. 24 261–24 272, 2021.\n[52] H. Liu, Z. Dai, D. So, and Q. V. Le, “Pay\nattention to MLPs,” Adv. Neural Inform.\nProcess. Syst., pp. 9204–9215, 2021.\n[53] Q. Hou, Z. Jiang, L. Yuan, M.-M. Cheng,\nS. Yan, and J. Feng, “Vision Permutator: A\npermutable MLP-like architecture for visual\nrecognition,” IEEE Trans. Pattern Anal.\nMach. Intell., vol. 45, no. 1, pp. 1328–1334,\n2022.\n[54] Z. Wang, Y. Hao, X. Gao, H. Zhang, S. Wang,\nT. Mu, and X. He, “Parameterization of\ncross-token relations with relative positional\nencoding for vision MLP,” in ACM Int. Conf.\nMultimedia, 2022, pp. 6288–6299.\n[55] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and\nY. Wang, “Transformer in transformer,” in\nAdv. Neural Inform. Process. Syst., 2021, pp.\n15 908–15 919.\n[56] Y. Li, K. Zhang, J. Cao, R. Timofte, and\nL. Van Gool, “LocalViT: Bringing local-\nity to vision transformers,” arXiv preprint\narXiv:2104.05707, 2021.\nSpringer Nature 2021 LATEX template\nHAT 15\n[57] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and\nW. Wu, “Incorporating convolution designs\ninto visual transformers,” in Int. Conf. Com-\nput. Vis., 2021, pp. 579–588.\n[58] D. Hendrycks and K. Gimpel, “Gaussian\nerror linear units (GELUs),” arXiv preprint\narXiv:1606.08415, 2016.\n[59] S. Elfwing, E. Uchibe, and K. Doya,\n“Sigmoid-weighted linear units for neural net-\nwork function approximation in reinforce-\nment learning,” Neural Networks, vol. 107,\npp. 3–11, 2018.\n[60] A. Paszke, S. Gross, F. Massa, A. Lerer,\nJ. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga et al., “PyTorch:\nAn imperative style, high-performance deep\nlearning library,” inAdv. Neural Inform. Pro-\ncess. Syst., 2019, pp. 8026–8037.\n[61] H. Zhang, M. Ciss´ e, Y. N. Dauphin, and\nD. Lopez-Paz, “mixup: Beyond empirical risk\nminimization,” in Int. Conf. Learn. Repre-\nsent., 2018.\n[62] I. Loshchilov and F. Hutter, “Decoupled\nweight decay regularization,” in Int. Conf.\nLearn. Represent., 2019.\n[63] ——, “SGDR: Stochastic gradient descent\nwith warm restarts,” in Int. Conf. Learn.\nRepresent., 2017.\n[64] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song,\nD. Liang, T. Lu, P. Luo, and L. Shao,\n“PVTv2: Improved baselines with pyramid\nvision transformer,” Computational Visual\nMedia, vol. 8, no. 3, pp. 415–424, 2022.\n[65] I. Radosavovic, R. P. Kosaraju, R. Girshick,\nK. He, and P. Doll´ ar, “Designing network\ndesign spaces,” in IEEE Conf. Comput. Vis.\nPattern Recog., 2020, pp. 10 428–10 436.\n[66] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai,\nL. Yuan, and L. Zhang, “CvT: Introducing\nconvolutions to vision transformers,” in Int.\nConf. Comput. Vis., 2021, pp. 22–31.\n[67] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren,\nX. Wei, H. Xia, and C. Shen, “Twins: Revis-\niting the design of spatial attention in vision\ntransformers,” in Adv. Neural Inform. Pro-\ncess. Syst., 2021, pp. 9355–9366.\n[68] B. Zhou, H. Zhao, X. Puig, S. Fidler,\nA. Barriuso, and A. Torralba, “Scene parsing\nthrough ADE20K dataset,” in IEEE Conf.\nComput. Vis. Pattern Recog., 2017, pp. 633–\n641.\n[69] A. Kirillov, R. Girshick, K. He, and P. Doll´ ar,\n“Panoptic feature pyramid networks,” in\nIEEE Conf. Comput. Vis. Pattern Recog.,\n2019, pp. 6399–6408.\n[70] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and\nP. Doll´ ar, “Focal loss for dense object detec-\ntion,” in Int. Conf. Comput. Vis., 2017, pp.\n2980–2988.\n[71] T.-Y. Lin, M. Maire, S. Belongie, J. Hays,\nP. Perona, D. Ramanan, P. Doll´ ar, and C. L.\nZitnick, “Microsoft COCO: Common objects\nin context,” in Eur. Conf. Comput. Vis. ,\n2014, pp. 740–755.\n[72] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan,\nL. Zhang, and J. Gao, “Multi-scale vision\nLongformer: A new vision transformer for\nhigh-resolution image encoding,” in Int.\nConf. Comput. Vis., 2021, pp. 2998–3008.\n[73] M. Contributors, “MMSegmentation: Open-\nMMLab semantic segmentation toolbox\nand benchmark,” https://github.com/\nopen-mmlab/mmsegmentation, 2020.\n[74] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong,\nX. Li, S. Sun, W. Feng, Z. Liu, J. Xu\net al., “MMDetection: Open MMLab detec-\ntion toolbox and benchmark,” arXiv preprint\narXiv:1906.07155, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7853598594665527
    },
    {
      "name": "Transformer",
      "score": 0.6848665475845337
    },
    {
      "name": "Security token",
      "score": 0.597676694393158
    },
    {
      "name": "Grid",
      "score": 0.5519864559173584
    },
    {
      "name": "Convolutional neural network",
      "score": 0.506738007068634
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.472675621509552
    },
    {
      "name": "Code (set theory)",
      "score": 0.4640136957168579
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.46279478073120117
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4557792842388153
    },
    {
      "name": "Programming language",
      "score": 0.07860496640205383
    },
    {
      "name": "Mathematics",
      "score": 0.06535428762435913
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}