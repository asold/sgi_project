{
  "title": "A Case Study on the Generative AI Project Life Cycle Using Large Language Models",
  "url": "https://openalex.org/W4393057931",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2556776446",
      "name": "Ajay Bandi",
      "affiliations": [
        "Northwest Missouri State University"
      ]
    },
    {
      "id": "https://openalex.org/A5094215211",
      "name": "Hemanth Kagitha",
      "affiliations": [
        "Northwest Missouri State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385416665",
    "https://openalex.org/W4391093460",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4385738962",
    "https://openalex.org/W4312705522",
    "https://openalex.org/W11313625",
    "https://openalex.org/W4383737134",
    "https://openalex.org/W1587277858",
    "https://openalex.org/W4384023759",
    "https://openalex.org/W2980272569",
    "https://openalex.org/W4386721862",
    "https://openalex.org/W4318350412",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4285293250",
    "https://openalex.org/W4390757471",
    "https://openalex.org/W4324297016",
    "https://openalex.org/W4200171560",
    "https://openalex.org/W4387902603",
    "https://openalex.org/W4324299222",
    "https://openalex.org/W4366587430",
    "https://openalex.org/W3089524665",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4386322180",
    "https://openalex.org/W4385776110",
    "https://openalex.org/W4387156851",
    "https://openalex.org/W3110665467",
    "https://openalex.org/W4387156820",
    "https://openalex.org/W4399203757",
    "https://openalex.org/W4389736593",
    "https://openalex.org/W4379251575",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W4387321091",
    "https://openalex.org/W4383473003",
    "https://openalex.org/W4389364428"
  ],
  "abstract": "Large Language Models represent a disruptive technology set to revolutionize the fu- ture of artificial intelligence. While numerous literature reviews and survey articles discuss their benefits and address security and compliance concerns, there remains a shortage of research exploring the implementation life cycle of generative AI systems. This paper addresses this gap by presenting the various phases of the generative AI life cycle and detailing the development of a chatbot designed to address inquiries from prospective stu- dents. Utilizing Google Flan LLM and a question-answering pipeline, we processed user prompts. In addition, we compiled an input file containing domain knowledge of the edu- cation program, which was preprocessed and condensed into vector embeddings using the HuggingFace library. Furthermore, we designed a chat interface for user interaction using Streamlit. The responses generated by the chatbot are both descriptive and contextu- ally pertinent to the prompts, with their quality improving in response to more detailed prompts. However, a significant constraint is the size limit of the input file, given the processing power limitations of CPUs.",
  "full_text": "EPiC Series in Computing\nVolume 98, 2024, Pages 189–199\nProceedings of 39th International Confer-\nence on Computers and Their Applications\nA Case Study on the Generative AI Project Life Cycle\nUsing Large Language Models\nAjay Bandi and Hemanth Kagitha\nSchool of Computer Science and Information Systems\nNorthwest Missouri State University\nMaryville, Missouri, USA\najay@nwmissouri.edu, hemanthkagithaa@gmail.com\nAbstract\nLarge Language Models represent a disruptive technology set to revolutionize the fu-\nture of artificial intelligence. While numerous literature reviews and survey articles discuss\ntheir benefits and address security and compliance concerns, there remains a shortage of\nresearch exploring the implementation life cycle of generative AI systems. This paper\naddresses this gap by presenting the various phases of the generative AI life cycle and\ndetailing the development of a chatbot designed to address inquiries from prospective stu-\ndents. Utilizing Google Flan LLM and a question-answering pipeline, we processed user\nprompts. In addition, we compiled an input file containing domain knowledge of the edu-\ncation program, which was preprocessed and condensed into vector embeddings using the\nHuggingFace library. Furthermore, we designed a chat interface for user interaction using\nStreamlit. The responses generated by the chatbot are both descriptive and contextu-\nally pertinent to the prompts, with their quality improving in response to more detailed\nprompts. However, a significant constraint is the size limit of the input file, given the\nprocessing power limitations of CPUs.\n1 Introduction\nLLM stands for Large Language Model, an artificial intelligence model capable of understanding\nand generating human-like text. These models are trained on vast amounts of text data and can\nperform tasks such as text generation, text summarization, machine translation, and question-\nanswering [32]. LLMs have various applications across industries including, natural language\nprocessing (NLP), chatbots, content generation, and sentiment analysis [18]. They are crucial\nin advancing AI technology and revolutionizing human-computer interactions [11]. LLMs utilize\na transformer architecture model, serving as a foundation for generative AI applications, along\nwith generative adversarial networks (GANs) [26] and variational autoencoders (VAEs) [30].\nSeveral researchers focus on reviewing the literature on generative AI and LLM technology in\nvarious application domains. Some of the application areas are dialog conversation in healthcare\n[27], detection of marine litter [20], drug detection [8], and in business and finance [12]. Mesk´ o\nand Topol [25] presented potential risks in using LLMs in healthcare, categorizing them into\nthree categories, and discussed the regulatory challenges of using LLMs. These risks include\nA. Bandi, M. Hossain and Y. Jin (eds.), CATA2024 (EPiC Series in Computing, vol. 98), pp. 189–199\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\ndata privacy, intellectual property rights, lack of informed consent, and bias, among several\nothers [34]. However, there is a gap in the literature regarding the finding of evidence supporting\nthe implementation of generative AI systems or their usage as case studies in research [9]. This\npaper focuses on implementing a generative AI chatbot for an educational program, that utilizes\nLLM to generate new responses for prospective graduate students.\nThe remainder of the paper is organized as follows. Section 2 describes the hardware,\nsoftware, and user experience requirements of generative AI systems. Section 3 discusses the\ndifferent phases of the generative AI project life cycle. Section 4 presents a case study of chatbot\nimplementation using LLM and its results. Section 5 presents research conclusions.\n2 Requirements of Generative AI systems\nUnderstanding the requirements of implementing a generative AI system is essential. This\nsection discusses the three types of requirements: hardware, software, and user experience [5].\nLLMs have significant hardware requirements due to their computational intensity [33].\nThey typically necessitate high-performance CPUs with multiple cores and high clock speeds\nto efficiently process large text datasets. Graphics Processing Units (GPUs) are commonly\nemployed to accelerate training and inference tasks by parallelizing computations [29]. Alter-\nnatively, Tensor Processing Units (TPUs), known for their exceptional speed and energy effi-\nciency, are increasingly utilized, especially in large-scale deep learning applications. Sufficient\nmemory, comprising both RAM and storage space, is essential for storing model parameters,\ninput data, and intermediate results during computations, while fast storage solutions such as\nSSDs or NVMe drives play a critical role in housing large datasets, model checkpoints, and\ntraining logs, thereby reducing data loading times and improving overall training through-\nput. Also, high-speed networking capabilities may be necessary for accessing and transferring\nlarge datasets stored on remote servers or cloud platforms. Overall, meeting the hardware re-\nquirements of LLMs often involves investing in robust computing infrastructure or leveraging\ncloud-based solutions to ensure efficient training and deployment processes [16].\nLLMs require a specific set of software tools and frameworks to facilitate their develop-\nment, training, and deployment [3]. Key components include deep learning frameworks like\nTensorFlow, PyTorch, or Hugging Face’s Transformers library, which provide the foundational\ninfrastructure for building and training neural network models. Natural Language Processing\n(NLP) libraries such as NLTK, spaCy, and Hugging Face’s Transformers offer specialized tools\nfor text processing and analysis, essential for tasks like tokenization and feature extraction [10].\nHardware acceleration libraries like CUDA and software integrations for GPUs or TPUs enable\nefficient computation during training and inference [15]. In addition, access to model repos-\nitories and pre-trained models, along with development environments like PyCharm Jupyter\nNotebooks, or Google Colabs streamline the development process [24]. Deployment frame-\nworks such as TensorFlow Serving and cloud computing platforms like Amazon Web Services\n(AWS) or Google Cloud Platform provide the infrastructure needed to deploy and manage\nLLMs in production environments. These platforms provide scalable and reliable [31] solutions\nfor hosting LLMs, allowing organizations to handle increased workloads and ensure consistent\nperformance seamlessly.\nGenerative AI systems require a user experience that prioritizes intuitive interaction, clear\ncommunication, and customization options. Users should find the interface user-friendly and\neasy to navigate, with clear communication about the system’s capabilities and limitations [23].\nFeedback mechanisms should indicate when the system is processing input and generating out-\nput, while robust error handling ensures a smooth experience [13]. Customization options allow\n190\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\nFigure 1: Phases of generative AI project life cycle\nusers to tailor the output to their preferences, while transparency about the system’s operation\nand data usage fosters trust. Privacy and security are paramount, with clear communication\nand control over user data [2, 6]. Guidance and assistance should be available, especially for\nusers unfamiliar with generative AI technology, and consistency across interactions and plat-\nforms ensures a cohesive experience. Accessibility considerations ensure the system is usable by\nall users, while performance optimization minimizes latency. Ethical considerations, including\naddressing biases and ethical implications, are also essential for responsible system design [28].\n3 Phases of Generative AI Project Life Cycle\nThe details and enhancements of the software development life cycle and data science or data\nanalytics life cycle projects are already known. This paper focuses on the generative AI (AIGC)\nproject life cycle. The different phases in this life cycle are built on the work authored by David\nBaum from Snowflake [7]. Figure 1 shows the different phases. The first phase of AIGC is\nspecifically on identifying the business use case and defining the scope. In this phase, the task\nis to determine the type of content generation required, such as customized descriptions of\ncostumes, translation between languages, text summarization, synthetic data generation, music\ncreation from lyrics, or providing rapid responses by generating answers for customers.\nThe second phase involves determining the intellectual data necessary for effectively cus-\ntomizing the model. LLMs are pre-trained on massive amounts of data sourced from various\nexisting repositories, including websites, research articles, and source code repositories [18].\nTypically, this dataset spans petabytes in size, encompassing domain knowledge. A clear un-\nderstanding of the business use case is crucial for defining the data and user requirements.\nThe third phase involves selecting an appropriate LLM from the available options. Numerous\nopen-source LLMs exist, including Bloom, Bret, Falcon, X Gen-7B, LLAMA, GPT-NeoX, and\nGPT-J, among others. Projects like LLAMA offer readily accessible resources that can be\nmodified and deployed within our environments [1]. These LLMs boast billions of parameters,\nenhancing their ability to produce precise and contextually relevant outcomes. However, their\nextensive parameterization demands significant training resources to tailor them to our specific\nrequirements. The decision between employing large or small models entails striking a balance\nbetween cost and performance. Developers must carefully evaluate their needs and resources\nto determine the most suitable LLM for their particular use case.\nThe fourth phase is adapting LLMs to the use case. To perform this prompt engineer-\n191\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\ning, fine-tuning parameters, and human reinforcement learning from human feedback (RLHF)\ntechniques are used to meet specific needs [4]. Prompt Engineering is the process of designing\neffective prompts or inputs for language models to generate desired outputs, particularly for\nthose based on the Transformer architecture like GPT (Generative Pre-trained Transformer)\nmodels [19]. Language models like GPT are trained to generate text based on the input they\nreceive, making the quality and specificity of the input. Fine-tuning allows for the adaptation\nof an LLM to specific tasks by updating its pre-trained parameters, thereby offering users the\nflexibility to customize LLMs according to their needs and achieve improved performance across\nvarious applications. This process involves selecting a relevant pre-trained LLM, refining it with\nrelated datasets, and training the model to generate responses tailored to specific prompts. The\nfine-tuned LLM is then evaluated to ensure it meets the desired requirements, with adjustments\nto parameters like learning rate and batch size made as necessary to optimize outcomes. Rein-\nforcement Learning from Human Feedback (RLHF) is a technique used to refine and enhance\nthe performance of artificial intelligence systems, particularly in the domain of natural language\nprocessing [4]. RLHF involves training models, such as chatbots, to engage in more natural and\ncontextually relevant conversations by incorporating direct feedback from human interactions.\nThis approach aims to improve the model’s understanding of human prompts, refine its abil-\nity to generate responses that align with user preferences, and mitigate the risk of generating\ninappropriate or harmful content. RLHF holds significant potential across various sectors, fa-\ncilitating the development of personalized assistants for businesses, customized learning plans\nfor educational purposes, individualized treatment strategies in healthcare, and tailored rec-\nommendations in entertainment. Furthermore, RLHF serves to enhance model performance\nwhile also addressing concerns related to the adoption of internet-trained models, including the\npropagation of undesirable language patterns [4].\nThe final phase is the implementation of the app by deploying it into containers. DevOps\nteams often utilize containerization software like Docker to streamline the deployment of LLM\napplications, ensuring consistency across diverse computing environments [17]. Despite the ben-\nefits containers offer for sophisticated AI models with specialized processing needs and access to\nlarge datasets, the complexity of managing containerized workloads at scale can divert valuable\ntime and resources from application development. A viable solution involves adopting a cloud\ndata platform that simplifies the deployment, management, and scalability of LLMs and other\ncontainerized workloads within a fully managed infrastructure. This approach allows teams to\nexecute LLM jobs in a governed environment, leverage configurable hardware options such as\nGPUs, and access a scalable pool of compute resources without the burden of infrastructure\nmanagement [21]. In addition, integrating with third-party providers via marketplace apps\nfurther enhances flexibility and accessibility for developers and data scientists, enabling them\nto focus on solving business challenges rather than managing compute and storage resources.\n4 Case Study\nThis section explains the phases of the generative AI project life cycle for the implemented\nchatbot. The detailed steps are given below.\n1. Identify the business use case: Develop a chatbot for a higher education graduate program\nto answer questions from students all over the world, with a majority of the student\npopulation being from India. The chatbot needs to generate new and contextually relevant\nanswers for the users. The chatbot also needs to understand the cultural vocabulary of\nthe users.\n192\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\nFigure 2: Architecture for implementation of chatbot using LLM\n2. Determine the intellectual or proprietary data: The data for the graduate program is\nobtained from the program coordinator and the website, and it has been saved in a .txt file\nusing key-value pairs. Each piece of information, such as program details, course offerings,\nfaculty information, and application deadlines, is stored with a corresponding key that\nuniquely identifies it. Information from the .txt file is then extracted and compressed into a\nnumerical representation within a continuous vector space. This numerical representation\nof words in vector format is referred to as embeddings. The purpose of embeddings is to\ncapture semantic and syntactic relationships between words in the .txt file.\n3. Selecting an LLM: LLMs are pre-trained with vast data corpora. In this case study, FLAN-\nT5 XXL by Google was chosen [14]. This model specializes in Text-to-Text Generation\nacross three different languages and is built using transformers, PyTorch, and TensorFlow\nframeworks. With a size of 11.3 billion parameters, it offers significant computational\npower. The datasets used to train this LLM are gsm8k, lambada, aqua rat, esnli, quasc,\nqrec, djaym7/wiki dialog, and qed. The rationale behind choosing this LLM lies in its\nsuitability for question-answering pipelines in chatbots and its prowess in text-to-text\ngeneration tasks. To implement the chatbot, Google Colab was utilized to write the\nsource code in Python and import the necessary libraries. Further details on the chatbot\nimplementation are provided in the final phase.\n4. Adapting an LLM for use: Adapting the question-answering pipeline of the LLM to\ndevelop a chatbot through fine-tuning and prompt engineering. Fine-tuning the LLM\ninvolves collecting user feedback, and iteratively collecting and integrating user feedback\nto enhance the model’s ability to generate accurate and contextually relevant responses.\n5. Implementing the app by deploying it to a cloud service platform: Implementing the\nchatbot app and utilizing Streamlit for the chat interface. Utilizing Streamlit, we aim to\ndesign a user-friendly chat interface modified to meet diverse user needs and preferences.\nOur interface will seamlessly accommodate a wide range of user inputs and responses, by\nproviding a realistic and engaging interaction experience.\n193\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\nFigure 3: User interface of chatbot\n4.1 Implementation of chatbot using LLM\nThis section presents the detailed steps to implement a chatbot. The source code of the chatbot\nis given in our GitHub repository1. The high-level steps are shown in Figure 2. The screenshot\nof the chat interface and the sample prompt and its response is shown in Figure 3.\nLangChain is a framework for developing applications powered by LLMs. It enables applica-\ntions that connect a language model to sources of context. This integrates various components\nsuch as embedding, vector databases, LLMs, etc. Using these components, we can provide\nexternal documents to the significant language models and build AI applications such as chat-\nbots seamlessly. Faiss-cpu, Facebook AI Similarity Search, is a Python library, used for efficient\nsimilarity search, embeddings of multimedia documents that are similar to each other, and clus-\ntering of dense vectors. The sentence-transformers package enables the utilization of pre-trained\nmodels for sentence embeddings in Python projects. Sentence embeddings are numerical repre-\nsentations of sentences that capture their semantic meanings. These embeddings can be useful\nfor NLP tasks, such as semantic similarity, clustering, classification, and information retrieval.\nThe TextLoader class is used to import the input .txt file. This file consists of the data\nin the form of key and value pairs. These key-value pairs are the information about the grad-\nuate program. The key is the sample prompt and the value is the relevant response to the\nprompt. The textwrap module is used for wrapping and formatting of plain text in docu-\nments. The os module is used for interacting with the operating system to access and manipu-\n1https://github.com/bandiajay/GenAIChatbot\n194\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\nTable 1: Sample prompts and responses from chatbot\nInput prompt Chatbot Response\nRelevant\nre-\nsponse\n1. tell me about the\ncourse overview\nNorthwest Missouri State University’s Master of\nScience in Applied Computer Science is a\n16-month program that offers a hands-on,\nprofessional education in today’s high demand\nareas. Yes\n2. How many credit\nhours are required to\ngraduate from ACS\nCourse?\nThirty credit hours of graduate credits are\nrequired for completion of the M.S. in Applied\nComputer Science. Yes\n3. Provide the list of all\ncourses that were taught\nby Dr. Ajay Bandi\nGenerative AI, Mobile computing iOS, Patterns\nand frameworks, Advance Databases, Java, Big\nData Yes\n4. What are the\ndocuments required for\ngetting admission in\nmasters in ACS course?\nOfficial English proficiency scores (TOEFL,\nIELTS, PTE, DUOLINGO) Bank statement and\nStatement of support. Copy of your passport\nOfficial transcripts from all colleges and\nuniversities you have attended or are now\nattending for undergraduate and graduate\ncoursework Yes\n5. Can you tell all the\navailable scholarships for\nACS students?\nAny student who gets admitted to our program\nwill receive the $500 scholarship for the first\nsemester. In any semester, if you get a full\ngraduate assistantship or 3/4th graduate\nassistantship, or 1/2 graduate assistantship, your\n100% or 75% or 50% of the tuition fee will be\nwaived off, respectively. Yes\n6. What is the tuition\nfee for a student in MS\nACS course? Total Graduate Cost for ACS is $16,802.20 Yes\n7. What is the Last date\nto apply for spring\nsemester ? November 1 Yes\nlate (read, write, open) the input file. The text is then split into a smaller chunk of data, using\na CharacterTextSplitter object, which is provided by langchain.text splitter module.\nThe size of each chunk is set to 100 characters, with no overlap between consecutive text chunks.\nThis is to control how the text will be split into smaller units. Then the langchain framework\nthat integrates with popular embedding models, such as ”HuggingFaceEmbeddings” allows to\ngenerate embeddings for text document. These embeddings capture semantic meaning and\ncontextual information, enabling more advanced NLP tasks such as similarity search.\nThe LLM used in this case study is google/flan-t5-xxl [14, 22], and it is adapted for the\nquestion-answering pipeline by importing the load qa chain function from the module called\nlangchain.chains.question answering. The HuggingFaceHub class from the LangChain\nmodule is used to obtain access to the Flan LLM without the need for explicit downloading.\n195\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\nThe user prompt is then converted into vector embeddings using the LLM. The randomness\nand diversity of the output has been adjusted by setting the temperature parameter to 0.8,\nand the maximum length parameter is set to 512, which defines the maximum length of the\ninput prompt that the LLM processes. Finally, a similarity search is performed on both the\nembedding spaces of the input file and the user prompt. A similarity metric is used to identify\nthe maximum similarity score between both vectors. Once similar vectors are identified, the\noutput can be generated based on the application context. An interactive web application is\ndeveloped using Streamlit library for the user to send a prompt and receive a response. A\nfeedback form is also provided for the user in the chat application.\n4.2 Results\nFigure 3 shows the user interface of the chatbot, where the user can enter prompts and receive\nresponses. Table 1 displays sample results of the Applied Computer Science (ACS) chatbot’s\nprompts and responses. The chatbot’s responses are descriptive and contextually relevant to\nthe prompts. However, for prompt #5, the expected answer is ‘Yes’, but the response provides\ndetailed information about all available scholarships for students. The LLM is adapting to the\nbusiness case by adjusting the prompts used to elicit responses. Through experimentation, it has\nbeen observed that providing more refined or optimized prompts leads to improved outcomes.\nAs a result, the LLM’s performance is enhanced as it learns to generate more accurate and\nrelevant responses tailored to the specific requirements of the business case. This iterative\nprocess of refining prompts allows the LLM to better understand the context leading to better\nresults.\nThe chatbot responses exhibit a high degree of relevance and accuracy across all input\nprompts. For instance, in response to Prompt #1 regarding the course overview, the chat-\nbot provides a concise yet informative overview of the Master of Science program in ACS,\ndemonstrating both relevance and accuracy in addressing the prompt. Similarly, for Prompt\n#2 concerning the required credit hours for graduation, the chatbot response accurately states\nthe requisite thirty credit hours, ensuring both relevance and accuracy. Moreover, in response\nto Prompt #3 querying about courses taught by Dr. Ajay Bandi, the chatbot lists relevant\ncourses, aligning closely with the prompt’s requirements and displaying accuracy in its response.\nAdditionally, for Prompt #4 regarding admission documents, the chatbot furnishes a compre-\nhensive list of required documents, effectively addressing the prompt with both relevance and\naccuracy. Furthermore, in response to Prompt #5 regarding available scholarships, the chatbot\nprovides relevant information about tuition fee waivers based on assistantships, demonstrating\nboth relevance and accuracy. Similarly, for Prompt #6 concerning tuition fees, the chatbot\nresponse accurately provides the total graduate cost for the ACS program, ensuring both rel-\nevance and accuracy. Finally, for Prompt #7 regarding the application deadline, the chatbot\nspecifies the last date to apply for the spring semester, exhibiting both pertinence and accuracy\nin its response. Overall, the chatbot’s responses consistently align with the respective prompts,\ndisplaying both relevance and accuracy in addressing various inquiries related to the Master of\nScience program in ACS.\n5 Conclusion\nIn conclusion, this paper highlights the transformative impact of LLMs on reshaping the land-\nscape of artificial intelligence. Through this study, we have presented the various stages of\nthe generative AI life cycle and illustrated the development of a chatbot tailored to address\n196\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\ninquiries from prospective students. Leveraging tools such as Google Flan LLM and the Hug-\ngingFace library, we have successfully processed user prompts and extracted domain knowledge\ninto compact vector embeddings. A similarity check between the input file embedding and the\nprompt embeddings is performed to generate responses for the user. Additionally, the inte-\ngration of Streamlit has facilitated seamless user interaction through the chat interface. Our\nfindings demonstrate that the responses generated by the chatbot are not only descriptive but\nalso contextually relevant, with their efficacy improving in response to more detailed prompts.\nHowever, a significant challenge lies in the limitation imposed by the size constraint of input\nfiles, particularly considering the computational constraints of CPUs. In the future, addressing\nthese constraints by utilizing GPUs and further refining the implementation process will be\nessential for unlocking the full potential of generative AI systems in real-world applications\nacross various domains.\nReferences\n[1] Meysam Alizadeh, Ma¨ el Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Ko-\nrobeynikova, and Fabrizio Gilardi. Open-source large language models outperform crowd workers\nand approach chatgpt in text-annotation tasks. arXiv preprint arXiv:2307.02179 , 2023.\n[2] Danielle Allen and E Glen Weyl. The real dangers of generative ai. Journal of Democracy ,\n35(1):147–162, 2024.\n[3] Chetan Arora, John Grundy, and Mohamed Abdelrazek. Advancing requirements engineering\nthrough generative ai: Assessing the role of llms. arXiv preprint arXiv:2310.13976 , 2023.\n[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,\n2022.\n[5] Ajay Bandi, Pydi Venkata Satya Ramesh Adapa, and Yudu Eswar Vinay Pratap Kumar Kuchi.\nThe power of generative ai: A review of requirements, models, input–output formats, evaluation\nmetrics, and challenges. Future Internet, 15(8):260, 2023.\n[6] Ajay Bandi, Abdelaziz Fellah, and Harish Bondalapati. Embedding security concepts in introduc-\ntory programming courses. Journal of Computing Sciences in Colleges , 34(4):78–89, 2019.\n[7] David Baum. Generative AI and LLMs: Snowflake Special Edition . John Wiley Sons, 2024.\n[8] Ankan Bera, Rik Das, Sayantani Ghosh, Raktim Chakraborty, Indranil Mitra, and Prasun Nandy.\nHarnessing transformers for detecting adverse drug reaction and customized causality explanation\nusing generative ai. In 2023 7th International Conference On Computing, Communication, Control\nAnd Automation (ICCUBEA) , pages 1–6, 2023.\n[9] Desir´ ee Bill and Theodor Eriksson. Fine-tuning a llm using reinforcement learning from human\nfeedback for a therapy chatbot application, 2023.\n[10] Nghi DQ Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven CH\nHoi. Codetf: One-stop transformer library for state-of-the-art code llm. arXiv preprint\narXiv:2306.00029, 2023.\n[11] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan\nYi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM\nTransactions on Intelligent Systems and Technology , 2023.\n[12] Boyang Chen, Zongxiao Wu, and Ruoran Zhao. From fiction to fact: the growing role of generative\nai in business and finance. Journal of Chinese Economic and Business Studies , 21(4):471–496,\n2023.\n[13] Wonchan Choi, Yan Zhang, and Besiki Stvilia. Exploring applications and user experience with\ngenerative ai tools: A content analysis of reddit posts on chatgpt. Proceedings of the Association\n197\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\nfor Information Science and Technology , 60(1):543–546, 2023.\n[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416 , 2022.\n[15] Pudi Dhilleswararao, Srinivas Boppu, M Sabarimalai Manikandan, and Linga Reddy Cenkera-\nmaddi. Efficient hardware architectures for accelerating deep neural networks: Survey. IEEE\nAccess, 2022.\n[16] Quang Do, Dan Roth, Mark Sammons, Yuancheng Tu, and V Vydiswaran. Robust, light-weight\napproaches to compute lexical similarity. Computer Science Research and Technical Reports,\nUniversity of Illinois , 9, 2009.\n[17] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muham-\nmad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. A survey on large language\nmodels: Applications, challenges, limitations, and practical usage. Authorea Preprints, 2023.\n[18] Harald Hammarstr¨ om and Wilco van den Heuvel. Introduction to the llm special issue 2012 on\nthe history, contact and classification of papuan languages. Language & Linguistics in Melanesia ,\n2012(Special Issue, Part 1):i–v, 2012.\n[19] Thomas F Heston and Charya Khun. Prompt engineering in medical education. International\nMedical Education, 2(3):198–205, 2023.\n[20] Jungseok Hong, Michael Fulton, and Junaed Sattar. A generative approach towards improved\nrobotic detection of marine litter. In 2020 IEEE international conference on robotics and automa-\ntion (ICRA), pages 10525–10531. IEEE, 2020.\n[21] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Princi-\nples, pages 611–626, 2023.\n[22] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the\ncarbon emissions of machine learning. arXiv preprint arXiv:1910.09700 , 2019.\n[23] Jie Li, Hancheng Cao, Laura Lin, Youyang Hou, Ruihao Zhu, and Abdallah El Ali. User\nexperience design professionals’ perceptions of generative artificial intelligence. arXiv preprint\narXiv:2309.15237, 2023.\n[24] Andrew M McNutt, Chenglong Wang, Robert A Deline, and Steven M Drucker. On the design of\nai-powered code assistants for notebooks. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems , pages 1–16, 2023.\n[25] Bertalan Mesk´ o and Eric J Topol. The imperative for regulatory oversight of large language models\n(or generative ai) in healthcare. NPJ digital medicine , 6(1):120, 2023.\n[26] Nicolas Morizet. Introduction to Generative Adversarial Networks . PhD thesis, Advestis, 2020.\n[27] Usman Naseem, Ajay Bandi, Shaina Raza, Junaid Rashid, and Bharathi Raja Chakravarthi. Incor-\nporating medical knowledge to transformer-based language models for medical dialogue generation.\nIn Proceedings of the 21st Workshop on Biomedical Language Processing , pages 110–115, 2022.\n[28] Tea Osm¨ eni and Maaruf Ali. Generative ai: Impactful considerations to responsible data practices\nin business execution. In 2023 International Conference on Computing, Networking, Telecommu-\nnications Engineering Sciences Applications (CoNTESA) , pages 75–82, 2023.\n[29] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang,\nChristopher Re, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large\nlanguage models with a single gpu. 2023.\n[30] Aman Singh and Tokunbo Ogunfunmi. An overview of variational autoencoders for source sepa-\nration, finance, and bio-signal applications. Entropy, 24(1):55, 2021.\n[31] Nina Singh, Katharine Lawrence, Safiya Richardson, and Devin M Mann. Centering health equity\nin large language model deployment. PLOS Digital Health , 2(10):e0000367, 2023.\n198\nA Case Study on the Generative AI Project Life Cycle Using LLMs A. Bandi and H. Kagitha\n[32] Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. The science of detecting llm-generated texts. arXiv\npreprint arXiv:2303.07205, 2023.\n[33] Jin Wang, Zishan Huang, Hengli Liu, Nianyi Yang, and Yinhao Xiao. Defecthunter: A novel\nllm-driven boosted-conformer-based code vulnerability detection mechanism. arXiv preprint\narXiv:2309.15324, 2023.\n[34] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, and Yue Zhang. A survey on large\nlanguage model (llm) security and privacy: The good, the bad, and the ugly. arXiv preprint\narXiv:2312.02003, 2023.\n199",
  "topic": "Chatbot",
  "concepts": [
    {
      "name": "Chatbot",
      "score": 0.928542971611023
    },
    {
      "name": "Computer science",
      "score": 0.7893351316452026
    },
    {
      "name": "Generative grammar",
      "score": 0.652220606803894
    },
    {
      "name": "Pipeline (software)",
      "score": 0.49324244260787964
    },
    {
      "name": "Interface (matter)",
      "score": 0.4884182810783386
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4811374247074127
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.44304123520851135
    },
    {
      "name": "Generative model",
      "score": 0.42950624227523804
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3656752407550812
    },
    {
      "name": "Programming language",
      "score": 0.20624974370002747
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    },
    {
      "name": "Bubble",
      "score": 0.0
    },
    {
      "name": "Maximum bubble pressure method",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I11123151",
      "name": "Northwest Missouri State University",
      "country": "US"
    }
  ]
}