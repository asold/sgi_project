{
  "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",
  "url": "https://openalex.org/W2963951265",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2798669999",
      "name": "Urvashi Khandelwal",
      "affiliations": [
        "Stanford University",
        "Laboratoire d'Informatique de Paris-Nord"
      ]
    },
    {
      "id": "https://openalex.org/A2105465932",
      "name": "He He",
      "affiliations": [
        "Stanford University",
        "Laboratoire d'Informatique de Paris-Nord"
      ]
    },
    {
      "id": "https://openalex.org/A2106317801",
      "name": "Peng Qi",
      "affiliations": [
        "Stanford University",
        "Laboratoire d'Informatique de Paris-Nord"
      ]
    },
    {
      "id": "https://openalex.org/A2089131864",
      "name": "Dan Jurafsky",
      "affiliations": [
        "Laboratoire d'Informatique de Paris-Nord",
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2962966012",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2508661145",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W1601924930",
    "https://openalex.org/W4919037",
    "https://openalex.org/W1712618182",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W4300553569",
    "https://openalex.org/W2964318358",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2605133364",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2275625487",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2964267515",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2964308564"
  ],
  "abstract": "We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.",
  "full_text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 284–294\nMelbourne, Australia, July 15 - 20, 2018.c⃝2018 Association for Computational Linguistics\n284\nSharp Nearby, Fuzzy Far Away: How Neural\nLanguage Models Use Context\nUrvashi Khandelwal, He He, Peng Qi, Dan Jurafsky\nComputer Science Department\nStanford University\n{urvashik,hehe,pengqi,jurafsky}@stanford.edu\nAbstract\nWe know very little about how neural lan-\nguage models (LM) use prior linguistic\ncontext. In this paper, we investigate the\nrole of context in an LSTM LM, through\nablation studies. Speciﬁcally, we ana-\nlyze the increase in perplexity when prior\ncontext words are shufﬂed, replaced, or\ndropped. On two standard datasets, Penn\nTreebank and WikiText-2, we ﬁnd that the\nmodel is capable of using about 200 to-\nkens of context on average, but sharply\ndistinguishes nearby context (recent 50 to-\nkens) from the distant history. The model\nis highly sensitive to the order of words\nwithin the most recent sentence, but ig-\nnores word order in the long-range context\n(beyond 50 tokens), suggesting the distant\npast is modeled only as a rough seman-\ntic ﬁeld or topic. We further ﬁnd that the\nneural caching model (Grave et al., 2017b)\nespecially helps the LSTM to copy words\nfrom within this distant context. Overall,\nour analysis not only provides a better un-\nderstanding of how neural LMs use their\ncontext, but also sheds light on recent suc-\ncess from cache-based models.\n1 Introduction\nLanguage models are an important component\nof natural language generation tasks, such as\nmachine translation and summarization. They\nuse context (a sequence of words) to estimate\na probability distribution of the upcoming word.\nFor several years now, neural language models\n(NLMs) (Graves, 2013; Jozefowicz et al., 2016;\nGrave et al., 2017a; Dauphin et al., 2017; Melis\net al., 2018; Yang et al., 2018) have consistently\noutperformed classical n-gram models, an im-\nprovement often attributed to their ability to model\nlong-range dependencies in faraway context. Yet,\nhow these NLMs use the context is largely unex-\nplained.\nRecent studies have begun to shed light on the\ninformation encoded by Long Short-Term Mem-\nory (LSTM) networks. They can remember sen-\ntence lengths, word identity, and word order (Adi\net al., 2017), can capture some syntactic structures\nsuch as subject-verb agreement (Linzen et al.,\n2016), and can model certain kinds of semantic\ncompositionality such as negation and intensiﬁca-\ntion (Li et al., 2016).\nHowever, all of the previous work studies\nLSTMs at the sentence level, even though they can\npotentially encode longer context. Our goal is to\ncomplement the prior work to provide a richer un-\nderstanding of the role of context, in particular,\nlong-range context beyond a sentence. We aim\nto answer the following questions: (i) How much\ncontext is used by NLMs, in terms of the number\nof tokens? (ii) Within this range, are nearby and\nlong-range contexts represented differently? (iii)\nHow do copy mechanisms help the model use dif-\nferent regions of context?\nWe investigate these questions via ablation stud-\nies on a standard LSTM language model (Merity\net al., 2018) on two benchmark language modeling\ndatasets: Penn Treebank and WikiText-2. Given a\npretrained language model, we perturb the prior\ncontext in various waysat test time, to study how\nmuch the perturbed information affects model per-\nformance. Speciﬁcally, we alter the context length\nto study how many tokens are used, permute to-\nkens to see if LSTMs care about word order in\nboth local and global contexts, and drop and re-\nplace target words to test the copying abilities of\nLSTMs with and without an external copy mech-\nanism, such as the neural cache (Grave et al.,\n2017b). The cache operates by ﬁrst recording tar-\n285\nget words and their context representations seen\nin the history, and then encouraging the model to\ncopy a word from the past when the current con-\ntext representation matches that word’s recorded\ncontext vector.\nWe ﬁnd that the LSTM is capable of using about\n200 tokens of context on average, with no observ-\nable differences from changing the hyperparame-\nter settings. Within this context range, word or-\nder is only relevant within the 20 most recent to-\nkens or about a sentence. In the long-range con-\ntext, order has almost no effect on performance,\nsuggesting that the model maintains a high-level,\nrough semantic representation of faraway words.\nFinally, we ﬁnd that LSTMs can regenerate some\nwords seen in the nearby context, but heavily rely\non the cache to help them copy words from the\nlong-range context.\n2 Language Modeling\nLanguage models assign probabilities to se-\nquences of words. In practice, the probability can\nbe factorized using the chain rule\nP (w1,...,w t)=\ntY\ni=1\nP (wi|wi\u00001,...,w 1),\nand language models compute the conditional\nprobability of atarget word wt given its preced-\ning context,w1,...,w t\u00001.\nLanguage models are trained to minimize the\nnegative log likelihood of the training corpus:\nNLL = \u0000 1\nT\nTX\nt=1\nlog P (wt|wt\u00001,...,w 1),\nand the model’s performance is usually evaluated\nby perplexity (PP) on a held-out set:\nPP =e x p (NLL).\nWhen testing the effect of ablations, we focus\non comparing differences in the language model’s\nlosses (NLL) on the dev set, which is equivalent to\nrelative improvements in perplexity.\n3 Approach\nOur goal is to investigate the effect of contextual\nfeatures such as the length of context, word or-\nder and more, on LSTM performance. Thus, we\nuse ablation analysis, during evaluation, to mea-\nsure changes in model performance in the absence\nof certain contextual information.\nPTB Wiki\nDev Test Dev Test\n# Tokens 73,760 82,430 217,646 245,569\nPerplexity (no cache) 59.07 56.89 67.29 64.51\nAvg. Sent. Len. 20.9 20.9 23.7 22.6\nTable 1: Dataset statistics and performance rele-\nvant to our experiments.\nTypically, when testing the language model on a\nheld-out sequence of words, all tokens prior to the\ntarget word are fed to the model; we call this the\ninﬁnite-context setting. In this study, we observe\nthe change in perplexity or NLL when the model\nis fed a perturbed context\u0000(wt\u00001,...,w 1), at test\ntime. \u0000 refers to the perturbation function, and we\nexperiment with perturbations such as dropping\ntokens, shufﬂing/reversing tokens, and replacing\ntokens with other words from the vocabulary.1 It\nis important to note that we do not train the model\nwith these perturbations. This is because the aim is\nto start with an LSTM that has been trained in the\nstandard fashion, and discover how much context\nit uses and which features in nearby vs. long-range\ncontext are important. Hence, the mismatch in\ntraining and test is a necessary part of experiment\ndesign, and all measured losses are upper bounds\nwhich would likely be lower, were the model also\ntrained to handle such perturbations.\nWe use a standard LSTM language model,\ntrained and ﬁnetuned using the Averaging SGD\noptimizer (Merity et al., 2018).2 We also augment\nthe model with a cacheonly for Section 6.2, in\norder to investigate why an external copy mech-\nanism is helpful. A short description of the ar-\nchitecture and a detailed list of hyperparameters is\nlisted in AppendixA, and we refer the reader to\nthe original paper for additional details.\nWe analyze two datasets commonly used for\nlanguage modeling, Penn Treebank (PTB) (Mar-\ncus et al. , 1993; Mikolov et al. , 2010) and\nWikitext-2 (Wiki) (Merity et al., 2017). PTB\nconsists of Wall Street Journal news articles with\n0.9M tokens for training and a 10K vocabulary.\nWiki is a larger and more diverse dataset, con-\ntaining Wikipedia articles across many topics with\n2.1M tokens for training and a 33K vocabulary.\nAdditional dataset statistics are provided in Ta-\n1Code for our experiments available at https://\ngithub.com/urvashik/lm-context-analysis\n2Public release of their code at https://github.\ncom/salesforce/awd-lstm-lm\n286\nble 1.\nIn this paper, we present results only on the dev\nsets, in order to avoid revealing details about the\ntest sets. However, we have conﬁrmed that all re-\nsults are consistent with those on the test sets. In\naddition, for all experiments we report averaged\nresults from three models trained with different\nrandom seeds. Some of the ﬁgures provided con-\ntain trends from only one of the two datasets and\nthe corresponding ﬁgures for the other dataset are\nprovided in AppendixB.\n4 How much context is used?\nLSTMs are designed to capture long-range depen-\ndencies in sequences (Hochreiter and Schmidhu-\nber, 1997). In practice, LSTM language models\nare provided an inﬁnite amount of prior context,\nwhich is as long as the test sequence goes. How-\never, it is unclear how much of this history has a\ndirect impact on model performance. In this sec-\ntion, we investigate how many tokens of context\nachieve a similar loss (or 1-2% difference in model\nperplexity) to providing the model inﬁnite context.\nWe consider this theeffective context size.\nLSTM language models have an effective con-\ntext size of about 200 tokens on average.We\ndetermine the effective context size by varying the\nnumber of tokens fed to the model. In particular,\nat test time, we feed the model the most recentn\ntokens:\n\u0000truncate(wt\u00001,...,w 1)=( wt\u00001,...,w t\u0000n), (1)\nwhere n> 0 and all tokens farther away from\nthe target wt are dropped.3 We compare the dev\nloss (NLL) from truncated context, to that of the\ninﬁnite-context setting where all previous words\nare fed to the model. The resulting increase in loss\nindicates how important the dropped tokens are for\nthe model.\nFigure 1a shows that the difference in dev loss,\nbetween truncated- and inﬁnite-context variants of\nthe test setting, gradually diminishes as we in-\ncrease n from 5 tokens to 1000 tokens. In particu-\nlar, we only see a 1% increase in perplexity as we\nmove beyond a context of 150 tokens on PTB and\n250 tokens on Wiki. Hence, we provide empirical\nevidence to show that LSTM language models do,\nin fact, model long-range dependencies, without\nhelp from extra context vectors or caches.\n3Words at the beginning of the test sequence with fewer\nthan n tokens in the context are ignored for loss computation.\nChanging hyperparameters does not change\nthe effective context size. NLM performance\nhas been shown to be sensitive to hyperparame-\nters such as the dropout rate and model size (Melis\net al., 2018). To investigate if these hyperpa-\nrameters affect the effective context size as well,\nwe train separate models by varying the follow-\ning hyperparameters one at a time: (1) number\nof timesteps for truncated back-propogation (2)\ndropout rate, (3) model size (hidden state size,\nnumber of layers, and word embedding size). In\nFigure 1b, we show that while different hyperpa-\nrameter settings result in different perplexities in\nthe inﬁnite-context setting, the trend of how per-\nplexity changes as we reduce the context size re-\nmains the same.\n4.1 Do different types of words need different\namounts of context?\nThe effective context size determined in the pre-\nvious section is aggregated over the entire cor-\npus, which ignores the type of the upcoming word.\nBoyd-Graber and Blei(2009) have previously in-\nvestigated the differences in context used by dif-\nferent types of words and found that function\nwords rely on less context than content words.\nWe investigate whether the effective context size\nvaries across different types of words, by catego-\nrizing them based on either frequency or parts-of-\nspeech. Speciﬁcally, we vary the number of con-\ntext tokens in the same way as the previous sec-\ntion, and aggregate loss over words within each\nclass separately.\nInfrequent words need more context than fre-\nquent words. We categorize words that appear\nat least 800 times in the training set asfrequent,\nand the rest asinfrequent. Figure 1c shows that\nthe loss of frequent words is insensitive to missing\ncontext beyond the 50 most recent tokens, which\nholds across the two datasets. Infrequent words,\non the other hand, require more than 200 tokens.\nContent words need more context than function\nwords. Given the parts-of-speech of each word,\nwe deﬁnecontent wordsas nouns, verbs and adjec-\ntives, and function words as prepositions and de-\nterminers.4 Figure 1d shows that the loss of nouns\nand verbs is affected by distant context, whereas\nwhen the target word is a determiner, the model\nonly relies on words within the last 10 tokens.\n4We obtain part-of-speech tags using Stanford\nCoreNLP (Manning et al., 2014).\n287\n(a) Varying context size.\n (b) Changing model hyperparameters.\n(c) Frequent vs. infrequent words.\n (d) Different parts-of-speech.\nFigure 1: Effects of varying the number of tokens provided in the context, as compared to the same\nmodel provided with inﬁnite context. Increase in loss represents an absolute increase in NLL over the\nentire corpus, due to restricted context. All curves are averaged over three random seeds, and error bars\nrepresent the standard deviation.(a) The model has an effective context size of 150 on PTB and 250 on\nWiki. (b) Changing model hyperparameters does not change the context usage trend, but does change\nmodel performance. We report perplexities to highlight the consistent trend.(c) Infrequent words need\nmore context than frequent words.(d) Content words need more context than function words.\nDiscussion. Overall, we ﬁnd that the model’s ef-\nfective context size is dynamic. It depends on\nthe target word, which is consistent with what we\nknow about language, e.g., determiners require\nless context than nouns (Boyd-Graber and Blei,\n2009). In addition, these ﬁndings are consistent\nwith those previously reported for different lan-\nguage models and datasets (Hill et al., 2016; Wang\nand Cho, 2016).\n5 Nearby vs. long-range context\nAn effective context size of 200 tokens allows for\nrepresenting linguistic information at many lev-\nels of abstraction, such as words, sentences, top-\nics, etc. In this section, we investigate the impor-\ntance of contextual information such as word order\nand word identity. Unlike prior work that studies\nLSTM embeddings at the sentence level, we look\nat both nearby and faraway context, and analyze\nhow the language model treats contextual informa-\ntion presented in different regions of the context.\n5.1 Does word order matter?\nAdi et al. (2017) have shown that LSTMs are\naware of word order within a sentence. We investi-\ngate whether LSTM language models are sensitive\nto word order within a larger context window. To\ndetermine the range in which word order affects\nmodel performance, we permute substrings in the\ncontext to observe their effect on dev loss com-\npared to the unperturbed baseline. In particular,\nwe perturb the context as follows,\n\u0000permute(wt\u00001,...,w t\u0000n)=\n(wt\u00001,. . ,⇢(wt\u0000s1\u00001,. . ,wt\u0000s2 ),. . ,wt\u0000n) (2)\nwhere ⇢ 2{ shu✏e, reverse} and (s1,s 2] denotes\nthe range of the substring to be permuted. We re-\nfer to this substring as thepermutable span. For\n288\n(a) Perturb order locally, within 20 tokens of each point.\n(b) Perturb global order, i.e. all tokens in the context before a\ngiven point, in Wiki.\nFigure 2: Effects of shufﬂing and reversing the order of words in 300 tokens of context, relative to an\nunperturbed baseline. All curves are averages from three random seeds, where error bars represent the\nstandard deviation. (a) Changing the order of words within a 20-token window has negligible effect on\nthe loss after the ﬁrst 20 tokens.(b) Changing the global order of words within the context does not\naffect loss beyond 50 tokens.\nthe following analysis, we distinguishlocal word\norder, within 20-token permutable spans which\nare the length of an average sentence, fromglobal\nword order, which extends beyond local spans to\ninclude all the farthest tokens in the history. We\nconsider selecting permutable spans within a con-\ntext ofn = 300 tokens, which is greater than the\neffective context size.\nLocal word order only matters for the most re-\ncent 20 tokens. We can locate the region of con-\ntext beyond which the local word order has no rel-\nevance, by permuting word order locally at various\npoints within the context. We accomplish this by\nvarying s1 and setting s2 = s1 + 20. Figure 2a\nshows that local word order matters very much\nwithin the most recent 20 tokens, and far less be-\nyond that.\nGlobal order of words only matters for the most\nrecent 50 tokens. Similar to the local word or-\nder experiment, we locate the point beyond which\nthe general location of words within the context\nis irrelevant, by permuting global word order. We\nachieve this by varyings1 and ﬁxings2 = n. Fig-\nure 2b demonstrates that after 50 tokens, shufﬂing\nor reversing the remaining words in the context has\nno effect on the model performance.\nIn order to determine whether this is due to in-\nsensitivity to word order or whether the language\nmodel is simply not sensitive to any changes in\nthe long-range context, we further replace words\nin the permutable span with a randomly sampled\nsequence of the same length from the training set.\nThe gap between the permutation and replacement\ncurves in Figure2b illustrates that the identity of\nwords in the far away context is still relevant, and\nonly the order of the words is not.\nDiscussion. These results suggest that word or-\nder matters only within the most recent sentence,\nbeyond which the order of sentences matters for\n2-3 sentences (determined by our experiments on\nglobal word order). After 50 tokens, word or-\nder has almost no effect, but the identity of those\nwords is still relevant, suggesting a high-level,\nrough semantic representation for these faraway\nwords. In light of these observations, we deﬁne 50\ntokens as the boundary between nearby and long-\nrange context, for the rest of this study. Next, we\ninvestigate the importance of different word types\nin the different regions of context.\n5.2 Types of words and the region of context\nOpen-class orcontent wordssuch as nouns, verbs,\nadjectives and adverbs, contribute more to the\nsemantic context of natural language thanfunc-\ntion words such as determiners and prepositions.\nGiven our observation that the language model\nrepresents long-range context as a rough seman-\ntic representation, a natural question to ask is how\nimportant are function words in the long-range\n289\nFigure 3: Effect of dropping content and function\nwords from 300 tokens of context relative to an un-\nperturbed baseline, on PTB. Error bars represent\n95% conﬁdence intervals. Dropping both content\nand function words 5 tokens away from the target\nresults in a nontrivial increase in loss, whereas be-\nyond 20 tokens, only content words are relevant.\ncontext? Below, we study the effect of these\ntwo classes of words on the model’s performance.\nFunction words are deﬁned as all words that are\nnot nouns, verbs, adjectives or adverbs.\nContent words matter more than function\nwords. To study the effect of content and func-\ntion words on model perplexity, we drop them\nfrom different regions of the context and compare\nthe resulting change in loss. Speciﬁcally, we per-\nturb the context as follows,\n\u0000drop(wt\u00001,...,w t\u0000n)=\n(wt\u00001,. . ,wt\u0000s1 ,f pos(y, (wt\u0000s1\u00001,. . ,wt\u0000n)))\n(3)\nwhere fpos(y, span) is a function that drops all\nwords with POS tagy in a given span.s1 denotes\nthe starting offset of the perturbed subsequence.\nFor these experiments, we sets1 2{ 5, 20, 100}.\nOn average, there are slightly more content words\nthan function words in any given text. As shown in\nSection 4, dropping more words results in higher\nloss. To eliminate the effect of dropping differ-\nent fractions of words, for each experiment where\nwe drop a speciﬁc word type, we add a control\nexperiment where the same number of tokens are\nsampled randomly from the context, and dropped.\nFigure 3 shows that dropping content words as\nclose as 5 tokens from the target word increases\nmodel perplexity by about 65%, whereas dropping\nthe same proportion of tokens at random, results in\na much smaller 17% increase. Dropping all func-\ntion words, on the other hand, is not very differ-\nent from dropping the same proportion of words\nat random, but still increases loss by about 15%.\nThis suggests that within the most recent sentence,\ncontent words are extremely important but func-\ntion words are also relevant since they help main-\ntain grammaticality and syntactic structure. On the\nother hand, beyond a sentence, only content words\nhave a sizeable inﬂuence on model performance.\n6 To cache or not to cache?\nAs shown in Section5.1, LSTM language models\nuse a high-level, rough semantic representation for\nlong-range context, suggesting that they might not\nbe using information from any speciﬁc words lo-\ncated far away.Adi et al.(2017) have also shown\nthat while LSTMs are aware of which words ap-\npear in their context, this awareness degrades with\nincreasing length of the sequence. However, the\nsuccess of copy mechanisms such as attention and\ncaching (Bahdanau et al., 2015; Hill et al., 2016;\nMerity et al., 2017; Grave et al., 2017a,b) suggests\nthat information in the distant context is very use-\nful. Given this fact, can LSTMs copy any words\nfrom context without relying on external copy\nmechanisms? Do they copy words from nearby\nand long-range context equally? How does the\ncaching model help? In this section, we investi-\ngate these questions by studying how LSTMs copy\nwords from different regions of context. More\nspeciﬁcally, we look at two regions of context,\nnearby (within 50 most recent tokens) and long-\nrange (beyond 50 tokens), and study three cate-\ngories of target words: those that can be copied\nfrom nearby context (Cnear), those that canonly be\ncopied from long-range context (Cfar), and those\nthat cannot be copied at all given a limited context\n(Cnone).\n6.1 Can LSTMs copy words without caches?\nEven without a cache, LSTMs often regenerate\nwords that have already appeared in prior context.\nWe investigate how much the model relies on the\nprevious occurrences of the upcoming target word,\nby analyzing the change in loss after dropping and\nreplacing this target word in the context.\nLSTMs can regenerate words seen in nearby\ncontext. In order to demonstrate the usefulness\n290\n(a) Dropping tokens\n (b) Perturbing occurrences of target word in context.\nFigure 4: Effects of perturbing the target word in the context compared to dropping long-range context\naltogether, on PTB. Error bars represent 95% conﬁdence intervals.(a) Words that can only be copied\nfrom long-range context are more sensitive to dropping all the distant words than to dropping the target.\nFor words that can be copied from nearby context, dropping only the target has a much larger effect\non loss compared to dropping the long-range context.(b) Replacing the target word with other tokens\nfrom vocabulary hurts more than dropping it from the context, for words that can be copied from nearby\ncontext, but has no effect on words that can only be copied from far away.\nof target word occurrences in context, we experi-\nment with dropping all the distant context versus\ndropping only occurrences of the target word from\nthe context. In particular, we compare removing\nall tokens after the 50 most recent tokens, (Equa-\ntion 1 with n = 50), versus removing only the\ntarget word, in context of sizen = 300:\n\u0000drop(wt\u00001,...,w t\u0000n)=\nfword(wt, (wt\u00001,...,w t\u0000n)), (4)\nwhere fword(w, span) drops words equal tow in a\ngiven span. We compare applying both perturba-\ntions to a baseline model with unperturbed context\nrestricted ton = 300. We also include the target\nwords that never appear in the context (Cnone) as a\ncontrol set for this experiment.\nThe results show that LSTMs rely on the rough\nsemantic representation of the faraway context to\ngenerate Cfar, but direclty copy Cnear from the\nnearby context. In Figure4a, the long-range con-\ntext bars show that for words that can only be\ncopied from long-range context (Cfar), removing\nall distant context is far more disruptive than re-\nmoving only occurrences of the target word (12%\nand 2% increase in perplexity, respectively). This\nsuggests that the model relies more on the rough\nsemantic representation of faraway context to pre-\ndict these Cfar tokens, rather than directly copy-\ning them from the distant context. On the other\nhand, for words that can be copied from nearby\ncontext (Cnear), removing all long-range context\nhas a smaller effect (about 3.5% increase in per-\nplexity) as seen in Figure4a, compared to remov-\ning the target word which increases perplexity by\nalmost 9%. This suggests that theseCnear tokens\nare more often copied from nearby context, than\ninferred from information found in the rough se-\nmantic representation of long-range context.\nHowever, is it possible that dropping the tar-\nget tokens altogether, hurts the model too much\nby adversely affecting grammaticality of the con-\ntext? We test this theory by replacing target words\nin the context with other words from the vocab-\nulary. This perturbation is similar to Equation4,\nexcept instead of dropping the token, we replace\nit with a different one. In particular, we exper-\niment with replacing the target with<unk>, to\nsee if having the generic word is better than not\nhaving any word. We also replace it with a word\nthat has the same part-of-speech tag and a simi-\nlar frequency in the dataset, to observe how much\nthis change confuses the model. Figure4b shows\nthat replacing the target with other words results\nin up to a 14% increase in perplexity forCnear,\nwhich suggests that the replacement token seems\nto confuse the model far more than when the to-\nken is simply dropped. However, the words that\nrely on the long-range context,Cfar, are largely\nunaffected by these changes, which conﬁrms our\nconclusion from dropping the target tokens:Cfar\n291\nwitnesses in the morris ﬁlm </s> served up as a solo however the music lacks the UNK provided by a context within anothermedium </s> UNK of mr. glass may agree with the critic richard UNK 's sense that the NUM music in twelve parts is as UNK andUNK as the UNK UNK </s> but while making the obvious point that both UNK develop variations from themes this comparisonUNK the intensely UNK nature of mr. glass\n</s> snack-food UNK increased a strong NUM NUM in the third quarter while domestic proﬁt increased in double UNK mr.calloway said </s> excluding the british snack-food business acquired in july snack-food international UNK jumped NUM NUMwith sales strong in spain mexico and brazil </s> total snack-food proﬁt rose NUM NUM </s> led by pizza hut and UNK bellrestaurant earnings increased about NUM NUM in the third quarter on a NUM NUM sales increase </s> UNK sales for pizza hutrose about NUM NUM while UNK bell 's increased NUM NUM as the chain continues to beneﬁt from its UNK strategy </s> UNKbell has turned around declining customer counts by permanently lowering the price of its UNK </s> same UNK for kentucky friedchicken which has struggled with increased competition in the fast-food chicken market and a lack of new products rose only NUMNUM </s> the operation which has been slow to respond to consumers ' shifting UNK away from fried foods has been developing aUNK product that may be introduced nationally at the end of next year </s> the new product has performed well in a market test inlas vegas nev. mr. calloway\nsend a delegation of congressional staffers to poland to assist its legislature the UNK in democratic procedures </s> senator peteUNK calls this effort the ﬁrst gift of democracy </s> the poles might do better to view it as a UNK horse </s> it is the vast shadowgovernment of NUM congressional staffers that helps create such legislative UNK as the NUM page UNK reconciliation bill thatclaimed to be the budget of the united states </s> maybe after the staffers explain their work to the poles they 'd be willing to comeback and do the same for the american people </s> UNK UNK plc a ﬁnancially troubled irish maker of ﬁne crystal and UNK chinareported that its pretax loss for the ﬁrst six months widened to NUM million irish punts $ NUM million from NUM million irishpunts a year earlier </s> the results for the half were worse than market expectations which suggested an interim loss of aroundNUM million irish punts </s> in a sharply weaker london market yesterday UNK shares were down NUM pence at NUM penceNUM cents </s> the company reported a loss after taxation and minority interests of NUM million irish\nsim has set a fresh target of $ NUM a share by the end of </s> reaching that goal says robert t. UNK applied 's chief ﬁnancial ofﬁcerwill require efﬁcient reinvestment of cash by applied and UNK of its healthy NUM NUM rate of return on operating capital </s> inbarry wright mr. sim sees a situation very similar to the one he faced when he joined applied as president and chief operating ofﬁcerin NUM </s> applied then a closely held company was UNK under the management of its controlling family </s> while proﬁtable itwas n't growing and was n't providing a satisfactory return on invested capital he says </s> mr. sim is conﬁdent that the drive todominate certain niche markets will work at barry wright as it has at applied </s> he also UNK an UNK UNK to develop a corporateculture that rewards managers who produce and where UNK is shared </s> mr. sim considers the new unit 's operationsfundamentally sound and adds that barry wright has been fairly successful in moving into markets that have n't interested largercompetitors </s> with a little patience these businesses will perform very UNK mr. sim\nwas openly sympathetic to swapo </s> shortly after that mr. UNK had scott stanley arrested and his UNK conﬁscated </s> mr.stanley is on trial over charges that he violated a UNK issued by the south african administrator general earlier this year which madeit a crime punishable by two years in prison for any person to UNK UNK or UNK the election commission </s> the stanley affairdoes n't UNK well for the future of democracy or freedom of anything in namibia when swapo starts running the government </s> tothe extent mr. stanley has done anything wrong it may be that he is out of step with the consensus of world intellectuals that theUNK guerrillas were above all else the victims of UNK by neighboring south africa </s> swapo has enjoyed favorable westernmedia treatment ever since the u.n. general assembly declared it the sole UNK representative of namibia 's people in </s> last yearthe u.s. UNK a peace settlement to remove cuba 's UNK UNK from UNK and hold free and fair elections that would end south africa's control of namibia </s> the elections are set for nov. NUM </s> in july mr. stanley\njuly snack-food international UNK jumped NUM NUM with sales strong in spain mexico and brazil </s> total snack-food proﬁt roseNUM NUM </s> led by pizza hut and UNK bell restaurant earnings increased about NUM NUM in the third quarter on a NUMNUM sales increase </s> UNK sales for pizza hut rose about NUM NUM while UNK bell 's increased NUM NUM as the chaincontinues to beneﬁt from its UNK strategy </s> UNK bell has turned around declining customer counts by permanently lowering theprice of its UNK </s> same UNK for kentucky fried chicken which has struggled with increased competition in the fast-foodchicken market and a lack of new products rose only NUM NUM </s> the operation which has been slow to respond to consumers 'shifting UNK away from fried foods has been developing a UNK product that may be introduced nationally at the end of next year</s> the new product has performed well in a market test in las vegas nev. mr. calloway said </s> after a four-year $ NUM billionacquisition binge that brought a major soft-drink company soda UNK a fast-food chain and an overseas snack-food giant to pepsi mr.calloway\nof london 's securities traders it was a day that started nervously in the small hours </s> by UNK the selling was at UNK fever </s>but as the day ended in a UNK wall UNK rally the city UNK a sigh of relief </s> so it went yesterday in the trading rooms of london's ﬁnancial district </s> in the wake of wall street 's plunge last friday the london market was considered especially vulnerable </s>and before the opening of trading here yesterday all eyes were on early trading in tokyo for a clue as to how widespread the fallout\nFigure 5: Success of neural cache on PTB. Brightly shaded region shows peaky distribution.\nmanagement equity participation </s> further many institutions today holding troubled retailers ' debt securities will be UNK toconsider additional retailing investments </s> it 's called bad money driving out good money said one retailing UNK </s>institutions that usually buy retail paper have to be more concerned </s> however the lower prices these retail chains are nowexpected to bring should make it easier for managers to raise the necessary capital and pay back the resulting debt </s> in additionthe fall selling season has generally been a good one especially for those retailers dependent on apparel sales for the majority of theirrevenues </s> what 's encouraging about this is that retail chains will be sold on the basis of their sales and earnings not liquidationvalues said joseph e. brooks chairman and chief\nofferings outside the u.s. </s> goldman sachs & co. will manage the offering </s> macmillan said berlitz intends to pay quarterlydividends on the stock </s> the company said it expects to pay the ﬁrst dividend of NUM cents a share in the NUM ﬁrst quarter </s>berlitz will borrow an amount equal to its expected net proceeds from the offerings plus $ NUM million in connection with a creditagreement with lenders </s> the total borrowing will be about $ NUM million the company said </s> proceeds from the borrowingsunder the credit agreement will be used to pay an $ NUM million cash dividend to macmillan and to lend the remainder of about $NUM million to maxwell communications in connection with a UNK note </s> proceeds from the offering will be used to repayborrowings under the short-term parts of a credit agreement </s> berlitz which is based in princeton n.j. provides languageinstruction and translation services through more than NUM language centers in NUM countries </s> in the past ﬁve years morethan NUM NUM of its sales have been outside the u.s. </s> macmillan has owned berlitz since NUM </s> in the ﬁrst six months\nsaid that despite losses on ual stock his ﬁrm 's health is excellent </s> the stock 's decline also has left the ual board in a UNK </s>although it may not be legally obligated to sell the company if the buy-out group ca n't revive its bid it may have to explorealternatives if the buyers come back with a bid much lower than the group 's original $ 300-a-share proposal </s> at a meeting sept.NUM to consider the labor-management bid the board also was informed by its investment adviser ﬁrst boston corp. of interestexpressed by buy-out funds including kohlberg kravis roberts & co. and UNK little & co. as well as by robert bass morgan stanley 'sbuy-out fund and pan am corp </s> the takeover-stock traders were hoping that mr. davis or one of the other interested parties mightUNK with the situation in disarray or that the board might consider a recapitalization </s> meanwhile japanese bankers said theywere still UNK about accepting citicorp 's latest proposal </s> macmillan inc. said it plans a public offering of NUM million sharesof its berlitz international inc. unit at $ NUM to $ NUM a share\ncapital markets to sell its hertz equipment rental corp. unit </s> there is no pressing need to sell the unit but we are doing it so wecan concentrate on our core business UNK automobiles in the u.s. and abroad said william UNK hertz 's executive vice president</s> we are only going to sell at the right price </s> hertz equipment had operating proﬁt before depreciation of $ NUM million onrevenue of $ NUM million in NUM </s> the closely held hertz corp. had annual revenue of close to $ NUM billion in NUM ofwhich $ NUM billion was contributed by its hertz rent a car operations world-wide </s> hertz equipment is a major supplier of rentalequipment in the u.s. france spain and the UNK </s> it supplies commercial and industrial equipment including UNK UNK UNKand electrical equipment UNK UNK UNK and trucks </s> UNK inc. reported a net loss of $ NUM million for the ﬁscal third quarterended aug. NUM </s> it said the loss resulted from UNK and introduction costs related to a new medical UNK equipment system</s> in the year-earlier quarter the company reported net income of $ NUM or\nacquisition of nine businesses that make up the group the biggest portion of which was related to the NUM purchase of a UNK co.unit </s> among other things the restructured facilities will substantially reduce the group 's required amortization of the term loanportion of the credit facilities through september NUM mlx said </s> certain details of the restructured facilities remain to benegotiated </s> the agreement is subject to completion of a deﬁnitive amendment and appropriate approvals </s> william p. UNKmlx chairman and chief executive said the pact will provide mlx with the additional time and ﬂexibility necessary to complete therestructuring of the company 's capital structure </s> mlx has ﬁled a registration statement with the securities and exchangecommission covering a proposed offering of $ NUM million in long-term senior subordinated notes and warrants </s> dow jones &co. said it acquired a NUM NUM interest in UNK corp. a subsidiary of oklahoma publishing co. oklahoma city that provideselectronic research services </s> terms were n't disclosed </s> customers of either UNK or dow jones UNK are able to access theinformation on both services </s> dow jones is the publisher of the wall street\nvideo games electronic information systems and playing cards posted a NUM NUM unconsolidated surge in pretax proﬁt to NUMbillion yen $ NUM million from NUM billion yen $ NUM million for the ﬁscal year ended aug. NUM </s> sales surged NUM NUMto NUM billion yen from NUM billion </s> net income rose NUM NUM to NUM billion yen from NUM billion </s> UNK net fellto NUM yen from NUM yen because of expenses and capital adjustments </s> without detailing speciﬁc product UNK UNKcredited its bullish UNK in sales including advanced computer games and television entertainment systems to surging UNK sales inforeign markets </s> export sales for leisure items alone for instance totaled NUM billion yen in the NUM months up from NUMbillion in the previous ﬁscal year </s> domestic leisure sales however were lower </s> hertz corp. of park UNK n.j. said it retainedmerrill lynch capital markets to sell its hertz equipment rental corp. unit </s> there is no pressing need to sell the unit but we aredoing it so we can concentrate on our core business UNK automobiles in the u.s. and abroad said william UNK hertz 's executivevice president\nso-called road show to market the package around the world </s> an increasing number of banks appear to be considering the option\nFigure 6: Failure of neural cache on PTB. Lightly shaded regions show ﬂat distribution.\nwords are predicted from the rough representation\nof faraway context instead of speciﬁc occurrences\nof certain words.\n6.2 How does the cache help?\nIf LSTMs can already regenerate words from\nnearby context, how are copy mechanisms help-\ning the model? We answer this question by ana-\nlyzing how the neural cache model (Grave et al.,\n2017b) helps with improving model performance.\nThe cache records the hidden state ht at each\ntimestep t, and computes a cache distribution over\nthe words in the history as follows:\nPcache(wt|wt\u00001,...,w 1; ht,...,h 1)\n/\nt\u00001X\ni=1\n[wi = wt]e x p (✓hT\ni ht),\n(5)\nwhere ✓ controls the ﬂatness of the distribution.\nThis cache distribution is then interpolated with\nthe model’s output distribution over the vocabu-\nlary. Consequently, certain words from the history\nare upweighted, encouraging the model to copy\nthem.\nCaches help words that can be copied from\nlong-range context the most. In order to study\nthe effectiveness of the cache for the three\nclasses of words (Cnear,C far,C none), we evaluate\nan LSTM language model with and without a\ncache, and measure the difference in perplexity for\nthese words. In both settings, the model is pro-\nvided all prior context (not just 300 tokens) in or-\nFigure 7: Model performance relative to using a\ncache. Error bars represent 95% conﬁdence inter-\nvals. Words that can only be copied from the dis-\ntant context beneﬁt the most from using a cache.\nder to replicate theGrave et al.(2017b) setup. The\namount of history recorded, known as the cache\nsize, is a hyperparameter set to 500 past timesteps\nfor PTB and 3,875 for Wiki, both values very sim-\nilar to the average document lengths in the respec-\ntive datasets.\nWe ﬁnd that the cache helps words that can\nonly be copied from long-range context (Cfar)\nmore than words that can be copied from nearby\n(Cnear). This is illustrated by Figure7 where with-\nout caching, Cnear words see a 22% increase in\nperplexity for PTB, and a 32% increase for Wiki,\nwhereas Cfar see a 28% increase in perplexity\nfor PTB, and a whopping 53% increase for Wiki.\nThus, the cache is, in a sense, complementary to\nthe standard model, since it especially helps regen-\nerate words from the long-range context where the\nlatter falls short.\n292\nHowever, the cache also hurts about 36% of\nthe words in PTB and 20% in Wiki, which are\nwords that cannot be copied from context (Cnone),\nas illustrated by bars for “none” in Figure7.W e\nalso provide some case studies showing success\n(Fig. 5) and failure (Fig.6) modes for the cache.\nWe ﬁnd that for the successful case, the cache\ndistribution is concentrated on a single word that\nit wants to copy. However, when the target is\nnot present in the history, the cache distribution\nis more ﬂat, illustrating the model’s confusion,\nshown in Figure6. This suggests that the neural\ncache model might beneﬁt from having the option\nto ignore the cache when it cannot make a conﬁ-\ndent choice.\n7 Discussion\nThe ﬁndings presented in this paper provide a\ngreat deal of insight into how LSTMs model con-\ntext. This information can prove extremely use-\nful for improving language models. For instance,\nthe discovery that some word types are more im-\nportant than others can help reﬁne word dropout\nstrategies by making them adaptive to the different\nword types. Results on the cache also show that\nwe can further improve performance by allowing\nthe model to ignore the cache distribution when it\nis extremely uncertain, such as in Figure6. Dif-\nferences in nearby vs. long-range context suggest\nthat memory models, which feed explicit context\nrepresentations to the LSTM (Ghosh et al., 2016;\nLau et al., 2017), could beneﬁt from representa-\ntions that speciﬁcally capture information orthog-\nonal to that modeled by the LSTM.\nIn addition, the empirical methods used in this\nstudy are model-agnostic and can generalize to\nmodels other than the standard LSTM. This opens\nthe path to generating a stronger understanding of\nmodel classes beyond test set perplexities, by com-\nparing them across additional axes of information\nsuch as how much context they use on average, or\nhow robust they are to shufﬂed contexts.\nGiven the empirical nature of this study and the\nfact that the model and data are tightly coupled,\nseparating model behavior from language charac-\nteristics, has proved challenging. More speciﬁ-\ncally, a number of confounding factors such as vo-\ncabulary size, dataset size etc. make this separa-\ntion difﬁcult. In an attempt to address this, we\nhave chosen PTB and Wiki - two standard lan-\nguage modeling datasets which are diverse in con-\ntent (news vs. factual articles) and writing style,\nand are structured differently (eg: Wiki articles are\n4-6x longer on average and contain extra informa-\ntion such as titles and paragraph/section markers).\nMaking the data sources diverse in nature, has pro-\nvided the opportunity to somewhat isolate effects\nof the model, while ensuring consistency in re-\nsults. An interesting extension to further study this\nseparation would lie in experimenting with differ-\nent model classes and even different languages.\nRecently, Chelba et al.(2017), in proposing a\nnew model, showed that on PTB, an LSTM lan-\nguage model with 13 tokens of context is similar\nto the inﬁnite-context LSTM performance, with\nclose to an 8%5 increase in perplexity. This is\ncompared to a 25% increase at 13 tokens of con-\ntext in our setup. We believe this difference is\nattributed to the fact that their model was trained\nwith restricted context and a different error propa-\ngation scheme, while ours is not. Further investi-\ngation would be an interesting direction for future\nwork.\n8 Conclusion\nIn this analytic study, we have empirically shown\nthat a standard LSTM language model can effec-\ntively use about 200 tokens of context on two\nbenchmark datasets, regardless of hyperparame-\nter settings such as model size. It is sensitive to\nword order in the nearby context, but less so in\nthe long-range context. In addition, the model is\nable to regenerate words from nearby context, but\nheavily relies on caches to copy words from far\naway. These ﬁndings not only help us better un-\nderstand these models but also suggest ways for\nimproving them, as discussed in Section7. While\nobservations in this paper are reported at the to-\nken level, deeper understanding of sentence-level\ninteractions warrants further investigation, which\nwe leave to future work.\nAcknowledgments\nWe thank Arun Chaganty, Kevin Clark, Reid\nPryzant, Yuhao Zhang and our anonymous review-\ners for their thoughtful comments and suggestions.\nWe gratefully acknowledge support of the DARPA\nCommunicating with Computers (CwC) program\nunder ARO prime contract no . W911NF15-1-\n0462 and the NSF via grant IIS-1514268.\n5Table 3, 91 perplexity for the 13-gram vs. 84 for the\ninﬁnite context model.\n293\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov,\nOfer Lavi, and Yoav Goldberg. 2017. Fine-\ngrained analysis of sentence embeddings using\nauxiliary prediction tasks . International Con-\nference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=BJh6Ztuxl.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate . International\nConference on Learning Representations (ICLR)\nhttps://arxiv.org/pdf/1409.0473.pdf.\nJordan Boyd-Graber and David Blei. 2009. Syn-\ntactic topic models . In Advances in neu-\nral information processing systems . pages 185–\n192. https://papers.nips.cc/paper/3398-syntactic-\ntopic-models.pdf.\nCiprian Chelba, Mohammad Norouzi, and Samy\nBengio. 2017. N-gram language model-\ning using recurrent neural network esti-\nmation. arXiv preprint arXiv:1703.10724\nhttps://arxiv.org/pdf/1703.10724.pdf.\nYann N Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2017. Language modeling\nwith gated convolutional networks . Interna-\ntional Conference on Machine Learning (ICML)\nhttps://arxiv.org/pdf/1612.08083.pdf.\nYarin Gal and Zoubin Ghahramani. 2016.A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural informa-\ntion processing systems (NIPS). pages 1019–1027.\nhttps://arxiv.org/pdf/1512.05287.pdf.\nShalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,\nTom Dean, and Larry Heck. 2016.Contextual lstm\n(clstm) models for large scale nlp tasks. Work-\nshop on Large-scale Deep Learning for Data Min-\ning, KDDhttps://arxiv.org/pdf/1602.06291.pdf.\nEdouard Grave, Moustapha M Cisse, and Ar-\nmand Joulin. 2017a. Unbounded cache model\nfor online language modeling with open vo-\ncabulary. In Advances in Neural Information\nProcessing Systems (NIPS) . pages 6044–6054.\nhttps://papers.nips.cc/paper/7185-unbounded-\ncache-model-for-online-language-modeling-with-\nopen-vocabulary.pdf.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving Neural Language Mod-\nels with a Continuous Cache . International\nConference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=B184E5qee.\nAlex Graves. 2013. Generating se-\nquences with recurrent neural net-\nworks. arXiv preprint arXiv:1308.0850\nhttps://arxiv.org/pdf/1308.0850.pdf.\nFelix Hill, Antoine Bordes, Sumit Chopra, and\nJason Weston. 2016. The goldilocks princi-\nple: Reading children’s books with explicit\nmemory representations . International Con-\nference on Learning Representations (ICLR)\nhttps://arxiv.org/pdf/1511.02301.pdf.\nSepp Hochreiter and J ¨urgen Schmidhu-\nber. 1997. Long short-term memory .\nNeural computation 9(8):1735–1780.\nhttps://doi.org/10.1162/neco.1997.9.8.1735.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. Inter-\nnational Conference on Learning Representations\n(ICLR) https://openreview.net/pdf?id=r1aPbsFle.\nRafal Jozefowicz, Oriol Vinyals, Mike Schus-\nter, Noam Shazeer, and Yonghui Wu. 2016.\nExploring the limits of language mod-\neling. arXiv preprint arXiv:1602.02410\nhttps://arxiv.org/pdf/1602.02410.pdf.\nJey Han Lau, Timothy Baldwin, and Trevor Cohn.\n2017. Topically Driven Neural Language Model.\nAssociation for Computational Linguistics (ACL)\nhttps://doi.org/10.18653/v1/P17-1033.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan\nJurafsky. 2016. Visualizing and understanding\nneural models in nlp . North American As-\nsociation of Computational Linguistics (NAACL)\nhttp://www.aclweb.org/anthology/N16-1082.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics (TACL)\nhttp://aclweb.org/anthology/Q16-1037.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford corenlp natural lan-\nguage processing toolkit. In Proceedings of 52nd\nannual meeting of the association for computational\nlinguistics: system demonstrations . pages 55–60.\nhttps://doi.org/10.3115/v1/P14-5010.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large\nannotated corpus of english: The penn tree-\nbank. Computational linguistics 19(2):313–330.\nhttp://aclweb.org/anthology/J93-2004.\nGabor Melis, Chris Dyer, and Phil Blunsom.\n2018. On the State of the Art of Evalua-\ntion in Neural Language Models. International\nConference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=ByJHuTgA-.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and Optimizing\nLSTM Language Models . International Con-\nference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=SyyGPP0TZ.\n294\nStephen Merity, Caiming Xiong, James Brad-\nbury, and Richard Socher. 2017. Pointer\nSentinel Mixture Models . International Con-\nference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=Byj72udxe.\nTom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nOﬁr Press and Lior Wolf. 2017.Using the output em-\nbedding to improve language models. European\nChapter of the Association for Computational Lin-\nguistics http://aclweb.org/anthology/E17-2025.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun,\nand Rob Fergus. 2013. Regularization of neural\nnetworks using dropconnect. InInternational Con-\nference on Machine Learning (ICML). pages 1058–\n1066.\nTian Wang and Kyunghyun Cho. 2016.Larger-Context\nLanguage Modelling with Recurrent Neural Net-\nwork. Association for Computational Linguistics\n(ACL) https://doi.org/10.18653/v1/P16-1125.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2018. Breaking the softmax\nbottleneck: a high-rank rnn language model. Inter-\nnational Conference on Learning Representations\n(ICLR) https://openreview.net/pdf?id=HkwZSG-\nCZ.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8203754425048828
    },
    {
      "name": "Perplexity",
      "score": 0.7779217958450317
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6984567642211914
    },
    {
      "name": "Treebank",
      "score": 0.6401761770248413
    },
    {
      "name": "Context model",
      "score": 0.6039546132087708
    },
    {
      "name": "Language model",
      "score": 0.596061646938324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5731213092803955
    },
    {
      "name": "Natural language processing",
      "score": 0.5158423781394958
    },
    {
      "name": "Sentence",
      "score": 0.4898049533367157
    },
    {
      "name": "Word (group theory)",
      "score": 0.44185879826545715
    },
    {
      "name": "Artificial neural network",
      "score": 0.4281248450279236
    },
    {
      "name": "Linguistics",
      "score": 0.18110913038253784
    },
    {
      "name": "History",
      "score": 0.0963488519191742
    },
    {
      "name": "Parsing",
      "score": 0.09560272097587585
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    }
  ]
}