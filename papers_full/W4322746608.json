{
    "title": "Reagent prediction with a molecular transformer improves reaction data quality",
    "url": "https://openalex.org/W4322746608",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2007226721",
            "name": "Mikhail Andronov",
            "affiliations": [
                "Pfizer (Germany)",
                "Dalle Molle Institute for Artificial Intelligence Research",
                "University of Applied Sciences and Arts of Southern Switzerland"
            ]
        },
        {
            "id": "https://openalex.org/A4225177277",
            "name": "Varvara Voinarovska",
            "affiliations": [
                "Helmholtz Zentrum München",
                "Target (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2122577243",
            "name": "Natalia Andronova",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2132110758",
            "name": "Michael Wand",
            "affiliations": [
                "Dalle Molle Institute for Artificial Intelligence Research",
                "University of Applied Sciences and Arts of Southern Switzerland"
            ]
        },
        {
            "id": "https://openalex.org/A4222964262",
            "name": "Djork-Arné Clevert",
            "affiliations": [
                "Pfizer (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2116333191",
            "name": "Jürgen Schmidhuber",
            "affiliations": [
                "King Abdullah University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2007226721",
            "name": "Mikhail Andronov",
            "affiliations": [
                "Pfizer (Germany)",
                "Dalle Molle Institute for Artificial Intelligence Research",
                "University of Applied Sciences and Arts of Southern Switzerland"
            ]
        },
        {
            "id": "https://openalex.org/A4225177277",
            "name": "Varvara Voinarovska",
            "affiliations": [
                "Helmholtz Zentrum München",
                "Target (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2122577243",
            "name": "Natalia Andronova",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2132110758",
            "name": "Michael Wand",
            "affiliations": [
                "Dalle Molle Institute for Artificial Intelligence Research",
                "University of Applied Sciences and Arts of Southern Switzerland"
            ]
        },
        {
            "id": "https://openalex.org/A4222964262",
            "name": "Djork-Arné Clevert",
            "affiliations": [
                "Pfizer (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2116333191",
            "name": "Jürgen Schmidhuber",
            "affiliations": [
                "King Abdullah University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2784330372",
        "https://openalex.org/W2789615344",
        "https://openalex.org/W2130244530",
        "https://openalex.org/W3009202547",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W2903262661",
        "https://openalex.org/W3010145447",
        "https://openalex.org/W4220776772",
        "https://openalex.org/W4205455490",
        "https://openalex.org/W2000614400",
        "https://openalex.org/W3010558108",
        "https://openalex.org/W2052882499",
        "https://openalex.org/W3094332970",
        "https://openalex.org/W4200150051",
        "https://openalex.org/W2965447776",
        "https://openalex.org/W4307410169",
        "https://openalex.org/W2901942917",
        "https://openalex.org/W3042125889",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2571050567",
        "https://openalex.org/W2769423117",
        "https://openalex.org/W2089217417",
        "https://openalex.org/W3088999551",
        "https://openalex.org/W3088265803",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3208238119",
        "https://openalex.org/W4220902634",
        "https://openalex.org/W2580919858",
        "https://openalex.org/W3131285796",
        "https://openalex.org/W2987091515",
        "https://openalex.org/W2551217916",
        "https://openalex.org/W2060586571",
        "https://openalex.org/W2968378480",
        "https://openalex.org/W4280597794",
        "https://openalex.org/W4254658870",
        "https://openalex.org/W4249735123",
        "https://openalex.org/W3181403764",
        "https://openalex.org/W2963396480",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2167277498",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W3094771832",
        "https://openalex.org/W2085126686",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W2913668833"
    ],
    "abstract": "A molecular transformer predicts reagents for organic reactions. It is also able to replace questionable reagents in reaction data, e.g. USPTO, to enable better product prediction models to be trained on these new data.",
    "full_text": "Reagent prediction with a molecular transformer\nimproves reaction data quality†\nMikhail Andronov, *ae Varvara Voinarovska, b Natalia Andronova, c\nMichael Wand, ad Djork-Arn´e Clevert e and Jürgen Schmidhuberf\nAutomated synthesis planning is key for eﬃcient generative chemistry. Since reactions of given reactants may\nyield diﬀerent products depending on conditions such as the chemical context imposed by speciﬁcr e a g e n t s ,\ncomputer-aided synthesis planning should beneﬁt from recommendations of reaction conditions. Traditional\nsynthesis planning software, however, typically proposes reactions without specifying such conditions, relying\non human organic chemists who know the conditions to carry out suggested reactions. In particular, reagent\nprediction for arbitrary reactions, a crucial aspect of condition recommendation, has been largely overlooked\nin cheminformatics until recently. Here we employ the Molecular Transformer, a state-of-the-art model for\nreaction prediction and single-step retrosynthesis, to tackle this problem. We train the model on the US\npatents dataset (USPTO) and test it on Reaxys to demonstrate its out-of-distribution generalization\ncapabilities. Our reagent prediction model also improves the quality of product prediction: the Molecular\nTransformer is able to substitute the reagents in the noisy USPTO data with reagents that enable product\nprediction models to outperform those trained on plain USPTO. This makes it possible to improve upon the\nstate-of-the-art in reaction product prediction on the USPTO MIT benchmark.\n1 Introduction\nIn pharmaceutical and other chemical industries, experts have to\ndeal with organic synthesis problems all the time. Chemical\nreactions are the way substances are converted into each other,\nand the set of possible organic reactions comprises thousands of\nreaction types and millions of examples.\n1 In an attempt to facili-\ntate the work with such a large number of options, chemists\nstarted creating automated computer-aided synthesis planning\n(CASP) systems. Currently, these systems, either based on expert-\ncurated rules\n2,3 or machine learning techniques,4 demonstrate\npromising results in the prediction of organic reaction products5,6\nand retrosynthesis paths.7,8\nHowever, there are caveats that are crucial for successful\nsynthesis planning and oen unaccounted for in CASP systems.\nMost importantly, one would like to take into account as much\ninformation about a chemical reaction as possible when\nmodeling it. Among the most important aspects of a reaction\nbesides reactants and products are reaction conditions (Fig. 1).\nThe conditions comprise temperature, pressure, and other phys-\nical parameters as well as the chemical environment imposed by\nreagents, which are catalysts, solvents, and other molecules\nnecessary for a reaction to occur. The conditions and reagents are\nintegral to any reaction. The same reaction under diﬀerent\nconditions can result in diﬀerent outcomes.\nIn cheminformatics, all tasks of reaction modeling rely heavily\non the datasets of known organic reactions. One of the most\nimportant of them at the moment is the USPTO dataset,\n9 which is\npublicly available and consists of reactions obtained by text\nmining from open-access texts from the United States Patents\ndatabase. The reagents in this dataset are noisy, they may be oen\nunspecied or incorrect, and the data does not report any\nFig. 1 The structure of a chemical reaction. The transformation of\nreactants into products depends on reagents. Reagents are molecules\nlike catalysts, redox agents, acids, and solvents. Reactants and reagents\ntogether are precursors. Reagents with temperature, pressure,\nconcentration etc. form conditions.\naIDSIA, USI, SUPSI, 6900 Lugano, Switzerland. E-mail: mikhail.andronov@idsia.ch\nbInstitute of Structural Biology, Molecular Targets and Therapeutics Center, Helmholtz\nMunich – Deutsches Forschungszentrum für Gesundheit und Umwelt (GmbH), 85764\nNeuherberg, Germany\ncVia Berna 9, 6900 Lugano, Switzerland\ndInstitute for Digital Technologies for Personalized Healthcare, SUPSI, 6900 Lugano,\nSwitzerland\neMachine Learning Research, Pzer Worldwide Research Development and Medical,\nLinkstr.10, Berlin, Germany\nfAI Initiative, KAUST, 23955 Thuwal, Saudi Arabia\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d2sc06798f\nCite this:Chem. Sci.,2 0 2 3 ,14,3 2 3 5\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 9th December 2022\nAccepted 12th February 2023\nDOI: 10.1039/d2sc06798f\nrsc.li/chemical-science\n© 2023 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 | 3235\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\ntemperature or pressure conditions. Therefore, the models for\nreaction product prediction or retrosynthesis cannot exactly\nbenet from full and reliable reaction information when devel-\noped using USPTO.\nThe reagent information is oen overlooked in models for\nsingle-step retrosynthesis: even though some systems allow the\nprediction of retrosynthetic steps encompassing reagents,\n7\nmost of them only suggest reactants,10,11 leaving the chemist\nwondering about the actual procedure needed to conduct the\nproposed reaction. A separate reagent prediction model may\nhelp in this case.\n1.1 Related work\nIn general, the proposal of suitable conditions for a novel or\ngiven reaction is a reaction modeling task machines can be used\nto solve. In fact, the conditions prediction subroutine is neces-\nsary for a successful CASP system that generates hundreds of\nplausible reactions that need to be validated.\nThere have been substantial eﬀorts to predict suitable reaction\nconditions or chemical contexts in various settings. For example,\nthere are reports on using DFT to select suitable solvents for\na reaction,\n12 or thermodynamic calculations to choose heteroge-\nneous catalysts.13 Some focused on optimizing reaction condi-\ntions for particular reaction types using an expert system14 or\nmachine learning.15–18 Gao et al.19 have used a fully connected\nneural network trained on the Reaxys20 data in the form of reac-\ntion ngerprints to predict reagents in a supervised classication\nmanner. Also, Ryouet al.21 have extended this approach by using\na graph network to encode the information in reaction graphs\ninstead of using reactionngerprints while also using Reaxys and\ntreating the task as a supervised classication task.\nThe broad task of organic reaction modeling, whether it is\nreagent prediction, single-step retrosynthesis or product predic-\ntion, can be formulated as a sequence-to-sequence translation if\nthe reactions are represented as reaction SMILES.\n22 The rst\nattempts to use deep learning to predict reaction outcomes were\nmade by Nam and Kim\n23 and Schwaller et al.24 This approach\nexperienced signicant advances with the adoption of the trans-\nformer25 as the deep learning model (transformers with“linear-\nized self-attention” date back to 1992 (ref. 26)). The transformer\nperformed very well in reaction prediction5,27 and single-step\nretrosynthesis7,28 and established state-of-the-art results in both\nthese elds. A trained large-scale transformer can perform both\nsingle-step retrosynthesis and product prediction with impressive\naccuracy.\n29 The transformer has also been demonstrated to be\nsuitable for multi-task reaction modeling: when trained in\na BERT-like30 fashion to predict any masked tokens in a reaction,\nit can do both forward prediction and single-step retrosynthesis,\nas well as reagent prediction.\n31,32\n1.2 Outline of the paper\nThis work proposes a deep learning method for reagent predic-\ntion. We treat the problem as a machine translation task and\ntrain a transformer25 model to predict the SMILES strings of\nreagents given the SMILES of reactants and products. Unlike the\nexisting approaches designed specically for reagent prediction,\nour formulation is not conned to a predened set of possible\nreagents and allows the prediction of reagents for arbitrary\nreaction types. Whereas in principle our model is not therst\ntransformer suitable for reagent prediction,\n31,32 it is therst one\nto be trained specically for reagent prediction in a machine\ntranslation setting. We also demonstrate that the reagent\nprediction model can be used to improve a product prediction\nmodel trained on the USPTO dataset. First we predict missing\nreagents for not well-specied reactions in USPTO. Then we train\na transformer for product prediction on the corrected USPTO\nand observe that it improves the accuracy of a basic Molecular\nTransformer,\n5 which is one of the state-of-the-art models, on the\nUSPTO MIT benchmark.\n2 Materials and methods\n2.1 Data\nThe largest and the most used open-access dataset of diverse\norganic chemical reactions at the moment is the dataset of\nabout 2m reactions from US patents, commonly referred to as\nthe USPTO dataset.\n9,33 It was assembled by text mining from\nopenly accessible patents. The reactions in it are represented as\nreaction SMILES with atom mapping.\nMany machine learning tools for reaction modeling are\ntrained on some preprocessed subsets of USPTO because it is an\nopen dataset. Alternatively, there are proprietary reaction\ndatasets. One of them, Reaxys, contains about 56 million hand-\ncurated reactions. While researchers also use it to train ML\nmodels,\n19,34 the problem with it is that those models and data\nsubsets are not allowed to be publicly shared.\nWe use the whole USPTO as the training dataset for the\nreagent prediction model. To train and test the product predic-\ntion model, we use the subset of USPTO called USPTO MIT or\nUSPTO 480K, which is a common benchmark for reaction\nproduct prediction.\n5,6 However, we do not use any subsets of\nUSPTO as a test dataset for the reagent prediction model. The\nproblem with the USPTO is that this dataset is assembled using\ntext mining, so the reactions in it are recorded with a signicant\namount of noise. For example, diﬀerent instances of the same\nreaction type may be written with diﬀerent amounts of detail.35\nFig. 2A shows examples of Suzuki coupling with the year and\npatent number. This reaction type makes up a signicant portion\nof USPTO36 reactions. The Suzuki coupling generally requires\na palladium catalyst, a base, and a suitable solvent. However, in\nmany reactions of this type in USPTO the necessary reagents are\nnot specied. This is also observed for other types of reactions. In\naddition, USPTO may contain reaction SMILES involving\nnonsensical molecules (Fig. 2B).\nIf such noisy reactions end up in the test dataset, it will not\nallow us to correctly evaluate the performance of the reagents\nmodel. Preliminary experiments in which we tested the reagents\nmodel on USPTO showed that oen the model prediction does\nnot match the ground truth sequence, even though the former is\nmore sensible than the latter. To overcome this problem, we\nassembled our test set from the reactions in the Reaxys database.\nSince Reaxys is comprised of reactions manually extracted from\n3236 | Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 © 2023 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nscientic papers, we assume that the quality of reagents infor-\nmation there is ensured by human experts.\nWhile gathering the Reaxys subset, we aimed at making it\nresemble the USPTO 50K37 dataset in its distribution of reaction\ntypes. The purpose of such a design is to make the test distri-\nbution close to the train distribution. We use USPTO 50K as\na proxy for the training set because USPTO 50K is the only open\nsubset of USPTO that contains reaction class labels. The subset\nof Reaxys used as the test comprises 96 729 reactions of 10\nbroad classes. Their distribution is displayed in Table 1. We\naimed at making the class proportions in USPTO 50K and the\nReaxys test set similar. The classes were determined using\nNameRXN soware.\n38\nTo further investigate the similarity of USPTO and the Reaxys\nsubset we use, we employ the technique of parametric t-SNE,\nwhich is a dimensionality reduction method aiming at preserving\ncloseness between points in higher-dimensional space. We\nrepresent each reaction as a reaction Morgan diﬀerence nger-\nprint\n39 of 2048 bits with both reagent- and non-reagent weight\nequal to 1. Using an implementation of parametric t-SNE from\nOpenTSNE,40 we project thengerprint vectors of reactions in\nUSPTO 50K on the 2D plane, and then use the same t-SNE model\nto obtain the projections of reactions in Reaxys. The absolute\nvalues of the coordinates of the t-SNE embeddings of reaction\nvectors bear no physical meaning. The closeness of the points in\n2D reects the closeness of the reaction vectors in thengerprint\nspace: similar reactions lie close together. Parametric t-SNE\nmeans that the coordinates of the 2D embeddings of reactions in\nReaxys would lie close to those for similar reactions in USPTO\nFig. 2 (A) Instances of Suzuki–Miyaura coupling present in the USPTO data and written with diﬀerent amounts of detail. Generally, this type of\nreaction requires a palladium catalyst, a base, and a solvent. Any of those species may be missing in the examples of the Suzuki–Miyaura reaction\nfound within USPTO. (B) An example of a nonsensical entry in the USPTO data. Colored circles represent the original USPTO atom mapping for\nthis reaction. To the left are the number of patents and publication years.\nTable 1 The proportion of reactions belonging to ten broad reaction\nclasses both in USPTO 50K and the Reaxys test set used to test reagent\nprediction models\nReaction class\nProportion,\nUSPTO\n50K (%)\nProportion,\nReaxys\ntest (%)\nHeteroatom alkylation and arylation 28.73 28.11\nAcylation and related processes 24.29 18.15\nDeprotections 16.97 17.37\nC–C bond formation 11.52 11.95\nReductions 9.72 11.10\nProtections 1.35 4.53\nFunctional group interconversion\n(FGI)\n3.89 4.13\nFunctional group addition (FGA) 0.51 2.46\nOxidations 1.70 2.10\nHeterocycle formation 1.31 0.09\n© 2023 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 | 3237\nEdge Article Chemical Science\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n50K. In other words, the closeness of similar reactions will be\npreserved across datasets, not only within one dataset. The local\nstructure of the second dataset is preserved but the absolute\nvalues of the coordinates of its points are determined by the\ncoordinates of the points in therst dataset.\nFig. S1† in the ESI shows the t-SNE maps of USPTO 50K and\nour Reaxys test set. The maps for individual datasets are shown\nat the top of thegure. On the bottom, it demonstrates the\noverlap of those maps. One can see that the local structures of\nboth datasets are similar: there is a signicant overlap between\nthe clusters of points in both datasets. Fig. 3 shows the over-\nlapping t-SNE maps for individual classes of reactions. One can\nsee that they also demonstrate a noticeable overlap, even though\nit is not ideal.\n2.2 Model\nFor both reagents and product prediction we used the trans-\nformer\n25 — a deep learning model for autoregressive sequence-\nto-sequence modeling based entirely on the attention mecha-\nnism without using recurrent neural network layers. Although it\nwas initially proposed for neural machine translation, it has\nbeen successfully adapted to work with chemical data in various\ncheminformatics problems.\n5,7,29,41\nThe transformer is an encoder –decoder neural network\narchitecture. The encoder is built of several layers which\nessentially consist of a multi-head attention part and a feed-\nforward layer. The multi-head attention updates the represen-\ntations of every token in a batch according to eqn (1).\nFig. 3 TSNE maps for reactions in USPTO 50K and Reaxys test for 9 reaction classes. The points which lie close together represent similar\nreactions. The absolute coordinates of the points have no physical meaning. On top and on the right of each graph, the estimates of the\ndistribution of the corresponding coordinates are shown. The functional group addition (FGA) class is not shown due to the low number of\nreactions of this class in the Reaxys test.\n3238\n| Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 © 2023 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nXnew ¼ softmax\n \nXWQðXWKÞT\nﬃﬃﬃﬃﬃdk\np\n!\nðXWVÞ (1)\nHere X is the matrix of the token embeddings,Xnew is the\nmatrix of the token embeddings aer a multi-head attention\nlayer, WQ, WK, andWV are matrices of trainable parameters, and\ndk is the number of columns inWK. This mechanism resembles\nan update mechanism used in graph neural networks if we treat\nthe batch of entries as nodes in a full graph.42 The decoder has\na similar structure and learns the embeddings of tokens in the\ntarget sequence. Ultimately, the model uses the representations\nof all tokens both in the input sequence and the output\nsequence to predict the next token in the output sequence. The\ndecoding stops when the model predicts the special“end-of-\nsequence” token. The ordering of the tokens is imposed by\nadding positional encodings (special periodic signals) to the\ntoken representations at the start of the training.\n25 By using\na beam search, one can obtain several translations ordered by\nprobability for an input sequence. The model produces output\nsequentially, token by token, treating the choice of each token\nas multi-class classication and conditioning this choice on the\ninput sequence and the tokens already decoded for the given\ninput sequence.\nIn our experiments, we used the OpenNMT\n43 implementa-\ntion of the transformer for both the reagent and product\nprediction. We chose to use this particular solution to be\nconsistent with Schwalleret al.\n5\n2.3 Preprocessing of the training set\nThe transformer predicts the reagents for a reaction in the form\nof a SMILES string. As input, our model takes a reaction as\na string with a“[” separator, where to the le of the separator\nare the SMILES of the reactants separated by dots, and to the\nright are the SMILES of the products. The target sequence for\neach reaction is the SMILES of the reagents. This allows the\nmodel to predict reagents for a broad range of reactions without\ncommon restrictions: the number of reagents in a reaction and\ntheir particular roles are not predetermined.\nTo train the reagents model, we take the copy of USPTO\nkindly provided by Schwalleret al.,\n5 but preprocess it tot our\nsetting. Instead of using the original train–validation–test split,\nwe unite all those subsets together and choose 5000 reactions\nrandomly for validation in this whole subset.\nOur preprocessing pipeline for reagent prediction is as\nfollows: for each reaction, we rst remove all the auxiliary\ninformation written in the form of ChemAxon extended SMILES\n(CXSMILES). Then, we mix together all the molecules which are\nnot products. The procedure of extracting the original data from\npatents included the detection of catalysts and solvents and\nplacing them in the reagent section. However, we do not place\nour trust in it since it is quite imprecise and also does not\naccount for other possible reagent types. Therefore, we separate\nreactants from reagents according to the ngerprint-based\nalgorithm described by Schneideret al.\n37 and implemented in\nRDKit. A small number of all reactions, in this case, end up with\nno reactants whatsoever. For them, we decide on the separation\nbased on the original atom mapping in USPTO: reactants are\nmolecules with atom mapping labels that also appear in the\nproducts. We avoid using this approach for all reactions as the\ndefault atom mapping in USPTO is not reliable enough. Then,\nwe canonicalize the SMILES of all molecules in a reaction, drop\natom mapping and remove the isotope information if there are\nany. Finally, we order all molecules in the reaction: the mole-\ncules with the longest SMILES strings comerst; strings of the\nsame length are ordered alphabetically. We also tried imple-\nmenting a step in which we remove all molecule duplicates in\nthe reaction. This would make the reactions unbalanced but the\nreaction SMILES shorter while preserving the chemical context.\nHowever, this step did not prove to be useful and eventually, we\ndid not include it in the preprocessing pipeline.\nAer processing every reaction in the described fashion, we\nproceed to remove rare reagents from the data,i.e. the reagents\nwhich appear in the training data less than 20 times. This lowers\nthe number of unique reagents in the training set from 37 602\n(from which 26 431 were encountered only once) to 1314. The\nmodel is unlikely to learn to predict a reagent from such few\nexamples even if they are correct, although the visual analysis\nshows that such rare reagents are in fact rather reactants in not\nwell-specied reactions. For example, if a reaction includes\nseveral isomers of one reactant, only one of which is reported to\nbecome a product, all the other isomers get recognized as\nreagents. By removing rare reagents, we alleviate this problem.\nFinally, we drop duplicate reactions and the reactions where\na product appears among reactants or reagents. In accordance\nwith the common procedure of data augmentation in reaction\nmodeling, we employ SMILES augmentation as implemented in\nthe PySMILESutils Python package.\n44 Only the SMILES of reac-\ntants and products get augmented in the case of reagent\nprediction. In addition to that, we use“role augmentations”:\nsome molecules from the side of the reagent have a chance to\nmove to the reactants in an augmented example.\nAll molecules in the target sequences are canonicalized and\nordered by their detailed roles: catalysts comerst, then redox\nagents, then acids and bases, then any other molecules and ions,\nbut solvents come last. In this case, we utilize the model's\nautoregressive nature to predict the most important reagents\nrst based solely on the input, and then the more interchange-\nable reagents based both on the input and the reagents sug-\ngested so far. A similar ordering of reagents by role was used by\nGao et al.\n19 and it generally follows the line of thought of\na chemist who would suggest reagents for a reaction based on\ntheir experience. The roles of the molecules are determined\nusing the following heuristics:\n(1) Every molecule in a SMILES string of reagents is assigned\na role in the following order of decreasing priority: solvent,\ncatalyst, oxidizing agent, reducing agent, acid, base, unspecied\nrole.\n(2) A molecule is a solvent if it is one of the standard 46\nsolvent molecules, like THF, hexane, benzeneetc.\n(3) A molecule is a catalyst if\n(a) it is a free metal.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 | 3239\nEdge Article Chemical Science\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n(b) it contains a cycle together with a metal or phosphorus\natom.\n(c) it is a metal halide.\n(4) A molecule is an oxidizing agent if\n(a) it contains a peroxide group.\n(b) it contains at least two oxygen atoms and a transition\nmetal or iodine atom.\n(c) it is a standard oxidizing agent like free halogens.\n(d) it is a standard halogenating agent.\n(e) it contains both a positively charged atom and a nega-\ntively charged oxygen but is not a nitrate anion.\n(5) A molecule is a reducing agent if it is one of the standard\nreducing agents or some hydride of boron, silicon or aluminum.\n(6) A molecule is an acid if\n(a) it is a derivative of sulphuric, sulfamic or phosphoric acid\nwith the acidic–OH group intact.\n(b) it is a carboxylic acid.\n(c) it is a hydrohalic acid or a common Lewis acid like\naluminium chloride.\n(7) A molecule is a base if\n(a) it is a tertiary or secondary amine.\n(b) it contains a negatively charged oxygen atom and consists\nof only C, O, S, and P atoms.\n(c) it consists only of lithium and carbon.\n(d) it is the hydride ion or the hydroxide ion.\nTo tokenize our sequences, we employ the standard atom-\nwise tokenization scheme.\n5 Additionally, we experimented with\nthe scheme in which entire molecules get their own tokens,\nnamely all solvents and some common reagents. However, this\ndoes not seem to improve the quality of a trained reagent\nprediction model, so we resort to standard atomwise tokeniza-\ntion in ournal model.\nFor product prediction, we employ the same procedure that\nwas employed by Schwalleret al.\n5 The tokenization is atomwise\nas well. Some of the current deep learning reaction prediction\nmethods use explicit reagent information to make predic-\ntions,\n24,45,46 and some allow mixing all the precursors together,\nbut the separation of reactant and reagent information improves\nthe performance of such models.\n5,47,48 We trained product\nprediction models both in the separated setting (reactants and\nreagents are separated by the token“>” in the input sequences)\nand the mixed setting (all molecules are separated by dots in the\ninput sequences). To evaluate the quality of the models, we used\nboth the USPTO MIT test set and our Reaxys test set on which we\ntested the reagent prediction model. We did not use SMILES\naugmentations for product prediction.\n2.4 Preprocessing of the test set\nWe use a subset of Reaxys data for testing purposes. To obtain it,\nwe use the Reaxys web interface. In Reaxys, reaction SMILES do\nnot contain reagents, which are enumerated separately by their\nIUPAC notation or common name and separated by semicolons.\nWe employ the PubChemPy‡ package to retrieve SMILES of\nreagents from PubChem.\n49 We drop reactions in which reagents\nwere absent or their SMILES could not be successfully retrieved.\nAer constructing full reaction SMILES for all reactions, we\ncanonicalize all molecules in the reactions and order them as we\ndid with the training set. Aer proprocessing, every reaction in\nour test set has non-empty SMILES of reactants, reagents, and\nproducts. Thenal test set comprises 96 729 reactions.\n2.5 Training details\nFor both reagent and product prediction, we used the same\ntransformer settings and hyperparameters used by Schwaller\net al.:\n5 Adam optimizer,50 Noam learning rate schedule25 with\n8000 warmup steps, a batch size of around 4096 tokens, accu-\nmulation count 4 and dropout rate 0.1. We did not conduct\nweight averaging across checkpoints. All the models were\ntrained on an Nvidia GeForce GTX TITAN X GPU with 12 GB\nmemory.\n3 Results and discussion\n3.1 Model performance\nReagent prediction is not as straightforward as forward reaction\nprediction. A reaction may be carried out using diﬀerent sets of\nreagents. To put it another way, there may be more than one\nplausible chemical context for a given reaction: catalysts, redox\nagents, acids, bases, and solvents in a reaction can be more or\nless replaceable. Therefore, multiple diﬀerent sets of molecules\nmight be correct predictions for a given transformation. Having\nthat in mind, we chose the performance measures to be the\nfollowing:\n(1) Exact match accuracy: the prediction of the model is\nconsidered correct if the symmetric diﬀerence between the set\nof predicted molecules and the set of the ground truth mole-\ncules is an empty set.\nExample: A.C.B is an exact match to A.B.C.\n(2) Partial match accuracy: the prediction counts as correct if\nthe ground truth contains at least one of the predicted\nmolecules.\nExample: A.B is a partial match to A.C.D.\n(3) Recall: the number of the correctly predicted molecules\ndivided by the number of molecules in the ground truth.\nExample: A.B.C has 100% recall of A.C, A.D has 50% recall of\nA.B.C.D.\nHere A, B, C, and D denote the SMILES strings of some\nmolecules.\nWe use beam search with beam size 5 to obtain predictions\nfrom the transformer. Therefore, all performance metrics report\nthe top-N predictions with N from 1 to 5. A correct top-N\nprediction means that the correct answer appeared among the\nrst N sequences decoded with the beam search.\nAdditionally, our test set contains duplicate reactions with\ndiﬀerent reported reagent sets. While gathering performance\nstatistics, we group predictions by unique reactions: if the\nmodel correctly predicts reagents for one of the duplicates, we\ncount the reaction as correctly predicted.\nThe performance on the test set is summarized in Table 2.\nThe model performs quite well on the test dataset. For each\ntest reaction, each of the top-5 predictions is a valid SMILES\nstring. An exact match of the prediction and the ground truth\n3240 | Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 © 2023 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nsequence is observed in 17.0% of the cases for top-1, 29.2% for\ntop-3, and 33.5% for top-5. At the same time, full recall is higher\nat 19.2%, 28.4%, and 42.8%, respectively. As for the partial\nmatch between predictions and ground truth sequences, it is\nmuch higher at 70.9%, 84.9%, and 88.9% in the top-1, top-3,\nand top-5 cases, respectively. Thus, the model cannot correctly\npredict a single reagent for 11.1% of the reactions in the test\ndataset. In our evaluation, we do not use any methods to assess\nthe plausibility of an incorrect prediction such as similarity\nmetrics for interchangeable solvents, so the performance scores\nshould be considered to be underestimates.\n3.2 Model con dence\nThe condence scores of the model are much lower than those\n5\nof the Molecular Transformer for product prediction (Fig. 4).\nThe probability of a decoded sequence is the product of the\nprobabilities of all predicted tokens in it. In particular, their\naverage value is between 0.1 and 0.2, whereas in reaction\nprediction most scores exceeded 0.9. The reason seems to be the\nnature of the problem, as several plausible sets of reagents may\nbe proposed for a reaction, and the answer is not as unambig-\nuous as in product prediction. Nonetheless, the condence of\nthe model is, on average, noticeably higher for correct predic-\ntions than for incorrect ones.\n3.3 Performance across publication years\nWe looked into the dependence of the model on the publication\ndate of reactions. As chemists discover new reaction types and\nnew possible reagents for known reactions, the statistical\nknowledge gathered by a reagent model can become outdated.\nFig. 5 shows both the exact and partial accuracy of our reagent\nmodel on reactions published every single year between 1980 and\n2022. Solid lines illustrate the moving average of accuracy overve\nyears with a centered window. Top-1 exact match accuracy tends\nto increase on average from 1980 to 1998, then we see a decrease\nuntil 2005 and a plateau aer that. The picture is similar for top-5\nexact accuracy and top-2 to top-4 as well. The dependence for total\nrecall is alike. Top-1 partial match accuracy tends to increase on\naverage from 1980 to 2008 and stagnate aer that. Top-2 to top-5\naccuracies demonstrate similar behavior as well.\n3.4 Performance across reaction classes\nWe investigated the performance of the model on diﬀerent\nclasses of reactions included in our test dataset. The middle and\nrightmost bars for each reaction class in Fig. 6 show the rate of\nexact and partial matches of the model prediction and ground\ntruth. The lemost bars reect the relative proportion of each\nreaction class in the test dataset. We can see that the quality of\nthe model predictions diﬀers noticeably between classes. This\ndiﬀerence in performance is most likely due to the diﬀerence\nbetween the data distribution in the train and in the test for\ndiﬀerent classes of reactions. The model demonstrates the best\nTable 2 The performance of the transformer for reagent prediction\non the test set obtained from Reaxys. All scores are given in percentage\npoints\nMetric Top-1 Top-2 Top-3 Top-4 Top-5\nExact match accuracy 17.0 24.7 29.2 31.8 33.5\nFull recall 19.2 28.4 35.1 39.3 42.8\nPartial match accuracy 70.9 80.5 84.9 87.3 88.9\nFig. 4 Conﬁdence scores of the model predictions. On the left,\na violin plot reﬂecting the distribution of conﬁdence scores across all\npredictions on the Reaxys test dataset. On the right, separate boxplots\nof the distributions for correct and incorrect predictions in terms of\ntop-1 exact matches.\nFig. 5 On the top, the number of test set reactions published each\nyear. On the bottom, the dependence of the reagent model's\nperformance on the reaction publication year. Solid lines depict the\nmoving average overﬁve years.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 | 3241\nEdge Article Chemical Science\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\ntop-5 exact match accuracy for FGA, FGI, heteroatom alkylation\nand arylation and the best top-5 partial match accuracy for C–C\nbond formation. At the same time, C–C bond formation has the\nlowest exact match accuracy. The reason for that must be the\nwide variety of interchangeable reagents in this reaction class,\nespecially metal-based catalysts. As we use USPTO 50K as\na proxy for the training set, we assume that the least represented\nreaction classes in the former are also the least represented in\nthe latter. The four least represented classes in USPTO 50K are\nFGA, oxidations, protections, and heterocycle formations.\nInterestingly, the model exhibits good generalization in all\nthese classes.\n3.5 Performance across reagent roles\nWe also examined the quality of the model predictions for each\nreagent role (Fig. 7). Therst and third columns in the table in\nthe gure show that in both the top-1 and top-5 cases the\nsolvents are the most diﬃcult to predict. This is most likely due\nto the fact that they are o en the most interchangeable.\nHowever, reactions need not involve reagents of every possible\nrole, and the picture is somewhat diﬀerent in the case where the\nroles in the ground truth sequence were strictly nonempty\nSMILES strings. In this case, it was most diﬃcult to predict\noxidizing agents. This eﬀect is likely related toawed heuristics\nfor the classication of reagents or to the strong diﬀerence\nbetween the typical oxidizing agents in the train and in the test.\nThe more detailed performance summary across both reagent\nroles and reaction classes is shown in Fig S2† in the ESI.\n3.6 Analysis of prevalence of reaction types\nIn the test set, there are 684 unique reaction types determined\nby NameRXN. These types can be split into ve “bins” by\noccurrence frequency. The summary of those bins is given in\nTable 3. The most common types are those which occur more\nthan a thousand times in the test set. There are only 20 such\ntypes. Among others, they include Suzuki coupling, Williamson\nether synthesis, aldehyde reductive amination, N-Boc protec-\ntion, and nitrile reduction. Heterocycle formation, oxidations,\nFGI, and FGA are not present among the common types. Out of\nall unique types, 245 are singular, meaning that they are rep-\nresented in the test set by only one instance. Frequent types\nhave 101 to 1000 instances, rare types have 11 to 100 instances\nand very rare types have from 2 to 10 reaction examples. The\nperformance of the reagent model decreases with the decrease\nin type prevalence, which is expected.\nFig. 6 Percentage of partial (rightmost, in red) and perfect matches (middle, in green) between the target sequence and the predicted sequence\nacross ten reaction classes in the Reaxys test set. The class proportions are leftmost, in black. All values are grouped by unique reactions.\nFig. 7 Comparison of the proportion of test examples on which the\nprediction matches the ground truth exactly in each reagent role. The\ncomparison is given for the top-1 and top-5 predictions both in\ngeneral and when the ground truth (GT) sequence is strictly not an\nempty string.\n3242\n| Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 © 2023 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nIn the“common” and “frequent” bins there are no reaction\ntypes with only a single possible reagent in any role. In the\n“rare” and “very rare” bins there is a small number of types in\nwhich it is the case. However, the analysis is limited by\nNameRXN and by the heuristics of role classication. If the type\nlabel is a name reaction, which is an infrequent case, then the\nreactions with that label may have a single option for one of the\nroles. For example, all instances of the rare “8.1.24 Ketone\nSwern oxidation” type have an oxidizing agent which is the\nsame for all, and the instances of the rare“9.1.2 Appel chlori-\nnation” and the very rare“1.7.8 Ullmann condensation” types\nall have their one specic catalyst. Also, some“very rare” types\nmay have only one option even for solvents in the test set, but it\nmay be an accident due to under-representation. The reagent\nmay or may not give a perfect top-1 prediction for all reactions\nof such types.\n3.7 Improving product prediction\nBesides predicting reagentsper se, our model has another appli-\ncation: we can use it to augment USPTO with more reagents for\nreactions that are lacking them, and train a product prediction\nmodel on this augmented dataset. As noted above, many reac-\ntions in the USPTO contain reagents that are under-specied, yet\nmany many reactions in USPTO contain the full set of reagents at\nthe same time. This allows us to use a trained reagent prediction\nmodel to recover missing reagents in some reactions. We apply\nthe model trained on the entire USPTO to the USPTO MIT subset.\nDuring training, we make sure that the USPTO MIT test set does\nnot overlap with the training set for the reagent model.\nThere can be various strategies for reagent string replace-\nment. We apply the following rule: if the top-1 prediction of the\nmodel contains more molecules than the original string, then\nthe prediction replaces that string. With that, we can improve\nthe reactions with missing reagents without corrupting the\ngood ones. We are aware, however, that this strategy is not ideal\nand we are convinced that better ones are possible. Some\nexamples of the reactions with reagents improved aer the\nreagent model inference are shown in Fig. 8.\nThe rst reaction is an example of peptide coupling. The\ntypical reagents in this case comprise HOBt or its analogs (e.g.\nHATU) usually used together with Hünig's base (DIPEA). The\nreagent model reintroduces the missing reagents to the reac-\ntion. The second one is an example of reductive amination, and\nthe information about the solvent alone is not enough. The\nmodel proposed a suitable reducing agent, sodium\ntriacetoxyborohydride. The last two reactions are Suzuki\ncoupling and Sonogashira reaction, respectively. The model\nsuggests the standard reagents that dene these reaction types.\nWe compared two models, both of which were standard\nMolecular Transformers with the same hyperparameters but\ntrained on diﬀerent data. Therst model, which we denote as\n“MT base”, was trained on the standard USPTO MIT. This is the\nmodel from the original Molecular Transformer paper.\n5 The\nsecond model, which we denote as“MT new”, was trained on\nUSPTO MIT in which some of the reactions had reagents\nreplaced according to the procedure described above. We chose\ntop-1 exact match accuracy as the quality metric. Before testing\non Reaxys, we reassigned the reactant–reagent partition in every\ntest reaction with the role assignment algorithm.\n37 This was\ndone to be consistent with the inference procedure, in which\nthe reagents that will be replaced are the reagents determined\nby this algorithm. Additionally, we trained another Molecular\nTransformer for product prediction without any reagents in the\nsource sequences. The performance summary of the models is\npresented in Table 4.\nThe results of the base model are reproduced as described by\nSchwaller et al.\n5 without SMILES augmentations, checkpoint\naveraging, or model ensembling. The new model performs\nbetter than the old model on both Reaxys and USPTO in both\nseparated and mixed settings. This performance improvement\nis statistically signicant. To prove the statistical signicance,\nwe employed McNemar's test.\n51 The details are provided in the\nESI.† The performance on Reaxys is worse than on USPTO\nbecause the distribution of data in Reaxys diﬀers more from the\ndistribution in the training set than in the USPTO test set.\nHowever, it is important to emphasize that the USPTO test set\nalso underwent a reagent change to test the new model. The\nperformance in the mixed setting is slightly worse than in the\nseparated setting, which is expected.\n5 The score of the model\ntrained with no reagents is expectedly the lowest both on Reaxys\nand USPTO. However, surprisingly, it is only 4.7 percentage\npoints below the base model's score on Reaxys and 3.7\npercentage points below the base model's score on USPTO.\nTherefore, we can conclude that even though the reagent\ninformation helps the product models trained on USPTO, which\nit should from a chemical perspective, the eﬀect is not that\ndrastic. However, we conjecture that such improvement will be\nmuch more noticeable on a larger scale,e.g. if all the models are\ntrained on the entire Reaxys. We suggest that this is a manifes-\ntation of USPTO's aws: the dataset does not contain many\nTable 3 The statistics of the reaction types in the Reaxys test set. The types are determined using NameRXN\nOccurrence N Criterion Reactions\nTop-1 exact\nacc., %\nTop-1 partial\nacc., %\nCommon 20 N > 1000 67 633 17.0 73.5\nFrequent 67 100 < N # 1000 23 685 16.2 65.1\nRare 120 10 < N # 100 4219 13.5 60.3\nVery rare 232 1 < N # 10 947 14.2 53.1\nSingular 245 N = 1 245 13.5 62.4\n© 2023 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 | 3243\nEdge Article Chemical Science\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nreactions in which the same reactant transforms into diﬀerent\nproducts under diﬀerent conditions.\n4 Conclusions\nA transformer neural network, which is one of the models used to\nachieve state-of-the-art results in reaction prediction, can also\nlearn to successfully predict reagents for organic reactions, which\nis important for recommending reaction conditions. The reagent\nprediction model receives an atom-mapping-free reaction\nSMILES string with no reagents and suggests multiple possible\nsets of reagents for it. Our work is therst to use the strategy of\ntraining a reagent prediction model on USPTO and testing it on\na Reaxys subset, demonstrating its generalization capabilities. We\nalso used the reagent prediction model to improve the perfor-\nmance of a product prediction model on USPTO MIT in a self-\nsupervised fashion. In order to do that, we used the reagent\nmodel to reconstruct the missing reagents to the reaction data\nbefore training the product prediction model on it. Since reagent\ninformation is important to predict reaction products, our\napproach allows a state-of-the-art model for reaction prediction to\nbe outperformed while being model-agnostic. In our work, in\nparticular, we improve upon the score of the Molecular Trans-\nformer on the USPTO MIT (USPTO 480K) benchmark dataset.\nData availability\nThe link to the repository with the paper code: https://\ngithub.com/Academich/reagents. The links to access the\nUSPTO data and Reaxys reaction ID's used in the experiments\nare provided in the ESI.†\nAuthor contributions\nM. A. conceptualized the paper idea. M. A. and V. V. gathered\nand preprocessed the data for experiments. J. S., D. C.\nTable 4 The top-1 exact match accuracy (%) of reaction product\nprediction both for the Molecular Transformer trained on the default\nUSPTO MIT (MT base) and the Molecular Transformer trained on the\nUSPTO MIT where in some of the reactions reagents were augmented\nby the reagent prediction model (MT new). The models were\ncompared both on the USPTO MIT test set and the Reaxys test set in\nthe separated setting, mixed setting, and no-reagents setting. There is\nno diﬀerence between the old and the new model in the latter case\nReaxys\nUSPTO\nMIT\nMT, no reagents 77.3 84.0\nMT base, mixed 82.0 87.7\nMT new, mixed 83.0 88.3\nMT base, separated 84.3 89.2\nMT new, separated 84.6 89.6\nFig. 8 Examples of reactions in the USPTO MIT training set for which the reagent model successfully improves reagents. Model predictions are\nabove the arrows (in green), and original reagents are below the arrows (in red).\n3244 | Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 © 2023 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nand M. W. acquired the research funding, administered the\nproject and provided supervision. M. A. developed the soware\nand carried out computational experiments. M. A. and N. A.\ndeveloped the methodology, designed experiments, and vali-\ndated the experimental results. M. A., V. V., M. W., and J. S.\nwrote the manuscript with inputs from all co-authors.\nConﬂicts of interest\nThere are no conicts to declare.\nAcknowledgements\nThis study was funded by the European Union’s Horizon 2020\nresearch and innovation programme under the Marie\nSkłodowska-Curie Actions, grant agreement “Advanced\nmachine learning for Innovative Drug Discovery (AIDD)” No.\n956832.\nNotes and references\n‡ https://github.com/mcs07/PubChemPy\n1 P.-M. Jacob and A. Lapkin,React. Chem. Eng., 2018, 3, 102–\n118.\n2 T. Klucznik, B. Mikulak-Klucznik, M. P. McCormack,\nH. Lima, S. Szymku´c, M. Bhowmick, K. Molga, Y. Zhou,\nL. Rickershauser, E. P. Gajewska, A. Toutchkine,\nP. Dittwald, M. P. Startek, G. J. Kirkovits, R. Roszak,\nA. Adamski, B. Sieredzi´nska, M. Mrksich, S. L. Trice and\nB. A. Grzybowski,Chem, 2018,4, 522–532.\n3 H. Gelernter, J. R. Rose and C. Chen,J. Chem. Inf. Comput.\nSci., 1990,30, 492–504.\n4 K. Lin, Y. Xu, J. Pei and L. Lai,Chem. Sci., 2020, 11, 3355–\n3364.\n5 P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter,\nC. Bekas and A. A. Lee,ACS Cent. Sci., 2019,5, 1572–1583.\n6 C. Coley, W. Jin, L. Rogers, T. F. Jamison, T. S. Jaakkola,\nW. H. Green, R. Barzilay and K. F. Jensen,Chem. Sci., 2019,\n10, 370–377.\n7 P. Schwaller, R. Petraglia, V. Zullo, V. H. Nair,\nR. A. Haeuselmann, R. Pisoni, C. Bekas, A. Iuliano and\nT. Laino,Chem. Sci., 2020,11, 3316–3325.\n8 C. Shi, M. Xu, H. Guo, M. Zhang and J. Tang,Proceedings of\nthe 37th International Conference on Machine Learning, 2020.\n9 Chemical reactions from US patents (1976-Sep-2016) dataset,\nhttps://gshare.com/articles/dataset/\nChemical_reactions_from_US_patents_1976-Sep2016_/\n5104873, (accessed October 29, 2020).\n10 M. H. Lin, Z. Tu and C. W. Coley,J. Cheminf., 2022,14, 15.\n11 P. Seidl, P. Renz, N. Dyubankova, P. Neves, J. Verhoeven,\nJ. K. Wegner, M. Segler, S. Hochreiter and G. Klambauer,J.\nChem. Inf. Model., 2022,62, 2111–2120.\n12 H. Struebing, Z. Ganase, P. G. Karamertzanis, E. Siougkrou,\nP. Haycock, P. M. Piccione, A. Armstrong, A. Galindo and\nC. S. Adjiman,Nat. Chem., 2013,5, 952–957.\n13 H. Toulhoat and P. Raybaud,Catal. Sci. Technol., 2020, 10,\n2069–2081.\n14 G. Marcou, J. Aires de Sousa, D. A. R. S. Latino, A. de Luca,\nD. Horvath, V. Rietsch and A. Varnek,J. Chem. Inf. Model.,\n2015, 55, 239–250.\n15 M. R. Maser, A. Y. Cui, S. Ryou, T. J. DeLano, Y. Yue and\nS. E. Reisman,J. Chem. Inf. Model., 2021,61, 156–166.\n16 V. A. Afonina, D. A. Mazitov, A. Nurmukhametova,\nM. D. Shevelev, D. A. Khasanova, R. I. Nugmanov,\nV. A. Burilov, T. I. Madzhidov and A. Varnek,Int. J. Mol.\nSci., 2022,23, 248.\n17 E. Walker, J. Kammeraad, J. Goetz, M. T. Robo, A. Tewari and\nP. M. Zimmerman,J. Chem. Inf. Model., 2019,59, 3645–3654.\n18 N. H. Angello, V. Rathore, W. Beker, A. Wołos, E. R. Jira,\nR. Roszak, T. C. Wu, C. M. Schroeder, A. Aspuru-Guzik,\nB. A. Grzybowski and M. D. Burke,Science, 2022, 378, 399–\n405.\n19 H. Gao, T. J. Struble, C. W. Coley, Y. Wang, W. H. Green and\nK. F. Jensen,ACS Cent. Sci., 2018,4, 1465–1476.\n20 Reaxys database, https://www.reaxys.com.\n21 S. Ryou, M. R. Maser, A. Y. Cui, T. J. DeLano, Y. Yue and\nS. E. Reisman, 2020, preprint, DOI: DOI: 10.48550/\narXiv.2007.04275.\n22 D. Weininger,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n23 J. Nam and J. Kim, 2016, preprint, DOI: DOI: 10.48550/\narXiv.1612.09529.\n24 P. Schwaller, T. Gaudin, D. L´anyi, C. Bekas and T. Laino,\nChem. Sci., 2018,9, 6091–6098.\n25 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin, Advances in\nNeural Information Processing Systems, 2017.\n26 J. Schmidhuber,Neural Comput., 1992,4, 131–139.\n27 G. Pesciullesi, P. Schwaller, T. Laino and J.-L. Reymond,Nat.\nCommun., 2020,11,1 –8.\n28 I. V. Tetko, P. Karpov, R. Van Deursen and G. Godin,Nat.\nCommun., 2020,11, 5575–5585.\n29 R. Irwin, S. Dimitriadis, J. He and E. J. Bjerrum,Mach. learn.:\nsci. technol., 2022,3, 015022.\n30 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, 2018,\npreprint, DOI:10.48550/arXiv.1810.04805.\n31 A. C. Vaucher, P. Schwaller and T. Laino,ChemRxiv, 2020,\npreprint, DOI:10.26434/chemrxiv.13273310.v1.\n32 J. Lu and Y. Zhang,J. Chem. Inf. Model., 2022,62, 1376–1387.\n33 D. M. Lowe,Extraction of chemical structures and reactions\nfrom the literature . PhD Dissertation, University of\nCambridge, Cambridge, UK, 2012, DOI: 10.17863/\nCAM.16293.\n34 M. H. Segler and M. P. Waller,Chem. – Eur. J., 2017,23, 5966–\n5971.\n35 M. Andronov, M. V. Fedorov and S. Sosnin,ACS Omega, 2021,\n6, 30743–30751.\n36 A. Thakkar, T. Kogej, J.-L. Reymond, O. Engkvist and\nE. J. Bjerrum,Chem. Sci., 2020,11, 154–168.\n37 N. Schneider, N. Stie and G. A. Landrum, J. Chem. Inf.\nModel., 2016,56, 2336–2346.\n38 NameRXN, https://www.nextmovesoware.com/\nnamerxn.html.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 | 3245\nEdge Article Chemical Science\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n39 N. Schneider, D. M. Lowe, R. A. Sayle and G. A. Landrum,J.\nChem. Inf. Model., 2015,55,3 9–53.\n40 P. G. Poliˇcar, M. Straˇzar and B. Zupan, bioRxiv, preprint,\n731877, 2019, DOI:10.1101/731877.\n41 N. Frey, R. Soklaski, S. Axelrod, S. Samsi, R. Gomez-\nBombarelli, C. Coley and V. Gadepally, 2022, preprint, DOI:\n10.26434/chemrxiv-2022-3s512.\n42 C. Joshi,The Gradient, 2020.\n43 G. Klein, Y. Kim, Y. Deng, J. Senellart and A. Rush,\nProceedings of ACL 2017 , System Demonstrations,\nVancouver, Canada, 2017, pp. 67–72.\n44 E. Bjerrum, T. Rastemo, R. Irwin, C. Kannas and\nS. Genheden, ChemRxiv, 2021, preprint, DOI: 10.26434/\nchemrxiv-2021-kzhbs.\n45 W. W. Qian, N. T. Russell, C. L. W. Simons, Y. Luo,\nM. D. Burke and J. Peng,ChemRxiv, 2020, preprint, DOI:\n10.26434/chemrxiv.11659563.v1.\n46 H. Bi, H. Wang, C. Shi, C. Coley, J. Tang and H. Guo,\nProceedings of the 38th International Conference on Machine\nLearning, 2021, pp. 904–913.\n47 M. Sacha, M. B ła, P. Byrski, P. Da ¸browski-Tuma´nski,\nM. Chromi´nski, R. Loska, P. Włodarczyk-Pruszy´nski and\nS. Jastrzebski,J. Chem. Inf. Model., 2021,61, 3273–3284.\n48 K. Do, T. Tran and S. Venkatesh,Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery &\nData Mining, 2019, pp. 750–760.\n49 Pubchem database, https://pubchem.ncbi.nlm.nih.gov/.\n50 D. P. Kingma and J. Ba, arXiv, 2015, DOI: 10.48550/\narXiv.1412.6980.\n51 T. G. Dietterich,Neural Comput., 1998,10, 1895–1923.\n3246 | Chem. Sci.,2 0 2 3 ,14,3 2 3 5–3246 © 2023 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 01 March 2023. Downloaded on 11/5/2025 4:08:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online"
}