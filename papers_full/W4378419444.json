{
    "title": "Vision Transformers for Small Histological Datasets Learned Through Knowledge Distillation",
    "url": "https://openalex.org/W4378419444",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5074795857",
            "name": "Neel Kanwal",
            "affiliations": [
                "University of Stavanger"
            ]
        },
        {
            "id": "https://openalex.org/A5035565080",
            "name": "Trygve Eftestøl",
            "affiliations": [
                "University of Stavanger"
            ]
        },
        {
            "id": "https://openalex.org/A5031734265",
            "name": "Farbod Khoraminia",
            "affiliations": [
                "Erasmus MC Cancer Institute"
            ]
        },
        {
            "id": "https://openalex.org/A5026058364",
            "name": "Tahlita C.M. Zuiverloon",
            "affiliations": [
                "Erasmus MC Cancer Institute"
            ]
        },
        {
            "id": "https://openalex.org/A5068843545",
            "name": "Kjersti Engan",
            "affiliations": [
                "University of Stavanger"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3145185940",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W4285022653",
        "https://openalex.org/W2963395517",
        "https://openalex.org/W3140093204",
        "https://openalex.org/W3131494624",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2801113172",
        "https://openalex.org/W2982083293",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W4388117480",
        "https://openalex.org/W4285036046",
        "https://openalex.org/W4285105280",
        "https://openalex.org/W3159493748",
        "https://openalex.org/W3194837369",
        "https://openalex.org/W4214837566",
        "https://openalex.org/W3113328935",
        "https://openalex.org/W3169077988",
        "https://openalex.org/W3090823571",
        "https://openalex.org/W2886595805",
        "https://openalex.org/W4221070057",
        "https://openalex.org/W4214634256",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W4308831279",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963371170"
    ],
    "abstract": null,
    "full_text": "Vision Transformers for Small Histological\nDatasets Learned through Knowledge Distillation\nNeel Kanwal1*\n , Trygve Eftestøl1, Farbod Khoraminia2, Tahlita CM\nZuiverloon2, and Kjersti Engan1\n1 Department of Electrical Engineering and Computer Science, University of\nStavanger, Norway\n2 Department of Urology, University Medical Center Rotterdam, Erasmus MC\nCancer Institute, Rotterdam, The Netherlands\n*Corresponding author: neel.kanwal@uis.no\nAbstract. Computational Pathology (CPATH) systems have the po-\ntential to automate diagnostic tasks. However, the artifacts on the digi-\ntized histological glass slides, known as Whole Slide Images (WSIs), may\nhamper the overall performance of CPATH systems. Deep Learning (DL)\nmodels such as Vision Transformers (ViTs) may detect and exclude arti-\nfacts before running the diagnostic algorithm. A simple way to develop\nrobust and generalized ViTs is to train them on massive datasets. Unfor-\ntunately, acquiring large medical datasets is expensive and inconvenient,\nprompting the need for a generalized artifact detection method for WSIs.\nIn this paper, we present a student-teacher recipe to improve the clas-\nsification performance of ViT for the air bubbles detection task. ViT,\ntrained under the student-teacher framework, boosts its performance by\ndistilling existing knowledge from the high-capacity teacher model. Our\nbest-performing ViT yields 0.961 and 0.911 F1-score and MCC, respec-\ntively, observing a 7% gain in MCC against stand-alone training. The\nproposed method presents a new perspective of leveraging knowledge\ndistillation over transfer learning to encourage the use of customized\ntransformers for efficient preprocessing pipelines in the CPATH systems.\nKeywords: ArtifactDetection · ComputationalPathology · DeepLearn-\ning · Knowledge Distillation· Vision Transformer· Whole Slide Images\n1 Introduction\nHistological examination of tissue samples is conducted by studying thin slices\nfrom a tumor specimen mounted on a glass slide. During the laboratory pro-\ncedures, the preparation of glass slides may introduce artifacts and variations\ncausing loss of visual [15,27]. Artifacts, such as air bubbles, occur when air is\ntrapped under the cover slip due to improper mounting procedure [16]. Eventu-\nally, the presence of air bubbles leaves an altered and fainted appearance [16,27].\nDuring the manual assessment, pathologists usually ignore regions containing\nartifacts as they are irrelevant for diagnosis.\nComputational Pathology (CPATH) systems are automated systems working\nwith a digitized glass slide, called Whole Slide Image (WSI), as input. CPATH\nsystems have the potential to automate diagnostic tasks and provide a second\nopinionorlocalizetheRegionsofInterest(ROIs)[14].Differenttypesofartifacts,\narXiv:2305.17370v1  [cs.CV]  27 May 2023\n2 Kanwal et al., Vision Transformers for Artifact Detection\nlike air bubbles, might be present on the WSI [16] and can deteriorate diagnostic\nCPATH results if included in the analysis. Therefore it has been proposed to\ndetect and exclude artifacts as a first step before using more relevant tissue in a\ndiagnostic or prognostic system [15,16]. The detection and exclusion of artifacts\ncan be regarded as (a part of) apreprocessing pipeline, which also might include\ncolor normalization and patching [16]. A complete preprocessing pipeline should\ndetect folded tissue, damaged tissue, blood, and blurred (out of focus) areas, as\nwell as air bubbles [16]. This might be done by an ensemble of models, one for\neach artifact, or by a multiclass model. In this paper, we consider detectingair\nbubbles artifact, which is not given much attention in the literature.\nDeep Learning (DL) methods have shown promising results in various med-\nical image analysis tasks [4,28], and can be used for detecting artifacts in a\npreprocessing pipeline. Supervised learning for generalized DL models requires\na significant amount of data and labels. In CPATH literature, little effort has\nbeen made to annotate artifacts; thus, publicly available datasets for histological\nartifacts are unavailable. Transfer Learning (TL) has been widely used for med-\nical images to deal with the lack of labeled training data [6,21]. TL methods use\nthe existing knowledge, such as ImageNet [2] weights, and fine-tune the model\nfor a different task. Although TL on ImageNet weights is useful to cope with a\nlack of data, ImageNet weights are mostly available for complicated Deep Con-\nvolutional Neural Networks (DCNN) architectures and carry a strong texture\nbias [5]. However, such DCNNs are typically computationally complex, whereas\na preprocessing pipeline, being a first step prior to diagnostic or prognostic mod-\nels, should have generalized and efficient DL models with high throughput. This\nis especially true with an ensemble of DCNN models for the different artifacts.\nAfter the success in natural language processing tasks,transformers have\nbeen given attention for vision tasks [3,17]. Vision Transformers (ViTs), using a\nconvolution-free approach, have surpassed DCNNs in accuracy and efficiency on\nimage classification benchmarks [1,3]. Unlike the convolution layer in DCNNs,\nwhich applies the same filter weights to all inputs, the multi-head attention [30]\nin ViTs attends to image-wide structural information [20]. Interestingly, ViTs\nare also shown to be more robust and generalized than DCNNs [1,20]; Unfor-\ntunately, the robustness and generalizability come from training on extremely\nlarge datasets [1,3,29], which contrasts with the biomedical scenario. These lim-\nitations bring us to the question:how can we train generalized ViTs on a small\nhistopathological dataset?.\nOne possible answer lies in Knowledge Distillation (KD) [10], which transfers\nknowledge from a usually large teacher model to another, typically smaller, stu-\ndent model. Motivated by the KD idea, we present a student-teacher recipe, as\nshown in Fig 1. We propose to use KD in combination with TL for detecting air\nbubbles on WSIs using a small training set. In short, we let the teacher model be\na complex ImageNet pretrained DCNN, and using KD, we train a small student\nmodel, which is a ViT. In the inference stage, we only need the small ViT, which\nis computationally efficient enough for a preprocessing pipeline implementation.\nKanwal et al., Vision Transformers for Artifact Detection 3\nWhole Slide ImagesPatches\nAir bubblesArtifact-free\nTeacher\nStudent\nConvolutional Neural Networks\nConvolution + ReluMax-pooling\nLinear Embedding for flattening cells \nPatch\nCell\n01214\n1\n2\n12\nTransformer Block\nTransformer Block\nTransformer Block\nNorm.\nMultihead  \nAttention\nNorm.\nFinal  \nLoss\nStudent  \nLoss\nDistillation\nLoss\n1/T\n1/T\nGround \nTruth \nσ\nLCE\nσ*\nlog-Softmax\nKLD\nKL  \nDivergence \nCross Entropy \nσ\nAir bubbles\nArtifact-free\nTraining\nPrediction\nT2\n \nMultilayer \nPerceptron\nNorm.\nQ K V\nTransformer \nBlock α\nβ\n1 2\n3\n4\nsn\ntn\nSoftmax\nxn\nFig. 1. An overview of our proposed air bubbles detection method by knowl-\nedge distillation:Predefined size patches for air bubbles and artifact-free classes are\nextracted from the WSI. A ViT student model is trained with the help of a DCNN\nteacher model by leveraging the transference of knowledge during the training process.\nThe student-teacher recipe weights the teacher and student’s outputs by the tempera-\nture (T). The overall training objective is to minimize the final loss, which is a linear\ncombination of student loss and distillation loss. Finally, the student model is used to\nperform predictions for binary air bubbles detection task.\nOur contributions in this paper can be summarized as follows:\n– We train several state-of-the-art DCNNs and ViTs to compare their per-\nformance on a binary air bubbles detection task. Later, we choose suitable ar-\nchitectures to test our student-teacher framework.\n– We conduct an in-depth comparison by initializing models with and with-\nout ImageNet weights and training ViT under a standalone vs. a student-teacher\nframework. We also assess the improvements in ViT‘s generalization capability\nover ImageNet transfer learning.\n– We run extensive experiments to test the student ViT’s performance under\ndifferent teacher models and distillation configurations on unseen data.\n2 Related Work\nArtifact and air bubbles detection: The detection of histopathological ar-\ntifacts has largely been overlooked during the development of CPATH systems,\nand the literature on air bubbles is scarce. Shakhawat et al. [11], in their quality\nevaluation method, detected air bubbles in two steps. First, the non-overlapping\n4 Kanwal et al., Vision Transformers for Artifact Detection\naffected patches were detected using a Support Vector Machine (SVM) classi-\nfier. Later, the remaining patches with fainted appearance were separated using\nhandcrafted Gray-level Co-occurrence Matrix (GLCM) features. This work was\nlater extended in [24], where a pretrained VGG16 [25] network was used to\ncompare the handcrafted features against the CNN-based method. Their ex-\nperiments concluded that handcrafted features provide stable classification, but\ntheir evaluation was based on a relatively smaller dataset. Recently, Raipuria\net al. [22] performed stress testing for common histological artifacts, including\nair bubbles, using a vision transformer [29] and a ResNet [9] model. Though,\nMobiletNet [12] and VGG16 [25] have been popular DCNN choices for artifact\ndetection [15]. DCNNs are found to be less robust than ViTs and exhibit strong\ntexture bias [20,22].\nKnowledge Distillation (KD): Originally proposed by Hinton et al. [10]\nfor model compression, KD sought to extract knowledge from an ensemble of\nCNN experts to a smaller two-layer CNN generalist network to make it per-\nform equally well. In short, KD aims to train a small student model under the\nguidance of a complicated teacher model, where the student model optimizes\nits learning by absorbing the hidden knowledge from the teacher. This transfer-\nence of knowledge can be accomplished by minimizing output logits of student\nand teacher networks through some distillation methods, such as logit-based,\nfeature-based, and relationship-based distillation methods [19].\nKD helps make computationally friendly deployment algorithms, making it\ninteresting for many biomedical imaging algorithms. Lingmei et al. [18] proposed\na CNN model for glioma classification. They used the KD approach to compress\nthe model and make it suitable for deployment on medical equipment. Salehi et\nal. [23] used a VGG16 [25] cloner network to calculate multi-level loss from a\nsource network for detecting anomalies. Their method relied on distilling inter-\nmediate knowledge from the ImageNet pretrained source network. In a similar\napproach, He et al. [8] used the KD technique to boost the performance of CNN\nfor ocular disease classification. They used fundus images and clinical informa-\ntion to train a ResNet [9] teacher first and used only the fundus images to train\na similar student network later. Guan et al. [7] detected Alzheimer’s disease\nby leveraging multi-modal data to train a teacher network. Their distillation\nscheme improved the prediction performance of the ResNet [9] student using a\nsingle imaging modality.\nHowever, all these works focused on using only CNN as a student network\nand did not explore the effects of different configurations and teacher networks\non the final classification outcome. In addition, the use of KD for histological\nartifacts has not been investigated yet.\n3 Data Materials and Method\nFig. 1 provides an overview of our air bubbles detection method using KD [10] in\na student-teacher recipe. We exploit KD for data-efficient training by leveraging\nthe transference of knowledge from the teacher model to the student model.\nOur proposed method uses a complex DCNN as the pre-trained teacher and a\nsmall ViT as the student when a small histological dataset is available. We are\nKanwal et al., Vision Transformers for Artifact Detection 5\ndoing a logit-based distillation [19] since our teacher and student models are very\ndifferent. The steps of our method are further described below.\n3.1 Dataset\nThe air bubbles dataset was prepared from 55 bladder biopsy WSIs, provided\nby Erasmus Medical Center (EMC), Rotterdam, The Netherlands. The glass\nslides were stained with Hematoxylin and Eosin (H&E) dyes and scanned with\nHamamatsu Nanozoomer at40× magnification. WSIs are stored inndpi format\nwith a pixel size of 0.227µm × 0.227 µm. These WSIs were manually annotated\nfor air bubbles and artifact-free tissue by a non-pathologist who has received\ntraining for the task. To prevent data leakage, the dataset was later split into\n35/10/10 training, validation, and test WSIs, respectively.\n3.2 Foreground Segmentation and Patching\nLet I40x\nWSI(i) correspond to a WSI at magnification level 40x (sometimes referred\nto as 400x).I40x\nWSI are very large gigapixel images, and it is not feasible to pro-\ncess the entire WSI at once. As such, all CPATH systems resort to patching\nor tiling of the image, or the ROI in the image, before further processing. Let\nT : I40x\nWSI(i)∈R → {xi\nj; j = 1··· J} represent the process of patching a ROI de-\nnoted byR of the imageI40x\nWSI(i) into a set ofJ patches, wherexi\nj ∈ RW×H×C and\nW, H, C present the width, height, and channels of the image, respectively. In\nthe patching process, foreground-background segmentation was performed first\nby transforming (Red, Green, Blue) RGB images to (Hue, Saturation, Value)\nHSV color space. Later, Otsu thresholding was applied to the value channel to\nobtain the foreground with tissue. The extracted foreground was later divided\nover a non-overlapping square grid, and patches with at least 70% overlap to the\nannotation region (R) were extracted.\nLet D = (X, y) = {(xn, yn)}N\nn=1 denote our prepared dataset of N patches\nfrom a set of WSIs andyn ∈ {0, 1} is the binary ground truth for then-th\ninstance, where 1 indicates a patch within a region marked as air bubbles. Fig 1\n(step 1) shows the patches xn of 224 × 224 × 3 pixels with air bubbles and\nartifact-free classes obtained from a WSI at 40x magnification.\n3.3 Selecting Student-Teacher Architectures\nLet‘s symbolize the student modelξ with parametersθ providing the prediction\noutput logits sn = ξθ(xn), and correspondingly, the teacher modelφ parame-\nterized byϕ providing the output logitstn = φϕ(xn).\nOur student model is a ViT, similar to the pioneering work [3], which lever-\nages multi-head self-attention mechanism [30] to capture content-dependant re-\nlations across the input patch. At the image pre-processing layer, the patches\nof 224 × 224 pixels are split into the non-overlapping cells of16 × 16 pixels.\nLater, the linear embedding layer flattens these cells, and positional encodings\nare added before feeding the embeddings to the pile of transformer blocks, as\nillustrated in Fig. 1 (step 2). Since convolutional networks have shown their effi-\ncacy in image recognition tasks, transferring knowledge from a DCNN network\n6 Kanwal et al., Vision Transformers for Artifact Detection\ncan help the ViT absorb inductive biases. Therefore, we rely on popular state-of-\nthe-art DCNNs for selecting teacher architecture. Nevertheless, we systemically\ndiscoverappropriatestudentandteachercandidatesduringtheexperimentslater\nto demonstrate the approach’s effectiveness over TL.\n3.4 Training Student under Knowledge Distillation\nAfterselectingstudentandteacherarchitectures,webegintheprocessoftraining\nthe studentξ. The goal is to trainξ with the assistance of aφ to improve the\nξ‘s generalization performance using additional knowledge beyond the labels.\nOur approach is similar to Hinton et al. [10] where model outputss, andt are\nnormalized by a temperatureT parameter before using the softmax functionσ.\nThe increasing value ofT softens the impact of the fluctuations in the output\nprobability distribution; therefore, more knowledge can be devolved with each\ninput xn. Instead of using softmax onsn, we take advantage of the log-softmax\nfunction σ∗, which stabilizes the distillation process by penalizing for incorrect\nclass. σ∗ also adds efficiency by optimizing gradient calculations.\nThe output logits for input patchxn can be written as;\nsn = ξθ(xn) and tn = φϕ(xn) (1)\nLet the log-softmax and softmax on logits,σ∗(s/T) and σ(t/T), for each\nelement can be defined as (see Eq. (2));\nσ∗(si/T) =log\n\u0012 exp (si/T)Pc\nj=1 exp (sj/T)\n\u0013\nand σ(ti/T) = exp (ti/T)Pc\nj=1 exp (tj/T) (2)\nwhere c is the total number of classes andT is the temperature. The class\nprobabilities at the output of theξ and φ model can thus be written as;\npξ = σ∗(s/T) =σ∗(ξθ(x)) and pφ = σ(t/T) =σ(φϕ(x)) (3)\nThe student lossLstudent (Eq. (4)) provides hard targets and is obtained by\napplying cross entropyLCE on ground truthy, ands when T is set to 1;\nLstudent = LCE (y, s) =−\ncX\ni=1\nyi · log(σ∗(si)) (4)\nDistillation lossLdistillation provides the soft targets and is computed from the\npξ andpφ by applying Kullback-Leibler divergenceKLD. Since the outputs from\nξ and φ were normalized byT, we multiply the loss withT2 to maintain their\nrelative contribution;\nLdistillation = T2 × KLD(pξ∥pφ) =T2 ·\ncX\ni=1\npξi log pξi\npφi\n(5)\nThe final loss function, as shown in Eq. (6), is a weighted average of student and\ndistillation losses whereα ∈ [0, 1);\nLFinal = α × Lstudent + β × Ldistillation · : β = 1− α (6)\nKanwal et al., Vision Transformers for Artifact Detection 7\nHigh entropy in soft targets offers significantly more information per training\npatch than hard targets [10], allowing the student ViT to train with fewer data\nand a higher learning rate. Therefore, using a smaller alpha can be beneficial if\nthe ξ is trained from scratch. Our standalone training setup for baseline compar-\nison can be obtained by puttingα and T equal to one and replacing log softmax\nwith softmax function.\n3.5 Prediction\nOnce the final loss is minimized based on the experimental setup (defined in\nSec. 4), we find predictions from the studentξ by settingT equal to one. For an\nunseen test patchx∗, output can be defined as (7);\nˆys = arg max(σ(s∗)) = arg max(σ(ξθ(x∗))) ∈ {0, 1} (7)\n4 Experimental Setup\nImplementation Details: The patch extraction was accomplished using the\nHistoLab library. Extracted patches were normalized to ImageNet [2] mean and\nstandard deviation. We augmented data at every training epoch using random\ngeometric transformations, such as rotations, horizontal and vertical flips. ViTs\nwere borrowed from Timm Library, and the experimental setup was built on\nthe Pytorch. We used four variants of ViTs with different parametric depths\nfrom [3,29], where the classifier was replaced by a fully connected (FC) layer.\nWe used four state-of-the-art DCNNs with varying parametric complexity. All\nDCNN backbones were initialized with ImageNet [2] weights, and classifiers were\nreplaced with three-layer FC classifiers. All classifiers were initialized with ran-\ndom weights. After hyper-parameter exploration, the final parameters were set\nto a batch size of 64, SGD optimizer, ReduceLROnPlateau scheduler with a\nlearning rate of 0.001, dropout of 0.2, cross-entropy loss, and early stopping\nwith the patience of 20 epochs on validation loss to prevent over-fitting. For KD\nparameters, values ofT ∈ {2, 5, 10, 20, 40} and α ∈ {0.3, 0.5, 0.7} were explored.\nThe best model weights are used to report the results. The NVIDIA GeForce\nA100 SXM 40GB GPU was utilized for training all models.\nEvaluation Metrics: We evaluate the presented method using accuracy,\nF1-score, and Mathew Correlation Coefficient (MCC). Let TP, FN, FP, and TN\ndescribe true positive, false negative, false positive, and false negative predic-\ntions. The accuracy, termed as(T P+ T N)/(T P+ F N+ F P+ T N), is the\nratio of correct predictions by the model. F1 is the harmonic mean, defined as\n2·(precision·recall)/(precision+recall) where Recall =T P/(T P+F N) and Pre-\ncision =T P/(T P+F P). MCC is an informative measure in binary classification\nover imbalanced datasets and is defined as Eq. (8).\nMCC = TP · TN − FP · FNp\n(TP + FP ) · (TP + FN ) · (TN + FP ) · (TN + FN )\n∈ [−1, 1] (8)\n8 Kanwal et al., Vision Transformers for Artifact Detection\nTable 1. Results from Exp. 1: Four variants of Deep Convolutional Neural Networks\n(DCNNs) and Vision Transformers (ViTs), with increasing parametric complexity, are\ntrained for the air bubbles detection task. The best outcomes in every section are\nbolded. ViT-tiny and MobileNet architectures provide the best results on the test set.\nValidation Set Test SetArchitecture Param.\n(#) Acc.(%) F1 MCC(⇑) Acc.(%) F1 MCC(⇑)\nDeep Convolutional Neural Networks (DCNNs)\nMobileNetv3 [12] 3.52M 98.28 0.983 0.965 93.88 0.945 0.876\nEfficientNet [26] 20.89M 96.52 0.966 0.931 92.54 0.935 0.851\nDenseNet161 [13] 27.66M 98.12 0.982 0.962 91.32 0.925 0.828\nVGG16 [25] 136.42M 98.34 0.984 0.966 92.31 0.932 0.846\nVision Transformers (ViTs)\nViT-tiny [29] 5.52M 98.67 0.987 0.973 92.35 0.933 0.847\nViT-small [29] 21.66M 97.01 0.971 0.941 91.16 0.922 0.822\nViT-large [3] 303.30M 98.12 0.982 0.962 92.08 0.928 0.839\nViT-huge [3] 630.76M 95.85 0.962 0.918 91.43 0.925 0.829\nResults from Literature (Validation Accuracy (%))\nDeiT-S in [22] 91.5-92 ResNet-50 in [22] 88-89 VGG16 in [24] 87.33\n5 Results and Discussion\n5.1 Exp. 1: Baseline Experiments for Architecture Decision\nIn this experiment, we only apply TL to a set of architectures. We evaluate state-\nof-the-art DCNNs, namely MobileNetv3 [12], EfficientNet [26], DenseNet161 [13]\nand VGG16 [25] architectures and a family of fourViTs [3,29], with increasing ar-\nchitecture size. Exp 1 provides a baseline as well as helps to choose architectures\nfor the KD setup in later experiments. Table 1 reports the results of the vali-\ndation and test set. DCNNs largely exceed the performance of ViTs, where top-\nperforming ViT lags the generalization performance of top-performing DCNNs\nby 3% in MCC. Moreover, architectures with sizeable parameters like VGG16\nand ViT-tiny and MobileNet, despite being architectures with fewer parameters,\nemerge as appropriate student and teacher candidates, respectively, based on the\ntest results and outperform the results from the literature.\n5.2 Exp. 2: How Important is Teacher‘s Knowledge?\nThis experiment evaluates the impact of existing teacher knowledge in the KD\nprocess to assess the real-life analogy where good teachers make good students.\nTherefore, we initialize MobileNet teachers with no knowledge (scratch), knowl-\nedge from a general domain (ImageNet), knowledge from another WSI artifact\n(damaged tissue [15]), and finally, domain-relevant knowledge (air bubbles) from\nthe previous experiment. In addition, we also select VGG16 with air bubble\nknowledge as a teacher to assess the effect of highly parametric DCNN in the\nKanwal et al., Vision Transformers for Artifact Detection 9\nTable 2. Results from Exp. 2: Knowledge Distillation (KD) outcome for selected\nteacher and student candidates from Exp.1. The values ofα, Tare fixed at 0.5 and 10,\nrespectively. The best results in every part are marked in bold, and the second best is\nunderlined. ViT-tiny, with two scratch and ImageNet initialization, is used for baseline\ncomparisons. Two teachers (MobileNet and VGG16) with air bubbles knowledge are\nused. While MobileNet is also initialized with knowledge of other domains to evaluate\nthe importance of teachers’ knowledge.\nValidation Set Test SetArchitecture (Initial.) Acc.(%) F1 MCC(⇑) Acc.(%) F1 MCC(⇑)\nBaseline (Initial.) - Standalone training\nViT-tiny (Scratch) 96.13 0.963 0.922 91.51 0.925 0.829\nViT-tiny (ImageNet [2]) 98.67 0.987 0.973 92.35 0.933 0.847\nTeacher (Initial.) - Student [ViT-tiny (Scratch)]\nMobileNet (Scratch) 96.13 0.962 0.924 87.92 0.889 0.756\nMobileNet (ImageNet [2]) 95.58 0.957 0.914 92.31 0.927 0.848\nMobileNet (Damaged [15]) 76.8 0.785 0.533 49.23 0.608 -0.075\nMobileNet (Air bubbles) 98.01 0.981 0.960 95.25 0.957 0.904\nVGG16 (Air bubbles) 97.18 0.973 0.944 93.42 0.940 0.867\nTeacher (Initial.) - Student [ViT-tiny (ImageNet)]\nMobileNet (Scratch) 98.73 0.983 0.971 93.38 0.941 0.866\nMobileNet (ImageNet [2]) 98.62 0.987 0.972 93.40 0.942 0.867\nMobileNet (Damaged [15]) 50.08 0.211 0.09 35.51 0.116 -0.294\nMobileNet (Air bubbles) 98.61 0.987 0.973 95.60 0.961 0.911\nVGG16 (Air bubbles) 98.67 0.986 0.972 94.19 0.948 0.882\nKD process. For this experiment, the values ofα, Tare fixed at 0.5 and 10,\nrespectively. The student is a ViT-tiny architecture initialized with random and\nImageNet weights separately.\nTable 2 exhibits that KD remarkably improves ViT‘s classification ability.\nEven without ImageNet knowledge, ViT-tiny, under the KD framework, sur-\npasses all metrics under both MobileNet and VGG16 teachers. However, the\nbest results are obtained using the MobileNet teacher, ascertaining that hid-\nden knowledge can be easily distilled from a simpler architecture. Interestingly,\nteachers with knowledge other than the relevant domain (air bubbles) produce\npoorly performing student. Although the student with ImageNet knowledge does\nnot indicate gain on the validation results relative to the baseline, it achieves 3%\nand 7% improvement in F1 and MCC scores on the test set, respectively.\nOverall, the test results demonstrate that the KD is promising to train gen-\neralized ViT-tiny with little data, even without pretrained weights. ViT sig-\nnificantly enhances its generalization against the baseline when trained in a\nstandalone setting. Especially when the teacher is enriched with the knowledge\nrelated to the task. KD, on top of ImageNet TL, provides a marginal gain in the\nperformance of ViT-tiny, overcoming the reliance on pretrained weights.\n10 Kanwal et al., Vision Transformers for Artifact Detection\n5.3 Exp. 3: Influence of KD Parameters\nSince the initialization of teachers with air bubbles knowledge has been shown\nto improve the learning process, it would be interesting to assess the influence\nof DCNN teachers under the different KD parameters (T and α). In this ex-\nperiment, we choseT ∈ {2, 5, 10, 20, 40} and α ∈ {0.3, 0.5, 0.7} to estimate the\ninfluence of teacher‘s output on ViT student, trained from scratch. The baseline\nexperiment corresponds toα and T = 1and uses sigmoid on ViT outputs. Fig. 2\n(a) and (b) show MCC values as the effect of temperature on simple DCNN\nlike MobileNet and complex DCNN like VGG16. Though the ViT-tiny student\ntrained under the VGG16 teacher scores better on the validation set whenT is\nhigh, the MobileNet teacher reveals better transference of hidden knowledge on\nall T values on the test set. Fig. 2 (c) depicts the effect ofα on ViT’s generaliza-\ntion results. Allα values give better results than the baseline, concluding that\nincluding distillation loss improves training compared to only student loss.\nTo sum up, the teacher‘s outcome strongly influences the student‘s general-\nizability in the KD process. Most of theT and α values deliver a noticeable gain\nover the standalone training in our case. However,intermediate T values and\nassigning equal weightto student and distillation loss is the most advantageous.\n(a) (b) (c) \nFig. 2. Results from Exp. 3: Knowledge Distillation (KD) improves the perfor-\nmance of the Vision Transformer (ViT-tiny) under the supervision of both MobileNet\nand VGG16 teachers. (a) and (b) shows an improved performance from the baseline\n(standalone training from scratch), under all temperature (T) values, on validation and\ntest set. (c) depicts the influence of giving higher/lower weightage to distillation loss\nfrom the teacher network (see Sec. 3). The MobileNet teacher, despite being simpler\narchitecture, enriches ViT-tiny‘s generalization capability on all chosenα and T values.\n6 Conclusion and Future Work\nThis paper presents the Knowledge Distillation (KD) to boost the generalization\nperformance of small Vision Transformers (ViTs) on a small histopathological\ndataset. The main motivation is to create a well-performing and efficient prepro-\ncessing pipeline that requires a generalized and computationally-friendly model.\nWe evaluated various pretrained DCNNs and ViTs for the air bubbles artifact\ndetection task. ViTs, trained in a standalone setting, underperform DCNNs on\nunseen data. Our approach exploits the KD, in the absence of pretrained weights,\nto enhance the performance of ViT by training under the guidance of a DCNN\nKanwal et al., Vision Transformers for Artifact Detection 11\nteacher. Our analysis found that KD provides significant gain under most dis-\ntillation settings when the teacher holds the knowledge of the same task. In\nconclusion, the ViT, when trained under KD, outperforms its state-of-the-art\nDCNN teacher and its counterpart in standalone training.\nIn future work, the method can be developed and tested on larger cohorts\nof histological data with stain variations and to detect multiple artifacts. More-\nover, artifact detection by ViT trained under the student-teacher recipe can be\ncombined as a preprocessing step with a diagnostic or prognostic algorithm in\nthe computational pathology system.\n7 Acknowledgment\nThis research is supported by the European Horizon 2020 program under Marie\nSkłodowska-Curie grant agreement No. 860627 (CLARIFY). The authors have\nno relevant financial or non-financial interests to disclose.\nReferences\n1. Bhojanapalli, S., Chakrabarti, A., Glasner, D., Li, D., Unterthiner, T., Veit, A.:\nUnderstanding robustness of transformers for image classification. In: Proc. of the\nIEEE International Conf. on Computer Vision (ICCV). pp. 10231–10241 (2021)\n2. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. In: 2009 IEEE ICCV. pp. 248–255. Ieee (2009)\n3. Dosovitskiy, A., Beyer, L., et al.: An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n4. Fuster, S., Khoraminia, F., et al.: Invasive cancerous area detection in non-muscle\ninvasive bladder cancer whole slide images. In: IEEE 14th Image, Video, and Mul-\ntidimensional Signal Processing Workshop (IVMSP). pp. 1–5. IEEE (2022)\n5. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.:\nImagenet-trained cnns are biased towards texture; increasing shape bias improves\naccuracy and robustness. arXiv preprint arXiv:1811.12231 (2018)\n6. Golatkar, A., Anand, D., et al.: Classification of breast cancer histology using\ndeep learning. In: International conf. image analysis and recognition. pp. 837–844.\nSpringer (2018)\n7. Guan, H., Wang, C., Tao, D.: Mri-based alzheimer’s disease prediction via distilling\nthe knowledge in multi-modal data. NeuroImage244, 118586 (2021)\n8. He, J., Li, C., Ye, J., Qiao, Y., Gu, L.: Self-speculation of clinical features based on\nknowledge distillation for accurate ocular disease classification. Biomedical Signal\nProcessing and Control67, 102491 (2021)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: Proceedings of the IEEE CVPR. pp. 770–778 (2016)\n10. Hinton, G., Vinyals, O., Dean, J., et al.: Distilling the knowledge in a neural net-\nwork. arXiv preprint arXiv:1503.025312(7) (2015)\n11. Hossain, M.S., Nakamura, T., Kimura, F., Yagi, Y., Yamaguchi, M.: Practical\nimage quality evaluation for whole slide imaging scanner. In: Biomedical Imaging\nand Sensing Conference. vol. 10711, pp. 203–206. SPIE (2018)\n12 Kanwal et al., Vision Transformers for Artifact Detection\n12. Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu,\nY., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: Proceedings of\nthe IEEE International Conference on Computer Vision. pp. 1314–1324 (2019)\n13. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected\nconvolutional networks. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition. pp. 4700–4708 (2017)\n14. Kanwal, N., Amundsen, R., Hardardottir, H., Janssen, E.A., Engan, K.: Detection\nand localization of melanoma skin cancer in histopathological whole slide images.\narXiv preprint arXiv:2302.03014 (2023)\n15. Kanwal, N., Fuster, S., et al.: Quantifying the effect of color processing on blood\nand damaged tissue detection in whole slide images. In: IEEE 14th Image, Video,\nand Multidimensional Signal Processing Workshop (IVMSP). pp. 1–5. IEEE (2022)\n16. Kanwal, N., Pérez-Bueno, F., Schmidt, A., Engan, K., Molina, R.: The devil is in\nthe details: Whole slide image acquisition and processing for artifacts detection,\ncolor variation, and data augmentation. IEEE Access10, 58821–58844 (2022)\n17. Kanwal, N., Rizzo, G.: Attention-based clinical note summarization. In: Proceed-\nings of the 37th ACM Symposium on Applied Computing. pp. 813–820 (2022)\n18. Lingmei, A., et al.: Noninvasive grading of glioma by knowledge distillation base\nlightweight convolutional neural network. In: IEEE 2021 AEMCSE. pp. 1109–1112\n19. Meng, H., Lin, Z.e.a.: Knowledge distillation in medical data mining: A survey. In:\n5th International Conf. on Crowd Science and Engineering. pp. 175–182 (2021)\n20. Naseer, M., Ranasinghe, K., Khan, S., Hayat, M., Shahbaz Khan, F., Yang, M.H.:\nIntriguing properties of vision transformers. NeurIPS34, 23296–23308 (2021)\n21. Noorbakhsh, J., Farahmand, S., et al.: Deep learning-based cross-classifications\nreveal conserved spatial behaviors within tumor histological images. Nature com-\nmunications 11(1), 1–14 (2020)\n22. Raipuria, G., Singhal, N.: Stress testing vision transformers using common\nhistopathological artifacts. In: Medical Imaging with Deep Learning (2022)\n23. Salehi, M., Sadjadi, N., Baselizadeh, S., Rohban, M.H., Rabiee, H.R.: Multireso-\nlution knowledge distillation for anomaly detection. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. pp. 14902–14912 (2021)\n24. Shakhawat, H.M., Nakamura, T., Kimura, F., Yagi, Y., Yamaguchi, M.: Automatic\nquality evaluation of whole slide images for the practical use of whole slide imaging\nscanner. ITE Trans. on Media Technology and Applications8(4), 252–268 (2020)\n25. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\n26. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural\nnetworks.In:Internationalconf.onmachinelearning.pp.6105–6114.PMLR(2019)\n27. Taqi, S.A., Sami, S.A., Sami, L.B., Zaki, S.A.: A review of artifacts in histopathol-\nogy. Journal of oral and maxillofacial pathology: JOMFP22(2), 279 (2018)\n28. Tomasetti, L., Khanmohammadi, M., Engan, K., Høllesli, L.J., Kurz, K.D.: Multi-\ninput segmentation of damaged brain in acute ischemic stroke patients using slow\nfusion with skip connection. arXiv preprint arXiv:2203.10039 (2022)\n29. Touvron, H., et al.: Training data-efficient image transformers & distillation\nthrough attention. In: Int. Conf. on Machine Learning. pp. 10347–10357 (2021)\n30. Vaswani, A., Shazeer, N., et al: Attention is all you need. Advances in neural\ninformation processing systems (2017)"
}