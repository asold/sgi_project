{
    "title": "Are genomic language models all you need? Exploring genomic language models on protein downstream tasks",
    "url": "https://openalex.org/W4402100089",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5098751393",
            "name": "Sam Boshar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4317142227",
            "name": "Evan Trop",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2500856740",
            "name": "Bernardo P. de Almeida",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2922404401",
            "name": "Liviu Copoiu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2305370047",
            "name": "Thomas Pierrot",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2769882797",
        "https://openalex.org/W3203588026",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W2941112903",
        "https://openalex.org/W2104972430",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W2144893575",
        "https://openalex.org/W4281993476",
        "https://openalex.org/W3015921770",
        "https://openalex.org/W3127238141",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W4230640326",
        "https://openalex.org/W2950374603",
        "https://openalex.org/W2007807048",
        "https://openalex.org/W4327550249",
        "https://openalex.org/W6837789219",
        "https://openalex.org/W3083874241",
        "https://openalex.org/W3174395937",
        "https://openalex.org/W4392095606",
        "https://openalex.org/W3217031179",
        "https://openalex.org/W3135682489",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W2735621019",
        "https://openalex.org/W2379594833",
        "https://openalex.org/W2068628419",
        "https://openalex.org/W2127435852",
        "https://openalex.org/W4309506674",
        "https://openalex.org/W6731867162"
    ],
    "abstract": "Abstract Motivation Large language models, trained on enormous corpora of biological sequences, are state-of-the-art for downstream genomic and proteomic tasks. Since the genome contains the information to encode all proteins, genomic language models (gLMs) hold the potential to make downstream predictions not only about DNA sequences, but also about proteins. However, the performance of gLMs on protein tasks remains unknown, due to few tasks pairing proteins with the coding DNA sequences (CDS) that can be processed by gLMs. Results In this work, we curated five such datasets and used them to evaluate the performance of gLMs and proteomic language models (pLMs). We show that gLMs are competitive and even outperform their pLMs counterparts on some tasks. The best performance was achieved using the retrieved CDS compared to sampling strategies. We found that training a joint genomic-proteomic model outperforms each individual approach, showing that they capture different but complementary sequence representations, as we demonstrate through model interpretation of their embeddings. Lastly, we explored different genomic tokenization schemes to improve downstream protein performance. We trained a new Nucleotide Transformer (50M) foundation model with 3mer tokenization that outperforms its 6mer counterpart on protein tasks while maintaining performance on genomics tasks. The application of gLMs to proteomics offers the potential to leverage rich CDS data, and in the spirit of the central dogma, the possibility of a unified and synergistic approach to genomics and proteomics. Availability and implementation We make our inference code, 3mer pre-trained model weights and datasets available.",
    "full_text": null
}