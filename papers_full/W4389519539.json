{
  "title": "WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models",
  "url": "https://openalex.org/W4389519539",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2248337351",
      "name": "Jun-Yan He",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A4304081856",
      "name": "Zhi-Qi Cheng",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2109258858",
      "name": "Chenyang Li",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2133907046",
      "name": "Jingdong Sun",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2798463504",
      "name": "Wangmeng Xiang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2530582112",
      "name": "Xianhui Lin",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2618178489",
      "name": "Kang Xiaoyang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2981323201",
      "name": "Zengke Jin",
      "affiliations": [
        "Royal College of Art",
        "Zhejiang Sci-Tech University"
      ]
    },
    {
      "id": "https://openalex.org/A2563109573",
      "name": "Yusen Hu",
      "affiliations": [
        "Carnegie Mellon University",
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A1897219996",
      "name": "Bin Luo",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2097758085",
      "name": "Geng Yifeng",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2970811884",
      "name": "Xuansong Xie",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3129831491",
    "https://openalex.org/W4298187450",
    "https://openalex.org/W4378771728",
    "https://openalex.org/W4377865162",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3168053944",
    "https://openalex.org/W2764273117",
    "https://openalex.org/W4385275531",
    "https://openalex.org/W3174301209",
    "https://openalex.org/W4308163867",
    "https://openalex.org/W2962974137",
    "https://openalex.org/W3034854791",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W4289785095",
    "https://openalex.org/W3107232127",
    "https://openalex.org/W4386076215",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W2563032747",
    "https://openalex.org/W1903594414",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3110257065",
    "https://openalex.org/W4362703229",
    "https://openalex.org/W4362509249",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4321473567",
    "https://openalex.org/W4321277285",
    "https://openalex.org/W4287029626",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W4320831569",
    "https://openalex.org/W2963097270",
    "https://openalex.org/W2624783673",
    "https://openalex.org/W4390874209",
    "https://openalex.org/W3162926177"
  ],
  "abstract": "Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Yusen Hu, Bin Luo, Yifeng Geng, Xuansong Xie. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 223‚Äì232\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nWordArt Designer: User-Driven Artistic Typography Synthesis using\nLarge Language Models\nJun-Yan He1 Zhi-Qi Cheng2‚àó Chenyang Li1 Jingdong Sun2 Wangmeng Xiang1\nXianhui Lin1 Xiaoyang Kang1 Zengke Jin3,4 Yusen Hu2,5\nBin Luo1 Yifeng Geng1 Xuansong Xie1\n1Alibaba DAMO Academy 2Carnegie Mellon University 3Zhejiang Sci-Tech University\n4Royal College of Art 5Imperial College London\nAbstract\nThis paper introduces WordArt Designer, a\nuser-driven framework for artistic typogra-\nphy synthesis, relying on the Large Language\nModel (LLM). The system incorporates four\nkey modules: the LLM Engine, SemTypo, Sty-\nTypo, and TexTypomodules. 1) The LLM En-\ngine, empowered by the LLM (e.g. GPT-3.5),\ninterprets user inputs and generates action-\nable prompts for the other modules, thereby\ntransforming abstract concepts into tangible\ndesigns. 2) The SemTypo module optimizes\nfont designs using semantic concepts, strik-\ning a balance between artistic transformation\nand readability. 3) Building on the semantic\nlayout provided by the SemTypo module, the\nStyTypo module creates smooth, refined im-\nages. 4) The TexTypo modulefurther enhances\nthe design‚Äôs aesthetics through texture render-\ning, enabling the generation of inventive tex-\ntured fonts. Notably, WordArt Designer high-\nlights the fusion of generative AI with artis-\ntic typography. Experience its capabilities on\nModelScope: https://www.modelscope.cn/\nstudios/WordArt/WordArt.\n1 Introduction\nTypography, a critical intersection of language and\ndesign, finds extensive applications across vari-\nous domains like advertising (Cheng et al., 2016,\n2017a,b; Sun et al., 2018), early childhood ed-\nucation (Vungthong et al., 2017), and historical\ntourism (Amar et al., 2017). Despite its widespread\nrelevance, the mastery of typography design re-\nmains an intricate task for non-professional design-\ners. Although attempts have been made to bridge\nthis gap between amateur designers and typogra-\nphy (Iluz et al., 2023; Tanveer et al., 2023), existing\nsolutions mainly generate semantically coherent\nand visually pleasing typography within predefined\nconcepts. These solutions often lack adaptability,\ncreativity, and computational efficiency.\n‚àóCorresponding author\nHi, WordArt Designer, can you design a piece of jewelry with the Chinese character ‚ÄúÁå´‚Äùfor my birthday?\nSure, here are the samples,\nWord\nDesigner\nSemTypo\n‚Ä¶\nStyTypo\nTexTypo\n‚Ä¶\nInputParsing\nConceptInterpretation\nTextureInterpretation\nLLMEngine\nFigure 1: Demonstration of WordArt Designer: Lever-\naging the power of the LLM (e.g. GPT-3.5), it integrates\nfour modules (LLM Engine, SemTypo, StyTypo, Tex-\nTypo) to transform user inputs into visually striking and\nsemantically rich multilingual typographic designs. It\ndemocratizes the art of typography design, enabling\nnon-professionals to realize their creative visions.\nTo overcome these limitations, we introduce Wor-\ndArt Designer (Fig. 1), a system composed of four\nprimary modules: the LLM Engine, SemTypo Mod-\nule, and StyTypo Module, supplemented by the\nTexTypo Module for texture rendering. This user-\nfocused system allows users to define their design\nneeds, including design concepts and domains. The\nsystem consists of:\n1. LLM Engine : Based on the power of the\nLLM (e.g. GPT-3.5), this engine interprets\nuser input and produces prompts for the Sem-\nTypo, StyTypo, and TexTypo modules.\n2. SemTypo Module: The SemTypo Module\ntransforms typography based on a provided\nsemantic concept. It involves a three-step pro-\ncess, including Character Extraction and Pa-\nrameterization, Region Selection for Trans-\nformation, and Semantic Transformation &\nDifferentiable Rasterization.\n223\nWord     Designerfrom Art to ProductCreative, Artistic, and Automatic\nFigure 2: Examples of artistic typography generated by WordArt Designer. These instances demonstrate the system‚Äôs\nability to produce aesthetically pleasing, semantically coherent, and stylistically diverse typographic designs.\n3. StyTypo Module: The StyTypo Module gen-\nerates smoother, more detailed images based\non the semantic layout image provided by the\nSemTypo module.\n4. TexTypo Module: The TexTypo Module\nmodifies ControlNet for texture rendering, en-\nsuring creativity while preserving legibility.\nThe workflow, illustrated in Fig. 1, begins with the\nLLM module interpreting user input. The output\nof each module serves as the input for the next,\nwith the final design decision made by the TexTypo\nmodule. This sequence ensures the final design\naligns with the user‚Äôs intent and maintains a unique\naesthetic appeal.\nThis design process is iterative, involving constant\ninteraction between the user‚Äôs input and the sys-\ntem‚Äôs modules. This user-centered approach guar-\nantees the creation of high-quality WordArt designs\n(See Fig. 2), making it an effective tool in creative\ndesign-dependent industries, such as food and jew-\nelry.\nExtensive experiments on WordArt Designer have\nvalidated its creativity, artistic expression, and ex-\npandability to different languages. The inclusion of\na ranking model significantly improves the success\nrate and overall quality of stylized images, ensuring\nthe production of high-quality WordArt designs.\nIn essence, WordArt Designer provides a creative,\nartistic, and fully automated solution for generating\nword art. Our research not only lays the ground-\nwork for future text synthesis studies but also in-\ntroduces numerous practical applications. Wor-\ndArt Designer can be employed in various areas,\nincluding media propaganda and product design,\nenhancing the efficiency and effectiveness of these\nsystems, thereby making them more practical for\neveryday use.\n2 Related work\nLLM and their Apps . Large Language Model\n(LLM) has been progressively improved and uti-\nlized in a wide range of applications (Anil et al.,\n2023; Raffel et al., 2020; Shoeybi et al., 2019; Ra-\njbhandari et al., 2020; Devlin et al., 2019; Cheng\net al., 2023). The outstanding performances ex-\nhibited by the ChatGPT and GPT series (Radford\net al., 2018; Brown et al., 2020; OpenAI, 2023)\nhave stimulated the widespread use of the LLM.\nThese models are adept at learning context from\nsimple prompts, leading to their increasing use as\nthe controlling component in intelligent systems\n(Wu et al., 2023; Shen et al., 2023). Building on\nthese insights, WordArt Designer incorporates the\nLLM to enhance system creativity and diversity.\nText Synthesis. While significant progress has\nbeen made in image synthesis, integrating legible\ntext into images remains challenging (Rombach\net al., 2022; Saharia et al., 2022). Some solutions,\nsuch as eDiff-I (Balaji et al., 2022) and DeepFloyd\n(Lab, 2023), employ robust LLMs, such as T5 (Raf-\nfel et al., 2020), for improved visual text genera-\ntion. Recent studies (Yang et al., 2023; Ma et al.,\n2023) have also looked into using glyph images\nas extra control conditions, while others like DS-\nFusion (Tanveer et al., 2023) introduce additional\nconstraints to synthesize more complex text forms,\nsuch as hieroglyphics.\nImage Synthesis. The surge in demand for per-\nsonalized image synthesis has spurred advances\nin interactive image editing (Meng et al., 2022;\nGal et al., 2023; Brooks et al., 2022; Zhao et al.,\n2018) and techniques incorporating additional con-\nditions, such as masks and depth maps (Rombach\net al., 2022; Huang et al., 2020). New research\n(Zhang and Agrawala, 2023; Mou et al., 2023;\nHuang et al., 2023) is exploring multi-condition\ncontrollable synthesis. For instance, ControlNet\n(Zhang and Agrawala, 2023) learns task-specific\nconditions end-to-end, providing more nuanced\ncontrol over the synthesis process.\nText-to-Image Synthesis. Significant strides in de-\nnoising diffusion probabilistic models have substan-\ntially enhanced text-to-image synthesis (Ho et al.,\n2020; Ramesh et al., 2021; Song et al., 2021; Dhari-\nwal and Nichol, 2021; Nichol and Dhariwal, 2021;\n224\nùë∏ùíäùíèùíë:\tDesign\tan\tart\twordof\t‚ÄúÁå´‚Äù\twith\tEnglish\tconcept\t‚Äúcat‚Äù\t\tfor\tjewelry\tdesign\nAinp: {‚Äúchar‚Äù: ‚ÄúÁå´‚Äù,  ‚Äúconcept‚Äù: ‚Äúcat‚Äù,  ‚Äúapplication‚Äù: ‚Äújewelry‚Äù}\nInitialStrokeSVGParameters SemTypo\nSemantic Typography Image Isem\nSemantic\tTypography\tModule\t\nRanking Model RM\nLLM Engine M A drawing of ‚Äúcat‚Äù, smooth, realistic ‚Ä¶ Formatted Prompt From ùë®ùíäùíèùíë:\nùë∏ùíîùíïùíö:\tPlease\tlistthe\trepresentative\tcategory\tor\tobject\tname\t\tin/of‚Äúcat‚Äù\t\tincluding\tin\treal-life,\tartist,\tand\tfilm\tworks.\nùë∏ùíïùíÜùíô:Please\tlistthe\trepresentative\tcategory\tor\tobject\tname\t\tin/of‚Äújewelry‚Äù\t\tincluding\tin\treal-life,\tartist,\tand\tfilm\tworks\n‚Ä¶‚Ä¶\nStyTypo\nA\tblack-and-white\tdrawing\tof\t\t‚ÄúHello\tkitty‚Äù,\t‚Ä¶\tFormatted Prompt From ùë®ùíîùíïùíö:\nTexTypo\n‚Äúgold‚Äù\t\ttexture,\t8k\t‚Ä¶\tFormatted Prompt From ùë®ùíïùíÜùíô:\nStylization\tTypography\tModule\t\nTexture\tTypography\tModule\t\nStylizationsuccessful or iteration> K ?\nParsing &SemTypo\nCocreation &StyTypo\nCocreation & TexTypo\nOverall Flow\nYes\nNo\nAtex:[{\"Object/Category Name\": \" Gold\", ‚Ä¶},{\"Object/Category Name\": \" Jade\", ‚Ä¶},{\"Object/Category Name\": \"‚ÄùCrystal\",...}, ‚Ä¶\nLLM Engine M[{\"Object/Category Name\": \"Hello Kitty\", ‚Ä¶},{\"Object/Category Name\": \"Garfield\", ‚Ä¶},{\"Object/Category Name\": \"Tabby Cat,...}, ‚Ä¶Asty: \nIsem\n{Isty}\nOutput:\t{Itex}\n{Isty} {Isty}\nLLM Engine M\nIsem\nIsty Itex\nInput Parsing\nStylization Prompt Synthesis\nTexture Prompt Synthesis\nFigure 3: The architectural framework of the proposed WordArt Designer system. This structure involves an LLM\nengine, the SemTypo module for Semantic Typography, the StyTypo module for Stylization Typography, and the\nTexTypo module for Texture Typography. These modules operate coherently, guided by a preset control flow, to\nfacilitate a seamless and innovative transformation of text into artistic typography.\nSaharia et al., 2022; Ramesh et al., 2022; Rombach\net al., 2022). Notable examples of these advance-\nments are latent diffusion models such as Imagen\n(Saharia et al., 2022), DALLE-2 (Ramesh et al.,\n2022) and LDM (Rombach et al., 2022), which\nhave enabled high-quality image generation.\n3 WordArt Designer\nThe WordArt Designer system utilizes an assort-\nment of typography synthesis modules, propelled\nby a Large Language Model (LLM) such as GPT-\n3.5), facilitating an interactive, user-centered de-\nsign process. As illustrated in Fig. 3, users define\ntheir design needs, including design concepts and\ndomains, e.g., \"A cat in jewelry design.\" The LLM\nengine interprets the input, generating prompts to\nguide SemTypo, StyTypo, and TexTypo modules,\nthus executing the user‚Äôs design vision.\nTo achieve automated WordArt design, we intro-\nduce a quality assessment feedback mechanism,\nwhich is vital for successful synthesis. The output\nfrom the ranking model is evaluated by the LLM\nengine to validate the quality of the synthesized\nimage, ensuring the creation of at least K quali-\nfied transformations. If this criterion is not met,\nthe LLM engine, along with the SemTypo and Sty-\nTypo modules and format directives, are restarted\nfor another design iteration. Subsequent sections\nwill delve into the details of each module‚Äôs func-\ntionality and operation.\n3.1 LLM Engine\nThe Large Language Model (LLM) engine is a cru-\ncial component for the WordArt designer. It serves\nas a knowledge engine and concretizes abstract\nnotions, like \"vegetables\" and \"fruit\", into texture\nprompts in the context of food, for the eventual syn-\nthesis of the artistic text. For most concrete nouns,\nsuch as \"cat\", \"dog\", \"flower\", etc., semantic typog-\nraphy can be successfully generated. However, for\nwords like abstract nouns and verbs, such as \"win-\nter\", \"hit\", etc., users often fail to provide desired\ndescriptions. The reason is that images compose\nhighly complex scenes for abstract concepts, which\nis not suitable for our WordArt designer system.\nTo address this issue, we employ the LLM to render\nabstract concepts into representative objects that\ncan be easily converted. Specifically, we can build\nour LLM engine using models like GPT-3.5 and\n225\nother LLMs, all of which have context-learning\ncapabilities. The prompts for input parsing, styliza-\ntion, and texture rendering are generated as:\nAinp = M(Qinp),Asty = M(Qsty),Atex = M(Qtex)\n(1)\nWhere Qinp, Qsty, and Qtex represent the stan-\ndard prompts for input parsing, stylization, and\ntexture rendering respectively. Qsty and Qtex are\nbuilt using formatted prompt templates with con-\ncepts derived from the input parsing. LLM engine\nhas ample capabilities to imbue our system with a\ncreative and engaging \"soul\", ensuring the quality\nof artistic text synthesis. We provide detailed tem-\nplates and full examples of prompts in Appendix A.\n3.2 SemTypo Module\nThe Semantic Typography (SemTypo) module al-\nters typographies based on a given semantic con-\ncept. It unfolds in three stages: (1) Character Ex-\ntraction and Parameterization, (2) Region Selection\nfor Transformation, and (3) Semantic Transforma-\ntion and Differentiable Rasterization.\nCharacter Parameterization. The first stage, as\ndisplayed in Fig. 3, starts by transforming the nat-\nural language input into a JSON format, specify-\ning the characters to modify, the semantic concept,\nand the application domain. The FreeType font li-\nbrary (David Turner et al., 1996) is then employed\nto extract character contours and convert them into\ncubic B√©zier curves characterized by a trainable set\nof parameters. For characters with surplus control\npoints, a subdivision routine fine-tunes the control\npoints Œ∏, using a differentiable vector graphic ras-\nterization scheme (Iluz et al., 2023).\nRegion Selection. Our unique contribution is the\nregion-based transformation method, the second\nstage of the SemTypo module. This approach\nfacilitates the selective transformation of certain\ncharacter segments, effectively reducing distortions\nthat typically affect typography generation in lan-\nguages with single-character words. We choose\nto transform a random contiguous subset of con-\ntrol points within a character, instead of the entire\ncharacter. We establish a splitting threshold of 20\npixels, with the set of control points randomly de-\ntermined within the range [500, min(1000/control\npoint count)], initiating from a random point.\nIn contrast to previous methods, such as the one by\nIluz et al. (Iluz et al., 2023), which used extra loss\nterms with inadequate success to maintain legibility\nof the synthesized typography, our method only\ninvolves loss computation from the transformed\nsections of the characters. This approach increases\nefficiency and guarantees careful manipulation of\ncharacter shapes, thus improving transformation\nquality.\nTransformation and Rasterization. In the final\nstage, the parameters are transformed and raster-\nized through the Differentiable Vector Graphics\n(DiffVG) scheme (Li et al., 2020). As shown in\nFig. 4, the transformed glyph imageIsem is created\nfrom the trainable parameters Œ∏of the SVG-format\ncharacter input, using DiffVG œï(¬∑). A segment of\nthe chosen character xis optimized and cropped to\nyield an enhanced image batch Xaug (Frans et al.,\n2022). The semantic concept Sand the augmented\nimage batch Xaug are both input into a vision-\nlanguage backbone model to compute the loss for\nparameter optimization. The Score Distillation\nSampling (SDS) loss is applied in the latent space\ncode z, as per the DreamFusion method (Poole\net al., 2023):\n‚àáŒ∏LSDS = Et,œµ[w(t)( ÀÜœµœï(atzt+œÉtœµ,y)‚àíœµ) ‚àÇz\n‚àÇXaug\n‚àÇXaug\n‚àÇŒ∏ ]\n(2)\nHere, t ‚àà{1,2,...,T }is uniformly sampled to\ndefine a noise latent code zt = atzt + œÉtœµ, with\nœµN ‚àº(0,1), and at, œÉt act as noise schedule regu-\nlators at time t. The multiplier w(t) is a constant,\ncontingent on at. This revised process refines ex-\npression and amplifies the variety of output.\n3.3 StyTypo Module\nThe Stylization Typography (StyTypo) module‚Äôs\nmain purpose is to generate smoother and more de-\ntailed images, enhancing the semantic layout image\nIsem. To speed up Isty generation, we use short it-\neration settings. However, this approach might lead\nto a lack of smoothness and details. To overcome\nthese potential drawbacks, the StyTypo module in-\ntroduces two main components: (1) stylized image\ngeneration, and (2) stylized image ranking and se-\nlection.\nStylized Images Generation . The Latent Diffu-\nsion Model (LDM) (Rombach et al., 2022) has\ngained attention for its ability to generate images\nbased on given input shapes. Therefore, we employ\nthe LDM‚Äôs depth2image methodology to stylize\ntypographic layouts, enhancing smoothness and in-\nfusing additional detail to create a unique \"sketch\"\nfor texture rendering. Fig. 5 illustrates this, where\n226\nInitial Stroke SVG Parameters ùúΩ\nDiffVG\nDeformation Image ùíôùë∞ & Region Cropped ùíô\nAugmentation ùëøùíÇùíñùíà\nùëßEncoderùëß!\nCat, smooth, realistic ‚Ä¶ Prompt from LLM :\nUNetùëß\nGradient flow\nClip\nFigure 4: Differential rasterization scheme of semantic typography. The character stroke inside the purple box is the\nselected part for optimization.\nthe top row images generated by the SemTypo mod-\nule, despite lacking smoothness and detail, provide\na comprehensive object representation. After be-\ning processed by the StyTypo module, the stylized\nimages on the lower row display an abundance of\ndetail and inventive renderings for each semantic\nimage input.\nSemantic Image ( ùë∞ùíîùíÜùíé)  generated by SemTypo\nStylization Image ( ùë∞ùíîùíïùíö) generated by StylTypo\n‚ÄúFlower‚Äù‚ÄúBird‚Äù‚ÄúMonkey‚Äù‚ÄúLemon‚Äù\nFigure 5: Comparison of the semantic and stylization\nimages. Stylization images contain more details.\nFormally, given a semantic typography image\nIsem from the SemTypo module, and a stylization\nprompt Asty synthesized by the LLM Engine M,\nwe can create the stylization image Isty as:\nIsty = StyTypo(Isem,Asty) (3)\nwhere StyTypo utilizes the depth2image pipeline\nderived from the LDM (Rombach et al., 2022) to\ncarry out the stylization.\nRanking and Selection. To augment the StyTypo\nmodule‚Äôs efficiency, we introduce a ranking model\nthat orders and filters the generated results. Specifi-\ncally, we establish a quality evaluation dataset con-\nsisting of stylized characters classified into two\ngroups: (1) Successful Stylization, and (2) Failed\nStylization. The dataset encompasses 141 single-\ncharacter Chinese characters and 5,814 stylized\ntypographic images. We leverage the ResNet18\nclassification model (He et al., 2016) to learn the\nquality distribution of the stylization images. Dur-\ning the filtering stage, the trained model serves as\na ranking model, providing ranking scores. Based\non these scores, the top ‚Äôx‚Äô results are selected.\n3.4 TexTypo Module\nTo advance the styling capacities of the Stylization\nTypography (StyTypo) module, we adapted Con-\ntrolNet (Zhang and Agrawala, 2023) for the pur-\npose of texture rendering, resulting in the creation\nof the Texture Typography (TexTypo) module.\nAs can be seen in Fig. 6, ControlNet‚Äôs original con-\ntrol conditions relied heavily on the Canny Edge\nand Depth data. This constraint tended to produce\nfonts that were lacking in creativity and artistic flair.\nTo counter this, we introduced Scribble conditions\nas an alternate control condition into ControlNet,\nwhich encourage the generation of more creatively\ntextured fonts without compromising on readabil-\nity.\nFigure 6: Comparison between Canny Edge and Scrib-\nble conditions for ControlNet texture rendering. The\nfirst row is generated using the Canny Edge condition,\nwhile the rest are from the Scribble condition.\nFurthermore, to cater to a range of industrial sec-\ntors, we have reconfigured ControlNet to incorpo-\nrate pre-trained stable diffusion models that are\nrelevant to different fields. These include, but are\nnot limited to, commercial advertising, fashion de-\nsign, gaming interfaces, tech products, and artistic\ncreations.\nTechnically, we provide the ControlNet parameters\nwith conditions Canny Edge, Depth, Scribble, as\nwell as original font images. The TexTypo model\nreceives these parameters and generates the tex-\n227\n‚ÄòÁãê‚Äô(Fox) ‚ÄòËõô‚Äô(Frog) ‚ÄòËè†‚Äô(Pineapple) ‚ÄòÁ´π‚Äô(Bamboo) ‚Äò‰Ωõ‚Äô(Buddha) ‚ÄòÁÇÆ‚Äô(Cannon)\nAnimal Plant/Fruit Abstract/Artificial object\n‚ÄòËä±‚Äô(Flower) ‚ÄòÁ™ó‚Äô(Window)\n‚ÄòÁãÆ‚Äô(Lion)\nFigure 7: Results showcasing the adaptability of the WordArt Designer. The first row targets the concept of ‚Äúfood‚Äù,\nwhich is further specified to ‚Äúcandy‚Äù, ‚Äúpasta‚Äù, ‚Äúcheese‚Äù, ‚Äúfruits‚Äù, ‚Äúbread‚Äù, ‚Äúvegetables‚Äù or ‚Äúchocolate‚Äù. The second\nrow targets ‚Äújewelry‚Äù, concretized to ‚Äújewels‚Äù, ‚Äúgold‚Äù or ‚Äújade‚Äù. The variety of styles highlighted underscores\nWordArt Designer‚Äôs versatility in creating unique artistic typography, pushing past traditional design boundaries.\ntured font image as,\nItex = TexTypo(Isty,Atex,Pcond), (4)\nwhere Atex represents the prompts synthesized by\nthe LLM engine M, and Pcond stands for the con-\ntrol parameters, resulting in a creatively rendered\ntextured font as the output.\n4 Deployment Details\nWordArt Designer tool has successfully been inte-\ngrated into ModelScope, utilizing the TongYi Qian-\nWen 14b model as the LLM engine. In terms of\ndeployment, the StyTypo and TexTypo Modules\nare hosted in Docker containers, ensuring both flex-\nibility and scalability in deployment. StyTypo is\npowered by a Linux platform with 48 cores, 384GB\nRAM, and 4 Nvidia V100 32GB GPUs, taking\nroughly 32 seconds to generate 4 images. In con-\ntrast, TexTypo operates within a similar Linux en-\nvironment but with 24 cores, 64GB RAM, and a\nsingle Nvidia V100 32GB GPU, and typically pro-\nduces 4 images within a span of 5 to 10 seconds.\nFor the Ranking Model, the mmpretrain (Contrib-\nutors, 2023) is used to train a ResNet 18 model (He\net al., 2016), with a total of 100 epoches at a batch\nsize of 32. The SGD optimizer was used with a\nlearning rate of 0.01. The training ran on a single\nNvidia V100 16GB GPU.\n5 Experiments\nCreativity & Artistic Ability. We operationalize\nthe concept of texture rendering to evaluate the\nCreativity and Artistic Ability of the WordArt De-\nsigner. The outcomes are demonstrated in Fig. 7.\nThe first row of art words is generated by embody-\ning the concept ‚Äúfood‚Äù, which is further specified\nto ‚Äúcandy‚Äù, ‚Äúpasta‚Äù, ‚Äúcheese‚Äù, ‚Äúfruits‚Äù, ‚Äúbread‚Äù,\n‚Äúvegetables‚Äù or ‚Äúchocolate‚Äù. The second row rep-\nresents the concept ‚Äújewelry‚Äù, concretized to ‚Äújew-\nels‚Äù, ‚Äúgold‚Äù or ‚Äújade‚Äù. The smart and reasonable\ntexture rendering contributes to the creativity and\nartistic appeal of the output.\nExpandability to Different Languages. Our Sem-\nTypo module, grounded on differentiable rasteri-\nzation, is theoretically compatible with all types\nof languages. Beyond Chinese (i.e., hieroglyphs),\nwe explore the expandability of WordArt Designer\nwith the representative language, English (i.e., the\nLatin alphabet). Fig. 8 presents a collection of\nrendered results for Chinese characters and corre-\nsponding English words, substantiating that Wor-\ndArt Designer effectively accommodates different\nlanguages.\nEffect of Ranking Model . To determine the ef-\nfectiveness of the ranking model, we divide the\naforementioned character dataset into a training\nand validation set by randomly selecting 20 charac-\nters for validation. We use precision and recall to\nmeasure the model‚Äôs ability to classify individual\nimages as successfully stylized or not. In addition,\nwe compare WordART Designer‚Äôs overall success\nrate on transforming a character using the Ranking\nModel and a Random Model (a character is deemed\nsuccessfully transformed if at least one of the out-\nput images is successfully stylized). As shown in\nTable 1, our ranking model significantly outper-\nforms the random model, indicating its efficacy.\nWhen top-10 images are selected, we guarantee\nthat each character has at least one successfully\nstylized image. To balance precision and recall,\nthe number of selected images should ideally range\nfrom 2 to 5.\n228\nTable 1: Ablation study of the ranking model on the\nvalidation set. ‚Äòp‚Äô, ‚Äòr‚Äô, and ‚Äòs‚Äô stand for precision, recall,\nand success rate, respectively. ‚Äòx‚Äô in ‚ÄòTopX‚Äô indicates\nthe number of stylized images retained. In the ranking-\nbased method, ‚ÄòTopX‚Äô are selected based on ranking\nscores, while for the random-based method, ‚ÄòTopX‚Äô are\nselected randomly. Results of the random-based method\nare obtained by averaging over 10,000 iterations. In-\ncreased values are indicated in blue.\nMethods Metric Top1 Top2 Top5 Top10\nRandom\np 18.3 18.1 18.2 18.2\nr 4.5 8.9 22.4 44.8\ns 18.3 33.1 63.4 86.5\nRanking\np 60.0‚Üë41.7 62.5‚Üë44.4 46.0‚Üë27.8 32.0‚Üë13.8\nr 14.6‚Üë10.1 30.8‚Üë21.9 56.3‚Üë33.9 78.8‚Üë34.0\ns 60.0‚Üë41.7 80.0‚Üë46.9 85.0‚Üë21.6 100.0‚Üë13.5\nat\nC  p\nBoa\near\n all\nFigure 8: Chinese Characters and their corresponding\nEnglish art words.\n‚ÄúÁãê‚ÄùÔºàFoxÔºâ‚ÄúËõô‚ÄùÔºàFrogÔºâ‚ÄúËè†‚ÄùÔºàPineappleÔºâ‚Äú‚Ωµ‚ÄùÔºàBambooÔºâ‚Äú‰Ωõ‚ÄùÔºàbuddhaÔºâ\nFigure 9: Various notable applications of our WordArt\nDesigner, including art word poster creation (row 1)\nand urban master plan design (row 2). Note that re-\nvAnimated is used as the base LDM (Rombach et al.,\n2022). For rows 1-2, we further apply the Lora models\nBlindbox and MasterPlan respectively.\n5.1 Application\nWordArt Image. We experiment with various ap-\nplication possibilities for WordArt Designer. The\nresults, exhibited in Fig. 9, are representative and\nnot cherry-picked. WordArt Designer exhibits\npromising potential in areas like art word poster de-\nsign and even city planning. We are confident that\nWordArt Designer will bring innovative inspiration\nto professional designers.\nWordArt Animation . We also utilize Con-\ntrolVideo (Zhang et al., 2023) to synthesize art\nword videos, illustrating the transformation of the\nword/character. The Chinese characters for Bam-\nboo‚Äù and Flower‚Äù are used in the video generation\nprocess, with the \"Van Gogh‚Äôs painting\" style ap-\nplied to the animations, proving useful for Chinese\neducation. Please refer to Fig. 10 for additional\nanimations.\n(a) Bamboo (van Gogh) (b) Follower (van Gogh)\nFigure 10: Art word animations derived from the Sem-\nTypo optimization process. CLICK the image to PLAY\nANIMATION! Best viewed with Adobe Acrobat DC.\n6 Ethical Considerations\nPotential ethical concerns include perpetuating cul-\ntural stereotypes due to the use of certain imagery\nor symbols in the process of artistic transforma-\ntions, or introducing bias against under-represented\ncultures. Another issue could be the potential in-\nclusion of copyrighted graphics. Users need to pay\nattention to these issues to ensure responsible and\nrespectful use of the system.\n7 Conclusion\nThis paper presents WordArt Designer, a frame-\nwork that harnesses Large Language Models\n(LLM), such as GPT-3.5, to automatically gener-\nate multilingual artistic typography. This system\nuses an LLM engine to parse and translates user\ninput into directives, guiding three modules, each\naccountable for different aspects of the typographic\ndesign. The superior performance of WordArt De-\nsigner highlights the potential of AI to augment\nartistic typography. Future work aims to further ex-\nplore the possibilities of integrating this technology\ninto other aspects of design, such as graphics and\ninteractive media.\nAcknowledgments\nThe contributions of Zhi-Qi Cheng in this project\nwere supported by the Army Research Labora-\ntory (W911NF-17-5-0003), the Air Force Research\nLaboratory (FA8750-19-2-0200), the U.S. Depart-\nment of Commerce, National Institute of Stan-\ndards and Technology (60NANB17D156), the In-\ntelligence Advanced Research Projects Activity\n(D17PC00340), and the US Department of Trans-\nportation (69A3551747111). Intel and IBM Fellow-\nships also provided additional support for Zhi-Qi\nCheng‚Äôs research work.\n229\nReferences\nJennifer Amar, Olivier Droulers, and Patrick Legoh√©rel.\n2017. Typography in destination advertising: An ex-\nploratory study and research perspectives.Tourism Man-\nagement, 63:77‚Äì86.\nRohan Anil, Andrew M. Dai, Orhan Firat, et al.\n2023. Palm 2 technical report. arXiv preprint ,\nabs/2305.10403.\nYogesh Balaji, Seungjun Nah, Xun Huang, et al. 2022.\nediff-i: Text-to-image diffusion models with an ensem-\nble of expert denoisers. arXiv preprint, abs/2211.01324.\nTim Brooks, Aleksander Holynski, and Alexei A. Efros.\n2022. Instructpix2pix: Learning to follow image editing\ninstructions. arXiv preprint, abs/2211.09800.\nTom B. Brown, Benjamin Mann, Nick Ryder, et al. 2020.\nLanguage models are few-shot learners. In NeurIPS.\nZhi-Qi Cheng, Qi Dai, Siyao Li, et al. 2023.\nChartreader: A unified framework for chart derendering\nand comprehension without heuristic rules. In Pro-\nceedings of the IEEE/CVF international conference on\ncomputer vision.\nZhi-Qi Cheng, Yang Liu, Xiao Wu, and Xian-Sheng\nHua. 2016. Video ecommerce: Towards online video\nadvertising. In Proceedings of the 24th ACM interna-\ntional conference on Multimedia, pages 1365‚Äì1374.\nZhi-Qi Cheng, Xiao Wu, Yang Liu, and Xian-Sheng\nHua. 2017a. Video ecommerce++: Toward large scale\nonline video advertising. IEEE transactions on multi-\nmedia, 19(6):1170‚Äì1183.\nZhi-Qi Cheng, Xiao Wu, Yang Liu, and Xian-Sheng\nHua. 2017b. Video2shop: Exact matching clothes in\nvideos to online shopping images. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 4048‚Äì4056.\nMMPreTrain Contributors. 2023. Openmmlab‚Äôs pre-\ntraining toolbox and benchmark. https://github.\ncom/open-mmlab/mmpretrain.\nDavid Turner, Robert Wilhelm, and Werner Lemberg.\n1996. FreeType 2.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of deep\nbidirectional transformers for language understanding.\nIn NAACL-HLT, pages 4171‚Äì4186.\nPrafulla Dhariwal and Alexander Quinn Nichol. 2021.\nDiffusion models beat gans on image synthesis. In\nNeurIPS, pages 8780‚Äì8794.\nKevin Frans, Lisa B. Soros, and Olaf Witkowski. 2022.\nClipdraw: Exploring text-to-drawing synthesis through\nlanguage-image encoders. In NeurIPS.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, et al. 2023. An\nimage is worth one word: Personalizing text-to-image\ngeneration using textual inversion. In ICLR.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, pages 770‚Äì778.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. In NeurIPS.\nLianghua Huang, Di Chen, Yu Liu, et al. 2023.\nComposer: Creative and controllable image synthe-\nsis with composable conditions. arXiv preprint ,\nabs/2302.09778.\nSiyu Huang, Haoyi Xiong, Zhi-Qi Cheng, et al. 2020.\nGenerating person images with appearance-aware pose\nstylizer. In Proceedings of the 29th International Joint\nConference on Artificial Intelligence.\nShir Iluz, Yael Vinker, Amir Hertz, et al. 2023. Word-\nas-image for semantic typography. SIGGRAPH.\nDeepFloyd Lab. 2023. Deepfloyd if.\nhttps://github.com/deep-floyd/IF.\nTzu-Mao Li, Michal Luk√°c, Micha√´l Gharbi, and\nJonathan Ragan-Kelley. 2020. Differentiable vector\ngraphics rasterization for editing and learning. SIG-\nGRAPH, 39(6):193:1‚Äì193:15.\nJian Ma, Mingjun Zhao, Chen Chen, et al. 2023.\nGlyphdraw: Learning to draw chinese characters in\nimage synthesis models coherently. arXiv preprint,\nabs/2303.17870.\nChenlin Meng, Yutong He, Yang Song, et al. 2022.\nSdedit: Guided image synthesis and editing with\nstochastic differential equations. In ICLR.\nChong Mou, Xintao Wang, Liangbin Xie, et al. 2023.\nT2i-adapter: Learning adapters to dig out more control-\nlable ability for text-to-image diffusion models. arXiv\npreprint, abs/2302.08453.\nAlexander Quinn Nichol and Prafulla Dhariwal. 2021.\nImproved denoising diffusion probabilistic models. In\nICML, volume 139, pages 8162‚Äì8171.\nOpenAI. 2023. GPT-4 technical report. arXiv preprint,\nabs/2303.08774.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben\nMildenhall. 2023. Dreamfusion: Text-to-3d using 2d\ndiffusion. In The Eleventh International Conference on\nLearning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding by generative pre-training. OPENAI,\nhttps://openai.com/research/language-unsupervised.\nColin Raffel, Noam Shazeer, Adam Roberts, et al. 2020.\nExploring the limits of transfer learning with a unified\ntext-to-text transformer. JMLR, 21:140:1‚Äì140:67.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and\nYuxiong He. 2020. Zero: memory optimizations toward\ntraining trillion parameter models. In SC, page 20.\n230\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, et al.\n2022. Hierarchical text-conditional image generation\nwith CLIP latents. arXiv preprint, abs/2204.06125.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, et al.\n2021. Zero-shot text-to-image generation. In ICML,\nvolume 139, pages 8821‚Äì8831. PMLR.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\net al. 2022. High-resolution image synthesis with latent\ndiffusion models. In CVPR, pages 10684‚Äì10695.\nChitwan Saharia, William Chan, Saurabh Saxena, et al.\n2022. Photorealistic text-to-image diffusion models\nwith deep language understanding. In NeurIPS.\nYongliang Shen, Kaitao Song, Xu Tan, et al. 2023. Hug-\nginggpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint, abs/2303.17580.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, et al.\n2019. Megatron-lm: Training multi-billion parame-\nter language models using model parallelism. arXiv\npreprint, abs/1909.08053.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma,\net al. 2021. Score-based generative modeling through\nstochastic differential equations. In ICLR.\nGuang-Lu Sun, Zhi-Qi Cheng, Xiao Wu, and Qiang\nPeng. 2018. Personalized clothing recommendation\ncombining user social circle and fashion style consis-\ntency. Multimedia Tools and Applications, 77:17731‚Äì\n17754.\nMaham Tanveer, Yizhi Wang, Ali Mahdavi-Amiri, and\nHao Zhang. 2023. Ds-fusion: Artistic typography via\ndiscriminated and stylized diffusion. arXiv preprint,\nabs/2303.09604.\nSompatu Vungthong, Emilia Djonov, and Jane Torr.\n2017. Images as a resource for supporting vocabu-\nlary learning: A multimodal analysis of thai efl tablet\napps for primary school children. TESOL Quarterly,\n51(1):32‚Äì58.\nChenfei Wu, Shengming Yin, Weizhen Qi, et al. 2023.\nVisual chatgpt: Talking, drawing and editing with visual\nfoundation models. arXiv preprint, abs/2303.04671.\nYukang Yang, Dongnan Gui, Yuhui Yuan, et al. 2023.\nGlyphcontrol: Glyph conditional control for visual text\ngeneration. arXiv preprint, abs/2305.18259.\nLvmin Zhang and Maneesh Agrawala. 2023. Adding\nconditional control to text-to-image diffusion models.\narXiv preprint, abs/2302.05543.\nYabo Zhang, Yuxiang Wei, Dongsheng Jiang, et al. 2023.\nControlvideo: Training-free controllable text-to-video\ngeneration. arXiv preprint, abs/2305.13077.\nBo Zhao, Xiao Wu, Zhi-Qi Cheng, et al. 2018. Multi-\nview image generation from a single-view. In Proceed-\nings of the 26th ACM international conference on Mul-\ntimedia, pages 383‚Äì391.\n231\nA Prompt Examples\nPrompt for GPT: ‚ÄúNow you are a creative and active explainer that helps people to understand the\nabstract concept, and concrete the abstract concepts, tell them the representative object in the abstract\nconcept. All the question is in a standard format \"Please list the representative category or object\nname in/of <CONCEPT>, including in real-life, artist, and film works.\" And the output must be in a\nstrict JSON format: {\"Object/Category Name\": \"Name\", \"description\": \"less than five critical words to\ndescribe\", \"reason\": \"your detailed reason for the choice\"}. Example: \" Please list the representative\ncategory or object name in/of cat including in real-life, artist, and film works.\" The response should be\n{\"Object/Category Name\": \"Hellokitty\", \"description\": \"cute, happiness\" \"less than five critical words to\ndescribe\", \"reason\": \"famous for the cartoon\"}. ‚Äù\nStylization and texture examples as follow:\nQs: ‚ÄúPlease list the representative category or object name in/of spring, including in real-life, artist,\nand film works.‚Äù\nAs: ‚Äú{\"Object/Category Name\": \"Rainbow\", \"description\": \"colorful, natural\", \"reason\": \"Rainbows are\na natural phenomenon that occurs after rain showers during spring. They are often depicted in artwork\nand films as a symbol of hope, joy, and promise. Additionally, rainbows are often used in fashion and\ndesign to represent spring and its vibrant colors.\"}‚Äù\nQt: ‚ÄúPlease list the representative category or object name in/of food, including in real-life, artist,\nand film works.‚Äù\nAt: ‚Äú {\"Object/Category Name\": \"Pizza\", \"description\": \"delicious, versatile\", \"reason\": \"Pizza is a\npopular food that is loved by many people around the world. It is a versatile food that can be customized\nwith a variety of toppings to suit different tastes and preferences. Pizza is often featured in films, TV shows,\nand commercials, and it is a staple food in many countries, including Italy and the United States.\"}‚Äù\nThe ‚ÄúObject/Category Name‚Äù and the ‚Äúdescription‚Äù are utilized to build the prompt for the StyTypo and\nTexTypo modules, and the ‚Äúreason‚Äù information can be applied to analyze the quality of the prompt.\nB Addtional Results\nFigure 11: Diversity of results. The LLM engine generates the texture prompt that can be explained in various\nconcretion objects/concepts. The 1st and 2nd rows are related to the concept ‚Äújewelry‚Äù that is concrete to ‚Äúgold‚Äù or\n‚Äújade‚Äù, respectively. The 3rd row is the concept ‚Äúfood‚Äù that is concrete to bread. It is worth noticing that the texture\nrendering is ‚Äúsmart‚Äù and ‚Äúreasonable‚Äù which leads to creativity and artistry.\n232",
  "topic": "Typography",
  "concepts": [
    {
      "name": "Typography",
      "score": 0.7998734712600708
    },
    {
      "name": "Bin",
      "score": 0.6273231506347656
    },
    {
      "name": "Computer science",
      "score": 0.6186692714691162
    },
    {
      "name": "Natural language processing",
      "score": 0.5149270296096802
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4365934133529663
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.4202905297279358
    },
    {
      "name": "Engineering drawing",
      "score": 0.37846267223358154
    },
    {
      "name": "Multimedia",
      "score": 0.3506662845611572
    },
    {
      "name": "Engineering",
      "score": 0.23986750841140747
    },
    {
      "name": "Art",
      "score": 0.17350825667381287
    },
    {
      "name": "Visual arts",
      "score": 0.15206211805343628
    },
    {
      "name": "Programming language",
      "score": 0.09225475788116455
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1328775524",
      "name": "Zhejiang Sci-Tech University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165526535",
      "name": "Royal College of Art",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    }
  ]
}