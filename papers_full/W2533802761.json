{
  "title": "Mobile Manipulation, Tool Use, and Intuitive Interaction for Cognitive Service Robot Cosero",
  "url": "https://openalex.org/W2533802761",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2023848331",
      "name": "Jörg Stückler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095651902",
      "name": "Max Schwarz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1926609319",
      "name": "Sven Behnke",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6729470857",
    "https://openalex.org/W2110498236",
    "https://openalex.org/W2119605622",
    "https://openalex.org/W6683069301",
    "https://openalex.org/W6660574318",
    "https://openalex.org/W6682717198",
    "https://openalex.org/W6675022821",
    "https://openalex.org/W6685946324",
    "https://openalex.org/W6683411478",
    "https://openalex.org/W6626919383",
    "https://openalex.org/W6668178746",
    "https://openalex.org/W6683228371",
    "https://openalex.org/W1503536476",
    "https://openalex.org/W2130422193",
    "https://openalex.org/W1905403358",
    "https://openalex.org/W6657534107",
    "https://openalex.org/W2193945050",
    "https://openalex.org/W6676253777",
    "https://openalex.org/W2015312730",
    "https://openalex.org/W6713668444",
    "https://openalex.org/W2159711224",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W6661986843",
    "https://openalex.org/W2549556196",
    "https://openalex.org/W6673865435",
    "https://openalex.org/W6654441867",
    "https://openalex.org/W6680009237",
    "https://openalex.org/W2134236847",
    "https://openalex.org/W2023694121",
    "https://openalex.org/W6651760904",
    "https://openalex.org/W7055104640",
    "https://openalex.org/W6756486208",
    "https://openalex.org/W6688017944",
    "https://openalex.org/W1458871814",
    "https://openalex.org/W6635399168",
    "https://openalex.org/W6650058522",
    "https://openalex.org/W6686272321",
    "https://openalex.org/W2136354320",
    "https://openalex.org/W167985534",
    "https://openalex.org/W1972447158",
    "https://openalex.org/W192285000",
    "https://openalex.org/W2174829817",
    "https://openalex.org/W2085978998",
    "https://openalex.org/W1966783348",
    "https://openalex.org/W1965851203",
    "https://openalex.org/W1566423514",
    "https://openalex.org/W305287582",
    "https://openalex.org/W1876900421",
    "https://openalex.org/W6630113953",
    "https://openalex.org/W6684392942",
    "https://openalex.org/W2039918963",
    "https://openalex.org/W1524347791",
    "https://openalex.org/W2181570018",
    "https://openalex.org/W1030232638",
    "https://openalex.org/W2204626327",
    "https://openalex.org/W2099870233",
    "https://openalex.org/W1998051757",
    "https://openalex.org/W2156047221",
    "https://openalex.org/W2005775757",
    "https://openalex.org/W2071008727",
    "https://openalex.org/W2613433911",
    "https://openalex.org/W1593727536",
    "https://openalex.org/W2157805061",
    "https://openalex.org/W2135585491",
    "https://openalex.org/W1502517891",
    "https://openalex.org/W2407451516",
    "https://openalex.org/W2153000379",
    "https://openalex.org/W2107758079",
    "https://openalex.org/W2016505519",
    "https://openalex.org/W2093497901",
    "https://openalex.org/W2540104143",
    "https://openalex.org/W2164598857",
    "https://openalex.org/W2045616829",
    "https://openalex.org/W2183897815",
    "https://openalex.org/W2040438273",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2028874916"
  ],
  "abstract": "Cognitive service robots that shall assist persons in need of performing their activities of daily living have recently received much attention in robotics research. Such robots require a vast set of control and perception capabilities to provide useful assistance through mobile manipulation and human–robot interaction. In this article, we present hardware design, perception, and control methods for our cognitive service robot Cosero. We complement autonomous capabilities with handheld teleoperation interfaces on three levels of autonomy. The robot demonstrated various advanced skills, including the use of tools. With our robot, we participated in the annual international RoboCup@Home competitions, winning them three times in a row.",
  "full_text": "November 2016 | Volume 3 | Article 581\nTechnology RepoRT\npublished: 07 November 2016\ndoi: 10.3389/frobt.2016.00058\nFr\nontiers in Robotics and AI | www.fr\nontiersin.org\nEdited by: \nJochen J. Steil,  \nBielefeld University, Germany\nReviewed by: \nNikolaus Vahrenkamp,  \nKarlsruhe Institute of Technology, \nGermany  \nFeng Wu,  \nUniversity of Science and Technology \nof China, China\n*Correspondence:\nSven Behnke  \nbehnke@cs.uni-bonn.de\nSpecialty section: \nThis article was submitted to \nHumanoid Robotics,  \na section of the journal  \nFrontiers in Robotics and AI\nReceived: 29 May 2016\nAccepted: 20 September 2016\nPublished: 07 November 2016\nCitation: \nStückler J, Schwarz M and Behnke S \n(2016) Mobile Manipulation, Tool \nUse, and Intuitive Interaction for \nCognitive Service Robot Cosero.  \nFront. Robot. AI 3:58.  \ndoi: 10.3389/frobt.2016.00058\nMobile Manipulation, Tool Use, and \nIntuitive Interaction for cognitive \nService Robot cosero\nJörg Stückler, Max Schwarz and Sven Behnke*\nInstitute for Computer Science VI, Autonomous Intelligent Systems, University of Bonn, Bonn, Germany\nCognitive service robots that shall assist persons in need of performing their activities of \ndaily living have recently received much attention in robotics research. Such robots require \na vast set of control and perception capabilities to provide useful assistance through \nmobile manipulation and human–robot interaction. In this article, we present hardware \ndesign, perception, and control methods for our cognitive service robot Cosero. We \ncomplement autonomous capabilities with handheld teleoperation interfaces on three \nlevels of autonomy. The robot demonstrated various advanced skills, including the use \nof tools. With our robot, we participated in the annual international RoboCup@Home \ncompetitions, winning them three times in a row.\nKeywords: cognitive service robots, mobile manipulation, object perception, human–robot interaction, shared \nautonomy, tool use\n1. InTRoDUcTIon\nIn recent years, personal service robots that shall assist, e.g., handicapped or elderly persons in their \nactivities of daily living have attracted increasing attention in robotics research. The everyday tasks \nthat we perform, for instance, in our households, are highly challenging to achieve with a robotic sys-\ntem, because the environment is complex, dynamic, and structured for human needs. Autonomous \nservice robots require versatile mobile manipulation and human–robot interaction skills in order to \nreally become useful. For example, they should fetch objects, serve drinks and meals, and help with \ncleaning. Many capabilities that would be required for a truly useful household robot are still beyond \nthe state-of-the-art in autonomous service robotics. Complementing autonomous capabilities of the \nrobot with user interfaces for teleoperation enables the use of human cognitive abilities whenever \nautonomy reaches its limits and, thus, could bring such robots faster toward real-word applications.\nWe develop cognitive service robots since 2009, according to the requirements of the annual \ninternational RoboCup@Home competitions (Wisspeintner et al., 2009). These competitions bench-\nmark integrated robot systems in predefined test procedures and in open demonstrations in which \nteams can show the best of their research. Benchmarked skills comprise mobility in dynamic indoor \nenvironments, object retrieval and placement, person perception, complex speech understanding, \nand gesture recognition.\nIn previous work, we developed the communication robot Robotinho (Nieuwenhuisen and \nBehnke, 2013) and our first-generation domestic service robot Dynamaid (Stückler and Behnke, \n2011). In this article, we focus on the recent developments targeted at our second-generation cog -\nnitive service robot Cosero, shown in Figure  1. Our robot is mobile through an omnidirectional \nwheeled drive. It is equipped with an anthropomorphic upper body with 7 degree of freedom (DoF) \narms that have human-like reach and features a communication head with an RGB-D camera and \na directed microphone.\nFIgURe 1 | overview of the cognitive service robot Cosero. The right column lists main capabilities of the system and references their corresponding sections \nin this article.\n2\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nAn overview of Cosero’s capabilities is given in Figure 1. We \ndiscuss related work in the next section. Section 3 details Cosero’s \nmechatronic design. The overall software architecture, low-level \ncontrol for omnidirectional driving, and compliant manipulation \nare presented in Section 4. Section 5 covers mapping, localization, \nand navigation in dynamic indoor environments. The perception \nof objects and different manipulation skills, including tool use \nand skill transfer to novel instances of a known object category, \nare presented in Section 6. Different human–robot interaction \nmodules, including the recognition of gestures and the under -\nstanding of speech commands, are discussed in Section 7. We \nreport results obtained in the RoboCup@Home competitions \n2011–2014 in Section 8.\n2. RelATeD WoRK\nAn increasing number of research groups worldwide are working \non complex robots for domestic service applications. The Armar \nIII robot developed at KIT (Asfour et al., 2006) is an early promi-\nnent example. Further robots with a humanoid upper body design \nare the robots Twendy-One (Iwata and Sugano, 2009) developed \nat Waseda University, and the CIROS robots developed at KIST \n(Sang, 2011). The Personal Robot 2 [PR2 (Meeussen et al., 2010)], \ndeveloped by Willow Garage, was adopted by multiple research \ngroups and led to wide-spread use of the ROS middleware \n(Quigley et al., 2009). The robot has two 7 DOF compliant arms \non a liftable torso. Its omnidirectional drive has four individually \nsteerable wheels, similar to our robot. PR2 is equipped with a \nvariety of sensors such as a 2D laser scanner on the mobile base, a \n3D laser scanner in the neck, and a structured light stereo camera \nrig in the head. Bohren et al. (2011) demonstrated an application \nin which a PR2 fetched drinks from a refrigerator and delivered \nthem to human users. Both the drink order and the location at \nwhich it had to be delivered were specified by the user in a web \nform. Beetz et al. (2011) used a PR2 and a custom-built robot to \ncooperatively prepare pancakes. Nguyen et al. (2013) proposed \na user interface for flexible behavior generation within the ROS \nframework for the PR2. The interface allows for configuring \nbehavior in hierarchical state machines and for specifying goals \nusing interactive markers in a 3D visualization of the robot and \nits internal world representation.\nAn impressive piece of engineering is the robot Rollin’ Justin \n(Borst et al., 2009), developed at DLR, Germany. Justin is equipped \nFIgURe 2 | overview of the mechatronic design of cosero. Sensors are colored green, actuators blue, and other components red. USB connections are \nshown in red, analog audio links are shown in blue, and the low-level servo connections are shown in magenta.\n3\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nwith larger-than-human compliantly controlled light weight \narms and two four-finger hands. The upper body is supported by a \nfour-wheeled mobile platform with individually steerable wheels, \nsimilar to our design. The robot demonstrated several dexter -\nous manipulation skills such as pouring of a drink into a glass \nor cleaning windows. It also prepared coffee in a pad machine \n(Bäuml et al., 2011). For this, the robot grasped coffee pads and \ninserted them into the coffee machine, which involved opening \nand closing the pad drawer. For cleaning windows, Leidner et al. \n(2014) proposed an object-centered hybrid reasoning approach \nthat parametrizes tool-use skills for the situation at hand.\nThe robot Herb (Srinivasa et al., 2010), jointly developed by \nIntel Research Labs and Carnegie Mellon University, manipu-\nlated doors and cabinets. It is equipped with a single arm and \nuses a Segway platform as drive. In the healthcare domain, \nJain and Kemp (2010) present EL-E, a mobile manipulator that \nassists motor-impaired patients by performing pick and place \noperations to retrieve objects. The Care-O-Bot 3 (Parlitz et  al., \n2008) is a domestic service robot developed at Fraunhofer IPA. \nThe robot is equipped with four individually steerable wheels, a \n7 DOF lightweight manipulator, and a tray for interaction with \npersons. Objects are not directly passed from the robot to persons \nbut placed on the tray. This concept was recently abandoned in \nfavor of a two-armed, more anthropomorphic design in its suc-\ncessor Care-O-Bot 4 (Kittmann et al., 2015). The robot HoLLie \n(Hermann et al., 2013) developed at the FZI Karlsruhe also has \nan omnidirectional drive and is equipped with two 6 DoF arms \nwith anthropomorphic hands. A bendable trunk allows HoLLie \nto pick objects from the floor.\nWhile the above systems showed impressive demonstra-\ntions, the research groups frequently focus on particular \naspects and neglect others. Despite many efforts, so far, no \ndomestic service robot has been developed that fully addresses \nthe functionalities needed to be useful in everyday environ-\nments. To benchmark progress and to facilitate research and \ndevelopment, robot competitions gained popularity in recent \nyears (Behnke, 2006).\nFor service robots, the RoboCup Federations hold annual \ncompetitions in its @Home league (Iocchi et al., 2015). Systems \ncompeting in the most recent competition, which was held 2015 \nin Suzhou, China, are described, e.g., by Chen et al. (2015), Seib \net al. (2015), zu Borgsen et al. (2015), and Lunenburg et al. (2015). \nMost of these custom-designed robots consist of a wheeled mobile \nbase with laser and RGB-D sensors and a single manipulator arm.\nCompetitions in different domains include RoboCup \nHumanoid Soccer (Gerndt et  al., 2015), the DARPA Robotics \nChallenge (Guizzo and Ackerman, 2015), and the DLR SpaceBot \nCup ( Stückler et  al., 2016). Recently, Atlas – an impressive \nhydraulic humanoid robot developed by Boston Dynamics for \nthe DARPA Robotics Challenge – has demonstrated some house-\nhold chores, which were programed by Team IHMC (Ackerman, \n2016).\n3. MechATRonIc DeSIgn\nFor the requirements of domestic service applications, and in \nparticular, the tasks of the RoboCup@Home competitions, we \ndeveloped a robot system that balances the aspects of robust \nmobility, human-like manipulation, and intuitive human–robot \ninteraction. Extending the functionality of its predecessor \nDynamaid (Stückler and Behnke, 2011), we designed our cogni-\ntive service robot Cosero (see Figure 1) to cover a wide range of \ntasks in everyday indoor environments.\nFigure 2 gives an overview of the mechatronic components \nof Cosero and shows their connectivity. The robot is equipped \nwith an anthropomorphic torso and two 7 DoF arms that provide \nadult-like reach. Compared to Dynamaid, the arms are twice \nas strong and support a payload of 1.5  kg each. The grippers \nhave twice as many fingers. They consist of two pairs of Festo \nFinGripper fingers – made from lightweight, deformable plastics \nmaterial – on rotary joints. When a gripper is closed on an object, \nthe bionic fin ray structure adapts the finger shape to the object \nsurface. By this, the contact surface between fingers and object \nincreases significantly compared to a rigid mechanical structure. \nFIgURe 3 | overview of the software architecture of cosero.\n4\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nA thin layer of anti-skidding material on the fingers establishes \na robust grip on objects. Having two fingers on each side of the \ngripper supports grasps stable for torques in the direction of the \nfingers and for forces in the direction between opponent fingers.\nCosero’s torso can be twisted around the vertical axis to extend \nits work space. A linear actuator moves the whole upper body up \nand down, allowing the robot to grasp objects from a wide range \nof heights – even from the floor. Its anthropomorphic upper body \nis mounted on a base with narrow footprint and omnidirectional \ndriving capabilities. By this, the robot can maneuver through \nnarrow passages that are typically found in indoor environments, \nand it is not limited in its mobile manipulation capabilities by \nnon-holonomic constraints. Many parts of the upper body, e.g., \nshoulders and wrist, are covered by 3D-printed shells. Together \nwith human-like proportions and a friendly face, this contributes \nto the human-like appearance of our robot, which facilitates \nintuitive interaction of human users with the robot.\nFor perceiving its environment, we equipped the robot with \nmultimodal sensors. Four laser range scanners on the ground, on \ntop of the mobile base, and in the torso (rollable and pitchable) \nmeasure distances to objects, persons, or obstacles for navigation \npurposes. The head is mounted on a pan-tilt joint and features a \nMicrosoft Kinect RGB-D camera for object and person percep-\ntion in 3D and a directed microphone for speech recognition. A \ncamera in the torso provides a lateral view onto objects in typical \nmanipulation height.\nA high-performance Intel Core-i7 quad-core notebook that \nis located on the rear part of the base is the main computer of \nthe robot. Cosero is powered by a rechargeable 5-cell 12 Ah LiPo \nbattery.\n4. SoFTWARe AnD conTRol \nARchITecTURe\nCosero’s autonomous behavior is generated in the modular mul-\ntithreaded control framework ROS (Quigley et al., 2009). The key \nmotivation for using ROS is its large community, which continu-\nously develops software modules and widens the scope of ROS. \nIn this manner, ROS has become a standard for sharing research \nwork related to robotics. The software modules for perception \nand control are organized in four layers, as shown in Figure 3. \nOn the sensorimotor layer, data is acquired from the sensors \nand position targets are generated and sent to the actuators. The \naction and perception layer contains modules for person and \nobject perception, safe local navigation, localization, and map-\nping. These modules process sensory information to estimate the \nstate of the environment and generate reactive action. Modules \non the subtask layer coordinate sensorimotor skills to achieve \nhigher-level actions like mobile manipulation, navigation, and \nhuman–robot interaction. For example, the mobile manipula-\ntion module combines safe omnidirectional driving with object \ndetection, recognition, and pose estimation, and with motion \nprimitives for grasping, carrying, and placing objects. Finally, at \nthe task layer, the subtasks are further combined to solve complex \ntasks that require navigation, mobile manipulation, and human–\nrobot interaction.\nThis modular architecture reduces complexity by decom-\nposing domestic service tasks into less complex modules. One \norganizing principle used is the successive perceptual abstraction \nwhen going up the hierarchy. On the other hand, higher-layer \nmodules configure lower-layer modules to make abstract action \ndecisions more concrete. Lower-layer modules are executed more \nfrequently than modules on higher layers to generate real-time \ncontrol commands for the actuators.\n4.1. omnidirectional Driving\nAs its predecessor Dynamaid (Stückler and Behnke, 2011), \nCosero supports omnidirectional driving for flexible navigation \nin restricted spaces. The linear ()xy,  and angular θ velocities of the \nbase can be set independently and can be changed continuously. \nThis target velocity is mapped to steering angles and velocities of \nthe individual pairs of wheels, so that the wheels are always rolling \nFIgURe 4 | compliant arm control (Stückler and Behnke, 2012). (A) Activation matrix for compliant control in an example arm pose. The task-space \ndimensions correspond to forward/backward (x), lateral (y), vertical (z), and rotations around the x-axis (roll), y-axis (pitch), and z-axis (yaw); Examples for use of \ncompliance in physical human–robot interaction: (B) object hand over from robot to user signaled by the user pulling on the object. (c) Cosero is guided by a \nperson. (D) Cosero follows the motion of a person to cooperatively carry a table.\n5\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\ninto the local driving direction, tangential to the instantaneous \ncenter of rotation of the base.\nCollision detection and avoidance using laser range sensors in \nthe robot enables safe omnidirectional driving. Obstacle avoid-\nance in 3D is performed by continuously tilting the laser scanner \nin the robot’s chest. During safe collision-aware driving, linear \nand rotational velocities are limited when necessary to avoid the \nclosest obstacles to the robot.\n4.2. compliant Arm Motion control\nFor the anthropomorphic arms, we implemented differential \ninverse kinematics with redundancy resolution and compliant \ncontrol in task space. We limit the torque of the joints according \nto how much they contribute to the achievement of the motion \nin task space. Our approach not only allows adjusting compliance \nin the null-space of the motion, but also in the individual dimen-\nsions in task space.\nOur method determines a compliance c  ∈  [0,1]\nn in linear \ndependency of the deviation of the actual state from the target \nstate in task space, such that the compliance is one for small \ndisplacements and zero for large ones. For each task dimen-\nsion, the motion can be set compliant in the positive and the \nnegative direction separately, allowing, e.g., for being compliant \nin upward direction, but stiff downwards. If a task dimension is \nnot set compliant, we wish to use high holding torques to position \ncontrol this dimension. If a task dimension is set compliant, the \nmaximal holding torque interpolates between a minimal value for \nfull compliance and a maximum torque for zero compliance. We \nthen measure the responsibility of each joint for the task-space \nmotion through the inverse of the forward kinematics Jacobian \nand determine a joint activation matrix from it. Figure 4 shows \nan example matrix. The torque limits are distributed to the joints \naccording to this activation matrix. Further details on our method \ncan be found in Stückler and Behnke (2012).\nWe applied compliant control to the opening and closing of \ndoors that can be moved without the handling of an unlocking \nmechanism. Refrigerators or cabinets are commonly equipped \nwith magnetically locked doors that can be pulled open without \nspecial manipulation of the handle. To open a door, our robot \ndrives in front of it, detects the door handle with the torso laser, \napproaches the handle, and grasps it. The drive moves backward \nwhile the gripper moves to a position on the side of the robot \nin which the opening angle of the door is sufficiently large to \napproach the open fridge or cabinet. The gripper follows the \nFIgURe 5 | 3D Surfel grid maps for navigation (Kläss et al., 2012). (A) Panorama image of an office. (B) 3D surfel map learned with our approach (surfel \norientation coded by color). (c) Navigation map derived from 3D surfel map.\n6\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nmotion of the door handle through compliance in the lateral and \nthe yaw directions. The robot moves backward until the gripper \nreaches its target position. For closing a door, the robot has to \napproach the open door leaf, grasp the handle, and move forward \nwhile it holds the handle at its initial grasping pose relative to \nthe robot. Since the door motion is constrained by the hinge, \nthe gripper will be pulled sideways. The drive corrects for this \nmotion to keep the handle at its initial pose relative to the robot \nand thus follows the circular trajectory implicitly. The closing of \nthe door can be detected when the arm is pushed back toward \nthe robot.\n5. nAVIgATIon\nOur robot navigates in indoor environments on horizontal \nsurfaces. The 2D laser scanner on the mobile base is used as the \nmain sensor for navigation. 2D occupancy maps of the environ-\nment are acquired using simultaneous localization and mapping \n[gMapping (Grisetti et  al., 2007)]. The robot localizes in these \nmaps using Monte Carlo localization (Fox, 2001) and navigates \nto goal poses by planning obstacle-free paths in the environment \nmap, extracting waypoints, and following them. Obstacle-free \nlocal driving commands are derived from paths that are planned \ntoward the next waypoint in local collision maps acquired with \nthe robot’s 3D laser scanners.\nSolely relying on a 2D map for localization and path planning, \nhowever, has several limitations. One problem of such 2D maps \noccurs in path planning, if non-traversable obstacles cannot be \nperceived on the height of a horizontal laser scanner. Localization \nwith 2D lasers imposes further restrictions if dynamic objects \noccur, or the environment changes in the scan plane of the laser. \nThen, localization may fail since large parts of the measurements \nare not explained by the map. To address such shortcomings, \nwe realized localization and navigation in 3D maps of the \nenvironment.\nWe choose to represent the map in a 3D surfel grid, which \nthe robot acquires from multiple 3D scans of the environment. \nFigure  5 demonstrates an example map generated with our \napproach. From the 3D maps, we extract 2D navigation maps by \nexploring the traversability of surfels. We check height changes \nbetween surfels, which exceed a threshold of few centimeters, \ndefining traversability of the ground, and for obstacles within the \nrobot height.\nFor localization in 3D surfel grid maps, we developed an \nefficient Monte Carlo method that can incorporate full 3D scans \nand 2D scan lines. When used with 3D scans, we extract surfels \nfrom the scans and evaluate the observation likelihood. From 2D \nscans, we extract line segments and align them with surfels in the \nmap. Localization in 3D maps is specifically useful in crowded \nenvironments. The robot can then focus on measurements above \nthe height of people to localize at the static parts of the environ-\nment. More general, by representing planar surface elements \nin the map, we can also rely for localization mainly on planar \nstructures, as they more likely occur in static environment parts. \nFor further details, please refer to Kläss et al. (2012).\n6. MAnIpUlATIon\nService tasks often involve the manipulation of objects and the \nuse of tools. In this section, we present perceptual and action \nmodules that we realized for Cosero. Several of these modules \nhave been combined with the control strategies in the previous \nsections to implemented tool-use skills and other domestic \nservice demonstrations.\n6.1. object perception\nWhen attempting manipulation, our robot captures the scene \ngeometry and appearance with its RGB-D camera. In many situ-\nations, objects are located well separated on horizontal support \nsurfaces, such as tables, shelves, or the floor. To ensure good \nvisibility, the camera is placed at an appropriate height above and \ndistance from the surface, pointing downwards with an angle of \napproximately 45°. To this end, the robot aligns itself with tables \nor shelves using the rollable laser scanner in its hip in its vertical \nscan plane position. Figure 6A shows a resulting screen capture.\n6.1.1. Object Segmentation\nAn initial step for the perception of objects in these simple scenes \nis to segment the captured RGB-D images into support planes \nand objects on these surfaces. Our plane segmentation algorithm \nrapidly estimates normals from the depth images of the RGB-D \ncamera and fits a horizontal plane through the points with roughly \nvertical normals by RANSAC (Stückler et al., 2013b). The points \nabove the detected support plane are grouped to object candidates \nbased on Euclidean distance. All points within a range threshold \nform a segment that is analyzed separately. In  Figure  6A, the \nFIgURe 6 | object perception (Stückler et al., 2013a). (A) Cosero capturing a tabletop scene with its RGB-D camera. The visible volume is indicated by the \ngreen lines. Three objects are detected on the table. Each object is represented by an ellipse fitted to its points, indicated in red. (B) RGB image of the tabletop with \nrecognized objects. (c) We recognize objects in RGB images and find location and size estimates. (D) Matched features vote for position in a 2D Hough space. (e) \nFrom the features (green dots) that consistently vote at a 2D location, we find a robust average of relative locations (yellow dots). (F) We also estimate principal \ndirections (yellow lines).\n7\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\ndetected segments are visualized using an ellipse fitted to their \n3D points.\n6.1.2. Object Recognition\nCosero recognizes objects by matching SURF interest points \n(Bay et  al., 2008) in RGB images to an object model database \n(Stückler et al., 2013a). An example recognition result is shown \nin Figure  6B. We store in an object model the SURF feature \ndescriptors along with their scale, orientation, relative location \nof the object center, and orientation and length of principal axes. \nThe bottom row of Figure 6 illustrates the recognition process. \nWe efficiently match features between the current image and the \nobject database according to the descriptor using kd-trees. Each \nmatched feature then casts a vote to the relative location, orienta-\ntion, and size of the object. We consider the relation between the \nfeature scales and orientation of the features to achieve scale- and \nrotation-invariant voting. Hence, our approach also considers \ngeometric consistency between features. When unlabeled object \ndetections are available through planar RGB-D segmentation \n(see Figure  6C), we project the detections into the image and \ndetermine the identity of the object in these regions of interest.\n6.1.3. Object Categorization and Pose Estimation\nFor categorizing objects, recognizing known instances, and \nestimating object pose, we developed an approach that analyzes \nan object, which has been isolated using tabletop segmentation. \nThe RGB-D region of interest is preprocessed by fading out the \nbackground of the RGB image (see Figure 7A, top left). The depth \nmeasurements are converted to an RGB image as well by render-\ning a view from a canonical elevation and encoding distance from \nthe estimated object vertical axis by color, as shown in Figure 7A \nbottom left. Both RGB images are presented to a convolutional \nneural network, which has been pretrained on the ImageNet data \nset for categorization of natural images. This produces semantic \nhigher-layer features, which are concatenated and used to recog-\nnize object category, object instance, and to estimate the azimuth \nviewing angle onto the object using support vector machines and \nsupport vector regression, respectively. This transfer learning \napproach has been evaluated on the popular Washington RGB-D \nObject data set and improved the state-of-the-art (Schwarz et al., \n2015b).\n6.1.4. Primitive-based Object Detection\nObjects are not always located on horizontal support surfaces. \nFor a bin picking demonstration, we developed an approach to \ndetect known objects, which are on top of a pile, in an arbitrary \npose in transport boxes. The objects are described by a graph of \nshape primitives. The bottom row of Figure 7 illustrates the object \ndetection process. First, individual primitives, like cylinders of \nappropriate diameter are detected using RANSAC. The relations \nFIgURe 7 | object perception. (A) Object categorization, instance recognition, and pose estimation based on features extracted by a pretrained convolutional \nneural network (Schwarz et al., 2015b). Depth is converted to a color image by rendering a canonical view and encoding distance from the object vertical axis; \nobject detection based on geometric primitives. (B) Point cloud captured by Cosero’s Kinect camera. (c) Detected cylinders. (D) Detected objects (Nieuwenhuisen \net al., 2013).\n8\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nbetween these are checked. If they match, the graph describing \nthe object model, an object instance is instantiated, verified, and \nregistered to the supporting 3D points. This yields object pose \nestimates in 6D. Further details are described in Nieuwenhuisen \net al. (2013), who demonstrated mobile bin picking with Cosero. \nThe method has been extended to the detection of object models \nthat combine 2D and 3D shape primitives by Berner et al. (2013).\n6.1.5. Object Tracking\nCosero tracks the pose of known objects using models repre-\nsented as multiresolution surfel maps [MRSMaps (Stückler and \nBehnke, 2014c)], which we learn from moving an RGB-D sensor \naround the object and performing SLAM. Our method estimates \nthe camera poses by efficiently registering RGB-D key frames. \nAfter loop closing and globally minimizing the registration error, \nthe RGB-D measurements are represented in a multiresolution \nsurfel grid, stored as an octree. Each volume element represents \nthe local shape of its points as well as their color distribution by \na Gaussian. Our MRSMaps also come with an efficient RGB-D \nregistration method, which we use for tracking the pose of objects \nin RGB-D images. The object pose can be initialized using our pla-\nnar segmentation approach. Figures 8A,B illustrates the tracking \nwith an example. To handle difficult situations, like occlusions, \nMcElhone et al. (2013) extended this approach to joint detection \nand tracking of objects modeled as MRSMaps using a particle \nfilter (see Figure 8C).\n6.1.6. Non-rigid Object Registration\nTo be able to manipulate not only known objects, but also \nobjects of the same category that differ in shape and appear -\nance, we extended the coherent point drift [CPD (Myronenko \nand Song, 2010)] method to efficiently perform deform-\nable registration between dense RGB-D point clouds (see \nFigure 8D). Instead of processing the dense point clouds of \nthe RGB-D images directly with CPD, we utilize MRSMaps to \nperform deformable registration on a compressed measure-\nment representation. The method recovers a smooth displace-\nment field, which maps the surface points between both point \nclouds. It can be used to establish shape correspondences \nbetween a partial view on an object in a current image and \na MRSMap object model. From the displacement field, the \nlocal frame transformation (i.e., 6D rotation and translation) \nat a point on the deformed surface can be estimated. By this, \nwe can determine how poses such as grasps or tool end effec-\ntors change by the deformation between objects (Figure 8E). \nFurther details on our deformable registration method can be \nfound in Stückler and Behnke (2014b).\n6.1.7. Object Localization using Bluetooth \nLow Energy Tags\nAs an alternative to tedious visual search for localizing objects, \nwe developed a method for localizing objects equipped with \nlow-cost Bluetooth low energy tags. Figure 9A shows examples. \nFIgURe 8 | object tracking using registration of object models. (A) Cosero approaching a watering can. (B) We train multiview 3D models of objects using \nmultiresolution surfel maps. The model is shown in green in the upper part. The model is registered with the current RGB-D frame to estimate its relative pose T, \nwhich is used to approach and grasp the watering can (Stückler et al., 2013a). (c) Joint object detection and tracking using a particle filter, despite occlusion \n(McElhone et al., 2013). Object manipulation skill transfer (Stückler and Behnke, 2014b): (D) an object manipulation skill is described by grasp poses and motions of \nthe tool tip relative to the affected object. (e) Once these poses are known for a new instance of the tool, the skill can be transferred.\nFIgURe 9 | Bluetooth low energy tag localization (Schwarz et al., 2015a). (A) Used Bluetooth tags (Estimote and StickNFind). (B) Signal strength-based tag \nlocalization by lateration. (c) Localization experiment with multiple tags.\n9\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nOther Bluetooth devices, such as smart phones and tablets can \nalso be localized. Our method requires the instrumentation of the \nenvironment with static Bluetooth receivers. The receivers report \nan RSSI (Received Signal Strength Indication) value, which \ndecreases with the distance of the tag from the receiver.\nFor position estimation, an over-constrained lateration \nproblem needs to be solved (see Figure 9B). The receivers report \nrelative distances to the tag. We simultaneously optimize for the \ntag position and a common scaling factor for signal strength. \nAs shown in Figure 9C, the resulting position estimates are still \nnoisy, so we smooth them with a windowed mean over 30 s. Such \ncoarse position estimates hint to typical placement locations in \nthe environment, from which our robot can retrieve the objects. \nFurther details of the method are reported by Schwarz et  al. \n(2015a).\n6.2. object grasping and placement\nGrasping objects from support surfaces is a fundamental capabil-\nity. For objects segmented above horizontal surfaces as described \nin Section 6.1.1, we developed an efficient approach that is illus-\ntrated in Figure  10. We consider two kinds of grasps on these \nobjects: top grasps that approach low objects from above and side \ngrasps that are suitable for vertically elongated objects such as \nbottles or cans. We plan grasps by first computing grasp candi-\ndates on the raw object point cloud as perceived with the RGB-D \ncamera. Our approach extracts the object principle axes in the \nhorizontal plane and its height. We sample pre-grasp postures for \ntop and side grasps and evaluate the grasps for feasibility under \nkinematic and collision constraints. The remaining grasps are \nranked according to efficiency and robustness criteria. The best \ngrasp is selected and finally executed with a parametrized motion \nFIgURe 10 | grasp and motion planning. (A) Object shape properties. The arrows mark the principal axes of the object. (B) We rank feasible, collision-free \ngrasps (red, size proportional to score), and select the most appropriate one (large, RGB-coded) (Stückler et al., 2013b). (c) Example side grasp. (D) Example top \ngrasp; motion planning for bin picking (Nieuwenhuisen et al., 2013): (e) Grasps are sampled on shape primitives and checked for collision-free approach. (F) The \nestimated object pose is used to filter out grasps that are not reachable or would lead to collisions. (g) Arm motion is planned for multiple segments using an \nobject-centered local multiresolution height map (reaching trajectory in red, pre-grasp pose larger coordinate frame). (h) Bin picking demonstration at RoboCup \n2012 in Mexico City (Stückler et al., 2014).\n10\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nprimitive. For collision detection, we take a conservative but \nefficient approach that checks simplified geometric constraints. \nFurther details are provided by Stückler et al. (2013b).\nFor 3D modeled objects, which are tracked using the method \ndescribed in Section 6.1.5, we define reference poses for grasping \nthem with one or two hands for their task-specific usage (see \nFigure  8D, bottom). The estimated 6D object pose is used to \ntransform the reference poses to the robot frame, parameterizing \nmotion primitives for task-specific object manipulation like \nwatering plants, pushing chairs, turning on devices, etc.\nOur robot also supports the placement of objects on planar \nsurfaces and the throwing of objects into trash bins.\n6.3. Bin picking\nObjects are not always placed well-separated on horizontal \nsupport surfaces but also come densely packed in transport \ncontainers. To show the utility of our robot in such scenarios, \nwe developed a mobile bin picking demonstration. Cosero \nnavigates to an allocentric pose in front of the bin. It aligns \nprecisely with the bin by perceiving it using its hip laser scanner \nin horizontal pose.\nObjects in the bin are detected using the shape primitive-\nbased method described in Section 6.1.4. We plan grasps in an \nefficient multistage process that successively prunes infeasible \ngrasps using tests with increasing complexity. In an initial offline \nstage, we find collision-free grasps on the object, irrespective of \nobject pose and not considering its scene context (Figure 10E). \nWe sample grasp poses on the shape primitives and retain the \nones, which allow for a collision-free gripper motion from pre-\ngrasp to grasp pose.\nDuring online planning, we transform the remaining grasp \nposes using the estimated object pose and check that a collision-\nfree solution of the inverse kinematics in the current situation \nexists (Figure 10F). We allow collisions of the fingers with other \nparts in the transport box in the direct vicinity of the object to \ngrasp, because the shape of the fingers allows for pushing them \ninto narrow gaps between objects. The feasible grasps are ranked \naccording to a score that incorporates efficiency and stability \ncriteria.\nThe final step is to identify the best-ranked grasp that is \nreachable from the current posture of the robot arm. To this \nend, we successively plan reaching motions for the found grasps \n(Figure  10G). We test the grasps in descending order of their \nscore. For motion planning, we employ LBKPIECE (Sucan \nand Kavraki, 2008). We split the reaching motion into multiple \nsegments. This allows for a quick evaluation if a valid reaching \nmotion can be found by planning in the descending order of the \nprobability that planning for a segment will fail. Planning in the \nvicinity of the object needs a more exact environment representa-\ntion as planning farther away from it. This is accomplished by \ncentering a local multiresolution height map at the object to grasp, \nwhich is used for collision checking. This approach also leads to \nlarger safety margins with increasing distance to the object. The \nobject removal motion is planned with the robot extended by the \ngrasped object. Further details are provided in Nieuwenhuisen \net al. (2013).\n6.4. Tool Use\nService tasks often involve the use of specialized tools. For a \nfirm grip on such tools, we designed 3D-printed tool adapters \nFIgURe 11 | Tool use. (A) Grasping sausages with a pair of tongs. Cosero perceives position and orientation of the sausages in RGB-D images (Stückler and \nBehnke, 2014a). (B) Bottle opening. (c) Plant watering skill transfer to unknown watering can (Stückler and Behnke, 2014b; cf. to Figure 8e).\n11\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nmatching the four-finger grippers of Cosero. When the gripper \ncloses on the adapter, the fingers bend around the shape of the \nadapter and establish form closure. The ridge on the center of \nthe adapter fits into the space between the fingers. It fixates the \nadapter for exerting torques in pitch direction. For some tools \nsuch as pairs of tongs, the opening of the gripper is also used to \noperate the tool. To create form closure with the fingers at various \nopening angles, the adapters have flat springs for each finger.\n6.4.1. Using a Pair of Tongs\nWhen grasping sausages from a barbecue, the robot should not \ndirectly grasp with its grippers. Instead, it uses a pair of tongs \nto keep the food clean and to keep the grippers clear of the hot \nbarbecue (see Figure 11A).\nWe segment the sausages from a plate or from the barbecue \nusing plane segmentation (Section 6.1.1) and adapt the grasping \nmotion to the position and orientation of the sausages. We exploit \nthat the height of the barbecue or the plates on the plane is known \nand discard points of these support objects. The remaining points \nare clustered by Euclidean distance. We then estimate the prin-\ncipal axes of the segments and compare length (first principal \naxis) and width (second principal axis) with the expected size \nof the sausages. If these measures are within nominal ranges, the \nsegment is classified as a sausage.\nWe extend the robot kinematic chain with the grasped tool. A \nparametrized motion primitive uses position and orientation of \nthe closest sausage to pick it up with the tongs. The robot holds \nthe tool above the objects on the table at all times during the \ndemonstrations, so that collisions with these objects are avoided.\n6.4.2. Bottle Opening\nOpening a capped bottle with a bottle-opening tool is challeng-\ning, since the tool must be accurately placed onto the cap (see \nFigure 11B). The robot first grasps the bottle with its left hand \nand the tool with its right hand. It holds both objects close to \neach other above a horizontal surface. In order to stabilize the \nbottle, it rests it on the horizontal surface, still holding it in the \nhand. To execute the opening motion precisely, the robot must \ncompensate for several sources of inaccuracy. First, an exact \ncalibration between the robot sensors and end effector may \nnot be known. Also, the pose of the tool in the gripper or the \nmanipulated object cannot be assumed to be known precisely. \nWe therefore implemented perception of the tips of the tool and \nthe manipulated object using the head-mounted RGB-D camera. \nDuring manipulation, our robot looks at the tool and bottle, \nsegments the objects from the surrounding using our efficient \nsegmentation method (see Section 6.1.1), and detects the endings \nof the objects in the segments.\nWe detect the tip of the opening tool in-hand by segment-\ning points in the depth image from the planar background. We \nselect the segment closest to the position of the robot gripper and \nsearch for the farthest position from the gripper along its forward \ndirection. The cap of the bottle in the other gripper is found in a \nsimilar way: Within the segment closest to the gripper, we search \nfor the highest point. Since we know the size of the opening tool \nand the bottle, we can verify the found positions by comparing \nthem to nominal positions. The bottle opening motion primitive \nis parameterized using the perceived cap and tool tip positions. \nThe robot verifies the success of the bottle-opening through the \nlowering of the estimated bottle top position.\n6.4.3. Watering Plants and Object Shape-Based \nSkill Transfer\nFor watering a plant with a watering can, our robot uses both arms \n(see Figure 11C). For grasping a previously known watering can, \nthe robot approaches the can using 6D object tracking (Section \n6.1.5; Figures 8A,B) and grasps the can at predefined reference \nposes. It navigates to an allocentric pose in front of the plant and \npositions itself relative to the plant pot that is perceived in its \nhorizontal laser scanner. Water is poured into the pot by moving \nthe can spout in a predetermined way through synchronized \nmotion of both arms.\nPreprograming such a tool-use skill for every shape variation of \nwatering cans is not desirable. We use the deformable registration \nmethod described in Section 6.1.6 to transfer the skill from the \nknown can instance to a novel can. The skill of using the water -\ning can is described by grasp poses relative to the object surface \nand motion trajectories of the can spout (see Figures 8D,E). To \ntransfer this skill to a new variant of cans, we segment the new \ncan from its support plane and establish shape correspondences \nto the object model of the known can. We estimate local frame \ntransformations of the grasp poses and the tool end effector of \nthe known can toward the observed can. The robot executes the \ntransformed grasps to pick up the new can. For watering a plant, \nthe robot moves the can end-effector frame relative to the plant in \nthe same way as for the modeled can. This constrains the motion \nof the arms to keep the relative position of the transformed grasp \nposes to the transformed tool end effector pose. Further details \nof our adaptive tool-use methods can be found in Stückler and \nBehnke (2014a).\nFIgURe 12 | person perception. (A) Persons are detected as legs (cyan spheres) and torsos (magenta spheres) in two horizontal laser range scans (cyan and \nmagenta dots, Droeschel et al., 2011). Detections are fused in a multi-hypothesis tracker (red and cyan boxes). Faces are detected with a camera mounted on a \npan-tilt unit. We validate tracks as persons (cyan box) when they are closest to the robot and match the line-of-sight toward a face (red arrow). (B) Enrollment of a \nnew face. (c) Gesture recognition (Droeschel et al., 2011). Faces are detected in the amplitude image. 3D point cloud with the resulting head, upper body, and arm \nsegments (green, red, and yellow) and the locations of the hand, elbow, and shoulder (green, light blue, and blue spheres).\n12\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\n7. InTUITIVe InTeRAcTIon\nIn addition to the mobile manipulation capabilities described \nso far, intuitive user interfaces are key for making service robots \nuseful. Speech, gestures, and body language are key modalities \nfor human–human interaction. Employing them for face-to-face \nhuman–robot interaction is a natural way to configure the high-\nlevel autonomous behaviors of our robot when the user is in its \nvicinity. In addition, we also developed a handheld teleoperation \ninterface, which is particularly useful for immobile users. The \nhandheld interface gives the user the ability to control the robot \non three levels of robot autonomy. Besides direct control on the \nbody level to move the base and the arms or to adjust the gaze \ndirection, this interface also allows for executing navigation or \nmanipulation skills, and for sequencing skills in prolonged tasks.\n7.1. perception of persons\nA key prerequisite for a robot that engages in human–robot \ninteraction is the perception of persons in its surrounding. This \nincludes the detection and tracking of people, the identification \nof persons, and the interpretation of their gestures.\n7.1.1. Person Detection and Tracking\nWe combine complementary information from laser range scan-\nners and camera images to continuously detect and keep track \nof humans. In horizontal laser scans, the measurable features of \npersons like the shape of legs are not very distinctive, such that \nparts of the environment may cause false detections. However, \nlaser scanners can be used to detect person candidates, to localize \nthem, and to keep track of them in a wide field-of-view at high \nrates. In camera images, we verify that a track belongs to a person \nby detecting more distinctive human features: faces and upper \nbodies.\nOne horizontal laser scanner mounted 30 cm above the ground \ndetects legs of people. We additionally detect torsos of people with \nthe laser scanner in the robot’s torso in horizontal scan position. \nIn a multi-hypothesis tracker, we fuse both kinds of detections to \ntracks (see Figure 12A). Position and velocity of each track are \nestimated by a Kalman filter (KF). In the KF prediction step, we \nuse odometry information to compensate for the motion of the \nrobot. After data association, the tracks are corrected with the \nobservations of legs and torsos. We use the Hungarian method \n(Kuhn, 1955) to associate each torso detection in a scan uniquely \nwith existing hypotheses. In contrast, as both legs of a person \nmay be detected in a scan, we allow multiple leg detections to be \nassigned to a hypothesis. Only unassociated torso detections are \nused to initialize new hypotheses. A new hypothesis is considered \na person candidate until it is verified as a person through vision. \nFor this, our robot actively looks at a hypothesis using its pan-tilt \nKinect camera and employs the face detector of Viola and Jones \n(2001) and HoG upper body detection Dalal and Triggs (2005). \nSpurious tracks with low detection rates are removed.\n7.1.2. Person Identification\nTo determine the identity of the tracked persons, we implemented \na face enrollment and identification system using the VeriLook \nSDK.\n1 In the enrollment phase, our robot approaches detected \npersons, looks at their face with its camera, and asks them to look \ninto the camera (Figure 12B). Face detection is done using the \nViola and Jones’ (2001) algorithm. The cut-out faces are passed \nto VeriLook, which extracts face descriptors that are stored \nin a repository. If the robot wants to identify a person later, it \napproaches the person, looks at their face, and compares the new \ndescriptor to the stored ones.\n7.1.3. Gesture Recognition\nGestures are a natural way of communication in human–robot \ninteraction. A pointing gesture, for example, can be used to draw \nthe robot’s attention to a certain object or location in the envi-\nronment. We implemented the recognition of pointing gestures, \nthe showing of objects, and stop gestures. The primary sensor in \nour system for perceiving a gesture is the depth camera mounted \non the robot’s pan-tilt unit. Starting from faces detected in the \namplitude image, we segment the person, its trunk, and arms in \nthe depth image and determine the position of the head, hand, \nshoulder, and elbow. This is illustrated in Figure 12C.\nWe detect pointing gestures when the arm is extended, and \nthe hand is held at a fixed location for a short time interval. To \n1 http://www.neurotechnology.com/verilook.html. \n13\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\ncompensate for the narrow field-of-view of the ToF camera, the \nrobot adjusts its gaze to keep the hand as well as the head of the \nperson in the image. After a pointing gesture has been detected, \nwe infer its intended pointing target. Especially for distant targets, \nthe line through eyes and hand yields a good approximation to the \nline toward the target. Showing of objects and stop gestures are \ndetected when the arm of the human extends toward the robot. \nFurther details of the method can be found in Droeschel et al. \n(2011). Droeschel and Behnke (2011) also developed an adaptive \nmethod for tracking an articulated 3D person model from the \nperspective of our robot.\n7.2. Multimodal Dialog System\nThe perception of persons is the basis for intuitive multimodal \ninteraction with users by means of speech, gestures, and body \nlanguage. To interact with a person, our robot approaches a per-\nson track and points its RGB-D camera and directed microphone \ntoward the user’s head. This not only allows for the capturing of \nhigh-resolution images and depth measurements of the user’s \nupper body and for recording of user utterances with a good \nsignal-to-noise ratio but also signals to the user that our robot is \nin a state where it is ready to receive user input.\n7.2.1. Speech Recognition and Synthesis\nFor speech recognition and synthesis, we use the Loquendo SDK.2 \nIts speech synthesis supports colorful intonation and sounds \nnatural. The robot generates speech depending on the task state, \ne.g., to inform the user about its current intent or to request user \ninput. Loquendo’s speech recognition is speaker independent and \nis based on predefined grammars that we attribute with semantic \ntags for natural language understanding. Again, the task state \ndetermines, which user utterances are understood.\n7.2.2. Interpretation of Dialogs\nOn the task level, our robot supports spoken dialogs for specify-\ning complex commands that sequence multiple skills. The ability \nto understand complex speech commands, to execute them, to \ndetect failures, and to plan alternative actions in case of failures \nis assessed in the General Purpose Service Robot test in the \nRoboCup@Home competition. We describe the capabilities of \nthe robot by a set of primitive skills that can be parameterized \nby specifying an object and/or a location. For instance, the skill \nnavigate_to_location depends on a goal location while \nfetch_object_from_location requires a target object \nand an object location.\nThe robot knows a set of specific objects that are referenced by \nthe object name in spoken commands. These specific objects are \nincluded in the visual object recognition database (Section 6.1.2). \nIt is also possible to define an unspecific object using labels such \nas “unknown, ” “some object, ” or the label of an object category \n(e.g., “tool”). If multiple skills with object references are chained, \nthe reflexive pronoun “it” refers to the last object that occurred \nin the task command. Hence, objects are referred to by labels and \nmay have the additional attributes of being specific, unspecific, \nand reflexive.\n2 http://www.nuance.com/support/loquendo. \nPersons are handled in a similar way, but the notion of a person \ncategory is not included in our system. Our robots can enroll new \npersons and link their identity with their face appearance in the \ndatabase of known persons (Section 7.1.2).\nSpecific locations, location categories, unspecific locations, or \nlocation-specific adjectives (like “back”) can be indicated by the \nuser as well. We provide sets of navigation goal poses for specific \nlocations as well as location categories. Different lists of poses are \nused for the purposes of object search, exploration for persons, or \nsimple presence at a spot.\nWe utilize semantic tags in Loquendo’s grammar specification \nto implement action, object, and location semantics in speech \nrecognition. We appropriately designed the grammar so that \nrecognition provides its semantic parse tree as a list of actions \nwith attributed objects and locations.\nAlternatively, for specific tasks, it is also possible to reference \nobjects or locations be pointing gestures, and to recognize objects \nthat the user shows the robot by holding them toward the robot.\nBehavior control interprets the recognized semantic parse tree \nand sequences actions in a finite state machine. The robot executes \nthis state machine and reports progress through speech synthesis. \nIn case of a failure (e.g., desired object not found), the failure \nis recorded, the robot returns to the user, and reports the error \nthrough speech. Note that our behavior control does not involve \na planning system. We observed that quite complex tasks can \nbe communicated to the robot as a spoken sequence of actions, \nincluding unspecific objects or locations and reflexive pronouns, \nwhich can be translated into finite state machine behavior.\n7.2.3. Synthesis of Body Language and Gestures\nBy moving in the environment, turning toward persons or toward \nmanipulation locations, etc., our robot generates body language. \nIts anthropomorphic upper body makes it easy to interpret the \nrobot actions. We also paid attention to make the robot motion \nlook human-like. For example, the robot orients is head, upper \nbody, and base into the driving direction to measure potential \nobstacles but also to indicate where it intends to go. According to \nthe rotated masses, different time scales are used for this turning \n(Faber et al., 2008). Similarly, our robot directs its head toward \nthe object that it wants to grasp, which not only provides good \nRGB-D measurements of the object but also makes manipulation \nactions predictable.\nAs part of its multimodal dialog system, our robot not only \nrecognizes gestures but also performs gestures such as pointing \nto a location or waving toward a user using parametrized motion \nprimitives.\n7.3. physical human–Robot Interaction\nOur robot does not only approach persons to communicate with \nthem using speech and vision but also interacts with users in \na physical way. Physical interaction occurs, for example, when \nhanding objects over or when collaboratively working with \nobjects. A key prerequisite for this direct human–robot interac-\ntion is compliant control of the arms (see Section 4.2). This, \nand also the lightweight robot construction, low torques of its \nactuators, and the friendly anthropomorphic design make a safe \ninteraction without fear possible.\n14\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\n7.3.1. Object Hand Over\nObject hand over from the robot to a human could be imple-\nmented with several strategies. For instance, object release could \nbe triggered by speech input or by specialized sensory input such \nas distance or touch sensors. We establish a very natural way \nof hand over by simply releasing the object when the human \npulls on the object. More in detail, the skill is executed in the \nfollowing way:\n• The robot approaches the user and holds the object toward the \nperson while uttering an object hand-over request,\n• The user intuitively understands the hand-over offer and pulls \non the object,\n• We control the motion of the end-effector compliant in for -\nward and upward direction as well as in pitch rotation. Our \nrobot releases the object when it detects a significant displace-\nment of its end effector.\nFigure 4B shows an example of such a hand over.\nWhen the user shall hand an object to the robot, the robot \noffers its open hand to the user. This signals the user to insert the \nobject into the gap between the robot fingers. The object insertion \nis detected using in-hand distance sensors, which triggers closing \nof the fingers to firmly grasp the object. We observed that users \nintuitively understand the hand-over request and leave space for \nthe closing fingers.\n7.3.2. Guiding the Robot at Its Hand\nA second example of physical human–robot interaction is tak-\ning the robot by its hand and guiding it. This is a simple and \nintuitive way to communicate locomotion intents to the robot \n(see Figure 4C). We combine person perception with compli-\nant control to implement this behavior using the following \nprocedure:\n• The robot extends one of its hands forward and waits for the \nuser,\n• As soon as the user appears in front of the robot and exerts \nforces on the hand, the robot starts to follow the motion of the \nhand by translational motion through its drive.\n• The robot avoids the user in a local potential field. It rotates \nwith the drive toward the user to keep the guide at a constant \nrelative angle (e.g., at 45°).\n7.3.3. Cooperatively Carrying a Table\nThe third example of physical human–robot interaction is the \ntask of cooperatively carrying a table (see Figure 4D). It combines \nobject perception, person awareness, and compliant control, and \nconsists of the following key steps:\n• The task starts when the human user appears in front of the \nrobot,\n• The robot approaches the table, grasps it, and waits for the \nperson to lift it,\n• When the robot detects the lifting of the table, it also lifts the \ntable and starts to follow the motion of the human,\n• The human user ends the carrying of the table by lowering the \ntable.\nWe apply our object registration and tracking method (Section \n6.1.5) to find the initial pose of the table toward the robot. The \nrobot then keeps track of the object while it drives toward a \npredefined approach pose, relative to the table. It grasps the table \nand waits, until the person lifts the table, which is indicated by a \nsignificant pitch rotation (0.02 rad) of the table.\nAs soon as the lifting is detected, the robot also lifts the table. \nIt sets the motion of the grippers compliant in the sagittal and lat-\neral direction and in yaw orientation. By this, the robot complies \nwhen the human pulls and pushes the table. The robot follows \nthe motion of the human by controlling its omnidirectional base \nto realign the hands to the initial grasping pose with respect to \nthe robot. During that, it keeps track of the table using MRSMap \nregistration. When the user puts the table down, it detects a \nsignificant pitch of the table, stops, and also lowers the table.\n7.4. handheld Teleoperation Interfaces\nDirect interaction of the user with the robot is not always feasible \nor desirable. In particular, if the user is immobile or at a remote \nlocation, means for controlling the robot from a distance are \nneeded. Such teleoperation interfaces must give the user good \nsituation awareness through the display of robot sensor informa-\ntion and must provide intuitive ways to specify robot tasks.\nSome teleoperation interfaces require special hardware, such \nas head-mounted displays or motion trackers (e.g., Rodehutskors \net al. (2015)), but the use of such complex interfaces is not fea-\nsible in a domestic service setting. Because modern handheld \ncomputers such as smart phones and tablets are already widely \nused and provide display and input modalities, we implemented \nteleoperation with a handheld computer on three levels of \nautonomy (Schwarz et  al., 2014): (I) the user directly controls \nbody parts such as the end effectors, the gaze direction, or the \nomnidirectional drive on the body level. (II) On the skill level, the \nuser controls robot skills, e.g., by setting navigation goals or com-\nmanding objects to be grasped. (III) On the task level , the user \nconfigures autonomous high-level behaviors that sequence skills.\nOur goal is to design a user interface in which the workload \nof the operator decreases with the level of robot autonomy. The \noperator selects the level of autonomy that is appropriate for the \ncurrent situation. If the autonomous execution of a task or a \nskill fails, the user can select a lower level – down to direct body \ncontrol – to solve the task.\n7.4.1. Main User Interface Design\nThe main user interface is split into a main interactive view in \nits center and two configuration columns on the left and right \nside (see Figure 13A). In the left column, further scaled-down \nviews are displayed that can be dragged into the main view. The \ndragged view then switches positions with the current main view. \nBelow the main view in the center, a log screen displays status \ninformation in textual form.\n7.4.2. Body-Level Teleoperation\nThe user has full control of the omnidirectional drive through \ntwo virtual joysticks in the lower left and right corners of the \nGUI. We intend the user to hold the mobile device in landscape \norientation with two hands at its left and right side and to control \nFIgURe 13 | handheld teleoperation interface (Schwarz et al., 2014). (A) Complete GUI with a view selection column on the left, a main view in the center, a \nconfiguration column on the right, and a log message window on the bottom center. Two joystick control UIs on the lower corners for controlling robot motion with \nthe thumbs. (B) Skill-level teleoperation user interface for navigation in a map (placed in center and right UI panel). The user may either select from a set of named \ngoal locations or choose to specify a goal pose on a map. The user specifies navigation goal poses by touching onto the goal position and dragging toward the \ndesired robot orientation.\n15\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nthe UI elements with the left and right thumb. Obstacle avoidance \ndecelerates the robot when the user drives onto an obstacle, but \nbody-level controls do not support autonomous driving around \nthe obstacle. In Muszynski et  al. (2012), we also evaluated the \nuse of the two virtual joysticks for end-effector control. In this \nmode, a centered slider at the bottom lets the user adjust the hand \nclosure.\nWe support swipe gestures on the camera image for changing \nthe gaze direction. By using swipe gestures instead of the joystick \npanels, one can control gaze and drive concurrently without \nswitching controls.\n7.4.3. Skill-Level Teleoperation\nThe skill-level user interfaces configure robot skills that require \nthe execution of a sequence of body motions (see Figure 13B). \nThe robot controls these body motions autonomously. By that, \nthe workload on the user is reduced. While the robot executes \nthe skill, the user can supervise its progress. Compared to body-\nlevel control, the skill-level UI does require less communication \nbandwidth, since images and control commands have to be \ntransmitted with less frequency. Hence, this mode is less affected \nby low quality or low bandwidth communication.\nOn the skill level, the user has access to the following autono-\nmous robot skills: navigation to goal poses in a map, navigation to \nsemantic goal locations, grasping objects in the view of the robot, \ngrasping semantically specific objects, receiving objects from a \nuser, handing objects over to a user, and throwing objects into \na trash bin. Execution failures are reported in the log so that the \nuser can respond appropriately.\n7.4.4. Task-Level Teleoperation\nThe task-level teleoperation UI is intended to provide the high-\nlevel behavior capabilities of the robot. These behaviors sequence \nmultiple skills in a finite state machine. The user can compose \nactions, objects, and locations similar to the speech-based imple-\nmentation of the parsing of complex speech commands described \nin Section 7.2.2.\nThis module allows the user to compose a sequence of skills \nin a two-stage user interface. On the top-level UI, the user adds \nand removes skills from the sequence. Skills can be added from a \ndisplayed list. Once a skill is selected, the user specifies location \nand object for the skill on a second-level UI. Finally, the user can \nstart the execution of the task by touching a button on the bottom \nof the UI. A monitoring UI lets the user keep track of the robot’s \nexecution status, but the user can watch the progress also in the \nbody-level and skill-level visualizations. Detected failures can be \ninstantly reported to the user on the handheld, instead of physi-\ncally returning to the user and reporting failures by speech. In \ncase a failure occurs, the current skill is stopped and the execution \nof the task is interrupted so that the user can take over control.\n8. ReSUlTS\nCompetitions and challenges have become important means in \nrobotics to benchmark complex robot systems (Behnke, 2006; \nGerndt et  al., 2015; Guizzo and Ackerman, 2015). Since 2009, \nwe compete with our cognitive service robots in the RoboCup@\nHome league (Wisspeintner et al., 2009; Iocchi et al., 2015), which \nis the top venue for benchmarking domestic service robots. In \nthis annual international competition, robots have to demon-\nstrate human–robot interaction and mobile manipulation in an \napartment-like and in other domestic environments. The compe-\ntition consists of several tests with predefined tasks, procedures, \nand performance measures that benchmark service robot skills in \nintegrated systems. In addition, open demonstrations allow teams \nto show the best of their research. For our competition entries, \nwe balanced mobile manipulation and human–robot interac-\ntion aspects. In the following, we report results of the RoboCup \ncompetitions in the years 2011–2014, where Cosero was used.\n8.1. Mobile Manipulation and Tool Use\nSeveral predefined tests in RoboCup@Home include object \nretrieval and placement. We often used open challenges to dem-\nonstrate further object manipulation capabilities such as tool use.\n16\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nAt RoboCup 2011 in the Go Get It! test, Cosero found a \nuser-specified object and delivered it. In the Shopping Mall  test, \nit learned a map of a previously unknown shopping mall and \nnavigated to a shown location. In the Demo Challenge, the robot \ncleaned the apartment. It was instructed by gestures (Section \n7.1.3) where to stow different kinds of laundry, picked white laun-\ndry from the floor, and put it into a basket. It then grasped carrots \nand tea boxes from a table. In the finals, our robot demonstrated \na cooking task. It moved to a cooking plate to switch it on. For \nthis, we applied our real-time object tracking method (Section \n6.1.5) in order to approach the cooking plate and to estimate the \nswitch grasping pose. Then, Cosero drove to the location of the \ndough and grasped it. Back at the cooking plate, it opened the \nbottle by unscrewing its lid and poured its contents into the pan. \nMeanwhile, our second robot Dynamaid opened a refrigerator \n(Section 4.2), picked a bottle of orange juice out of it, and placed \nthe bottle on the breakfast table.\nAt RoboCup 2012 in the Clean Up test, our robot Cosero had \nto find objects that were distributed in the apartment, recognize \nthem, and bring them to their place. Our robot detected three \nobjects, from which two were correctly recognized as unknown \nobjects. It grasped all three objects and deposited them in the \ntrash bin. In the Open Challenge, we demonstrated a housekeep-\ning scenario. Cosero took over an empty cup from a person and \nthrew it into the trash bin. Afterward, it approached a watering \ncan and watered a plant. In the Restaurant test, our robot Cosero \nwas guided through a previously unknown bar. The guide showed \nthe robot where the shelves with items and the individual tables \nwere. Our robot built a map of this environment, took an order, \nand navigated to the food shelf to search for requested snacks. In \nthe final, Cosero demonstrated the approaching, bimanual grasp-\ning, and moving of a chair to a target pose. It also approached \nand grasped a watering can with both hands and watered a plant. \nFor this, approaching and bimanual grasping of the chair and \nthe watering can was realized through registration of learned 3D \nmodels of the objects (Section 6.1.5). After this demonstration, \nour second robot Dynamaid fetched a drink and delivered it to \nthe jury. In the meantime, Cosero approached a transport box, \nanalyzed its contents (Section 6.1.4), and grasped a perceived \nobject using grasp and motion planning described in Section 6.3 \n(Figure 10H).\nAt RoboCup 2013, Cosero found in the Clean Up test two \nobjects and brought one of it to its correct place. In the Restaurant \ntest, Cosero was shown the environment and the location of food \nand drinks, which it later found again. In the Demo Challenge, \nwe demonstrated a care scenario in which the robot extended \nthe mobility of a user with its mobile manipulation capabilities \nthrough the teleoperation interface described in Section 7.4. \nCosero also moved a chair to its location.\nIn the Open Challenge, Cosero demonstrated tool-use skill \ntransfer based on our deformable registration method (Section \n6.1.6). The jury chose one of two unknown cans. The watering skill \nwas trained for a third instance of cans before. Cosero success-\nfully transferred the tool-use skill and executed it (Figure 11C).\nIn the final, Cosero demonstrated grasping of sausages with a \npair of tongs. The robot received the tongs through object hand \nover from a team member. It coarsely drove behind the barbecue \nthat was placed on a table by navigating in the environment map \nand tracked the 6-DoF pose of the barbecue using MRSMaps \n(Section 6.1.5) to accurately position itself relative to the bar -\nbecue. It picked one of two raw sausages from a plate next to \nthe barbecue with the tongs (Section 6.4.1) and placed it on the \nbarbecue. While the sausage was grilled, Cosero handed the tongs \nback to a human and went to fetch and open a beer. It picked the \nbottle opener from a shelf and the beer bottle with its other hand \nfrom a table. Then it executed the bottle opening skill described \nin Section 6.4.2. After our robot placed the bottle opener on the \ntable, it delivered the beer to a jury member. Then it received the \ntongs again and returned to the barbecue to grasp the sausage and \nto place it on a clean plate.\nIn the finals of German Open 2014, Cosero demonstrated again \nthe use of the tongs and the bottle opener (Figures 11A,B). This \ntime, the sausage was placed on the grill in advance. Accordingly, \nthe task of Cosero was to pick it from the barbecue and place it \non a plate, which was located on a tray. Our robot grasped the \ntray with both hands and delivered the sausage to a jury member \n(Figure 10D).\nAt RoboCup 2014 in Brazil, Cosero demonstrated in the Basic \nFunctionality test object recognition and grasping as well as navi-\ngation in the arena where an additional obstacle was placed and \na door was closed. It demonstrated opening a bottle in the Open \nChallenge (Figure 14A). In the Enduring General Purpose Service \nRobot test, our robot recognized two complex speech commands \n(Section 7.2.2) and carried out the requested actions. In the final, \nCosero demonstrated the use of tools. It grasped a dustpan and \na swab in order to clean some dirt from the floor (Figure 14B). \nAlthough the dirt detection failed, our robot executed the clean-\ning motion and continued the demo by pouring out the contents \nof the dustpan into the dustbin. It placed the tools back on a table \nand started to make caipirinha. For this, it used a muddler to \nmuddle lime pieces (Figure 14C).\n8.2. human–Robot Interaction\nPerson detection (Section 7.1.1) followed by face enrollment \nand later identification (Section 7.1.2) has been demonstrated by \nCosero at multiple occasions during RoboCup@Home competi-\ntions throughout the years 2011–2014. At the 2011 RoboCup in \nthe Follow Me test, Cosero met a previously unknown person and \nfollowed him reliably through an unknown environment. Cosero \nshowed that it can distinguish the person from others and that \nit recognizes stop gestures (Section 7.1.3). In 2012, the Follow \nMe test was made more difficult. Cosero learned the face of the \nguide and was not disturbed later by another person blocking the \nline-of-sight. It followed the guide into an elevator and left it on \nanother floor.\nIn Who Is Who, two previously unknown persons introduced \nthemselves to Cosero. Later in the test, our robot found one of \nthe previously unknown persons, two team members, and one \nunknown person and recognized their identity correctly. In the \n2012 Who Is Who test, Cosero learned the faces of three persons, \ntook an order, fetched three drinks into a basket and with each \nof its grippers, and successfully delivered two of them within the \ntime limit of 5 min. In 2013, face recognition has been embed-\nded in the Cocktail Party test, where the robot took drink orders, \nFIgURe 14 | Tool use at Robocup 2014 in João pessoa, Brazil. (A) Bottle opener, (B) Dustpan and swab, and (c) Muddler.\n17\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nfetched the drinks, and delivered these to persons identified after \nthey moved to a different room.\nThe recognition of pointing gestures (Section 7.1.3) has been \ndemonstrated in several tests, e.g., in the German Open 2011 \nfinal, where a jury member showed our robot the exit door, in the \nRoboCup 2011 Demo Challenge, where our robot tidied up the \napartment by moving objects into shelves indicated by pointing \ngestures, in the RoboCup 2011 finals, where a user showed our \nrobot where it finds a bottle of dough to make an omelet, in the \nGerman Open 2012 Demo Challenge, where a human showed the \nrobot in which baskets to put colored and white laundry, and in \nthe RoboCup 2012 Demo Challenge, where the robot picked up \nan object referenced by pointing from the floor. In the 2012 Open \nChallenge, Cosero demonstrated that it could recognize a waving \nperson. It took over an empty cup from this person and threw it \ninto the trash bin.\nAt RoboCup 2013, the Emergency Situation test was intro-\nduced. Here, Cosero found a standing person, asked the person \nif he required help, and guided him to the exit.\nCooperative carrying of a table by Cosero and a human \n(Section 7.3.3) was demonstrated in the RoboCup 2011 final \n(Figure  4D) and in the German Open 2012 Open Challenge. \nIn this test, also guiding the robot by taking its hand (Section \n7.3.2) was demonstrated (Figure 4C). Human–robot object hand \nover in both directions was demonstrated very often, e.g., at the \nGerman Open 2014 (Figure 4C).\nTask-level behavior generation according to complex speech \ncommands as described in Section 7.2.2 is tested in the General \nPurpose Service Robot test of the RoboCup@Home competition. \nAt RoboCup 2012 in Mexico, Cosero recognized speech com-\nmands from two out of three categories. It recognized a complex \nspeech command consisting of three skills. It also understood a \nspeech command with unspecific information and posed adequate \nquestions to retrieve missing information. In 2013, the Enduring \nGeneral Purpose Service Robot test was introduced, where three \nrobots were tested in a round-robin procedure for up to 40 min. \nAgain, Cosero performed well in this test, understanding two \ncommands in two command categories. In the first trial, Cosero \nunderstood a complex command composed of three skills. The \nsecond complex command was sequencing navigation skills and \nwas solved by Cosero easily. It then received a command with \nunspecific information where it also asked questions to make the \ntask specific. It now should grasp from the armrest of a couch, but \ncould not find the object. Cosero detected this error, returned to \nthe user, and reported the problem.\nAt RoboCup 2012 and German Open 2013, we also demon-\nstrated teleoperation using a handheld device (Section 7.4). In \nthe Demo Challenge at RoboCup 2012, we showed an elderly \ncare scenario in which a user commanded the robot to fetch a \ndrink from another room. At first, the person let the robot fetch \na specific beverage. The robot drove to the assumed location of \nthe drink, but since it was not available, the remote user had to \ntake a different choice. The user switched to the skill-level control \nuser interface and selected one of the other beverages that were \nperceived by the robot on the table and displayed in live images on \nthe handheld screen. Finally, the robot grasped the selected drink, \nbrought it to the user, and handed it over. At German Open 2013, \nwe extended the Demo Challenge with receiving objects from \nusers and putting the object in a waste bin.\nWe demonstrated our signal strength-based object localization \napproach (Section 6.1.7) publicly during the Demo Challenge at \nthe 2014 RoboCup German Open competition in Magdeburg. A \nuser asked Cosero to retrieve his medicine that he could not find. \nThe medicine had been placed at one of two locations, which was \nchosen by a jury member. A Bluetooth tag had been attached to \nthe medicine, which was localized coarsely using signal strength-\nbased lateration from four receivers in the room corners. Cosero \ndrove to the table close to the estimated medicine position, \nsearched, detected, and grasped the medicine, and brought it to \nthe user. In a second run, the robot localized and retrieved the \nobject from the other location.\n8.3. competition Results\nWith Cosero, we participated in four international RoboCup@\nHome and four RoboCup German Open @Home competitions \nin the years 2011–2014. Our robot systems performed consist-\nently well in the predefined tests and our open demonstrations \nconvinced the juries, which consisted of team leaders, members \nof the executive committee of the league, and representatives of \nthe media, science, and industry.\nOur team NimbRo@Home won the international RoboCup@\nHome competitions in 2011 [Istanbul (Stückler et  al., 2012)], \n2012 [Mexico City (Stückler et al., 2013a)], and 2013 [Eindhoven \n(Stückler et al., 2014)]. Our team also continuously achieved 1st \nTABle 1 | Robocup@home competition results 2011–2014.\n@home \ncompetition\nWinner \n(normal score)\nSecond place \n(normal score)\nThird place \n(normal score)\n2014, João \nPessoa\nWright Eagle, \nChina\nTech United \nEindhoven\nNimbRo@Home\nGerman Open \n2014\nNimbRo@Home \n(100)\nTech United \nEindhoven (59) \nToBI Bielefeld (50)\n2013, Eindhoven NimbRo@Home \n(99)\nWright Eagle, \nChina (86)\nTech United \nEindhoven (73)\nGerman Open \n2013\nNimbRo@Home \n(100)\nSmartBots@Ulm \n(67)\nHomer, University \nof Koblenz (61)\n2012, Mexico \ncity\nNimbRo@Home \n(100)\neR@sers, Japan \n(74)\nToBi Bielefeld (64)\nGerman Open \n2012\nNimbRo@Home \n(100)\nb-it-bots, St. \nAugustin (56)\nGolem, Mexico \n(39)\n2011, Istanbul NimbRo@Home \n(100)\nWright Eagle, \nChina\nb-it-bots, Sankt \nAugustin\nGerman Open \n2011\nNimbRo@Home SmartBots, Ulm b-it-bots, Sankt \nAugustin\n18\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nplace in the RoboCup German Open competitions of the league \nfrom 2011 to 2014. Table  1  summarizes these results. Detailed \ncompetition reports, including pictures and videos, can be found \non our website.\n3\n8.4. lessons learned\nOur continuous success in international RoboCup@Home \ncompetitions demonstrates our achievements in designing \nand integrating a domestic service robot system that balances \nmobile manipulation and human–robot interaction capabilities. \nCurrently, our system is limited to short task demonstrations \n(ca. 10  min) in partially controlled competition environments. \nThe development of the system gave us many insights into future \nresearch steps that are necessary to potentially scale domestic \nrobot systems further toward real application scenarios.\n• A soft and compliant mechatronic design would increase the \ninherent safety of our robot,\n• We designed special tool handles to overcome limitations of \nour current gripper design. Dexterous human-like hands with \ndelicate tactile sensing would allow for more complex manip-\nulation skills without such special tool handles,\n• Locomotion with a wheeled base is limited to flat ground with \nsmall slopes and steps. A future direction could be to combine \nwheeled with legged locomotion to also pass over steps or \nstairs,\n• Our navigation system currently models the environment in \nstatic maps. Changes in the environment are handled using \nprobabilistic measurement models and probabilistic state esti-\nmation. A dynamic environment representation could handle \nchanges more flexibly and could allow for keeping track of the \nmoving objects,\n• Object recognition and handling is mostly limited to small-\nscale predefined sets of objects. We explored first steps toward \n3 http://www.ais.uni-bonn.de/nimbro/@Home. \nscaling the system to a larger variety of unknown objects with \nour shape-based skill transfer approach,\n• Object perception in our system is currently focused on the \nrobot on-board sensory percepts. External sensors such as \nBluetooth tags give an important further cue for the perception \nof the state of objects. It could be a viable option to instrument \nthe environment with various further sensors to increase the \nawareness on the objects in the environment.\n• Our robot perceives person through detection and facial \nidentification. It can also interpret a set of short gestures. The \nobservation of prolonged human actions and behavior, the \nunderstanding of user intents, and the predictions of future \nactions would allow our system to achieve increased levels of \nhuman–robot interaction.\n9. conclUSIon\nIn this paper, we detailed our approaches to realize mobile \nmanipulation, tool use, and intuitive human–robot interaction \nwith our cognitive service robot Cosero.\nWe equipped our robot with an anthropomorphic upper \nbody and an omnidirectional drive to perform tasks in typical \nhousehold scenarios. Through compliant control of the arms, \nour robot interacts physically with humans and manipulates \nobjects such as doors without accurate models. We proposed \nseveral object perception methods to implement the variety of \nmanipulation skills of our robots. We segment scenes at high \nframe rate into support surfaces and objects. In order to align \nto objects for grasping, we register RGB-D measurements on \nthe object with a 3D model using multiresolution surfel maps \n(MRSMaps). Through deformable registration of MRSMaps, we \ntransfer object manipulation skills to differently shaped instances \nof the same object category. Tool use is one of the most complex \nmanipulation skills for humans and robots in daily life. We \nimplemented several tool-use strategies using our perception \nand control methods.\nFor human–robot interaction, communication partners are \nperceived using laser range sensors and vision. Our robot can \nrecognize and synthesize speech and several gestures. It can \nparse the semantics of complex speech commands and gener -\nate behavior accordingly. To control the robot on three levels \nof autonomy, we developed teleoperation user interfaces for \nhandheld devices.\nThe outstanding results achieved at multiple national and \ninternational RoboCup@Home competitions clearly demonstrate \nour success in designing a balanced system that incorporates \nmobile manipulation and intuitive human–robot interfaces. The \ndevelopment and benchmarking of the system gave us many \ninsights into the requirements for complex personal service \nrobots in scenarios such as cleaning the home or assisting the \nelderly. Challenges like RoboCup@Home show that a successful \nsystem not only consists of valid solutions to isolated action and \nperception problems. The proper integration of the overall system \nis equally important.\nDespite a large number of successful demonstrations, our \nsystem is currently limited to short tasks in partially controlled \n19\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nenvironments. In order to scale toward real application in \ndomestic service scenarios, we need to address open issues. On \nthe mechatronic side, a soft and compliant robot design would \nfurther increase the inherent safety of the robot, and more dex-\nterous hands would enable more complex manipulation skills \nand reduce the need for special tool handles. Object recognition \nand handling that scales to the large variety of objects in our daily \nhomes is still an open research problem. Significant progress has \nbeen made, e.g., through deep learning methods, but occlusions \nand material properties like transparency or highly reflective \nsurfaces make it still challenging to analyze typical household \nscenes. Similarly, perceiving people and understanding their \nactions in the many situations possible in everyday environ-\nments is a challenge. One promising approach to address these \nchallenges is to instrument the environment with a multitude \nof sensors in order to track all objects continuously with high \naccuracy (Fox, 2016).\nAUThoR conTRIBUTIonS\nJS served as Team Leader of the NimbRo@Home team \n2009–2013. He contributed most of the developed methods. MS \nwas team member since 2009 and team leader since 2014. He \ncontributed object perception using pretrained neural networks \nand Bluetooth tags. SB initiated and managed the project and \ncompiled the paper.\nAcKnoWleDgMenTS\nThe authors thank the members of team NimbRo@Home for \ntheir support in preparing and running the RoboCup competi-\ntion systems: Nikita Araslanov, Ishrat Badami, David Droeschel, \nKathrin Gräve, Dirk Holz, Jochen Kläss, Manus McElhone, \nMatthias Nieuwenhuisen, Michael Schreiber, David Schwarz, \nRicarda Steffens, and Angeliki Topalidou-Kyniazopoulou.\nReFeRenceS\nAckerman, E. (2016). “IHMC’s ATLAS robot learning to do some chores, ” in \nAutomaton, IEEE Spectrum Blog. Available at: http://spectrum.ieee.org/\nautomaton/robotics/humanoids/atlas-does-some-chores\nAsfour, T., Regenstein, K., Azad, P ., Schröder, J., Bierbaum, A., Vahrenkamp, N., \net al. (2006). “ ARMAR-III: an integrated humanoid platform for sensory-motor \ncontrol, ” in Proc. IEEE-RAS International Conference on Humanoid Robots \n(Humanoids) (Genova: IEEE), 169–175.\nBäuml, B., Schmidt, F ., Wimböck, T., Birbach, O., Dietrich, A., Fuchs, M., et al. \n(2011). “Catching flying balls and preparing coffee: Humanoid Rollin’Justin \nperforms dynamic and sensitive tasks, ” in IEEE International Conference on \nRobotics and Automation (ICRA) (Shanghai, China), 3443–3444.\nBay, H., Ess, A., Tuytelaars, T., and Van Gool, L. (2008). Speeded-up robust fea-\ntures (SURF). Comput. Vision Image Understand. 110, 346–359. doi:10.1016/j.\ncviu.2007.09.014 \nBeetz, M., Klank, U., Kresse, I., Maldonado, A., Mösenlechner, L., Pangercic, D., \net al. (2011). “Robotic roommates making pancakes, ” in IEEE-RAS International \nConference on Humanoid Robots (Humanoids) (Bled: IEEE), 529–536.\nBehnke, S. (2006). “Robot competitions – ideal benchmarks for robotics research, ” \nin IROS Workshop on Benchmarks in Robotics Research, Beijing.\nBerner, A., Li, J., Holz, D., Stückler, J., Behnke, S., and Klein, R. (2013). “Combining \ncontour and shape primitives for object detection and pose estimation of pre-\nfabricated parts, ” in IEEE International Conference on Image Processing (ICIP) \n(Melbourne: IEEE), 3326–3330.\nBohren, J., Rusu, R., Jones, E., Marder-Eppstein, E., Pantofaru, C., Wise, M., et al. \n(2011). “Towards autonomous robotic butlers: lessons learned with the PR2, ” in \nIEEE International Conference on Robotics and Automation (ICRA) (Shanghai: \nIEEE), 5568–5575.\nBorst, C., Wimböck, T., Schmidt, F ., Fuchs, M., Brunner, B., Zacharias, F ., et al. \n(2009). “Rollin’Justin–Mobile platform with variable base, ” in IEEE International \nConference on Robotics and Automation (ICRA) (Kobe: IEEE), 1597–1598.\nChen, X., Shuai, W ., Liu, J., Liu, S., Wang, X., Wang, N., et al. (2015). “KeJia: the \nintelligent domestic robot for RoboCup@Home 2015, ” in RoboCup@Home \nLeague Team Descriptions for the Competition in Hefei, China. Available at: \nhttp://robocup2015.oss-cn-shenzhen.aliyuncs.com/TeamDescriptionPapers/\nRoboCup@Home/RoboCup_Symposium_2015_submission_124.pdf\nDalal, N., and Triggs, B. (2005). “Histograms of oriented gradients for human \ndetection, ” in International Conference on Computer Vision and Pattern \nRecognition (CVPR) (San Diego: IEEE), 886–893.\nDroeschel, D., and Behnke, S. (2011). “3D body pose estimation using an adaptive \nperson model for articulated ICP , ” in 4th International Conference on Intelligent \nRobotics and Applications (ICIRA) (Aachen: IEEE), 157–167.\nDroeschel, D., Stückler, J., and Behnke, S. (2011). “Learning to interpret pointing \ngestures with a time-of-flight camera, ” in 6th ACM International Conference on \nHuman-Robot Interaction (HRI) (Lausanne: ACM/IEEE), 481–488.\nFaber, F ., Bennewitz, M., and Behnke, S. (2008). “Controlling the gaze direction of a \nhumanoid robot with redundant joints, ” in 17th IEEE International Symposium \non Robot and Human Interactive Communication (RO-MAN) (Munich: IEEE), \n413–418.\nFox, D. (2001). “KLD-sampling: adaptive particle filters and mobile robot localiza-\ntion, ” in Advances in Neural Information Processing Systems (NIPS) (Vancouver: \nMIT Press), 26–32.\nFox, D. (2016). “The 100-100 tracking challenge, ” in Keynote at IEEE International \nConference on Robotics and Automation (ICRA), Stockholm.\nGerndt, R., Seifert, D., Baltes, J., Sadeghnejad, S., and Behnke, S. (2015). Humanoid \nrobots in soccer–robots versus humans in RoboCup 2050. IEEE Robot. Autom. \nMag. 22, 147–154. doi:10.1109/MRA.2015.2448811 \nGrisetti, G., Stachniss, C., and Burgard, W . (2007). Improved techniques for grid \nmapping with Rao-Blackwellized particle filters. IEEE Trans. Rob. 23, 34–46. \ndoi:10.1109/TRO.2006.889486 \nGuizzo, E., and Ackerman, E. (2015). The hard lessons of DARPA ’s robotics chal-\nlenge. Spectrum 52, 11–13. doi:10.1109/MSPEC.2015.7164385 \nHermann, A., Sun, J., Xue, Z., Rhl, S. W ., Oberländer, J., Roennau, A., et al. (2013). \n“Hardware and software architecture of the bimanual mobile manipulation \nrobot HoLLiE and its actuated upper body, ” in IEEE/ASME International \nConference on Advanced Intelligent Mechatronics (AIM) (Wollongong: IEEE), \n286–292.\nIocchi, L., Holz, D., Ruiz-del Solar, J., Sugiura, K., and van der Zant, T. (2015). \nRoboCup@Home: analysis and results of evolving competitions for domestic \nand service robots. Artif. Intell. 229, 258–281. doi:10.1016/j.artint.2015.08.002 \nIwata, H., and Sugano, S. (2009). “Design of human symbiotic robot TWENDY-\nONE, ” in IEEE International Conference on Robotics and Automation (ICRA) \n(Kobe: IEEE), 580–586.\nJain, A., and Kemp, C. C. (2010). EL-E: an assistive mobile manipulator that \nautonomously fetches objects from flat surfaces. Auton. Robots 28, 45–64. \ndoi:10.1007/s10514-009-9148-5 \nKittmann, R., Fröhlich, T., Schäfer, J., Reiser, U., Weißhardt, F ., and Haug, A. \n(2015). “Let me introduce myself: i am Care-O-bot 4, a gentleman robot, ” in \nMensch und Computer 2015 – Proceedings (Berlin: DeGruyter Oldenbourg),  \n223–232.\nKläss, J., Stückler, J., and Behnke, S. (2012). “Efficient mobile robot navigation \nusing 3D surfel grid maps, ” in 7th German Conference on Robotics (ROBOTIK), \nMunich.\nKuhn, H. (1955). The hungarian method for the assignment problem. Naval Res. \nLogist. Q. 2, 83–97. doi:10.1002/nav.3800020109 \nLeidner, D., Dietrich, A., Schmidt, F ., Borst, C., and Albu-Schäffer, A. (2014). \n“Object-centered hybrid reasoning for whole-body mobile manipulation, ” in \nIEEE International Conference on Robotics and Automation (ICRA) (Hong Kong: \nIEEE), 1828–1835.\nLunenburg, J., van den Dries, S., Ferreira, L. B., and van de Molengraft, R. \n(2015). “Tech United Eindhoven @Home 2015 team description paper, ” in \n20\nStückler et al.\nMobile Manipulation, Tool Use, and Intuitive Interaction for Cosero\nFrontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 58\nRoboCup@Home League Team Descriptions for the Competition in Hefei,  \nChina. Available at: http://robocup2015.oss-cn-shenzhen.aliyuncs.com/\nTeamDescriptionPapers/RoboCup@Home/RoboCup_Symposium_2015_sub-\nmission_147.pdf\nMcElhone, M., Stückler, J., and Behnke, S. (2013). “Joint detection and pose \ntracking of multi-resolution surfel models in RGB-D, ” in European Conference \non Mobile Robots (ECMR) (Barcelona: IEEE), 131–137.\nMeeussen, W ., Wise, M., Glaser, S., Chitta, S., McGann, C., Mihelich, P ., et  al. \n(2010). “ Autonomous door opening and plugging in with a personal robot, ” in \nIEEE International Conference on Robotics and Automation (ICRA) (Anchorage: \nIEEE), 729–736.\nMuszynski, S., Stückler, J., and Behnke, S. (2012). “ Adjustable autonomy for mobile \nteleoperation of personal service robots, ” in IEEE International Symposium \non Robot and Human Interactive Communication (RO-MAN) (France: IEEE), \n933–940.\nMyronenko, A., and Song, X. (2010). Point set registration: coherent point \ndrift. IEEE Trans. Pattern Anal. Mach. Intell. 32, 2262–2275. doi:10.1109/\nTPAMI.2010.46 \nNguyen, H., Ciocarlie, M., Hsiao, K., and Kemp, C. (2013). “ROS commander: \nflexible behavior creation for home robots, ” in IEEE International Conference on \nRobotics and Automation (ICRA) (Karlsruhe: IEEE), 467–474.\nNieuwenhuisen, M., and Behnke, S. (2013). Human-like interaction skills for \nthe mobile communication robot Robotinho. Int. J. Soc. Rob. 5, 549–561. \ndoi:10.1007/s12369-013-0206-y \nNieuwenhuisen, M., Droeschel, D., Holz, D., Stückler, J., Berner, A., Li, J., et al. \n(2013). “Mobile bin picking with an anthropomorphic service robot, ” in IEEE \nInternational Conference on Robotics and Automation (ICRA) (Karlsruhe: \nIEEE), 2327–2334.\nParlitz, C., Hägele, M., Klein, P ., Seifert, J., and Dautenhahn, K. (2008). “Care-o-\nbot 3 – rationale for human-robot interaction design, ” in 39th International \nSymposium on Robotics (ISR) (Seoul: IEEE), 275–280.\nQuigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., et al. (2009). “ROS: \nan open-source robot operating system, ” in ICRA Workshop on Open Source \nSoftware, Kobe.\nRodehutskors, T., Schwarz, M., and Behnke, S. (2015). “Intuitive bimanual tele-\nmanipulation under communication restrictions by immersive 3D visualiza-\ntion and motion tracking, ” in IEEE-RAS International Conference on Humanoid \nRobots (Humanoids) (Seoul: IEEE), 276–283.\nSang, H. (2011). Introduction of CIROS. Available at: http://www.irobotics.re.kr/\neng_sub1_3\nSchwarz, D., Schwarz, M., Stückler, J., and Behnke, S. (2015a). “Cosero, find my \nkeys! object localization and retrieval using Bluetooth low energy tags, ” in \nRoboCup 2014: Robot Soccer World Cup XVIII, Volume 8992 of Lecture Notes in \nComputer Science (Springer), 195–206.\nSchwarz, M., Schulz, H., and Behnke, S. (2015b). “RGB-D object recognition and \npose estimation based on pre-trained convolutional neural network features, ” \nin IEEE International Conference on Robotics and Automation (ICRA) (Seattle: \nIEEE), 1329–1335.\nSchwarz, M., Stückler, J., and Behnke, S. (2014). “Mobile teleoperation interfaces \nwith adjustable autonomy for personal service robots, ” in Proceedings of the \n2014 ACM/IEEE International Conference on Human-robot Interaction (HRI) \n(Bielefeld: ACM), 288–289.\nSeib, V ., Manthe, S., Holzmann, J., Memmesheimer, R., Peters, A., Bonse, M., et al. \n(2015). “RoboCup 2015 – homer@UniKoblenz (Germany), ” in RoboCup@\nHome League Team Descriptions for the Competition in Hefei, China. Available at: \nhttp://robocup2015.oss-cn-shenzhen.aliyuncs.com/TeamDescriptionPapers/\nRoboCup@Home/RoboCup_Symposium_2015_submission_131.pdf\nSrinivasa, S., Ferguson, D., Helfrich, C., Berenson, D., Romea, A. C., Diankov, R., \net al. (2010). Herb: a home exploring robotic butler. Auton. Robots 28, 5–20. \ndoi:10.1007/s10514-009-9160-9 \nStückler, J., Badami, I., Droeschel, D., Gräve, K., Holz, D., McElhone, M., et  al. \n(2013a). “NimbRo@Home: winning team of the RoboCup@Home competition \n2012, ” in RoboCup 2012: Robot Soccer World Cup XVI, Volume 7500 of Lecture \nNotes in Computer Science, eds X. Chen, P . Stone, L. E. Sucar, and T. van der \nZant (Springer), 94–105.\nStückler, J., Steffens, R., Holz, D., and Behnke, S. (2013b). Efficient 3D object per-\nception and grasp planning for mobile manipulation in domestic environments. \nRob. Auton. Syst. 61, 1106–1115. doi:10.1016/j.robot.2012.08.003 \nStückler, J., and Behnke, S. (2011). Dynamaid, an anthropomorphic robot for \nresearch on domestic service applications. Automatika J Control Meas. Electron. \nComput. Commun. 52, 233–243. \nStückler, J., and Behnke, S. (2012). “Compliant task-space control with back-driv-\nable servo actuators, ” in RoboCup 2011: Robot Soccer World Cup XV , Volume \n7416 of Lecture Notes in Computer Science (Springer), 78–89.\nStückler, J., and Behnke, S. (2014a). “ Adaptive tool-use strategies for anthropomor-\nphic service robots, ” in 14th IEEE/RAS International Conference of Humanoids \nRobotics (Humanoids) (Madrid: IEEE), 755–760.\nStückler, J., and Behnke, S. (2014b). “Efficient deformable registration of \n multi-resolution surfel maps for object manipulation skill transfer, ” in IEEE \nInternational Conference on Robotics and Automation (ICRA) (Hong Kong: \nIEEE), 994–1001.\nStückler, J., and Behnke, S. (2014c). Multi-resolution surfel maps for efficient dense \n3D modeling and tracking. J. Visual Commun. Image Represent. 25, 137–147. \ndoi:10.1016/j.jvcir.2013.02.008 \nStückler, J., Droeschel, D., Gräve, K., Holz, D., Kläß, J., Schreiber, M., et  al. \n(2012). “Towards robust mobility, flexible object manipulation, and intuitive \nmultimodal interaction for domestic service robots, ” in RoboCup 2011: Robot \nSoccer World Cup XV , Volume 7416 of Lecture Notes in Computer Science, eds Th. \nRoefer, N. M. Mayer, J. Savage, and U. Saranlı (Springer), 51–62.\nStückler, J., Droeschel, D., Gräve, K., Holz, D., Schreiber, M., Topalidou-\nKyniazopoulou, A., et al. (2014). “Increasing flexibility of mobile manipulation \nand intuitive human-robot interaction in RoboCup@Home, ” in RoboCup \n2013: Robot Soccer World Cup XVII, Volume 8371 of Lecture Notes in Computer \nScience, eds S. Behnke, M. M. Veloso, A. Visser, and R. Xiong (Springer),  \n135–146.\nStückler, J., Schwarz, M., Schadler, M., Topalidou-Kyniazopoulou, A., and \nBehnke, S. (2016). NimbRo explorer: semiautonomous exploration and \nmobile manipulation in rough terrain. J. Field Rob. 33, 411–430. doi:10.1002/\nrob.21592 \nSucan, I. A., and Kavraki, L. E. (2008). “Kinodynamic motion planning by \ninterior-exterior cell exploration, ” in Algorithmic Foundation of Robotics VIII \n(Workshop Proceedings), Guanajuato.\nViola, P ., and Jones, M. (2001). “Rapid object detection using a boosted cascade \nof simple features, ” in IEEE International Conference on Computer Vision and \nPattern Recognition (CVPR) (Kuaui: IEEE), 511–518.\nWisspeintner, T., Van Der Zant, T., Iocchi, L., and Schiffer, S. (2009). RoboCup@\nHome: scientific competition and benchmarking for domestic service robots. \nInteract. Stud. 10, 392–426. doi:10.1075/is.10.3.06wis \nzu Borgsen, S. M., Korthals, T., Ziegler, L., and Wachsmuth, S. (2015). “ToBI – \nteam of bielefeld: the human-robot interaction system for RoboCup@Home \n2015, ” in RoboCup@Home League Team Descriptions for the Competition in \nHefei, China. Available at: http://robocup2015.oss-cn-shenzhen.aliyuncs.com/\nTeamDescriptionPapers/RoboCup@Home/RoboCup_Symposium_2015_sub-\nmission_96.pdf\nConflict of Interest Statement: The authors declare that the research was con-\nducted in the absence of any commercial or financial relationships that could be \nconstrued as a potential conflict of interest.\nCopyright © 2016 Stückler, Schwarz and Behnke. This is an open-access article \ndistributed under the terms of the Creative Commons Attribution License (CC BY). \nThe use, distribution or reproduction in other forums is permitted, provided the \noriginal author(s) or licensor are credited and that the original publication in this \njournal is cited, in accordance with accepted academic practice. No use, distribution \nor reproduction is permitted which does not comply with these terms.",
  "topic": "Teleoperation",
  "concepts": [
    {
      "name": "Teleoperation",
      "score": 0.7847455739974976
    },
    {
      "name": "Human–computer interaction",
      "score": 0.7457059621810913
    },
    {
      "name": "Computer science",
      "score": 0.7303662896156311
    },
    {
      "name": "Robot",
      "score": 0.6665251851081848
    },
    {
      "name": "Robotics",
      "score": 0.6011987328529358
    },
    {
      "name": "Mobile robot",
      "score": 0.5922889709472656
    },
    {
      "name": "Service robot",
      "score": 0.5611758232116699
    },
    {
      "name": "Service (business)",
      "score": 0.5289366841316223
    },
    {
      "name": "Perception",
      "score": 0.5275694131851196
    },
    {
      "name": "Cognitive robotics",
      "score": 0.5159705281257629
    },
    {
      "name": "Artificial intelligence",
      "score": 0.470229834318161
    },
    {
      "name": "Human–robot interaction",
      "score": 0.46573320031166077
    },
    {
      "name": "Autonomy",
      "score": 0.4632834792137146
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4576711654663086
    },
    {
      "name": "Cognition",
      "score": 0.4425515830516815
    },
    {
      "name": "Psychology",
      "score": 0.09503084421157837
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135140700",
      "name": "University of Bonn",
      "country": "DE"
    }
  ],
  "cited_by": 30
}