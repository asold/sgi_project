{
  "title": "An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models",
  "url": "https://openalex.org/W2951514583",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2749853622",
      "name": "Chronopoulou, Alexandra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281516578",
      "name": "Baziotis, Christos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213689020",
      "name": "Potamianos, Alexandros",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2516255829",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W1614862348",
    "https://openalex.org/W2962809001",
    "https://openalex.org/W2963357986",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2750747353",
    "https://openalex.org/W1988733743",
    "https://openalex.org/W2904683980",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962974772",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963871484",
    "https://openalex.org/W2963119602",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3106003309",
    "https://openalex.org/W2964101952",
    "https://openalex.org/W2143017621"
  ],
  "abstract": "A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.",
  "full_text": "An Embarrassingly Simple Approach for Transfer Learning from\nPretrained Language Models\nAlexandra Chronopoulou1, Christos Baziotis1, Alexandros Potamianos1,2,3\n1School of ECE, National Technical University of Athens, Athens, Greece\n2 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, USA\n3 Behavioral Signal Technologies, Los Angeles, USA\nel12068@central.ntua.gr, cbaziotis@mail.ntua.gr\npotam@central.ntua.gr\nAbstract\nA growing number of state-of-the-art trans-\nfer learning methods employ language mod-\nels pretrained on large generic corpora. In this\npaper we present a conceptually simple and\neffective transfer learning approach that ad-\ndresses the problem of catastrophic forgetting.\nSpeciﬁcally, we combine the task-speciﬁc op-\ntimization function with an auxiliary language\nmodel objective, which is adjusted during the\ntraining process. This preserves language reg-\nularities captured by language models, while\nenabling sufﬁcient adaptation for solving the\ntarget task. Our method does not require pre-\ntraining or ﬁnetuning separate components of\nthe network and we train our models end-to-\nend in a single step. We present results on a va-\nriety of challenging affective and text classiﬁ-\ncation tasks, surpassing well established trans-\nfer learning methods with greater level of com-\nplexity.\n1 Introduction\nPretrained word representations captured by Lan-\nguage Models (LMs) have recently become pop-\nular in Natural Language Processing (NLP). Pre-\ntrained LMs encode contextual information and\nhigh-level features of language, modeling syntax\nand semantics, producing state-of-the-art results\nacross a wide range of tasks, such as named entity\nrecognition (Peters et al., 2017), machine transla-\ntion (Ramachandran et al., 2017) and text classiﬁ-\ncation (Howard and Ruder, 2018).\nHowever, in cases where contextual embed-\ndings from language models are used as additional\nfeatures (e.g. ELMo (Peters et al., 2018)), results\ncome at a high computational cost and require\ntask-speciﬁc architectures. At the same time, ap-\nproaches that rely on ﬁne-tuning a LM to the task\nat hand (e.g. ULMFiT (Howard and Ruder, 2018))\ndepend on pretraining the model on an exten-\nsive vocabulary and on employing a sophisticated\nslanted triangular learning rate scheme to adapt the\nparameters of the LM to the target dataset.\nWe propose a simple and effective transfer\nlearning approach, that leverages LM contextual\nrepresentations and does not require any elaborate\nscheduling schemes during training. We initially\ntrain a LM on a Twitter corpus and then transfer\nits weights. We add a task-speciﬁc recurrent layer\nand a classiﬁcation layer. The transferred model\nis trained end-to-end using an auxiliary LM loss,\nwhich allows us to explicitly control the weighting\nof the pretrained part of the model and ensure that\nthe distilled knowledge it encodes is preserved.\nOur contributions are summarized as follows:\n1) We show that transfer learning from language\nmodels can achieve competitive results, while also\nbeing intuitively simple and computationally ef-\nfective. 2) We address the problem of catastrophic\nforgetting, by adding an auxiliary LM objective\nand using an unfreezing method. 3) Our results\nshow that our approach is competitive with more\nsophisticated transfer learning methods. We make\nour code widely available. 1\n2 Related Work\nUnsupervised pretraining has played a key role in\ndeep neural networks, building on the premise that\nrepresentations learned for one task can be use-\nful for another task. In NLP, pretrained word vec-\ntors (Mikolov et al., 2013; Pennington et al., 2014)\nare widely used, improving performance in vari-\nous downstream tasks, such as part-of-speech tag-\nging (Collobert et al., 2011) and question answer-\ning (Xiong et al., 2016). These pretrained word\nvectors serve as initialization of the embedding\nlayer and remain frozen during training, while our\npretrained language model also initializes the hid-\nden layers of the model and is ﬁne-tuned to each\n1www.github.com/alexandra-chron/siatl\narXiv:1902.10547v3  [cs.CL]  31 May 2019\nclassiﬁcation task.\nAiming to learn from unlabeled data, Dai and\nLe (2015) use unsupervised objectives such as se-\nquence autoencoding and language modeling for\nas pretraining methods. The pretrained model is\nthen ﬁne-tuned to the target task. However, the\nﬁne-tuning procedure of the language model to the\ntarget task does not include an auxiliary objective.\nRamachandran et al. (2017) also pretrain encoder-\ndecoder pairs using language models and ﬁne-tune\nthem to a speciﬁc task, using an auxiliary lan-\nguage modeling objective to prevent catastrophic\nforgetting. This approach, nevertheless, is only\nevaluated on machine translation tasks; moreover,\nthe seq2seq (Sutskever et al., 2014) and language\nmodeling losses are weighted equally throughout\ntraining. By contrast, we propose a weighted sum\nof losses, where the language modeling contribu-\ntion gradually decreases. ELMo embeddings (Pe-\nters et al., 2018) are obtained from language mod-\nels and improve the results in a variety of tasks\nas additional contextual representations. However,\nELMo embeddings rely on character-level models,\nwhereas our approach uses a word-level LM. They\nare, furthermore, concatenated to pretrained word\nvectors and remain ﬁxed during training. We in-\nstead propose a ﬁne-tuning procedure, aiming to\nadjust a generic architecture to different end tasks.\nMoreover, BERT (Devlin et al., 2018) pretrains\nlanguage models and ﬁne-tunes them on the tar-\nget task. An auxiliary task (next sentence predic-\ntion) is used to enhance the representations of the\nLM. BERT ﬁne-tunes masked bi-directional LMs.\nNevertheless, we are limited to a uni-directional\nmodel. Training BERT requires vast computa-\ntional resources, while our model only requires 1\nGPU. We note that our approach is not orthogo-\nnal to BERT and could be used to improve it, by\nadding an auxiliary LM objective and weighing its\ncontribution.\nTowards the same direction, ULMFiT (Howard\nand Ruder, 2018) shows impressive results on a\nvariety of tasks by employing pretrained LMs.\nThe proposed pipeline requires three distinct steps,\nthat include (1) pretraining the LM, (2) ﬁne-tuning\nit on a target dataset with an elaborate schedul-\ning procedure and (3) transferring it to a classiﬁca-\ntion model. Our proposed model is closely related\nto ULMFiT. However, ULMFiT trains a LM and\nﬁne-tunes it to the target dataset, before transfer-\nring it to a classiﬁcation model. While ﬁne-tuning\nthe LM to the target dataset, the metric (e.g. ac-\ncuracy) that we intend to optimize cannot be ob-\nserved. We propose adopting a multi-task learning\nperspective, via the addition of an auxiliary LM\nloss to the transferred model, to control the loss\nof the pretrained and the new task simultaneously.\nThe intuition is that we should avoid catastrophic\nforgetting, but at the same time allow the LM to\ndistill the knowledge of the prior data distribution\nand keep the most useful features.\nMulti-Task Learning (MTL) via hard parame-\nter sharing (Caruana, 1993) in neural networks\nhas proven to be effective in many NLP prob-\nlems (Collobert and Weston, 2008). More re-\ncently, alternative approaches have been suggested\nthat only share parameters across lower layers (So-\ngaard and Goldberg, 2016). By introducing part-\nof-speech tags at the lower levels of the network,\nthe proposed model achieves competitive results\non chunking and CCG super tagging. Our auxil-\niary language model objective follows this line of\nthought and intends to boost the performance of\nthe higher classiﬁcation layer.\n3 Our Model\nWe introduceSiATL, which stands forSingle-step\nAuxiliary loss Transfer Learning. In our proposed\napproach, we ﬁrst train a LM. We then transfer its\nweights and add a task-speciﬁc recurrent layer to\nthe ﬁnal classiﬁer. We also employ an auxiliary\nLM loss to avoid catastrophic forgetting.\nLM Pretraining.We train a word-level language\nmodel, which consists of an embedding LSTM\nlayer (Hochreiter and Schmidhuber, 1997), 2 hid-\nden LSTM layers and a linear layer. We want to\nminimize the negative log-likelihood of the LM:\nL(ˆp) =−1\nN\nN∑\nn=1\nTn\n∑\nt=1\nlogˆp(xn\nt |xn\n1 ,...,x n\nt−1) (1)\nwhere ˆp(xn\nt |xn\n1 ,...,x n\nt−1) is the distribution of the\ntth word in the nth sentence given the t−1 words\npreceding it and N is total number of sentences.\nTransfer & auxiliary loss. We transfer the\nweights of the pretrained model and add one\nLSTM with a self-attention mechanism (Lin et al.,\n2017; Bahdanau et al., 2015).\nIn order to adapt the contribution of the pretrained\nmodel to the task at hand, we introduce an auxil-\niary LM loss during training. The joint loss is the\nFigure 1: High-level overview of our proposed TL ar-\nchitecture. We transfer the pretrained LM add an extra\nrecurrent layer and an auxiliary LM loss.\nweighted sum of the task-speciﬁc loss Ltask and\nthe auxiliary LM loss LLM , where γ is a weight-\ning parameter to enable adaptation to the target\ntask but at the same time keep the useful knowl-\nedge from the source task. Speciﬁcally:\nL= Ltask + γLLM (2)\nExponential decay ofγ. An advantage of the pro-\nposed TL method is that the contribution of the\nLM can be explicitly controlled in each training\nepoch. In the ﬁrst few epochs, the LM should con-\ntribute more to the joint loss of SiATL so that the\ntask-speciﬁc layers adapt to the new data distribu-\ntion. After the knowledge of the pretrained LM\nis transferred to the new domain, the task-speciﬁc\ncomponent of the loss function is more important\nand γ should become smaller. This is also crucial\ndue to the fact that the new, task-speciﬁc LSTM\nlayer is randomly initialized. Therefore, by back-\npropagating the gradients of this layer to the pre-\ntrained LM in the ﬁrst few epochs, we would add\nnoise to the pretrained representation. To avoid\nthis issue, we choose to initially pay attention to\nthe LM objective and gradually focus on the clas-\nsiﬁcation task. In this paper, we use an exponential\ndecay for γover the training epochs.\nSequential Unfreezing.Instead of ﬁne-tuning all\nthe layers simultaneously, we propose unfreezing\nthem sequentially, according to Howard and Ruder\n(2018); Chronopoulou et al. (2018). We ﬁrst ﬁne-\ntune only the extra, randomly initialized LSTM\nand the output layer for n−1 epochs. At the nth\nepoch, we unfreeze the pretrained hidden layers.\nWe let the model ﬁne-tune, until epoch k−1. Fi-\nnally, at epoch k, we also unfreeze the embedding\nlayer and let the network train until convergence.\nThe values of n and k are obtained through grid\nsearch. We ﬁnd the sequential unfreezing scheme\nimportant, as it minimizes the risk of overﬁtting to\nsmall datasets.\nOptimizers. While pretraining the LM, we use\nStochastic Gradient Descent (SGD). When we\ntransfer the LM and ﬁne-tune on each classiﬁca-\ntion task, we use 2 different optimizers: SGD for\nthe pretrained LM (embedding and hidden layer)\nwith a small learning rate, in order to preserve its\ncontextual information. As for the new, randomly\ninitialized LSTM and classiﬁcation layers, we em-\nploy Adam (Kingma and Ba, 2015), in order to al-\nlow them to train fast and adapt to the target task.\nDataset Domain # classes # examples\nIrony18 Tweets 4 4618\nSent17 Tweets 3 61854\nSCv2 Debate Forums 2 3260\nSCv1 Debate Forums 2 1995\nPsychExp Experiences 7 7480\nTable 1: Datasets used for the downstream tasks.\n4 Experiments and Results\n4.1 Datasets\nTo pretrain the language model, we collect a\ndataset of 20 million English Twitter messages,\nincluding approximately 2M unique tokens. We\nuse the 70K most frequent tokens as vocabu-\nlary. We evaluate our model on ﬁve datasets:\nSent17 for sentiment analysis (Rosenthal et al.,\n2017), PsychExp for emotion recognition (Wall-\nbott and Scherer, 1986), Irony18 for irony detec-\ntion (Van Hee et al., 2018), SCv1 and SCv2 for\nsarcasm detection (Oraby et al., 2016; Lukin and\nWalker, 2013). More details about the datasets can\nbe found in Table 1.\n4.2 Experimental Setup\nTo preprocess the tweets, we use Ekphra-\nsis (Baziotis et al., 2017). For the generic datasets,\nwe use NLTK (Loper and Bird, 2002). For the\nNBoW baseline, we use word2vec (Mikolov et al.,\n2013) 300-dimensional embeddings as features.\nIrony18 Sent17 SCv2 SCv1 PsychExp\nBoW 43.7 61.0 65.1 60.9 25.8\nNBoW 45.2 63.0 61.1 51.9 20.3\nP-LM 42.7 ± 0.6 61.2 ± 0.7 69.4 ± 0.4 48.5 ± 1.5 38.3 ± 0.3\nP-LM + su 41.8 ± 1.2 62.1 ± 0.8 69.9 ± 1.0 48.4 ± 1.7 38.7 ± 1.0\nP-LM + aux 45.5 ± 0.9 65.1 ± 0.6 72.6 ± 0.7 55.8 ± 1.0 40.9 ± 0.5\nSiATL (P-LM + aux + su) 47.0 ± 1.1 66.5 ± 0.2 75.0 ± 0.7 56.8 ± 2.0 45.8 ± 1.6\nULMFiT (Wiki-103) 23.6 ± 1.6 60.5 ± 0.5 68.7 ± 0.6 56.6 ± 0.5 21.8 ± 0.3\nULMFiT (Twitter) 41.6 ± 0.7 65.6 ± 0.4 67.2 ± 0.9 44.0 ± 0.7 40.2 ± 1.1\nState of the art 53.6 68.5 76.0 69.0 57.0\n(Baziotis et al., 2018) (Cliche, 2017) (Ilic et al., 2018) (Felbo et al., 2017)\nTable 2: Ablation study on various downstream datasets. Average over ﬁve runs with standard deviation. BoW\nstands for Bag of Words, NBoW for Neural Bag of Words. P-LM stands for a classiﬁer initialized with our\npretrained LM, su for sequential unfreezing and aux for the auxiliary LM loss. In all cases, F1 is employed.\nFor the neural models, we use an LM with an em-\nbedding size of 400, 2 hidden layers, 1000 neurons\nper layer, embedding dropout 0.1, hidden dropout\n0.3 and batch size 32. We add Gaussian noise of\nsize 0.01 to the embedding layer. A clip norm of\n5 is applied, as an extra safety measure against ex-\nploding gradients. For each text classiﬁcation neu-\nral network, we add on top of the transferred LM\nan LSTM layer of size 100 with self-attention and\na softmax classiﬁcation layer. In the pretraining\nstep, SGD with a learning rate of 0.0001 is em-\nployed. In the transferred model, SGD with the\nsame learning rate is used for the pretrained layers.\nHowever, we use Adam (Kingma and Ba, 2015)\nwith a learning rate of 0.0005 for the newly added\nLSTM and classiﬁcation layers. For developing\nour models, we use PyTorch (Paszke et al., 2017)\nand Scikit-learn (Pedregosa et al., 2011).\n5 Results & Discussion\nBaselines and Comparison.Table 2 summarizes\nour results. The top two rows detail the baseline\nperformance of the BoW and NBoW models. We\nobserve that when enough data is available (e.g.\nSent17), baselines provide decent results. Next,\nthe results for the generic classiﬁer initialized from\na pretrained LM (P-LM) are shown with and with-\nout sequential unfreezing, followed by the results\nof the proposed model SiATL. SiATL is also di-\nrectly compared with its close relative ULMFiT\n(trained on Wiki-103 or Twitter) and the state-of-\nthe-art for each task; ULMFiT also ﬁne-tunes a\nLM for classiﬁcation tasks. The proposed SiATL\nmethod consistently outperforms the baselines, the\nP-LM method and ULMFiT in all datasets. Even\nthough we do not perform any elaborate learn-\ning rate scheduling and we limit ourselves to pre-\ntraining in Twitter, we obtain higher results in two\nTwitter datasets and three generic.\nAuxiliary LM objective.The effect of the auxil-\niary objective is highlighted in very small datasets,\nsuch as SCv1, where it results in an impressive\nboost in performance (7%). We hypothesize that\nwhen the classiﬁer is simply initialized with the\npretrained LM, it overﬁts quickly, as the target vo-\ncabulary is very limited. The auxiliary LM loss,\nhowever, permits reﬁned adjustments to the model\nand ﬁne-grained adaptation to the target task.\nExponential decay ofγ. For the optimal γ in-\nterval, we empirically ﬁnd that exponentially de-\ncaying γfrom 0.2 to 0.1 over the number of train-\ning epochs provides best results for our classiﬁca-\ntion tasks. A heatmap of γis depicted in Figure 3.\nWe observe that small values of γ should be em-\nployed, in order to scale the LM loss in the same\norder of magnitude as the classiﬁcation loss over\nthe training period. Nevertheless, the use of ex-\nponential decay instead of linear decay does not\nprovide a signiﬁcant improvement, as our model\nis not sensitive to the way of decaying hyperpa-\nrameter γ.\nSequential Unfreezing. Results show that se-\nquential unfreezing is crucial to the proposed\nmethod, as it allows the pretrained LM to adapt\nto the target word distribution. The performance\nimprovement is more pronounced when there is\na mismatch between the LM and task domains,\ni.e., the non-Twitter domain tasks. Speciﬁcally\nfor the PsychExp and SCv2 datasets, sequentially\nunfreezing yields signiﬁcant improvement in F1\nbuilding upon our intuition.\nNumber of training examples.Transfer learning\nis particularly useful when limited training data\nare available. We notice that for our largest dataset\nFigure 2: Results of SiATL, our proposed approach\n(continuous lines) and ULMFiT (dashed lines) for dif-\nferent datasets (indicated by different markers) as a\nfunction of the number of training examples.\nSent17, SiATL outperforms ULMFiT only by a\nsmall margin when trained on all the training ex-\namples available (see Table 2), while for the small\nSCv2 dataset, SiATL outperforms ULMFiT by a\nlarge margin and ranks very close to the state-of-\nthe-art model (Ilic et al., 2018). Moreover, the\nperformance of SiATL vs ULMFiT as a function\nof the training dataset size is shown in Figure 2.\nNote that the proposed model achieves competi-\ntive results on less than 1000 training examples for\nthe Irony18, SCv2, SCv1and PsychExp datasets,\ndemonstrating the robustness of SiATL even when\ntrained on a handful of training examples.\nCatastrophic forgetting.We observe that SiATL\nindeed provides a way of mitigating catastrophic\nforgetting. Empirical results that are shown in Ta-\nble 2 indicate that by only adding the auxiliary lan-\nguage modeling objective, we obtain better results\non all downstream tasks. Speciﬁcally, a compari-\nson of theP-LM + auxmodel and theP-LM model\nshows that the performance of SiATL on classiﬁ-\ncation tasks is improved by the auxiliary objective.\nWe hypothesize that the language model objective\nacts as a regularizer that prevents the loss of the\nmost generalizable features.\n6 Conclusions and Future Work\nWe introduce SiATL, a simple and efﬁcient trans-\nfer learning method for text classiﬁcation tasks.\nOur approach is based on pretraining a LM and\nFigure 3: Heatmap of the effect of γto F1-score, eval-\nuated on SCv2. The horizontal axis depicts the initial\nvalue of γand the vertical axis the ﬁnal value of γ.\ntransferring its weights to a classiﬁer with a task-\nspeciﬁc layer. The model is trained using a task-\nspeciﬁc functional with an auxiliary LM loss.\nSiATL avoids catastrophic forgetting of the lan-\nguage distribution learned by the pretrained LM.\nExperiments on various text classiﬁcation tasks\nyield competitive results, demonstrating the efﬁ-\ncacy of our approach. Furthermore, our method\noutperforms more sophisticated transfer learning\napproaches, such as ULMFiT in all tasks.\nIn future work, we plan to move from Twitter to\nmore generic domains and evaluate our approach\nto more tasks. Additionally, we aim at exploring\nways for scaling our approach to larger vocabu-\nlary sizes (Kumar and Tsvetkov, 2019) and for\nbetter handling of out-of-vocabulary words (OOV)\n(Mielke and Eisner, 2018; Sennrich et al., 2015) in\norder to be applicable to diverse datasets.\nFinally, we want to explore approaches for im-\nproving the adaptive layer unfreezing process and\nthe contribution of the language model objective\n(value of γ) to the target task.\nAcknowledgments\nWe would like to thank Katerina Margatina and\nGeorgios Paraskevopoulos for their helpful sug-\ngestions and comments. This work has been par-\ntially supported by computational time granted\nfrom the Greek Research & Technology Network\n(GR-NET) in the National HPC facility - ARIS.\nAlso, the authors would like to thank NVIDIA for\nsupporting this work by donating a TitanX GPU.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe International Conference on Learning Represen-\ntations, San Diego, California.\nChristos Baziotis, Athanasiou Nikolaos, Pinelopi\nPapalampidi, Athanasia Kolovou, Georgios\nParaskevopoulos, Nikolaos Ellinas, and Alexandros\nPotamianos. 2018. Ntua-slp at semeval-2018 task\n3: Tracking ironic tweets using ensembles of word\nand character level attentive rnns. In Proceedings\nof the 12th International Workshop on Semantic\nEvaluation (SemEval-2018), pages 613–621, New\nOrleans, Louisiana.\nChristos Baziotis, Nikos Pelekis, and Christos Doulk-\neridis. 2017. Datastories at semeval-2017 task\n4: Deep lstm with attention for message-level and\ntopic-based sentiment analysis. In Proceedings of\nthe 11th International Workshop on Semantic Eval-\nuation (SemEval-2017), pages 747–754, Vancouver,\nCanada.\nRich Caruana. 1993. Multitask learning: A\nknowledge-based source of inductive bias. In Ma-\nchine Learning: Proceedings of the Tenth Interna-\ntional Conference, pages 41–48.\nAlexandra Chronopoulou, Aikaterini Margatina, Chris-\ntos Baziotis, and Alexandros Potamianos. 2018.\nNtua-slp at iest 2018: Ensemble of neural transfer\nmethods for implicit emotion classiﬁcation. In Pro-\nceedings of the 9th Workshop on Computational Ap-\nproaches to Subjectivity, Sentiment and Social Me-\ndia Analysis, pages 57–64, Brussels, Belgium.\nMathieu Cliche. 2017. Bb_twtr at semeval-2017 task\n4: Twitter sentiment analysis with cnns and lstms. In\nProceedings of the 11th International Workshop on\nSemantic Evaluation (SemEval-2017), pages 573–\n580, Vancouver, Canada.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the International Conference on Ma-\nchine learning, pages 160–167.\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research,\npages 2493–2537.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Proceedings of the Advances\nin Neural Information Processing Systems, pages\n3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBjarke Felbo, Alan Mislove, Anders Sogaard, Iyad\nRahwan, and Sune Lehmann. 2017. Using millions\nof emoji occurrences to learn any-domain represen-\ntations for detecting sentiment, emotion and sar-\ncasm. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1615–1625, Copenhagen, Denmark.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, (8):1735–\n1780.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁca-\ntion. In Proceedings of the Annual Meeting of the\nACL, pages 328–339, Melbourne, Australia.\nSuzana Ilic, Edison Marrese-Taylor, Jorge A. Bal-\nazs, and Yutaka Matsuo. 2018. Deep contextu-\nalized word representations for detecting sarcasm\nand irony. In Proceedings of the 9th Workshop\non Computational Approaches to Subjectivity, Sen-\ntiment and Social Media Analysis, pages 2–7.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the International Conference on Learning Repre-\nsentations.\nSachin Kumar and Yulia Tsvetkov. 2019. V on mises-\nﬁsher loss for training sequence to sequence models\nwith continuous outputs. In International Confer-\nence on Learning Representations.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. arXiv preprint arXiv:1703.03130.\nEdward Loper and Steven Bird. 2002. Nltk: The natu-\nral language toolkit. In Proceedings of the ACL-02\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Compu-\ntational Linguistics, pages 63–70.\nStephanie Lukin and Marilyn Walker. 2013. Really?\nwell. apparently bootstrapping improves the perfor-\nmance of sarcasm and nastiness classiﬁers for online\ndialogue. In Proceedings of the Workshop on Lan-\nguage Analysis in Social Media, pages 30–40, At-\nlanta, Georgia.\nSebastian J. Mielke and Jason Eisner. 2018. Spell once,\nsummon anywhere: A two-level open-vocabulary\nlanguage model. CoRR, abs/1804.08205.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Proceedings of the Advances in Neural Infor-\nmation Processing Systems, pages 3111–3119.\nShereen Oraby, Vrindavan Harrison, Lena Reed,\nErnesto Hernandez, Ellen Riloff, and Marilyn A.\nWalker. 2016. Creating and characterizing a diverse\ncorpus of sarcasm in dialogue. In Proceedings of the\nSIGDIAL 2016 Conference, The 17th Annual Meet-\ning of the Special Interest Group on Discourse and\nDialogue, pages 31–41.\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nFabian Pedregosa, Gael Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, et al. 2011. Scikit-learn:\nMachine learning in python. Journal of machine\nlearning research, pages 2825–2830.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1532–1543, Doha, Qatar.\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proceedings of the Annual Meeting of the ACL,\npages 1756–1765, Vancouver, Canada.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the Conference of\nthe NAACL:HLT, pages 2227–2237, New Orleans,\nLouisiana.\nPrajit Ramachandran, Peter Liu, and Quoc Le. 2017.\nUnsupervised pretraining for sequence to sequence\nlearning. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing,\npages 383–391, Copenhagen, Denmark.\nSara Rosenthal, Noura Farra, and Preslav Nakov.\n2017. Semeval-2017 task 4: Sentiment analysis in\ntwitter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages 502–518, Vancouver, Canada.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nAnders Sogaard and Yoav Goldberg. 2016. Deep\nmulti-task learning with low level tasks supervised\nat lower layers. In Proceedings of the Annual Meet-\ning of the ACL, pages 231–235, Berlin, Germany.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Proceedings of the Advances in Neural\nInformation Processing Systems, pages 3104–3112.\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. Semeval-2018 task 3: Irony detection in en-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2018), pages 39–50, New Orleans, Louisiana.\nHarald G. Wallbott and Klaus R. Scherer. 1986. How\nuniversal and speciﬁc is emotional experience? ev-\nidence from 27 countries on ﬁve continents. In-\nformation (International Social Science Council),\n(4):763–795.\nCaiming Xiong, Victor Zhong, and Richard Socher.\n2016. Dynamic coattention networks for question\nanswering.",
  "topic": "Transfer of learning",
  "concepts": [
    {
      "name": "Transfer of learning",
      "score": 0.8013759851455688
    },
    {
      "name": "Computer science",
      "score": 0.7986684441566467
    },
    {
      "name": "Forgetting",
      "score": 0.7907018661499023
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6576850414276123
    },
    {
      "name": "Task (project management)",
      "score": 0.6173040270805359
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5860275626182556
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5577538013458252
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5525571703910828
    },
    {
      "name": "Process (computing)",
      "score": 0.5158745050430298
    },
    {
      "name": "Language model",
      "score": 0.5121233463287354
    },
    {
      "name": "Natural language processing",
      "score": 0.46860846877098083
    },
    {
      "name": "Machine learning",
      "score": 0.4633977711200714
    },
    {
      "name": "Transfer of training",
      "score": 0.42274805903434753
    },
    {
      "name": "Multi-task learning",
      "score": 0.42029470205307007
    },
    {
      "name": "Engineering",
      "score": 0.07617822289466858
    },
    {
      "name": "Cognitive psychology",
      "score": 0.07068446278572083
    },
    {
      "name": "Programming language",
      "score": 0.06780815124511719
    },
    {
      "name": "Psychology",
      "score": 0.06722098588943481
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Knowledge management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174458059",
      "name": "National Technical University of Athens",
      "country": "GR"
    }
  ],
  "cited_by": 19
}