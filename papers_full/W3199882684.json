{
    "title": "Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?",
    "url": "https://openalex.org/W3199882684",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4225070944",
            "name": "Rozanova, Julia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4209587674",
            "name": "Ferreira, Deborah",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Dubba, Krishna",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2234675106",
            "name": "Cheng Weiwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2756805886",
            "name": "Zhang Dell",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4209587675",
            "name": "Freitas, Andre",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3041421885",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2765874585",
        "https://openalex.org/W108987309",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3121854931",
        "https://openalex.org/W2170329722",
        "https://openalex.org/W3104953317",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W3099178230",
        "https://openalex.org/W2952502547",
        "https://openalex.org/W2121892411",
        "https://openalex.org/W2914924671",
        "https://openalex.org/W2889037901"
    ],
    "abstract": "Models designed for intelligent process automation are required to be capable of grounding user interface elements. This task of interface element grounding is centred on linking instructions in natural language to their target referents. Even though BERT and similar pre-trained language models have excelled in several NLP tasks, their use has not been widely explored for the UI grounding domain. This work concentrates on testing and probing the grounding abilities of three different transformer-based models: BERT, RoBERTa and LayoutLM. Our primary focus is on these models' spatial reasoning skills, given their importance in this domain. We observe that LayoutLM has a promising advantage for applications in this domain, even though it was created for a different original purpose (representing scanned documents): the learned spatial features appear to be transferable to the UI grounding setting, especially as they demonstrate the ability to discriminate between target directions in natural language instructions.",
    "full_text": "Grounding Natural Language Instructions: Can Large Language Models Capture\nSpatial Information?\nJulia Rozanova*1, Deborah Ferreira*1, Krishna Dubba3,\nWeiwei Cheng3, Dell Zhang4, Andre Freitas12\n1Department of Computer Science, University of Manchester, UK\n2Idiap Research Institute, Switzerland\n3Blue Prism AI Labs, London, UK\n4ByteDance AI Lab, London, UK\n{julia.rozanova, deborah.ferreira, andre.freitas}@manchester.ac.uk, {krishna.dubba,\nweiwei.cheng}@blueprism.com, dell.z@ieee.org\nAbstract\nModels designed for intelligent process automation are re-\nquired to be capable of grounding user interface elements.\nThis task of interface element grounding is centred on link-\ning instructions in natural language to their target referents.\nEven though BERT and similar pre-trained language mod-\nels have excelled in several NLP tasks, their use has not been\nwidely explored for the UI grounding domain. This work con-\ncentrates on testing and probing the grounding abilities of\nthree different transformer-based models: BERT, RoBERTa\nand LayoutLM. Our primary focus is on these models’ spa-\ntial reasoning skills, given their importance in this domain.\nWe observe that LayoutLM has a promising advantage for\napplications in this domain, even though it was created for a\ndifferent original purpose (representing scanned documents):\nthe learned spatial features appear to be transferable to the UI\ngrounding setting, especially as they demonstrate the ability\nto discriminate between target directions in natural language\ninstructions.\nIntroduction\nThe majority of human-computer interactions are mediated\nby Graphical User Interfaces (GUIs) within desktop and mo-\nbile environments. GUIs provide the interaction backbone in\nwhich end-users compose and repurpose atomic functional\nelements embedded within existing applications.\nRecently, there is an emerging effort to build models to\nsupport the automation of long-tail processes within a multi-\napplication Desktop/GUI environment (Ferreira et al. 2020).\nThis effort encompasses the interfacing of Human-Desktop\nInteraction (HDI) with natural language, both for HDI un-\nderstanding (the creation of natural language descriptions\nthat describe HDIs) and HDI semantic parsing (the execu-\ntion of HDI actions from commands expressed in natural\nlanguage).\nOne fundamental skill required for designing such mod-\nels is being able to ground UI element mentions in natu-\nral language instructions, in the style of a mapping task. A\n*Equal contribution.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nnatural language command in most cases will contain an\naction verb. Often, this verb has as an argument an object\nthat refers to an interface element. A model geared towards\nunderstanding this instruction should be aware of the map-\nping between words and interface elements’ concrete reali-\nsation. This mapping is named interface element grounding\ntask (Pasupat et al. 2018) or UI grounding. Consider the in-\nstruction “Click on the cancel button.”: in order to ground\nthis instruction, one is expected to ﬁnd the cancel button in\na screen. Applications of a UI grounding model ranges from\nintelligent process automation to the creation of accessibility\ntools.\nTransformer-based models have achieved excellent per-\nformance in tasks requiring semantic inference, with state\nof the art results in various NLP tasks. However, such mod-\nels’ expertise in reasoning with elements positioned in a 2-\ndimensional space has limited research, especially in the in-\nterface element grounding domain.\nThis work focuses on testing and probing the grounding\nabilities of three different transformer-based models, namely\nBERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), and\nLayoutLM (Xu et al. 2020). This process is performed in\ntwo parts: ﬁrst, evaluating the accuracy for UI grounding for\ndifferent reasoning types, and second, probing the generated\nrepresentations using spatial auxiliary tasks.\nThe contributions of this work can be summarised as fol-\nlows:\n• We provide a ﬁrst systematic study of the spatial reason-\ning capabilities of transformer-based models in the UI\ngrounding setting.\n• We compare different pre-trained transformer-based ar-\nchitectures and present a probing-based analysis of their\nspatial reasoning capabilities.\n• We empirically demonstrate that the semantic representa-\ntion of textual and layout information from scanned doc-\numents can also be successfully transferred to represent\nuser interfaces.\n• We propose a novel application domain for transformer-\nbased models.\narXiv:2109.08634v1  [cs.CL]  17 Sep 2021\n• We demonstrate the need for more complex spatial rea-\nsoning datasets and models capable of relative spatial\nreasoning.\nProblem Formulation: Grounding Interface\nElements\nGiven a command (noun phrase) c contained in a natural\nlanguage command and an user interface composed of a set\nU = {u1, u2, . . . , u2}of interface elements, where each el-\nement i is composed of a given text contentuiT and a bound-\ning box uiBB = [uix0 , uix1 , uiy0 , uiy1 ], the grounding task\naims to select the correct element ui ∈U which is referred\nin phrase c, assuming that this phrase only refers to a unique\nelement.\nGiven U and c, the following condition distribution over the\nelements is deﬁned:\np(ui|p) ∝exp[s(f(c), g(ui))] (1)\nwhere g(·) and f(·) are embedding functions for the UI\nelements and phrase, respectively, and s(·, ·) is a scoring\nfunction between command and UI element. To obtain the\ncorrect interface element, one needs to maximise the log-\nlikelihood for the correct interface element.\nTo solve the grounding task, a model needs mainly to fo-\ncus on three types of reasoning (Li et al. 2020; Pasupat et al.\n2018), subject to what is required by the command c. The\ntypes of reasoning are extractive, absolute spatial and rela-\ntive spatial.\nExtractive reasoning is required in the most trivial cases,\nwhere one needs only to match the textual contentuiT of the\ninterface element with the text in c. This scenario includes\ncases where there is an exact keyword matching and also\nwhere some paraphrasing is required. As shown in example\n(a) from Figure 1, for this type of reasoning, a model does\nnot require any spatial information about the interface ele-\nment.\nWhen spatial reasoning is needed, the task increases in\ncomplexity. Absolute spatial reasoning requires a model to\nunderstand the positional information contained in the com-\nmand and to be able to locate the interface element inside a\nscreen. Figure 1(b) presents an example of this case, where\nit is not possible to identify the element without having the\nvalues of its bounding box. However, it does not require the\nrepresentation of surrounding interface elements, unlike rel-\native spatial reasoning.\nRelative spatial reasoning is the most challenging type\nconsidering a model is expected to interpret a relative spa-\ntial command, demanding an understanding of the screen as\na whole. In this type of command, an interface element is\nreferred to in relation to another element. As shown in Fig-\nure 1(c), this type of instruction can be easily mistaken with\nan extractive reasoning command in circumstances where a\nmodel is not knowledgeable of the spatial nature of the task\ngiven then extra noise with textual information from a dif-\nferent element.\nExtractive reasoning\nClick on the button complete verification(a)\nRelative spatial reasoning\nClick on the button under the button complete verification(b)\nAbsolute spatial reasoning\nClick on the item at the bottom left of the screen(c)\nFigure 1: Example of three different commands, requiring\ndifferent types of reasoning, and matching interface ele-\nments in a mobile screen. The screen presented here is part\nof the Rico dataset (Deka et al. 2017).\nRelated Work\nUI Grounding\nPast research on grounding interface elements has mainly fo-\ncused on applications to mobile screens and web elements.\nLi et al. (2020) proposes Seq2Act, a two-stage Transformer\nmodel for grounding UI elements in an Android environ-\nment. In our work, we compare pre-trained models with the\nSeq2Act model. Pasupat et al. (2018) proposes an embed-\nding based model for grounding instructions based on web\nelements. They are able to represent not only positional in-\nformation but also properties of UI elements.\nThere is still limited research in the task of interface ele-\nment grounding. However, a considerable amount of litera-\nture has been published on semantic parsing for task execu-\ntion. Goldwasser and Roth (2011) and V olkova et al. (2013)\nfocus on using natural language instructions to learn the con-\ncepts and rules of an environment, where these elements can\nbe used to execute and verify tasks. Similarly, the work by\nBranavan, Silver, and Barzilay (2012) use text from manuals\nto learn the rules of the game Civilization II.\nFor goal-oriented tasks, the parsing can be modelled\naround a reward/punishment function. Branavan, Zettle-\nmoyer, and Barzilay (2010) explicitly models an environ-\nment where a reward function is used to evaluate how well a\ncommand sequence achieves a described task. MacGlashan\net al. (2014) propose a learning algorithm for generating\nhigh-level task deﬁnitions from instruction, derived from on-\nline human-delivered reward and punishment.\nProbing\nThere is often interest in the intermediate features or infor-\nmation captured by neural models. There are two broad cate-\ngories of approaches to determining the extent to which cer-\ntain features or phenomena are captured: either behavioural\nanalysis or structural analysis. The common practise of eval-\nuating a model’s accuracy on a targeted challenge dataset\n(designed to be representative of the feature or phenomenon\nin question) is a form of behavioural analysis. On the other\nhand, structural analysis concerns the structural properties\nof the representation space, such as emergent feature clus-\nters or geometric regularities.\nA recently popular style of structural analysis is method-\nical diagnostic classiﬁcation or probing1. First introduced\nin Alain and Bengio (2018) as diagnostic classiﬁcation ,\nprobing has been popular in the study of natural language\nprocessing models, where it has shown the prevalence of\nrich syntactic features (for example in Hewitt and Manning\n(2019a)) and semantic features (Vuli ´c et al. 2020). Given\nthat the aim of probing studies is very different from the\nusual machine learning goal of achieving the maximum pos-\nsible accuracy on test sets, there has been much discussion\non the suggested best practices and possible pitfalls when\ncarrying out probing studies (Zhang and Bowman 2018; He-\nwitt and Liang 2019; V oita and Titov 2020; Pimentel et al.\n2020c,a). We attempt to take on most of the suggestions in\nour methodology, especially the control tasks and selectiv-\nity measure in Hewitt and Liang (2019) and the complex-\nity ranges in Hewitt and Manning (2019a); Pimentel et al.\n(2020b).\nExperimental Setup\nThe experiments in this work are separated into two parts.\nThe ﬁrst part evaluates the accuracy of distinct models on\nthe task of grounding UI elements for mobile devices, while\nthe second part assesses the spatial reasoning capabilities of\nthe models trained during the ﬁrst stage. All the datasets gen-\nerated, trained models and code used in this work are made\navailable in http://github.com/debymf/ipa probing.\nThe stages of the experiments are described in the follow-\ning subsections.\n1The term probing is sometimes also used for studies featuring\nonly behavioural analyses, but we refer speciﬁcally to the method-\nology surrounding auxiliary classiﬁcation on top of intermediate\nmodel representations, as in Alain and Bengio (2018); Hewitt and\nManning (2019a) and Pimentel et al. (2020c)\nInterface Element Grounding\nDatasets For this work, we focused on datasets containing\ncommands for grounding mobile UI elements. This choice\nwas due to the more extensive availability of datasets, un-\nlike desktop applications. However, the concepts and ex-\nperiments presented here can easily be transported to dif-\nferent interface environments, given their similar structure.\nThe datasets used for our experiments are PixelHelp and Ri-\ncoSCA (Li et al. 2020).\nRicoSCA is a dataset containing commands with its re-\nspective matching interface element, composed of textual\ncontent and bounding box. The dataset contains a total\nof 329,411 instructions 2 which we split between train/de-\nv/set as shown in Table 1. RicoSCA contains three types\nof commands, Name-Type, Absolute-Location and Relative-\nLocation, which we map to extractive, spatial absolute and\nspatial relative reasoning, respectively.\nPixelHelp contains 187 multi-step instructions, where 88\nare general tasks, such as conﬁguring accounts, 38 are e-\nmail tasks, 31 are browser tasks, and 30 photos related tasks.\nPixelHelp is only used for testing, given its smaller size. In\nthis work, we focus only on single-step commands; there-\nfore, PixelHelp cannot be used in its original format. To ob-\ntain single-step commands from the multi-step ones found in\nPixelHelp, we used the action phrase extraction model pro-\nposed by Li et al. (2020). We obtained a total of 780 single\nstep commands. This dataset does not contain annotations on\nthe type of reasoning required for each command. To gen-\nerate such data, two expert human annotators enriched the\ndataset with the correct types, obtaining a .78 Kappa score,\nproviding strong reliability. The details of this dataset are\nalso shown in Table 1.\nDataset Number of Instructions\nExtractive Absolute Relative Total\nRicoSCA - Train 100,921 6,531 57,222 164,674\nRicoSCA - Dev 40,504 2,529 22,577 65,610\nRicoSCA - Test 60,922 3,897 34,308 99,127\nPixelHelp 673 100 7 780\nTable 1: Number of instructions for each dataset, indicating\nthe type of reasoning required for grounding.\nModels For the grounding of mobile UI elements, we\ncompare the performance of four different models: BERT\n(base), RoBERTa (base), LayoutLM (base) and Seq2Act.\nBERT (Devlin et al. 2019) and RoBERTa (Liu et al. 2019)\nare known to obtain excellent results in tasks where di-\nrect semantic similarity is required. However, there is still\nno conclusive evidence that this model can reason spa-\ntially, especially for the task of grounding UI elements.Lay-\noutLM (Xu et al. 2020) is a ”Language” Model for the com-\n2The number of instructions is different from the reported value\nin the original paper since only the generation script is provided,\nand that does not ensure that the same dataset is generated. In order\nto ensure reproducibility, we will make available our version of the\ndataset with respective splits.\nbination of text and document layout, geared towards doc-\nument image understanding tasks. This model takes as in-\nput a bounding box that denotes the relative position of a\ntoken within a document. This positional embedding can\ncapture the relationship among tokens within a document.\nLayoutLM was initially proposed as a pre-trained model for\nphysical documents, making this work a novel application\nfor such a model. This model can also take as input images\nrepresenting a layout structure; however, we only consider\nthe interface elements’ positional features as input in this\nwork.\nIn order to apply such pre-trained models, we reformu-\nlate the interface element grounding problem as a pairwise\nrelevance classiﬁcation problem. Given a natural language\ncommand c and interface elements U = {u1, . . . , un}, a\nfunction f(c, ui) is deﬁned, where f(c, ui) = 1if ui is the\ntarget element in c and f(c, ui) = 0otherwise.\nBERT and RoBERTa take as input the commandc and the\ntext uiT from the interface element being tested in the format\n[c||uiT ]. These models cannot receive the element’s position\nas input since they cannot interpret such a structure. On the\nother hand, LayoutLM also takes as input the bounding box\nuiBB of the interface element, hence, it uses two different\ninputs: the textual information [c||uiT ], in the same manner\nas BERT, and the positional information[uiBB ].\nThese models are trained using the RicoSCA training and\ndevelopment set and tested for the PixelHelp and RicoSCA\ntesting sets. The supporting pipelines for inferring the cor-\nrect interface element using these models are illustrated in\nFigure 2.\nBoth models are trained 3 with the objective to minimise\nCross Entropy loss for the pair classiﬁcation task, so that the\nclassiﬁer is trained on the conditional likelihood p(ui |c)\nand select the UI element arg maxui∈U p(ui |c)\nThe last compared model isSeq2Act (Li et al. 2020), com-\nposed of two different parts, an action phrase-extraction step\nand a grounding step. The ﬁrst stage is a Transformer ar-\nchitecture which maps multi-step instructions to single-step\nones. The grounding step matches these single-step com-\nmands to UI objects using another Transformer that con-\ntextually represents UI objects, and grounds object descrip-\ntions. Given the focus of this work, we are only interested in\nthe results from the grounding stage. Seq2Act is trained us-\ning the full RicoSCA dataset; therefore, it was necessary to\nretrain the model using only the training split for RicoSCA.\nGiven that the model was trained on a smaller split of the\nRicoSCA dataset, the obtained results differed slightly from\nthe original paper.\nProbing Methodology\nFollowing recent works on the developing best practices in\nprobing studies (Hewitt and Liang 2019; Hewitt and Man-\nning 2019b; Pimentel et al. 2020c,a; V oita et al. 2019), we\nhave followed three key principles:\n3The models are trained with a learning rate of 1e-5, batch size\nof 16, a maximum sequence length of 256 and 5 epochs. Using 4\nTesla V100s, both models take around 21 hours to complete train-\ning.\n1. Prioritising probe families with simple structure,\n2. Using a range of probing model complexities with in a\nfamily of probing models, and\n3. Using randomised control tasks (Hewitt and Liang 2019)\nin order to ensure maximum selectivity for reported accu-\nracies.\nProbe Complexity Control For each auxilliary task, an\nMLP probe architectures, training 50 probes of varying com-\nplexities using the Probe-Ably framework of (Ferreira et al.\n2021)4 with the default conﬁgurations for the MLP model.\nWe use the number of parameters as a naive approximation\nof model complexity.\nControl Task and Selectivity Measure\nFor the a given set of probe hyperparameters, we train the\nprobes on a 30 −20 −50 train-dev-test split of the auxiliary\ntask data. For each fully trained probe, we report both the\ntest accuracy and the selectivity measure (Hewitt and Liang\n2019) in its “entropic form”: namely, the difference between\nthe cross-entropy loss of the auxiliary task test set and the\ncross-entropy loss of the randomised control task training\nset. We use a simple control task where input representation\nvectors have been assigned ﬁxed random labels from among\nthe auxiliary task target classes. Tracking the selectivity en-\nsures that we are not using a probe that is complex enough\nto be overly expressive to the point of having the capacity to\noverﬁt the randomised control training set.\nAuxiliary tasks Towards probing the representation gen-\nerated by LayoutLM, BERT and RoBERTa, we deﬁne four\ndifferent auxiliary tasks, two for each spatial reasoning type:\n• Absolute spatial reasoning probing(Figure 3a): Given\nthe representation of a command and an interface ele-\nment, the ﬁrst task is determining if the element is lo-\ncated at the bottom or top of the screen ( AT1) while the\nsecond task focus on determining if the element is at the\nleft or the right of the screen (AT2).\n• Relative spatial reasoning probing(Figure 3b): Given\nthe representation of a command and an interface ele-\nment, the third task requires positioning the interface el-\nement at the top or bottom of the element targeted by the\ncommand (AT3). Similarly, the last task indicates if the\nelement is at the target’s left or right (AT4).\nResults and Discussion\nThis section presents the results for both parts of this work,\nthe interface element grounding and the spatial reasoning\nprobing.\nUI Grounding Performance\nTable 2 presents the accuracy of all models for the task\nof interface element grounding. The results for the Ri-\ncoSCA dataset are separated by the type of reasoning re-\nquired: extractive reasoning (Ext), absolute spatial reason-\ning (Abs) and relative spatial reasoning (Rel). We choose\n4https://github.com/ai-systems/Probe-Ably\nModel\n[Command [SEP] Element 1 text]\n[Command [SEP] Element 2 text]\n[Command [SEP] Element n text]\n.......\n[Command [SEP] Element 1 text]\n[x0, x1, y0, y1]\n[Command [SEP] Element 2 text]\n[x0, x1, y0, y1]\n[Command [SEP] Element n text]\n[x0, x1, y0, y1]\n.......\n.......\nLinear Layer \n+\nSoftmax\nInput for BERT and RoBERTa\nInput for LayoutLM\nInput Command + Element\nObtaining representation for each \ncommand + element\nInfer the most likely combination\nof command and element\nEmbedding vectors\n.......\nSelected\n UI Element\nFigure 2: Inference pipeline for LayoutLM, BERT and RoBERTa. Both models generate representations based on the instruction\nand interface elements. The predicted interface element is inferred from the output probabilities for the pair classiﬁcation setting.\nBest seen in colour.\nEmbedding vector\n(a) Determining the abso-\nlute position.\nEmbedding vector\n(b) Determining the rela-\ntive position.\nFigure 3: Illustration of the two types of auxiliary tasks,\nwhich requires determining the relative position from the ob-\ntained representation.\nnot to present the same for PixelHelp, given the small rep-\nresentativity of instructions demanding spatial reasoning in\nthis dataset.\nAs expected, BERT and RoBERTa obtains outstanding\nperformance for instructions requiring lexical overlap and\nsemantic similarity. It easily outperforms the state-of-the-art\nmodel, Seq2Act, raising the importance of pre-trained model\nbaselines in such studies: often, the basic similarity features\ncaptured by out-of-the-box language models are sufﬁcient\nfor complex tasks without needing to introduce new archi-\ntectures. Even though such models do not have access to\nthe interface elements’ position, they seems to manage (par-\ntially) the commands requiring spatial reasoning.\nSimilarly, LayoutLM achieves almost perfect accuracy for\nextractive reasoning commands while also having a supe-\nrior performance for absolute reasoning. This performance\nis likely due to the fact that LayoutLM is integrated with\nBERT, with the added capability of representing positional\ninformation.\nThis result confers evidence that the structure learned\nfrom documents layout can also be transported to interface\nelements. LayoutLM achieves a lower performance on the\nrelative spatial reasoning but still outperforms BERT for\nthis task type. We hypothesise that such results could be\nimproved by modifying the pair classiﬁcation task to a se-\nlection across elements task. In this case, the model would\nbe aware of all the screen elements when inferring a deci-\nsion. However, such models are still very computationally\ndemanding, and given that a screen can easily have up to300\nUI elements, there is a need for more scalable approaches.\nEven though Seq2Act is designed with the understanding\nof spatial reasoning in mind, we can notice that the model\nModel\nDatasets\nPixelHelp RicoSCA Dev RicoSCA Test\nExt Abs Rel All Ext Abs Rel All\nBERT-Base 86.28 99.61 67.12 54.75 78.07 99.54 67.07 55.69 77.92\nRoBERTa-Base 87.17 99.60 67.03 56.31 78.12 99.53 68.03 56.47 77.98\nLayoutLM 71.41 98.51 86.75 61.40 77.62 98.40 90.80 60.53 77.51\nSeq2Act 75.86 96.64 67.14 33.23 73.95 96.75 65.89 31.75 73.78\nTable 2: Accuracy for the compared models. For the RicoSCA dataset, we compare the performance for different types of\nrequired reasoning for grounding. The Seq2Act results presented here are obtained from partial matching, since we are consid-\nering single-step commands.\nis outperformed in almost all aspects by transformers that\nencode only text.\nThe overall obtained results highlight the need for more\nrobust datasets for testing UI element grounding. Future\nwork proposing new datasets and approaches capable of en-\ncoding positional information should initially ensure that\nthe dataset evaluates such properties. The training protocol\npresent in this work can be employed to remove trivial ex-\namples from the dataset. Instances of the dataset that can be\npredicted with high certainty by models that do not encode\npositions should be removed for testing speciﬁc spatial rea-\nsoning.\nProbing Results\nIn Figures 4 (a-d) we present theauxiliary task test accuracy\nand the selectivity of 50 trained MLP probes with varying\ncomplexity. 5 Regions of maximum selectivity indicate that\nthe accuracy scores are more trustworthy: low selectivity in-\ndicates little difference between the control task scores and\nthe auxiliary task scores. It is important to ensure that the\nselectivity doesn’t drop off (indicating an overly expressive\nprobe), which is why the selectivity plots are included.\nAs we can see from the MLP probes, LayoutLM outper-\nform BERT and RoBERTa in the auxiliary spatial reason-\ning tasks, which shows that the generated representations are\nbetter able to discriminate between spatial labels. This sug-\ngests its promising application for grounding UI elements\nand tasks with similar requirements, not limited to the do-\nmain of scanned documents.\nOverall, we can see that the TaskAT2 was the easiest one\nfor all models. In this task, the model was required to decide\nif an element is to the right or the left of the screen (con-\nsidering a point in the middle). There is likely a bias in the\ndataset that makes it easier to distinguish between elements\non the right or left of the screen.\nAgain, such bias can inﬂuence the interpretation of the\nresults, given the false interpretation that such models can\nunderstand positions such as left or right. However, such a\nconclusion for BERT and RoBERTa is impracticable, given\nthat they are unable to encode positions. The probing frame-\nwork provided here can be applied to verify the quality of\ndatasets used for grounding UI elements.\n5Seq2Act does not generate a representation; hence, it was not\nincluded in the probing stage.\nConclusion\nThis paper presents an analysis of the UI grounding capa-\nbilities of three pre-trained models: LayoutLM, BERT and\nRoBERTa. This analysis is performed in two parts: the ﬁrst\nis a traditional analysis of the task accuracy, while in the\nsecond part, we perform a more profound analysis of the\nspatial reasoning performed by such models using probing\ntechniques.\nThe results presented in this work highlights the need for\ndatasets that go beyond lexical matching, focusing more on\nchallenging spatial reasoning. Even though RicoSCA con-\ntains a substantial amount of spatially focused commands, it\nis still a synthetic dataset with limited demonstration of the\nkinds of complex descriptions that would occur in natural\nlanguage instructions. There is a need for human-generated\ndatasets of this kind, but such datasets are still challenging\nto obtain, especially those of a more natural variety (which\nis critical for future applications such as accessibility tools).\nOverall, We conclude that the greatest challenge in this\narea is the interpretation of instructions which require rel-\native spatial reasoning that complicates the grounding task,\nbinding it more tightly to a visual screen state. While there is\ninsufﬁcient data to create strong models speciﬁcally for UI\ngrounding of natural language instructions that include spa-\ntial descriptors, we have shown that document-based models\nsuch as LayoutLM learn a reasonable amount of spatially-\nrelevant features (supported by the probing ﬁndings) that\nmake them transferable to the UI grounding task.\nReferences\nAlain, G.; and Bengio, Y . 2018. Understanding intermediate\nlayers using linear classiﬁer probes. arXiv:1610.01644.\nBranavan, S.; Silver, D.; and Barzilay, R. 2012. Learning to\nwin by reading manuals in a monte-carlo framework. Jour-\nnal of Artiﬁcial Intelligence Research, 43: 661–704.\nBranavan, S.; Zettlemoyer, L. S.; and Barzilay, R. 2010.\nReading between the lines: Learning to map high-level in-\nstructions to commands. In Proceedings of the 48th an-\nnual meeting of the association for computational linguis-\ntics, 1268–1277. Association for Computational Linguistics.\nDeka, B.; Huang, Z.; Franzen, C.; Hibschman, J.; Afergan,\nD.; Li, Y .; Nichols, J.; and Kumar, R. 2017. Rico: A Mobile\nApp Dataset for Building Data-Driven Design Applications.\n(a) AT1 probing.\n(b) AT2 probing.\n(c) AT3 probing.\n(d) AT4 probing.\nFigure 4: Probing results for the both spatial reasoning task for MLP probes with varying complexity (approximated here by\nthe number of parameters).\nIn Proceedings of the 30th Annual Symposium on User In-\nterface Software and Technology, UIST ’17.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nFerreira, D.; Rozanova, J.; Dubba, K.; Zhang, D.; and Fre-\nitas, A. 2020. On the Evaluation of Intelligent Process Au-\ntomation. arXiv preprint arXiv:2001.02639.\nFerreira, D.; Rozanova, J.; Thayaparan, M.; Valentino, M.;\nand Freitas, A. 2021. Does My Representation Capture X?\nProbe-Ably. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing: System Demonstrations, 194–201. Online: Associ-\nation for Computational Linguistics.\nGoldwasser, D.; and Roth, D. 2011. Learning from natu-\nral instructions. In 22nd International Joint Conference on\nArtiﬁcial Intelligence, IJCAI 2011, 1794–1800.\nHewitt, J.; and Liang, P. 2019. Designing and Interpreting\nProbes with Control Tasks. InProceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP) , 2733–2743.\nHong Kong, China: Association for Computational Linguis-\ntics.\nHewitt, J.; and Manning, C. D. 2019a. A Structural Probe\nfor Finding Syntax in Word Representations. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers),\n4129–4138. Minneapolis, Minnesota: Association for Com-\nputational Linguistics.\nHewitt, J.; and Manning, C. D. 2019b. A Structural Probe\nfor Finding Syntax in Word Representations. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers),\n4129–4138. Minneapolis, Minnesota: Association for Com-\nputational Linguistics.\nLi, Y .; He, J.; Zhou, X.; Zhang, Y .; and Baldridge, J. 2020.\nMapping Natural Language Instructions to Mobile UI Ac-\ntion Sequences. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, 8198–\n8210.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMacGlashan, J.; Littman, M.; Loftin, R.; Peng, B.; Roberts,\nD.; and Taylor, M. 2014. Training an agent to ground com-\nmands with reward and punishment. In Workshops at the\nTwenty-Eighth AAAI Conference on Artiﬁcial Intelligence.\nPasupat, P.; Jiang, T.-S.; Liu, E.; Guu, K.; and Liang, P.\n2018. Mapping natural language commands to web ele-\nments. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 4970–4976.\nPimentel, T.; Saphra, N.; Williams, A.; and Cotterell, R.\n2020a. Pareto Probing: Trading Off Accuracy for Complex-\nity. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 3138–\n3153. Online: Association for Computational Linguistics.\nPimentel, T.; Saphra, N.; Williams, A.; and Cotterell, R.\n2020b. Pareto Probing: Trading Off Accuracy for Complex-\nity. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 3138–\n3153. Online: Association for Computational Linguistics.\nPimentel, T.; Valvoda, J.; Hall Maudslay, R.; Zmigrod,\nR.; Williams, A.; and Cotterell, R. 2020c. Information-\nTheoretic Probing for Linguistic Structure. In Proceedings\nof the 58th Annual Meeting of the Association for Com-\nputational Linguistics, 4609–4622. Online: Association for\nComputational Linguistics.\nV oita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and Titov,\nI. 2019. Analyzing Multi-Head Self-Attention: Specialized\nHeads Do the Heavy Lifting, the Rest Can Be Pruned. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 5797–5808. Florence, Italy:\nAssociation for Computational Linguistics.\nV oita, E.; and Titov, I. 2020. Information-Theoretic Prob-\ning with Minimum Description Length. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 183–196. Online: Association\nfor Computational Linguistics.\nV olkova, S.; Choudhury, P.; Quirk, C.; Dolan, B.; and Zettle-\nmoyer, L. 2013. Lightly supervised learning of procedural\ndialog systems. In Proceedings of the 51st Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 1669–1679.\nVuli´c, I.; Ponti, E. M.; Litschko, R.; Glava ˇs, G.; and Ko-\nrhonen, A. 2020. Probing Pretrained Language Models for\nLexical Semantics. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning (EMNLP), 7222–7240. Online: Association for Compu-\ntational Linguistics.\nXu, Y .; Li, M.; Cui, L.; Huang, S.; Wei, F.; and Zhou, M.\n2020. Layoutlm: Pre-training of text and layout for docu-\nment image understanding. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining, 1192–1200.\nZhang, K.; and Bowman, S. 2018. Language Modeling\nTeaches You More than Translation Does: Lessons Learned\nThrough Auxiliary Syntactic Task Analysis. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP , 359–361. Brussels,\nBelgium: Association for Computational Linguistics."
}