{
  "title": "Multidirection and Multiscale Pyramid in Transformer for Video-Based Pedestrian Retrieval",
  "url": "https://openalex.org/W4221155406",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222361762",
      "name": "Zang, Xianghao",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2031667691",
      "name": "Li, Ge",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1970887178",
      "name": "Gao Wei",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096748692",
    "https://openalex.org/W3106853541",
    "https://openalex.org/W3107901913",
    "https://openalex.org/W2948383821",
    "https://openalex.org/W3109286435",
    "https://openalex.org/W3174381846",
    "https://openalex.org/W2955147859",
    "https://openalex.org/W3217561483",
    "https://openalex.org/W3168915470",
    "https://openalex.org/W3170294077",
    "https://openalex.org/W3180143550",
    "https://openalex.org/W6771967508",
    "https://openalex.org/W6797737728",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2959022568",
    "https://openalex.org/W2986451890",
    "https://openalex.org/W3035486808",
    "https://openalex.org/W2767073696",
    "https://openalex.org/W3034417718",
    "https://openalex.org/W3208037308",
    "https://openalex.org/W2963180826",
    "https://openalex.org/W3035252826",
    "https://openalex.org/W6747428963",
    "https://openalex.org/W6726951882",
    "https://openalex.org/W1988492566",
    "https://openalex.org/W6622321702",
    "https://openalex.org/W2979372598",
    "https://openalex.org/W2982119297",
    "https://openalex.org/W2997677148",
    "https://openalex.org/W2997429385",
    "https://openalex.org/W2997243220",
    "https://openalex.org/W2997109336",
    "https://openalex.org/W3092761888",
    "https://openalex.org/W3035635522",
    "https://openalex.org/W760855798",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W2998792609",
    "https://openalex.org/W2520433280"
  ],
  "abstract": "In video surveillance, pedestrian retrieval (also called person\\nre-identification) is a critical task. This task aims to retrieve the\\npedestrian of interest from non-overlapping cameras. Recently,\\ntransformer-based models have achieved significant progress for this task.\\nHowever, these models still suffer from ignoring fine-grained, part-informed\\ninformation. This paper proposes a multi-direction and multi-scale Pyramid in\\nTransformer (PiT) to solve this problem. In transformer-based architecture,\\neach pedestrian image is split into many patches. Then, these patches are fed\\nto transformer layers to obtain the feature representation of this image. To\\nexplore the fine-grained information, this paper proposes to apply vertical\\ndivision and horizontal division on these patches to generate\\ndifferent-direction human parts. These parts provide more fine-grained\\ninformation. To fuse multi-scale feature representation, this paper presents a\\npyramid structure containing global-level information and many pieces of\\nlocal-level information from different scales. The feature pyramids of all the\\npedestrian images from the same video are fused to form the final\\nmulti-direction and multi-scale feature representation. Experimental results on\\ntwo challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed\\nPiT achieves state-of-the-art performance. Extensive ablation studies\\ndemonstrate the superiority of the proposed pyramid structure. The code is\\navailable at https://git.openi.org.cn/zangxh/PiT.git.\\n",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nMulti-direction and Multi-scale Pyramid in\nTransformer for Video-based Pedestrian Retrieval\nXianghao Zang, Ge Li, and Wei Gao\nAbstract—In video surveillance, pedestrian retrieval (also\ncalled person re-identiﬁcation) is a critical task. This task aims\nto retrieve the pedestrian of interest from non-overlapping\ncameras. Recently, transformer-based models have achieved\nsigniﬁcant progress for this task. However, these models still\nsuffer from ignoring ﬁne-grained, part-informed information.\nThis paper proposes a multi-direction and multi-scale Pyramid\nin Transformer (PiT) to solve this problem. In transformer-\nbased architecture, each pedestrian image is split into many\npatches. Then, these patches are fed to transformer layers to\nobtain the feature representation of this image. To explore the\nﬁne-grained information, this paper proposes to apply vertical\ndivision and horizontal division on these patches to generate\ndifferent-direction human parts. These parts provide more ﬁne-\ngrained information. To fuse multi-scale feature representation,\nthis paper presents a pyramid structure containing global-level\ninformation and many pieces of local-level information from\ndifferent scales. The feature pyramids of all the pedestrian images\nfrom the same video are fused to form the ﬁnal multi-direction\nand multi-scale feature representation. Experimental results on\ntwo challenging video-based benchmarks, MARS and iLIDS-\nVID, show the proposed PiT achieves state-of-the-art perfor-\nmance. Extensive ablation studies demonstrate the superiority\nof the proposed pyramid structure. The code is available at\nhttps://git.openi.org.cn/zangxh/PiT.git.\nIndex Terms—video-based pedestrian retrieval, vision trans-\nformer, multi-direction and multi-scale pyramid.\nI. I NTRODUCTION\nP\nEDESTRIAN retrieval is a critical task in intelligent\nsurveillance [1] [2] [3]. Given a pedestrian image as the\nquery, pedestrian retrieval aims to ﬁnd the right images in a\nlarge gallery. The query image and the matched gallery im-\nages must be from different cameras. Pedestrian retrieval has\nimportant practical applications in both society and industry,\nsuch as ﬁnding the criminal suspects and tracking pedestrian\nmovement. Compared to the image-based pedestrian retrieval,\nthe video-based one can provide much more gait and view\ninformation of pedestrians and alleviate the negative effects\nof occlusion situations. Therefore, video-based pedestrian re-\ntrieval is getting more and more attention from researchers\n[4].\nFor CNN-based pedestrian retrieval, dividing the feature\nmap into multiple horizontal stripes and individually training\neach one are general operations. After the training is complete,\nThis work was supported by the National Key R&D Program of China\n(2020AAA0103501). (Corresponding author: Wei Gao. )\nXianghao Zang, Ge Li and Wei Gao are with School of Electronic and\nComputer Engineering, Peking University, Shenzhen 518055, China (e-mail:\nzangxh@pku.edu.cn; geli@ece.pku.edu.cn; gaowei262@pku.edu.cn).\nfeatures\n1\n*\n*\n*\n*\nvD\nhD\npD\nFig. 1: Multi-direction and multi-scale pyramid in transformer\nfor video-based pedestrian retrieval. Different layers employ\ndifferent-direction division strategies. After the process of\ntransformer layer, part-informed features are extracted. Each\nlayer contains features with different scales. The four layers\nhave 1, Dv, Dh, and Dp features, respectively.\nall the stripe features are assembled to generate the convolution\ndescriptor for each image, which provides the model a rich\nfeature representation [5]. Based on the horizontal division\nstrategy, a pyramid of stripes is proposed to exploit the\npartial information of each pedestrian [6]. Although these\nmethods above improve the model performance, the direction\nof division strategy is limited.\nRecently, the transformer structure has achieved incredible\nprogress in computer vision. The transformer structure is a\npopular model in natural language processing (NLP) and can\nhandle the sequence data effectively. In computer vision, the\ninput image is split into many patches. These patches are\nregarded as tokens similar to the words in the NLP task.\nA sequence of feature embedding of these patches is fed\nto the transformer layers. With the help of the multi-head\nself-attention module, the transformer can obtain global-level\nrelationships among all the patches without losing information.\nMeanwhile, the CNN convolution kernel can only perceive\nlimited scope, and the down-sampling operation inevitably\nloses much information. Moreover, the transformer structure\nhas achieved competitive performance compared with CNN\n[7]. Although the transformer has a global perception, the\nﬁne-grained information may be neglected, which results in\na limited performance.\nThis paper proposes a multi-direction and multi-scale Pyra-\n0000–0000/00$00.00 © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any\ncurrent or future media, including reprinting/republishing this material or promotional purposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other works.\narXiv:2202.06014v2  [cs.CV]  6 Apr 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nmid in Transformer (PiT) for video pedestrian retrieval, as\nillustrated in Fig. 1. The pyramid contains four layers, which\nadopt “no division”, vertical, horizontal, and patch-based divi-\nsion strategies, respectively. The ﬁrst layer includes a global-\nlevel feature of the pedestrian image. The second, third, and\nfourth layers contain Dv, Dh, and Dp part-level features. In\nthis way, the proposed PiT applies multi-direction division\nstrategies in the pedestrian image and extracts a multi-scale\nfeature representation for each pedestrian image.\nConcretely, each pedestrian image is split into many\npatches. A class token and the feature embeddings of all\npatches are ﬂattened and fed to multiple transformer layers.\nThen the processed patch tokens are rearranged into a two-\ndimension structure according to their original positions. Dif-\nferent division strategies are applied to this two-dimension\nstructure, which generates different-direction parts. The class\ntoken and patch tokens within the same part are ﬂattened to\nform a new token sequence. After the process of the last trans-\nformer layer, the class token learns the multi-direction part-\ninformed information. A feature pyramid for each pedestrian\nimage is obtained by combining all the features with different\nscales from different layers. The corresponding features of all\nthe images within the same video are fused to generate the\nﬁnal multi-direction and multi-scale feature pyramid.\nThe traditional CNN-based methods usually apply horizon-\ntal division to feature map [5], which is reasonable because\neach horizontal stripe usually contains the head, torso, or legs.\nHowever, the vertical division can divide the human body\ninto the right limb, head and torso, and the left limb, which\nintroduces part-informed clues with more dimensions. Ap-\nplying vertical and horizontal division simultaneously, which\nforms the patch-based division, can also provide more ﬁne-\ngrained information. Combining these multi-direction and\nmulti-scale features can effectively improve the model perfor-\nmance. Experiments on two challenging video-based bench-\nmarks, MARS [8] and iLIDS-VID [9], show the proposed PiT\nachieves state-of-the-art performance. Extensive ablation stud-\nies also demonstrate the superiority of the proposed pyramid\nstructure.\nThe main contributions of this paper can be summarized as\nfollows:\n• Multi-direction: the proposed vertical and horizontal\ndivision strategies in transformer introduce ﬁne-grained,\npart-informed information from different directions.\n• Multi-scale: the global and local-level features with\ndifferent scales form a feature pyramid. This multi-scale\ncombination makes the feature representation rich and\ndiscriminative.\n• Performance: the proposed PiT achieves state-of-the-\nart performance on two challenging video-based bench-\nmarks, and extensive ablation studies demonstrate the\nsuperiority of the proposed multi-direction and multi-\nscale pyramid structure.\nThe rest of this paper is organized as follows: the related\nworks are reviewed and analyzed in Section II, and then the\nproposed method is introduced in Section III. Experimental\nresults and analysis are presented in Section IV, and Section\nV concludes this paper.\nII. R ELATED WORK\nA. Video-based Pedestrian Retrieval\nAn early method for image-based pedestrian retrieval fused\nvarious features, such as RGB, HSV , HoG, LOMO, etc., to\nachieve the multi-feature fusion and overcome the challenge\nfrom pedestrian appearance changes [10]. Whereas video-\nbased pedestrian retrieval has multiple images for each pedes-\ntrian. These consecutive pedestrian images can provide abun-\ndant temporal and spatial information, which can alleviate the\nnegative effects of appearance change, occlusion, pose varia-\ntion, etc [11]. Therefore, existing methods focus on exploiting\nboth spatial and temporal clues from pedestrian video. GRL\n[12] employ video-level features to guide the generation of\ncorrelation map and disentangle the frame-level features into\nhigh-correlation and low-correlation features. BiCnet-TKS\n[13] introduced a bilateral complementary network to mine\nthe divergent body parts of each pedestrian and proposed a\ntemporal kernel selection module to explore temporal relations\nadaptively. CTL [14] employed a key-point estimator to extract\nmulti-scale semantic features to form a topology graph. Then\na 3D graph convolution is used to capture hierarchical spatial\nand temporal dependencies. Besides, AGW + [15] employed a\nframe-level average pooling for video feature representation,\nwhich is simple but effective.\nThese methods above mainly utilized CNN to extract spatial\nand temporal clues. However, the CNN convolution kernel\ncannot capture long-range relationships, and the CNN down-\nsampling operation results in inevitable information loss.\nB. Vision Transformer\nRecently, transformer architecture has become a de-facto\nstandard for natural language processing. ViT [7] introduced\nthis architecture to computer vision and achieved better per-\nformance than many state-of-the-art methods on the image\nclassiﬁcation task. Following this improvement, many works\nwere proposed to improve the performance of the transformer-\nbased framework. For the video-based classiﬁcation task, Swin\ntransformer [16] proposed a hierarchical structure and utilized\nshifted windows to solve the non-overlapping patch division\nproblem. A 3D shifted window is also proposed to preprocess\nvideo data.\nThese methods above demonstrated the transformer struc-\nture could perform well for the video classiﬁcation task.\nHowever, the video-based pedestrian retrieval is very different\nfrom the video classiﬁcation task. The video-based pedestrian\nretrieval task depends highly on appearance information rather\nthan motion information. Therefore, the transformer structure\nwith the ability to perceive more ﬁne-grained information is\nneeded for the video-based pedestrian retrieval task.\nC. Division Strategies for Pedestrian Retrieval\nFor the pedestrian retrieval task, dividing the feature map\ninto multiple stripes is a typical operation. PCB [5] proposed\nto horizontally divide the feature map into multiple stripes\nand achieved signiﬁcant performance improvement compared\nwith the original model. SPP [17] divided the feature map into\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nPatch-based Division\nNo Division\nVertical Division\nHorizontal Division\nConvolutional Projection of Flattened Patches\nTransformer Blocks\n \n \n \n* *\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n*\n*\n \n \n \n \n \n*\n \n \n \n \n \n1\n*\nTransformer \nBlock\nTransformer \nBlock\nTransformer \nBlock\nTransformer \nBlock\n \n \n \n \n \n*\n*\n*\n*\nLg\nvL\nhL\npL\npos\ncls\nview\ncls\nm\nPm\nvD\nhD\npD\ncls\nm\ncls\nm\ncls\nm\ncls\nm\nglobal\ncls\nvertical\ncls\nhorizontal\ncls\npatch\ncls\nˆP\nFig. 2: The proposed feature Pyramid in Transformer (PiT) for each pedestrian image. The feature pyramid with four layers\ncontains multi-direction and multi-scale feature representations.\nequal patches and assembled the multi-scale feature patches\ninto a pyramid structure. HPP [6] employed the horizontal\ndivision to form a multi-scale pyramid and improved the model\nperformance for this task.\nAlthough the models above utilized different division strate-\ngies, performance comparison among different division strate-\ngies in a uniﬁed framework has not been conducted, thus the\ndifferences between these strategies remain to be explored.\nIII. M ULTI -DIRECTION AND MULTI -SCALE PYRAMID IN\nTRANSFORMER\nA. Transformer-based Framework\nThe Vision Transformer (ViT) [7] is employed to construct\nthe framework, as illustrated in Fig. 2. Given some pedes-\ntrian videos {V1,V2,···} and pedestrian IDs {y1,y2,···},\neach video V contains K pedestrian images I, as V =\n{I1,I2,··· ,IK}. A convolution layer is used to embed the\npedestrian image into multiple feature embeddings. Con-\ncretely, after applying convolution operation on the pedestrian\nimage, a feature map f ∈Rh×w×c is obtained. Then the fea-\nture map is ﬂattened to generate N features, where N = h·w\nand the size of each feature is 1 ×c. In this way, each feature\ncan be treated as the feature embedding of each image patch,\nand the size of each image patch is the same as convolution\nkernel size k. The convolution stride sdetermines the interval\nof the adjacent image patches.\nThe feature embedding of each image patch from the image\nI is also called patch token p. A class token φcls with the size\nof 1 ×cis also introduced to represent the feature embedding\nof the whole image. After ﬂattening class and patch tokens\ninto a sequence {φcls; p1; ... ; pN}, the patch tokens lose the\nlocation information in original pedestrian image. Therefore,\na position embedding φpos ∈R(N+1)×c is employed to retain\nthis location information. A camera embedding ˆφview is also\nintroduced to keep the camera information. Since the class\ntoken and all the patch tokens belong to the same camera,\nthe size of ˆφview is set to 1 ×c. Then ˆφview is copied N + 1\ntimes to form a new embedding φview ∈R(N+1)×c. After these\noperations, the token sequence z0 from the pedestrian image\nI is calculated as follows,\nz0 = [φcls; p1; ... ; pN] + λ1φpos + λ2φview\n= [φ0\ncls; p0\n1; ··· ; p0\nN], (1)\nwhere λ1 and λ2 are trade-off parameters. Then mtransformer\nlayers are employed to learn the relationship between the class\ntoken and all patch tokens. Each transformer layer is composed\nof a Multi-head Self-Attention ( MSA) block and a Multi-Layer\nPerception ( MLP) block. A LayerNorm ( LN) layer is applied\nbefore MSA and MLP blocks, and a shortcut connection is also\nemployed as follows,\nz′= zd−1 + MSA(LN(zd−1)),\nzd = z′+ MLP(LN(z′)). (2)\nAfter m transformer layers, the vector sequence zm is\nobtained.\nzm = [φm\ncls; pm\n1 ; pm\n2 ; ··· ; pm\nN] := [φm\ncls; Pm]. (3)\nIn Eq. 3, N patch tokens {pm\ni }N\ni=1 are denoted as Pm,\nPm = [pm\n1 ; pm\n2 ; ··· ; pm\nN]. (4)\nB. Multi-direction and Multi-scale Pyramid\nTo explore the ﬁne-grained, part-informed information, the\npatch tokens Pm are rearranged into a new form ˆP according\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nto their original positions in pedestrian image I,\n[pm\n1 ; pm\n2 ; ··· ; pm\nN]  \nPm∈RN×c\nrearrange\n−−−−−→\n\n\npm\n1 pm\n2 ··· pm\nw\npm\nw+1 pm\nw+2 ··· pm\n2w\n... ... ... ...\n··· ··· ··· pm\nN\n\n\n  \nˆP∈Rh×w×c\n. (5)\nThe rearranged patch tokens ˆP have the same size as\nthe feature map f ∈ Rh×w×c. We learn from the division\nstrategies in Convolution Neural Network (CNN) and apply\nsimilar strategies to patch tokens ˆP.\n1) Multi-direction Division Strategies: The rearranged to-\nkens ˆP are copied four times. Multi-direction division strate-\ngies are applied to these four copies.\nFor the ﬁrst copy, “no division” is applied to the tokes ˆP.\nThen the class token φm\ncls along with ˆP are ﬂattened to a new\ntoken sequence zglobal ∈R(N+1)×c, which equals zm.\nzglobal = [φm\ncls; pm\n1 ; pm\n2 ; ··· ; pm\nN] ⇔zm. (6)\nThe transformer layer Lg receives this sequence and outputs\na new sequence z′\nglobal ∈R(N+1)×c. We follow the general\noperation in [7] [16], which discard all the patch tokens\nand only keep the class token for the following operations.\nTherefore, only the class token φglobal\ncls ∈R1×c is kept as the\nfeature representation of the whole pedestrian image I.\nFor the second copy, “vertical division” is applied to the\npatch tokens ˆP along the vertical direction to generate Dv\nparts. Each part has N/Dv patch tokens. The class token is\ncopied Dv times, and each one is assigned to one part. Then\nthe class token and all the patch tokens in the correspond-\ning part are ﬂattened along the vertical direction to form a\nnew vector sequence zvertical ∈R(N/Dv+1)×c. There are Dv\nsequences in total. For example, the ﬁrst sequence zvertical,1 is\nshown as follows,\nzvertical,1 = [φm\ncls; pm\n1 ; pm\nw+1; pm\n2w+1; ···]. (7)\nEach sequence zvertical,i is followed by the parameter-sharing\ntransformer layer Lv. In this way, the relationship between\nthe class token and a speciﬁc vertical part is explored. After\nthe process of the transformer layer Lv, Dv class tokens\n{φvertical\ncls,i }Dv\ni=1 are kept and denoted as φvertical\ncls . Moreover, this\npaper ﬁrst proposes the “vertical division” strategy, which\nextracts ﬁne-grained feature representation and also introduces\nsigniﬁcant performance improvement.\nFor the third copy, “horizontal division” is applied by\ndividing the patch tokens ˆP into Dh parts. Horizontal division\nstrategy is applied along the horizontal direction, and each part\nhas N/Dh patch tokens. After the division operation, the class\ntoken is copied Dh times. Each one and its corresponding\npatch tokens are ﬂattened along the horizontal direction to\nform the token sequence zhorizontal ∈R(N/Dh+1)×c. The ﬁrst\nsequence zhorizontal,1 is shown below as an example,\nzhorizontal,1 = [φm\ncls; pm\n1 ; pm\n2 ; pm\n3 ; ··· ; pm\nN/Dh]. (8)\nAll the sequences {zhorizontal,i}Dh\ni=1 are followed by the\nparameter-sharing transformer layers Lh, and Dh class tokens\n*\n*\n* \n*\n*\n* \n*\n*\n* \n  \n*\n*\n*\n \n \nFC Classifier\n*\n*\n*\n*\nBN\nBN\nBN\nBN\nFC Classifier\nFC Classifier\n \nFC Classifier\nFC Classifier\n \nFC Classifier\nFC Classifier\n \nbranches\nOne \nbranch\nbranches\nbranches\nAVG\nAVG\nAVG\nAVG\nI1\nI2\nKI\nglobal\ncls\nvertical\ncls\nhorizontal\ncls\npatch\ncls\n* class token\n * average class token\n probability\nvD\nhD\npD\ncls\ncls\np\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\nFig. 3: The generation and training process of the feature\npyramid for each video. The corresponding features of each\npedestrian image are averaged to generate the ﬁnal feature,\nand each feature is trained individually.\n{φhorizontal\ncls,i }Dh\ni=1 are obtained and denoted as φhorizontal\ncls . The\n“horizontal division” is proposed for CNN in previous work\n[6]. However, we propose a manner to apply this strategy to\nthe new structure, i.e., vision transformer, which is proven to\nbe effective.\nFor the fourth copy, the vertical and horizontal division\nstrategies are applied simultaneously to the rearranged patch\ntokens ˆP to form “patch-based division”. Each part has N/Dp\npatch tokens and has a more ﬁne-grained receptive ﬁeld. After\nthe division operation, the class token and all patch tokens\nin the corresponding part are ﬂattened along the horizontal\ndirection to form Dp token sequences zpatch ∈R(N/Dp+1)×c,\nwhere Dp = Dv ×Dh. The ﬁrst sequence zpatch,1 is shown\nbelow as an example,\nzpatch,1 = [φm\ncls; pm\n1 ; pm\n2 ; ··· ; pm\nN/Dv ; pm\nw+1; pm\nw+2; ···]. (9)\nEach token sequence zpatch,i is followed by the transformer\nlayers Lp, which generate Dp class tokens {φpatch\ncls,i }Dp\ni=1. These\nclass tokens are denoted as φpatch\ncls . A similar patch-based divi-\nsion strategy was proposed in CNN structure [17]. However,\nthis paper proposes multi-direction division strategies, which\nare very different from the previous model.\nAfter the transformer layer, each class token has the percep-\ntion within its corresponding global/vertical/horizontal/patch-\nbased part, making it obtain more ﬁne-grained local informa-\ntion.\n2) Multi-scale Pyramid Structure: The rearranged ˆP is\ndivided using different scales, which generates multi-scale\nfeature representations. These features are concatenated to\nform a pyramid structure zφ ∈R(1+Dv+Dh+Dp)×c for each\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\npedestrian image I as follows,\nzφ = [φglobal\ncls ; φvertical\ncls ; φhorizontal\ncls ; φpatch\ncls ], (10)\nwhere φglobal\ncls , φvertical\ncls , φhorizontal\ncls , and φpatch\ncls are in the space of\nR1×c, RDv×c, RDh×c, and RDp×c, respectively.\nThere are K images in each pedestrian video V, and each\nimage is represented by zφ. The feature pyramid ¯zφ of the\nvideo V is generated as illustrated in Fig. 3. Each feature of\nthe video V is the corresponding average feature within it. For\nexample, the class token ¯φglobal\ncls is calculated as follows,\n¯φglobal\ncls =\nK∑\nk=1\nφglobal\ncls,k . (11)\nThe feature pyramid ¯zφ ∈R(1+Dv+Dh+Dp)×c of video V\nis expressed as follow,\n¯zφ = [¯φglobal\ncls ; ¯φvertical\ncls ; ¯φhorizontal\ncls ; ¯φpatch\ncls ]. (12)\nThe CNN-based methods usually design various fusion\nstrategies to combine pedestrian image features within the\nsame video. The feature map from CNN contains rich spatial\nand temporal information, and a sophisticated fusion strat-\negy can effectively combine these features. However, in a\ntransformer-based framework, the class token is not generated\nfrom the input image directly. In other words, these class\ntokens do not contain spatial and temporal information ex-\nplicitly. Therefore, each feature of the video V is obtained in\nan average manner as Eq. 11.\nThe multi-scale pyramid structure ¯zφ ∈R(1+Dv+Dh+Dp)×c\ncontains part-informed information of different scales, which\nis more rich and discriminative than the original global-level\nfeature representation. The ablation study in the latter section\ndemonstrates the superiority of this multi-scale pyramid struc-\nture.\nC. Model Training and Testing\nThis section describes the details of model training and\ntesting. To simplify the expression, the class token φθ\ncls ∈R1×c\nis used to represent each element in ¯zφ. Each class token φθ\ncls\nis followed by a BatchNorm layer and a classiﬁer layer to\ngenerate the ﬁnal probability pθ, as illustrated in Fig. 3. There\nare 1+Dv+Dh+Dp classiﬁer layers to train each token φθ\ncls\nindividually. The classiﬁcation loss Lcls and triplet loss Ltri\nare used to supervise this model by averaging the losses of\nthese independent branches.\nLcls = − 1\nTNc\nT∑\ni=1\nNc∑\nj=1\nyjlog pθ\ni,j, (13)\nLtri = 1\nBT\nT∑\ni=1\n∑\na∈bi\nln{1+ exp[maxd(φθ\ni,a,φθ\ni,p)\n−min d(φθ\ni,a,φθ\ni,n)]},\n(14)\nwhere T=1+Dv+Dh+Dp, Nc is the class number for a\nspeciﬁc benchmark, bi represents the i th mini-batch, B is the\nnumber of pedestrian images in this mini-batch, a,p,n are\nanchor, positive, negative samples, respectively. Function d(·)\ncalculates the Euclidean distance between two features. The\noverall loss function Lis calculated as follows,\nL= Lcls + Ltri. (15)\nIn the testing process, the multi-direction and multi-scale\nfeature pyramid ¯zφ ∈R(1+Dv+Dh+Dp)×c is used to represent\nthe feature representation of pedestrian video V.\nIV. E XPERIMENTS\nA. Experimental Setting\n1) Benchmarks: The proposed Pyramid in Transformer\n(PiT) is evaluated in two challenging benchmarks: MARS\n[8] and iLIDS-VID [9]. There are also two other popular\nbenchmarks: DukeMTMC-VideoReID and PRID2011. Exist-\ning methods [12] [13] [34] have achieved more than 0.95 in\nterms of Rank-1 metric on these two benchmarks. However,\nthere is still much room for improvement on the challenging\nMARS and iLIDS-VID benchmarks.\n• MARS is the largest video-based pedestrian retrieval\nbenchmark and captured by six cameras on a university\ncampus. It contains 20,478 videos from 1,261 identities.\nThese videos are generated by employing the DPM\ndetector and GMMCP tracker, which results in many\nvideos with poor qualities.\n• iLIDS-VID is captured by two cameras in an airport hall.\nIt contains 600 videos from 300 identities. This bench-\nmark is very challenging due to pervasive background\nclutter, mutual occlusions, and lighting variations.\n2) Evaluation Protocol and Metrics: For MARS bench-\nmark, the standard training and testing split provided by\n[8] is used for training the proposed PiT. The Cumulative\nMatching Characteristic (CMC) curve and mean Average\nPrecision (mAP) are employed for evaluation. For iLIDS-VID\nbenchmark, the whole set of videos is randomly divided into\ntwo halves. Then the trials are repeated ten times, and the\nCMC curve is used to evaluate the average results. For conve-\nnience, Rank-1, Rank-5, Rank-10, and Rank-20 are employed\nto represent the CMC curve.\n3) Implementation Details: The proposed PiT is imple-\nmented using Pytorch. The transformer ViT-B16 [7], pre-\ntrained on ImageNet, is employed as the backbone. We follow\nthe operation in [35] to preprocess all the videos. Speciﬁcally,\neach video is divided equally into K snippets, where K equals\n8. The ﬁrst pedestrian image in each snippet is selected as\nthe keyframe, and K images are used to represent this video.\nEach pedestrian image is resized to 256 ×128. The batch\nsize and parameter m are set to 16 and 11. The kernel size\nk and stride s of the convolution layer are set to 16 and 12.\nThe dimension h×w×c of feature embedding outputted by\nconvolution layer is 21×10×768. The trade-off parameters λ1\nand λ2 are set to 1.0 and 1.5. The division parameters Dv, Dh,\nDp are 2, 3, and 6. The standard Stochastic Gradient Descent\n(SGD) with momentum and an initial learning rate of 0.01 is\nused to train these models 120 epochs for each benchmark.\nCosine annealing is employed to schedule the learning rate.\nThe convolution layer and transformer layers are frozen in the\nﬁrst ﬁve epochs to train the classiﬁer layers. After these ﬁve\nepochs, the whole network is trained.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nTABLE I: Performance comparisons between proposed PiT and state-of-the-art methods on MARS and\niLIDS-VID.\nMethods Venue\nMARS iLIDS-VID\nRank-1 Rank-5 Rank-10 mAP Rank-1 Rank-5 Rank-10 Rank-20\nADFD [18] CVPR2019 87.00 95.40 78.20 86.30 97.40 99.70\nGLTR [19] ICCV2019 87.02 95.76 78.47 86.00 98.00\nCOSAM [20] ICCV2019 84.90 95.50 79.90 79.60 95.30\nAGW+ [15] TPAMI2020 87.60 83.00 83.20 98.30\nRTF [21] AAAI2020 87.10 85.20 87.70\nFGRA [22] AAAI2020 87.30 96.00 81.20 88.00 96.70 98.00 99.30\nRGSATR [23] AAAI2020 89.40 96.90 84.00 86.00 98.00 99.40\nAMEM [24] AAAI2020 86.70 94.00 79.30 87.20 97.70 99.50\nCSTNet [25] IJCAI2020 90.20 96.80 83.90 87.80 98.50 99.60\nASTA-Net [26] ACM MM2020 90.40 97.00 84.10 88.10 98.60\nMG-RAFA [27] CVPR2020 88.80 97.00 85.90 88.60 98.00 99.70\nMGH [28] CVPR2020 90.00 96.70 85.80 85.60 97.10 99.50\nSTGCN [29] CVPR2020 89.95 96.41 83.70\nVRSTC [30] CVPR2020 88.50 96.50 97.40 82.30 83.40 95.50 97.70 99.50\nTCLNet-tri* [31] ECCV2020 89.80 85.10 86.60\nAP3D [32] ECCV2020 90.70 85.60 88.70\nAFA [33] ECCV2020 90.20 96.60 82.90 88.50 96.80 99.70\nSSN3D [34] AAAI2021 90.10 96.60 98.00 86.20 88.90 97.30 98.80\nGRL [12] CVPR2021 91.00 96.70 84.80 90.40 98.30 99.80\nBiCnet-TKS [13] CVPR2021 90.20 86.00\nCTL [14] CVPR2021 91.40 96.80 86.70 89.70 97.00 100.00\nProposed PiT 90.22 97.23 98.04 86.80 92.07 98.93 99.80 100.00\n1 The best results are in bold.\nB. Comparison with State-of-the-art Methods\nTable I shows the comparison between the proposed PiT\nand twenty other state-of-the-art methods in terms of mAP\nscore and CMC accuracy. These state-of-the-art methods are\nall within three years and employed ResNet50 as their back-\nbone to explore the spatial and temporal information among\npedestrian images. They used attribute information [18] [24],\nattention mechanism [20] [27], graph convolution [28] [29]\n[14], 3D convolution [32] [34], relation-guided models [22]\n[23] [12], Generative Adversarial Networks (GAN) [30], and\nnew network architectures [15] [21] [25] [26] [31] [33] [19]\n[13], respectively, to generate the feature representation of\neach pedestrian video. Meanwhile, the proposed PiT employs\na transformer-based framework and utilizes the simple average\nfusion to obtain the multi-direction and multi-scale feature\npyramid.\n1) Performances on MARS: Compared with other state-\nof-the-art methods, the proposed PiT achieves the best mAP\nscore and competitive CMC accuracy. The best competitor,\nCTL [14], utilized a key-points estimator to extract human\nbody local features as graph nodes and achieved topology\nlearning for video-based pedestrian retrieval. In comparison,\nthe proposed PiT does not explore the relationship among\ndifferent pedestrian images within the same video and reaches\na better mAP value. This demonstrates the proposed feature\npyramid containing more ﬁne-grained local information has a\nbetter generalization performance.\n2) Performances on iLIDS-VID: Compared with other\nmethods, the proposed PiT achieves state-of-the-art perfor-\nmance. The best competitor, GRL [12], used global correlation\nestimation to disentangle features into high-correlation and\nlow-correlation features. Then GRL proposed temporal re-\nciprocating learning to enhance the high-correlation semantic\nclues and accumulate the low-correlation sub-critical clues\nTABLE II: Performance comparisons between different-\ndirection division strategies.\nOne layer Parameter\nMARS iLIDS-VID\nRank-1 mAP Rank-1\nNo Division (Baseline) 1×210 87.33 84.00 89.87\nVertical Division\n105×2 88.26 85.07 89.73\n70×3 88.64 85.72 90.60\n42×5 88.80 85.56 90.93\n35×6 89.08 85.96 90.40\n30×7 89.78 85.99 89.93\nHorizontal Division\n2×105 88.26 85.33 90.07\n3×70 89.35 85.86 90.20\n5×42 89.46 86.24 91.40\n6×35 89.18 85.94 90.73\n7×30 89.35 86.00 90.67\nPatch-based Division\n6p 89.13 86.01 91.00\n14p 89.24 86.11 91.67\n15p 89.73 86.17 90.80\nfor the ﬁnal feature representation. Meanwhile, the proposed\nPiT has a concise network structure and achieves 1.67%\nperformance improvement on the metric of Rank-1 than GRL.\nC. Ablation Study\nDifferent division strategies in a uniﬁed framework are\ncompared in this section. In this paper, each image is split\ninto 210 patches. They are then further divided into 2,3,5,6,7\nparts. In other words, the optional values of parameters Dv\nand Dh are 2,3,5,6,7. To explicitly show the division details,\n105×2, 70×3, 42×5, 35×6, and 30×7 are used to represent\nthe division parameters using vertical division. 2×105, 3×70,\n5×42, 6×35, and 7×30 are denoted as division parameters\nusing horizontal division. Applying 105×2 vertical division\nand 3×70 horizontal division simultaneously forms a patch-\nbased division and generates 6 parts. 105×2 and 7×30 gener-\nate 14 parts, and 42×5 and 3×70 generate 15 parts (i.e., the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nTABLE III: Performance comparisons between different combinations.\nType Parameter\nMARS iLIDS-VID\nRank-1 mAP Rank-1\nFour Layers\nVertical Division 1×210 105×2 42×5 30×7 89.34 86.30 91.13\nHorizontal Division 1×210 2×105 5×42 7×30 89.56 86.32 91.40\nPatch-based Division 1×210 6p 14p 15p 89.51 85.96 90.47\nProposed PiT 1×210 105×2 3×70 6p 90.22 86.80 92.07\noptional values of parameter Dp are 6,14,15). These patch-\nbased division strategies are denoted as 6p,14p,15p. “No\ndivision” is denoted as 1×210.\n1) Effectiveness of Different-direction Division Strategies:\nThis section compares different-direction division strategies\nin the transformer-based framework. The proposed pyramid\nwith only one layer is used to compare the performances, as\nillustrated in Table II. Compared with the baseline method, this\ntable shows that using different-direction division strategies\nimproves the performance. Two conclusions can be made.\nFirst, although horizontal division is a commonly used strat-\negy, the vertical and patch-based division strategies can also\nimprove performance effectively. Second, the best division\nstrategy and the number of parts are different for different\nbenchmarks. These conclusions demonstrate the need to adopt\na strategy based on the practical scene.\n2 4 6 8 10 12 14\nNumber of Parts\n84.0\n84.5\n85.0\n85.5\n86.0mAP\nVertical Division\nHorizontal Division\nPatch-based DivisionBaseline\nFig. 4: The mAP score changes of different-direction division\nstrategies on MARS benchmark. The values in this ﬁgure\nfollow the data in Table II.\nTo allow for the performance analysis visually, the mAP\nscores of different division strategies on MARS benchmark are\nillustrated in Fig. 4. In this ﬁgure, increasing the number of\nparts can improve the performance gradually. However, more\nnumber of parts also introduces more computation complexity.\nFor different division strategies, dividing the patch tokens into\nsix parts achieves a better trade-off between the performance\nand computation complexity. Therefore, the parameter of\nproposed PiT is 1×210 105×2 3×70 6p, and its fourth layer\nsplits the patch tokens into six parts.\n2) Effectiveness of Multi-scale Pyramid Structure: This\nsection shows the performances of pyramid structures with\ndifferent layers, as illustrated in Table IV. In this table, the\nproposed pyramid with one layer only employs “no division”.\nThe proposed pyramid with two layers additionally uses the\nvertical division strategy. Then the horizontal division and\nTABLE IV: Performance comparisons between the proposed\npyramid with different numbers of layers.\nNumber of\nLayers Parameter\nMARS iLIDS-VID\nRank-1 mAP Rank-1\n1 1×210 87.33 84.00 89.87\n2 1×210 105×2 88.59 85.65 90.20\n3 1×210 105×2 3×70 88.75 85.72 91.13\n4 1×210 105×2 3×70 6p 90.22 86.80 92.07\npatch-based division strategies are added one by one. Each\nlayer contains part-informed information with different scales.\nAs the number of layers increases, the performance improves\ngradually. These improvements demonstrate that fusing multi-\nscale feature representations can improve performance effec-\ntively.\n3) Effectiveness of Multi-direction Pyramid Structure: This\nsection compares performances between multi-direction and\nsingle-direction pyramid structures, as illustrated in Table III.\nAll the pyramids in this table have four layers, and the differ-\nences between them are dependent upon which division strat-\negy is employed. The type “Vertical Division” only employs\nvertical division strategies. The types “Horizontal Division”\nand “Patch-based Division” have the same meanings. For\nvertical and horizontal division strategies, the part numbers\n2,5,7 are chosen to form the bottom three layers. For patch-\nbased division, 6p,14p,15p are employed to form the bottom\nthree layers.\nCompared with other single-direction pyramids, the pro-\nposed PiT achieves the best performance. These comparisons\ndemonstrate fusing multi-direction division strategies provides\nmore improvement. On the other side, the type “Horizontal\nDivision” performs better than the “Vertical Division”. This\nshows the horizontal division strategy is more suitable for\nthe pedestrian retrieval task. In conclusion, the proposed PiT\nfusing multi-direction and multi-scale feature representations\nis the best combination.\nTABLE V: Performance comparisons between proposed PiT\nwith different parameters.\nParameter\nMARS iLIDS-VID\nRank-1 mAP Rank-1\nFour\nLayers\n1×210 105×2 3×70 6p 90.22 86.80 92.07\n1×210 105×2 7×30 14p 89.24 86.13 91.20\n1×210 42×5 3×70 15p 89.24 85.82 90.40\n4) Performances of Proposed Pyramid with Different Pa-\nrameters: The proposed PiT contains four layers, and each\nlayer adopts different division strategies. The performances\nof the proposed PiT with different parameters are presented in\nTable V. For example, the parameter1×210 105×2 3×70 6p\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE VI: Computation complexity and running time of the proposed PiT with different parameters.\nNumber of\nLayers Parameter MACs Trainable\nParameters\nUsing One 24G NVIDIA TITAN RTX GPU\nMARS iLIDS-VID (10 trials)\nRunning Time Rank-1 Running Time Rank-1\n1 1×210 18.05G 85.76M 4.25h 87.33 8.00h 89.87\n2 1×210 105×2 19.56G 93.09M 4.45h 88.59 8.75h 90.20\n3 1×210 105×2 3×70 21.06G 100.53M 4.70h 88.75 9.50h 91.13\n4 1×210 105×2 3×70 6p 22.59G 108.32M 5.00h 90.22 10.70h 92.07\nrepresents “no division” ( 1×210), vertical division ( 105×2),\nhorizontal division ( 3×70), and patch-based division ( 6p) are\nemployed.\nAs illustrated in Table V, the proposed PiT with parameter\n1×210 105×2 3×70 6p achieves the best performance. The\nother two pyramids introduce more ﬁne-grained parts, yet\ntheir performances get poor. On the other side, the pyramid\nwith the parameter 1×210 105×2 7×30 14p splits the patch\ntokens into more horizontal parts than using the parameter\n1×210 42×5 3×70 15p and achieves better performance.\nAlthough their fourth layers have a close number of parts,\nmore horizontal parts introduce more performance improve-\nment.\n5) Qualitative Analysis: The retrieval examples are illus-\ntrated in Fig. 6. One pedestrian image is selected to represent\nthe video for convenience, and the top eight retrieval results\nfor each query are illustrated in this ﬁgure. We select the\nquery pedestrian video from MARS benchmark according to\nthe Average Precision (AP) value. Fig. 6(a)(b)(c)(d) show the\nsuccessful cases where the proposed PiT has a better AP than\nthe baseline method, and Fig. 6(e)(f) show the failed cases\nwhere the baseline method performs better.\nFor the successful case in Fig. 6(a)(b), the proposed PiT\nretrieves many correct videos in the gallery. In contrast, the\nbaseline method retrieves many incorrect results, including a\nman in a blue shirt in Fig. 6(a) and a road sign in the second\nand third places in Fig. 6(b). For the failed case in Fig. 6(d),\nthe baseline method puts the correct videos in the top two\nplaces. However, the proposed PiT also retrieves pedestrians\nwith similar appearances. In Fig. 6, we employ the AP value to\ndetermine whether the proposed method is successful or not.\nFor practical application, people usually look for the person\nof interest from the top-k results, not just the top-1 result.\nTherefore, the proposed PiT can be utilized effectively for\nFig. 6(e)(f).\nTo explore the difference between the proposed PiT and the\nbaseline method, the attention maps of the query pedestrian\nimage are illustrated in Fig. 5. With the same input image, the\nproposed PiT and the baseline method have different attention\nmaps. The baseline method cannot extract the ﬁne-grained\nlocal features. Therefore, it cannot reduce the unfavorable\nimpacts from the man in a blue shirt in Fig. 5(a) and the\nroad sign in Fig. 5(b). Therefore, more ﬁne-grained feature\nrepresentation can help the model recognize the pedestrian of\ninterest.\n6) Computation Complexity and Running Time: Table VI\nadopts Multiply-ACcumulate operations (MACs) and trainable\nparameters to show the computation complexity. We use one\n24G NVIDIA TITAN RTX GPU to conduct the experiments.\n（a）\n（b）\n（c）\nProposed PiT The Baseline Method\nFig. 5: Attention map examples. Three images in each group\nare the input image, the attention map of this image, and the\nproduct result between input image and its attention map.\nFor the MARS benchmark, the training and testing processes\nof the proposed PiT take 5.00 hours, including training 8,298\nvideos from 625 pedestrian IDs and testing another 11,310\nvideos for evaluation. For the iLIDS-VID benchmark, the\nexperiments include ten trials to ensure statistical stability, and\nthe total running time takes 10.70 hours. Each trial contains\ntraining 300 videos from 150 IDs and testing another 300\nvideos. In Table VI, the proposed PiT has acceptable MACs,\ntrainable parameters, and running time, which have the same\norder of magnitude as the baseline method. The superiority\nof the proposed method can be seen from the performance\nimprovement in terms of the Rank-1 metric.\nV. C ONCLUSION\nThis paper proposes a multi-direction and multi-scale Pyra-\nmid in Transformer (PiT) for video-based pedestrian retrieval.\nThe proposed PiT contains four layers, and each layer applies\ndifferent division strategies on the patch tokens to generate\ndifferent-direction parts. The class token and the patch tokens\nin each generated part are fed to the corresponding transformer\nlayer. In this way, the class token perceives the ﬁne-grained,\npart-informed features. Then multi-direction and multi-scale\nfeatures are combined to form a feature pyramid for each\npedestrian image. The feature pyramids of pedestrian images\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\n（a）\n（b）\n（c）\n（d）\n（e）\nQuery Proposed PiT The Baseline Method\n（f）\nFig. 6: Retrieval examples. Each video is represented by one pedestrian image within it. The query video is selected from\nMARS benchmark, and the top eight retrieval results for each query are illustrated in this ﬁgure. (a)(b)(c)(d) show the successful\ncases, and (e)(f) show the failed cases. The correct results are in green boxes.\nbelonging to the same video are fused to generate the ﬁnal\nfeature pyramid. Experimental results on two challenging\nbenchmarks, MARS and iLIDS-VID, show the proposed PiT\nachieves state-of-the-art results. The comprehensive ablation\nstudies demonstrate the superiority of the proposed multi-\ndirection and multi-scale pyramid structure.\nREFERENCES\n[1] M. Ye, Y . Cheng, X. Lan, and H. Zhu, “Improving night-time pedestrian\nretrieval with distribution alignment and contextual distance,” IEEE\nTransactions on Industrial Informatics , vol. 16, no. 1, pp. 615–624,\n2019.\n[2] J. Garc ´ıa, A. Gardel, I. Bravo, and J. L. L´azaro, “Multiple view oriented\nmatching algorithm for people reidentiﬁcation,” IEEE Transactions on\nIndustrial Informatics, vol. 10, no. 3, pp. 1841–1851, 2014.\n[3] X. Zang, G. Li, W. Gao, and X. Shu, “Learning to disentangle scenes\nfor person re-identiﬁcation,” Image and Vision Computing , vol. 116, p.\n104330, 2021.\n[4] Z. Zeng, Z. Li, D. Cheng, H. Zhang, K. Zhan, and Y . Yang, “Two-\nstream multirate recurrent neural network for video-based pedestrian\nreidentiﬁcation,” IEEE Transactions on Industrial Informatics , vol. 14,\nno. 7, pp. 3179–3186, 2017.\n[5] Y . Sun, L. Zheng, Y . Yang, Q. Tian, and S. Wang, “Beyond part models:\nPerson retrieval with reﬁned part pooling (and a strong convolutional\nbaseline),” in Proceedings of the European Conference on Computer\nVision (ECCV), 2018, Conference Proceedings, pp. 480–496.\n[6] Y . Fu, Y . Wei, Y . Zhou, H. Shi, G. Huang, X. Wang, Z. Yao, and\nT. Huang, “Horizontal pyramid matching for person re-identiﬁcation,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 33,\n2019, Conference Proceedings, pp. 8295–8302.\n[7] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, and S. Gelly,\n“An image is worth 16x16 words: Transformers for image recognition at\nscale,” in International Conference on Learning Representations , 2020,\nConference Proceedings.\n[8] L. Zheng, Z. Bie, Y . Sun, J. Wang, C. Su, S. Wang, and Q. Tian,\n“Mars: A video benchmark for large-scale person re-identiﬁcation,” in\nProceedings of the European Conference on Computer Vision (ECCV) .\nSpringer, 2016, Conference Proceedings, pp. 868–884.\n[9] T. Wang, S. Gong, X. Zhu, and S. Wang, “Person re-identiﬁcation by\nvideo ranking,” inProceedings of the European Conference on Computer\nVision (ECCV). Springer, 2014, Conference Proceedings, pp. 688–703.\n[10] R. Zhou, X. Chang, L. Shi, Y .-D. Shen, Y . Yang, and F. Nie, “Person\nreidentiﬁcation via multi-feature fusion with adaptive graph learning,”\nIEEE Transactions on Neural Networks and Learning Systems , vol. 31,\nno. 5, pp. 1592–1601, 2019.\n[11] X. Zang, G. Li, W. Gao, and X. Shu, “Exploiting robust unsupervised\nvideo person re-identiﬁcation,” IET Image Processing , 2021.\n[12] X. Liu, P. Zhang, C. Yu, H. Lu, and X. Yang, “Watching you: Global-\nguided reciprocal learning for video-based person re-identiﬁcation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, Conference Proceedings, pp. 13 334–13 343.\n[13] R. Hou, H. Chang, B. Ma, R. Huang, and S. Shan, “Bicnet-tks:\nLearning efﬁcient spatial-temporal representation for video person re-\nidentiﬁcation,” in Proceedings of the IEEE/CVF Conference on Com-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nputer Vision and Pattern Recognition , 2021, Conference Proceedings,\npp. 2014–2023.\n[14] J. Liu, Z.-J. Zha, W. Wu, K. Zheng, and Q. Sun, “Spatial-temporal\ncorrelation and topology learning for person re-identiﬁcation in videos,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, Conference Proceedings, pp. 4370–4379.\n[15] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. Hoi, “Deep learning\nfor person re-identiﬁcation: A survey and outlook,” IEEE Transactions\non Pattern Analysis and Machine Intelligence , 2021.\n[16] Z. Liu, J. Ning, Y . Cao, Y . Wei, Z. Zhang, S. Lin, and H. Hu, “Video\nswin transformer,” arXiv preprint arXiv:2106.13230 , 2021.\n[17] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\nconvolutional networks for visual recognition,” IEEE Transactions on\nPattern Analysis and Machine Intelligence , pp. 1904–16, 2014.\n[18] Y . Zhao, X. Shen, Z. Jin, H. Lu, and X.-s. Hua, “Attribute-driven\nfeature disentangling and temporal aggregation for video person re-\nidentiﬁcation,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2019, Conference Proceedings, pp.\n4913–4922.\n[19] J. Li, J. Wang, Q. Tian, W. Gao, and S. Zhang, “Global-local temporal\nrepresentations for video person re-identiﬁcation,” in Proceedings of the\nIEEE International Conference on Computer Vision , 2019, Conference\nProceedings, pp. 3958–3967.\n[20] A. Subramaniam, A. Nambiar, and A. Mittal, “Co-segmentation inspired\nattention networks for video-based person re-identiﬁcation,” in Proceed-\nings of the IEEE International Conference on Computer Vision , 2019,\nConference Proceedings, pp. 562–572.\n[21] X. Jiang, Y . Gong, X. Guo, Q. Yang, F. Huang, W.-S. Zheng, F. Zheng,\nand X. Sun, “Rethinking temporal fusion for video-based person re-\nidentiﬁcation on semantic and time aspect,” in Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , vol. 34, 2020, Conference\nProceedings, pp. 11 133–11 140.\n[22] Z. Chen, Z. Zhou, J. Huang, P. Zhang, and B. Li, “Frame-guided region-\naligned representation for video person re-identiﬁcation,” in Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence , vol. 34, 2020, Con-\nference Proceedings, pp. 10 591–10 598.\n[23] X. Li, W. Zhou, Y . Zhou, and H. Li, “Relation-guided spatial attention\nand temporal reﬁnement for video-based person re-identiﬁcation,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 34,\n2020, Conference Proceedings, pp. 11 434–11 441.\n[24] S. Li, H. Yu, and H. Hu, “Appearance and motion enhancement for\nvideo-based person re-identiﬁcation,” in Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol. 34, 2020, Conference Proceedings,\npp. 11 394–11 401.\n[25] J. Liu, Z. J. Zha, X. Zhu, and N. Jiang, “Co-saliency spatio-temporal\ninteraction network for person re-identiﬁcation in videos,” in Twenty-\nNinth International Joint Conference on Artiﬁcial Intelligence and Sev-\nenteenth Paciﬁc Rim International Conference on Artiﬁcial Intelligence\nIJCAI-PRICAI-20, 2020, Conference Proceedings.\n[26] X. Zhu, J. Liu, H. Wu, M. Wang, and Z.-J. Zha, “Asta-net: Adaptive\nspatio-temporal attention network for person re-identiﬁcation in videos,”\nin Proceedings of the 28th ACM International Conference on Multime-\ndia, 2020, Conference Proceedings, pp. 1706–1715.\n[27] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, “Multi-granularity\nreference-aided attentive feature aggregation for video-based person re-\nidentiﬁcation,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2020, Conference Proceedings,\npp. 10 407–10 416.\n[28] Y . Yan, J. Qin, J. Chen, L. Liu, F. Zhu, Y . Tai, and L. Shao, “Learning\nmulti-granular hypergraphs for video-based person re-identiﬁcation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, Conference Proceedings, pp. 2899–2908.\n[29] J. Yang, W.-S. Zheng, Q. Yang, Y .-C. Chen, and Q. Tian, “Spatial-\ntemporal graph convolutional network for video-based person re-\nidentiﬁcation,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2020, Conference Proceedings,\npp. 3289–3299.\n[30] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, “Vrstc:\nOcclusion-free video person re-identiﬁcation,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , 2019,\nConference Proceedings, pp. 7183–7192.\n[31] R. Hou, H. Chang, B. Ma, S. Shan, and X. Chen, “Temporal complemen-\ntary learning for video person re-identiﬁcation,” in European Conference\non Computer Vision. Springer, 2020, Conference Proceedings, pp. 388–\n405.\n[32] X. Gu, H. Chang, B. Ma, H. Zhang, and X. Chen, “Appearance-\npreserving 3d convolution for video-based person re-identiﬁcation,” in\nEuropean Conference on Computer Vision. Springer, 2020, Conference\nProceedings, pp. 228–243.\n[33] G. Chen, Y . Rao, J. Lu, and J. Zhou, “Temporal coherence or temporal\nmotion: Which is more critical for video-based person re-identiﬁcation?”\nin European Conference on Computer Vision . Springer, 2020, Confer-\nence Proceedings, pp. 660–676.\n[34] X. Jiang, Y . Qiao, J. Yan, Q. Li, W. Zheng, and D. Chen, “Ssn3d:\nSelf-separated network to align parts for 3d convolution in video person\nre-identiﬁcation,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 35, 2021, Conference Proceedings, pp. 1691–1699.\n[35] A. Porrello, L. Bergamini, and S. Calderara, “Robust re-identiﬁcation\nby multiple views knowledge distillation,” in European Conference on\nComputer Vision. Springer, 2020, Conference Proceedings, pp. 93–110.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7189433574676514
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.6959078311920166
    },
    {
      "name": "Transformer",
      "score": 0.6617811322212219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6503891944885254
    },
    {
      "name": "Computer vision",
      "score": 0.6143194437026978
    },
    {
      "name": "Pedestrian",
      "score": 0.589629590511322
    },
    {
      "name": "Feature extraction",
      "score": 0.48391228914260864
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4405214488506317
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37684327363967896
    },
    {
      "name": "Engineering",
      "score": 0.17335665225982666
    },
    {
      "name": "Voltage",
      "score": 0.12152469158172607
    },
    {
      "name": "Mathematics",
      "score": 0.075055330991745
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Transport engineering",
      "score": 0.0
    }
  ]
}