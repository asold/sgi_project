{
  "title": "Novel Transformer Networks for Improved Sequence Labeling in genomics",
  "url": "https://openalex.org/W3096508121",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2800756041",
      "name": "Jim Clauwaert",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A18839523",
      "name": "Willem Waegeman",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A2800756041",
      "name": "Jim Clauwaert",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A18839523",
      "name": "Willem Waegeman",
      "affiliations": [
        "Ghent University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1019830208",
    "https://openalex.org/W6759074601",
    "https://openalex.org/W2587688848",
    "https://openalex.org/W6765947112",
    "https://openalex.org/W6768800271",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6646363707",
    "https://openalex.org/W6682301761",
    "https://openalex.org/W6651112702",
    "https://openalex.org/W2151181684",
    "https://openalex.org/W1975924111",
    "https://openalex.org/W2048869777",
    "https://openalex.org/W6654988515",
    "https://openalex.org/W2052024190",
    "https://openalex.org/W6638583595",
    "https://openalex.org/W6763187818",
    "https://openalex.org/W6744294035",
    "https://openalex.org/W6748890919",
    "https://openalex.org/W6637573405",
    "https://openalex.org/W6757069749",
    "https://openalex.org/W1996475592",
    "https://openalex.org/W6749192820",
    "https://openalex.org/W6655685001",
    "https://openalex.org/W6677222045",
    "https://openalex.org/W2333207005",
    "https://openalex.org/W2343039591",
    "https://openalex.org/W6764680637",
    "https://openalex.org/W2991019646",
    "https://openalex.org/W6642556909",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6805081885",
    "https://openalex.org/W6728895442",
    "https://openalex.org/W2912984848",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W2889772740",
    "https://openalex.org/W6741119803",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4294170691"
  ],
  "abstract": "In genomics, a wide range of machine learning methodologies have been investigated to annotate biological sequences for positions of interest such as transcription start sites, translation initiation sites, methylation sites, splice sites and promoter start sites. In recent years, this area has been dominated by convolutional neural networks, which typically outperform previously-designed methods as a result of automated scanning for influential sequence motifs. However, those architectures do not allow for the efficient processing of the full genomic sequence. As an improvement, we introduce transformer architectures for whole genome sequence labeling tasks. We show that these architectures, recently introduced for natural language processing, are better suited for processing and annotating long DNA sequences. We apply existing networks and introduce an optimized method for the calculation of attention from input nucleotides. To demonstrate this, we evaluate our architecture on several sequence labeling tasks, and find it to achieve state-of-the-art performances when comparing it to specialized models for the annotation of transcription start sites, translation initiation sites and 4mC methylation in E. coli.",
  "full_text": "Novel Transformer Networks for Improved\nSequence Labeling in genomics\nJim Clauwaert and Willem Waegeman\nAbstract— In genomics, a wide range of machine learning methodologies have been investigated to annotate biological sequences for\npositions of interest such as transcription start sites, translation initiation sites, methylation sites, splice sites and promoter start sites. In\nrecent years, this area has been dominated by convolutional neural networks, which typically outperform previously-designed methods\nas a result of automated scanning for inﬂuential sequence motifs. However, those architectures do not allow for the efﬁcient processing\nof the full genomic sequence. As an improvement, we introduce transformer architectures for whole genome sequence labeling tasks.\nWe show that these architectures, recently introduced for natural language processing, are better suited for processing and annotating\nlong DNA sequences. We apply existing networks and introduce an optimized method for the calculation of attention from input\nnucleotides. To demonstrate this, we evaluate our architecture on several sequence labeling tasks, and ﬁnd it to achieve state-of-the-art\nperformances when comparing it to specialized models for the annotation of transcription start sites, translation initiation sites and 4mC\nmethylation inE. coli.\nIndex Terms—Genomics, deep learning, transformer networks, sequence labeling\nÇ\n1I NTRODUCTION\nI\nN the last 30 years, a major effort has been invested into\nuncovering the relation between the genome and the bio-\nlogical processes it interacts with. A thorough understand-\ning of the inﬂuence of the DNA sequence is of importance\nfor the manipulation of biological systems, e.g., to facilitate\nthe forward engineering of biological pathways. In recent\nyears, machine learning methodologies play an increasingly\nimport role in the construction of predictive tools. These\ntasks include the annotation of genomic positions of rele-\nvance, such as transcription start sites, translation initiation\nsites, methylation sites, splice sites and promoter start sites.\nEarly methods for labeling of the DNA sequence were\nfocused on the extraction of important features to train\nsupervised learning models, such as tree-based methods or\nkernel methods. More recently, convolutional neural net-\nworks (CNNs) have been popular, initiated from the work\nof Alipanahi et al. [1]. The popularity of the CNN can be\nattributed to the automatic optimization of motifs or other\nfeatures of interest during the training phase.\nThe prokaryotic and eukaryotic genome is built from107\nand 1010 nucleotides. Given its size, only a small fragment\nof the genome sequence is bound to determine the existence\nof certain genomic sites. In order to create a feasible sample\ninput, only a short fragment of the genome sequence is used\nto predict the occurrence of these sites. The boundaries of\nthis region with respect to the position of interest is denoted\nas the receptive ﬁeld. Due to the model architecture of\nconventional machine learning and deep learning techni-\nques such as convolutional neural networks, where the\ninput is structured according to the relative distances\ntowards the output label, custom input samples are created\nfrom the genome, in accordance to the selected receptive\nﬁeld around each nucleotide position. However, input sam-\nples of neighboring positions are created from largely over-\nlapping regions on the genome. When evaluating all\npositions on the genome, the combined sequence length of\nthe input samples is several times larger than the length of\nthe original genome, and scales with the size of the recep-\ntive ﬁeld.\nIn practice, existing studies do not apply the full genome\nfor training or evaluation. This task is too resource-heavy\nfor a multitude of machine learning methodologies that\nhave not been created to handle millions of samples. Addi-\ntionally, the majority of the annotation tasks represent a\npositive and negative set that is heavily imbalanced, which\ncan hinder the success of learning approaches. For example,\nthe detection of transcription start sites (TSSs) has several\nthousand times more negative than positive labels. In some\ncases, the site of interest is constrained to a subset of posi-\ntions. This is exempliﬁed by the site at which translation of\nthe RNA is initiated, denoted as the Translation Initiation\nSite (TISs), where valid positions can be delimited by three\nnucleotides being either ATG, TTG or GTG [2]. For annota-\ntion tasks that can not be constrained to a smaller set, the\nnegative set is sampled (e.g., prediction of TSS [3] [4] or\nmethylation [5]). In general, the size of the sampled negative\nset is chosen to be of the same order of magnitude as the size\nof the positive set, constituting only a fraction of the original\nnegative set size (0.01% for TSS inE. coli). However, given\nthe comparative sizes of the sampled and full negative set,\nperformance metrics are not guaranteed to correctly reﬂect\nthe models predictive capability. When considering the task\n/C15 The authors are with KERMIT, Department of Data Analysis and Mathe-\nmatical Modelling, Ghent University, 9000 Gent, Belgium.\nE-mail: {jim.clauwaert, willem.waegeman}@ugent.be.\nManuscript received 28 Feb. 2020; revised 3 July 2020; accepted 13 Oct. 2020.\nDate of publication 30 Oct. 2020; date of current version 3 Feb. 2022.\n(Corresponding author: Jim Clauwaert.)\nDigital Object Identiﬁer no. 10.1109/TCBB.2020.3035021\nIEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 1, JANUARY/FEBRUARY 2022 97\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nfor which the model is optimized, it is plausible that the\nresulting performances generalize poorly when applying\nthe model on the full genome.\nTransformer networks have recently been introduced in\nnatural language processing [6]. These architectures are based\non attention and outperform recurrent neural networks on\nnatural language sequence-to-sequence labeling benchmarks.\nIn 2019, Daiet al.[7] deﬁned the transformer-XL, an extension\nof the transformer unit for tasks constituting long sequences\nthrough introduction of a recurrence mechanism, showing\npromise towards evaluating the genome sequence. In this\nstudy, we introduce a novel transformer-based model for\nDNA sequence labeling tasks. The genome is processed as is,\nwhere nucleotide inputs contribute to the prediction of multi-\nple outputs. The size of the receptive ﬁeld does not inﬂuence\nthe amount of data processed, nor does it inﬂuence the\namount of parameters of the model. In contrast to recurrent\nneural networks, which share these advantageous properties,\nthe transformer-XL architecture iterates the genome sequence\nin segments of multiple nucleotides, offering superior process-\ning times. By applying a model on the full genome, no compli-\ncations arise that are linked to subsampling of the negative set.\nWe deﬁne for the ﬁrst time a transformer architecture for\nDNA sequence labeling by building upon recent innova-\ntions in the ﬁeld of natural language processing. Second, we\nsubstantiate and implement adaptations to the model that\nmake it better suited to extract information from nucleotide\nsequences, an extension that proves to drastically improve\nthe predictive capabilities of the model. Third, a benchmark\nis performed with recent studies for three different annota-\ntion tasks: transcription start sites, translation initiation sites\nand methylation sites. We prove that the novel transformer\nnetwork attains state-of-the-art performances, while retain-\ning fast training times.\n2R ELATED WORK\nStudies exploring datamethods for statistical inference based\nsolely on the nucleotide sequence go back as far as 1983, with\nHarret al.[8] publishing mathematical formulas on the creation\nof a consensus sequence for TSSs inE. coli. Stormo [9] describes\nover ﬁfteen mathematical approaches in relation to processing\nDNA sequences between 1983 and 2000, ranging from: algo-\nrithms designed to identify consensus sequences [10], [11],\ntune weight matrices [12] and rank alignments [13], [14].\nWith the increased knowledge in the ﬁeld of molecular\nbiology and the failing attempts to create robust correlations\nbetween the DNA sequence and properties of interest,\nefforts towards feature engineering were made, extracting\nphysical, chemical and biological meaning that could show\nrelatedness towards the biological process. Several impor-\ntant descriptors of sequences include, but are not limited to:\nthe GC-content, bendability [15], ﬂexibility [16] and free\nenergy [17]. Recently, Nikamet al. published Seq2Feature,\nan online tool that can extract up to 252 protein and 41\nDNA sequence-based descriptors [18].\nThe rise of novel machine learning methodologies, such\nas Random Forests and support vector machines, have\nresulted in many applications for the creation of tools to\nannotate the prokaryotic genome. Liuet al.propose stacked\nnetworks that apply Random Forests [19] for two-step\nsigma factor prediction inE. coli. Support vector machines\nare applied by Manavalanet al.to predict phage virion pro-\nteins present in the bacterial genome [20]. Further examples\nof the application of support vector machines include the\nwork of: Goelet al.[21], who propose an improved method\nfor splice site prediction in Eukaryotes; and, Wanget al.\n[22], who introduce the detection ofs70 promoters using\nevolutionary driven image creation.\nSuccessful gains in the ﬁeld of machine learning and\ngenome annotation can be attributed to the use of deep learn-\ning methods. In 1992, Hortonet al.[23] published the use of\nthe ﬁrst perceptron neural network, applied for promoter site\nprediction in a sequence library originating fromE. coli. How-\never, the popular application of deep learning started with\nCNNs, initially designed for networks specializing in image\nrecognition. These incorporate the optimization (extraction)\nof relevant features from the nucleotide sequence during the\ntraining phase of the model. Automatic training of position\nweight matrices has achieved state-of-the-art results for the\nprediction of regions with high DNA-protein afﬁnity [1] in\neukaryotes. As of today, several studies have been applying\nCNNs for prokaryotes. These include models for the annota-\ntion of methylation sites [24], origin of replication sites [25],\n[26], recombination spots [27],[28], TSSs [4].\nRecurrent neural network architectures have not been\napplied on the full genome due to their long processing times,\nbut can be used to process individual samples of decreased\nlengths featuring expression and/or nucleotide sequence\ndata. In combination with convolutional layers, they have\nbeen to used to detect TISs [2] forE. coli. More applications of\nrecurrent neural networks exist for Eukaryotes, such as\nmiRNA target prediction [29] and, combined with convolu-\ntional layers, the annotation of methylation states [30], [31]\nand detection of protein binding sites on RNA [32].\nThe only type of machine learning method that has been\nsuccessfully applied on the full genomic sequence are hid-\nden Markov models. However, due to the limited capacity\nof a hidden Markov model, this method is nowadays rarely\nused for new studies. Some applications for prokaryotes\ninclude the detection of genes inE. coli[33] and the recogni-\ntion of repetitive DNA sequences [34].\n3T RANSFORMER NETWORK\nHere we describe our transformer network for DNA\nsequence labeling. In Section 3.1, we adapt the transformer\narchitecture of Daiet al. [7] to DNA sequences. Unlike the\nnatural language processing tasks for which the trans-\nformer-XL architecture was ﬁrst described, the annotation\nof DNA sequences is not an autoregressive task. Addition-\nally, only a few input and output classes exist, resulting in\nthe models being less complex. To better extract features\nfrom the nucleotide sequence, an adaptation to the calcula-\ntion of attention is described in Section 3.2.\nIn this paper, we adopt the annotation of normal lower-\ncase letters for parameters (e.g.,l), bold lowercase letters for\nvectors (e.g.,q) and uppercase letters for matrices (e.g.,H).\n3.1 Basic Model\nIn essence, the annotation of DNA is a sequence labeling\ntask that has correspondences to natural language\n98 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 1, JANUARY/FEBRUARY 2022\nprocessing. Representing a DNA sequence of lengthp as\nðx1;x 2; ... ;x pÞ, where xi 2f A; C; T; Gg, the tasks described\nin this paper are binary classiﬁcation problems, with each\nposition xi having a corresponding labelyi 2f 0; 1g. A posi-\ntive label denotes the occurrence of an event at that position.\nThe model processes the genome in sequential segments\nof l nucleotides. During training, a non-linear transforma-\ntion function E is optimized that maps the input classes\nfA; C; T; Gg to a vector embeddinghh of length dmodel. For\nnucleotide xi on the genome:\nhh ¼ EðxiÞ;x i 2f A; T; C; Gg; (1)\nwhere hh 2 Rdmodel .\nThe inputs at each segment are processed through k\nlayers. Within each layer, multi-head attention is calculated\nfor each hidden statehh using the collection of hidden states\nwithin each segment, represented as rows in the matrix\nH 2 Rl/C2dmodel .\nNext, for each hidden state ofhh, the output of the multi-\nhead attention step (MultiHead) is summed with the input,\ni.e., a residual connection. The ﬁnal mathematical step\nwithin each layer is layer normalization [35]. The operations\nfor hidden stateshh in layert at positionn in segments are\nperformed in parallel:\nhhðs;tþ1;nÞ ¼ LayerNormðhhðs;t;nÞ þ MultiHeadðHðs;tÞÞÞ;\nor,\nHðs;tþ1Þ ¼ LayerNormðHðs;tÞ þ MultiHeadðHðs;tÞÞÞ;\nwhere t 2½ 0;k ½ and n 2½ 1;l /C138 .\nAfter a forward pass throughk layers, a ﬁnal linear com-\nbination reduces the dimension of the output hidden state\n(dmodel) to the amount of output classes. In this study, only\nbinary classiﬁcation is performed. A softmax layer is\napplied before obtaining the prediction value^yi for nucleo-\ntide xi.\n3.1.1 Multi-Head Attention\nThe core functionality of the transformer network is the\nattention head. The attention head evaluates the hidden\nstates in H with one another to obtain an output scorezz.\nThe superscript denoting the layer and segment of the fol-\nlowing equations are dropped as identical operations are\nperformed at each layer and segment.\nThe query (qq), key (kk) and value (vv) vectors are calculated\nfrom the hidden statehh:\nqqðnÞ;kkðnÞ;vvðnÞ ¼ hhðnÞWq;hhðnÞWk;hhðnÞWv;\nwhere Wq;W k;W v 2 Rdhead/C2dmodel and qq; kk; vv 2 Rdhead . Theqq\nand kk vectors are used to obtain a score between two hidden\nstates, expressing their relevance with one another in regard\nto the information represented byvv.\nFor each hidden state at positionn of the segment, the\nattention score zz is calculated by evaluation of its vectorqq\nwith the kk and vv vectors derived from the other hidden\nstates in the segment:\nzzðnÞ ¼ softmax\nXl\ni¼1\nqqðnÞ /C1 kkðiÞ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃdhead\np\n !\n/C1 vvðiÞ:\nThe softmax function is used to rescale the weights assigned\nto the vectorsvv to sum to 1. Division by the square root of\ndhead is applied to stabilize gradients [6].\nThe calculation of attention within the attention head is\nperformed in parallel for all hidden states inH:\nQ; K; V¼ HWq>;H Wk>;H Wv>\nZ ¼ AttentionðHÞ¼ softmaxð QK>\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃdhead\np ÞV;\nwhere Q; K; V2 Rl/C2dhead and Z 2 Rl/C2dhead . Here, thesoftmax\nfunction is applied to every row ofQK>.\nTo increase the capacity of the model, the input is proc-\nessed by multiple attention heads (nhead) present within\neach layer, each featuring a unique set of weight matrices\nWq;W k;W v – optimized during training. Having multiple\nsets ofWv, Wq and Wk allows the model to extract multiple\ntypes of information from the hidden states.\nThe output of the multi-head attention unit is obtained\nby concatenation of allZ matrices along the second dimen-\nsion and multiplication byWm. This creates an output with\ndimensions equal toH:\nMultiHeadðHÞ¼ ColConcatðZð1ÞðHÞ; ... ;Z ðnheadÞðHÞÞWm;\nwhere Wm 2 Rnheaddhead/C2dmodel .\nNext to the information content of the input, positional\ninformation of the hidden states is relevant towards the cal-\nculation of attention. Unlike the majority of other machine\nlearning methods in the ﬁeld (e.g., linear regression, convo-\nlutional/recurrent neural networks), the architecture of the\nmodel does not inherently incorporate the relative position-\ning of the inputs. Positional information is added by intro-\nduction of a bias related to the vector representation and\nrelative distance of the evaluated hidden states [7].\n3.1.2 Recurrence\nTo process the full genome sequence, a recurrence mecha-\nnism is applied, as described by Daiet al.[7]. This allows for\nthe processing of a single input (i.e., the genome) in sequen-\ntial segments of lengthl. In contrast with calculation of the\nattention heads described in the previous section, only\nupstream hidden states are used to calculate the output of\nhh. In each layer, hidden states½hhðnþ1Þ; ... ;hhðlÞ/C138 are masked\nwhen processingzzðnÞ, n 2½ 1;l /C138 .\nIn order to extend the receptive ﬁeld of information\navailable past one segment, hidden states of the previous\nsegment s /C0 1 are accessible for the calculation ofhhðs;tþ1Þ.\nThe segment lengthl denotes the span of hidden states used\nto calculate attention. Therefore, Hðs;t;nÞ, representing the\ncollection of hidden states used for the calculation of multi-\nhead attention at positionn in layert þ 1 of segments, con-\nsists ofl hidden states spanning over segments and s /C0 1:\nHðs;t;nÞ ¼½ SGðhhðs/C0 1;t;nþ1Þ ... hhðs/C0 1;t;lÞÞ hhðs;t;1Þ ... hhðs;t;nÞ/C138 :\nSG denotes the stop-gradient, signifying that during\ntraining, no weight updates of the model are performed\nCLAUWAERT AND WAEGEMAN: NOVEL TRANSFORMER NETWORKS FOR IMPROVED SEQUENCE LABELING IN GENOMICS 99\nbased on the partial derivatives of given hidden states with\nthe loss. This alleviates training times, as full backpropaga-\ntion through intermediary values would require the model\nto retain the hidden states from as many segments as there\nare layers present in the model, a process that quickly\nbecomes unfeasible for a model with a large segment length\nor high amount of layers. Fig. 1 gives an illustration of the\nmodel architecture adopting the recurrence mechanism.\n3.2 Extension: Convolution OverQ, K and V\nImportant differences exist between the input sequence of\nthe genome and typical natural language processing tasks.\nThe genome constitutes a very long sentence, showing low\ncontextual complexity at input level. Indeed, only four\ninput classes exist. Attention is calculated based on the indi-\nvidual hidden stateshh. For example, in the ﬁrst layer, hid-\nden states of the segment solely contain information on the\nnucleotide classes. In previous studies, meaningful sites\nand regions of interest on the genome are speciﬁed by (sets\nof) motifs from neighboring nucleotides.\nTo expand the information contained inqq, kk and vv to rep-\nresent k-mers rather than single nucleotides, a 1D-convolu-\ntional layer is implemented that convolves over theqq, kk and\nvv vectors derived from neighboring hidden states, present\nas adjoining rows inQ, K and V . The length of the motif, k-\nmer or kernel is denoted bydconv.\nTo ensure that the dimensions ofqq, kk and vv remain identi-\ncal after the convolutional step, as many sets of weight ker-\nnels are trained asdhead. Furthermore, through padding, the\nsize of the ﬁrst dimension of the matricesQ, K and V can be\nkept constant. Applied onqq we get:\nqconv;ðnÞ\nc ¼\nXdconv\ni¼1\nXdhead\nj¼1\nqðfðn;iÞÞ\nj Wconv;q\ni;j;c ;\nfðn; iÞ¼ n /C0 dconv\n2\n/C24/C25\nþ i;\nwhere c 2½ 1;d head/C138 and Wconv;q 2 Rdhead/C2dconv/C2dhead . Wconv;q is\nthe tensor of weights used to convolveqq. Applied on theQ\nmatrix the operation is represented as:\nQconv\nn ¼\nXdconv\ni¼1\nXdhead\nj¼1\nQfðn;iÞ;jWconv;q\ni;j :\nA unique set of weights is optimized to calculateQconv,\nKconv and V conv for each layer. To reduce the total amount of\nparameter weights of the model, identical weights are used\nto convolveQ, K and V for all attention heads in the multi-\nhead attention module. Fig. 2 gives a visualization of the\nintermediate results and mathematical steps performed to\ncalculate attention within the attention head of the extended\nmodel.\n4E XPERIMENTS ANDANALYSIS\n4.1 Experimental Setup\nTo highlight the applicability of the new model architecture\nfor genome annotation tasks, it has been evaluated on multiple\nprediction problems inE. coli. All tasks have been previously\nstudied both with and without deep learning techniques.\nThese are the annotation of Translation Start Sites (TSSs), spe-\nciﬁcally linked to promoter sites for the transcription factor\ns70, the Translation Initiation Sites (TISs) and N4-methylcyto-\nsine sites. The genome was labeled using the RegulonDB [36]\ndatabase for TSSs, Ensembl [37] for TISs and MethSMRT [38]\nfor the 4mC-methylations. These data sets contain the posi-\ntively labeled positions on the genome, with all other positions\nbeing negative.\nFor every prediction task, the full genome is labeled at\nsingle nucleotide resolution, resulting in a total sample size\nof several millions. A high imbalance exists between the\npositive and negative set, the former generally being over\nfour orders of magnitudes smaller than the latter. An over-\nview of the data sets and the resulting sample sizes when\nlabeling the genome are given in Table 1.\nIt can be important to include information located down-\nstream of the position of interest to the model, as it is rele-\nvant for the prediction of the site. For all three prediction\nproblems, the nucleotide sequence up to 20 nucleotides\ndownstream of the labeled position is regarded as relevant.\nIn order to make this information accessible to predict the\nlabel at positionn, the labels can be shifted downstream. In\naccordance with the downstream bound taken by recent\nstudies for the annotation of all three annotation tasks [2],\n[4], [5], labels have been shifted downstream by 20 nucleoti-\ndes, placing them at position +20 from their respective\nnucleotide site.\nThe receptive ﬁeld of the model is deﬁned as the nucleo-\ntide region that is linked indirectly, through calculation of\nattention in previous layers, to the output. As the span of\nhidden states used to calculate attention is equal to the seg-\nment lengthl, the receptive ﬁeld has a nucleotide coverage\nthat is equal to the multiplication of the amount of layers\nwith the segment length (k /C2 l). After shifting the labels\ndownstream, the range of the nucleotide sequence within\nthe receptive ﬁeld of the model at positioni is delimited as\n/C138 i /C0 k /C2 l þ 20;i þ 20/C138 .\nFig. 1. Illustration of the connectivity of intermediary values of the trans-\nformer architecture. The full genome is processed in sequential seg-\nments s with lengthl. First, the input nucleotides are transformed into a\nvector embeddinghhð0;:Þ, after which they are processed byk consecutive\nresidual blocks (hhð0;:Þ ! ... ! hhðk;:Þ). The output probability is obtained\nby sending the ﬁnal hidden state through a set of fully-connected layers.\nFor the calculation at each residual block, the lastl hidden states of the\nprevious layer are applied. For example,hhð1;lÞ is calculated based on the\nhidden statesfhhð0;1Þ; ... ;hhð0;lÞg. Intermediary hidden states from the pre-\nvious segment (s /C0 1) are made accessible for the calculation of the hid-\nden states in segments.\n100 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 1, JANUARY/FEBRUARY 2022\nAs the model sequentially iterates the genome, the train-\ning, test and validation set are created by splitting the\ngenome at three positions that constitute 70% (4,131,280–\n2,738,785), 20% (2,738,785–3,667,115) and 10% (3,667,115–\n4,131,280), respectively. An identical split was performed for\neach of the prediction tasks. Split positions given are those\nfrom the RefSeq database and, therefore, include both the\nsense and antisense sequence within given ranges. The model\nis optimized using the cross-entropy loss and Adam step\nupdate algorithm. Surprisingly, weighing of the loss in order\nto account for the imbalance in class distributions did not\nhave an effect on the performances, and was therefore not\ndone. Model training is stopped when a minimum loss on\nthe validation set is reached. All performance metrics listed\nare obtained on the test set. Models were trained and evalu-\nated using a single GeForce GTX 1080 Ti and programmed\nusing PyTorch [39].\nHyperparameter tuning was performed using random grid\nsearch. To better handle the sheer amount of hyperparameters,\nmodels were optimized using a reduced training set (30%) for\nall three problems. Hyperparameter sets were evaluated based\non the minimum loss on the validation set. Due to the limited\namount of input and output classes, it quickly became obvious\nthat the required vector size of the hidden states (dmodel)w a s\nmuch smaller (32) as compared to the one used for natural lan-\nguage processing tasks (512). Similar observations were made\nto the overall complexity of the model, reﬂected by e.g., the\namount of layers and attention heads. Given a minimal capac-\nity, the model returned stable performances, with more com-\nplex models only increasing processing times. The ﬁnal\nhyperparameter set results in a model size that is as minimal\nas possible, without negatively inhibiting the performances.\nThis set proved to work well on all annotation tasks. As such,\nonly a single model architecture has been used to evaluate all\nthree annotation tasks. Relevant model parameters and opti-\nmization parameters are listed in Table 2.\nThe Area Under the Receiver Operating Characteristics\nCurve (ROC AUC) is the metric selected to evaluate the\nmodel performance. This measure is commonly used for\nbinary classiﬁcation and is independent of the relative sizes\nof the positive and negative sets.\n4.2 Improvement by Convolution Over Q, K and V\nThe use of nucleotide embeddings as inputs to the trans-\nformer network is an important difference with natural lan-\nguage processing, where nucleotides, featuring only four\ninput classes, feature lower contextual complexity than\nwords. Essentially, the equivalent for words are motifs or k-\nmers present within the DNA sequence. These are of impor-\ntance towards the biological process of the prediction task\ndue to their afﬁnity towards the domains of related proteins.\nIn order to investigate ways to improve the model, the\nreduction of the input and output resolution of the model\nwas ﬁrst investigated. The use of k-mers as inputs increases\nthe information content of the input embeddings and can\nfacilitate the detection of relevant motifs. The use of k-mers\nresults in a higher amount of input classes (i.e.,4k) and\nspeeds up processing time as the sequence length is divided\nFig. 2. An overview of mathematical operations performed by the atten-\ntion head to calculate attentionzz for each hidden statehh. Operations to\ncalculate attention are performed in parallel forl hidden states (H ! Z).\nThe query qq, kk and vv vectors are obtained through multiplication ofH\nwith the model weight matricesWq, Wk and Wv, resulting in theQ, K\nand V matrix. A single convolutional layer applied onQ, K and V , using\nas many kernels asdhead, results in the transformation of the individualqq,\nkk and vv vector representations of each input to be derived from theqqkk\nand vv vectors ofdconv bordering nucleotides. AttentionZ is thereafter cal-\nculated. Through padding, dimensions ofQ, K and V are kept constant\nbefore and after the convolutional layer . Matrix dimensions are given\nalong the edges of the matrices. The schema is kept simple for better\nunderstanding and does not include the relative position encodings\nadded to the Q and K matrix, nor does it incorporate the recurrence\nmechanism.\nTABLE 1\nOverview of the Data Set Properties Uzsed in This Study\nDataset source Positive labels Negative labels Annotation\nRegulonDB [36] 1,694 (0.02%) 9,281,610 (99.98%) TSS\nEnsembl [37] 4,376 (0.05%) 9,278,928 (99.95%) TIS\nMethSMRT [38] 5,534 (0.06%) 9,277,770 (99.94%) 4mC\nFrom left to right: the name of the database, positive labels, negative labels and\nannotation task performed (TSS:s70 Transcription Start Site; TIS: translation\nInitiation Site; 4mC: 4mC methylation sites). All data sets are derived fromE.\ncoli MG1655 (accession: NC_000913.3, size: 9,283,304 nucleotides).\nTABLE 2\nOverview of the Hyperparameters that Deﬁne\nthe Model Architecture\nHyperparameter symbol value Hyperparameter symbol value\nlayers k 6 segment length l 512\ndim. head dhead 6 dim. model dmodel 32\nheads in layer nhead 6 conv. kernel size dconv 7\nlearning rate lr 0.0002 batch size bs 10\nA single set of hyperparameters was selected to train a single model that\nshowed to work well on all prediction tasks.\nCLAUWAERT AND WAEGEMAN: NOVEL TRANSFORMER NETWORKS FOR IMPROVED SEQUENCE LABELING IN GENOMICS 101\nby the k-mer size, albeit at the cost of a decreased output\nresolution of the model predictions.\nThe reduced performances resulting from applying k-\nmer inputs underline the disadvantages of this approach.\nFirst, different unique sets of k-mers can be used to repre-\nsent the DNA sequence, determined by the position where\nsplits are performed. Therefore, motifs of relevance to the\nprediction problem can be represented by multiple sets of\ninput classes. Given the low amount of positive samples of\nthe investigated prediction problems, all possible input\nclass combinations that are of importance are more likely to\nbe only present in either the training, validation or test set.\nTherefore, higher values ofk quickly results in the overﬁt-\nting of the model on the training set.\nThe high similarity between the k-mers with largely equal\nsequences (e.g., AAAAAA and AAAAAT) can be mapped\nthrough the embedding of the input classes, obtained by\nEquation 1. Embedding representations for each input class\ncan either be optimized during training or before. In case of\nthe optimization during training, embeddings are in func-\ntion of the prediction problem (i.e., loss on the labeling), an\noption less suited for a setting with a small positive set and\nhigh amounts of input classes. For the unsupervised setting,\nvector embedding can be mapped to the input classes using\nplethora of prokaryotic genomes. This has been done using\nthe word2vec methodology for all classes present in a 3-mer\nor 6-mer setting, resulting in a slight improvement of the\nmodel performances, albeit lower than the performance of\nthe model trained at single-nucleotide resolution [40].\nAlternatively, the use of a convolutional layer in the\nattention heads of the neural network has been investigated.\nTheoretically, the implementation of a convolutional layer\nextends the the information embedded inkk, qq and vv to be\nderived from dconv neighboring hidden states, without\nchanging the input/output resolution of the model.\nThis extra step increases the contextual complexity\nwithin the attention heads without extending training times\nsubstantially, albeit at an increase of the number of model\nweights. An overview of the mathematical steps performed\nin the adjusted attention head is shown in Fig. 2. To\nevaluate, performances were compared for different sizes of\ndconv for the prediction of TSSs, TISs and methylation sites.\nApplication of the transformer network with no convolu-\ntional layer results in a ROC AUC of 0.919, 0.996 and 0.951 for\nthe annotation of TSSs, TISs and 4mC methylation sites,\nrespectively. Addition of the convolutional layer improves\nthese performances, wheredconv ¼ 7 gives the best results for\nall three annotation tasks. The increased performance score is\nmost notable for the annotation of TSSs, showing an improve-\nment from 0.919 to 0.977, where the difference with a perfect\nscore is almost divided by four. Performances are given in\nTable 3. Additionally, the total amount of model parameters\nand average durations to iterate over one epoch are given.\nThe loss curves on the TSS prediction task for all model\ntypes are given in Fig. 3, and show a stable convergence of\nthe loss for both the training and validation set fordconv <\n7. In contrast, fordconv > 7, the loss curve on the validation\nset shows a stronger similarity to a hyperbolic, a pattern\nthat clearly demonstrates overﬁtting of the model to the\ntraining set due to the increased amount of parameter\nweights in the model.\nImportantly, the capacity of the model can be increased\nthrough selection of the total number of layers k, the\namount and dimension of the attention heads nhead and\ndhead and the dimension of the hidden statesdmodel. Nonethe-\nless, during hyperparameter tuning, increasing the capacity\nof the model did not further improve the performance on\nthe test set. The addition of the convolutional layer is thus a\nnecessary enhancement of the model in this setting.\n4.3 Selection of lmem\nThe parameterlmem denotes the amount of last-most hidden\nstates of the previous segment (s /C0 1) stored in memory,\nthereby delimiting the amount of hidden states made acces-\nsible for the calculation of attention in layers. Traditionally,\nlmem is set tol during training time, ensuring the calculation\nof attention at each position in segments to have access tol\nhidden states (as stored inHðn;t;sÞ) [7]. Aslmem determines\nthe shapes ofH, K and V , it is a major factor inﬂuencing the\nmemory requirement and processing time of the model.\nIn order to reduce training times to enable the applicabil-\nity of the framework for larger genomes, data from several\nmodels (dconv ¼ 7) for the annotation of s70 TSSs was\nTABLE 3\nComparison Between the Performance of the Transformer\nModel With Different Kernel Sizes of the Convolutional Layer on\nthe Test Sets of the Genome Annotation\nTask dconv Epoch time (s) Model weights ROC AUC\nTSS 0 337.5 185,346 0.919\nTSS 3 364.8 241,218 0.966\nTSS 7 371.0 314,946 0.972\nTSS 11 379.2 388,674 0.970\nTSS 15 383.8 462,402 0.973\nTIS 0 337.5 185,346 0.996\nTIS 7 371.0 314,946 0.998\n4mC 0 337.5 185,346 0.951\n4mC 7 371.0 314,946 0.985\ndconv ¼ 0 constitutes the transformer network with no convolutional step. For\nall settings, dconv ¼ 7 results in the best performances. For the annotation of\ns70 Transcription Start Sites (TSSs), the performances for all evaluateddconv\nare given, similar to those given in Fig. 3. Performances are evaluated using\nthe Area Under the Receiver Operating Characteristics Curve (ROC AUC).\nFor each setting, the total amount of model weights and time (in seconds) to\niterate one epoch during training is given.\nFig. 3. The smoothed loss on the training and validation set of thes70\nTSS data set for different values ofdconv. In line with the loss curve of the\nvalidation set, the best performance on the test set was obtained for\ndconv ¼ 7. It can be observed that higher values ofdconv quickly results in\noverﬁtting of the model while lower values result in convergence of both\nthe training and validation set at a higher loss.\n102 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 1, JANUARY/FEBRUARY 2022\ncollected for different values of lmem during training\n(denoted by ltrain\nmem). Additionally, as the segment lengthl is\nnot tied to the model weights and can be altered after train-\ning, performance metrics for different segment lengths of\nthe model for the annotation of the test set (ltest) were also\nobtained. ltest\nmem is always set equal toltest.\nThe processing time to iterate the genome is reduced by\nhalve forltrain\nmem ¼ 0. Fig. 4 shows the loss on the training and\nvalidation set in function of time for the different values of\nltrain\nmem. Interestingly, after training of 75 epochs, lower losses\non the training set are obtained for lower values ofltrain\nmem.N o\nbackpropagation is performed through the hidden states of\nthe previous segment (see Section 3.1.2), even though the\nabove elements contribute to the loss during training. The\ninability to properly update the weights of the model in\nfunction of hidden states from previous segments are a\nlikely cause for the slower convergence of the training loss\nfor ltrain\nmem > 0. Therefore, processing times are reduced both\nby the reduction of epoch time and the fewer epochs\nrequired until a minimum on the validation loss is obtained.\nTable 4 shows the ROC AUC performances and training\ntimes for the annotation of s70 TSSs. Models trained for\nltrain\nmem ¼ 0 are not penalized in their performance on the test\nset. In contrast, for all values ofltest\nmem, higher performances as\ncompared to the traditional setting (ltrain\nmem ¼ 512) are obtained.\nThe strong variation of hidden states applied to calculate\nattention forhhn, ranging between 1 to 512 and dependent on\nthe position ofn within s, does not negatively inﬂuence the\nperformance. The discussed variation might in fact contrib-\nute to regularization of the model weights, given the more\nstable results of the model on varying values ofltest.\nT h er e c e p t i v eﬁ e l do ft h em o d e l sp r e d i c t i o n sf o rltrain\nmem ¼\n512 s p a n s3 , 0 7 2n u c l e o t i d e s(l /C2 k), a region multiple times\nlarger than the circa 80 nucleotides window used in previ-\nous studies [4][3][41]. Reduction ofltest can offer insights\ninto the DNA region relevant towards the prediction task.\nThis is illustrated by the performances of the model for\nltest equal to 64 and 512, where the reduction of the recep-\ntive ﬁeld to 384 nucleotides does not negatively inﬂuence\nperformances (for ltrain\nmem = 0), revealing the excluded region\nto be of no importance towards the identiﬁcation of a TSS.\nOverall, given the inﬂuence of the segment lengths on\nboth the training time and performance, a closer look into\nthe behavior of the model for varying values ofl and lmem\nshould be made for the genome annotation tasks. In this\nstudy, the model parameters ltrain ¼ 512, ltrain\nmem ¼ 0 and\nltest ¼ ltest\nmem ¼ 512 proved to work best for all three annota-\ntion tasks.\n4.4 Benchmarking\nAs a ﬁnal step, the proposed transformer model\n(dconv ¼ 7;l train\nmem ¼ 0) has been evaluated with the studies\nreporting state-of-the-art performances on all three annota-\ntion tasks. For each setting, the same annotations have been\napplied to train and evaluate all models, meaning that the\npositive set between these studies is identical. Differences\nexist between the negative sets, as our method processes the\nfull genome. This is in contrast to the use of a subsampled\nnegative set by recent studies [3], [4], [19], [22], [41], [41],\n[42]. In the majority of studies, custom sampling methods\nare used, where the samples of the negative set is not pub-\nlicly available [3], [4], [5], [41], [42]. As the transformer\narchitecture iterates the genome sequentially, the creation of\na training, test and validation set have are obtained by slic-\ning the genome at three points. This results in neighboring\nnucleotides being grouped in the same set, which might\nintroduce a bias. However, it was made sure that the rela-\ntive frequencies of the input and output classes were identi-\ncal for all sets. No bias was detected, as the performances of\nthe transformer model proved to be robust after training\nand evaluating the model using various sets.\nUniquely, the transformer model processes the full nega-\ntive set for training and evaluation purposes. Several met-\nrics used to represent the performance are directly\ninﬂuenced by the (relative) sizes of the negative and posi-\ntive sets. These include the accuracy and Area Under the\nPrecision-Recall Curve (PR AUC). Furthermore, the sensi-\ntivity (recall), speciﬁcity, precision and Matthew Correlation\nCoefﬁcient are all metrics that depend on the threshold used\nto group the model output probabilities into positive and\nnegative predictions. This threshold can be selected to maxi-\nmize any metric, such as the accuracy, recall or precision.\nFig. 5 shows the effect of subsampling the negative set on\nthe performance metrics and the selection of the optimal\nthreshold to delineate the positive from the negative predic-\ntions. In the ﬁrst setting, 1500 instances are sampled from\nthe positive and negative output distributions, representing\nthe models ability to categorize both classes. The ROC AUC\nFig. 4. The smoothed loss on the training and validation set of thes70\nTSS data set for different values ofltrain\nmem. The losses are given w.r .t. train-\ning times. All settings were trained for 75 epochs. While increased values\nof lmem strongly inﬂuence the convergence time of the loss for both the\ntraining and validation set, it does not alter the minimum loss on the vali-\ndation set.\nTABLE 4\nPerformances, Given as Area Under the Receiver Operating\nCharacteristics Curve (ROC AUC), of the Transformer Model on\nthe Test Set for the Prediction ofs70 Transcription Start Sites\n(TSS) Using Different Values ofltrain\nmem and ltest\nTask ltrain\nmem Epoch ltest ¼ ltest\nmem\ntime (s) 8 32 64 512\nROC AUC\nTSS 0 371.2 0.664 0.957 0.977 0.977\nTSS 1 384.2 0.638 0.945 0.968 0.969\nTSS 32 415.5 0.611 0.940 0.969 0.971\nTSS 128 482.6 0.593 0.907 0.963 0.973\nTSS 512 728.7 0.570 0.883 0.954 0.972\nFor each setting,ltrain ¼ 512 and ltest ¼ ltest\nmem. The time (in seconds) to iterate\none epoch during training is also given.\nCLAUWAERT AND WAEGEMAN: NOVEL TRANSFORMER NETWORKS FOR IMPROVED SEQUENCE LABELING IN GENOMICS 103\nand PR AUC are both 0.983. The optimal threshold returns a\nprecision and recall of 0.94 and 0.93, respectively. Evaluat-\ning the performance of the model for an increased negative\nset size with 9,000,000 instances, sampled from the same\ndistribution, reveals two problems: the PR AUC perfor-\nmance measure is decreased to 0.25 as a direct result of false\npositives, and the use of the threshold selected from the ﬁrst\nset-up does not properly balance the trade-off between false\nnegatives and false positives. Speciﬁcally, the accuracy, pre-\ncision and recall equal to 0.933, 0.0046 and 0.93, as com-\npared to 0.999, 0.713 and 0.136 for a threshold optimized for\nthe accuracy on the second setting.\nThe ROC AUC metric is independent to the relative sizes\nof the positive and negative set. Scores in Table 5 list the\nROC AUC scores of recent studies claiming state-of-the-art\nperformances. For CNNs, the model architectures have\nbeen implemented as described in each study to attain the\nROC AUC performance metrics. As such, performances\nhave been obtained from these models using exactly the\nsame positive, negative, training, validation and test set\nused to obtain performances on the transformer model.\nThe ROC AUC of 0.977, 0.998 and 0.985 for the annota-\ntion ofs70 TSSs, TISs and 4mC methylation sites represent a\nsubstantial improvement of the performances obtained by\nprevious studies. In essence, the improvement of the ROC\nAUC is substantial as it more than halves the area above the\ncurve (0.949! 0.977, 0.995! 0.998 and 0.960! 0.985). The\nscore for the model implemented in accordance to the work\nof Khanal et al. [5] shows a strong variation with the\nreported score (0.652/0.960). It was not possible to pinpoint\nthe cause for this discrepancy, but it could be related to the\nincreased size of the negative set. As the transformer model\noutperforms either, this was not investigated further.\nWith the exception of the CNN model for 4mC methyla-\ntion, the amount of model weights is in line with previous\nneural networks developed. A single architecture for the\ntransformer model (dconv ¼ 7;l train\nmem ¼ 0) was trained to per-\nform all three annotation tasks, proving the robustness of\nthe novel framework for genome annotation tasks. Imple-\nmentation of the convolutional layer within the attention\nheads proved to be required to achieve state-of-the-art\nresults using the proposed attention networks.\n5C ONCLUSIONS ANDFUTURE WORK\nIn this paper, we introduced a novel framework for full\ngenome annotation tasks by applying the transformer-XL net-\nwork architecture. To extend the calculation of attention\nbeyond hidden states derived from the nucleotides inputs, of\nwhich only four input classes exist, a convolutional layer over\nQ, K and V was added. As an effect, calculation of relevance\n(QK>) and linear combination withV processes information\nto be derived from multiple neighboring hidden states. An\nimprovement in predictive performance was obtained, which\nindicates that the technique enhances the detection of nucleo-\ntide motifs that are relevant to the prediction task.\nThe efﬁcacy of the transformer network was demonstrated\non three different tasks inE. coli: the annotation of transcrip-\ntion start sites, transcription initiation sites and 4mC methyla-\ntion sites. In recent studies applying machine learning\ntechniques for genome annotation tasks, the lack of an existing\nbenchmark data set and the existence of custom negative sets\nhinders the straightforward and clear comparison of existing\nmethodologies. In a balanced setting, the sampled negative\nset constitutes only a fraction of the negative samples within\nthe genome (e.g., 0.02%–0.1% for TSSs). Therefore, sampling\nof the negative set makes the trained model susceptible to\nFig. 5. Illustration of varying model performance characteristics as an\neffect of different sample sizes. The distributions of the outputs and pre-\ncision recall-curves are given. (a) An equal positive and negative sample\nsize of 1500 gives an optimal accuracy of 0.936. The coinciding thresh-\nold selected to optimize accuracy (gray striped line) gives a precision of\n0.94 and recall of 0.93. (b) Given the same distribution characteristics\nbut with negative sample size of 9,000,000, performance of the PR AUC\nis drastically reduced (0.25) as a result of false positives. Moreover , the\nthreshold selected to optimize accuracy is adjusted (green full line). The\nresulting accuracy , precision and recall for the new setting are 0.999,\n0.711 and 0.136. Using of the threshold from the previous setting gives\nthe scores of 0.933, 0.0046 and 0.93.\nTABLE 5\nPerformances, Given as Area Under the Receiver Operating\nCharacteristics Curve (ROC AUC), of Recent Studies and the\nTransformer Based Model on the Annotation ofs70 Transcription\nStart Sites (TSS), Translation Initiation Sites (TIS) and 4mC\nMethylation (4mC)\nTask Study Approach Model weights ROC AUC\nTSS Lin et al.[3] SVM - 0.909\nTSS Rahman et al.[41] SVM - 0.90\nTSS Umarov et al.[4] CNN 395,236 0.949*\nTSS This paper transformer 314,946 0.977\nTIS Clauwaert et al.[2] CNN 445,238 0.995*\nTIS This paper transformer 314,946 0.998\n4mC Chen et al.[42] SVM - 0.886\n4mC Khanal et al.[5] CNN 16,634 0.652*/0.960\n4mC This paper transformer 314,946 0.985\nPerformances listed are those reported by the paper, and generally constitute a\nmuch smaller negative set. Additionally, performances with an asterisk (*) are\nobtained by implementation of the model architecture and training on the full\ngenome. Applied machine learning approaches include Convolutional Neural\nNetworks (CNN) and Support Vector Machines (SVM). When applicable, the\namount of model weights is given.\n104 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 1, JANUARY/FEBRUARY 2022\noverﬁtting due to bad generalization towards the true nega-\ntive set. Furthermore, application of the full genome for evalu-\nation purposes ensures the resulting performances to\ncorrectly reﬂect the model’s capability in a practical setting.\nBoth the application of the full negative and postive set and\nthe easy construction of the training, test and validation set\nfacilitate future benchmarking efforts.\nModels were trained within 2–3 hours. A single iteration\nover the prokaryotic genome on a single GeForce GTX 1080\nTi takes ca. six minutes. The transformer architecture does\nnot assert the relative positions of the input nucleotides\nw.r.t. the output label, a property that makes the methodol-\nogy well-suited for the processing of genome sequences.\nFirst, to annotate each position on the genome, inputs only\nhave to be processed once, as intermediary values are\nshared between multiple outputs. Second, increasing the\nreceptive ﬁeld of the model, deﬁned throughl and lmem,\ndoes not require training a new neural network, and is unre-\nlated to the total amount of model parameters. These advan-\ntages improve the scalability of this technique. Speciﬁcally,\na model with a receptive ﬁeld spanning 3,072 nucleotides\n(l ¼ 512, ltrain\nmem ¼ 512) can process the full genome in ca. 12\nminutes, as shown in Table 4. Moreover, as shown in this\npaper, evaluation of the test set for different values ofl,\nreveals the minimal receptive ﬁeld required by the model to\nobtain optimal performances.\nNext to state-of-the-art performances, the application of\nthe transformer-XL architecture and the evaluation on the\nfull genome sequence offer new opportunities. For example,\nthe probability proﬁle of the model along the genome\nsequence could result in a better understanding of the\nmodel and the biological process. In natural language proc-\nessing, evaluation of attention (QK>) has connected seman-\ntically relevant words [6]. Investigation into the proﬁle of\nthe attention scores might pinpoint biological sites of rele-\nvancerite in a similar fashion.\nGiven the success of the models in this study, trans-\nformer based networks might show to be valuable in other\nbranches featuring sequence labeling tasks, such as second-\nary structure prediction of proteins. Due to the size of the\neukaryotic genome, application of the technique on these\ngenomes is not feasible at this point. Nevertheless, trans-\nformer-based models have not been studied before in this\nsetting, and several areas have potential for further optimi-\nzation of the training process time. These include the gen-\neral architecture of the model, batch size, ltrain, ltrain\nmem,\nlearning rate schedules, etc.\nACKNOWLEDGMENTS\nT h ea u t h o r sa c k n o w l e d g et h esupport of Ghent Univer-\nsity. Funding: JC is partially supported by the Special\nResearch Fund (BOF24j2016001002). This research\nreceived funding from the Flemish Government under\nthe “Onderzoeksprogramma Artiﬁci €ele Intelligentie (AI)\nVlaanderen” programme. Code: The code used to train\nt h em o d e l si sh o s t e da th t t p s : / / g i t h u b . c o m / j d c l a / D N A -\ntransformer.\nREFERENCES\n[1] B. Alipanahi, A. Delong, M. T. Weirauch, and B. J. Frey,\n“Predicting the sequence speciﬁcities of DNA- and RNA-binding\nproteins by deep learning,”Nat. Biotechnol., vol. 33, no. 8, pp. 831–\n838, 2015.\n[2] J. Clauwaert, G. Menschaert, and W. Waegeman, “DeepRibo: A\nneural network for precise gene annotation of prokaryotes by com-\nbining ribosome proﬁling signal and binding site patterns,”Nucleic\nAcids Res., vol. 47, no. 6, pp. e36–e36, Apr. 2019. [Online]. Available:\nhttps://academic.oup.com/nar/article/47/6/e36/5310036\n[3] H. Lin, Z. Liang, H. Tang, and W. Chen, “Identifying sigma70 pro-\nmoters with novel pseudo nucleotide composition,”IEEE/ACM Trans.\nComput. Biol. Bioinf., vol. 16, no. 4, pp. 1316–1321, Jul./Aug. 2019.\n[4] R. K. Umarov and V. V. Solovyev, “Recognition of prokaryotic\nand eukaryotic promoters using convolutional deep learning neu-\nral networks,” PLoS One, vol. 12, no. 2, Feb. 2017, Art. no.\ne0171410. [Online]. Available: https://journals.plos.org/\nplosone/article?id=10.1371/journal.pone.01714%10\n[5] J. Khanal, I. Nazari, H. Tayara, and K. T. Chong, “4mCCNN: Iden-\ntiﬁcation of N4-methylcytosine sites in prokaryotes using convo-\nlutional neural network,”IEEE Access, vol. 7, pp. 145455–145461,\n2019. [Online]. Available: https://ieeexplore.ieee.org/document/\n8846679/\n[6] A. Vaswani et al., “Attention is all you need,” inP r o c .A d v .C o n f .N e u r a l\nInf. Process. Syst., vol. 30, pp. 5998–6008, 2017.\n[7] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdi-\nnov, “Transformer-XL: Attentive language models beyond a\nﬁxed-length context,” Jan. 2019, arXiv: 1901.02860 [cs, stat] .\n[Online]. Available: http://arxiv.org/abs/1901.02860\n[8] R. Harr, M. Hastrom, and P. Gustafsson, “Search algorithm for\npattern match analysis of nuleic add sequences,”Nucl. Acids Res.,\nvol. 11, no. 9, pp. 2943–2957, May 1983. [Online]. Available:\nhttps://academic.oup.com/nar/article/11/9/2943/1006534\n[9] G. D. Stormo, “DNA binding sites: Representation and discov-\nery,” Bioinformatics, vol. 16, no. 1, pp. 16–23, Jan. 2000. [Online].\nAvailable: https://academic.oup.com/bioinformatics/article/\n16/1/16/243066\n[10] M. A. Roytberg, “A search for common patterns in many\nsequences,” Bioinformatics, vol. 8, no. 1, pp. 57–64, 1992. [Online].\nAvailable: https://academic.oup.com/bioinformatics/article-\nlookup/doi/10.1093/bioi% nformatics/8.1.57\n[11] C. Lef/C18evre and J.-E. Ikeda, “Pattern recognition in DNA sequences\nand its application to consensus foot-printing,”Bioinformatics, vol.\n9, no. 3, pp. 349–354, Jun. 1993. [Online]. Available: https://\nacademic.oup.com/bioinformatics/article/9/3/349/225342\n[12] G. D. Stormo and G. W. Hartzell, “Identifying protein-binding sites\nfrom unaligned DNA fragments.”Proc. Nat. Acad. Sci. United States\nAmerica, vol. 86, no. 4, pp. 1183–1187, Feb. 1989. [Online]. Available:\nhttp://www.pnas.org/cgi/doi/10.1073/pnas.86.4.1183\n[13] C. E. Lawrence and A. A. Reilly, “An expectation maximization\n(EM) algorithm for the identiﬁcation and characterization of com-\nmon sites in unaligned biopolymer sequences,”Proteins: Struct.\nFunction Genetics, vol. 7, no. 1, pp. 41–51, 1990. [Online]. Available:\nhttp://doi.wiley.com/10.1002/prot.340070105\n[14] J. Zhu and M. Q. Zhang, “SCPD: A promoter database of the yeast\nSaccharomyces cerevisiae,”Bioinformatics, vol. 15, no. 7, pp. 607–\n611, Jul. 1999. [Online]. Available: https://academic.oup.com/\nbioinformatics/article/15/7/607/278230\n[15] T. ºozinski, W. T. Markiewicz, T. K. Wyrzykiewicz, and\nK. L. Wierzchowski, “Effect of the sequence-dependent structure\nof the 17 bp AT spacer on the strength of consensus-likeE.coli pro-\nmoters in vivo,” Nucl. Acids Res., vol. 17, no. 10, pp. 3855–3863,\n1989. [Online]. Available: https://academic.oup.com/nar/article-\nlookup/doi/10.1093/nar/17.10.3855\n[16] D. G. Ayers, D. T. Auble, and P. L. deHaseth, “Promoter recogni-\ntion by Escherichia coli RNA polymerase: Role of the spacer DNA\nin functional complex formation,” J. Mol. Biol., vol. 207, no. 4,\npp. 749–756, Jun. 1989. [Online]. Available: http://www.\nsciencedirect.com/science/article/pii/0022283689902416\n[17] A. Kanhere and M. Bansal, “A novel method for prokaryotic pro-\nmoter prediction based on DNA stability,”BMC Bioinf., vol. 6,\nno. 1, Jan. 2005, Art. no. 1. [Online]. Available: https://doi.org/\n10.1186/1471–2105-6-1\nCLAUWAERT AND WAEGEMAN: NOVEL TRANSFORMER NETWORKS FOR IMPROVED SEQUENCE LABELING IN GENOMICS 105\n[18] R. Nikam and M. M. Gromiha, “Seq2Feature: A comprehensive\nweb-based feature extraction tool,”Bioinformatics, vol. 35, no. 22,\npp. 4797–4799, Nov. 2019. [Online]. Available: https://academic.\noup.com/bioinformatics/article/35/22/4797/5499130\n[19] B. Liu, F. Yang, D.-S. Huang, and K.-C. Chou, “iPromoter-2L: A\ntwo-layer predictor for identifying promoters and their types by\nmulti-window-based PseKNC,” Bioinformatics, vol. 34, no. 1,\npp. 33–40, Jan. 2018. [Online]. Available: https://academic.oup.\ncom/bioinformatics/article/34/1/33/4158035\n[20] B. Manavalan, T. H. Shin, and G. Lee, “PVP-SVM: Sequence-based\nprediction of phage virion proteins using a support vector machine,”\nFront. Microbiol., vol. 9, 2018, Art. no. 476. [Online]. Available:\nhttps://www.frontiersin.org/articles/10.3389/fmicb.2018.00476/\nfull\n[21] N. Goel, S. Singh, and T. C. Aseri, “An improved method for\nsplice site prediction in DNA sequences using support vector\nmachines,” Procedia Comput. Sci., vol. 57, pp. 358–367, Jan. 2015.\n[Online]. Available: http://www.sciencedirect.com/science/\narticle/pii/S1877050915018797\n[22] S. Wang, X. Cheng, Y. Li, M. Wu, and Y. Zhao, “Image-based pro-\nmoter prediction: A promoter prediction method based on evolu-\ntionarily generated patterns,” Sci. Rep., vol. 8, no. 1, Dec. 2018,\nArt. no. 17695. [Online]. Available: http://www.nature.com/\narticles/s41598–018-36308-0\n[23] P. B. Horton and M. Kanehisa, “An assessment of neural network\nand statistical approaches for prediction of E.coli Promoter sites,”\nNucleic Acids Res., vol. 20, no. 16, pp. 4331–4338, 1992. [Online].\nAvailable: https://academic.oup.com/nar/article-lookup/doi/\n10.1093/nar/20.16.4331\n[24] P. Feng, H. Yang, H. Ding, H. Lin, W. Chen, and K.-C. Chou,\n“iDNA6mA-PseKNC: Identifying DNA N6-methyladenosine sites\nby incorporating nucleotide physicochemical properties into\nPseKNC,” Genomics, vol. 111, no. 1, pp. 96–102, Jan. 2019. [Online].\nAvailable: http://www.sciencedirect.com/science/article/pii/\nS0888754318300090\n[25] F.-Y. Dao et al., “Identify origin of replication in Saccharomyces\ncerevisiae using two-step feature selection technique,”Bioinfor-\nmatics, vol. 35, no. 12, pp. 2075–2083, Jun. 2019. [Online]. Avail-\nable: https://academic.oup.com/bioinformatics/article/35/12/\n2075/5182294\n[26] W.-C. Li, E.-Z. Deng, H. Ding, W. Chen, and H. Lin, “iORI-\nPseKNC: A predictor for identifying origin of replication with\npseudo k-tuple nucleotide composition,”Chemometrics Intell. Lab.\nSyst., vol. 141, pp. 100–106, Feb. 2015. [Online]. Available: http://\nwww.sciencedirect.com/science/article/pii/S0169743914002640\n[27] W. Chen, P.-M. Feng, H. Lin, and K.-C. Chou, “iRSpot-PseDNC:\nIdentify recombination spots with pseudo dinucleotide\ncomposition,” Nucleic Acids Res., vol. 41, no. 6, Apr. 2013, Art. no.\ne68. [Online]. Available: https://www.ncbi.nlm.nih.gov/pmc/\narticles/PMC3616736/\n[28] H. Yang et al., “iRSpot-Pse6NC: Identifying recombination spots\nin Saccharomyces cerevisiaeby incorporating hexamer composition\ninto general PseKNC,”Int. J. Biol. Sci., vol. 14, no. 8, pp. 883–891,\nMay 2018. [Online]. Available: https://www.ncbi.nlm.nih.gov/\npmc/articles/PMC6036749/\n[29] B. Lee, J. Baek, S. Park, and S. Yoon, “deepTarget: End-to-end\nlearning framework for microRNA target prediction using deep\nrecurrent neural networks,” inProc. 7th ACM Int. Conf. Bioinfor-\nmatics. Comput. Biol. Health Informat., 2016, pp. 434–442.\n[30] Y. Wang et al., “Predicting DNA methylation state of CpG dinu-\ncleotide using genome topological features and deep networks,”\nSci. Rep., vol. 6, no. 1, pp. 1–15, Jan. 2016.\n[31] C. Angermueller, H. J. Lee, W. Reik, and O. Stegle, “DeepCpG:\nAccurate prediction of single-cell DNA methylation states using\ndeep learning,”Genome Biol., vol. 18, no. 1, Apr. 2017, Art. no. 67.\n[Online]. Available: https://doi.org/10.1186/s13059–017-1189-z\n[32] Y. Ju, L. Yuan, Y. Yang, and H. Zhao, “CircSLNN: Identifying\nRBP-binding sites on circRNAs via sequence labeling neural\nnetworks,” Front. Genetics, vol. 10, 2019, Art. no. 1184.\n[33] A. Krogh, I. S. Mian, and D. Haussler, “A hidden Markov model\nthat ﬁnds genes in E.coli DNA,”Nucleic Acids Res., vol. 22, no. 22,\npp. 4768–4778, Nov. 1994. [Online]. Available: https://academic.\noup.com/nar/article/22/22/4768/2400333\n[34] T. J. Wheeleret al., “Dfam: A database of repetitive DNA based on\nproﬁle hidden Markov models,”Nucleic Acids Res., vol. 41, no. D1,\npp. D70–D82, Jan. 2013. [Online]. Available: https://academic.\noup.com/nar/article/41/D1/D70/1073076\n[35] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer Normalization,” Jul.\n2016, arXiv:1607.06450 [cs, stat]. [Online]. Available: http://arxiv.\norg/abs/1607.06450\n[36] A. Santos-Zavaletaet al., “RegulonDB v 10.5: Tackling challenges to\nunify classic and high throughput knowledge of gene regulation in\nE. coli K-12,”Nucleic Acids Res., vol. 47, no. D1, pp. D212–D220, Jan.\n2019. [Online]. Available: https://academic.oup.com/nar/article/\n47/D1/D212/5160972\n[37] F. Cunningham et al., “Ensembl 2019,”Nucleic Acids Res., vol. 47,\nno. D1, pp. D745–D751, Jan. 2019. [Online]. Available: https://\nacademic.oup.com/nar/article/47/D1/D745/5165265\n[38] P. Ye, Y. Luan, K. Chen, Y. Liu, C. Xiao, and Z. Xie, “MethSMRT: An\nintegrative database for DNA N6-methyladenine and N4-methylcy-\ntosine generated by single-molecular real-time sequencing,”Nucleic\nAcids Res., vol. 45, no. D1, pp. D85–D89, Jan. 2017. [Online]. Avail-\nable: https://academic.oup.com/nar/article/45/D1/D85/2290905\n[39] A. Paszke et al., “Automatic differentiation in PyTorch,” inProc.\n31st Conf. Neural Inf. Process. Syst., 2017, pp. 1–4.\n[40] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their\ncompositionality,” in Proc. Adv. Conf. Neural Inf. Process. Syst.,\n2013, pp. 3111–3119.\n[41] M. S. Rahman, U. Aktar, M. R. Jani, and S. Shatabda, “iPro70-\nFMWin: Identifying Sigma70 promoters using multiple window-\ning and minimal features,”Mol. Genetics Genomics, vol. 294, no. 1,\npp. 69–84, Feb. 2019. [Online]. Available: https://doi.org/\n10.1007/s00438-018-1487-5\n[42] W. Chen, H. Yang, P. Feng, H. Ding, and H. Lin, “iDNA4mC:\nIdentifying DNA N4-methylcytosine sites based on nucleotide\nchemical properties,”Bioinformatics, vol. 33, no. 22, pp. 3518–3523,\nNov. 2017. [Online]. Available: https://academic.oup.com/\nbioinformatics/article/33/22/3518/4036387\nJim Clauwaert is currently working toward the\nPhD degree in the Department of Data Analysis\nand Mathematical Modelling at the faculty of Bio-\nscience Engineering, Ghent University . His\nresearch focuses on the development and appli-\ncation of deep learning methodologies for genet-\nics and synthetic biology .\nWillem Waegeman is an associate professor\nwith the Department of Data Analysis and Mathe-\nmatical Modelling at the faculty of Bioscience\nEngineering, Ghent University . His work focuses\non machine learning and data science, including\ntheoretical research and various applications in\nthe life sciences. Currently , he is mainly active in\nthe ﬁeld of multi-target prediction. In previous\nwork, ideas from other domains have been\nincluded as well, such as decision theory , statis-\ntics, fuzzy set theory, and graph theory .\n\" For more information on this or any other computing topic,\nplease visit our Digital Library atwww.computer .org/csdl.\n106 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 1, JANUARY/FEBRUARY 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5994406938552856
    },
    {
      "name": "Annotation",
      "score": 0.5373984575271606
    },
    {
      "name": "Machine translation",
      "score": 0.5044289827346802
    },
    {
      "name": "Transformer",
      "score": 0.5009889602661133
    },
    {
      "name": "Transcription (linguistics)",
      "score": 0.48392540216445923
    },
    {
      "name": "Genomics",
      "score": 0.4807029366493225
    },
    {
      "name": "Computational biology",
      "score": 0.4560842216014862
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4452192783355713
    },
    {
      "name": "Sequence motif",
      "score": 0.41275230050086975
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39639168977737427
    },
    {
      "name": "Genome",
      "score": 0.35762888193130493
    },
    {
      "name": "Biology",
      "score": 0.30555325746536255
    },
    {
      "name": "DNA",
      "score": 0.21710434556007385
    },
    {
      "name": "Genetics",
      "score": 0.2024330198764801
    },
    {
      "name": "Gene",
      "score": 0.12053632736206055
    },
    {
      "name": "Engineering",
      "score": 0.10108301043510437
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32597200",
      "name": "Ghent University",
      "country": "BE"
    }
  ]
}