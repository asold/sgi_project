{
    "title": "Collaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation",
    "url": "https://openalex.org/W4406317518",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A4292719436",
            "name": "Lv, Zheqi",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2370234944",
            "name": "Zhan, Tianyu",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A1462929032",
            "name": "Wang Wenjie",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2364048635",
            "name": "Lin Xin-Yu",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2353524629",
            "name": "Zhang, Shengyu",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2353846860",
            "name": "Zhang, Wenqiao",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2225948551",
            "name": "Li Jiwei",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2430772236",
            "name": "Kuang, Kun",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A1841886282",
            "name": "Wu Fei",
            "affiliations": [
                "Zhejiang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4386728933",
        "https://openalex.org/W4224316819",
        "https://openalex.org/W3140581741",
        "https://openalex.org/W3176915718",
        "https://openalex.org/W4368755500",
        "https://openalex.org/W6600210674",
        "https://openalex.org/W4296591867",
        "https://openalex.org/W6600130426",
        "https://openalex.org/W4387967965",
        "https://openalex.org/W4387848745",
        "https://openalex.org/W4288084628",
        "https://openalex.org/W4382239956",
        "https://openalex.org/W4401863553",
        "https://openalex.org/W6600062020",
        "https://openalex.org/W6601033504",
        "https://openalex.org/W4393148126",
        "https://openalex.org/W4367046711",
        "https://openalex.org/W4401857150",
        "https://openalex.org/W4403780716",
        "https://openalex.org/W4396758715",
        "https://openalex.org/W4283662558",
        "https://openalex.org/W4367047075",
        "https://openalex.org/W4213227052",
        "https://openalex.org/W4389523765",
        "https://openalex.org/W4402671715",
        "https://openalex.org/W2783272285",
        "https://openalex.org/W4403791661",
        "https://openalex.org/W4401834466",
        "https://openalex.org/W4284974283",
        "https://openalex.org/W3172419167",
        "https://openalex.org/W6600281463",
        "https://openalex.org/W4384652102",
        "https://openalex.org/W3192113933",
        "https://openalex.org/W4312824836",
        "https://openalex.org/W4403578191",
        "https://openalex.org/W6600195515",
        "https://openalex.org/W4387846482",
        "https://openalex.org/W4400527101",
        "https://openalex.org/W4394994587",
        "https://openalex.org/W4304080519",
        "https://openalex.org/W2723293840",
        "https://openalex.org/W6600008909",
        "https://openalex.org/W4401024755",
        "https://openalex.org/W4403792529",
        "https://openalex.org/W4401024639",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W4401856724",
        "https://openalex.org/W3205778609",
        "https://openalex.org/W2623399293"
    ],
    "abstract": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising research direction that has demonstrated exceptional performance in this field. However, its inability to capture real-time user preferences greatly limits the practical application of LLM4Rec because (i) LLMs are costly to train and infer frequently, and (ii) LLMs struggle to access real-time data (its large number of parameters poses an obstacle to deployment on devices). Fortunately, small recommendation models (SRMs) can effectively supplement these shortcomings of LLM4Rec diagrams by consuming minimal resources for frequent training and inference, and by conveniently accessing real-time data on devices. In light of this, we designed the Device-Cloud LLM-SRM Collaborative Recommendation Framework (LSC4Rec) under a device-cloud collaboration setting. LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the benefits of cloud and edge computing, achieving a complementary synergy. We enhance the practicability of LSC4Rec by designing three strategies: collaborative training, collaborative inference, and intelligent request. During training, LLM generates candidate lists to enhance the ranking ability of SRM in collaborative scenarios and enables SRM to update adaptively to capture real-time user interests. During inference, LLM and SRM are deployed on the cloud and on the device, respectively. LLM generates candidate lists and initial ranking results based on user behavior, and SRM get reranking results based on the candidate list, with final results integrating both LLM's and SRM's scores. The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists. Our comprehensive and extensive experimental analysis validates the effectiveness of each strategy in LSC4Rec.",
    "full_text": "Collaboration of Large Language Models and Small\nRecommendation Models for Device-Cloud Recommendation\nZheqi Lv\nZhejiang University\nHangzhou, China\nzheqilv@zju.edu.cn\nTianyu Zhan\nZhejiang University\nHangzhou, China\nyuzt@zju.edu.cn\nWenjie Wangâˆ—\nNational University of Singapore\nSingapore, Singapore\nwenjiewang96@gmail.com\nXinyu Lin\nNational University of Singapore\nSingapore, Singapore\nxylin1028@gmail.com\nShengyu Zhangâˆ—\nZhejiang University\nHangzhou, China\nsy_zhang@zju.edu.cn\nWenqiao Zhang\nZhejiang University\nHangzhou, China\nwenqiaozhang@zju.edu.cn\nJiwei Li\nZhejiang University\nHangzhou, China\njiwei_li@zju.edu.cn\nKun Kuangâˆ—\nZhejiang University\nHangzhou, China\nkunkuang@zju.edu.cn\nFei Wu\nZhejiang University\nHangzhou, China\nwufei@zju.edu.cn\nAbstract\nLarge Language Models (LLMs) for Recommendation (LLM4Rec) is\na promising research direction that has demonstrated exceptional\nperformance in this field. However, its inability to capture real-time\nuser preferences greatly limits the practical application of LLM4Rec\nbecause (i) LLMs are costly to train and infer frequently, and (ii)\nLLMs struggle to access real-time data (its large number of pa-\nrameters poses an obstacle to deployment on devices). Fortunately,\nsmall recommendation models (SRMs) can effectively supplement\nthese shortcomings of LLM4Rec diagrams by consuming minimal\nresources for frequent training and inference, and by conveniently\naccessing real-time data on devices.\nIn light of this, we designed the Device-Cloud LLM-SRM\nCollaborative Recommendation Framework (LSC4Rec) under a\ndevice-cloud collaboration setting. LSC4Rec aims to integrate the\nadvantages of both LLMs and SRMs, as well as the benefits of cloud\nand edge computing, achieving a complementary synergy. We en-\nhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent re-\nquest. During training, LLM generates candidate lists to enhance\nthe ranking ability of SRM in collaborative scenarios and enables\nSRM to update adaptively to capture real-time user interests. Dur-\ning inference, LLM and SRM are deployed on the cloud and on\nthe device, respectively. LLM generates candidate lists and initial\nranking results based on user behavior, and SRM get reranking\nresults based on the candidate list, with final results integrating\nâˆ—Corresponding authors.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™25, Toronto, ON, Canada\nÂ© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-1245-6/25/08\nhttps://doi.org/10.1145/3690624.3709335\nboth LLMâ€™s and SRMâ€™s scores. The device determines whether a\nnew candidate list is needed by comparing the consistency of the\nLLMâ€™s and SRMâ€™s sorted lists. Our comprehensive and extensive\nexperimental analysis validates the effectiveness of each strategy\nin LSC4Rec.\nCCS Concepts\nâ€¢ Computing methodologies â†’Cooperation and coordina-\ntion; â€¢ Information systems â†’Retrieval models and ranking .\nKeywords\nSequential Recommendation, Device-Cloud Collaboration, Large\nLanguage Model\nACM Reference Format:\nZheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao\nZhang, Jiwei Li, Kun Kuang, and Fei Wu. 2025. Collaboration of Large\nLanguage Models and Small Recommendation Models for Device-Cloud\nRecommendation. In Proceedings of the 31st ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining V.1 (KDD â€™25), August 3â€“7, 2025,\nToronto, ON, Canada. ACM, New York, NY, USA, 12 pages. https://doi.org/\n10.1145/3690624.3709335\n1 Introduction\nTraditional sequential recommendation models, such as DIN [64],\nGRU4Rec [12], SASRec [15], and so on, have achieved great success\nin both academia and industry. Here, we refer to these models that\ndo not use large language models (LLMs) as their backbone and\ndo not integrate user behavior into textual prompts as input for\nrecommendation as small recommendation models (SRMs). Cur-\nrently, LLMs have been explored for recommendation scenarios\n(LLM4Rec) [11, 45, 61], demonstrating stronger multi-task gener-\nalization capabilities and superior single-task performance, which\nposition LLM4Rec as a highly promising research direction.\nHowever, LLMs face challenges in capturing real-time user pref-\nerences for two reasons: (i) Expensive to train and inference\nfrequently. LLMs are built with an immense number of parame-\nters [1, 11, 42], posing difficulties for the efficiency of LLMâ€™s training\narXiv:2501.05647v2  [cs.IR]  25 Feb 2025\nKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Zheqi Lv et al.\n(a) Recommendation System Paradigm\nOutput\nOutput\n(b) LSC4Rec\nâ‡“\nLLM for Recall\nSRM for Rerank\nLSC for Update\nSRM for Rerank\nOutput\n(a1) SRM for Rank\n(a2) LLM for Rank\nâ‡‘\nğ‘£! ğ‘£\"\nğ‘£# â€¦\nğ‘£! ğ‘£\"\nğ‘£# â€¦\nğ‘£! ğ‘£\"\nğ‘£# â€¦\nğ‘£! ğ‘£\"\nğ‘£# â€¦\n Recall\nAgain\nCollaborative\nRank\nReal-time\nBehaviors\nReal-time\nBehaviors\nLLM\nSRM\nLLM\nSRMReal-time\nBehaviors\nLagged\nBehaviors\nCandidate\nList Reach consensus\nFail to \nReach consensus\n(c) A Case of LSC4Rec\nRanking\nby LLM\nRanking\nby SRM\nCollaborative\nRank\nğ‘£! ğ‘£\"\nğ‘£# â€¦\nLegend\nCandidate\nList\nOn Device On Cloud\nğ‘£! ğ‘£\"\nğ‘£# â€¦\nğ‘£! ğ‘£\"\nğ‘£$ â€¦\nReal-time\nBehaviors\nHistory\nBehaviors\nLLM SRM\nCandidate\nList\nConsensus\nJudgement\nRanking\nby LLM\nRanking\nby SRM\nFail to Reach\nConsensus\nSuccess to \nConsensus\nFrom Devices\nto Cloud\nFrom Cloud\nto Devicesâ‡“ â‡‘\nâ€¦\nâ€¦\nRecall\nAgain\nâ€¦\nâ€¦\n â€¦\nConsensus \nJudgement\nEnhance\nEnhance\nFigure 1: (a) respectively describes the conventional SRM4Rec and LLM4Rec diagrams. (b) is an overview of the proposed\nLSC4Rec, a device-cloud collaboration framework, which includes a collaborative training strategy, a collaborative inference\nstrategy, and a collaborative update strategy. (c) is a qualitative case of our LSC4Rec framework, showing effective collaborative\ninference and collaborative-decision request.\nand inference. Moreover, the rapid increase of user behavior data\nexacerbates the situation, making frequent training of LLMs im-\npractical. (ii) Difficult to access real-time user behavior data. In\npractical applications, user real-time behaviors are generated on de-\nvices while trainable LLMs are typically deployed on the cloud due\nto computational resource considerations [ 33â€“35, 48, 49]. Trans-\nmitting tremendous user data to the cloud in real time could cause\nnon-trivial communication costs and latency.\nAs a counterpart, SRMs could potentially supplement the short-\ncomings faced by LLM4Rec. (i) SRM can be trained and inferred\nfrequently at a significantly lower cost. The smaller number of\nparameters and simpler model structure of SRM make its training\nand inference less resource-intensive, allowing for frequent training\nand inference for SRM. (ii) SRM can easily access all real-time\ndata. The minimal use of storage and computational resources in\nSRM enables its deployment on devices. Therefore, accessing and\nhandling real-time data could be much more efficient for SRM.\nIn this work, we propose complementing LLMs with SRMs for\nefficient and effective device-cloudCollaborative Recommendation,\nnamely LSC4Rec. The basic intuition of LSC4Rec lies in the dual\noptimization of on-cloud LLMs for item candidate generation and\non-device SRMs for item reranking. In a recommendation session,\non-cloud LLMs leverage the strong user preference capturing ability\nto generate an item candidate list, on-device SRMs are adapted\nto encode the userâ€™s real-time behaviors and promptly score the\nitems in the candidate list for real-time reranking. In such a device-\ncloud framework, LSC4Rec can reduce the training and inference\nfrequency of on-cloud LLMs by using on-device SRMs to capture\nusersâ€™ real-time preferences. Meanwhile, LSC4Rec can diminish the\ncommunication costs between the cloud and devices.\nHowever, it is non-trivial to instantiate LSC4Rec due to the fol-\nlowing issues:\nâ€¢How can the SRM contribute to the LSC4Rec framework? Sim-\nply training the SRM and LLM separately and then deploying\nthem in the LSC4Rec framework is empirically found inef-\nfective, where SRM struggles to effectively rerank the item\ncandidates from LLMs. Due to the prior knowledge of LLMs,\ntheir generated item candidates have a different distribution\nof negative samples from that of randomly sampled negative\nones in SRM. Therefore, SRM cannot accurately distinguish\nthe positive and negative sampled generated by the LLM dur-\ning inference (Refer to empirical evidence in Table 4).\nâ€¢How to fully utilize the capabilities of the LLM during the\ninference stage? In the inference stage, after the LLM gener-\nates a candidate list, only using the SRM to score items in the\ncandidate list based on real-time data is equivalent to discard-\ning part of the LLMâ€™s powerful generalization ability. This\nresults in limited effectiveness of the LLM during inference,\nmaking it difficult to provide continuous benefits to the SRMâ€™s\nranking.\nâ€¢How to determine the shift of user preferences and update\nuser behaviors from devices to the cloud? In recommendation\nscenarios, user preferences are shifting over time. Therefore, it\nis essential to identify the user preference shifts and promptly\nupdate user behaviors from the device to the cloud, thereby ob-\ntaining a new candidate list from the LLM that better matches\nthe userâ€™s latest preferences.\nTo address the aforementioned challenges of LSC4Rec, as shown\nin Figure 1, we designed collaborative training, collaborative infer-\nence, and collaborative-decision request. Specifically,\nâ€¢Collaborative Training. Both LLM and SRM are first pre-\ntrained based on historical data separately. Thereafter, we\nsegment and combine items in the candidate list generated by\nLLM and feed this enhanced data to SRM to improve its ability\nto rank the candidate list retrieved by LLM. At the same time,\nwe allow SRM to undergo adaptive training to adapt to the\ndata distribution on the device, further enhancing its ability\nto extract real-time user interests more effectively.\nâ€¢Collaborative Inference. LLM infers a list of candidate items\nand their ranking based on user behavior, which may come\nfrom a pool of millions or even billions of items. Then, SRM\nCollaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada\nextracts user interests from real-time behavior to rerank the\ncandidate items and obtain a ranking, and the reranking re-\nsults of SRM are integrated with the initial ranking results\nof LLM through our designed score integration strategy to\nproduce collaborative inference results. The score integration\nstrategy includes normalizing the scores of LLM and SRM to\nthe same range, score filtering, and weight allocation.\nâ€¢Collaborative-Decision Request. It determines the necessity\nof requesting a new candidate list from LLM by comparing\nthe similarity between these two sets of interests. Before out-\nputting the results, LSC4Rec needs to compare the inconsis-\ntency of the initial ranking results based on LLM with the\nreranking results based on SRM. If there is low inconsistency\n(i.e., similar ranking results), the collaborative inference result\nis output. Otherwise, real-time behavior is transmitted to the\nLLM in the cloud to generate a new candidate list and ranking\nresults based on short-term interests.\nIn summary, our contributions include four aspects:\nâ€¢We concentrate on a promising but under-explored research\ndirection, specifically the challenges faced by LLMs in captur-\ning user preferences in real-time during practical applications.\nâ€¢We propose a novel collaborative recommendation framework\ncalled LSC4Rec. LSC4Rec effectively combines the strengths\nof LLMs and SRMs in a device-cloud system, paving the way\nfor a promising direction in future research.\nâ€¢We design collaborative training, collaborative inference, and\ncollaborative-decision request to address the three challenges\nof LSC4Rec and make it more practical.\nâ€¢We have conducted comprehensive and extensive experiments\non various LLMs and SRMs across multiple datasets. Results\nshow the effectiveness of LSC4Rec.\n2 Related Work\n2.1 LLM for Recommendation\nLLMs are increasingly prominent in natural language process-\ning [16, 18, 21, 25, 26, 40, 43, 46, 50, 65, 66], driving research in\ntheir application for recommendation systems [6, 11, 17, 22, 23, 31,\n52, 57, 62]. The advent of generative models like GPT transformed\nLLM-based recommendation into generative processes, treating rec-\nommendations as natural language tasks [44]. Early methods relied\non prompting [10, 39] or in-context learning [24] but often fell short\nof task-specific models. Recent advancements involve fine-tuning\nLLMs for better alignment with recommendation tasks. P5 [11] in-\ntroduced a unified framework for fine-tuning FLAN-T5 [36] across\nvarious recommendation tasks. InstructRec [53] and TALLRec [1]\nfurther adapted FLAN-T5 and LLaMA models, respectively, for\nrecommendation tasks using instruction tuning. GenRecâ€™s [13] ap-\nproach involves direct instruction tuning on the LLaMA model for\nthe generative recommendation, showcasing the evolving strategies\nin integrating LLMs with recommendation systems. P5 [ 11] and\nPOD [17] are two large recommendation models based on the T5,\naiming to unify the multiple recommendation tasks. They trans-\nform the recommendation task into textual format and use the text\nas input to the LLM to obtain recommendation results. LLMs pos-\nsess stronger generalization capabilities compared to SRMs but face\nchallenges in terms of high costs and deployment, making it diffi-\ncult to integrate real-time user preferences. The aforementioned\nstudies have not addressed this issue.\n2.2 Sequential Recommendation\nSequential recommendation algorithms model usersâ€™ behavior se-\nquences and predict their next actions. They have been widely\napplied in scenarios such as e-commerce, short video recommenda-\ntions, education, and healthcare. Classic recommendation models\nsuch as Caser [41], GRU4Rec [12], DIN [64], and SASRec [15] are\nstill commonly used today. In recent years, many studies have fo-\ncused on different aspects of sequential recommendation, such as\npersonalization [32â€“34, 38], multimodality [14, 51, 54, 55], privacy\nprotection [20], user feedback [9, 56], cross-domain [27, 63], cold\nstart [4, 28], long-tail [59], session-based [37], debias [3], structure\nlearning [7], disentanglement [5], LLM-based [2, 58], generative-\nbased [23, 60], fairness [47], etc. Although Sequential Recommen-\ndation Models (SRMs) require fewer hardware resources and are\neasier to deploy on devices, they have limitations in feature extrac-\ntion and generalization capabilities. These studies, however, have\nnot addressed the deployment of SRMs on devices or the associated\nlimitations.\n2.3 Device-Cloud Recommendation\nIn more realistic recommendation scenarios, the collaboration of\ndevice intelligence and cloud intelligence is often required [8, 19, 29,\n30, 33â€“35, 48, 49]. For instance, DCCL [49] and MetaController [48]\nmade early attempts at device-cloud collaborative recommendation\nmodels. DUET [34] and IntellectReq [ 33] explored ways to help\ndevice models overcome the generalization limitations imposed by\ntheir model size. Their specific approach involves having the device\nmodel handle personalization, while the cloud model generates\nthe necessary parameters for the device model based on data from\nthe device. However, when LLMs are used as cloud models, they\nintroduce greater cloud computing loads and pose challenges in\ndirectly collaborating with SRMs. These studies do not address\ndevice-cloud collaborative recommendation strategies when LLMs\nserve as the cloud model.\n3 Methodology\n3.1 Problem Formulation and Notation\n3.1.1 Data. We use X= {ğ‘¢,ğ‘£,ğ‘  }to represent a piece of data, and\nY= {ğ‘¦}to represent the corresponding label. Specifically, ğ‘¢,ğ‘£,ğ‘ \nrepresent user ID, item ID and userâ€™s click sequence respectively.\nHistorical data collected for a period of time from devices is rep-\nresented by Dğ» = {X(ğ‘–)\nğ» ,Y(ğ‘–)\nğ» }Nğ»\nğ‘–=1 . The data augmented based\non Dğ» is denoted as Dğ»âˆ’Aug. In the inference stage, users will\ngenerate real-time behaviors on the devices. These real-time be-\nhavioral data are represented as Dğ‘… = {X(ğ‘–)\nğ‘… ,Y(ğ‘–)\nğ‘… }Nğ‘…\nğ‘–=1. The lagged\nbehavioral data are represented as D\nâ€²\nğ‘…. D\nâ€²\nğ‘… is also data generated\nby users during the inference stage, and it represents the last set of\ndata uploaded to the cloud by the user. Since the device-to-cloud\nsynchronization is not real-time, there is a time difference between\nD\nâ€²\nğ‘… and Dğ‘…. More precisely, D\nâ€²\nğ‘… lags behind Dğ‘… by Î”ğ‘¡ time units.\nKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Zheqi Lv et al.\n(a) Collaborative Training\nâ‡“LLM for Recall\nSRM for Rerank\nğ‘£!ğ‘£\"ğ‘£#â€¦\nğ‘£!ğ‘£\"ğ‘£$â€¦\n(b) Collaborative Inference\n!ğ‘¦ ğ‘¦Loss\n!ğ‘¦ ğ‘¦Loss\nIndependent Training\nğ‘£!ğ‘£\"ğ‘£$â€¦ !ğ‘¦ ğ‘¦Loss\nCollaborative Training\nNormalizationWeighted Sum\n(c) Collaborative-decision Request\nOutput\nRank1 2 3 â€¦Item355170382907â€¦\nRank1 2 3 â€¦Item703835512907â€¦\nDiff.1 1 0 â€¦ConsensusList\nSmallDifferenceLargeDifference\nResult Fusion Recall AgainCollaborative Inference\nAdaptive Re-Training!ğ‘¦Loss\nğ‘£!ğ‘£\"ğ‘£#â€¦\nğ‘£!ğ‘£\"ğ‘£#â€¦\nReal-timeBehaviors\nReal-timeBehaviors\nLagged BehaviorsHistory Behaviors\nLegend\nOn DeviceOn Cloud\nğ‘£!ğ‘£\"ğ‘£$â€¦ğ‘£!ğ‘£\"ğ‘£#â€¦ Real-timeBehaviorsHistoryBehaviors\nLLM SRM\nCandidateList ConsensusJudgement\nRankingby LLM Rankingby SRM\nFailtoReachConsensusSuccess to ConsensusFrom Devicesto CloudFrom Cloud to Devicesâ‡“ â‡‘\nFigure 2: Overview of the LSC4Rec. (a) describes collaborative training, including independent training, cooperative training,\nand adaptive re-training. (b) describes collaborative inference, including cooperative inference and result fusion. (c) describes\ncollaborative-decision request.\n3.1.2 Model. LLM and SRM are denoted as Mğ¿ and Mğ‘†, respec-\ntively. The output of LLM, that is, the candidate list recalled by LLM\nbased on Dğ» or Dğ‘…, is denoted as S= {ğ‘£ğ‘–}Ncandidate\n1 . The order of\nitems in this list is considered to be the initial ranking, denoted\nas Ë†ğ‘Œinit. The output of SRM re-ranking based on real-time data is\ndenoted as Ë†ğ‘Œrerank.\n3.1.3 Formula. To facilitate the description of the training process,\nhere we use ğœ™(D)to represent that the model is trained using data\nD, ğœ™ğ‘…ğ‘’(D)to represent that the model is re-trained using data D,\nğ´ğ‘¢ğ‘”(D)to represent data augmentation based on data D, Ã‰ to\nrepresent collaborative algorithms, Î  to represent the function that\ndetermines the inconsistency or similarity of the sorting of two\nlists. We formulate our proposed LSC4Rec as follows,\nCollaborative Training:\nMğ¿ = ğœ™(Dğ»),Mğ‘† = ğœ™(Dğ»);\nMğ‘† = ğœ™ğ‘…ğ‘’({Dğ»,Dğ»âˆ’Aug = ğ´ğ‘¢ğ‘”(Mğ¿(Dğ»))});\nMğ‘† = ğœ™(Dğ‘…).\n(1)\nCollaborative Inference:\nS, Ë†ğ‘Œinit = Mğ¿(D\nâ€²\nğ‘…);\nË†ğ‘Œrerank = Mğ‘†(S; Dğ‘…);\nË†ğ‘Œ = Ë†ğ‘Œinit\nÃŠ Ë†ğ‘Œrerank.\n(2)\nCollaborative-Decision Request:\nRequest = 1(Î (Ë†Yinit,Ë†Yrerank)â‰¥ Threshold). (3)\nNote that the function symbols here are rough and are only used\nto formulate the task description. The details of the formula will be\nelaborated in subsequent subsections of this section.\n3.2 LSC4Rec\nThis section describes our proposed LSC4Rec. As shown in Fig-\nure 2, our LSC4Rec framework includes (a) Collaborative Training,\n(b) Collaborative Inference, and (c) Collaborative-Decision Request.\nCollaborative Training aims to enable SRM to provide benefits to the\nLSC4Rec framework. Collaborative Inference aims to complement\nthe strengths of LLM and SRM during the inference stage, result-\ning in more accurate recommendation outcomes. Collaborative-\nDecision Request targets to determine whether there has been a\nshift in user preferences, in order to decide whether the candidate\nlist needs to be updated.\n3.2.1 Retropy of Existing Sequential Recommendation Diagrams.\nHere we review the paradigm of sequential recommendation.\nIn the training stage, if we disregard the process of data textual-\nization required for LLM4Rec, the method by which LLM4Rec and\nSRM4Rec obtain loss is the same, and can be formalized as follows,\nL=\nâˆ‘ï¸\nğ‘¢,ğ‘£,ğ‘ ,ğ‘¦âˆˆDğ»\nğ‘™(ğ‘¦, Ë†ğ‘¦ = M(ğ‘¢,ğ‘£,ğ‘  )). (4)\nIn the above equation, ğ‘™ represents loss function, Mgenerically\nrepresents both LLM and SRM. M(Â·) denotes the output of the\nmodel when Â·is given as input. During gradient backpropagation,\nLLM4Rec and SRM4Rec differ slightly. LLM4Rec has the option to\nupdate the instruction, prompt, and model, while SRM4Rec typically\nfocuses on updating the model itself.\nIn the inference stage, if we disregard the process of data textu-\nalization required for LLM4Rec, the inference processes of both are\nessentially consistent.\n3.2.2 Collaborative training. Collaborative Training consists of In-\ndependent training, Cooperative training, and Adaptive re-training.\nIndependent training. When LLM and SRM are trained sepa-\nrately, the calculation of loss is the same as equation 4. The back-\npropagation for SRM can be formalized asarg minğœƒğ‘† ğ¿. In the above\nformula, ğœƒğ‘† represents the parameter of Mğ‘†. Based on prior knowl-\nedge, we fix the instruction and prompt of the LLM and then fine-\ntune the modelâ€™s parameters. The backpropagation of LLM can be\nformalized as arg minğœƒğ¿ ğ¿. In the above equation, ğœƒğ¿ represents the\nparameter of Mğ¿.\nCooperative training.After independent training, we inputXğ»\ninto the LLM to obtain inference results. For each sample ğ‘¥, we\ninfer Nğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ items, forming the candidate list ğ‘†. Furthermore,\nwe freely combine items that are difficult for the LLM to distinguish,\nthat is, items with similar tokens, to obtain an enhanced candidate\nlist ğ‘†Aug. Subsequently, we also input each sample ğ‘¥ into the SRM,\nallowing the SRM to re-rank within the candidate list ğ‘†Aug. The\nresulting prediction is denoted as Ë†ğ‘¦. So the loss function can be\nformulated as,\nğ‘† = Mğ¿(ğ‘¥), ğ‘† Aug = Augment(ğ‘†), (5)\nCollaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada\nL=\nâˆ‘ï¸\nğ‘¢,ğ‘£,ğ‘ ,ğ‘¦âˆˆDğ»\nğ‘™(ğ‘¦, Ë†ğ‘¦ = Mğ‘†(ğ‘¢,ğ‘£,ğ‘  )|ğ‘†Aug). (6)\nWe fix the model parameters of LLM so that the gradient generated\nby loss only backpropagates to SRM, so the optimization function\nis arg minğœƒğ‘† ğ¿.\nAdaptive re-training. After LLM and SRM are trained, LLM is\ndeployed in the cloud and SRM on the device. However, as men-\ntioned in the Introduction section, due to network bandwidth limi-\ntations and delays in data processing on the device side, it is difficult\nfor LLM to access real-time data. Even if it can access such data,\nits large number of parameters makes it challenging to train and\ndeploy the model quickly. Therefore, SRM needs to be trained based\non real-time data on the device, with the optimization goal being\narg minğœƒğ‘† ğ¿, where the loss function is as follows,\nL=\nâˆ‘ï¸\nğ‘¢,ğ‘£,ğ‘ ,ğ‘¦âˆˆDğ‘…\nğ‘™(ğ‘¦, Ë†ğ‘¦ = Mğ‘†(ğ‘¢,ğ‘£,ğ‘  )). (7)\n3.2.3 Collaborative inference. Collaborative Inference consists of\nCooperative Inference and Result Fusion.\nCooperative Inference. After training is completed, LLM and\nSRM are deployed separately in the cloud and on the device. Here,\nwe introduce how to merge the initial ranking resultsË†ğ‘Œğ‘–ğ‘›ğ‘–ğ‘¡ output by\nLLM with the re-ranking resultsË†ğ‘Œrerank of SRM during the inference\nstage.\nS, Ë†ğ‘Œinit = Mğ¿(D\nâ€²\nğ‘…);\nË†ğ‘Œrerank = Mğ‘†(S; Dğ‘…).\n(8)\nResult Fusion. LLM and SRM have different ranking principles;\nLLM is generative while SRM is non-generative, leading to different\nranking scales for each. Therefore, an important step is normaliza-\ntion, which places Ë†ğ‘Œinit and Ë†ğ‘Œrerank on the same scale to better merge\nthe ranking results. We use Ë†ğ‘ƒinit to represent the interaction proba-\nbility between users and products, and Ë†ğ‘Œinit is also obtained from\nË†ğ‘ƒinit. Similarly, Ë†ğ‘ƒrerank is also used to obtain Ë†ğ‘Œrerank. The process of\nnormalization can be formalized as follows,\nï£±ï£´ï£´ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£´ï£´ï£³\nË†ğ‘ƒnorm\ninit =\nË†ğ‘ƒinit âˆ’min(Ë†ğ‘ƒinit)\nmax(Ë†ğ‘ƒinit)âˆ’min(Ë†ğ‘ƒinit)\n,\nË†ğ‘ƒnorm\nrerank =\nË†ğ‘ƒrerank âˆ’min(Ë†ğ‘ƒrerank)\nmax(Ë†ğ‘ƒrerank)âˆ’min(Ë†ğ‘ƒrerank)\n.\n(9)\nAfter normalization and filtering, we adjust the hyperparameter\nğ›¼ and ğ›½ to merge Ë†ğ‘Œinit and Ë†ğ‘Œrerank,\n( Ë†ğ‘ƒ = ğ›¼Â·Ë†ğ‘ƒğ‘›ğ‘œğ‘Ÿğ‘š\ninit +(1 âˆ’ğ›¼)Â· Ë†ğ‘ƒğ‘›ğ‘œğ‘Ÿğ‘š\nrerank,\nË†ğ‘Œ = sort(ğ‘†, Ë†ğ‘ƒ).\n(10)\n3.2.4 Collaborative-Decision Request. After collaborative training\nand collaborative inference, the LSC4Rec has formed a complete\nframework. This framework can compensate for the performance\ndecline due to LLMâ€™s difficulty in obtaining real-time data as much\nas possible through SRM, making LSC4Rec more effective than\nusing LLM4Rec or SRM4Rec independently in this system. However,\na noteworthy point is that when LSC4Rec should invoke LLM to\nprovide new inference results based on real-time data has not been\naddressed. To fill this gap, we designed the Collaborative-Decision\nRequest feature for LSC4Rec.\nWe utilize the positional difference in rankings of the same ele-\nment by Ë†ğ‘¦init and Ë†ğ‘¦rerank to calculate a inconsistency score ğ‘. As-\nsuming posp (q)represents the position of element ğ‘in list ğ‘, the\ninconsistency score ğ‘ can be formalized as follows,\nğ‘ = 1\nğ‘›\nâˆ‘ï¸\nğ‘¢âˆˆË†ğ‘¦initâˆ©Ë†ğ‘¦rerank\n|posË†yinit (u)âˆ’posË†yrerank (u)|, (11)\nwhere Ë†ğ‘¦init and Ë†ğ‘¦rerank In the above formula, Ë†ğ‘¦init âˆ©Ë†ğ‘¦rerank = Ë†ğ‘¦init =\nË†ğ‘¦rerank. This is because the elements in lists Ë†ğ‘¦init and Ë†ğ‘¦rerank are\nexactly the same, only their order differs.\nRequest = 1(c â‰¥Threshold). (12)\nIn the equation above, 1(Â·)is the indicator function. To get the\nthreshold, we need to collect user data for a period of time, then\nget the inconsistency ğ‘ corresponding to these data on the cloud\nand sort them, and then set the threshold according to the load of\nthe cloud server. That is, the threshold can be obtained during the\ntraining on the training set. For example, if the load of the cloud\nserver needs to be reduced by 90%, that is, when the load is only 10%\nof the previous value, only the minimum 10% position value needs\nto be sent to each device as the threshold. During the inference\nprocess, each device determines whether it needs to upload real-\ntime data to the LLM for inference, based on formulas 11 and 12.\nThis is done to ensure the most optimal LLM invocation under\nany device-cloud communication resources and LLM invocation\nresources.\n4 Experiments\n4.1 Experimental Setup\n4.1.1 Datasets. We evaluated our method on three widely used\ndatasets, which are all collected from an e-commerce platform\nAmazon1 and cover various product categories such as books, elec-\ntronics, home goods, and more, they usually include user reviews,\nratings, product descriptions, prices, and other information. We\nalso evaluated our method on Yelp2 which contains details about\nbusinesses, user reviews, ratings, business categories, addresses,\nand more. In accordance with convention, all user-item pairs in\nthe datasets were considered as positive samples. For the training\nand test sets, user-item pairs that did not exist in the datasets were\nsampled as negative samples [11, 17]. During testing, we rank on a\nsample set consisting of all items. In these datasets, we use a userâ€™s\nlast behavior for testing, the second to last behavior for validation,\nand the remaining behaviors for training.\n4.1.2 Baselines. We used the following SRMs and LLMs for the\nexperiments.\nâ€¢SRMs. DIN [64], GRU4Rec [12], and SASRec [15] are three\nhighly prevalent SRMs in both academic research and the\nindustry. They each incorporate different techniques, such as\nGRU (Gated Recurrent Unit), Attention, and Self-Attention,\nto enhance the recommendation process. One point to note is\nthat although models like BERT4Rec and E4SRec achieve bet-\nter performance, their significantly larger number of model\nparameters results in far greater resource consumption com-\npared to the aforementioned lightweight SRMs. Therefore, we\n1https://jmcauley.ucsd.edu/data/amazon/\n2https://www.yelp.com/dataset\nKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Zheqi Lv et al.\nTable 1: Performance comparison of baselines and LSC4Rec.\nDataset Model Metric\nNDCG@5 NDCG@10NDCG@20HR@5 HR@10 HR@20 Precision@5Precision@10Precision@20\nBeauty\nDIN (RT) 0.0079 0.0106 0.0135 0.0131 0.0226 0.0311 0.0026 0.0022 0.0018\nGRU4Rec (RT) 0.0081 0.0105 0.0129 0.0136 0.0212 0.0308 0.0026 0.0022 0.0017\nSASRec (RT) 0.0066 0.0096 0.0127 0.0111 0.0207 0.0310 0.0022 0.0021 0.0018\nP5 (RT) 0.0227 0.0257 0.0289 0.0305 0.0400 0.0525 0.0061 0.0040 0.0026\nDIN (NRT) 0.0017 0.0026 0.0042 0.0021 0.0039 0.0083 0.0014 0.0014 0.0015\nGRU4Rec (NRT) 0.0017 0.0023 0.0031 0.0020 0.0033 0.0054 0.0014 0.0012 0.0010\nSASRec (NRT) 0.0014 0.0021 0.0032 0.0018 0.0033 0.0060 0.0013 0.0011 0.0011\nP5 (NRT) 0.0087 0.0108 0.0136 0.0132 0.0200 0.0310 0.0027 0.0020 0.0016\nOurs (P5+SASRec)0.0094 0.0126 0.0154 0.0150 0.0248 0.0361 0.0030 0.0025 0.0018\nImprove 8.41% 16.16% 13.45% 13.14% 23.91% 16.59% 13.21% 24.00% 16.77%\nToys\nDIN (RT) 0.0046 0.0063 0.0081 0.0076 0.0128 0.0202 0.0015 0.0013 0.0010\nGRU4Rec (RT) 0.0050 0.0073 0.0098 0.0085 0.0156 0.0253 0.0017 0.0016 0.0013\nSASRec (RT) 0.0052 0.0073 0.0101 0.0080 0.0148 0.0259 0.0016 0.0015 0.0013\nP5 (RT) 0.0187 0.0204 0.0221 0.0237 0.0291 0.0358 0.0047 0.0029 0.0018\nDIN (NRT) 0.0007 0.0010 0.0017 0.0010 0.0016 0.0035 0.0006 0.0005 0.0006\nGRU4Rec (NRT) 0.0009 0.0015 0.0020 0.0011 0.0023 0.0037 0.0008 0.0008 0.0007\nSASRec (NRT) 0.0015 0.0020 0.0026 0.0019 0.0030 0.0046 0.0012 0.0010 0.0008\nP5 (NRT) 0.0066 0.0078 0.0090 0.0096 0.0133 0.0183 0.0019 0.0013 0.0009\nOurs (P5+SASRec)0.0066 0.0084 0.0096 0.0105 0.0158 0.0209 0.0021 0.0016 0.0010\nImprove 0.46% 7.33% 6.76% 8.62% 18.52% 13.74% 8.29% 18.80% 13.04%\nYelp\nDIN (RT) 0.0067 0.0094 0.0126 0.0108 0.0191 0.0318 0.0022 0.0019 0.0016\nGRU4Rec (RT) 0.0045 0.0063 0.0084 0.0075 0.0131 0.0217 0.0015 0.0013 0.0011\nSASRec (RT) 0.0056 0.0079 0.0110 0.0087 0.0159 0.0281 0.0017 0.0016 0.0014\nP5 (RT) 0.0226 0.0253 0.0276 0.0299 0.0382 0.0471 0.0060 0.0038 0.0024\nDIN (NRT) 0.0033 0.0044 0.0057 0.0040 0.0062 0.0096 0.0026 0.0020 0.0014\nGRU4Rec (NRT) 0.0017 0.0026 0.0036 0.0022 0.0041 0.0065 0.0013 0.0013 0.0011\nSASRec (NRT) 0.0021 0.0030 0.0039 0.0024 0.0044 0.0066 0.0016 0.0015 0.0012\nP5 (NRT) 0.0093 0.0112 0.0132 0.0136 0.0196 0.0272 0.0027 0.0020 0.0014\nOurs (P5+SASRec)0.0095 0.0116 0.0144 0.0145 0.0213 0.0323 0.0029 0.0021 0.0016\nImprove 1.72% 3.65% 9.50% 7.00% 8.88% 18.85% 7.01% 8.67% 19.12%\nTable 2: The impact of choosing different LLMs on performance.\nDataset LLM Metric\nNDCG@5NDCG@10NDCG@20HR@5 HR@10 HR@20 Precision@5Precision@10Precision@20\nBeauty\nPOD (NRT) 0.0079 0.0111 0.0134 0.0139 0.0239 0.0327 0.0028 0.0024 0.0016\nOurs(POD+SASRec)0.0091 0.0125 0.0150 0.0150 0.0256 0.0356 0.0030 0.0026 0.0018\nImprov 15.59% 12.41% 12.27% 8.37% 7.29% 8.62% 8.30% 7.11% 8.54%\nTable 3: The impact of choosing different SRMs on performance.\nDataset SRM Metric\nNDCG@5NDCG@10NDCG@20HR@5 HR@10 HR@20 Precision@5Precision@10Precision@20\nBeauty\nDIN 0.0091 0.0118 0.0147 0.0145 0.0230 0.0343 0.0029 0.0023 0.0017\nGRU4Rec 0.0092 0.0124 0.0151 0.0146 0.0247 0.0353 0.0029 0.0024 0.0017\nSASRec 0.0094 0.0126 0.0154 0.0150 0.0248 0.0361 0.0030 0.0025 0.0018\nchoose not to use them as models on the device.\nâ€¢LLMs. P5 [11] and POD [17] are two well-known LLM\nparadigms for recommendation, aiming to transform the\nrecommendation task into textual format and use the text as\ninput to the LLM to obtain recommendation results.\nIn the subsequent experimental results, unless otherwise specified,\nLLM defaults to P5 and SRM defaults to SASRec.\n4.1.3 Evaluation metrics. In the experiments, we use the widely\nadopted NDCG, HitRate(HR) and Precision as the metrics to evalu-\nate model performance. The details of the metrics are in Appendix.\n4.2 Experimental Results\nWe use bold to denote the best value and underline to denote\nthe second-best value (if applicable). Typically, we conduct such\ncomparisons for each dataset. However, in some cases, such as\nin Figure 5, we compare values for each Request Frequency. In\nthe experimental analysis section, when results across all datasets\nwould take up significant space and consistent conclusions are\ndrawn for each dataset, we only present the results for one dataset\n(e.g., Figure 5) to save space for more critical content.\n4.2.1 Overall performance. Table 1 presents the performance com-\nparison between LSC4Rec, SRMs, and LLMs under both real-time\n(RT) and near-real-time (NRT) data settings. Compared to real-time\nCollaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada\nTable 4: The ablation studies of the collaborative training.\nTraining Method Metric\nDatasetCollaborative\nTraining\nAdaptive\nRe-trainingNDCG@5NDCG@10NDCG@20HR@5 HR@10HR@20Precision@5Precision@10Precision@20\nBeauty\n% % 0.0073 0.0101 0.0130 0.0116 0.0204 0.0322 0.0023 0.0020 0.0016\n! % 0.0089 0.0113 0.0138 0.0143 0.0216 0.0318 0.0029 0.0022 0.0016\n% ! 0.0093 0.0122 0.0150 0.0145 0.0236 0.0347 0.0029 0.0024 0.0017\n! ! 0.0094 0.0126 0.0154 0.0150 0.0248 0.0361 0.0030 0.0025 0.0018\nToys\n% % 0.0047 0.0065 0.0082 0.0074 0.0130 0.0194 0.0015 0.0013 0.0009\n! % 0.0062 0.0074 0.0087 0.0098 0.0136 0.0189 0.0020 0.0014 0.0009\n% ! 0.0064 0.0080 0.0095 0.0099 0.0146 0.0208 0.0020 0.0015 0.0010\n! ! 0.0066 0.0084 0.0096 0.0105 0.0158 0.0209 0.0021 0.0016 0.0010\nYelp\n% % 0.0068 0.0090 0.0120 0.0104 0.0173 0.0291 0.0021 0.0017 0.0014\n! % 0.0089 0.0110 0.0130 0.0137 0.0202 0.0279 0.0027 0.0019 0.0014\n% ! 0.0083 0.0109 0.0136 0.0131 0.0209 0.0317 0.0026 0.0020 0.0015\n! ! 0.0095 0.0116 0.0144 0.0145 0.0213 0.0323 0.0029 0.0021 0.0016\nTable 5: The ablation studies of the collaborative inference.\nDataset\nInference Method Metric\nLLM SRM Result\nFusion NDCG@5NDCG@10NDCG@20HR@5 HR@10HR@20Precision@5Precision@10Precision@20\nBeauty\n! % % 0.0087 0.0108 0.0136 0.0132 0.0200 0.0310 0.0027 0.0020 0.0016\n% ! % 0.0014 0.0021 0.0032 0.0018 0.0033 0.0060 0.0013 0.0011 0.0011\n! ! % 0.0094 0.0122 0.0144 0.0150 0.0238 0.0329 0.0030 0.0024 0.0016\n! ! ! 0.0094 0.0126 0.0154 0.0150 0.0248 0.0361 0.0030 0.0025 0.0018\nToys\n! % % 0.0087 0.0108 0.0136 0.0132 0.0200 0.0310 0.0027 0.0020 0.0016\n% ! % 0.0015 0.0020 0.0026 0.0019 0.0030 0.0046 0.0012 0.0010 0.0008\n! ! % 0.0059 0.0074 0.0089 0.0092 0.0138 0.0198 0.0018 0.0014 0.0010\n! ! ! 0.0094 0.0126 0.0154 0.0150 0.0248 0.0361 0.0030 0.0025 0.0018\nYelp\n! % % 0.0087 0.0108 0.0136 0.0132 0.0200 0.0310 0.0027 0.0020 0.0016\n% ! % 0.0041 0.0058 0.0083 0.0066 0.0120 0.0218 0.0013 0.0012 0.0011\n! ! % 0.0083 0.0108 0.0134 0.0127 0.0205 0.0311 0.0025 0.0021 0.0016\n! ! ! 0.0094 0.0126 0.0154 0.0150 0.0248 0.0361 0.0030 0.0025 0.0018\ndata, near-real-time data for each user excludes the most recent\ntwo interactions. As our study focuses on sequential recommenda-\ntion tasks, we trained and tested LLMs exclusively on sequential\nrecommendation datasets, excluding other types of datasets such\nas rating prediction. For LSC4Rec, LLMs first receive the same\ntwo-click-delayed near-real-time data to generate a candidate list\nof length 50. This candidate list is then re-ranked by SRMs using\nreal-time data. In our setup, LLMs are instantiated as P5 or POD,\nand SRMs are set to SASRec (the evaluation of collaboration with\nother SRMs can be found in Table 3). From Table 1, we can draw\nthe following conclusions:\nâ€¢Necessity of LLMs: Under various conditions of accessing\nreal-time data, the performance of LLMs is superior to that\nof SRMs in most cases, highlighting the necessity of utilizing\non-cloud LLMs.\nâ€¢Necessity of SRMs: Although the performance of SRM based\non real-time data sometimes surpasses that of LSC4Rec in\nsome metrics, achieving this in practical applications is quite\nchallenging. This is because SRMs often cannot access real-\ntime data (if deployed on the cloud) and all of the item em-\nbeddings (if deployed on the device). Nevertheless, the high\nperformance of SRM with real-time data highlights the unique\nadvantages that on-device SRMs bring to LSC4Rec, which\nLLMs lack.\nâ€¢Impact of Real-Time Data Absence on LLMs and SRMs:\nCompared to real-time data, the performance of both LLMs\nand SRMs drops significantly when using near-real-time data\ndelayed by two user interactions. This underscores the neg-\native impact of lacking access to real-time user behavior on\nmodels, especially on cloud-based LLMs.\nâ€¢Effectiveness of LSC4Rec: Our LSC4Rec method achieves\nsignificant performance improvements across all three\ndatasets. Specifically, when LLM is set to P5 and SRM to\nSASRec, LSC4Rec achieves average improvements of 16.18%,\n10.62%, and 9.38% on the Beauty, Toys, and Yelp datasets, re-\nspectively. These results indicate that our method effectively\nmitigates the performance degradation of LLMs caused by the\nlack of access to real-time user behavior.\n4.2.2 The impact of choosing different LLMs on performance. To ob-\nserve the impact of small model selection on LSC4Rec, we conducted\nexperiments using various small models. As shown in Table 2, the\nexperimental results indicate that: LSC4Rec achieves significant\nperformance improvements on POD, indicating that the LSC4Rec\nframework can effectively mitigate the performance degradation\nof various LLMs caused by their inability to access real-time data.\n4.2.3 The impact of choosing different SRMs on performance. To\nobserve the impact of small model selection on LSC4Rec, we con-\nducted experiments based on various small models. As shown in\nTable 3, the experimental results indicate that:\nKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Zheqi Lv et al.\nTable 6: The ablation studies of the collaborative-decision request.\nMetricDataset Decision\nMethod\nRequest\nFrequencyNDCG@5NDCG@10NDCG@20HR@5 HR@10 HR@20 Precision@5Precision@10Precision@20\nRandom 5% 0.0091 0.0114 0.0142 0.0138 0.0209 0.0321 0.0028 0.0021 0.0016\nInconsistency 5% 0.0125 0.0146 0.0173 0.0173 0.0240 0.0349 0.0035 0.0024 0.0017\nImprovement 36.66% 28.14% 21.89% 25.71% 14.80% 8.63% 25.82% 14.83% 8.07%\nRandom 10% 0.0097 0.0120 0.0149 0.0145 0.0219 0.0332 0.0029 0.0022 0.0017\nInconsistency 10% 0.0134 0.0156 0.0183 0.0184 0.0254 0.0363 0.0037 0.0025 0.0018\nImprovement 37.91% 29.56% 23.13% 26.50% 15.91% 9.31% 26.46% 15.98% 9.04%\nRandom 20% 0.0104 0.0127 0.0157 0.0153 0.0227 0.0344 0.0031 0.0023 0.0017\nInconsistency 20% 0.0140 0.0162 0.0190 0.0192 0.0263 0.0373 0.0038 0.0026 0.0019\nImprovement 34.72% 27.42% 21.19% 25.44% 15.54% 8.32% 25.49% 15.42% 8.14%\nRandom 40% 0.0137 0.0162 0.0192 0.0195 0.0274 0.0393 0.0039 0.0027 0.0020\nInconsistency 40% 0.0164 0.0189 0.0219 0.0225 0.0305 0.0423 0.0045 0.0031 0.0021\nBeauty\nImprovement 19.82% 16.92% 14.18% 15.89% 11.58% 7.74% 15.94% 11.31% 8.16%\nâ€¢The choice of small models indeed has some influence on\nLSC4Rec, which can be attributed to differences in the abil-\nity of different SRMs to interpret user preferences. Over-\nall, SASRec provides the most significant improvement for\nLSC4Rec. We conducted analyses on the Beauty, Toys, and\nYelp datasets. Although SASRec does not outperform GRU4Rec\nand DIN across all metrics on all datasets, it generally pro-\nvides the most significant improvements to LSC4Rec. There-\nfore, to save space, we present only the results on the Beauty\ndataset.\nâ€¢Within the LSC4Rec framework, regardless of the SRM used,\nthe performance is superior to that of LLMs without access\nto real-time data.\n4.3 Ablation Study\nFor simplicity, we use â€œw.â€ to represent â€œwithâ€ and â€œw/o.â€ to repre-\nsent â€œwithoutâ€. In these experiments, the SRM on the device uses\nthe SASRec model.\n4.3.1 The impact of collaborative training. As shown in Table 4,\nwe analyze the effectiveness of each component in collaborative\ntraining via ablation study. There are four rows for each dataset,\ncorresponding to four types of ablations, we provide the following\nexplanations for them:\nâ€¢w/o. Collaborative Training and w/o. Adaptive Re-training\n(Row.1) indicates that both the LLMs and SRMs are trained\nindependently based on historical user data.\nâ€¢w. Collaborative Training and w/o. Adaptive Re-training\n(Row.2) indicates that after both the LLMs and SRMs are\ntrained independently based on historical user data, the SRM\nthen learns to re-rank the candidate list generated by the LLM.\nâ€¢w/o. Collaborative Training and w. Adaptive Re-training\n(Row.3) indicates that after both the LLMs and SRMs are\ntrained independently based on historical user data, the SRM\nis then re-trained on the device based on the userâ€™s real-time\ndata.\nâ€¢w.Collaborative Training andw.Adaptive Re-training (Row.4)\nindicates that after both the LLMs and SRMs are trained in-\ndependently based on historical user data, the SRM learns to\nre-rank the candidate list generated by the LLM. Then the\nSRM is re-trained on the device based on the userâ€™s real-time\ndata.\nThe experimental results show that with cooperative training\nand adaptive re-training performs the best, followed by with adap-\ntive re-training and without cooperative training. The worst case\noccurs when neither is used, and the gap compared to the other\nthree cases is significant. This indicates that on-device SRM can-\nnot perform well within the LSC4Rec framework solely through\nindependent training. These results demonstrate the effectiveness\nof cooperative training and adaptive re-training.\n4.3.2 The impact of collaborative Inference. To analyze the effec-\ntiveness of each component in our collaborative inference strategy,\nwe conducted ablation experiments on the inference strategy. As\nshown in Table 5, each dataset has four rows of experimental re-\nsults, corresponding to four types of ablations. For the four rows of\ndata for each dataset, we provide the following explanations:\nâ€¢w. LLM, w/o. SRM, w/o. Result Fusion (Row.1) indicates that only\nuse on-cloud LLM to recall the candidate list, the initial ranking\nresults can be regarded as the full-ranking results based on the\nless real-time data.\nâ€¢w/o. LLM, w. SRM, w/o. Result Fusion (Row.2) indicates that only\nuse on-device SRM to do ranking.\nâ€¢w. LLM, w. SRM, w/o. Result Fusion (Row.3) indicates first to\nlet on-cloud LLM recall a candidate list, then on-device SRM do\nrerank the candidate list based on the real-time\nâ€¢w. LLM, w. SRM, w. Result Fusion (Row.4) indicates first to let\non-cloud LLM recall a candidate list and initial ranking resuls,\nthen on-device SRM do rerank the candidate list based on the\nreal-time, further, reranking and initial ranking ard fused as the\nfinal ranking results.\nThe experimental results show that the collaborative inference and\nresult fusion of LLMs and SRMs perform the best, followed by col-\nlaborative inference of LLMs and SRMs without result fusion. Using\neither the LLM or the SRM alone results in poorer performance.\nThe above results demonstrate the effectiveness of our designed\ncollaborative inference and result fusion.\n4.3.3 The impact of collaborative-decision request. As shown in\nTable 6, we analyze the collaborative-decision request.\nâ€¢Random (Row.1) indicates that choosing some users to update\nthe candidate list and corresponded initial ranking results ran-\ndomly.\nCollaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada\nTable 7: The impact of length of candidate list on performance\nDataset Candidate\nLength\nMetrics\nNDCG@5NDCG@10NDCG@20HR@5 HR@10 HR@20 Precision@5Precision@10Precision@20\nBeauty 20 0.0097 0.0124 0.0143 0.0152 0.0238 0.0312 0.0030 0.0024 0.0016\n50 0.0094 0.0126 0.0154 0.0150 0.0248 0.0361 0.0030 0.0025 0.0018\nToys 20 0.0067 0.0080 0.0090 0.0106 0.0148 0.0186 0.0021 0.0015 0.0009\n50 0.0066 0.0084 0.0096 0.0105 0.0158 0.0209 0.0021 0.0016 0.0010\nYelp 20 0.0099 0.0121 0.0136 0.0148 0.0214 0.0276 0.0030 0.0021 0.0014\n50 0.0095 0.0116 0.0144 0.0145 0.0213 0.0323 0.0029 0.0021 0.0016\nâ€¢Inconsistency (Row.2) indicates that choosing some users to\nupdate the candidate list and corresponded initial ranking re-\nsults based on the ranking results of the LLM and SRM. High\ninconsistency can get more priority to update.\nThe experimental results indicate that our strategy of selecting\nuser data for generating new candidate lists and initial ranking\nresults with LLM, based on the inconsistency of ranking between\nLLM and SRM, achieves an average improvement of about 25%\ncompared to random selection. This is because our strategy tends\nto prioritize updates of candidate lists and initial rankings with\nthe highest benefit, while random updating lacks the capability to\ncalculate the benefits of updates. At the same time, we can observe\nthat the peak of improvement occurs at a request frequency of 10%,\nand then as the frequency increases, the magnitude of improvement\ndiminishes. This is because when the request frequency increases\n(e.g., to 20%, 40%), our strategy has already updated the parts with\nthe highest benefits, while random updates mix high and low benefit\nupdates, leading to a decrease in improvement. In summary, the\nabove results all demonstrate that our strategy effectively selects\nthe users with the highest benefits for updating candidate lists and\ninitial rankings.\n4.4 In-Depth Analysis\n4.4.1 The impact of length of candidate list. In Table 7, we ana-\nlyze the impact of the length of the LLMâ€™s recall list on perfor-\nmance. When the length of the candidate list is 20 and 50, the range\nof performance variation is not significant, which indicates that\nLSC4Rec is not very sensitive to this hyperparameter of the recall\nlistâ€™s length. Additionally, shorter candidate lists tend to perform\nbetter on stricter metrics (e.g., @5), while longer candidate lists\nshow advantages on more lenient metrics (e.g., @20). The advantage\nof a longer candidate list lies in its higher likelihood of including\nthe ground-truth item, but the downside is the greater challenge for\nthe SRM to re-rank longer candidate lists effectively. On a relatively\nbalanced metric (e.g., @10), the advantages and disadvantages of\nlonger candidate lists are approximately equal.\n4.4.2 Analysis of resource consumption. To validate the resource\nconsumption of our method, we analyzed the resource usage of our\napproach in Table 8. Since the choice of either P5 or POD as the\nLLM does not affect the conclusion, we will use one of the models as\nan example. Here, we choose P5 to describe the conclusion. In terms\nof the number of parameters, if we uses P5 as LLM and SASRec as\nSRM, the total number of parameters is the sum of P5 and SASRec\n(on-device consumption is same as SASRec). In terms of training\nduration, since our method does not require real-time training of\nLLMs, the time consumption is reduced compared to training LLMs.\nTable 8: Resource consumption comparison.\nModel #ParameterTraining Time\n(s/epoch)\nInference Time\n(s/sample)\nOn-device\nDIN 0.82M 38.50 0.00074\nGRU4Rec 0.81M 38.30 0.00085\nSASRec 1.62M 39.40 0.00143\nOn-cloud\nP5 60.75M 897.83 0.10082\nPOD 60.77M 2277.16 0.32563\nIn terms of inference duration, because our method does not need\nto call the LLM at every moment, and often does not need to call the\nLLM at all, the time consumption is significantly reduced compared\nto the LLM, and has not increased much compared to the SRM,\nwhich does not affect the real-time nature of the recommendations.\nOverall, our method leverages the advantages of LLMs with low\nresource consumption to achieve better performance.\n5 Conclusion\nThe proposed Device-Cloud Collaborative Framework for LLM and\nSRM (LSC4Rec) effectively combines the strengths of large language\nmodels and small recommendation models. By leveraging collab-\norative training, collaborative inference, and intelligent request\nstrategies, LSC4Rec addresses the limitations of LLMs in captur-\ning real-time user preferences while optimizing the utilization of\ncloud and edge computing resources. Extensive experimental re-\nsults validate the effectiveness of each strategy, highlighting the\nframeworkâ€™s potential for practical applications. Future work will\nfocus on further refining the collaboration mechanisms to enhance\nrecommendation performance and user satisfaction.\nACKNOWLEDGEMENTS\nThis work was supported by 2030 National Science and Technol-\nogy Major Project (2022ZD0119100), Scientific Research Fund of\nZhejiang Provincial Education Department (Y202353679), National\nNatural Science Foundation of China (No. 62402429, 62376243,\n62441605, 62037001), the Key Research and Development Program\nof Zhejiang Province (No. 2024C03270), ZJU Kunpeng &Ascend\nCenter of Excellence, the Key Research and Development Projects\nin Zhejiang Province (No.2024C01106), Zhejiang University Edu-\ncation Foundation Qizhen Scholar Foundation, the Starry Night\nScience Fund at Shanghai Institute for Advanced Study (Zhejiang\nUniversity). This work was also supported by Ant group.\nKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Zheqi Lv et al.\nReferences\n[1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.\n2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large\nLanguage Model with Recommendation. (2023), 1007â€“1014.\n[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\nHe. 2023. Tallrec: An effective and efficient tuning framework to align large\nlanguage model with recommendation. InProceedings of the 17th ACM Conference\non Recommender Systems . ACM, 1007â€“1014.\n[3] Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. 2022.\nIntent contrastive learning for sequential recommendation. In Proceedings of the\nACM Web Conference 2022 . 2172â€“2182.\n[4] Zhengyu Chen, Donglin Wang, and Shiqian Yin. 2021. Improving cold-start rec-\nommendation via multi-prior meta-learning. InAdvances in Information Retrieval:\n43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28â€“April\n1, 2021, Proceedings, Part II 43 . Springer, 249â€“256.\n[5] Zhengyu Chen, Ziqing Xu, and Donglin Wang. 2021. Deep transfer tensor decom-\nposition with orthogonal constraint for recommender systems. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , Vol. 35. 4010â€“4018.\n[6] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxi-\nang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPTâ€™s Capabilities in\nRecommender Systems. In RecSys. ACM, 1126â€“1132.\n[7] Kairui Fu, Qiaowei Miao, Shengyu Zhang, Kun Kuang, and Fei Wu. 2023. End-to-\nEnd Optimization of Quantization-Based Structure Learning and Interventional\nNext-Item Recommendation. In CAAI International Conference on Artificial Intel-\nligence. Springer, 415â€“429.\n[8] Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, and Jiwei Li. 2024. DIET:\nCustomized Slimming for Incompatible Networks in Sequential Recommendation.\nIn KDD. ACM, 816â€“826.\n[9] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei,\nBiao Li, Yuan Zhang, and Peng Jiang. 2023. CIRS: Bursting Filter Bubbles by Coun-\nterfactual Interactive Recommender System. ACM Transactions on Information\nSystems (TOIS) 42, 1, Article 14 (aug 2023), 27 pages. doi:10.1145/3594871\n[10] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei\nZhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented\nrecommender system. arXiv preprint arXiv:2303.14524 (2023).\n[11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.\nRecommendation as Language Processing (RLP): A Unified Pretrain, Personalized\nPrompt & Predict Paradigm (P5). In RecSys. ACM, 299â€“315.\n[12] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2016. Session-based recommendations with recurrent neural networks. Interna-\ntional Conference on Learning Representations 2016 (2016).\n[13] Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, and\nYongfeng Zhang. 2023. Genrec: Large language model for generative recommen-\ndation. arXiv e-prints (2023), arXivâ€“2307.\n[14] Wei Ji, Xiangyan Liu, An Zhang, Yinwei Wei, Yongxin Ni, and Xiang Wang. 2023.\nOnline distillation-enhanced multi-modal transformer for sequential recommen-\ndation. In Proceedings of the 31st ACM International Conference on Multimedia .\n955â€“965.\n[15] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec-\nommendation. In IEEE International Conference on Data Mining, ICDM 2018,\nSingapore, November 17-20, 2018 . IEEE Computer Society, 197â€“206. doi:10.1109/\nICDM.2018.00035\n[16] Heng Li, Zhiyuan Yao, Bang Wu, Cuiying Gao, Teng Xu, Wei Yuan, and Xiapu Luo.\n2025. Automated Malware Assembly Line: Uniting Piggybacking and Adversarial\nExample in Android Malware Generation. In32nd Annual Network and Distributed\nSystem Security Symposium, NDSS 2025, San Diego, California, USA, February 24 -\nFebruary 28, 2025 . The Internet Society.\n[17] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt Distillation for Efficient\nLLM-based Recommendation. In CIKM. ACM, 1348â€“1357.\n[18] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke\nZhu, and Lei Zhang. 2024. Tokenpacker: Efficient visual projector for multimodal\nllm. arXiv preprint arXiv:2407.02392 (2024).\n[19] Yangfan Li, Kenli Li, Wei Wei, Tianyi Zhou, and Cen Chen. 2022. CoRec: an\nefficient internet behavior-based recommendation framework with edge-cloud\ncollaboration on deep convolution neural networks. ACM Transactions on Sensor\nNetworks 19, 2 (2022), 1â€“28.\n[20] Xinting Liao, Weiming Liu, Xiaolin Zheng, Binhui Yao, and Chaochao Chen. 2023.\nPpgencdr: A stable and robust framework for privacy-preserving cross-domain\nrecommendation. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 37. 4453â€“4461.\n[21] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker,\nTeddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Letâ€™s\nverify step by step. arXiv preprint arXiv:2305.20050 (2023).\n[22] Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-Seng Chua.\n2024. Bridging items and language: A transition paradigm for large language\nmodel-based recommendation. InProceedings of the 30th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining . 1816â€“1826.\n[23] Xinyu Lin, Chaoqun Yang, Wenjie Wang, Yongqi Li, Cunxiao Du, Fuli Feng,\nSee-Kiong Ng, and Tat-Seng Chua. 2024. Efficient Inference for Large Language\nModel-based Generative Recommendation. CoRR abs/2410.05165 (2024).\n[24] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is ChatGPT a\nGood Recommender? A Preliminary Study. CoRR abs/2304.10149 (2023).\n[25] Kai Liu, Ze Chen, Zhihang Fu, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu,\nand Jieping Ye. 2024. Structure-aware Domain Knowledge Injection for Large\nLanguage Models. arXiv preprint arXiv:2407.16724 (2024).\n[26] Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu\nChen, Yue Wu, and Jieping Ye. 2024. Enhancing LLMâ€™s Cognition via Structuriza-\ntion. Advances in Neural Information Processing Systems 38 (2024).\n[27] Weiming Liu, Chaochao Chen, Xinting Liao, Mengling Hu, Yanchao Tan, Fan\nWang, Xiaolin Zheng, and Yew Soon Ong. 2024. Learning Accurate and Bidirec-\ntional Transformation via Dynamic Embedding Transportation for Cross-Domain\nRecommendation. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 38. 8815â€“8823.\n[28] Weiming Liu, Xiaolin Zheng, Chaochao Chen, Jiajie Su, Xinting Liao, Mengling\nHu, and Yanchao Tan. 2023. Joint internal multi-interest exploration and external\ndomain alignment for cross domain sequential recommendation. In Proceedings\nof the ACM Web Conference 2023 . 383â€“394.\n[29] Jing Long, Guanhua Ye, Tong Chen, Yang Wang, Meng Wang, and Hongzhi Yin.\n2024. Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI\nRecommendations. In KDD. ACM, 2026â€“2036.\n[30] Jing Long, Guanhua Ye, Tong Chen, Yang Wang, Meng Wang, and Hongzhi\nYin. 2024. Diffusion-based cloud-edge-device collaborative learning for next\nPOI recommendations. In Proceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining . 2026â€“2036.\n[31] Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou, Zongpeng Li,\nYuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2023. RecRanker: Instruction\nTuning Large Language Model as Ranker for Top-k Recommendation. CoRR\nabs/2312.16018 (2023).\n[32] Zheqi Lv, Shaoxuan He, Tianyu Zhan, Shengyu Zhang, Wenqiao Zhang, Jingyuan\nChen, Zhou Zhao, and Fei Wu. 2024. Semantic Codebook Learning for Dynamic\nRecommendation Models. In ACM Multimedia. ACM, 9611â€“9620.\n[33] Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024.\nIntelligent Model Update Strategy for Sequential Recommendation. In WWW.\nACM, 3117â€“3128.\n[34] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei\nWang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, and Fei Wu.\n2023. DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation\nFramework for Efficient Device Model Generalization. In WWW. ACM, 3077â€“\n3085.\n[35] Xufeng Qian, Yue Xu, Fuyu Lv, Shengyu Zhang, Ziwen Jiang, Qingwen Liu, Xiaoyi\nZeng, Tat-Seng Chua, and Fei Wu. 2022. Intelligent request strategy design in\nrecommender system. In Proceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining . 3772â€“3782.\n[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research 21, 1 (2020), 5485â€“5551.\n[37] Jiajie Su, Chaochao Chen, Weiming Liu, Fei Wu, Xiaolin Zheng, and Haoming\nLyu. 2023. Enhancing hierarchy-aware graph networks with deep dual clustering\nfor session-based recommendation. In Proceedings of the ACM Web Conference\n2023. 165â€“176.\n[38] Teng Sun, Chun Wang, Xuemeng Song, Fuli Feng, and Liqiang Nie. 2022. Re-\nsponse generation by jointly modeling personalized linguistic styles and emotions.\nACM Transactions on Multimedia Computing, Communications, and Applications\n(TOMM) 18, 2 (2022), 1â€“20.\n[39] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin\nChen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investi-\ngating Large Language Models as Re-Ranking Agents. (2023), 14918â€“14937.\n[40] Yuchong Sun, Che Liu, Kun Zhou, Jinwen Huang, Ruihua Song, Wayne Xin Zhao,\nFuzheng Zhang, Di Zhang, and Kun Gai. 2024. Parrot: Enhancing multi-turn\ninstruction following for large language models. InProceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\n9729â€“9750.\n[41] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-\ntion via convolutional sequence embedding. In Proceedings of the eleventh ACM\ninternational conference on web search and data mining . 565â€“573.\n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-\nple. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR\nabs/2302.13971 (2023). doi:10.48550/ARXIV.2302.13971 arXiv:2302.13971\n[43] Jiawei Wang, Da Cao, Shaofei Lu, Zhanchang Ma, Junbin Xiao, and Tat-Seng\nChua. 2024. Causal-driven Large Language Models with Faithful Reasoning for\nKnowledge Question Answering. In Proceedings of the 32nd ACM International\nConference on Multimedia . 4331â€“4340.\nCollaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada\n[44] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al . 2023. A Survey on Large\nLanguage Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023).\n[45] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen.\n2024. A survey on large language models for recommendation. World Wide Web\n(WWW) 27, 5 (2024), 60.\n[46] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. NExT-\nGPT: Any-to-Any Multimodal LLM. In ICML. OpenReview.net.\n[47] Chen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun Xu, and Tat-Seng Chua. 2024.\nA Study of Implicit Ranking Unfairness in Large Language Models. In EMNLP\n(Findings). Association for Computational Linguistics, 7957â€“7970.\n[48] Jiangchao Yao, Feng Wang, Xichen Ding, Shaohu Chen, Bo Han, Jingren Zhou,\nand Hongxia Yang. 2022. Device-cloud Collaborative Recommendation via Meta\nController. In KDD. ACM, 4353â€“4362.\n[49] Jiangchao Yao, Feng Wang, Kunyang Jia, Bo Han, Jingren Zhou, and Hongxia\nYang. 2021. Device-Cloud Collaborative Learning for Recommendation. In KDD.\nACM, 3865â€“3874.\n[50] Honglei Zhang, He Liu, Haoxuan Li, and Yidong Li. 2024. Transfr: Transferable\nfederated recommendation with pre-trained language models. arXiv:2402.01124\n(2024).\n[51] Jinghao Zhang, Qiang Liu, Shu Wu, and Liang Wang. 2023. Mining Stable\nPreferences: Adaptive Modality Decorrelation for Multimedia Recommendation.\nIn Proceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 443â€“452.\n[52] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-\nRong Wen. 2023. Recommendation as Instruction Following: A Large Language\nModel Empowered Recommendation Approach. CoRR abs/2305.07001 (2023).\ndoi:10.48550/ARXIV.2305.07001 arXiv:2305.07001\n[53] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong\nWen. 2023. Recommendation as Instruction Following: A Large Language Model\nEmpowered Recommendation Approach. CoRR abs/2305.07001 (2023).\n[54] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang.\n2021. Mining latent structures for multimedia recommendation. In Proceedings\nof the 29th ACM international conference on multimedia . 3872â€“3880.\n[55] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Mengqi Zhang, Shu Wu, and Liang Wang.\n2022. Latent structure mining with contrastive modality fusion for multimedia\nrecommendation. IEEE Transactions on Knowledge and Data Engineering 35, 9\n(2022), 9154â€“9167.\n[56] Kepu Zhang, Teng Shi, Sunhao Dai, Xiao Zhang, Yinfeng Li, Jing Lu, Xiaoxue\nZang, Yang Song, and Jun Xu. 2024. SAQRec: Aligning Recommender Systems to\nUser Satisfaction via Questionnaire Feedback. In CIKM. ACM, 3165â€“3175.\n[57] Shuo Zhang, Boci Peng, Xinping Zhao, Boren Hu, Yun Zhu, Yanjia Zeng, and\nXuming Hu. 2024. LLaSA: Large Language and E-Commerce Shopping Assistant.\nIn Amazon KDD Cup 2024 Workshop .\n[58] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He.\n2023. Collm: Integrating collaborative embeddings into large language models\nfor recommendation. arXiv preprint arXiv:2310.19488 (2023).\n[59] Jujia Zhao, Wenjie Wang, Xinyu Lin, Leigang Qu, Jizhi Zhang, and Tat-Seng\nChua. 2023. Popularity-aware Distributionally Robust Optimization for Recom-\nmendation System. In CIKM. ACM, 4967â€“4973.\n[60] Jujia Zhao, Wang Wenjie, Yiyan Xu, Teng Sun, Fuli Feng, and Tat-Seng Chua. 2024.\nDenoising diffusion recommender model. In Proceedings of the 47th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n1370â€“1379.\n[61] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen\nWen, Fei Wang, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2024. Recommender\nSystems in the Era of Large Language Models (LLMs). IEEE Trans. Knowl. Data\nEng. 36, 11 (2024), 6889â€“6907.\n[62] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen\nWen, Fei Wang, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2024. Recommender\nSystems in the Era of Large Language Models (LLMs). IEEE Trans. Knowl. Data\nEng. 36, 11 (2024), 6889â€“6907.\n[63] Xiaolin Zheng, Jiajie Su, Weiming Liu, and Chaochao Chen. 2022. DDGHM:\nDual dynamic graph with hybrid metric training for cross-domain sequential\nrecommendation. In Proceedings of the 30th ACM International Conference on\nMultimedia. 471â€“481.\n[64] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma,\nYanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for\nClick-Through Rate Prediction. In KDD. ACM, 1059â€“1068.\n[65] Didi Zhu, Zhongyisun Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Chao\nWu, and Kun Kuang. 2024. Model Tailor: Mitigating Catastrophic Forgetting in\nMulti-modal Large Language Models. In Forty-first International Conference on\nMachine Learning .\n[66] Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024. Efficient Tuning\nand Inference for Large Language Models on Textual Graphs. In Proceedings of\nthe Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-\n24, Kate Larson (Ed.). International Joint Conferences on Artificial Intelligence\nOrganization, 5734â€“5742. doi:10.24963/ijcai.2024/634 Main Track.\nKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Zheqi Lv et al.\nA Appendix\nThis is the Appendix for â€œCollaboration of Large Language Models\nand Small Recommendation Models for Device-Cloud Recommen-\ndationâ€.\nA.1 Supplementary Methodology\nA.1.1 Pseudo code of LSCRec. Algorithm 1 shows the pseudo code\nof LSCRec. (ğ‘¥)represents that ğ‘¥ is a intermediate variable.\nAlgorithm 1: Pseudo Code of LSC4Rec\nStrategy 1: â–· Collaborative Training\nTarget: Pretrained LLMMğ¿, Pretrained SRMMğ‘†â†¦â†’Updated\nSRM Mğ‘†\nInput: Historical dataDğ»\nOutput: Prediction Ë†ğ‘Œ\nStrategy 2: â–· Collaborative Inference\nTarget: Real-time dataDğ‘… â†¦â†’Prediction Ë†ğ‘Œ\nInput: Real-time dataDğ‘…\nOutput: (Candidate Item SetS), (Initial RankingË†ğ‘Œinit gener-\nated byMğ¿), (Re-RankingË†ğ‘Œrerank generated byMğ‘†), Predic-\ntion Ë†ğ‘Œ\nStrategy 3: â–· Collaborative-decision Request\nTarget: Initial RankingË†ğ‘Œinit, Re-Ranking Ë†ğ‘Œrerank â†¦â†’Inconsis-\ntencyğ‘\nInput: Initial RankingË†ğ‘Œinit, Re-Ranking Ë†ğ‘Œrerank\nOutput: Inconsistencyğ‘\nOverview: â–· Training Procedure\nInput: Historical data Dğ», Real-time data Dğ‘…\nOutput: Prediction Ë†ğ‘¦.\nInitialization: Randomly initialize the Mğ¿ and Mğ‘†\nrepeat\nif Mğ¿ and Mğ‘† have not yet been well-trained then\nCalculate loss as follows (see Eq.4 for the details),\nL= Ã\nğ‘¢,ğ‘£,ğ‘ ,ğ‘¦âˆˆDğ» ğ‘™(ğ‘¦, Ë†ğ‘¦ = M(ğ‘¢,ğ‘£,ğ‘  )).\nend\nuntil Convergence;\nrepeat\nif Mğ‘† has not yet been well-trained then\nCalculate loss as follows (see Eq.5âˆ¼7 for the details),\nL= Ã\nğ‘¢,ğ‘£,ğ‘ ,ğ‘¦âˆˆDğ» ğ‘™(ğ‘¦, Ë†ğ‘¦ = Mğ‘†(ğ‘¢,ğ‘£,ğ‘  )|ğ‘†ğ´ğ‘¢ğ‘”).\nend\nuntil Convergence;\nreturn Mğ¿, Mğ‘†.\nOverview: â–· Inference Procedure\nInput: Real-time data Dğ‘…\nOutput: Prediction Ë†ğ‘Œ, Inconsistency ğ‘\nğ‘†, Ë†ğ‘Œinit predicted by LLM, Ë†ğ‘Œrerank predicted by SRM based on\nğ‘†.\nğ‘ calculated based on Ë†ğ‘Œinit and Ë†ğ‘Œrerank to help decide\nwhether to request new ğ‘†, Ë†ğ‘Œinit from LLM.\nA.2 Supplementary Experiments\nA.2.1 Datasets. The statistics of the datasets used in the experi-\nments is shown in Table 9.\nTable 9: Statistics of Datasets.\nDataset Beauty Toys Yelp\n#User 22,363 19,412 30,431\n#Item 12,101 11,924 20,033\n#interaction198,502 167,597 316,354\nDensity 0.000733520.000724060.00051893\nA.2.2 Hyperparameters and Training Schedules. We summarize\nthe hyperparameters and training schedules of the LLM and SRM\nused in the experiments. Table 11 shows the settings of the SRM\ntraining. Table 12 shows the setting of the collaborative training,\ncollaborative inference, and collaborative-decision request.\nTable 10: Hyperparameters of LLM training.\nDatasetLLM Hyperparameter Setting\nBeauty\nToys\nYelp\nP5\nGPU Tesla A100 (40GB)\nBatch Size 32\nMax Text Length 512\nGen Max Length 64\nLearning Rate 1e-3\nWeight Decay 0.05(Beauty, Toys), 0.02(Yelp)\nOptimizer Adamw\nTask Sequential\nNegative Sampling Rate 1:99\nBeam Search 20/50\nBeauty\nToys\nYelp\nPOD\nGPU Tesla A100 (40GB)\nBatch Size 64\nLearning Rate 5e-4\nWeight Decay 1e-2\nOptimizer Adamw\nTask Sequential\nNegative Sampling Rate 1:99\nBeam Search 20/50\nTable 11: Hyperparameters of SRM training.\nDataset SRM Hyperparameter Setting\nBeauty\nToys\nYelp\nDIN\nGRU4Rec\nSASRec\nGPU Tesla A100 (40GB)\nOptimizer Adam\nLearning rate 1e-3\nBatch size 1024\nSequence length 10\nthe Dimension of Embedding1Ã—32\nthe Amount of MLP 2\nTable 12: Hyperparameters of collaborative training, collab-\norative inference, and collaborative-decision request.\nDataset LLM SRM Hyperparameter Setting\nBeauty\nToys\nYelp\nP5\nPOD\nDIN\nGRU4Rec\nSASRec\nLength of Candidate list20/50\nValid user behavior lengthDynamic\nUser behavior length 10\nSequence length 50\nLearning Rate 1e-3\nDimension of embedding1Ã—32\nOptimizer Adam"
}