{
  "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
  "url": "https://openalex.org/W2963524349",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2947404197",
      "name": "Yusu Qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2288298763",
      "name": "Urwa Muaz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100259285",
      "name": "Ben Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2326278589",
      "name": "Jae Won Hyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3128232076",
    "https://openalex.org/W3037697022",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962787423",
    "https://openalex.org/W2921633540",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2887768933",
    "https://openalex.org/W3122810052",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W196214544"
  ],
  "abstract": "Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 223–228\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n223\nReducing Gender Bias in Word-Level Language Models with a\nGender-Equalizing Loss Function\nYusu Qian∗\nTandon School\nof Engineering\nNew York University\n6 MetroTech Center\nBrooklyn, NY , 11201\nyq729@nyu.edu\nUrwa Muaz∗\nTandon School\nof Engineering\nNew York University\n6 MetroTech Center\nBrooklyn, NY , 11201\num367@nyu.edu\nBen Zhang\nCenter for\nData Science\nNew York University\n60 Fifth Avenue\nNew York, NY , 10012\nbz957@nyu.edu\nJae Won Hyun\nDepartment of\nComputer Science\nNew York University\n251 Mercer St\nNew York, NY , 10012\njaewhyun@nyu.edu\nAbstract\nGender bias exists in natural language datasets\nwhich neural language models tend to learn,\nresulting in biased text generation. In this\nresearch, we propose a debiasing approach\nbased on the loss function modiﬁcation. We\nintroduce a new term to the loss function\nwhich attempts to equalize the probabilities of\nmale and female words in the output. Using\nan array of bias evaluation metrics, we provide\nempirical evidence that our approach success-\nfully mitigates gender bias in language mod-\nels without increasing perplexity by much. In\ncomparison to existing debiasing strategies,\ndata augmentation, and word embedding de-\nbiasing, our method performs better in sev-\neral aspects, especially in reducing gender bias\nin occupation words. Finally, we introduce a\ncombination of data augmentation and our ap-\nproach, and show that it outperforms existing\nstrategies in all bias evaluation metrics.\n1 Introduction\nNatural Language Processing (NLP) models are\nshown to capture unwanted biases and stereotypes\nfound in the training data which raise concerns\nabout socioeconomic, ethnic and gender discrimi-\nnation when these models are deployed for public\nuse (Lu et al., 2018; Zhao et al., 2018).\nThere are numerous studies that identify al-\ngorithmic bias in NLP applications. Lapowsky\n(2018) showed ethnic bias in Google autocom-\nplete suggestions whereas Lambrecht and Tucker\n(2018) found gender bias in advertisement deliv-\nery systems. Additionally, Zhao et al. (2018)\ndemonstrated that coreference resolution systems\nexhibit gender bias.\nLanguage modelling is a pivotal task in NLP\nwith important downstream applications such as\ntext generation (Sutskever et al., 2011). Recent\n∗Yusu Qian and Urwa Muaz contributed equally to the\npaper.\nstudies by Lu et al. (2018) and Bordia and Bow-\nman (2019) have shown that this task is vulnerable\nto gender bias in the training corpus. Two prior\nworks focused on reducing bias in language mod-\nelling by data preprocessing (Lu et al., 2018) and\nword embedding debiasing (Bordia and Bowman,\n2019). In this study, we investigate the efﬁcacy\nof bias reduction during training by introducing a\nnew loss function which encourages the language\nmodel to equalize the probabilities of predicting\ngendered word pairs like he and she. Although we\nrecognize that gender is non-binary, for the pur-\npose of this study, we focus on female and male\nwords.\nOur main contributions are summarized as fol-\nlows: i) to our best knowledge, this study is the\nﬁrst one to investigate bias alleviation in text gen-\neration by direct modiﬁcation of the loss func-\ntion; ii) our new loss function effectively reduces\ngender bias in the language models during train-\ning by equalizing the probabilities of male and\nfemale words in the output; iii) we show that\nend-to-end debiasing of the language model can\nachieve word embedding debiasing; iv) we pro-\nvide an interpretation of our results and draw a\ncomparison to other existing debiasing methods.\nWe show that our method, combined with an ex-\nisting method, counterfactual data augmentation,\nachieves the best result and outperforms all exist-\ning methods.\n2 Related Work\nRecently, the study of bias in NLP applications\nhas received increasing attention from researchers.\nMost relevant work in this domain can be broadly\ndivided into two categories: word embedding de-\nbiasing and data debiasing by preprocessing.\nWord Embedding Debiasing Bolukbasi et al.\n(2016) introduced the idea of gender subspace as\nlow dimensional space in an embedding that cap-\n224\ntures the gender information. Bolukbasi et al.\n(2016) and Zhao et al. (2017) deﬁned gender bias\nas a projection of gender-neutral words on a gen-\nder subspace and removed bias by minimizing this\nprojection. Gonen and Goldberg (2019) proved\nthat bias removal techniques based on minimiz-\ning projection onto the gender space are insufﬁ-\ncient. They showed that male and female stereo-\ntyped words cluster together even after such debi-\nasing treatments. Thus, gender bias still remains\nin the embeddings and is easily recoverable.\nBordia and Bowman (2019) introduced a co-\noccurrence based metric to measure gender bias\nin texts and showed that the standard datasets used\nfor language model training exhibit strong gender\nbias. They also showed that the models trained\non these datasets amplify bias measured on the\nmodel-generated texts. Using the same deﬁni-\ntion of embedding gender bias as Bolukbasi et al.\n(2016), Bordia and Bowman (2019) introduced a\nregularization term that aims to minimize the pro-\njection of neutral words onto the gender subspace.\nThroughout this paper,we refer to this approach as\nREG. They found that REG reduces bias in the\ngenerated texts for some regularization coefﬁcient\nvalues. But, this bias deﬁnition is shown to be in-\ncomplete by Gonen and Goldberg (2019). Instead\nof explicit geometric debiasing of the word em-\nbedding, we implement a loss function that mini-\nmizes bias in the output and thus adjust the whole\nnetwork accordingly. For each model, we analyze\nthe generated word embedding to understand how\nit is affected by output debiasing.\nData Debiasing Lu et al. (2018) showed that\ngender bias in coreference resolution and language\nmodelling can be mitigated through a data aug-\nmentation technique that expands the corpus by\nswapping the gender pairs like he and she, or fa-\nther and mother. They called this Counterfactual\nData Augmentation (CDA) and concluded that it\noutperforms the word embedding debiasing strat-\negy proposed by Bolukbasi et al. (2016). CDA\ndoubles the size of the training data and increases\ntime needed to train language models. In this\nstudy, we intend to reduce bias during training\nwithout requiring an additional data preprocessing\nstep.\n3 Methodology\n3.1 Dataset\nFor the training data, we use Daily Mail news ar-\nticles released by Hermann et al. (2015). This\ndataset is composed of 219,506 articles covering a\ndiverse range of topics including business, sports,\ntravel, etc., and is claimed to be biased and sen-\nsational (Bordia and Bowman, 2019). For man-\nageability, we randomly subsample 5% of the text.\nThe subsample has around 8.25 million tokens in\ntotal.\n3.2 Language Model\nWe use a pre-trained 300-dimensional word em-\nbedding, GloVe, by Pennington et al. (2014). We\napply random search to the hyperparameter tuning\nof the LSTM language model. The best hyperpa-\nrameters are as follows: 2 hidden layers each with\n300 units, a sequence length of 35, a learning rate\nof 20 with an annealing schedule of decay start-\ning from 0.25 to 0.95, a dropout rate of 0.25 and\na gradient clip of 0.25. We train our models for\n150 epochs, use a batch size of 48, and set early\nstopping with a patience of 5.\n3.3 Loss Function\nLanguage models are usually trained using cross-\nentropy loss. Cross-entropy loss at time step tis\nLCE (t) =−\n∑\nw∈V\nyw,t log (ˆyw,t) ,\nwhere V is the vocabulary, yis the one hot vector\nof ground truth and ˆyindicates the output softmax\nprobability of the model.\nWe introduce a loss term LB, which aims to\nequalize the predicted probabilities of gender pairs\nsuch as woman and man.\nLB(t) = 1\nG\nG∑\ni\n⏐⏐⏐⏐log ˆyfi,t\nˆymi,t\n⏐⏐⏐⏐\nfand mare a set of corresponding gender pairs,G\nis the size of the gender pairs set, and ˆy indicates\nthe output softmax probability. We use gender\npairs provided by Zhao et al. (2017). By consider-\ning only gender pairs we ensure that only gender\ninformation is neutralized and distribution over se-\nmantic concepts is not altered. For example, it\nwill try to equalize the probabilities of congress-\nman with congresswoman and actor with actress\nbut distribution of congressman, congresswoman\n225\nversus actor, actress will not be affected. Overall\nloss can be written as\nL= 1\nT\nT∑\nt=1\nLCE (t) +λLB(t) ,\nwhere λis a hyperparameter and T is the corpus\nsize. We observe that among the similar minima\nof the loss function, LB encourages the model\nto converge towards a minimum that exhibits the\nlowest gender bias.\n3.4 Model Evaluation\nLanguage models are evaluated using perplexity,\nwhich is a standard measure of performance for\nunseen data. For bias evaluation, we use an array\nof metrics to provide a holistic diagnosis of the\nmodel behavior under debiasing treatment. These\nmetrics are discussed in detail below. In all the\nevaluation metrics requiring gender pairs, we use\ngender pairs provided by Zhao et al. (2017). This\nlist contains 223 pairs, all other words are consid-\nered gender-neutral.\n3.4.1 Co-occurrence Bias\nCo-occurrence bias is computed from the model-\ngenerated texts by comparing the occurrences of\nall gender-neutral words with female and male\nwords. A word is considered to be biased towards\na certain gender if it occurs more frequently with\nwords of that gender. This deﬁnition was ﬁrst used\nby Zhao et al. (2017) and later adapted by Bor-\ndia and Bowman (2019). Using the deﬁnition of\ngender bias similar to the one used by Bordia and\nBowman (2019), we deﬁne gender bias as\nBN = 1\nN\n∑\nw∈N\n⏐⏐⏐⏐log c(w,m)\nc(w,f)\n⏐⏐⏐⏐,\nwhere N is a set of gender-neutral words, and\nc(w,g) is the occurrences of a word wwith words\nof gender g in the same window. This score\nis designed to capture unequal co-occurrences of\nneutral words with male and female words. Co-\noccurrences are computed using a sliding window\nof size 10 extending equally in both directions.\nFurthermore, we only consider words that occur\nmore than 20 times with gendered words to ex-\nclude random effects.\nWe also evaluate a normalized version of BN\nwhich we denote by conditional co-occurrence\nbias, BN\nc . This is deﬁned as\nBN\nc = 1\nN\n∑\nw∈N\n⏐⏐⏐⏐log P(w|m)\nP(w|f)\n⏐⏐⏐⏐,\nwhere\nP(w|g) =c(w,g)\nc(g) .\nBN\nc is less affected by the disparity in the general\ndistribution of male and female words in the text.\nThe disparity between the occurrences of the two\ngenders means that text is more inclined to men-\ntion one over the other, so it can also be considered\na form of bias. We report the ratio of occurrence\nof male and female words in the model generated\ntext, GR, as\nGR= c(m)\nc(f) .\n3.4.2 Causal Bias\nAnother way of quantifying bias in NLP models is\nbased on the idea of causal testing. The model is\nexposed to paired samples which differ only in one\nattribute (e.g. gender) and the disparity in the out-\nput is interpreted as bias related to that attribute.\nZhao et al. (2018) and Lu et al. (2018) applied this\nmethod to measure bias in coreference resolution\nand Lu et al. (2018) also used it for evaluating gen-\nder bias in language modelling.\nFollowing the approach similar to Lu et al.\n(2018), we limit this bias evaluation to a set of\ngender-neutral occupations. We create a list of\nsentences based on a set of templates. There are\ntwo sets of templates used for evaluating causal\noccupation bias (Table 1). The ﬁrst set of tem-\nplates is designed to measure how the probabilities\nof occupation words depend on the gender infor-\nmation in the seed. Below is an example of the\nﬁrst set of templates:\n[Genderedword] isa |[occupation] .\nHere, the vertical bar separates the seed sequence\nthat is fed into the language models from the target\noccupation, for which we observe the output soft-\nmax probability. We measure causal occupation\nbias conditioned on gender as\nCB|g= 1\n|O|\n1\nG\n∑\no∈O\nG∑\ni\n⏐⏐⏐⏐log p(o|fi)\np(o|mi)\n⏐⏐⏐⏐,\nwhere Ois a set of gender-neutral occupations and\nG is the size of the gender pairs set. For exam-\nple, P(doctor|he) is the softmax probability of\n226\nHe is a |\ndoctor log P(t|s1)\nP(t|s2)\nShe is a |\ns1\ns2\nt\n(a) Occupation bias conditioned on gendered words\nThe doctor is a |\nman\nlog P(t1|s)\nP(t2|s)\nwoman\ns t1\nt2\n(b) Occupation bias conditioned on occupations\nTable 1: Example templates of two types of occupation bias\nthe word doctor where the seed sequence is He\nis a. The second set of templates like below, aims\nto capture how the probabilities of gendered words\ndepend on the occupation words in the seed.\nThe[occupation] isa |[genderedword] .\nCausal occupation bias conditioned on occupation\nis represented as\nCB|o= 1\n|O|\n1\nG\n∑\no∈O\nG∑\ni\n⏐⏐⏐⏐log p(fi|o)\np(mi|o)\n⏐⏐⏐⏐,\nwhere Ois a set of gender-neutral occupations and\nGis the size of the gender pairs set. For example,\nP(man|doctor) is the softmax probability ofman\nwhere the seed sequence is The doctor is a.\nWe believe that bothCB|gand CB|ocontribute\nto gender bias in the model-generated texts. We\nalso note that CB|ois more easily inﬂuenced by\nthe general disparity in male and female word\nprobabilities.\n3.4.3 Word Embedding Bias\nOur debiasing approach does not explicitly ad-\ndress the bias in the embedding layer. Therefore,\nwe use gender-neutral occupations to measure the\nembedding bias to observe if debiasing the output\nlayer also decreases the bias in the embedding. We\ndeﬁne the embedding bias, EBd, as the difference\nbetween the Euclidean distance of an occupation\nword to male words and the distance of the occu-\npation word to the female counterparts. This deﬁ-\nnition is equivalent to bias by projection described\nby Bolukbasi et al. (2016). We deﬁne EBd as\nEBd =\n∑\no∈O\nG∑\ni\n|∥E(o) −E(mi)∥2\n−∥E(o) −E(fi)∥2|,\nwhere O is a set of gender-neutral occupations,\nGis the size of the gender pairs set and E is the\nword-to-vector dictionary.\n3.5 Existing Approaches\nWe apply CDA where we swap all the gendered\nwords using a bidirectional dictionary of gender\npairs described by Lu et al. (2018). This creates\na dataset twice the size of the original data, with\nexactly the same contextual distributions for both\ngenders and we use it to train the language models.\nWe also implement the bias regularization\nmethod of Bordia and Bowman (2019) which\ndebiases the word embedding during language\nmodel training by minimizing the projection of\nneutral words on the gender axis. We use hyper-\nparameter tuning to ﬁnd the best regularization co-\nefﬁcient and report results from the model trained\nwith this coefﬁcient. We later refer to this strategy\nas REG.\n4 Experiments\nInitially, we measure the co-occurrence bias in the\ntraining data. After training the baseline model,\nwe implement our loss function and tune for the\nλ hyperparameter. We test the existing debias-\ning approaches, CDA and REG, as well but since\nBordia and Bowman (2019) reported that results\nﬂuctuate substantially with different REG regu-\nlarization coefﬁcients, we perform hyperparame-\nter tuning and report the best results in Table 2.\nAdditionally, we implement a combination of our\nloss function and CDA and tune for λ. Finally,\nbias evaluation is performed for all the trained\nmodels. Causal occupation bias is measured di-\nrectly from the models using template datasets dis-\ncussed above and co-occurrence bias is measured\nfrom the model-generated texts, which consist of\n10,000 documents of 500 words each.\n4.1 Results\nResults for the experiments are listed in Table 2.\nIt is interesting to observe that the baseline model\nampliﬁes the bias in the training data set as mea-\nsured by BN and BN\nc . From measurements us-\ning the described bias metrics, our method effec-\ntively mitigates bias in language modelling with-\n227\nModel BN BN\nc GR Ppl. CB |o CB |g EB d\nDataset 0.340 0.213 - - - -\nBaseline 0.531 0.282 1.415 117.845 1.447 97.762 0.528\nREG 0.381 0.329 1.028 114.438 1.861 108.740 0.373\nCDA 0.208 0.149 1.037 117.976 0.703 56.82 0.268\nλ0.01 0.492 0.245 1.445 118.585 0.111 9.306 0.077\nλ0.1 0.459 0.208 1.463 118.713 0.013 2.326 0.018\nλ0.5 0.312 0.173 1.252 120.344 0.000 1.159 0.006\nλ0.8 0.226 0.151 1.096 119.792 0.001 1.448 0.002\nλ1 0.218 0.153 1.049 120.973 0.000 0.999 0.002\nλ2 0.221 0.157 1.020 123.248 0.000 0.471 0.000\nλ0.5 + CDA 0.205 0.145 1.012 117.971 0.000 0.153 0.000\nTable 2: Evaluation results for models trained on Daily Mail and their generated texts\nout a signiﬁcant increase in perplexity. At λvalue\nof 1, it reduces BN by 58.95%, BN\nc by 45.74%,\nCB|o by 100%, CB|g by 98.52% and EBd by\n98.98%. Compared to the results of CDA and\nREG, it achieves the best results in both occupa-\ntion biases, CB|gand CB|o, and EBd. We notice\nthat all methods result in GR around 1, indicat-\ning that there are near equal amounts of female\nand male words in the generated texts. In our ex-\nperiments we note that with increasing λ, the bias\nsteadily decreases and perplexity tends to slightly\nincrease. This indicates that there is a trade-off\nbetween bias and perplexity.\nREG is not very effective in mitigating bias\nwhen compared to other methods, and fails to\nachieve the best result in any of the bias metrics\nthat we used. But REG results in the best perplex-\nity and even does better than the baseline model in\nthis respect. This indicates that REG has a slight\nregularization effect. Additionally, it is interesting\nto note that our loss function outperforms REG\nin EBd even though REG explicitly aims to re-\nduce gender bias in the embeddings. Although our\nmethod does not explicitly attempt geometric de-\nbiasing of the word embedding, the results show\nthat it results in the most debiased embedding as\ncompared to other methods. Furthermore, Gonen\nand Goldberg (2019) emphasizes that geometric\ngender bias in word embeddings is not completely\nunderstood and existing word embedding debias-\ning strategies are insufﬁcient. Our approach pro-\nvides an appealing end-to-end solution for model\ndebiasing without relying on any measure of bias\nin the word embedding. We believe this concept is\ngeneralizable to other NLP applications.\nOur method outperforms CDA in CB|g, CB|o,\nand EBd. While CDA achieves slightly better re-\nsults for co-occurrence biases, BN and BN\nc , and\nresults in a better perplexity. With a marginal\ndifferences, our results are comparable to those\nof CDA and both models seem to have similar\nbias mitigation effects. However, our method does\nnot require a data augmentation step and allows\ntraining of an unbiased model directly from bi-\nased datasets. For this reason, it also requires less\ntime to train than CDA since its training data has\na smaller size without data augmentation. Fur-\nthermore, CDA fails to effectively mitigate occu-\npation bias when compared to our approach. Al-\nthough the training data for CDA does not con-\ntain gender bias, the model still exhibits some gen-\nder bias when measured with our causal occupa-\ntion bias metrics. This reinforces the concept that\nsome model-level constraints are essential to debi-\nasing a model and dataset debiasing alone cannot\nbe trusted.\nFinally, we note that the combination of CDA\nand our loss function outperforms all the methods\nin all measures of biases without compromising\nperplexity. Therefore, it can be argued that a cas-\ncade of these approaches can be used to optimally\ndebias the language models.\n5 Conclusion and Discussion\nIn this research, we propose a new approach for\nmitigating gender bias in neural language models\nand empirically show its effectiveness in reducing\nbias as measured with different evaluation metrics.\nOur research also highlights the fact that debias-\ning the model with bias penalties in the loss func-\ntion is an effective method. We emphasize that\nloss function based debiasing is powerful and gen-\n228\neralizable to other downstream NLP applications.\nThe research also reinforces the idea that geomet-\nric debiasing of the word embedding is not a com-\nplete solution for debiasing the downstream appli-\ncations but encourages end-to-end approaches to\ndebiasing.\nAll the debiasing techniques experimented in\nthis paper rely on a predeﬁned set of gender pairs\nin some way. CDA used gender pairs for ﬂipping,\nREG uses it for gender space deﬁnition and our\ntechnique uses them for computing loss. This re-\nliance on pre-deﬁned set of gender pairs can be\nconsidered a limitation of these methods. It also\nresults in another concern. There are gender asso-\nciated words which do not have pairs, like preg-\nnant. These words are not treated properly by\ntechniques relying on gender pairs.\nFuture work includes designing a context-aware\nversion of our loss function which can distinguish\nbetween the unbiased and biased mentions of the\ngendered words and only penalize the biased ver-\nsion. Another interesting direction is exploring the\napplication of this method in mitigating racial bias\nwhich brings more challenges.\n6 Acknowledgment\nWe are grateful to Sam Bowman for helpful ad-\nvice, Shikha Bordia, Cuiying Yang, Gang Qian,\nXiyu Miao, Qianyi Fan, Tian Liu, and Stanislav\nSobolevsky for discussions, and reviewers for de-\ntailed feedback.\nReferences\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. In NIPS’16\nProceedings of the 30th International Conference\non Neural Information Processing Systems, pages\n4356–4364.\nShikha Bordia and Samuel R. Bowman. 2019. Iden-\ntifying and reducing gender bias in word-level lan-\nguage models. ArXiv:1904.03035.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nArXiv:1903.03862.\nKarl Hermann, Tom Koisk, Edward Grefenstette, Lasse\nEspeholt, Will Kay, Mustafa Suleyman, and Phil\nBlunsom. 2015. Teaching machines to read and\ncomprehend. In NIPS’15 Proceedings of the 28th\nInternational Conference on Neural Information\nProcessing Systems, pages 1693–1701.\nAnja Lambrecht and Catherine E. Tucker. 2018. Al-\ngorithmic bias? an empirical study into apparent\ngender-based discrimination in the display of stem\ncareer ads.\nIssie Lapowsky. 2018. Google autocomplete still\nmakes vile suggestions.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam\nAmancharla, and Anupam Datta. 2018. Gen-\nder bias in neural natural language processing.\nArXiv:1807.11714v1.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1532–1543. Association for Com-\nputational Linguistics.\nIlya Sutskever, James Martens, and Geoffrey Hinton.\n2011. Generating text with recurrent neural net-\nworks. In ICML’11 Proceedings of the 28th Inter-\nnational Conference on International Conference on\nMachine Learning, pages 1017–1024.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chag. 2017. Men also like\nshopping: Reducing gender bias ampliﬁcation using\ncorpus-level constraints. In Conference on Empiri-\ncal Methods in Natural Language Processing.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and\nChang Kaiwei. 2018. Learning gender-neutral word\nembeddings. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4847–4853. Association for Com-\nputational Linguistics.",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.9885027408599854
    },
    {
      "name": "Perplexity",
      "score": 0.8924466371536255
    },
    {
      "name": "Computer science",
      "score": 0.7611904144287109
    },
    {
      "name": "Word (group theory)",
      "score": 0.5890509486198425
    },
    {
      "name": "Function (biology)",
      "score": 0.5679176449775696
    },
    {
      "name": "Gender bias",
      "score": 0.5545620918273926
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5344471335411072
    },
    {
      "name": "Language model",
      "score": 0.47477030754089355
    },
    {
      "name": "Natural language processing",
      "score": 0.46513810753822327
    },
    {
      "name": "Machine learning",
      "score": 0.3948691487312317
    },
    {
      "name": "Psychology",
      "score": 0.18879982829093933
    },
    {
      "name": "Linguistics",
      "score": 0.11229285597801208
    },
    {
      "name": "Social psychology",
      "score": 0.09085160493850708
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210128391",
      "name": "Brooklyn Technical High School",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I150149174",
      "name": "Mercer University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210097151",
      "name": "Metropolitan Community College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 56
}