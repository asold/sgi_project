{
  "title": "Navigating China’s regulatory approach to generative artificial intelligence and large language models",
  "url": "https://openalex.org/W4406078572",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2531675602",
      "name": "Mimi Zou",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2096491802",
      "name": "Lu Zhang",
      "affiliations": [
        "China University of Political Science and Law"
      ]
    },
    {
      "id": "https://openalex.org/A2531675602",
      "name": "Mimi Zou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096491802",
      "name": "Lu Zhang",
      "affiliations": [
        "China University of Political Science and Law"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4320711939",
    "https://openalex.org/W6799200175",
    "https://openalex.org/W4400313784",
    "https://openalex.org/W4396990444",
    "https://openalex.org/W2988293491",
    "https://openalex.org/W3186999898"
  ],
  "abstract": "Abstract The rapid development of generative artificial intelligence (AI) systems, particularly those fuelled by increasingly advanced large language models, has raised concerns of their potential risks among policymakers globally. In July 2023, Chinese regulators enacted the Interim Measures for the Management of Generative AI Services (“the Measures”). The Measures aim to mitigate various risks associated with public-facing generative AI services, particularly those concerning information content safety and security. China’s approach to regulating AI to date has sought to address the risks associated with rapidly advancing AI technologies while fostering innovation and development. Tensions between these policy objectives are reflected in the provisions of the Measures. As Beijing moves towards establishing a comprehensive legal framework for AI governance, there will be growing interest in how China’s approach may influence AI governance and regulation at a global level.",
  "full_text": "Cambridge Forum on AI: Law and Governance (2025), 1, e8, 1–16\ndoi:10.1017/cfl.2024.4\nRESEARCH ARTICLE\nNavigating China’s regulatory approach to generative\nartificial intelligence and large language models\nMimi Zou1\n and Lu Zhang2\n1School of Private and Commercial Law, University of New South W ales, Sydney, Australia and2Guangming School of\nJournalism and Communication, China University of Political Science and Law, Beijing, China\nCorresponding author: Mimi Zou; Email: mimi.zou@unsw.edu.au\n(Received 07 August 2024; revised 09 October 2024; accepted 11 October 2024)\nAbstract\nThe rapid development of generative artificial intelligence (AI) systems, particularly those fuelled by\nincreasingly advanced large language models, has raised concerns of their potential risks among policy-\nmakers globally. In July 2023, Chinese regulators enacted the Interim Measures for the Management of\nGenerative AI Services (“the Measures”). The Measures aim to mitigate various risks associated with public-\nfacing generative AI services, particularly those concerning information content safety and security. China’ s\napproach to regulating AI to date has sought to address the risks associated with rapidly advancing AI tech-\nnologies while fostering innovation and development. T ensions between these policy objectives are reflected\nin the provisions of the Measures. As Beijing moves towards establishing a comprehensive legal framework\nfor AI governance, there will be growing interest in how China’ s approach may influence AI governance\nand regulation at a global level.\nKeywords generative AI; large language models; AI safety; AI regulation; Chinese AI regulation\n1. Introduction\nSince the launch of ChatGPT in late 2022, generative artificial intelligence (AI) systems have garnered\nglobal attention at an unprecedented level. Although US tech companies are generally regarded as\n“first-movers, ” especially in rolling out different commercial and industry applications of such tech-\nnology, the rapid development and deployment of generative AI systems based on advanced large\nlanguage models (LLMs) have extended beyond Silicon V alley. As a significant player in AI globally,\nChina has also seen the mushrooming of generative AI applications for public consumption, includ-\ning Baidu’ s “W enxin Yiyuan, ” Huawei’ s “Pangu, ” T encent’ s “Hunyuan Assistant, ” Alibaba’ s “T ongyi\nQianwen, ” SenseTime’ s “Ririxin” and ByteDance’ s “Dou Bao. ”\nThe swift rise of new LLMs poses a range of risks and challenges, which have led to calls for\nstrengthening AI regulation around the world. Two types of regulatory approaches have emerged.\nOne approach favours a lighter-touch regulatory path. The other approach, perhaps most exempli-\nfied by the EU’ s AI Act, is a more stringent approach that advocates for dedicated regulatory bodies\nand requires riskier AI applications to be subject to rigorous testing and pre-market approval pro-\ncesses. Some commentators anticipate that, just as the EU’ s General Data Protection Regulation has\nhad a global impact, the AI Act is likely to extend the so-called “Brussels effect” to the field of AI\nregulation (Rotenberg & Hickok, 2022).\n© The Author(s), 2025. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the Creative\nCommons Attribution licence (http://creativecommons.org/licenses/by/4.0), which permits unrestricted re-use, distribution and\nreproduction, provided the original article is properly cited.\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n2 Mimi Zou and Lu Zhang\nWhile the AI Act has garnered significant global attention, China’ s AI regulations have received\ncomparatively little notice. This is despite China being, in 2023 (before the passage of the AI Act), the\n“first country to implement detailed, binding regulations on some of the most common applications\nof AI” (Sheehan, 2024). As a leading AI power, China’ s regulatory approach can potentially have a\nsignificant influence on global AI governance trends and shape international standards. Our paper\naims to address this knowledge gap by critically examining China’ s approach to regulating generative\nAI to date. W e focus on the Interim Measures for the Management of Generative Artificial Intelligence\nServices (“the Measures”) issued in July 2023 by several ministries and departments under the State\nCouncil, the executive arm of the Chinese state.\nAccording to Article 2 of the Measures, its regulatory scope applies to services that provide gen-\nerated text, images, audio, video, and other content to the public within the territory of China. The\nMeasures do not cover research and development (R&D), enterprise and industry applications that do\nnot directly provide services to the public. The regulation touches on a range of risks associated with\nthe application of generative AI, including content security, personal data protection, data security,\nintellectual property violations, among others. It also introduces a multi-tiered system of obligations\non providers of generative AI services to mitigate such risks.\nAt the time of its introduction in July 2023, the Measures were the first legally binding instrument\nin the world specifically aimed at regulating generative AI. Some have described this regulation as\nreflecting a “vertical approach” to AI regulation (Jia et al., 2023). Such an approach seeks to address\nspecific risks arising from types of AI technologies. In contrast, a “horizontal approach, ” which is\nlargely reflected in the EU’ s AI Act, encompasses comprehensive and overarching regulations that\napply broadly to AI systems (Shaughnessy & Sheehan, 2023). Moreover, China’ s approach has been\ndescribed as “highly reactive” and “adaptive, ” with regulatory bodies swiftly responding to sudden\nchanges and uncertainties (Migliorini,2024; A. H. Zhang, 2022 ). Over the past decade, China’ s cyber\nlaws and regulations have reflected a dual strategy of maintaining control over a variety of risks emerg-\ning from a rapidly advancing technological landscape, while simultaneously fostering development\nand innovation within that same environment.\nT o contextualise the Measures, section 2 of this article provides an overview of China’ s approach\nto regulating AI to date. Section 3 examines the Measures’ underlying policy motivations related to\nAI development and security. Section 4 sheds light on the key provisions of the Measures, focusing on\nthe principal obligations on generative AI service providers to safeguard information content security.\nSection 5 addresses the remaining challenges in the implementation of the Measures and considers\nthe future trajectory of China’ s AI governance.\n2. Overview of China’s approach to regulating AI\nChina’ s AI governance has been primarily characterised by a state-led approach that aims to harness\nAI’ s potential for the country’s international competitiveness, economic growth and social gover-\nnance (Roberts et al., 2021). This approach is intrinsically tied to China’ s ambitions of becoming\na global leader in AI technology. Similarly, it has been observed that “China’ s engagement with\nthe Internet has been guided by its techno-nationalist strategy of development, coupled with the\nimperative of maintaining social stability” (Ebert Stiftung, 2023).\nThe country’s New Generation Artificial Intelligence Development Plan, launched in 2017, out-\nlined a strategy to be a globally dominant player in AI by 2030 (State Council, 2017). This global\nleadership role includes the commercialisation of AI, as well as the shaping of technical and ethical\nstandards. Compared with the more market-driven approaches seen in the US or the rights-based\nframeworks prevalent in the EU, China’ s AI governance has generally prioritised security interests\nand social stability while seeking to promote development of the AI industry.\nIn terms of “methods” for regulating emerging technologies, two primary pathways have been\nobserved. The first pathway involves establishing rules that are tailored to specific technological\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 3\napplications or scenarios. The second pathway adopts a more comprehensive approach that encom-\npasses all potential applications or scenarios (Hacker et al., 2023). Some commentators have referred\nto “vertical” and “horizontal” approaches to AI regulation (Shaughnessy & Sheehan, 2023). V ertical\napproaches typically emphasise distinct regulatory frameworks for specific applications or manifes-\ntations of the technology. On the other hand, horizontal approaches favour the development of broad\nregulatory frameworks that cover almost all potential uses of AI.\nThe EU has leaned towards a horizontal approach, with the AI Act representing “a single piece\nof horizontal legislation to fix the broad scope of what applications of AI are to be regulated”\n(Shaughnessy & Sheehan,2023). Nevertheless, the rapid advancements in and public adoption of new\ngenerative AI tools have challenged the AI Act’ s originally proposed categories of risk that applied\nbroadly to all AI applications. This prompted changes to the final version of the Act that placed\nfoundation models and generative AI under a specific category of general-purpose AI (GPAI). Such\nadvanced AI models and systems are regulated through a separate tiered approach, with more strin-\ngent obligations for “high-impact” GPAI models that pose a “systemic risk, ” as defined in Articles 3\n(64) and (65) of the EU’ s AI Act. These include foundation models trained with significant amounts\nof data, and where their advanced levels of complexity, capabilities and performance can disseminate\nsystemic risks along the value chain.\nChina has primarily adopted a “vertical” approach to regulating AI to date. An important advan-\ntage of this approach is the speed at which legally binding instruments can be introduced or amended\nto respond to emerging issues. For example, on 11 April 2023, the Cyberspace Administration of\nChina (CAC), the primary internet and cyber regulator in China, released an initial draft of the\nMeasures for public feedback, with a submission deadline of 10 May 2023. The entire regulatory\nprocess, including drafting the initial version, gathering public input, conducting discussions and\nrevisions, and obtaining final approval from relevant authorities, was completed within three months.\nThe Measures were promulgated on 13 July 2023 by the CAC (as the lead) along with the National\nDevelopment and Reform Commission, Ministry of Education, Ministry of Science and T echnology,\nMinistry of Industry and Information T echnology, Ministry of Public Security and National Radio\nand T elevision Administration. The Measures took effect on 15 August 2023. This rapid regulatory\nprocess contrasts with the legislative process of higher-level laws by the National People’ s Congress,\nwhich can take several years.\nAt the time of writing, China has not established a more comprehensive legislative frame-\nwork on AI. Policymakers have instead focused on administrative regulations that target specific\nAIdevelopments or applications considered to be “riskier” in terms of security or safety. Besides\nthe Measures, China’ s AI regulations include the Provisions on the Management of Algorithmic\nRecommendations in Internet Information Services in (“ Algorithmic Recommendations Provisions”)\nand the Regulations on the Deep Synthesis Management of Internet Information Service (“Deep\nSynthesis Regulations”), both introduced by the CAC with a range of other government authorities\nin 2021 and 2022 respectively.\nA common thread running through these regulations is their emphasis on information and con-\ntent safety and security risks arising from AI. In an important standards framework on AI Safety\nGovernance issued by the National T echnical Committee 260 on Cybersecurity of the Standardization\nAdministration of China (2024, September), risks of information and content safety were identified\nin Section 3.2.1(a) of this framework as follow:\nAI-generated or synthesized content can lead to the spread of false information, discrimination\nand bias, privacy leakage, and infringement issues, threatening the safety of citizens’ lives and\nproperty, national security, ideological security, and causing ethical risks. If users’ inputs con-\ntain harmful content, the model may output illegal or damaging information without robust\nsecurity mechanisms.\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n4 Mimi Zou and Lu Zhang\nThe Algorithmic Recommendations Provisions aim to control algorithms used for information\ndissemination, introducing new obligations for online news platforms that require them to intervene\nin content recommendations. The Provisions also grant users specific rights, including the ability\nto turn off algorithmic recommendation services, the option to delete tags, and the right to receive\nexplanations when an algorithm significantly affects their interests. The Deep Synthesis Regulations\naddress risks in the information environment arising from “deep synthesis technology. ” Instead of\nusing the politically charged term “deepfakes, ” the regulations employ the more neutral technical\nterm “deep synthesis technology. ” This is defined as the use of deep learning, augmented reality, and\nother algorithms to enable content synthesis or generation, encompassing text, images, audio, video,\nvirtual scenes and more.\nT echnical standards and guidelines play an important role in the operationalisation of laws and\nregulations. Since the introduction of the Measures, the National T echnical Committee 260 on\nCybersecurity of Standardization Administration of China released the Implementation Guidelines\non Cybersecurity Standards for Content Labelling Method in Generative Artificial Intelligence\nServices TC260-PG-20233A on 25 August 2023. While such Guidelines are “voluntary” in nature,\nthey enable regulators to test adoption among industry actors before developing mandatory standards\nor further regulations later on. This approach can be seen in the CAC’s release of a draft regula-\ntion and a draft mandatory standard on AI labelling in September 2024 (discussed later in section\n4.3). Other technical standards specifically related to the Measures at the time of writing include the\nBasic Security Requirements for Generative Artificial Intelligence Services TC260-003 introduced by\nT echnical Committee 260 on 29 Februrary 2024. This set of mandatory technical standards aims to\nprovide basic safety and security requirements for generative AI service providers, including corpus,\nmodel, security measures, and security assessments. It applies to service providers in relation to their\nobligations to conduct security assessments and improve security levels under the Measures. The\nstandards also provide an important point of reference for relevant regulatory authorities to assess\nthe security level of generative AI services.\nHigh-level policy documents play an important role in shaping the trajectory of laws and reg-\nulations. One important document is the aforementioned New Generation Artificial Intelligence\nDevelopment Plan, which proposed three phases of development in China’ s AI governance frame-\nwork (State Council,2017). In the first phase, by 2020, China will “initially establish AI ethical norms,\npolicies, and regulations in some areas. ” By 2025, the second phase, there will be “initial establish-\nment of AI laws and regulations, ethical norms and policy systems, and the formation of AI security\nassessment and control capabilities. ” Finally, by 2030, China will have “constructed more compre-\nhensive AI laws and regulations, and an ethical norms and policy system. ” This Plan, while broadly\nframed, is being implemented. The first phase saw the release various guidelines and ethical norms\nby relevant government bodies. At the highest level, General Offices of the CPC Central Committee\nand the State Council ( 2022) issued guidelines in 2022 aimed at enhancing ethical governance in\nscience and technology. The Ministry of Science T echnology ( 2021) also released a document that\noutlined the fundamental ethical principles guiding China’ s AI policy framework. The next phase of\nimplementation included the introduction of administrative regulations including the Algorithmic\nRecommendations Provisions, the Deep Synthesis Regulations and the Measures.\nAt the time of writing, there are signs of regulatory efforts towards a comprehensive AI legislation.\nIn June 2023, an AI Law was added to the State Council’ s Legislative Plan (State Council, 2023). The\nnew law is expected to expand upon current regulations, forming a more comprehensive legislative\nframework that will become the cornerstone of China’ s AI policy (Sheehan, 2023). In August 2023,\nthe Chinese Academy of Social Sciences (2023), an important and influential think tank regularly\ninvolved in lawmaking processes, released a Model AI Law 1.1 Expert Draft. A more comprehensive\nAI legislation in China may eventually resemble the EU’ s AI Act. However, the journey from initial\nlegislative planning to the eventual enactment of such a law at the national level could take a few\nyears. Meanwhile, regulators have swiftly introduced specific measures to mitigate the risks posed by\nnew, potentially disruptive technologies.\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 5\nIt is important to note that administrative regulations like the Measures, which have “interim” in its\ntitle, are often used by lawmakers to test certain approaches before more significant and higher-level\nlegal instruments are considered. In this way, lawmakers can undertake and learn from some degree of\nregulatory experimentation. The term “interim” implies that the Measures are likely to be temporary,\nallowing for future regulatory development. Such an approach has been described as an “iterative”\nmodel of governance to address new problems and challenges as they arise, in which administrative\nregulations and guidelines are continuously introduced, updated, replaced and/or repealed (Sheehan,\n2023). However, this can also result in a patchwork of multiple, overlapping regulatory instruments\nin the lead-up to a more comprehensive legislation.\nWhile it may be state-led, China’ s regulatory approach to AI is not as “top-down” as some may\nassume. Indeed, the development and passing of China’ s AI regulations have involved a wide range\nof stakeholders, including different policymakers and regulatory bodies as well as key players outside\nthe party-state structure. These non-state actors have included technology companies, technologists,\nacademic experts, journalists and policy researchers. This involvement of different actors serves mul-\ntiple policy objectives and creates a negotiable policy space where these actors can influence China’ s\npresent and future AI regulations (Migliorini, 2024). Sheehan has described these actors’ roles as “a\nmix of public advocacy, intellectual debate, technical workshopping, and bureaucratic wrangling”\n(Sheehan, 2024).\nAn example of this dynamic, multi-actor policy process can be gleaned from the initial draft of\nthe Measures that was released for public consultation in April 2023. The initial draft did not merely\nrestrict its application to public-facing generative AI services. The said draft also imposed additional\nobligations on generative AI service providers, such as requiring providers to fine-tune their models\nwithin three months of discovering unlawful content. Following feedback from different actors dur-\ning the public consultation, the final version of the Measures limited its scope to public-facing service\nproviders and reduced the range of obligations on service providers.\nAs we have seen, China’ s regulation of AI has evolved rapidly, based on a vertical, reiterative\napproach that targets particular technological developments and applications that regulators con-\nsider to pose a certain (high) degree of risks, with a consistent focus on AI-based content security\nor safety risks. The balancing act between maintaining control over such risks while fostering indus-\ntry development and innovation is particularly evident in the regulation of generative AI, where the\npotential for both breakthrough innovations and significant risks to information and content secu-\nrity is high. Having established this regulatory context, we now turn our attention to a more detailed\nexamination of the policy objectives underlying the Measures in the next section.\n3. The Measures: balancing AI development and security\nConfronted with the potentially transformative impact of advanced AI systems and models, Chinese\npolicymakers are grappling with the challenge of balancing two main regulatory goals: AI develop-\nment and security. The notion of “development” in this context refers to China’ s desire to propel the\ngrowth of the AI industry and boost its standing as a global AI powerhouse. Meanwhile, “security”\n(sometimes referred to as “safety”) has been at the centrepiece of all of China’ s cyber and technology-\nrelated laws and regulations to date. For policymakers, there is a fundamental need for guardrails to\nprevent and mitigate a range of security risks associated with the rapid development of emerging AI\ntechnologies (Qiao, 2023).\nThe dual pillars of “development” and “security” are reflected in Article 1 of the Measures. Article\n1 sets out the following regulatory objectives: fostering responsible growth of generative AI while\nensuring national security and public interests and protection of the rights of citizens, legal entities,\nand other organisations. Additionally, Article 3 reinforces the equal importance of “development”\nand “security” as well as emphasises the principle of balancing innovation promotion with lawful\ngovernance.\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n6 Mimi Zou and Lu Zhang\nAlthough both terms were presented side-by-side in policy statements related to the Measures,\n“development” appeared to be consistently placed before “security. ” The emphasis on development\nbecame more pronounced in the final version of the Measures in comparison with the initial draft\nthat focused much more on security concerns. Some prominent scholars in China have argued that\nthe primary “insecurity” is the underdevelopment of China’ s AI industry. Notably, Professor Liming\nW ang has underscored that, when compared to other potential risks, the most significant danger for\nChina is the risk of falling behind technologically (L. M. W ang, 2023).\nA key question is the extent to which and how the Measures will support the development of the\ngenerative AI industry in China. Within the Chinese legal framework, regulations often derive their\nauthority from higher-level laws. Article 1 of the Measures explicitly cites, as its legal foundation, the\nLaw on the Promotion of Scientific and T echnological Progress (2021 amendment). The Law on the\nPromotion of Scientific and T echnological Progress has the overarching goal of\ncomprehensively promoting scientific and technological progress, harnessing the role of science\nand technology as the primary productive force, innovation as the primary driving force, and\ntalent as the primary resource. Facilitating the transformation of scientific and technological\nachievements into practical productive forces, driving technological innovation to support and\nlead economic and social development, and comprehensively building a socialist modernised\ncountry.\nBy anchoring the Measures to the Law on the Promotion of Scientific and T echnological Progress,\nthere is an emphasis on aligning the development of generative AI with broader policy objectives\naimed at fortifying China’ s technological advancements. In essence, the Measures are linked to a\nwider strategy of Beijing for achieving global leadership as well as self-sufficiency in respect of AI\ndevelopmen. As early as 2017, the Chinese leadership has made it clear that “China should strengthen\ndeployment around core technologies, top talents, standards, and norms, aiming to gain leadership\nin the new round of international scientific and technological competition (in AI)” (State Council,\n2017).\nArticle 2 of the Measures is of particular significance, delineating the Measures’ application to\n“services that provide generated text, images, audio, video, and other content to the public within\nthe territory of the People’ s Republic of China using generative AI technology. ” Article 3 makes it\nclear that the regulatory scope excludes “industry organisations, enterprises, education and research\ninstitutions, public cultural institutions, and relevant professional institutions that do not provide\ngenerative AI services to the general public. ” As discussed earlier, the draft version had originally\ncontemplated a much wider regulatory scope. The change made in the final version of the Measures,\nwhich excludes uses of generative AI for scientific research and enterprise or industrial uses from\nits scope, reflects more strongly the goal of facilitating more widespread adoption and further\ndevelopment of the technology (Xu, 2023).\nAn important example of policymakers’ attempts to balance development and security concerns\nin the Measures is reflected by the distinction between the technology itself and the services built\nupon it. This differentiation seeks to recognise that certain areas of generative AI development and\napplication require more stringent oversight, while other areas may benefit from a more permissive\nor light-touch approach. In practice, this means that regulation tends to focus on areas where public\ninterests, safety and security are most at stake.\nThe Measures make an important distinction between “core” generative AI technologies on the one\nhand, and services arising from these technologies on the other. The former is primarily addressed\nin Chapter II of the Measures, titled “T echnology Development and Governance, ” which empha-\nsises the promotion of innovation and the application of generative AI across diverse industries\nand domains. The provisions here contain words such as “encourage, ” “support” and “promote, ”\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 7\nparticularly in relation to “trial and error” in technology R&D. Chapter II reflects policymakers’ con-\nsideration that the Measures should not excessively interfere with the development of core generative\nAI technologies.\nThe Measures focuses on regulating service providers. According to Article 22, generative AI ser-\nvice providers refer to organisations or individuals who offer services that are based on utilising\ngenerative AI. This includes services provided through application programming interfaces or other\nmethods. The obligations on providers of generative AI services under Chapter II of the Measures\ninclude ensuring pre-training, optimisation training and all data-related activities adhere to exist-\ning laws and regulations (particularly the Cybersecurity Law, Data Security Law, and intellectual\nproperty laws). Service providers must also ensure that the data sources and foundational mod-\nels used are lawfully obtained and employ effective measures to increase the quality of the training\ndata by “enhancing its truthfulness, accuracy, objectivity, and diversity. ” Chapter III of the Measures,\ntitled “Service Standards, ” outline a range of obligations on generative AI service providers. These\nobligations include safeguarding user personal information, meeting requirements regarding user\nidentification, delivering secure and stable services, and reporting unlawful content, among others.\nWhile the intention of differentiating between technology and services is to promote technological\ndevelopment while mitigating risks associated with generative AI applications, the Measures have\nbeen criticised for its broad definition of “service providers. ” This broad definition includes services\nthat develop foundation models. There have been calls for more granularity in regulatory oversight\nover different types of actors involved in generative AI technology development and service provision,\nan issue that we will return to later in this paper.\nOn the whole, the Measures attempt to avoid excessive interference in technological develop-\nment. Nevertheless, there are also considerable provisions that lean towards prioritising security over\ndevelopmental objectives, particularly in addressing content risks associated with generative AI. This\nemphasis aligns with the broader trajectory of AI regulations in China, as discussed earlier, which\nreflect the importance of maintaining control over the information environment. In the following\nsection, we assess the key provisions of the Measures, focusing on information and content safety\nobligations imposed on generative AI service providers.\n4. Key obligations on regulated actors\nWhile the Measures set out a range of obligations on various actors, the main provisions are targeted\nat services involved in the generation, processing and delivery of content using generative AI. For pol-\nicymakers, the ever-increasing capabilities of the technology pose significant content security risks,\nfor instance, it can make it very challenging for humans or even AI to identify false or misleading\ncontent. The Measures represent a shift in the existing regulatory landscape for information content\nsecurity, blurring traditional distinctions between content producers and service providers.\n4.1 Existing content production and service provision regulations\nBy way of background, existing regulatory frameworks for information content security in China\nhave traditionally distinguished between content production and technical service provision. Content\nproducers generally bear direct liability for content-related infringements. Service providers refer to\norganisations or individuals that provide online information service activities that assist in carrying\nout such infringements. The latter are held liable only when they have knowledge or should have\nknown about the direct infringement (Liu, 2016). This distinction is reflected in the framework for\nregulating liability for harms caused by content generated or shared on platforms (Y ao & Li, 2023).\nA common approach taken by Chinese courts has been to characterise a platform’ s role in informa-\ntion dissemination as either content provision or technical services, as echoed in Provisions of the\nSupreme People’ s Court on Several Issues Concerning the Application of Law in the Trial of Civil\nDisputes Involving Infringement of the Right to Online Communication (2020).\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n8 Mimi Zou and Lu Zhang\nA key framework for information content governance in China is set out by the Provisions on the\nGovernance of the Online Information Content Ecosystem (the Provisions), an administrative regu-\nlation introduced in 2020. The Provisions distinguish between online information content producers\n(Chapter II), online information content service platforms (Chapter III) and online information con-\ntent service users (Chapter IV). According to Article 41 of the Provisions, online information content\nproducers are organisations or individuals involved in creating, duplicating or disseminating online\ninformation content. Online information content service platforms refer to online information ser-\nvice providers offering content transmission services. Finally, online information content service\nusers are organisations or individuals that use such services. Article 8 of the Provisions requires\nonline information content service platforms to meet their obligations to manage information con-\ntent (such as taking appropriate measures when discovering illegal or harmful content) and enhance\nthe governance of their information content ecosystems.\nIt is important to understand the concept of “principal obligations” ( zhuti zheren) in the above\nregulatory framework. The concept was originally used in a political context, referring to the respon-\nsibilities of party organisations and government departments. In 2014, it was introduced for the first\ntime in the legal context in the amendment to Article 3 of the W orkplace Safety Law as the basis of a\ncorporation’ s responsibility for workplace safety. However, when applied in the context of information\ncontent regulation, this concept has not been thoroughly clarified and explained. One scholar sug-\ngests that the term can be understood as the ultimate responsibility of the service platform provider\nfor any unlawful activity that takes place on the platform (Y e,2018). Some have argued that it refers to\na form of strict liability (Liu,2022). This conceptual ambiguity gives rise to regulatory implementation\nand compliance challenges in practice.\nThe Deep Synthesis Regulations followed a similar distinction between content providers and\ntechnical service providers. It outlines three main categories of regulated entities: (1) deep synthe-\nsis service providers, (2) deep synthesis service technology support providers and (3) deep synthesis\nservice users. The Deep Synthesis Regulations assign a range of information security-related obli-\ngations on deep synthesis service providers, such as establishing management rules, strengthening\ncontent review and having in place mechanisms for “debunking rumours” (Article 7 to Article 12\nof the Measures). Such obligations are like those of online information content service platforms\nunder the Provisions on the Governance of the Online Information Content Ecosystem. In other\nwords, deep synthesis technology service providers must comply with relevant obligations related to\ninformation content management.\nPolicymakers to date have not provided an official explanation on the relationship between the\nDeep Synthesis Regulations and the Measures. It has been suggested that existing regulations are\nsufficient to address the content-related risks associated with generative AI (Tang, 2023). However,\nwe contend that the advancements in the LLMs underlying the latest generative AI applications since\nthe launch of ChatGPT have considerably magnified the degree of such risks beyond that envisaged\nby the Deep Synthesis Regulations. This amplification of risks necessitated the swift introduction of\nthe Measures by Chinese policymakers, only months after the introduction of the Deep Synthesis\nRegulations.\n4.2 Dual obligations on generative AI service providers\nArticle 9 of the Measures makes it clear that generative AI service providers must assume the obliga-\ntions of content producers and meet relevant obligations as related to information content security.\nThis is the first time in Chinese regulations that AI service providers are deemed “content producers”\nand assigned relevant obligations.\nThe Measures impose a dual set of obligations on generative AI service providers relating to both\ncontent generation and the provision of technical services (see Table 1 below). Article 14 requires\nservice providers, upon discovering illegal content, to promptly respond by stopping its generation,\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 9\nTable 1.SummaryofobligationsofserviceprovidersundertheMeasures\nContentproducerobligations Serviceproviderobligations\n• Article4:Notgeneratingcontentprohibitedbylaws\nandregulations.\n• Article7:Ensuringthattrainingdataprocessing\nactivitiesforgeneratingcontentarelawful.\n• Article8:Requirementsfordataannotation\nactivities.\n• Article9:Assumingtheresponsibilityofanonline\ninformationcontentproducer,fulfillingonlineinfor-\nmationsecurityobligations,andfulfillingpersonal\ninformationprotectionobligations.\n• Article12:Identificationofgeneratedcontent.\n• Article10:Takingmeasurestopreventminorsfrom\nexcessivelyrelyingonandbecomingaddictedtoAI\nservices.\n• Article11:Protectingtheinputandusagerecordsof\nusersandmeetingobligationsaspersonalinforma-\ntionhandlers(includingacceptingandhandling\nrequestsforaccess,copying,correction,sup-\nplementation,deletionofpersonalinformation,\netc).\n• Article13:Providingsecure,stable,andcontinuous\nservicestoensurenormaluserusage.\n• Article14:Discoveringillegalcontent,timelyremoval\nandrectification,preservingrecords,andreporting\ntothecompetentauthority.\n• Article15:Establishingasoundcomplaintand\nreportingmechanism.\n• Article17:Conductingsecurityassessmentsand\nfulfillingalgorithmfilingprocedures.\nhalting its transmission, and/or removing it. Furthermore, they must implement AI model optimisa-\ntion and training to rectify the situation, and report incidents to regulators. According to Article 10,\nservice providers must take appropriate measures against users engaged in illegal activities, including\nwarnings, restricting functionalities, and suspending or terminating services.\nThese obligations on generative AI service providers under the Measures are consistent with the\nset of “principal obligations” for information management imposed on online information con-\ntent service platforms under the Provisions on the Governance of the Online Information Content\nEcosystem as mentioned earlier. At the same time, the Measures specify in Chapter III (titled “Service\nStandards”) that generative AI service providers must assume the obligations of online information\ncontent producers “in accordance with the law. ” Such obligations include labelling the generated con-\ntent (Article 12), safeguarding personal data (Article 9) and establishing a comprehensive complaint\nreporting mechanism (Article 15).\nUnder the Measures, generative AI service providers bear direct liability for content generation\nas well as indirect liability for failing to take effective measures to stop the dissemination of unlaw-\nful content. Some commentators have argued that this dual set of obligations are too onerous for\ngenerative AI service providers and do not create a favourable environment for the healthy devel-\nopment of the industry (Y ao & Li, 2023). Others have pointed out that other regulations to date,\nincluding the Provisions on the Governance of the Online Information Content Ecosystem and the\nDeep Synthesis Regulations, have separated the obligations of information content producers from\nthose of technology service providers (Xu, 2023).\nY et, we contend that the Measures are responsive to the characteristics of generative AI services\nthat entail advanced technological integration of content production, service provision and user util-\nisation. The obligations on generative AI services providers reflect the significance of certain service\nproviders’ involvement in the content generation process. While user prompts are part of the pro-\ncess, the pre-training and fine-tuning of the AI models by service providers also directly impact on\nthe content generation.\n4.3 Algorithm disclosure and AI-generated content labelling\nAn emerging trend in AI regulatory tools involves various disclosure requirements, driven by the\nneed for greater algorithmic transparency. A primary goal of these requirements is to enhance the\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n10 Mimi Zou and Lu Zhang\nforeseeability of and control over associated risks through the disclosure of key information about\nthe underlying algorithms. Several regulatory tools in this realm can found in the Measures, which\nare centred around algorithm filing for certain types of generative AI services and the labelling of\nAI-generated content. This is where the Measures intersect with the two other AI-related regulations\ndiscussed in section 2.\nAccording to Article 17 of the Measures, providers of generative AI services with “public opinion\nproperties or social mobilisation capacity” must comply with the Algorithmic Recommendations\nProvisions. The Algorithmic Recommendations Provisions established a mandatory algorithm filing\nand registry system for algorithm recommendation service providers with “public opinion properties\nor social mobilisation capacity. ” These service providers are typically the larger social media platforms.\nSuch providers must file with the CAC certain information on their services and algorithms, includ-\ning the service form, fields of application, algorithm type, algorithm self-assessment, content to be\ndisplayed and other relevant information. If such information changes, the providers must also under-\ntake modification procedures through the filing system within 10 days of the change. The providers\nmust make public its filing index. Where relevant, providers must also carry out security assessments\nwith the CAC.\nLabelling AI-generated content can also be considered another form of transparency-enhancing\ntool for generative AI. AI labelling requirements can be found in the EU AI Act, EU Digital Services\nAct and the US Executive Order on the Safe, Secure, and Trustworthy Development and Use of AI.\nLabelling involves embedding identifiable information or markers within the content itself, indicating\nits origin or creator. This serves as a means of disclosing the content’ s AI-generated nature and its\nsource.\nArticle 12 of the Measures requires generative AI service providers to label generated content\nsuch as images and videos in accordance with the Deep Synthesis Management Regulations. Two key\nprovisions of the Deep Synthesis Management Regulations are particularly relevant here. Article 16\nspecifies that service providers must use technical measures to add unobtrusive marks that do not\naffect the user’ s experience. However, where deep synthesis services may “lead to public confusion or\nmisidentification, ” significant markings must be provided to the service users, according to Article 17\nof the Deep Synthesis Management Regulations. However, there is no definition of what could “lead\nto public confusion or misidentification. ” Indeed, a broad interpretation of this concept could impact\non everyday uses of generative AI services by the public (Y ao & Li, 2023).\nAs mentioned earlier, the National T echnical Committee 260 on Cybersecurity of Standardization\nAdministration of China ( 2023, August 25 ) issued technical guidelines on various methods for\ncontent labelling by AI-generated content service providers. According to the guidelines, implicit\nlabelling should not be directly perceptible to humans, such as embedding in file metadata, but can\nbe extracted from the content using technical means. Explicit labelling, on the other hand, involves\ninserting a semi-transparent label in the interactive interface or background of the content. The label\nshould at least contain information such as “ AI-generated. ”\nThe substance of these Guidelines have been developed into administrative regulations and\nmandatory standards. On 14 September 2024, the CAC released two draft regulatory instruments\nfor public consultation. The first is the draft Measures for the Labelling of AI-Generated Synthetic\nContent, which seeks to regulate how Internet Information Service Providers (IISPs) label AI-\ngenerated content. The regulation, in its draft version, requires both explicit and implicit labelling\nof such content. It further specifies when and how these labels must be applied. IISPs must also reg-\nulate the dissemination of AI-generated content on their platforms, verifying and adding labels as\nnecessary. The draft also outlines obligations for internet application distribution platforms and sets\nrules for interactions between IISPs and users regarding labelling. The second is a related national\nstandard, the Cybersecurity T echnology – Labelling Method for Content Generated by AI, which\nprovides detailed technical requirements for both explicit and implicit labelling. Compliance with\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 11\nthis standard is intended to be mandatory and was developed through extensive consultation with\nindustry experts and enterprises.\nOverall, much of the labelling obligations are imposed on the AI service provider, not the user. The\nuser’ s obligations are limited to not destroying or tampering with the labelling or markings. Article\n18 of the Deep Synthesis Management Regulations stipulates that “no organisation or individual shall\nuse technical means to delete, tamper with, or conceal deep synthesis marks. ” As examined below, the\nMeasures impose significantly fewer obligations on users of generative AI services compared to the\nextensive responsibilities placed on service providers.\n4.4 Obligations on users of generative AI services\nWhile regulators recognise the need to distinguish generative AI service providers and users, this\ndistinction is not always straightforward in practice. This blurred line reflects the unique nature of\ngenerative AI, where outputs are often a result of complex interactions between the AI model and\nuser inputs. The Measures attempt to address this complexity by regulating certain aspects of user\nbehaviour and conduct, acknowledging the dual role of users as both consumers and co-creators of\nAI-generated content. This is reflected in Article 4 of the Measures which imposes obligations on both\nproviders and users to avoid generating false or harmful information, avoid infringing upon others’\nrights, and improve the accuracy and reliability of generated content.\nThe Measures also incorporate provisions aimed at empowering users and ensuring accountability\nof service providers. Article 18 grants users the right to file complaints or reports with the relevant\nauthorities if they find the service provider is not complying with laws, administrative regulations or\nthe provisions of the Measures. Complementing this, Article 14 requires service providers to estab-\nlish a sound mechanism for promptly receiving and handling public complaints and reports and\nresponding to the outcomes.\nHowever, the Measures do not address appeal and recourse mechanisms for situations where ser-\nvice providers infringe upon users’ rights, such as imposing unreasonable restrictions on use and/or\ntermination of services. For example, if a service provider unilaterally determines that a user has gen-\nerated illegal content and decides to restrict or remove the user from using its services, the Measures\ndo not specify whether the user has the right to appeal to the service provider or another body, and\nif so, what the procedure for such an appeal should be. This regulatory gap could potentially lead to\narbitrary decisions by service providers, with users having limited recourse to challenge these actions.\nIt may also hinder the development of fair and consistent standards for content moderation across\ndifferent service providers.\nNevertheless, an analysis of the Measures reveals an apparent imbalance in the obligations imposed\non service providers versus users. While service providers face a range of obligations and poten-\ntial liabilities, the obligations placed on users appear to be relatively light in comparison. Article 4,\nwhile placing high-level obligations on users, does not attribute direct legal liability to them. This\napproach raises questions regarding how user obligations will be enforced in practice. The disparity\nin obligations between service providers and users under the Measures could potentially lead to sce-\nnarios where service providers bear disproportionate responsibility for AI-generated content that is\nsubstantially influenced or directed by user inputs.\n5. Challenges and outlook for regulating generative AI in China\nHaving examined the key provisions of the Measures, we now turn our attention to the challenges\nand regulatory gaps that might be addressed and consider the impact of the Measures on China’ s\nongoing efforts to develop a comprehensive AI Act. W e will further examine how China’ s approach\nmight influence AI governance and regulation at a global level.\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n12 Mimi Zou and Lu Zhang\n5.1 Filling the regulatory gaps\nOverall, the Measures were introduced relatively quickly, and the matters it addressed were at a\nbroad, principled level. This has left significant room for interpretation and implementation. Many\nprovisions remain relatively high-level and lack specific operational details, which can make it diffi-\ncult for service providers and enforcement authorities to implement. Some obligations will need to\nrely on subsequent rules, standards, or guidelines to ensure implementation. Examples in this cat-\negory include the interpretation of “the accuracy and reliability of generated content” in Article 4,\ndisputes over intellectual property rights of generated content in Article 7, the user complaint report-\ning system in Article 15 and the interaction between service providers and regulatory authorities\nduring supervisions and inspections in Article 19.\nThe Measures focus on the administrative obligations of service providers but overlook the provi-\nsions related to civil obligations. The content security obligations under the Measures mainly manifest\nas public law obligations. Their enforcement primarily follows public law procedures, with regulatory\nauthorities (government departments) holding generative AI service providers accountable. Article\n21 of the Measures primarily entails administrative sanctions like warnings, notifications, criticisms,\nand corrective actions. Such an approach neglects addressing situations where private individuals or\nentities could initiate legal actions against service providers.\nWhile the Chinese Civil Code could potentially fill in this gap, there are limitations. For example,\nChinese laws and regulations currently lack provisions regarding the determination of damages and\nthe burden of proof for civil liability arising from AI. This gap is particularly significant given the\ncomplexity, autonomy and opacity of AI systems. The Measures do not specify damages caused by\nAI services or the burden of proof for establishing damages. In contrast, the EU has been seeking to\ndeveloprules applicable to civil liability for persons harmed by AI (the AI Liability Directive) along-\nside the AI Act. W e propose that this is an area where Chinese legislators could take more targeted\ncivil liability provisions. These provisions should address the unique challenges such as the complex-\nity of determining causation and the potential for autonomous decision-making. If introduced, such\nprovisions may end up in a comprehensive AI legislation (discussed below).\nIn our view, implementation guidelines and technical standards are likely to become increasingly\nimportant, such as the abovementione standards issued by the National T echnical Committee 260\non Cybersecurity of Standardization Administration of China. The two draft instruments released\nfor public consultation by the State Council in September 2024, the Measures for the Labelling of\nAI-Generated Synthetic Content and the Cybersecurity T echnology – Labelling Method for Content\nGenerated by AI, will provide more detailed compliance directions for regulated actors once they take\neffect. For regulatory authorities, effectively coordinating the relationship between “hard law” instru-\nments like laws and regulations and “soft law” tools will become a crucial aspect of AI governance in\nChina.\n5.2 A more balanced approach to content producer obligations\nThe current provisions of the Measures reflect a regulatory approach that prioritises control at the\nservice provider level. While this may simplify enforcement and align with existing content regulation\nframeworks, it may not fully address the unique challenges posed by generative AI.\nW e propose that the Measures should delineate a clearer and balanced system of responsibility\nfor different regulated actors. Generative AI service providers are designated as “content producers”\nunder the Measures, which mean that they assume almost all responsibilities for information content\nsecurity. Despite Article 4 stating that both users and service providers have responsibility, all obli-\ngations found in Chapter IV are directed at service providers. Given that users actively contribute to\ncontent creation and dissemination on platforms, requiring some accountability on the part of users\ncould foster more responsible usage and deter unlawful activities.\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 13\nSome have called for greater responsibility on users and platforms that disseminate the generated\ncontent (Dong & Chen, 2024). Nevertheless, the Measures’ broad definition of generative AI service\nproviders encompasses a wide range of platforms in China that offer generative AI-powered services\nto the public. Consequently, these platforms are subject to the stringent content producer obligations\noutlined in the Measures.\nT o strike a balance between generative AI development and security, the Measures should con-\nsider compliance “safe harbours. ” Such a system would still place “content producer” responsibility\non service providers but offer exemption from liability under certain circumstances (Shen,2023). Safe\nharbours are commonly found in the context of online service providers’ liability for user-generated\ncontent. A well-known example is the US Digital Millennium Copyright Act that includes safe har-\nbour provisions for internet service providers (ISPs). These provisions protect ISPs from copyright\ninfringement liability for content posted by users if the ISPs meet specific requirements, such as\npromptly removing infringing content upon receiving a valid takedown notice from the copyright\nholder. Safe harbour provisions for generative AI service providers can help to prevent or mitigate\ncontent-related risks while encouraging innovation and industry development goals.\nFurthermore, a tiered approach espoused in Article 3 of the Measures needs to deploy a variety\nof regulatory approaches and tools to achieve both development and security goals. Currently, the\nMeasures do not distinguish between foundation model providers and providers of “downstream”\ngenerative AI application services. Both categories of providers are deemed “service providers”\nand therefore subject to the same obligations under the Measures. A more reasonable approach\nfor balancing both policy goals would be to adopt a multi-tiered governance system that provides\nroom for foundation model providers to experiment, while requiring greater regulatory scrutiny for\n“downstream” providers with reasonable safe harbours.\nAs generative AI and LLMs advance at a rapid pace, policymakers face the challenge of develop-\ning more sophisticated regulatory frameworks. These frameworks must address the intricate nature\nof AI-based content generation, which blurs the lines between user input and machine output. The\nMeasures reflect an important first step. W e propose that the implementation of the Measures need\nto balance user empowerment with appropriate levels of user responsibility and accountability. This\ncould involve implementing a tiered system of user responsibility based on the level of human\ninput and the intended use of the generated content. Additionally, policymakers need to establish\nclearer guidelines for determining liability in cases where harmful content emerges from the complex\ninteraction between user prompts and AI systems.\n5.3 Developing a comprehensive AI law\nMany provisions in the Measures represent restatements or refinements of existing regulations,\nincluding those concerning generation of false information, data tagging, preventing discrimina-\ntion in algorithmic design, algorithm transparency, personal information protection, user complaint\nmechanisms, among others. As analysed in section 4 , the obligations on content producers reflect\nexisting obligations found in the Provisions on the Governance of the Online Information Content\nEcosystem. Furthermore, the Measures directly incorporate provisions from existing regulations. For\ninstance, the obligations related to algorithm filing and security assessment under Article 17 of the\nMeasures are directly linked to the Algorithmic Recommendations Provisions. The labelling of AI-\ngenerated content requirements under Article 12 of the Measures incorporate the Deep Synthesis\nRegulations.\nIn our view, there is not a considerable deal of novel institutional design in the Measures.\nOne commentator points out that the new obligations introduced by the Measures centre on two\naspects: generative AI service providers taking on the obligations of content producers and specific\nrequirements relating to training data of generative AI models (X.R. W ang,2023).\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n14 Mimi Zou and Lu Zhang\nW e suggest that the Measures may be intentionally conservative in its regulatory architecture.\nOn the one hand, the Measures can be seen as a prompt response by policymakers to the risks and\nchallenges posed by rapidly evolving AI technologies. On the other hand, Chinese policymakers are\nseeking coherence and consistency among different AI-related laws and regulations, recognising the\ndownsides and costs of regulatory fragmentation. At the time of writing, it appears that China is\nworking on establishing a general legislation like the EU’ s AI Act. China has undertaken such an\napproach in other areas of technology regulation, with the enactment of comprehensive laws such\nCybersecurity Law, Data Security Law, and the Personal Information Protection Law. These national-\nlevel laws provided the foundation for introducing a range of regulatory instruments such as those\naddressing algorithm assessment and filing and deepfake prevention, as mentioned earlier. If China\ngoes down this path, we believe two aspects of regulatory design should be considered by legislators.\nFirst, the primary focus of a general AI law should be on integrating disparate regulatory\napproaches, standardising existing rules and addressing gaps. For example, an algorithm filing sys-\ntem has been introduced, but the Algorithmic Recommendations Provisions do not lay out the next\nphase of the system. For example, there need to be further clarification on whether registration serves\nas a prerequisite for ex-ante liability or as a basis for ex-post liability. This distinction will shape how\nthe registration process is perceived by and its actual effects on the parties involved. A general AI law\nneeds to address important gaps like these, or at least lay the foundation for doing so.\nSecond, a general AI legislation must clearly define liability of different regulated actors, consid-\nering the complexity, autonomy and opacity of AI systems and applications of AI . As mentioned\nearlier, specific provisions covering issues like damage determination and burden of proof are cru-\ncial. Establishing clear rules for different types of redress (from administrative to civil claims) can help\nto mitigate technology risks while reducing compliance costs for businesses and regulatory challenges\nfor government agencies.\n5.4 Implications for global AI governance\nIn recent years, the global competition in AI has expanded from the technological domain to the reg-\nulatory domain. Several Chinese scholars have pointed out that the Measures not only respond to the\nregulatory and governance needs of domestic generative AI technology and industry but also empha-\nsise China’ s increasing involvement in global AI governance and rule-marking (L. Zhang,2023). On\none hand, countries grapple with intense competition in AI technology, several major powers are also\nengaged in a global race to set the rules and standards that should govern and regulate AI. At the time\nof writing, the EU has been the most active player in seeking to establish global standards through\nthe introduction of the AI Act.\nChina also hopes that the introduction of the Measures will showcase its pioneering, agile\napproach to AI governance (Yi, 2023). Formulating standards and norms have consistently been an\nimportant aspect of China’ s AI development plans, as reflected in the 2017 New Generation Artificial\nIntelligence Development Plan. It remains to be seen whether the regulatory approaches and tools\nreflected in the Measures will be adopted and adapted elsewhere. China’ s model may not be easily\nadaptable in jurisdictions with more open information environments.\nW e agree with the proposition that “despite China’ s drastically different political system, policy-\nmakers in the United States and elsewhere can learn from its regulations” (Sheehan, 2023). China’ s\n“vertical, ” “adaptive” approach to regulating AI to date, as well as the technical and bureaucratic tools\nsuch as algorithmic disclosure, labelling of AI-generated content and technical standards arising from\nChina’ s AI regulations can be applied in other parts of the world. There is no doubt that different pol-\nicy motivations (and prioritisation thereof) will continue to lead to shape each jurisdiction’ s stance\ntowards AI governance. The idea of regulatory interoperability and consistency has underlined the\nquest of global governance for AI (Engler, 2022). The risks associated with AI are not confined to\nindividual countries but are international in scope, requiring coordinated international action.\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 15\nPromoting international cooperation on AI governance faces numerous real-world obstacles aris-\ning from the geopolitical landscape and technological competition between major players. Fostering\ndomestic AI development around the concept of digital sovereignty could have negative spillover\neffects on the global AI market. At the same time, a “race to the bottom” where countries compete\nto have more lenient regulations can exacerbate the risks of significant harms that such technologies\ncan create (Criddle et al., 2023).\n6. Conclusion\nThe Measures focus on information content security risks arising from applications of generative AI\nfor the public. Our analysis has shed light on the primary obligations assigned to service providers.\nWhile these Measures establish some broad principles and rules for regulating generative AI, a more\nintricate regulatory framework of “hard” and “soft” law instruments will be essential for effective\nimplementation. A more nuanced stance to regulating the diverse actors and activities, such as a\nclearer delineation of user liability and offering certain exemptions from liability for service providers,\nrequires further consideration by policymakers, especially if China is heading towards a more com-\nprehensive AI legal framework. The rapid advancements in generative AI pose universal regulatory\nchallenges. Globally, countries are navigating new territory when it comes to coordinating a cohesive\napproach. China’ s path in international AI cooperation and competition remains uncertain.\nCompeting interests. The author declares none.\nReferences\nCriddle, C., Espinoza, E., & Liu, Q. (2023, September, 13). The global race to set the rules for AI . Financial Times. Retrieved\nfrom https://www.ft.com/content/59b9ef36-771f-4f91-89d1-ef89f4a2ec4e.\nDong, H. J., & Chen, J. K. (2024). Meta-regulation: An ideal alternative to the primary responsibility as the regulatory model\nof generative AI in China. Computer Law & Security Review, 54, 106016. [in Chinese].\nEbert Stiftung, F . (2023). China’s regulations on algorithms: Context, impact, and comparisons with the EU . Retrieved from\nhttps://library.fes.de/pdf-files/bueros/bruessel/19904.pdf.\nEngler, A. (2022, February 1). The EU and U.S. are starting to align on AI regulation. Brookings. Retrieved from https://www.\nbrookings.edu/articles/the-eu-and-u-s-are-starting-to-align-on-ai-regulation.\nGeneral Offices of the CPC Central Committee and State Council (2022). Opinion on Strengthening the Governance of\nScience and T echnology Ethics. Retrieved fromhttps://www.gov.cn/zhengce/2022-03/20/content_5680105.htm\nHacker, P ., Engel, A., & Mauer, M.(2023, February 5). Regulating ChatGPT and other large generative AI Models. Retrieved\nfrom https://www.arxiv.org/abs/2302.02337.\nInterim Measures for the Management of Generative Artificial Intelligence Services (2023). Issued by the Cyberspace\nAdministration of China and others. July 10, 2023. Effective 2023, August 15.\nJia, K., Zhao, J., & Fu, H. (2023). Dealing with uncertainty challenges: The theoretical definition of agile governance of\nalgorithms. Journal of Library and Information Knowledge, 1, 1–10. [in Chinese].\nLaw on the Promotion of Scientific and T echnological Progress (1993). Promulgated by the National People’ s Congress of\nthe People’ s Republic of China. July 2, 1993. Amended 2007, December 29, and 2021, December 24.\nLiu, Q. (2022). On the subject responsibility of internet platforms.Journal of East China University of Political Science and Law,\n5, 79–93. [in Chinese].\nLiu, W . J.(2016). Identification of information network communication behaviors.Legal Studies, 3(3), 122–139. [in Chinese].\nMigliorini, S. (2024). China’ s Interim Measures on Generative AI: Origin, content and significance.Computer Law & Security\nReview, 53, 105985.\nMinistry of Science and T echnology(2021). Ethical Guidelines for the New Generation of Artificial Intelligence. [in Chinese].\nhttps://www.most.gov.cn/kjbgz/202 109/t20210926_177063.html.\nNational T echnical Committee 260 on Cybersecurity of Standardization Administration of China .(2023, August 25).\nImplementation Guidelines for Cybersecurity Standards: Content Marking Methods in Generative AI Services (TC260-PG-\n20233A). [in Chinese].\nNational T echnical Committee 260 on Cybersecurity of Standardization Administration of China (2024, February 29).\nBasic Security Requirements For Generative Artificial Intelligence Services (TC260-003). Retrieved from https://www.tc260.\norg.cn/upload/2024-03-01/1709282398070082466.pdf. [in Chinese].\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press\n16 Mimi Zou and Lu Zhang\nNational T echnical Committee 260 on Cybersecurity of the Standardization Administration of China (2024,\nSeptember). AI Safety Governance Framework Version 1.0 . Retrieved from https://www.tc260.org.cn/upload/2024-09-09/\n1725849192841090989.pdf.\nProvisions of the Supreme People’s Court on Several Issues Concerning the Application of Law in the Trial of Civil\nDisputes Involving Infringement of the Right to Network Communication . Fa Shi [2020] No. 19, issued on 2020,\nDecember 29. [in Chinese].\nProvisions on the Governance of the Online Information Content Ecosystem (2019). Issued by the Cyberspace\nAdministration of China and others. 2019, December 15, effective 2020, March 1.\nProvisions on the Management of Algorithmic Recommendations in Internet Information Services (2022). Issued by the\nCyberspace Administration of China and others. 2021, December 31, effective 2022, March 1.\nQiao, Y .(2023, August 29). Promoting development through safety, encouraging innovation through governance . Cyberspace\nAdministration of China. Retrieved from https://www.cac.gov.cn/2023-08/29/c_1694965943880030.htm. [in Chinese]\nRegulations on the Deep Synthesis Management of Internet Information Service. Issued by the Cyberspace Administration of\nChina. 2022, November 25, effective 2023, January 1.\nRoberts, H., Cowls, J., Morley, J., Taddeo, M., W ang, V ., & Floridi, L.(2021). The Chinese approach to artificial intelligence:\nan analysis of policy, ethics, and regulation. AI & Society, 36(1), 59–77.\nRotenberg, M., & Hickok, M. (2022, August 22). Artificial intelligence and democratic values: Next steps for the United States.\nThe Council on Foreign Relations. Retrieved from www.cfr.org/blog/artificial-intelligence-and-democratic-values-next-\nsteps-united-states.\nShaughnessy, O. M., & Sheehan, M. (2023, February 14). Lessons from the world’s two experiments in AI gov-\nernance. Retrieved from https://www.carnegieendowment.org/2023/02/14/lessons-from-world-s-two-experiments-in-ai-\ngovernance-pub-89035.\nSheehan, M. (2023). China’ s AI regulations and how they get made. Journal of International Relations and Sustainable\nDevelopment, 24, 108–125.\nSheehan, M. (2024). Tracing the roots of China’s AI regulations. Carnegie Endowment for International Peace, USA. Retrieved\nfrom https://carnegieendowment.org/2024/02/27/tracing-roots-of-china-s-ai-regulations-pub-91815.\nShen, W .(2023). Practice and legal reflection on technological safe harbors. Peking University Law Journal, 4, 906–922. [in\nChinese].\nState Council(2017). Notice from the State Council on the Issuance of the New Generation Artificial Intelligence Development\nPlan (Guofa. 2017, No. 35). [in Chinese].\nState Council (2023). State Council General Office Notice on Issuing the Legislative Work Plan For the Y ear 2023 . State\nCouncil Document No. 18 of 2023. Retrieved fromhttps://www.gov.cn/zhengce/content/202306/content_6884925.htm. [in\nChinese].\nTang, L. Y .(2023). Legal regulation and the Chinese path of chatGPT under embodied ethics. Eastern Jurisprudence, 3, 34–46.\n[in Chinese].\nW ang, L. M.(2023, July 7). The greatest risk of generative AI lies in technological backwardness and the risk of being strangled or\nconstrained. Caixin News. Retrieved from https://news.caijingmobile.com/article/detail/496460. [in Chinese].\nW ang, X. R.(2023, April 12). Expert interpretation: Key systems and compliance obligations of the draft for soliciting opinions on\nthe management measures for generative artificial intelligence services. CCIA Data Security W orking Committee. Retrieved\nfrom https://mp.weixin.qq.com/s/CEzbAwv40O6_GSFCEakMmw. [in Chinese].\nXu, W .(2023). On the legal status and responsibilities of generative artificial intelligence service providers—taking chatGPT\nas an example. Legal Science, 4, 69–80. [in Chinese].\nY ao, Z. W ., & Li, Z. L.(2023). Legal regulation of content risks in generative artificial intelligence. Journal of Xi’ an Jiaotong\nUniversity (Social Sciences), 5, 1–20. [in Chinese].\nY e, Y . Q.(2018). Responsibilities of internet platform: From regulation to governance. Financial and Economic Law, 5, 49–63.\n[in Chinese].\nYi, W .(2023, July 27). How to Interpret China’s First Effort to Regulate Generative AI Measures. China Briefing. Retrieved from\nhttps://www.china-briefing.com/news/how-to-interpret-chinas-first-effort-to-regulate-generative-ai-measures/.\nZhang, A. H. (2022). Agility over stability: China’ s great reversal in regulating the platform economy. Harvard International\nLaw Journal, 63(2), 457–514.\nZhang, L. (2023). Exploration of governance and regulation of general artificial intelligence risks—issues and challenges\ntriggered by chatGPT .E-Government, 9, 107–117. [in Chinese].\nCite this article: Zou M and Zhang L. (2025). Navigating China’ s regulatory approach to generative artificial intelligence and\nlarge language models. Cambridge Forum on AI: Law and Governance 1, e8, 1–16. https://doi.org/10.1017/cfl.2024.4\nhttps://doi.org/10.1017/cfl.2024.4 Published online by Cambridge University Press",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7324391603469849
    },
    {
      "name": "Computer science",
      "score": 0.4697943925857544
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4589439928531647
    },
    {
      "name": "China",
      "score": 0.449537068605423
    },
    {
      "name": "Political science",
      "score": 0.1186496913433075
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I177955009",
      "name": "China University of Political Science and Law",
      "country": "CN"
    }
  ],
  "cited_by": 4
}