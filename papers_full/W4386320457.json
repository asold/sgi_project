{
  "title": "FedBEVT: Federated Learning Bird's Eye View Perception Transformer in Road Traffic Systems",
  "url": "https://openalex.org/W4386320457",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5102008145",
      "name": "Rui Song",
      "affiliations": [
        "Fraunhofer Institute for Transportation and Infrastructure Systems",
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5100700826",
      "name": "Runsheng Xu",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A5032935966",
      "name": "Andreas Festag",
      "affiliations": [
        "Technische Hochschule Ingolstadt"
      ]
    },
    {
      "id": "https://openalex.org/A5068374815",
      "name": "Jiaqi Ma",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A5063781430",
      "name": "Alois Knoll",
      "affiliations": [
        "Technical University of Munich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4366085645",
    "https://openalex.org/W4380318404",
    "https://openalex.org/W6839180059",
    "https://openalex.org/W6840665015",
    "https://openalex.org/W4386025570",
    "https://openalex.org/W4383066393",
    "https://openalex.org/W3202375294",
    "https://openalex.org/W4385269011",
    "https://openalex.org/W4312894406",
    "https://openalex.org/W6811230113",
    "https://openalex.org/W4312641958",
    "https://openalex.org/W4225793049",
    "https://openalex.org/W4392016505",
    "https://openalex.org/W4214530037",
    "https://openalex.org/W4390872833",
    "https://openalex.org/W6838844109",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W3109395584",
    "https://openalex.org/W3199933970",
    "https://openalex.org/W3101755493",
    "https://openalex.org/W3207378505",
    "https://openalex.org/W3175663678",
    "https://openalex.org/W3206772271",
    "https://openalex.org/W4282049157",
    "https://openalex.org/W3137531938",
    "https://openalex.org/W3213080933",
    "https://openalex.org/W3136170095",
    "https://openalex.org/W4380451020",
    "https://openalex.org/W3019945581",
    "https://openalex.org/W4285222486",
    "https://openalex.org/W4206298942",
    "https://openalex.org/W3093606573",
    "https://openalex.org/W4313042636",
    "https://openalex.org/W4308068552",
    "https://openalex.org/W3023574229",
    "https://openalex.org/W6728757088",
    "https://openalex.org/W6770590064",
    "https://openalex.org/W6790936303",
    "https://openalex.org/W6810249531",
    "https://openalex.org/W3133814152",
    "https://openalex.org/W6803116340",
    "https://openalex.org/W6791189488",
    "https://openalex.org/W6772318479",
    "https://openalex.org/W4387969057",
    "https://openalex.org/W6791102956",
    "https://openalex.org/W3112044954",
    "https://openalex.org/W6768467352",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W4312604822",
    "https://openalex.org/W4386066469",
    "https://openalex.org/W4295832403",
    "https://openalex.org/W4312352414",
    "https://openalex.org/W6746839373",
    "https://openalex.org/W2767079719",
    "https://openalex.org/W6745935785",
    "https://openalex.org/W3210076120",
    "https://openalex.org/W3201193904",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W4377713808",
    "https://openalex.org/W4321609084",
    "https://openalex.org/W4226305814",
    "https://openalex.org/W2990789643",
    "https://openalex.org/W2976621868",
    "https://openalex.org/W4290056039",
    "https://openalex.org/W4287326337",
    "https://openalex.org/W4281255813"
  ],
  "abstract": "958",
  "full_text": "1\nFedBEVT: Federated Learning Bird’s Eye View\nPerception Transformer in Road Traffic Systems\nRui Song*, Member, IEEE, Runsheng Xu*,\nAndreas Festag, Senior Member, IEEE, Jiaqi Ma, Member, IEEE, and Alois Knoll, Fellow, IEEE\nAbstract—Bird’s eye view (BEV) perception is becoming in-\ncreasingly important in the field of autonomous driving. It\nuses multi-view camera data to learn a transformer model that\ndirectly projects the perception of the road environment onto the\nBEV perspective. However, training a transformer model often\nrequires a large amount of data, and as camera data for road\ntraffic are often private, they are typically not shared. Federated\nlearning offers a solution that enables clients to collaborate and\ntrain models without exchanging data but model parameters.\nIn this paper, we introduce FedBEVT, a federated transformer\nlearning approach for BEV perception. In order to address\ntwo common data heterogeneity issues in FedBEVT: (i) diverse\nsensor poses, and (ii) varying sensor numbers in perception\nsystems, we propose two approaches – Federated Learning\nwith Camera-Attentive Personalization (FedCaP) and Adaptive\nMulti-Camera Masking (AMCM), respectively. To evaluate our\nmethod in real-world settings, we create a dataset consisting\nof four typical federated use cases. Our findings suggest that\nFedBEVT outperforms the baseline approaches in all four use\ncases, demonstrating the potential of our approach for improving\nBEV perception in autonomous driving.\nIndex Terms—Federated learning, bird’s eye view, road envi-\nronmental perception, vision transformer, cooperative intelligent\ntransportation systems\nI. I NTRODUCTION\nRecently, there has been a significant surge of interest in\nthe bird’s-eye-view (BEV) perception for autonomous driv-\ning. BEV representations of traffic scenarios are particularly\nappealing for several reasons. Firstly, as all autonomous ve-\nhicles (A Vs) are located at ground level [1], [2], the BEV\nrepresentation can omit the z-axis to make the perception\nresults more efficient [3]. Secondly, it provides rich context\nand geometry information that can be directly utilized for\ndownstream tasks such as planning [4], [5]. Especially, within\nthe first comprehensive survey addressing control systems for\nautonomous vehicles and connected and automated vehicles\n* The first two authors contributed equally.\nThis work was supported by the German Federal Ministry for Digital\nand Transport (BMVI) in the projects “KIVI – KI im Verkehr Ingolstadt” and\n”5GoIng – 5G Innovation Concept Ingolstadt”.\nRui Song and Andreas Festag are with Fraunhofer Institute for\nTransportation and Infrastructure Systems IVI, Ingolstadt, Germany, e-mail:\n{rui.song, andreas.festag}@ivi.fraunhofer.de.\nRui Song and Alois Knoll are with the Chair of Robotics, Artificial In-\ntelligence and Real-Time Systems, Technical University of Munich, Garching,\nGermany, e-mail: rui.song@tum.de, knoll@in.tum.de.\nRunsheng Xu and Jiaqi Ma with the Department of Civil and Environ-\nmental Engineering, University of California, Los Angeles, CA 90024 USA,\ne-mail: rxx3386@ucla. edu, majiaqimark@gmail.com.\nAndreas Festag is with Technische Hochschule Ingolstadt, CARISSMA\nInstitute for Electric, COnnected, and Secure Mobility (C-ECOS), Ingolstadt,\nGermany, e-mail: andreas.festag@carissma.eu.\nFig. 1. FedBEVT with camera-attentive personlization. The positional em-\nbeddings are considered as private parameters for each client. Other parts of\nBEVT (in gray) are shared to the server for an aggregation.\n[5], the authors thoroughly expound on the pivotal role of\nperception systems in enabling low-level control. Finally, the\nBEV representation can be a unified space for data from\ndifferent sensor modalities (e.g., camera, LiDAR) and times-\ntamps to fuse without much extra effort [6]. Traditionally,\nachieving temporal alignment in multi-source sensor fusion\nrequires additional robust algorithms, such as the popular\nestimation-prediction integrated framework proposed in [7]\nand the computation light multi-sensor fusion localization\nalgorithm in GPS challenging scenarios [8].\nBEV perception research has begun exploring the use of\nmulti-view camera data for predicting BEV maps due to their\nlow cost [9]–[13]. Nevertheless, inferring 3D information from\n2D data is challenging, as mono cameras only provide 2D\ninformation. Recent works have thus utilized vision trans-\nformers, known for their ability to reason about correlations\nbetween different data, to solve the 2D-to-3D problem [3], [9],\n[11], [14]–[16]. Despite promising results, these transformer-\nbased methods have been trained on limited public datasets,\nsuch as NuScenes [17], which may not generalize well. Com-\npanies such as automotive original equipment manufacturers\n(OEMs), sensor suppliers, software solution providers, and\nresearch institutes, need to install multi-sensor systems on\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nseveral vehicles and gather large amounts of data during\nextended driving sessions that can be used as the training\ndataset. Nevertheless, such data is often private and can be\nprohibitively expensive, with some being completely unshared.\nIn this context, the advent of federated learning provides a\nsolution that enables collaborative training processes without\ndata exchange and addresses the issue of data privacy.\nThe federated learning approach offers many benefits but\ncan lead to data heterogeneity while training across clients. In\nthe context of camera-based BEV perception, this challenge\nof data heterogeneity is caused by the wide range of sensor\nconfigurations, with variations in the number of cameras\nand poses of installed sensors on vehicles or transportation\ninfrastructure. [18]. These variations can result in significant\ndifferences in data characteristics across clients, which vanilla\nfederated learning methods may not be able to overcome. In\nthis paper, we aim to address the practical question of how to\nleverage the advantages of external data while minimizing the\nimpact of data heterogeneity in a federated learning framework\nto achieve the highest possible performance improvement for\nlocal models.\nTo address the above data heterogeneity challenge, we\npropose a new federated learning framework for BEV per-\nception transformers (BEVT), called FedBEVT (as shown in\nFig. 1) that enables selective sharing of specific parts of the\nBEV transformer with the server. Following the approach\nof most transformer-based BEV perception works [3], [11],\n[12], we split a BEV vision model into five components:\nencoder for image feature extraction, positional embedding\nthat contains camera geometry information, cross-attention\nmodule to project front views to bird’s-eye view, convolution\nlayers to refine the features in the transformers, and decoder\nto transfer the BEV representation to the final prediction.\nGiven the variability of camera poses across different\nclients, sharing the entire model for global aggregation results\nin unsatisfactory performance. To address this, we adopt\ncamera-attentive personalization in FedBEVT by privatizing\nthe positional embedding that contains camera intrinsic and\nextrinsic information. Additionally, to address the varying\nnumber of cameras in each client, we introduce adaptive multi-\ncamera masking (AMCM) to train multi-view data with a\nconsistent BEV size by overlapping masks, which are built\nbased on the combined field of views of all cameras. This\nensures a consistent BEV embedding size during federated\nlearning and enables clients with varying camera systems to\nlearn together. To evaluate the effectiveness of our methods,\nwe create a dataset with these variations and distribute it\namong various clients in federated settings, simulating real-\nworld federated learning scenarios in intelligent traffic sys-\ntems. Our experiments demonstrate that our method achieves\nbetter test accuracy with reduced communication costs for\nmost clients.\nOur contributions can be summarized as followings:\n• We present a federated learning framework for the BEV\ntransformer in road traffic perception applications. To\nthe best of our knowledge, this is the first federated\ntransformer training framework specifically designed for\nthe BEV perception task.\n• We provide a benchmark multi-view camera dataset for\nBEV perception in road traffic scenarios under federated\nsettings. To address the challenge of data heterogeneity,\nwe consider two popular data variations in federated\nlearning application scenarios in C-ITS domain: ( i) di-\nverse sensor poses and ( ii) varying sensor numbers in\nperception systems.\n• We propose a federated learning framework based on\ncamera-attentive personalization (FedCaP) for training\nBEV transformer models. By considering the relationship\nbetween training data and the transformer, we adjust the\npersonalization aspect of the transformer in federated\nlearning, providing each client with a customized model\nthat better fits their local data, thereby improving the\naccuracy of local BEV perception.\n• We introduce an adaptive multi-camera masking method\nthat enables clients with varying numbers of cameras to\ntrain models using federated learning.\n• We formulate four typical use cases in road traffic systems\nand divide benchmark data into different clients in fed-\nerated settings accordingly. These distributed datasets are\nused to demonstrate the performance of training BEVT\nthrough our federated learning system and baselines.\nOur open-source implementation of FedBEVT is publicly\navailable in GitHub 1.\nII. R ELATED WORK\nA. BEV Perception in Road Traffic Systems\nBEV perception involves transforming input image se-\nquences from a perspective view to a BEV , enabling percep-\ntion tasks such as 3D bounding box detection or semantic\nmap segmentation. There are two main approaches to BEV\nperception: geometric-based and transformer-based methods.\nGeometric-based methods leverage the natural geometric pro-\njection relationship between camera extrinsic and intrinsic\nparameters to project the perspective view to a BEV . For\nexample, LSS [18] uses a categorical distribution over depth\nand a context vector to lift 2D images to a frustum-shaped\npoint cloud, which is then splatted onto the BEV plane using\nthe camera extrinsics and intrinsics. BEVDet [10] follows a\nsimilar framework, with the addition of a BEV encoder to\nfurther refine the projected BEV representations. Transformer-\nbased methods, on the other hand, can implicitly utilize\nthe camera geometry information with learnable embeddings.\nBEVFormer [9] uses the geometry information to obtain the\ninitialized sampling offset and applies deformable attention to\nquery the image features into the learnable BEV embedding.\nAlthough it achieves high accuracy, it is slow. CVT [11]\ndevelops positional embeddings for each individual camera\ndepending on their intrinsic and extrinsic calibrations. These\nembeddings are added to the image features and learnable\nBEV embeddings, and a vanilla cross-view attention trans-\nformer is applied to fetch the image features to the BEV\nembedding. Building upon CVT, CoBEVT [3] replaces the\nvanilla attention with a novel fused axial attention to largely\n1https://github.com/rruisong/FedBEVT\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nsave computation cost and construct a hierarchical structure to\nexplore the multi-scale features. As CoBEVT achieves a good\ntrade-off between inference speed and accuracy, this paper\nselects it as the deployed model for federated learning study.\nB. Federated Learning for Intelligent Traffic Systems\nFederated learning brings great opportunities and potential\nto the intelligent traffic systems. It allows the use of more\ndata to train better-performing models while respecting privacy\nregulations [19]–[22]. Previous research has commenced the\nexploration of using federated learning to train models perti-\nnent to applications in intelligent transportation systems, such\nas 2D object recognition and detection [23]–[25], behavior\nprediction [26], localization [27], [28], and blockchain-based\nautonomous vehicles [29]–[32]. Nevertheless, the application\nof federated learning for BEV perception remains unexplored.\nMoreover, existing studies [33]–[35] primarily address chal-\nlenges of heterogeneity in federated learning in the context\nof system diversity and label shift, while the issue of het-\nerogeneous sensor configurations, a primary cause of data\nheterogeneity, has not been investigated. To bridge these gaps,\nour work concentrates on the application of BEV perception\nand strives to address the heterogeneity resulting from sensor\nconfigurations.\nC. Personalized Federated Learning\nFederated learning is a collaborative approach to machine\nlearning that involves a large number of clients working\ntogether within a network [36]. Typically, a server initiates\nthe learning process and clients download the global model\nto train it on their local data. The trained local models are\nthen uploaded back to the server, and the global model is\nupdated through aggregation. After several communication\nrounds, a model that performs well on the global data is\nobtained. However, such models may not work well with local\ndata, as the local data distribution in most federated learning\napplications is often different from the global data distribution.\nTo address the challenge of local data distribution discrep-\nancies in federated learning, personalized federated learning\nhas been proposed as a solution [37]–[39]. This approach\ncustomizes the model in each client to account for the unique\ncharacteristics of their local data. One of the most popular\nand effective methods for achieving personalized federated\nlearning is the architecture-based approach [40]. This method\ndecouples the model’s parameters, allowing only a subset\nof parameters to be shared and aggregated among clients,\nwhile the private parameters are learned solely on local\ndata. Previous research has attempted to select these private\nparameters based on model architecture [41]–[44] or data\nsimilarities [45]–[47]. However, none of these methods have\nbeen proposed specifically for training personalized trans-\nformer models, which have a significantly different structure\ncompared to other machine learning models. Therefore, we\nfurther study on developing personalized federated learning\nmethods that are tailored to the unique characteristics of\ntransformer models.\nFig. 2. Illustration of AMCM used for two vehicles equipped with different\nnumbers of cameras. By adapting the mask to the total field of view in\neach perception system, the size of BEV embeddings can be maintained and\nadjusted for use in FedBEVT.\nD. Federated Learning for Transformer\nTransformer is a neural network architecture that uses self-\nattention mechanisms and was originally designed for natural\nlanguage processing tasks [48]. It has also been effective in\ncomputer vision tasks such as object recognition (ViT) [49]–\n[51]. As demonstrated in [52], the Transformer architecture\nhas displayed a heightened level of performance compared\nto traditional convolutional neural networks when trained\non extensive datasets. Moreover, this architecture showcases\npromising potential in the domain of detecting small objects,\nas exemplified in UA V imagery. Federated learning offers the\npotential to further enhance the training process by incorporat-\ning data from multiple clients, making it a promising approach\nfor training Transformer models. Recent experimental results\nin [53] have shown that the Transformer architecture is more\nrobust to distribution shifts and improves federated learning\nfor object detection tasks.\nHowever, its application to the BEV semantic segmentation\ntask, which is crucial in autonomous driving and intelligent\ntraffic systems, remains an area for further research. Our\nobservation has revealed that variations in camera setup are the\nprimary source of data heterogeneity in federated learning for\nBEV semantic segmentation tasks. To tackle this challenge,\nour federated learning framework incorporates personalized\npositional embedding and integrates cross attention and CNN\nlayers to boost the transformer’s performance and leverage\nlarger data amounts.\nIII. M ETHODOLOGY\nA. Problem Formulation\nIn a federated learning training scenario, we consider K\nclients, each with their own local dataset consisting of Nk data\npoints paired with the corresponding BEV semantic masks\nYk. Each data point Xk,i contains Lk images from a multi-\nview camera system, resulting in the format (Xk, Yk). Here,\nXk ∈ RNk×Lk×H×W×3 and Yk ∈ RNk×h×w×2, where H and\nW represent the height and width of one image, and h and\nw represent the height and width of the BEV semantic mask.\nThe semantic mask has two classes, background and vehicles.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nFig. 3. The system overview of FedBEVT illustrates vehicle clients from public traffic and clients from private data resources. The federated learning server\nmanages the training process with protocols for model compression, client selection, and secure aggregation similar to those in a typical federated learning\nframework for deep learning models. Note that the data heterogeneity exists due to various clients in intelligent road traffic systems.\nThese clients may come from a variety of industries, including\noriginal equipment manufacturers (OEMs), logistics and taxi\ncompanies, car rental companies, or infrastructure owners such\nas traffic management departments.\nIn general federated learning, each client receives a global\nmodel from the server and trains locally for several epochs\nbefore returning the trained model to the server for fusion\ninto a new global model. This process continues for a certain\nnumber of communication rounds until a trained model is\nobtained. The goal of federated learning is to minimize the\nfollowing:\nf(w) = 1\nN\nKX\nk=1\nNkX\ni=1\nfi(w) = EXi∈X[fi(w)], (1)\nwhere fi(w) = l(BevT (Xi), Yi; w), N is the number of data\npoints in all clients, l is the loss function, and BevT is a\ntransformer-based BEV model with three major components:\nimage encoder, BEV transformer, and BEV decoder. The\nimage feature encoder utilizes a CNN network to encode\nmulti-camera images. The core of the BEV transformer is\nusually a multi-layer attention structure [48]. In this paper,\nwe employ the Fax-Attention schema [3], which involves both\nsparse cross-view attention and self-attention between image\nfeatures and the BEV query. Additionally, BEV query and\npositional embeddings are required as inputs to the BEVT.\nThe BEV query is a learnable embedding to represent the\ngrid world, and the ultimate goal is to locate the position of\nother traffic objects in this query. The positional embeddings\ncontain position information of each transformer image fea-\nture, including information related to the coordinate system\ntransformation for converting the camera perspective to the\nBEV perspective. This improves the intuitiveness of the input\nfor generating BEV features. We adopt the approach from [11]\nand represent the output of the positional embeddings as\nfollows, assuming the extrinsic parameters of each camera j\nare Ri,j,k ∈ R3×3 for rotation and ti,j,k ∈ R3:\nzi,k = Φ(vk)(T(Ri,j,k, ti,j,k, pi,j,k)Lk\nj=1). (2)\nHere, the image coordinate-system position features in p are\nfirst transformed into the 3D vehicle coordinate-system using\nT. Next, the resulting positions are mapped by Φ(vk), pro-\nducing a positional embedding zi,k. We denote the parameters\nin Φ as vk, to distinguish other parameters in the transformer\nmodel u used in Sec. III-B.\nThe BEV features are obtained by passing through multiple\nFax-Attention layers and processed by a decoder network to\nproduce semantic segmentation outcomes in the BEV perspec-\ntive, which enables direct determination of the position of\nsurrounding cars on the motion plane. Our approach addresses\ntwo key challenges: ( i) developing tailored models for unique\nsensor systems, and ( ii) effectively training models using data\nfrom varying numbers of cameras.\nB. Camera-Attentive Personalization\nThe variation in camera positions across clients can cause\nthe traditional federated learning results to deviate from the\noptimal values of local data. This is due to the encoding of\ncamera position differences into positional embeddings during\ntransformer model construction, which directly affects the\nparameter training.\nTo tackle this challenge, we introduce federated learning\nwith camera-attentive personalization (FedCaP), which in-\nvolves separating the locally trained parameters vk in MLP for\npositional embedding, as shown in Eq. 2, from the transformer\nand treating them as private parameters for a given client k.\nApart from vk, we share the remaining parameters in wk in the\nBEVT and refer to these public parameters as uk for the client\nk. As a result, the global update in each communication round\nis represented by the aggregated public parameters from all\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nclients, which we denote as u. Then the goal of local training\nis minimizing:\nFk(u, vk) = 1\nNk\nNkX\ni=1\nfi(u, vk), (3)\nand the global goal is rewritten from Eq. 1:\nf(u, v1, .., vK) =\nKX\nk=1\nNk\nN Fk(u, vk). (4)\nEach client uploads the encoder, decoder, and other trans-\nformer parts in BEVT. After the server aggregates these\nparameters, the global parameters are coupled with the private\nparameters to train the next round of data.\nIn particular, The local update for each client, k, consists\nof two stages:\nvk ← vk − ηv∇vF(BevT ; B),\nuk ← uk − ηu∇uF(BevT ; B), (5)\nwhere ηv and ηu are the learning rates for vk and uk,\nrespectively2, B is a batch of the local dataset Dk, and the\nbatch size is B. The number of local training epochs is denoted\nby E, and each individual epoch is referenced by the index\ne. The shared parameters, u, are updated globally through\naggregation as follows:\nu ←\nKX\nk=1\nNk\nN uk. (6)\nIf we consider that only a subset St (where M = |St|) of all\nclients are selected for aggregation, 6 can be reformulated as:\nu ←\nX\nk∈St\nNk\nN uk. (7)\nThe loss function for the client k is expressed as: F(u, vk) =\nEB⊆Dk F(u, vk; B). The pseudocode of FedCaP is outlined in\nAlgorithm 1.\nC. Adaptive Multi-Camera Masking\nFor each client, multi-view data points may come from a\ndifferent number of cameras. To facilitate federated learning\nacross these clients, we introduce adaptive multi-camera mask-\ning (AMCM), which adaptively overlays a mask on the BEV\nquery based on the comprehensive Field-of-View (FoV). In\nessence, all clients initialize the BEV query with a uniform\nsize and spatial dimensions.\nFig. 2 demonstrates this process using two clients: the first\nemploys four cameras, while the second uses only two, a front\nview and a rear view. Based on the total FoV applicable to each\nclient type, the BEV query is masked, thus enabling activation\nstrictly within the FoV area. The relationship between each\ncamera and the BEV query leads to the projection of each\nclient’s data points onto a BEV query of consistent size. This\nfacilitates a more effective aggregation process in federated\nlearning.\n2To consolidate the hyperparameters, we set these learning rates to be\nequal, such that η = ηv = ηu.\nAlgorithm 1 FedCaP: Federated Learning with\nCamera-attentive Personalization\nServer Input: number of the communication round T\nClient Input: learning rate ηv, ηu\nClient Input: number of local training epochs Ek\nOutput: BevT with public parameter u and private parameter\nvk for each client\n1: Server initializes u0 to all clients\n2: for communication round t = 1, 2, ..., Tdo\n3: St ← ClientSelection (S) ▷ optional client selection\noperation in FL orchestration\n4: for each client k ∈ St in parallel do\n5: if client k is selected for the first time then\n6: client k encodes the camera calibration param-\neters into vk based on Equation 2\n7: end if\n8: ˜∆uk ← ClientUpdate (u)\n9: uk ← u + ˜∆uk\n10: end for\n11: u ← Aggregate({uk|∀k ∈ St}) ▷ secure aggregation\nin the server\n12: end for\n13:\n14: ClientUpdate(u)\n15: uk ← u\n16: BevT ← Rebuild(uk, vk)\n17: for epoch e = 1, 2, ..., Edo\n18: for each batch B ⊆ Dk do\n19: vk ← vk − ηv∇vF(BevT ; B) ▷ update private\nmodel weights locally\n20: uk ← uk − ηu∇uF(BevT ; B) ▷ update public\nmodel weights locally\n21: end for\n22: end for\n23: ∆uk ← uk − u\n24: ˜∆uk ← Compression(∆uk) ▷ optional compression\noperation in FL orchestration\n25: return ˜∆uk\nD. System Framework\nThe system overview of our federated learning framework is\nshown in Fig. 3. The framework considers both public traffic\nand private data resources as clients. Since connections in\nwireless networks for CA Vs can be unstable, we select clients\nto avoid the straggler effect during the federated learning\nprocess. On the other hand, clients from private data resources\nare typically customers of the results of federated learning,\nand their local trained models are always aggregated in all\ncommunication rounds due to the high network quality.\nTo decrease the volume of communication in each round,\nclients can implement compression techniques, such as spar-\nsification and quantization, prior to communication [54], [55].\nTo enhance the model exposure in the networks, we incorpo-\nrate a secure aggregation protocol, featuring secret sharing in\nfederated learning [56].\nOur personalized federated learning approach requires\nclients to share only a portion of the model while keeping the\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\npositional embeddings (camera and image embedding) local.\nAfter the secure aggregation, the customized BEVT is rebuilt\nin each client by concatenating the global and local model\npartitions. The concatenated model is then trained on the local\ndataset and decoupled again for further model aggregation.\nIV. D ATASET\nOur federated learning approach requires a diverse dataset\nthat comprises various vehicles equipped with different sensor\ninstallation positions and numbers, paired with BEV ground\ntruth in different cities. However, since there is no real-\nworld dataset that meets our requirements, we resort to using\nthe high-fidelity simulator CARLA [57] and the full-stack\nautonomous driving simulation framework OpenCDA [58]\nto gather the necessary data. To simulate different camera\nsensor installation positions, we employ three distinct types\nof collection vehicles: a compact car, a pickup truck, and\na bus. Each collection vehicle drives through various cities\nwith consistent weather conditions and is equipped with four\ncameras to provide a 360-degree surrounding view, similar\nto [3]. We employ post-processing techniques to control the\nnumber of cameras in each frame, ensuring that the collected\ndata meets our quality standards. However, the installation\nposes of these cameras vary significantly, depending on the\ndifferent vehicle models. In total, the car, bus, and truck\ndatasets comprise 8352, 1796, and 1800 frames, respectively,\ncontaining 52, 14, and 9 unique scenarios. Each scenario\ncontains diverse traffic situations and road types, following\na similar collection protocol in OPV2V [59], to enrich the\ncomplexities.\nIn Tab. I, we present the sensor configuration parameters\nemployed for data collection across a variety of vehicular\nclients, including cars, buses, and trucks. A qualitative illus-\ntration of the disparities in multi-camera input data from bus,\ntruck, and car clients is provided in Figure 4.\nTABLE I\nSENSOR CONFIGURATION PARAMETERS USED FOR DATA COLLECTION IN\nCAR , BUS ,TRUCK CLIENTS .\nVeh. Type Camera Height (m) Roll (deg) Pitch (deg) Yaw (deg)\nCar\nFront 1.8 0 0 0\nLeft 1.8 0 0 100\nRight 1.8 0 0 -100\nRear 1.8 0 0 180\nBus\nFront 3.2 0 -5 0\nLeft 3.2 0 -5 100\nRight 3.2 0 -5 -100\nRear 3.2 0 -5 180\nTruck\nFront 4.8 0 -5 0\nLeft 4.8 0 -5 100\nRight 4.8 0 -5 -100\nRear 4.8 0 -5 -80\nV. E XPERIMENT\nA. Experimental Setup\nUse cases. In road traffic system scenarios, the federated\nlearning clients typically include industrial companies, such\nas automotive OEMs, and individual connected vehicles in\npublic traffic. Generally, the datasets stored in these connected\nvehicles are relatively modest in size, while the quantity\nof these vehicles is substantial. In contrast, automotive cor-\nporations usually have significantly larger datasets, despite\nbeing considerably fewer in number compared to vehicles. We\nconsider these two factors in our experimental design.\nSimultaneously, each client’s data originate from their re-\nspective sensor systems, resulting in data heterogeneity among\nclients. Our experimental design takes this source of data\nvariance into account by adopting differing sensor system\nconfigurations across various types of vehicles (car, bus, and\ntruck). The specific sensor system configurations used in our\nexperiments are listed in Tab. I.\nTo comprehensively evaluate the performance of each\nmethod in potential federated learning application scenarios\nin road traffic systems, we define four use cases (UCs) as\nfollows:\n• UC 1 considers two industrial companies, training BEVT\ntogether. The local dataset in each is collected from trucks\nand buses. To further enlarge the size of training dataset,\nan open dataset is employed in federated learning and\nconsidered as a virtual client. The dataset in this virtual\nclient is stored in the server and set as the third data silo.\n• UC 2 involves four industrial companies. The aim is\nto upgrade their models using the datasets from other\nclients, but without accessing others’ raw data directly.\n• UC 3 represents a typical way of using public traffic data\ncollected in vehicular networks for training, which often\ninvolves a larger number of clients (24 clients in total\nfor federated learning). Each client owns a small amount\nof dataset collected from one or two specific driving\nscenario, e.g., a urban travel with crowded road traffic\non a sunny day.\n• UC 4 addresses federated learning for clients with dif-\nferent numbers of cameras, which can happen when\ndataset is collected from different sensor systems among\nmanufacturers.\nWe summarize the motivation and the description of the four\nUCs in Tab. II, and the distribution of data among clients in\nFig. 5.\nBEVT architecture. We begin by feeding our input images\nXi,k ∈ RLk×H×W×3 through a 3-layer ResNet34 encoder. To\nensure consistency across inputs, we use the AMCM to resize\nall inputs to have L′\nk = 4. The image features are then encoded\nat different spatial resolutions, resulting in tensors of shape\nRL′\nk×64×64×128, RL′\nk×32×32×256, RL′\nk×16×16×512. Next, we\nperform FAX cross attention-based transformer operations\nbetween the BEV embeddings in R128×128×128 (query) and\nthe encoded image features (key and value). This step yields\nBEV features in R32×32×128. To convert these BEV features\ninto our final BEV results in {0, 1}256×256, we use a decoder\nwith a 3-layer bilinear upsample module.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nFig. 4. Visualization of four-view camera data points and BEV groundtruth (GT) for car, bus and truck clients in the FedBEVT dataset.\nTABLE II\nOVERVIEW OF MOTIVATIONS AND DESCRIPTIONS FOR THE FOUR USE CASES .\nID Motivation Description\nUC 1 Companies improve the model training with data from other industrial\ncompanies and open datasets.\n2 clients with data from buses and trucks, respectively;\n1 additional virtual client with data from cars.\nUC 2 Companies improve the model training with data from each other. 2 clients with data from buses and trucks, respectively; 2 clients\nwith data from cars.\nUC 3 A model is trained on data from a number of connected vehicles, each\ncontains only data collected in one or two specific scenarios.\n3 clients with data from buses, 4 clients with data from trucks,\nand 17 clients with data from cars.\nUC 4 Companies improve the model training with data from each other, the\nnumber of sensors at each data point is different.\n3 clients with data from cars, the numbers of cameras on the\ncars from the clients are 1, 3, 4, respectively.\nFig. 5. Distribution of federated dataset for evaluation of the FedBEVT in UC 1-4. The specific camera configurations for each vehicle type are documented\nin Table I. This diversity in configuration contributes to data heterogeneity.\nHyperparameters. Our evaluation is conducted on a cluster\nwith a computer cluster with 4× NVIDIA-A100-PCIE-40GB\nGPUs and 4× 32-Core-AMD-EPYC-7513 CPUs. The envi-\nronment is a Linux system with Pytorch 1.8.1 and Cuda 11.1.\nDuring the training process, we use an initial learning rate of\n2e−5 for a warm-up phase consisting of 20 communication\nrounds where the learning rate remains constant. Afterwards,\nwe employ a Cosine Annealing learning rate scheduler [60].\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nEach client is locally trained for one epoch with a batch size\nof 4 using the AdamW optimizer [61].\nBaselines. Given the limited research on federated learning\non BEVT, we conduct the first trial of training fedAvg on this\nplatform. However, to validate the effectiveness of FedCaP,\nwe also incorporate recent research findings on federated\ntransformer learning as a baseline on BEVT. In addition to\nshowcasing the results of local training on each client, we\ncompare FedCaP with the following baselines:\n• FedAvg [36] is the original algorithm for federated learn-\ning, which aims to train a common global model for all\nclients.\n• FedRep [45] shares the data representation across clients\nand learns unique local heads for each client. Note that\nwe only allows client to personalize their image encoder\nlayers as local heads.\n• FedTP [62] uses personalized attention for each client\nwhile aggregating the other parameters among the clients.\nB. Performance\nTo compare the performance of FedCaP with other base-\nlines, we compare the Average Intersection over Union (IoU)\nachieved by trained models in the four UCs.\nUC 1. As shown in Tab. III, it is obvious that FedBEVT\nachieves an IoU improvement of over 50% compared to local\ntraining, due to its indirect utilization of other clients’ local\ntraining data. Additionally, FedCaP outperforms the basic\nFedAvg and the other two personalized federated learning\napproaches in the overall performance. Despite the fact that\nFedCaP trains a slightly more promising model to FedCaP\non the bus client, the communication rounds required are\nsignificantly higher than FedCaP.\nUC 2. Tab. IV presents the results for UC 2, where the data\nvolume for each client is more balanced, but still leading\nto outcomes similar to UC 1. In the car client A, FedRep\ntraining shows comparable results to FedCaP, while FedCaP\noutperforms other methods in the other clients.\nUC 3. We conduct a targeted comparison of the performance\nof FedCaP and FedAvg on 24 clients with only one or two\nscenario data points. Since UC 3 is aimed at scenarios with\npoor network environments, we restrict each method to only\n100 communication rounds of training. Fig. 6 illustrates that\nmore than 80% (20 out of 24) of clients achieve superior\npersonalized models with FedCaP.\nUC 4. Although AMCM allows clients with different numbers\nof cameras to train a model jointly and enriches the data\nresources, it can also result in data heterogeneity across clients.\nTherefore, if the data is sufficient for training, AMCM may\nlead to worse training results. As Tab. V shows, the client with\nmono-camera data can achieve better results without AMCM\nbecause the other two clients both have front cameras and\ncan train their model based solely on that data. However,\nwhen training a model for clients with tri-cameras, AMCM\ncan enable federated learning with more clients and improve\nthe training performance. The performance can be further\nenhanced using FedCaP, which reduces the effects of AMCM.\nFor the client with Quad-cam, data heterogeneity becomes a\nFig. 6. Comparison between FedCaP and FedAvg in UC 3, where the\nfederated learning is organized with 24 clients including buses, trucks and\ncars. Each has the local data from only one or two scenarios.\nbigger issue due to the significantly different data in the mono-\ncamera client. Nonetheless, FedCaP can alleviate such effects\nand achieve the best model among the three approaches.\nC. Effectiveness\nTo validate the personalized FedBEVT is effective in ad-\ndressing the variations across local datasets in clients, we\nconduct ablation experiments using UC 1 as a representative\nexample to train models for different clients.\nWe first train models using only local data and tested them\non various clients. In particular, we train a model using virtual\ncar client with OPV2V data. Although it achieves an IoU\nof 30.48% on car client data, its performance significantly\ndecreases on the bus and truck testsets (7.39% and 2.01%), and\nit even performs worse than models trained using local truck\ndata on the truck testset. This motivates us to use FedBEVT.\nSubsequently, we employ different personalized FedBEVT,\nnamely FedTP, FedRep, and FedCaP, to train models for\ndifferent clients and evaluate them on their own testsets and the\ntestsets of other clients. The results shown in Tab. VI, Tab. VII\nand Tab. VIII are consistent, with only locally personalized\nmodels being the most suitable for local data. Since the camera\npose and quantity of a car typically do not change once it is\nproduced, the locally collected training dataset and the data\nused for future inference have strong similarities. This aligns\nwith our experimental design. Therefore, these results further\nemphasize the significance of personalized FedBEVT for road\ntraffic perception in BEV .\nD. Visual Analysis\nIn Fig. 7, we visually show that the BEV maps generated\nby FedCaP are more accurate and holistic compared to other\nmethods in test datasets from all three vehicle types, namely\nbus, truck, and car. We present the front camera view of each\nmulti-view camera system in the first column. It is evident that\nthe camera systems vary significantly in height across different\nclients. Though a higher camera may offer a better BEV view,\nit also reduces the number of pixels occupied by each object,\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nTABLE III\nCOMPARISON BETWEEN FEDCAP AND OTHER BASELINES IN UC 1.\nMethod\nBus client Truck client Virtual car client 1\nN1 = 1388 N2 = 1448 N3 = 6372\nTrain Loss IoU ↑ Com.↓ Train Loss IoU ↑ Com.↓ Train Loss IoU ↑ Com.↓\nLocal Training 0.0716 5.42% - 0.1740 4.16% - - - -\nFedAvg [36] 0.0343 16.29% 315 0.0448 11.10% 290 0.0096 34.86% 410\nFedRep [45] 0.0176 19.40% 350 0.0303 14.20% 325 0.0073 34.37% 380\nFedTP [62] 0.0281 10.72% 195 0.0457 8.82% 260 0.0122 33.50% 390\nFedCaP (Ours) 0.0528 19.32% 240 0.04332 15.01% 295 0.0223 35.44% 350\n1 We set the virtual car client with OPV2V [59] dataset.\nTABLE IV\nCOMPARISON BETWEEN FEDCAP AND OTHER BASELINES IN UC 2.\nMethod\nBus client Truck client Car client A Car client B\nN1 = 1388 N2 = 1448 N3 = 2140 N4 = 1384\nTrain Loss IoU ↑ Com.↓ Train Loss IoU ↑ Com.↓ Train Loss IoU ↑ Com.↓ Train Loss IoU ↑ Com.↓\nLocal Training 0.0716 5.42% - 0.1740 4.16% - 0.0288 13.15% - 0.0298 9.41% -\nFedAvg [36] 0.2202 6.92% 105 0.0483 5.89% 180 0.0518 14.09% 175 0.0899 14.61% 175\nFedRep [45] 0.0170 7.45% 320 0.1319 7.03% 105 0.0352 18.94% 345 0.0394 15.41% 330\nFedTP [62] 0.0765 6.73% 175 0.1015 5.42% 140 0.0183 17.29% 335 0.0389 15.56% 300\nFedCaP (Ours) 0.0061 10.32% 340 0.1294 7.33% 65 0.0162 18.40% 360 0.0268 16.16% 370\nTABLE V\nEVALUATION OF EFFECTIVENESS OF ADAPTIVE MULTI -CAMERA\nMASKING (AMCM) IN UC 4.\nMethod\nMono-cam Tri-cam Quad-cam\nN1 = 1152 N2 = 1896 N3 = 1560\n[1]1 [1,2,3]1 [1,2,3,4]1\nFedAvg2 34.14% 15.50% 17.87%\nFedAvg + AMCM 30.17% 19.20% 16.95%\nFedCaP + AMCM 31.89% 20.89% 20.59%\n1 The numbers within the brackets represent the indices of the cameras in\nthe perception system. Specifically, the front camera is denoted by 1, the\nleft camera by 2, the right camera by 3, and the rear camera by 4.\n2 FedAvg without AMCM: The cameras with the same setup in each client\nis used for federated learning. For instance, to train a model for the client\nof Mono-cam, all data from front camera in other clients are used for\nfederated learning. One significant limitation of using FedAvg without\nAMCM is that it can only train a model for clients with a particular\ncamera system.\nTABLE VI\nAVERAGE IOU OF BEV DETECTION FOR EACH CLIENT USING\nPERSONALIZED MODELS TRAINED WITH FEDTP.\nLocal Testset\nPer. Model Bus client Truck client Car client\nBus client 10.72% 4.61% 9.12%\nTruck client 6.92% 8.82% 2.10%\nCar client 12.45% 6.76% 33.50%\nTABLE VII\nAVERAGE IOU OF BEV DETECTION FOR EACH CLIENT USING\nPERSONALIZED MODELS TRAINED WITH FEDREP.\nLocal Testset\nPer. Model Bus client Truck client Car client\nBus client 19.40% 17.73% 9.39%\nTruck client 9.59% 14.20% 4.48%\nCar client 26.04% 20.39% 34.37%\nTABLE VIII\nAVERAGE IOU OF BEV DETECTION FOR EACH CLIENT USING\nPERSONALIZED MODELS TRAINED WITH FEDCAP.\nLocal Testset\nPer. Model Bus client Truck client Car client\nBus client 19.32% 17.48% 13.71%\nTruck client 13.01% 15.01% 8.89\nCar client 26.94% 25.26% 35.44%\nthereby making object detection more challenging. However,\nthe ground truth for the BEV view remains the same – see the\nsecond column – as the height information in the z-axis is no\nlonger relevant.\nFedRep. Although FedRep achieves a relatively good ability\nof object recognition, it may miss some object vehicles. It\ntrains the initial layers of the encoder to some extent to\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nFig. 7. Visual comparison of BEV results from different federated learning approaches.\naccount for differences in image pixels. However, FedRep does\nnot explicitly consider differences in the sensor perspectives.\nWhen the data heterogeneity caused by various sensor heights\nmatters, it leads to a decrease in the object recognition ability.\nFedTP. We observe that the model trained by FedTP is more\nlikely to recognize roadside trees and buildings as object\nvehicles. FedTP directly decouples the parameters of cross-\nattention, which considers data differences, but this may cause\na deviation in the optimization direction of attention and the\noriginal head when the model is coupled again.\nFedCaP. By privatizing the embeddings related to extrinsic\nand intrinsic parameters, FedCaP straightforwardly considers\nthe differences in camera system configurations among clients.\nIt results in promising overall accuracy and ability of object\nrecognition. As shown in Fig. 7, it is the only method that\nrecognizes all objects compared to the other methods for the\ndata from buses. As for the data from trucks, it is the only\nmethod that recognizes objects on the left side of a crossroad.\nFor the data from cars, it recognizes all objects and accurately\nestimate their size.\nVI. C ONCLUSION\nThis work investigates the efficiency of federated learning in\ntraining a transformer-based model for BEV perception in road\ntraffic. Our analysis identifies two potential data heterogeneity\nissues that can impede the performance of federated learning\napproaches. To address these challenges, we propose two\nnovel techniques, i.e., FedCaP and AMCM. We evaluate the\neffectiveness of our proposed approaches by collecting a new\ndataset and distributing it to clients such that typical use cases\nin federated settings are created. Our experimental results\ndemonstrate that the proposed methods significantly enhance\nthe overall performance of federated learning by personalizing\nthe positional embeddings and increasing the data resources\navailable for training. In conclusion, our work highlights\nthe potential of using federated learning for BEV perception\nmodels and presents effective solutions to overcome challenges\nof data heterogeneity.\nVII. A CKNOWLEDGMENT\nThe project extensively utilizes the toolchains in the\nOpenCDA ecosystem [63], including the OpenCOOD [59] and\nthe OpenCDA simulation tools [58].\nREFERENCES\n[1] X. Xia et al., “An automated driving systems data acquisition and ana-\nlytics platform,” Transportation research part C: emerging technologies,\nvol. 151, pp. 104–120, 2023, DOI: 10.1016/j.trc.2023.104120.\n[2] Z. Meng, X. Xia, R. Xu, W. Liu, and J. Ma, “HYDRO-3D: Hy-\nbrid object detection and tracking for cooperative perception us-\ning 3D LiDAR,” IEEE Transactions on Intelligent Vehicles , 2023,\nDOI: 10.1109/TIV .2023.3282567.\n[3] R. Xu et al. , “CoBEVT: Cooperative bird’s eye view semantic\nsegmentation with sparse transformers,” in Conference on Robot\nLearning (CoRL) . PMLR, Dec. 2022. [Online]. Available: https:\n//proceedings.mlr.press/v205/xu23a/xu23a.pdf\n[4] Y . Ma et al., “Vision-centric BEV perception: A survey,” arXiv preprint\narXiv:2208.02797, 2022.\n[5] W. Liu et al., “A systematic survey of control techniques and applications\nin connected and automated vehicles,” IEEE Internet of Things Journal ,\n2023.\n[6] Z. Liu et al., “BEVFusion: Multi-task multi-sensor fusion with unified\nbird’s-eye view representation,” arXiv preprint arXiv:2205.13542, 2022.\n[7] W. Liu, X. Xia, L. Xiong, Y . Lu, L. Gao, and Z. Yu, “Automated vehicle\nsideslip angle estimation considering signal measurement characteristic,”\nIEEE Sensors Journal , vol. 21, no. 19, pp. 21 675–21 687, 2021,\nDOI: 10.1109/JSEN.2021.3059050.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\n[8] X. Xia, N. P. Bhatt, A. Khajepour, and E. Hashemi, “Inte-\ngrated inertial-LiDAR-based map matching localization for vary-\ning environments,” IEEE Transactions on Intelligent Vehicles , 2023,\nDOI: 10.1109/TIV .2023.3298892.\n[9] Z. Li et al. , “BEVformer: Learning bird’s-eye-view representation\nfrom multi-camera images via spatiotemporal transformers,” in Com-\nputer Vision–ECCV 2022 . Cham: Springer, Oct. 2022, pp. 1–18,\nDOI: 10.1007/978-3-031-20077-9 1.\n[10] J. Huang, G. Huang, Z. Zhu, Y . Yun, and D. Du, “BEVDet: High-\nperformance multi-camera 3D object detection in bird-eye-view,” arXiv\npreprint arXiv:2112.11790, 2021.\n[11] B. Zhou and P. Kr ¨ahenb¨uhl, “Cross-view transformers for real-time\nmap-view semantic segmentation,” in 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2022, pp. 13 750–\n13 759, DOI: 10.1109/CVPR52688.2022.01339.\n[12] Y . Liu, T. Wang, X. Zhang, and J. Sun, “PETR: Position embedding\ntransformation for multi-view 3D object detection,” in Computer Vision–\nECCV 2022 . Springer, 2022, pp. 531–548, DOI: 10.1007/978-3-031-\n19812-0 31.\n[13] A. Swerdlow, R. Xu, and B. Zhou, “Street-view image generation from\na bird’s-eye view layout,” arXiv preprint arXiv:2301.04634 , 2023.\n[14] A. Hu et al. , “FIERY: Future instance segmentation in bird’s-\neye view from surround monocular cameras,” in IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , 2021,\nURL: 10.1109/ICCV48922.2021.01499.\n[15] Y . Liu et al. , “PETRv2: A unified framework for 3D perception from\nmulti-camera images,” arXiv preprint arXiv:2206.01256 , 2022.\n[16] Y . Zhang et al. , “BEVerse: Unified perception and prediction in\nbirds-eye-view for vision-centric autonomous driving,” arXiv preprint\narXiv:2205.09743, 2022.\n[17] H. Caesar et al. , “nuScenes: A multimodal dataset for autonomous\ndriving,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2020, pp. 11 621–11 631,\nDOI: 10.1109/CVPR42600.2020.01164.\n[18] J. Philion and S. Fidler, “Lift, splat, shoot: Encoding images from\narbitrary camera rigs by implicitly unprojecting to 3D,” in Computer\nVision–ECCV 2020. Springer, 2020, pp. 194–210, DOI: 10.1007/978-\n3-030-58568-6 12.\n[19] H. Zhang, J. Bosch, and H. H. Olsson, “Real-time end-to-end federated\nlearning: An automotive case study,” in 2021 IEEE 45th Annual Com-\nputers, Software, and Applications Conference (COMPSAC) . IEEE,\n2021, pp. 459–468, DOI: 10.1109/COMPSAC51774.2021.00070.\n[20] A. M. Elbir, B. Soner, S. C ¸¨oleri, D. G ¨und¨uz, and M. Bennis, “Federated\nlearning in vehicular networks,” in 2022 IEEE International Mediter-\nranean Conference on Communications and Networking (MeditCom) .\nIEEE, 2022, pp. 72–77, DOI: 10.1109/MeditCom55741.2022.9928621.\n[21] A. Nguyen et al., “Deep federated learning for autonomous driving,” in\n2022 IEEE Intelligent Vehicles Symposium (IV) , 2022, pp. 1824–1830,\nDOI: 10.1109/IV51971.2022.9827020.\n[22] Y . Li, X. Tao, X. Zhang, J. Liu, and J. Xu, “Privacy-preserved fed-\nerated learning for autonomous driving,” IEEE Transactions on Intel-\nligent Transportation Systems , vol. 23, no. 7, pp. 8423–8434, 2022,\nDOI: 10.1109/TITS.2021.3081560.\n[23] D. Jallepalli, N. C. Ravikumar, P. V . Badarinath, S. Uchil, and M. A.\nSuresh, “Federated learning for object detection in autonomous vehi-\ncles,” in 2021 IEEE Seventh International Conference on Big Data\nComputing Service and Applications (BigDataService) , 2021, pp. 107–\n114, DOI: 10.1109/BigDataService52369.2021.00018.\n[24] K. Xie et al. , “Efficient federated learning with spike neural networks\nfor traffic sign recognition,”IEEE Transactions on Vehicular Technology,\nvol. 71, no. 9, pp. 9980–9992, 2022, DOI: 10.1109/TVT.2022.3178808.\n[25] X. Kong, K. Wang, M. Hou, X. Hao, G. Shen, X. Chen,\nand F. Xia, “A federated learning-based license plate recognition\nscheme for 5G-enabled Internet of vehicles,” IEEE Transactions\non Industrial Informatics , vol. 17, no. 12, pp. 8523–8530, 2021,\nDOI: 10.1109/TII.2021.3067324.\n[26] M. P. Aparna, R. Gandhiraj, and M. Panda, “Steering angle prediction\nfor autonomous driving using federated learning: The impact of vehicle-\nto-everything communication,” in 2021 12th International Conference\non Computing Communication and Networking Technologies (ICCCNT),\n2021, pp. 1–7, DOI: 10.1109/ICCCNT51525.2021.9580097.\n[27] X. Kong, H. Gao, G. Shen, G. Duan, and S. K. Das, “FedVCP: A\nfederated-learning-based cooperative positioning scheme for social in-\nternet of vehicles,” IEEE Transactions on Computational Social Systems,\nvol. 9, no. 1, pp. 197–206, 2022, DOI: 10.1109/TCSS.2021.3062053.\n[28] H. Zhang et al. , “Reconfigurable holographic surface aided collabo-\nrative wireless SLAM using federated learning for autonomous driv-\ning,” IEEE Transactions on Intelligent Vehicles , pp. 1–17, 2023,\nDOI: 10.1109/TIV .2023.3285592.\n[29] S. R. Pokhrel and J. Choi, “Federated learning with blockchain for\nautonomous vehicles: Analysis and design challenges,” IEEE Trans-\nactions on Communications , vol. 68, no. 8, pp. 4734–4746, 2020,\nDOI: 10.1109/TCOMM.2020.2990686.\n[30] C. Zhu, X. Zhu, J. Ren, and T. Qin, “Blockchain-enabled federated\nlearning for UA V edge computing network: Issues and solutions,”\nIEEE Access , vol. 10, pp. 56 591–56 610, 2022, DOI: 10.1109/AC-\nCESS.2022.3174865.\n[31] Y . He, K. Huang, G. Zhang, F. R. Yu, J. Chen, and J. Li, “Bift:\nA blockchain-based federated learning system for connected and au-\ntonomous vehicles,” IEEE Internet of Things Journal , vol. 9, no. 14, pp.\n12 311–12 322, 2022, DOI: 10.1109/JIOT.2021.3135342.\n[32] W. Zhang et al., “Blockchain-based federated learning for device failure\ndetection in industrial IoT,” IEEE Internet of Things Journal , vol. 8,\nno. 7, pp. 5926–5937, 2020, DOI: 10.1109/JIOT.2020.3032544.\n[33] L. Fantauzzo et al. , “FedDrive: Generalizing federated learning to\nsemantic segmentation in autonomous driving,” in 2022 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) ,\n2022, pp. 11 504–11 511, DOI: 10.1109/IROS47612.2022.9981098.\n[34] R. Song, L. Zhou, V . Lakshminarasimhan, A. Festag, and A. Knoll,\n“Federated learning framework coping with hierarchical heterogeneity\nin cooperative ITS,” in 2022 IEEE 25th International Conference\non Intelligent Transportation Systems (ITSC) , 2022, pp. 3502–3508,\nDOI: 10.1109/ITSC55140.2022.9922064.\n[35] Z. Du et al., “Federated learning for vehicular internet of things: Recent\nadvances and open issues,” IEEE Open Journal of the Computer Society,\nvol. 1, pp. 45–61, 2020, DOI: 10.1109/OJCS.2020.2992630.\n[36] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A.\ny Arcas, “Communication-efficient learning of deep networks from\ndecentralized data,” in Artificial Intelligence and Statistics . PMLR,\n2017, pp. 1273–1282. [Online]. Available: https://proceedings.mlr.press/\nv205/xu23a/xu23a.pdf\n[37] M. G. Arivazhagan, V . Aggarwal, A. K. Singh, and S. Choud-\nhary, “Federated learning with personalization layers,” arXiv preprint\narXiv:1912.00818, 2019.\n[38] F. Hanzely, B. Zhao, and M. Kolar, “Personalized federated learning:\nA unified framework and universal optimization techniques,” arXiv\npreprint arXiv:2102.09743, 2021.\n[39] K. Pillutla et al., “Federated learning with partial model personalization,”\nin International Conference on Machine Learning , vol. 162. PMLR,\nJul. 2022, pp. 17 716–17 758. [Online]. Available: https://proceedings.\nmlr.press/v162/pillutla22a/pillutla22a.pdf\n[40] A. Z. Tan, H. Yu, L. Cui, and Q. Yang, “Towards personalized federated\nlearning,” IEEE Transactions on Neural Networks and Learning Systems,\n2022, DOI: 10.1109/TNNLS.2022.3160699.\n[41] J. Zhang, S. Guo, X. Ma, H. Wang, W. Xu, and F. Wu, “Parameterized\nknowledge transfer for personalized federated learning,” Advances\nin Neural Information Processing Systems (NIPS 2021) , vol. 34, pp.\n10 092–10 104, 2021. [Online]. Available: https://proceedings.neurips.cc/\npaper/2021/hash/5383c7318a3158b9bc261d0b6996f7c2-Abstract.html\n[42] A. Shamsian, A. Navon, E. Fetaya, and G. Chechik, “Personalized\nfederated learning using hypernetworks,” in International Conference on\nMachine Learning. PMLR, 2021, pp. 9489–9502. [Online]. Available:\nhttp://proceedings.mlr.press/v139/shamsian21a/shamsian21a.pdf\n[43] P. P. Liang, T. Liu, L. Ziyin, R. Salakhutdinov, and L.-P. Morency,\n“Think locally, act globally: Federated learning with local and\nglobal representations,” NeurIPS Workshop on Federated Learning ,\nSep. 2019. [Online]. Available: https://proceedings.neurips.cc/paper\nfiles/paper/2019/file/3a0844cee4fcf57de0c71e9ad3035478-Paper.pdf\n[44] L. Yi, G. Wang, X. Liu, Z. Shi, and H. Yu, “Fedgh: Heterogeneous\nfederated learning with generalized global header,” arXiv preprint\narXiv:2303.13137, 2023.\n[45] L. Collins, H. Hassani, A. Mokhtari, and S. Shakkottai, “Exploiting\nshared representations for personalized federated learning,” in\nInternational Conference on Machine Learning . PMLR, Jul. 2021,\npp. 2089–2099. [Online]. Available: https://proceedings.mlr.press/v139/\ncollins21a.html\n[46] Y . Huang, L. Chu, Z. Zhou, L. Wang, J. Liu, J. Pei, and Y . Zhang, “Per-\nsonalized cross-silo federated learning on non-iid data,” in Proceedings\nof the AAAI Conference on Artificial Intelligence , vol. 35, no. 9, 2021,\npp. 7865–7873, DOI: 10.1609/aaai.v35i9.16960.\n[47] D. Bui et al. , “Federated user representation learning,” arXiv preprint\narXiv:1909.12535, 2019.\n[48] A. Vaswani et al. , “Attention is all you need,” Advances in Neural\nInformation Processing Systems (NIPS 2017) , vol. 30, pp. 5998–6008,\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\n2017. [Online]. Available: https://papers.nips.cc/paper files/paper/2017/\nfile/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[49] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” in International Conference on Learning\nRepresentations, 2021. [Online]. Available: https://openreview.net/\nforum?id=YicbFdNTTy\n[50] R. Xu et al. , “V2X-ViT: Vehicle-to-everything cooperative perception\nwith vision transformer,” in European Conference on Computer Vision\n(ECCV), 2022, pp. 107–124, DOI: 10.1007/978-3-031-19842-7 7.\n[51] ——, “V2V4Real: A real-world large-scale dataset for vehicle-to-vehicle\ncooperative perception,” arXiv preprint arXiv:2303.07601 , 2023.\n[52] W. Liu, K. Quijano, and M. M. Crawford, “YOLOv5-Tassel: Detect-\ning tassels in RGB UA V imagery with improved YOLOv5 based on\ntransfer learning,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 15, pp. 8085–8094, 2022,\nDOI: 10.1109/JSTARS.2022.3206399.\n[53] L. Qu et al. , “Rethinking architecture design for tackling data het-\nerogeneity in federated learning,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2022, pp.\n10 061–10 071, DOI: 10.1109/CVPR52688.2022.00982.\n[54] Y . Lin, S. Han, H. Mao, Y . Wang, and B. Dally, “Deep gradient compres-\nsion: Reducing the communication bandwidth for distributed training,”\nin International Conference on Learning Representations , 2018.\n[Online]. Available: https://openreview.net/forum?id=SkhQHMW0W\n[55] R. Song, L. Zhou, L. Lyu, A. Festag, and A. Knoll, “ResFed: Commu-\nnication efficient federated learning by transmitting deep compressed\nresiduals,” arXiv preprint arXiv:2212.05602 , 2022.\n[56] K. Bonawitz et al. , “Practical secure aggregation for privacy-\npreserving machine learning,” in ACM SIGSAC Conference on\nComputer and Communications Security , 2017, pp. 1175–1191,\nDOI: 10.1145/3133956.3133982.\n[57] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V . Koltun,\n“CARLA: An open urban driving simulator,” in Conference on\nRobot Learning . PMLR, 2017, pp. 1–16. [Online]. Available:\nhttp://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf\n[58] R. Xu et al. , “OpenCDA: An open cooperative driving automation\nframework integrated with co-simulation,” in IEEE International Intelli-\ngent Transportation Systems Conference (ITSC) , 2021, pp. 1155–1162,\nDOI: 10.1109/ITSC48978.2021.9564825.\n[59] ——, “OPV2V: An open benchmark dataset and fusion pipeline for per-\nception with vehicle-to-vehicle communication,” in 2022 International\nConference on Robotics and Automation (ICRA) . IEEE, 2022, pp.\n2583–2589, DOI: 10.1109/ICRA46639.2022.9812038.\n[60] I. Loshchilov and F. Hutter, “SGDR: Stochastic gradient descent\nwith warm restarts,” in International Conference on Learning\nRepresentations, 2017. [Online]. Available: https://openreview.net/\nforum?id=Skq89Scxx\n[61] ——, “Decoupled weight decay regularization,” in International\nConference on Learning Representations , 2019. [Online]. Available:\nhttps://openreview.net/forum?id=Bkg6RiCqY7\n[62] H. Li et al., “FedTP: Federated learning by transformer personalization,”\nIEEE Transactions on Neural Networks and Learning Systems, pp. 1–15,\n2023, DOI: 10.1109/TNNLS.2023.3269062.\n[63] R. Xu et al. , “The OpenCDA open-source ecosystem for cooperative\ndriving automation research,” IEEE Transactions on Intelligent Vehicles,\nvol. 8, no. 4, pp. 2698–2711, 2023, DOI: 10.1109/TIV .2023.3244948.\nRui Song (Member, IEEE) received his B.S. degree\nin Vehicle Engineering from Hefei University of\nTechnology (HFUT) in 2013 and M.S. degree in\nMechanical Engineering from Karlsruhe Institute\nof Technology (KIT), Germany, in 2016. He then\ngained working experience in research & devel-\nopment of the automotive industry, focusing on\ndigital validation of autonomous driving functions.\nCurrently, he is a researcher at Fraunhofer Institute\nfor Transportation and Infrastructure Systems (IVI)\nand pursues the Ph.D. degree with the Chair of\nRobotics, Artificial Intelligence and Real-Time Systems of the Technical\nUniversity of Munich (TUM). His research interests are federated learning,\nvehicular communication, cooperative perception and cooperative intelligent\ntransportation systems (C-ITS).\nRunsheng Xu received the B.S. degree in electrical\nengineering from North China Electrical Power Uni-\nversity, Beijing, China, in 2016, and the M.S. degree\nin electrical engineering from Northwestern Univer-\nsity, Evanston, IL, USA, in 2017. He is currently\nworking toward the Ph.D. degree with the UCLA\nMobility Lab, University of California, Los Ange-\nles, Los Angeles, CA, USA. His research interests\ninclude computer vision, autonomous driving per-\nception system, and cooperative driving automation.\nAndreas Festag (Senior Member, IEEE) received\nthe Ph.D. degree in electrical engineering from the\nBerlin Institute of Technology, in 2003. As a Re-\nsearcher, he worked with the Telecommunication\nNetworks Group (TKN), Berlin Institute of Tech-\nnology; Heinrich-Hertz-Institute (HHI), Berlin; NEC\nLaboratories, Heidelberg; V odafone Chair Mobile\nCommunication Systems at the Dresden University\nof Technology; and the Fraunhofer Institute for\nTransportation and Infrastructure Systems (IVI). He\nis a Professor with Technische Hochschule Ingol-\nstadt and the Center of Automotive Research on Integrated Safety Systems\nand Measurement Area (CARISSMA), the research and test center for vehicle\nsafety. His research interests include architecture, design, and performance\nevaluation of wireless and mobile communication systems and protocols, with\na focus on vehicular communication and cooperative intelligent transportation\nsystems (ITS). He actively contributes to European standardization in ITS.\nJiaqi Ma (Member, IEEE) received the Ph.D. degree\nin transportation engineering from the University of\nVirginia, Charlottesville, V A, USA, in 2014. He is\ncurrently an Associate Professor with the UCLA\nSamueli School of Engineering and a Faculty Lead\nwith the New Mobility Program, UCLA Institute\nof Transportation Studies, Los Angeles, CA, USA.\nHis research interests include intelligent transporta-\ntion systems, autonomous driving, and cooperative\ndriving automation. He is a Member of the TRB\nStanding Committee on Vehicle-Highway Automa-\ntion, TRB Standing Committee on Artificial Intelligence and Advanced\nComputing Applications, and American Society of Civil Engineers Connected\nand Autonomous Vehicles Impacts Committee, and the Co-Chair of the IEEE\nITS Society Technical Committee on Smart Mobility and Transportation 5.0.\nHe is the Editor in Chief of the IEEE OPEN JOURNAL OF INTELLIGENT\nTRANSPORTATION SYSTEMS.\nAlois Knoll (Fellow, IEEE) is a professor of com-\nputer science with the Department of Informatics\nof the Technical University Munich. His research\ninterests include robotics, artificial intelligence and\nreal-time systems. He has (co)authored more than\n600 technical papers and guest-edited international\njournals. He initiated the First IEEE/RAS Confer-\nence on Humanoid Robots and was General and\nProgram Chair of various IEEE conferences. He\nis the specialty chief editor of the Frontiers in\nNeurorobotics.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3310674\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7001497745513916
    },
    {
      "name": "Personalization",
      "score": 0.6921640634536743
    },
    {
      "name": "Perception",
      "score": 0.6886471509933472
    },
    {
      "name": "Federated learning",
      "score": 0.6079413890838623
    },
    {
      "name": "Transformer",
      "score": 0.5462380051612854
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4293856918811798
    },
    {
      "name": "Machine learning",
      "score": 0.4016497731208801
    },
    {
      "name": "Human–computer interaction",
      "score": 0.37663859128952026
    },
    {
      "name": "Engineering",
      "score": 0.15964481234550476
    },
    {
      "name": "World Wide Web",
      "score": 0.13305750489234924
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210111151",
      "name": "Fraunhofer Institute for Transportation and Infrastructure Systems",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210106192",
      "name": "Technische Hochschule Ingolstadt",
      "country": "DE"
    }
  ]
}