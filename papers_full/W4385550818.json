{
  "title": "Performance Analysis of Large Language Models for Medical Text Summarization",
  "url": "https://openalex.org/W4385550818",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2126270501",
      "name": "Jaskaran Singh",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2539376773",
      "name": "Tirth Patel",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2114211957",
      "name": "Amandeep Singh",
      "affiliations": [
        "University of Toronto"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2994033896",
    "https://openalex.org/W3207956290",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W1573040851",
    "https://openalex.org/W3187773669",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3164702673"
  ],
  "abstract": "Due to the rapid expansion of medical literature, keeping pace with the latest research and clinical guidelines has become more challenging for healthcare professionals.To overcome this challenge, effective text summarization is crucial for improving access to knowledge, enhancing clinical decision-making, and ultimately benefiting patient outcomes. In this study, a medical text summarization system that employs large language models (LLMs) was fine-tuned and evaluated with the objective of generating precise, logical, and brief summaries of medical literature, emphasizing clinical relevance and ease of understanding. We plan to evaluate the performance of GPT3, GPT4 and the fine-tuned T5, BART, and Pegasus models trained on the standard PubMed dataset using standard evaluation metrics.",
  "full_text": "Performance Analysis of Large Language Models for\nMedical Text Summarization\nAmandeep Singh\nUniversity of Toronto\namasin@cs.toronto.edu\nJaskaran Singh\nUniversity of Toronto\njaskaransb@cs.toronto.edu\nTirth Patel\nUniversity of Toronto\ntirth@cs.toronto.edu\nAbstract\nDue to the rapid expansion of medical literature, keeping pace with the latest\nresearch and clinical guidelines has become more challenging for healthcare pro-\nfessionals. To overcome this challenge, effective text summarization is crucial for\nimproving access to knowledge, enhancing clinical decision-making, and ultimately\nbenefiting patient outcomes. In this study, a medical text summarization system\nthat employs large language models (LLMs) was fine-tuned and evaluated with the\nobjective of generating precise, logical, and brief summaries of medical literature,\nemphasizing clinical relevance and ease of understanding. We plan to evaluate the\nperformance of GPT3, GPT4 and the fine-tuned T5, BART and Pegasus model\ntrained on the standard PubMed dataset using standard evaluation metrics.\n1 Introduction\nThere has been a constant increase in the growth of medical literature and clinical guidelines in the\nrecent years, which has made it difficult for healthcare practitioners and researchers to stay updated\nwith latest innovation and research in the field. Due to a lot of information, it causes delays in research\nand hinders decision making which can impact patient care. There has been a breakthrough in terms\nof advancements in Natural Language Processing (NLP) and Large Language Models (LLMs). Now,\nwe have LLMs whose architecture have been optimized specifically for summarization tasks, and\nthese include models like BART, Pegasus and T5. Although, these models are very effective in\nsummarization, still their performance in medical literature needs to be thoroughly investigated and\nalso how they compete with SoTA generative models like GPT-3 and GPT-4. The primary objective is\nto fine tune BART, Pegasus and T5 models on the PubMed dataset and then evaluate their performance\nbased on popular metrics including ROUGE, METOER, Bert Score, BLEU and SacreBLEU. The\nmodels will also be compared against GPT-3 and GPT-4 models on evaluation metrics. We aim to\nachieve a fine-tuned model which can generate concise, logical, and accurate summaries for medical\ntexts and presents comparable results with GPT models.\n2 Related Works\nIn recent years, there has been an increasing interest in summarizing medical texts, due to the\noverwhelming volume of information and the complexity of medical language. One of the earliest\napproaches to text summarization involved using rule-based methods [13]. To address the limitations\nof rule-based methods, researchers turned to machine learning techniques. A recent study used a\nNaive Bayes model to summarize legal texts [1] with a ROUGE-1 score of 0.385.\nAnother approach to text summarization involved using deep learning models, such as RNNs and\nCNNs. In comparison, for summarizing legal texts, the LSTM + Glove model is used to summarize\nand it achieved high performance, with a ROUGE-1 score of 0.436 [1]. More recently, large language\nmodels (LLMs) gave accurate results in downstream tasks of NLP. Three notable LLMs that have\nshown impressive performance in text summarization are T5, Pegasus, and BART [12, 14, 6]. T5\nwas created by a team of researchers at Google in 2019 and is designed for text-to-text transfer\nlearning[12]. Pegasus is a text summarization model also developed by researchers at Google in\n2020[14]. BART was developed by researchers at Facebook AI Research in 2020 and is a denoising\nsequence-to-sequence pre-training model for language-related tasks[6]. Many studies have compared\nthese three models, and with popular datasets like XSum, Pegasus and T5 mostly stand out as the\ntop-2 models when evaluated on the ROUGE metric [11]. In the last few years, the sucess obtained\nvia zero-hot and few-shot prompting with GPT models is phenomenal, and the summaries generated\nare overwhelmingly prefered by humans as they don’t suffer from issues like poor factuality. [3] But,\nthe zero shot prompting by text-davinci-002 when compared with finetuned models like BART and\nT5 scored less on rougeL metric. However, with release of GPT-3 and GPT-4, using LLMs shows\ngreat potential as a research topic to explore for text summarization.\n3 Methodology\nIn this empirical study, we aim to compare and analyze the performance of fine-tuned three state-\nof-the-art (SOTA) models for abstractive text summarization: BART, Pegasus, and T5 trained on\nPubMed dataset. Furthermore, we evaluate the summaries generated by GPT3 & GPT-4 with the\nabove mentioned fine-tuned models. In this section, we shall discuss the model architecture and the\nevaluation metrics used in this experimentation.\n3.1 Fine-tuning LLMs for Summarization\n3.1.1 T5\nThe T5 (Text-to-Text Transfer Transformer) architecture is a potent and adaptable natural language\nprocessing model created for numerous needs. In both the encoder and decoder blocks, it uses stacked\nmulti-head self-attention and position-wise feed-forward layers, with residual connections and layer\nnormalization applied at each step. By adding sinusoidal positional encoding to the input embeddings\nbefore processing, T5 adds positional information.[12] The self-attention mechanism, which uses\nmultiple heads of attention, enables the model to pay attention to several input sequence elements\nat once and learn various contextual relationships. T5 uses a denoising autoencoder setup with a\nmasked language modelling aim during the pre-training phase, masking input tokens at random and\ntraining the model to predict them from unmasked tokens[12]. T5 leverages the Transformer model’s\npower while incorporating innovative approaches, such as text-to-text formatting and task-specific\nprefixes, making it an effective and adaptable model for numerous natural language processing tasks.\n3.1.2 BART\nBART, also called the Bidirectional and Auto-Regressive Transformers is a model developed by AI\nteam at Facebook. It is used for various NLP Tasks like summarization, translation, and generation.\nBART[6] is based on the transformer architecture and the training steps combines the BERT’s\nbidirectional context understanding with GPT’s autoregressive generation capabilities. During\npreprocessing, first some of the tokens of input are masked / deleted to create a corrupted version\nfrom original sequence. The second step is the denoising part where BART learns to reconstruct\nthe original sentence from the corrupted version. During the reconstruction, the models is able to\nlearn the structure and semantics of the passed input. The architecture involves the encoder-decoder\nwhere encoder does the mapping for corrupted input to a latent representation, and the decoder then\ngenerates the original sentence from the representation formed by the encoder. It’s architecture\nmakes it suitable for summarization tasks and has shown lot of state-of-art results[6] on tasks like\nsummarization of CNN / Daily Mail and XSum dataset.\n3.1.3 Pegasus\nPegasus is a large language model, designed for text summarization. It is based on transformer\narchitecture and utilizes masked language modeling for generating text. Particularly the model\nfocuses on self-supervised summarization and gap sentence generation. The Pegasus large model had\nbeen trained with 16 layers, 1024 Hidden States fully connected layers having 4096 as out size[14].\nThe model size total is 568 million parameters. Some text is masked, namely mask 1, which is used\nfor target text generation for the decoder. Particularly, it is termed as gap sentence generation, in\n2\nwhich the sentences for masking are chosen based on different strategies namely random, leading,\nand based on the rouge score with respect to documents. It further uses a masked language model\napproach and masked some text for prediction by the encoder. Different models have been trained for\n12 summarization tasks.\n3.2 Evaluation Metrics\nThe standard evaluation metrics used for summarization tasks include ROUGE scores, METEOR and\nBert Scores and are used for evaluation purpose. The above mentioned metrics captures the structural\nand semantic similarity between the generated and reference summary. Other metrics also used for\nevaluation include BLEU score and SaceBLEU. The detailed discussion on these evaluation metrics\nhas been explained in the Appendix.\n4 Experimental Results\nWe have fine-tuned the T5-Base, BART-Base & Pegasus-Large model on 1000 samples from PubMed\nSummarization dataset. The hyperparameters used in training both of the models are as follows:\nmax_input_tok_length=1024, max_target_tok_length=256, min_summary_char_length=30.\n(a) ROUGE-1 Score\n (b) ROUGE-2 Score\n (c) ROUGE-L Score\n(d) Bertscore-F1 Score\n (e) METEOR Score\n (f) SacreBLEU Score\nFigure 1: Model’s Training Metrics (Performance vs Epochs)\n(a) T5-Base Model\n (b) BART-Base Model\n (c) Pegasus-Large Model\nFigure 2: Optimal Learning Rate\nGiven the substantial computing resources needed for this process, it is vital to optimise the learning\nrate while fine-tuning large language models (LLMs). Instead of depending on trial and error,\nvisualising the losses at each iteration with the learning rate offers a trajectory where the loss first\ndeclines and then starts to grow . The point just before the lowest loss value, which is seen as a valley\nin the plots, is where the most ideal learning rate may be found.\n3\nTable 1: Model performance comparison\nModel Rouge-1 Rouge-2 Rouge-LBertscore\nPrecision\nBertscore\nRecall\nBertScore\nF1 BLEU SacreBLEU METEOR\nBART 0.373 0.160 0.257 0.652 0.604 0.625 0.076 8.70 0.237\nT5 0.400 0.135 0.242 0.631 0.624 0.625 0.085 9.09 0.268\nPEGASUS 0.440 0.174 0.250 0.637 0.661 0.646 0.118 12.78 0.345\nGPT3 0.457 0.178 0.278 0.656 0.661 0.658 0.117 8.41 0.316\nGPT4 0.498 0.210 0.318 0.668 0.699 0.683 0.123 10.53 0.371\nPrompting GPT-3 & GPT-4: For GPT-3, we used the OpenAI API (text-davinci-003) to send a post\nrequest and process the output. The prompt used in experimentation is Professionally summarize\nthis medical article like a doctor with about [actual summary word count] words : [text]. The\nhyperparameter temperature is set at 0.4 with max_tokens equals 256. For GPT-4, we manually\nprompted (prompt same as GPT-3) the ChatGPT-4 and noted the generated summaries (see Appendix).\nWe evaluated all the models on 11 medical documents and the relevant metrics are reported in the\ntable 1.\n5 Discussion\nAs evident from the result1, the GPT-4 model outperforms all others under consideration in terms of\nmost of the metrics. As compared to non-GPT models, it is not fine-tuned on the data. Only for the\nstandardized version of the bleu score i.e. sacrebleu, the performance of GPT-4 is lower. It can be\nattributed to the paraphrasing nature of chatgpt-4 for which the tokens will not match between target\nand reference sentences, thus reducing Sacrebleu’s score.\nThe Pegasus large and GPT3 models have similar performance on the metrics. Only the meteor score\nis relatively higher for Pegasus i.e 0.345. The reason behind that is the meteor prefers long contiguous\nmatches with references[9]. So definitely such a metric will prefer fine-tuned model on a dataset.\nThis aspect is further bolstered by the bert scores which are higher for GPT-3 as the metric utilizes\ncontextual embeddings does do similarity matching by the context than token matching[15]. Size\nwise the Pegasus has 568 million parameters and GPT 3 has 175 billion parameters. So evidently a\nlarge zero-shot learner has been able to produce results competitive enough with a fine-tuned model.\nT5 base and Bart base have 220 million and 140 million parameters, which are significantly lower\nthan the other models under consideration. They have similar performances on rouge, bleu, and\nmeteor scores with each other. But with significantly less computation cost they have produced\nresults marginally poor than substantially large models. For instance, T5 and BART f1 scores are\n0.625 which is bordering 0.646 for Pegasus large and 0.658 for GPT-3.\nThe training curves1 of the three models under consideration for different metrics plateaus after a\ncertain number of epochs. The BART curve is oscillatory in nature initially, which can be attributed\nto the nature of its loss curve, which is explained later. The T5 had the highest metrics initially but\nPegasaus surpassed it, after fine-tuning for a number of epochs. The In addition to the learning curve,\nthe T5 loss decreases with increasing learning rate and stays stable. For Pegasus, there is a limited\nincrease in the loss by increasing the learning rate after 10−2. Whereas it increases significantly for\nBART after increasing the rate beyond the 10−3. It suggests that the loss function of the BART has\nhigh curvature[2].\n6 Conclusion\nFrom the result, it is observed that GPT-4 outperforms the summarization models in their niche\ntasks and even without fine-tuning. Medical summarization is very technical in nature and zero-shot\nprediction providing competitive results reaffirms belief in the generalized nature of their learning[4].\nThe trends are similar to GPT-4 performance in medical competitive tests of UCMLE, thus again\nhighlighting the complex data analysis capabilities of such LLMS[8]. As far as the comparative study\nof this work is considered It has been proven that very large models which are zero-shot learners\ngive comparable or better performance than the fine-tuned models. Further, it has been observed that\nsmaller models are marginally poor than the comparatively very large models. So these can point to\nfuture work, which can analyze the computational cost, and accuracy analysis based on the number\nof parameters. Even further focus on comparing the models on these attributes by fine-tuning them\nwith larger data sets. And thus inspecting the marginal utility of spending extra computational cost.\n4\nReferences\n[1] Deepa Anand and Rupali Wagh. Effective deep learning approaches for summarization of legal\ntexts. Journal of King Saud University - Computer and Information Sciences, 34(5):2141–2150,\n2022.\n[2] Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David\nCardoze, George Dahl, Zachary Nado, and Orhan Firat. A loss curvature perspective on training\ninstability in deep learning, 2021.\n[3] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the era\nof gpt-3. 2022.\n[4] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners, 2023.\n[5] Alon Lavie and Abhaya Agarwal. Meteor: An automatic metric for mt evaluation with high\nlevels of correlation with human judgments. pages 228–231, 07 2007.\n[6] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, and comprehension. pages 7871–7880,\nJuly 2020.\n[7] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational\nLinguistics.\n[8] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-\nbilities of gpt-4 on medical challenge problems, 2023.\n[9] Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui. Jointly modeling embedding\nand translation to bridge video and language. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 4594–4602, 2016.\n[10] Matt Post. A call for clarity in reporting bleu scores, 2018.\n[11] Diyah Puspitaningrum. A survey of recent abstract summarization techniques, 2021.\n[12] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. arXiv, 2019.\n[13] Mengqian Wang, Manhua Wang, Fei Yu, Yue Yang, Jennifer Walker, and Javed Mostafa. A\nsystematic review of automatic text summarization for biomedical literature and ehrs. Journal\nof the American Medical Informatics Association, 28, 08 2021.\n[14] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with\nextracted gap-sentences for abstractive summarization. arXiv, 2019.\n[15] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:\nEvaluating text generation with bert, 2020.\n7 Contributions\nName Contributions\nAmandeep Singh Fine-tuning & Inferencing of Pegasus Model, Assembling Codebase, Report Writing\nJaskaran Singh Fine-tuning & Inferencing of BART Model, Prompting & Evaluating GPT-3, Report Writing\nTirth Patel Fine-tuning & Inferencing of T5 Model, Prompting & Evaluating GPT-4, Report Writing\n5\n8 Appendix\n8.1 Evaluation Metrics\n8.1.1 ROUGE Score\nA popular metric in natural language processing called ROUGE is frequently used to assess the\neffectiveness of text summarization software[7]. By considering the overlap of n-grams, ROUGE-N is\nhelpful in identifying the level of similarity between the candidate summary and reference summaries.\nThe three types of ROUGE scores we used are ROUGE-1, ROUGE-2, and ROUGE-L.\nROUGE-N: Depending on the n-gram we wanted to compare, we can compute the ROUGE-1,\nROUGE-2 etc. This metric is a combination of precision and recall, which are used to determine the\noverlap of n-grams between the generated summary and the reference summary. This metric tries to\ncapture the word-level similarity. ROUGE-N is calculated as:\nROUGE-N =\nP\nSϵReferenceSummaries\nP\ngramnϵS Countmatch(gramn)P\nSϵReferenceSummaries\nP\ngramnϵS Countmatch(gramn)\nROUGE-L: Instead of focusing on N-grams like ROUGE-1/ROUGE-2, ROUGE-L uses the Longest\nCommon Subsequence (LCS) between the generated summary and the reference summary and tries\nto capture the overall structure and word order within the sentences generated, making it a more\ncomprehensive evaluation metrics.\n8.1.2 BLEU\nBLEU stands for Bilingual Evaluation Understudy. It is a metric used to evaluate the quality of\ntranslation by a Machine Translation System. The BLEU scores a translation on a scale of 0 to 1\nwhere a score of 1 shows perfect translation. BLEU score can also be used for summarization tasks\nwhere we can use the summary as machine-generated text and the reference sentence is the original\nsummary. Although, the BLEU score is not designed for summarization tasks, as it does not consider\nsemantic similarity, and won’t capture the nuances of paraphrasing and rephrasing. Hence, it only\nconsiders the overlap of similar words or word sequences, but still, it is a good metric to understand\nhow similar the summary is when compared to the original summary. The formula for the BLEU\nscore is given as follows:\nBleuc = BP ∗ e\nPn\n1 wn∗log(pn)\nBLEUc represents the BLEU for candidate c, which is a translation produced by a machine. BP is\nthe brevity penalty and is a factor that accounts for the difference in the length of reference sentences\nand the candidate sentence.\nBP = 1 if brevityi < 1\ne1−brevityi if brevityi ≥ 1\nbrevityi = ri/ci , where ci is the length of ith candidate and ri is the nearest length among the\nreferences. wi is the weight assigned to each n-gram and usually has a value equal to 1/n. pi is the\nprecision of i-grams of the candidate sentence with respect to the reference sentences. The precision\npi is given by the formula as\npi = counti\nmax(1, totali)\nWhere counti is the count of i-grams in candidate sentences that appear in the reference sentences.\ntotali is the total count of i-grams in C.\n8.1.3 METEOR\nMETEOR (Metric for Evaluation of Translation with Explicit Ordering) [ 5] is a metric used to\nevaluate quality for machine translations, summarizations, and other generative tasks. It works upon\nthe limitations of the BLEU Score metric, where BLEU and ROUGE, as it also incorporates the\nlinguistic features including word reordering, paraphrasing, and synonyms. The METEOR score is\n6\ngiven by the harmonic mean of precision and the recall score with a reordering penalty is used to\nconsider the differences in word order.\nMET EOR= (1− P enalty) ∗ Fmean\nWhere Fmean is the harmonium mean of unigram precision and recall.\nFmean = P ∗ R\n(α ∗ P + (1− α) ∗ R)\nα here controls the importance between precision and recall. Penalty and it is given as\nP = γ ∗ (Chunkinessβ)\nChunkiness here represents the ratio of total contiguous matches of unigrams to total unigram matches.\nγ and α are parameters that control the penalty strength.\n8.1.4 SacreBLEU\nSacreBLEU is a more consistent and standard implementation of BLEU metric which improves\non various aspects like tokenization and data preparation. It works as a more standard approach\nto calculate the BLEU score[ 10], which makes sure that the results tasks are comparable across\ndifferent tasks and systems. The metric calculation involves a consistent tokenization algorithm, in\nwhich things are punctuations and whitespaces are handled similarly and the preprocessing includes\nhandling of lowercase and Unicode characters. Along with consistent techniques, it also ensures\nbetter reproducibility. This also has support for widely used test datasets like WMT, and IWSLT.\nWMT (Workshop on Machine Translation) and IWSLT (International Workshop on Spoken Language\nTranslation) are two popular datasets used to evaluate the performance of machine translation systems.\nand makes it easier for researchers to evaluate their models and easily compare them with existing\ntechniques. As SacreBLEU is an implementation of BLEU only, the mathematical formula is the\nsame, the key difference is in the standardization of implementation. The value ranges from 0 to\n100. Its range is different from BLEU because the BLEU score obtained is multiplied by 100 to\nexpress it in the form of a percentage. It considers a lot of factors when comparing the generated\ntext, which includes things like stem and synonym matching, alignment, and recording along with\nthe exact word matching.\n8.1.5 BertScore\nBertScore[15] is an evaluation metric that uses pre-trained contextual embedding. It matches\nthe candidate and reference sentence, using cosine similarity. Given a reference sentence x=\n<x1, x2..., xn > and reference sentence. It is similar to similarity measures using with a difference\nthat contextual embedding vectors are used and the vectors are normalized. Based on these vectors\nrecall, precision and F1 scores are defined. x^ =< x^1, x^\n2, ...x^\nn>\nRBERT = 1\n|x|\nX\nxiϵx\nmax\nx^jϵx^\nxT\ni x^j\nPBERT = 1\n|x^|\nX\nx^jϵx^\nmax\nxiϵx\nxT\ni x^j\nFBERT = 2∗ PBERT ∗ RBERT\nPBERT + RBERT\nFurther, it has been found that rare words give more information on sentence similarity. So idf with re-\nspect to sentences is defined. Given M reference sentences,{x(i)}M\n1 , the idf score of a word is given as\nid f(w) =−log 1\nM\nMX\n1\n||[wϵxi]\nwhere || is the indicator function and recall considering this idf is given as\nRBERT =\nP\nxiϵx id f(xi) max\nx^jϵx^\nxT\ni x^j\nP\nxiϵx id f(xi)\n7\n8.2 GPT Prompts\nFigure 3: GPT-3 Prompt & API\nFigure 4: GPT-4 Prompt\nFigure 5: GPT-4 Summarization\nGitHub Link: https://github.com/tiru-patel/Medical_Text_Summarization_using_LLMs\n8",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9625421762466431
    },
    {
      "name": "Pace",
      "score": 0.7195666432380676
    },
    {
      "name": "Relevance (law)",
      "score": 0.7129796147346497
    },
    {
      "name": "Computer science",
      "score": 0.6573184728622437
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.4590519666671753
    },
    {
      "name": "Medical literature",
      "score": 0.4201364815235138
    },
    {
      "name": "Data science",
      "score": 0.3768322765827179
    },
    {
      "name": "Natural language processing",
      "score": 0.3003915250301361
    },
    {
      "name": "Medicine",
      "score": 0.19187703728675842
    },
    {
      "name": "Pathology",
      "score": 0.07666146755218506
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ],
  "cited_by": 1
}