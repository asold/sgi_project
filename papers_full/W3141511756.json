{
  "title": "DA-DETR: Domain Adaptive Detection Transformer by Hybrid Attention",
  "url": "https://openalex.org/W3141511756",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2096209862",
      "name": "Jingyi Zhang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2103474193",
      "name": "Jia-xing Huang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2184138644",
      "name": "Zhi-peng Luo",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2559725607",
      "name": "Gongjie Zhang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2113179205",
      "name": "Shijian Lu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3034937575",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2037227137",
    "https://openalex.org/W3139180514",
    "https://openalex.org/W3119579108",
    "https://openalex.org/W2945164022",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3109679703",
    "https://openalex.org/W2963107255",
    "https://openalex.org/W2962687275",
    "https://openalex.org/W3034779842",
    "https://openalex.org/W3098218837",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2989604896",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W2968634921",
    "https://openalex.org/W3035673985",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2798593490",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W2979548969",
    "https://openalex.org/W3110486195",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W2748021867",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2962823940",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2990740643",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W2964115968",
    "https://openalex.org/W3102057471",
    "https://openalex.org/W2989236540",
    "https://openalex.org/W2984776865",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2968762770",
    "https://openalex.org/W3134953589",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3134841676",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2969583814",
    "https://openalex.org/W3035175896"
  ],
  "abstract": "The prevalent approach in domain adaptive object detection adopts a two-stage architecture (Faster R-CNN) that involves a number of hyper-parameters and hand-crafted designs such as anchors, region pooling, non-maximum suppression, etc. Such architecture makes it very complicated while adopting certain existing domain adaptation methods with different ways of feature alignment. In this work, we adopt a one-stage detector and design DA-DETR, a simple yet effective domain adaptive object detection network that performs inter-domain alignment with a single discriminator. DA-DETR introduces a hybrid attention module that explicitly pinpoints the hard-aligned features for simple yet effective alignment across domains. It greatly simplifies traditional domain adaptation pipelines by eliminating sophisticated routines that involve multiple adversarial learning frameworks with different types of features. Despite its simplicity, extensive experiments show that DA-DETR demonstrates superior accuracy as compared with highly-optimized state-of-the-art approaches.",
  "full_text": "DA-DETR: Domain Adaptive Detection Transformer with Information Fusion\nJingyi Zhang1,* Jiaxing Huang1,∗ Zhipeng Luo1,2 Gongjie Zhang1,3 Xiaoqin Zhang4 Shijian Lu1,†\n1 S-lab, Nanyang Technological University 2 SenseTime Research\n3 Black Sesame Technologies 4 Wenzhou University\nAbstract\nThe recent detection transformer (DETR) simplifies the\nobject detection pipeline by removing hand-crafted designs\nand hyperparameters as employed in conventional two-\nstage object detectors. However, how to leverage the sim-\nple yet effective DETR architecture in domain adaptive ob-\nject detection is largely neglected. Inspired by the unique\nDETR attention mechanisms, we design DA-DETR, a do-\nmain adaptive object detection transformer that introduces\ninformation fusion for effective transfer from a labeled\nsource domain to an unlabeled target domain. DA-DETR\nintroduces a novel CNN-Transformer Blender (CTBlender)\nthat fuses the CNN features and Transformer features inge-\nniously for effective feature alignment and knowledge trans-\nfer across domains. Specifically, CTBlender employs the\nTransformer features to modulate the CNN features across\nmultiple scales where the high-level semantic information\nand the low-level spatial information are fused for accu-\nrate object identification and localization. Extensive experi-\nments show that DA-DETR achieves superior detection per-\nformance consistently across multiple widely adopted do-\nmain adaptation benchmarks.\n1. Introduction\nObject detection aims to predict a bounding box and\na class label for interested objects in images and it has\nbeen a longstanding challenge in the computer vision re-\nsearch. Most existing work adopts a two-stage detection\npipeline that involves heuristic anchor designs, complicated\npost-processing such as non-maximum suppression (NMS),\netc. The recent detection transformer (DETR) [5] has at-\ntracted increasing attention which greatly simplifies the\ntwo-stage detection pipeline by removing hand-crafted an-\nchors [21, 22, 49] and NMS [21, 22, 49]. Despite its great\ndetection performance under a fully supervised setup, how\nto leverage the simple yet effective DETR architecture in\ndomain adaptive object detection is largely neglected.\n*Equal contribution, {jingyi.zhang, jiaxing.huang}@ntu.edu.sg.\n†Corresponding author, shijian.lu@ntu.edu.sg.\nFigure 1. The vanilla Deformable-DETR [81] trained with la-\nbeled source data cannot handle target data well due to cross-\ndomain shift. The introduction of adversarial feature alignment\nin Deformable-DETR + Direct-align [19] improves the detection\nclearly. The proposed DA-DETR fuses CNN features and trans-\nformer features ingeniously which achieves superior unsupervised\ndomain adaptation consistently across four widely adopted bench-\nmarks including Cityscapes → Foggy cityscapes in (a), SIM 10k\n→ Cityscapes in (b), KITTI → Cityscapes in (c) and PASCAL\nVOC → Clipart1k in (d).\nDifferent from the conventional CNN-based detection\narchitectures such as Faster RCNN [49], DETR has a CNN\nbackbone followed by a transformer head consisting of an\nencoder-decoder structure. The CNN backbone and the\ntransformer head learn different types of features [17,48,69]\n- the former largely captures low-level localization features\n(e.g., edges and lines around object boundaries) while the\nlatter largely captures global inter-pixel relationship and\nhigh-level semantic features. At the other end, many prior\nstudies show that fusing different types of features often\nis often helpful in various visual recognition tasks [9, 11].\nHence, it is very meaningful to investigate how to fuse the\ntwo types of DETR features to address the domain adaptive\nobject detection challenge effectively.\nWe design DA-DETR, a simple yet effective Domain\nAdaptive DETR that introduces information fusion into\nthe DETR architecture for effective domain adaptive ob-\nject detection. The core design is a CNN-Transformer\nBlender (CTBlender) that employs the high-level semantic\narXiv:2103.17084v2  [cs.CV]  22 Mar 2023\nfeatures in the Transformer head to conditionally modulate\nthe low-level localization features in the CNN backbone.\nCTBlender consists of two sequential fusion components,\nincluding split-merge fusion (SMF) that fuses CNN and\nTransformer features within an image and scale aggregation\nfusion (SAF) that fuses the SMF features across multiple\nfeature scales. Different from the existing weight-and-sum\nfusion [9, 11], SMF first splits CNN features into multiple\ngroups with different semantic information as captured by\nthe Transformer head and then merges them with channel\nshuffling for effective information communication among\ndifferent groups. The SMF features of each scale are then\naggregated by SAF for fusing both semantic and localiza-\ntion information across multiple feature scales. Hence, CT-\nBlender captures both semantic and localization features in-\ngeniously which enables comprehensive and effective inter-\ndomain feature alignment with a single discriminator.\nThe main contributions of this work can be summarized\nin three aspects. First, we propose DA-DETR, a simple yet\neffective domain adaptive detection transformer that intro-\nduces information fusion for effective domain adaptive ob-\nject detection. To the best of our knowledge, this is the first\nwork that explores information fusion for domain adaptive\nobject detection. Second, we design a CNN-Transformer\nBlender that fuses the CNN features and Transformer fea-\ntures ingeniously for effective feature alignment and knowl-\nedge transfer across domains. Third, extensive experi-\nments show that DA-DETR achieves superior object detec-\ntion over multiple widely studied domain adaptation bench-\nmarks as compared with the state-of-the-art as shown in\nFig. 1.\n2. Related Work\nTransformers [56] have achieved great success in various\nneural language processing (NLP) tasks [3, 14, 40, 46, 47]\ndue to their computational efficiency and scalability. In-\nspired by the success of transformers in NLP, several\nstudies [5, 8, 12, 15, 16, 62, 70, 71, 77, 78, 81] attempt to\nadapt transformers to computer vision tasks. For example,\nViT [16] adopts transformer for image classification, which\nsplits each image into patches and treats them as input to-\nkens to transformers. SETR [78] extends ViT to seman-\ntic segmentation by introducing multiple decoders designed\nfor pixel-wise segmentation. For object detection, differ-\nent from CNN-based detector, DETR [5] treats detection\nas a set prediction task [50], which eliminates the depen-\ndence on various heuristic and hand-crafted designs such\nas anchor generation, ROI pooling and non-maximum sup-\npression. Existing vision transformers achieve very promis-\ning performance in supervised learning. However, how to\nadapt and generalize them to unsupervised domain adapta-\ntion tasks has been largely neglected. In this work, we in-\nvestigate domain adaptive detection transformers in an un-\nsupervised manner.\nUnsupervised Domain Adaptation (UDA) has been stud-\nied extensively in recent years, largely for alleviating data\nannotation constraint in deep network training in various vi-\nsual recognition tasks [7,19,23,28,30–32,41,42,44,52,58,\n63,66,73,74,82,83]. For object detection, the target of UDA\nis to mitigate the domain gap between a source domain and\na target domain, so that the source data can be employed to\ntrain better detectors for target data. Most existing domain\nadaptive detectors [6, 7, 36, 51, 55, 57, 64, 76, 79, 80] adopt\nCNN-based detector (i.e., Faster R-CNN) and achieve UDA\nvia adversarial learning [7, 26, 36, 51, 55, 64, 79], image\ntranslation [1, 1, 29, 33, 35, 38, 54, 67] and self-training [30,\n67]. However, little research [59, 68] is conducted on\nhow to adopt DETR in domain adaptive detection tasks,\ne.g., SFA [59] tackles the domain adaptive object detection\nvia query-based feature alignment and token-wise feature\nalignment. Differently, we introduce the information fusion\nidea into the DETR architecture for effective domain adap-\ntive object detection.\nFeature Fusion is often helpful in various visual recogni-\ntion tasks. For example, ResNet [25] fuses features of dif-\nferent network layers with skip connections which achieves\nobvious performance gains along with increased network\ndepth. Feature Pyramid Network (FPN) [39] fuses features\nof different scales to build high-level semantic feature maps,\nand it usually improves the object detection with clear mar-\ngins in various detection tasks. Some work [9, 11] instead\nachieves feature fusion with channel-wise attention [27,45]\nand spatial-wise attention [4, 37, 60, 61]. For example,\nAFF [11] presents a multi-scale channel attention module\nfor better fusing features from different layers and scales\nthat capture different types of semantics. LS-DeconvNet [9]\nintroduces a gated fusion layer to fuse RGB and depth fea-\ntures effectively. We explore information fusion for do-\nmain adaptive detection transformer, and design a CNN-\nTransformer Blender for fusing CNN features and Trans-\nformer features for effective domain adaptive object detec-\ntion. Different from [9, 11] that adopts a weight-and-sum\nstrategy, our CNN-Transformer Blender employs the Trans-\nformer features to modulate the CNN features for more ef-\nfective adversarial feature alignment.\n3. Preliminaries of Detection Transformer\nDETR [5] consists of a CNN backbone [25] to extract\nfeatures, an encoder-decoder transformer and a simple feed\nforward network (FFN) to make final detection prediction.\nGiven an image x, the CNN backbone G first generates fea-\nture f and then reshapes f to a vector. The encoder-decoder\nin DETR follows the standard architecture of the trans-\nformer [56], which consists of multiple multi-head self-\nFigure 2. Overview of the proposed DA-DETR: the proposed DA-DETR consists of a base detector (including a backbone G and a\ntransformer encoder-decoder), a discriminator and a CNN-Transformer Blender (CTBlender). Given an input image from either source or\ntarget domain, the backbone G first produces multi-scale CNN features fl (l = 1, 2, 3, 4) and then feeds them to the transformer encoder\nto obtain Transformer features pl (l = 1, 2, 3, 4). For supervised learning, the Transformer features generated by the source images are\nfurther fed to the decoder to compute supervised detection loss Ldet with the corresponding ground truth. For unsupervised learning,\nCTBlender takes fl and pl as inputs for feature fusion. Finally, the output of CTBlender is fed to the discriminator for computing an\nadversarial loss Ladv which drives adversarial alignment of source and target features.\nattention modules that are defined by:\nMSA (zq, f) =\nHX\nh=1\nPH[\nX\nk\nSAhqk · PH\n′fk], (1)\nwhere MSA (·) consists of H single attention heads, zq and\nfk denotes representation features of query element and key\nelement, PH ∈ Rd×dh and PH\n′ ∈ Rd×dh are learnable\nprojection weights ( dh = d/H, where d is the dimension\nof f). Each self-attention weight SAhqk is a type of scaled\ndot-product attention, which maps a query and a set of key-\nvalue pairs into an output:\nSAhqk ∝ exp (zT\nq UT\nmVmfc\n√dh\n), (2)\nwhere Um, Vm ∈ Rdh×d are also learnable weights.\nWe adopt Deformable-DETR [81] as the base detector.\nDifferent from the conventional DETR [5], Deformable-\nDETR replaces the normal attention with the deformable\nattention which improves the convergence speed greatly:\nDeformableMSA (zq, pq, f)\n=\nHX\nh=1\nPH[\nX\nk\nSAhqk · PH\n′f(pq + δphqk)],\n(3)\nwhere δphqk and SAhqk denote the sampling offset and at-\ntention weight of the k-th sampling point in the m-th at-\ntention head, respectively. Such sampling design signifi-\ncantly mitigates the slow convergence and high complex-\nity issues of DETR [5]. In addition, Deformable-DETR\nis extended to aggregating multi-scale features as shown in\nFig. 2. The multi-scale feature maps fl (l = 1, 2, 3, 4) are\nextracted from the output of Block C3-C5 in the ResNet\nbackbone [25]. More specifically, f1, f2 and f3 are ex-\ntracted from the output feature maps of Block C3-C4 via a\n1×1 convolution. The lowest resolution feature map, i.e.,\nf4, is extracted by 3×3 stride 2 convolution on the output\nfeature maps of Block C5. Such multi-scale design enables\nattention to capture relationships among different-scale fea-\ntures effectively.\n4. Method\n4.1. Task Definition\nThe work focuses on the problem of unsupervised do-\nmain adaptation (UDA) in object detection. It involves a\nsource domain Ds and a target domain Dt, where Ds =\b\u0000\nxi\ns, yi\ns\n\u0001\tNs\ni=1 is fully labeled, and yi\ns represents the labels\nof the sample image xi\ns. The goal is to train a detection\ntransformer that well performs on unlabeled target-domain\ndata xi\nt. The baseline model is trained with the labeled\nsource data (i.e., Ds) only:\nLdet = l(T(G(xs)), ys), (4)\nwhere G denotes backbone, T denotes transformer encoder-\ndecoder and l (·) denotes the supervised detection loss that\nconsists of a matching cost and a Hungarian loss [5, 81] for\nobject category and object box predictions.\n4.2. Framework Overview\nAs shown in Fig. 2, the proposed DA-DETR con-\nsists of a base detector (including a backbone G and a\ntransformer encoder-decoder T), a discriminator Cd and\na CNN-Transformer Blender (CTBlender). We adopt the\ndeformable-DETR [81] as the base detector, where G ex-\ntracts features from the input images and T predicts a set\nof bounding boxes and pre-defined semantic categories ac-\ncording to the extracted features. CTBlender consists of\ntwo sub-modules including a split-merge fusion (SMF) and\na scale aggregation fusion (SAF) as in Fig. 3. Taking the\nFigure 3. Overview of the proposed CNN-Transformer Blender (CTBlender). CTBlender consists of split-merge fusion (SMF) and scale\naggregation fusion (SAF) as illustrated. In SMF, Transformer features of all four scales are adopted to modulate the CNN features. Take\nthe first level f1 and p1 as an example. The Transformer feature p1 and the CNN feature f1 are divided into K groups (e.g., K = 2),\nand further fed to SMF to perform spatial-wise fusion and channel-wise fusion, respectively. The fused group features are then merged\nto generate the final fused features ˆf1 for the first scale. In SAF, ˆfl (l = 1, 2, 3, 4) are aggregated with different scale-wise weights to\ngenerate the final feature V a.\nCNN features from G and the Transformer features from\nthe encoder E as inputs, CTBlender fuses the positional and\nsemantic information in Transformer features and the local-\nization information in CNN features for comprehensive and\neffective feature alignment across domains.\nGiven an input image from either source or target do-\nmain, the backbone G will first produce multi-scale features\nfl (l = 1, 2, 3, 4) and then feeds them to the transformer en-\ncoder to obtain Transformer features pl (l = 1, 2, 3, 4). For\nsupervised learning, the Transformer features generated by\nsource images xs ∈ Ds are further fed to decoder to pre-\ndict a set of bounding boxes and pre-defined semantic cate-\ngories, which will be used to calculate a detection loss Ldet\nunder the supervision of the corresponding ground-truth la-\nbel ys ∈ Ds. For unsupervised learning, CTBlender takes\nfl (l = 1, 2, 3, 4) and pl (l = 1, 2, 3, 4) generated by both\nsource and target images ( i.e., xs ∈ Ds and xt ∈ Dt) as\ninputs. Finally, the output of CTBlender is fed to the dis-\ncriminator Cd to compute an adversarial lossLadv for inter-\ndomain feature alignment. The overall network is optimized\nby the adversarial loss Ladv and the detection loss Ldet.\n4.3. CNN-Transformer Blender\nOne key component in DA-DETR is a CNN-Transformer\nBlender (CTBlender) that fuses different features for effec-\ntive domain alignment. CTBlender takes multi-scale CNN\nfeatures and the corresponding multi-scale Transformer fea-\ntures as the input, where the semantic and positional infor-\nmation in the Transformer features are fused with the lo-\ncalization information in the CNN features via split-merge\nfusion (SMF). The SMF-fused features are then aggregated\nacross multiple scales via scale aggregation fusion (SAF).\nSplit-Merge Fusion. In SMF, the rich semantic and posi-\ntional information in the the multi-scale Transformer fea-\ntures p =\n\b\npl\tL\nl=1 are exploited to fuse with multi-scale\nCNN features f =\n\b\nfl\tL\nl=1 for adversarial feature align-\nment across domains. As SMF operations at each feature\nscale are the same, we take the first scale l = 1as an exam-\nple to illustrate how we perform split-merge fusion.\nInspired by the split-fuse-merge in [72], SMF first splits\nCNN features into multiple groups and then fuses them\nwith the Transformer features. After that, the fused fea-\ntures are merged with channel shuffling for effective infor-\nmation communication among different groups. Given a\nTransformer feature p1 ∈ RC×H×W and a backbone CNN\nfeature f1 ∈ RC×H×W (C, H, W indicate the number of\nchannel of feature map, and the height and the width of fea-\nture map, respectively),p1 is first split intoK groups evenly\nalong channels, i.e.,\n\b\np1\nk\n\tK\nk=1 ∈ R(C/K)×H×W , where\neach group captures different semantic information of the\ninput image. The fusion in each group is then achieved via\nspatial-wise fusion and channel-wise fusion, respectively.\nFor the spatial-wise fusion, the split Transformer fea-\ntures are firstly fed into a normalization layer and then re-\nweighted by a learnable weight map and a learnable bias\nmap:\nˆp1\nks = fs\n\u0000\nws · GN\n\u0000\np1\nk\n\u0001\n+ bs\n\u0001\n, (5)\nwhere fs(·) is an activation function that limits the input in\nthe range of [0, 1].\nFor the channel-wise fusion, the split Transformer fea-\nture is firstly compacted by the Global Average Pooling\n(GAP) and then re-weighted by a learnable weight vector\nand a learnable bias vector:\nˆp1\nkc = fs\n\u0000\nwc · GAP(p1\nk) +bc\n\u0001\n, (6)\nwhere fs(·) is an activation function that limits the input to\nthe range of [0, 1].\nSimilar to the operation for Transformer feature p1, the\nCNN feature f1 is also divided into K groups along the\nchannels, i.e.,\n\b\nf1\nk\n\tK\nk=1 ∈ R(C/K)×H×W .\nWe further adopt shuffle operation to enable information\ncommunication across channels [43, 75]. Specifically, we\nfirst re-weight the split CNN feature by the corresponding\nre-weighted Transformer feature (i.e., ˆp1\nks and ˆp1\nkc) to gen-\nerate re-weighted split CNN feature ˆf1\nk .\nThen we shuffle ˆf1\nk along channels to enable informa-\ntion flow across channels for better feature fusion. Lastly,\nwe conduct the above operations K times to generate\nK shuffled features for each group, i.e.,\nn\nˆf1\nk\noK\nk=1\n∈\nR(C/K)×H×W . The shuffled features are concatenated to\nobtain the fused feature map ˆf1 ∈ RC×H×W :\nˆf1 = fc\n\u0010\nˆf1\n1 , ...,ˆf1\nk , ...,ˆf1\nK\n\u0011\n, (7)\nwhere similar operations are conducted to get the results of\nall levels ˆf =\nn\nˆfl\noL\nl=1\n.\nScale Aggregation Fusion. To explicitly perform feature\nfusion of different scales, we design a scale aggregation\nfusion (SAF) to aggregate features ˆf with different scale\nweights as illustrated in the bottom part of Fig. 3.\nSpecifically, we compact each scale of feature ˆf =n\nˆfl\noL\nl=1\ninto a channel-wise vector u =\n\b\nul\tL\nl=1 ∈\nRC×1×1 via a Global Average Pooling (GAP) layer. The\nscale weights αl are obtained from channel-wise vectorsul.\nFirstly, the channel-wise vectors are merged together to ob-\ntain merged vector um by an element-wise addition.\nThen, a fully connected layer separates um to L scale-\nweight vectors αl ∈ RC×1×1. Finally, V a is obtained by\nDirect-align +SMF +SAF mAP\nShuffling Splitting\n28.5\n✓ 38.4\n✓ ✓ 41.4\n✓ ✓ 41.8\n✓ ✓ ✓ 42.3\n✓ ✓ 41.7\n✓ ✓ ✓ ✓ 43.5\nTable 1. Ablation study of DA-DETR over domain adaptation task\nCityscapes → Foggy Cityscapes.\nV a =\nLX\nl=1\nˆfl · αl, (8)\nwhere V a is a highly embedded feature that captures rich\nsemantic information and localization information.\n4.4. Network Training\nThe network is trained with two losses,i.e., a supervised\nobject detection loss Ldet as defined in Eq. 4 and an adver-\nsarial alignment loss Ladv that is defined as follow:\nLadv = E(f,p)∈Ds log Cd (H(f, p)))\n+E(f,p)∈Dt log (1− Cd (H(f, p))) , (9)\nwhere f = G (x) and p = E (G (x)). G denotes back-\nbone; E denotes transformer encoder; H denotes CNN-\nTransformer Blender (CTBlender) and Cd denotes the dis-\ncriminator. Both source images xs and target images xt are\nutilized to compute adversarial loss.\nIn summary, the overall optimization objective of DA-\nDETR is formulated by\nmax\nCd\nmin\nG,T,H\nLdet(G, T) − λLadv(H, Cd), (10)\nwhere T denotes the transformer in DETR, λ is the weight\nfactor that balances the influences ofLdet and Ladv in train-\ning. Note that we adopt a gradient reverse layer (GRL) [19]\nto enable the gradient of Ladv to be reversed before back-\npropagating to H from Cd.\n5. Experiments\nThis section presents experimentation including experi-\nment setups, implementation details, ablation studies, com-\nparisons with the state-of-the-art and discussion. More de-\ntails are to be described in the ensuing subsections.\n5.1. Experiment Setups\nDatasets. Following [7, 26, 33, 35, 51, 64], we evalu-\nate DA-DETR under four widely adopted domain adap-\ntation scenarios with eight datasets as listed: 1) Nor-\nmal Weather to Foggy Weather (Cityscapes [10] → Foggy\nCityscapes → Foggy cityscapes\nMethod Backbone person rider car truck bus train mcycle bicycle mAP\nDeformable-DETR [81] ResNet-50 37.7 39.1 44.2 17.2 26.8 5.8 21.6 35.5 28.5\nDAF [7] ResNet-50 48.2 48.8 61.5 22.6 43.1 20.2 30.3 42.1 39.6\nSWDA [51] ResNet-50 49.0 49.0 61.4 23.9 43.1 22.9 31.0 45.2 40.7\nSCL [55] ResNet-50 49.4 48.6 61.2 27.2 41.1 34.8 28.5 42.5 41.7\nGPA [65] ResNet-50 49.5 46.7 58.6 26.4 42.2 32.3 29.1 41.8 40.8\nCRDA [64] ResNet-50 49.8 48.4 61.9 22.3 40.7 30.0 29.9 45.4 41.1\nCF [79] ResNet-50 49.6 49.7 62.6 23.3 43.4 27.4 30.2 44.8 41.4\nSAP [36] ResNet-50 49.3 49.9 62.5 23.0 44.1 29.4 31.3 45.8 41.9\nSFA [59] ResNet-50 46.5 48.6 62.6 25.1 46.2 29.4 28.3 44.0 41.3\nMTTrans [68] ResNet-50 47.7 49.9 65.2 25.8 45.9 33.8 32.6 46.5 43.4\nDA-DETR ResNet-50 49.9 50.0 63.1 24.0 45.8 37.5 31.6 46.3 43.5\nTable 2. Experimental results (%) of the scenario Normal weather to Foggy weather: Cityscapes → Foggy Cityscapes.\nSIM 10k → Cityscapes\nMethod Backbone mAP on Car\nDeformable-DETR [81] ResNet-50 47.4\nDAF [7] ResNet-50 49.8\nSWDA [51] ResNet-50 50.5\nSCL [55] ResNet-50 51.6\nGPA [65] ResNet-50 51.3\nCRDA [64] ResNet-50 52.1\nCF [79] ResNet-50 52.5\nSAP [36] ResNet-50 52.3\nSFA [59] ResNet-50 52.6\nMTTrans [68] ResNet-50 57.9\nDA-DETR ResNet-50 54.7\nTable 3. Experimental results (%) of the scenario Synthetic scene\nto Real scene: SIM 10k → Cityscapes.\nKITTI → Cityscapes\nMethod Backbone mAP on Car\nDeformable-DETR [81] ResNet-50 39.5\nDAF [7] ResNet-50 43.6\nSWDA [51] ResNet-50 44.3\nSCL [55] ResNet-50 44.5\nGPA [65] ResNet-50 43.2\nCRDA [64] ResNet-50 44.8\nCF [79] ResNet-50 45.2\nSAP [36] ResNet-50 46.5\nSFA [59] ResNet-50 46.7\nDA-DETR ResNet-50 48.9\nTable 4. Experimental results (%) of the scenario Cross-camera\nAdaptation: KITTI → Cityscapes.\nCityscapes [53]); 2) Synthetic Scene to Real Scene (SIM\n10k [34] → Cityscapes [10]); 3) Cross-camera Adaptation\n(KITTI [20] → Cityscapes [10]) and 4) Real-world Images\nto Artistic Images (PASCAL VOC [18] → Clipart1k, Wa-\ntercolor2k, Comic2k [33]).\n5.2. Implementation Details\nIn all experiments, we adopt deformable-DETR [81] as\nthe base detector. Since there is only few prior study [59]\non transformer-based domain adaptive detection, we mod-\nify existing object detectors using Faster R-CNN [7, 24, 36,\n51, 64, 79] to the transformer-based domain adaptive detec-\ntion for fair comparisons. The modification is accomplished\nby keeping domain adaptation modules unchanged but re-\nplacing post-processing modules in Faster R-CNN (e.g., re-\ngion proposal network, proposal classification module, etc.)\nby the encoder-decoder module of deformable DETR. In\naddition, we adopt ResNet-50 [25] (pre-trained on Ima-\ngeNet [13]) as backbone, and use SGD optimizer [2] with a\nmomentum 0.9 and a weight decay1e−4 in all experiments\nwith deformable-DETR.\nIn all experiments, the weight factor λ in Eq. 10 is fixed\nat 0.1 and the number of split groups K in SMF is fixed\nat 32. All the experiments are implemented in Pytorch.\nFor evaluation metrics, we report average precision (AP)\nfor each object category and mean average precision (mAP)\nof all object categories with a threshold of intersection over\nunion (IoU) at 0.5 as in [7, 51, 64].\n5.3. Ablation Studies\nThe proposed CTBlender consists of split-merge fusion\n(SMF) and scale aggregation fusion (SAF). We first study\nthe two fusion modules to examine how they contribute to\nthe overall unsupervised domain adaptive detection perfor-\nmance. Table 1 shows experimental results over the valida-\ntion data of Foggy Cityscapes under the adaptation scenario\n‘normal weather to foggy weather’.\nAs Table 1 shows, the Baseline [81] trained using the\nlabeled source data only does not perform well due to do-\nmain shifts. The model Direct-align aligns CNN features\ndirectly via adversarial learning which improves the Base-\nline from 28.5% to 38.4% in mAP. The proposed SMF is\nevaluated under three settings including with Splitting op-\nPASCAL VOC→Clipart1k\nMethod aero bcyc. bird boat bott. bus car cat chair cow table dog horse bike pers. plant sheep sofa train tv mAP\nDeformable-DETR [81]24.8 50.5 14.0 22.8 11.5 50.7 28.7 3.0 26.5 32.6 22.1 17.4 19.6 73.1 54.2 20.8 11.5 12.6 55.2 30.3 29.1DAF [7] 33.5 39.6 24.9 31.4 19.0 61.8 34.5 11.0 29.2 28.5 22.6 20.9 26.5 61.4 51.6 26.7 8.3 23.1 59.7 39.5 32.7SWDA [51] 38.6 53.0 29.4 39.5 25.2 64.8 36.9 21.4 37.9 39.5 30.7 28.7 31.4 73.7 63.4 33.5 15.8 29.2 61.3 41.2 39.8SCL [55] 32.3 46.8 31.9 36.0 36.8 43.6 40.9 24.4 35.1 37.8 18.1 34.9 32.6 67.3 64.5 43.2 14.5 30.4 53.5 43.6 38.4GAP [65] 28.9 42.4 32.4 36.8 36.5 40.8 39.1 23.2 34.6 39.1 16.6 33.1 36.4 65.2 66.0 40.1 14.3 30.6 56.4 39.5 37.6SFA [59] 35.2 47.6 33.5 38.3 39.6 40.4 38.527.2 37.6 43.1 23.9 31.6 32.5 72.5 66.8 43.0 18.5 29.0 53.0 44.9 39.8\nDA-DETR 43.1 47.7 31.5 33.7 21.4 62.8 42.6 14.8 39.5 44.2 35.9 27.5 31.8 72.6 65.6 42.2 17.3 31.1 71.3 50.1 41.3\nTable 5. Experimental results (%) of the scenario Real-world images to Clipart-style images: PASCAL VOC → Clipart1k.\nPASCAL VOC→Watercolor2k\nMethod bike bird car cat dog person mAP\nDeformable-DETR [81]43.3 39.9 21.0 50.3 13.7 49.1 36.2\nDAF [7] 58.0 41.7 30.2 32.7 34.5 66.9 44.0\nSWDA [51] 58.7 53.7 25.3 40.2 32.8 70.2 46.8\nUaDAN [24] 57.2 47.8 31.0 37.8 34.9 70.3 48.2\nDA-DETR 58.6 53.7 31.9 46.2 40.2 73.0 50.6\nPASCAL VOC→Comic2k\nMethod bike bird car cat dog person mAP\nDeformable-DETR [81]22.3 13.6 19.6 16.6 18.9 33.1 20.3\nDAF [7] 27.8 17.5 28.7 24.5 20.8 45.5 27.5\nSWDA [51] 36.6 12.8 29.5 16.5 33.2 61.7 31.7\nUaDAN [24] 37.3 17.3 25.3 28.5 29.0 61.9 33.2\nDA-DETR 44.2 18.1 25.0 27.7 33.0 62.4 35.1\nTable 6. Experimental results (%) of the scenarios Real-world im-\nages to Watercolor-style images: PASCAL VOC→ Watercolor2k\nand Real-world images to Comic-style images: PASCAL VOC→\nComic2k.\neration, with Shuffling operation and with both. It can be\nobserved that SMF under all three settings outperforms the\nDirect-align consistently, while the SMF with bothSplitting\nand Shuffling performs the best. In addition, including SAF\nalone over the Direct-align improves mAP by 3.3%. The\nincorporation of SMF and SAF achieves the best mAP at\n43.5%, demonstrating that SMF and SAF are complemen-\ntary to each other.\n5.4. Comparisons with the State-of-the-Art\nWe evaluate DA-DETR under four domain-shift scenar-\nios: 1) Normal weather to Foggy weather; 2) Synthetic\nscene to Real scene; 3) Cross-camera Adaptations and 4)\nReal-world images to Artistic images. In each domain-shift\nscenario, we compare DA-DETR with a number of state-of-\nthe-art unsupervised domain adaptive methods.\nNormal Weather to Foggy Weather: We first study the\nadaptation from normal weather to foggy weather on the\ntask Cityscapes → Foggy cityscapes. As Table 2 shows,\nDA-DETR outperforms the baseline deformable DETR [81]\ngreatly. It also outperforms the state-of-the-art [59] by 2.2%\nin mAP. For certain categories such as ‘train’ that can not\nbe well detected by existing methods, DA-DETR achieves\nthe best AP of 37.5. Such experimental results verify that\nthe proposed CTBlender helps to identify both the semantic\ninformation and the localization information effectively.\nSynthetic Scene to Real Scene: Table 3 shows experi-\nments of adaptation from synthetic to real scenes on the task\nSIM 10k → Cityscapes. We can observe that DA-DETR\nachieves the best accuracy with a mAP 54.7%, showing that\nDA-DETR is powerful when there is only one object cate-\ngory ‘car’ in cross-domain detection task.\nCross-camera Adaptation: Table 4 shows experiments of\ncross-camera adaptation over the task KITTI→ Cityscapes.\nWe can observe that DA-DETR outperforms the state-of-\nthe-art and improves the baseline model [81] from 39.5%\nto 48.9% in mAP. These experiments further show that the\nproposed DA-DETR can well generalize to different do-\nmain adaptation tasks.\nReal-world Images to Artistic Images: We evaluate the\nadaptation from real-world images to clipart-style images\non the task PASCAL VOC → Clipart1k. Table 5 shows ex-\nperimental results, where DA-DETR achieves the best mAP\nof 41.3%. In addition, DA-DETR improves the baseline\nby large margins for certain categories that are not well de-\ntected by the baseline model [81] such as bird and sofa. This\nexperiment shows that DA-DETR can handle domain adap-\ntation with multiple categories effectively.\nTo demonstrate the generalization capability of DA-\nDETR, we also evaluate it over the tasks PASCAL VOC\n→ Watercolor2k and PASCAL VOC → Comic2k, respec-\ntively. As Table 6 shows, DA-DETR outperforms the base-\nline [81] by large margins, and it also outperforms all state-\nof-the-art methods over two tasks consistently.\n5.5. Discussion\nEffectiveness of Split Fusion in CTBlender. As described\nin Section 4, the split operation in SMF splits input fea-\nture into K groups which helps to encode different semantic\ninformation into the fused feature. We examine the effec-\ntiveness of the split operation over domain adaptation task\nCityscapes → Foggy cityscapes by visualizing the weight\ngenerated by each group. We sampled 4 groups from the to-\ntal 32 groups for each image and highlighted the produced\nweight over the sample images as shown in Fig. 4. We\ncan observe that SMF captures different foreground regions\nover different groups, demonstrating that the split operation\nOriginal image k=8 k=16 k=24 k=32\nFigure 4. Visualization of generated weight in SMF for each group. We take two sample images from validation set of Cityscapes, which\nare shown in the first column. For each image, we sample 4 groups from the total 32 groups and highlight the generated attention over\neach sample image as shown in columns 2 to 5, respectively. We can observe that the generated weight in different groups detect different\nforeground regions effectively. k denotes the kth group defined in Section 4.\nCityscapes → Foggy Cityscapes\nMethod Aligned Features mAP\nDeformable-DETR [81] N.A. 28.5\nDirect-Align [19] CNN features 38.4\nDirect-Align [19] Transformer features 38.9\nDirect-Align [19] 40.2\nAddition [25] 42.1\nMultiplication [61] CNN features\nand\nTransformer features\n41.9\nConvolution [72] 41.8\nAFF [11] 42.4\nLS-DeconvNet [9] 42.6\nCTBlender(ours) 43.5\nTable 7. Comparing the proposed CTBlender with conventional\nfusion mechanisms in cross-domain alignment (on the domain\nadaptive object detection task Cityscapes → Foggy Cityscapes).\nhelps to learn different semantic features effectively.\nAnalysis of CNN Features and Transformer Features.\nWe study how CNN features and Transformer features\naffect unsupervised domain adaptation by examining the\nadaptation performance of the direct alignment of CNN\nfeatures f, the direct alignment of Transformer features p\nand both, respectively. As shown in Rows 1-4 of Table 7,\naligning CNN features and Transformer features simultane-\nously brings clear further performance improvement over\neither CNN feature alignment or Transformer feature align-\nment. This shows that either the localization information in\nCNN features or the semantic information in Transformer\nfeatures can facilitate domain adaptation in some degree\nwhile these two types of information are complementary for\ncross-domain alignment.\nComparison with Conventional Fusion Mechanisms.We\nstudy how different feature fusion strategies affect the do-\nmain adaptation performance by comparing our CTBlender\nwith existing feature fusion strategies [9,11,25,61,72], e.g.,\nfusing CNN features and Transformer features via 1) ad-\ndition [25], multiplication [61] and convolution [72], and\n2) attention-based fusion [9, 11]. As shown in the bottom\npart of Table 7, all fusion strategies improve the Direct-\nAlign baseline clearly, demonstrating the effectiveness of\naligning the fused features in UDA. In addition, we can ob-\nserve that our CTBlender performs the best clearly, largely\nattributed to its split-merge fusion and scale aggregation fu-\nsion designs that fuses CNN features and Transformer fea-\ntures ingeniously. Specifically, the split-merge fusion in CT-\nBlender splits the CNN/Transformer feature which enables\nto fuse them along spatial and channel dimensions respec-\ntively, leading to comprehensive information fusion along\nboth feature dimensions. Besides, the scale aggregation fu-\nsion in CTBlender aggregates rich information across multi-\nple image scales, leading to effective cross-domain feature\nalignment for different scales that facilitates cross-domain\ndetection against large scale variance.\n6. Conclusion\nThis paper presents DA-DETR, an unsupervised domain\nadaptive detection transformer that introduces information\nfusion into the DETR framework for effective knowledge\ntransfer from a labeled source domain to an unlabeled tar-\nget domain. We design a novel CNN-Transformer Blender\nthat fuses the CNN features and Transformer features inge-\nniously for effective feature alignment and domain adapta-\ntion across domains. Extensive experiments over multiple\ndomain adaptation scenarios show that DA-DETR achieves\nsuperior performance in unsupervised domain adaptive ob-\nject detection. Moving forwards, we plan to continue to in-\nvestigate innovative cross-domain alignment strategies for\nbetter domain adaptive transformer detection.\nAcknowledgement. This study is supported under the\nRIE2020 Industry Alignment Fund – Industry Collabora-\ntion Projects (IAF-ICP) Funding Initiative, as well as cash\nand in-kind contribution from the industry partner(s).\nReferences\n[1] Vinicius F Arruda, Thiago M Paix ˜ao, Rodrigo F Berriel, Al-\nberto F De Souza, Claudine Badue, Nicu Sebe, and Thi-\nago Oliveira-Santos. Cross-domain car detection using un-\nsupervised image-to-image translation: From day to night.\nIn 2019 International Joint Conference on Neural Networks\n(IJCNN), pages 1–8. IEEE, 2019. 2\n[2] L ´eon Bottou. Large-scale machine learning with stochastic\ngradient descent. InProceedings of COMPSTAT’2010, pages\n177–186. Springer, 2010. 6\n[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 2\n[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision Workshops, pages\n0–0, 2019. 2\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020. 1,\n2, 3\n[6] Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, and\nQi Dou. Harmonizing transferability and discriminability for\nadapting object detectors. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 8869–8878, 2020. 2\n[7] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and\nLuc Van Gool. Domain adaptive faster r-cnn for object de-\ntection in the wild. InProceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3339–3348,\n2018. 2, 5, 6, 7\n[8] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\npixel classification is not all you need for semantic segmenta-\ntion. In Thirty-Fifth Conference on Neural Information Pro-\ncessing Systems, 2021. 2\n[9] Yanhua Cheng, Rui Cai, Zhiwei Li, Xin Zhao, and Kaiqi\nHuang. Locality-sensitive deconvolution networks with\ngated fusion for rgb-d indoor semantic segmentation. InPro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 3029–3037, 2017. 1, 2, 8\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 3213–3223, 2016. 5, 6\n[11] Yimian Dai, Fabian Gieseke, Stefan Oehmcke, Yiquan Wu,\nand Kobus Barnard. Attentional feature fusion. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 3560–3569, 2021. 1, 2, 8\n[12] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.\nUp-detr: Unsupervised pre-training for object detection with\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 1601–\n1610, 2021. 2\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 6\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2\n[15] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and\nYichen Wei. Solq: Segmenting objects by learning queries.\narXiv preprint arXiv:2106.02351, 2021. 2\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2\n[17] St ´ephane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S\nMorcos, Giulio Biroli, and Levent Sagun. Convit: Improv-\ning vision transformers with soft convolutional inductive bi-\nases. In International Conference on Machine Learning ,\npages 2286–2296. PMLR, 2021. 1\n[18] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-\npher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes challenge: A retrospective.Inter-\nnational journal of computer vision, 111(1):98–136, 2015. 6\n[19] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International conference\non machine learning, pages 1180–1189. PMLR, 2015. 1, 2,\n5, 8\n[20] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. The Inter-\nnational Journal of Robotics Research , 32(11):1231–1237,\n2013. 6\n[21] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440–1448,\n2015. 1\n[22] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n580–587, 2014. 1\n[23] Dayan Guan, Jiaxing Huang, Shijian Lu, and Aoran\nXiao. Scale variance minimization for unsupervised do-\nmain adaptation in image segmentation. Pattern Recogni-\ntion, 112:107764, 2021. 2\n[24] Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu, and\nYanpeng Cao. Uncertainty-aware unsupervised domain\nadaptation in object detection. IEEE Transactions on Mul-\ntimedia, 2021. 6, 7\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2, 3, 6, 8\n[26] Zhenwei He and Lei Zhang. Multi-adversarial faster-rcnn\nfor unrestricted object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 6668–6677, 2019. 2, 5\n[27] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132–7141, 2018. 2\n[28] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu.\nCross-view regularization for domain adaptive panoptic seg-\nmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10133–\n10144, 2021. 2\n[29] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu.\nFsdr: Frequency space domain randomization for domain\ngeneralization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6891–\n6902, 2021. 2\n[30] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu.\nModel adaptation: Historical contrastive learning for unsu-\npervised domain adaptation without source data. Advances\nin Neural Information Processing Systems , 34:3635–3649,\n2021. 2\n[31] Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu, and\nLing Shao. Category contrast for unsupervised domain adap-\ntation in visual tasks. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1203–1214, 2022. 2\n[32] Jiaxing Huang, Shijian Lu, Dayan Guan, and Xiaobing\nZhang. Contextual-relation consistent domain adaptation for\nsemantic segmentation. In Computer Vision–ECCV 2020:\n16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XV, pages 705–722. Springer, 2020.\n2\n[33] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiy-\noharu Aizawa. Cross-domain weakly-supervised object de-\ntection through progressive domain adaptation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 5001–5009, 2018. 2, 5, 6\n[34] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta,\nSharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.\nDriving in the matrix: Can virtual worlds replace human-\ngenerated annotations for real world tasks? arXiv preprint\narXiv:1610.01983, 2016. 6\n[35] Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon\nChoi, and Changick Kim. Diversify and match: A domain\nadaptive representation learning paradigm for object detec-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 12456–12465, 2019.\n2, 5\n[36] Congcong Li, Dawei Du, Libo Zhang, Longyin Wen, Tiejian\nLuo, Yanjun Wu, and Pengfei Zhu. Spatial attention pyramid\nnetwork for unsupervised domain adaptation. In European\nConference on Computer Vision , pages 481–497. Springer,\n2020. 2, 6\n[37] Xiang Li, Xiaolin Hu, and Jian Yang. Spatial group-wise en-\nhance: Improving semantic feature learning in convolutional\nnetworks. arXiv preprint arXiv:1905.09646, 2019. 2\n[38] Che-Tsung Lin. Cross domain adaptation for on-road ob-\nject detection using multimodal structure-consistent image-\nto-image translation. In 2019 IEEE International Conference\non Image Processing (ICIP), pages 3029–3030. IEEE, 2019.\n2\n[39] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyra-\nmid networks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117–2125, 2017. 2\n[40] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019. 2\n[41] Zhipeng Luo, Zhongang Cai, Changqing Zhou, Gongjie\nZhang, Haiyu Zhao, Shuai Yi, Shijian Lu, Hongsheng\nLi, Shanghang Zhang, and Ziwei Liu. Unsupervised do-\nmain adaptive 3d detection with multi-level consistency. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 8866–8875, 2021. 2\n[42] Zhipeng Luo, Xiaobing Zhang, Shijian Lu, and Shuai Yi.\nDomain consistency regularization for unsupervised multi-\nsource domain adaptive classification. Pattern Recognition,\n132:108955, 2022. 2\n[43] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShufflenet v2: Practical guidelines for efficient cnn architec-\nture design. In Proceedings of the European conference on\ncomputer vision (ECCV), pages 116–131, 2018. 5\n[44] Pedro O Pinheiro. Unsupervised domain adaptation with\nsimilarity learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 8004–\n8013, 2018. 2\n[45] Wang Qilong, Wu Banggu, Zhu Pengfei, Li Peihua, Zuo\nWangmeng, and Hu Qinghua. Eca-net: Efficient channel at-\ntention for deep convolutional neural networks. 2020. 2\n[46] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 2\n[47] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 2\n[48] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-\nformers see like convolutional neural networks? Advances\nin Neural Information Processing Systems, 34:12116–12128,\n2021. 1\n[49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. arXiv preprint arXiv:1506.01497, 2015.\n1\n[50] S Hamid Rezatofighi, Vijay Kumar BG, Anton Milan, Ehsan\nAbbasnejad, Anthony Dick, and Ian Reid. Deepsetnet: Pre-\ndicting sets with deep neural networks. In 2017 IEEE In-\nternational Conference on Computer Vision (ICCV) , pages\n5257–5266. IEEE, 2017. 2\n[51] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate\nSaenko. Strong-weak distribution alignment for adaptive\nobject detection. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 6956–\n6965, 2019. 2, 5, 6, 7\n[52] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat-\nsuya Harada. Maximum classifier discrepancy for unsuper-\nvised domain adaptation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n3723–3732, 2018. 2\n[53] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Seman-\ntic foggy scene understanding with synthetic data. Interna-\ntional Journal of Computer Vision , 126(9):973–992, 2018.\n6\n[54] Yuhu Shan, Wen Feng Lu, and Chee Meng Chew. Pixel and\nfeature level based domain adaptation for object detection in\nautonomous driving. Neurocomputing, 367:31–38, 2019. 2\n[55] Zhiqiang Shen, Harsh Maheshwari, Weichen Yao, and Mar-\nios Savvides. Scl: Towards accurate domain adaptive object\ndetection via gradient detach based stacked complementary\nlosses. arXiv preprint arXiv:1911.02559, 2019. 2, 6, 7\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 2\n[57] Vibashan VS, Vikram Gupta, Poojan Oza, Vishwanath A\nSindagi, and Vishal M Patel. Mega-cda: Memory guided\nattention for category-aware unsupervised domain adaptive\nobject detection. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n4516–4526, 2021. 2\n[58] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu\nCord, and Patrick P ´erez. Advent: Adversarial entropy min-\nimization for domain adaptation in semantic segmentation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2517–2526, 2019. 2\n[59] Wen Wang, Yang Cao, Jing Zhang, Fengxiang He, Zheng-\nJun Zha, Yonggang Wen, and Dacheng Tao. Exploring\nsequence feature alignment for domain adaptive detection\ntransformers. In Proceedings of the 29th ACM International\nConference on Multimedia, pages 1730–1738, 2021. 2, 6, 7\n[60] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018. 2\n[61] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\nKweon. Cbam: Convolutional block attention module. In\nProceedings of the European conference on computer vision\n(ECCV), pages 3–19, 2018. 2, 8\n[62] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. Segformer: Simple and ef-\nficient design for semantic segmentation with transformers.\narXiv preprint arXiv:2105.15203, 2021. 2\n[63] Yun Xing, Dayan Guan, Jiaxing Huang, and Shijian Lu. Do-\nmain adaptive video segmentation via temporal pseudo su-\npervision. In Computer Vision–ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23–27, 2022, Proceed-\nings, Part XXX, pages 621–639. Springer, 2022. 2\n[64] Chang-Dong Xu, Xing-Ran Zhao, Xin Jin, and Xiu-Shen\nWei. Exploring categorical regularization for domain adap-\ntive object detection. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11724–11733, 2020. 2, 5, 6\n[65] Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wen-\njun Zhang. Cross-domain detection via graph-induced proto-\ntype alignment. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12355–\n12364, 2020. 6, 7\n[66] Yanchao Yang and Stefano Soatto. Fda: Fourier domain\nadaptation for semantic segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4085–4095, 2020. 2\n[67] Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Kari-\nanakis, Pei Yu, Dimitrios Lymberopoulos, and Xiang Chen.\nUnsupervised domain adaptation for object detection via\ncross-domain semi-supervised learning. arXiv preprint\narXiv:1911.07158, 2019. 2\n[68] Jinze Yu, Jiaming Liu, Xiaobao Wei, Haoyi Zhou, Yohei\nNakata, Denis Gudovskiy, Tomoyuki Okuno, Jianxin Li,\nKurt Keutzer, and Shanghang Zhang. Mttrans: Cross-\ndomain object detection with mean teacher transformer. In\nComputer Vision–ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23–27, 2022, Proceedings, Part IX,\npages 629–645. Springer, 2022. 2, 6\n[69] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 558–567,\n2021. 1\n[70] Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu, and\nEric P Xing. Meta-detr: Image-level few-shot detection with\ninter-class correlation exploitation. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2022. 2\n[71] Gongjie Zhang, Zhipeng Luo, Yingchen Yu, Zichen Tian,\nJingyi Zhang, and Shijian Lu. Towards efficient use of multi-\nscale features in transformer-based object detectors. arXiv\npreprint arXiv:2208.11356, 2022. 2\n[72] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,\nHaibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller,\nR Manmatha, et al. Resnest: Split-attention networks. arXiv\npreprint arXiv:2004.08955, 2020. 4, 8\n[73] Jingyi Zhang, Jiaxing Huang, Zichen Tian, and Shijian Lu.\nSpectral unsupervised domain adaptation for visual recogni-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 9829–9840,\n2022. 2\n[74] Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang,\nand Fang Wen. Prototypical pseudo label denoising and tar-\nget structure learning for domain adaptive semantic segmen-\ntation. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 12414–12424,\n2021. 2\n[75] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShufflenet: An extremely efficient convolutional neural net-\nwork for mobile devices. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n6848–6856, 2018. 5\n[76] Yixin Zhang, Zilei Wang, and Yushi Mao. Rpn prototype\nalignment for domain adaptive object detector. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12425–12434, 2021. 2\n[77] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and\nVladlen Koltun. Point transformer. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 16259–16268, 2021. 2\n[78] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic seg-\nmentation from a sequence-to-sequence perspective with\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6881–\n6890, 2021. 2\n[79] Yangtao Zheng, Di Huang, Songtao Liu, and Yunhong Wang.\nCross-domain object detection through coarse-to-fine feature\nadaptation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13766–\n13775, 2020. 2, 6\n[80] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and\nDahua Lin. Adapting object detectors via selective cross-\ndomain alignment. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n687–696, 2019. 2\n[81] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 1, 2, 3, 6, 7, 8\n[82] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang.\nUnsupervised domain adaptation for semantic segmentation\nvia class-balanced self-training. In Proceedings of the Eu-\nropean conference on computer vision (ECCV) , pages 289–\n305, 2018. 2\n[83] Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jin-\nsong Wang. Confidence regularized self-training. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 5982–5991, 2019. 2",
  "concepts": [
    {
      "name": "Discriminator",
      "score": 0.79612797498703
    },
    {
      "name": "Computer science",
      "score": 0.7956774234771729
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5307761430740356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5218133926391602
    },
    {
      "name": "Pooling",
      "score": 0.48337024450302124
    },
    {
      "name": "Object detection",
      "score": 0.4691901206970215
    },
    {
      "name": "Machine learning",
      "score": 0.4439474046230316
    },
    {
      "name": "Domain adaptation",
      "score": 0.43998560309410095
    },
    {
      "name": "Data mining",
      "score": 0.40502604842185974
    },
    {
      "name": "Detector",
      "score": 0.30980807542800903
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.29080379009246826
    },
    {
      "name": "Classifier (UML)",
      "score": 0.08130753040313721
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "topic": "Discriminator",
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ]
}