{
  "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection",
  "url": "https://openalex.org/W3175189620",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102210614",
      "name": "Zhou Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125992512",
      "name": "Kang Le",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2373136998",
      "name": "Cheng Zhi-yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097056482",
      "name": "He, Bo",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Xin, Jingyu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2984287396",
    "https://openalex.org/W3112016895",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035413240",
    "https://openalex.org/W3035084815",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3108842862",
    "https://openalex.org/W3016787936",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3010426832",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2980037812",
    "https://openalex.org/W2609286783",
    "https://openalex.org/W2942642798",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W3013092127",
    "https://openalex.org/W3035717782",
    "https://openalex.org/W3034822180",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2797664359"
  ],
  "abstract": "With rapidly evolving internet technologies and emerging tools, sports related videos generated online are increasing at an unprecedentedly fast pace. To automate sports video editing/highlight generation process, a key task is to precisely recognize and locate the events in the long untrimmed videos. In this tech report, we present a two-stage paradigm to detect what and when events happen in soccer broadcast videos. Specifically, we fine-tune multiple action recognition models on soccer data to extract high-level semantic features, and design a transformer based temporal detection module to locate the target events. This approach achieved the state-of-the-art performance in both two tasks, i.e., action spotting and replay grounding, in the SoccerNet-v2 Challenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features are released at https://github.com/baidu-research/vidpress-sports. By sharing these features with the broader community, we hope to accelerate the research into soccer video understanding.",
  "full_text": "Feature Combination Meets Attention: Baidu Soccer Embeddings and\nTransformer based Temporal Detection\nXin Zhou* Le Kang* Zhiyu Cheng* Bo He Jingyu Xin\nBaidu Research\n1195 Bordeaux Dr, Sunnyvale, CA 94089 USA\n{zhouxin16, kangle01, zhiyucheng, v hebo02, xinjingyu}@baidu.com\nAbstract\nWith rapidly evolving internet technologies and emerging tools, sports related videos generated online are increasing\nat an unprecedentedly fast pace. To automate sports video editing/highlight generation process, a key task is to precisely\nrecognize and locate the events in the long untrimmed videos. In this tech report, we present a two-stage paradigm to detect\nwhat and when events happen in soccer broadcast videos. Speciﬁcally, we ﬁne-tune multiple action recognition models on\nsoccer data to extract high-level semantic features, and design a transformer based temporal detection module to locate the\ntarget events. This approach achieved the state-of-the-art performance in both two tasks, i.e., action spotting and replay\ngrounding, in the SoccerNet-v2 Challenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features are\nreleased at https://github.com/baidu-research/vidpress-sports. By sharing these features with the broader community, we\nhope to accelerate the research into soccer video understanding.\n1. Introduction\nCreating sports highlight videos often involves human efforts to manually edit the original untrimmed videos. The most\npopular sports videos often comprise short clips of a few seconds, while for machines to understand the video and spot\nkey events precisely is very challenging. In this tech report, we present a two-stage paradigm (see Figure.1) to solve two\nproblems in understanding soccer videos and detecting target events: action spotting and replay grounding, which are deﬁned\nin SoccerNet-v2 challenge [4]. The action spotting task aims at spotting actions such as goal, shots-on target, shots-off target,\nyellow card, red card, etc, in a complete video of soccer game. The replay grounding task is to ground the timestamps of the\nactions represented in a speciﬁc replay. In our approach, the same ﬁrst stage is shared in both two tasks, which uses ﬁne-tuned\naction recognition models to extract semantic features, and the second stage consists of a temporal detection module tailored\nfor each task.\n1.1. Related work\nIn sports analytics, many computer vision technologies are developed to understand sports broadcasts [15]. Speciﬁcally\nin soccer, researchers propose algorithms to detect players on ﬁeld in real time [2], analyze pass feasibility using player’s\nbody orientation [1], incorporate both audio and video streams to detect events [17], recognize group activities on the ﬁeld\nusing broadcast stream and trajectory data [14], aggregate deep frame features to spot major game events [8], and leverage\nthe temporal context information around the actions to handle the intrinsic temporal patterns representing these actions [3, 9].\n1.2. Contributions\nIn this tech report, our main contributions can be summarized as the following:\n• Taking advantage of multiple recent action recognition models pretrained on large-scale video datasets, we extract\nsemantic features of soccer videos by ﬁne-tuning each model as the feature extractor, on an auxiliary snippet dataset which\nis derived from the original SoccerNet-v2 dataset. We concatenate and normalize the features obtained from each model to\n* denotes equal contributions. The order was determined using Python’s random.shufﬂe()\n1\narXiv:2106.14447v1  [cs.CV]  28 Jun 2021\nFigure 1: Our two-stage paradigm for action spotting and replay grounding in soccer videos\ngenerate Baidu soccer embeddings. The proposed feature combination signiﬁcantly improves both action spotting and replay\ngrounding performance.\n• We propose a novel transformer based temporal detection module which achieves the state-of-the-art performance in\nboth action spotting task and replay grounding task in the SoccerNet-v2 Challenge, under 2021 CVPR ActivityNet Workshop.\n2. Feature Extraction\nBoth the previous competitive method NetVLAD++ [9] for action spotting and the baseline methodCALF more negative\n(Cmn) [4] for replay grounding use per-frame features extracted by ResNet [11] pretrained on ImageNet [12]. However, we\nbelieve that features that are tailored for the soccer broadcast videos can improve the performance of the spotting module. We\nﬁne-tune multiple action recognition models on snippets of SoccerNet-v2 videos, and in the test stage we also extract features\nfrom videos (clips of frames), rather than on a per-frame basis. We ﬁne-tune multiple action recognition models on the task\nof action classiﬁcation. The models we use include TPN [19], GTA [10], VTN [13], irCSN [16], and I3D-Slow [6]. In order\nto perform such ﬁne-tuning, we construct an 18-class snippet dataset by extracting snippets, each with 5 seconds long, from\nall the videos. Each snippet centers at one of the 17 classes of events or randomly samples from background (non-event). We\napply each ﬁne-tuned action recognition model on the temporal sliding windows of videos, and concatenate output features\nalong the feature dimension.\nHere we brieﬂy introduce the ﬁve pretrained action recognition models we choose to ﬁne-tune on soccer data. The\ntemporal pyramid network (TPN) [19] efﬁciently incorporate visual information of different speeds at feature level, and\ncan be integrated into 2D or 3D backbone networks. It can achieve 78.9% top-1 accuracy on Kinetics-400 dataset with a\nResNet-101 backbone network. The global temporal attention (GTA) mechanism for video action classiﬁcation proposed\nin [10] models global spatial and temporal interactions in a decoupled manner. It captures temporal relationships on both\npixels and semantically similar regions. On Kinetics-400 dataset, GTA achieves 79.8% top-1 accuracy when applied on\nSlowFast-R101 network. The video transformer network (VTN) [13] adopts transformer based network structure for video\nunderstanding. It trains faster than 3D CNN networks and can achieve 78.6% top-1 accuracy on Kinetics-400 dataset.\nThe interaction-reduced channel-separated convolutional networks (irCSN) introduced in [16] factorizes 3D convolutions\nby separating channel interactions and spatio-temporal interactions. In this way, a regularization effect is observed when\ntraining the network. The authors also pretrained this network on a large-scale dataset, IG-65M [7], before ﬁne-tuning on\nKinetics-400 where achieved 82.6% top-1 accuracy. The I3D-Slow network preserves the slow pathway, which operates at\nlow frame rate and captures spatial semantics in the SlowFast framework [6]. It is pretrained with OmniSource [5] data and\ncan reach 80.4% top-1 accuracy on Kinetics-400.\n3. Temporal Detection\nIn this section, we present the temporal detection module in our two-stage paradigm for soccer video understanding.\nSpeciﬁcally, 1) NetVLAD++ and transformer for action spotting task, and 2) Cmn and transformer for replay grounding task.\n2\n<latexit sha1_base64=\"sK4xAQrYX31Eg+47rCcw+ghZq30=\">AAAB63icbVBNSwMxEJ2tX3X9qnr0EiyFeim7PajHohePFe0HtEvJptk2NMkuSVYoS3+CJ0FBvPqLPPlvTNs9aOuDgcd7M8zMCxPOtPG8b6ewsbm1vVPcdff2Dw6PSscnbR2nitAWiXmsuiHWlDNJW4YZTruJoliEnHbCye3c7zxRpVksH800oYHAI8kiRrCx0kMVXwxKZa/mLYDWiZ+TMuRoDkpf/WFMUkGlIRxr3fO9xAQZVoYRTmdupZ9qmmAywSPas1RiQXWQLW6doYpVhiiKlS1p0EJ1f01kWGg9FaHtFNiM9ao3F//zeqmJroOMySQ1VJLloijlyMRo/jgaMkWJ4VNLMFHMHovIGCtMjI3HtSn4qz+vk3a95l/W6vf1cuMmz6MIZ3AOVfDhChpwB01oAYERPMMrvDnCeXHenY9la8HJZ07hD5zPH++7jXo=</latexit>\n(a)\n<latexit sha1_base64=\"p35c2oVGdVidj53ZVd4rahczjaQ=\">AAAB63icbVBNSwMxEJ2tX3X9qnr0EiyFeim7PajHohePFe0HtEvJptk2NMkuSVYoS3+CJ0FBvPqLPPlvTNs9aOuDgcd7M8zMCxPOtPG8b6ewsbm1vVPcdff2Dw6PSscnbR2nitAWiXmsuiHWlDNJW4YZTruJoliEnHbCye3c7zxRpVksH800oYHAI8kiRrCx0kM1vBiUyl7NWwCtEz8nZcjRHJS++sOYpIJKQzjWuud7iQkyrAwjnM7cSj/VNMFkgke0Z6nEguogW9w6QxWrDFEUK1vSoIXq/prIsNB6KkLbKbAZ61VvLv7n9VITXQcZk0lqqCTLRVHKkYnR/HE0ZIoSw6eWYKKYPRaRMVaYGBuPa1PwV39eJ+16zb+s1e/r5cZNnkcRzuAcquDDFTTgDprQAgIjeIZXeHOE8+K8Ox/L1oKTz5zCHzifP/FBjXs=</latexit>\n(b)\nTransformer Encoding Layer\nSigmoid\nClip Features\nPositional \nEncoding\nBCELoss\nTransformer Encoding Layer\nTransformer Encoding Layer\nTransformer Encoding Layer\nSigmoid\nCandidate Clip\nFeatures\nReplay Probability\nPositional \nEncoding\nReplay Clip \nFeatures\nConcatenate\nBCELoss\nOffset in [0,1]\nL2 Loss\nTransformer Encoding Layer\nTransformer Encoding Layer\nTransformer Encoding Layer\nFigure 2: Our transformer based models for (a) action spotting and (b) replay grounding\n3.1. Action Spotting\nGiven the combined features described in Section 2 as input, a NetVLAD++ [9] model can yield much higher performances\nthan the original ResNet features. We also implemented other methods including 1D-ResNet [11] and Transformer [18], and\nthey can achieve similar results. Since the Transformer obtains the best performance on the challenge set, we describe its\nimplementation details as follows.\nFor the transformer model, as in [18], we use sine, cosine positional encoding. We only use the encoder part of the model\nto get an output of dimension 18 to represent the 18-class probabilities. As shown in Figure 2(a), we create three transformer\nencoding layers after the positional encoding. We choose 4 heads and hidden dimension of 64 for the encoding layers. In\ntraining, we adopt mix-up augmentation [20] to reduce over-ﬁtting.\nTo further improve the performance, we make the following adjustments: (a) train the video recognition models for feature\nextraction on an enlarged dataset, which is the aggregation of train, valid, and test sets in the snippet dataset we mentioned\nin Section 2, together with snippets extracted from extra 77 games we collected in Spain Laliga 2019-2021 seasons; (b) train\nthe spotting module on the aggregation of train, valid, and test sets; (c) change hyper parameters including feature dimension\nsize, batch size, learning rate, chunk size and NMS window size. Details will be elaborated in the experiment section.\n3.2. Replay Grounding\nIn this section, we ﬁrst analyze replay annotations of 500 games in the SoccerNet-v2 dataset, and then discuss the baseline\ngrounding module Cmn and our transformer based grounding module.\n3.2.1 Replay pattern analysis\nTo better choose hyperparameters for our model and improve grounding results, we analyze all replay annotations. Figure\n3(a) shows the distributions of time intervals between the end timestamp of a replay and the original event’s timestamp. We\nfound that 92.83% of the time intervals fall in the range of 0 ∼ 120 seconds. Therefore, for efﬁcient training and inference,\nwe design the transformer based grounding module and design ﬁltering strategies in post processing for Cmn module to focus\nin this range. Figure 3(b) shows the number of different types of events in ground-truth replay annotations. We found that the\ntop 3 events in terms of total counts are foul, goal, and shots-off target respectively. This observation helps us design fusion\nstrategies in post processing which will be described in the experiments session.\n3.2.2 Transformer based grounding module\nTo capture the relationship between the replay clip and candidate clip, we apply a transformer encoder. Following the\nconﬁgurations in [18], we choose sine, cosine positional encoding. As shown in Figure 2(b), the input consists of semantic\nfeatures of a candidate clip and a replay clip. We stack up 4 encoding layers and get an output with 2 dimensions (replay\n3\n<latexit sha1_base64=\"sK4xAQrYX31Eg+47rCcw+ghZq30=\">AAAB63icbVBNSwMxEJ2tX3X9qnr0EiyFeim7PajHohePFe0HtEvJptk2NMkuSVYoS3+CJ0FBvPqLPPlvTNs9aOuDgcd7M8zMCxPOtPG8b6ewsbm1vVPcdff2Dw6PSscnbR2nitAWiXmsuiHWlDNJW4YZTruJoliEnHbCye3c7zxRpVksH800oYHAI8kiRrCx0kMVXwxKZa/mLYDWiZ+TMuRoDkpf/WFMUkGlIRxr3fO9xAQZVoYRTmdupZ9qmmAywSPas1RiQXWQLW6doYpVhiiKlS1p0EJ1f01kWGg9FaHtFNiM9ao3F//zeqmJroOMySQ1VJLloijlyMRo/jgaMkWJ4VNLMFHMHovIGCtMjI3HtSn4qz+vk3a95l/W6vf1cuMmz6MIZ3AOVfDhChpwB01oAYERPMMrvDnCeXHenY9la8HJZ07hD5zPH++7jXo=</latexit>\n(a)\n<latexit sha1_base64=\"p35c2oVGdVidj53ZVd4rahczjaQ=\">AAAB63icbVBNSwMxEJ2tX3X9qnr0EiyFeim7PajHohePFe0HtEvJptk2NMkuSVYoS3+CJ0FBvPqLPPlvTNs9aOuDgcd7M8zMCxPOtPG8b6ewsbm1vVPcdff2Dw6PSscnbR2nitAWiXmsuiHWlDNJW4YZTruJoliEnHbCye3c7zxRpVksH800oYHAI8kiRrCx0kM1vBiUyl7NWwCtEz8nZcjRHJS++sOYpIJKQzjWuud7iQkyrAwjnM7cSj/VNMFkgke0Z6nEguogW9w6QxWrDFEUK1vSoIXq/prIsNB6KkLbKbAZ61VvLv7n9VITXQcZk0lqqCTLRVHKkYnR/HE0ZIoSw6eWYKKYPRaRMVaYGBuPa1PwV39eJ+16zb+s1e/r5cZNnkcRzuAcquDDFTTgDprQAgIjeIZXeHOE8+K8Ox/L1oKTz5zCHzifP/FBjXs=</latexit>\n(b)\nFigure 3: Replay pattern analysis. (a) Time intervals between the end timestamp of a replay and the original event’s timestamp. (b) Replay\nevents.\nTable 1: Video action recognition models for extracting semantic features\nArch Backbone Dim Pretrain\nTPN [19] ResNet50/101 2048 K400\nGTA [10] ResNet50 2048 K400\nVTN [13] ViT-Base 384 K400\nirCSN [16] ResNet152 2048 IG65M + K400\nI3D-Slow [6] ResNet101 2048 OmniSource\nprobability and positional offset) which align with the spotting output dimensions in the baseline approach in Cmn from\n[4]. Unlike the baseline grounding module Cmn, we disabled segmentation loss. We apply the binary cross-entropy loss\n(BCELoss) to train the replay probability and L2 loss to train the positional offset.\nSince our ﬁne-tuned features work better with shorter clips (feature extractors trained on 5-second snippets) and to prevent\nthe module from over-ﬁtting, we adjust the video chunk size to 30 seconds. We also only ﬁne-tune the grounding module on\nvideo chunks extracted from at most120 seconds before the start of replays since the feature extractors are already trained on\nall event snippets from full match videos, and most replays happen within the120 seconds after the original events according\nto our replay pattern analysis. In the 120 seconds clip, 4 positive and 4 negative samples are given to the transformer such\nthat we have sufﬁcient data to better learn the grounding component of the output.\n4. Experiments\n4.1. Dataset and Evaluation\nThe SoccerNet-v2 dataset contains broadcast videos of 550 soccer games. We mainly use the LQ version of these videos\nat 25fps with a resolution of398×224. In addition, we collect broadcast videos of 77 extra games in Spain Laliga 2019-2021\nseasons. We check the extra videos and guarantee that they do not contain any game from the SoccerNet-v2 challenge set.\nWe annotate these videos in the similar protocol as SoccerNet-v2, and convert videos into LQ in order to ﬁne-tune feature\nextractors.\nWe report the performance of our methods using the Average-mAP metric introduced by SoccerNet-v2.\n4.2. Implementation Details\nFor the feature extraction stage, Table 1 shows all the action recognition models we use with their main conﬁgura-\ntions. These models are pretrained from various large-scale datasets, including Kinetics-400 (K400)[21], IG65M [7], and\nOmnisource[5]. All models are ﬁne-tuned on SoccerNet-v2 snippets to reach a reasonable top-1 classiﬁcation accuracy\nbetween 78% and 82% (1-view test) on the test set. At test time, all models slide on the videos and produce features at 1fps.\nTo boost the performance on the challenge set, we also ﬁne-tune the feature extractors (action recognition models) on an\nenlarged snippet dataset, which contains snippets from the train, valid, test videos of SoccerNet-v2 and videos of 77 extra\ngames. We denote the produced features as mega features if the extractors are ﬁne-tuned on the enlarged snippet dataset.\n4\nIn our experiments, the spotting or grounding module is trained in two modes, regular and ultra. In the regular mode,\ntrain, valid, and test set each serves its own purpose following the same setting as the reference NetVLAD++ [9] method for\naction spotting or Cmn [4] for replay grounding. In the ultra mode, we make the spotting/grounding module to learn from as\nmuch data as possible, thus we use all features from train, valid, and test sets to train the spotting/grounding module, for a\nﬁxed amount of epochs.\nFor the action spotting task, we use a learning rate of 10−4 for the NetVLAD++ model. The ultra mode training stops at\n40 epochs. For the tranformer model, we use a learning rate of 5 × 10−4 and stop at 50 epochs. For the replay grounding\ntask, a learning rate of 2 × 10−4 is adopted using the transformer model and training stops at 40 epochs in ultra mode.\n4.3. Results and Analysis\n4.3.1 Action Spotting\nTable 2 shows the performance of our methods with different conﬁgurations. When using only one (ordinary) feature from\nTPN-r50 and perform spotting using NetVLAD++ in the regular mode, as shown in the ﬁrst row of Table 2, we achieve an\nAverage-mAP of 62.96% and 62.35% on the test and challenge set, respectively, which is about 9% ∼ 10% gain over than\nthe reference NetVLAD++’s 53.30% and 52.54% on test and challenge sets, respectively. This result shows the superior\nadvantage of using a recent action recognition model ﬁne-tuned on SoccerNet-v2 as a feature extractor.\nWhen using 4 features or 5 features combined as the input of the spotting module, as shown in row 3 and 6 in Table 2, we\nobtain about 5% ∼ 9% gain on both the test and the challenge sets over the 1 feature case. Such comparison shows feature\ncombination also signiﬁcantly improves the performance.\nTraining the spotting module in the ultra mode with 4 mega features results in a challenge Averge-mAP of68.68% (row 5\nin Table 2), compared to the regular mode with the ordinary features at67.51% (row 3 in Table 2) and the regular mode with\nthe mega features at 67.57% (row 4 in Table 2). This comparison indicates that it improves the generalization power only if\nboth stages use more data for training.\nComparing row 6 and 8 in Table 2, we can see that adjusting chunk size/NMS window size from 15/30 to 7/20 leads to\nadditional 1.5% ∼ 2% gain on the test and challenge sets.\nOur transformer based spotting module, trained in the ultra mode with mega features plus adjusted chunk size/NMS\nwindow size, achieves the best challenge Average-mAP at 74.84% as shown in row 10 in Table 2. While the NetVLAD++\nbased module, trained in the same setting, achieves similar performance: 74.63% on the challenge set.\n4.3.2 Replay grounding\nWe summarize our experimental results for replay grounding task in Table 3. As we can see from the table, taking the\nﬁne-tuned features as input signiﬁcantly improved the grounding results compared to the baseline average-AP in Row 1. In\naddition, based on the same grounding module, combining more features extracted with different action recognition models\nleads to further improvements. We also observed superior performance by using a large batch size of 128.\nTo further improve the performance, we also investigated several post-processing techniques to reﬁne the grounding\nmodule outputs, based on our analysis of the replay pattern:\n• Filtering: For Cmn based grounding module, we eliminate all spotting results 1) after the end timestamp of the replay,\n2) more than 100 seconds (threshold) prior to the end timestamp of the replay. Note the ﬁltering threshold in Row 9 was set\nto 120 seconds.\n• Fusion: Taking advantage of the action spotting results, we incorporate the action classiﬁcation information into the\nreplay grounding task. For each queried replay clip with start time T, we adopt the following procedures. First, we ﬁlter\nspotting predictions with top-3 most frequent labels of replay actions ( i.e., ’Foul’, ’Goal’, ’Shots-off target’) and with the\npredicted scores higher than S. Second, the ﬁrst and second nearest spotting predictions to the replay clip start time T are\nselected, and satisfying the constraint that each prediction falls into the temporal window range[T−W,T], because the actual\naction should happen before the relay action. Third, we use the spotting conﬁdence score as the replay grounding conﬁdence\nscore, and the score of the nearest prediction is multiplied with a factor β1 and the second-nearest prediction is multiplied\nwith β2. Through experiments, we ﬁnd that W = 42,S = 0.02,β1 = 1.25,β2 = 0.8 achieves the best performance.\n• NMS: We combine the grounding results from Cmn model and transformer model, normalize all event scores to[0.0,1.0,\nand apply an NMS (Non Maximum Suppression) to reduce positive spots within a window size of 25.\nThe combined post processing techniques achieved a decent performance improvement, around 12% comparing Row 5\nand Row 8. However, our best result is achieved using the transformer based grounding module described in Section 3.2.2\nand trained in ultra mode, which is 71.9% as shown in Row 9. Speciﬁcally, we trained the transformer in 40 epochs and it\n5\nTable 2: Experimental results using different features, models, and window sizes. In the features column, each number stands for total\ntypes of features used: 1 for TPN-r50 only; 4 for TPN-r50, TPN-r101, GTA, and irCSN; 5 for TPN-r101, VTN, GTA, irCSN, and I3D-\nSlow; 6 for TPN-r50, TPN-r101, VTN, GTA, irCSN, and I3D-Slow. In the spotting column, NV stands for NetVLAD++, and TF stands\nfor Transformer. In the test column, “-” means the result is not meaningful, due to the reason that test set is used in ﬁne-tuning. In the\nchallenge column, “N/A” means it is not evaluated due to limited availability.\nRow Features Spotting Chunk/NMS Test Challenge\n1 ResNet NV 15/30 53.30 52.54\n2 1 NV 15/30 62.96 62.35\n3 4 NV 15/30 67.97 67.51\n4 4 mega NV 15/30 - 67.57\n5 4 mega NV ultra 15/30 - 68.68\n6 5 NV 15/30 72.64 71.08\n7 5 TF 7/20. 73.77 N/A\n8 5 NV 7/20 74.05 73.19\n9 6 mega NV ultra 7/20 - 74.63\n10 6 mega TF ultra 7/20 - 74.84\nTable 3: Experimental results using different number of features; grounding models, including CALF-more-negative (Cmn), Transformer\n(TF); batch size (BS); and post-processing (PP) techniques, including ﬁltering (FT), fusion (FS), and NMS. For features, 1 for VTN; 2 for\nTPN-r101 and irCSN; 3 for TPN-r101, irCSN and GTA; 5 for TPN-r101, VTN, GTA, irCSN, and I3D-Slow. TF-u denotes training the\ntransformer in ultra mode. The evaluation metric is average-AP as deﬁned in [4]\nRow Features Grounding PP BS Challenge\n1 ResNet Cmn N/A 32 40.75\n2 2 Cmn N/A 32 52.11\n3 3 Cmn N/A 64 55.79\n4 1 TF N/A 128 58.62\n5 5 Cmn N/A 128 59.26\n6 5 Cmn FT 128 62.19\n7 5 Cmn FT,FS 128 64.01\n8 5 Cmn,TF-u FT,FS,NMS 128 71.59\n9 5 TF-u FT 128 71.9\ntook about 3 hours on a TitanX GPU, which is signiﬁcantly faster than training the Siamese neural network in the baseline\napproach.\n5. Conclusion\nWe present the two-stage paradigm for the action spotting and replay grounding tasks, and ﬁne-tune action recognition\nmodels on soccer videos to extract semantic features and using Transformer in the spotting and grounding modules. We\nachieve the state-of-the-art results on the challenge set of SoccerNet-v2. Developing the proposed action spotting and replay\ngrounding pipeline in soccer videos is the ﬁrst step for machines to fully understand sports videos, which can dramatically\nbeneﬁt sports media, broadcasters, YouTubers or other short video creators. We believe the presented methodology can be\nextended to detect and locate events in other sports videos. We released our semantic features to support further soccer video\nunderstanding research.\nReferences\n[1] A. Arbu ´es-Sang¨uesa, A. Mart´ın, J. Fern´andez, C. Ballester, and G. Haro. Using player’s body-orientation to model pass feasibility in\nsoccer. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 3875–3884, 2020.\n1\n[2] Anthony Cioppa, Adrien Deliege, Noor Ul Huda, Rikke Gade, Marc Van Droogenbroeck, and Thomas B. Moeslund. Multimodal and\nmultiview distillation for real-time player detection on a football ﬁeld. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops, June 2020. 1\n6\n[3] Anthony Cioppa, Adrien Deli `ege, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck, Rikke Gade, and Thomas B. Moes-\nlund. A context-aware loss function for action spotting in soccer videos. In The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2020. 1\n[4] A. Deli `ege, A. Cioppa, Silvio Giancola, M. J. Seikavandi, J. V . Dueholm, Kamal Nasrollahi, Bernard Ghanem, T. Moeslund, and\nMarc Van Droogenbroeck. Soccernet-v2 : A dataset and benchmarks for holistic understanding of broadcast soccer videos. ArXiv,\nabs/2011.13367, 2020. 1, 2, 4, 5, 6\n[5] Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, and Dahua Lin. Omni-sourced webly-supervised learning for video recog-\nnition. arXiv preprint arXiv:2003.13042, 2020. 2, 4\n[6] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages 6201–6210, 2019. 2, 4\n[7] Deepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan, Heng Wang, and D. Mahajan. Large-scale weakly-supervised pre-training\nfor video action recognition. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12038–12047,\n2019. 2, 4\n[8] Silvio Giancola, Mohieddine Amine, Tarek Dghaily, and Bernard Ghanem. Soccernet: A scalable dataset for action spotting in soccer\nvideos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2018. 1\n[9] Silvio Giancola and Bernard Ghanem. Temporally-aware feature pooling for action spotting in video broadcasts. In The IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2021. 1, 2, 3, 5\n[10] Bo He, Xitong Yang, Zuxuan Wu, Hao Chen, Ser-Nam Lim, and Abhinav Shrivastava. Gta: Global temporal attention for video\naction understanding. arXiv preprint arXiv:2012.08510, 2020. 2, 4\n[11] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. 2, 3\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. Advances\nin neural information processing systems, 25:1097–1105, 2012. 2\n[13] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. arXiv preprint arXiv:2102.00719, 2021.\n2, 4\n[14] Ryan Sanford, Siavash Gorji, Luiz G. Hafemann, Bahareh Pourbabaee, and Mehrsan Javan. Group activity detection from trajectory\nand video data in soccer. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages\n3932–3940, 2020. 1\n[15] Graham Thomas, Rikke Gade, Thomas B. Moeslund, Peter Carr, and Adrian Hilton. Computer vision for sports: Current applications\nand research topics. Computer Vision and Image Understanding, 159:3–18, 2017. Computer Vision in Sports. 1\n[16] Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torresani. Video classiﬁcation with channel-separated convolutional networks. In\n2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5551–5560, 2019. 2, 4\n[17] Bastien Vanderplaetse and St´ephane Dupont. Improved soccer action spotting using both audio and video streams. In2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 3921–3931, 2020. 1\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. arXiv preprint arXiv:1706.03762, 2017. 3\n[19] Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, and B. Zhou. Temporal pyramid network for action recognition. 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pages 588–597, 2020. 2, 4\n[20] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412, 2017. 3\n[21] Andrew Zisserman, Joao Carreira, Karen Simonyan, Will Kay, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, and Mustafa Suleyman. The kinetics human action video dataset. 2017. 4\n7",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8075892925262451
    },
    {
      "name": "Pace",
      "score": 0.6380830407142639
    },
    {
      "name": "Transformer",
      "score": 0.5571030378341675
    },
    {
      "name": "Embedding",
      "score": 0.5274921655654907
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4238012731075287
    },
    {
      "name": "Key (lock)",
      "score": 0.41600728034973145
    },
    {
      "name": "Feature extraction",
      "score": 0.4153429865837097
    },
    {
      "name": "The Internet",
      "score": 0.4139171242713928
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3951362371444702
    },
    {
      "name": "Machine learning",
      "score": 0.3892914354801178
    },
    {
      "name": "Multimedia",
      "score": 0.35778945684432983
    },
    {
      "name": "World Wide Web",
      "score": 0.19741150736808777
    },
    {
      "name": "Computer security",
      "score": 0.10361415147781372
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    }
  ],
  "cited_by": 20
}