{
  "title": "Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos",
  "url": "https://openalex.org/W4308235761",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2613781093",
      "name": "Gargi V. Pillai",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    },
    {
      "id": "https://openalex.org/A2096123286",
      "name": "Ashish Verma",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    },
    {
      "id": "https://openalex.org/A2107633960",
      "name": "Debashis Sen",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3129071613",
    "https://openalex.org/W2905312532",
    "https://openalex.org/W3011179489",
    "https://openalex.org/W2970724283",
    "https://openalex.org/W3190318906",
    "https://openalex.org/W3014352273",
    "https://openalex.org/W3114278947",
    "https://openalex.org/W3126615635",
    "https://openalex.org/W3190308053",
    "https://openalex.org/W2925312408",
    "https://openalex.org/W3157342628",
    "https://openalex.org/W2753526808",
    "https://openalex.org/W2963610939",
    "https://openalex.org/W3016705886",
    "https://openalex.org/W3108027406",
    "https://openalex.org/W3094527412",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2560474170",
    "https://openalex.org/W3042642865",
    "https://openalex.org/W2162616721",
    "https://openalex.org/W2163612318",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Anomaly detection in videos is a challenging task as anomalies in different videos are of different kinds. Therefore, a promising way to approach video anomaly detection is by learning the non-anomalous nature of the video at hand. To this end, we propose a one-class few-shot learning driven transformer based approach for anomaly detection in videos that is self-context aware. Features from the first few consecutive non-anomalous frames in a video are used to train the transformer in predicting the non-anomalous feature of the subsequent frame. This takes place under the attention of a self-context learned from the input features themselves. After the learning, given a few previous frames, the video-specific transformer is used to infer if a frame is anomalous or not by comparing the feature predicted by it with the actual. The effectiveness of the proposed method with respect to the state-of-the-art is demonstrated through qualitative and quantitative results on different standard datasets. We also study the positive effect of the self-context used in our approach.",
  "full_text": "TRANSFORMER BASED SELF-CONTEXT AWARE PREDICTION FOR FEW-SHOT\nANOMALY DETECTION IN VIDEOS\nGargi V . Pillai, Ashish Verma, Debashis Sen\nDepartment of E&ECE, Indian Institute of Technology Kharagpur, India\nABSTRACT\nAnomaly detection in videos is a challenging task as anoma-\nlies in different videos are of different kinds. Therefore, a\npromising way to approach video anomaly detection is by\nlearning the non-anomalous nature of the video at hand. To\nthis end, we propose a one-class few-shot learning driven\ntransformer based approach for anomaly detection in videos\nthat is self-context aware. Features from the first few con-\nsecutive non-anomalous frames in a video are used to train\nthe transformer in predicting the non-anomalous feature of\nthe subsequent frame. This takes place under the attention\nof a self-context learned from the input features themselves.\nAfter the learning, given a few previous frames, the video-\nspecific transformer is used to infer if a frame is anomalous or\nnot by comparing the feature predicted by it with the actual.\nThe effectiveness of the proposed method with respect to the\nstate-of-the-art is demonstrated through qualitative and quan-\ntitative results on different standard datasets. We also study\nthe positive effect of the self-context used in our approach.\nIndex Terms— Anomaly detection, feature prediction,\ntransformer network, self-context\n1. INTRODUCTION\nWith an increase in demand of video surveillance systems for\nvideo anomaly detection (V AD), it is becoming increasingly\nimportant to develop intelligent surveillance systems to auto-\nmatically detect anomalies in different kinds of scenarios [1].\nOvercoming the challenging nature of V AD, many recent\napproaches on V AD based on deep learning have achieved\nsignificant improvements over classical approaches [2, 3].\nThese approaches can be generally categorized as reconstruc-\ntion based [2,4] and prediction based approaches [1,3]. Most\ndeep learning based V AD approaches model normality by\ntraining on data without anomaly and infers abnormality in\nthe testing data using prediction or reconstruction error.\nReconstruction based V AD approaches detect anomalies\nby reconstruction of video frames, where low and high recon-\nstruction errors represent normality and abnormality, respec-\ntively. Reconstruction based deep approaches largely include\nthose based on autoencoders (AE) [2, 5] and its variants such\nas convolutional autoencoders (CAE) [6–9]. Prediction based\ndeep approaches detect anomalies by predicting current frame\nFrame: \nAnomalous\nFrame: \nNon Anomalous\nInitial Few Non-Anomalous \nFrames (Learning)\\\nPrevious Few Frames (Inference)\nF2 F3 FT \nConcatenated Features\nF1\nMSE\n. . .\nI1,  I2,  … , IT+1\nFT+1\n^\nFT+1\nPretrained\nResNet152\nTransformer based Self-Context \naware Prediction Module\nPretrained\nFlowNet2\nInput \nSequence\nSelf-\nContext\nEncoder Decoder\nLoss\nLearningInference\nFig. 1: Schematic of the proposed self-context aware video anomaly\ndetection through one-class few-shot learning. F1, F2, · · ·, FT+1\nrepresent the concatenated spatial and temporal features of a set of\nT + 1consecutive frames I1, I2, · · ·, IT+1 in the video. Previous\nT frames are used for anomaly detection in the current (T + 1)th\nframe, by comparing its predicted feature vector to the actual during\ninference. The same loss is used in the network learning using its\ninitial few non-anomalous frames for the video at hand.\nfeatures using that of previous frames. A low /high prediction\nerror signifies the presence of normal /abnormal events. Pre-\ndiction based approaches mostly include those using autore-\ngressive models (AR) [10, 11], convolutional long short-term\nmemory (CLSTM) [12] and generative adversarial networks\n(GAN) [3, 13–15]. Prediction-based approaches have been\nfound to be successful in learning the invariances related to\nthe temporal changes when an anomaly is not present, and\nhence, they perform well in V AD [16].\nMost of the above deep learning based approaches that\nperform well in V AD are trained on a dataset of videos. Given\nthe varying kinds of anomaly from one video to another, the\nscope of designing a model that can be trained through few-\nshot learning in the single video at hand would be interesting\nto explore. This can significantly reduce the training data re-\nquirement and be suitable for videos in different scenarios.\nIn the last few years, transformers have been found to be ex-\ntremely effective in sequence prediction [17]. In transform-\ners, all sequence positions relevant to the encoder’s input are\nattended by every sequence position related to the data fed\ninto the decoder [18]. This aspect can be leveraged to capture\nboth the relation among and the context of the non-anomalous\narXiv:2503.00670v1  [cs.CV]  2 Mar 2025\nframes of a video through only few-shot learning, as trans-\nformers are highly effective in modeling dependencies within\nsamples in a sequence.\nIn this paper, we propose an approach for anomaly detec-\ntion in a video based on one-class few-shot learning of the\ntransformer network only using the initial few frames in that\nvideo. Our transformer network’s encoder gets the features\nof a few consecutive video frames as the input and its de-\ncoder predicts the feature vector of the subsequent frame in\nthe video as the output. The input features into the encoder\nare employed as the input to the decoder as well, which allows\nthem to act as a self-context attending over all the frames in\nthe input sequence.\nThe one-class few-shot learning is performed consid-\nering the initial few non-anomalous video frames for the\nencoder and decoder inputs. The transformer learns the rela-\ntion among the non-anomalous frames to predict the feature\nof the subsequent non-anomalous video frame, given the\nself-context. After the learning using the few initial non-\nanomalous frames of the video at hand, anomaly detection\nis performed in the rest of the video or continuously for the\nduration required. A frame is marked as anomalous when\nits feature vector predicted by our network using a few pre-\nvious frames differs from the actual feature vector. During\nthis inference, we use the actual or predicted (if the actual is\nanomalous) features of the previous few frames.\nThe feature vector of a video frame is obtained by con-\ncatenating the spatial and temporal features extracted using\nthe pre-trained ResNet152 [19] and FlowNet2 [20] networks,\nrespectively. Additionally, temporal consistency is also im-\nposed to reduce false positives during the anomaly detection.\nThe main contributions of this paper are:\n• Video-specific one-class few-shot learning based V AD,\nwhere the non-anomalous nature is learned for the\nvideo at hand without any training on a dataset.\n• The use of the transformer network for prediction-\nbased V AD in a way where its sequence dependency\nmodeling capabilities are thoroughly exploited under\nthe attention of a self-context.\nWe demonstrate the superiority of our approach over the state-\nof-the-art through experimental results on standard datasets\nwith different kinds of videos. An ablation study reveals that\nthe use of the proposed self-context provides a significant\nboost to our V AD performance.\nSection 2 describes our approach, the experimental results\nare given in Section 3, and Section 4 concludes the paper.\n2. THE PROPOSED SELF-CONTEXT AWARE\nPREDICTION FOR VIDEO ANOMALY DETECTION\nA pictorial overview of our proposed approach is shown in\nFig 1. The central issue of the V AD problem in our hand\ncan be formulated as: Given a few successive previous video\nframes represented by their features, we need to estimate the\nMSE\nÔ1,\n-\n-\nÔmn \nȒ1,\n-\n-\nȒ512\nO1,\n-\nOmn\nR1,\n-\nR512\nMSE> Th\nAnomalous\nNon\nAnomalous\nYes\nNo\nConcatenated \nFeatures\nR1,\n-\nR512\nO1,\n-\nOmn\nO1,\n-\nOmn\nR1,\n-\nR512\nO1,\n-\nOmn\nR1,\n-\nR512\nF2 FTF1\n. . . \nZ\nZ\n^\nω \nLinear\nMSA\nLN\nMLP\nMCA\n(Fl-1)(Fl-T)\nFT+1(Fl)FT+1^ ^(Fl)\nUse Fl  instead of  Fl  in the subsequent predictions\n^\nEncoder Decoder\nLayer 1\nLayer 1\nLayer 2\nLayer 2\nZ\nφ \nTransformer Linear\nPositional \nEncoding\nlearning\ninference\nPrediction for features of l   video frameth\nSelf-Context \nFig. 2: Architecture of the proposed transformer based prediction\nmodule that leverages a self-context.\nfeatures in the next frame to decide if it is anomalous or not.\n2.1. Video Frame Feature Extraction\nWe denote the feature vector representing the tth frame in a\nsequence of video frames asFt, which is obtained by concate-\nnating spatial and temporal features following usual norms [6,\n21]. We consider the features Ri, i= 1, 2, . . . ,512, extracted\nusing the pretrained ResNet512 of [19] on the video frame as\nthe spatial features, and the featuresOi, i= 1, 2, . . . , mn,ex-\ntracted using pretrained FlowNet2 of [20] on the video with\nframe size m × n.\n2.2. Our Transformer based Prediction for V AD\nThe architecture of the transformer based self-context aware\nprediction module is shown in Fig. 2, on which a one-class\nfew-shot learning strategy is applied for anomaly detection\nduring the inference. We consider a transformer [18] with\nsubstantially fewer number of encoder and decoder layers\nthan usual, which we find is sufficient for our learning prob-\nlem. While the encoder of the network contains the two stan-\ndard modules of Multi-head Self-Attention (MSA) & Layer\nNormalization (LN), and Multi-Layer Perceptron (MLP) &\nLN both repeated only twice, the decoder contains the afore-\nsaid modules along with the standard module of Multi-head\nCross-Attention (MCA) & LN again all of them repeated only\ntwice (See Fig 2).\nThe feature vectors extracted from a few consecutive\nvideo frames are given as the sequence input to the encoder,\nacting upon which the decoder only predicts the feature vec-\ntor of the next video frame after the sequence. The same\ninput sequence is also fed into the decoder, and therefore, a\nlearned representation of the input sequence (from encoder)\nis attended by another learned representation of the same\nsequence (from decoder) at the MCA modules in the decoder\nforming the self-context.\nConsider a sequence of feature vectors extracted from T\nconsecutive video frames as F1, F2, . . . , FT , and we estimate\nˆFT+1 in an attempt to predict FT+1 from Ft, t= 1, 2, . . . , T.\nThe feature vectors F1, F2, . . . , FT are subjected to a learn-\nable linear layer ω obtaining T vectors of a dimension as re-\nquired by the transformer. Positional encoding [18] is then\napplied to embed the sequence position information yielding\nT position-aware feature vectors z1, z2, ..., zT , which are fed\nas the sequence input to the transformer’s encoderΓE consist-\ning of just2 layers of the standard MSA & LN and MLP & LN\nmodules of [18]. The encoder provides a latent representation\nu of the input feature vector sequence Ft, t= 1 , 2, . . . , T.\nTherefore, we have:\nzt = ω (Ft) + P(t), t ∈ 1, ..., T (1)\nu = Γ E(Z), Z = {z1, z2, ..., zT } (2)\nwhere P(t) denotes the positional code value for the tth fea-\nture vector Ft. The output u from the last layer of the encoder\nis fed into the decoder at all its MCA modules.\nThe position-aware feature vectors z1, z2, ..., zT are also\nfed to the transformer’s decoder ΓD. The decoder’s output\nis subjected to a learnable linear layer ϕ, which provides\nonly the estimate ˆFT+1 for the input feature vector sequence\nFt, t= 1, 2, . . . , T. Therefore, we have:\nˆFT+1 = ϕ(ΓD(Z, u)), Z = {z1, z2, ..., zT } (3)\nNote that, as our transformer predicts only ˆFT+1 from the\ninput sequence Ft, t= 1 , 2, . . . , T, which does not contain\nFT+1, we do not require the mask function [18] used in the\nstandard transformer. Further, we also do not require any re-\ncursion, where the output of the decoder is supplied as its in-\nput. The decoder ΓD consists of just 2 layers of the standard\nMSA & LN, MCA& LN, MLP & LN modules and both the\nMCA modules receive u as the ‘value’ and ‘key’ quantities\nfrom the encoder ΓE to be attended by the ‘query’ from ΓD\nitself.\nOne-class Few-shot Learning:For the video at hand, we\nconsider that an initial consecutive sequence of N frames\n(few-shot) are non-anomalous (one class), and their feature\nvectors F1, F2, ..., FN are used to pool multiple sets of T + 1\nconsecutive feature vectors for the transformer learning. A\nset of consecutive T video frames’ feature vectors are used\nto predict the (T + 1)th frame’s feature vector. In an epoch,\nevery set of T + 1 consecutive feature vectors available from\nthe N feature vectors are considered in random order with\none set representing a learning iteration. For the learning, we\nconsider the mean square error (MSE) loss function (∼ norm\nof vector difference) between a predicted feature vector ˆFT+1\nand the corresponding actual feature vector FT+1 as follows:\nLMSE = 1\nD\nDX\nj=1\n(FT+1(j) − ˆFT+1(j))2 (4)\nwhere, FT+1 is of dimension D = 512 + mn.\nInference for Anomaly Detection:All the frames in the\nvideo except the few initial ones used for learning are consid-\nered here for anomaly detection. To detect whether a frame\nis anomalous or not, the previousT frames are considered for\nthe input to the transformer. As our transformer is trained\nto predict the non-anomalous feature vector of the current\nframe given a sequence of previous frames as input, the cur-\nrent frame will naturally be marked as anomalous if the pre-\ndicted feature vector differs from its actual feature vector. We\ncompute the difference (anomaly score) as the LMSE (norm\nsquare of vector difference) between the actual and the pre-\ndicted feature vectors. If for the tth frame, LMSE (t) ≥ T h,\nthen the frame is marked as an anomaly, whereT his the post-\nconvergence averageLMSE for all the N −T predictions per-\nformed during the learning in the video at hand. Note that, as\nour transformer works with non-anomalous features of con-\nsecutive frames as inputs for the prediction, the predicted fea-\nture vectors of the frames already marked as anomalous are\nconsidered in subsequent predictions for anomaly detection\nin the forthcoming frames, instead of the corresponding actual\nfeature vectors. Finally, a temporal consistency is imposed by\nconsidering a frame as anomalous only when the frames in\ntheir immediate temporal neighborhood are also anomalous.\nTable 1: Result comparison for UCSD Ped2, CUHK Avenue, and\nShanghaiTech Campus datasets using Frame-level AUC (%)\nMethod Ped2 Avenue ShanghaiTech\nConvLSTM-AE [12], 2017 88.1 77.0 ×\nZhu et al. [2], 2018 97.1 × ×\nFFP+MC [13], 2018 95.4 85.1 72.8\nISTL [4], 2019 91.1 76.8 ×\nAbati et al. [10], 2019 95.4 × 72.5\nChen et. al. [14], 2020 96.6 × ×\nDSTN [3], 2020 95.5 87.9 ×\nMultispace [15], 2020 95.4 86.8 73.6\nST-CaAE [6], 2020 92.9 83.5 ×\nDoshi et al. [21], 2020 97.8 86.4 71.6\nWang et al. [7], 2021 96.0 86.3 74.5\nAnomaly3D [8], 2021 95.8 89.2 80.6\nMsm-net [16], 2021 96.8 87.4 74.2\nROADMAP [1], 2021 96.3 88.3 76.6\nTRD [11], 2021 98.2 89.3 80.2\nChang et al. [5], 2022 96.7 87.1 73.7\nSTCEN [9], 2022 96.9 86.6 73.8\nOurs 98.6 89.4 80.6\n3. EXPERIMENTAL RESULTS\nDatasets and Evaluation Metrics:We present the qualita-\ntive and quantitative results of our V AD approach consider-\ning videos from three frequently used datasets, namely, the\nUCSD Pedestrian 2 (Ped2) dataset [22], the CUHK Avenue\ndataset [23], and the ShanghaiTech Campus dataset [13]. The\nAnomaly Score\nFrames\nAnomaly Score\nThreshold (Th) \nRegular Activity\nAnomalous Activity:\nVehicle \nRegular Activity Anomalous\nActivity:\nRunning \nAnomalous\nActivity:\nSkipping \nAnomaly Score \nThreshold (Th)\nAnomaly Score\nFrames\nAnomaly Score \nThreshold (Th)\nAnomaly Score\nFrames\nAnomalous Activity:\nSkipping\nRegular Activity\n0 2 4 6 8 10 12 14 16 18 20\nEpoch\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nLoss\nPed2 Video\nAvenue Video\nShanghaiTech Video\n(a) (b) (c) (d)\nFig. 3: Anomaly scores of few frames in videos from (a) UCSD Ped2 dataset, (b) ShanghaiTech Campus dataset, and (c) CUHK Avenue\ndataset. (d) Learning curves for a video each from the 3 datasets.\nanomaly in the Ped2 dataset is non-pedestrians in a pedestrian\npath, anomalies in the Avenue dataset are running, moving in\nwrong direction and strange actions such as throwing, danc-\ning etc., and anomalies in the ShanghaiTech dataset include\nrunning, skating and biking. We compare our model with sev-\neral state-of-the-art methods using AUC at frame level [10,\n15] and ROC curve, which are standard measures [10, 15].\nImplementation Details: In the proposed network, we\nconsider just 2 layers of the modules in the transformer’s en-\ncoder and decoder with the input dimension as512. The num-\nber of heads used in the various multi-head attention modules\nis 2. We take N = 50, which is the number of initial non-\nanomalous frames considered for learning, and consider T =\n10, which is the number of previous frames given at the input\nto perform the prediction related to the current frame. We use\nthe Adam optimizer (lr = 0.01,β = (0.9,0.98)) for the training,\nwhich runs for 100 epochs in a video.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nFalse Positive Rate\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate\nTRD [11]\nZhu et al. [2]\nST-CaAE [6]\nDSTN[3]\nMultispace [15]\nAbati et al. [10]\nOurs\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nFalse Positive Rate\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate\nTRD [11]\nST-CaAE [6]\nMultispace [15]\nOurs\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nFalse Positive Rate\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate\nTRD [11]\nMultispace [15]\nAbati et al. [10]\nOurs\n(a) (b) (c)\nFig. 4: Frame-level ROC curves for (a) UCSD Ped2, (b) CUHK\nAvenue, and (c) ShanghaiTech Campus datasets.\n3.1. Results and Discussion\nOur approach is quantitatively compared with around 17 ex-\nisting approaches including the state-of-the-art on the three\ndatasets. Table 1 shows the quantitative results of the differ-\nent approaches including ours. We observe that our approach\nperforms the best in terms of frame-level AUC on the Ped2\nand Avenue datasets, and performs at par with the state-of-\nthe-art on the ShanghaiTech dataset. The ROC curves for the\nthree datasets shown in Figs. 4(a), (b) and (c) correspond to\nsome of the AUC results in Table 1 including ours, where\nour approach is found to perform at least as good as the other\ncompared methods. From the above observations, we find that\nthe performance of our approach surpasses the state-of-the-art\non the three datasets, which contain varied types of normality\nranging from low to high complexity. For qualitative analysis,\nwe visualize the anomaly score ( LMSE ) for a set of frames\nin a video each from the three datasets in Figs. 3(a), (b) and\n(c), where the corresponding thresholds on the anomaly score\nto detect anomalous activities are also given. As observed\nin the figure, our approach successfully detects a vehicle in\npedestrian path, skipping and running as anomalous events as\nrequired. In Fig. 3(d), loss curves of our one-class few-shot\nlearning on videos from the three datasets are given, which\nshows its efficiency represented by the fast convergence.\nAblation study: To understand the effect of the features\nused and the proposed use of self-context in our approach,\nan ablation study is performed using the Avenue dataset. Ta-\nble 2 lists the results, where it is observed that Model I ,which\nuses only the temporal features, performs better than Model\nII, which uses only the spatial features. Temporal features\nmay be more important as anomalies in videos are mostly de-\nscribed by movements. Additionally, we observe that Model\nIV using both the feature types performs better than Model II,\nindicating the positive contribution of spatial features, which\ncapture variations in the target /object appearance. Finally,\nconsider the improvement achieved by Model IV (our final\nmodel) compared to Model III. The use of the input to the en-\ncoder into the decoder as well to form the self-context in our\nmodel is not considered in Model III, where a single pipeline\nwith 2 layers of MSA & LN and MLP & LN modules are\nonly used. The improvement highlights the significance of\nour proposed exploitation of a self-context for V AD.\nTable 2: An ablation study of our approach on the CUHK Avenue\ndataset. DSF - Self-Context in the Decoder.\nModel ResNet FlowNet DSF AUC %\nModel I × ✓ ✓ 79.5\nModel II ✓ × ✓ 55.0\nModel III ✓ ✓ × 81.5\nModel IV ✓ ✓ ✓ 89.4\n4. CONCLUSION\nA video anomaly detection approach has been proposed based\non a one-class few-shot learning driven transformer predic-\ntion network that considers a self-context. Our learning strat-\negy works on the video at hand, which not only reduces the\ntraining data requirement but also allows the capture of the\nvideo-relevant non-anomalous nature. Our approach has been\nfound to perform well in comparison to the state-of-the-art on\nvideos with different anomaly varieties, with the use of self-\ncontext resulting in a significant performance increase.\n5. AKNOWLEDGEMENT\nDebashis Sen acknowledges the Science and Engineering Re-\nsearch Board (SERB), India for its assistance.\n6. REFERENCES\n[1] Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao,\nKe Yang, Jian Tang, Jieping Ye, Jingyu Wang, and\nQi Qi, “Robust unsupervised video anomaly detection\nby multipath frame prediction,” IEEE Trans. Neural\nNetw. Learn. Syst., 2021.\n[2] Huihui Zhu, Bin Liu, Yan Lu, Weihai Li, and Nenghai\nYu, “Real-time anomaly detection with HMOF feature,”\nICVIP, pp. 49–54, 2018.\n[3] Thittaporn Ganokratanaa, Supavadee Aramvith, and\nNicu Sebe, “Unsupervised anomaly detection and lo-\ncalization based on deep spatiotemporal translation net-\nwork,” IEEE Access, vol. 8, pp. 50312–50329, 2020.\n[4] Rashmika Nawaratne, Damminda Alahakoon, Daswin\nDe Silva, and Xinghuo Yu, “Spatiotemporal anomaly\ndetection using deep learning for real-time video\nsurveillance,” IEEE Trans. Industr. Inform., vol. 16, no.\n1, pp. 393–402, 2020.\n[5] Yunpeng Chang, Zhigang Tu, Wei Xie, Bin Luo, Shifu\nZhang, Haigang Sui, and Junsong Yuan, “Video\nanomaly detection with spatio-temporal dissociation,”\nPatt. Recognit., vol. 122, pp. 108213, 2022.\n[6] Nanjun Li, Faliang Chang, and Chunsheng Liu,\n“Spatial-temporal cascade autoencoder for video\nanomaly detection in crowded scenes,” IEEE Trans.\nMultimedia, vol. 23, pp. 203–215, 2020.\n[7] Wenqian Wang, Faliang Chang, and Huadong Mi, “In-\ntermediate fused network with multiple timescales for\nanomaly detection,” Neurocomputing, vol. 433, pp. 37–\n49, 2021.\n[8] Mujtaba Asad, Jie Yang, Enmei Tu, Liming Chen, and\nXiangjian He, “Anomaly3d: Video anomaly detection\nbased on 3d-normality clusters,” J. Vis. Commun. Image\nRepresent., vol. 75, pp. 103047, 2021.\n[9] Yi Hao, Jie Li, Nannan Wang, Xiaoyu Wang, and Xinbo\nGao, “Spatiotemporal consistency-enhanced network\nfor video anomaly detection,” Patt. Recognit., vol. 121,\npp. 108232, 2022.\n[10] Davide Abati, Angelo Porrello, Simone Calderara, and\nRita Cucchiara, “Latent space autoregression for nov-\nelty detection,” CVPR, pp. 481–490, 2019.\n[11] Gargi V Pillai and Debashis Sen, “Anomaly detection in\nnonstationary videos using time-recursive differencing\nnetwork-based prediction,” IEEE Geosci. Remote. Sens.\nLett., 2021.\n[12] Weixin Luo, Wen Liu, and Shenghua Gao, “Remember-\ning history with convolutional LSTM for anomaly de-\ntection,” ICME, pp. 439–444, 2017.\n[13] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao,\n“Future frame prediction for anomaly detection–a new\nbaseline,” CVPR, pp. 6536–6545, 2018.\n[14] Dongyue Chen, Pengtao Wang, Lingyi Yue, Yuxin\nZhang, and Tong Jia, “Anomaly detection in surveil-\nlance video based on bidirectional prediction,” Image\nVis. Comput., vol. 98, pp. 103915, 2020.\n[15] Yu Zhang, Xiushan Nie, Rundong He, Meng Chen, and\nYilong Yin, “Normality learning in multispace for video\nanomaly detection,” IEEE Trans. Circuits Syst. Video\nTechnol., 2020.\n[16] Yiheng Cai, Jiaqi Liu, Yajun Guo, Shaobin Hu, and Shi-\nnan Lang, “Video anomaly detection with multi-scale\nfeature and temporal information fusion,” Neurocom-\nputing, vol. 423, pp. 264–273, 2021.\n[17] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chun-\nhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia,\n“End-to-end video instance segmentation with trans-\nformers,” in CVPR, 2021, pp. 8741–8750.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” Adv.\nNeural Inf. Process. Syst., vol. 30, 2017.\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun, “Deep residual learning for image recognition,”\nCVPR, pp. 770–778, 2016.\n[20] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Ke-\nuper, Alexey Dosovitskiy, and Thomas Brox, “Flownet\n2.0: Evolution of optical flow estimation with deep net-\nworks,” CVPR, pp. 2462–2470, 2017.\n[21] Keval Doshi and Yasin Yilmaz, “Continual learning for\nanomaly detection in surveillance videos,” CVPRW, pp.\n254–255, 2020.\n[22] Antoni B Chan and Nuno Vasconcelos, “Modeling,\nclustering, and segmenting video with mixtures of dy-\nnamic textures,” IEEE Trans. Patt. Anal. Mach. Intell.,\nvol. 30, no. 5, pp. 909–926, 2008.\n[23] Cewu Lu, Jianping Shi, and Jiaya Jia, “Abnormal event\ndetection at 150 fps in matlab,” ICCV, pp. 2720–2727,\n2013.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7634429335594177
    },
    {
      "name": "Computer science",
      "score": 0.7580835819244385
    },
    {
      "name": "Anomaly detection",
      "score": 0.6861602067947388
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6279832720756531
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4562973976135254
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.44094035029411316
    },
    {
      "name": "Feature extraction",
      "score": 0.43862199783325195
    },
    {
      "name": "One shot",
      "score": 0.43475306034088135
    },
    {
      "name": "Frame (networking)",
      "score": 0.42795413732528687
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4237518906593323
    },
    {
      "name": "Computer vision",
      "score": 0.38864025473594666
    },
    {
      "name": "Machine learning",
      "score": 0.3273748755455017
    },
    {
      "name": "Engineering",
      "score": 0.12327119708061218
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145894827",
      "name": "Indian Institute of Technology Kharagpur",
      "country": "IN"
    }
  ],
  "cited_by": 14
}