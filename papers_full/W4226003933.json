{
  "title": "Better Language Model with Hypernym Class Prediction",
  "url": "https://openalex.org/W4226003933",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100671881",
      "name": "He Bai",
      "affiliations": [
        "University of Waterloo",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5100450972",
      "name": "Tong Wang",
      "affiliations": [
        "University of Waterloo",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5108137151",
      "name": "Alessandro Sordoni",
      "affiliations": [
        "University of Waterloo",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5100739668",
      "name": "Peng Shi",
      "affiliations": [
        "University of Waterloo",
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3126553126",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3112776819",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W2740782137",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2925513271",
    "https://openalex.org/W36903255",
    "https://openalex.org/W4288617998",
    "https://openalex.org/W3174401451",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W3021336872",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3205278400",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1593239840",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W3018305985",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W3168698433"
  ],
  "abstract": "Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1352 - 1362\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nBetter Language Model with Hypernym Class Prediction\nHe Bai∗\nUniversity of Waterloo\nhe.bai@uwaterloo.ca\nTong Wang\nMicrosoft Research\nTong.Wang@microsoft.com\nAlessandro Sordoni\nMicrosoft Research\nalsordon@microsoft.com\nPeng Shi\nUniversity of Waterloo\npeng.shi@uwaterloo.ca\nAbstract\nClass-based language models (LMs) have\nbeen long devised to address context sparsity\nin n-gram LMs. In this study, we revisit this\napproach in the context of neural LMs. We\nhypothesize that class-based prediction leads\nto an implicit context aggregation for similar\nwords and thus can improve generalization for\nrare words. We map words that have a com-\nmon WordNet hypernym to the same class and\ntrain large neural LMs by gradually annealing\nfrom predicting the class to token prediction\nduring training. Empirically, this curriculum\nlearning strategy consistently improves per-\nplexity over various large, highly-performant\nstate-of-the-art Transformer-based models on\ntwo datasets, WikiText-103 and ARXIV. Our\nanalysis shows that the performance improve-\nment is achieved without sacriﬁcing perfor-\nmance on rare words. Finally, we document\nother attempts that failed to yield empirical\ngains, and discuss future directions for the\nadoption of class-based LMs on a larger scale.\n1 Introduction\nOver the course of the past decades, language mod-\neling (LM) has transitioned from n-gram to neu-\nral models (Bengio et al., 2003; Mnih and Hinton,\n2007; Devlin et al., 2019; Brown et al., 2020). Per-\nformance improvement of today’s neural LMs is\noften achieved at the cost of increased computa-\ntional resources. For example, to capture long-term\ndependencies, various extensions of Transformer-\nbased LMs have been proposed (Dai et al., 2019;\nRae et al., 2020). These modiﬁcations bring about\nsigniﬁcant improvements on held-out perplexity,\nbut training cost also increases signiﬁcantly due to\nlarge GPU memory consumption and more compu-\ntations at each training step.\nIn parallel, alternative training strategies have\nalso been proposed (Guu et al., 2020; Ziegler\n∗Most of the work was done during the internship\nat Microsoft Research. Code: https://github.com/\nrichardbaihe/robustLM.git\nA final tor ch  used to enter Empire Stadium  that\nwas made of stainless  steel  and powered by a\nmagnesium  candle \nOriginal Text:\nReplaced with hypernym class:\nA final instrumentality .n.03  used to enter Empire\nstructur e.n.01  that was made of alloy .n.01 \nalloy .n.01  and powered by a metallic_element.n.01 \ninstrumentality .n.03 \nFigure 1: An example of word prediction training text\nand hypernym class prediction training text.\nand Rush, 2019; Deng et al., 2020). In this pa-\nper, we explore the effectiveness of class-based\nlanguage models (CLMs, Brown et al. 1992) in\nthe context of neural LMs. CLMs group indi-\nvidual words into coarser-grained classes and has\nproven effective in alleviating context sparsity in\nn-gram LMs (Dagan et al., 1999). It has been also\nused to improve computational efﬁciency in neural\nLMs (Morin and Bengio, 2005; Grave et al., 2017a).\nMore recently, Levine et al. (2020) pretrain masked\nLMs (Devlin et al., 2019) by predicting WordNet\nsupersense labels. However, the work focuses on\nword-sense disambiguation tasks and doesn’t pro-\nvide clear evidence of gains in terms of perplexity.\nIn this paper, we revisit CLM and assign words\nto classes by leveraging hypernym relations from\nthe WordNet (Miller, 1995). Our proposal, dubbed\nHypernym Class Prediction (HCP) is simple and\neffective: for each batch, we substitute a subset\nof the tokens with their WordNet hypernyms (see\nFigure 1). Then, we train an autoregressive LM\non the resulting sentences using a mixed vocabu-\nlary composed of hypernyms and tokens. Crucially,\nwe anneal the substitution rate during training, i.e.,\nwe gently switch from hypernym prediction to to-\nken prediction, following a curriculum learning\napproach. Note that this approach does not re-\nquire WordNet information at inference time nor\n1352\nincreases training time.\nOur approach is motivated by two hypotheses.\nFirstly, mapping words to their hypernyms gives\nrise to a natural gradation of difﬁculty in the pre-\ndiction task. Prior work has shown that LM bene-\nﬁts from training on instances of increasing difﬁ-\nculty (Bengio et al., 2009; Press et al., 2021). We\nthus postulate that, when coupled with the right\ncurriculum, HCP can improve LM training and per-\nplexity. Secondly, we hypothesize that HCP can\nimprove rare word generalization through implicit\ncontext sharing. Neural models still struggle to\nlearn reliable representations for rare words (Schick\nand Schütze, 2020). With CLM-based models, data\nsparsity for rare words can be abated, e.g., when\nthe representation of their contexts are potentially\ndrawn closer to those of their more frequent sib-\nlings by way of label (hypernym) sharing.\nEmpirically, the proposed method consis-\ntently yields about 0.6–1.9% relative reduction\nin perplexity over baselines on the WikiText-\n103 dataset (Merity et al., 2016), and 1.3–3.1%\non the ARXIV dataset (Lazaridou et al., 2021).\nThese improvements are observed with respect to\nmemory-augmented (Dai et al., 2019) and segment-\naware (Bai et al., 2021) LMs. Importantly, the\nproposed method improves performance for both\nrare and frequent words. We also observe that this\nis in contrast with performance improvements in\nregular LMs, which seem to be achieved at the cost\nof worsened performance on rare words.\nTo the best of our knowledge, this is the ﬁrst\nwork that shows how perplexity of Transformer\nLMs can be improved by leveraging hypernymy re-\nlationships. We provide an extensive ablation study\nhighlighting crucial elements of HCP. Amongst\nthose, we found particularly important to adopt a\ncurriculum learning approach, rather than multi-\nobjective learning or adaptive-softmax, and exclud-\ning frequent words from the hypernym prediction\ntask. We highlight the simplicity and effectiveness\nof the proposed method as our main contribution,\nand hope this study would facilitate further explo-\nration in this line of research.\n2 Related Work\nTransformer-based models are now popular lan-\nguage models. Dai et al. (2019) propose\nTransformer-XL by extending the vanilla Trans-\nformer (Vaswani et al., 2017) with a memory seg-\nment, which can encode more context tokens to\npredict the next token. Rae et al. (2020) extend\nTransformer-XL with a compressed memory seg-\nment to further encode long-time context memory.\nOther works explore different sparse Transformers\nto encode much longer sequences for LM (Beltagy\net al., 2020; Roy et al., 2021). Bai et al. (2021)\npropose a segment-aware Transformer (Segatron)\nto encode more positional information for language\nmodeling. Despite their effectiveness, neural mod-\nels still struggle to learn reliable representations\nfor rare words. Some approaches have been pro-\nposed to tackle this challenge by way of morphol-\nogy (Luong et al., 2013), lexical similarity (Khas-\nsanov et al., 2019), context similarity (Schick and\nSchütze, 2020; Khandelwal et al., 2020) and tok-\nenization (Kudo and Richardson, 2018).\nIn addition to the model modiﬁcations, other\nwork investigated curriculum learning to train LMs.\nBengio et al. (2009) ﬁrst ﬁnd that curriculum learn-\ning could beneﬁt LM training by training with high-\nfrequency tokens ﬁrst and low-frequency tokens\nlater. Wu et al. (2021) ﬁnd that curricula works\nwell when the training data is noisy or the training\ndata is too large to iterate multiple epochs. Press\net al. (2021) ﬁnd that training Transformer-based\nLMs with short sequences ﬁrst could improve con-\nvergence speed and perplexity.\nRelated work aimed at integrating WordNet in-\nformation into pretrained language models. Levine\net al. (2020) propose SenseBERT by adding the\nword sense (WordNet supersense) prediction as an\nadditional task during BERT (Devlin et al., 2019)\npre-training. SenseBERT outperforms BERT on\nboth word supersense disambiguation (Raganato\net al., 2017) task and word in context (Pilehvar and\nCamacho-Collados, 2019) task. Recently, Porada\net al. (2021) use WordNet hypernymy chains as in-\nput to a Roberta (Liu et al., 2019) model to predict\nthe plausibility of input events. In this work, our\nfocus is to improve performance of auto-regressive\nLMs. We show that a multi-task strategy harms\nperformance in this setting, and give a successful\nrecipe to consistently boost LM performance with\nclass-based predictions.\n3 Method\nCoupling class-based LM (CLM) and curriculum\nlearning, HCP is to gradually anneal class predic-\ntion to token prediction during LM training. In this\nsection, we ﬁrst describe how we instantiate word\nclasses by leveraging hypernym relation from the\n1353\nEntity.n.01physical_entity.n.01 matter.n.03 substance.n.01chemical_element.n.01\nabstraction.n.06relation.n.01 part.n.01\nEntity.n.01physical_entity.n.01 matter.n.03 substance.n.01chemical_element.n.01\niron.n.01abstraction.n.06relation.n.01 part.n.01\nmetallic_element.n.01\nEntity.n.01physical_entity.n.01 object.n.01whole.n.02artifact.n.01instrumentality.n.03\nfurnishing.n.02furniture.n.01table.n.02desk.n.01\nmagnesium.n.01\nmetallic_element.n.01\nFigure 2: Hypernym-paths of synsets “magnesium.n.01”, “iron.n.01”, and “desk.n.01”, corresponding to the word\nmagnesium, iron, and desk respectively.\ndef token2class(token2freq, d, f):\n# token2freq is a dictionary whose key is the token and\nvalue is the tokens/quotesingle.ts1occurrences)\n# d is the depth, f is the occurrence threthold\nrtn = {}\nfor token, freq in token2freq.items():\nif freq > f:\ncontinue\nfor synset in wordnet.synsets(token):\nfor path in synset.hypernym_paths():\nif len(path)>=d and /quotesingle.ts1noun/quotesingle.ts1in path[d=1]:\nrtn[token] = path[d=1]\nbreak\nif token in rtn:\nbreak\nreturn rtn\nCode 1: Pseudocode for token to class mapping.\nWordNet. We then present how to incorporate the\nproposed Hypernym Class Prediction task into LM\ntraining via curriculum learning.\n3.1 Hypernymy as Word Classes\nWordNet (Miller, 1995) is a lexical database that\ngroups words into sets of cognitive synonyms\nknown as synsets, which are in turn organized into\na directed graph by various lexical relations includ-\ning the hypernymy ( is-a) relation. As shown in\nFigure 2, each vertex is a synset, labeled by the\ntext within the box, and each edge points from the\nhypernym (supertype) to the hyponym (subtype).\nNote that a word form (spelling) may be associated\nwith multiple synsets – each corresponding to a\ndifferent sense of the word, which are sorted by\nthe frequency of the sense estimated from a sense-\nannotated corpus. For example, iron has 6 synsets,\namong which “iron.n.01” is the most common one.\nHence, if two words share the same hypernym\nat a certain level in their hypernym-paths (to the\nroot in WordNet), we could say they are similar\nat that level. Here we use \"Depth\" to quantify the\nhypernym-path level. In Figure 2, for example, at\nDepth 6, iron and magnesium are mapped to the\nsame group named “metallic_element.n.01”, while\ndesk is mapped to “instrumentality.n.03”. At Depth\n2, all these three words share the same (indirect)\nhypernym “physical_entity.n.01”.\nIn this work, we map each token in our training\nset into its hypernym class if this token (1) has a\nnoun synset in the WordNet, (2) with a hypernym-\npath longer than a given depth d, and (3) has fre-\nquency below a given threshold f in the training\ncorpus. We only consider nouns because it is not\nonly the most common class in the WordNet but\nalso a difﬁcult class for LMs to learn (Lazaridou\net al., 2021). For tokens with multiple synsets, we\niterate over the synsets in the order of sense fre-\nquency and break the loop once found. We select\nthe most frequent synset no less than the required\ndepth. The mapping pseudocode is illustrated in\nCode 1, which is a data pre-processing algorithm\nconducted only once before the training and takes\nno more than 5 minutes in our implementation.\n3.2 Hypernym Class Prediction\nWe ﬁrst partition the vocabulary into Vx and V¬x\nbased on whether or not a token has a hypernym\nin the WordNet, and Vh denotes the set of all hy-\npernyms. The original task in a Transformer-based\nLM is then to predict the token wj’s probability\nwith the output x from the last layer:\nP(y = wj|x) =\nexp(xTvwj )∑\nwk∈Vx∪V¬x exp(xTvwk ) (1)\nwhere wk is the kth word in the original vocabu-\nlary and vwk is its embedding. Here we assume\nthe output layer weights are tied with the input em-\n1354\n0 20k 40k ... ... N\nTraining Steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0HCP probability\nb\na*N\nconstant\nlinear-decay\nFigure 3: Probabilities of HCP step over training pro-\ncess with different pacing functions.\nbeddings. We call any training step predicted with\nEq. 1 a token prediction step.\nTo do the Hypernym Class Prediction step, we\nreplace all tokens in Vx in a batch of training data\nwith their corresponding hypernym classes in Vh.\nAfter the replacement, only hypernym classes in\nVh and tokens in V¬x can be found in that batch.\nThen, the LM probability prediction becomes:\nP(y = wj|x) =\nexp(xTvwj )∑\nwk∈Vh∪V¬x exp(xTvwk ) (2)\nwhere wj could be either a token or a hypernym\nclass. We called this batch step is a Hypernym\nClass Prediction (HCP) step.\nNote that Eq. 2 is different from the multi-\nobjective learning target, where the hypernym class\nwould be predicted separately:\nP(y = wj|x) =\nexp(xTvwj )∑\nwk∈Vh exp(xTvwk ) (3)\nwhere wj is a hypernym class. We will elaborate\non this difference in the experiment results part.\n3.3 Training Method\nWe train a LM by switching from HCP to token pre-\ndiction. For the example in Figure 2, our target is to\nteach a model to distinguish whether the next token\nbelongs to the metallic element class or instrumen-\ntality class during the earlier stage in training, and\nto predict the exact word from magnesium, iron,\nand desk later.\nInspired by Bengio et al. (2009), we choose cur-\nriculum learning to achieve this. Curriculum learn-\ning usually deﬁnes a score function and a pacing\nfunction, where the score function maps from a\ntraining example to a difﬁculty score, while the\npacing function determines the amount of the easi-\nest/hardest examples that will be added into each\nepoch. We use a simple scoring function which\ntreats HCP as an easier task than token prediction.\nTherefore, there is no need to sort all training ex-\namples. The pacing function determines whether\nthe current training step is a HCP step, i.e. whether\ntokens will be substituted with their hypernyms.\nOur pacing function can be deﬁned as:\nP(y = c|t) =\n{ b t < a∗N\n0 t ≥a ∗N (4)\nor\nP(y = c|t) =\n{ b −b ∗ t\na∗N t < a∗N\n0 t ≥a ∗N (5)\nwhere P(y = c|t) is the probability that the current\nstep t is a hypernym class prediction step. N is the\ntotal training steps. a and b are hyper-parameters.\nSo, Eq. 4 is a constant pacing function in the ﬁrsta∗\nN steps, while Eq. 5 is a linear decay function. We\nplot these two functions in Figure 3. According to\nour experimental results Tab. 5, these two functions\nare both effective in improving the language model.\n4 Experiments\nWe conduct experiments on two datasets.\nWikiText-103(Merity et al., 2016) is a large word-\nlevel dataset with long-distance dependencies for\nlanguage modeling. There are 103M tokens and\n28K articles (3.6K tokens per article on average).\nThe original vocabulary size is 271121, among\nwhich we ﬁnd 3383 hypernym classes for 71567\ntokens with d = 6 and f = 6000 (Section 3.1).\nARXIV (Lazaridou et al., 2021) is collected from\npublicly available arXiv abstracts1 with an average\nof 172 words per abstract and partitioned into\ntraining (1986–Sept 2017), evaluation (Aug–Dec\n2017), and test (2018–2019). Following Lazaridou\net al. (2021), we use the BPE (Sennrich et al., 2015)\ntokenization for this dataset. The ﬁnal vocabulary\nsize is 48935, and we ﬁnd 1148 hypernym classes\nfor 5969 tokens among the vocabulary with d = 6\nand f = 1000.\nSeveral variants of the Transformer model have\nbeen used for our experiments:\n• small model: 12 layers, 10 heads, hidden size\n300, batch size 256, training steps 100k;\n• base model: 16 layers, 10 heads, hidden size\n410, batch size 64, training steps 200k;\n1https://arxiv.org/help/oa/index\n1355\nModel #Param. Valid PPL Test PPL\nLSTM+Neural cache (Grave et al., 2017b) - - 40.8\nTransformer small 91M 34.5 36.5\n+ HCP 34.1 35.9\nTransformer base 151M 29.2 30.7\n+ HCP 29.1 30.2\nTransformer-XL base, M=150 (Dai et al., 2019) 151M - 24.0\nSegatron-XL base (Bai et al., 2021), M=150 151M - 22.5\n+ HCP 21.9 22.1\nTransformer Large 257M 24.0 25.8 (80k steps)\n+ HCP 23.7 25.3 (80k steps)\nAdaptive Input (Baevski and Auli, 2019) 247M - 18.7 (286k steps)\nTransformer-XL large, M=384 (Dai et al., 2019) 257M - 18.3 (400k steps)\nCompressive Transformer, M=1024 (Rae et al., 2020) 257M 16.0 17.1 (400k steps)\nSegatron-XL large, M=384 (Bai et al., 2021) 257M - 17.1 (350k steps)\n+ HCP 16.1 17.0 (350k steps)\nTable 1: Results on WikiText-103 dataset with different models.\n• large model: 18 layers, 16 heads, hidden size\n1024 batch size 128.\nThe input lengths are 150 for the base model\nand 384 for the large model. The memory\nlength is equal to the input length for both train-\ning and testing. The hyper-parameters used for\nthe ARXIV dataset are as same as the WikiText-\n103, except the ARXIV base model’s input length\nis 384. The number of training steps varies greatly\nfor the large model in previous work, so we experi-\nment on both the lower (80k) higher (350k) ends.\n4.1 Main results\nOur main results are shown in Table 1. We can\nsee that all architectures could beneﬁt from HCP:\nTransformer-small improved 0.6 ppl, Transformer-\nbase improved 0.5, Segatron-XL base improved\n0.4, Transformer-large improved 0.5, and Segatron-\nXL large improved 0.1. We also plot the validation\nperplexities of small and large models trained with\nand without HCP in Figure 4. In the beginning, the\nperplexity of the HCP models is higher due to the\nmixed training steps from the two tasks, but we can\nsee that HCP perplexity goes down faster than the\nbaseline method. And after fully switching to token\nprediction, HCP outperforms the baseline method\nquickly and the gap between these two methods\nremains stable. These results suggest that HCP is\nindeed effective in improving LM training.\nFor experiments on the ARXIV dataset, we ﬁrst\n20k 40k 60k 80k\nTraining Steps\n33\n34\n35\n36\n37\n38\n39\n40Valid PPL\nTransformer-small\nTransformer-small-HCP\n20k 40k 60k 80k\nTraining Steps\n23\n24\n25\n26\n27\n28\n29\n30Valid PPL\nTransformer-large\nTransformer-large-HCP\nFigure 4: Valid perplexity curves during the training of\nsmall and large models with WikiText-103\ncompare the Segatron-XL base model trained with\nand without HCP. The results are shown in Table 2.\nThe improvements over the validation set and test\nset are 0.6 and 0.75 respectively. For the large\nmodel, we use the same model architecture and\n1356\nModel #Param. Valid PPL Test PPL\nSegatron-XL base 59M 22.39 24.21\n+ HCP 21.79 23.46\nTransformer-XL large (Lazaridou et al., 2021) 287M - 23.07\nSegatron-XL large 283M 21.28 22.99 (80k steps)\n+ HCP 283M 20.93 22.60 (80k steps)\nTable 2: Results on ARXIV dataset with different models.\n0 12k 24k 36k 48k 60k\nTraining Steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0HCP probability\npacing function\n12k 24k 36k 48k 60k\nTraining Steps\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n4.0Valid log(PPL)\nbaseline_freq<500\nHCP_freq<500\n12k 24k 36k 48k 60k\nTraining Steps\n3.6\n3.7\n3.8\n3.9\n4.0Valid log(PPL)\nbaseline_freq<300\nHCP_freq<300\n12k 24k 36k 48k 60k\nTraining Steps\n4.0\n4.2\n4.4\n4.6\n4.8\n5.0\n5.2Valid log(PPL)\nbaseline_freq<100\nHCP_freq<100\n12k 24k 36k 48k 60k\nTraining Steps\n4.4\n4.6\n4.8\n5.0\n5.2\n5.4Valid log(PPL)\nbaseline_freq<50\nHCP_freq<50\n12k 24k 36k 48k 60k\nTraining Steps\n4.8\n5.0\n5.2\n5.4Valid log(PPL)\nbaseline_freq<20\nHCP_freq<20\nFigure 5: Frequency-stratiﬁed validation log(perplexity) of baseline model (Transformer-small) and HCP\nmodel (Transformer-small-HCP) with WikiText-103.\nhyper-parameters as theWikiText-103large model\nbut change the vocabulary to BPE sub-tokens. The\nﬁnal perplexity outperforms its counterparts about\n0.4 and outperforms a larger model trained with\n1024 input sequence length over 0.47, while our\nmodel length is 384.\n4.2 Generalization on Rare Tokens\nIn addition to the overall perplexity comparison, we\nalso conduct comparisons with frequency-stratiﬁed\nvalidation subsets, to show the perplexity of tokens\nthat has been replaced with the hypernym classes\nduring training. Results are shown in Figure 5. We\ncan see that, after the ﬁrst 12k hypernym class pre-\ndiction steps, there is a large gap between our HCP\nmodel and the baseline model as the HCP model\nonly learn to predict the hypernym class instead of\nthe token itself. After that, in the next 12k steps,\nHCP’s PPL decreases faster, achieves similar PPL\nat 24k steps, and ﬁnally outperforms the baseline\nmethod in all frequency groups. The results show\nthat our proposed training method can beneﬁt the\nlearning of the replaced tokens in various frequen-\ncies. Strikingly, we observe that, for the baseline,\nmore training steps lead to a degradation of per-\nformance for rare tokens, a behavior that deserves\ninvestigation in future work.\nWe further conduct pairwise model comparisons\nwith tokens that have been replaced during HCP\ntraining on the WikiText-103 test set. Given two\nmodels, we compare the prediction probabilities\nfor each occurrence of a target token, and register\na “win” for the model with a higher probability.\nWe then calculate the percentage of winnings (as\nwell as ties) for each model by tallying over all\noccurrences of the token. The results are then strat-\niﬁed by token frequency and plotted in Figure 6.\nThe better model is placed on the right in both\n1357\nall_tokens\nfreq<1000\nfreq<500\nfreq<400\nfreq<300\nfreq<100\nfreq<50\nfreq<30\nfreq<20\nfreq<10\nfreq<5\n0% 25% 50% 75% 100%\nBaseline_better Indistinguishable HCP_LM_better\n(a) Baseline model and HCP model\nall_tokens\nfreq<1000\nfreq<500\nfreq<400\nfreq<300\nfreq<100\nfreq<50\nfreq<30\nfreq<20\nfreq<10\nfreq<5\n0% 25% 50% 75% 100%\nSubOpt_better indistinguishable Baseline_better (b) Baseline model and sub-optimal model\nFigure 6: Pairwise comparison results. The baseline model and HCP model are trained without and with hypernym\nclass prediction respectively. The sub-optimal model is trained without HCP and trained with different hyper-\nparameters, whose perplexity is increased by 0.9 compared with the baseline model.\nsub-ﬁgures.\nIn Figure 6(a), we see that HCP outperforms\nthe baseline model on all frequency strata. Inter-\nestingly, the performance gap widens as frequency\ndecreases, indicating that HCP is beneﬁcial in mod-\neling rare tokens. In Figure 6(b), we compare the\nbaseline model against an under-optimized model\nof identical architecture but slightly different hyper-\nparameters.2 Here, the (optimal) baseline outper-\nforms the sub-optimal model on all but the least\nfrequent stratum, suggesting the possibility that per-\nplexity reduction (resulting from hyperparameter\ntuning in this case) might be achieved by improv-\ning frequent word prediction at the expense of rare\nwords. This is inline with observations made re-\ncently in vision tasks (Sagawa et al., 2020).\n4.3 Ablation study\nWe conduct ablation studies with WikiText-\n103 dataset and Transformer small model to in-\nvestigate how to map words to hypernym classes,\nhow to select curriculum learning pacing functions\nand to show why we use curriculum training.\n4.3.1 Hypernym-path Depth\nThe hypernym classes are chosen from the\nhypernym-paths in WordNet. Considering that a\nhypernym-path consists of multiple hypernyms, it\n2The sub-optimal model has batch size 128 instead of the\noptimal 64, and the perplexity gap between these two models\nis observed to be slightly larger than that between HCP and\nthe baseline (0.9 vs 0.5).\nDepth Valid PPL #Classes\nBaseline 34.5 0\nd = 4 34.54 145\nd = 5 34.29 1169\nd = 6 34.05 3383\nd = 7 34.37 6604\nd = 8 34.25 9063\nTable 3: Clustering words into classes with different\nlayer’s hypernym parents. The average depth is 8.03.\n#Classes denotes the total number of hypernym classes.\nis not straightforward to tell which layer is the best.\nBut the best depth d should be some layer in the\nmiddle. Because a small depth might map multi-\nple distant words into the same class, while a large\ndepth will result in too many classes which are hard\nfor a model to learn. The extreme examples could\nbe d = 1 and d = ∞, corresponding to mapping\nall candidate words into the class “Entity.n.01” and\nmapping each word into itself respectively. In Ta-\nble 3, we show evaluation results among different\ndepth selections. We ﬁnd that depth 6th is the best\nchoice, with the lowest valid perplexity. The re-\nsults also conﬁrm our assumption that the best one\nwould be some middle layer.\n4.3.2 Filter Frequency\nIn addition to the hypernym-path depth, we also\ninvestigate how to select frequency thresholdf. As\nwe mentioned above, our target is to map similar\n1358\nFilterFreq. Valid PPL #Rep.\nBaseline 34.5 0\nf = 3000 34.14 70859\nf = 5000 34.50 71735\nf = 6000 34.05 71971\nf = 7000 34.32 72153\nf = 8000 34.35 72291\nf = ∞ 40.10 73067\nTable 4: Ignoring words whose frequency more than a\nthreshold f during hypernym class clustering. #Rep.\ndenotes the number of tokens in the vocabulary that\nwill mapped.\nwords into the same class, where predicting a hyper-\nnym class might be easier than predicting multiple\ndifferent words. After the mapping process, low-\nfrequency words can be clustered into hypernym\nclasses with higher frequency. Table 4 shows the\nresults of different f. We can see that f = 6000\nachieves the best results while f = ∞(without\nﬁlter) is the worst. We hypothesize this might be\ndue to two reasons. First, for some high-frequency\ncommon words, the model can learn them well al-\nready, while mapping them into hypernym classes\nmay be superﬂuous or even harmful. Second, in-\ncluding frequent words skews the marginal distri-\nbution over hypernym classes, causing hypernym\nprediction to be more class-imbalanced, which in\nturn might lead to collapsed representation in the\nresulting LM (Fang et al., 2021). This hypothesis\ndeserves further investigation. It should be noted\nthat although the difference of #Rep.Tokens looks\nminor, the difference in the token’s appearance is\nsigniﬁcant. For example, f = ∞maps only 776\nadditional tokens compared with f = 8000, but\neach token’s appearance is more than 8000, which\nexplains the different perplexities in Table 4.\n4.3.3 Pacing Function\nTable 5 shows the results of models trained with var-\nious curriculum pacing functions. We also report\nthe validation perplexities of the tokens that have\never been replaced with hypernym class (Rep.PPL)\nduring training and tokens without hypernym\nclass (NonRep.PPL).\nFor the constant pacing function, we ﬁx b = 1\nand change the value of a, In this case, the models\nare always training with HCP in the ﬁrst a ∗100k\nsteps and then switch to the token prediction train-\ning, which is a pre-training pacing function. We can\nsee that all models outperform the baseline model\nover the validation perplexity. Rep.PPL improves\nfrom 348 to 339. The perplexity of NonRep.PPL\nbetween baseline model and HCP models are sim-\nilar, except the model trained with a = 4, which\nindicates the pre-training should not take up too\nmany steps.\nFor the linear pacing function, we choose some\nspeciﬁc a and b to achieve the same HCP steps as\nthe constant functions above. For simplicity, we\nalso set a = b. In Table 5, we can see that the over-\nall perplexity of the linear functions is similar to the\ncorresponding constant functions, where the Non-\nRep.PPL is slightly decreased while the Rep.PPL\nis slightly increased. We conduct a grid search over\ndifferent pacing functions with Transformer small\nmodel and WikiText-103, and ﬁnally, use the con-\nstant function with a = 0.12 and b = 0.8 for all\nbase models and large models.\nCurriculum hyper-parameters could be trans-\nferred to the ARXIV dataset successfully. However,\nwe tune the frequency threshold f on each dataset,\nbecause different tokenization methods change the\nfrequency distribution. All HCP models in Table 2\nare using d = 6, f = 1000, and the constant pacing\nfunction with a = 0.12 and b = 0.8.\n4.3.4 Other Training Objectives\nWe also experimented with two other methods to in-\ncorporate hypernym information into LM training.\nAlthough neither method has yielded any empiri-\ncal gain, we nonetheless report these methods and\noffer possible explanations for their failure.\nMulti-objective Training Multi-objective (or\nmulti-task) training consists in a weighted sum of\ntoken and hypernym prediction losses. We set the\nweight of the hypernym prediction loss to 0.2. The\nprediction of a token is calculated with Eq. 1. The\nprediction of a hypernym class is calculated with\nEq. 3, where x can be the output vector from any\nlayer in the Transformer LM. Table 6 lists the re-\nsults using the last layer and the 8th layer. Using\nthe last layer signiﬁcantly undermines the original\ntoken prediction results. Using the 8th layer is bet-\nter but the ﬁnal perplexity is still no better than\nthe baseline model. Simply forcing the language\nmodel to predict the hypernym class for each token\nis harmful to LM performance. We also tried to re-\nplace Eq. 3 with Eq. 2, by mixing Vh and V¬w to-\ngether when predicting the hypernym classes (mix\nvocab). This signiﬁcantly improves multi-objective\n1359\nConstant Func. HCP steps Valid PPL NonRep.PPL Rep.PPL\na=0 b=0 0 34.5 22.07 348.87\na=0.1 b=1 10k 34.18 22.08 339.30\na=0.2 b=1 20k 34.15 22.07 339.34\na=0.3 b=1 30k 34.26 22.07 338.14\na=0.4 b=1 40k 34.39 22.26 338.31\nLinear Func.\na=0.45 b=0.45 10k 34.14 22.04 340.55\na=0.64 b=0.64 20k 34.05 21.96 341.33\na=0.78 b=0.78 30k 34.26 22.05 346.77\na=0.90 b=0.90 40k 34.56 22.12 354.40\nTable 5: Training N steps hypernym class prediction among 100k training steps with different pacing functions.\nNonRep.PPL denotes non-replaced tokens’ perplexity, and Rep.PPL denotes replaced tokens’ perplexity.\nValid PPL Test PPL NonRep.PPL Rep.PPL\nBaseline 34.50 36.46 22.07 348.87\nAdaptive Softmax 36.32 38.16 22.48 435.93\nMulti-obj\nlast layer 46.06 48.49 27.81 627.23\n8th layer 43.42 45.37 26.13 597.66\n8th layer + mix vocab 35.97 38.02 22.98 365.27\nHypernym Class Prediction 34.05 35.87 21.96 341.33\nTable 6: Results obtained by alternative strategies for integrating hypernymy information into the LM: adaptive\nsoftmax and multi-objective training. Both under-perform the proposed HCP method.\ntraining. Learning to predict the hypernym class\nfrom a mixed vocabulary Vh ∪V¬w is better than\nonly hypernym classes Vh.\nAdaptive Softmax Another method is the\nadaptive-softmax (Grave et al., 2017a), where the\nmodel ﬁrst predict the hypernym probability among\nVh ∪V¬w and then predict the token probability\namong the tokens with the same hypernym class.\nIn Table 6, we can see that the adaptive-softmax\nis no better than the multi-objective trained model.\nBy looking into the poor perplexity of Rep.PPL,\nwe ﬁnd this method cannot improve the prediction\nof tokens in Vw. We believe this is due to the noise\nof hypernym class mapping, where we choose the\nﬁrst synset path as the token’s hypernym synset\nwithout considering the context. Such noise will\naffect the adaptive-softmax prediction but is not an\nissue for curriculum training as the ﬁnal training\nstage is fully trained with the original text.\n5 Conclusion\nIn this work, we propose a new LM training strat-\negy with WordNet’s super-subordinate relation and\ncurriculum learning. Although WordNet is an ex-\nternal resources, it’s not clear how to get lower\nperplexity using WordNet before this work. Con-\nsistent perplexity reduction can be observed over\nvarious models. Both rare and frequent tokens can\nbe modeling better with our proposed method while\nother optimization method may sacriﬁce the perfor-\nmance on rare tokens.\nWe’d like to address the limitations of this work:\nother methods to map words to classes; LM experi-\nments with other languages; pre-training LM with\nour proposed method and testing on downstream\ntasks. We hope to investigate these directions in\nthe future.\n1360\nReferences\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions.\nHe Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan,\nKun Xiong, Wen Gao, and Ming Li. 2021. Sega-\ntron: Segment-aware transformer for language mod-\neling and understanding. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 35(14):12526–\n12534.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. The journal of machine learning re-\nsearch, 3:1137–1155.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum Learning.\nIn Proceedings of the 26th Annual International\nConference on Machine Learning , ICML ’09, page\n41–48, New York, NY , USA. Association for Com-\nputing Machinery.\nPeter F. Brown, Vincent J. Della Pietra, Peter V . deS-\nouza, Jenifer C. Lai, and Robert L. Mercer. 1992.\nClass-based n-gram models of natural language.\nComputational Linguistics, 18(4):467–480.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nIdo Dagan, Lillian Lee, and Fernando CN Pereira.\n1999. Similarity-based models of word cooccur-\nrence probabilities. Machine learning, 34(1):43–69.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nYuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam,\nand Marc’Aurelio Ranzato. 2020. Residual energy-\nbased models for text generation. In International\nConference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nCong Fang, Hangfeng He, Qi Long, and Weijie J. Su.\n2021. Exploring deep neural networks via layer-\npeeled model: Minority collapse in imbalanced\ntraining. Proceedings of the National Academy of\nSciences, 118(43).\nÉdouard Grave, Armand Joulin, Moustapha Cissé,\nDavid Grangier, and Hervé Jégou. 2017a. Efﬁcient\nsoftmax approximation for GPUs. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1302–1310. PMLR.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving neural language models with a\ncontinuous cache. In ICLR 2017, Toulon, France,\nApril 24-26, 2017.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nYerbolat Khassanov, Zhiping Zeng, Van Tung Pham,\nHaihua Xu, and Eng Siong Chng. 2019. Enriching\nrare word representations in neural language mod-\nels by embedding matrix augmentation. Interspeech\n2019.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nSebastian Ruder, Dani Yogatama, et al. 2021. Pit-\nfalls of static language modelling. arXiv preprint\narXiv:2102.01951.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. SenseBERT:\nDriving some sense into BERT. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 4656–4667, On-\nline. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\n1361\nThang Luong, Richard Socher, and Christopher Man-\nning. 2013. Better word representations with re-\ncursive neural networks for morphology. In Pro-\nceedings of the Seventeenth Conference on Computa-\ntional Natural Language Learning , pages 104–113,\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nGeorge A. Miller. 1995. WordNet: A lexical database\nfor English. Commun. ACM, 38(11):39–41.\nAndriy Mnih and Geoffrey Hinton. 2007. Three new\ngraphical models for statistical language modelling.\nIn Proceedings of the 24th International Conference\non Machine Learning , ICML ’07, page 641–648,\nNew York, NY , USA. Association for Computing\nMachinery.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nInternational workshop on artiﬁcial intelligence and\nstatistics, pages 246–252. PMLR.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning represen-\ntations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1267–1273, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nIan Porada, Kaheer Suleman, Adam Trischler, and\nJackie Chi Kit Cheung. 2021. Modeling event plau-\nsibility with consistent conceptual abstraction. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 1732–1743, Online. Association for Compu-\ntational Linguistics.\nOﬁr Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using\nshorter inputs. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5493–5505, Online. Association for\nComputational Linguistics.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learn-\ning Representations.\nAlessandro Raganato, Jose Camacho-Collados, and\nRoberto Navigli. 2017. Word sense disambiguation:\nA uniﬁed evaluation framework and empirical com-\nparison. In Proceedings of the 15th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers , pages\n99–110, Valencia, Spain. Association for Computa-\ntional Linguistics.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efﬁcient content-based\nsparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:53–68.\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and\nPercy Liang. 2020. An investigation of why over-\nparameterization exacerbates spurious correlations.\nIn International Conference on Machine Learning ,\npages 8346–8356. PMLR.\nTimo Schick and Hinrich Schütze. 2020. Rare words:\nA major problem for contextualized representation\nand how to ﬁx it by attentive mimicking. In Pro-\nceedings of the Thirty-Fourth AAAI Conference on\nArtiﬁcial Intelligence.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nXiaoxia Wu, Ethan Dyer, and Behnam Neyshabur.\n2021. When do curricula work? In International\nConference on Learning Representations.\nZachary M. Ziegler and Alexander M. Rush. 2019. La-\ntent normalizing ﬂows for discrete sequences.\n1362",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8769111633300781
    },
    {
      "name": "Computer science",
      "score": 0.8220131397247314
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6346392631530762
    },
    {
      "name": "Transformer",
      "score": 0.6160334348678589
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5926822423934937
    },
    {
      "name": "Language model",
      "score": 0.5740702152252197
    },
    {
      "name": "WordNet",
      "score": 0.5572696328163147
    },
    {
      "name": "Security token",
      "score": 0.5186668634414673
    },
    {
      "name": "Natural language processing",
      "score": 0.48467662930488586
    },
    {
      "name": "Machine learning",
      "score": 0.4702151119709015
    },
    {
      "name": "Context (archaeology)",
      "score": 0.41626009345054626
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}