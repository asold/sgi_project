{
  "title": "Distributed training of foundation models for ophthalmic diagnosis",
  "url": "https://openalex.org/W4406693520",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5108849568",
      "name": "Sina Gholami",
      "affiliations": [
        "University of North Carolina at Charlotte"
      ]
    },
    {
      "id": "https://openalex.org/A5010405454",
      "name": "Fatema-E Jannat",
      "affiliations": [
        "University of North Carolina at Charlotte"
      ]
    },
    {
      "id": "https://openalex.org/A5055312303",
      "name": "Atalie C. Thompson",
      "affiliations": [
        "Wake Forest University"
      ]
    },
    {
      "id": "https://openalex.org/A5102533124",
      "name": "Sally Shin Yee Ong",
      "affiliations": [
        "Wake Forest University"
      ]
    },
    {
      "id": "https://openalex.org/A5026587624",
      "name": "Jennifer I. Lim",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5035477663",
      "name": "Theodore Leng",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5115978974",
      "name": "Hamed Tabkhivayghan",
      "affiliations": [
        "University of North Carolina at Charlotte"
      ]
    },
    {
      "id": "https://openalex.org/A5072253597",
      "name": "Minhaj Nur Alam",
      "affiliations": [
        "University of North Carolina at Charlotte"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2037780881",
    "https://openalex.org/W3110220146",
    "https://openalex.org/W3199869910",
    "https://openalex.org/W2036910617",
    "https://openalex.org/W2032687008",
    "https://openalex.org/W4388347813",
    "https://openalex.org/W4281694288",
    "https://openalex.org/W4366281042",
    "https://openalex.org/W3184618443",
    "https://openalex.org/W2589074029",
    "https://openalex.org/W2768948787",
    "https://openalex.org/W2973920946",
    "https://openalex.org/W2985431718",
    "https://openalex.org/W2952162556",
    "https://openalex.org/W2772059204",
    "https://openalex.org/W2891146096",
    "https://openalex.org/W2796809202",
    "https://openalex.org/W2901158775",
    "https://openalex.org/W2772246530",
    "https://openalex.org/W2073244572",
    "https://openalex.org/W2561588396",
    "https://openalex.org/W61099616",
    "https://openalex.org/W2557738935",
    "https://openalex.org/W2893365278",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4387225742",
    "https://openalex.org/W4386697749",
    "https://openalex.org/W4396214501",
    "https://openalex.org/W4365452246",
    "https://openalex.org/W4400229603",
    "https://openalex.org/W4394969664",
    "https://openalex.org/W4387580598",
    "https://openalex.org/W4400251867",
    "https://openalex.org/W4391901171",
    "https://openalex.org/W4309586674",
    "https://openalex.org/W3088234149",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W3046183292",
    "https://openalex.org/W4307323645",
    "https://openalex.org/W3196371845",
    "https://openalex.org/W4229029907",
    "https://openalex.org/W6752029299",
    "https://openalex.org/W3047304572",
    "https://openalex.org/W4295939964",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4394619956",
    "https://openalex.org/W4200469286",
    "https://openalex.org/W3087413301",
    "https://openalex.org/W4281789486",
    "https://openalex.org/W4318566866",
    "https://openalex.org/W4403650311",
    "https://openalex.org/W4403015917",
    "https://openalex.org/W4388265368",
    "https://openalex.org/W2979798680",
    "https://openalex.org/W3204892857",
    "https://openalex.org/W4388290915",
    "https://openalex.org/W4394728153",
    "https://openalex.org/W2788633781",
    "https://openalex.org/W2013754472",
    "https://openalex.org/W4391289335",
    "https://openalex.org/W2996651271"
  ],
  "abstract": null,
  "full_text": "communicationsengineering Article\nhttps://doi.org/10.1038/s44172-025-00341-5\nDistributed training of foundation models\nfor ophthalmic diagnosis\nCheck for updates\nSina Gholami 1,F a t e m a - EJ a n n a t1, Atalie Carina Thompson2, Sally Shin Yee Ong2,J e n n i f e rI .L i m3,\nTheodore Leng 4,H a m e dT a b k h i v a y g h a n1 &M i n h a jN u rA l a m1\nVision impairment affects nearly 2.2 billion people globally, and nearly half of these cases could be\nprevented with early diagnosis and intervention— underscoring the urgent need for reliable and\nscalable detection methods for conditions like diabetic retinopathy and age-related macular\ndegeneration. Here we propose a distributed deep learning framework that integrates self-supervised\nand domain-adaptive federated learning to enhance the detection of eye diseases from optical\ncoherence tomography images. We employed a self-supervised, mask-based pre-training strategy to\ndevelop a robust foundation encoder. This encoder was trained on seven optical coherence\ntomography datasets, and we compared its performance under local, centralized, and federated\nlearning settings. Our results show that self-supervised methods— both centralized and federated—\nimproved the area under the curve by at least 10% compared to local models. Additionally,\nincorporating domain adaptation into the federated learning framework further boosted performance\nand generalization across different populations and imaging conditions. This approach supports\ncollaborative model development without data sharing, providing a scalable, privacy-preserving\nsolution for effective retinal disease screening and diagnosis in diverse clinical settings.\nT h eg l o b a lp r e v a l e n c eo fo p h t h a l m i cd i seases, such as diabetic retinopathy\n(DR) and age-related macular degeneration (AMD), leading to vision\nimpairment (VI) and blindness, has increased steadily over time due to the\naging of the population, widespread use of technology and the growing\nnumber of diabetes cases worldwide\n1– 5. Currently, VI affects nearly 2.2\nbillion people globally, of whom almost 1 billion could have been spared\nthrough early diagnosis and intervention6. Detecting individuals at risk from\nthe earliest stages is vital because timely treatment can prevent or slow\ndisease progression and avert irreversible vision loss7,8.M a n yo p h t h a l m i c\nconditions cause structural damage before patients notice visual symptoms,\nunderscoring the importance of early intervention. Deep learning (DL)\ntechniques have shown considerable potential in improving diagnostic\naccuracy and speed, both crucial for timely and effective treatment\n9,10.\nVarious studies11– 18 have successfully differentiated between normal and\npathological optical coherence tomography (OCT) images, identifying\nconditions such as AMD, DR, choroidal neovascularization (CNV), and\ndiabetic macular edema (DME). Other studies have leveraged machine\nlearning (ML) and DL methodologiesto pinpoint risk factors for DR and\nAMD progression, achieving high accuracy, sensitivity, and speciﬁcity.\n19– 27.\nThese ﬁndings highlight the capabilityof DL to enhance automated vision\nscreening processes and facilitate the creation of computer-aided diag-\nnostic tools.\nIn recent years, foundation models have utilized self-supervised\nlearning (SSL) as a pre-training strategy, allowing models to extract\nmeaningful representations from unlabeled data by understanding its\nunderlying structure\n28– 31. Such models are often more robust and can gen-\neralize well on a wide spectrum of data and downstream tasks (i.e., diagnosis,\nprognosis, segmentation, and classiﬁcation). In our recent work, we\ndemonstrated that a contrastive learning (CL) based SSL method enhanced\nmodel representation learning capability, and improved DR classiﬁcation\nperformance using fundus images\n32. However, the CL was less effective in\ngrayscale images such as OCT. Mask imaging modeling (MIM)33 is a\nrecently developed SSL technique that inherently improves the model’s\ncapability of learning images’ nuances and their pathologies (even in\ngrayscale images)34,35, and recent literature shows the growing signiﬁcance of\nSSL techniques like MIM in ophthalmology-focused DL research. Yet, the\nefﬁcacy of these technologies in practical settings is often compromised by\nthe homogeneity of the training datasets, leading to failure in clinical\ndeployment. For these algorithms to be truly effective within the clinical\nworkﬂow, they require access to extensive, diverse datasets from multiple\n1Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC, USA.2Department of Ophthalmology, Wake Forest\nSchool of Medicine, Winston-Salem, NC, USA.3Department of Ophthalmology and Visual Science, University of ILlinois at Chicago, Chicago, IL, USA.4Byers Eye\nInstitute at Stanford, Stanford University School of Medicine, Stanford, CA, USA.e-mail: malam8@charlotte.edu\nCommunications Engineering|             (2025) 4:6 1\n1234567890():,;\n1234567890():,;\ninstitutions to enhance their generalizability, transferability, and scalability.\nWithout access to such centralized datasets, extensive applicability and\nimplementation of artiﬁcial intelligence (AI) systems in the medical sphere\nremain elusive. Hosting large amounts of data in the medical domain is\ninfeasible due to privacy laws, sharing constraints, and patient con-\nﬁdentiality. Additionally, it requires considerable computational resources,\nwhich may pose a challenge for communities with limited computational\ninfrastructure\n36. Plus, identifying a wide range of diseases from diverse\ndemographics is necessary, but annotating it can be expensive. Therefore,\nmodel generalizability and large-scale deployment remain a major chal-\nlenge. Automated eye disease detection systems generally encounter three\nprimary challenges. First, many recent DL advancements in computer\nvision and imaging are tailored for general domains or natural images, often\nnot transferable to medical image applications. Second, the most effective\nsolutions predominantly depend on supervised learning using large net-\nworks, which call for a substantial volume of annotated data, which can limit\nfeasibility. This is especially pertinent in medical imaging (even more severe\nin the ophthalmic domain), as annotating image datasets is intricate,\nrequires a lot of effort, and involves the input of expert clinicians with\ncompeting clinical demands. Andﬁnally, smaller institutions may not have\naccess to robust AI models due to a lack of infrastructure and large training\ndatasets.\nTowards addressing this challenge, our study presents an SSL-MIM-\nbased framework for training and testing a foundation model to classify\nmulti-class datasets, distinguishing between normal and pathological\nconditions such as AMD, DME, and CNV using OCT images. Several\nstudies have explored the application of federated learning (FDL)\n37 in\nophthalmology, addressing various aspects such as FDL aggregation\npolicies\n38, DL model architectures39, data heterogeneity40, and future\nchallenges and directions for FDL in thisﬁeld41 and showed the value of\nFDL in ophthalmology. Building upon these advancements, we adopt a\ncollaborative and decentralized training approach using FDL to over-\ncome the challenges of reliable model development across multiple col-\nlaborative institutions. FDL introduces an innovative training paradigm\nthat circumvents the need for direct access to raw data and enables a\ndecentralized approach to collaborative model training. However, the\nnon-independent and identically distributed (Non-IID) nature of data in\nFDL can lead to a suboptimal shared model\n42. Data collected from diverse\nsources is inherently heterogeneous, as each participating device or\ninstitution may have unique data distributions inﬂuenced by demo-\ngraphic differences, imaging protocols, and device manufacturers. Con-\nsequently, when a model trained on one distribution is applied to data\nfrom another, its performance can degrade dramatically, a phenomenon\nknown as the domain shift problem. Therefore, employing techniques\nthat adapt the FDL model to perform effectively on previously unseen\ndata distributions is crucial.\nIn this study, We have developed a domain-adaptive FDL (DAD-FDL)\nframework to train the foundation model and compared it with the baseline\napproaches (centralized, local, and FDL without domain adaptation) to\ndemonstrate the advantage of FDL in real-world scenarios where the data is\nscarce or impossible to share. Foundation models are developed using\nextensive datasets through SSL, crafted to adaptﬂexibly to various down-\nstream tasks, typically byﬁne-tuning\n28.F o re x a m p l e ,R E T F o u n d30,a n\nophthalmology-focused foundation model, was trained on a vast dataset of\n1.6 million OCT images, a scale of datathat may be inaccessible to individual\ninstitutions. To address these limitations, FDL can provide a solution by\nenabling decentralized training across multiple sites.This collaborative\napproach allows the aggregated model to be trained on a larger and more\ndemographically diverse dataset, enhancing its robustness across different\npopulations. Our contributions:\n We have shown the advantage of SSL over conventional supervised\nlearning in different multi-class classiﬁcation problems;\n We have developed an FDL framework to train any SSL models that\ncan be used as a backbone model for any DL problem and deployed\nover two separate networks;\n We have integrated a domain adaptation technique into FDL to\naddress the data heterogeneity across multiple institutions; and\n We have validated our algorithm in both public and private clinical\ndatasets;\n We have open-sourced our model weights, models, and codebase for\nthe research community:Project hosted on GitHub.\nResults\nThis study establishes the effectiveness of distributed AI frameworks that\nintegrate SSL and DAD-FDL in substantially enhancing diagnostic accuracy\nfor ophthalmic diseases. Ourﬁndings clearly show that models employing\nSSL, especially when paired with DAD-FDL, achieve remarkable\nimprovements in seven distinct downstream task accuracy compared to\nlocalized or centralized learning approaches. In particular, SSL boosts the\narea under the receiver operating characteristic curve (AUC-ROC) by over\n10% across multiple datasets, while DAD-FDL models consistently\ndemonstrate superior generalizability across diverse test sets. The DAD\ncomponent directly tackles the challenges presented by non-IID data dis-\ntributions, ensuring reliable performance across varying demographics and\nimaging protocols. These results underscore the immense potential of FDL\nparadigms to bridge the gap between large-scale AI development and real-\nworld clinical applications, all whilesafeguarding data privacy and enhan-\ncing model robustness. We have also assessed our MIM framework with\nvarious hyperparameters and thoroughly established baseline approaches\nfor each test set (check Supplementary information).\nThe deep learning framework\nThis section overviews the phases of ourframeworks for training and testing\nDL models on various multi-class classiﬁcation problems using OCT ima-\nges. Speciﬁcally, we conducted a detailed experiment involving SSL and FDL\nacross seven distinct OCT datasets (DS1 to DS7), each encompassing a\nunique set of pathologies, thereby regarded as a distinct downstream task.\nFigure S1 depicts the datasets and each downstream task. Our benchmarks\na r es t r u c t u r e di n t of o u rm a i np h a s e sa ss h o w ni nF i g .1 accordingly: 1- local/\nindividual learning, 2- centralizedlearning, 3- FDL, and 4- DAD-FDL. Each\ndataset was hosted on a separate machine (across two separate institutions in\nCharlotte and Chicago) and evaluated using internal (local) and external test\nsets (other test sets) on its multi-class classiﬁcation problem (more details on\nthe data in Dataset subsection of the Supplementary information). We\ndeveloped seven conventional DL models to establish baseline comparisons\nduring the local learning phase. Figure2a outlines the pipeline of local\nlearning in which the images and theircorresponding labels were fed to the\nDL model comprised of a classiﬁcation encoder and multi-layer perception.\nIn centralized learning, data from publicly available datasets (DS1 to DS6)\nwere gathered on a single machine to pre-train our model by applying image\ntransformations and MIM (Fig.2b). Subsequently, the pre-trained encoders\nwere ﬁne-tuned for seven multi-class classiﬁcations using the dataset labels.\nIn the FDL scenario, rather than centralizing the data on a single computer,\neach machine hosting a dataset operated as an FDL node, collaboratively\npre-training an encoder and sharing its parameters with a server at the QIAI\nlab at the University of North Carolina at Charlotte (UNCC). The server\nthen aggregated the weights and redistributed them to all nodes, consisting\nof six machines at UNCC and one at the University of Illinois at Chicago\n(UIC). Finally, each nodeﬁne-tuned the model on its downstream task and\nevaluated it. In DAD-FDL,the domain-adaptive conﬁguration was dis-\ntributed from the server to all nodes before pre-training, while the sub-\nsequent steps followed the same approach as FDL.\nCentralized learning with self-supervised learning\nFor the centralized learning scenario, we gathered all the OCT images into a\nsingle machine with two steps: pre-training andﬁne-tuning. First, we pre-\ntrained a SwinV2 encoder using MIM (Fig.1). MIM is an unsupervised\nlearning technique where parts of an image are intentionally masked, and\nthe neural network is trained to predict the masked parts. Masking was\nexclusively concentrated on the central 60% of the image, while the top and\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 2\nbottom 20% were left unmasked (center masking). We implemented 50%\nmasking of the image patches, as each patch was 4 × 4 pixels, focusing\nexclusively on the center of the image, while the top and bottom remained\nunmasked. The MIM architecture comprised an encoder and a decoder\n(Fig. 2b). The resized and masked image was fed into the encoder (E\n*), and\nthen the decoder (D) reconstructed the masked patches. The reconstructed\nsamples from DS1 to DS6 can be seen in Fig.3. We used SwinV2 tiny\narchitecture for the encoder, and thedecoder was simply a linear layer to\nspeed up the pre-training step inspired by Xie et al.43. The loss was the mean\nsquared error (MSE) between the masked input and predicted patches; in\nour experiments, it was a better choice than Hinge loss (check Supplemen-\ntary information). Later, the pre-trained model was used as the backbone of\nthe classiﬁcation model andﬁne-tuned over each multi-labeled problem.\nThis way the model’s representation learning was guided to optimize its\nperformance for each multi-class classiﬁcation problem. The example of\nreconstructed images using the best MIM model is shown in Fig.3a\n1.\nFederated self-supervised learning\nSuccessful implementation of FDL in ophthalmic diagnosis could hold\nimmense potential for enhancing modelperformance and transferability\nacross sub-population data, especially by providing access to robust models\nin underserved populations. Our FDL phase also has two main steps: pre-\ntraining andﬁne-tuning. For pre-training the server should be run initially\nand wait for the FDL nodes to connect. Once the server connection to all\nFDL nodes is established, the server disseminates the training conﬁguration\ncomprised of initial parameters of the shared model and any hyperpara-\nmeter, such as the maximum number of epochs, learning rate, and others\nthat impacted the pre-training, to each node (Fig.4a. Each node then\nupdated its local model with the received parameters and started its pre-\ntraining process using local data. Afterc o m p l e t i n gt h et a s k ,e a c hn o d es e n d s\nthe pre-trained model’s parameters to the server (Fig.4b). The server then\naggregated the weights based on a predeﬁned strategy (weighted average in\nour case). The aggregated weights were then returned to each node, which\nupdates its local model upon receiving the FDL node. This marked the\ncompletion of one round (r) of the process. Each FDL node then used the\nFDL encoder as the backbone of its multi-class classiﬁcation problem and\nﬁne-tuned it after completing one round (Fig.4c ) .E v e n t u a l l y ,t h em o d e l\nwith the highest score on the validation set was saved for testing purposes.\nWe have built our framework on top of an open-source FDL framework\ncalled Flower\n44, which has NVIDIA Flare45 in its backend. This framework\ndeals with communication between different nodes and the global server.\nEach node is connected to the server using its unique IP and port. The server\nis responsible for initial hyperparameter setup, weights dissemination, and\nFDL aggregation.\nDomain adaptation\nSince the data comes from various demographics and devices, each dataset\nhas a unique image intensity and different imaging protocols. Data’sn o n -\nIID nature can cause a discrepancy that introduces domain shift, leading to\nlocally trained models focusing on different aspects of the learning task. This\ndiscrepancy in learning focus can result in models performing locally but\npoorly when averaged with others. Whenthere is a substantial domain shift,\naveraging these models can dilute the effectiveness of each model’s specia-\nlization, leading to a global model that does not perform optimally on any of\nthe local distributions\n46– 50. To alleviate this issue, we integrated an on-ﬂy\ndomain adaptation approach to match the image texture using noise\ntransfer for unsupervised domain adaptation\n51. We put a target image in the\nserver, and the server shared the information of this image with the FDL\nFig. 1 | Overview of the four phases of our fra-\nmework. aLocal learning phase in which a baseline\nmodel is trained in a particular dataset and evaluated\nover test set(s).b Centralized learning approach,\ncomprising pre-training,ﬁne-tuning, and evalua-\ntion. c Federated learning (FDL) approach where the\npre-training phase is conducted via FDL.d domain\nadaptation (DAD)-FDL pipeline, where the DAD\nconﬁguration is distributed before pre-training.\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 3\nnodes. The target image can be any arbitrary image and we selected one of\nthe training data from DS1 since the DS1 dataset had representative data\nfrom all the sub-population spectrum in DS2-DS7. Then each node per-\nformed this method to adapt its local data to the pixel intensity distribution\nof the target image. The original image and transformed samples from DS1\nto DS7 are shown in Fig.5. We also evaluated the performance of DAD-FDL\nmodels using different target images from DS2 to DS6 and reported the\nr e s u l t si nt h es e c t i o nD o m a i na d a p t i ve federated learning with different\ntarget images of Supplementary information. Following that, upon com-\nparing the results of using different target images, we observed that adopting\nFig. 2 | DL pipelines of the local learning and pre-training. aLocal learning\npipeline, in which a pair of images and their label are input to the model after\nundergoing four transformations: rotation, color jittering, Gaussian blur, and Sobel\nﬁlter. b Pre-training phase, during which the input image is masked and given to the\nreconstruction network to train the encoder over time.\nFig. 3 | Reconstructed images from the MIM network, where 50% of the images are masked. a1– f1 Columns of original image samples from DS1 to DS6, respectively.a2– f1\nColumn of masked images.a3– f3 Columns of reconstructed images.\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 4\na target image from DS1 yielded the highest performance, as reported in\nTables S2 to S6, in comparison to Table1.\nModel performances\nTable 1 shows the performance of models trained following different\napproaches on six test sets (DS1-DS6). In addition to local and centralized\nlearning, we reported the performance of FDL models with and without\ndomain adaptation (DAD-FDL-1 and FDL-1). Furthermore, we extended\nthe training of these FDL models for an additionalﬁve rounds to evaluate the\nperformance gains of this distributed learning method, considering the\ntrade-off with increased trainingtime (FDL-5 and DAD-FDL-5 in Table1).\nDuring the evaluation, the test sets were adjusted to match the categories of\neach dataset. For example, DS3 was trained with labels Normal, AMD, DR,\nand Others, but when tested on DS4, which includes Normal, Drusen, and\nCNV, CNV was excluded and Drusen was categorized as AMD. Further-\nmore, the effects of various factors on model performance were analyzed,\nincluding image size on internal (Figure S5) and external test sets (Fig-\nure S6), loss functions (Figure S2a), masking types (Figure S2b), masking\nratios (Figure S3a), and patch sizes(Figure S3b). Our experiment showed\nthat the SSL models (centralized, FDL, and DAD-FDL) performed better\nthan the local ones. The performance difference on respective test sets (e.g.,\nDS1 over Test-1) was at least 10%, indicating SSL’s power in boosting model\nefﬁcacy. Exceptionally, thedifference was high in two cases: about 30% and\n40% increases for DS4 and DS6 respectively when applying SSL. Also, we\nadded an“Average” column to show the overall average performance of the\nmodels over all test sets, which implies the model’s generalization capability.\nThe advantage of DAD-FDL 5 in producing more generalizable models is\nevident, constantly achieving the highest average AUC-ROC.\nTo further validate the trend in Table1, we leveraged our private\ndataset (UIC retina clinic) to perform a multi-class classiﬁcation of the DR\ninto four stages: Control, Mild, Moderate, and Severe. The local model was\ntrained only using the DS7 data while the centralized, FDL, and DAD-FDL\nmodels were built on top of a pre-trained encoder on DS1 to DS6 andﬁne-\ntuned on DS7. Our results show that the DAD-FDL-1 and DAD-FDL-5\nmodels achieved macro AUC-ROC of 0.7161 and 0.8753, respectively,\ncompared to the centralized model at 0.7824 and the local model at 0.6815,\ns h o w ni nF i g .6 (FDL-1 and FDL-5 achieved macro AUC of 0.7314 and\n0.8486, and per class macro AUC of all classes for local, central, FDL and\nDAD-FDL are presented in S4). This supports the effectiveness of applying\nSSL and FDL to extract improved representations and enhance the gen-\neralizability of models across various classiﬁcation tasks. To have an idea of\nthe performance of popular DL baseline models on 224 × 224 test images, we\nevaluated pre-trained ResNet50, ResNeXt101-64 × 4, vision transformers\n(ViT)\n52 Large with a patch size of 32, and ConvNeXT Large on\nImageNet1k53, with the results summarized in Table S1.\nDiscussion\nIn this study, we propose an integratedFDL-SSL framework for ophthalmic\ndisease classiﬁcation. We evaluated our framework on multi-class classiﬁ-\ncation and compared it with baseline approaches - evaluating the effect of\nSSL pre-training and domain adaptivedistributed learning. In the local\ntraining baseline experiments, a pre-training phase was excluded to avoid\nredundancy, as the data used for both pre-training andﬁne-tuning would be\nidentical, with only the classiﬁcation labels differing. This overlap renders\nthe pre-training step unnecessary. While the results demonstrated the\nadvantages of our SSL approach, alternative approaches such as fully\nsupervised learning could be considered. Comparing these results with prior\nwork\n38 reveals an improvement in model performance. Also, a fully\nsupervised learning model that includes every available label in the training\ndatasets (DS1-DS6) would be limited in its applicability to tasks like DS7,\nFig. 4 | Main stages of pre-training andﬁne-tuning via federated learning (FDL)\nat the University of North Carolina at Charlotte (UNCC) and the University of\nIllinois Chicago (UIC). aServer shares the initial model parameter and the\nconﬁguration to all nodes.b Each FDL node pre-trains its model and sends the\nmodel’s weights to the server.c Finally, the server aggregates the weights and reci-\nprocates them to each client, and they start theﬁne-tuning step.\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 5\nwhere data restrictions prevent information from leaving the hospital pre-\nmises. Furthermore, achieving a fairc o m p a r i s o na c r o s sm o d e l sw o u l d\nnecessitate sharing complete label information across nodes, which conﬂicts\nwith the foundational principles of FDL that emphasize limited data shar-\ning. In addition, differences in dataset annotations across nodes made SSL\nan effective strategy, as SSL can exploit unlabeled data without reliance on\nuniform annotations. Lastly, during FDL training, nodes may lack access to\nthe full label set, which could lead to performance degradation in a purely\nsupervised setting.\nViTs have been vastly used and showcased the effectiveness of\nattention mechanisms\n54,55 in detecting ophthalmic disease56,57. ViTs have\na weaker inductive bias and have large effective receptiveﬁelds, allowing\nmore ﬂexible and global feature detection across the entire image, which\ncan be advantageous for certain complex vision tasks such as MIM58,59,\nthereby we used SwinV2, a hierarchical ViT, as the encoder of our\nmodels.\nThe essential advantage of foundation models and SSL is the capability\nof using a large amount of unannotated data to train the network to\nunderstand the structure of the input images such as RETFound, RET-\nCLIP\n60,a n dF L A I R61. These foundation models are general-purpose, and\nthey were pre-trained on large datasets, thereby they can beﬁne-tuned with\nfew-shot learning to provide a specialized model for downstream tasks. The\nbiggest hurdle to training such models is hosting large amounts of patient\ndata or gathering and annotating these data while covering different dis-\neases. FDL is a robust training AI framework in which it is possible to\nleverage multi-institutional data from different demographics without\ncompromising any sharing concerns which helps create more accurate,\nscalable, and universally applicable models. To evaluate the performance\ngain of applying different implemented techniques, we conducted a fair\ncomparison between the four approaches we deﬁned: local, centralized,\nFDL, and DAD-FDL. The centralized, FDL, and DAD-FDL models were\nexpected to outperform local ones in each test set since they utilized all six\nFig. 5 | Samples from DS1 to DS7 and their unsupervised noise-transformed\nversion, where all the transformed images have similar pixel value intensity, are\nshown in rows d to i in alphabetical order. a1– g1 Columns of original images from\nDS1 to DS7, respectively.a2– g2 Columns of transformed images.h Target image\nused to transform other images based on its pixel value intensity.\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 6\ndatasets (Table1); in contrast, local models were trained only on their\nrespective local data. This performance boost is noticeable in all datasets,\nmost signiﬁcantly in DS4 and DS6. This performance improvement is\nevident across all datasets, with the most signiﬁcant gains in DS4 and DS6.\nFor DS4, the average macro AUC-ROC increased from 0.6298 for the local\nmodel to 0.837, 0.8233, and 0.8756 for the centralized, FDL-1, and DAD-\nFDL-5 models with CI above 95% and p-value less than 0.05. For DS6, the\nmacro AUC-ROC rose from 0.4769 for the local model to 0.7847 for cen-\ntralized, 0.6934 for FDL-1, and 0.8992 for DAD-FDL-5 with the same CI\nand p-value. We believe that having a similar device for OCT capturing for\nDS1 and DS4 contributed to the improved performance of SSL models. For\nDS6, the local models had difﬁculty accurately classifying the seven distinct\nclasses, likely due to the relatively small number of input images provided for\ntraining. However, when a pre-trained model was set as the backbone, the\nmodel performance improved signiﬁcantly, highlighting the beneﬁts of\nunsupervised learning.\nOur ﬁndings pointed out the superiority of the centralized model over\nthe others in their respective test sets on DS1 to DS6 except for DS4.\nHowever, when considering average performance, DAD-FDL-5 has the\nupper hand across all test sets. Several factors contributed to this advantage.\nPrimarily, the integration of domainadaptation ensured uniform image\nintensities across all locally trained encoders, facilitating a consistent opti-\nmization trajectory for the models. The key idea was to apply this method on\nthe ﬂy to reduce the cost and burden of generating and storing new images,\nwhich can signiﬁcantly prolong model development. Using this approach,\nwe could quickly transfer the target image style to training images within a\nTable 1 | Multi-class classiﬁcation performance analysis of models obtained from different approaches over the six test sets\nFine-tuned Dataset Model AUC-ROC 1 Average\nTest-1 Test-2 Test-3 Test-4 Test-5 Test-6\nDS1 Local 80.71 90.53 95.92 46.74 86.25 75.52 79.28\nCentralized 96.19 98.81 59.56 59.51 92 81.99 81.34\nFDL-1 90.39 93.47 60.74 58.37 90 85.17 79.69\nDAD-FDL-1 92.31 97.01 73.55 58.46 85.37 85.16 81.98\nFDL-5 93.65 96.85 72.24 59.44 81.25 82.77 81.03\nDAD-FDL-5 94.54 98.18 78.03 58.58 92.75 85.08 84.53\nDS2 Local 69.37 83.46 34.79 54.92 57.5 50.09 49.73\nCentralized 74.14 97.86 57.64 57.27 77.13 63.39 71.24\nFDL-1 65.09 87.37 67.51 57.04 69.5 53.19 66.62\nDAD-FDL-1 66.61 92.68 65.45 55.87 82.25 62.05 70.82\nFDL-5 75.14 93.21 82.25 66.71 83.75 78.77 79.97\nDAD-FDL 5 75.22 96.43 86.47 69.15 82 74.59 80.64\nDS3 Local 35.99 83.18 83.92 27.7 73.46 72.4 62.78\nCentralized 32.38 88.07 92.18 30.25 78.69 83.03 67.43\nFDL-1 30.71 82.18 83 28.78 59.72 70.03 59.07\nDAD-FDL-1 34.16 91.22 89.57 33 79.49 84.89 68.72\nFDL-5 37.26 88.8 84.32 34.21 66.64 92.57 67.3\nDAD-FDL-5 36.91 95.43 90.57 35.63 90.06 89.2 79.97\nDS4 Local 67.05 80.99 70.58 61.3 39.88 58.05 62.98\nCentralized 85.19 89.75 67.37 93.07 95.63 71.17 83.7\nFDL-1 78 75.18 90.12 87.93 95.25 67.49 82.33\nDAD-FDL-1 79.26 72.03 74.13 88.8 91.75 55.87 76.97\nFDL-5 86.78 96.24 84.4 94.29 98.5 65.38 87.56\nDAD-FDL-5 89.79 99.17 83.87 94.51 98 73.91 89.88\nDS5 Local 22.8 56.26 69.37 23.82 62.77 47.01 47.01\nCentralized 27.17 76.97 90.04 29.44 86.96 56.38 61.16\nFDL-1 23.38 48.69 38.83 23.55 42.9 43.19 36.76\nDAD-FDL-1 33.14 88.92 69.6 30.3 81.3 56.4 59.94\nFDL-5 28.94 76.59 56.29 27.39 64.88 53.52 51.27\nDAD-FDL-5 36.16 98.94 80.74 34.97 83.02 92.86 71.12\nDS6 Local 53.18 55.58 55.39 55.57 14.88 51.54 47.69\nCentralized 60.88 66.15 93.94 67.9 88.88 93.09 78.47\nFDL-1 61.46 59.67 78.46 67.73 80.88 67.82 69.34\nDAD-FDL-1 70.46 78.54 84.94 70.23 92.5 84.89 80.26\nFDL-5 75.69 81.94 93.24 71.59 87.75 72.19 80.40\nDAD-FDL-5 81.95 93.04 95.71 77.47 98.25 93.07 89.92\nFor each dataset, the highest AUC-ROC value of the corresponding test set is shown in italics, and the highest average AUC-ROC value is indicated in bold. All results in this study are with the signiﬁcant\nstatistical analysis of the conﬁdence interval (CI)≥95% and thep-value ≤ 0.05.\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 7\nnegligible amount of time. Second, theDAD-FDL-5 models were trained for\nﬁve rounds; after each round, performance was assessed on the validation\nset, and improvements in AUC-ROC and F1 scores led to the selection of the\nmodel for ﬁnal testing. Consequently, the most accurate model on the\nvalidation set was chosen for evaluation on the test set. On the other hand,\nthe pre-training phase was carried out just once in centralized, FDL-1, or\nDAD-FDL-1, and the process was not repeatedﬁve times. This demon-\nstrates the advantages of distributed learning and suggests that pre-training\ncontinuation can improve the model’s generalization predictability, albeit at\nthe cost of additional training time.\nThe effectiveness of our method was further validated using our clinical\ndataset from UIC, where the majority of the data was designated for testing.\nThis approach enabled evaluating different models on a distinct dataset and\nveriﬁed previous outcomes. The results on DS7 reafﬁrmed the superiority of\nSSL models, indicating that continued training can yield models with\nenhanced generalizability, applicable to a wide range of classiﬁcation chal-\nlenges beyond disease classiﬁcation, including the classiﬁcation of different\nDR stages. Analyzing the AUC-ROC ofDS7 (Fig. S4), the model exhibited\nthe highest performance in the“Moderate” category for the local model,\nwhile achieving the best results in the“Control” category for the centralized\nmodel, which aligns with the natureof datasets DS1 to DS6, primarily\nconsisting of normal or control OCT images. The FDL-1 and DAD-FDL-1\nmodels display a similar pattern to the local model, with peak performance\nobserved in the“Moderate” class, followed by“Control”, “Severe”,a n d\nﬁnally“Mild”, indicating that FDL maintains the performance trend of local\nlearning while beneﬁting from the distributed learning framework. In\ncontrast, FDL-5 and DAD-FDL-5 demonstrate a performance trend akin to\ncentralized learning, suggesting that as pre-training progresses, the FDL\nglobal model increasingly resembles the centralized model in its perfor-\nmance characteristics.\nOne limitation of this study was the small image (128 × 128) and\nencoder sizes (4.5 Gigabyte FLOPS), demonstrating the practicality of using\nsmaller images and models for multi-class classiﬁcation while still achieving\nreasonable performance. The mask patch size is closely related to the image\nsize, and different mask patch sizes can be tested with various image sizes to\nﬁnd the best combinations (demonstrated and discussed in the Results\nsection). Additionally, different datasets can apply different mask patch sizes\nand masking ratios depending on their data. Other approaches, such as\ndynamic masking ratios, patch sizes,and semantic masking that involve\nmasking only the pixels containing retinal information, can also be explored,\nas we realized there is no single best masking ratio and masking size for all\nOCT datasets. Moreover, expanding the dataset could facilitate future work\nin prognosis, allowing for a more comprehensive exploration of prediction\nmodeling.\nTo shed light on the models\n’ ability in multi-class classiﬁcation and\nenhance the transparency of the model’s decision-making process, we\ncreated our pipeline for Eigen Grad-Cam developed by ref. Gildenblat and\nContributors62 to visualize the areas of an image that the model focuses on\nwhen making a prediction. In Fig.7, it is evident that the DAD-FDL-5 model\nsuccessfully identiﬁed important features of the retina speciﬁct oe a c h\ncategory, surpassing the local model behaviors. The color map in thisﬁgure\nhighlights the regions contributing most signiﬁcantly to the model’sp r e -\ndictions. Red and yellow areas indicate the highest activation, representing\nthe most critical regions for the model’s decision-making process. Green\nareas show moderate activation, contributing to a lesser extent, while blue\nareas indicate minimal activation and are less relevant for the prediction. For\nexample, in the case of DS3, the local model failed to identify relevant\nregions in the image, while the centralized and DAD-FDL-5 found the most\nsigniﬁcant image pixels. In DS6, the DAD-FDL-5 model identiﬁed the\ncorrect category by pinpointing the whole retinal layers, while the cen-\ntralized and local models failed to do so.\nSeveral studies have trained DL mode l su s i n gt h es a m ep u b l i cd a t a s e t\nwe employed, reporting results for DS1 and DS2\n63– 65. However, none of these\nworks addressed the data leakage originally present in DS1 or mentioned\nusing a patient-wise split to create the test set to avoid such leakage. Our SSL\nframework outperformed the classiﬁcation tasks on DS3, DS4, and DS5 as\nreported in refs.35,66,67, respectively. Although\n68 achieved a higher AUC-\nROC compared to our method (using a larger network, ResNet50, and\nhigher resolution images, 512 vs. 128), our approach still demonstrated\nacceptable performance, achieving 0.9451 compared to their 0.98.\nThe advantage of our work lies in our ability to leverage large amounts\nof unlabeled data by using unsupervised learning to train a backbone model\nfor various tasks. This approach is particularly beneﬁcial in ﬁelds like\nophthalmology, where data annotation is both costly and tedious. In our\nstudy, we focused on multi-class classiﬁcation due to its applicability in real-\nworld scenarios where subjects’data might exhibit characteristics of parti-\ncular pathologies. An AI system can assist clinicians in classifying these\ndiseases accurately. This model can beﬁne-tuned on local data to perform\neffectively on in-domain patient demographics. The importance of multi-\nclass classiﬁcation lies in its ability to handle complex problems involving\nmore than two categories, providing granular and actionable insights. It\nfacilitates improved decision-making by distinguishing between multiple\ncategories and is often more efﬁcient than maintaining multiple binary\nclassiﬁers. This reduces computational overhead and complexity while\nenhancing overall accuracy by considering all possible classes in a single\nmodel, making them versatile and robust solutions for diverse and dynamic\nenvironments.\nOne of the beneﬁts of FDL is the ability to continuously train and\ndevelop models without manually distributing a backbone model. Yet,\nprivacy is a primary concern in FDL. To address this, FLower employs\nencryption and authentication using the secure socket layer communication\nprotocol to establish secure connections between nodes and prevent any\nbreaches of conﬁdentiality integrity, or availability\n69.\nAnother signiﬁcant challenge in FDL is ensuring that all machines\noperate smoothly throughout the training process, with none experiencing\ncrashes. This requires constant monitoring of all nodes to prevent inter-\nruptions. Solutions exist for handling node failures, such as continuing\ntraining with a fraction of the nodes.However, guaranteeing full partici-\npation from all nodes at all times is still difﬁcult, and allowing a node to\nrejoin the training without halting and restarting the entire process poses a\nfurther complication.\nIn conclusion, our approach demonstrated the practicality of com-\nbining a domain-adaptive FDL with SSL to develop a robust DL encoder.\nThis work focuses on applying FDL in training a multi-purpose model that\nbeneﬁted from SSL, resulting in a scalable AI system that improves retinal\nclassiﬁcation diagnostics, elevates healthcare services, and paves the path for\ntraining foundation models through FDL. Ideally, using the same device for\nall data collection would improve performance; however, expecting all\ninstitutions to have identical equipment is impractical. Therefore, further\ninvestigation is warranted to standardize theﬁeld of view and image reso-\nlution details.\nFig. 6 | Macro AUC-ROC plot of four models over DS7.Local, centralized, DAD-\nFDL-1, and DAD-FDL-5, with DAD-FDL-5 outperforming other methods on DS7.\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 8\nMethods\nWe leveraged SSL, and DAD-FDL to train DL encoders that can be used for\ndownstream tasks such as multi-class classiﬁcation for our study. Our\napproach combines these techniques’strengths to provide an accurate and\ngeneralizable model. During SSL, the model learns to capture the OCT\nimage structure and pathologies indicative of the diseases. Then the model\nweights are used as transfer learning weights to initialize a new DL model for\ndifferent multi-class classiﬁcation problems (Fig. S1). An FDL framework\nfacilitates collaborative training between multiple hospitals without\nexchanging data and preserves patient privacy. Therefore, we carried out a\nFig. 7 | Eigen Grad-CAM62 inference from the norm layer of theﬁnal Swin\nTransformer BlockV2.Models’inference samples (choroidal neovasculature\n(CNV), diabetic macular edema (DME), diabetic retinopathy (DR), Drusen, normal\nand age-related macular degeneration (AMD)) from DS1 to DS7 datasets, respec-\ntively. Checkmarks and crosses indicate correct and incorrect predictions.\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 9\nstep-by-step experiment by adding each technique to the previous one on\nthe OCT datasets to achieve this goal through local learning, centralized\nlearning, and FDL.\nLocal learning\nWe trained seven DL models (DS1– DS7) using a transformers SwinV2 tiny\nencoder without applying SSL techniques. The SwinV2 model is structured\naround hierarchical representationwhere an image is processed through a\nseries of transformer blocks that gradually reduce the spatial dimension\nwhile increasing the feature dimension. This approach lets the model cap-\nture ﬁne-grained details early on and more abstract representations in\ndeeper layers. The classiﬁcation loss for this training phase was focal loss\n70\nbetween the actual label (y) and the predicted one (^y) since the datasets\ncontain an unequal number of categories. The pipeline of the training is\ncomprised ofE/C3 :ðÞthat is the SwinV2-encoder and a projection head,G(. ),\nthat has two dense layers with Gaussian error linear units, dropouts (20%),\nand a classiﬁcation head based on the numberof classes in the respective\ndataset shown in Fig.2a). The models were trained with 100 epochs and\nearly stopping over the validation loss with a patience of ten.\nDomain adaptation implementation\nDAD was implemented using singularvalue decomposition-based noise\nadaptation (SVDNA)51.T h ea u t h o r s’ proposed algorithm is presented in\nAlgorithm 1.\nAlgorithm 1.S V D N A\nLetUΣVT be the singular value decomposition of the respective images\nimsource; imtarget 2Rn × n and k the noise transfer threshold.\nimsource ¼UsΣsVT\ns , imtarget ¼Ut Σt VT\nt\nUr  u1\ns ; ... ; uk\ns ; ukþ1\nt ; ... ; un\nt\nΣr  diagðσ1\ns ; ... ; σk\ns ; σkþ1\nt ; ... ; σn\nt Þ\nVT\nr  v1T\ns ; ... ; vkT\ns ; vkþ1T\nt ; ... ; vnT\nt\nImnoised  UrΣrVT\nr\nImclipped ← clip_values_to_interval(Imnoised, [0, 255])\nImrestyled_final← histogram_matching(source =Imclipped,t a r g e t=imtarget)\nThis algorithm is implemented as a transform function using the\nPyTorch library, allowing it to run on theﬂy, transform the image, and feed\nit directly to the model. In our experiments, we setk = 50.\nTraining details\nAll the images were resized to 128 × 128 or 224 × 224 and normalized with\nthe mean and standard deviation 0.5 to have values in [− 1, 1] during all\nphases. To reduce overﬁtting and create robust models, we applied several\naugmentation techniques. Custom rotation with angles of 0°, 90°, 180° and,\n270°, Gaussian blur with a kernel size of (5, 5) andσ ∈(0.75, 1.5), Sobelﬁlter\nand elastic transform withα ∈ (50, 150) andσ ∈ (5, 10) were randomly\nselected with a probability of 20% to introduce to help models become\ninvariant to certain changes or perturbations in the input data, such as slight\nrotations during the pre-training. We applied color jittering with a bright-\nness and contrast range of 50%, during theﬁne-tuning and local learning\ninstead of elastic transformation. Allthe experiments were run using Python\n3.10.13, PyTorch 2.5.1, and CUDA version 12.4.\nFor the pre-training phase (in both central and FDL), we used Adam\nwith weight decay optimizer (AdamW)\n71 with a learning rate of 2 × 10−4,\ncosine annealing learning scheduler with a warm-up learning rate of\n1×1 0−6 and minimum learning rate of 1 × 10−5, ten warm-up epochs,\nand weight decay (WD) of 0.05. These parameters are chosen based on\nthe Simmim model\n43 pre-trainig phase, and later tuned for the best\nresults. For theﬁne-tuning phase, we again used AdamW with a learning\nrate of 3 × 10−5, step size learning rate scheduler (StepLR) withγ = 0.5,\nand WD of 10−6. β1 and β2 for AdamW were set to 0.9 and 0.999 in both\nphases, respectively. For the local models, we used the same optimizer\nand hyperparameters to conduct a fair comparison between the different\napproaches. In all of our assessments, we considered both the macro\nAUC-ROC and macro F1 scores, as the model should be able to identify\ndifferent categories equally.\nIn the central approach, each epoch took about 4 minutes, while in\nFDL, it took approximately 20 minutes, increasing to 22 minutes with DAD\napplied.\nExperimental setup\nGiven that the nodes were equipped with graphical processing units\n(GPUs), a Python environment (version 3.10.13) with PyTorch 2.5.1 and\nCUDA 12.3 was set up using Miniconda and deployed across all nodes. Each\nnode ran three distinct scripts for local, central, and federated training,\nwhich required access to the local dataset path, server IP, server port, and\ndata module Python code to prepare the data for training and testing. All\nnecessary codes, requirements, and public test datasets were shared between\nn o d e si na d v a n c e .\nFor local learning, the script loads the data using the provided data path\nand data module code, trains the local models, and tests them on the shared\ntest sets. The model was pre-trained on a Lambda GPU workstation in\ncentralized learning and manually transferred to all seven nodes. Each node\nthen ﬁne-tuned the pre-trained model on their local labeled data. In fed-\nerated training, the entire pre-training andﬁne-tuning process was executed\nin one uninterrupted sequence: the server node was initiated with the ser-\nver.py code, and each node ran the corresponding client.py code.\nThe GPU node speciﬁcations are provided as follows for DS1 through\nDS8, respectively: DS1-2 × NVIDIARTX A6000, DS2-Nvidia GeForce RTX\n3060Ti, DS3-3 × NVIDIA RTX 6000 Ada generation, DS4-2 × Nvidia Titan\nV, DS5-Nvidia GeForce RTX 3060Ti, DS6-Nvidia GeForce RTX, DS7-\nNvidia RTX A500, DS8-NVidia GeforeRTX 3060Ti. Due to varying GPU\nmemory across nodes, a batch size of 32 was used for all steps, except during\ncentralized pre-training on the Lambda GPU workstation, where a batch\nsize of 128 was employed.\nStatistical analysis\nIn this study, we employed rigorous statistical tests to conﬁrm the sig-\nniﬁcance of our model’s performance improvements. Speciﬁcally, we\nused the Wilcoxon signed-rank test for paired comparisons of AUC-\nROC scores. This test does not assume a normal distribution, making it\nwell-suited for assessing performance differences on the same test sets\nunder different training conditions. Since we evaluated several models\nacross different datasets, we applied the Bonferroni correction to adjust\nthe signiﬁcance level to address the issue of multiple comparisons. This\nadjustment helps reduce the risk of Type I errors, commonly known as\nfalse positives. After implementing these corrections, all results reported\nas statistically signiﬁcant have p-values below the adjusted threshold, and\ntheir conﬁdence intervals are at least 95%. All statistical analyses were\nconducted using Python. These measures ensure that our reported per-\nformance gains are robust and reliable and not merely the result of\nchance.\nReporting summary\nFurther information on research designis available in the Nature Portfolio\nReporting Summary linked to this article.\nData availability\nThe Dataset section of the Supplementary information describes all publicly\navailable datasets used in this study in detail. The private dataset from the\nUniversity of Illinois at Chicago (UIC) can be requested from the corre-\nsponding author, subject to reasonable conditions.\nCode availability\nThe codes that support theﬁndings of this study are available in GitHub72– 76.\nReceived: 31 August 2024; Accepted: 7 January 2025;\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 10\nReferences\n1. Akpek, E. K. & Smith, R. A. Overview of age-related ocular conditions.\nAm. J. Manag Care19, S67–75 (2013).\n2. Bressler, N. M. Age-related macular degeneration is the leading cause\nof blindness.JAMA 291, 1900–1901 (2004).\n3. Steinmetz, J. D. et al. Causes of blindness and vision impairment\nin 2020 and trends over 30 years, and prevalence of avoidable\nblindness in relation to vision 2020: the right to sight: an analysis\nfor the global burden of disease study.Lancet Glob. Health 9,\ne144–e160 (2021).\n4. Wang, Y. et al. Global incidence, progression, and risk factors of age-\nrelated macular degeneration and projection of disease statistics in 30\nyears: a modeling study.Gerontology 68, 721–735 (2022).\n5. Doroudian, S. Collaboration in immersive environments: challenges\nand solutions. Preprint athttps://arxiv.org/abs/2311.00689 (2023).\n6. World Health Organization. Blindness and visual impairment (2023).\nhttps://www.who.int/news-room/fact-sheets/detail/blindness-and-\nvisual-impairment.\n7. Scott, A. W. & Bressler, S. B. Long-term follow-up of vascular\nendothelial growth factor inhibitor therapy for neovascular age-\nrelated macular degeneration.Curr. Opin. Ophthalmol.24, 190–196\n(2013).\n8. Mohamed, Q., Gillies, M. C. & Wong, T. Y. Management of diabetic\nretinopathy: a systematic review.JAMA 298, 902–916 (2007).\n9. El-Bouzaidi, Y. E. I. & Abdoun, O. Advances in artiﬁcial intelligence for\naccurate and timely diagnosis of covid-19: a comprehensive review of\nmedical imaging analysis.Scientiﬁc African22, e01961 (2023).\n10. Khan, A. I., Quadri, S., Banday, S. & Shah, J. L. Deep diagnosis: a real-\ntime apple leaf disease detection system based on deep learning.\nComput. Electron. Agric.198, 107093 (2022).\n11. Muchuchuti, S. & Viriri, S. Retinal disease detection using deep\nlearning techniques: a comprehensive review.J. Imaging9, 84 (2023).\n12. Tuncer, S. A., Çínar, A. & Fírat, M. Hybrid cnn based computer-aided\ndiagnosis system for choroidal neovascularization, diabetic macular\nedema, drusen disease detection from oct images.Traitement du\nSignal 38 (2021).\n13. Lee, C. S., Baughman, D. M. & Lee, A. Y. Deep learning is effective for\nclassifying normal versus age-related macular degeneration oct\nimages. Ophthalmol. Retin.1, 322–327 (2017).\n14. Awais, M., Müller, H., Tang, T. B. & Meriaudeau, F. Classiﬁcation of sd-\noct images using a deep learning approach. In:2017 Organizing\nCommittee of the IEEE International Conference on Signal and Image\nProcessing Applications (ICSIPA), 489–492 (IEEE, 2017).\n15. Kugelman, J. et al. Automatic choroidal segmentation in oct\nimages using supervised deep learning methods.Sci. Rep.9,1 3 2 9 8\n(2019).\n16. Li, F. et al. Deep learning-based automated detection of retinal\ndiseases using optical coherence tomography images.Biomed. Opt.\nexpress 10, 6204–6226 (2019).\n17. Wang, D. & Wang, L. On oct image classiﬁcation via deep learning.\nIEEE Photonics J.11,1 –14 (2019).\n18. Alam, M., Le, D., Lim, J. I., Chan, R. V. & Yao, X. Supervised machine\nlearning based multi-task artiﬁcial intelligence classiﬁcation of\nretinopathies. J. Clin. Med.8, 872 (2019).\n19. Schlegl, T. et al. Fully automated detection and quantiﬁcation of\nmacular ﬂuid in oct using deep learning.Ophthalmology 125, 549–558\n(2018).\n20. Burlina, P. M. et al. Use of deep learning for detailed severity\ncharacterization and estimation of 5-year risk among patients with\nage-related macular degeneration.JAMA Ophthalmol.136,\n1359–1366 (2018).\n21. Grassmann, F. et al. A deep learning algorithm for prediction of age-\nrelated eye disease study severity scale for age-related macular\ndegeneration from color fundus photography.Ophthalmology 125,\n1410–1420 (2018).\n22. Peng, Y. et al. Deepseenet: a deep learning model for automated\nclassiﬁcation of patient-based age-related macular degeneration\nseverity from color fundus photographs.Ophthalmology 126,\n565–575 (2019).\n23. Ting, D. S. W. et al. Development and validation of a deep learning\nsystem for diabetic retinopathy and related eye diseases using retinal\nimages from multiethnic populations with diabetes.JAMA 318,\n2211–2223 (2017).\n24. Abràmoff, M. D. et al. Automated analysis of retinal images for\ndetection of referable diabetic retinopathy.JAMA Ophthalmol.131,\n351–357 (2013).\n25. Tufail, A. et al. Automated diabetic retinopathy image assessment\nsoftware: diagnostic accuracy and cost-effectiveness compared with\nhuman graders.Ophthalmology 124, 343–351 (2017).\n26. Gulshan, V. et al. Development and validation of a deep learning\nalgorithm for detection of diabetic retinopathy in retinal fundus\nphotographs. JAMA 316, 2402–2410 (2016).\n27. Li, Z. et al. An automated grading system for detection of vision-\nthreatening referable diabetic retinopathy on the basis of color fundus\nphotographs. Diab. Care41, 2509–2516 (2018).\n28. Bommasani, R. et al. On the opportunities and risks of foundation\nmodels. Preprint athttps://arxiv.org/abs/2108.07258 (2021).\n29. Wang, Z., Liu, C., Zhang, S. & Dou, Q. Foundation model for\nendoscopy video analysis via large-scale self-supervised pre-train. In:\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, 101–111 (Springer, 2023).\n30. Zhou, Y. et al. A foundation model for generalizable disease detection\nfrom retinal images.Nature 622, 156–163 (2023).\n31. Haghighi, T. et al. Eye-llama, an in-domain large language model for\nophthalmology. bioRxiv https://doi.org/10.1101/2024.04.26.591355\n(2024).\n32. Alam, M. N. et al. Contrastive learning-based pretraining improves\nrepresentation and transferability of diabetic retinopathy\nclassiﬁcation models.Sci. Rep.13, 6047 (2023).\n33. Bao, H., Dong, L., Piao, S. & Wei, F. BEiT: BERT pre-training of image\ntransformers. International Conference on Learning Representations\nhttps://openreview.net/forum?id=p-BhZSz59o4 (2022).\n34. Jannat, F.-E., Gholami, S., Alam, M. N. & Tabkhi, H. Oct-selfnet: A self-\nsupervised framework with multi-modal datasets for generalized and\nrobust retinal disease detection. Preprint athttps://arxiv.org/abs/\n2401.12344 (2024).\n35. Cai, Z., Lin, L., He, H., Cheng, P. & Tang, X. Uni4eye++: A general\nmasked image modeling multi-modal pre-training framework for\nophthalmic image classiﬁcation and segmentation.IEEE Trans. Med.\nImaging 99 (2024).\n36. Rashvand, N. et al. Distributed learning for automatic modulation\nrecognition in bandwidth-limited networks. In:Signal Processing,\nSensor/Information Fusion, and Target Recognition XXXIII, Vol. 13057\n(eds Kadar, I., Blasch, E. P. & Grewe, L. L.) 345–357 (SPIE, 2024).\n37. McMahan, B., Moore, E., Ramage, D., Hampson, S. & y Arcas, B. A.\nCommunication-efﬁcient learning of deep networks from\ndecentralized data. In:Artiﬁcial intelligence and statistics(eds Aarti, S.\n& Jerry, Z.) 1273–1282 (PMLR, 2017).\n38. Gholami, S. et al. Federated learning for diagnosis of age-related\nmacular degeneration.Front. Med.10, 1259017 (2023).\n39. Sonti, M. & Kokil, P. Automatic diagnosis of age-related macular\ndegeneration via federated learning. In:International Conference on\nComputer Vision and Image Processing(eds Harkeerat, K. et al.)\n128–136 (Springer, 2023).\n40. Amgain, S. et al. Investigation of federated learning algorithms for\nretinal optical coherence tomography image classiﬁcation with\nstatistical heterogeneity. Preprint athttps://arxiv.org/abs/2402.10035\n(2024).\n41. Nguyen, T. X. et al. Federated learning in ocular imaging: current\nprogress and future direction.Diagnostics 12, 2835 (2022).\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 11\n42. Sery, T., Shlezinger, N., Cohen, K. & Eldar, Y. C. Over-the-air federated\nlearning from heterogeneous data.IEEE Trans. Signal Process.69,\n3796–3811 (2021).\n43. Xie, Z. et al. Simmim: A simple framework for masked image modeling.\nIn: Proc. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR)(Program Chairs and Organizing Committee of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR)) 9653–9663 (IEEE/CVF, 2022).\n44. Beutel, D. J. et al. Flower: A friendly federated learning research\nframework. Preprint athttps://arxiv.org/abs/2007.14390 (2020).\n45. Roth, H. R. et al. Nvidiaﬂare: Federated learning from simulation to\nreal-world. Preprint athttps://arxiv.org/abs/2210.13291 (2022).\n46. Zhu, H., Xu, J., Liu, S. & Jin, Y. Federated learning on non-iid data: a\nsurvey. Neurocomputing 465, 371–390 (2021).\n47. Sun, B., Huo, H., Yang, Y. & Bai, B. Partialfed: Cross-domain\npersonalized federated learning via partial initialization.Adv. Neural\nInf. Process. Syst.34, 23309–23320 (2021).\n48. Ma, X., Zhu, J., Lin, Z., Chen, S. & Qin, Y. A state-of-the-art survey on\nsolving non-iid data in federated learning.Future Gener. Comput.\nSyst. 135, 244–258 (2022).\n49. Zhao, Y. et al. Federated learning with non-iid data. Preprint athttps://\narxiv.org/abs/1806.00582 (2018).\n50. Wang, H., Kaplan, Z., Niu, D. & Li, B. Optimizing federated learning on\nnon-iid data with reinforcement learning. In:IEEE INFOCOM 2020-\nIEEE Conference on Computer Communications(Program Chairs and\nOrganizing Committee of the IEEE INFOCOM 2020) 1698–1707 (IEEE,\n2020).\n51. Koch, V. et al. Noise transfer for unsupervised domain adaptation of\nretinal oct images. In:International Conference on Medical Image\nComputing and Computer-Assisted Intervention(eds Wang, L., Dou,\nQ., Fletcher, P.T., Speidel, S. & Li, S.) 699–708 (Springer, 2022).\n52. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale.International Conference on Learning\nRepresentations https://openreview.net/forum?id=YicbFdNTTy\n(2021).\n53. Russakovsky, O. et al. Imagenet large scale visual recognition\nchallenge. Int. J. Comput. Vis.115, 211–252 (2015).\n54. Vaswani, A. et al. Attention is all you need.Adv. Neural Inform.\nProcess. Syst.30 (2017).\n55. Rashvand, N. et al. Enhancing automatic modulation recognition for\niot applications using transformers.IoT 5, 212–226 (2024).\n56. Xu, L., Wang, L., Cheng, S. & Li, Y. Mhanet: A hybrid attention\nmechanism for retinal diseases classiﬁcation.\nPLoS ONE16,\ne0261285 (2021).\n57. Sun, Y., Zhang, H. & Yao, X. Automatic diagnosis of macular diseases\nfrom oct volume based on its two-dimensional feature map and\nconvolutional neural network with attention mechanism.J. Biomed.\nOpt. 25, 096004–096004 (2020).\n58. Deininger, L. et al. A comparative study between vision transformers\nand cnns in digital pathology. Preprint athttps://arxiv.org/abs/2206.\n00389 (2022).\n59. Li, J. et al. Transforming medical imaging with transformers? a\ncomparative review of key properties, current progresses, and future\nperspectives. Med. image Anal.85, 102762 (2023).\n60. Du, J. et al. InMedical Image Computing and Computer Assisted\nIntervention—MICCAI 2024(eds Linguraru, M. G. et al.) 709–719\n(Springer Nature Switzerland, 2024).\n61. Silva-Rodríguez, J., Chakor, H., Kobbi, R., Dolz, J. & Ben Ayed, I.\nA foundation language-image model of the retina (ﬂair): encoding\nexpert knowledge in text supervision.Med. Image Anal.99, 103357\n(2025).\n62. Gildenblat, J. & Contributors. Pytorch library for cam methods.https://\ngithub.com/jacobgil/pytorch-grad-cam (2021).\n63. Muni Nagamani, G. & Rayachoti, E. Deep learning network (dl-net)\nbased classiﬁcation and segmentation of multi-class retinal\ndiseases using oct scans.Biomed. Signal Process. Control88,\n105619 (2024).\n64. Upadhyay, P. K., Rastogi, S. & Kumar, K. Coherent convolution neural\nnetwork based retinal disease detection using optical coherence\ntomographic images.J. King Saud. Univ. - Comput. Inf. Sci.34,\n9688–9695 (2022).\n65. Kamran, S. A., Saha, S., Sabbir, A. S. & Tavakkoli, A. Optic-net: A novel\nconvolutional neural network for diagnosis of retinal diseases from\noptical tomography images. In:2019 18th IEEE International\nConference On Machine Learning And Applications (ICMLA)\n(Program Chairs and Organizing Committee of the IEEE International\nConference on Machine Learning and Applications (ICMLA))\n(Program Chairs and Organizing Committee of the IEEE International\nConference on Machine Learning and Applications (ICMLA)) 964–971\n(IEEE, 2019).\n66. Sotoudeh-Paima, S., Jodeiri, A., Hajizadeh, F. & Soltanian-Zadeh, H.\nMulti-scale convolutional neural network for automated amd\nclassiﬁcation using retinal oct images.Comput. Biol. Med.144,\n105368 (2022).\n67. Baharlouei, Z., Rabbani, H. & Plonka, G. Wavelet scattering transform\napplication in classiﬁcation of retinal abnormalities using oct images.\nSci. Rep.13, 19013 (2023).\n68. Kulyabin, M. et al. Octdl: Optical coherence tomography dataset for\nimage-based deep learning methods.Sci. Data11, 365 (2024).\n69. Zibaeirad, A., Koleini, F., Bi, S., Hou, T. & Wang, T. A comprehensive\nsurvey on the security of smart grid: Challenges, mitigations, and\nfuture research opportunities. Preprint athttps://arxiv.org/abs/2407.\n07966 (2024).\n70. Ross, T.-Y. & Dollár, G. Focal loss for dense object detection. In:Proc.\nIEEE Conference on Computer Vision and Pattern Recognition\n(Program Chairs and Organizing Committee of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), IEEE/CVF)\n2980–2988 (CVPR, IEEE/CVF, 2017).\n71. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization.\nInternational Conference on Learning Representationshttps://\nopenreview.net/forum?id=Bkg6RiCqY7 (2019).\n72. UNCC, Q. Distributed-training-of-foundation-models-for-\nophthalmic-diagnosis repository hosted on github.https://github.\ncom/QIAIUNCC/Distributed-Training-of-Foundation-Models-for-\nOphthalmic-Diagnosis (2025) (Accessed 2 Jan 2025).\n73. Kermany, D. S. et al. Identifying medical diagnoses and treatable\ndiseases by image-based deep learning.cell 172, 1122–1131 (2018).\n74. Srinivasan, P. P. et al. Fully auto635 mated detection of diabetic\nmacular edema and dry age-related macular degeneration from\noptical coherence tomography images.Biomed. Opt. Express5,\n3568–3577 (2014).\n75. Li, M. et al. Octa-500: a retinal dataset for optical coherence\ntomography angiography study.Med. Image Anal.93, 103092 (2024).\n76. Gholami, P., Roy, P., Parthasarathy, M. K. & Lakshminarayanan, V.\nOctid: Optical coherence tomography image database.Comput.\nElectr. Eng.81, 106532 (2020).\nAcknowledgements\nThis study is supported by NEI R15EY035804, R21EY035271 (MNA), UNC\nCharlotte Faculty Research Grant (MNA), NC Diabetes Research Center\nP30DK124723 (SSO), Research to Prevent Blindness (TL) and NIH Grant\nP30EY026877 (TL).\nAuthor contributions\nStudy conception and design: Sina Gholami, Fatema-E Jannat, Hamed\nTabkhivayghan, and Minhaj Nur Alam; Data collection and analysis: Sina\nGholami, Jennifer I.Lim, and Minhaj Nur Alam; Data visualization: Sina\nGholami, Hamed Tabkhivayghan, and Minhaj Nur Alam; Interpretation of\nresults: Sina Gholami and Minhaj Nur Alam; Manuscript preparation: Sina\nGholami, Minhaj Nur Alam; Manuscript review: Sina Gholami, Fatema-E\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 12\nJannat, Atalie Carina Thompson, Sally Shin Yee Ong, Jennifer I.Lim, Theo-\ndore Leng, Hamed Tabkhivayghan and Minhaj Nur Alam; Supervision: Sina\nGholami and Minhaj Nur Alam; Funding acquisition: Minhaj Nur Alam; Pro-\nject administration: Sina Gholami and Minhaj Nur Alam.\nCompeting interests\nThe authors declare no competing interests.\nEthics\nUIC dataset (DS7) was approved by the institutional review board of the\nUniversity of Illinois at Chicago and complied with the ethical standards\nstated in the Declaration of Helsinki.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s44172-025-00341-5.\nCorrespondenceand requests for materials should be addressed to\nMinhaj Nur Alam.\nPeer review informationCommunications Engineeringthanks Kaveri A.\nThakoor, Amr Elsawy, and the other, anonymous, reviewer for their\ncontribution to the peer review of this work. Primary Handling Editor:\nAnastasiia Vasylchenkova.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’sn o t eSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s44172-025-00341-5 Article\nCommunications Engineering|             (2025) 4:6 13",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.8059992790222168
    },
    {
      "name": "Training (meteorology)",
      "score": 0.6559433937072754
    },
    {
      "name": "Computer science",
      "score": 0.4328831136226654
    },
    {
      "name": "Optometry",
      "score": 0.34065869450569153
    },
    {
      "name": "Medicine",
      "score": 0.31991827487945557
    },
    {
      "name": "History",
      "score": 0.13449633121490479
    },
    {
      "name": "Geography",
      "score": 0.1177147626876831
    },
    {
      "name": "Meteorology",
      "score": 0.11456793546676636
    },
    {
      "name": "Archaeology",
      "score": 0.06415796279907227
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102149020",
      "name": "University of North Carolina at Charlotte",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I47251452",
      "name": "Wake Forest University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}