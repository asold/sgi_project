{
    "title": "Exploring the Pitfalls of Large Language Models: Inconsistency and Inaccuracy in Answering Pathology Board Examination-Style Questions",
    "url": "https://openalex.org/W4385650540",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2157102271",
            "name": "Shunsuke Koga",
            "affiliations": [
                "Hospital of the University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2157102271",
            "name": "Shunsuke Koga",
            "affiliations": [
                "Hospital of the University of Pennsylvania"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6798182279",
        "https://openalex.org/W4378647938",
        "https://openalex.org/W4376108530",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4380685958",
        "https://openalex.org/W4385230848",
        "https://openalex.org/W4380291159",
        "https://openalex.org/W4380423243",
        "https://openalex.org/W2953048903",
        "https://openalex.org/W3194565976",
        "https://openalex.org/W4323322475",
        "https://openalex.org/W4385330587",
        "https://openalex.org/W4385647263",
        "https://openalex.org/W2136464700",
        "https://openalex.org/W2167283539",
        "https://openalex.org/W3183757054",
        "https://openalex.org/W4392681182",
        "https://openalex.org/W4384821243"
    ],
    "abstract": "Abstract In the rapidly advancing field of artificial intelligence, large language models (LLMs) such as ChatGPT and Google Bard are making significant progress, with applications extending across various fields, including medicine. This study explores their potential utility and pitfalls by assessing the performance of these LLMs in answering 150 multiple-choice questions, encompassing 15 subspecialties in pathology, sourced from the PathologyOutlines.com Question Bank, a resource for pathology examination preparation. Overall, ChatGPT outperformed Google Bard, scoring 122 out of 150, while Google Bard achieved a score of 70. Additionally, we explored the consistency of these LLMs by applying a test-retest approach over a two-week interval. ChatGPT showed a consistency rate of 85%, while Google Bard exhibited a consistency rate of 61%. In-depth analysis of incorrect responses identified potential factual inaccuracies and interpretive errors. While LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models.",
    "full_text": " 1\nExploring the Pitfalls of Large Language Models: Inconsistency and Inaccuracy in \nAnswering Pathology Board Examination-Style Questions \nShunsuke Koga 1 \n \n1 Department of Pathology and Laboratory Medicine, Hospital of the University of Pennsylvania, \nPhiladelphia, Pennsylvania, USA \n \nRunning title: Pitfalls of Large Language Models  \n \nCorresponding Author: \nShunsuke Koga, MD, PhD \nDepartment of Pathology and Laboratory Medicine \nHospital of the University of Pennsylvania,  \n3400 Spruce Street, Philadelphia \nPennsylvania, 19104, USA  \nTel: (267) 872-3856 \nEmail: shunsuke.koga@pennmedicine.upenn.edu \n \nAbbreviations: AI - Artificial Intelligence; LLMs - Large Language Models \nKeywords: artificial intelligence, large language models, pathology, ChatGPT, Google Bard, \ninconsistency, comparative study, application in medicine. \n \n \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n 2\nAbstract \nIn the rapidly advancing field of artificial intelligence, large language models (LLMs) such as \nChatGPT and Google Bard are making significant progress, with applications extending across \nvarious fields, including medicine. This study explores their potential utility and pitfalls by \nassessing the performance of these LLMs in answering 150 multiple-choice questions, \nencompassing 15 subspecialties in pathology, sourced from the PathologyOutlines.com \nQuestion Bank, a resource for pathology examination preparation. Overall, ChatGPT \noutperformed Google Bard, scoring 122 out of 150, while Google Bard achieved a score of 70. \nAdditionally, we explored the consistency of these LLMs by applying a test-retest approach over \na two-week interval. ChatGPT showed a consistency rate of 85%, while Google Bard exhibited \na consistency rate of 61%. In-depth analysis of incorrect responses identified potential factual \ninaccuracies and interpretive errors. While LLMs have potential to enhance medical education \nand assist clinical decision-making, their current limitations underscore the need for continued \ndevelopment and the critical role of human expertise in the application of such models. \n \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 3\nIntroduction \nOver the past decade, artificial intelligence (AI) has made significant progress, particularly in the \ndevelopment of large language models (LLMs). These models use extensive text data for \ntraining, enabling them to generate human-like text, understand context, respond to queries, \nand facilitate language translation 1.  \nChatGPT, including its advanced versions GPT-3.5 and GPT-4, is an LLM developed by \nOpenAI that has been extensively used in a range of applications, such as writing assistance \nand programming support. Another notable LLM is Google Bard developed by Google. \nImportantly, Google Bard has the unique ability to access real-world, current information through \nGoogle Search. This capability renders it especially beneficial for answering queries that require \nthe latest data. \n LLMs, including ChatGPT, have demonstrated their potential in medical education \n2, 3. \nChatGPT not only exhibited promising performance in the United States Medical Licensing \nExam \n4, but also in equivalent medical exams in Japan and Italy 5, 6. Current research is \nexamining how LLMs perform in particular medical disciplines. For instance, GPT-4 \noutperformed other LLMs like GPT-3.5 and Google Bard in a neurosurgery oral boards \nexamination\n7. In another study comparing ChatGPT and Google Bard's responses to lung \ncancer inquiries, both models showed accuracy, though not without flaws 8. \nWhile AI and machine learning techniques, particularly image analysis 9-11, have been \nexplored in pathology, the evaluation of LLMs has not been extensively investigated 12, 13. This \nstudy seeks to address this gap by assessing the performance of ChatGPT and Google Bard \nwithin the field of pathology. The focus is on evaluating and comparing their accuracy and \nconsistency in answering board examination-style questions in pathology.  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 4\n \nMethods \nQuestion Selection and Evaluation \nThe study compared the performance of two LLMs, ChatGPT (GPT-4) and Google Bard, using \nquestions from the PathologyOutlines.com Question Bank \n(https://www.pathologyoutlines.com/review-questions), a resource for pathology examination \npreparation. \nThe question bank had 3365 questions across pathology subspecialties, but for this \nstudy, we selected 150 multiple-choice questions, with 10 from each of 15 subspecialties, to \nensure a balanced dataset: Autopsy & forensics, Bone, joints & soft tissues, Breast, \nDermatopathology, Gastrointestinal & liver, Genitourinary & adrenal, Gynecological, Head & \nNeck, Hematopathology, Informatics & Digital Pathology, Medical renal, Neuropathology, Stains \n& CD markers/Immunohistochemistry, Thoracic, and Clinical Pathology. Each question was \npresented in a single best answer, multiple-choice format. Both LLMs were presented with the \nsame set of questions. No additional context or hints were provided to the models apart from the \nquestions themselves, to simulate real-world application. Questions containing images were \nexcluded from the question bank as ChatGPT is not capable of processing image data. \nTo evaluate the consistency of both LLMs, a test-retest approach was implemented. The \nsame set of 150 questions was posed to the models on two separate occasions, with a two-\nweek interval. By comparing the answers on both performances, we aimed to assess the \nconsistency in their responses. \n \nStatistical analyses \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 5\nStatistical analyses were performed using R version 4.3.1 (R Foundation for Statistical \nComputing, Vienna, Austria). A Chi-square test was conducted to compare the performance of \nChatGPT and Google Bard. Statistical significance was set at p < 0.05.  \n \nResults \nOverall test scores \nThe performance of the two LLMs, ChatGPT and Google Bard, was evaluated across 15 \nsubspecialties in pathology. Overall, ChatGPT significantly outperformed Google Bard across all \nsubspecialties; ChatGPT achieved a total score of 122 out of 150, compared to Google Bard's \nscore of 70 (p < .001). Detailed performance outcomes of each LLM across all subspecialties \nare presented in Table 1.  \n \nAssessment of Consistency \nIn the assessment of consistency of both LLMs, test scores were largely consistent between the \nfirst and second sessions. The scores of ChatGPT were 122 and 126 out of 150 in the first and \nsecond tests, respectively, while Google Bard scored 70 and 69 in the same tests. Despite the \nrelative stability in test scores, a detailed inspection revealed significant changes; identical \nanswers in both sessions were present in only 85% (127/150) of ChatGPT's responses and a \nlower 61% (92/150) of Google Bard's (Table 2). ChatGPT initially provided 28 incorrect answers. \nIn the retest, it corrected 11 of these but also altered 7 correct answers to incorrect ones. \nAmong the initial errors, ChatGPT repeated 5. Google Bard exhibited a similar but more \npronounced pattern. Starting with 80 incorrect responses, it corrected 19 in the retest, made \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 6\nnew errors in 20 previously correct answers, and repeated 19 of its original mistakes in the \nretest. \n \nEvaluation of Incorrect Responses \nWe identified incorrect answers from both LLMs in our study. One example was a question \nabout Lynch syndrome: \"Lynch syndrome usually arises from a germline mutation in a gene \ncoding for a mismatch repair protein. A germline mutation in which of the following genes could \nalso cause Lynch syndrome?\" with options A. BRAF, B. CDH1, C. EPCAM, D. MUTYH. The \ncorrect answer was C. EPCAM 14. However, Google Bard incorrectly chose option D. MUTYH \nand justified its answer by associating MUTYH mutations with Lynch syndrome, which is a \nfactual inaccuracy as MUTYH mutations cause a different type of hereditary colorectal cancer \nknown as MUTYH-associated polyposis 15. In contrast, ChatGPT correctly selected the answer \nas C. EPCAM. \nAnother example was a question about nemaline myopathy: “In which gene are de novo \nmutations most commonly associated with nemaline myopathy?” with options A. NEB, B. \nKLHL40, C. TPM3, D. ACTA1, and E. TNNT1. The correct answer was D. ACTA1, but both \nmodels selected A. NEB. Specifically, ChatGPT justified its answer by stating that NEB is the \nmost commonly involved gene in cases of nemaline myopathy, while Google Bard indicated that \nde novo mutations in the NEB gene are the most common cause of this condition. These \nresponses exhibit factual and interpretative inaccuracies, as NEB mutations, while common in \nnemaline myopathy, are typically inherited in an autosomal recessive manner, not de novo \n16. \n \nDiscussion \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 7\nOur study further underscores the strengths and weaknesses of LLMs in medicine. While \nChatGPT consistently surpassed Google Bard in accuracy and consistency, neither model \nanswered all questions correctly, suggesting knowledge or comprehension gaps. Additionally, \nretesting after two weeks revealed inconsistencies in both LLMs' responses to the same \nquestions, highlighting potential reliability issues 17. \nIn assessing LLMs' comprehension of medical queries, our study identified two error \ntypes. Firstly, Google Bard displayed factual inaccuracies, incorrectly linking MUTYH to Lynch \nsyndrome despite vast data access. Secondly, ChatGPT exhibited interpretation errors. While \nanswering a question about \"de novo\" mutations in nemaline myopathy, it correctly identified \nNEB as a common cause but overlooked the specific \"de novo\" context, highlighting LLMs' \npotential for nuanced misunderstandings.  \nAnother important consideration in the application of LLMs in medical fields is their \nconsistency or reliability, defined as the models' ability to provide the same answer to identical \nprompts when asked on different occasions. Our assessment of consistency revealed a \nsuboptimal consistency rate for both LLMs (i.e., 85% in ChatGPT and 61% in Google Bard), \nwhich are  consistent with the results of another study that evaluated ChatGPT's responses to \nsurgical case questions \n18. Such inconsistencies underline the current limitations of LLMs and \nhighlights the necessity for further development and refinement to improve their consistency for \neffective use in the medical field. \nThere are some limitations of our study. Firstly, there was no direct comparison with \nhuman performance. While our results shed light on the capabilities of LLMs in answering \ncomplex medical questions, understanding how their performance compares directly to medical \nstudents or professionals remains crucial. Additionally, our focus was largely on pathology \nquestions in English language. To generalize our findings, future studies should encompass \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 8\ndifferent medical specialties and languages. Lastly, the challenge of incorporating images into \nour evaluation also presents a significant limitation.  \nIn conclusion, our study indicates that LLMs have the potential to assist in clinical \ndecision-making in the future. Both models demonstrated inconsistencies and inaccuracies, \nemphasizing the need for their further development and rigorous validation. While the potential \nof these AI models is promising, human oversight and expertise remain crucial in the medical \nfield. \n \nAcknowledgement  \nThis manuscript was edited and proofread by ChatGPT (GPT-4, OpenAI), and the author \nverified the final content.  \n \nDisclosure Statement \nNone. \n \nAuthor Contributions \nConception and design of the study: SK \nAcquisition and analysis of data: SK \nDrafting the manuscript: SK \n \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 9\nReferences \n1 Chen M, Tworek J, Jun H , et al. Evaluating Large Language Models Trained on Code.  \n2021; arXiv:2107.03374. \n2 Koga S. The Potential of ChatGPT in Medical Education: Focusing on USMLE \nPreparation. Ann Biomed Eng. 2023. \n3 Feng S, Shen Y . ChatGPT and the Future of Medical Education. Acad Med. 2023; 98: \n867-68. \n4 Kung TH, Cheatham M, Medenilla A , et al. Performance of ChatGPT on USMLE: \nPotential for AI-assisted medical education using large language models. PLOS Digit Health. \n2023; 2: e0000198. \n5 Takagi S, Watari T, Erabi A, Sakaguchi K. Performance of GPT-3.5 and GPT-4 on the \nJapanese Medical Licensing Examination: Comparison Study. JMIR Med Educ. 2023; 9: e48002. \n6 Alessandri Bonetti M, Giorgino R, Gallo Afflitto G, De Lorenzi F, Egro FM. How Does \nChatGPT Perform on the Italian Residency Admission National Exam Compared to 15,869 \nMedical Graduates? Ann Biomed Eng. 2023. \n7 Ali R, Tang OY , Connolly ID , et al. Performance of ChatGPT, GPT-4, and Google Bard on \na Neurosurgery Oral Boards Preparation Question Bank. Neurosurgery. 2023. \n8 Rahsepar AA, Tavakoli N, Kim GHJ, Hassani C, Abtin F, Bedayat A. How AI Responds to \nCommon Lung Cancer Questions: ChatGPT vs Google Bard. Radiology. 2023; 307: e230922. \n9 Komura D, Ishikawa S. Machine learning approaches for pathologic diagnosis. Virchows \nArch. 2019; 475: 131-38. \n10 Koga S, Ikeda A, Dickson DW. Deep learning-based model for diagnosing Alzheimer's \ndisease and tauopathies. Neuropathol Appl Neurobiol. 2022; 48: e12759. \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 10\n11 Kim M, Sekiya H, Yao G , et al. Diagnosis of Alzheimer Disease and Tauopathies on \nWhole-Slide Histopathology Images Using a Weakly Supervised Deep Learning Algorithm. Lab \nInvest. 2023; 103: 100127. \n12 Schukow C, Smith SC, Landgrebe E , et al. Application of ChatGPT in Routine Diagnostic \nPathology: Promises, Pitfalls, and Potential Future Directions. Adv Anat Pathol. 2023. \n13 Koga S, Martin NB, Dickson DW. Evaluating the performance of large language models: \nChatGPT and Google Bard in generating differential diagnoses in clinicopathological \nconferences of neurodegenerative disorders. Brain Pathol. 2023: e13207. \n14 Tutlewska K, Lubinski J, Kurzawski G. Germline deletions in the EPCAM gene as a \ncause of Lynch syndrome - literature review. Hered Cancer Clin Pract. 2013; 11: 9. \n15 Nielsen M, Morreau H, Vasen HF, Hes FJ. MUTYH-associated polyposis (MAP). Crit Rev \nOncol Hematol. 2011; 79: 1-16. \n16 Laitila J, Wallgren-Pettersson C. Recent advances in nemaline myopathy. Neuromuscul \nDisord. 2021; 31: 955-67. \n17 Chen L, Zaharia M, Zou J. How is ChatGPT's behavior changing over time?  2023; \narXiv:2307.09009. \n18 Beaulieu-Jones BR, Shah S, Berrigan MT, Marwaha JS, Lai S-L, Brat GA. Evaluating \nCapabilities of Large Language Models: Performance of GPT4 on Surgical Knowledge \nAssessments. medRxiv. 2023: 2023.07.16.23292743. \n \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 11\nTable 1: Performance scores of ChatGPT and Google Bard across pathology subspecialties \nSubspeciality ChatGPT Google Bard \nAutopsy & Forensics 9 6 \nBone, Joints & Soft Tissues 7 3 \nBreast 7 3 \nDermatopathology 8 5 \nGastrointestinal & Liver 9 6 \nGenitourinary & Adrenal 6 4 \nGynecological 9 7 \nHead & Neck 9 3 \nHematopathology 9 5 \nInformatics & Digital Pathology 9 7 \nMedical Renal 9 3 \nNeuropathology 8 5 \nStains & CD markers/Immunohistochemistry 8 3 \nThoracic 8 5 \nClinical Pathology 7 5 \nTotal 122 70 \n \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint \n 12\nTable 2: Consistency of Model Responses \nOutcome ChatGPT Google Bard \nNo change in response 127 (85%) 92 (61%) \nCorrect to incorrect response 7 (5%) 20 (13%) \nIncorrect to another incorrect response 5 (3%) 19 (13%) \nIncorrect to correct response 11 (7%) 19 (13%) \n \n \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.03.23293401doi: medRxiv preprint "
}