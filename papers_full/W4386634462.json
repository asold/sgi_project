{
  "title": "An Efficient Human Activity Recognition Using Hybrid Features and Transformer Model",
  "url": "https://openalex.org/W4386634462",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A51646430",
      "name": "Oumaima Saidani",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A2778570605",
      "name": "Majed Alsafyani",
      "affiliations": [
        "Taif University"
      ]
    },
    {
      "id": "https://openalex.org/A1274299711",
      "name": "Roobaea Alroobaea",
      "affiliations": [
        "Taif University"
      ]
    },
    {
      "id": "https://openalex.org/A1973886088",
      "name": "Nazik Alturki",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A2715761483",
      "name": "Rashid Jahangir",
      "affiliations": [
        "COMSATS University Islamabad"
      ]
    },
    {
      "id": "https://openalex.org/A2803713657",
      "name": "Leila Jamel",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3091978306",
    "https://openalex.org/W2770265759",
    "https://openalex.org/W4205964587",
    "https://openalex.org/W3183457194",
    "https://openalex.org/W2954709787",
    "https://openalex.org/W4286681770",
    "https://openalex.org/W4280644903",
    "https://openalex.org/W2342792048",
    "https://openalex.org/W4384502051",
    "https://openalex.org/W2907255073",
    "https://openalex.org/W4296251855",
    "https://openalex.org/W2984877669",
    "https://openalex.org/W4320015922",
    "https://openalex.org/W6776937213",
    "https://openalex.org/W2759690896",
    "https://openalex.org/W3166580990",
    "https://openalex.org/W3088369757",
    "https://openalex.org/W2795342689",
    "https://openalex.org/W3207244090",
    "https://openalex.org/W3094378471",
    "https://openalex.org/W2981293741",
    "https://openalex.org/W3180345578",
    "https://openalex.org/W1840860027",
    "https://openalex.org/W2066837458",
    "https://openalex.org/W3011785450",
    "https://openalex.org/W6748428005",
    "https://openalex.org/W4206703604",
    "https://openalex.org/W3154985639",
    "https://openalex.org/W3003646123",
    "https://openalex.org/W2998376881",
    "https://openalex.org/W3105107487",
    "https://openalex.org/W4285079306",
    "https://openalex.org/W3093389651",
    "https://openalex.org/W3215162869",
    "https://openalex.org/W2790363406",
    "https://openalex.org/W3023215041",
    "https://openalex.org/W4207050394"
  ],
  "abstract": "Human activity recognition is a challenging and active research topic in computer science due to its applications in video surveillance, health monitoring, rehabilitation, human-robot interaction, robotics, gesture and posture analysis, and sports. In the past, various studies have utilized manual features to identify human activities and obtained good accuracy. Nonetheless, the performance of such features degraded in complex situations. Therefore, recent research used deep learning (DL) techniques to capture the local features automatically from given activity instances. Though automatic feature extraction overcomes the problems of manual features, there is still a need to enhance the efficiency and accuracy of existing techniques. The motivation behind this research is to improve the efficiency and accuracy of HAR systems. This research proposed a HAR system, which applies data enhancement techniques before capturing robust and discriminative features set from each activity instance. The captured feature set is given to the transformer model for activities recognition using the PAMAP2, UCI HAR, and WISDM datasets. The achieved results revealed that the proposed HAR model outperformed the baseline methods. Specifically, the proposed HAR achieved 98.2&#x0025; accuracy for PAMAP2 with all instances in 12 activities, 98.6&#x0025; accuracy for UCI HAR with all instances in 6 activities, 97.3&#x0025; for WISDM with all instances in 6 activities. The advantage of the proposed hybrid features is the capability to capture both low-level and high-level information from the sensor data, potentially enhancing the discriminative power of the system. In addition, this study employed a transformer a model due to its ability to capture long-range dependencies, which are beneficial in recognizing complex human activities patterns.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nAn Efficient Human Activity Recognition\nusing Hybrid Features and Transformer\nModel\nOUMAIMA SAIDANI1, MAJED ALSAFYANI2, ROOBAEA ALROOBAEA2, NAZIK ALTURKI1,\nRASHID JAHANGIR3, LEILA JAMEL MENZLI1\n1Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, P.O. Box 84428, Riyadh\n11671, Saudi Arabia.\n2Department of Computer Science, College of Computers and Information Technology, Taif University, P. O. Box 11099, Taif 21944, Saudi Arabia.\n3Department of Computer Science, COMSATS University Islamabad, Vehari Campus, Pakistan.\nCorresponding authors: Nazik Alturki (namalturki@pnu.edu.sa)\nThis work is supported by Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2023R333),\nPrincess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia and Taif University Researchers Supporting Project number\n(TURSP-2020/36), Taif University, Taif, Saudi Arabia.\nABSTRACT Human activity recognition is a challenging and active research topic in computer science\ndue to its applications in video surveillance, health monitoring, rehabilitation, human-robot interaction,\nrobotics, gesture and posture analysis, and sports. In the past, various studies have utilized manual features\nto identify human activities and obtained good accuracy. Nonetheless, the performance of such features\ndegraded in complex situations. Therefore, recent research used deep learning (DL) techniques to capture the\nlocal features automatically from given activity instances. Though automatic feature extraction overcomes\nthe problems of manual features, there is still a need to enhance the efficiency and accuracy of existing\ntechniques. The motivation behind this research is to improve the efficiency and accuracy of HAR systems.\nThis research proposed a HAR system, which applies data enhancement techniques before capturing\nrobust and discriminative features set from each activity instance. The captured feature set is given to the\ntransformer model for activities recognition using the PAMAP2, UCI HAR, and WISDM datasets. The\nachieved results revealed that the proposed HAR model outperformed the baseline methods. Specifically,\nthe proposed HAR achieved 98.2% accuracy for PAMAP2 with all instances in 12 activities, 98.6% accuracy\nfor UCI HAR with all instances in 6 activities, 97.3% for WISDM with all instances in 6 activities.\nThe advantage of the proposed hybrid features is the capability to capture both low-level and high-level\ninformation from the sensor data, potentially enhancing the discriminative power of the system. In addition,\nthis study employed a transformer a model due to its ability to capture long-range dependencies, which are\nbeneficial in recognizing complex human activities patterns.\nINDEX TERMS Human Activity Recognition, HAR, Transformer, Hybrid Features, PAMP2\nI. INTRODUCTION\nHuman activity recognition (HAR) is the process of automat-\nically identifying actions and behaviors based on sensors data\n[1]. HAR has become a rapidly growing field of research due\nto its of applications in several areas, including healthcare\n[2], sports [3], robotics [4], and security [5] as shown in\nFigure 1. The aim of HAR system is to propose and de-\nvelop accurate and robust algorithms that can identify and\nrecognize human activities with high precision, regardless of\nthe environment or context. The importance of HAR lies in\nits ability to monitor and track human activities in real-time,\nwhich can provide valuable insights and support for several\napplications. For instance, in healthcare, HAR system can\nbe employed to monitor the activities of elderly or disabled\nindividuals and provide assistance when needed. In sports,\nHAR system can be utilized to monitor and analyze the\nmovements of athletes to identify areas for improvement. In\nrobotics, HAR can be used to enable robots to interact with\nhumans in a more natural and intuitive manner. In security,\nHAR system can be utilized to detect and monitor suspicious\nactivities in public places. The process of HAR typically\ninvolves three main stages: data collection, feature retrieval,\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nand classification [6] as shown in Figure 2. In the first stage,\nsensor data is collected from various sources, such as ac-\ncelerometers, gyroscopes, and video cameras. In the second\nstage, relevant features are retrieved from the raw signal\ndata, such as frequency-domain, time-domain, and statistical\nfeatures. In the last stage, classification algorithms are em-\nployed to classify the retrieved features into different activity\nclasses, such as walking, running, sitting, and standing. Deep\nlearning (DL) methods have shown great potential in HAR\ndue to their ability to learn complex and high-level features\nfrom raw sensors data. DL methods, such as recurrent neural\nnetworks (RNNs) and convolutional neural networks (CNNs)\nhave shown great potential in accurately identifying activities\nfrom raw sensor data, such as accelerometer and gyroscope\nreadings. By leveraging the power of DL, researchers aim\nto improve the reliability and accuracy of HAR systems,\nultimately leading to more efficient and effective applications\nin various domains.\nFIGURE 1: Applications of Human Activity Recognition Sys-\ntem\nThe key challenge in HAR is to deal with the variability\nand complexity of human activities [7]. Human activities\ncan vary greatly in terms of duration, intensity, and context,\nmaking it difficult to build robust and accurate recognition al-\ngorithms. Additionally, the choice of sensors and their place-\nment can also impact the performance of HAR algorithms.\nFor example, wearable sensors may provide more accurate\ndata but may be more intrusive for the user, while non-\nwearable sensors may be less accurate but more convenient\nfor the user. Recently, there has been a growing interest\nin developing HAR algorithms that can work in real-world\nenvironments, where the activities may be unstructured and\nunpredictable. One approach to address this challenge is to\nuse multi-modal data fusion, where raw data from various\nsensors is merged to offer a more detailed view of the activity\nbeing performed [8].\nIn this study, several features are captured from smart-\nphone time-series sensors data for HAR. These features\ninclude: tonnetz representations, mel-spectrogram, spectral\ncontrast, chromagram and MFCCs and its variants. Subse-\nquently, these features were given to the transformer model\nto recognize human activities. To evaluate the performance,\nthree public datasets (PAMAP2, UCI, WISDM) were uti-\nlized. The proposed HAR technique achieved better recog-\nnition results for all three datasets. Following are the four\ncontributions of this study.\n• Captured robust and relevant features for HAR.\n• The data enhancement methods are employed to en-\nhance the size of training data.\n• A transformer model was employed to increase the iden-\ntification rate of human activities and lessen the training\ntime of model under limited computational resources.\n• The HAR technique using a transformer model achieved\nbetter identification results compared to the existing\nHAR techniques.\nThe remaining paper is organized as follows. Section\nII reports the related work on HAR methods. Section III\ndescribes the datasets, extraction of relevant features, trans-\nformer model employed for HAR and performance evalu-\nation metrics. Section IV presents the results of different\nexperiments and discussion of the findings. Finally, the con-\nclusions and future directions are presented in Section V.\nII. RELATED WORK\nNumerous research has been conducted for HAR from the\nmobile and wearable sensors data using DL approaches.\nNevertheless, the efficiency and performance of HAR sys-\ntems depends on the retrieval of relevant feature and the\nselection of the suitable model that recognize the human\nactivities. This section reviews the latest studies published\nfor the development of HAR systems using DL approaches.\nA. MOBILE ACCELEROMETER-BASED MODELS\nWan, et al. [9] designed a mobile accelerometer-based model\nfor HAR. The proposed model collected the sensory data\nfrom the daily activities of various individuals. The collected\ndata was preprocessed by normalization, denoising and seg-\nmentation. In second phase, the discriminative and robust\nfeature vectors were extracted from the three-axis accelerom-\neters preprocessed data. These feature vectors were fed as\ninput to a CNN for the retrieval of local features. Finally,\nfive DL models were evaluated on UCI HAR and PAMP2\ndatasets. The achieved results revealed that the proposed\napproach outperformed the existing approaches. In [10] pro-\nposed a novel DL model by fusing RNN and inception\nneural network. The multi-channel sensors data was given\nto the HAR model. The authors derived multi-dimensions\nfeatures by using several kernel-based convolutional layers.\nThe results verified that the proposed method obtained the\nconsistent better results on widely used HAR datasets, when\ncompared with baseline methods.\nAnother study [35] proposed a DL-based technique for\nHAR with smartphone gyroscope and accelerometer data.\nThe authors combine three DL models named CNN, autoen-\ncoders, and LSTM. The CNN model was used to extract\nautomatic features, autoencoders were employed to reduce\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nFIGURE 2: An illustration of HAR process using conventional approaches.\nthe dimensionality and LSTM was adopted for temporal\nmodeling. The proposed ConvAE-LSTM architecture was\nevaluated on four benchmark datasets (PAMAP2, WISDM,\nOPPORTUNITY and UCI). The results revealed that the\nproposed technique improved the performance in terms of\naccuracy and computational time over baseline methods.\nB. DEEP CONVOLUTIONAL NEURAL NETWORKS\nIn another research [11], authors presented a deep CNN\nmodel to identify human activities efficiently from mobile\nsensors data. The authors provided a method to extract au-\ntomatic robust features from raw time-series data. Experi-\nments revealed that the deep convnets retrieved complex and\nrelevant features with each additional layer. The proposed\nmodel also achieved optimal results on moving activities and\noverall accuracy of 95% on the test set. Similarly, Hassan,\net al. [12] proposed a mobile sensors-based technique for\nHAR. In first phase, the authors extracted the efficient fea-\ntures (median, mean, autoregressive coefficients etc.) from\nraw data. Secondly, the features were handled by principal\ncomponent analysis and linear discriminant analysis to make\nthem more relevant and robust. Lastly, the feature were\nutilized to train the Deep Belief Network for identification\nof activities. The authors’ proposed technique was compared\nwith baseline methods such as artificial neural network and\nsupport vector machine (SVM). The experiments showed that\nthe proposed technique outperformed traditional methods. To\nchoose the optimum parameters of the CNN, Raziani and\nAzimbagirad [31] introduced a one-dimensional CNN for\nHAR and investigated seven metaheuristic techniques. The\nUCI HAR dataset was used to evaluate the optimization algo-\nrithms. The achieved performance revealed the robustness of\nmetaheuristic algorithms to optimize the parameters of CNN.\nTo capture wide range of receptive fields of HAR in\neach feature layer, Tang, et al. [32] proposed a CNN that\nused the concept of hierarchical-split (HS). The experiments\nwere conducted on benchmarks datasets and results demon-\nstrated that the HS method achieved impressive performance\ncompared to the baseline models on WISDM, UCI-HAR,\nPAMAP2, and UNIMIB-SHAR. Finally, the authors demon-\nstrated the multiscale receptive fields to derive the discrimi-\nnative features.\nC. HYBRID DL MODELS AND FEDERATED LEARNING\nTo investigate the efficiency of integration of DL models\nin recognizing human activities, [13] applied four different\nhybrid DL models. Each model integrated a CNN with a\nvariant of RNN. A PAMAP2 dataset was used to analyse\nthe results of the hybrid DL models. The achieved results\nshowed an optimal performance of each hybrid DL model as\ncompared to the RNN and CNN individually. Nevertheless,\nthis performance was achieved at the cost of high com-\nputational time. To facilitate the informed and data-driven\ndecision making, [14] proposed a deep neural network for\nHAR using multiple sensor data. Exclusively, the reported\nmethod encoded the time-series signal data to ensure the\nrelevant features for HAR. A residual network was adopted\nby combining two deep networks and training diverse sensor\ndata. Moreover, distinct layers were utilized to deal with\nthe variations in dataset size. The model was tested on\ntwo HAR datasets, which comprised several heterogeneous\nsmartphone sensor combinations. The findings demonstrated\nthat the HAR model outperformed other competing models.\nThe traditional machine learning approaches for HAR have\nfailed to protect users’ sensitive information and privacy in\nthe process of achieving high performance. To handle this,\n[15] designed a federated learning model for HAR. This\nmodel enabled all users to recognize its activity safely. To\nretrieve suitable features from data, this model designed a\nperceptive extraction network. This network was responsible\nto discover local features from data and the relation network\nfocused on extracting global relationships in sensors data.\nThe authors used four datasets, i.e., WISDM, UCI-HAR,\nPAMAP2, and OPPORTUNITY to evaluate the performance.\nThe results demonstrated that perceptive extraction network\noutperformed 14 baseline HAR techniques on these datasets.\nIn another study [33], the authors explored environmental\ncontexts, such as noise level and illumination, to support\nraw sensors data using a hybrid CNN–LSTM model. The\nproposed approach performed fusion of rich contextual data\nwith low-level sensor data to enhance the generalization\nand recognition accuracy. In first experiment, the authors\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nemployed triaxial sensing signals for training the baseline\nmodels. While in second experiment, the triaxial sensing\nsignals were combined with contextual information. The\nresults revealed that contextual information (noise and light)\nachieved better accuracy compared to the baseline HAR\nmodels.\nD. USER-INDEPENDENT DL TECHNIQUE\nTo overcome the issue of computational cost and handcrafted\nfeature engineering, [16] presented a user-independent DL\ntechnique online HAR. This study employed CNN to cap-\nture local feature fused with statistical features to retain\ninformation regarding global form of time-series. Moreover,\nthis study examined the effect of the length of time-series\non model performance and limited it to 1 second for real-\ntime HAR. The performance of the technique was evaluated\non two widely used UCI HAR and WISDM datasets. The\nfindings showed that the proposed technique outperformed\nbaseline techniques.\nIII. PROPOSED METHODOLOGY\nThis section reports the comprehensive research methodol-\nogy used to recognize human activities from sensors raw\ndata. In the proposed method, various features including\nspectrograms, tonnetz, chroma, MFCCs, and spectral con-\ntrast are retrieved from each data sample and used for training\nthe transformer model. Lastly, the HAR model is evaluated\nemploying three datasets (PAMAP2, UCI, WISDM) based\non training time, accuracy, and robustness. The description\nof proposed HAR methodology is illustrated in Figure 3.\nThe details of various phases is presented in subsequent\nsubsections.\nA. DATASETS\nThis research utilized three public datasets: PAMAP2, UCI\nand WISDM. These datasets include a wide range of phys-\nical activities, including postures and both repetitive and\nnon-repetitive movements. Furthermore, the Pamap2 dataset\ncontains 27 signals collected in a laboratory, whereas the\nOpportunity dataset includes 113 signals collected in the wild\nenvironment. Each dataset used different number of sensors:\n9 and 39 sensors, respectively. The detail of each dataset is\npresented below.\n1) WISDM Dataset\nThis dataset recorded by the Wireless Sensor Data Mining\nLab, is a publicly available benchmark HAR dataset. This\ndataset was collected by performing specific set of daily ac-\ntivities from 36 subjects. The participants placed an Android\nmobile in front pocket of their pants and performed vari-\nous activities including sitting, jogging, upstairs, downstairs,\nstanding, and walking for specific duration. An embedded\n3-axial (x, y, and z) accelerometer was utilized to measure\nchanges in linear acceleration, which provides valuable in-\nformation about human movement and activity patterns after\nevery 50 ms.\nWISDM dataset contains 1098209 activities instances as\nshown in Figure 5. A designated subject monitored the data\ncollection process to ensure the data of high quality. The\nsignals of all activities are depicted in Figure 7.\n2) UCI HAR Dataset\nThis balance dataset as shown in Figure 4 is widely utilized\nin human activity recognition research and is collected by\nrecording the 6 daily movements of 30 subjects. Strapping a\nSamsung Galaxy SII on the waist, all the subjects performed\n6 activities including sit, stand, walk, lying, upstairs, and\ndownstairs. The smartphone embedded gyroscope and an\naccelerometer sampling at 50 Hz are used to record 3-axial\n(x, y, z) angular velocity as well as linear acceleration. The\nangular velocity and linear acceleration collected separately\nfrom a smartphone are utilized as activity data. The instances\nof each activity have been video-recorded and labelled man-\nually.\n3) PAMAP2 Dataset\nThe Pamap2 dataset contains recordings of 12 distinct phys-\nical activities from 9 subjects (8 male and 1 female) wearing\n3 Inertial Measurement Units (IMUs). These IMUs were\nplaced on the body at three different locations: wrist, chest,\nand ankle. Each IMU includes a 3D magnetometer, a 3D\ngyroscope, and two 3D accelerometers. The magnetometer\nmeasures the magnetic field, the gyroscope measures angular\nvelocity, and the accelerometer measures linear acceleration.\nThe magnetometer measures the magnetic field, which can\nbe useful in certain scenarios. For instance, it can help to\ndetermine the orientation or direction of motion. However,\nthe impact of using a magnetometer for activity recognition\nmay vary depending on the context and the specific activ-\nities being recognized. Considering the typology of phys-\nical activity formerly described, Pamap2 dataset contains\n9 repetitive activities (running, walking, cycling, ascending\nstairs, vacuum cleaning, Nordic walk, descending stairs, rope\njumping, and ironing) and 3 postures (standing, lying, and\nsitting). Most of the activity instances in this dataset lasted\nfour minutes except rope jumping, descending stairs, and\nascending stairs because of the constraints of the building or\nto avoid the fatigue of the participants as shown in Figure 6.\nTable 1 presents the summary about these datasets.\nB. DATA AUGMENTATION\nThis technique is a widely used in HAR systems to enhance\ngeneralization and robustness while preventing overfitting.\nThis approach not only improves performance, but also\npromotes data invariance by improving the distribution of\nthe data [17]. Since DL techniques require a large data\nsize, three methods including time stretch, pitch shift and\nGaussian noise are used to increase the training data, and to\nimprove the generalization of the model. Time stretch was\nused to change the temporal duration of an activity sequence\nwithout altering its content. This method simulates variations\nin the speed at which activities are performed, which can\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nFIGURE 3: Proposed Methodology for HAR\nTABLE 1: Information of datasets employed in experiments.\nPAMAP2 UCI HAR WISDM\nTotal Instances 18664 10299 1098209\nSubjects 9 30 36\nActivities 12 6 6\nDevice IMUs Samsung Galaxy S II Android Mobile\nMagnetometer ✓ × ×\nGyroscope ✓ ✓ ×\nAccelerometer ✓ ✓ ✓\nSample Rate 100Hz 50Hz 20Hz\nActivity Label count count count\nDownstairs A1 981 986 100427\nJogging A 2 × × 342177\nSitting A3 1783 1286 59939\nStanding A4 1832 1374 48395\nUpstairs A5 1102 1073 122869\nWalking A6 2321 1226 424400\nLying A 7 × 1407 ×\nVaccum A 8 1685 × ×\nRunning A 9 931 × ×\nNordic Walk A 10 1821 × ×\nJumping A 11 449 × ×\nIroning A 12 2317 × ×\nCycling A 13 1585 × ×\nshows the common activity in each dataset\noccur naturally in real-world scenarios. Time stretch helps the model handle activities performed at different speeds,\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nFIGURE 4: % of activities in UCI HAR dataset.\nFIGURE 5: % of activities in WISDM dataset.\nFIGURE 6: % of activities in PAMP2 dataset.\nmaking it more robust to variations in the execution tempo\nand enables the model to recognize the same activity, even\nif it occurs faster or slower than during training. Secondly,\npitch shift method was used to alter the frequency or pitch of\nan activity without changing its duration. By applying pitch\nshift, the model becomes more capable of recognizing activ-\nities even when the pitch of activity is altered. Finally, the\nGaussian Noise technique was applied to introduce random\nvariations to the data points, which helps mimic noise and\nuncertainties present in real-world sensor measurements and\ndata acquisition. By including Gaussian noise in the data,\nthe model becomes less sensitive to small fluctuations or\nmeasurement errors that might be present in the input data\nduring actual deployment [18]. It is significant to select the\ncorrect value of \"σ\" parameter for the amplitude of noise.\nC. FEATURE EXTRACTION\nRaw sensor data can be quite high-dimensional which can\nlead to challenges in training deep neural networks effi-\nciently, as the models need to process and learn from vast\namounts of data. Secondly, Raw sensor data can be mas-\nsive, which could lead to longer training times and poten-\ntially require more advanced hardware infrastructure. Lastly,\nDL models are prone to overfitting, when trained on high-\ndimensional raw data. Converting raw signals to classical\nfeature sets can help reduce the dimensionality and make\nthe data more compact and representative for DL models.\nTherefore, the retrieval of robust and discriminative features\nthat correctly recognize activities from sensors time series\ndata is an important phase to achieve the high performance of\nthe HAR system [19]. The relevant features can significantly\naffect the accuracy and efficiency of HAR system, while\nirrelevant features can enhance the model training time [20].\nThis research used Librosa python library to capture the\nseven features from each sample of time series sensor data.\nThese features include:\n• Tonnetz representation\n• Mel-spectrogram\n• Spectral contrast\n• Chromagram\n• MFCCs\n• delta MFCCs\n• delta-delta MFCCs\nMFCCs and its variants are widely utilized in the area of\nsignal processing and HAR [21], [22]. To capture MFCCs\nfeature, the signal is split into frames of fixed length. Sec-\nondly, an operation called windowing is performed to reduce\nthe silence from each frame of a signal. Thet-domain activity\nsignal is then transformed into thef-domain by computing the\nFast Fourier Transform (FFT). The Mel scale filter is used\nto measure all values calculated from the FFT. Afterwards,\npowers logs are calculated at every Mel-frequency and lastly\neach log Mel spectrum is converted back to t-domain by\napplying DCT (Discrete Cosine Transform). Finally, derived\namplitudes from resulting spectrum are known as MFCCs.\nThis research captured 40 MFCCs feature along with its vari-\nants (40 ∆ MFCCs and 40 ∆∆ MFCCs). Though, MFCCs\nare proven very successful in monitoring and identifying the\ntimbre variations in activity signal, they strive to differentiate\nthe pitch and representations of harmony [23]. To resolve this\nproblem, chroma features are derived from the activity signal\nthrough binning techniques and short-time Fourier Transform\n(STFT). In this research, the chromagram information is\ncaptured for each frame of the activity signal and converted\nit to one coefficient.\nTo extract melspectrogram feature, a signal was split into a\nnumber of frames and then FFT was calculated for all frames.\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\n(a) sitting\n (b) lying\n(c) upstairs\n (d) downstairs\n(e) walking\n (f) standing\nFIGURE 7: The signals of six activities from WISDM datase.\nAfterwards, a Mel-scale was produced by first splitting\nthe spectrum of frequency into equally spaced frequencies.\nLastly, the frequencies were computed on the Mel-scale for\neach frame of the activity signal. Tonnetz feature is a 6-\ndimensions pitch space, which shows the pitch relationships\nin the rise and fall of activity signal. In this paper, the tonal\ncentroid representations were computed for each frame of\nthe activity signal. Spectral contrast measures the RMS (root\nmean square) variation between spectral peak and spectral\ndepression for each frame. This research extracted 273 fea-\ntures (6 tonnetz, 128 melspectrogram, 7 spectral contrast, 12\nchromagram, 40 MFCCs, 40 delta-MFCCs, 40 delta-delta\nMFCCs) to integrate the diverse characteristics of signal such\nas pitch, harmony, timber etc. into one training sample. The\ncorrelation between extracted features is shown in Figure 8.\nD. TRANSFORMER MODEL\nTransformer models are a type of DL techniques that have\ngained popularity in recent years for their ability to model\nsequential data. This technique has demonstrated great po-\ntential in various natural language processing and computer\nFIGURE 8: Correlation Heatmap between Audio Features\nvision tasks, including HAR. Transformer models are partic-\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nularly well-suited for this task because they can capture long-\nrange temporal dependencies between activities and their\nassociated sensor data. In traditional HAR models, such as\nCNNs and RNNs, the input data is typically processed in a\nfixed-length sequence, where each sample is represented by\na fixed number of features. However, HAR data is inherently\nsequential and varies in length and complexity, making it\nchallenging to model effectively. Transformer models ad-\ndress this challenge by allowing for the modeling of long-\nrange temporal dependencies between the sensor data and ac-\ntivity labels. Additionally, transformer models can be trained\non data with varying lengths, making them more flexible and\nadaptable to different types of data.\nIn the context of HAR, transformer models can be used to\nmodel the sequential nature of the sensor data and activity\nlabels. The transformer model comprises of an encoder and\na decoder. The encoder takes in the sensor data as input\nand generates a set of latent representations that capture\nthe temporal dependencies between the sensor readings. The\ndecoder then takes these latent representations and predicts\nthe corresponding label.This technique has been shown to\nachieve optimum performance on various HAR benchmarks\nand has potential to improve the effectiveness of HAR sys-\ntems. The structure of the proposed transformer is given in\nTable 2.\nE. PERFORMANCE METRICS\nTo compute the performance of HAR system, this study\nutilized four distinct metrics including precision, recall, F1-\nvalue, and accuracy. These metrics have been commonly\nemployed to evaluate the various HAR and related appli-\ncations [24]. The performance of every daily activity was\ncalculated by utilizing the confusion matrix which comprise\nof TN (true-negative), TP (true-positive) when classifica-\ntion algorithm accurately recognizes and FN (false-negative),\nFP (false-positive) when classification algorithm recognizes\nincorrectly. Accuracy computes the accuracy recognize in-\nstances of activity class from the total instances of that\nactivity class using Eq (1). The high value of accuracy\nindicates that the proposed algorithm has better performance\nand efficiency.\nAccuray = 1\nN\nNX\ni=1\n\u0012 TP + TN\nTP + TN + FP + FN\n\u0013\ni\n(1)\nWhere N shows the total of instances of activity class.\nRecall is determined by the ratio of correctly recognized\npositive instances of a class and the sum of correctly rec-\nognized positive instances and incorrect recognized negative\ninstances as expressed in Eq (2).\nRecall = 1\nN\nNX\ni=1\n\u0012 TP\nTP + FN\n\u0013\ni\n(2)\nThe precision is calculated by the ratio of correctly rec-\nognized positive instances of activity class and the sum of\ncorrectly and incorrectly recognized positive instances as\nshown in Eq (3).\nPrecision = 1\nN\nNX\ni=1\n\u0012 TP\nTP + FP\n\u0013\ni\n(3)\nF1-value is commonly used when the dataset is imbalanced\nto find the accuracy of single class. As the datasets utilized in\nthis paper are imbalanced thus, to calculate the performance\nof each activity class, F1-value metric was used to validate\nthe completeness of HAR models. F1-value is a mean of\nprecision and recall as provided in Eq (4).\nF1 − value = 1\nN\nNX\ni=1\n2 ×\n\u0012recall × precision\nrecall + precision\n\u0013\ni\n(4)\nIV. EXPERIMENTS\nFor the HAR, this research evaluated the proposed approach\nusing three datasets. The percentage spilt method was em-\nployed to evaluate the proposed HAR models, where a set of\n80% features was utilized to train the transformer model for\nHAR and remaining 20% features were utilized to train the\nmodel [25], [26]. According to existing literature [27], the\noptimal performance is obtained when 20-30% of the data\nis used to test the model while remaining data is utilized to\ntrain the deep learning model. For this dataset split, the deep\nlearning model obtain an accurate and valid performance and\ndo not achieve the overestimated performance. The training\naccuracy of proposed transformer model for the WISDM,\nPAMAP2, and UCI HAR that used the 273 extracted fea-\nture as input is given in Table 3. The transformer model\nachieved the optimal performance for PAMAP2 and UCI\nHAR datasets except WISDM where the proposed approach\nattained 98% accuracy which is very close to optimal perfor-\nmance. In this research, the proposed transformer technique\nreduced the loss and improved the accuracy for both testing\nand training data samples, which reveal the effectiveness\nand significance of the transformer model for all datasets as\nshown in Figure 9.\nA. MODELS PREDICTION PERFORMANCE\nThis research used three datasets that have distinct human\nactivities including WISDM, PAMAP2, and UCI HAR etc.\nThe transform model was evaluated on these datasets, and\nits recognition performance is given in Table 4. These tables\nshow the performance reports of the WISDM, PAMAP2, and\nUCI HAR transformer to the recall, precision, F1-score, and\nrecognition accuracy for all human activities in the dataset,\nwhich reveals the reliability of the transformer model com-\npared to the existing techniques [16, 28-30].\nThe proposed HAR transformer model for PAMAP2\ndataset achieved an outstanding performance (98% accu-\nracy) and outperformed the baseline method for PAMAP2\nHAR methods [28] for all human activities. The baseline\nHAR models for PAMAP2 dataset achieved low accuracy\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nTABLE 2: Proposed Transformer Model Structure.\nLayer (type) Output Shape Param # Connected to\ninputs (InputLayer) [(None, None, 273)] 0 []\ntf.math.reduce_sum(TFOpLambda) (None, None) 0 [’inputs[0][0]’]\ntf.cast (TFOpLambda) (None, None) 0 [’tf.math.reduce_sum[0][0]’]\nenc_padding_mask (Lambda) (None, 1, 1, None) 0 [’tf.cast[0][0]’]\nencoder (Functional) (None, None, 273) 5239008 [’inputs[0][0]’, ’enc_padding_mask[0][0]’]\ntf.reshape (TFOpLambda) (None, 273) 0 [’encoder[0][0]’]\noutputs (Dense) (None, 12) 2466 [’tf.reshape[0][0]’]\nTotal params: 5,241,474\nTrainable params: 5,241,474\nNon-trainable params: 0\nEpochs: 100\nLearning Rate: 0.001\nBatch Size: 16\nDropout: 0.1\nNumber of Layers: 6\nAttention Heads: 1\nOptimizers: Adam\nLoss Function: Categorical Crossentropy\nTABLE 3: Training performance of HAR models utilizing the PAMAP2, UCI HAR, and WISDM.\nfeatures Dataset Precision Recall F1-score Accuracy\n273 PAMAP2 1.00 1.00 1.00 1.00\nUCI HAR 1.00 1.00 1.00 1.00\nWISDM 0.98 0.98 0.98 0.98\nTABLE 4: Performance of HAR system for WISDM, UCI HAR, and PAMAP2.\nPerformance of each activity in all datasets of HAR (%)\nDataset Matric A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12 A13\nPrecision 0.97 × 0.99 0.99 0.98 0.99 0.99 0.98 0.98 0.99 0.91 0.99 0.99\nRecall 0.99 × 0.98 0.99 0.98 0.99 0.99 0.98 0.99 0.98 0.94 0.99 0.98\nF1-score 0.98 × 0.99 0.99 0.98 0.99 0.99 0.98 0.98 0.98 0.93 0.99 0.99\nAccuracy 0.99 × 0.98 0.99 0.98 0.99 0.99 0.98 0.99 0.98 0.94 0.99 0.98\nPAMAP2 G-mean 0.97 × 0.97 0.98 0.96 0.98 0.98 0.96 0.97 0.97 0.98 0.97 0.98\nPrecision 0.98 × 0.97 0.98 0.99 0.99 0.98 × × × × × ×\nRecall 0.98 × 0.98 0.99 0.97 0.98 0.99 × × × × × ×\nF1-score 0.98 × 0.97 0.98 0.98 0.98 0.99 × × × × × ×\nAccuracy 0.98 × 0.98 0.99 0.97 0.98 0.99 × × × × × ×\nUCI HAR G-mean 0.96 × 0.95 0.97 0.96 0.97 0.98 × × × × × ×\nPrecision 0.95 0.99 0.93 0.90 0.95 0.99 × × × × × × ×\nRecall 0.99 0.97 0.96 0.96 0.98 0.98 × × × × × × ×\nF1-score 0.97 0.98 0.95 0.93 0.97 0.98 × × × × × × ×\nAccuracy 0.99 1.00 0.98 0.98 0.99 1.00 × × × × × × ×\nWISDM G-mean 0.95 0.97 0.91 0.90 0.95 0.98 × × × × × × ×\nA1=Downstairs, A2=Jogging, A3=Sitting, A4=Standing, A5=Upstairs, A6=Walking, A7=Lying, A8=Vaccum,\nA9=Running, A10=Nordic Walk, A11=Jumping, A12=Ironing, A13=Cycling\nfor the vacuum cleaning activity due to the mix-up informa-\ntion with other human activities, nevertheless the proposed\nmodel recognized all human activities from relevant features\nwith enhanced performance. The activity-wise performance\nand the inter-activity confusion matrix for HAR model of\nPAMAP2 dataset is demonstrated in Figure 10c. The figure\ndemonstrates the actually recognized instances diagonally\nand inter-activity confusion in the corresponding rows.\nThe confusion matrix of proposed HAR model for UCI\nHAR shows reveals that the highest accuracies obtained\nfor lying, standing, and downstairs were 99%, 99%, and\n98% respectively. The walking and upstairs activities were\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\n(a)\n(b)\n(c)\nFIGURE 9: The validation and training performance of HAR models for the (a) UCI HAR (b) WISDM (c) and PAMAP2.\nalso recognized with improved accuracy as compared to\nexisting methods for UCI HAR [28, 29] and the average\nrecognition accuracy of transformer also outperformed the\nbaseline models [28, 29]. In the WISDM confusion matrix\n(Figure 10b), the highest accuracy of 100%, 100%, 99%,\nand 99% were achieved for jogging, walking, downstairs,\nand upstairs respectively. The lowest accuracy of 97% was\nachieved for sitting. The high F1-measure of 98%, 98%, 97%\nand 97% were obtained for jogging, walking, downstairs and\nupstairs respectively, and standing obtained the lowest F1-\nmeasure of 93%. Similarly, the highest precision of 99%,\n99% and 95% were for jogging, walking, and downstairs\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\n(a)\n (b)\n(c)\nFIGURE 10: Confusion matrixes of the PAMAP2, WISDM, UCI HAR. (a) The UCI HAR; (b) the WISDM; (c) the PAMAP2. The\ny-axes show the actual/true classes and x-axis show the predicted classes.\nrespectively, and lowest precision of 93% was achieved for\nsitting. Our proposed HAR models recognized all activities\nin each dataset with an enhanced recognition rate and low\ncomputational time due to the robust and relevant feature and\nsimple transformer model architecture.\nB. COMPARISON WITH DEEP LEARNING ALGORITHMS\nTo build the human activity recognition system, the master\nfeature was fed to distinct DL models: RNN, Long Short-\nterm Memory (LSTM) ,BLSTM, 1D CNN, and DeepCon-\nvLSTM. In addition, 15 experiments (5 DL techniques x 3\ndatasets) were performed to assess how well the extracted\nfeatures and DL models performed together.\nFigure 11 demonstrates how the proposed transformer\nmodel outperformed all DL models, with weighted accuracy\nfor PAMAP2, UCI HAR, and WISDM of 98.2%, 98.6%,\nand 97.3%, respectively. Using the PAMAP2 dataset, the\nDeepConvLSTM, and BLSTM models obtained high average\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\naccuracy of 93.6% and 90.5%, respectively, as opposed to\n84.8% for LSTM, 81.1% for RNN, and 68.4% for 1D-CNN.\nMoreover, employing UCI HAR and WISDM, DeepConvL-\nSTM beat the other four ML models. The LSTM, RNN, and\n1D-CNN models obtained the lowest average accuracy. In\nconclusion, utilizing all three datasets, the proposed model\nfor HAR outperformed the DL models and achieved the\nhighest weighted accuracy.\nC. COMPARATIVE ANALYSIS WITH BASELINE\nTECHNIQUES\nTo demonstrate the significance and reliability of proposed\nmodel for HAR, this research employed UCI HAR, WISDM,\nand PAMAP2 to compare the performance with the baseline\ntechniques. An inclusive summary of comparative examina-\ntion is given in Table 5. The accuracy of the HAR technique is\nconsiderably better than existing techniques, which demon-\nstrate the reliability of proposed technique. Nevertheless, in\nfew cases the recognition rate of proposed technique for\nspecific activity is less compared to the baseline techniques.\nFor instance, the HAR system for PAMAP2 dataset in [29]\nrecognized the sitting activity with a 100% accuracy, while\nthe recognition accuracy of proposed technique for PAMAP2\ndataset for sitting activity is 98%. However, the proposed\ntechnique outperformed baseline techniques by obtaining\nan average accuracy of 98%. The proposed technique for\nHAR recognized each activity with low computation time,\nhigh accuracy, and is helpful for the real-time healthcare\napplications. Therefore, it can be concluded that the proposed\nHAR technique is more generic, accurate, and reliable than\nthe baseline techniques.\nV. CONCLUSION\nThe extraction of discriminative and robust features and\naccurate recognition are the key issues that make HAR a\nchallenging task. This research extracted a diverse set of\nfeatures to capture a wide range of patterns and informa-\ntion from the sensor data, making the model more robust.\nThe extracted features were used as input to Transformer\nmodel, constructed using a fewer number of layers to de-\nrive long-range dependencies leading to improved accuracy.\nThe proposed technique was evaluated on three datasets:\nWISDM, PAMAP2, and UCI HAR, obtaining accuracy of\n97.3%, 98.2%, and 98.6% respectively. The comparison of\nperformance with stat-of-the-art HAR techniques revealed\nthe significance and robustness of the transformer model.\nThe findings also revealed that transformer is specifically\nsuited to all three datasets. The drawback of this approach\nis that it requires domain expertise to extract and combine a\nlarge number of diverse features. While this research shows\npromising results on all three datasets, the performance on\nother datasets remains to be seen. The model’s effective-\nness might vary depending on the characteristics of differ-\nent datasets. Moreover, a comparative investigation of HAR\nsystems based on DL approaches using other datasets is also\nscheduled for future work. To further enhance performance,\ncombining device-based activity recognition and device-free\nactivity recognition can be explored. This approach can\nleverage the strengths of both methods to improve accuracy\nand robustness. Another aspect that can be addressed is\nthe imbalanced classification problems in activity recogni-\ntion. Imbalanced datasets, where certain activity classes have\nsignificantly fewer samples than others, can lead to biased\nmodels that perform poorly on minority classes.\nACKNOWLEDGMENT\nThis work is supported by Princess Nourah bint Abdul-\nrahman University Researchers Supporting Project number\n(PNURSP2023R333), Princess Nourah bint Abdulrahman\nUniversity, Riyadh, Saudi Arabia and Taif University Re-\nsearchers Supporting Project number (TURSP-2020/36), Taif\nUniversity, Taif, Saudi Arabia.\nREFERENCES\n[1] N. A. Choudhury and B. Soni, \"An Adaptive Batch Size based-CNN-\nLSTM Framework for Human Activity Recognition in Uncontrolled Envi-\nronment,\" IEEE Transactions on Industrial Informatics, 2023.\n[2] A. Subasi, K. Khateeb, T. Brahimi and A. Sarirete, \"Human activity recog-\nnition using machine learning methods in a smart healthcare environment,\"\nin Innovation in health informatics: Elsevier, 2020, pp. 123-144.\n[3] M. A. Khatun, M. A. Yousuf, S. Ahmed, M. Z. Uddin, S. A. Alyami et al.,\n\"Deep CNN-LSTM with self-attention model for human activity recogni-\ntion using wearable sensor,\" IEEE Journal of Translational Engineering in\nHealth and Medicine, vol. 10, pp. 1-16, 2022.\n[4] M. Tammvee and G. Anbarjafari, \"Human activity recognition-based path\nplanning for autonomous vehicles,\" Signal, image and video processing,\nvol. 15, no. 4, pp. 809-816, 2021.\n[5] A. Sunil, M. H. Sheth and E. Shreyas, \"Usual and unusual human activity\nrecognition in video using deep learning and artificial intelligence for secu-\nrity applications,\" in 2021 Fourth International Conference on Electrical,\nComputer and Communication Technologies (ICECCT), pp. 1-6, 2021.\n[6] B. Zhang, H. Xu, H. Xiong, X. Sun, L. Shi et al., \"A spatiotemporal\nmulti-feature extraction framework with space and channel based squeeze-\nand-excitation blocks for human activity recognition,\" Journal of Ambient\nIntelligence and Humanized Computing, vol. 12, pp. 7983-7995, 2021.\n[7] I. Jegham, A. B. Khalifa, I. Alouani and M. A. Mahjoub, \"Vision-based hu-\nman action recognition: An overview and real world challenges,\" Forensic\nScience International: Digital Investigation, vol. 32, pp. 200901, 2020.\n[8] S. K. Yadav, K. Tiwari, H. M. Pandey and S. A. Akbar, \"A review of\nmultimodal human activity recognition with special emphasis on classifi-\ncation, applications, challenges and future directions,\" Knowledge-Based\nSystems, vol. 223, pp. 106970, 2021.\n[9] S. Wan, L. Qi, X. Xu, C. Tong and Z. Gu, \"Deep learning models for real-\ntime human activity recognition with smartphones,\" Mobile Networks and\nApplications, vol. 25, no. 2, pp. 743-755, 2020.\n[10] C. Xu, D. Chai, J. He, X. Zhang and S. Duan, \"InnoHAR: A deep neural\nnetwork for complex human activity recognition,\" Ieee Access, vol. 7, pp.\n9893-9902, 2019.\n[11] C. A. Ronao and S.-B. Cho, \"Human activity recognition with smartphone\nsensors using deep learning neural networks,\" Expert systems with appli-\ncations, vol. 59, pp. 235-244, 2016.\n[12] M. M. Hassan, M. Z. Uddin, A. Mohamed and A. Almogren, \"A robust\nhuman activity recognition system using smartphone sensors and deep\nlearning,\" Future Generation Computer Systems, vol. 81, pp. 307-313,\n2018.\n[13] S. Abbaspour, F. Fotouhi, A. Sedaghatbaf, H. Fotouhi, M. Vahabi et al., \"A\ncomparative analysis of hybrid deep learning models for human activity\nrecognition,\" Sensors, vol. 20, no. 19, pp. 5707, 2020.\n[14] Z. Qin, Y . Zhang, S. Meng, Z. Qin and K.-K. R. Choo, \"Imaging and\nfusing time series for wearable sensor-based human activity recognition,\"\nInformation Fusion, vol. 53, pp. 80-87, 2020.\n[15] Z. Xiao, X. Xu, H. Xing, F. Song, X. Wang et al., \"A federated learning\nsystem with enhanced feature extraction for human activity recognition,\"\nKnowledge-Based Systems, vol. 229, pp. 107338, 2021.\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\nFIGURE 11: Comparison of proposed transformer model with DL classifiers\nTABLE 5: Comparison of HAR model and existing HAR models utilizing the PAMAP2, UCI HAR and the WISDM datasets.\nStudy Dataset Accuracy (%) of all Activities\nA1 A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12 A13 Avg\n[16] UCI HAR 99 × 94 95 99 99 99 × × × × × × 98\nWISDM 87 98 83 93 72 98 × × × × × × × 93\n[28] PAMAP2 94 × 90 91 90 91 96 84 89 95 96 86 89 91\nUCI HAR 86 × 88 94 91 98 98 × × × × × × 93\n[29] PAMAP2 94 × 100 97 82 98 100 93 99 100 82 87 98 94\nUCI HAR 96 × 90 98 96 98 100 × × × × × × 96\nWISDM 89 98 96 98 87 98 × × × × × × × 96\n[30] PAMAP2 93 × 90 99 98 99 99 98 94 100 100 97 96 97\nWISDM 91 98 98 100 88 99 × × × × × × × 97\n[34] PAMAP2 96 × 97 100 90 100 97 97 98 95 75 97 100 95\nUCI HAR 95 × 98 93 98 99 100 × × × × × × 97\nWISDM 98 99 95 99 99 94 × × × × × × × 97\nOur model PAMAP2 99 × 98 99 98 99 99 98 99 98 94 99 98 98\nUCI HAR 98 × 98 99 97 98 99 × × × × × × 99\nWISDM 99 100 98 98 99 100 × × × × × × × 97\nA1=Downstairs, A2=Jogging, A3=Sitting, A4=Standing, A5=Upstairs, A6=Walking, A7=Lying, A8=Vaccum,\nA9=Running, A10=Nordic Walk, A11=Jumping, A12=Ironing, A13=Cycling\n[16] A. Ignatov, \"Real-time human activity recognition from accelerometer data\nusing Convolutional Neural Networks,\" Applied Soft Computing, vol. 62,\npp. 915-922, 2018.\n[17] S. Chen, E. Dobriban and J. H. Lee, \"A group-theoretic framework for data\naugmentation,\" Journal of Machine Learning Research, vol. 21, no. 245,\npp. 1-71, 2020.\n[18] J. Zhang, F. Wu, B. Wei, Q. Zhang, H. Huang et al., \"Data augmentation\nand dense-LSTM for human activity recognition using WiFi signal,\" IEEE\nInternet of Things Journal, vol. 8, no. 6, pp. 4628-4641, 2020.\n[19] D. Thakur and S. Biswas, \"Feature fusion using deep learning for smart-\nphone based human activity recognition,\" International Journal of Infor-\nmation Technology, vol. 13, no. 4, pp. 1615-1624, 2021.\n[20] S. Ahmed, K. K. Ghosh, S. Mirjalili and R. Sarkar, \"AIEOU: Automata-\nbased improved equilibrium optimizer with U-shaped transfer function for\nfeature selection,\" Knowledge-Based Systems, vol. 228, pp. 107283, 2021.\n[21] Y . Zhan, J. Nishimura and T. Kuroda, \"Human activity recognition from\nenvironmental background sounds for wireless sensor networks,\" IEEJ\nTransactions on Electronics, Information and Systems, vol. 130, no. 4, pp.\n565-572, 2010.\n[22] R. San-Segundo, J. M. Montero, R. Barra-Chicote, F. Fernández and J.\nM. Pardo, \"Feature extraction from smartphone inertial signals for human\nactivity segmentation,\" Signal Processing, vol. 120, pp. 359-372, 2016.\n[23] R. Jain, B. Jain and M. Puri, \"Learning Theory (Supervised/Unsupervised)\nfor Signal Processing,\" in Machine Learning in Signal Processing: Chap-\nman and Hall/CRC, 2021, pp. 17-53.\n[24] H. F. Nweke, Y . W. Teh, M. A. Al-Garadi and U. R. Alo, \"Deep learning\nalgorithms for human activity recognition using mobile and wearable\nsensor networks: State of the art and research challenges,\" Expert Systems\nwith Applications, vol. 105, pp. 233-261, 2018.\n[25] E. Garcia-Ceja, M. Riegler, A. K. Kvernberg and J. Torresen, \"User-\nadaptive models for activity and emotion recognition using deep transfer\nlearning and data augmentation,\" User Modeling and User-Adapted Inter-\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nOumaima et al.: An Efficient Human Activity Recognition using Hybrid Features and Transformer Model\naction, pp. 1-29, 2019.\n[26] W. Nie, M. Ren, J. Nie and S. Zhao, \"C-GCN: Correlation based Graph\nConvolutional Network for Audio-video Emotion Recognition,\" IEEE\nTransactions on Multimedia, 2020.\n[27] A. Gholamy, V . Kreinovich and O. Kosheleva, \"Why 70/30 or 80/20\nrelation between training and testing sets: A pedagogical explanation,\"\n2018.\n[28] S. Wan, L. Qi, X. Xu, C. Tong and Z. Gu, \"Deep learning models for real-\ntime human activity recognition with smartphones,\" Mobile Networks and\nApplications, vol. 25, pp. 743-755, 2020.\n[29] K. Xia, J. Huang and H. Wang, \"LSTM-CNN architecture for human\nactivity recognition,\" IEEE Access, vol. 8, pp. 56855-56866, 2020.\n[30] Y . Li and L. Wang, \"Human activity recognition based on residual network\nand BiLSTM,\" Sensors, vol. 22, no. 2, pp. 635, 2022.\n[31] S. Raziani and M. Azimbagirad, \"Deep CNN hyperparameter optimization\nalgorithms for sensor-based human activity recognition,\" Neuroscience\nInformatics, vol. 2, no. 3, p. 100078, 2022.\n[32] Y . Tang, L. Zhang, F. Min, and J. He, \"Multiscale deep feature learning for\nhuman activity recognition using wearable sensors,\" IEEE Transactions on\nIndustrial Electronics, vol. 70, no. 2, pp. 2106-2116, 2022.\n[33] A. Omolaja, A. Otebolaku, and A. Alfoudi, \"Context-aware complex\nhuman activity recognition using hybrid deep learning models,\" Applied\nSciences, vol. 12, no. 18, p. 9305, 2022.\n[34] W. Ding, M. Abdel-Basset, and R. Mohamed, \"HAR-DeepConvLG: Hy-\nbrid Deep Learning–based Model for Human Activity Recognition in IoT\nApplications,\" Information Sciences, p. 119394, 2023.\n[35] D. Thakur, S. Biswas, E. S. Ho, and S. Chattopadhyay, \"Convae-lstm: Con-\nvolutional autoencoder long short-term memory network for smartphone-\nbased human activity recognition,\" IEEE Access, vol. 10, pp. 4137-4156,\n2022.\nOUMAIMA SAIDANI received the Ph.D. degree\nin Computer Sciences from Paris 1-Pantheon Sor-\nbonne University, France, and the M.Sc. degree\nin Computer Sciences from Paris 9—Dauphine\nUniversity, France. She is currently an Assistant\nProfessor with the Information Systems Depart-\nment, College of Computer and Information Sci-\nences (CCIS-IS), Princess Nourah bint Abdulrah-\nman University (PNU), KSA. Her research in-\nterests include information systems engineering,\nbusiness process engineering, process mining, context-aware computing,\nDeep learning learning.\nMAJED ALSAFYANI received the bachelor’s\ndegree (Hons.) in computer science from the Uni-\nversity of Hertfordshire (UH), U.K., in 2013, and\nthe master’s degree (Merit) in Advance computer\nscience from the University of Hertfordshire (UH),\nU.K., in 2014, and the Ph.D. degree in com-\nputer science from the University of Hertford-\nshire, U.K., in 2015 and 2020, respectively. He is\ncurrently an Assistant Professor with the College\nof Computers and Information Technology, Taif\nUniversity, Saudi Arabia. His research interests on image processing, ma-\nchine learning, applications of Internet of Things, artificial intelligence and\nsoftware engineering.\nROOBAEA ALROOBAEA received the bach-\nelor’s degree (Hons.) in computer science from\nKing Abdul-Aziz University (KAU), Saudi Ara-\nbia, in 2008, and the master’s degree in informa-\ntion systems and the Ph.D. degree in computer\nscience from the University of East Anglia, U.K.,\nin 2012 and 2016, respectively. He is currently\nan Associate Professor with the College of Com-\nputers and Information Technology, Taif Univer-\nsity, Saudi Arabia. His research interests include\nhuman–computer interaction, software engineering, cloud computing, the\nInternet of Things, artificial intelligence, and machine learning.\nPLACE\nPHOTO\nHERE\nNAZIK ALTURKI received the Ph.D. degree\nin information systems from The University of\nMelbourne. She is currently an Assistant Profes-\nsor with the Information Systems Department,\nCollege of Computer and Information Sciences,\nPrincess Nourah bint Abdulrahman University,\nRiyadh, Saudi Arabia. Her research interests in-\nclude health informatics, big data, data analytics,\nand mining.\nRASHID JAHANGIR received the bachelor’s de-\ngree in computer engineering from the University\nof Engineering and Technology (UET) Lahore,\nPakistan, the master’s degree from the University\nof New South Wales (UNSW), Sydney, Australia,\nand the Ph.D. degree from the Faculty of Com-\nputer Science and Information Technology, Uni-\nversity of Malaya (UM), Kuala Lumpur, Malaysia.\nHe has been an Assistant Professor with the COM-\nSATS University Islamabad, Vehari Campus, Pak-\nistan, since 2014. He worked as a Software Engineer at Software House,\nLahore, for two years. He has vast experience in teaching and research.\nHe has also published several articles in academic journals indexed in well-\nreputed databases, such as ISI and Scopus. He is also an Active Reviewer for\nvarious journals, including Artificial Intelligence Reviews, Expert Systems\nwith Applications Multimedia Tools and Applications, IEEE ACCESS, and\nSocial Network Analysis and Mining. He is working on digital signal\nprocessing and deep learning. His research interests include deep learning,\npattern recognition, machine learning, and data mining.\nLEILA JAMEL MENZLI is an Assistant Pro-\nfessor in the Information Systems department-\nCollege of Computer and Information Sciences\nat Princess Nourah Bint Abdulrahman Univer-\nsity (PNU), KSA. She is a researcher in RIADI\nLaboratory-Tunisia. She received a Ph.D degree in\nComputer Sciences and Information Systems and\nan engineering degree in Computer Sciences. Her\nresearch interests are: Business Process Reengi-\nneering, Process Modeling, BPM, Data Sciences,\nML, Process Mining, e-learning and software engineering. She was the\nProgram Leader of the IS program and the ABET and NCAAA accreditation\ncommittees in the CCIS- at PNU. She worked as a head of the Information\nSystems Security department at the Premier Ministry in Tunisia. She is the\nreviewer of many international journals and conferences. She was a member\nof scientific/steering committees of many international conferences.\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314492\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8132804036140442
    },
    {
      "name": "Discriminative model",
      "score": 0.7447683811187744
    },
    {
      "name": "Activity recognition",
      "score": 0.7278558611869812
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7045185565948486
    },
    {
      "name": "Transformer",
      "score": 0.5667092204093933
    },
    {
      "name": "Machine learning",
      "score": 0.5232071876525879
    },
    {
      "name": "Feature extraction",
      "score": 0.5204953551292419
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41814783215522766
    },
    {
      "name": "Engineering",
      "score": 0.09115752577781677
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I106778892",
      "name": "Princess Nourah bint Abdulrahman University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I179331831",
      "name": "Taif University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I16076960",
      "name": "COMSATS University Islamabad",
      "country": "PK"
    }
  ]
}