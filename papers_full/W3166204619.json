{
  "title": "BioM-Transformers: Building Large Biomedical Language Models with BERT, ALBERT and ELECTRA",
  "url": "https://openalex.org/W3166204619",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3166846399",
      "name": "Sultan Alrowili",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109851914",
      "name": "Vijay Shanker",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3116099796",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W1623072288",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3106298421",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2951562155",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3033737024",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models.",
  "full_text": "Proceedings of the BioNLP 2021 workshop, pages 221–227\nJune 11, 2021. ©2021 Association for Computational Linguistics\n221\nBioM-Transformers: Building Large Biomedical Language Models with\nBERT, ALBERT and ELECTRA\nSultan Alrowili\nUniversity of Delaware\nNewark, Delaware, USA\nalrowili@udel.edu\nK. Vijay-Shanker\nUniversity of Delaware\nNewark, Delaware, USA\nvijay@udel.edu\nAbstract\nThe impact of design choices on the per-\nformance of biomedical language models re-\ncently has been a subject for investigation. In\nthis paper, we empirically study biomedical\ndomain adaptation with large transformer mod-\nels using different design choices. We eval-\nuate the performance of our pretrained mod-\nels against other existing biomedical language\nmodels in the literature. Our results show that\nwe achieve state-of-the-art results on several\nbiomedical domain tasks despite using similar\nor less computational cost compared to other\nmodels in the literature. Our ﬁndings high-\nlight the signiﬁcant effect of design choices on\nimproving the performance of biomedical lan-\nguage models.\n1 Introduction\nThe amount of biomedical literature has grown sub-\nstantially in recent years. This growth created a\ndemand for powerful biomedical language models.\nTransformer-based language models, such as BERT\n(Devlin et al., 2019), have shown effectiveness in\ncapturing the contextual representation of corpora\nat large volume. To address the lack of biomedi-\ncal contextual representation, both BioBERT (Lee\net al., 2019), and SciBERT (Beltagy et al., 2019)\nhave adapted BERT to the biomedical domain.\nRecently, several Transformer-based mod-\nels have been introduced, including Megatron\n(Shoeybi et al., 2020), RoBERTa (Liu et al., 2019),\nALBERT (Lan et al., 2020) and ELECTRA (Clark\net al., 2020). These models show impressive per-\nformance gains over BERT in the general domain\nleading most NLP leader boards. However, these\nmodels have been evaluated with environmental de-\nsign factors varying in several dimensions (e.g., vo-\ncabulary and corpora domain, loss function, train-\ning steps, batch size, and model’s scale). Under-\nstanding the contribution of these factors to the\nperformance of the language models is challenging,\nespecially when our goal is to shift the contextual\nrepresentations to the biomedical domain.\nThis challenge motivates us to investigate the\nimpact of design choices on the performance of\nbiomedical language models. Moreover, highlight-\ning this impact is critical when evaluating new\napplications in BioNLP, where each application\nmay evaluate its performance against other mod-\nels that use different design setups. In this work,\nwe pretrain and evaluate different variants of large\nbiomedical Transformer-based models across dif-\nferent design factors.\nThus, our contributions in this paper includes :\n(i) We pretrain four different variations of\nTransformer-based models including:\nELECTRABase, ELECTRA Large, BERTLarge\nand ALBERT xxlarge on biomedical domain\ncorpora using Tensor Processing Units TPUs.\n(ii) We ﬁne-tune and evaluate our pretrained mod-\nels on several downstream biomedical tasks.\nWe present a comprehensive evaluation that\nhighlights the impact of design choices on the\nperformance of biomedical language models.\n(iii) We released our pretrained models along with\nour Github repository.1\n2 Related Work\n2.1 Transformer-based Language Models\nThe introduction of the BERT model (Devlin\net al., 2019) has initiated the advancement of\nTransformer-based models. Consequently, the in-\nvestigation of the architecture and design choices\nof BERT introduced new state-of-the-art models.\nBy exploiting the advantage of using the large\nbatch size and increasing the size of the corpus,\n1Our pre-trained models and our Github repository\nare accessible at https://github.com/salrowili/\nBioM-Transformers.\n222\nRoBERTa (Liu et al., 2019) has achieved signiﬁ-\ncant performance gains on all downstream tasks.\nThe loss function and scalability of BERT were\nalso a subject for investigation by ELECTRA\n(Clark et al., 2020) and ALBERT (Lan et al., 2020).\nELECTRA reaches state-of-the-art results by in-\ntroducing a binary loss function. This loss func-\ntion uses generative and discriminative models to\naccelerate the learning curve. Furthermore, the\nALBERT model introduces multiple ideas to the\nBERT model to improve performance and scalabil-\nity, including parameter-sharing technique, LAMB\noptimizer, and factorization of embedding layers.\nBoth ELECTRA and ALBERT are now leading\nmost of NLP benchmarks, including SQuAD (Ra-\njpurkar et al., 2016) and GLUE (Wang et al., 2018).\n2.2 Biomedical Language Models\nIn this section, we will brieﬂy summarize the\ncurrent state-of-the-art biomedical language mod-\nels. We should also note that there are other in-\nsightful models in literature such as ClinicalBERT\n(Alsentzer et al., 2019), BlueBERT (Peng et al.,\n2019), BioELECTRA (Ozyurt, 2020) and BioMed-\nBERT (Chakraborty et al., 2020) .\nBioBERT (Lee et al., 2019) is a BERTBase model\nthat has been pretrained on biomedical corpora,\nincluding PubMed and PMC articles for 23 days\non eight V100 GPUs. In our evaluation, we use\nBioBERTBasev1.1, which extends the pre-training\nsteps of BioBERTB to 1M steps and was trained on\nPubMed abstracts only.\nSciBERT (Beltagy et al., 2019) is a BERT Base\nmodel that has been pretrained on 1.14M biomed-\nical and computer science papers from Semantic\nScholar Corpus .\nPubMedBERT (Gu et al., 2021) follows a similar\napproach of BioBERT by pretraining the BERT\nmodel on large biomedical corpora, including\nPubMed abstracts and PMC articles. PubMed-\nBERT, in contrast to BioBERT, is pretrained using\na large batch size (8192) and studies various effects\non domain adaptation. The paper also introduces\nthe BLURB benchmark, which is a collection of\ndownstream biomedical tasks.\nBioMegaTron345m (Shin et al., 2020) is a large-\nscale model (345m parameters) by NVIDIA based\non MegaTron architecture. (Shoeybi et al., 2020).\nBioMegaTron introduces a variety of large biomed-\nical language models examining the choice of cor-\npora and vocabulary domain.\nBioRoBERTa (Lewis et al., 2020) extends the\nstate-of-the-art results by testing different design\nchoices. Similar to BioMegaTron’s approach,\nBioRoBERTa models investigate the effect of vo-\ncabulary and corpora domain on the performance\nof biomedical language model.\n3 Pretraining our Language Models\nWe pretrain all our models using the original im-\nplementation of BERT, ALBERT, and ELECTRA.\nWe use TensorFlow 1.15 and TPUv3-512 units\nto pretrain our large models and TPUv3-32 to\npretrain our BioM-ELECTRAB model.\n3.1 BioM-ALBERT\nInitially, we pretrain our model BioM-\nALBERTxxlarge on PubMed abstracts only.\nBioM-ALBERTxxlarge is based on ALBERTxxlarge\narchitecture which has larger hidden layer size\n(4096) than both BERTL and ELECTRAL (1024).\nWe build our speciﬁc domain vocabulary, which\nhas a size of 30K words, using the sentence piece\nmodel (Kudo and Richardson, 2018). We maintain\nthe same hyperparameters that (Lan et al., 2020)\nuse, except that we increase the batch size to\n8192, decrease the initializer range to 0.01. We\npretrain BioM-ALBERTxxlarge with a learning rate\nof 1.76e-3 for 264K steps.\nTable 1 show the details of our pretrained mod-\nels compared to the existing model in the litera-\nture. The goal to pretrain BioM-ALBERT xxlarge\nis to understand the impact of using ALBERT’s\ntechniques on domain adaptation. Moreover, we\nintroduce PMC articles at 264k step, to study the\ninﬂuence of adding PMC articles on the language\nmodel. BioM-ALBERTxxlarge is the ﬁrst model that\nwe pretrain and ﬁne-tune among our large models.\n3.2 BioM-ELECTRA\nWe build our BioM-ELECTRA Base and BioM-\nELECTRALarge based on ELECTRA architec-\nture (Clark et al., 2020). We pre-train BioM-\nELECTRAL on PubMed abstracts only using spe-\nciﬁc domain vocabulary generated by PubMed-\nBERT, which has a size of 28,895 words. Our\nevaluation of BioM-ALBERTxxlarge on downstream\ntasks, inﬂuences our decision to pretrain BioM-\nELECTRA on PubMed abstracts only. We use\n223\nModel Steps Batch C Corpus Vocabulary\nRoBERTaBase 500k 8192 4.00x Web crawl 50K Web crawl\nELECTRABase++ 4M 256 1.00x XLNET Data 30K Wikipedia + Books\nSciBERTBase - - - Semantic Scholar 30K PMC+CS\nBioBERTBase 1M 256 0.25x PubMed Abstracts 30K Wikipedia + Books\nPubMedBERTBase 64K 8192 0.50x PubMed Abstracts 29K PubMed Abstracts\nPubMedBERTBase+ 64K 8192 0.50x PubMed+PMC 30K PubMed+PMC\nBioM-ELECTRABase 500K 1024 0.50x PubMed Abstracts 29K PubMedBERT\nELECTRALarge 1.7M 2048 3.40x XLNET Data 30K Wikipedia + Books\nALBERTxxlarge 1.5M 4096 6.00x Wikipedia + Books 30k Wikpedia + Books\nBioRoBERTaLarge 500K 8192 4.00x PubMed+PMC+M 50K PubMed+PMC+M\nBioM-BERTLarge 690K 4096 2.76x PubMed+PMC 30k Wikipedia + Books\nBioM-ELECTRALarge 434K 4096 1.73x PubMed Abstracts 29K PubMedBERT\nBioMegaTron345m 800K 512 0.40x PubMed+PMC-CC 50K PubMed Abstracts\nBioM-ALBERTxxlarge 264K 8192 2.11x PubMed Abstracts 30k PubMed (ours)\nTable 1: Design choices for our pretrained models and state-of-the-art models. The computational ratio (C) rep-\nresents the ratio between the number of steps multiplied by the batch size where ELECTRA base++ is the baseline.\nXLNet (Yang et al., 2020) data set consist of 33B tokens (130GB) of English corpora. We split the table based on\nthe scale and the domain of language models. CC: Commercial use Collection.\nsimilar pre-training hyperparameters setting de-\nscribed by (Clark et al., 2020) except that we use a\nlarger batch size for BioM-ELECTRAbase (1024)\nand BioM-ELECTRAlarge (4096). We pretrain our\nBioM-ELECTRAbase for 500K steps and BioM-\nELECTRAlarge model for 434K steps .\nThe main objective to pretrain BioM-\nELECTRABase is to study the effect of ELECTRA\nfunction by comparing its performance with\nPubMedBERTBase and RoBERTa Base . Fur-\nthermore, we build our BioM-ELECTRA Large\nmodel to study the effect of model scale by\ncomparing it with BioM-ELECTRA Base and\nPubMedBERTBase where other factors are similar.\nWe should also note that we choose general\ndomain model ELECTRAB++ as a baseline model\ninstead of ELECTRA B model. The difference\nbetween ELECTRAB and ELECTRAB++ is that\nELECTRAB is pretrained with less steps (1M) and\non smaller corpora (Wikipedia+ Books) (Clark\net al., 2020).\n3.3 BioM-BERT\nWe pretrain BioM-BERTLarge model on PubMed\nabstracts and PMC articles using the same vo-\ncabulary of BioBERT Base. BioBERT Base uses\na general domain vocabulary pretrained on En-\nglish Wikipedia and Books Corpus. Our BioM-\nBERTLarge model aims to study the effect of using\ngeneral domain vocabulary and PubMed + PMC\ncorpora on downstream biomedical tasks. We use a\nbatch size of 4096, a learning rate of 2e-4, and we\nset the pretraining steps to 700K. However, since\nwe use preemptible TPUs, our TPUs preempted\nat 690K. We use the ELECTRA implementation\nof BERT to pretrain our BERT Large model. This\nimplementation uses a dynamic masking feature\nwithout using next-sentence prediction objective.\n4 Fine-Tuning\n4.1 Downstream Tasks\nOur choices of downstream biomedical tasks are\nsimilar to (Shin et al., 2020). For Named Entity\nRecognition (NER) and Relation Extraction (RE),\nwe generate our training, development, and test\ndata using the same script that PubMedBERT uses\n(Gu et al., 2021).\nNamed Entity RecognitionOur choices for NER\ntasks including: BC5CDR-Chemical, BC5CDR-\nDisease (Li et al., 2016) and NCBI-Disease task.\n(Do˘gan et al., 2014). These tasks aim to identify\nchemical and disease entities using IOB tagging\nformat (Ramshaw and Marcus, 1995). For NER\ntasks, we use entity-Level F1 score, which is a\ncommon standard in the literature.\nRelation Extraction is a text classiﬁcation task\nwhere we classify each sequence from a list\nof labels (classes). For RE task, we choose\nthe ChemProt task (Krallinger et al., 2015) ,\nwhich is a task that classiﬁes chemical-protein\ninteractions. We use micro-level F1 score on the\n224\nﬁve most common classes. We reproduce the\nresults of BioRoBERTaL 2 on ChemProt task since\nBioRoBERTa uses a different pre-processing script\nthan (Gu et al., 2021).\nQuestion Answering We use the same\nBioASQ7B-factoid dataset that (Lee et al.,\n2019) use, which is in the format of SQuADv1.1.\nWe use Mean Reciprocal Rank (MMR) as an\nevaluation metric for this task. Moreover, as it is\na common practice, we ﬁne-tune our models on\nBioASQ task using a checkpoint ﬁne-tuned on\nSQuAD2.0 task (Rajpurkar et al., 2016).\n4.2 Fine-Tuning Hyperparameters\nWe conduct a hyperparameters grid search using\nthe development data set on TPUv3-8. We use Ten-\nsorFlow 1.15 to ﬁne-tune our model for all tasks,\nexcept that we use Transformers library (Wolf et al.,\n2020) to ﬁne-tune our BioM-ALBERT on NER\ntasks. Since we are ﬁne-tuning different architec-\ntures, we extend our grid search range to : learn-\ning rate (1e-4, 2e-4, 1e-5 - 7e-5), batch size (24,\n32, 48, 64, 128) and (2-5) epochs . We ﬁxed our\nchoices of hyperparameters for each set of tasks,\nmodel’s scale, and architecture. The details of our\nﬁne-tuning hyperparameters can be found in Ap-\npendix A.1.\n5 Results and Discussion\nTable 2 shows our evaluation results. We categorize\nmodels into four categories based on the domain\nand the scale of each model. We show the results of\nBioM-BERTL and BioM-ALBERTxxlarge at differ-\nent steps. We report entity-level F1 for NER tasks,\nmicro-level F1 for ChemProt, F1 for SQuAD2.0,\nand Mean Reciprocal Rank (MMR) for BioASQ.\nWe add SQuAD results to track the direction of\ncontextual representation between the general and\nbiomedical domain.\n5.1 ELECTRA Objective\nThe effect of the ELECTRA objective can be seen\nfrom comparing both PubMedBERTB and BioM-\nELECTRAB, where they both use similar design\nchoices, vocabulary set, and C ratio. Our evaluation\nshows that the ELECTRA function improves the\nperformance on ChemProt, SQuAD, and BioASQ\ntasks. On the SQuAD task, our BioM-ELECTRAB\n2BioRoBERTA released their models at https://\ngithub.com/facebookresearch/bio-lm. We use\nfollowing hyperparameters to reproduce results (lr: 2e-5 ,\nbatch size: 16, epochs : 10, seeds: 10, 42, 1234, 12345, 666).\nexceeds RoBERTaB despite using biomedical cor-\npora and less C ratio. On NER tasks, BioM-\nELECTRAB performs better on the NCBI-disease\nand worse on the BC5-CDR task. In contrast,\nBioM-ELECTRAlarge performs better than other\nlarge models on the BC5-CDR dataset, which ex-\ncludes the assumption that ELECTRA function\nnegatively affects BioM-ELECTRAB performance\non BC5-CDR tasks\n5.2 Named Entity Recognition\nSpeciﬁc domain vocabulary signiﬁcantly improves\nthe results on NER tasks. Results of BioM-\nELECTRAL and BioRoBERTaL show that biomed-\nical corpora choices have a marginal effect on NER\ntasks. Our results also show that the gap between\nbase-scale and large-scale biomedical models on\nNER tasks is relatively smaller than RE and QA\ntasks, especially for NCBI-Disease task.\n5.3 Relation Extraction\nOn ChemProt task, BioM-BERTLarge achieve 78.8\nF1 score at 100K step with a C ratio of 0.4x match-\ning the performance of BioRoBERTaL which has a\nC ratio of 4.0x. At 1.6x C ratio (400K), it exceeds\nby a signiﬁcant margin all large-scale biomedical\nmodels. BioM-BERTL is the only large model in\nTable 2 that has PP design choice, which highlights\nthe critical impact of general domain vocabulary\non some RE tasks such as ChemProt.\n5.4 Question Answering\nOur results highlight that question answering tasks\nare sensitive to out-of-domain corpora. This sensi-\ntivity can be clearly seen when we introduce (PP)\ndesign to BioM-ALBERTxxlarge. The performance\ndecreases signiﬁcantly on the BioASQ challenge.\nIn contrast, the performance on the SQuAD dataset\nincrease to 88.0%. This increase is not caused by\nextending the training steps since SQuAD score\nremains stable at 215K and 264K steps.\nMoreover, we can observe a gap of 3.9%\nin the SQuAD benchmark between BioM-\nELECTRALarge and BioM-ELECTRABase. How-\never, this gap is not reﬂected in the BioASQ bench-\nmark since it is in the format of SQuADv1.1, high-\nlighting the need to have a biomedical questing\nanswering task in the format of SQuADv2.0.\nFurthermore, our evaluation shows that\nELECTRAB++ model achieve state-of-the-art\nresult on BioASQ for base-scale models. We\nattribute this performance to the fact that we use\n225\nModel Design BC5CDR- NCBI- Chem- QA\nC Design Chem. Dise. Dise. Prot SQuAD BioASQ\nRoBERTaB 4.00x G 89.4 80.7 86.6 73.0 83.7 -\nELECTRAB++ 1.00x G 90.7 83.0 86.3 73.7 86.2 52.5\nSciBERTB - S V 92.5 84.7 88.3 75.0 - -\nBioBERTB 0.25x P 92.6 84.7 89.1 76.1 - 41.1\nPubMedBERTB 0.50x P V 93.3 85.6 87.9 77.2 79.1 51.6\nPubMedBERTB+ 0.50x PP V 93.4 85.6 88.3 77.0 80.9 51.9\nBioM-ELECTRAB 0.50x P V 93.1 85.2 88.4 77.6 84.4 52.3\nELECTRAL 3.40x G 91.6 84.4 87.6 75.3 90.7 53.0\nALBERTxxlarge 6.00x G 89.7 81.7 85.5 75.8 90.2 53.1\nBioRoBERTaL 4.00x PPM V 93.7 85.2 89.0 78.8 - -\nBioM-BERTL\n100K 0.40x PP - - 87.8 78.8 84.0 -\n400K 1.60x PP - - 88.5 79.8 86.5 -\n690K 2.76x PP 92.4 84.5 88.6 80.0 87.3 53.4\nBioM-ELECTRAL 1.73x P V 93.8 85.9 89.0 78.6 88.3 54.1\nBioMegaTron345m 0.40x PP V 92.5 88.5 87.0 77.0 84.2 52.5\nBioM-ALBERTxxlarge\n215K 1.70x P V - - - 79.0 87.0 55.1\n264K 2.11x P V 93.5 85.2 88.7 79.3 87.0 56.9\n+64K 2.60x PP V - - - 79.2 88.0 54.5\nTable 2: Evaluation results of our pretrained models. For NER and ChemProt, we use reported results of SciBERTB,\nRoBERTaB, BioBERTB, PubMedBERTB, PubMedBERTB++ (Gu et al., 2021), BioMegaTron (Shin et al., 2020),\nBioRoBERTaL (Lewis et al., 2020). We generate QA results for all models, except that we use reported results\nfor BioMegaTron, BioBERT (Shin et al., 2020), RoBERTa B (Dai et al., 2020). BioMegaTron uses sub-tokens\nevaluation for NER tasks rather than whole-entity evaluation and uses different pre-processed data set for ChemProt\ntask. Our results are the average scores of ﬁve different runs. B: Base, L: Large, P: PubMed, PP: PubMed+PMC,\nPPM: PubMed+PMC+MMIC, V: Speciﬁc domain vocabulary, S: Semantic Scholar, G: General domain model.\na SQuAD ﬁne-tuned checkpoint to ﬁne-tune our\nmodels on BioASQ task. In contrast, the gap\nbetween the general and biomedical domain is\nworse on NER and RE tasks since we are not using\nany general domain ﬁne-tune checkpoints.\n5.5 Fine-Tuning Time\nTable 3 shows the ﬁne-tuning efﬁciency. All base-\nscale models in Table 2 have similar ﬁne-tuning\ntime to BioM-ELECTRAB since they are built on\nBERTB architecture. Also all models that are based\non BERTL, such as BioRoBERTa L have similar\nﬁne-tuning time to BioM-ELECTRAL. Our evalu-\nation shows that hidden layer size (H) signiﬁcantly\ninﬂuences the ﬁne-tuning time.\n6 Conclusion\nWe introduce four biomedical Transformer-based\nlanguage models. Our results show that lan-\nguage models with general domain vocabulary\nand PubMed+PMC corpora perform better on the\nModel H Time Ratio\nBioM-ELECTRAB 768 03:01 0.35x\nBioM-ELECTRAL 1024 08:27 1.00x\nBioM-ALBERTxxlarge 4096 31:15 3.67x\nTable 3: Fine-Tuning time of our pre-trained models.\nWe ﬁne-tune all models on ChemProt data set for 3\nepochs with a batch size of 32 and max seq. length\nof 128 on 3090RTX GPU with PyTorch (FP16).\nChemProt task. Language models with speciﬁc\ndomain vocabulary and PubMed abstracts perform\nbetter on NER and QA tasks. In the future, we\nare planning to extend our evaluation to additional\nbiomedical tasks and investigate implementing\nearly existing (Zhou et al., 2020) to reduce the ﬁne-\ntuning time. Also, we are planning to build an End-\nto-End ensemble QA system with our large models\nand Sentence-BERT (Reimers and Gurevych, 2019)\nto address pandemic issues such as COVID-19.\n226\nAcknowledgment\nWe would like to acknowledge the support we have\nfrom Tensorﬂow Research Cloud (TFRC) team\nto grant us access to TPUv3 units. The authors\nalso would like to thank Professor Dr. Li Liao for\nhis insightful discussion and comments on BioM-\nALBERT model, anonymous reviewers from the\nBioNLP2021 workshop for their constructive feed-\nback on our initial manuscript, Hoo Chang Shin\nfor clarifying the experimental design of BioMega-\nTron.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop,\npages 72–78, Minneapolis, Minnesota, USA. Asso-\nciation for Computational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSouradip Chakraborty, Ekaba Bisong, Shweta Bhatt,\nThomas Wagner, Riley Elliott, and Francesco\nMosconi. 2020. BioMedBERT: A pre-trained\nbiomedical language model for QA and IR. In\nProceedings of the 28th International Confer-\nence on Computational Linguistics, pages 669–679,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc V .\nLe. 2020. Funnel-transformer: Filtering out sequen-\ntial redundancy for efﬁcient language processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRezarta Islamaj Do ˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for\ndisease name recognition and concept normaliza-\ntion. Journal of biomedical informatics, 47:1–10.\n24393765[pmid].\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2021. Domain-\nspeciﬁc language model pretraining for biomedical\nnatural language processing.\nMartin Krallinger, Obdulia Rabal, Florian Leitner,\nMiguel Vazquez, David Salgado, Zhiyong Lu,\nRobert Leaman, Yanan Lu, Donghong Ji, Daniel M.\nLowe, Roger A. Sayle, Riza Theresa Batista-\nNavarro, Rafal Rak, Torsten Huber, Tim Rock-\ntäschel, Sérgio Matos, David Campos, Buzhou\nTang, Hua Xu, Tsendsuren Munkhdalai, Keun Ho\nRyu, S. V . Ramanan, Senthil Nathan, Slavko Žit-\nnik, Marko Bajec, Lutz Weber, Matthias Irmer,\nSaber A. Akhondi, Jan A. Kors, Shuo Xu, Xin\nAn, Utpal Kumar Sikdar, Asif Ekbal, Masaharu\nYoshioka, Thaer M. Dieb, Miji Choi, Karin Ver-\nspoor, Madian Khabsa, C. Lee Giles, Hongfang Liu,\nKomandur Elayavilli Ravikumar, Andre Lamurias,\nFrancisco M. Couto, Hong-Jie Dai, Richard Tzong-\nHan Tsai, Caglar Ata, Tolga Can, Anabel Usié,\nRui Alves, Isabel Segura-Bedmar, Paloma Martínez,\nJulen Oyarzabal, and Alfonso Valencia. 2015. The\nchemdner corpus of chemicals and drugs and its an-\nnotation principles. Journal of Cheminformatics,\n7(1):S2.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomed-\nical and clinical tasks: Understanding and extend-\ning the state-of-the-art. In Proceedings of the 3rd\nClinical Natural Language Processing Workshop,\npages 146–157, Online. Association for Computa-\ntional Linguistics.\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers,\nand Zhiyong Lu. 2016. BioCreative V CDR task\ncorpus: a resource for chemical disease relation ex-\ntraction. Database, 2016. Baw068.\n227\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nIbrahim Burak Ozyurt. 2020. On the effectiveness of\nsmall, discriminatively pre-trained language repre-\nsentation models for biomedical text mining. In Pro-\nceedings of the First Workshop on Scholarly Docu-\nment Processing, pages 104–112, Online. Associa-\ntion for Computational Linguistics.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of bert and elmo on ten\nbenchmarking datasets. In Proceedings of the 2019\nWorkshop on Biomedical Natural Language Process-\ning (BioNLP 2019), pages 58–65.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nLance Ramshaw and Mitch Marcus. 1995. Text chunk-\ning using transformation-based learning. In Third\nWorkshop on Very Large Corpora.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. BioMegatron: Larger\nbiomedical domain language model. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4700–4706, Online. Association for Computational\nLinguistics.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2020. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2020.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit.\nA Appendix\nA.1 Fine-Tuning Hyperparameters\nTask Model E LR B\nNER ELECTRA B 5 2e-4 48\nNER BioM-ELECTRA B 5 2e-4 48\nNER BioM-ELECTRA L 5 7e-5 32\nNER ELECTRA L 5 7e-5 32\nNER BioM-BERT L 5 7e-5 32\nNER BioM-ALBERT xxl 4 3e-5 16\nNER ALBERT xxl 4 3e-5 16\nRE ELECTRA B 4 1e-4 32\nRE BioM-ELECTRA B 4 1e-4 32\nRE BioM-ELECTRA L 4 7e-5 32\nRE ELECTRA L 4 7e-5 32\nRE BioM-BERT L 4 7e-5 32\nRE BioM-ALBERT xxl 5 3e-5 128\nRE ALBERT xxl 5 3e-5 128\nSQ. PubMedBERT 2 5e-5 32\nSQ. BioM-ELECTRA B 3 1e-4 32\nSQ. BioM-ELECTRA L 3 5e-5 32\nSQ. BioM-BERT L 5 5e-5 48\nSQ. BioM-ALBERT xxl 2 3e-5 128\nBio. BioM-ELECTRA B 4 2e-5 24\nBio. ELECTRA B 4 2e-5 24\nBio. BioM-ELECTRA L 4 2e-5 24\nBio. ELECTRA L 4 2e-5 24\nBio. PubMedBERT 3 1e-5 128\nBio. BioM-ALBERT xxl 3 1e-5 128\nBio. ALBERT xxl 3 1e-5 128\nTable 4: Fine-Tuning hyperparameters of our pre-\ntrained models and base-line general models. We ﬁne-\ntune all listed models with TensorFlow 1.15 on TPUv3-\n8 unit. (SQ.: SQuAD2.0, Bio.: BioASQ7B-Factoid, E:\nEpochs, LR: learning rate, B: Batch size).",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.756872832775116
    },
    {
      "name": "Language model",
      "score": 0.7134446501731873
    },
    {
      "name": "Computer science",
      "score": 0.7120939493179321
    },
    {
      "name": "Domain adaptation",
      "score": 0.48577240109443665
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4264025390148163
    },
    {
      "name": "Natural language processing",
      "score": 0.335773766040802
    },
    {
      "name": "Engineering",
      "score": 0.16180935502052307
    },
    {
      "name": "Electrical engineering",
      "score": 0.05986487865447998
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}