{
    "title": "Phrase Based Language Model For Statistical Machine Translation",
    "url": "https://openalex.org/W1822873414",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100746377",
            "name": "Jia Xu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5045291858",
            "name": "Geliang Chen",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2273998515",
        "https://openalex.org/W2168420582",
        "https://openalex.org/W2166905217",
        "https://openalex.org/W189068596",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W2135258175",
        "https://openalex.org/W2626190081",
        "https://openalex.org/W2160825952",
        "https://openalex.org/W2006969979"
    ],
    "abstract": "We consider phrase based Language Models (LM), which generalize the commonly used word level models. Similar concept on phrase based LMs appears in speech recognition, which is rather specialized and thus less suitable for machine translation (MT). In contrast to the dependency LM, we first introduce the exhaustive phrase-based LMs tailored for MT use. Preliminary experimental results show that our approach outperform word based LMs with the respect to perplexity and translation quality.",
    "full_text": "arXiv:1501.04324v1  [cs.CL]  18 Jan 2015\nPhrase Based Language Model for Statistical Machine Translation\nJia Xu and Geliang Chen*\nIIIS, Tsinghua University\nxu@tsinghua.edu.cn, cglcgl200208@126.com\n— Working Paper —\nJuly 1, 2021\nAbstract\nWe consider phrase based Language Mod-\nels (LM), which generalize the commonly\nused word level models. Similar concept on\nphrase based LMs appears in speech recog-\nnition, which is rather specialized and thus\nless suitable for machine translation (MT).\nIn contrast to the dependency LM, we\nﬁrst introduce the exhaustive phrase-based\nLMs tailored for MT use. Preliminary ex-\nperimental results show that our approach\noutperform word based LMs with the re-\nspect to perplexity and translation quality.\n1 Introduction\nStatistical language models estimating the\ndistribution of various natural language\nphenomena are crucial for many applica-\ntions. In machine translation, it measures\nthe ﬂuency and well-formness of a trans-\nlation, and therefore is important for the\ntranslation quality, see (Och, 2002) and\n(Koehn, Och and Marcu, 2003) etc.\nCommon applications of LMs include esti-\nmating the distribution based on N-gram cover-\nage of words, to predict word and word orders,\nas in (Stolcke, 2002) and (Laﬀerty et. al., 2001).\nThe independence assumption for each word is\none of the simplifying method widely adopted.\nHowever, it does not hold in textual data, and\n* This version of the paper was submitted for review\nto EMNLP 2013. The title, the idea and the content of\nthis paper was presented by the ﬁrst author in the ma-\nchine translation group meeting at the MSRA-NLC lab\n(Microsoft Research Asia, Natural Language Computing)\non July 16, 2013.\nunderlying content structures need to be inves-\ntigated as discussed in (Gao et. al., 2004).\nWe model the prediction of phrase and phrase\norders. By considering all word sequences as\nphrases, the dependency inside a phrase is pre-\nserved, and the phrase level structure of a sen-\ntence can be learned from observations. This\ncan be considered as an n-gram model on the\nn-gram of words, therefore word based LM is a\nspecial case of phrase based LM if only single-\nword phrases are considered. Intuitively our ap-\nproach has the following advantages:\n1) Long distance dependency : The phrase\nbased LM can capture the long distance rela-\ntionship easily. To capture the sentence level de-\npendency, e.g. between the ﬁrst and last word of\nthe sentence in Table 1, we need a 7-gram word\nbased LM, but only a 3-gram phrase based LM,\nif we take “played the basketball” and “the day\nbefore yesterday” as phrases.\n2) Consistent translation unit with phrase\nbased MT: Some words may acquire meaning\nonly in context, such as “day”, or “the” in “the\nday before yesterday” in Table 1. Considering\nthe frequent phrases as single units will reduce\nthe entropy of the language model. More im-\nportantly, current MT is performed on phrases,\nwhich is taken as the translation unit. The\ntranslation task is to predict the next phrase,\nwhich corresponds to the phrased based LM.\n3) Fewer independence assumptions in statis-\ntical models: The sentence probability is com-\nputed as the product of the single word prob-\nabilities in the word based n-gram LM and the\nproduct of the phrase probabilities in the phrase\nbased n-gram LM, given their histories. The less\nWords John played basketball the day before yesterday\nwI\n1 w1 w2 w3 w4 w5 w6 w7\nSegmentations John played basketball the day before yesterday\nkJ\n1 k1 = 1 k2 = 3 k3 = 7\npJ\n1 p1 = w1 p2 = w2w3 p3 = w4w5w6w7\nRe-ordered John\nthe day before yesterday play basketball\nTranslation 约翰 昨天 打篮球\nTable 1: Phrase segmentation example.\nwords/phrases in a sentence, the fewer mistakes\nthe LM may contain due to less independence\nassumption on words/phrases. Once the phrase\nsegmentation is ﬁxed, the number of elements\nvia phrase based LM is much less than that via\nthe word based LM. Therefore, our approach is\nless likely to obtain errors due to assumptions.\n4) Phrase boundaries as additional informa-\ntion: We consider diﬀerent segmentation of\nphrases in one sentence as a hidden variable,\nwhich provides additional constraints to align\nphrases in translation. Therefore, the constraint\nalignment in the blocks of words can provide\nmore information than the word based LM.\nComparison to Previous Work In the\ndependency or structured LM, phrases cor-\nresponding to the grammars are considered,\nand dependencies are extracted, such as in\n(Gao et. al., 2004) and in (Shen et. al., 2008).\nHowever, in the phrase based SMT, even phrases\nviolating the grammar structure may help as a\ntranslation unit. For instance, the partial phrase\n“the day before” may appear both in “the day\nbefore yesterday” and “the day before Spring”.\nMost importantly, the phrase candidates in our\nphrase based LM are same as that in the phrase\nbased translation, therefore are more consistent\nin the whole translation process, as mentioned\nin item 2 in Section 1.\nSome researchers have proposed their\nphrase based LM for speech recognition. In\n(Kuo and Reichl, 1999) and (Tang, 2002),\nnew phrases are added to the lexi-\ncon with diﬀerent measure function.\nIn (Heeman and Damnati, 1997), a diﬀer-\nent LM was proposed which derived the phrase\nprobabilities from a language model built at\nthe lexical level. Nonetheless, these methods do\nnot consider the dependency between phrases\nand the re-ordering problem, and therefore are\nnot suitable for the MT application.\n2 Phrase Based LM\nWe are given a sentence as a sequence of words\nwI\n1 = w1w2 · · ·wi · · ·wI (i ∈ 1, 2, · · ·, I ), where I\nis the sentence length.\nIn the word based LM (Stolcke, 2002), the\nprobability of a sentence Pr(wI\n1) 1is deﬁned as\nthe product of the probabilities of each word\ngiven its previous n − 1 words:\nP(wI\n1) =\nI∏\ni=1\nP(wi|wi−1\ni−n+1) (1)\nThe positions of phrase boundaries on a word\nsequence wI\n1 is indicated by k0 ≡ 0 and K =\nkJ\n1 = k1k2 · · ·kj · · ·kJ (j ∈ 1, 2, · · ·, J ), where\nkj ∈ { 1, 2, · · ·, I }, k j−1 < k j , k J ≡ I, and J is\nthe number of phrases in the sentence. We use\nkj to indicate that the j-th phrase segmentation\nis placed after the word wkj and in front of word\nwkj +1, where 1 ≤ j ≤ J. k0 is a boundary on\nthe left side of the ﬁrst word w1, which is deﬁned\nas 0, and kJ is always placed after the last word\nwI and therefore equals I.\nAn example is illustrated in Table 1. The En-\nglish sentence ( wI\n1) contains seven words ( I =\n7), where w1 denotes “John”, etc. The ﬁrst\nphrase segmentation boundary is placed after\nthe ﬁrst word, and the second boundary is after\nthe third word ( k = 3) and so on. The phrase se-\nquence pJ\n1 in this sentence have a diﬀerent order\n1The notational convention will be as follows: we use\nthe symbol Pr to denote general probability distributions\nwith (almost) no speciﬁc assumptions. In contrast, for\nmodel-based probability distributions, we use the generic\nsymbol P(∆).\nthan that in its translation, on the phrase level.\nHence, the phrase based LM advances the word\nbased LM in learning the phrase re-ordering.\n(1) Model description Given a sequence of\nwords wI\n1 and its phrase segmentation bound-\naries kJ\n1 , a sentence can also be represented\nin the form of a sequence of phrases pJ\n1 =\np1p2 · · ·pj · · ·pJ (j ∈ 1, 2, · · ·, J ), and each in-\ndividual phrase pj is deﬁned as\npj = wkj−1+1 · · ·wkj = wkj\nkj−1+1\nIn phrase based LM, we consider the phrase seg-\nmentation kJ\n1 as hidden variable and the Equa-\ntion 1 can be extended as follows:\nPr(wI\n1) =\n∑\nK\nPr(wI\n1, K )\n=\n∑\nkJ\n1 ,J\nPr(pJ\n1 |kJ\n1 ) ·Pr(kJ\n1 ) (2)\n(2) Sentence probability For the segmen-\ntation prior probability, we assume a uniform\ndistribution for simplicity, i.e. P(kJ\n1 ) = 1 / |K|,\nwhere the number of diﬀerent K, i.e. |K|= 2 I if\nnot considering the maximum phrase or phrase\nn-gram length; To compute the Pr(wm\n1 ), we con-\nsider either two approaches:\n• Sum Model (Baum-Welch)\nWe consider all 2 I segmentation candidates.\nEquation 2 is deﬁned as\nPrsum(wI\n1) ≈\n∑\nkJ\n1 ,J\nJ∏\nj=1\nP(pj |pj−1\nj−n+1) ·P(kJ\n1 ),\n• Max Model (Viterbi)\nThe sentence probability formula of the sec-\nond model is deﬁned as\nPmax(wI\n1) ≈ max\nkJ\n1 ,J\nJ∏\nj=1\nP(pj |pj−1\nj−n+1) ·P(kJ\n1 ).\nIn practice we select the segmentation that\nmaximizes the perplexity of the sentence\ninstead of the probability to consider the\nlength normalization.\n(3) Perplexity Sentence perplexity and text\nperplexity in the sum model use the same def-\ninition as that in the word based LM. Sentence\nperplexity in the max model is deﬁned as\nP P L(wI\n1) = argmin\nkJ\n1 ,J\n[P(wI\n1, k J\n1 )]−1/J\n.\n(4) Parameter estimation We apply maxi-\nmum likelihood to estimate probabilities in both\nsum model and max model :\nP(pi|pi−1\ni−n+1) = C(pi)\nC(pi−1\ni−n+1), (3)\nwhere C(·) is the frequency of a phrase. The uni-\ngram phrase probability is P(p) = C(p)\nC , and C is\nthe frequency of all single phrases, in the train-\ning text. Since we generate exponential number\nof phrases to the sentence length, the number of\nparameters is huge. Therefore, we set the max-\nimum n-gram length on the phrase level (note\nnot the phrase length) as N = 3 in experiments.\n(5) Smoothing For the unseen events, we\nperform Good-Turing smoothing as commonly\ndone in word based LMs. Moreover, we inter-\npolate between the phrase probability and the\nproduct of single word probabilities in a phrase\nusing a convex optimization:\nP∗(pj |pj−1\nj−n+1) =\nλP(pj |pj−1\nj−n+1) + (1 − λ)\n∏ j′\ni=1 P(wi)\n( ∑\nw P(w)\n) j′\nwhere phrase pj is made up of j′ words wj′\n1 . The\nidea of this interpolation is to make the probabil-\nity of a phrase consisting of of j′ words smooth\nwith a j′-word unigram probability after nor-\nmalization. In our experiments, we set λ = 0 . 4\nfor convenience.\n(6) Algorithm of calculating phrase n-\ngram counts The training task is to calculate\nn-gram counts on the phrase level in Equation 3.\nGiven a training corpus W S\n1 , where there are\nS sentences Ws (s = 1 , 2, · · ·, S ), our goal is to\nto compute C(·), for all phrase n-grams that the\nnumber of phrases is no greater than N. There-\nfore, for each sentence wI\n1, we should ﬁnd out\nevery n-gram phrases that 0 < n < N .\nData Sentences Words Vocabulary\nTraining 54887 576778 23350\nDev2010 202 1887 636\nTst2010 247 2170 617\nTst2011 334 2916 765\nTable 2: Statistics of corpora with sentence length\nno greater than 15 in training and 10 in test.\nn Base Sum Sum+S. Max Max+S.\n1 676.1 85.5 112.5 625.7 1129.4\n2 180.8 52.6 72.1 161.1 306.2\n3 162.3 52.5 72.2 140.4 266.5\n4 162.5 52.6 72.3 141.1 267.6\nTable 3: Perplexities on Tst2011 calculated based on\nvarious n-gram LMs with n = 1 , 2, 3, 4.\nWe do Dynamic Programming to collect the\nphrase n-grams in one sentence wI\n1:\nQ(1, d ; wI\n1 ) = {p = wd\nb , ∀1 ≤ b ≤ d ≤ I}\nQ(n, d ; wI\n1) =\n∪ b Q(n − 1, b − 1; wI\n1 ) ⊕ p = wd\nb , ∀n ≤ b ≤ d ≤ I,\nwhere Q(·) is the auxiliary function denoting the\nmultiset of all phrase n-grams or unigram end-\ning at position d (1 < n ≤ N). b denotes the\nstarting word position of the last phrase in the\nmultiset. The {·} is a multiset, and ⊕ means\nto append the element to each element in the\nmultiset. ∪ b denotes the union of multisets. Af-\nter appending p, we consider all b that is no less\nthan n and no greater than d.\nThe phrase counts C(·) is the sum of all phrase\nn-grams from all sentences W S\n1 , with each sen-\ntence Ws = wI\n1, and |·|is the number of elements\nin a multiset:\nC(pn\n1 ) =\nS∑\ns=1\n|pn\n1 ∈ ∪ |Ws|\nd=n Q(n, d ; Ws)|\n3 Experiments\nThis is an ongoing work, and we per-\nformed preliminary experiments on the\nIWSLT (IWSLT, 2011) task, then evalu-\nated the LM performance by measuring the LM\nperplexity and the MT translation performance.\nModel Dev2010 Tst2010 Tst2011\nBase 11.26 13.10 15.05\nWord 11.92 12.93 14.76\nSum 11.86 12.77 14.80\nSum+S. 12.02 12.54 14.76\nMax 11.61 12.99 15.34\nMax+S. 11.56 13.55 15.27\nTable 4: Translation performance on N-best list us-\ning diﬀerent LMs in BLEU[%].\nBase: but we need a success\nMax: but we need a way to success .\nRef: we certainly need one to succeed .\nBase: there is a speciﬁc steps that\nMax: there is a speciﬁc steps .\nRef: there is step-by-step instructions on this .\nTable 5: Examples of sentence outputs with baseline\nmethod and with the max model.\nBecause of the computational requirement, we\nonly employed sentences which contain no\nmore than 15 words in the training corpus and\nno more than 10 words in the test corpora\n(Dev2010, on Tst2010 and on Tst2011), as\nshown in Table 2.\nWe took word based LM in Equation 1 as the\nbaseline method (Base). We calculated the per-\nplexities of Tst2011 with diﬀerent n-gram orders\nusing both sum model and max model, with\nand without smoothing (S.) as in Section 2. Ta-\nble 3 shows that perplexities in our approaches\nare all lower than those in the baseline.\nFor MT, we selected the single best trans-\nlation output based on the LM perplexity of\nthe 100-best translation candiates, using diﬀer-\nent LMs as shown in Table 4. Max model\nalong with smoothing outperforms the baseline\nmethod under all three test sets with the BLEU\nscore (Papineni et. al., 2002) increase of 0.3%\non Dev2010, 0.45% on Tst2010, and 0.22% on\nTst2011, respectively.\nTable 5 shows two examples from the Tst2010,\nwhere we can see that our max modelgenerates\nbetter selection results than the baseline method\nin these cases.\n4 Conclusion\nWe showed the preliminary results that a phrase\nbased LM can improve the performance of MT\nsystems and the LM perplexity. We presented\ntwo phrase based models which consider phrases\nas the basic components of a sentence and per-\nform exhaustive search. Our future work will\nfocus on the eﬃciency for a larger data track\nas well as the improvements on the smoothing\nmethods.\nReferences\n[Aho and Ullman1972] Alfred V. Aho and Jeﬀrey D.\nUllman. 1972. The Theory of Parsing, Transla-\ntion and Compiling, volume 1. Prentice-Hall, En-\nglewood Cliﬀs, NJ.\n[American Psychological Association1983] American\nPsychological Association. 1983. Publications\nManual. American Psychological Association,\nWashington, DC.\n[Association for Computing Machinery1983]\nAssociation for Computing Machinery. 1983.\nComputing Reviews, 24(11):503–512.\n[Chandra et al.1981] Ashok K. Chandra, Dexter C.\nKozen, and Larry J. Stockmeyer. 1981. Alter-\nnation. Journal of the Association for Computing\nMachinery, 28(1):114–133.\n[Gusﬁeld1997] Dan Gusﬁeld. 1997. Algorithms on\nStrings, Trees and Sequences. Cambridge Univer-\nsity Press, Cambridge, UK.\n[Kuo and Reichl1999] Hong-Kwang Jeﬀ Kuo and\nWolfgang Reichl. 1999. Phrase-Based Lan-\nguage Models for Speech Recognition. In EU-\nROSPEECH.\n[Tang2002] Haijiang Tang. 2002. Building Phrase\nBased Language Model from Large Corpus. Mas-\nter thesis, The Hong Kong University of Science\nand Technology, Hong Kong.\n[Och2002] F. Och. 2002. Statistical Machine Trans-\nlation: From Single Word Models to Alignment\nTemplates. Ph.D. thesis, RWTH Aachen, Ger-\nmany.\n[Papineni et. al.2002] K. Papineni, S. Roukos, T.\nWard, and W. Zhu. 2002. Bleu: a method for\nautomatic evaluation of machine translation. In\nProc. of ACL, pp. 311–318.\n[Koehn, Och and Marcu2003] P. Koehn, F. Och, and\nD. Marcu. 2003. Statistical phrasebased transla-\ntion. In Proc. of HLT-NAACL, pp. 48–54.\n[Heeman and Damnati1997] Peter A. Heeman and\nGeraldine Damnati. 1997. Deriving Phrase-based\nLanguage Models. In Automatic Speech Recogni-\ntion and Understanding, 1997. Proceedings., 1997\nIEEE Workshop onpp. 41–48. IEEE.\n[Brown, S. Della Pietra, V. Della Pietra, and R. Mercer1993]\nP. Brown, S. Della Pietra, V. Della Pietra, and\nR. Mercer. 1993. The mathematics of statistical\nmachine translation: Parameter estimation.\nComputational linguistics, 19(2), 263-311.\n[Gao et. al.2004] J. Gao, J. Y. Nie, G. Wu, and G.\nCao. 2004. Dependence language model for infor-\nmation retrieval. In Proc. of ACM, pp. 170-177.\nACM.\n[Shen et. al.2008] L. Shen, J. Xu, and R. M.\nWeischedel. 2008. A New String-to-Dependency\nMachine Translation Algorithm with a Target De-\npendency Language Model. In ACL, pp. 577-585.\n[Rosenfeld2000] R. Rosenfeld. 2000. Two decades of\nstatistical language modeling: Where do we go\nfrom here?. Proc. of IEEE, 88(8), 1270-1278.\n[Stolcke2002] A. Stolcke. 2002. SRILM-an extensible\nlanguage modeling toolkit. In INTERSPEECH.\n[Laﬀerty et. al.2001] Laﬀerty, John, Andrew McCal-\nlum, and Fernando CN Pereira. 2001. Conditional\nrandom ﬁelds: Probabilistic models for segment-\ning and labeling sequence data. In Intl. Conf. on\nMachine Learning,\n[Roark, Saraclar, Collins, and Johnson2004] B.\nRoark, M. Saraclar, M. Collins, and M. Johnson.\n2004. Discriminative language modeling with\nconditional random ﬁelds and the perceptron\nalgorithm. In Proc. of ACL, pp. 47. ACL.\n[Chelba1997] C. Chelba. 1997. A structured lan-\nguage model. In Proc. of ACL, pp. 498-500. ACL.\n[IWSLT2011] IWSLT. 2011. http://iwslt2011.org/.\nHomepage."
}