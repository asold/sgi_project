{
  "title": "Binarized LSTM Language Model",
  "url": "https://openalex.org/W2803431233",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2101220011",
      "name": "Xuan Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1936178441",
      "name": "Di Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108696738",
      "name": "Kai Yu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2553427643",
    "https://openalex.org/W2554592357",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2112184938",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2963547822",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2749241159",
    "https://openalex.org/W2755192592",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2319920447",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2267635276",
    "https://openalex.org/W1902934009",
    "https://openalex.org/W2053921957",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2515753980",
    "https://openalex.org/W2067438047",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W4295262505",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W2963114950"
  ],
  "abstract": "Long short-term memory (LSTM) language model (LM) has been widely investigated for automatic speech recognition (ASR) and natural language processing (NLP). Although excellent performance is obtained for large vocabulary tasks, tremendous memory consumption prohibits the use of LSTM LM in low-resource devices. The memory consumption mainly comes from the word embedding layer. In this paper, a novel binarized LSTM LM is proposed to address the problem. Words are encoded into binary vectors and other LSTM parameters are further binarized to achieve high memory compression. This is the first effort to investigate binary LSTM for large vocabulary LM. Experiments on both English and Chinese LM and ASR tasks showed that can achieve a compression ratio of 11.3 without any loss of LM and ASR performances and a compression ratio of 31.6 with acceptable minor performance degradation.",
  "full_text": "Proceedings of NAACL-HLT 2018, pages 2113–2121\nNew Orleans, Louisiana, June 1 - 6, 2018.c⃝2018 Association for Computational Linguistics\nBinarized LSTM Language Model\nXuan Liu∗, Di Cao∗, Kai Yu\nKey Laboratory of Shanghai Education Commission for\nIntelligent Interaction and Cognitive Engineering,\nSpeechLab, Department of Computer Science and Engineering,\nBrain Science and Technology Research Center,\nShanghai Jiao Tong University, Shanghai, China\n{liuxuan0526, caodi0207, ky219.cam}@gmail.com\nAbstract\nThe long short-term memory (LSTM) lan-\nguage model (LM) has been widely investi-\ngated for automatic speech recognition (ASR)\nand natural language processing (NLP). Al-\nthough excellent performance is obtained for\nlarge vocabulary tasks, tremendous memory\nconsumption prohibits the use of LSTM LMs\nin low-resource devices. The memory con-\nsumption mainly comes from the word em-\nbedding layer. In this paper, a novel binarized\nLSTM LM is proposed to address the problem.\nWords are encoded into binary vectors and\nother LSTM parameters are further binarized\nto achieve high memory compression. This\nis the ﬁrst effort to investigate binary LSTMs\nfor large vocabulary language modeling. Ex-\nperiments on both English and Chinese LM\nand ASR tasks showed that binarization can\nachieve a compression ratio of 11.3 without\nany loss of LM and ASR performance and a\ncompression ratio of 31.6 with acceptable mi-\nnor performance degradation.\n1 Introduction\nLanguage models (LMs) play an important role\nin natural language processing (NLP) tasks. N-\ngram language models used to be the most pop-\nular language models. Considering the previous\nN-1 words, N-gram language models predict the\nnext word. However, this leads to the loss of long-\nterm dependencies. The sample space size in-\ncreases exponentially as N grows, which induces\ndata sparseness ( Cao and Yu , 2017).\nNeural network (NN) based models were ﬁrst\nintroduced into language modeling in 2003 ( Ben-\ngio et al. , 2003). Given contexts with a ﬁxed\nsize, the model can calculate the probability dis-\ntribution of the next word. However, the prob-\nlem of long-term dependencies still remained, be-\n∗Both authors contributed equally to this work.\ncause the context window is ﬁxed. Currently, re-\ncurrent neural network (RNN) based models are\nwidely used on natural language processing (NLP)\ntasks for excellent performance ( Mikolov et al. ,\n2010). Recurrent structures in neural networks can\nsolve the problem of long-term dependencies to\na great extent. Some gate based structures, such\nas long short-term memory (LSTM) ( Hochreiter\nand Schmidhuber , 1997) and gated recurrent unit\n(GRU) ( Chung et al. , 2014) improve the recur-\nrent structures and achieve state-of-the-art perfor-\nmance on most NLP tasks.\nHowever, neural network models occupy\ntremendous memory space so that it is almost im-\npossible to put the models into low-resource de-\nvices. In practice, the vocabulary is usually very\nlarge. So the memory consumption mainly comes\nfrom the embedding layers. And, the word embed-\nding parameters are ﬂoating point values, which\nadds to the memory consumption.\nThe ﬁrst contribution in this paper is that a\nnovel language model, the binarized embedding\nlanguage model (BELM) is proposed to reduce the\nmemory consumption. Words are represented in\nthe form of binarized vectors. Thus, the consump-\ntion of memory space is signiﬁcantly reduced. An-\nother contribution in the paper is that we binarize\nthe LSTM language model combined with the bi-\nnarized embeddings to further compress the pa-\nrameter space. All the parameters in the LSTM\nlanguage model are binarized.\nExperiments are conducted in language mod-\neling and automatic speech recognition (ASR)\nrescoring tasks. Our model performs well with-\nout any loss of performance at a compression ratio\nof 11.3 and still has acceptable results with only\na minor loss of performance even at a compres-\nsion ratio of 31.6. Investigations are also made\nto evaluate whether the binarized embeddings lose\ninformation. Experiments are conducted on word\n2113\nsimilarity tasks. The results show the binarized\nembeddings generated by our models still perform\nwell on the two datasets.\nThe rest of the paper is organized as follows,\nsection 2 is the related work. Section 3 explains\nthe proposed language model and section 4 shows\nthe experimental setup and results. Finally, con-\nclusions will be given in section 5 and we describe\nfuture work in section 6.\n2 Related Work\nNowadays, with the development of deep learn-\ning, neural networks have yielded good results in\nmany areas. However, neural networks may re-\nquire tremendous memory space, making it difﬁ-\ncult to run such models on low-resource devices.\nThus, it is necessary to compress neural networks.\nIn recent years, many methods of compress-\ning neural networks have been proposed. Pruning\n(Han et al. , 2015) reduces the number of parame-\nters of the neural network by removing all connec-\ntions with the weights below a threshold. Quanti-\nzation ( Han et al. , 2015) clusters weights to sev-\neral clusters. A few bits are used to represent the\nneurons and to index a few ﬂoat values.\nBinarization is also a method to compress neu-\nral networks. BNNs( Courbariaux et al. , 2016) are\nbinarized deep neural networks. The weights and\nactivations are constrained to 1 or □1. BNNs can\ndrastically reduce memory size and replace most\narithmetic operations with bit-wise operations.\nDifferent from pruning and quantization, bina-\nrization does not necessarily require pre-training\nand can achieve a great compression ratio. Many\nbinarization methods have been proposed ( Cour-\nbariaux et al. , 2015, 2016; Rastegari et al. , 2016;\nXiang et al. , 2017). However, only a few ( Hou\net al. , 2016; Edel and K ¨oppe, 2016) are related to\nrecurrent neural network. ( Hou et al. , 2016) imple-\nments a character level binarized language model\nwith a vocabulary size of 87. However, they did\nnot do a comprehensive study on binarized large\nvocabulary LSTM language models.\n3 Binarized Language Model\n3.1 LSTM Language Model\nThe RNN language model is proposed to deal with\nsequential data. Due to the vanishing and explod-\ning gradients problem, it is difﬁcult for a RNN\nlanguage model to learn long-term dependencies.\nThe LSTM, which strengthens the recurrent neural\nmodel with a gating mechanism, tackles this prob-\nlem and is widely used in natural language pro-\ncessing tasks.\nThe goal of a language model is to compute the\nprobability of a sentence (x1, . . . , x N ). A typical\nmethod is to decompose this probability word by\nword.\nP (x1, ..., x N ) =\nN∏\nt=1\nP (xt|x1, ..., x t□1) (1)\n(Hochreiter and Schmidhuber , 1997) proposed\na Long Short-Term Memory Network, which can\nbe used for sequence processing tasks. Con-\nsider an one-layer LSTM network, where N is the\nlength of the sentence, and xt is the input at the\nt-th moment. yt is the output at the t-th moment,\nwhich is equal to xt+1 in a language model. De-\nnote ht and ct as the hidden vector and the cell\nvector at the t-th moment, which is used for repre-\nsenting the history of (x1, ..., x t□1). h0 and c0 are\ninitialized with zero. Given xt, ht□1 and ct□1,\nthe model calculates the probability of outputting\nyt.\nThe ﬁrst step of an LSTM language model is to\nextract the representation et of the input xt from\nthe embeddings We. Since xt is a one-hot vec-\ntor, this operation can be implemented by indexing\nrather than multiplication.\net = Wext (2)\nAfter that, et, along with ht□1 and ct□1 are fed\ninto the LSTM cell. The hidden vector ht and the\ncell vector ct can be computed according to:\nft = sigmoid (Wf {ht□1, et} + bf )\nit = sigmoid (Wi {ht□1, et} + bi)\not = sigmoid (Wo {ht□1, et} + bo)\nˆct = tanh (Wˆ c{ht□1, et} + bˆc)\nct =ft ·ct□1 + it ·ˆct\nht =ot ·tanh (ct)\n(3)\nThe word probability distribution at the t-th mo-\nment can be calculated by:\nP (yt|x1, ..., x t) =pt = softmax(Wyht) (4)\nThe probability of taking yt as the output at the\nt-th moment is:\npyt = pt × yt (5)\n2114\n3.2 Binarized Embedding Language Model\nThe binarized embedding language model\n(BELM) is a novel LSTM language model with\nbinarized input embeddings and output embed-\ndings. For a one-layer LSTM language model\nwith a vocabulary size of V , embedding and\nhidden layer size of H. The size in bytes of the\ninput embeddings, the output embeddings, and the\nLSTM cells are 4V H , 4V H and 32H2 + 16H.\nWhen V is much larger than H, which is often\nthe case for language models, the parameters of\nthe input embeddings and the output embeddings\noccupy most of the space. If the embeddings of\nthe input layer and the output layer are binarized,\nthe input layer and the output layer will only take\n1/ 32 of the original memory consumption, which\ncan greatly reduce the memory consumption of\nrunning neural language model.\nIt is important to ﬁnd good binary embeddings.\nDirectly binarizing well-trained word embeddings\ncannot yield good binarized representations. In-\nstead, we train good binary embeddings from\nscratch. The training approach is similar to the\nmethods proposed in ( Courbariaux et al. , 2016;\nRastegari et al. , 2016). At run-time, the input em-\nbedding and the output embedding are binarized\nmatrices. However, at train-time, ﬂoat versions of\nthe embeddings, which are used for calculating the\nbinarized version of embeddings, are still main-\ntained. In the propagation step, a deterministic\nfunction sign is used to binarize the ﬂoat versions\nof the embeddings. In the back-propagation step,\nthe ﬂoat versions of the embeddings are updated\naccording to the gradient of the binarized embed-\nding.\nwb = sign (w) =\n{\n+ 1 if w > 0,\n□ 1 otherwise. (6)\nThe derivative of the sign function is zero al-\nmost everywhere, and it is impossible to back-\npropagate through this function. As introduced in\n(Hubara et al. , 2016), a straight-through estimator\nis used to get the gradient. Assume the gradient of\nthe binarized weight ∂C\n∂Wb has been obtained, the\ngradient of the ﬂoat version of the weight is:\n∂C\n∂W = ∂C\n∂Wb (7)\nA typical weight initialization method initial-\nizes each neuron’s weights randomly from the\nGaussian distribution N(0,\n√\n1/H ). This initial-\nization approach can maximize the gradients and\nmitigate the vanishing gradients problem. From\nthis perspective, 1 or □1 is too large. So, in\npractice, we binarize the embeddings to a smaller\nscale. Although the weight is binarized to a ﬂoat-\ning point number, the matrix can also be saved one\nbit per neuron, as long as the ﬁxed ﬂoat value is\nmemorized separately.\nbinarize (w) =\n{\n+\n√\n1/H if w > 0,\n□\n√\n1/H otherwise.\n(8)\nSince directly binarizing the input embeddings\nWe and the output embeddings Wy will limit\nthe scale of the embeddings, additional linear lay-\ners (without activation) are added behind the input\nembedding layer and in front of the output em-\nbedding layer to enhance the model. Denote Wb\ne\nand Wb\ny as the binarized weights corresponding\nto We and Wy. Denote WTe and bTe, WTy and\nbTy as the weights and the biases of the ﬁrst and\nthe second linear layer. The input of the LSTM et\nand the word probability pt of the binarized em-\nbedding language model are calculated according\nto:\net =WTe\n(\nWb\ne xt\n)\n+ bTe\npt = softmax\n(\nWb\ny\n(\nWTy ht + bTy\n)) (9)\nThe additional linear layer before the output\nembedding layer is very important for the bina-\nrized embedding language model, especially for\nlow dimensional models. Removing this layer will\nresult in an obvious decrease in performance.\n3.3 Binarized LSTM Language Model\nSubsection 3.2 explains how to binarize the em-\nbedding layer, but the LSTM network can also be\nbinarized. In a binarized LSTM language model,\nall the matrices in the parameters are binarized,\nwhich can save much more memory space. Im-\nplementing the binarized linear layer is important\nfor designing a binarized LSTM language model\n(BLLM). In a binarized linear layer, there are three\nparameters, W, γ and b. W is a matrix, γ and b\nare vectors. The matrix W, which takes up most\nof the space in a linear layer, is binarized. γ and\nb remain ﬂoating point values. b is the bias of the\nlinear layer, and γ is introduced to ﬁx the scale\nproblem of the binary matrix.\n2115\nThe forward- and back-propagation algorithms\nare shown in Algorithm 1 and Algorithm 2. The\nstructure of this linear layer is very similar to\nthe structure of batch normalization ( Ioffe and\nSzegedy, 2015), except the output of each di-\nmension over the mini-batches is not normalized.\nBatch normalization is hard to apply to a recurrent\nneural network, due to the dependency over en-\ntire sequences. However, the structure of the batch\nnormalization is quite useful. Since binarizing W\nwould ﬁx the scale of the weight, additional free-\ndom is needed to overcome this issue. The shift\noperation can rescale the output to a reasonable\nrange.\nAlgorithm 1The propagation of linear layer\nInput: input x, weights W, γ and b\nOutput: output y\n1: Wb = binarize (W)\n2: s = Wbx\n3: y = s ·exp (γ) +b\nAlgorithm 2The back-propagation of linear layer\nInput: input x, weights W, γ and b, binarized\nweight Wb, temporary value s (calculated in the\npropagation period), the gradient of the output ∂C\n∂y ,\nlearning rate η, binary weight range α\nOutput: the gradient of the input ∂C\n∂x , the gra-\ndient of the weight ∂C\n∂W , ∂C\n∂γ , ∂C\n∂b , update the\nweights\n1: ∂C\n∂b = ∂C\n∂y\n2: ∂C\n∂γ = ∂C\n∂y ·s ·exp (γ)\n3: ∂C\n∂s = ∂C\n∂y ·exp (γ), ∂C\n∂Wb = ∂C\n∂s x, ∂C\n∂W =\n∂C\n∂Wb\n4: ∂C\n∂x = ∂C\n∂s Wb\n5: update W, γ, b according to ∂C\n∂W , ∂C\n∂γ , ∂C\n∂b\nwith learning rate η.\n6: clamp(W, □α, α )\n7: return ∂C\n∂x\nThe structure of the input embeddings and the\noutput embeddings of the binarized LSTM lan-\nguage model is similar to the binarized embedding\nlanguage model. The embeddings are binarized\nand additional linear layers are added after the in-\nput embedding layer and in front of the output\nembedding layer. However, the additional linear\nlayers are also binarized according to Algorithm 1\nand Algorithm 2.\n3.4 Memory Reduction\nDenote the size of the vocabulary as V , and the\nsize of the embedding and hidden layer as H. The\nmemory consumptions of a one-layer LSTM lan-\nguage model, BELM and BLLM are listed in Ta-\nble 1.\nModel Memory (bytes)\nLSTM 8V H + 32H2 + 16H\nBELM 0. 25V H + 40H2 + 24H\nBLLM 0. 25V H + 1. 25H2 + 48H\nTable 1: Memory Requirements\nFor a language model, the vocabulary size is\nusually much larger than the hidden layer size.\nThe main memory consumption comes from the\nembedding layers, which require 8V H bytes for\nan LSTM language model. Binarized embeddings\ncan reduce this term to 0. 25V H bytes. Further\ncompression of the LSTM can drop the coefﬁcient\nof H2 from 32 to 1. 25.\n4 Experiments\n4.1 Experimental Setup\nOur model is evaluated on the English Penn\nTreeBank (PTB) ( Marcus et al. , 1993), Chinese\nshort message (SMS) and SWB-Fisher (SWB).\nThe Penn TreeBank corpus is a famous English\ndataset, with a vocabulary size of 10K and 4.8%\nwords out of vocabulary (OOV), which is widely\nused to evaluate the performance of a language\nmodel. The training set contains approximately\n42K sentences with 887K words. The Chinese\nSMS corpus is collected from short messages. The\ncorpus has a vocabulary size of about 40K. The\ntraining set contains 380K sentences with 1931K\nwords. The SWB-Fisher corpus is an English\ncorpus containing approximately 2.5M sentences\nwith 24.9M words. The corpus has a vocabulary\nsize of about 30K. hub5e is the dataset for the\nSWB ASR task.\nWe also evaluate the word embeddings pro-\nduced by our models on two word similarity\ndatasets. The models are trained on the Text8\ncorpus to extract the word embeddings. The\nText8 corpus is published by Google and col-\nlected from Wikipedia. Text8 contains about\n17M words with a vocabulary size of about 47k.\nThe WordSimilarity-353(WS-353) Test Collection\ncontains two sets of English word pairs along with\n2116\nhuman-assigned similarity judgments. The col-\nlection can be used to train and test computer al-\ngorithms implementing semantic similarity mea-\nsures. A combined set (combined) is provided\nthat contains a list of all 353 words, along with\ntheir mean similarity scores. ( Finkelstein et al. ,\n2001) The MEN dataset consists of 3,000 word\npairs, randomly selected from words that occur at\nleast 700 times in the freely available ukWaC and\nWackypedia corpora combined (size: 1.9B and\n820M tokens, respectively) and at least 50 times\n(as tags) in the open-sourced subset of the ESP\ngame dataset. In order to avoid picking unrelated\npairs only, the pairs are sampled so that they repre-\nsent a balanced range of relatedness levels accord-\ning to a text-based semantic score ( Bruni et al. ,\n2014).\nFirst, we conduct experiments on the PTB,\nSWB and Text8 corpora respectively to evaluate\nlanguage modeling performance. We use perplex-\nity (PPL) as the metric to evaluate models of dif-\nferent sizes. Then, the models are evaluated on\nASR rescoring tasks. Rescoring the 100-best sen-\ntences generated by the weighted ﬁnite state trans-\nducer (WFST), the model is evaluated by word er-\nror rate (WER). Finally, we conduct experiments\non word similarity tasks to evaluate whether the\nword embeddings produced by our models lose\nany information.\n4.2 Experiments in Language Modeling\nFor traditional RNN based language models, the\nmemory consumption mainly comes from the em-\nbedding layers (both input and output layers).\nHowever, when the hidden layer size grows, the\nmemory consumption of the RNN module also be-\ncomes larger. So the total memory usage relates to\nboth the vocabulary size and hidden layer size, as\nmentioned in section 3.4.\nExperiments are conducted in language mod-\neling to evaluate the model on the PTB, SWB,\nand SMS corpora respectively. In language mod-\neling tasks, we regularize the networks using\ndropout(Zaremba et al. , 2014). We use stochas-\ntic gradient descent (SGD) for optimization. The\nbatch size is set to 64. For the PTB corpus, the\ndropout rate is tuned for different training settings.\nFor the SWB corpus, we do not use dropout tech-\nnique. For the SMS corpus, the dropout rate is\nset to 0.25. We train models of different sizes\non the three corpora and record the memory us-\nage of the trained models. The initial learning rate\nis set to 1.0 for all settings. Since PTB is a rel-\natively small dataset and the convergence rates of\nthe BELM and the BLLM are slower than LSTM\nlanguage model, we reduce the learning rate by\nhalf every three epochs if the perplexity on the\nvalidation set is not reduced. For the other experi-\nments, the learning rate is always reduced by half\nevery epoch if the perplexity on the validation set\nis not reduced. As introduced in section 3, the bias\nof the output embedding layer is omitted. Adding\nbias term in the output embedding layer leads to\nsmall performance degradation in the BELM and\nthe BLLM model, although it leads to a small im-\nprovement in the LSTM model. This phenomenon\nmay be related to optimization problems.\nHidden\nsize LSTM BELM BLLM\nMemory\nPPL 500 48.0M\n91.8\n11.3M\n88.0\n1.6M\n95.2\nMemory\nPPL 1000 112.0M\n89.4\n42.5M\n85.7\n3.8M\n94.9\nTable 2: Performances on the English PTB corpus\nHidden\nsize LSTM BELM BLLM\nMemory\nPPL 500 129.1M\n57.6\n13.8M\n58.4\n4.1M\n60.4\nMemory\nPPL 1000 274.2M\n56.1\n47.6M\n55.6\n8.9M\n56.2\nTable 3: Performance on the English SWB corpus\nHidden\nsize LSTM BELM BLLM\nMemory\nPPL 500 170.8M\n90.0\n15.1M\n89.8\n5.4M\n96.8\nMemory\nPPL 1000 357.6M\n89.5\n50.2M\n87.8\n11.5M\n94.3\nTable 4: Performance on the Chinese SMS corpus\nBecause the total memory usage relates to both\nthe vocabulary size and hidden layer size, the\nmemory reduction on various corpora is quite dif-\nferent. For our BELM model, the ﬂoating point\nembedding parameters are replaced by single bits,\nwhich could signiﬁcantly reduce the memory us-\nage. On the PTB corpus, the BELM models even\n2117\noutperform the baseline LSTM LM. The small\nmodel (500 LSTM units) has a relative PPL im-\nprovement of 4.1 % and achieves a compression ra-\ntio of 4.3 and the large model (1000 LSTM units)\nalso has a relative PPL improvement of 4.1 % and\nachieves a compression ratio of 2.6. On the SWB\ncorpus, the BELM models still perform well com-\npared with the baseline model and achieve com-\npression ratios of 9.4 and 5.8 respectively for the\nsmall and large models. On the SMS corpus, the\nBELMs model also gains relative PPL improve-\nments of 0.2 % and 1.9 %, and achieves compres-\nsion ratios of 11.3 and 7.1 respectively. In sum-\nmary, the BELM model performs as well as the\nbaseline model both on English and Chinese cor-\npora, and reduces the memory consumption to a\nlarge extent.\nThe BLLM model, however, does not outper-\nform the baseline model, but still has acceptable\nresults with a minor loss of performance. Since\nboth the LSTM model and the embeddings are bi-\nnarized, the total compression ratio is quite sig-\nniﬁcant. The average compression ratio is about\n32.0, so the memory consumption of the language\nmodel is signiﬁcantly reduced.\nWe also study the performance of pruning the\nLSTM language model. We prune each parame-\nter matrix and the embedding layers with various\npruning rates respectively, and ﬁne-tune the model\nwith various dropout rates. In our experiments,\npruning 75% parameter nodes hardly affects the\nperformance. However, if we try pruning more\nparameter nodes, the perplexity increases rapidly.\nFor example, for the English PTB dataset, when\nwe prune 95% parameter nodes of the embedding\nlayers of an LSTM language model (500 LSTM\nunits), the perpexity will increase from 91.8 to\n112.3. When we prune 95% parameter nodes of\nan LSTM language model (500 LSTM units), the\nperplexity will increase from 91.8 to 132.3. There-\nfore, the effect of pruning is not as good as bina-\nrization for the language modeling task.\nBinarization can be considered as a special case\nof quantization, which quantizes the parameters to\npairs of opposite numbers. So, compared to nor-\nmal quantization, binarization can achieve a better\ncompression ratio. In addition, for binarization,\nwe do not need to determine the position of each\nunique values in advance. Therefore, binarization\nis more ﬂexible than quantization.\nWe then study the effect of extra binary linear\nlayers in the BLLM. The additional binary linear\nlayer after the input embedding layer and the ad-\nditional binary linear layer in front of the output\nembedding layer are removed respectively in this\nexperiment. We use well-trained embeddings to\ninitialize the corresponding embedding layers and\ndo the binarization using the method proposed in\n(Rastegari et al. , 2016) when the additional binary\nlinear layer is removed. The perplexities are listed\nin Table 5. No-i means no additional binary linear\nlayer after the input embedding layer. No-o means\nno additional binary linear layer in front of the out-\nput embedding layer. No-io means no additional\nbinary linear layers. The experiment is conducted\non the PTB corpus.\nHidden\nsize BLLMBLLM\nno-i\nBLLM\nno-o\nBLLM\nno-io\nPPL 500 95.2 95.2 101.7 100.3\nPPL 1000 94.9 94.5 96.7 96.3\nTable 5: Performances on the English PTB corpus\nIf the additional binary linear layer after the in-\nput embedding layer is removed, the performance\ndoes not drop, and even becomes better when the\nhidden layer size is 1000. Although the additional\nbinary layer after the input embedding layer is re-\nmoved, the ﬂoat version of the input embeddings\nof BLLM no-i is initialized with well-trained em-\nbeddings, while the BLLM is not initialized with\nthe well-trained embeddings. We think initializa-\ntion is the reason why the BLLM no-i performs\ncomparatively to the BLLM. We also observe a\nPPL increase of 1-2 points for BLLM no-i if the\ninput embeddings are not pre-trained (not listed in\nthe table). This phenomenon prompts us to pre-\ntrain embeddings, which we leave to future work.\nOnce the additional binary linear layer in front of\nthe output embedding layer is removed, the perfor-\nmance degradation is serious. This shows that the\noutput embeddings of the language model should\nnot be directly binarized; the additional binary lin-\near layer should be inserted to enhance the model’s\ncapacity, especially for low dimensional models.\n4.3 Experiments on ASR Rescoring Tasks\nExperiments are conducted on the ASR rescoring\ntask to evaluate the model on the hub5e and SMS\ncorpora. Hub5e is a test dataset of the SWB cor-\npus which we use for ASR rescoring tasks. For\nthe hub5e dateset, A VDCNN ( Qian et al. , 2016)\n2118\n(very deep CNN) model on the 300-hour task is\napplied as the acoustic model. For the Chinese\nSMS dataset, the acoustic model is a CD-DNN-\nHMM model. The weighted ﬁnite state trans-\nducer (WFST) is produced with a 4-gram language\nmodel. Then our language models are utilized to\nrescore the 100-best candidates. The models are\nevaluated by the metric of word error rate (WER).\nModel Hidden\nsize hub5e SMS\nLSTM 8.7 10.5\nBELM 500 8.5 10.3\nBLLM 8.7 10.8\nLSTM 8.5 10.4\nBELM 1000 8.5 10.2\nBLLM 8.4 10.3\nTable 6: Performances on ASR rescoring tasks\nTable 6 shows the results on ASR rescoring\ntasks. The BELM model and BLLM model\nperform well both on the English and Chinese\ndatasets. The BELM model achieves an absolute\n0.2% WER improvement compared with the base-\nline model in three of the experiments. The BLLM\nmodel also has good results, even though it per-\nforms not so well in language modeling. The re-\nsults show that our language models work well on\nASR rescoring tasks even with much less memory\nconsumption.\n4.4 Investigation of Binarized Embeddings\nThe experiments above show the good perfor-\nmances of our models. We also want to investigate\nwhether the binarized embeddings lose any infor-\nmation. So, the embeddings are evaluated on two\nword similarity tasks. Experiments are conducted\non the WS-353 and MEN tasks. We have trained\nthe baseline LSTM model, the BELM model and\nBLLM model of a medium size on the Text8 cor-\npus. We binarize the embeddings of the trained\nbaseline LSTM model to investigate whether there\nis any loss of information by the simple binariza-\ntion method (labeled LSTM-bin in the table be-\nlow). For each dimension, we calculate the mean\nand set the value to 1 if it is bigger than the mean,\notherwise, we set it to -1.\nThe embedding size and the hidden layer size\nare set to 500. We use stochastic gradient descent\n(SGD) to optimize our models. We use cosine dis-\ntance to evaluate the similarity of the word pairs.\nSpearman’s rank correlation coefﬁcient is calcu-\nlated to evaluate the correlation between the two\nscores given by our models and domain experts.\nModel PPL\nLSTM 166.0\nBELM 164.7\nBLLM 172.3\nTable 7: Language modeling performance on the\nText8 corpus\nModel WS-353 MEN\nLSTM 53.1 46.3\nLSTM-bin 25.5 19.4\nBELM 49.1 47.0\nBLLM 56.0 52.2\nTable 8: Performances on the word similarity tasks\nTable 7 shows our models perform well in lan-\nguage modeling on the Text8 corpus. Table 8\nsummarizes the performance of the word embed-\ndings in the similarity tasks. The embeddings\ngenerated by the simple binarization method per-\nform obviously worse than the other embeddings,\nwhich indicates that much information is lost. The\nBELM model outperforms the baseline model on\nthe MEN task, although it doesnt perform as well\nas the baseline model on the WS-353 task. How-\never, the MEN dataset contains many more word\npairs, which makes the results on this dataset more\nconvincing. The BLLM model signiﬁcantly out-\nperforms the baseline model on the two tasks. The\nresults indicate that the binarized embeddings of\nthe BLLM do not lose any semantic information\nalthough the parameters are represented only by\n-1 and 1.\nWe suspect that binarization plays a role in reg-\nularization and produces more robust vectors. We\nalso give an example visualization of some word\nvectors. The dimension of the embeddings of the\nBLLM is reduced by TSNE ( Maaten and Hinton ,\n2008). The words which are the closest to father\n(according to the cosine distance of word vectors)\nare shown in Figure 1.\nIn this ﬁgure, mother and parents are the clos-\nest words to father, which is quite understand-\nable. The words husband, wife, grandfather\nand grandmother also gather together and most\nwords in the ﬁgure are related to father, indicat-\n2119\n−15 −10 −5 0 5 10\n−15\n−10\n−5\n0\n5\nfathermother\nson\neldest\nparents\nuncle\ngrandfather\nwife\nhusbanddaughter\nmaternal\ncreator\npaternal\nbrother\ngrandson\nheir\nnephew\ngrandmother\nking\nbirthplace\nFigure 1: Visualization of the Binarized Embeddings\ning the embeddings indeed carry semantic infor-\nmation.\n5 Conclusion\nIn this paper, a novel language model, the bina-\nrized embedding language model (BELM) is pro-\nposed to solve the problem that NN based lan-\nguage models occupy tremendous space. For tra-\nditional RNN based language models, the memory\nconsumption mainly comes from the embedding\nlayers (both input and output layers). However,\nwhen the hidden layer size grows, the memory\nconsumption of the RNN module also becomes\nlarger. So, the total memory usage relates to both\nthe vocabulary size and hidden layer size. In the\nBELM model, words are represented in the form\nof binarized vectors, which only contain parame-\nters of -1 or 1. For further compression, we bina-\nrize the long short-term memory language model\ncombined with the binarized embeddings. Thus,\nthe total memory usage can be signiﬁcantly re-\nduced. Experiments are conducted on language\nmodeling and ASR rescoring tasks on various cor-\npora. The results show that the BELM model per-\nforms well without any loss of performances at\ncompression ratios of 2.6 to 11.3, depending on\nthe hidden and vocabulary size. The BLLM model\ncompresses the model parameters almost thirty-\ntwo times with a slight loss of performance. We\nalso evaluate the embeddings on word similarity\ntasks. The results show the binarized embeddings\neven perform much better than the baseline em-\nbeddings.\n6 Future Work\nIn the future, we will study how to improve the\nperformance of the BLLM model. And, we will\nresearch methods to accelerate the training and re-\nduce the memory consumption during training.\nAcknowledgments\nThe corresponding author is Kai Yu. This work\nhas been supported by the National Key Re-\nsearch and Development Program of China under\nGrant No.2017YFB1002102, and the China NSFC\nprojects (No. 61573241). Experiments have been\ncarried out on the PI supercomputer at Shanghai\nJiao Tong University.\nReferences\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research\n3(Feb):1137–1155.\nElia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.\nMultimodal distributional semantics. J. Artif. Intell.\nRes.(JAIR) 49(2014):1–47.\nDi Cao and Kai Yu. 2017. Deep attentive structured\nlanguage model based on lstm. In International\nConference on Intelligent Science and Big Data En-\ngineering. Springer, pages 169–180.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nMatthieu Courbariaux, Yoshua Bengio, and Jean-Pierre\nDavid. 2015. Binaryconnect: Training deep neural\nnetworks with binary weights during propagations.\nIn Advances in Neural Information Processing Sys-\ntems. pages 3123–3131.\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2016. Bina-\nrized neural networks: Training deep neural net-\nworks with weights and activations constrained to+\n1 or-1. arXiv preprint arXiv:1602.02830.\nMarcus Edel and Enrico K ¨oppe. 2016. Binarized-\nblstm-rnn based human activity recognition. In\nIndoor Positioning and Indoor Navigation (IPIN),\n2016 International Conference on. IEEE, pages 1–\n7.\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Eytan\nRuppin. 2001. Placing search in context: The con-\ncept revisited. In Proceedings of the 10th interna-\ntional conference on World Wide Web. ACM, pages\n406–414.\n2120\nSong Han, Huizi Mao, and William J Dally. 2015.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. arXiv preprint arXiv:1510.00149.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation\n9(8):1735–1780.\nLu Hou, Quanming Yao, and James T Kwok. 2016.\nLoss-aware binarization of deep networks. arXiv\npreprint arXiv:1611.01600.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2016. Binarized\nneural networks. In Advances in neural information\nprocessing systems. pages 4107–4115.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. In International\nConference on Machine Learning. pages 448–456.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research9(Nov):2579–2605.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large anno-\ntated corpus of English: the penn treebank. MIT\nPress.\nTomas Mikolov, Martin Karaﬁt, Lukas Burget, Jan\nCernock, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, Conference of the International\nSpeech Communication Association, Makuhari,\nChiba, Japan, September. pages 1045–1048.\nYanmin Qian, Mengxiao Bi, Tian Tan, Kai Yu, Yan-\nmin Qian, Mengxiao Bi, Tian Tan, and Kai Yu.\n2016. Very deep convolutional neural networks for\nnoise robust speech recognition. IEEE/ACM Trans-\nactions on Audio Speech & Language Processing\n24(12):2263–2276.\nMohammad Rastegari, Vicente Ordonez, Joseph Red-\nmon, and Ali Farhadi. 2016. Xnor-net: Imagenet\nclassiﬁcation using binary convolutional neural net-\nworks. In European Conference on Computer Vi-\nsion. Springer, pages 525–542.\nXu Xiang, Yanmin Qian, and Kai Yu. 2017. Binary\ndeep neural networks for speech recognition. Proc.\nInterspeech 2017pages 533–537.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\n2121",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8660892844200134
    },
    {
      "name": "Speech recognition",
      "score": 0.6377900838851929
    },
    {
      "name": "Vocabulary",
      "score": 0.610909640789032
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6061683893203735
    },
    {
      "name": "Embedding",
      "score": 0.584148645401001
    },
    {
      "name": "Language model",
      "score": 0.5691434741020203
    },
    {
      "name": "Natural language processing",
      "score": 0.540005087852478
    },
    {
      "name": "Word (group theory)",
      "score": 0.5300626158714294
    },
    {
      "name": "Binary number",
      "score": 0.5271705389022827
    },
    {
      "name": "Long short term memory",
      "score": 0.41319477558135986
    },
    {
      "name": "Recurrent neural network",
      "score": 0.2048267126083374
    },
    {
      "name": "Arithmetic",
      "score": 0.14708611369132996
    },
    {
      "name": "Artificial neural network",
      "score": 0.13867971301078796
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}