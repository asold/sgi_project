{
  "title": "On the Role of Large Language Models in Crowdsourcing Misinformation Assessment",
  "url": "https://openalex.org/W4399205250",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2717539136",
      "name": "Jiechen Xu",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2009623512",
      "name": "Lei Han",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2122552307",
      "name": "Shazia Sadiq",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A1990425973",
      "name": "Gianluca Demartini",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2717539136",
      "name": "Jiechen Xu",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2009623512",
      "name": "Lei Han",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2122552307",
      "name": "Shazia Sadiq",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A1990425973",
      "name": "Gianluca Demartini",
      "affiliations": [
        "University of Queensland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2765320432",
    "https://openalex.org/W2400269077",
    "https://openalex.org/W6838946122",
    "https://openalex.org/W4224315181",
    "https://openalex.org/W6747635453",
    "https://openalex.org/W2794740256",
    "https://openalex.org/W4296959159",
    "https://openalex.org/W6800326007",
    "https://openalex.org/W4318620371",
    "https://openalex.org/W3160168168",
    "https://openalex.org/W4224989665",
    "https://openalex.org/W4366547816",
    "https://openalex.org/W2800029302",
    "https://openalex.org/W136732505",
    "https://openalex.org/W3016244823",
    "https://openalex.org/W3163939341",
    "https://openalex.org/W1988511622",
    "https://openalex.org/W2912571444",
    "https://openalex.org/W4285043710",
    "https://openalex.org/W3098335998",
    "https://openalex.org/W3201642427",
    "https://openalex.org/W2949397667",
    "https://openalex.org/W4224990314",
    "https://openalex.org/W3173265486",
    "https://openalex.org/W4311559656",
    "https://openalex.org/W3174253067",
    "https://openalex.org/W2128213565",
    "https://openalex.org/W6839316307",
    "https://openalex.org/W6809838524",
    "https://openalex.org/W4313216063",
    "https://openalex.org/W4282045675",
    "https://openalex.org/W4298541566",
    "https://openalex.org/W4312514456",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W3041635673",
    "https://openalex.org/W2783668259",
    "https://openalex.org/W4366003124",
    "https://openalex.org/W4300952844",
    "https://openalex.org/W3205934028",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4380302056",
    "https://openalex.org/W4320858112",
    "https://openalex.org/W4309416490",
    "https://openalex.org/W4283170948",
    "https://openalex.org/W4245383073"
  ],
  "abstract": "The proliferation of online misinformation significantly undermines the credibility of web content. Recently, crowd workers have been successfully employed to assess misinformation to address the limited scalability of professional fact-checkers. An alternative approach to crowdsourcing is the use of large language models (LLMs). These models are however also not perfect. In this paper, we investigate the scenario of crowd workers working in collaboration with LLMs to assess misinformation. We perform a study where we ask crowd workers to judge the truthfulness of statements under different conditions: with and without LLMs labels and explanations. Our results show that crowd workers tend to overestimate truthfulness when exposed to LLM-generated information. Crowd workers are misled by wrong LLM labels, but, on the other hand, their self-reported confidence is lower when they make mistakes due to relying on the LLM. We also observe diverse behaviors among crowd workers when the LLM is presented, indicating that leveraging LLMs can be considered a distinct working strategy.",
  "full_text": "On the Role of Large Language Models in Crowdsourcing Misinformation\nAssessment\nJiechen Xu, Lei Han, Shazia Sadiq, Gianluca Demartini\nThe University of Queensland, Australia\njiechen.xu@uq.net.au, l.han@uq.edu.au, shazia@eecs.uq.edu.au, g.demartini@uq.edu.au\nAbstract\nThe proliferation of online misinformation significantly un-\ndermines the credibility of web content. Recently, crowd\nworkers have been successfully employed to assess misinfor-\nmation to address the limited scalability of professional fact-\ncheckers. An alternative approach to crowdsourcing is the use\nof large language models (LLMs). These models are however\nalso not perfect. In this paper, we investigate the scenario of\ncrowd workers working in collaboration with LLMs to as-\nsess misinformation. We perform a study where we ask crowd\nworkers to judge the truthfulness of statements under differ-\nent conditions: with and without LLMs labels and explana-\ntions. Our results show that crowd workers tend to overesti-\nmate truthfulness when exposed to LLM-generated informa-\ntion. Crowd workers are misled by wrong LLM labels, but, on\nthe other hand, their self-reported confidence is lower when\nthey make mistakes due to relying on the LLM. We also ob-\nserve diverse behaviors among crowd workers when the LLM\nis presented, indicating that leveraging LLMs can be consid-\nered a distinct working strategy.\nIntroduction\nThe spread of misinformation online has been creating in-\ncreasing concern for society. The risk is that organized ef-\nforts to spread misinformation could mislead large parts\nof the population and drive social behavior in desired di-\nrections. There have been different approaches proposed to\ndeal with this growing challenge. For example, researchers\nhave been developing effective methods to automatically de-\ntect online misinformation (Gatto, Basak, and Preum 2023;\nWeinzierl, Hopfer, and Harabagiu 2021; Shu, Wang, and\nLiu 2019). This is used to support expert fact-checkers who\nwould be otherwise overwhelmed by the amount of content\nto be fact-checked. An interesting alternative to using ex-\nperts or ML to deal with misinformation is the use of crowd-\nsourcing. Recent research has looked at the feasibility of\ncrowdsourcing as a way to detect and label misinformation\n(La Barbera et al. 2020; Roitero et al. 2020).\nMore recently, the introduction of Large Language Mod-\nels (LLMs) has triggered a new line of research looking at\nboth (i) the way LLMs can do tasks in place of humans as\nwell as (ii) how human behavior is affected by the interaction\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nwith LLMs. Examples include mathematical formalization\n(Wu et al. 2022), news summarization (Zhang et al. 2023),\nor coding (Xu et al. 2022a). While powerful, existing limi-\ntations of LLMs include the imperfect output generated by\nLLMs also including completely made-up information re-\nsulting from the generative process.\nIn this paper, we look at the impact of LLMs supporting\ncrowd workers assessing the truthfulness of political state-\nments. We are specifically interested in the dimensions of\ntrust and reliance in the LLMs used by crowd workers as\nco-pilots in their judgment tasks. We analyze if and how\nsome crowd workers believe that the LLM provides accu-\nrate judgments (trust) more than others and if and how some\ncrowd workers make use of LLM recommendations (re-\nliance) more than others.\nOur research questions include the following:\n• RQ1: Does presenting LLM-generated outputs (truthful-\nness labels and explanations) have an effect on the quality\nof assessments from crowd workers?\n• RQ2: What is the effect on crowd workers’ confidence\nlevel to their assessments from LLM-generated outputs?\n• RQ3: What is the impact of LLM-generated outputs on\ncrowd workers in terms of the reliance on LLM and the\nsubjective trust in LLM.\n• RQ4: How does LLM-generated output affect crowd\nworkers’ behaviors when they assess misinformation?\nWe observed a strong influence of GPT-3.5 on crowd\nworker decisions. Our findings show that crowd workers\ntend to overestimate truthfulness when exposed to LLM-\ngenerated information.\nRelated Work\nCrowdsourcing for Misinformation Detection\nThe spread of online misinformation has raised concerns\nabout how the public can be informed and make meaning-\nful decisions in a democratic society (Kumar, West, and\nLeskovec 2016). To deal with this issue expert human fact-\ncheckers with a background in journalism follow a foren-\nsic process to debunk misinformation by providing evidence\nand explanations to news readers. However, given the vast\namount of misinformation to be fact-checked, human ex-\nperts face growing challenges. To this end, automated meth-\nProceedings of the Eighteenth International AAAI Conference on Web and Social Media (ICWSM2024)\n1674\nods have been developed (Guo, Schlichtkrull, and Vlachos\n2022).\nTo deal with the challenges of fact-checking and with the\nlimitations of automated methods in terms of accuracy and\npossible bias, crowdsourcing has been considered as an al-\nternative to automated methods given its ability to scale to\nlarge amounts of data. Roitero et al. (2020) looked at how\ncrowd workers compare with expert fact-checkers highlight-\ning a good level of agreement between the two. Later, Roi-\ntero et al. (2021) looked at the longitudinal dimension of\ncrowdsourced truthfulness assessment observing a consis-\ntency in the generated labels. La Barbera et al. (2020) ob-\nserved a political bias in crowd-generated truthfulness la-\nbels. In summary, the key conclusion that can be drawn from\nthis body of work is that the crowd can be a viable alterna-\ntive to experts or to automatically generate misinformation\nlabels with the risk of potential bias.\nAs compared to the existing literature, in this paper, we\nfocus on understanding the impact of text generated by\nLLMs in support of crowd workers assessing the truthful-\nness of political claims. Related to this, there is a study that\nhas looked at the impact of presenting crowd workers assess-\ning truthfulness with external information about how other\nworkers have judged the same claim (Xu et al. 2022b).\nExternal Input in Micro-task Crowdsourcing\nTo have a better understanding of crowd workers and po-\ntentially boost the outcome of crowdsourcing tasks, a num-\nber of studies have been conducted to look at expanding the\ncurrent micro-task design by adding external information to\ncrowd workers. Zhu et al. (2014) embed reviews from peer\ncrowd workers to varied types of crowdsourcing tasks and\naim to optimize the quality of task outcomes. The study im-\nplies that the strategy of including reviews should be contin-\ngent on the level of subjectivity inherent in the task. Simi-\nlarly, Lekschas et al. (2021) employ crowd workers to com-\nment on the works created by artists who are able to con-\ntinuously improve their quality of artworks after viewing\nthe crowd-based comments. Doroudi et al. (2016) conduct\na study towards enhancing the quality of a problem-solving\ntask by showing novice crowd workers with example solu-\ntions crafted by either experts or peer crowd workers. The\nresult indicates new crowd workers can develop effective\ntask-completion strategies from given examples. Hettiachchi\net al. (2021) underscore how utilizing ‘visible gold’ (i.e.,\nquestions that offer feedback to workers as indicators of\naccuracy) can significantly improve the quality of work in\na crowdsourced face detection task. In the context of rele-\nvance assessment, Eickhoff (2018) enrich the task design by\nimplanting assessment results from previous crowd work-\ners. The results indicate a significant bandwagon effect, that\nis, crowd workers tend to exclusively rely on the provided\nanswers to complete the task. Similarly, Xu et al. (2023) in-\nvestigate the effect of human and machine metadata in the\ncrowdsourced relevance assessment. They have identified a\nstrong preference for crowd workers on human metadata and\nshowed the connection between the metadata quality and the\nassessment quality. Duan, Ho, and Yin (2022) apply a sim-\nilar approach that adding information either generated by\nhumans or machines to a recidivism risk evaluation task to\nexplore the impact of changing task design with respect to\ncrowd workers’ performance. Heuer and Glassman (2022)\nprovide online fact-checkers with a checklist that aims to as-\nsist fact-checkers. They find that the usefulness of the check-\nlist is non-trivial. This body of literature has comprehen-\nsively investigated the impact of such external information\non crowd workers. Our work aims to expand existing re-\nsearch by involving LLMs as the source of information.\nHuman-LLM Interaction\nLLMs have recently received a lot of attention from the re-\nsearch community as well as from the general public. With\nthe term LLM, we typically refer to transformer models pre-\ntrained with large amounts of documents from which tokens\nare extracted in which the decoder is used to generate nat-\nural language text. Popular examples of LLMs include gen-\nerative pre-trained transformer (GPT) models (Brown et al.\n2020). While significant work has been conducted to under-\nstand the effectiveness of these models on a large number of\nhuman tasks (Zhang et al. 2023), in this work, we instead\nfocus on how these LLMs may impact human decision-\nmaking processes when human decision-makers are exposed\nto LLM output. Related to the topic of human-AI interac-\ntion, recent work as looked at how human decision-makers\nrely on AI. For example, Jahanbakhsh et al. (2023) explore\nthe use of a personalized AI that learns from individual user\nassessments and predicts how the same user would assess\nother content for identifying misinformation on social me-\ndia. A user study shows that the AI’s predictions influence\nusers’ judgment, but this influence can be reduced when\nusers provide reasoning for their assessment. Epstein et al.\n(2022) look at the effectiveness of AI-generated warnings\non social media platforms to combat misinformation. They\nfind that providing explanations for the warnings can in-\ncrease users’ discernment, but the effectiveness can be un-\ndermined by various factors (e.g., user attention and educa-\ntion level). Vasconcelos et al. (2023) investigate the effect\nof AI-generated explanations to reduce users’ over-reliance\non AI and show the scenarios that explanations can mitigate\nover-reliance.\nMethodology\nDataset\nIn this study, we leverage a dataset containing over 12,000\npolitical statements from a well-known political fact-\nchecking website: PolitiFact (Wang 2017). Statements in\nthis dataset are mainly made by US politicians belonging\nto either the Democrat Party or the Republican Party, evalu-\nated for truthfulness by PolitiFact’s editors using a six-level\nscale: (0) pants-on-fire, (1) false, (2) barely-true, (3) half-\ntrue, (4) mostly-true and (5) true. From this dataset, we then\nselect a subset of 120 statements that are balanced in terms\nof the two main parties (60∗2) and truthfulness level (20∗6).\nThis subset of statements has also been used by related work\nthat looked at crowdsourcing misinformation assessments\n(Roitero et al. 2020; Draws et al. 2022; Xu et al. 2022b).\n1675\nFigure 1: Task interface for the condition Label+Exp.\nTo investigate the impact of LLMs on crowdsourcing mis-\ninformation assessment, we utilize the GPT-3.5 model de-\nveloped by OpenAI (Brown et al. 2020). Specifically, we\nemploy the ‘text-davinci-003‘ variant of the model to evalu-\nate the truthfulness of statements from thePolitiFactdataset.\nVia prompting the model, we explicitly request two types of\noutput from GPT-3.5 for each statement: (i) a truthfulness\nlabel (from 0 to 5, same as PolifiFact) and (ii) a short nat-\nural language explanation to justify the decision in a fixed\nformat. Note that the explanations may not directly reveal\nthe corresponding truthfulness label produced by the model.\nWe set parameters ‘temperature‘ and ‘top-p‘ to 0 and 1.0 to\ncontrol GPT-3.5 to generate consistent responses. Here is an\nexample of GPT-3.5-generated results:\nStatement: The War in Afghanistan is officially the\nlongest war Americans have ever been asked to endure.\nSpeaker: Dennis Kucinich\nYear: 2010\nTruthfulness label: 5\nExplanation: The War in Afghanistan began in 2001\nand is still ongoing, making it the longest war in US\nhistory.\nBy merging the LLM-generated labels and editorial labels\ninto binary (mapping PolitiFact labels 0-2: true, 3-5: false),\nwe observed a model accuracy rate of approximately 0.68.\nThis level of effectiveness is comparable to a recent study\nconducted by Hoes, Altay, and Bermeo (2023).\nExperimental Conditions\nWe run an experiment using a 2 × 2 factorial design based\non the presence of two kinds of LLM-generated data (i.e.,\ntruthfulness label and explanation). Hence, our experiment\ncontains 4 conditions:\n• Baseline. In this condition, we do not provide any kind\nof LLM-generated answer, which serves as the baseline\nin our study and is comparable to the design used in the\nliterature on crowdsourced misinformation assessments.\n• Label. In this condition, crowd assessors are exposed\nto LLM-generated truthfulness labels, presented on the\nsame scale as the labels they are required to provide.\n• Explanation. This condition contains solely the natural\nlanguage explanations generated by GPT-3.5.\n• Label+Exp. For this condition, we show both labels and\nexplanations to crowd workers.\nCrowdsourcing Task Design\nWe divide the selected 120 political statements into 20\ntasks/units each of them containing 6 statements. To ensure\nall units are balanced in terms of truthfulness level and polit-\nical parties to mitigate the cognitive bias of crowd workers\n(La Barbera et al. 2020; Draws et al. 2022), we merge the\noriginal 6-level truthfulness scale into a 3-level one (true,\nin between and false) based on the observations made by\nRoitero et al. (2020). The set of labels given by PolitiFact\neditors is considered the ground truth in this study. In ad-\ndition, we compose each unit using 4 statements that are\ncorrectly labeled by GPT-3.5, while 2 statements were in-\ncorrectly labeled to mimic the actual GPT-3.5’s accuracy on\nour dataset. The order of the 6 statements is then shuffled\nfor every crowd worker and every task to remove potential\nlearning effects (He, Kuiper, and Gadiraju 2023).\nCrowd assessors are recruited from the online crowd-\nsourcing platform Prolific, which has been shown as an ef-\nfective choice for running complex human assessment stud-\nies (Xu, Zhou, and Gadiraju 2020). To conduct this study,\nwe recruit participants who are physically located in the\nUSA and use English as their first language. Additionally,\npotential participants are required to meet certain criteria,\nincluding an approval rate of at least 80% and a minimum\ncompletion of 10 prior tasks on the platform. These criteria\nare regarded as a method to obtain reliable crowd assessors.\nCrowd workers will be assigned randomly to one task unit\nin one of the four experimental conditions, and participat-\ning multiple times is not allowed. Prior to starting the task,\nparticipants were required to review an information sheet ex-\nplaining the data collection process and the intended use of\nthe collected data and consent to it1. Then, participants will\nbe redirected to an external page hosted by the authors’ insti-\ntution to continue the task. Figure 1 shows the task interface\nfor one of the experimental conditions, which comprises the\nfollowing elements on the left-hand side: (a) an instruction\nbutton, (b) the statement information including speaker and\nyear, (c) GPT-3.5’s output (varies across conditions), (d) ra-\ndio buttons for the workers to indicate their confidence levels\nand (e) a text box for an optional justification text input. On\nthe right-hand side, we provide a customized search engine2\nthat allows crowd assessors to find evidence to support their\nassessments (Roitero et al. 2020). A search result consists\nof the title of the webpage, the URL, and a snippet of text\n1This study has been approved by the authors’ institution IRB.\n2The search engine leverages Bing APIs and does not return\nany result from the PolitiFact website as crowd assessors may oth-\nerwise be able to directly retrieve the ground truth.\n1676\nfrom the web page which may contain the query terms. Note\nthat the initial state of the task interface does not include any\nsearch result or any search query placeholder.\nIn addition to assessing the 6 political statements, par-\nticipants are requested to complete a pre-task questionnaire\n(ATI, TIA-PtT) and an exit questionnaire (TIA-Trust). To\nprevent crowd workers from submitting random answers, we\nembed attention-check questions throughout the task ses-\nsion. Submissions from workers who failed to answer any\nattention-check question are not used in data analysis. We\ncompensate participants\nC1.2 per task based on the average\ntask completion time measured during a pilot study and the\nProlific recommended hourly rate.\nMeasures\nMeasuring Assessment Quality. To address RQ1, we\nleverage various metrics to evaluate the quality of labels\ngiven by crowd workers compared to the labels from Poli-\ntiFact’s editors (ground truth). First, we consider two kinds\nof errors between crowd workers and the editors: Eedit and\nAEedit. Eedit represents the extent of disparity (ranges in\n[−2, 2]) between labels provided by a crowd worker and\nan editor for a given statement, indicating the presence of\noverestimation (positive values) or underestimation (nega-\ntive values) tendencies. AEedit quantifies the magnitude of\nannotation bias by calculating the absolute error between\nlabels assigned by crowd workers and editors, with val-\nues ranging from 0 to 2. This set of error-based evalua-\ntion was also used by Draws et al. (2022). We also con-\nsider to evaluate the inter-rater agreement of collected labels\n(Checco et al. 2017). Here, we apply two types of agree-\nment: (i) external agreement, which measures the agreement\nbetween labels from crowd workers and the ground truth,\nand (ii) internal agreement, which assesses the agreement\namong crowd workers working on the same set of state-\nments. Specifically, we utilize Krippendorff’s α (Krippen-\ndorff 2011) to calculate both types of agreement which has\nvalue range from -1 (completely disagree) to 0 (random) to\n1 (completely agree).\nMeasuring Reliance and Trust. Reliance is measured by\nanalyzing the alignment of labels generated by the LLM and\ncrowd workers. With a similar idea to measuring quality, we\nembed error-based metrics including ELLM and AELLM\nwhich compute the error and absolute error between crowd\nworkers and LLM, respectively. In addition, we applyAgree-\nment Fraction (He, Kuiper, and Gadiraju 2023), which cal-\nculates the rate of crowd workers’ decisions that agree with\nthe LLM’s advice. Following previous research (Tolmeijer\net al. 2022), we consider two validated questionnaires us-\ning Likert scales to measure the subjective trust in LLM:\nTrust in Automation (TiA) (K ¨orber 2019) and Affinity for\nTechnology Interaction Scale(ATI) (Franke, Attig, and Wes-\nsel 2019). For TIA, we apply two sub-scales related to our\nstudy: Propensity to Trust (TiA-PtT), Trust in Automation\n(TiA-Trust).\nConfidence and Behavioral Indicators. In our study, we\napply self-reported confidence as the metric of confidence\nCompleted Abandoned Failed\nBaseline 60 (58.8%) 30 (29.4%) 12 (11.8%)\nLabel 60 (53.6%) 45 (40.2%) 7 (6.2%)\nExplanation 60 (51.3%) 44 (37.6%) 13 (11.1%)\nLabel+Exp 60 (56.6%) 33 (31.1%) 13 (12.3%)\nTable 1: Crowd worker participation rates.\nof crowd workers in their assessments, using a 5-level Lik-\nert scale following prior research (Qu et al. 2022) to address\nRQ3. To study behavioral aspects (RQ4), we consider the\ntime usage and the interaction with the search engine as be-\nhavioral indicators in this work. Given the nature of the mis-\ninformation assessing task, making use of a search engine\ncan be regarded as the proxy to measure workers’ efforts\n(Roitero et al. 2020). We particularly examine the Number\nof Queries as the evidence of the extent to which workers\nactively engage with the search engine.\nResults\nDemographics\nIn this study, we recruited 437 crowd workers through the\nProlific platform. Table 1 illustrates the number of workers\nwho completed the task, abandoned the task, and failed the\nattention check questions across experimental conditions.\nOut of all the participants, 46% identified as female, while\n51% identified as male. The remaining participants chose\nnot to disclose their gender identity. The average age of the\nparticipants is found to be 36 years, with a standard devi-\nation of 13, indicating a diverse range of age groups. We\nfound that most workers were well-educated since 63% of\nthem claim that they acquired a 4-year college degree or\nabove. To ensure that demographic variables (e.g., age and\neducation level) have no effect to the dependent variables\n(e.g., MEedit) in our study, we leverage Spearman corre-\nlation tests and Pearson correlation tests to relate demo-\ngraphic variables to the dependent variables included in re-\nsults section. All tests resulted in p >0.05, which indicates\nwe can ignore demographic variables in our analysis.\nRQ1: Quality of Assessments\nNext, we report about the effects of LLM-generated answers\non crowd worker assessment quality. To test, we propose the\nfollowing Null Hypothesis for this section:\nNull Hypothesis 1 LLM-generated answers (i.e., la-\nbels and explanations) do not have an effect on assess-\nment quality.\nOverestimated Truthfulness Impacted by LLM. First,\nwe consider the crowd workers’ judgment mean error as\ncompared to experts MEedit, which looks at the mean dif-\nference between labels from crowd workers labels and Poli-\ntiFact’s editors. Figure 2a shows the distribution of MEedit\nover four experimental conditions. We notice that the me-\ndian value for condition Baseline is around 0 while other\nthree conditions have higher median values. To examine\n1677\n(a)\n (b)\nFigure 2: MEedit and MAEedit over conditions.\nCondition % Over % Under Accuracy\nBaseline 27.50 30.83 41.67\nLabel 38.06 18.61 43.33\nExplanation 36.11 25.28 38.61\nLabel+Exp 37.78 20.56 41.67\nTable 2: Percentages of assessments for overestimation, un-\nderestimation and accuracy across conditions.\nthe effect of showing labels and/or explanations generated\nby GPT-3.5, an aligned ranks transformation ANOVA (ART\nANOV A) (Wobbrock et al. 2011) is applied due to the data\nin Fig. 2a being not normally distributed. The ART ANOV A\nis a non-parametric test designed to analyze multiple inde-\npendent variables and their interactions, offering flexibility\nin capturing complex relationships and measures within the\ndata. The result of ART ANOV A shows a significant inter-\naction effect between the two factors (F = 10.8796, p=\n0.0011, partial η2 = 0.0440). A post-hoc pair-wise compar-\nison using Tukey’s HSD adjustment shows condition Base-\nline has significantly lower MEedit compared to conditions\nhaving LLM-generated information (p < .01 for Label, Ex-\nplanation and Label+Exp). Within conditions Label, Expla-\nnation and Label+Explanation, we do not find any of them\nhaving MEedit significantly different to others, while we\nobserve that Label condition has a higher median as shown\nin Fig. 2a. Table 2 shows the distribution of assessments be-\ning overestimated, underestimated, and accurate across dif-\nferent conditions. It is evident that crowd workers tend to\noverestimate truthfulness when exposed to LLM-generated\ninformation and hence, Null Hypothesis 1 should be re-\njected. Conversely, workers who do not have an access to\nGPT-3.5’s assistance demonstrate a higher rate of underes-\ntimation errors. In term of accuracy, the values of Baseline,\nLabel and Label+Exp are comparable, while condition Ex-\nplanation achieves the lowest value due to an increase in un-\nFigure 3: Confusion matrix for numbers of assessments by\nGPT-3.5 against the ground truth labels. Labels for row and\ncolumn are ground truth labels and GPT-3.5’s labels, respec-\ntively. Notation: 0 – false, 1 – in-between, 2 – true.\nderestimation errors.\nTo have a better understanding on the reason why crowd\nworkers tend to overestimate the truthfulness, we report\nthe judging performance of GPT-3.5 on the same state-\nment corpus. We calculate the mean error between GPT-\n3.5 and the ground truth (E LLM\nedit), which results in\nM ± SD(ELLM edit) = 0.5 ± 0.22. This observation in-\ndicates that GPT-3.5 tends to overestimate truthfulness la-\nbels, which is aligned with the recent results in the literature\nshowing how LLMs are more effective when assessing true\nstatements than false ones (Hoes, Altay, and Bermeo 2023).\nFigure 3 shows presents the number of labels given by GPT-\n3.5 for different truthfulness levels in the ground truth. From\nthe top left to the bottom right, the three cells on the diago-\nnal represents the labels that GPT-3.5 is aligned with ground\ntruth. Simultaneously, the upper section of the matrix dis-\n1678\nExternal Internal\nConditions M SD M SD\nBaseline 0.24 0.38 -0.05 0.39\nLabel 0.27 0.35 -0.00 0.40\nExplanation 0.14 0.38 -0.04 0.37\nLabel+Exp 0.27 0.31 -0.04 0.39\nTable 3: Mean and standard deviation of Krippendorff’s α\nover conditions.\nExternal Internal\nVariables F p F p\nLabel 3.22 0.07 0.20 0.65\nExplanation 1.13 0.29 0.06 0.80\nLabel×Exp 1.01 0.31 0.29 0.59\nTable 4: ANOV A results of Krippendorff’s α for external\nagreement and internal agreement over conditions.\nplays the count of judgments made by GPT-3.5 that have\noverestimated truthfulness, while the lower section contains\njudgments with underestimations of truthfulness. It is evi-\ndent that GPT-3.5 tends to exhibit a bias towards overes-\ntimating the truthfulness of this set of political statements,\nwhich subsequently appears to have influenced the crowd\nworkers’ assessments. These findings explains why crowd\nworkers who are exposed to LLMs data make more overes-\ntimation errors.\nFigure 2b shows the the distributions of MAEedit across\nconditions. By running ART ANOV A, neither interaction ef-\nfect nor main effect for each factor is identified (p= 0.6172\nfor label×explanation, p = 0.062 and p = 0.078 for label\nand explanation, respectively). Crowd workers who are ex-\nposed to LLM-generated answers make more overestimation\nerrors. However, it is counter intuitive that no similar obser-\nvation can be found via analyzing MAEedit. This indicates\nhow the level of annotation bias among all experimental con-\nditions are comparable. A possible explanation is that crowd\nworkers in Baseline are making more underestimation errors\n(see Tab. 2), which generates a similar bias level compared\nto other conditions.\nInternal and External Agreement. We now turn to an-\nalyze crowd workers’ assessment quality by computing in-\nternal and external agreement. We use Krippendorff’s α to\nmeasure agreement. The results are summarized in Table 3.\nIn terms of external agreement, we can observe that Label\nand Label+Exp lead to best judgment quality with respect\nto mean α and, condition Explanation leads to lowest qual-\nity. After conducting tests for normality and homogeneity of\nvariances, we apply two-way ANOVAon external agreement\nand internal agreement. The results for ANOV A are shown\nin Table 4. In summary, we have not found any significant\neffect from LLM-generated labels and explanations in terms\nof external agreement among these four conditions. As an\nimportant measure for the quality of labels acquired from\ncrowd workers (Checco et al. 2017), our findings with re-\ngard to external agreement show that being exposed to data\nfrom a LLM does not have an impact on crowd workers’\njudgment qualities . On the other hand, the analysis in the\nprior section indicates a gap between the quality of labels\ncollected in the Baseline condition and the LLM-supported\nconditions. This contradiction can also be the result of more\nunderestimation errors appearing in the Baseline condition.\nOn the other hand, we notice that the mean Krippendorff’s\nα for internal agreement is fairly low across all experimen-\ntal conditions. This result is consistent with previous re-\nsearch conducted on crowdsourcing misinformation assess-\nment (Roitero et al. 2020; Xu, Zhou, and Gadiraju 2020),\neven though we use a different crowdsourcing platform. We\nalso did not identify any condition that is statistically signif-\nicantly different to others based on internal agreement. This\nindicates that providing answers from a LLM does not im-\nprove the consistency of labels generated by crowd workers.\nRQ2: Self-reported Confidence\nNext, we explore the impact of LLM-generated answers on\ncrowd workers’ self-reported confidence in their judgment.\nTo this end, we test the following Null Hypothesis:\nNull Hypothesis 2 LLM-generated answers do not\nhave an effect on self-reported confidence levels.\nFor each statement, we ask participants to choose their level\nof confidence on a Likert scale from 0 (completely unconfi-\ndent) to 4 (completely confident). To this end, we compute\nthe average confidence level for each crowd worker. Again,\nwe utilize ART ANOV A to examine potential significant dif-\nferences among the four experimental conditions. The result\nshows that neither the interaction effect (F = 0.0259, p=\n0.8722, partial η2 = 0.0011) nor the main effect for two\nindependent variables (label: F = 0.0060, p= 0.9381, par-\ntial η2 = 0.0002; explanation: F = 3.4619, p= 0.0640,\npartial η2 = 0.0145) are significant, inferring that neither\nkind of LLM’s answer has an effect on of crowd workers’\nself-reported confidence levels. Given this result, Null Hy-\npothesis 2 can be accepted. In the following, we consider the\nrelation between assessment quality and self-reported confi-\ndence level. Here, we utilize external agreement as the met-\nric for evaluating assessment quality. Figure 4 shows the lin-\near regressions conducted for each of the four experimen-\ntal conditions. We observe a positive correlation between\nexternal agreement and mean confidence (β = 0.8425,\nt(58) = 2.708, p = 0.009) in condition Label+Exp. A fol-\nlow up analysis of variance (ANOV A) shows that external\nagreement has a statistically significant effect on mean con-\nfidence (F (1, 58) = 7.331, p <0.01). The regression sug-\ngests that under Label+Exp condition, crowd workers ex-\nhibit higher levels of confidence when they have answers\nthat are more aligned with the ground truth. This shows how\ndata generated by the LLM may reinforce pre-existing be-\nlieves in the human subjects participating to our study.\nRQ3: Reliance and Trust\nNext, we investigate the extent to which crowd workers rely\nand trust GPT-3.5’s answers in different conditions. To this\nend, we propose two Null Hypotheses to test:\n1679\nFigure 4: Linear regressions of external agreement and mean confidence among experiment conditions. From left to right:\nBaseline, Label+Exp, Explanation and Label.\nNull Hypothesis 3 The presence of LLM-generated\nanswers does not affect the level of reliance of crowd\nworkers on the LLM.\nNull Hypothesis 4 The presence of LLM-generated\nanswers does not impact the trust that crowd workers\nhave in the LLM.\nReliance on LLM’s Advice. We first look at the relation-\nship between labels provided by crowd workers and GPT-3.5\nfor which we start with computing MELLM (i.e., mean er-\nror between crowd workers and GPT-3.5), and MAELLM\n(i.e., mean absolute error between crowd workers and GPT-\n3.5). Then, we also measure the reliance of crowd workers\non the LLM using Agreement Fraction.\nFigure 5a shows crowd workers’MELLM across four ex-\nperimental conditions. Note that condition Baseline is pre-\nsented in Fig. 5a for comparison purposes only as its inter-\nface does not include any information generated by GPT-\n3.5. This situation also applies to other measures reported in\nthis section. By looking at the value distributions in Fig 5a,\nwe can observe how the majority of crowd workers exhibit\na tendency to assign lower truthfulness labels relative to\nthe GPT-3.5’s assessments (e.g., assigning labels of 0 (i.e.,\nfalse) or 1 (i.e., in between) while GPT-3.5 assigning a la-\nbel of 2 (i.e., true)). We conduct a ART ANOV A to inves-\ntigate the effects on MELLM and, the result reveals a sig-\nnificant interaction effect (F = 7.5526, p= 0.0065, partial\nη2 = 0.3010). The post-hoc pair-wise test (Tukey’s HSD)\nshows that condition Baseline significantly differs form the\nrest of conditions in terms of MELLM (p <0.01 for Label\nand Label+Exp, p <0.05 for Explanation).\nFigure 5b shows the distribution of MAELLM over con-\nditions. Without considering the direction of differences be-\ntween labels from crowd workers and GPT-3.5, crowd work-\ners achieve smaller mean absolute errors in conditions pre-\nsenting GPT-3.5’s responses. The same set of statistical\ntests identify a significant interaction effect on MAELLM\n(F = 10.3143, p= 0.0015, partial η2 = 0.0419) and then,\ncondition Baseline is significantly different from other con-\nditions (p < 0.01 for all comparisons). We also have dis-\ncovered significant differences between condition Explana-\ntion and the other two conditions containing LLM-generated\ndata (p < 0.01 for Label, p < 0.05 for Label+Exp). Fig-\nure 5c shows the distributions of Agreement Fraction scores\nacross conditions. Form this we can draw similar observa-\ntions as from Fig. 5b in an opposite direction: Crowd work-\ners obtain higher Agreement Fraction in conditions in which\ncrowd workers have access to LLM’s advice. Condition Ex-\nplanation reaches significantly lower values of Agreement\nFraction as compared to Label and Label+Exp (verified by\nART ANOV A and post-hoc Tukey’s HSD test).\nThese observations indicate a strong influence of GPT-\n3.5 on crowd worker decisions, which rejects Null Hypoth-\nesis 3. When either LLM-generated labels or explanations\nare available, labels generated by crowd workers are more\naligned with those of the LLM indicating a strong level of\nreliance. In other words, crowd worker judging behaviour\nexhibits a bandwagon effect when GPT-3.5 comes in to as-\nsist. We also notice that the bandwagon effect is present in\ncondition Explanation although it is not as evident as in con-\ndition Label or Label+Exp, indicating that this effect may be\nmitigated by not explicitly revealing the truthfulness labels\ngenerated by the model.\nTrust in the LLM. During the task, participants are asked\nto complete questionnaires which are used as the mea-\nsurement of subjective trust in the LLM. For the post-\nquestionnaire (TiA-Trust), the questions are rephrased (e.g.,\nfrom ‘I trust the system.’ to ‘I believe the AI is judging the\ntruthfulness of statements correctly.’) to highlight to crowd\nworkers that they are reporting about their trust in the LLM.\nNote that for condition Baseline, we do not ask workers to\nanswer the trust questionnaires. As trust data is not avail-\nable for the Baseline condition and one of the remaining\ndistributions lacks normality, we apply the Kruskal-Wallis\nH Test to examine the effect of showing LLM-generated\nanswers on trust scores. The obtained result reveals that\nthere is no significant difference among the three conditions\n1680\n(a)\n (b)\n (c)\nFigure 5: (a) MELLM , (b) MAELLM and (c) Agreement Fraction over experiment conditions.\n(H = 0.3190, p= 0.8526), implying that the variations in\nLLM-generated answers do not significantly influence the\nlevel of trust of crowd workers towards the AI system. This\nindicates that Null Hypothesis 4 is acceptable. With the ob-\nservation that self-reported confidence has a positive corre-\nlation with external agreement for condition Label+Exp, we\nare motivated to explore whether trust scores have a correla-\ntion with confidence scores. Based on the analysis of regres-\nsion models we can confirm a positive correlation between\nself-reported confidence scores and trust scores (ANOV A,\nF(1, 58) = 5.4020, p = 0.0237) only in condition Explana-\ntion. By exclusively presenting crowd workers with natural\nlanguage explanations generated by the LLM, we observe\nan increase in their confidence levels when they report trust-\ning the generative AI system. However, we do not observe\nany other correlation between trust scores and other mea-\nsurements (e.g., external agreement).\nPrior work by Tolmeijer et al. (2021) looking at trust for-\nmation in the context of AI used as an assistant to complete\ncertain tasks (e.g., searching for suitable apartments) claims\nthat the first impression is vital in terms of building trust\nin the system. Keeping this in mind, we investigate whether\nit is the case in our study and whether the first impression\nhas the same effect across our experimental conditions. As\nmentioned in Methodology, the order of the six statements\nare randomized for every crowd worker. Hence, we divide\nour participants into two groups: Good Impression (GI, the\nLLM’s judgment is correct on the first statement) and Bad\nImpression (BI, the LLM’s judgment is incorrect on the first\nstatement). The number of workers belonging to group BI\nare 32 (Label), 35 (Explanation), 33 (Label+Exp). As shown\nin Figure 6, we look at the mean trust scores for the two\ngroups with a breakdown over experimental conditions. We\nconduct a Mann Whitney U test to determine differences be-\ntween GI and BI in each condition. We find that the mean\ntrust scores for GI in condition Explanation is significantly\nhigher than that for the BI group (p = 0.0186), indicating\nthat crowd workers who have a good first impression report\nhigher trust in the LLM. In contrast, we do not identify sim-\nilar patterns in conditions Label and Label+Exp. This sug-\ngests that exclusively showing LLM-generated explanations\nhas a stronger effect on crowd workers’ ‘first impression’.\nFigure 6: Distributions of mean trust score for theBI and GI\ngroups over conditions.\nOne possible explanation is that crowd workers have the\nability to validate LLM-generated explanations using their\nown knowledge or by using the customized search engine\nembedded in the task. As a result, they may choose to trust\nor disregard the LLM explanations based on their perceived\nstatement credibility.\nRQ4: Behavioral Indicators\nNext we report on how LLM-generated answers impact\nworkers’ judging behaviors by looking at two indicators: (i)\nutilization of the search engine and (ii) assessment time. We\npropose two Null Hypotheses to test:\nNull Hypothesis 5 LLM-generated answers do not\nhave an effect on the way that crowd workers interact\nwith provided search engine.\nNull Hypothesis 6 LLM-generated answers do not\nhave an effect on assessment time.\nUtilization of the Search Engine. We first explore the in-\nteraction between crowd workers and the customized search\nengine provided with the assessment task and look at the im-\npact of this interaction on their behaviour. Specifically, we\n1681\nFigure 7: Average number of queries over experimental con-\nditions.\nfocus on the search queries made by crowd workers. Prior\nstudies showed that making use of a search engine can be\nconsidered as an indicator of putting more effort in the task\n(Roitero et al. 2020).\nWe first look at the Number of Queries (Nq ) through-\nout the task. Figure 7 shows the distribution of the num-\nber of queries issued by crowd workers across the four ex-\nperimental conditions. Due to the non-normal distributions,\nwe continue to apply ART ANOV A to investigate the ef-\nfects caused by the presence of LLM-generated answers.\nThe result suggests a significant interaction effect on\nNq\n(F = 6.5817, p= 0.0109, partial η2 = 0.0271). Sub-\nsequently, a pair-wise comparison (Tukey’s HSD) reveals\nthat crowd workers in condition Baseline issue significantly\nmore queries as compared to workers in other conditions\n(p < 0.01 for all pairs). This can be a signal that crowd\nworkers are taking the strategy of referring to the answers\n(labels and/or explanations) from GPT-3.5 to reduce the ef-\nforts towards retrieving relevant evidence to support their\njudgments (i.e., utilizing the search engine). Then, we take\nthe median value of Nq for each condition as the pivot\nand, divide crowd workers into two groups: Search-Active\nGroup (SA) and Search-Light Group (SL). By doing this,\nwe are able to investigate the differences in performance be-\ntween these two groups. We find that, for SA participants,\nthere is no significant difference among the four experi-\nmental conditions in terms of MELLM (Kruskal-Wallis H\nTest, H = 6.8693, p= 0.0762), whereas in the condition\nBaseline SA participants have lower MELLM compared to\nthose in the condition Label (Kruskal-Wallis H Test, H =\n11.6941, p= 0.0085, post-hoc Dunn’s Testp = 0.0030) and\nLabel+Exp (post-hoc Dunn’s Test p = 0.0463). As we have\nshown that labels in condition Baseline are significantly less\naligned with GPT-3.5’s labels as compared to the other con-\nditions. Hence, we reckon that crowd workers are able to\nmitigate the bias from the LLM by leveraging the search en-\ngine results (i.e., group SA).\nFigure 8: Average task completion time (s) across experi-\nmental conditions.\nAssessment Time. Next, we focus on how much time\ncrowd workers spent on the assessment task under differ-\nent experimental conditions. Here, we utilize the average\ntime taken across six statements as the metric to measure\ncrowd worker effort. Figure 8 shows the average time used\nover experimental conditions. The median time spent under\ncondition Baseline is higher than that in the other three con-\nditions. We perform ART ANOV A to analyze the effects of\nLLM-generated labels and explanations towards task time.\nThe interaction effect between the two factors is significant\n(F = 5.5908, p= 0.0189, partial η2 = 0.0239). After con-\nducting a post-hoc pair-wise comparison with Tukey’s HSD\nadjustment, we observe that crowd workers in the Base-\nline condition spend significantly more time as compared\nto those in the Label condition (p = 0.0371). Considering\nthe Label condition where only generated labels are avail-\nable and the tendency of crowd workers to rely heavily on\nthose labels, it is clear why they take less time to complete\nthe task. On the other hand, crowd workers in Label condi-\ntion are producing labels of comparable quality to those in\nthe Baseline condition when looking at external agreement\nmeasures. This implies that solely providing LLM-generated\nlabels can be an effective method to speed up crowdsourcing\nof misinformation assessment.\nDiscussion\nRQ1: Quality. One notable observation regarding assess-\nment quality is the significant overestimation errors made\nby crowd workers when LLM-generated information is pre-\nsented. On the other hand, Crowd workers who are exposed\nto LLM-generated answers have fewer underestimation er-\nrors than those in the baseline condition. Given the similar\ntendency of LLM-generated labels leaning towards overes-\ntimating compared to the ground truth in the task corpus\nused in this study, it can be inferred that crowd workers’\ndecisions are influenced by the information provided by the\nLLM. Although other studies addressing crowdsourced mis-\ninformation judgments have also reported the native overes-\ntimation errors made by crowd workers (Maddalena, Ceolin,\nand Mizzaro 2018; Roitero et al. 2020), our finding indi-\n1682\ncates the impact of LLMs by looking at the judging quality\nacross LLM-available (i.e. ‘Label+Exp’, ‘Label’ and ‘Ex-\nplanation’) and no-LLM (i.e., ‘Baseline’) conditions. We\nalso investigate judgment quality with respect to external\nand internal agreement. Statistical tests show that the judg-\nment quality does not vary across conditions in terms of both\ntypes of agreement. Furthermore, we have observed similar\nlevels of accuracy over the four conditions. In other words,\ncrowd workers are mimicking the same overestimation er-\nrors done by the LLM causing their judgment quality to be\nnot ideal for true statements. On the other hand, in the ab-\nsence of support from the LLM, crowd workers make more\nunderestimation errors and in turn, reach a similar level of\njudgment quality as LLM-supported workers. This shows\nhow being exposed to LLM answers changes the labeling\npattern of crowd workers.\nRQ2: Confidence. Our analysis shows that the impact of\nanswers from the LLM is minimal towards self-reported\nconfidence level, as indicated by the similar distributions of\nconfidence scores across the different experimental condi-\ntions. A linear regression shows a positive correlation be-\ntween mean confidence and external agreement in the con-\ndition where both types of LLM outputs are presented. This\nsuggests that in this particular condition, the LLM’s outputs\nmay reinforce pre-existing beliefs, leading to higher levels\nof confidence among crowd workers.\nRQ3: Reliance and Trust. We investigate the extent to\nwhich crowd workers rely on the LLM. It is evident that\ncrowd workers’ labels have higher agreement with the labels\ngiven by the LLM, indicating a strong influence of the LLM.\nThis influence can be classified as a bandwagon effect, sim-\nilar to what has been observed in other crowdsourcing tasks\ninvolving advice from humans (Eickhoff 2018). This effect\ncan be mitigated by concealing the truthfulness labels, as\nobserved in the condition Explanation.\nSolely showing LLM-generated explanations can also en-\nhance confidence. Exclusively presenting explanations also\ntriggers the effect of the ‘first impression’, which means that\ncrowd workers express more trust in the model when the\nmodel initially provides correct information. This confirms\nthe presence of reliance on LLM and the importance of LLM\nbeing accurate before being deployed as a co-pilot with hu-\nman annotators.\nRQ4: Behavior. To address RQ4, we investigated two be-\nhavioral indicators: the utilization of the custom Search En-\ngine and task completion time. We observe a significant de-\ncrease in the use of the search engine when crowd workers\nare assisted by the LLM, suggesting that their working strat-\negy relies on LLM answers to reduce the efforts they need\nto invest to complete the task. One piece of evidence that\nconfirms this is that crowd workers in the condition Label\nspend significantly less time compared to those in the con-\ndition Baseline. We also notice that crowd workers who ac-\ntively utilize the search engine exhibit lower reliance on the\nanswers generated by the LLM. This suggests that in cases\nwhere the requester can disregard the potential over-reliance\neffect (Vasconcelos et al. 2023), they can consider display-\ning labels generated by an AI in order to expedite the pro-\ncess of manual misinformation assessment. These observa-\ntions confirm previous results that show how crowd workers\nmake use of additional tools and metadata to improve their\nwork efficiency and, consequently, their hourly wage.\nImplications\nOur results have direct implications on how to embed LLM-\ngenerated answers in crowdsourced misinformation assess-\nment tasks. Showing LLM-generated answers can be a\ndouble-edged sword. The presence of such answers does\nenhance the efficiency of crowd assessors, offering benefits\nto their hourly wage. However, this efficiency improvement\nis accompanied by possibly biased assessments as crowd\nworkers tend to excessively rely on the AI’s judgment. A\npotential approach to mitigate this over-reliance is to include\nexplanations only, without the accompanying labels.\nThis study also raises some ethical concerns about how\nLLMs may be used to produce misinformation. Our results\nshow that when the LLM offers correct explanations at the\nstart, crowd workers report higher levels of trust in the sys-\ntem. This implies that by controlling the correctness of a\nfew first pieces of LLM-generated information, individuals\nmight perceive AI-generated information as reliable and this\ncould be leveraged to propagate misinformation\nLimitations\nThese are possible limitations for our study: i) Dataset: In\nthis study, we only consider one dataset that is based on US\npoliticians. This dataset may introduce inherent biases (e.g.,\npolitical bias) in the decision-making process of US-based\ncrowd workers; ii) Choice of LLM: In this study, we uti-\nlize GPT-3.5 to generate outputs. However, it is worth to ex-\nplore alternative LLMs, considering that generative models\nmay introduce biases and generate fictional content (Galle-\ngos et al. 2023); iii) Interfance Design: In our study we show\nLLMs outputs in a static way. The way LLMs are integrated\nin the task may be done differently and can be more inter-\nactive such as providing feedback (Hettiachchi et al. 2021)\nand being a part of an iterative workflow (Little et al. 2010).\nConclusions\nIn this paper, we addressed research questions related to the\nimpact of LLM output on human decision-making behav-\nior when assessing the truthfulness of political statements.\nWe conducted a large-scale user study by means of crowd-\nsourcing measuring the impact of being exposed to the mis-\ninformation classification decisions generated by an LLM,\ntogether with a natural language explanation of such deci-\nsions. Our results show that crowd workers rely extensively\non LLM decisions up to the point of making judgment er-\nrors due to misleading information generated by the LLM.\nSuch over-reliance on the LLM is also shown in a reduced\ntime needed to judge truthfulness and a reduced use of web\nsearch to find supporting evidence.\n1683\nAcknowledgments\nThis work is partially supported by the Australian Re-\nsearch Council (ARC) Training Centre for Information Re-\nsilience (Grant No. IC200100022) and by the Swiss Na-\ntional Science Foundation (SNSF) under contract number\nCRSII5\n205975.\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nNeurIPS, 33: 1877–1901.\nChecco, A.; Roitero, K.; Maddalena, E.; Mizzaro, S.; and\nDemartini, G. 2017. Let’s agree to disagree: Fixing agree-\nment measures for crowdsourcing. In Proceedings of the\nAAAI Conference on Human Computation and Crowdsourc-\ning, volume 5, 11–20.\nDoroudi, S.; Kamar, E.; Brunskill, E.; and Horvitz, E. 2016.\nToward a learning science for complex crowdsourcing tasks.\nIn Proceedings of the 2016 CHI Conference on Human Fac-\ntors in Computing Systems, 2623–2634. ACM.\nDraws, T.; La Barbera, D.; Soprano, M.; Roitero, K.; Ce-\nolin, D.; Checco, A.; and Mizzaro, S. 2022. The effects of\ncrowd worker biases in fact-checking tasks. In 2022 ACM\nConference on Fairness, Accountability, and Transparency,\n2114–2124.\nDuan, X.; Ho, C.-J.; and Yin, M. 2022. The influences of\ntask design on crowdsourced judgement: A case study of\nrecidivism risk evaluation. In Proceedings of the ACM Web\nConference 2022, 1685–1696.\nEickhoff, C. 2018. Cognitive Biases in Crowdsourcing. In\nProceedings of WSDM, 162–170. ACM.\nEpstein, Z.; Foppiani, N.; Hilgard, S.; Sharma, S.; Glass-\nman, E.; and Rand, D. 2022. Do explanations increase the\neffectiveness of AI-crowd generated fake news warnings? In\nICWSM, volume 16, 183–193.\nFranke, T.; Attig, C.; and Wessel, D. 2019. A personal re-\nsource for technology interaction: development and valida-\ntion of the affinity for technology interaction (ATI) scale.In-\nternational Journal of Human–Computer Interaction, 35(6):\n456–467.\nGallegos, I. O.; Rossi, R. A.; Barrow, J.; Tanjim, M. M.;\nKim, S.; Dernoncourt, F.; Yu, T.; Zhang, R.; and Ahmed,\nN. K. 2023. Bias and fairness in large language models: A\nsurvey. arXiv preprint arXiv:2309.00770.\nGatto, J.; Basak, M.; and Preum, S. M. 2023. Scope of Pre-\ntrained Language Models for Detecting Conflicting Health\nInformation. In ICWSM, volume 17, 221–232.\nGuo, Z.; Schlichtkrull, M.; and Vlachos, A. 2022. A survey\non automated fact-checking. Transactions of the Association\nfor Computational Linguistics, 10: 178–206.\nHe, G.; Kuiper, L.; and Gadiraju, U. 2023. Knowing About\nKnowing: An Illusion of Human Competence Can Hinder\nAppropriate Reliance on AI Systems. In Proceedings of the\n2023 CHI Conference on Human Factors in Computing Sys-\ntems, 1–18.\nHettiachchi, D.; Schaekermann, M.; McKinney, T. J.; and\nLease, M. 2021. The challenge of variable effort crowd-\nsourcing and how visible gold can help. Proceedings of the\nACM on Human-Computer Interaction, 5(CSCW2): 1–26.\nHeuer, H.; and Glassman, E. L. 2022. A Comparative Evalu-\nation of Interventions Against Misinformation: Augmenting\nthe WHO Checklist. In Proceedings of the 2022 CHI Con-\nference on Human Factors in Computing Systems, 1–21.\nHoes, E.; Altay, S.; and Bermeo, J. 2023. Using ChatGPT to\nFight Misinformation: ChatGPT Nails 72% of 12,000 Veri-\nfied Claims.\nJahanbakhsh, F.; Katsis, Y .; Wang, D.; Popa, L.; and Muller,\nM. 2023. Exploring the Use of Personalized AI for Identi-\nfying Misinformation on Social Media. In Proceedings of\nthe 2023 CHI Conference on Human Factors in Computing\nSystems, 1–27.\nK¨orber, M. 2019. Theoretical considerations and develop-\nment of a questionnaire to measure trust in automation. In\nProceedings of the 20th Congress of the International Er-\ngonomics Association (IEA 2018) Volume VI: Transport Er-\ngonomics and Human Factors (TEHF), Aerospace Human\nFactors and Ergonomics 20, 13–30. Springer.\nKrippendorff, K. 2011. Computing Krippendorff’s alpha-\nreliability.\nKumar, S.; West, R.; and Leskovec, J. 2016. Disinforma-\ntion on the web: Impact, characteristics, and detection of\nwikipedia hoaxes. In Proceedings of the 25th international\nconference on World Wide Web, 591–602.\nLa Barbera, D.; Roitero, K.; Demartini, G.; Mizzaro, S.; and\nSpina, D. 2020. Crowdsourcing truthfulness: The impact of\njudgment scale and assessor bias. In42nd European Confer-\nence on IR Research, ECIR 2020, Part II, 207–214. Springer.\nLekschas, F.; Ampanavos, S.; Siangliulue, P.; Pfister, H.; and\nGajos, K. Z. 2021. Ask Me or Tell Me? Enhancing the Effec-\ntiveness of Crowdsourced Design Feedback. InProceedings\nof the 2021 CHI Conference on Human Factors in Comput-\ning Systems, 1–12.\nLittle, G.; Chilton, L. B.; Goldman, M.; and Miller, R. C.\n2010. Exploring iterative and parallel human computation\nprocesses. In Proceedings of the ACM SIGKDD workshop\non human computation, 68–76.\nMaddalena, E.; Ceolin, D.; and Mizzaro, S. 2018. Multidi-\nmensional News Quality: A Comparison of Crowdsourcing\nand Nichesourcing. In CIKM Workshops.\nQu, Y .; Roitero, K.; Barbera, D. L.; Spina, D.; Mizzaro, S.;\nand Demartini, G. 2022. Combining human and machine\nconfidence in truthfulness assessment.ACM Journal of Data\nand Information Quality, 15(1): 1–17.\nRoitero, K.; Soprano, M.; Fan, S.; Spina, D.; Mizzaro, S.;\nand Demartini, G. 2020. Can The Crowd Identify Misin-\nformation Objectively? The Effects of Judgment Scale and\nAssessor’s Background. In ACM SIGIR, 439–448.\nRoitero, K.; Soprano, M.; Portelli, B.; De Luise, M.; Spina,\nD.; Mea, V . D.; Serra, G.; Mizzaro, S.; and Demartini, G.\n2021. Can the crowd judge truthfulness? A longitudinal\nstudy on recent misinformation about COVID-19. Personal\nand Ubiquitous Computing, 1–31.\n1684\nShu, K.; Wang, S.; and Liu, H. 2019. Beyond news con-\ntents: The role of social context for fake news detection. In\nProceedings of the twelfth ACM international conference on\nweb search and data mining, 312–320.\nTolmeijer, S.; Christen, M.; Kandul, S.; Kneer, M.; and\nBernstein, A. 2022. Capable but amoral? Comparing AI and\nhuman expert collaboration in ethical decision making. In\nProceedings of the 2022 CHI Conference on Human Fac-\ntors in Computing Systems, 1–17.\nTolmeijer, S.; Gadiraju, U.; Ghantasala, R.; Gupta, A.; and\nBernstein, A. 2021. Second chance for a first impression?\nTrust development in intelligent system interaction. In Pro-\nceedings of the 29th ACM Conference on user modeling,\nadaptation and personalization, 77–87.\nVasconcelos, H.; J¨orke, M.; Grunde-McLaughlin, M.; Ger-\nstenberg, T.; Bernstein, M. S.; and Krishna, R. 2023. Ex-\nplanations can reduce overreliance on ai systems during\ndecision-making. Proceedings of the ACM on Human-\nComputer Interaction, 7(CSCW1): 1–38.\nWang, W. Y . 2017. ” liar, liar pants on fire”: A new\nbenchmark dataset for fake news detection. arXiv preprint\narXiv:1705.00648.\nWeinzierl, M.; Hopfer, S.; and Harabagiu, S. M. 2021. Mis-\ninformation adoption or rejection in the era of covid-19. In\nICWSM, volume 15, 787–795.\nWobbrock, J. O.; Findlater, L.; Gergle, D.; and Higgins, J. J.\n2011. The aligned rank transform for nonparametric facto-\nrial analyses using only anova procedures. In Proceedings\nof the SIGCHI conference on human factors in computing\nsystems, 143–146.\nWu, Y .; Jiang, A. Q.; Li, W.; Rabe, M.; Staats, C.; Jamnik,\nM.; and Szegedy, C. 2022. Autoformalization with large\nlanguage models. NeurIPS, 35: 32353–32368.\nXu, F. F.; Alon, U.; Neubig, G.; and Hellendoorn, V . J.\n2022a. A systematic evaluation of large language models\nof code. In Proceedings of the 6th ACM SIGPLAN Interna-\ntional Symposium on Machine Programming, 1–10.\nXu, J.; Han, L.; Fan, S.; Sadiq, S.; and Demartini, G. 2022b.\nDoes Evidence from Peers Help Crowd Workers in Assess-\ning Truthfulness? In Companion Proceedings of the Web\nConference 2022, 302–306.\nXu, J.; Han, L.; Sadiq, S.; and Demartini, G. 2023. On\nthe role of human and machine metadata in relevance judg-\nment tasks. Information Processing & Management, 60(2):\n103177.\nXu, L.; Zhou, X.; and Gadiraju, U. 2020. How does team\ncomposition affect knowledge gain of users in collaborative\nweb search? In Proceedings of the 31st ACM conference on\nhypertext and social media, 91–100.\nZhang, T.; Ladhak, F.; Durmus, E.; Liang, P.; McKeown,\nK.; and Hashimoto, T. B. 2023. Benchmarking large lan-\nguage models for news summarization. arXiv preprint\narXiv:2301.13848.\nZhu, H.; Dow, S. P.; Kraut, R. E.; and Kittur, A. 2014. Re-\nviewing versus doing: Learning and performance in crowd\nassessment. In 17th ACM CSCW Conference, 1445–1455.\n1685\nPaper Checklist\n1. For most authors...\n(a) Would answering this research question advance sci-\nence without violating social contracts, such as violat-\ning privacy norms, perpetuating unfair profiling, exac-\nerbating the socio-economic divide, or implying disre-\nspect to societies or cultures? Yes\n(b) Do your main claims in the abstract and introduction\naccurately reflect the paper’s contributions and scope?\nYes\n(c) Do you clarify how the proposed methodological ap-\nproach is appropriate for the claims made? Yes\n(d) Do you clarify what are possible artifacts in the data\nused, given population-specific distributions? Yes\n(e) Did you describe the limitations of your work? Yes\n(f) Did you discuss any potential negative societal im-\npacts of your work? Yes, in the Limitation section\n(g) Did you discuss any potential misuse of your work?\nYes, in the Limitation section\n(h) Did you describe steps taken to prevent or mitigate po-\ntential negative outcomes of the research, such as data\nand model documentation, data anonymization, re-\nsponsible release, access control, and the reproducibil-\nity of findings? NA\n(i) Have you read the ethics review guidelines and en-\nsured that your paper conforms to them? Yes\n2. Additionally, if your study involves hypotheses testing...\n(a) Did you clearly state the assumptions underlying all\ntheoretical results? Yes\n(b) Have you provided justifications for all theoretical re-\nsults? Yes\n(c) Did you discuss competing hypotheses or theories that\nmight challenge or complement your theoretical re-\nsults? Yes\n(d) Have you considered alternative mechanisms or expla-\nnations that might account for the same outcomes ob-\nserved in your study? Yes\n(e) Did you address potential biases or limitations in your\ntheoretical framework? Yes\n(f) Have you related your theoretical results to the existing\nliterature in social science? Yes\n(g) Did you discuss the implications of your theoretical\nresults for policy, practice, or further research in the\nsocial science domain? Yes\n3. Additionally, if you are including theoretical proofs...\n(a) Did you state the full set of assumptions of all theoret-\nical results? NA\n(b) Did you include complete proofs of all theoretical re-\nsults? NA\n4. Additionally, if you ran machine learning experiments...\n(a) Did you include the code, data, and instructions\nneeded to reproduce the main experimental results (ei-\nther in the supplemental material or as a URL)? NA\n(b) Did you specify all the training details (e.g., data splits,\nhyperparameters, how they were chosen)? NA\n(c) Did you report error bars (e.g., with respect to the ran-\ndom seed after running experiments multiple times)?\nNA\n(d) Did you include the total amount of compute and the\ntype of resources used (e.g., type of GPUs, internal\ncluster, or cloud provider)? NA\n(e) Do you justify how the proposed evaluation is suffi-\ncient and appropriate to the claims made? NA\n(f) Do you discuss what is “the cost“ of misclassification\nand fault (in)tolerance? NA\n5. Additionally, if you are using existing assets (e.g., code,\ndata, models) or curating/releasing new assets, without\ncompromising anonymity...\n(a) If your work uses existing assets, did you cite the cre-\nators? Yes, see Dataset section\n(b) Did you mention the license of the assets? Yes\n(c) Did you include any new assets in the supplemental\nmaterial or as a URL? No, we did not include any new\nassets\n(d) Did you discuss whether and how consent was ob-\ntained from people whose data you’re using/curating?\nNo, it is an open-source dataset\n(e) Did you discuss whether the data you are using/cu-\nrating contains personally identifiable information or\noffensive content? No\n(f) If you are curating or releasing new datasets, did you\ndiscuss how you intend to make your datasets FAIR\n(see FORCE11 (2020))? NA, we are not curating or\nreleasing new datasets.\n(g) If you are curating or releasing new datasets, did you\ncreate a Datasheet for the Dataset (see Gebru et al.\n(2021))? NA\n6. Additionally, if you used crowdsourcing or conducted\nresearch with human subjects, without compromising\nanonymity...\n(a) Did you include the full text of instructions given to\nparticipants and screenshots? No, we have only in-\ncluded a screen shot as the full instruction would com-\npromise anonymity.\n(b) Did you describe any potential participant risks, with\nmentions of Institutional Review Board (IRB) ap-\nprovals? Yes, see the Crowdsourcing Task Design sec-\ntion.\n(c) Did you include the estimated hourly wage paid to\nparticipants and the total amount spent on participant\ncompensation? Yes, see the Crowdsourcing Task De-\nsign section.\n(d) Did you discuss how data is stored, shared, and dei-\ndentified? Yes\n1686",
  "topic": "Crowdsourcing",
  "concepts": [
    {
      "name": "Crowdsourcing",
      "score": 0.9447097778320312
    },
    {
      "name": "Misinformation",
      "score": 0.8376822471618652
    },
    {
      "name": "Computer science",
      "score": 0.4338231086730957
    },
    {
      "name": "Natural language processing",
      "score": 0.38506120443344116
    },
    {
      "name": "Data science",
      "score": 0.35973089933395386
    },
    {
      "name": "Linguistics",
      "score": 0.329585999250412
    },
    {
      "name": "World Wide Web",
      "score": 0.16505789756774902
    },
    {
      "name": "Computer security",
      "score": 0.09969845414161682
    },
    {
      "name": "Philosophy",
      "score": 0.08228304982185364
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165143802",
      "name": "The University of Queensland",
      "country": "AU"
    }
  ]
}