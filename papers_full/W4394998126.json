{
  "title": "De novo drug design as GPT language modeling: large chemistry models with supervised and reinforcement learning",
  "url": "https://openalex.org/W4394998126",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Ye, Gavin",
      "affiliations": [
        "University Prep"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2273267066",
    "https://openalex.org/W4308043267",
    "https://openalex.org/W4283586469",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3009321976",
    "https://openalex.org/W3013657235",
    "https://openalex.org/W3043969542",
    "https://openalex.org/W4362664882",
    "https://openalex.org/W4280597794",
    "https://openalex.org/W2037493760",
    "https://openalex.org/W3047278342",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2096864392",
    "https://openalex.org/W2777416523",
    "https://openalex.org/W4307468223",
    "https://openalex.org/W2909063104",
    "https://openalex.org/W4379468083",
    "https://openalex.org/W4300961340",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4389157038",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W3109549311",
    "https://openalex.org/W2169303530",
    "https://openalex.org/W3161951461",
    "https://openalex.org/W4308068572",
    "https://openalex.org/W4387692254",
    "https://openalex.org/W2773987374",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3100751385",
    "https://openalex.org/W3045928028"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nJournal of Computer-Aided Molecular Design (2024) 38:20 \nhttps://doi.org/10.1007/s10822-024-00559-z\nDe novo drug design as¬†GPT language modeling: large chemistry \nmodels with¬†supervised and¬†reinforcement learning\nGavin¬†Ye1\nReceived: 8 December 2023 / Accepted: 22 March 2024 / Published online: 22 April 2024 \n¬© The Author(s) 2024\nAbstract\nIn recent years, generative machine learning algorithms have been successful in designing innovative drug-like molecules. \nSMILES is a sequence-like language used in most effective drug design models. Due to data‚Äôs sequential structure, models \nsuch as recurrent neural networks and transformers can design pharmacological compounds with optimized efficacy. Large \nlanguage models have advanced recently, but their implications on drug design have not yet been explored. Although one \nstudy successfully pre-trained a large chemistry model (LCM), its application to specific tasks in drug discovery is unknown. \nIn this study, the drug design task is modeled as a causal language modeling problem. Thus, the procedure of reward mod-\neling, supervised fine-tuning, and proximal policy optimization was used to transfer the LCM to drug design, similar to \nOpen AI‚Äôs ChatGPT and InstructGPT procedures. By combining the SMILES sequence with chemical descriptors, the novel \nefficacy evaluation model exceeded its performance compared to previous studies. After proximal policy optimization, the \ndrug design model generated molecules with 99.2% having efficacy  pIC50 > 7 towards the amyloid precursor protein, with \n100% of the generated molecules being valid and novel. This demonstrated the applicability of LCMs in drug discovery, \nwith benefits including less data consumption while fine-tuning. The applicability of LCMs to drug discovery opens the \ndoor for larger studies involving reinforcement-learning with human feedback, where chemists provide feedback to LCMs \nand generate higher-quality molecules. LCMs‚Äô ability to design similar molecules from datasets paves the way for more \naccessible, non-patented alternatives to drug molecules.\nKeywords Large language model¬†¬∑ GPT¬†¬∑ Large chemistry model¬†¬∑ De novo drug design¬†¬∑ Reinforcement learning¬†¬∑ Efficacy \noptimization\nDrug discovery is one of the most time-consuming and \ncostly aspects of developing a drug. It is estimated to take \nabout 10‚Äì15¬†years, with a cost of $1.395 billion per drug \ndiscovered and approved [1]. Scientists have attributed the \nvastness of the chemical space (estimated to have ~1060 cur-\nrently synthesizable molecules) as one of the main chal-\nlenges in discovering drugs. In simpler terms, it is impos-\nsible to enumerate all possible synthesizable molecules to \nperform virtual screening for drug efficacy. Machine learn-\ning (ML) has emerged as one of the most promising tools in \ndrug discovery and can speed up this process. Specifically, \nscientists have used large datasets of known (drug or drug-\nlike) molecules to train generative machine learning models \nfor designing drug-like molecules with certain desired prop-\nerties from scratch, or in other words, to perform de novo \ndrug design [2, 3].\nThere are multiple ways to store these models in a way \nthat a machine can understand, one of which is the simplified \nmolecular-input line-entry system (SMILES). In SMILES, \nbonds and other geometric information are stored using \nsymbols such as ‚Äú=‚Äù [4 ]. One can think of it as a chemical \nlanguage that is used to store the molecule‚Äôs structure and \nits constituents without losing information.\nWith representation systems that can denote mole-\ncules and their structures using concise, one-dimensional \nsequences, scientists have used ML models such as trans-\nformer models and recurrent neural networks (RNNs), which \nare models commonly used for language processing, for \nchemistry [3, 5, 6]. This makes these sequence processing \nmodels a state-of-the-art approach in de novo drug design. \nApproaches that uses long short-term memory (LSTM) \n * Gavin Ye \n yeeeyee004@gmail.com\n1 Columbia Grammar & Preparatory School, New¬†York, NY, \nUSA\n Journal of Computer-Aided Molecular Design (2024) 38:20\n20 Page 2 of 15\nneural networks and generative adversarial networks (GANs) \ntended to yield low validity in drug design (i.e., many of the \ndesigned molecules are chemically invalid or contain syn-\ntax errors) [3 , 6, 7]. However, the recent success of large \nlanguage models (LLMs) and their implications for drug \ndiscovery are unexplored. Although there are studies like \nRegression Transformers [8 ] and ChemGPT [9 ] that have \nsuccessfully adapted large generative pretrained transform-\ners (GPTs) into general LCMs, their roles in drug design \nare still unexplored. Specifically, one reserach [9] examined \nmodel scaling behavior in the context of molecule mode-\nling: the study created ChemGPT, which has successfully \ntransformed large language models into pretrained large \nchemistry models, generating valid SELFIES molecules \n[9]. Their study concluded that the neural scaling (the trend \nof drastic increases in model performance as model size \nincreases over several orders of magnitude) phenomenon \nfrom NLP also transfers or appears in the field of molecule \nmodeling [9], potentially implicating further breakthroughs. \nSince ChemGPT was not further trained for specific chemi-\ncal tasks, the effect of neural scaling in drug design or other \nspecific chemical applications are unknown. In the present \nstudy, a LLM (now a LCM) is adapted (trained) to drug \ndesign for molecules with high drug efficacy for the first \ntime. For case study, the LCM is used to target the amyloid \nbeta precursor protein, a known drug target for Alzheimer‚Äôs \ndisease [10, 11]. APP is a drug target in treating AD, as \nsuccessful inhibition of APP from accumulating amyloid-Œ≤ \npeptide could prevent the pathogenesis of AD [10, 11].\nThe approach used here can easily be adapted to other tar-\nget proteins. The proposed drug design model can be more \ndata efficient, since LLMs are ‚Äúfew-shot‚Äù learners [12]. In \nother words, the LCM can adapt to specific drug designing \ntasks with less training data once it is trained for drug design \nfor the first time.\nFor the first time, the present study uses a drug efficacy \nevaluation deep learning model that processes both chemi-\ncal property descriptors and (sequence-denoted) chemical \nstructure by using both recurrent neural networks and dense \nfeed-forward layers. The sequential representations used are \nSMILES and self-referencing embedded strings (SELFIES). \nDeep learning refers to the family of artificial neural network \nmodels that have multiple layers, which includes traditional \nfeed-forward networks and recurrent neural networks. My \nstudy uses this in silico method for evaluating drug efficacy \ntowards the drug target, amyloid-precursor protein (APP). \nThis efficacy evaluation model is later used with my drug \ndesign model in a feedback loop for optimizing properties \nsuch as drug efficacy via a reinforcement learning optimi-\nzation technique known as proximal policy optimization  \n(PPO).\nMethods\nMethod overview\nThe method is organized into three parts: Part 1 addresses \nObjective 1, Part 2 addresses Objective 2, and so forth \n(Fig.¬†1). As stated in the objectives of this study, one of the \nmain goals was to transfer LLM into LCM and adapt LCM \nfor drug design. To achieve Objective 1 in evaluating drug \nefficacy, I employed the reward modeling step, in which a \nnovel quantitative structure‚Äìactivity relationship (QSAR) \nFig. 1  Method overview. The steps are listed in chronological order \nand in which they appear in the paper. The black arrows denote the \nlevel of specificity, and the white arrows denote the chronological \norder. The first part is to train an evaluation model that estimates the \ndrug efficacy given the structure of the molecule. The second part is \nto train the drug design GPT model to learn how to generate similar \ndrug-like molecules. The third step is to optimize for desired proper -\nties such as drug efficacy\nJournal of Computer-Aided Molecular Design (2024) 38:20 \n Page 3 of 15 20\nefficacy evaluation model structure was used. To further \nfinetune my drug design model in generating similar drug-\nlike molecules, I used the supervised finetuning (SFT) step. \nFinally, to use my efficacy evaluation model to optimize my \ndrug design model for designing higher efficacy molecules \n(also known as developmental candidates), I employed a \ntransformer reinforcement learning (TRL) step by using the \nproximal policy optimization (PPO) algorithm. The three-\nstep training approach used here is similar to a popular Open \nAI training approach for training LLMs for ChatGPT. This \ninvestigates whether the natural language processing (NLP) \ntraining and finetuning schema can transfer for designing \neffective drug molecules.\nFor drug design and efficacy optimization, the drug target \nprotein selected is APP, a target that has therapeutic effects \nin treating Alzheimer‚Äôs [10, 11]. However, the same meth-\nodology can be applied to transfer LCM for targeting a dif-\nferent protein using a different dataset.\nQSAR evaluation model using LSTMs\nTo address Objective 1 and to better model drug efficacy \ntowards APP, I devised a novel LSTM QSAR evaluation \nmodel design. My evaluation model takes both the structural \ninformation denoted in sequence and the chemical proper -\nties of the molecule denoted via numerical descriptors. To \ninvestigate the explainability of my evaluation model, I used \nthe Exmol library for the first time for efficacy evaluation for \naiding ML drug design.\nData preprocessing\nI selected the publicly available APP dataset from Bind-\ningDB [13] as the training and evaluation sets. It contains \nexperimental data on ligand‚Äìprotein interactions (with bind-\ning affinity measured, and drug efficacy measured in IC50). \nFor easier processing and better performance of the model, \nI converted IC50 values to pIC50 values. IC50 measures \nhow much concentration of the drug is needed to have a \n50% effect on the biological and biochemical function of the \nreceptor protein. pIC50 can be calculated by using the equa-\ntion, pIC50 =‚àí log10\n/bracketleft.s1IC50\n/bracketright.s1 , which is a better approach due to \neasier interpretability and easier processing (not having to \ndeal with extremely small and large floating-point numbers). \nThe higher the pIC50 value, the higher the drug efficacy.\nTo ensure the uniqueness of the SMILES of the ligands \nand to avoid discrepancies between the drug efficacy of the \nsame ligand in the data, I used the arithmetic mean of a \nmolecule‚Äôs multiple recorded  pIC50 values, as now each mol-\necule corresponds to one drug efficacy value only. Since \nmost of the measurements in the dataset are in  pIC50 (con-\nverted from  IC50), I filtered and used the experimental data \nmeasured in  pIC50, leaving 1032 unique experimentally \ntested ligands in total, some of which are patented mole-\ncules. I later split the dataset into 80% for training and 20% \nfor evaluation.\nAlternative sequential representations\nTo investigate the most suitable sequential representation \nof molecules for my efficacy evaluation model, I trained \nevaluation models using SMILES and SELFIES. There \nare many different ways to encode SMILES into tensors to \nfeed them into the neural network QSAR model. The most \ncommon and straightforward approach is to use a custom \nPytorch embedding layer, and map each character of the \nsequence into an unique integer. The embedding layer is a \nneural network layer whose goal is to help learn the semantic \nrelationships between tokens. Semantic relationships can be, \nfor instance, that ‚Äú(‚Äù are always paired with ‚Äú)‚Äù, and ‚Äúc‚Äù is \nthe same as ‚ÄúC‚Äù but part of an aromatic ring. To map each \ncharacter to integer, thus turning the whole sequence into a \ntensor, I modified and used the vocabulary list from Abbasi‚Äôs \nstudy [3], which contains an almost complete list of valid \ncharacters in SMILES. Alternatively, instead of SMILES \nrepresentation, one can also use SELFIES, which requires no \nadditional data preprocessing since it is already in tokenized \nformat. Thus, although SMILES can be sometimes more \nconcise, SELFIES can be more ‚Äúmachine-friendly,‚Äù which \nis why I compared QSAR evaluation models trained using \nSMILES representation and those trained using SELFIES \nrepresentation.\nTo select the best way to process sequential information \nfor my evaluation model, I used the DeepChem‚Äôs Mol2Vec \nembedding system [ 14]. Mol2Vec [14] is a pretrained \nembedding layer network on PubChem database‚Äôs mol-\necules [15]; it is the analog of the Word2Vec embedding \nnetwork‚Äîa commonly used pretrained embedding system \nfrom NLP. I tested these 3 different embedding techniques \nand selected the more suitable, best performing representa-\ntion and embedding system for the final efficacy evaluation \nmodel.\nFeature engineering and¬†QSAR evaluation model \ndesigning\nI designed the QSAR efficacy evaluation model, which is \nnovel since it takes in both sequential representation and \nnumerical chemical descriptors of molecules calculated \nfrom RDKit [ 16] to achieve better performance compared \nto previous models (shown in Fig.¬† 2). RDKit Tools is a \npython library that has helper functions such as to calcu-\nlate chemical descriptors or to parse and validate SMILES. \nThe numerical chemical descriptors from RDKit were cal-\nculated and used as features, in addition to the sequential \n Journal of Computer-Aided Molecular Design (2024) 38:20\n20 Page 4 of 15\nrepresentation input feature (in SMILES or SELFIES). \nThese chemical descriptors are numbers representing the \nmolecule‚Äôs chemical properties, such as the polar surface \narea, etc. Numerical descriptors have shown to be useful \nfor QSAR, even outperforming one-hot-encoded structure \nrepresentation when non-sequential processing models \nare used such as support vector regression and random \nforest [17]. I filtered out the non-numerical descriptors \nfrom RDKit and used DeepChem‚Äôs RDKit descriptors for \nQSAR. I normalized the descriptors via scikit-learn scaler \n(normalizer) since different RDKit descriptors span in dif-\nferent scales. Features with drastically different scales can \nbe hard to learn for neural networks.\nTo train my evaluation model, I searched for the optimal \nhyperparameters via grid search. The tuned, final hyperpa-\nrameters for the LSTM (evaluation) model used a learning \nrate of 0.001 and a dropout of 0.3 to prevent overfitting. The \nembedding layer supports 701 different SELFIES encoded \ntokens. The learning rate controls how much the model \nchanges when making a mistake, and the dropout prevents \noverfitting by training only a certain percentage of randomly \npicked neurons at a time. The loss function is defined as the \nmean squared error (MSE) loss, a common loss metric for \nregression.\nI trained the evaluation model with a batch size of 16. \nThe model itself takes in two inputs, the first one being a \nsequence of token IDs where each id is an integer denoting \na token, the second input being a feature vector of numerical, \nRDKit generated chemical descriptors of the molecule. In \nmy evaluation model (Fig.¬†2), each layer consists of 256 neu-\nrons. Inside the model, the first, sequential input is passed \ninto the embedding layer, and the second, numerical, input \nis passed into a dense layer directly. Then, the output of the \nembedding layer is passed into two layers of LSTM, and the \noutput of the two LSTM layers are concatenated with the \noutput of the first dense layer, into one final feature vector, \nwhich is fed into a dense, feed-forward layer, whose output is \nfinally fed into one neuron in the next layer and the predicted \nvalue is outputted from this neuron (Fig.¬†2). The number of \ndense layers processing the chemical descriptors were tuned, \nand the hyperparameters were tuned. The activation func-\ntion used for the layers was the rectified linear unit (ReLU).\nTo train and compare my QSAR evaluation model that \nuses Mol2Vec embedding system [14] and my model that \nuses SMILES representation, I used the same model struc-\nture, except only 82 tokens (excluding default ones such as \nthe end of sequence token) were used for the embedding \nlayer for the evaluation model that uses pure SMILES. Dur-\ning training, I used the Adam optimizer with betas. They \nwere used to average the gradient of previous n  batches, \nwhich has been shown helpful for models‚Äô lost functions to \ncross through saddle points and local minimas. To decrease \nthe learning rate when the model was converging (and \nwhen the validation metrics were not improving), I used \nthe Reduce Learning Rate on Plateau scheduler, making the \ntraining more stable. I trained the evaluation model for 100 \nepochs, and I selected the best performing model as the final \nefficacy evaluation model.\nQSAR performance evaluation\nTo better measure the performance of my evaluation model \nin addition to the loss metric, I used the concordance corre-\nlation coefficient (CCC) as it is a better metric for both bias \nand variance (i.e., for both ‚Äúaccuracy‚Äù and ‚Äúprecision‚Äù). To \nexplore how efficiently my evaluation model uses its data \ncompared to previous models, I conducted another experi-\nment with the same procedure except the training sets used \nhave size [825, 700, 500, 400, 250, 100]. Since the dataset \nonly had 1032 unique SMILES, the maximum size used \nwas 825 molecules for training (using a 80‚Äì20 train-test \ndata split). I shuffled the dataset and split into training and \nevaluation once; I used a portion of the training set to train \nthe evaluation model. The evaluation set stayed unchanged \nduring the experiment. Splitting only once throughout this \nFig. 2  QSAR evaluation model: multi-feature LSTM architecture. \nThe QSAR architecture takes in two different representations of a \nmolecule: in sequential representation using SELFIES, and in chemi-\ncal descriptors generated by RDKit. The two inputs go through dif-\nferent neural network layers, then the output of the layers are concat-\nenated and go through one final layer to produce the predicted  pIC50 \nvalue. For the embedding layer, a custom embedding layer and Deep-\nChem embedding layers were tested\nJournal of Computer-Aided Molecular Design (2024) 38:20 \n Page 5 of 15 20\nexperiment reduces the effect of randomness on performance \nsince each time the trained model is evaluated on the same \nset of molecules.\nModel explainability\nTo open and take ‚Äúsneak peeks‚Äù into the black box of this \nLSTM neural network in order to provide explainability to \nmy evaluation model, I used the Exmol python library. It is \nhard to interpret neural networks since they are black boxes. \nUnlike decision tree-based models, which are self-explan-\natory with its branch denoting a specific condition of some \nfeature, a neural network has so many parameters, weights, \nand biases that it is impossible for any human to investi-\ngate why and what patterns and strategies did the evaluation \nmodel learned; this is known as the black box phenomenon: \ninput and output can be observed, but not anything that hap-\npens in between. Exmol is a python library that uses differ -\nent chemical descriptors and surrogate models to explain a \nchemistry related regression or classification model [18]. \nThis is a relatively new tool as the library is released in \nlate 2022. This is the first time it is used for aiding ML \nde novo drug design by making drug efficacy evaluation \nmodel interpretable. I used the default Exmol descriptors, \nand the results were automatically graphed by the library. A \nnatural language explanation can also be generated, which \nexplains what is happening behind the evaluation model \n[18]. Furthermore, I compared the Exmol descriptors with \nfindings from previous studies on drug efficacy and activity \nto determine the credibility of using Exmol. If the result \nproves credible, then Exmol serves as a convincing way of \ninterpreting QSAR neural networks. If credible, explainabil-\nity of QSAR efficacy evaluation models can provide invalu-\nable information for human chemists on what makes a ligand \neffective towards the drug target.\nLCMs for¬†adapting to¬†drug discovery\nIn my study, I treat the problem of drug design as a causal \nlanguage modeling  (CLM) problem, like many other GPT \nmodels in NLP. In CLM, during training, the model tries to \npredict the next word in the sequence in some dataset, and \nthen in generation, it generates new sequences word-by-word \nand token-by-token.\nTo perform causal language modeling for supervised fine-\ntuning, in addition to using Pytorch, I used the Hugging Face \ntransformers library and the Hugging Face transformer rein-\nforcement learning (TRL) library. Hugging Face is a website \nand a community that has public, open-source repositories \nof large language models, such as Meta‚Äôs Llama2 [19], and \nFalcon LLM [20]. This addresses Objective 2, to train my \ndrug design model to design similar drug-like molecules, \nadapting the model to drug design from general chemical \ntasks.\nThe Hugging Face transformers library provides many \nhelper functions needed to train one‚Äôs own transformer. \nThe transformer reinforcement learning (TRL) library has \nfunctions and classes that allow customized reinforcement \nlearning training. I used TRL since it can easily interact with \nHugging Face causal language models and contains state-of-\nthe-art reinforcement learning algorithms that are commonly \nused in the field of natural language processing.\nSupervised finetuning of¬†LCM\nThe base transformer model used was the GPT-Neo lan-\nguage model [21], pretrained by the study on ChemGPT [9] \nusing 10 million molecules from PubChem [15] to generate \ngeneral molecules. To adapt the LCM to the APP dataset, \nI devised this supervised finetuning step to train the LCM.\nData preparation\nTo perform supervised finetuning, I used the same Bind-\ningDB dataset [13] from the QSAR training. I used the same \ndataset cleaning technique, and the dataset was also mapped \nfrom tokenized SELFIES into numerical token identifiers \n(token IDs) using a modified version based on ChemGPT‚Äôs \nauto tokenizer from Hugging Face library. The tokenizer \nwas updated to support all of the tokens used in the data-\nset, and thus the embedding layer of my drug design model \nwas resized. These new tokens include chirality informa-\ntion, such as the ‚Äú[C@Hexpl]‚Äù and ‚Äú[C@@expl]‚Äù SELFIES \nencoded tokens. Similar to the procedures for QSAR, I split \nthe dataset into 80% for training and 20% for evaluation.\nAfter the tokenizer was prepared and the training dataset \nwas tokenized, to investigate whether chunking (one NLP \ndata processing technique) helps LCM performance, I com-\npared LCMs that were tuned with and without this chunk -\ning preprocessing. Chunking refers to merging and split-\nting multiple input sequences together to convert all input \nsequences into chunks having the same length. Each chunk \nis an incomplete sequence, with tokens representing the start \nand the end of an individual sequence respectively. In other \nwords, after chunking, many of the chunk sequences will \nnot contain both the start token and the end token unless the \nchunk contains a complete sequence. This is done because \ntransformers have a limited context window (can focus and \nprocess a certain amount of words at a time), and it is more \nefficient for the transformer model to utilize the full win-\ndow during training. However, natural language modeling \nmodels are usually trained via documents and encyclo-\npedia articles that are far longer than the context window \nlength. In contrast, for drug design, all of the ligands from \nthe dataset have a length less than 200 when represented in \n Journal of Computer-Aided Molecular Design (2024) 38:20\n20 Page 6 of 15\nSELFIES encoded tokens; all are smaller than the window \nlength. Thus, I trained the drug design model with and with-\nout this technique and compared the loss values to evaluate \nwhether the chunking technique affected the performance, \nor if chunking caused the drug design model to incorrectly \nassume relationships between the independent short SELF -\nIES sequences. I chose the better performing supervised-\ntrained drug design model for Part 3 of the method.\nSupervised training\nFinally, after data preparation, I used the supervised fine-\ntuning trainer from Hugging Face to configure and train the \ndrug design LCM. The trainer automatically batched input \n(token ID) sequences and pad them into the same length \nbehind the scene, thus no further data preprocessing was \nneeded. I used these padding tokens to account for the fact \nthat different sequences have different lengths, while the \ninput tensor must be rectangular‚Äîeach sequence having \nthe same length. In addition, I also used the attention masks \nto tell the transformer model to ignore padding tokens. I \nused a batch size of 16, and the learning rate was set to \n7 ‚ãÖ 10‚àí5 . I used a weight decay of 0.01 to prevent overfitting \nand trained the drug design model for 10 epochs. I tuned \nthe hyperparameters via Optuna [ 22], a Python Bayesian \nparameter optimizer, which was used since it is compatible \nwith Hugging Face, and the Bayesian parameter optimizer \nadapts to the performance of drug design models with differ-\nent hyperparameters rather than simply enumerating through \na grid of combinations of hyperparameters. This can be rel-\nevant and important when the training process is relatively \ntime consuming.\nModel evaluation using cross‚Äëentropy loss\nTo train the drug design LCM via supervised finetuning, I \nused the loss function, which is defined as the cross-entropy \nloss (Eq.¬†1):\nwhere L represents cross-entropy loss, which takes in the \ntrue next token, p , as a probability distribution where the \ncorrect token has a 100% probability and other tokens having \na 0% probability, and the model‚Äôs distribution (which is the \nlogit) is denoted as q . I used the cross-entropy loss to define \nhow close the model‚Äôs logit is from the true distribution, \nso the gradient can be computed and the model can be cor -\nrected. In causal language modeling, when generating each \ntoken, the cross-entropy loss was used for correcting a model \nwhen its response deviates from the sequence or molecule it \nis trying to model. In other words, the gradient is computed \n(1)L(p, q) =‚àí\n/uni2211.s1\nx‚ààvocabs\np(x)log(q(x))\nand the model learns from errors it makes anytime it gener-\nates a token that differs from the token from the ‚Äúcorrect \nanswer‚Äù sequence, which is the desired next token.\nTo evaluate my drug design model‚Äôs performance after \nthe training was complete, I repeatedly prompted my model \nwith the start token for performing de novo drug design. I \ngraphed and compared the distribution of drug efficacy, and \nother chemical properties of the generated molecules with \nthe dataset molecules. The validity and novelty of the gener-\nated molecules were also determined. Validity is defined as \nthe percentage of generated sequences that are parasable to \nrepresent actual molecules; novelty is defined as the percent-\nage of generated sequences that do not exist in the dataset \n(both training and evaluation set). This supervised finetuning \ntrained drug design model serves as the basis model for PPO \ntraining in the next step.\nProximal policy optimization (PPO)\nThe role of PPO is to incentivize my drug design model \nfor designing higher efficacy molecules. To the best of my \nknowledge, no published study on machine learning aided \ndrug design has used PPO.\nReinforcement learning notations\nTo use reinforcement learning for drug design LCM, one \nwill need to define each component of the reinforcement \nlearning (RL) agent. In this case, the LCM, or LLM, is the \npolicy function of an agent (denoted as œÄŒ∏ (at|st)), where \neach action of the agent is to generate the next token, and the \nstate st represents some already generated sequence (recall \nthe CLM procedure). The advantage function, or A(s t,at) or \nAt represents how good or bad some decision a t is. It can \nbe described mathematically as A t = Q(st,at)‚àíV(st), where \nQ(s,a) represents the expected desiredness of performing \nsome action a at some state, subtracted from V(s), the desir-\nability of this current state; thus, the whole term determines \nhow much more desiredness or advantage one get by per -\nforming this action. To define and incentivize for ‚Äúdesired, \neffective molecules‚Äù using RL, one method is to take the \ngradient with respect to the policy‚Äôs parameters that opti-\nmizes the advantage function, which is one of the main ideas \nbehind the proximal policy optimization algorithm.\nThe value head\nThe purpose of the value head is to make the LCM drug \ndesign model prefer a higher reward given by designing \ndesired, effective molecules. During reinforcement learning, \nthe value head is connected to the generative LCM model, \nwhich is an additional feed-forward neural network that \nJournal of Computer-Aided Molecular Design (2024) 38:20 \n Page 7 of 15 20\nrepresents the value function. This is the part of the model \nthat ‚Äúcommunicates‚Äù with the efficacy evaluation model to \ninterpret the evaluated efficacy ‚Äúscore‚Äù.\nPPO definition for¬†drug design & implementation \ndetails\nTo incentivize the drug design LCM to generate higher effi-\ncacy molecules towards APP, I used a PPO reinforcement \nlearning schema. To do so, I used the efficacy evaluation \nmodel trained earlier for scoring molecules, designed by the \ndrug design LCM trained from Part 2. I repeated the training \nloop multiple times throughout the algorithm for multiple \nepochs.\nThrough reinforcement learning, the drug design \nmodel finds out what sequences of tokens work well \n(have high drug efficacy) and what does not. I used \nthe PPO Trainer from Hugging Face for this train-\ning process, which updates the drug design model \nbehind the scene when given the three parameters: \n(Q = query = input,A = generated,R = reward = drug eÔ¨Écacy ) . The \nPPO algorithm was implemented in the trainer‚Äôs step func-\ntion, and the training loop was designed and implemented \nby myself. The present study should be the first time that \nreinforcement learning optimization (specifically PPO) is \napplied to LCMs.\nFormally, the PPO algorithm can be described as maxi-\nmizing the following equation, for some hyperparameter \nepsilon denoting how much a policy is allowed to change \neach update, shown in (4):\nwhere r/u1D70B(/u1D703) = clip\n/parenleft.s3\n/u1D70B/u1D703(at/uni007C.varst)\n/u1D70B/u1D703old (at/uni007C.varst) ,1 ¬± /u1D700\n/parenright.s3\n.\nWhere ObjectivePPO  represents the objective of PPO, s t \nrepresents the expected value at some time t , and the con-\nventional notations for RL are used.\nTo make the training more stable, PPO limits the amount \nof changes possible per training step, minimizing the conse-\nquence of an inaccurate value-head (V(s)). Since the advan-\ntage term /uni0302.s1A t depends on what a model estimates how good \nan action is at some state (or Q(s,a)), it depends on the value \nfunction, which is an approximation made by the value head \nof the policy network. To make the training more stable, \none needs to limit the amount of change it can perform in a \nsingle update to prevent an initially inaccurate value head \nfrom misleading the policy network, by setting a maximum \nthreshold, for instance. This technique is called ‚Äúclipping.‚Äù \nKL divergence is a similar popular technique that can be \nused for limiting the amount of change compared to the old \npolicy in PPO training. Simply put, KL divergence takes in \nthe new and old policy distribution (the probability of taking \n(2)ObjectivePPO (ùúÉ) = ÃÇE t\n/bracketleft.s1rùúã (ùúÉ) ‚ãÖ ÃÇA t\n/bracketright.s1\naction a for all actions) and outputs a nonzero real number‚Äî\nthe larger the output the larger the difference. Formally, it \nis defined as in (3):\nwhere the P and Q distributions are the policies, and i is \nany action that the policy can take. In the drug designing \ncase, each action corresponds to generating some unique \ntoken IDs representing some information of the designed \nmolecule. The policies are the drug design LCMs.\nPPO training loop\nThe initial PPO policy network in this case is the supervised \nfinetuned drug design LCM. I employed the default KL-\ndivergence penalty to ensure my drug design model did not \ndeviate too much away from the original supervised finetun-\ning trained model.\nFor training, I trained the drug design LCM for 14 epochs, \nwith a learning rate set to 1.41. For this training, I used the \nsame training set that was used for supervised finetuning \nfrom Part 2, except, some of the molecules were not used for \ngeneration at all and were replaced with the start token for \ngenerating from scratch; molecules that were used only had \nthe first 2‚Äì8 tokens to guide the drug design model during \nPPO. This is a common technique in finetuning LLM when \nusing PPO. The training loop (Fig.¬†3) created can be broken \n(3)D KL (P/uni007C.var/uni007C.varQ)=\n/uni2211.s1\ni\nP(i)log\n/bracketleft.s3P(i)\nQ (i)\n/bracketright.s3\nFig. 3  Feedback loop of proximal policy optimization. The \nPPO training loop is summarized in the diagram. In a nutshell, \nthe designed molecules of my drug design model (shown as \n‚ÄúChemGPT‚Äù) are scored by the reward (evaluation) model. Each \nscore is then combined with the KL divergence for more stable \nmodel performance and to ensure model convergence. My initial drug \ndesign model served as a constraint to limit deviation of my updated \ndrug design model. The combined value is used to update the drug \ndesign model‚Äôs parameters by taking the gradient of the objective \n(with respect to /u1D703 , the parameters). Notice that the ‚Äú( +)‚Äù means com-\nbined since the reward model outputs a reward while the KL diver -\ngence outputs a penalty (-reward). This training loop is repeated mul-\ntiple times to maximize the reward received by the model\n Journal of Computer-Aided Molecular Design (2024) 38:20\n20 Page 8 of 15\ndown into three parts: action (generation), reward prediction, \nand optimization.\nDuring generation, I set the top p value to 1.0 (or 100%), \nencouraging the generator to explore more, rather than to \nonly exploit what it already knows to maximize reward. In \nother words, top p = 1.0 allows the drug design LCM to con-\nsider all possible actions, with better actions having higher \nprobability to be chosen. In contrast to greedy strategy (top \nk = 1), which chooses the token that it currently estimates is \nthe most optimal, this top p = 1 policy does not have early \nsuboptimal convergence, since one of the pitfalls of greedy \nstrategy is that the locally best choice of action isn‚Äôt always \nthe best choice overall. The drug design LCM generates a \nlist of token IDs sequences, each representing the SELFIES \nencoded sequence of the designed molecule.\nIn the second step, my code from the training loop \ndecoded these generated sequences of token IDs into \nSMILES sequences, which were then parsed and calculated \nusing RDKit for their chemical descriptors. With the original \ngenerated sequences padded and the chemical descriptors \nready, I used the trained QSAR evaluation model for esti-\nmating the reward received (the efficacy) for each designed \nmolecule.\nFinally, my training loop passed the input, the generated \ntoken IDs, and the rewards into its PPO train step function \nfor running the PPO algorithm for updating the policy net-\nwork (my drug design model). I repeated this 3-part process \nfor each batch from each epoch until the PPO training was \ncomplete.\nPPO evaluation\nTo evaluate the extent to which the post-PPO LCM designed \nhigh efficacy molecules, I graphed and compared a kernel \ndensity estimation distribution of drug efficacy  (pIC50) of \ngenerated molecules with the drug efficacy distribution of \nthe dataset molecules.\nResults & discussion\nOne of the main goals of this study was to transfer a LLM \ninto a large chemistry model (LCM) and adapt LCM for \ndrug design for the first time. The successful application of \nLLM (or LCM) and NLP training schemas in drug discovery \nin this study is novel, and the application of reinforcement \nlearning on LCM to drug design is a strategy that had not \nbeen investigated in previous studies. In the present study, I \nchose APP for the case study of drug design, and the same \nLCM can be transferred to target a new protein utilizing a \ndifferent dataset using the same methodology. This study \nused a three-step NLP strategy, which used a PPO algorithm \nwith my QSAR efficacy evaluation model to optimize the \ndrug efficacy of the molecules designed by my drug design \nLCM. Furthermore, this study employed an unexplored \napproach for QSAR using a combination of sequential and \nnumerical chemical descriptors, which achieved a better \nperformance. Then, the Exmol library was used as a way to \ninterpret my QSAR efficacy evaluation model. This utiliza-\ntion of Exmol library on pIC50 efficacy for ML drug design \nrepresents a previously unexplored approach.\nThe novel QSAR model outperforms \ntraditional QSAR models\nTo better model drug efficacy for generated molecules and \nto later use it for efficacy optimization, I devised and used \nthe QSAR evaluation model that takes a combination of \nsequential and chemical descriptor data for the first time. \nThe performance of my evaluation model on the experi-\nmental data was evaluated and compared with prior QSAR \nmodels (on the same dataset): the LSTM model from Abbasi \net¬†al.‚Äôs study [3 ] and the ECIF random forest model [23] \n(see Fig.¬†4). The result showed that the addition of chemical \ndescriptors increased the model performance (concordance \ncorrelation coefficient (CCC) of 0.91 vs. 0.79), being 2.34 \ntimes better in performance (by a 2.34 times smaller MSE \nloss) compared to previous study‚Äôs LSTM model.\nHaving a much more accurate reward (evaluation) model \nis important as it guides the generative drug design model in \nPPO training towards designing higher efficacy molecules by \nscoring. Thus, my evaluation model with SELFIES-encoded \nsequence input was chosen as the reward model. Although \nthe SELFIES-based and SMILES sequence-based models \nhad relatively similar performance, the SELFIES-based \nmodel can more easily interact with my drug design model, \nwhich generates molecules in SELFIES tokens. A possi-\nble explanation for this result is that with the addition of \nnumerical descriptors, my evaluation model no longer needs \nto rediscover how to estimate these descriptors or patterns \n(such as the amount of hydrogen bonding) themselves using \nthe sequence and can focus on other aspects of the structure \nprovided by the sequence. The results demonstrates that \nObjective 1 was achieved successfully: my QSAR evalua-\ntion models with combined sequence and numerical descrip-\ntor input indeed achieve better performance than traditional \nstructure-only QSAR models.\nThe increased performance in the evaluation model \nmakes computational drug discovery more reliable (for APP \nin this case study). The drug discovery process is accelerated \nby this more than twofold improvement in performance, as \na more accurate evaluation model reduces the failure rate of \ndesigned effective molecules and is therefore more likely to \nhold true during actual in-vitro experimental validations.\nJournal of Computer-Aided Molecular Design (2024) 38:20 \n Page 9 of 15 20\nNovel QSAR structure is¬†data‚Äëefficient\nTo explore how efficiently my QSAR evaluation model \nuses its data compared to previous models, another experi-\nment was conducted with different sizes of training sets.\nA size-difference of around 150 datapoints was used to \nestimate the performance of my final evaluation model over \neach dataset since enumerating all possible dataset sizes \ncan be time consuming. The result from Fig.¬† 5 implies that \nmy SELFIES-based evaluation model, using 70% less data, \nFig. 4  Performance of QSAR efficacy evaluation model exceeds \nprevious studies model performance. A‚ÄìC the performance of each \nmodel on the testing set over three different metrics. The models \ntested are Abbasi et¬† al.‚Äôs LSTM model [3], the ECIF random forest \nmodel [23], and my models trained with three different molecule rep-\nresentation formats. MSE, or mean squared error, is a metric to meas-\nure the overall difference between experimental and predicted  pIC50 \nvalues, punishing larger differences more due to the squared terms. \nThe green highlighted bars represent models from the present study, \nand the blue highlighted bars are from previous studies. D‚ÄìE com-\npares the performance of the QSAR model over the entire BindingDB \ndataset (on the left) vs. the performance of the ECIF random forest \nmodel. (on the right). The regression line on the right is much more \nflat (which means a weaker performance) than the regression line \non the left, which is consistent from findings of previous study that \nsequential QSAR LSTM models outperform previous QSAR models \n[3].  Ki is an alternative way of measuring efficacy, which was used \nsince the ECIF model outputs  pKi values‚Äîalthough efficacy was \nmeasured in different metrics, both are from the same BindingDB \ndataset\n Journal of Computer-Aided Molecular Design (2024) 38:20\n20 Page 10 of 15\nachieves similar performance compared to traditional LSTM \nQSAR regressors (250 dataset molecules vs. 825 molecules). \nThis implies that the addition of chemical descriptors does \nindeed decrease the amount of data required for the model \nto learn through the structural relationships. This also dem-\nonstrated that my evaluation model structure in the present \nstudy performs better even when information is more limited \n(fewer experimental data available), making it applicable in \nearlier stages of drug discovery, when the disease or the drug \ntarget is first discovered.\nNeural network QSAR model can become \nexplainable\nTo retrieve valuable information on what makes a mole-\ncule effective, my QSAR efficacy evaluation model must \nbe explainable. To attempt to explain the QSAR evaluation \nmodel, which is a neural network black box, the Exmol \npython library was used for efficacy for the first time: Exmol \nuses molecule descriptors and surrogate models to record \ncorrelation between molecule descriptor and predicted activ-\nity by querying different variants of a molecule (shown in \nFig.¬†6). The results show that having a tertiary carbon hin-\nders a molecule‚Äôs efficacy towards APP, so does an alkyne \ngroup, too many aromatic rings, too many methyl groups, or \nlargely separated nitrogen.\nPrevious studies had shown that having an excessive \namount of aromatic rings (more than 3) is negatively \ncorrelated with the druggability and developability of a \nmolecule, regardless of the target protein [24]. Similar to \nthe excessive number of methyl and alkynes groups, the \ntertiary carbon and separated nitrogen descriptors are more \nFig. 5  Novel QSAR model maintains outperformance as dataset size \ndecreased. Mean squared error (MSE) values are used in the figure \nas a metric for model performance, where the lower the MSE value, \nthe better the performance. The QSAR (evaluation) model designed \nin this study was trained with different sizes of datasets. The evalu-\nation dataset used remained unchanged throughout the experiment. \nThe rightmost three values are ones that are smaller than the MSE \nvalue for a traditional LSTM QSAR model [3] trained using the same \n825 molecules dataset. The loss of the QSAR model increases as the \ntraining dataset size decreases; however the QSAR model still per -\nformed better, until the dataset size became around 250\nFig. 6  Correlation between descriptor and efficacy by QSAR model \ndemonstrates explainability. The Exmol library [18] was used in \nattempts to make the QSAR model explainable. A molecule was \nsampled from the training set and different variants of the molecule \nwere produced by the Exmol and the relative increase or decrease in \npredicted efficacy was recorded. The sign of the t-value represents \nnegative or positive correlation, while the magnitude represents the \nstatistical significance. The yellow dotted line represent statistical sig-\nnificance threshold, and the red bars represent negative correlation. \nAll of the top 5 default molecule descriptors are statistically signifi-\ncant\nJournal of Computer-Aided Molecular Design (2024) 38:20 \n Page 11 of 15 20\ngeneral and no previous studies have proven or disproven \ntheir hindrance on APP efficacy of a molecule. One pos-\nsible explanation is that these correlations only apply to \nAPP only and do not generalize to protein‚Äìligand interac-\ntions overall. Future studies can investigate the potential \nof using more customized, more specific molecule descrip-\ntors with Exmol for more detailed information.\nAlthough more customized, specific molecule descrip-\ntors for APP can make one retrieve more specific informa-\ntion from these ‚Äúsneak peeks,‚Äù the results suggest that the \nexplainability of QSAR evaluation models can provide cred-\nible information for human chemists on what makes a ligand \neffective towards a drug target. This can accelerate chem -\nists‚Äô understanding of drug targets and effective ligands. In \naddition, future study can investigate the potential in the \ninformation derived from this approach to serve as addi-\ntional conditions or optimization goals for a drug design \nmodel‚Äîbreaking down an abstract metric into multiple \neasier goals can potentially increase a drug design model‚Äôs \nperformance. Having a better understanding of the essence \nwhich makes a molecule effective can be important since \nalthough a drug-like molecule can be potentially discarded \nin later stage, this information can be reused throughout the \nentire process: for instance, when optimizing the molecule \nat a later stage such as for lowering toxicity, chemists would \nknow what are essential for the efficacy and must not change \nwhile what can be optimized and adjusted.\nSupervised finetuning can help drug design \nLCM to¬†model drug‚Äëlike molecules\nTo better model the molecules from the dataset (to be ‚Äúdrug-\nlike‚Äù) before optimizing for any properties, this study used \nsupervised finetuning (SFT) for the LCM to model the mol-\necules from the same BindingDB dataset used for reward \nmodeling. Different configurations with different hyperpa-\nrameters were tried, and the best model successfully learned \nto generate similar molecules and the distribution of mol-\necules from the dataset (cross-entropy loss of 0.1253; with \nchunk preprocessing). Next, the property distribution of 168 \nsampled generated molecules was calculated and compared \nwith the dataset‚Äôs molecules.\nA kernel density estimation (KDE) distribution graph is a \nsmooth curve that estimates and interpolates the frequency \ndistribution of some continuous data from a set of recorded \ndata points. The KDE distribution plots (Fig.¬† 7) suggest \nthat my drug design model successfully modeled the vari-\nous property distributions of the molecules from the dataset, \nfurther supporting the small loss value.\nHaving the drug design LCM learn from the distribu-\ntion of molecules in the dataset is important: ChemGPT \nis a general pretrained model that has not been tuned for \ndrug-like or even organic molecules. In addition to helping \nthe drug design LCM better converge in the PPO optimiza-\ntion process, the fine-tuned drug design LCM serves as a \nbasis model, preventing the optimizing LCM from deviat-\ning and generating invalid and not drug-like molecules. My \ndrug design model‚Äôs successful convergence, as portrayed \nby the figure (Fig.¬† 7), further proved LCM‚Äôs ability to use \nCLM to model drug-like molecules in a supervised setting, \nas shown in [9].\nA potential implication of this is to use LCMs for learn-\ning or ‚Äúdeep-faking‚Äù patented drug molecules, which can \nbe used by a future study for discovering alternative, more \naccessible, and similar drug molecules such as for aiding \ndeveloping regions. This can be useful such as for tuberculo-\nsis, where effective drugs exist but are inaccessible in many \ndeveloping regions due to high cost.\nProximal policy optimization can optimize \nefficacy for¬†drug design LCM\nTo optimize the drug design LCM in generating molecules \nwith high drug efficacy and other properties (the novelty \nand  pIC50 are optimized as examples), the PPO algorithm \nwas employed using the reward and SFT-trained drug design \nmodels. The KDE distribution of drug efficacy  (pIC50) of \nmolecules after PPO was graphed and compared with the \ndrug efficacy distribution of the dataset (Fig.¬† 8). My drug \ndesign model‚Äôs ability to generate desired molecules is \nshown by a 15.49 times more effective mean efficacy value \n(in  IC50) in the generated set of molecules than in the dataset.\nA comparison to previous ML drug design models is \nshown (Table¬† 1). The present study‚Äôs GPT model outper -\nforms existing drug design models in designing effective, \nnovel, and chemically valid molecules. Samples of designed \nmolecules are also shown (Fig.¬† 9). Most of the generated \nmolecules exhibit desired properties such as high drug effi-\ncacy. Compared to the dataset (Fig.¬†8), on average the  pIC50 \nvalue of the generated molecules is higher by 1.19, and the \nvariance in  pIC50 is much less in the generated set than in \nthe training set. 99.2% of the molecules had  pIC50 > 7 in \nthe PPO training process. These results signify that NLP \ntechniques and training schemas can also be transferred \nto the realm of molecule property optimization and drug \ndesign. This opens the door for other techniques, such as \nreinforcement learning from human feedback, to be used \nfor LCMs. In addition, recent years have shown different \napproaches for multi-objective PPO algorithms applied to \ndifferent fields [25, 26]; these algorithms‚Äô implication on \nLCM can be further explored and compared to the com-\nmon and straightforward ‚Äúweighted sum‚Äù approach. Since \nthere are many factors that need to be optimized in the drug \ndevelopment process before a drug candidate becomes an \n Journal of Computer-Aided Molecular Design (2024) 38:20\n20 Page 12 of 15\napproved drug molecule, future studies can investigate the \naddition of optimizing for low toxicity or other properties \nusing similar techniques as PPO.\nThe fact that my drug design model can design mol-\necules with both high efficacy and 100% novelty signifies a \npromising way of using LCMs trained with NLP techniques \nand algorithms. In addition, notice when Open AI train their \nLLMs such as Instruct GPT and Chat GPT, reinforcement \nlearning from human feedback (RLHF) is often involved \n[29]. To apply this training strategy for drug design, chem-\nists can help, from rating, ranking, and prioritizing certain \ndesigned drug molecules, to designing better alternatives as \nexamples and provide direct feedback for the model. RLHF \nis shown to be one of the best ways to incorporate human \ninsights into a GPT model [30], which this strategy can \npotentially help the drug design model pick up chemists‚Äô \n‚Äúintuitions‚Äô‚Äô in designing drug molecules‚Äîpatterns that are \nhard to quantify and hard to use quantitative metric to meas-\nure and optimize [30]. This will be the first time chemists \ncan directly interact with the drug design model, ‚Äúcom-\nmunicating‚Äù their experiences to the drug design model, \nincreasing the model performance and potentially making \nmachine designed molecules indistinguishable from human \ndesigned ones. However, although this proposed RLHF \nstrategy of having chemists provide feedback to a large \nchemistry model to improve its performance is a promising \none, this requires the involvement of multiple chemists, and \nthe cost would be unattainable and thus infeasible for the \npresent study.\nConclusion\nA three-step LLM training process from Natural Language \nProcessing was successfully applied in the context of drug \ndesign for LCM. This is an unprecedented strategy for large \nchemistry models, consisting of reward modeling, super -\nvised finetuning, and proximal policy optimization (or RL \nFig. 7  Molecule property distribution of generated is similar to dis-\ntribution of dataset Molecules. 168 generated molecules‚Äô proper -\nties were calculated and graphed. Density is proportional to the \nfrequency, except the graph is normalized, thus sample size is not \nconsidered in the comparison. The graphs showed that most gener -\nated molecules do lie in the same approximate range as the drug can-\ndidates from the dataset in different molecule properties. The differ -\nence in the tails of the distributions can be attributed to the difference \nin the sample sizes: 168 vs. 1024\nJournal of Computer-Aided Molecular Design (2024) 38:20 \n Page 13 of 15 20\noptimization). To begin, in the reward modeling step, using \npublicly available experimental dataset from BindingDB \n[13] for protein-molecule interactions, and by combining \nsequential molecule representation with numerical chemical \ndescriptors, this strategy modeled drug efficacy 2.34 times \nbetter accuracy than previous sequential and molecule fin-\ngerprint or descriptor-based QSAR evaluation models. \nIn addition, the Exmol was used for the efficacy evalua-\ntion model for the first time and proved capable in adding \nexplainability to evaluation models, especially when given \ncustomized, specific molecule descriptors in future studies. \nThen, in the Supervised Finetuning step, my drug design \nmodel successfully learned to generate molecules similar to \nthose from the dataset. Finally, my evaluation model and my \ndrug design model were used in the PPO optimization loop, \nwhere the designed molecules are analyzed by my evalua-\ntion model for the drug designing LCM model to improve, \nwhere 99.2% of the designed molecules have high efficacy \n (pIC50 > 7) and all are valid and novel. This approach is the \nfirst time it is employed on a LCM. The ability for my drug \ndesign model in generating molecules with high efficacy and \nnovelty signifies that NLP techniques and training schemas \ncan also be transferred in the realm of molecule modeling \nand drug design.\nAlthough the GPT-Neo [21] with 19 million parameters \nwas used as the base model due to limitations in computa-\ntional resources in this study, the exact same methodology \ncan be used for finetuning larger base models with much \nmore parameters. The investigation of larger and more recent \nLLM such as LLama2-7B [19] (7 billion parameters) can be \nexplored in future studies, where ChemGPT‚Äôs [9] pretraining \nprocedure can be used for learning the SELFIES molecule \nrepresentation, and the methodology in this study can be \nused for applying the LCM to drug design. Implications and \nfuture studies on LCM on drug design include designing \nnovel developmental drug candidates satisfying multiple \nconstraints and making patented drugs accessible by gen -\nerating similar alternative molecules, all of which have the \npotential to transform drug discovery.\nAlthough this study focused on a case-study in gener -\nating highly effective molecules towards the APP target \nprotein, the same methodology can be applied to trans-\nfer LCM for targeting a different protein using a different \ndataset, facilitating and speeding up the discovery of more \npotential drug molecules for treating different diseases. \nThis LCM showed exciting potential for NLP techniques \nFig. 8  Average Drug Efficacy  (pIC50) of Generated Molecules \nExceeds Dataset Molecules. Two hundred and fifty-six molecules \nwere sampled and their drug efficacy calculated. Density is propor -\ntional to the frequency, except the graph is normalized, thus sam-\nple size is not considered in the comparison. The peak in the green \ndistribution curve shows that most generated molecules have a drug \nefficacy around 8, while the blue curve, which represents the dataset \nmolecules, has two smaller peaks, meaning most molecules had a \n pIC50 efficacy value of either around 5 or around 8\nTable 1  Comparising to previous drug design models\nPreivous state-of-the-art drug design models (i.e., models that design \ndrug-like molecules and optimizes for higher efficacy toward a cer -\ntain drug target) and their metrics of molecule efficacy, validity, and \nnovelty are shown. The best performance values for each metric are \nhighlighted in bold. Pereira et¬† al. (LSTM) [27] and Abbasi et¬† al. \n(GAN) [3] are two state-of-the-art drug design models. Popova et¬†al. \n(RNN) [28] has multiple configurations of the molecule design mod-\nels, the efficacy maximizing model is used\n*As the model from Pereira et¬†al. (LSTM) contain bugs and cannot be \ncurrently run without errors, the metrics are default values reported \nfrom the paper, and the efficacy toward APP is unknown\nMetrics Present \nstudy (%)\nPereira \net¬†al. \n(LSTM)\nAbbasi et¬†al. \n(GAN) (%)\nPopova \net¬†al. (RNN) \n(%)\n% High \nefficacy \n (pIC50 > 7)\n99.2 ‚Äì 59 42.2\n% Validity 100 91.8%* 62.3 42.6\n% Novelty 100 91.3%* 100 100\n Journal of Computer-Aided Molecular Design (2024) 38:20\n20 Page 14 of 15\n(and LLM in general) to be applied in drug design, an \nimperative stride towards progress in the realm of large \nscientific models and drug design.\nAcknowledgements The author would like to thank his MIT PRIMES \nmentor, Dr. Gil Alterovitz, for his invaluable support and feedback on \nthe research. The author also would like to thank Mr. Ilya Yashin for \nhis invaluable guidance in paper editing. Finally, this author would \nlike to thank the MIT PRIMES program for supporting this research \nopportunity by providing mentorship.\nAuthor contributions The sole author confirms the contributions for \nthe following: study conception and design, method, result collection \n& analysis, interpretation of results, and manuscript preparation.\nFunding This research did not receive any specific grant from funding \nagencies in the public, commercial, or not-for-profit sectors.\nData availability The author is currently unable to specify which data \nhas been produced. The sources and code will be available (upon \nrequest to yeeeyee004@gmail.com). Model will be available at https:// \nhuggi ngface. co/ Cocon ut104/ Effic acyGPT- DrugD esign\nDeclarations \nCompeting interests This study declares no competing interests.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article‚Äôs Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article‚Äôs Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nFig. 9  Sample of designed molecules. Sample of designed molecules \nare shown in the context of the property distributions of the dataset \nand the generated molecules. Both are novel molecules that do not \nexist in the dataset. The molecules are sampled post PPO, with a top \np = 1 sampling strategy\nJournal of Computer-Aided Molecular Design (2024) 38:20 \n Page 15 of 15 20\nReferences\n 1. DiMasi JA, Grabowski HG, Hansen RW (2016) Innovation in the \npharmaceutical industry: new estimates of R&D costs. J Health \nEcon 47:20‚Äì33. https:// doi. org/ 10. 1016/j. jheal eco. 2016. 01. 012\n 2. Tripathi S et¬†al (2022) Recent advances and application of gen-\nerative adversarial networks in drug discovery, development, and \ntargeting. Artif Intell Life Sci 2:100045. https:// doi. org/ 10. 1016/j. \nailsci. 2022. 100045\n 3. Abbasi M et¬†al (2022) Designing optimized drug candidates with \ngenerative adversarial network. J Cheminformatics 14(1):40. \nhttps:// doi. org/ 10. 1186/ s13321- 022- 00623-6\n 4. Weininger D (1988) SMILES, a chemical language and informa-\ntion system. 1. Introduction to methodology and encoding rules. J \nChem Inf Comput Sci 28(1):31‚Äì36. https:// doi. org/ 10. 1021/ ci000 \n57a005\n 5. Krenn M, H√§se F, Nigam A, Friederich P, Aspuru-Guzik A \n(2020) Self-referencing embedded strings (SELFIES): a 100% \nrobust molecular string representation. Mach Learn Sci Technol \n1(4):045024. https:// doi. org/ 10. 1088/ 2632- 2153/ aba947\n 6. Yasonik J (2020) Multiobjective de novo drug design with recur-\nrent neural networks and nondominated sorting. J Cheminformat-\nics 12(1):14. https:// doi. org/ 10. 1186/ s13321- 020- 00419-6\n 7. Gao K, Nguyen DD, Tu M, Wei G-W (2020) Generative network \ncomplex for the automated generation of drug-like molecules. J \nChem Inf Model 60(12):5682‚Äì5698. https:// doi. org/ 10. 1021/ acs. \njcim. 0c005 99\n 8. Born J, Manica M (2023) Regression transformer enables con-\ncurrent sequence regression and generation for molecular lan -\nguage modelling. Nat Mach Intell. https:// doi. org/ 10. 1038/  \ns42256- 023- 00639-z\n 9. Frey N et¬†al (2022) Neural scaling of deep chemical models. \nChemRxiv. https:// doi. org/ 10. 26434/ chemr xiv- 2022- 3s512\n 10. Yang S et¬†al (2012) A peptide binding to the Œ≤-site of APP \nimproves spatial memory and attenuates AŒ≤ burden in Alzhei-\nmer‚Äôs disease transgenic mice. PLoS ONE 7(11):e48540. https:// \ndoi. org/ 10. 1371/ journ al. pone. 00485 40\n 11. Zhao J, Liu X, Xia W, Zhang Y, Wang C (2020) Targeting amy -\nloidogenic processing of APP in Alzheimer‚Äôs disease. Front Mol \nNeurosci 13:137. https:// doi. org/ 10. 3389/ fnmol. 2020. 00137\n 12. Brown T B et¬†al. (2020) Language models are few-shot learners. \nArXiv, 2020. doi: https:// doi. org/ 10. 48550/ arXiv. 2005. 14165.\n 13. Liu T, Lin Y, Wen X, Jorissen RN, Gilson MK (2007) BindingDB: \na web-accessible database of experimentally determined protein‚Äì\nligand binding affinities. Nucleic Acids Res. https:// doi. org/ 10.  \n1093/ nar/ gkl999\n 14. Jaeger S, Fulle S, Turk S (2018) Mol2vec: unsupervised machine \nlearning approach with chemical intuition. J Chem Inf Model \n58(1):27‚Äì35. https:// doi. org/ 10. 1021/ acs. jcim. 7b006 16\n 15. Kim S et¬†al (2023) PubChem 2023 update. Nucleic Acids Res \n51(D1):D1373‚ÄìD1380. https:// doi. org/ 10. 1093/ nar/ gkac9 56\n 16. Landrum G et¬† al (2023) rdkit/rdkit: 2023_03_2 (Q1 2023) \nRelease. Zenodo. https:// doi. org/ 10. 5281/ zenodo. 80538 10\n 17. Kaneko H (2023) Molecular descriptors, structure generation, \nand inverse QSAR/QSPR based on SELFIES. ACS Omega \n8(24):21781‚Äì21786. https:// doi. org/ 10. 1021/ acsom ega. 3c013 32\n 18. Gandhi HA, White AD (2022) Explaining molecular properties \nwith natural language. Chemistry. https:// doi. org/ 10. 26434/ chemr \nxiv- 2022- v5p6m- v3\n 19. Touvron H et¬†al. (2023) Llama 2: open foundation and fine-tuned \nchat models. arXiv, 2023. doi: https:// doi. org/ 10. 48550/ arXiv. \n2307. 09288.\n 20. Almazrouei E et¬†al. (2023) The Falcon Series of Open Language \nModels. arXiv, 2023. doi: https:// doi. org/ 10. 48550/ arXiv. 2311. \n16867.\n 21. Black S, Leo G, Wang P, Leahy C, Biderman S (2021) GPT-neo: \nlarge scale autoregressive language modeling with mesh-tensor -\nflow. Zenodo. https:// doi. org/ 10. 5281/ zenodo. 52977 15\n 22. Akiba T, Sano S, Yanase T, Ohta T and Koyama M (2019) Optuna: \na next-generation hyperparameter optimization framework. In: \nProceedings of the 25th ACM SIGKDD International Confer -\nence on Knowledge Discovery & Data Mining, in KDD ‚Äò19. New \nYork, Association for Computing Machinery, pp. 2623‚Äì2631. doi: \nhttps:// doi. org/ 10. 1145/ 32925 00. 33307 01.\n 23. S√°nchez-Cruz N, Medina-Franco JL, Mestres J, Barril X (2021) \nExtended connectivity interaction features: improving binding \naffinity prediction through chemical description. Bioinforma Oxf \nEngl 37(10):1376‚Äì1382. https:// doi. org/ 10. 1093/ bioin forma tics/ \nbtaa9 82\n 24. Ritchie TJ, Macdonald SJF (2009) The impact of aromatic ring \ncount on compound developability‚Äìare too many aromatic rings \na liability in drug design? Drug Discov Today 14(21‚Äì22):1011‚Äì\n1020. https:// doi. org/ 10. 1016/j. drudis. 2009. 07. 014\n 25. Khoi ND, Van CP, Tran HV, Truong CD (2020) Multi-objective \nexploration for proximal policy optimization. In: 2020 Applying \nNew Technology in Green Buildings (ATiGB). doi: https:// doi. \norg/ 10. 1109/ ATiGB 50996. 2021. 94233 19.\n 26. Koeberle Y, Sabatini S, Tsishkou D and Sabourin C (2022) \nExploring the trade off between human driving imitation and \nsafety for traffic simulation. In: 2022 IEEE 25th International \nConference on Intelligent Transportation Systems (ITSC). pp. \n779‚Äì786. doi: https:// doi. org/ 10. 1109/ ITSC5 5140. 2022. 99223 47.\n 27. Pereira TO, Abbasi M, Oliveira RI, Guedes RA, Salvador JAR, \nArrais JP (2023) Artificial intelligence for prediction of biological \nactivities and generation of molecular hits using stereochemical \ninformation. J Comput Aided Mol Des 37(12):791‚Äì806. https://  \ndoi. org/ 10. 1007/ s10822- 023- 00539-9\n 28. Popova M, Isayev O, Tropsha A (2018) Deep reinforcement learn-\ning for de-novo drug design. Sci Adv 4(7):eaap7885. https:// doi. \norg/ 10. 1126/ sciadv. aap78 85\n 29. Christiano P, Leike J, Brown TB, Martic M, Legg S and Amodei \nD (2023) Deep reinforcement learning from human preferences. \narXiv. doi: https:// doi. org/ 10. 48550/ arXiv. 1706. 03741.\n 30. Ouyang L et¬†al. (2022) Training language models to follow \ninstructions with human feedback. arXiv. doi: https:// doi. org/ 10. \n48550/ arXiv. 2203. 02155.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7021470665931702
    },
    {
      "name": "Reinforcement learning",
      "score": 0.6932797431945801
    },
    {
      "name": "Machine learning",
      "score": 0.5358802676200867
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5271527171134949
    },
    {
      "name": "Drug discovery",
      "score": 0.5032557845115662
    },
    {
      "name": "Language model",
      "score": 0.499436616897583
    },
    {
      "name": "Bioinformatics",
      "score": 0.1885630190372467
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799496773",
      "name": "University Prep",
      "country": "US"
    }
  ],
  "cited_by": 13
}