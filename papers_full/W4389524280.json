{
  "title": "Making Large Language Models Better Data Creators",
  "url": "https://openalex.org/W4389524280",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2133699502",
      "name": "Dong-Ho Lee",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A704051514",
      "name": "Jay Pujara",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2223714952",
      "name": "Mohit Sewak",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A4222579952",
      "name": "Ryen White",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": null,
      "name": "Sujay Jauhar",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4382318449",
    "https://openalex.org/W4205737716",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W3175270222",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W4281790610",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3194676777",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4225948283",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W4281483047"
  ],
  "abstract": "Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15349‚Äì15360\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nMaking Large Language Models Better Data Creators\nDong-Ho Lee1‚àó, Jay Pujara1, Mohit Sewak2, Ryen W. White2, Sujay Kumar Jauhar2\n1Information Sciences Institute, University of Southern California\n2Microsoft Research\n{dongho.lee}@usc.edu, {jpujara}@isi.edu, {mohit.sewak,ryenw,sjauhar}@microsoft.com\nAbstract\nAlthough large language models (LLMs) have\nadvanced the state-of-the-art in NLP signifi-\ncantly, deploying them for downstream appli-\ncations is still challenging due to cost, respon-\nsiveness, control, or concerns around privacy\nand security. As such, trainable models are\nstill the preferred option in some cases. How-\never, these models still require human-labeled\ndata for optimal performance, which is ex-\npensive and time-consuming to obtain. In or-\nder to address this issue, several techniques\nto reduce human effort involve labeling or\ngenerating data using LLMs. Although these\nmethods are effective for certain applications,\nin practice they encounter difficulties in real-\nworld scenarios. Labeling data requires care-\nful data selection, while generating data ne-\ncessitates task-specific prompt engineering. In\nthis paper, we propose a unified data creation\npipeline that requires only a single formatting\nexample, and which is applicable to a broad\nrange of tasks, including traditionally prob-\nlematic ones with semantically devoid label\nspaces. In our experiments we demonstrate that\ninstruction-following LLMs are highly cost-\neffective data creators, and that models trained\nwith these data exhibit performance better than\nthose trained with human-labeled data (by up to\n17.5%) on out-of-distribution evaluation, while\nmaintaining comparable performance on in-\ndistribution tasks. These results have important\nimplications for the robustness of NLP systems\ndeployed in the real-world.\n1 Introduction\nLarge language models (LLMs) have revolution-\nized the field of NLP, yielding impressive perfor-\nmance on various conventional natural language\nunderstanding (NLU) and generation (NLG) tasks.\nThey are able to do this with only a handful ( i.e.,\nfew-shot) or sometimes even no training examples\n(i.e., zero-shot) (Brown et al., 2020; Du et al., 2022;\n‚àóWork done during Microsoft Research Internship.\n(a) GPT as Labeler\n(b) GPT as Generator\n(c) GPT with Single-formatting Example\nHas the UK been hit by a hurricane?\nUnlabeledInstances LLMInput YesOutput\nLLMYesOutput\nGenerate ‚ÄòInput‚Äô where the label is yesHas the UK been hit by a hurricane?Input\nLLM\nQuestion: Has the UK been hit by a hurricane?Option: [yes, no]Answer: yesInput / Output(Single-formatting Example)\nQuestion: Does France have a Prime Minister?Option: [yes, no]Answer: yes\nQuestion: Have the San Jose Sharks won a Stanley Cup?Option: [yes, no]Answer: yes ‚Ä¶Following ‚Äòexample‚Äô to generate diverse examples\nFigure 1: Existing LLM-based data augmentation needs\nunlabeled examples (labeler) or label-specific prompts\n(generator), while our framework generates examples\nfor a variety of tasks in a unified way.\nRae et al., 2021; Thoppilan et al., 2022; Chowdhery\net al., 2022). However, despite their effectiveness,\nthere is a continued demand for the deployment of\nsmaller trainable or tunable models in real-world\nscenarios due to cost constraints, existing service-\nlevel agreement response times, or privacy and\nsecurity concerns around using black-box APIs.\nUnfortunately, application-specific custom models\nsometimes require large amounts of high-quality\nhuman-labeled data, in order to perform well. Thus,\nin order to reduce time and cost in the model de-\nployment cycle, recent work has focused on trying\nto obtain training data by leveraging LLMs as ei-\nther labelers to annotate unlabeled data (Yoo et al.,\n2021; Wang et al., 2021a; Lang et al., 2022), or\ngenerators to generate new data samples (Meng\net al., 2022; Ye et al., 2022; Gao et al., 2022).\nDespite initial successes, constraints for these\ntechniques continue to hinder their applicability in\nbroader real-world settings. First, in the context of\nusing LLMs as labelers, it is essential to have raw\n15349\ndata that closely resembles the distribution of data\nin the predictive task. Most previous research has\nassumed access to a training dataset from which\nthe labels are elided; however, for cold-start prob-\nlems in the real-world, no such assumptions can be\nmade. Curating raw data for tasks in specialized\ndomains, such as those in the biomedical or legal\nfields, can be particularly challenging. Conversely,\nsampling a large volume of data at random can re-\nsult in an imbalanced label distribution due to rare\nevents (Markov et al., 2022).\nMeanwhile, leveraging LLMs as generators\nrequires careful curation of few-shot exam-\nples (Hartvigsen et al., 2022), or composition of\nprompts that highlight the semantic meaning of la-\nbels (Wang et al., 2021b; Meng et al., 2022; Ye\net al., 2022; Gao et al., 2022), such as positive v.\nnegative in sentiment classification. The latter has\nbeen a bottleneck to the broader applicability of\nLLMs as generators, however, since not all tasks\nhave labels that are semantically meaningful, or are\nenumerable. Consider, for example the label yes\nv. no, which have no meaning when taken without\ncontext; or the options of a multiple choice QA\n(see Figure 1), which are an effectively open-ended\nlabel-set that varies from instance to instance. For\nthese kinds of problems LLMs as generators con-\ntinue to be inadequate.\nIn this paper, we first present a formal frame-\nwork for characterizing different approaches for\nLLM data creation. Specifically, we use graphical\nmodels as a way to characterize and unify disparate\napproaches that include LLMs as either labelers or\ngenerators (Section 2). Next, we propose a novel\ndata creation pipeline that only requires a single for-\nmatting example to generate heterogeneous labeled\ndata for various downstream applications, includ-\ning those that focus on specialized domains. In con-\ntrast to current methods that require dataset-specific\ncomponents (e.g., label description, example selec-\ntion), our pipeline serves as a unified solution that\ncan be applied to a wide range of tasks, includ-\ning those where the label set is either semantically\ndevoid of meaning, or unenumerable.\nOur data creation pipeline leverages an\ninstruction-following LLM as a generator in con-\njunction with a single formatting example as a\nsimple yet effective way of imposing structured\nconstraints. Specifically, our approach iteratively\nconditions the generator on an instruction and a\nunique formatting example in a JSON format to\nyield multiple examples that vary in content but are\nformatted uniformly (Section 3.1 ‚àí3.2). Further-\nmore, as an efficient means of diversifying the gen-\nerated data, we propose a ‚Äúself-reference‚Äù strategy,\nwhich iteratively samples from the pool of newly\ncreated examples to seed the prompt for the next\nround of generation (Section 3.4). Specifically, we\noutline 4 distinct instantiations of ‚Äúself-reference‚Äù\nincluding random, contrastive, similar, and tree\nsampling for controlled diversification of data.\nWe evaluate our data creation pipeline on a bat-\ntery of tests involving three distinct types of tasks,\nnamely multiple-choice question answering (QA),\nopen-book yes/no QA, and closed-book yes/no QA.\nThe datasets for these tasks range across a variety\nof domains, including specialized ones such as the\nbiomedical domain. Furthermore, for each cate-\ngory of task, we use a minimum of two datasets\nin order to compare the out-of-distribution (OOD)\ngeneralization of models using original data to syn-\nthetically generated LLM data. Our results demon-\nstrate that leveraging LLMs as generators using our\nformatting-based creation approach is a highly cost-\neffective way of creating data that can be effectively\nused to train models for a variety of downstream\ntasks, including those in specialized domains, and\nones where labels are devoid of semantic meaning\nor vary across the data. For in-distribution (ID)\nsettings, naturally having access to large amounts\nof high-quality manually curated and labeled data\nis still ideal. However, when only a small amount\nof human-labeled data is available, our approach\nyields results that are often comparable, and some-\ntimes even better than the original datasets. This\nhighlights the potential role LLMs can play in the\nmodel development cycle, especially in resource-\npoor or specialized domains. Further, for the OOD\nsettings, models trained on data generated by our\npipeline consistently, and by large margins, outper-\nform their counterparts trained on data from human\nsources. This robustness and generalizability has\nimportant implications for the deployment of real-\nworld systems that deal with data that are variable,\nchaotic and often very different from curated aca-\ndemic datasets. We are realeasing our code and\nprompts to the community to spur future research\nin the area1.\n1https://github.com/microsoft/llm-data-creation\n15350\n2 Formalization of LLM-based data\ncreation\nIn this section, we attempt to draw different data\ncreation strategies using LLMs into a unified frame-\nwork, and discuss related research using this frame-\nwork.\n2.1 LLM-based data creation\nAssume a large language model M(e.g., GPT-\n3) that has been pre-trained to maximize the like-\nlihood of generating each token in a sequence\nx = [x1, x2, . . . , xn] by conditioning on previous\ntokens. Then, Mis capable of generating new text\nthrough recursive sampling of tokens from its out-\nput probability distribution. Given such a model\nMand label space Y, the goal of data creation is\nto induce samples (x, y) where y ‚ààY. Based on\ndifferent instantiations of this general framework,\nother inputs may be included, such as a label de-\nscriptive prompt Wy for each y ‚ààY, in-domain\nunlabeled example xu ‚ààDU , or a small number\nof example pairs (xl, yl) ‚ààDL along with their\ncorresponding explanation el.\n2.2 Formal Framework and Related Work\nGiven these basic components, there are two broad\nstrategies for LLM data creation, namely using an\nLLM as a labeler or as generator. Graphical mod-\nels for each of these two strategies is presented\nin Figure 2 to summarize the conditional interac-\ntions and independence assumptions that describe\nthe relationships between common framework con-\nstituents. The rest of this section discusses existing\nwork using the unified language of these graphical\nmodels.\nUsing LLMs as labelers. Mcan be used as a\nlabeler for unlabeled data (See Figure 2 (a)). Here,\napproaches assume that unlabeled data DU is pro-\nvided as input and the distribution of DU is similar\nto the distribution for the target task. Labeling\ncan be achieved either by conditioning Mon a\nfew labeled examples (xl, yl) ‚ààDL (Brown et al.,\n2020; Yoo et al., 2021; Wang et al., 2021a; Lang\net al., 2022), or by leveraging instructive prompts\nW describing the task without any labeled exam-\nples (Brown et al., 2020). When providing the\nmodel Mwith a small number of labeled examples\n(xl, yl) ‚ààDL, ‚Äì often referred to as the few-shot\nsetting (and contrasted with the zero-shot setting,\nwhere no examples are provided) ‚Äì recent studies\nùëÄùíô! ùíö\n(ùíô\",ùíö\")(a) Labeler‚ààùê∑#\n‚ààùê∑$\nùíÜ\"\nùëÄùíö\nùíô!\n(b) Generator\n‚ààùê∑$ùëäùíö\nùíô\nFigure 2: Graphical models for using LLM Mas\n(a) labeler which outputs label y for unlabeled data\nxu ‚àà DU using instructive prompt Wy or few-shot\nexamples (xl, yl) ‚ààDL with or without explanation el,\nand as (b) generatorwhich generates multiple data x\nfor label y with label-descriptive prompt Wor using\nin-domain unlabeled example xu ‚ààDU .\nhave shown that curating diverse and representa-\ntive samples is critical to the ability of the model\nto label new samples (Liu et al., 2022; Rubin et al.,\n2022; Su et al., 2022). This can be challenging,\nparticularly in specialized domains such as, for ex-\nample, the legal or biomedical domain ‚Äì where\nonly a small number of samples may be curated.\nOur paper proposes a pipeline (Section 3 capable of\ntackling these challenges, and is particularly useful\nin resource-poor domains (Section 5.1). Further-\nmore, providing intermediate reasoning steps (i.e.,\nchains-of-thought) as explanations el in prompts\nenables better labeling in both few-shot (Wei et al.,\n2022; Zhou et al., 2022; Lampinen et al., 2022) and\nzero-shot setting (Kojima et al., 2022).\nUsing LLMs as generators. An altogether dif-\nferent data creation approach uses the LLM M\ndirectly as generator (See Figure 2 (b)). While\na labeler predicts y by conditioning on an input\nx, a generator does the reverse by generating x\ngiven y. However, like with LLMs as labelers, a\nsmall number of relevant samples can be used to\ncondition Mfor generating data. Hartvigsen et al.\n(2022) feeds human-curated examples of the target\nlabel (e.g., implicit hate speech) intoMto generate\nhuman-like examples for the target label. In con-\ntrast, Wang et al. (2021b) conditions Mon both\nin-domain unlabeled examples and target label y\nto generate domain-related data for y. Meanwhile\na number of different efforts (Meng et al., 2022;\n15351\nùëÄùëäùë∞ (ùíô,ùíö)\n(ùíô\",ùíö\")Ours\nFigure 3: Framework Overviewof example-based data\ncreation which outputs multiple complete data (x, y)\nstarting with an initial seed formatting example(xf , yf )\nand the instruction WI.\nYe et al., 2022; Gao et al., 2022) condition Mon\nin-domain unlabeled example and well-formatted\ndescriptive prompts Wy for target label y to gen-\nerate data. One important caveat with all these\napproaches is that the label y, upon which outputs\nare conditioned, needs to be inherently meaningful\nin order for instructions to be formulated in a way\nthat prompt the model Minto generating coherent\noutputs. For example, when y is an entailement\nrelationship, a corresponding prompt Wy might\ninclude ‚Äúxu ‚ààDU . In other words... ‚Äù; or when\ny is the sentiment of a movie review, Wy might\ninclude ‚ÄúThe movie review is...‚Äù. Contrast this with\nthe scenario where y is an index or binary response,\nwhich has no semantic meaning without context\nand is therefore difficult to condition on. In this pa-\nper, we devise a unified approach to tackling these\nscenarios (Section 3), yielding a method for data\ncreation that is broadly applicable.\nDifferences with Instruction-following Data\nGeneration. Recent studies use Mto create data\nfor training instruction-following models (Wang\net al., 2023b; Taori et al., 2023; Xu et al., 2023;\nChiang et al., 2023; Mukherjee et al., 2023). Such\nstudies leverage Mas a labeler, producing a coher-\nent response y for a given instruction x. The main\nfocus is on covering a wide range of inputs x and\ntheir corresponding responses y to encompass the\ndiversity of instructions potentially encountered in\nreal-world interactions with users. While recent\nstudies have indicated that such models trained\nwith auto-generated instruction-response pairs can\nyield logical and coherent responses to user instruc-\ntions, their performance on NLU tasks remains\nsub-par (Wang et al., 2023a). Meanwhile, the focus\nof our work is the generation of data specifically\ntailored for natural language understanding (NLU)\ntasks with a focus on accurate responses.\nInstruction\n- You are creating {number_of_examples} examples that\nfollow the format of the example provided,\nbut with a different content.\n- The created examples **must** all have different answers.\n- The output **must** be in unnumbered JSON format.\n- [fixed_only] The created examples **must** have\nthe same options as the provided example.\nTable 1: Instruction WI used in the paper.\n3 Example-based Data Creation\nThis paper proposes a unified data creation ap-\nproach using LLMs, which does not need in-\ndomain unlabeled examples xu ‚àà DU or data-\nspecific label-descriptive prompts Wy. As illus-\ntrated in Figure 3, our framework iteratively cre-\nates data DG beginning with a single initial for-\nmatting example (xf , yf ) and an instruction WI\n(Section 3.1). The process begins by converting\n(xf , yf ) into a structured prompt Wf (Section 3.2-\n3.3). After conditioning Mon [WI; Wf ] to gener-\nate data, we sample an instance from the pool of\nnewly created data to serve as formatting example\nfor the next iteration (Section 3.4). We continue\nto iterate in this manner until the required number\nof instances k is obtained, after discarding dupli-\ncate and ill-formatted data. This is done by caching\ndata and checking newly created candidates against\npreviously generated ones for duplicates, and by\nusing the python json.loads() method for veri-\nfying the validity of the json output. The resulting\ndata creation pipeline is generally suitable for most\nclassification tasks. Although, in this paper we\nspecifically focus on tasks where the label set is\npotentially open-ended (e.g., multiple-choice QA),\nor lacks inherent semantic meaning ( e.g., binary\nQA) ‚Äì problem spaces that have posed challenges\nto past work in LLM data creation.\n3.1 Instruction\nThe goal of our framework is to have the model\nM generate a diverse set of examples in the\nsame format as the input formatting example\n(xf , yf ). To ensure format consistency and ex-\nample diversity, we use the system instruction\nWI in Table 1. We generate data in batches of\n{number_of_examples}, not only to account for\nthe token generation limits of LLMs, but also to\nencourage content diversity through subsequent\nsampling of Wf (Sec 3.4). In this paper, we set\n{number_of_examples} to 5 and do not vary it.\nIn order to mitigate label bias, we encourage mod-\n15352\nels to strive for maximal variance in their generated\nresponses, avoiding repetitions in data where the\nanswer is consistently ‚Äúyes‚Äù, for example.\n3.2 Formatting Example\nThe only assumed input to our example-based data\ncreation pipeline is a single formatting example\n(xf , yf ) and its corresponding label space Y. This\nexample is formatted as a JSON-structured prompt\nWf as shown in Figure 4. Given the one-shot JSON\nstructured format prompt, it is expected that the\nmodel yields a syntactically correct output that con-\nforms to the JSON schema. While generating a\ncomplex structured output like JSON can be chal-\nlenging, its easy parsing acts as a way to validate\noutputs at creation time and for effective usage in\ntraining downstream models.\n3.3 Structure of Formatting Example\nRecall that our focus in this paper is on data cre-\nation for tasks that are challenging because their\noutput label space is open-ended, or because they\nlack inherent semantic meaning. We refer to these\ndistinct settings using the shorthand variable and\nfixed, and note that the input formatting example\n(xf , yf ) is different for each of these label space\nsettings. Specifically, the major difference is the\norder of presentation of prompt components.\nVariable Option. The variable option format is\nstructured in a logical sequence beginning with the\nquestion xf , followed by a list of answer candidates\nY, and finally the correct answer yf .\nFixed Option. In contrast, for the variable op-\ntion, the expected format consists of the answer\ncandidates Yfirst, followed by the correct answer\nyf , and finally the question xf . This inversion\nof prompt components is added to ensure that the\nauto-regressive model creates questions with pre-\ndetermined optionssince the model, as a free gen-\nerator, can produce inconsistent output, resulting\nin answer options Ythat do not belong to the fixed\npre-determined set.\n3.4 Self-Reference\nRelying on a single formatting example (xf , yf )\nas a reference point for all iterations of data cre-\nation may limit the ability of the pipeline to yield\ndata that is broad coverage, diverse and balanced.\nTo overcome this, we propose ‚Äúself-reference‚Äù,\nwherein the formatting example fi = (xfi, yfi) for\nall subsequent generation steps i >0 are sampled\n{‚ÄúOptions‚Äù: [‚Äúyes‚Äù, ‚Äúno‚Äù, ‚Äúmaybe‚Äù],‚ÄúAnswer‚Äù: ‚Äúyes‚Äù,‚ÄúQuestion‚Äù: ‚ÄúIs batman and robin a sequel to batman   forever?‚Äù,‚ÄúContext‚Äù: ‚ÄúWith the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. ‚Ä¶‚Äù}\n{\"Question\": \"I am black when you buy me, red when you use me. When I turn white, you know it's time to throw me away. What am I?\",\"Options\": [\"charcoal\", \"rose flower\", \"ink\", \"fruit\", \"shoe\"],\"Answer\": \"charcoal\"} (a) Variant (multiple-choice QA)\n(b) Fixed (yes-no QA)\nFigure 4: Example of formatting example prompt\nWf where ‚Äúoptions‚Äù contain the label space Yof the\ntask, ‚Äúanswer‚Äù contains yf and ‚Äúquestion‚Äù contains xf .\n‚ÄúContext‚Äù is optional and depends on the task.\nfrom the outputs (xgi‚àí1 , ygi‚àí1 ) ‚ààDGi‚àí1 generated\nat iteration i‚àí1. We experiment with four different\nsampling strategies.\nRandom selection. During each iteration, a for-\nmatting example for the next step is randomly cho-\nsen from the output of the current step.\nContrastive selection. For each iteration, we se-\nlect the example that displays the greatest semantic\ncontrast to the preceding formatting example. In\nthis approach, we use a pre-trained bidirectional-\nencoder (Reimers and Gurevych, 2019) to generate\nembeddings for examples, and compute the cosine\nsimilarity between xf and xgi‚àí1 , selecting the in-\nstance xgi‚àí1 with the lowest similarity.\nSimilar selection. This sampling approach\nworks analogously to Contrastive selection, except\nthat instead of selecting the xgi‚àí1 with the lowest\ncosine similarity to xf , we select the one with the\nhighest similarity.\nTree selection. Iterative sampling of data may\nresult in significant domain drift from the first seed\nexample to data generated in later steps of the gen-\neration pipeline, due to unexpected content varia-\ntions produced by the model. To avoid this issue,\nwe use all the generated outputs from one step\nas formatting examples for subsequent iterations.\nThis approach can be viewed as a breadth-first tree\ntraversal over generated examples, and is in con-\ntrast with the other three sampling approaches that\nuse a depth-first exploration strategy. Our hypoth-\n15353\nesis is that the minimum height of the exploration\ntree yields samples that are more topically coher-\nent.\n4 Experimental Setup\nIn this section we describe the experimental setup\nthat we use to evaluate our single-shot example-\nbased data creation framework.\n4.1 Datasets\nWe evaluate on three types of different tasks:\nmultiple-choice question answering (QA), open-\nbook yes/no QA, and closed-book yes/no QA ‚Äì as\nshown in Table 2. Multiple-choice QA is used\nevaluate our data creation pipeline in a variable\nlabel space setting, while the other tasks are used\nfor fixed label space settings. In order to demon-\nstrate the domain generalization capability of our\napproach, we additionally use a minimum of two\ndatasets for each category of task. The datasets\nrange broadly in the reasoning abilities they de-\nmand from models, requiring them to solve diverse\nproblems such as filling in the blank in a sentence\n(PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi\net al., 2021)), choosing the most suitable option\namong multiple choices (CommonsenseQA (Tal-\nmor et al., 2019), RiddleSense (Lin et al., 2021)),\ncomprehending a given passage to make a pre-\ndiction (BoolQ with context (Clark et al., 2019),\nPubMedQA (Jin et al., 2019), BioASQ (Tsatsa-\nronis et al., 2015)), and answering based on in-\nherent knowledge (BoolQ without context (Clark\net al., 2019), StrategyQA (Geva et al., 2021),\nCREAK (Onoe et al., 2021)). Details of the various\ndatasets are presented in Appendix A.1.\n4.2 Evaluation Details\nIn order to demonstrate the efficacy of our data\ncreation framework, we present a comparison of\na downstream model when it is trained on (1) the\noriginal train dataset denoted by DL; and (2) an\nLLM created dataset, denoted by DG, where a\nsingle seed formatting example is randomly se-\nlected from DL. We are unable to conduct a\ncomparative analysis with other LLM-based data\ngeneration methods as they do not provide solu-\ntions or prompts engineered for the tasks listed\nin Table 2. The base model used in this paper is\nRoBERTa-large (Liu et al., 2019).\nTask Label Space Domain Dataset\nmultiple-choice QA Variant (2) CommonsensePIQAWinogrande\nmultiple-choice QA Variant (5) CommonsenseCommonsenseQARiddleSense\nopen-book yes/no Fixed (2)Knowledge BoolQ (w/ context)Biomedical PubMedQABiomedical BioASQ\nclosed-book yes/no Fixed (3)Knowledge BoolQ (w/o context)Knowledge StrategyQAKnowledge CREAK\nTable 2: Datasets used in the paper.The numbers\nenclosed in parentheses indicate the number of labels\nwithin the label space.\n4.3 Implementation Details\nThroughout the entire process of data creation, we\nuse gpt-3.5-turbo language model, as of June\n2023, with specific settings of temperature and\ntopp set to 1. When conducting fine-tuning experi-\nments, we leverage the Adam optimizer (Kingma\nand Ba, 2014) with a maximum sequence length of\n256. In each experiment, we perform a grid search\non development data for the optimal learning rate\nin [3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 5e-6, 3e-6, 1e-6,\n5e-7], and batch size in [4, 8, 16]. All experiments\nare conducted on an RTX A5000 with FP32.\n5 Experimental Results\nWe conduct a comprehensive set of experiments\nthat seek to evaluate the effectiveness of data cre-\nated by LLMs for focused downstream applica-\ntion modeling. Our first evaluation involves exam-\nining the performance of models trained on data\ngenerated by variants of our single-shot data cre-\nation pipeline and comparing them against man-\nually curated training data. We investigate both\nin-distributed (ID) and out-of-distribution test data\nsettings, each of which are explained in greater\ndetail below.\n5.1 Performance Comparison\nID Performance. In the ID setting the test data\nis drawn from the same distribution as the train-\ning data; specifically, a portion of the full human-\nsourced dataset is held out as a test set to evaluate\nmodels trained on either human-labeled or LLM\ncreated data. Table 3 summarizes the performance\nof models trained on two types of datasets: the orig-\ninal dataset, denoted by DL, and datasets created\nusing the different ‚Äúself-reference‚Äù variants of our\nsingle-shot pipeline ‚Äì the corresponding rows in\nthe table marked as DG. In both settings, and for a\n15354\nMCQA (2) MCQA (5) Open Yes/No Closed Yes/No\nTrained on‚Üì PIQA WinoGrande CommonsenseQA RiddleSense BoolQ PubMedQA BioASQ BoolQ StrategyQA CREAK\n# Examples inD 14,113 160 8,500 3,510 9,427 450 670 9,427 2,061 10,176\nDL 80.95 51.41 68.17 56.48 85.62 55.20 87.14 65.68 49.56 81.19\nDG(Random) 66.20 51.26 42.06 37.85 68.99 59.80 80.71 52.23 53.04 67.93\nDG(Contrastive) 66.15 52.36 41.57 38.43 66.66 59.20 67.14 61.28 49.56 67.93\nDG(Similar) 67.15 52.05 47.62 42.09 69.60 60.60 83.57 61.28 49.56 69.24\nDG(Tree) 68.35 52.81 48.50 42.26 69.66 61.60 85.71 61.28 56.52 72.74\n(DG-DL)/DL ‚àí18.43% +2.65% ‚àí40.55% ‚àí33.64% ‚àí22.91% +10.38% ‚àí1.66% ‚àí7.18% +12.31% ‚àí11.61%\nTable 3: ID Performance (Accuracy)comparison between models trained on original train dataset DL (First group)\nand LLM-created train dataset DG (Second group). The optimal variant for data-creation in the second group is\nshown in bold, and the second best is underlined. The third group of the table presents the percentage difference\nbetween the best variant in the second group and the first group.\nMCQA (2) MCQA (5) Open Yes/No Closed Yes/No\nTrain‚Üí PIQA WinoGrande CommonsenseQA RiddleSense BoolQ PubMedQA BioASQ PubMedQA StrategyQA CREAKTrained on‚ÜìTest‚Üí WinoGrande PIQA RiddleSense CommonsenseQA PubMedQA BoolQ PubMedQA BioASQ CREAK StrategyQA\nDL 52.05 44.65 41.51 40.93 62.80 58.65 67.14 56.20 49.27 48.69\nDG(Random) 51.57 49.10 38.51 41.33 59.00 55.77 66.42 59.40 49.27 48.69DG(Contrastive) 50.31 49.50 32.94 42.35 59.00 59.87 75.00 55.20 49.27 46.95DG(Similar) 48.42 52.25 43.42 42.62 64.60 62.50 77.85 63.00 49.27 51.30DG(Tree) 50.31 49.55 40.09 43.35 64.60 61.28 81.42 66.00 57.72 54.78\n(DG-DL)/DL ‚àí0.93% +14.54% +4.39% +5.58% +2.78% +6.16% +17.53% +14.84% +14.63% +11.11%\nTable 4: OOD Performance (Accuracy)comparison between models trained on original train dataset DL (First\ngroup) and LLM-created train dataset DG (Second group). The best dataset for each OOD experiment is shown in\nbold, and the second best is underlined. The third group of the table presents the percentage difference between the\nbest variant in the second group and the first group.\ngiven task, the number of samples for each dataset\nis identical. We also compute the percentage dif-\nferential between the two types of datasets, shown\nin the table as (DG - DL)/DL. These findings\nconfirm that while there is no substitute for large\namounts of hand-crafted data, ‚Äì demonstrated by\ndrops of up to ‚àí40.55% when using synthetically\ncreated data ‚Äì LLMs can play an important role\nwhen access is only available to very little data, and\nin specialized domains. This is demonstrated by the\nsimilar or often better performance of DG models\non WinoGrande, PubMedQA, BioASQ and Strate-\ngyQA. Meanwhile, a comparison between different\n‚Äúself-reference‚Äù sampling strategies demonstrates\nthe importance of mitigating domain drift in our\nsingle-shot approach, where the sole true format-\nting example is the only anchor point to the data\ndistribution we seek to generate. The Tree-based\nexploration strategy limits the semantic distance\nbetween the seed sample and instances created later\nin the generation process and therefore yield higher\nperformance on ID data.\nOOD Performance. While ID data is useful to\ngain insights of a system in controlled settings, real-\nworld applications must deal with data that is often\nfar more variable and chaotic. Therefore, we com-\npare manually curated data (i.e., original training\ndata) to LLM generated data in an OOD setting.\nSpecifically, since we have used at least two eval-\nuation datasets for each category of downstream\napplication, we use one dataset for training and\nevaluate on a different dataset. Note that in this\nsetting, while the training data can either be manu-\nally curated (i.e. DL), or generated by an LLM (i.e.\nDG), the test dataset is always manually curated\n(i.e., original test data). Table 4 presents a compre-\nhensive analysis of the OOD performance of mod-\nels trained on DL and DG. The results show that\nmodels trained on LLM data are consistently and\nsometimes significantly better at OOD predictive\nperformance than their hand-crafted counterparts.\nThis has important implications for the robustness\nand generalizability of real-world systems that of-\nten deal with inputs that are very different from\ncarefully curated academic datasets. We note that\na combination of human and LLM created data\nmay yield even higher gains in OOD performance\nand leave a comprehensive evaluation of this to fu-\nture work. Finally, a comparison of ‚Äúself-reference‚Äù\nstrategies on the OOD setting shows that while the\nTree-based exploration approach is still a consis-\ntently strong one, other sampling approaches are\nsometimes comparable or better. This is under-\nstandable since some degree of controlled noise is\nhelpful, and can be viewed as a regularizer, when\ntrying to generalize to OOD test data.\n15355\n0-10% 0-20% 0-30% 0-40% 0-50% 0-60% 0-70% 0-80% 0-90% 0-100%0\n10\n20\n30\n40\n50\n60\n Accuracy on Riddlesense by each accumuulated data percentage\nHuman\nRandom\nContrast\nSimilar\nTree\nFigure 5: Performance (Accuracy) on RiddleSense us-\ning cumulative data splits of the full data.\n5.2 Distribution shift during creation.\nOne natural question about the different ‚Äúself-\nreference‚Äù strategies is whether the domain drift\nthat they induce from iterative sampling detrimen-\ntally impacts samples generated later in the cre-\nation process. In other words does inclusion of\nparts of the dataset generated later lead to perfor-\nmance drops or plateaus? In order to answer this\nquestion we perform an evaluation of cumulative\ndata splits on one of our benchmark datasets (Rid-\ndlesense). Specifically we use incremental per-\ncentages of training data ‚Äì in 10% blocks ‚Äì for all\nhuman labeled and synthetically generated datasets,\nand evaluate the performance of these models on\nthe corresponding test set. The results of this ex-\nperiment are shown in Figure 5.\nThere are several interesting insights to be\ngained from these results. Firstly, using human-\nlabeled data leads to much faster convergence; this\nmakes sense since the evaluation is performed on\nID test data. Random and Contrastive sampling\nstrategies ‚Äì both of which performed less well on\nour main evaluation ‚Äì do exhibit drops in perfor-\nmance with later cumulative splits. Meanwhile,\nSimilar and Tree approaches ‚Äì which were consis-\ntently better sampling strategies ‚Äì demonstrate a\nsteady rise in performance with added data. Jointly\nthese results indicate that judicious selection of ex-\namples for subsequent prompts is needed to counter\ndomain drift. Lastly, the final upward trend of all\ndatasets is meaningful because they indicate that\nmodels trained on all the datasets do generally ben-\nefit from more data. While this additional data is\nlikely difficult and expensive to obtain from human\nannotators, LLMs can create arbitrarily more data\nat a fraction of the cost.\n5.3 Data creation cost\nTable 5 presents the expenses incurred by lever-\naging an instruction-following LLM APIs in our\nDataset # Train Random Diverse Similar Tree\nPIQA 14,113 3.60 2.82 3.62 3.97\nWinoGrande 160 0.02 0.02 0.03 0.02\nCommonsenseQA 8,500 2.73 2.71 2.77 1.73\nRiddleSense 3,510 0.95 0.95 1.00 1.05\nBoolQ 9,427 5.13 2.24 4.95 4.2\nPUbMedQA 450 0.17 0.15 0.17 0.17\nBioASQ 670 0.24 0.23 0.33 0.22\nBoolQ 9,427 3.13 4.10 3.22 3.11\nStrategyQA 2,061 0.66 0.70 0.81 0.66\nCREAK 10,176 3.24 3.20 4.14 3.50\nTable 5: API Usage Cost (USD)of data creation strat-\negy. The cost of utilizing API is calculated in USD,\nbased on the current pricing of gpt-3.5-turbo as of\nJune 2023, with a rate of 0.002 USD per 1K tokens. The\ncheapist strategy is shown in bold.\ndata creation pipeline for each dataset in our evalu-\nation benchmark. The results demonstrate that data\ncreation with LLMs is highly cost-effective, and\ncosts well under $5 USD for every single dataset\nwe created. Factored into this is the cost for data\nrejected because it was duplicated or ill-formed.\nFurthermore, our Tree-based ‚Äúself-reference‚Äù strat-\negy ‚Äì which was the most performant on quantita-\ntive analyses ‚Äì was also among the more econom-\nical ones. It was the most economical on half the\ndatasets, while the Contrastive strategy incurred\nthe lowest cost on the others. These expenses are\nbased on the pricing of gpt-3.5-turbo from Ope-\nnAI as of June 2023.\n6 Conclusion\nIn this paper, we have presented a formal frame-\nwork for data creation using LLMs and proposed a\nsingle-shot formatting example-based data creation\npipeline that leverages an instruction-following\nLLM. Specifically, we showed how multiple var-\nied examples can be generated from a single seed\nexample in a machine-friendly JSON format by\nconditioning the LLM on a structured prompt con-\nsisting of instructions and a formatted example. We\nfurther expand the diversity of the output by intro-\nducing a ‚Äúself-reference‚Äù mechanism that selects\nformatting examples for subsequent iterations of\ngeneration from newly created data, and present\nfour different instantiations of the sampling strat-\negy. While prior efforts at LLM data creation in\nthe literature have avoided domains where the la-\nbel space is open-ended and varies from instance\nto instance, or is semantically devoid of inherent\nmeaning, our structured prompts are able to tackle\nboth. On a battery of evaluations our findings indi-\ncate that LLMs can act as highly cost-effective data\n15356\ngenerators for the development of small trainable\nor tunable models in downstream applications. For\nexample, a budget of $5 USD is enough to generate\n2M tokens with gpt-3.5-turbo, and depending\non the task can yield several thousand data samples.\nThese models exhibit noteworthy predictive abili-\nties in generalizing to out-of-distribution data, a key\ndesiderata in real-world systems where data can be\nmessy, variable and evolving. The impact of these\nfindings are meaningful in a number of enterprise\nscenarios, including for applications that require\nstrict governance on predictive models due to pri-\nvacy and security concerns, or due to response-time\nservice-level agreements, or indeed for small busi-\nnesses where human annotation costs, especially\nfrom domain experts, can be prohibitively high.\n7 Limitation\nDespite the capability of our pipeline to inte-\ngrate with a variety of other instruction-following\nLLMs, our testing is restricted to ChatGPT ( i.e.,\ngpt-3.5-turbo) due to performance bottlenecks\nand time-out issues with LLM APIs in general.\nWhile we also experimented with several recent\nopen-source instruction-following models that dis-\ntill ChatGPT responses, their abilities to generate\nwell-formatted JSON and comprehend instructions\nwere limited. We expect that the integration of\nour pipeline with other open-source LLMs will be\npossible in time, as the open-source community\nattains a performance standard commensurate with\ncommercial products.\nAn important consideration in our single-shot\nexample-based data creation pipeline is the selec-\ntion of the initial seed formatting sample. We did\nnot perform an exhaustive analysis to understand\nthe impact of this seed selection on data creation\nquality, again due to performance bottlenecks with\nLLM APIs. While we select this seed at random in\nthis paper, it is possible that a more carefully con-\nsidered approach for crafting or selection of this\nexample may yield better results.\n8 Acknowledgement\nThis work was funded in part by the Defense Ad-\nvanced Research Projects Agency (DARPA) and\nArmy Research Office (ARO) under Contract No.\nN660011924033, Contract No. W911NF-21-C-\n0002 and Contract No. HR00112390061, and with\nsupport from the Keston Exploratory Research\nAward. The views and conclusions contained\nherein are those of the authors and should not be\ninterpreted as necessarily representing the official\npolicies, either expressed or implied, of DARPA,\nARO or the U.S. Government.\nReferences\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924‚Äì2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.\nGlam: Efficient scaling of language models with\nmixture-of-experts. In International Conference on\nMachine Learning, pages 5547‚Äì5569. PMLR.\nJiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng\nYe, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan\nLiang, Zhenguo Li, and Lingpeng Kong. 2022. Self-\nguided noise-free data generation for efficient zero-\nshot learning. In The Eleventh International Confer-\nence on Learning Representations.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346‚Äì\n361.\n15357\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxiGen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3309‚Äì3326, Dublin, Ireland.\nAssociation for Computational Linguistics.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. PubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567‚Äì\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\nChan, Kory Matthewson, Michael Henry Tessler,\nAntonia Creswell, James L McClelland, Jane X\nWang, and Felix Hill. 2022. Can language models\nlearn from explanations in context? arXiv preprint\narXiv:2204.02329.\nHunter Lang, Monica N Agrawal, Yoon Kim, and\nDavid Sontag. 2022. Co-training improves prompt-\nbased learning for large language models. In Inter-\nnational Conference on Machine Learning , pages\n11985‚Äì12003. PMLR.\nBill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee,\nand Xiang Ren. 2021. RiddleSense: Reasoning\nabout riddle questions featuring linguistic creativ-\nity and commonsense knowledge. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 1504‚Äì1515, Online. Association\nfor Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100‚Äì114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTodor Markov, Chong Zhang, Sandhini Agarwal, Tyna\nEloundou, Teddy Lee, Steven Adler, Angela Jiang,\nand Lilian Weng. 2022. A holistic approach to un-\ndesired content detection in the real world. arXiv\npreprint arXiv:2208.03274.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding. arXiv\npreprint arXiv:2202.04538.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023. Orca: Progressive learning from\ncomplex explanation traces of gpt-4. arXiv preprint\narXiv:2306.02707.\nYasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and\nGreg Durrett. 2021. Creak: A dataset for com-\nmonsense reasoning over entity knowledge. arXiv\npreprint arXiv:2109.01653.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655‚Äì2671, Seattle, United States.\nAssociation for Computational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commun.\nACM, 64(9):99‚Äì106.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. 2022. Selec-\ntive annotation makes language models better few-\nshot learners. arXiv preprint arXiv:2209.01975.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149‚Äì4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n15358\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, et al. 2015. An overview of the bioasq large-scale\nbiomedical semantic indexing and question answer-\ning competition. BMC bioinformatics, 16(1):1‚Äì28.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021a. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4195‚Äì4205, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith,\nIz Beltagy, et al. 2023a. How far can camels go?\nexploring the state of instruction tuning on open re-\nsources. arXiv preprint arXiv:2306.04751.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484‚Äì13508, Toronto, Canada. Association\nfor Computational Linguistics.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021b. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022. Zerogen: Efficient zero-shot learning via\ndataset generation. arXiv preprint arXiv:2202.07922.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo\nLee, and Woomyoung Park. 2021. GPT3Mix: Lever-\naging large-scale language models for text augmen-\ntation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2225‚Äì2239,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\n15359\nA Appendix\nA.1 Details of Dataset\n‚Ä¢ PIQA (Bisk et al., 2020) is a binary-choice\nquestion answering task, which chooses the\nmost suitable solution for questions related to\nphysical commonsense.\n‚Ä¢ WinoGrande (Sakaguchi et al., 2021) is a\ntask that involves selecting the correct binary\noption to fill in a given sentence that requires\ncommonsense reasoning.\n‚Ä¢ CommonsenseQA (Talmor et al., 2019) is\na multiple-choice question answering task,\nwhich picks the most appropriate answer on\ngeneral commonsense questions.\n‚Ä¢ Riddlesense (Lin et al., 2021) is a multiple-\nchoice questions answering task, which picks\nthe most appropriate answer on riddle-style\nquestions that need cognitive process.\n‚Ä¢ BoolQ (Clark et al., 2019) is a question an-\nswering task that answering questions with\na simple ‚Äúyes‚Äù or ‚Äúno‚Äù response. Questions\nare naturally occurring queries sourced from\nthe Google search engine. In an open-book\nsetting, the model must comprehend the given\ncontext in order to provide an answer, whereas\nin a closed-book setting, the answer must be\nprovided directly without any context.\n‚Ä¢ PubmedQA (Jin et al., 2019) is a task that\ninvolves answering research questions pertain-\ning to the corresponding abstracts of biomedi-\ncal research papers, and the answers are pro-\nvided in the form of ‚Äúyes‚Äù, ‚Äúno‚Äù, or ‚Äúmaybe‚Äù.\nIn our study, we treat ‚Äúmaybe‚Äù as ‚Äúno‚Äù to\nensure consistent output format with other\ndatasets.\n‚Ä¢ BioASQ (Tsatsaronis et al., 2015) offers a\nrange of question answering tasks, covering\nvarious categories such as factoid, list, sum-\nmary, and yes/no questions based on the con-\ntent of biomedical research papers that have\nbeen reviewed by experts in the field. For the\npurpose of this study, our focus will be re-\nstricted to questions that have binary answers\nof ‚Äúyes‚Äù or ‚Äúno‚Äù.\n‚Ä¢ StrategyQA (Geva et al., 2021) is a bench-\nmark for question-answering that specifically\ntargets open-domain questions where the nec-\nessary reasoning path is not explicitly stated in\nthe question, and needs to be inferred through\na strategic approach. The answers to these\nquestions are either ‚Äúyes‚Äù or ‚Äúno‚Äù.\n‚Ä¢ CREAK (Onoe et al., 2021) has been specifi-\ncally formulated for the purpose of common-\nsense reasoning pertaining to entity knowl-\nedge. The dataset comprises assertions of en-\ntities, for which the answers need to be speci-\nfied as either True or False.\nA.2 Data Statistics\nDataset # Train # Valid # Test\nPIQA 14,113 1,838 2,000\nWinoGrande (XS) 160 633 634\nCommonsenseQA 8,500 1,221 1,241\nRiddlesense 3,510 1,021 1,202\nBoolQ 9,27 1,35 1,365\nPubmedQA 450 50 500\nBioASQ 670 75 140\nStrategyQA 2,061 114 115\nCREAK 10,176 685 686\nTable 6: Data statistics.Each dataset. We use in-house\ntest set which is randomly splitted from the train set for\nthose dataset that do not provide test set.\n15360",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8232492208480835
    },
    {
      "name": "Disk formatting",
      "score": 0.5916126370429993
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5835818648338318
    },
    {
      "name": "Data science",
      "score": 0.5428699254989624
    },
    {
      "name": "Task (project management)",
      "score": 0.5414445996284485
    },
    {
      "name": "Pipeline (software)",
      "score": 0.5015244483947754
    },
    {
      "name": "Machine learning",
      "score": 0.4912852644920349
    },
    {
      "name": "Data modeling",
      "score": 0.4522736966609955
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43762728571891785
    },
    {
      "name": "Software engineering",
      "score": 0.21337607502937317
    },
    {
      "name": "Programming language",
      "score": 0.10157114267349243
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}