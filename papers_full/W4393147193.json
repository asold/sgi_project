{
  "title": "Editing Language Model-Based Knowledge Graph Embeddings",
  "url": "https://openalex.org/W4393147193",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2110418884",
      "name": "Siyuan Cheng",
      "affiliations": [
        "Zhejiang University",
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5086969601",
      "name": "Bozhong Tian",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1983188002",
      "name": "Xi Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2110561275",
      "name": "Qingbin Liu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2114954316",
      "name": "Huajun Chen",
      "affiliations": [
        "Zhejiang University",
        "Zhejiang University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110418884",
      "name": "Siyuan Cheng",
      "affiliations": [
        "Zhejiang University",
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5086969601",
      "name": "Bozhong Tian",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2114954316",
      "name": "Huajun Chen",
      "affiliations": [
        null,
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3171331476",
    "https://openalex.org/W3154575616",
    "https://openalex.org/W4297847420",
    "https://openalex.org/W3194836374",
    "https://openalex.org/W4385262399",
    "https://openalex.org/W6794025307",
    "https://openalex.org/W6740216407",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6806534568",
    "https://openalex.org/W4367832894",
    "https://openalex.org/W6855402859",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W3209475026",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W6810241339",
    "https://openalex.org/W6802629465",
    "https://openalex.org/W6681270447",
    "https://openalex.org/W205829674",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6771486561",
    "https://openalex.org/W2909137510",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2984902757",
    "https://openalex.org/W6745537798",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W4226142803",
    "https://openalex.org/W2759136286",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W6810313136",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W3212116457",
    "https://openalex.org/W4308302660",
    "https://openalex.org/W6846354700",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W4221021831",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W4282939402",
    "https://openalex.org/W3212059402",
    "https://openalex.org/W6767905578",
    "https://openalex.org/W4377865177",
    "https://openalex.org/W6846002521",
    "https://openalex.org/W4221157572",
    "https://openalex.org/W2990995117",
    "https://openalex.org/W3099206682",
    "https://openalex.org/W4221160815",
    "https://openalex.org/W4310978166",
    "https://openalex.org/W2997897037",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W4389520380",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4206028705",
    "https://openalex.org/W4287820586",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4385573272",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2949972983",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W4394743141",
    "https://openalex.org/W3196642073",
    "https://openalex.org/W4385728911",
    "https://openalex.org/W2995448904",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W4389520370",
    "https://openalex.org/W4315881234",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, making them difficult to modify post-deployment without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. This task is designed to facilitate rapid, data-efficient updates to KG embeddings without compromising the performance of other aspects. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hypernetwork to edit/add facts. Our comprehensive experimental results reveal that KGEditor excels in updating specific facts without impacting the overall performance, even when faced with limited training resources. Code and datasets will be available at https://github.com/AnonymousForPapers/DeltaKG.",
  "full_text": "Editing Language Model-Based Knowledge Graph Embeddings\nSiyuan Cheng1, 2, 4*, Ningyu Zhang1, 2‚Ä†,\nBozhong Tian1, 2*, Xi Chen4*, Qingbin Liu4, Huajun Chen1, 2, 3‚Ä†\n1Zhejiang University\n2Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph\n3Donghai Laboratory\n4Platform and Content Group, Tencent\n{sycheng, zhangningyu, tbozhong, huajunsir}@zju.edu.cn\n{jasonxchen, qingbinliu}@tencent.com\nAbstract\nRecently decades have witnessed the empirical success of\nframing Knowledge Graph (KG) embeddings via language\nmodels. However, language model-based KG embeddings\nare usually deployed as static artifacts, making them diffi-\ncult to modify post-deployment without re-training after de-\nployment. To address this issue, we propose a new task of\nediting language model-based KG embeddings in this pa-\nper. This task is designed to facilitate rapid, data-efficient\nupdates to KG embeddings without compromising the per-\nformance of other aspects. We build four new datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR,\nand evaluate several knowledge editing baselines demon-\nstrating the limited ability of previous models to handle\nthe proposed challenging task. We further propose a sim-\nple yet strong baseline dubbed KGEditor, which utilizes ad-\nditional parametric layers of the hypernetwork to edit/add\nfacts. Our comprehensive experimental results reveal that\nKGEditor excels in updating specific facts without impact-\ning the overall performance, even when faced with limited\ntraining resources. Code and datasets will be available at\nhttps://github.com/AnonymousForPapers/DeltaKG.\nIntroduction\nKnowledge Graphs (KGs) represent large-scale, multi-\nrelational graphs containing a wealth of symbolic facts.\nThese structures offer invaluable back-end support for\nvarious knowledge-intensive tasks, such as information\nretrieval, question-answering, and recommender systems.\n(Yang et al. 2021, 2022; Wu et al. 2022b,a). To better uti-\nlize that symbolic knowledge in KGs for machine learn-\ning models, many KG embedding approaches have been de-\nvoted to representing KGs in low-dimension vector spaces\n(Wang et al. 2017; Zhang et al. 2022a). Traditional KG em-\nbedding models, e.g., TransE (Bordes et al. 2013), RotatE\n(Sun et al. 2019), are naturally taxonomized as structure-\nbased methods (Xie et al. 2016; Zhang et al. 2020b,a; Wang\net al. 2022b). These approaches employ supervised machine\n*These authors contributed equally.\n‚Ä†Corresponding author.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n2022\tWorld\tCup\nJoe\tBidenDonald\tTrump\nPresidentOf\nKGE\nModel\nBiden\nEDIT\nADD\nArgentina, Award, 2022 World Cup Champions\nBrand new knowledge\naward\n‚àÜùëä#\n‚àÜùëä# KGE\nModel\nPresidentOf\nFigure 1: Top: Illustration of the EDIT task. We edit the\nwrong fact knowledge stored in the KG embeddings. Bot-\ntom: Illustration of the ADD task. We add brand-new knowl-\nedge into the model without re-training.\nlearning to optimize target objectives by utilizing scoring\nfunctions that preserve the inherent structure of KGs.\nHowever, a recent shift in KG embedding methodologies\nhas emerged, moving away from the explicit modeling of\nstructure (Yao, Mao, and Luo 2019; Zhang et al. 2020c;\nWang et al. 2022b,a). Instead, contemporary techniques fo-\ncus on incorporating text descriptions through the use of ex-\npressive black-box models, e.g., pre-trained language mod-\nels. This new paradigm operates under the assumption that\nthe model will inherently capture the underlying structure\nwithout requiring explicit instruction. Leveraging language\nmodels to frame KG embeddings has emerged as a highly\npromising approach, yielding considerable empirical suc-\ncess. This technique offers the potential to generate informa-\ntive representations for long-tail entities and those on-the-\nfly emerging entities. However, KG embeddings with lan-\nguage models are usually deployed as static artifacts, which\nare challenging to modify without re-training. To respond to\nchanges (e.g., emerging new facts) or a correction for facts\nof existing KGs, the ability to conduct flexible knowledge\nupdates to KG embeddings after deployment is desirable.\nTo address this need, we introduce a new task of editing\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17835\nlanguage model-based KG embeddings, which aims to en-\nable data-efficient and fast updates to KG embeddings for\na small region of parametric space without influencing the\nperformance of the rest. It‚Äôs important to note that while\nediting KG embeddings primarily centers on link prediction\ntasks, the editing of standard language models mainly ad-\ndresses tasks such as question answering, each presenting its\nunique set of challenges. For instance, KG embeddings con-\ntend with the complex issue of handling numerous many-to-\nmany (N-M) triples. Intuitively, we define two tasks for pre-\ntrained KG embeddings, namely, EDIT and ADD, to support\nediting incorrect or adding new facts without re-training the\nwhole model during deployment as shown in Figure 1. We\ntreat editing the memories of KG embeddings as alearning-\nto-update problem, which can ensure that knowledge rep-\nresented in KG embeddings remains accurate and up-to-\ndate. To evaluate the performance of the proposed task, we\nestablish three guiding principles: Knowledge Reliability,\nwhich indicates that those edited or newly added knowl-\nedge should correctly be inferred through link prediction;\nKnowledge Locality, which means that editing KG embed-\ndings will not affect the rest of the acquired knowledge when\nsuccessfully updating specific facts; Knowledge Efficiency\nindicates being able to modify a model with low training\nresources. Specifically, we build four new datasets based\non FB15k237 and WN18RR for EDIT and ADD tasks.\nWe leverage several approaches, including Knowledge Ed-\nitor (KE) (Cao, Aziz, and Titov 2021), MEND (Mitchell\net al. 2022), and CALINET (Dong et al. 2022) as base-\nlines. Editing KGE proves more challenging than refining\nlanguage models. Experimental results demonstrate that cur-\nrent methods struggle with efficiently modifying KG em-\nbeddings. We further propose a simple yet effective strong\nbaseline dubbed Knowledge Graph Embeddings Editor\n(KGEditor), which can efficiently manipulate knowledge in\nembeddings by editing additional parametric layers. Our ex-\nperiments demonstrate that KGEditor can effectively mod-\nify incorrect knowledge or add new knowledge while pre-\nserving the integrity of other information. We summarize\nour contributions as follows:\n‚Ä¢ We propose a new task of editing language model-based\nKG embeddings. The proposed task with datasets may\nopen new avenues for improving KG embedding via\nknowledge editing.\n‚Ä¢ We introduce KGEditor that can efficiently modify in-\ncorrect knowledge or add new knowledge without affect-\ning the rest of the acquired knowledge.\n‚Ä¢ We conduct extensive comparisons with in-depth analy-\nsis of four datasets and report empirical results with in-\nsightful findings, demonstrating the effectiveness of the\nproposed approach.\nRelated Work\nKnowledge Graph Embedding\nEarly KG embedding approaches primarily focused on de-\nriving embeddings from structured information alone. Most\nexisting KG embedding methods utilize the translation-\nbased paradigm such as TransE (Bordes et al. 2013), TransR\n(Lin et al. 2015), TransH (Wang et al. 2014), and Ro-\ntatE (Sun et al. 2019) or semantic matching paradigms,\nincluding DistMult (Yang et al. 2015), RESCAL (Nickel,\nTresp, and Kriegel 2011), and HolE (Nickel, Rosasco, and\nPoggio 2016). Additionally, there has been significant in-\nterest in explicitly utilizing structural information through\ngraph convolution networks (Kipf and Welling 2017; Velick-\novic et al. 2018; Vashishth et al. 2020; Liu et al. 2021b;\nZhang et al. 2022b). Since pre-trained language models\n(PLMs) (Zhao et al. 2023) have made waves for widespread\ntasks, framing KG embedding via language models is an\nincreasing technique that has led to empirical success (Pan\net al. 2023). On the one hand, several works have emerged\nto leverage PLMs for KG embeddings. Some studies (Yao,\nMao, and Luo 2019; Zhang et al. 2020c; Wang et al. 2021c,\n2022a) leverage finetuning the PLMs, for example, KG-\nBERT (Yao, Mao, and Luo 2019), which takes the first step\nto utilize BERT (Devlin et al. 2019) for KG embeddings\nby regarding a triple as a sequence and turning link predic-\ntion into a sequence classification task. StAR (Wang et al.\n2021a) proposes a structure-augmented text representation\napproach that employs both spatial measurement and deter-\nministic classifier for KG embeddings, respectively. LMKE\n(Wang et al. 2022b) adopts language models which utilize\ndescription-based KG embedding learning with a contrastive\nlearning framework. On the other hand, some studies (Lv\net al. 2022) adopt the prompt tuning (Liu et al. 2022; Chen\net al. 2022b; Zhang, Li, and et al. 2022) with language\nmodels. PKGC (Lv et al. 2022) converts triples into natu-\nral prompt sentences for KG embedding learning. There are\nalso some other studies (Xie et al. 2022; Saxena, Kochsiek,\nand Gemulla 2022; Chen et al. 2022a) formulate KG em-\nbedding learning as sequence-to-sequence generation with\nlanguage models. Nonetheless, KG embeddings in previous\nstudies are usually deployed as static artifacts whose behav-\nior is challenging to modify after deployment. We propose\na novel task: editing language model-based KG embeddings\nto correct or modify facts in existing KGs, marking the first\ninitiative in this field to our knowledge.\nEditing Factual Knowledge\nEditable training (Sinitsin et al. 2020; Yao et al. 2023)\nrepresents an early, model-agnostic attempt to facilitate\nrapid editing of trained models. Subsequently, numerous\nreliable and effective approaches have been proposed to\nenable model editing without the necessity for resource-\nintensive re-training (Cohen et al. 2023; Geva et al. 2023;\nHase et al. 2023; Han et al. 2023). These methods can\nbe broadly classified into two distinct categories: external\nmodel-based editor and additional parameter-based editor.\nThe first category, external model-based editors, employs\nextrinsic model editors to manipulate model parameters.\nFor instance, KnowledgeEditor (KE) (Cao, Aziz, and Titov\n2021) adopts a hyper-network-based approach to edit knowl-\nedge within language models. MEND (Mitchell et al. 2022)\nutilizes a hypernetwork (MLP) to predict the rank-1 de-\ncomposition of fine-tuning gradients. The predicted gradi-\nents are used to update a subset of the parameters of PLMs.\nThe second category, additional parameter-based editors,\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17836\ninvolves using supplementary parameters to adjust the fi-\nnal output of the model, thereby achieving model editing.\nBuilding upon prior research (Dai et al. 2022a), which sug-\ngests that Feed-Forward Networks (FFNs) in PLMs may\nstore factual knowledge, CALINET (Dong et al. 2022) en-\nhances a specific FFN within the PLM by incorporating ad-\nditional parameters composed of multiple calibration mem-\nory slots. Existing approaches predominantly focus on mod-\nifying knowledge within pre-trained language models, con-\nstraining the manipulation and interpretation of facts. In con-\ntrast, by editing knowledge in KG embeddings, our proposed\ntasks enable adding and modifying knowledge, thereby ex-\ntending the applicability of KG embeddings across various\ndownstream tasks. Previous methods for updating knowl-\nedge graph embeddings (KGE), such as OUKE (Fei, Wu,\nand Khan 2021) and RotatH (Wei et al. 2021), mainly fo-\ncused on modifying the entity and relation vectors in score\nfunction-based KGE models. These methods employed dif-\nferent dataset versions as snapshots to represent changes\nwithin the knowledge graph and validated these changes di-\nrectly using hit@k. However, this approach falls short of ef-\nfectively measuring the efficacy of edits. Our paper improves\nthese methods by creating datasets and introducing metrics\nfor a more accurate evaluation of KGE updates.\nMethodology\nTask Definition\nA KG can be represented asG = (E, R, T ), where E, R and\nT are sets of entities, relation types, and triples. Each triple\nin T takes the form (h, r, t), where h, t‚àà Eare the head and\ntail entities. For the EDIT task, knowledge requiring edits\nis defined as (h, r, y, a) or (y, r, t, a), where y is an incor-\nrect/outdated entity. We denote the original input entity asx\nand the predicted target as y. a points to the desired edited\nentity (change from y to a). E.g., <Donald Trump,\npresident\nof, U.S.A.> is an outdated triple, which\nshould be changed to <Joe Biden, president of,\nU.S.A.> to update the KG embeddings. Apart from out-\ndated information, numerous new facts may be absent in\ncurrent KGs. Consequently, it becomes essential to flexi-\nbly incorporate these new triples into the KG embeddings.\nThus, we introduce the ADD task to integrate new knowl-\nedge seamlessly. Note that the ADD task is similar to the\ninductive setting in KG completion but without re-training\nthe model. Formally, we define the task of editing language\nmodel-based KG embeddings as follows:\np(a | ÀúW, œï, x) ‚Üê p(y | W, œï, x) (1)\nwhere W and ÀúW denote the original and edited parameters\nof KG embeddings, respectively. In this paper,x refers to the\noriginal input (e.g., (?, h, r)), output entity (y ), and desired\nedited entity (a). œï represents the external parameters of the\neditor network. At the same time, we need to strive to main-\ntain the stability of the model for other correct knowledge,\nas described below:\np(y‚Ä≤ | ÀúW, œï, x‚Ä≤) ‚Üê p(y‚Ä≤ | W, œï, x‚Ä≤) (2)\nwhere x‚Ä≤ and y‚Ä≤ represent the input and label of the factual\nknowledge stored in the model, respectively, withx‚Ä≤ Ã∏= x.\nEvaluation Metrics We set three principles to measure\nour proposed tasks‚Äô efficacy.\nKnowledge Reliability evaluates whether edited or new\nknowledge is correctly inferred through link prediction.\nChanges in KG embeddings should align with the intended\nedits without bringing unintended biases/errors. For editing\neffect validation, we adopt the KG completion setting. By\nranking candidate entity scores, we generate an entity list.\nWe define theSuccess@1 metric (Succ@k)by counting the\nnumber of correct triples that appear at position k.\nKnowledge Locality seeks to evaluate whether editing\nKG embeddings will impact the rest of the acquired knowl-\nedge Ox when successfully updating specific facts. We in-\ntroduce the Retain Knowledge (RK@k) metric, indicating\nthat the entities predicted by the original model are still cor-\nrectly inferred by the edited model. Specifically, we sample\nthe rank k triples predicted by the original model as a refer-\nence dataset (L-test in Table 1). We posit that if the predicted\ntriples remain unchanged after editing, then the model ad-\nheres to knowledge locality and does not affect the rest of\nthe facts. We calculate the proportion of retaining knowl-\nedge as a measure of the stability of the model editing:\nRK@k =\nPf(x‚Ä≤; ÀúW)‚â§k\nPf(x‚Ä≤; W)‚â§k\n(3)\nwhere x‚Ä≤ are randomly sampled from the reference dataset.\nf is the function with inputx and parameterized byW, com-\nputing the rank of x. f(x‚Ä≤; W)‚â§k signifies the count of rank\nvalue under k predicted by the model with W parameters.\nBesides, to better present the effect of models, we intro-\nduce two additional metrics. Edited Knowledge Rate of\nChange ERroc and Retaining Knowledge Rate of Change\nRKroc are denoted as:\nERroc = |Redit ‚àí Rorigin|\nRorigin\n,\nRKroc = |Rs edit ‚àí Rs origin|\nRs edit\n(4)\nwhere Redit and Rorigin represent the mean rank on the test\nset before and after editing. Rs edit and Rs origin denote the\nreference L-test‚Äôs mean rank before and after editing.\nKnowledge Efficiency aims to evaluate the efficiency of\nthe editors. We leverage the tuning parameters (including ex-\nternal model or additional parameters; see Params in Table\n2, 3) as a metric for evaluation.\nDatasets Construction\nWe construct four datasets for EDIT and ADD tasks, based\non two benchmarks FB15k-237 (Toutanova et al. 2015), and\nWN18RR (Dettmers et al. 2018). After initially training KG\nembeddings with language models, we sample challenging\ntriples as candidates, as detailed below. Recent research in-\ndicates that pre-trained language models can learn signifi-\ncant factual knowledge (Petroni et al. 2019; Cao et al. 2021),\nand predict the correct tail entity with a strong bias. This\nmay hinder the proper evaluation of the editing task. Thus,\nwe exclude relatively simple triples inferred from language\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17837\nJoe Biden\nOrig\nin Triples\nL-Test Dataset\nPLM-KGE Model\nEdit Dataset\nCorrupt Triples\nDonald Trump  PresidentOf  U.S.A.\nU.S.A.\n Da Vinci\nFrance\nGolden Label: Joe Biden\nMona Lisa  PaintedBy  Van Gogh\nGolden Label: Da Vinci\niPhone\nTraining\nFilter\nFrance\nStadium\nPLM\nVan Gogh\nParis\nMona Lisa\nisCapitalOf\nPaintedBy\nPresidentOf\nDonald Trump U.S.A.PresidentOf\nMona Lisa PaintedBy\nFranceParis isCapitalOf\nApple Produce\nisCapitalOfParis\nSoccer isPlayedIn\nFigure 2: Data Construction Process. Step 1: Randomly dis-\nrupt the triples from the existing knowledge graph to pro-\nduce a corrupted dataset. Step 2: Employ this dataset to fine-\ntune a pre-existing pre-trained KGE model, yielding a model\nneeding editing. Step 3: Filtering, we reassess the data with\nthe pre-trained KGE model, accurately sorting the correctly\nlabeled data into the L-Test dataset and allocating the misla-\nbeled data to a designated dataset for correction.\nmodels‚Äô internal knowledge for precise assessment. Specif-\nically, we select data with link prediction ranks >2,500. For\nEDIT, the Top-1 facts from the origin model replace the en-\ntities in the training set of FB15k-237 to build the pre-train\ndataset (original pre-train data), while golden facts serve as\ntarget edited data. For ADD, we leverage the original train-\ning set of FB15k-237 to build the pre-train dataset (origi-\nnal pre-train data) and the data from the inductive setting as\nthey are not seen before. Unlike EDIT, for ADD, since new\nknowledge is not present during training, we directly use\nthe training set to evaluate the ability to incorporate new\nfacts. We adopt the same strategy to construct two datasets\nfrom WN18RR. To assess knowledge locality, we further\ncreate a reference dataset (L-Test) based on the link predic-\ntion performance with a rank value belowk (the same k as in\nthe RK metric). Five human experts scrutinize all datasets\nfor ethical concerns. Ultimately, we establish four datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR.\nTable 1 provides details of the datasets. The creation of the\ndatasets for the EDIT task is shown in Figure 2.\nLanguage Model-based KGE\nThis section introduces the technical background of lan-\nguage model-based KG embeddings. Specifically, we il-\nlustrate two kinds of methodologies to leverage language\nmodels, namely: finetuning (FT-KGE) and prompt tuning\n(PT-KGE). Note that our task is orthogonal to language-\nbased KG embeddings, performance differences between\nFT-KGE and PT-KGE are discussed in the next section.\nFT-KGE methods, such as KG-BERT (Yao, Mao, and\nLuo 2019), which regard triples in KGs as textual sequences.\nSpecifically, they are trained with the description of triples\nthat represent relations and entities connected by the [SEP]\nand [CLS] tokens and then take the description sequences as\nthe input for finetuning. Normally, they use the presentation\nof [CLS] token to conduct a binary classification, note as:\np(yœÑ |x) = p([CLS]|x; W) (5)\nwhere yœÑ ‚àà [0, 1] is the label of whether the triple is positive.\nFB15k-237 WN18RR\nTASK\nPre-train Train Test L-Test Pre-train Train Test L-Test\nADD 215,082\n2,000 - 16,872 69,721 2,000 - 10,000\nEDIT 310,117 3,087 3,087 7,051 93,003 1,491 830 5,003\nTable 1: The statistics of datasets for the EDIT and ADD\ntasks. L-Test is the test set for knowledge locality to evaluate\nthe rest of the knowledge in KGE.\nPT-KGEmethods like PKGC (Lv et al. 2022), utilize nat-\nural language prompts to elicit knowledge from pre-trained\nmodels for KG embeddings. Prompt tuning uses PLMs as di-\nrect predictors to complete a cloze task, linking pre-training\nand finetuning. In this paper, we implement PT-KGE with\nentity vocabulary expansion, treating each unique entity as\ncommon knowledge embeddings. Specifically, we consider\nentities e ‚àà Eas special tokens in the language model, turn-\ning link prediction into a masked entity prediction. Formally,\nwe can obtain the correct entity by ranking the probability of\neach entity in the knowledge graph as follows:\np(y|x) = p([MASK] = y|x; W) (6)\nwhere p(y|x) is denoted as the probability distribution of\nthe entity in the KG.\nEditing KGE Baselines\nWith the pre-trained KG embeddings via the sections men-\ntioned above, we introduce several baselines to edit KG em-\nbeddings. Concretely, we divide baseline models into two\ncategories. The first is the external model-based editor,\nwhich uses an extrinsic model editor to control the model pa-\nrameters such as KE and MEND. Another is the additional\nparameter-based editor, which introduces additional pa-\nrameters to adjust the final output of the model to achieve\nthe model editing. There are also some non-editing methods\nin the baseline: Finetune and K-Adapter (Wang et al. 2021b).\nExternal Model-based Editor This method employs a\nhypernetwork to learn the weight update ‚àÜ for editing the\nlanguage model. KE utilizes a hypernetwork (specifically, a\nbidirectional-LSTM) with constrained optimization, which\nis used to predict the weight update during inference. Thus,\nKE can modify facts without affecting the other knowledge\nin parametric space. MEND conducts efficient local edits\nto language models with a single input-output pair, which\nlearns to transform the gradient of finetuned language mod-\nels, which utilizes a low-rank decomposition of gradients.\nHence, MEND can edit the parameters of large models, such\nas BART (Lewis et al. 2020), GPT-3 (Brown and et al. 2020),\nand T5 (Raffel et al. 2020), with little resource cost. To apply\nKE and MEND to our task setting, we remove the semanti-\ncally equivalent set1 of the original task and only utilize the\ntriples that need to be edited as input x.\n1Semantic equivalence refers to a declaration that two data el-\nements from different vocabularies contain data having a similar\nmeaning (Cao, Aziz, and Titov 2021).\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17838\nBiden\nU.S.A.Trump\na)\nb)\nùëì(ùë•;ùëä+‚àÜùëä()\nùëì(ùë•;ùëä+‚àÜùëä()‚àÄùë•ÃÖ\nùë• ùëé\nùë¶‚Ä≤\nPre-trained Language ModelÔºàFeedForwardNetworkÔºâùë•\nùêπùêπùëÅ(ùêª)‚àÜùêπùêπùëÅ(ùêª)+\nùë•\nEditing Knowledge\nRetraining knowledge\nPresidentOfùë•ùë¶ùëé Editor\nSlots\nc)\n<Joe Biden>\nKGEModel\nSlots\n<?, president of, U.S.A.>\n+\nTrumpBiden\n= TrumpBidenEditor\nThe external model-based editor\nThe additional parameter-based editor\nKGEditor\n‚àÜùëä(\nùë¶2 ‚àÜùëä(\nùë¶:\nùëé:ùë¶ùëé()(    )ùêπùêπùëÅ‚Ä≤(ùêª)\nùêπùêπùëÅ(ùêª)‚àÜùêπùêπùëÅ(ùêª)ùêπùêπùëÅ‚Ä≤(ùêª)\nùë•:\nArgentina\n2022\tWorld\tCupArgentina\n<Argentina>\nKGEModel\nSlots\n<?, award, 2022 world cup>\n+ =\nEmerging entityEditor\naward\n üá¶üá∑\nüá´üá∑\n‚àÜùëä(\nùë•:ùëé:\nùëé:\nùêπùêπùëÅ‚Ä≤(ùêª)ùêπùêπùëÅ(ùêª)‚àÜùêπùêπùëÅ(ùêª)\nFigure 3: The introduction of baselines and KGEditor. The external model-based editors (a) utilize a hyper external network\nto obtain the parameters‚Äô shift and add to the original model parameters for editing (replacing the origin entity y with the\nalternative entity a). The additional parameter-based editors (b) rectify the erroneous knowledge stored in FFN by adjusting its\npredicted distributions from FFN(H; W) to FFN‚Ä≤(H). KGEditor (c) utilizes a hyper external network to update the knowledge\nin FFN (Top right: EDIT, Bottom right: ADD).\nAdditional Parameter-based Editor Since previous\nstudy (Dai et al. 2022a) illustrates that the FFNs in PLMs\nmay store factual knowledge, it is intuitive to edit models by\nmodifying the FFNs. This paradigm introduces extra train-\nable parameters within the language models. CALINET ex-\ntends the FFN with additional knowledge editing parame-\nters, consisting of several calibration memory slots. To ap-\nply CALINET to the proposed task, we leverage the same\narchitecture with FFN but with a smaller intermediate di-\nmension d and add its output to the original FFN output as\nan adjustment term to edit knowledge.\nThe Proposed Strong Baseline: KGEditor\nNote that external model-based edit methods are flexible but\nhave to optimize many editor parameters. In contrast, the ad-\nditional parameter-based method only tunes a small number\nof additional parameters (0.9M) but encounters poor empiri-\ncal performance due to the challenge of manipulating knowl-\nedge in PLMs. Intuitively, we capitalize on the advantages\nof both approaches and propose a simple yet robust baseline\nKGEditor, as shown in Figure 3, which employs additional\nparameters through hypernetwork. We construct an addi-\ntional layer with the same architecture of FFN and leverage\nits parameters for knowledge editing. Moreover, we lever-\nage an additional hypernetwork to generate the additional\nlayer. With KGEditor, we can optimize fewer parameters\nwhile keeping the performance of editing KG embeddings.\nSpecifically, when computing the output of FFN, we add the\noutput of the additional FFNs to the original FFN output as\nan adjustment term for editing:\nF F N‚Ä≤(H) =F F N(H; W) +‚ñ≥F F N(H; ÀúW + ‚ñ≥ ÀúW) (7)\nwhere W is denoted as the origin FFNs‚Äô weight, and ÀúW\nis the weight of additional FFNs. ‚ñ≥ ÀúW is the parameters‚Äô\nshift generated by the hypernetwork. Inspired by KE (Cao,\nAziz, and Titov 2021), we build the hypernetworks with\na bidirectional LSTM. We encode ‚ü®x, y, a‚ü© and then con-\ncatenate them with a special separator to feed the bidirec-\ntional LSTM. Then, we feed the last hidden states of bidi-\nrectional LSTM into the FNN to generate a single vector h\nfor knowledge editing. To predict the shift for the weight\nmatrix ÀúWn√óm, we leverage FNN conditioned on h which\npredict vectors Œ±, Œ≤‚àà Rm, Œ≥, Œ¥‚àà Rn and a scalar Œ∑ ‚àà R.\nFormally, we have the following:\n‚ñ≥ ÀúW = œÉ(Œ∑) ¬∑\n\u0010\nÀÜŒ± ‚äô ‚àáWL(W; x, a) +ÀÜŒ≤\n\u0011\n,\nwith ÀÜŒ± = ÀÜœÉ(Œ±)Œ≥‚ä§ and ÀÜŒ≤ = ÀÜœÉ(Œ≤)Œ¥‚ä§ ,\n(8)\nwhere ÀÜœÉ refers to the Softmax function ( i.e., x 7‚Üí\nexp(x)/ P\ni exp(xi)) and œÉ refers to the Sigmoid function\n(i.e., x 7‚Üí (1 + exp(‚àíx))‚àí1). ‚àáWL(W; x, a) presents the\ngradient, which contains rich information regarding how to\naccess the knowledge in W. Note that the hypernetwork œï\nparameters can linearly scale with the size of ÀúW, making the\nKGEditor efficient for editing KG embeddings in terms of\ncomputational resources and time.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17839\nMethod P\narams E-FB15k237 E-WN18RR\nTime ‚Üì Succ@1 ‚Üë Succ@3 ‚Üë E\nRroc ‚Üë RK@3 ‚Üë RKroc ‚Üì Time ‚Üì Succ@1 ‚Üë Succ@3 ‚Üë ERroc ‚Üë RK@3 ‚Üë RKroc ‚Üì\nNo Model\nEdit\nKGE FT 121M\n0.103 0.472 0.746 0.998 0.543 0.977 0.109 0.758 0.863 0.998 0.847 0.746\nKGE ZSL 0M\n0.000 0.000 0.000 - 1.000 0.000 0.000 0.000 0.000 - 1.000 0.000\nK-Adapter 32.5M 0.056 0.329 0.348 0.926 0.001 0.999 0.061 0.638 0.752 0.992 0.009 0.999\nModel Edit\nMethod\nCALINET 0.9M\n0.257 0.328 0.348\n0.937 0.353 0.997 0.238 0.538 0.649 0.991 0.446\n0.994\nKE 88.9M 0.368 0.702 0.969 0.999 0.912 0.685 0.386 0.599\n0.682 0.978 0.935 0.041\nMEND 59.1M\n0.280 0.828 0.950 0.954 0.750 0.993\n0.260 0.815 0.827 0.948 0.957 0.772\nKGEditor 38.9M 0.226 0.866\n0.986 0.999 0.874 0.635 0.232\n0.833 0.844 0.991 0.956 0.256\nTable 2: The main result of the EDIT task on E-FB15k237 and E-WN18RR. Bold indicates the best result among Model Edit\nMethod, while underline represents the second-best result. The same format applies to Table 3.\nMethod P\narams A-FB15k237 A-WN18RR\nTime ‚Üì Succ@1 ‚Üë Succ@3 ‚Üë E\nRroc ‚Üë RK@3 ‚Üë RKroc ‚Üì Time ‚Üì Succ@1 ‚Üë Succ@3 ‚Üë ERroc ‚Üë RK@3 ‚Üë RKroc ‚Üì\nNo Model\nEdit\nKGE FT 121M\n0.100 0.906 0.976 0.999 0.223 0.997 0.108 0.997 0.999 0.999 0.554 0.996\nKGE ZSL 0M\n0.000 0.000 0.000 - 1.000 0.000 0.000 0.000 0.000 - 1.000 0.000\nK-Adapter 32.5M 0.055 0.871 0.981 0.999 0.000 0.999 0.061 0.898 0.978 0.999 0.002 0.999\nModel Edit\nMethod\nCALINET 0.9M\n0.261 0.714 0.870 0.997 0.034 0.999\n0.275 0.832 0.913\n0.995 0.511 0.989\nKE\n88.9M 0.362 0.648 0.884 0.997 0.926 0.971 0.384 0.986\n0.996 0.999 0.975\n0.090\nMEND 59.1M 0.400 0.517 0.745 0.991 0.499 0.977 0.350 0.999 1.0 0.999 0.810 0.987\nKGEditor 58.7M 0.203 0.796\n0.923 0.998 0.899 0.920 0.203 0.998 1.0 0.999 0.956 0.300\nTable 3: The main result of the ADD task on A-FB15k237 and A-WN18RR.\nEvaluation\nSettings\nWe employ the constructed datasets, E-FB15k237, A-\nFB15k237, E-WN18RR, and A-WN18RR for evaluations.\nInitially, we utilize the pre-train datasets (the identical train-\ning set for the ADD task and the corrupted training set\nfor the EDIT task) to initialize the KG embeddings with\nBERT. We then train the editor using the training dataset\nand evaluate using the testing dataset. Regarding the ADD\ntask, the evaluation is conducted based on the performance\nof the training set, as they are not present in the pre-training\ndataset. We adopt PT-KGE as the default setting for the\nmain experiments. Additionally, We supply the datasets, pre-\ntrained KG embeddings, and a leaderboard for reference.\nMain Results\nOur comparison of KGEditor spans multiple baselines like\nMEND, KE, CALINET, and variants: KGE\nFT involving di-\nrect fine-tuning of all parameters, KGE ZSL for inferring\ntriples without parameter adjustments, and K-Adapter using\nan adaptor to fine-tune a subset of parameters. Our experi-\nments aim to 1) evaluate the efficacy of various approaches\nfor editing KG embeddings (Table 2,3), 2) investigate the\nperformance when varying the number (n) of target triples\n(Figure 5), and 3) examine the impact of distinct KGE ini-\ntialization approaches on performance(Figure 4).\nKE\nMEND CailNet KGEditor\n0.0\n0.5\n1.0\n1.5\nPT-KGE FT-KGE\nKE\nMEND CailNet KGEditor\n0.0\n0.5\n1.0\n1.5ùêëùêäùê´ùê®ùêú\nùêÑùêëùê´ùê®ùêú\nThe ùê∏ùëÖ!\"# score on different KGE The ùëÖùêæ!\"# score on different KGE\nFigure 4: Left: ERroc and Right: RKroc. The performance\nof different KGE initializations with different models.\nEDIT&ADD Task As Table 2 shows, KGE ZSL fails to\ninfer any facts on both datasets for the EDIT task. Intrigu-\ningly, we also find that fine-tuning all (KGE FT) or a sub-\nset (K-Adaptor) of parameters does not result in satisfactory\nperformance. We think this is due to the numerous N-M\nfacts, and fine-tuning with more data may lead to confu-\nsion about the knowledge. We find that well-crafted knowl-\nedge editing baselines surpass KGE FT and K-Adaptor, in-\ndicating the need for specialized architectures for editing\nknowledge to more effectively identify accurate facts. Fur-\nthermore, we notice that CALIET exhibits poor knowledge\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17840\n1 2 4 8 16 32\n0.2\n0.4\n0.6\n0.8\n1.0\nn Edits\nSucc@1\nThe Succ@1 score changes with the n edits\nMEND KE KGEditor\n1 2 4 8 16 32\n0.6\n0.7\n0.8\n0.9\n1.0\nn Edits\nRK@3\nThe RK@3 score changes with the n edits\n1 2 4 8 16 32\n0.2\n0.4\n0.6\n0.8\n1.0\nn Edits\nSucc@1\nThe Succ@1 score changes with the n edits\nMEND KE KGEditor\n1 2 4 8 16 32\n0.2\n0.4\n0.6\n0.8\n1.0\nn Edits\nSucc@1\nThe Succ@1 score changes with the n edits\nMEND KE KGEditor\n1 2 4 8 16 32\n0.2\n0.4\n0.6\n0.8\n1.0\nn Edits\nSucc@1\nThe Succ@1 score changes with the n edits\nMEND KE KGEditor\n1 2 4 8 16 32\n0.6\n0.7\n0.8\n0.9\n1.0\nn Edits\nRK@3\nThe RK@3 score changes with the n edits\n1 2 4 8 16 32\n0.2\n0.4\n0.6\n0.8\n1.0\nn Edits\nSucc@1\nThe Succ@1 score changes with the n edits\nMEND KE KGEditor\nThe Succ@1 changes with the n edits The RK@3 changes with the n edits\nFigure 5: Left: the variation of Succ@1 value with n ed-\nits. All models drop when n edits become larger. Right: the\nRK@3 score drops when n changes, KE and KGEditor can\nremain stable performance, while MEND decreases.\nlocality (RK@3 and RKroc), indicating that merely mod-\nifying a few parameters in FFNs cannot guarantee precise\nknowledge editing. Our model KGEditor surpasses nearly\nall baselines, utilizing fewer tunable parameters than KE and\nMEND. Regarding time and resource expenditure, KGEdi-\ntor exhibits markedly enhanced editing efficiency compared\nto KGE FT. Note that our model employs a hypernetwork to\nguide the parameter editing of the FFN, resulting in greater\nparameter efficiency than KE and MEND while simultane-\nously achieving superior performance to CALINET.\nFor the ADD task, as shown in Table 3, we observe\nthat KGE ZSL continues to fail in inferring any facts on\nboth datasets (We filter all facts that can be directly in-\nferred by probing the PLM). Finetuning either all parameters\n(KGE FT) or a subset (K-Adaptor) maximizes the perfor-\nmance of reliability, but compromises previously acquired\nknowledge, reflecting poor knowledge locality. Given that\nthe ADD task introduces unseen facts during KG embed-\nding initialization, finetuning parameters naturally capture\nnew knowledge while sacrificing prior information.\nFT-KGE&PT-KGE We evaluate various KGE initializa-\ntion methods with different models on the EDIT task. Fig-\nure 4 contrasts the performance of FT-KGE and PT-KGE in\nterms of knowledge reliability and locality. We observe that\nthe editing performance of PT-KGE is superior to that of\nFT-KGE, suggesting that employing prompt-based models\nis more suitable for editing KG embeddings. Prior research\n(Liu et al. 2021a; Meng et al. 2022) has demonstrated that\nthe prompt-based approaches are more effective in harness-\ning the ‚Äúmodeledge‚Äù (Han and et al. 2021) in PLMs, making\nit reasonable to utilize PT-KGE for editing purposes.\nNumber of Edited Knowledge We further analyze the\nimpact of varying the number of edited facts. We employ dif-\nferent numbers of edits in the E-FB15k237, with the number\nof edits n ‚àà {1, 2, 4, 8, 16, 32}. Figure 5 illustrates a pro-\nnounced impact of n edits on all models‚Äô knowledge relia-\nbility, measured by Succ@1. Furthermore, we observe that\nKE and KGEditor show stable performance in knowledge\nMila KunisLuc BessonEva LongoriaCharlotte GainsbourgDemi MooreMarc ShaimanJennifer ConnellyNaomi Watts\nEdit Knowledge:(Marc Shaiman,type of union,Domestic partnership)\nBefore EditAfter Editor\nFigure 6: We edit the fact ( Mila Kunis, type of union,\nDomestic partnership) and replace the head entity by Marc\nShaiman (Golden Label). The points close to the center\nrefer to the entities the model prefers. Left: predictions of the\noriginal KG embeddings. Right: predictions after editing.\nlocality, unlike MEND, which sees a notable decline. We at-\ntribute this to KE andKGEditor employing FFN (termed as\nknowledge neurons (Dai et al. 2022b)), which can be scaled\nto accommodate a higher number of edited facts.\nWe visualize entities before and after editing for clearer\nmodel insights. Figure 6 illustrates a substantial shift in pre-\ndicted entities‚Äô positions before and after the model editor‚Äôs\napplication. These points signify that the inferred entity is\nprovided with a head entity and relation, with the right center\nrepresenting the golden standard. Upon editing the model,\nthe correct entity distinctly emerges close to the circle‚Äôs cen-\nter (knowledge reliability), while the relevant distances of\nother entities remain largely unaltered, thereby showcasing\nthe effectiveness of editing KG embeddings.\nDiscussion and Conclusion\nOur proposed task of editing the KGE model allows for di-\nrect modification of knowledge to suit specific tasks, thereby\nimproving the efficiency and accuracy of the editing process.\nContrary to earlier pre-training language model tasks for\nediting, our approach relies on KG facts to modify knowl-\nedge, without using pre-trained model knowledge. These\nmethods enhance performance and offer important insights\nfor research in knowledge representation and reasoning.\nFuture Work\nEditing KGE models presents ongoing issues, notably\nhandling intricate knowledge and many-to-many relations.\nEdited facts, arising from such relations, can bias the model\nto the edited entity, overlooking other valid entities. Besides,\nthe experimental KGE models are small, all leveraging the\nstandard BERT. Yet, with rising large-scale generative mod-\nels (LLMs) like LLaMA (Touvron et al. 2023), ChatGLM\n(Zeng et al. 2022), and Alpaca (Taori et al. 2023), the de-\nmand to edit LLMs grows. In the future, we aim to design\nmodels to edit knowledge with many-to-many relations and\nintegrate LLMs editing techniques.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17841\nAcknowledgments\nWe thank the anonymous reviewers for their kind comments.\nThis work was supported by the National Natural Science\nFoundation of China (No. 62206246), the Fundamental Re-\nsearch Funds for the Central Universities (226-2023-00138),\nZhejiang Provincial Natural Science Foundation of China\n(No. LGG22F030011), Ningbo Natural Science Founda-\ntion (2021J190), Yongjiang Talent Introduction Programme\n(2021A-156-G), CCF-Tencent Rhino-Bird Open Research\nFund, and Information Technology Center and State Key\nLab of CAD&CG, Zhejiang University.\nReferences\nBordes, A.; Usunier, N.; Garc ¬¥ƒ±a-Dur¬¥an, A.; Weston, J.; and\nYakhnenko, O. 2013. Translating Embeddings for Modeling\nMulti-relational Data. In NeurIPS.\nBrown, T. B.; and et al. 2020. Language Models are Few-\nShot Learners. In NeurIPS.\nCao, B.; Lin, H.; Han, X.; Sun, L.; Yan, L.; Liao, M.; Xue,\nT.; and Xu, J. 2021. Knowledgeable or Educated Guess?\nRevisiting Language Models as Knowledge Bases. In ACL.\nCao, N. D.; Aziz, W.; and Titov, I. 2021. Editing Factual\nKnowledge in Language Models. In EMNLP.\nChen, C.; Wang, Y .; Li, B.; and Lam, K. 2022a. Knowl-\nedge Is Flat: A Seq2Seq Generative Framework for Various\nKnowledge Graph Completion. In COLING.\nChen, X.; Zhang, N.; Xie, X.; Deng, S.; Yao, Y .; Tan,\nC.; Huang, F.; Si, L.; and Chen, H. 2022b. Know-\nPrompt: Knowledge-aware Prompt-tuning with Synergistic\nOptimization for Relation Extraction. In WWW.\nCohen, R.; Biran, E.; Yoran, O.; Globerson, A.; and Geva,\nM. 2023. Evaluating the Ripple Effects of Knowledge Edit-\ning in Language Models. CoRR, abs/2307.12976.\nDai, D.; Dong, L.; Hao, Y .; Sui, Z.; Chang, B.; and Wei, F.\n2022a. Knowledge Neurons in Pretrained Transformers. In\nACL.\nDai, D.; Dong, L.; Hao, Y .; Sui, Z.; Chang, B.; and Wei, F.\n2022b. Knowledge Neurons in Pretrained Transformers. In\nACL.\nDettmers, T.; Minervini, P.; Stenetorp, P.; and Riedel, S.\n2018. Convolutional 2D Knowledge Graph Embeddings. In\nAAAI.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., NAACL.\nDong, Q.; Dai, D.; Song, Y .; Xu, J.; Sui, Z.; and Li, L.\n2022. Calibrating Factual Knowledge in Pretrained Lan-\nguage Models. In EMNLP, Findings of EMNLP.\nFei, L.; Wu, T.; and Khan, A. 2021. Online Updates of\nKnowledge Graph Embedding. In Complex Networks 2021,\nvolume 1016 ofStudies in Computational Intelligence, 523‚Äì\n535. Springer.\nGeva, M.; Bastings, J.; Filippova, K.; and Globerson, A.\n2023. Dissecting Recall of Factual Associations in Auto-\nRegressive Language Models. CoRR, abs/2304.14767.\nHan, X.; and et al. 2021. Pre-trained models: Past, present\nand future. AI Open.\nHan, X.; Li, R.; Li, X.; and Pan, J. Z. 2023. A divide and con-\nquer framework for Knowledge Editing. Knowledge-Based\nSystems, 110826.\nHase, P.; Bansal, M.; Kim, B.; and Ghandeharioun, A. 2023.\nDoes Localization Inform Editing? Surprising Differences\nin Causality-Based Localization vs. Knowledge Editing in\nLanguage Models. CoRR, abs/2301.04213.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Classi-\nfication with Graph Convolutional Networks. In ICLR.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020. BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In ACL.\nLin, Y .; Liu, Z.; Sun, M.; Liu, Y .; and Zhu, X. 2015. Learn-\ning Entity and Relation Embeddings for Knowledge Graph\nCompletion. In Bonet, B.; and Koenig, S., eds., AAAI.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2021a. Pre-train, prompt, and predict: A systematic sur-\nvey of prompting methods in natural language processing.\nACM Computing Surveys.\nLiu, S.; Grau, B. C.; Horrocks, I.; and Kostylev, E. V . 2021b.\nINDIGO: GNN-Based Inductive Knowledge Graph Com-\npletion Using Pair-Wise Encoding. In NeurIPS, 2034‚Äì2045.\nLiu, X.; Ji, K.; Fu, Y .; Tam, W.; Du, Z.; Yang, Z.; and Tang,\nJ. 2022. P-Tuning: Prompt Tuning Can Be Comparable to\nFine-tuning Across Scales and Tasks. In ACL.\nLv, X.; Lin, Y .; Cao, Y .; Hou, L.; Li, J.; Liu, Z.; Li, P.; and\nZhou, J. 2022. Do Pre-trained Models Benefit Knowledge\nGraph Completion? A Reliable Evaluation and a Reasonable\nApproach. In Findings of ACL.\nMeng, K.; Bau, D.; Andonian, A.; and Belinkov, Y . 2022.\nLocating and Editing Factual Knowledge in GPT. In\nNeurIPS.\nMitchell, E.; Lin, C.; Bosselut, A.; Finn, C.; and Manning,\nC. D. 2022. Fast Model Editing at Scale. In ICLR.\nNickel, M.; Rosasco, L.; and Poggio, T. A. 2016. Holo-\ngraphic Embeddings of Knowledge Graphs. In AAAI.\nNickel, M.; Tresp, V .; and Kriegel, H. 2011. A Three-Way\nModel for Collective Learning on Multi-Relational Data. In\nICML.\nPan, S.; Luo, L.; Wang, Y .; Chen, C.; Wang, J.; and Wu,\nX. 2023. Unifying Large Language Models and Knowledge\nGraphs: A Roadmap. CoRR, abs/2306.08302.\nPetroni, F.; Rockt ¬®aschel, T.; Riedel, S.; Lewis, P. S. H.;\nBakhtin, A.; Wu, Y .; and Miller, A. H. 2019. Language Mod-\nels as Knowledge Bases? In EMNLP.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res., 21: 140:1‚Äì140:67.\nSaxena, A.; Kochsiek, A.; and Gemulla, R. 2022. Sequence-\nto-Sequence Knowledge Graph Completion and Question\nAnswering. In ACL.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17842\nSinitsin, A.; Plokhotnyuk, V .; Pyrkin, D. V .; Popov, S.; and\nBabenko, A. 2020. Editable Neural Networks. In ICLR.\nSun, Z.; Deng, Z.; Nie, J.; and Tang, J. 2019. RotatE: Knowl-\nedge Graph Embedding by Relational Rotation in Complex\nSpace. In ICLR.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford\nalpaca. Accessed: 2023-03-\n13.\nToutanova, K.; Chen, D.; Pantel, P.; Poon, H.; Choudhury,\nP.; and Gamon, M. 2015. Representing Text for Joint Em-\nbedding of Text and Knowledge Bases. In EMNLP.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023. LLaMA: Open and Efficient Foundation Language\nModels. CoRR, abs/2302.13971.\nVashishth, S.; Sanyal, S.; Nitin, V .; and Talukdar, P. P. 2020.\nComposition-based Multi-Relational Graph Convolutional\nNetworks. In ICLR.\nVelickovic, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li`o,\nP.; and Bengio, Y . 2018. Graph Attention Networks. In\nICLR.\nWang, B.; Shen, T.; Long, G.; Zhou, T.; Wang, Y .; and\nChang, Y . 2021a. Structure-Augmented Text Representa-\ntion Learning for Efficient Knowledge Graph Completion.\nIn WWW.\nWang, L.; Zhao, W.; Wei, Z.; and Liu, J. 2022a. SimKGC:\nSimple Contrastive Knowledge Graph Completion with Pre-\ntrained Language Models. In ACL.\nWang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017. Knowl-\nedge Graph Embedding: A Survey of Approaches and Ap-\nplications. TKDE.\nWang, R.; Tang, D.; Duan, N.; Wei, Z.; Huang, X.; Ji, J.;\nCao, G.; Jiang, D.; and Zhou, M. 2021b. K-Adapter: Infus-\ning Knowledge into Pre-Trained Models with Adapters. In\nFindings of ACL.\nWang, X.; Gao, T.; Zhu, Z.; Zhang, Z.; Liu, Z.; Li, J.; and\nTang, J. 2021c. KEPLER: A Unified Model for Knowl-\nedge Embedding and Pre-trained Language Representation.\nTrans. Assoc. Comput. Linguistics, 9: 176‚Äì194.\nWang, X.; He, Q.; Liang, J.; and Xiao, Y . 2022b. Language\nModels as Knowledge Embeddings. In IJCAI.\nWang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowl-\nedge Graph Embedding by Translating on Hyperplanes. In\nAAAI.\nWei, Y .; Chen, W.; Li, Z.; and Zhao, L. 2021. Incremen-\ntal Update of Knowledge Graph Embedding by Rotating on\nHyperplanes. In ICWS 2021, 516‚Äì524. IEEE.\nWu, T.; Shiri, F.; Kang, J.; Qi, G.; Haffari, G.; and Li, Y .-F.\n2022a. KC-GEE: Knowledge-based Conditioning for Gen-\nerative Event Extraction.\nWu, T.; Wang, G.; Zhao, J.; Liu, Z.; Qi, G.; Li, Y .; and Haf-\nfari, G. 2022b. Towards relation extraction from speech. In\nGoldberg, Y .; Kozareva, Z.; and Zhang, Y ., eds., Proceed-\nings of the 2022 Conference on Empirical Methods in Natu-\nral Language Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, 10751‚Äì10762. Asso-\nciation for Computational Linguistics.\nXie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M. 2016. Rep-\nresentation Learning of Knowledge Graphs with Entity De-\nscriptions. In Schuurmans, D.; and Wellman, M. P., eds.,\nAAAI.\nXie, X.; Zhang, N.; Li, Z.; Deng, S.; Chen, H.; Xiong, F.;\nChen, M.; and Chen, H. 2022. From Discrimination to\nGeneration: Knowledge Graph Completion with Generative\nTransformer. In WWW.\nYang, B.; Yih, W.; He, X.; Gao, J.; and Deng, L. 2015. Em-\nbedding Entities and Relations for Learning and Inference\nin Knowledge Bases. In ICLR.\nYang, Z.; Ma, J.; Chen, H.; Zhang, J.; and Chang, Y .\n2022. Context-Aware Attentive Multilevel Feature Fusion\nfor Named Entity Recognition. IEEE Transactions on Neu-\nral Networks and Learning Systems.\nYang, Z.; Ma, J.; Chen, H.; Zhang, Y .; and Chang, Y . 2021.\nHiTRANS: A Hierarchical Transformer Network for Nested\nNamed Entity Recognition. In Findings of ACL.\nYao, L.; Mao, C.; and Luo, Y . 2019. KG-BERT: BERT for\nKnowledge Graph Completion. CoRR, abs/1909.03193.\nYao, Y .; Wang, P.; Tian, B.; Cheng, S.; Li, Z.; Deng, S.;\nChen, H.; and Zhang, N. 2023. Editing Large Language\nModels: Problems, Methods, and Opportunities. CoRR,\nabs/2305.13172.\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang,\nZ.; Xu, Y .; Zheng, W.; Xia, X.; Tam, W. L.; Ma, Z.; Xue, Y .;\nZhai, J.; Chen, W.; Zhang, P.; Dong, Y .; and Tang, J. 2022.\nGLM-130B: An Open Bilingual Pre-trained Model. CoRR,\nabs/2210.02414.\nZhang, N.; Bi, Z.; Liang, X.; Cheng, S.; Hong, H.; Deng, S.;\nZhang, Q.; Lian, J.; and Chen, H. 2022a. OntoProtein: Pro-\ntein Pretraining With Gene Ontology Embedding. In ICLR.\nZhang, N.; Deng, S.; Sun, Z.; Chen, J.; Zhang, W.; and Chen,\nH. 2020a. Relation Adversarial Network for Low Resource\nKnowledge Graph Completion. In WWW.\nZhang, N.; Li, L.; and et al. 2022. Differentiable\nPrompt Makes Pre-trained Language Models Better Few-\nshot Learners. In ICLR.\nZhang, Z.; Cai, J.; Zhang, Y .; and Wang, J. 2020b. Learning\nHierarchy-Aware Knowledge Graph Embeddings for Link\nPrediction. In AAAI.\nZhang, Z.; Liu, X.; Zhang, Y .; Su, Q.; Sun, X.; and He, B.\n2020c. Pretrain-KGE: Learning Knowledge Representation\nfrom Pretrained Language Models. In Findings of EMNLP.\nZhang, Z.; Wang, J.; Ye, J.; and Wu, F. 2022b. Rethinking\nGraph Convolutional Networks in Knowledge Graph Com-\npletion. In WWW.\nZhao, W. X.; Zhou, K.; Li, J.; and et al. 2023. A Survey of\nLarge Language Models. CoRR, abs/2303.18223.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17843",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6370530724525452
    },
    {
      "name": "Graph",
      "score": 0.4708632528781891
    },
    {
      "name": "Knowledge graph",
      "score": 0.4479026794433594
    },
    {
      "name": "Natural language processing",
      "score": 0.39486801624298096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3297259211540222
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3263748288154602
    }
  ]
}