{
  "title": "High entropy alloy property predictions using a transformer-based language model",
  "url": "https://openalex.org/W4409211114",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2316823663",
      "name": "Spyros Kamnis",
      "affiliations": [
        "University of Thessaly",
        "Castolin Eutectic (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A4220109959",
      "name": "Konstantinos Delibasis",
      "affiliations": [
        "University of Thessaly"
      ]
    },
    {
      "id": "https://openalex.org/A2316823663",
      "name": "Spyros Kamnis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4220109959",
      "name": "Konstantinos Delibasis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2003975937",
    "https://openalex.org/W2058085399",
    "https://openalex.org/W2534691303",
    "https://openalex.org/W3163005862",
    "https://openalex.org/W2939669192",
    "https://openalex.org/W2007115257",
    "https://openalex.org/W4390404105",
    "https://openalex.org/W3026701187",
    "https://openalex.org/W3007409912",
    "https://openalex.org/W2039061417",
    "https://openalex.org/W3215095412",
    "https://openalex.org/W2963238192",
    "https://openalex.org/W2793419846",
    "https://openalex.org/W3084235614",
    "https://openalex.org/W2945222528",
    "https://openalex.org/W2014539364",
    "https://openalex.org/W2078509656",
    "https://openalex.org/W2034957637",
    "https://openalex.org/W2034554206",
    "https://openalex.org/W2075087568",
    "https://openalex.org/W2999583717",
    "https://openalex.org/W4365148383",
    "https://openalex.org/W1532236978",
    "https://openalex.org/W4321602393",
    "https://openalex.org/W4283034711",
    "https://openalex.org/W2554813092",
    "https://openalex.org/W4313478764",
    "https://openalex.org/W4385973579",
    "https://openalex.org/W2078880044",
    "https://openalex.org/W2025910434",
    "https://openalex.org/W3000508506",
    "https://openalex.org/W4290997390",
    "https://openalex.org/W2591503963",
    "https://openalex.org/W4312076959",
    "https://openalex.org/W2942285390",
    "https://openalex.org/W4311266020",
    "https://openalex.org/W3153798888",
    "https://openalex.org/W4366769286",
    "https://openalex.org/W4390511864",
    "https://openalex.org/W4309477536",
    "https://openalex.org/W4389099741",
    "https://openalex.org/W3114674604",
    "https://openalex.org/W3098278721",
    "https://openalex.org/W2981474778",
    "https://openalex.org/W3049169802",
    "https://openalex.org/W3037248195"
  ],
  "abstract": "Abstract This study introduces a language transformer-based machine learning model to predict key mechanical properties of high-entropy alloys (HEAs), addressing the challenges due to their complex, multi-principal element compositions and limited experimental data. By pre-training the transformer on extensive synthetic materials data and fine-tuning it with specific HEA datasets, the model effectively captures intricate elemental interactions through self-attention mechanisms. This approach mitigates data scarcity issues via transfer learning, enhancing predictive accuracy for properties like elongation (%) and ultimate tensile strength compared to traditional regression models such as random forests and Gaussian processes. The model’s interpretability is enhanced by visualizing attention weights, revealing significant elemental relationships that align with known metallurgical principles. This work demonstrates the potential of transformer models to accelerate materials discovery and optimization, enabling accurate property predictions, thereby advancing the field of materials informatics. To fully realize the model’s potential in practical applications, future studies should incorporate more advanced preprocessing methods, realistic constraints during synthetic dataset generation, and more refined tokenization techniques.",
  "full_text": "High entropy alloy property \npredictions using a transformer-\nbased language model\nSpyros Kamnis1,2 & Konstantinos Delibasis1\nThis study introduces a language transformer-based machine learning model to predict key mechanical \nproperties of high-entropy alloys (HEAs), addressing the challenges due to their complex, multi-\nprincipal element compositions and limited experimental data. By pre-training the transformer on \nextensive synthetic materials data and fine-tuning it with specific HEA datasets, the model effectively \ncaptures intricate elemental interactions through self-attention mechanisms. This approach mitigates \ndata scarcity issues via transfer learning, enhancing predictive accuracy for properties like elongation \n(%) and ultimate tensile strength compared to traditional regression models such as random forests \nand Gaussian processes. The model’s interpretability is enhanced by visualizing attention weights, \nrevealing significant elemental relationships that align with known metallurgical principles. This work \ndemonstrates the potential of transformer models to accelerate materials discovery and optimization, \nenabling accurate property predictions, thereby advancing the field of materials informatics. To fully \nrealize the model’s potential in practical applications, future studies should incorporate more advanced \npreprocessing methods, realistic constraints during synthetic dataset generation, and more refined \ntokenization techniques.\nKeywords High entropy alloys, Language models, Materials, Design, Machine learning\nHigh-entropy alloys (HEAs) are an innovative class of materials distinguished by their multi-principal element \ncompositions, typically consisting of five or more elements in near-equiatomic ratios. Unlike traditional alloys, \nwhich are based on one or two primary elements, HEAs utilize high configurational entropy to stabilize simple \nsolid-solution phases instead of intermetallic compounds. This unique compositional approach results in \nexceptional properties, including high strength, excellent ductility, superior wear resistance, and remarkable \nthermal stability. These attributes make HEAs promising candidates for advanced applications in the aerospace, \nautomotive, and energy sectors. Designing HEAs involves navigating an extensive compositional space due to \nthe vast number of possible element combinations and concentrations1–10.\nTraditional design methods rely on phase diagrams and atomistic simulations to predict phase stability and \nmaterial properties. Phase diagrams are essential for understanding equilibrium phases and transformations; \nhowever, they become less reliable when extrapolating beyond known Gibbs energies. In multi-component \nsystems, interpolating Gibbs energies becomes increasingly challenging as the number of elements increases, \ncomplicating the prediction of phase formations in unexplored compositional regions. Atomistic simulations, \nsuch as those based on density functional theory (DFT), provide detailed insights into the electronic structure \nand thermodynamic properties of materials. DFT can predict phase stability, mechanical properties, and \nelectronic behaviour from first principles. However, the computational cost of DFT grows significantly with \nsystem size and complexity. For HEAs, which involve multiple principal elements and complex crystal structures, \nconstructing accurate DFT models requires large supercells to capture the inherent disorder and configurational \nentropy, making such calculations computationally prohibitive11–15\nTo address these challenges, machine learning (ML) algorithms have been employed to predict the properties \nand phase formations of HEAs, thereby guiding experimental efforts more efficiently. Conventional ML models, \nincluding artificial neural networks (ANNs) 16–23, support vector machines (SVMs) 24–26, Gaussian process \n(GP)27–32, k-nearest neighbours (KNN)33,34 and random forests (RFs)35,36, have been used to correlate elemental \nfeatures and processing parameters with material properties. Advanced algorithms, such as deep learning (DL) \nmodels—including deep neural networks (DNNs) and convolutional neural networks (CNNs) 37–39 have also \nbeen applied to capture the complex nonlinear relationships present in HEA systems.\n1Department of Computer Science and Biomedical Informatics, University of Thessaly, 35100 Lamia, Greece. \n2Castolin Eutectic-Monitor Coatings Ltd., Newcastle upon Tyne NE29 8SE, UK. email: spyros.kamnis@gmail.com\nOPEN\nScientific Reports |        (2025) 15:11861 1| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports\n\nDespite these advancements, ML applications in HEA design encounter significant obstacles. A primary \nchallenge is the reliance on large, high-quality datasets, which are often limited due to experimental constraints. \nInsufficient data prevents the training of robust ML models, often leading to overfitting and poor generalization \non unseen compositions. Additionally, traditional ML models may struggle to capture the high-dimensional \nfeature spaces and long-range elemental interactions characteristic of HEAs. The “black-box” nature of \ncomplex ML models also poses interpretability issues, making it difficult to discern the underlying factors \ninfluencing predictions. To overcome these limitations, we propose a new approach that involves pre-training \na transformer-based model on extensive materials data and fine-tuning it with experimental data specific to \nHEAs. Transformers, originally developed for natural language processing tasks 40,41, utilize self-attention \nmechanisms to model complex relationships within sequences, effectively capturing long-range dependencies \nand interactions. By pre-training on large-scale materials datasets, the transformer model learns generalized \nrepresentations of elemental properties and interactions. Fine-tuning with experimental HEA data allows the \nmodel to adapt these representations to the specific complexities of HEAs, enhancing its predictive capabilities \neven with limited data42.\nOur approach offers several advantages over traditional methods and addresses some of the pressing challenges \nin the field. By leveraging pre-trained models, we mitigate data scarcity issues through the transfer of knowledge \nfrom larger, accurately calculated datasets that do not rely solely on experimental data. This transfer significantly \nimproves model performance on small HEA datasets, enhancing data efficiency. The transformer’s ability to \ncapture complex, high-dimensional relationships enhances its generalization to new, unseen alloy compositions, \nfacilitating the discovery of novel HEAs with desired properties. Moreover, unlike DFT simulations, our method \ndoes not require extensive computational resources once the model is pre-trained, making it more practical \nfor screening large compositional spaces. Additionally, the self-attention mechanism, inherent in transformers, \nprovides valuable insights into feature importance and elemental interactions, addressing the interpretability \nchallenges often associated with traditional deep learning models. Furthermore, fine-tuning allows the model \nto be easily adapted to different HEA systems or target properties without the need to retrain from scratch, \nenhancing its adaptability and making it a versatile tool for materials design.\nIn this work, we compare our proposed transformer-based approach with traditional regression models \nsuch as random forests (RF), Gaussian processes (GP), and gradient boosting in their ability to predict two \nmacroscopic mechanical properties: elongation at break and ultimate tensile strength (UTS). While regression \nmodels may perform variably depending on the task and dataset size, the proposed model consistently achieves \nhigher performance across different tasks, offering universal applicability by integrating the strengths of various \nmodels into a single framework. We assess the impact of pre-training dataset size on model performance, \nconfirming that larger datasets enhance the model’s capability to capture complex elemental interactions. \nAdditionally, we employ interpretability techniques like attention weight visualization to elucidate the model’s \ndecision-making process. This work builds on our previous short communication publication43 and serves as a \nproof of concept for applying transformer-based models to HEA design. We recognize that further enhancements \nto the pre-training datasets, particularly by incorporating more and diverse thermodynamic properties, could \nsignificantly enrich the model’s understanding of material behaviours. Additionally, experimenting with different \ntransformer architectures and large language models (LLMs) may prove pivotal in achieving even greater \npredictive performance. These efforts could refine the model’s ability to capture intricate elemental interactions \nand thermodynamic principles, ultimately accelerating the discovery of HEAs with optimized properties.\nData and methodology\nA comprehensive comparison has been undertaken for various regression models to predict material properties \nusing two distinct datasets: ultimate tensile strength (UTS) and elongation. The UTS dataset is characterized by \nits relative simplicity, reduced experimental uncertainties and larger dataset making it relatively straightforward \nfor predictive modelling. In contrast, the elongation dataset presents a more formidable challenge due to smaller \ndataset size, inherent measurement errors and noise, reflecting real-world complexities where data imperfections \nare commonplace.\nA critical aspect of this work is the utilization of raw, experimental-driven data without any preprocessing \nsteps such as cleansing and outlier detection. Traditionally, these preprocessing techniques are employed to \nenhance model accuracy by mitigating the impact of anomalies and noise. However, our deliberate omission \nof these steps serves a dual purpose: it allows us to assess how different models inherently handle noisy and \nimperfect data, and it provides insights into their robustness in less controlled environments.\nThe selection of regression models was based on popularity as state-of-the-art in the field of HEA material \ninformatics while encompassing algorithms known for their efficacy with varying dataset sizes. Some models, \nlike random forests and gradient boosting regressors, are renowned for their superior performance with large, \ncomplex datasets due to their ensemble learning capabilities. Conversely, models such as Gaussian process, \nsupport vector regression and K-nearest neighbours are often preferred for smaller datasets where overfitting is \na concern.\nAssumptions and scope\nThis study aims to evaluate the predictive capabilities of a transformer-based model for mechanical properties \nof high-entropy alloys (HEAs) under a consistent and controlled dataset. To maintain this focus, several \nsimplifying assumptions were made. First, the dataset used does not account for variations in cooling rates \nor second-phase effects that can significantly influence microstructural evolution and mechanical behavior in \nas-cast HEAs. While these factors are critical for a comprehensive understanding of material properties, their \ninclusion requires detailed experimental datasets and microstructural characterization, which are beyond the \nscope of this work.\nScientific Reports |        (2025) 15:11861 2| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nSecond, both tensile and compressive test data were considered collectively, assuming equivalent trends in \nmaterial response. This decision was made to emphasize model comparison rather than develop a universally \napplicable model for HEA behavior. Additionally, hyperparameter optimization was intentionally omitted to \nensure fair comparisons between models using standard configurations, avoiding computational overhead and \npotential overfitting risks. These omissions highlight the study’s primary aim—to demonstrate the feasibility and \nadvantages of transformer-based models over traditional regressors using identical datasets and experimental \nconditions. Future work could extend this framework to incorporate additional physical phenomena, \nmicrostructural effects, and detailed hyperparameter tuning to further enhance the model’s generalizability and \naccuracy.\nDataset analysis-supervised learning\nThe elongation and UTS data are for as-cast HEAs tested under both compression and tension and at Room \ntemperature. The dataset is available at:  h t t p s :  / / g i t h  u b . c o m  / S P S - C  o a t i n  g s / L a n  g u a g e -  M o d e l -  f o r - H E A.\nElongation fine tuning dataset\nThe bar chart in Fig.  1d shows the total amount of each element present in the dataset, with Ni, Fe, Co, and \nCr being dominant. This overrepresentation may introduce bias into the model, as the predictions could skew \ntoward alloys with these elements, reducing the accuracy for compositions with less frequent elements like Si, Sc, \nor Sn. This imbalance could limit the generalizability of the model to diverse alloy compositions. The histogram \n(Fig.  1b) for elongation percentages reveals a broad distribution, with peaks around 10% and 50%, showing \nvariability in elongation behaviour across different compositions. The correlation heatmap (Fig.  1a) provides \nadditional insight into feature importance. While properties like modulus mismatch and ionization energy show \nmoderate correlation with elongation, many other features (e.g., number of elements, melting temperature) show \nweak or negative correlations. These weak correlations suggest that compositional features alone may not be \nsufficient to predict elongation accurately, as elongation is influenced by further microstructural characteristics. \nFigure 1c chart shows the distribution of the number of elements per composition revealing that most alloys \ncontain 5 or 6 elements as expected for a HEA focused task. This skew may lead to biased model performance \nfor alloys with 7+ elements, which are underrepresented. In conclusion, the dataset’s small size, compositional \nimbalance, and non-linear trends in elongation introduce significant challenges for a regression model. The \nmodel is likely to struggle with generalizing to less frequent element combinations. For a production ready \npredictive model for this target property, additional data and are required to improve prediction accuracy. This \nis out of this work’s scope.\nUltimate tensile strength (UTS) fine tuning dataset\nSimilarly, the bar chart in Fig. 2d illustrates the total occurrence of each element in the dataset, showing that Ni, \nFe, Ti, Co, and Cr are the most prevalent. Likewise, this dominance of specific elements could introduce bias \ninto the model’s predictions, making it more accurate for alloys with these common elements. The histogram \n(Fig. 2b) displaying the UTS distribution shows a considerable spread, with UTS values varying broadly across the \ndataset. Despite this wide spread, the distribution is more uniform than elongation, suggesting that a regression \nmodel might perform better on UTS predictions compared to elongation, given that UTS often has a more direct \nrelationship with elemental composition. In heatmap (Fig. 2a) appears that the most correlated features include \nshear modulus difference, mixing entropy, and electronegativity difference. Several other features, like cohesive \nenergy and melting temperature, also contribute moderately, while negative correlations, such as with modulus \nmismatch and mean VEC, indicate that not all factors have a straightforward impact on UTS. These correlations \nsuggest that the dataset captures meaningful relationships, but the weaker features may hinder the model’s \nability to make precise predictions. The composition of alloys based on the number of elements, as shown in \nFig. 2c, reveals that most alloys contain 5–6 elements. This skew is common in HEA studies but could present \na challenge, as compositions with 7 or more elements are underrepresented. In conclusion, despite the small \ndataset and compositional imbalances, the regression task for UTS predictions may result in better performance \nthan elongation, as the correlations between features and UTS are stronger and more direct.\nOutlier detection\nThe two datasets were analysed to identify to what extend outliers may be contained in the dataset. The analysis \nhas been done using the Z-scores approach. For a given dataset, the Z-score of an individual data point xi is \ncalculated using the following formula:\n Zi = xi − µ\nσ\nwhere xi =  individual data point, µ =  mean (average) of the dataset and σ =  standard deviation of the dataset.\nThis begins by calculating the mean of the target property measurements. The mean serves as a central \nreference point, around which most data points are expected to cluster. Alongside the mean, the standard \ndeviation is determined to assess how much the target property values vary or spread out from this average. \nA smaller standard deviation indicates that the data points are tightly grouped around the mean, suggesting \nconsistency within the dataset. Conversely, a larger standard deviation implies greater variability, meaning \nthe data points are more dispersed over a wider range of values. Once the mean and standard deviation are \nestablished, each individual measurement is standardized to determine its relative position within the overall \ndata distribution.\nScientific Reports |        (2025) 15:11861 3| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nThis standardization process results in the Z-score for each data point. A positive Z-score indicates that \nthe value is above the mean, while a negative Z-score signifies that it is below the mean. This standardization \ntransforms the data into a common scale, allowing for meaningful comparisons across different measurements. \nTo identify outliers, the absolute value of each Z-score is considered, ensuring that both unusually high and \nunusually low values are accounted regardless of their direction. A common practice is to set a threshold for the \nZ-score, typically at 3. According to the empirical rule, in a normal distribution, approximately 99.7% of data \npoints lie within three standard deviations from the mean. Therefore, any data point with an absolute Z-score \nexceeding this threshold is regarded as rare and potentially erroneous, qualifying it as an outlier.\nFig. 1. Dataset analysis of the elongation target property.\n \nScientific Reports |        (2025) 15:11861 4| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nIn Fig.  3a, elongation values decrease rapidly from the highest point, with most data following a smooth \ndownward trend. A single outlier, marked in red, stands out at the beginning of the plot, showing a much higher \nvalue compared to the other points. This deviation is also captured by the Z-score approach, as it significantly \nexceeds the threshold. In Fig. 3b, the data follows a rising trend with most points forming a nearly linear pattern \nuntil the higher values, where the points sharply increase. Although no outliers are explicitly marked, data points \nat the upper end of the scale some deviation from the mean are expected to form potential outliers. Overall, the \nZ-scores method identifies some extreme values but not significant outliers were detected to denote extensive \nrare events, or anomalies.\nFig. 2. Dataset analysis of the UTS target property.\n \nScientific Reports |        (2025) 15:11861 5| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nFeature engineering\nThe feature engineering process begins by parsing chemical formulas to extract individual elements and their \nrespective fractions within each alloy. Using regular expressions, the script identifies each element and determines \nits proportion, ensuring accurate representation of the alloy’s composition. Once the elemental composition is \nestablished, the script computes a series of thermodynamic properties that are pivotal for understanding and \npredicting the behaviour of materials. The formulas for calculating the properties are included in Table 1. These \nproperties among others, include mean valence electron concentration (mean VEC), which averages the number \nFig. 3. Z-score for both datasets (a Elongation and b UTS).\n \nScientific Reports |        (2025) 15:11861 6| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nof valence electrons per atom and influences electrical and magnetic characteristics. Electronegativity difference \nassesses the disparity in electron-attracting abilities among elements, affecting bond strength and corrosion \nresistance. Atomic radius difference evaluates variations in atomic sizes, impacting lattice strain and mechanical \nstrength.\nThe script further estimates the Y oung’s modulus, a measure of the alloy’s stiffness and resistance to \nelastic deformation, and shear modulus, which quantifies resistance to shear forces and overall mechanical \nrobustness. Modulus mismatch and shear modulus difference highlight disparities in mechanical properties \namong constituent elements, indicating potential internal stresses that could affect durability and performance. \nMixing enthalpy and mixing entropy provide insights into the thermodynamic stability and disorder introduced \nduring alloy formation, influencing phase formation and material homogeneity. Additional features such as \nelectron work function measure the energy required to remove electrons from the material’s surface, impacting \nelectrical conductivity and catalytic activity. Melting temperature predicts the thermal stability and suitability \nof alloys for high-temperature applications, while cohesive energy reflects the bond strength within the alloy, \ninfluencing hardness and durability. Average Ionization Energy assesses the energy required to ionize atoms, \naffecting electrical and thermal properties. The script also includes electronegativity difference using the Pauling \nscale and latent heat, which represents the energy involved in phase transitions crucial for processing and \nthermal management. By integrating these features into the dataset, the approach provides a comprehensive \nand multifaceted view of each alloy’s properties. This enriched dataset equips machine learning models with the \nnecessary information to predict complex material behaviours.\nGeneration of pre-training dataset\nIn the preparation of the pre-training dataset for predicting material properties, a systematic approach was \nemployed to generate and curate a diverse set of alloy compositions. Initially, a comprehensive list of metallic \nelements was established, each assigned specific weights to influence their selection probability. This weighting \nmechanism ensured a balanced and realistic distribution of elements across the generated alloys, reflecting their \nnatural abundance and relevance in material science. To achieve this diversity, compositions were categorized \nProperty Symbol Formula Units Description\nMean VEC VEC\n∑\ni\nxi · V ECi Electrons/atom Average number of valence electrons per atom\nElectronegativity difference δx\n√∑\ni\nxi\n(\nXi − X\n)2\nNone Standard deviation of electronegativity\nAtomic radius difference δr\n∑\ni\nxi\n(ri−r\nr\n)2\n× 100 % Percentage difference in atomic radius\nCalculated Y oung’s modulus E\n∑\ni\nxi · Ei GPa Weighted average of Y oung’s modulus\nMixing enthalpy ∆Hmix\n∑\ni\n∑\nj̸ =i\nxixj ∆Hij kJ/mol Enthalpy change during mixing of components\nMixing entropy ∆Smix\n−R\n∑\ni\nxilnxi J/(mol K) Entropy change during mixing of components\nWork function φ\n∑\ni\nxi · φi eV Average electron work function of the alloy\nShear modulus G\n∑\ni\nxi · Gi GPa Weighted average of shear modulus\nModulus mismatch ∆M\n∑\ni\nxi\n(\n2 (Gi−G)\nGi+G\n)2\nNone Mismatch in modulus across alloy components\nShear modulus difference ∆G\n∑\ni\nxi\n(\n1 − Gi\nG\n)2\nNone Difference in shear modulus relative to the mean\nMelting temperature Tm\n∑\ni\nxi · Ti K Weighted average melting point of the alloy\nCohesive energy Ecoh\n∑\ni\nxi · Ecoh,i eV/atom Average energy needed to break atomic bonds\nIonization energy IE\n∑\ni\nxi · IEi eV Average first ionization energy\nPauling electronegativity ∆Xp\n∑\ni\n∑\nj̸ =i\nxixj (XP,i − XP,j )2\nNone Measure of electronegativity difference using Pauling scale\nTable 1. Thermodynamic properties used as input features. xi: is the atomic fraction of component i, Xi: \nElectronegativity of component i, ri: atomic radius of component i, X and  r are the average electronegativity \nand atomic radius of the alloy, Ei, Gi, Ti, φi, Ecoh,i, IEi  are the Y oung’s modulus, shear modulus, melting \ntemperature, work function, cohesive energy, and ionization energy of component  i, respectively. ∆Hij  is the \nmixing enthalpy between components i, j and R is the universal gas constant.\n \nScientific Reports |        (2025) 15:11861 7| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\ninto equimolar and non-equimolar types. Equimolar alloys featured elements in equal proportions, promoting \nuniformity and simplifying the analysis of elemental interactions. In contrast, non-equimolar alloys incorporated \nunequal proportions of elements, introducing variability and complexity that better mimic real-world materials. \nUniqueness of each alloy composition was a critical consideration. By ensuring that no element was duplicated \nwithin a single alloy, the dataset maintained chemical validity and prevented redundancy. This was achieved \nthrough careful selection processes that randomly chose elements based on their weighted probabilities while \nenforcing the constraint of unique elemental presence within each composition.\nThe distribution of elements within the dataset was rigorously validated to confirm adherence to the intended \nfrequencies and proportions. Visualization techniques, were employed to assess the occurrence of each element, \nensuring that the dataset accurately represented the desired elemental distribution. Additionally, comprehensive \nchecks were conducted to identify and eliminate any duplicate entries that could potentially skew the dataset’s \nintegrity. Further refinement involved filtering out any compositions with zero values in critical feature columns. \nThis step was essential to prevent the introduction of misleading or incomplete data points that could adversely \naffect the pre-training process. The final dataset was enriched with a wide array of the thermodynamic properties \n(features) as presented in Table 1.\nIt is acknowledged that certain combinations may violate thermodynamic stability or solubility limits. \nWhile these constraints were not explicitly imposed during dataset generation, the fine-tuning phase using \nexperimentally validated HEA datasets addresses this limitation by refining the model’s predictive capabilities \nfor physically feasible compositions. Future iterations of this work will incorporate thermodynamic stability \nconstraints to enhance the pre-training dataset’s physical validity.\nTo further assess the pre-training dataset quality, we applied a t-distributed stochastic neighbor embedding \n(t-SNE) approach (Fig.  4). This dimensionality reduction tool transforms high-dimensional data into a two-\ndimensional space, allowing for the visualization of complex relationships and structures within the data. When \nanalysing pre-training and fine-tuning datasets, t-SNE plots may offer valuable insights into how these datasets \ninteract within a shared feature space. By representing each data point in two dimensions, t-SNE helps illustrate \nthe similarity and clustering of data from different sources, such as pre-training datasets and specific fine-tuning \ntasks like elongation and UTS.\nIn this work we utilise three pre-training datasets with varying sizes: 6 K, 75 K, and 150 K entries. As the \nsize of the pre-training dataset increases to 150 K entries, the t-SNE plot reveals a denser and more compact \nfeature space. This density indicates that the model has captured a broader and more nuanced array of patterns \nand representations from the extensive pre-training data. Consequently, the fine-tuning datasets for elongation \nand UTS show greater overlap with these dense pre-training clusters. This enhanced overlap suggests that the \nmodel can effectively leverage the rich, generalized features learned during pre-training, leading to improved \nperformance in downstream regression tasks. Moreover, a denser feature space facilitates better generalization, \nenabling the model to perform reliably on unseen data that falls within the comprehensive feature landscape \nestablished by the larger pre-training dataset.\nHowever, while t-SNE plots are powerful for visualizing data relationships, they come with certain limitations. \nThe stochastic nature of t-SNE means that results can vary between runs, potentially affecting reproducibility. \nAdditionally, t-SNE is primarily effective at preserving local structures, which means that the global relationships \nbetween clusters might be distorted, leading to possible misinterpretations of the data’s true structure. \nComputational intensity is another concern, especially with very large datasets, as t-SNE can be resource-\ndemanding and time-consuming. Furthermore, the visualization outcome is sensitive to hyperparameters \nFig. 4. t-SNE representation of pre-training and fine-tuning data.\n \nScientific Reports |        (2025) 15:11861 8| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nlike perplexity and learning rate, requiring careful tuning to avoid misleading representations. Despite these \nlimitations, when used appropriately, t-SNE plots remain a valuable tool for assessing the alignment and overlap \nbetween pre-training and fine-tuning datasets, providing critical insights that can guide model improvements \nand enhance overall performance.\nPre-training the transformer for downstream tasks\nThe pre-training phase of our study leverages a BERT-based masked language model (MLM)44 explicitly tailored \nfor chemical composition data integrated with numerical features. The process is shown schematically in Fig. 5. \nBERT is selected for material predictions after pre-training and fine-tuning because effectively understands \ncomplex contexts through its bidirectional processing, which is particularly important for analyzing elemental \nsequences where the relationship between elements depends on their surrounding context. By leveraging \nextensive pre-trained knowledge for transfer learning, BERT enhances its adaptability to various prediction tasks. \nIts ability to handle structured and intricate input representations, such as chemical formulas and elemental \narrangements, combined with state-of-the-art performance and rich feature extraction, makes it highly suitable \nfor material science applications.\nThis process begins with a custom tokenization strategy designed to accurately parse and represent chemical \nformulas. For instance, a composition like “Co1.2 Fe0.8 Ni1” is processed by extracting elemental fraction pairs \n(Co, 1.2), (Fe, 0.8), and (Ni, 1). These pairs are then sorted alphabetically and concatenated to form tokens \nsuch as “Co1.2 Fe0.8 Ni1” . Mathematically, each token Ti is constructed as Ti = Ei ◦ fi, where Ei is the \nelement symbol and fi its fraction. Once tokenized, these chemical compositions are combined with additional \nnumerical features from the dataset. Numerical columns are converted to strings and appended to the tokenized \ntext, resulting in a comprehensive input like “Co1.2 Fe0.8 Ni1 300 500” , where the numbers represent specific \nthermodynamic properties.\nThe combined text data is then split into training and validation sets (80–20%) to facilitate effective model \nevaluation. Utilizing the BERT tokenizer, each text sequence undergoes further tokenization, padding, and \ntruncation to ensure uniform input lengths. During this stage, the masked language modeling (MLM) objective is \napplied by randomly masking a subset of tokens within the input. For example, “Co1.2 Fe0.8 Ni1 300 500” might \nbecome “[MASK] Fe0.8 Ni1 300 500” . The MLM head, comprising a linear layer, predicts the masked tokens \nbased on their surrounding context. Specifically, the MLM head maps the final hidden states corresponding to \nthe masked token positions to a probability distribution over the tokenizer’s vocabulary:\n ˆTi = softmax (WmlmHi + bmlm)\nwhere Hi is the hidden state for the masked token, and Wmlm and bmlm are the weights and biases of the MLM \nhead. The model is trained to minimize the cross-entropy loss function:\nFig. 5. ( a) Transformer pre-training and (b) fine-tuning.\n \nScientific Reports |        (2025) 15:11861 9| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\n \nLMLM = −\n∑\ni∈M\nlogP (Ti|Context)\nwhere M represents the set of masked token positions, and P(Ti|Context) is the predicted probability of the \ntrue token given its surrounding context. Through this masked token prediction task, the model learns to \ngenerate contextualized embeddings that capture the intricate relationships between different elements within \na composition and their associated thermodynamic properties. For instance, the model internalizes patterns \nsuch as how varying fractions of Fe might correlate with specific material thermodynamic properties like VEC \nor δ. A custom callback monitors validation loss during training, ensuring that the best-performing model is \nsaved for subsequent fine-tuning. Upon completion, the pre-trained model encapsulates a deep understanding \nof chemical compositions, providing a robust foundation for downstream regression tasks aimed at predicting \nmaterial properties based on complex chemical data. This meticulous pretraining approach, combining domain-\nspecific tokenization with the powerful contextual learning capabilities of BERT, establishes a solid groundwork \nfor accurately modeling and predicting thermodynamic properties from chemical compositions.\nFine-tuning process of BERT model for regression\nThe fine-tuning phase builds upon the pre-trained BERT-based MLM 44 to adapt it for regression tasks aimed \nat predicting thermodynamic properties from chemical compositions and numerical features. The process is \nshown schematically in Fig. 5a and b\nThis process begins by loading the pre-tokenized dataset, which comprises chemical compositions and their \nassociated numerical attributes. To ensure robust evaluation and to mitigate overfitting, we employ K-fold \ncross-validation (K =5 ). For each fold, the dataset is partitioned into training and validation subsets (80–20%). \nWithin each training subset, numerical features are normalized using the Standard Scaler, transforming each \nfeature xj  into a standardized form:\n \n˜xj = xj − µxj\nσxj\nwhere µxj  and σxj  represent the mean and standard deviation of feature xj  within the training set. This \nnormalization ensures that all numerical features contribute equally to the model’s learning process. To prevent \ndata leaks the scaler is applied after data split into training and validation and not before.\nThe normalized numerical features are then concatenated with the tokenized chemical compositions to form \ncomprehensive input sequences, such as “Co1.2 Fe0.8 Ni1.0 7 400” , where the numbers correspond to specific \nthermodynamic and target properties like UTS and elongation. Utilizing the BERT tokenizer, each combined \ntext sequence undergoes further tokenization, padding, and truncation to a uniform length of 512 tokens, \nensuring consistency across inputs. These tokenized sequences are organized into custom PyTorch datasets, \nfacilitating efficient data handling during training.\nCentral to the fine-tuning process is the incorporation of the multi-head self-attention mechanism within \nthe BERT architecture. For each token in the input sequence, the model computes three distinct vectors: Query \n(Q), Key (K), and Value (V). These vectors are derived through learned linear transformations of the token’s \nhidden state:\n Qi = WQHi, Ki = WKHi, Vi = WV Hi\nwhere Hi is the hidden state of the i-th token, and WQ, WK, WV  are learned weight matrices. The self-\nattention mechanism calculates attention scores by taking the dot product of the Query vector of one token with \nthe key vectors of all tokens in the sequence, scaling them by the square root of the dimensionality \n(√dk\n)\n, and \napplying a softmax function to obtain attention weights:\n \nαij =\nexp\n(Qi·K⊤\nj√\ndk\n)\n∑L\nk=1 exp\n(\nQi·K⊤\nk√\ndk\n)\nThese attention weights are then used to compute a weighted sum of the value vectors, producing an output that \ncaptures contextual information from the entire sequence. By employing multiple attention heads, the model \ncan attend to different aspects of the input simultaneously, enhancing its ability to capture complex relationships \nbetween elements in the chemical compositions and their corresponding thermodynamic and mechanical \nproperties.\nFollowing the self-attention layers, the model includes a regression head-a linear layer that maps the [CLS] \ntoken’s final hidden state to a single scalar value:\n ˆy = W⊤\nreghCLS + breg\nwhere hCLS  is the hidden state of the [CLS] token, Wreg  is the weight vector, and breg  is the bias term. \nThis regression head enables the model to output continuous predictions corresponding to the thermodynamic \nproperties of interest.\nScientific Reports |        (2025) 15:11861 10| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nThe optimization process utilizes the AdamW optimizer that incorporates weight decay for regularization. \nParameters from specific Transformer layers, particularly the last three layers, are subjected to weight decay \n(λ =0 .02), while others are exempt. This selective regularization helps prevent overfitting by penalizing \nlarge weights in critical parts of the model. The learning rate is set to η =6 × 10−5, and a linear learning rate \nscheduler with warm-up steps is applied to facilitate smooth convergence during training.\nTraining is managed using the Trainer API, which oversees the optimization loop, handles gradient \ncalculations, and applies the defined learning rate schedule. The mean squared error (MSE), mean absolute error \n(MAE), and the coefficient of determination \n(\nR2)\n are employed as evaluation metrics to assess the model’s \nperformance:\n \nMSE = 1\nN\nN∑\ni=1\n(ˆyi − yi)2 , MAE = 1\nN\nN∑\ni=1\n|ˆyi − yi| ,R 2 =1 −\n∑N\ni=1 (ˆyi − yi)2\n∑N\ni=1 (yi − y)2\nwhere ˆyi and yi denote the predicted and true values, respectively, and y is the mean of the true values.\nA custom callback monitors validation loss throughout the training process, ensuring that the model with the \nlowest validation loss is saved for each fold. This best-performing model is then used for evaluation and further \nanalysis. Through the integration of the multi-head self-attention mechanism, the model effectively learns to \ngenerate contextualized embeddings that capture the intricate relationships between different elements within a \nchemical composition and their influence on thermodynamic properties. For example, the model may recognize \nthat higher fractions of Ni correlate with increased elongation values, adjusting its internal representations \naccordingly.\nAfter completing all folds, the performance metrics are aggregated to provide an overall assessment of the \nmodel’s predictive capabilities. Residual analyses, including scatter plots of actual versus predicted values and \ndistribution plots of residuals, are conducted to visualize and interpret the model’s accuracy and potential biases. \nThe fine-tuning process results in a robust model capable of accurately predicting material properties based on \ncomplex chemical compositions and numerical data, thereby offering valuable insights for materials science \napplications.\nMultiple regressor training\nIn this phase of our study, we focus on training and evaluating a suite of regression models to predict mechanical \nmacroscopic properties, specifically elongation and UTS, based on chemical compositions and associated \nthermodynamic features (Table 1). The approach employs various machine learning algorithms to ascertain the \nmost effective model for our predictive task, ensuring comprehensive coverage of different modeling paradigms.\nThe process commences with data loading, where the relevant dataset is imported using pandas. The \ndataset contains a ‘composition’ column, representing chemical formulas (Feature), 14 thermodynamic \nnumerical property columns (Features) and a ‘target property’ column. To prepare the data for regression, \nnon-numeric columns, particularly ‘composition’ , are processed to extract elemental fractions. A custom \nfunction leverages regular expressions to parse each chemical composition string, extracting element-symbol \nand fraction pairs. For example, a composition like “Co1.2 Fe0.8 Ni1” is transformed into a dictionary: \nE (S)= {(Co, 1.2) , (Fe, 0.8) , (Ni, 1.0)}. These elemental fractions are then organized into a DataFrame, with \nmissing elements filled with zeros to maintain consistent feature dimensions across samples.\nSubsequently, the numerical features are combined with the extracted elemental fractions to form the feature \nmatrix X, while the target vector y comprises the mechanical property values that we want to train the model to \npredict. To ensure that all features contribute uniformly to the learning process, we apply standard normalization \nusing the standard scaler in a similar way as for the language model approach. The same K-fold cross-validation \nwith K = 5 is also employed for the regression models ensuring that each model is evaluated on diverse data \nsplits, enhancing the reliability of performance metrics.\nThe Gaussian Process Regressor models the target variable y as a realization of a Gaussian process, \ncharacterized by a mean function m(x) and a covariance function k (x, x′): y (x) ∼ GP(m (x) ,k (x, x′)) . \nGiven training data Xtrain  and ytrain , the GPR predicts the distribution of y∗ at a new input x∗ as: \ny∗|x∗, Xtrain , ytrain ∼N\n(\nµ∗,σ 2\n∗\n)\n, where:\n \nµ∗ = k⊤\n∗\n(\nK + σ2\nnI\n)−1\nytrain\nσ2\n∗ = k (x∗, x∗) − k⊤\n∗\n(\nK + σ2\nnI\n)−1\nk∗\nHere, K is the covariance matrix computed from the training inputs, k∗ is the covariance vector between the new \ninput and training inputs, and σ2\nn represents the noise variance. Complete list of symbol explanations is included \nin Appendix A.\nThe random forest regressor operates by constructing an ensemble of decision trees during training. Each \ntree Ti is trained on a bootstrap sample of the training data and makes individual predictions Ti (x). The final \nprediction is the average of all tree predictions:\n \nˆy (x)= 1\nN\nN∑\ni=1\nTi (x)\nScientific Reports |        (2025) 15:11861 11| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nThis aggregation reduces variance and enhances generalization by leveraging the wisdom of multiple trees. \nThe Decision Tree Regressor partitions the feature space into hierarchical regions based on feature thresholds, \nforming a tree-like structure. Each leaf node represents a region where the prediction is the mean of the target \nvalues of the training samples within that region:\n \nˆy (x)= 1\n|L (x)|\n∑\ni∈L(x)\nyi\nwhere L(x) denotes the set of training samples falling into the same leaf node as the input x.\nThe gradient boosting regressor builds an additive model by sequentially training decision trees to minimize \na specified loss function. At each iteration m, a new tree Tm is trained on the residuals from the previous \nensemble:\n ˆy(m) (x)=ˆy(m−1) (x)+ ηTm (x)\nwhere η is the learning rate. The residuals are computed as:\n r(m)\ni = yi − ˆy(m−1) (xi)\nThis approach allows the model to focus on correcting the errors of the prior ensemble, leading to improved \nperformance over iterations. The K-nearest neighbors regressor predicts the target value for a new input x by \naveraging the target values of its K nearest neighbors in the feature space:\n \nˆy (x)= 1\nK\nK∑\ni=1\nyi\nwhere the nearest neighbors are determined based on a distance metric, typically Euclidean distance. This non-\nparametric method relies on the assumption that similar inputs have similar target values.\nResults and discussions\nIn this study, all models utilized the same set of features as depicted in Table 1. Hyperparameter tuning was not \nperformed for the models in this work. The decision to omit hyperparameter optimization was made to maintain \nconsistency across all models and to focus on evaluating their baseline performances under default settings. By \nusing the default hyperparameters provided by the relevant machine learning libraries (scikit-learn, Pytorch \nTransformers), we aimed to reduce computational complexity and avoid potential data leakage that could arise \nfrom extensive hyperparameter searches, especially given the limited size of our dataset. Additionally, all models \nwere trained on the same uncleaned dataset to ensure that the comparisons were fair and solely attributed to the \nmodels’ inherent capabilities rather than pre-processing differences.\nModel comparisons were made against appropriate baselines by evaluating each model’s performance \nusing standard metrics such as R-squared (R2), mean squared error (MSE) and mean average error (MAE). To \nenhance reproducibility and usability, the dataset used for training and evaluating the models is openly available \nalongside the codebase at [  h t t p s :  / / g i t h  u b . c o m  / S P S - C  o a t i n  g s / L a n  g u a g e -  M o d e l -  f o r - H E A]. The training runs \nwere conducted on a computing infrastructure comprising two NVidia A5000 GPUs with 24 Gb RAM each \nrunning on Windows OS, using Python 3.9 with all dependencies documented in the github link alongside the \nrequirements txt file. The total time taken to pre-train the transformer with 150 K entries was approximately \n48  h. Fine-tuning was much faster requiring approximately 10  min for each dataset and model parameters. \nFinally, no specific benchmark frameworks were used as the objective of this work is not to present a model \nwith higher accuracy than others reported in the literature, but rather to demonstrate the language model \nadvantages compared to other models when using same datasets, pre-processing, feature engineering, and \ntraining frameworks.\nModel evaluation\nAn extensive analysis has been performed to evaluate the performance of pre-trained Transformer models in \npredicting material properties, with emphasis on elongation and UTS, in comparison with traditional machine \nlearning models. The results, presented in Tables 2, 3, 4, 5 and 6, consistently demonstrate the superior predictive \ncapabilities of the pre-trained Transformer models. This superiority is attributed to the Transformer’s advanced \narchitecture, effective pre-training strategies, and its ability to learn complex representations from large datasets, \neven when only elemental compositions are provided as input without additional engineered features.\nTable 2 shows the performance of various models on elongation prediction. The pre-trained Transformer \nachieved the lowest mean squared error (MSE) of 0.428 and mean absolute error (MAE) of 0.449, along with the \nhighest coefficient of determination (R2) of 0.561. In contrast, the Transformer without pre-training recorded a \nhigher mean MSE of 0.457 and a lower mean R 2 of 0.531. Traditional models like random forest and Gaussian \nprocess performed reasonably well but did not surpass the pre-trained Transformer, with mean MSE values of \n0.452 and 0.470, respectively. The significant performance gap between the pre-trained and non-pre-trained \nTransformers underscores the critical role of pre-training in enhancing the model’s ability to capture the \nunderlying patterns related to elongation.\nScientific Reports |        (2025) 15:11861 12| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nSimilarly, Table 3 presents the models’ performance on UTS prediction, where the pre-trained Transformer \nagain outperformed all other models. It achieved the lowest mean MSE of 0.213 and the highest mean R 2 of \n0.785. The non-pre-trained Transformer exhibited a substantial decline in performance, with a mean MSE of \n0.336 and a mean R2 of 0.619. Baseline models such as gradient boosting and random forest showed competitive \nperformance but still fell short of the pre-trained Transformer’s accuracy. These findings highlight the \neffectiveness of pre-training in equipping the Transformer with a superior understanding of the relationships \nbetween material compositions and properties.\nTable 4 delves into the impact of the pre-training dataset size on the Transformer’s performance. As the \nnumber of compositions used for pre-training increased from 6000 to 150,000, there was a notable improvement \nFine tuning Mean MSE elongation (↓) Mean MSE UTS (↓) Mean MAE elongation (↓) Mean MAE UTS (↓) Mean R2 elongation (↑) Mean R2 UTS (↑)\nAll layers 0.428 0.250 0.449 0.348 0.561 0.754\nLayers 11, 12 0.445 0.212 0.478 0.335 0.545 0.785\nLayers 10, 11, 12 0.471 0.237 0.499 0.344 0.518 0.767\nLayers 9, 10, 11, 12 0.445 0.218 0.475 0.349 0.540 0.781\nLayers 4, 6, 8, 10, 12 0.446 0.210 0.484 0.342 0.538 0.786\nTable 5. Effect of number of trained attention layers on the fine tuned transformer model performance (test \nset). Metrics for scaled values. The bold indicate the best mean results in terms of the metrics used with fivefold \nvalidation. Layer-wise fine-tuning. The attention weights are kept frozen for the unselected layers. 150,000 \nCompositions pre-trained model with all input features.\n \nPre-trained model Mean MSE elongation (↓) Mean MSE UTS (↓) Mean MAE elongation (↓) Mean MAE UTS (↓) Mean R2 elongation (↑) Mean R2 UTS (↑)\n6.000 compositions 0.434 0.382 0.498 0.448 0.553 0.617\n75.000 compositions 0.439 0.312 0.478 0.414 0.551 0.685\n150.000 compositions 0.428 0.251 0.449 0.349 0.561 0.754\nTable 4. Effect of pre-training dataset size on the fine tuned transformer model performance (test set). Metrics \nfor scaled values. The bold indicate the best mean results in terms of the metrics used with fivefold validation. \nThe transformer models were trained across all layers using all features as model inputs.\n \nModel Mean MSE (↓) Best k-fold MSE (↓) Mean MAE (↓) Best k-fold MAE (↓) Mean R2 (↑) Best k-fold R2 (↑)\nGausian process 0.282 0.171 0.370 0.286 0.719 0.78\nRandom forest 0.251 0.146 0.354 0.287 0.747 0.86\nK-NN 0.375 0.287 0.434 0.372 0.621 0.70\nDescition trees 0.571 0.336 0.505 0.370 0.390 0.65\nGradient boosting 0.226 0.085 0.341 0.211 0.764 0.86\nTransformer (pre-trained) 0.213 0.153 0.336 0.291 0.785 0.79\nTransformer (not pre-trained) 0.336 0.256 0.420 0.353 0.619 0.73\nTable 3. Performance of transformer model and baseline models on UTS (test set). Metrics for scaled values. \nThe bold indicate the best mean results in terms of the metrics used with fivefold validation.\n \nModel Mean MSE (↓) Best k-fold MSE (↓) Mean MAE (↓) Best k-fold MAE (↓) Mean R2 (↑) Best k-fold R2 (↑)\nGausian process 0.470 0.389 0.524 0.438 0.522 0.60\nRandom forest 0.452 0.358 0.504 0.431 0.530 0.67\nK-NN 0.623 0.509 0.618 0.566 0.356 0.46\nDescition trees 0.728 0.515 0.570 0.504 0.245 0.49\nGradient boosting 0.538 0.416 0.539 0.454 0.442 0.59\nTransformer (pre-trained) 0.428 0.349 0.449 0.392 0.561 0.67\nTransformer (not pre-trained) 0.457 0.338 0.498 0.408 0.531 0.65\nTable 2. Performance of transformer model and baseline models on elongation (test set). Metrics for scaled \nvalues. The bold indicate the best mean results in terms of the metrics used with fivefold validation. The \ntransformer models were trained across all layers using all features as model inputs.\n \nScientific Reports |        (2025) 15:11861 13| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nin the model’s predictive capabilities, particularly for UTS. The mean MSE for UTS decreased from 0.382 to \n0.251, and the mean R 2 increased from 0.617 to 0.754. This trend indicates that a larger pre-training dataset \nenables the Transformer to learn more comprehensive and robust representations of material behaviors. For \nelongation, the improvements were more modest but still evident, suggesting that elongation may be influenced \nby factors that require both extensive data exposure and fine-tuning strategies.\nIn Table 5, the effect of fine-tuning is explored for different numbers of attention layers within the \nTransformer. The results reveal that fine-tuning only the deeper layers, specifically layers 4, 6 8, 10 and 12, \nyielded the best performance for UTS prediction, with a mean MSE of 0.210 and a mean R 2 of 0.786. This \nsuggests that higher-level abstractions captured in these layers are particularly pertinent to UTS. In contrast, \nfine-tuning all layers resulted in the best performance for elongation prediction, achieving a mean MSE of 0.428 \nand a mean R 2 of 0.561. This indicates that both low-level and high-level features are important for accurately \npredicting elongation, and that the contributions from all layers collectively enhance the model’s performance \nfor this property.\nTable 6 examines the Transformer’s performance when only elemental compositions are used as input, \nwithout additional engineered features. Remarkably, the Transformer maintained superior performance over \nbaseline models even in this scenario. For elongation prediction, the Transformer fine-tuned on layers 10–12 \nachieved a mean MSE of 0.436 and a mean R2 of 0.554, outperforming the random forest and Gaussian process \nmodels, which had mean MSE values of 0.484 and 0.510, respectively. For UTS prediction, the Transformer’s \nperformance remained robust, further emphasizing its ability to extract meaningful patterns directly from raw \ninput data. The baseline models, on the other hand, exhibited reduced performance without engineered features, \nhighlighting their reliance on such inputs for accurate predictions.\nThe consistent superiority of the pre-trained Transformer models across all evaluated scenarios can be \nattributed to several key factors. Firstly, the Transformer’s architecture, equipped with self-attention mechanisms, \nexcels at capturing complex relationships within the data. The self-attention layers allow the model to weigh the \nimportance of each element in the composition relative to others, effectively modelling the interactions that \ngovern material properties. Pre-training on a large and diverse dataset enhances this capability by exposing the \nmodel to a wide range of compositions and associated behaviours, enabling it to learn generalizable patterns that \ncan be fine-tuned for specific tasks.\nThe benefits of transfer learning are evident in the performance gains observed with the pre-trained \nTransformer. By leveraging knowledge acquired during pre-training, the model requires less data and fewer \nepochs to converge during fine-tuning, improving generalization and reducing the risk of overfitting. This is \nparticularly advantageous when fine-tuning datasets are limited in size or diversity, as the model can draw upon \nthe rich representations learned during pre-training to make accurate predictions. The strategic fine-tuning \nof Transformer layers plays a significant role in optimizing performance for different material properties. The \nfact that fine-tuning deeper layers enhances UTS prediction suggests that UTS is more sensitive to high-level \nfeatures and long-range dependencies within the material composition. Conversely, the necessity of fine-tuning \nall layers for optimal elongation prediction indicates that both local interactions and global patterns contribute \nto elongation, necessitating adjustments across the entire network.\nThe Transformer’s robustness to limited feature sets underscores its strength in intrinsic feature extraction. \nIts ability to perform well without additional engineered features simplifies the modelling process and reduces \nthe dependency on domain-specific expertise for feature engineering. This is particularly valuable in materials \nscience, where complex interactions and high-dimensional data can make feature engineering challenging.\nThe combined plots in Figs. 6 and 7 visualize the aggregated predictions and actual values from all validation \nsets across the K-fold cross-validation process. In each fold, the model is trained on part of the data and \nevaluated on a separate validation (test) set, ensuring predictions are made on unseen data. By collecting these \npredictions and actual values from each fold, the combined plots provide a comprehensive view of the model’s \nModels trained with composition as \ninput only\nMean MSE elongation \n(↓) Mean MSE UTS (↓)\nMean MAE elongation \n(↓)\nMean MAE \nUTS (↓)\nMean R2 elongation \n(↑)\nMean \nR2 \nUTS \n(↑)\nAll layers no features 0.453 0.250 0.492 0.361 0.532 0.755\nLayers 11, 12 no features 0.462 0.258 0.497 0.350 0.525 0.745\nLayers 10, 11, 12 no features 0.436 0.244 0.480 0.370 0.554 0.758\nLayers 9, 10, 11, 12 no features 0.444 0.258 0.485 0.361 0.542 0.744\nLayers 4, 6, 8, 10, 12 no features 0.459 0.261 0.504 0.359 0.527 0.743\nRandom forest no features 0.484 0.294 0.527 0.385 0.508 0.710\nGaussian process no features 0.510 0.308 0.544 0.388 0.482 0.691\nGradient boosting no features 0.527 0.358 0.570 0.427 0.463 0.642\nTable 6. Effect of features on the fine tuned transformer model performance (test set). Metrics for scaled \nvalues. The bold indicate the best mean results in terms of the metrics used with fivefold validation. Layer-wise \nfine-tuning, the attention weights are kept frozen for the unselected layers. 150,000 compositions pre-trained \nmodel. No features indicate that only the composition elements and their fractions are used as inputs to the \nmodel during training.\n \nScientific Reports |        (2025) 15:11861 14| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\noverall performance on the entire dataset as test data. This approach allows for a holistic assessment of the \nmodel’s generalization ability and helps identify patterns or systematic errors across the full dataset.\nAn analysis of the “ Actual versus Predicted” scatter plots (Fig.  6) for ultimate tensile strength (UTS) \nreveals that while both the Transformer and random forest models capture the general trend along the 45° \nline, the Transformer model displays a tighter clustering of data points around this line. This indicates a higher \nalignment between predicted and actual values, reflecting better accuracy and better generalization to unseen \ndata. In contrast, the Random Forest model shows a broader dispersion of points around the line. The residual \nhistograms reinforce these findings. The Transformer’s residuals are sharply cantered around zero with a narrow \ndistribution, indicating lower prediction variability and errors. Conversely, the Random Forest’s residuals are \nmore widely spread and less centered, pointing to greater variability and a higher average prediction error. \nExamining the residual box plots in Fig.  7 provides further insight. The Transformer model’s residuals exhibit \na narrower interquartile range (IQR) and fewer outliers, signifying consistent low-error predictions across \ndifferent validation folds. The random forest model displays a wider IQR and more outliers, indicating more \nsignificant deviations from actual values and less consistent performance. In summary, the Transformer model \nshows tighter clustering in the scatter plots, sharper and more cantered residual distribution, and narrower \nresidual range.\nModel interpretability\nThe attention mechanisms of a pre-trained Transformer model can be analysed when processing a chemical \ncomposition input, specifically focusing on how the model attends to different elements within the material. \nIt begins by tokenizing the input text of elemental compositions and mapping these tokens back to their \ncorresponding chemical elements using offset mappings. The model’s attention weights from the last layer are \nextracted and averaged across all heads to form a simplified attention matrix. This matrix represents how much \nthe model’s tokens attend to each other, essentially capturing the relationships between different elements in \nthe composition. By grouping tokens corresponding to the same element and computing the average attention \nbetween these groups, a reduced attention matrix is created (Fig. 8). This matrix is then symmetrized (averaged \nwith its transpose) and visualized using a heatmap, with the diagonal masked to exclude self-attention. The \nFig. 6. Scatter plot showing the relationship between actual and predicted values as well as the histogram of \nresiduals (actual minus predicted values).\n \nScientific Reports |        (2025) 15:11861 15| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nresulting visualization highlights the strength of associations the model has learned between different elements, \noffering insights into potential chemical interactions and the model’s focus when making predictions about \nproperties like ultimate tensile strength (UTS).\nNotably, the heatmap reveals high attention weights between Ni (Nickel) and Fe (Iron), as well as between \nNi and Co (Cobalt), suggesting that the model has recognized these elements as having substantial interactions \nor associations within the alloy system. These observations align with established metallurgical principles \nregarding HEAs. Elements such as Ni, Fe, and Co are known to exhibit significant mutual solubility and tend \nto form stable face-centered cubic (FCC) solid solutions due to their similar atomic sizes and crystal structures. \nTheir interactions contribute to the unique mechanical properties of HEAs, including enhanced ductility, \ntoughness, and strength. The Transformer’s high attention weights between these elements indicate that the \nmodel effectively captures these critical relationships, which are essential for accurate predictions of properties \nlike ultimate tensile strength (UTS).\nFurthermore, the heatmap shows moderate attention weights between Cr (Chromium) and Mn (Manganese), \nas well as between Cr and Fe. Chromium plays a crucial role in improving oxidation resistance and contributing \nto phase stability, while Manganese influences stacking fault energy and stabilizes certain phases, affecting \ndeformation mechanisms such as twinning and slip. The model’s attention to these element pairs suggests it \nrecognizes their influence on the alloy’s overall mechanical behaviour. By highlighting these interactions, the \nTransformer model demonstrates an ability to internalize complex metallurgical relationships that govern the \nproperties of HEAs.\nFig. 7. Box plot of residuals (actual minus predicted values).\n \nScientific Reports |        (2025) 15:11861 16| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nIn summary, the model’s focus on pairs like Ni–Fe and Ni–Co corresponds with known metallurgical \nphenomena, indicating that it captures the underlying physics and chemistry governing the alloy’s behaviour. \nThese findings are also supported by the literature where Ni content has a significant influence on hardness \nand crystal structure 45. This alignment not only validates the model’s predictive capabilities but also enhances \nits interpretability, offering valuable insights into how specific elemental interactions contribute to material \nproperties. Such insights are instrumental in advancing materials informatics, as they bridge the gap between \ndata-driven models and domain-specific knowledge, ultimately aiding in the design and discovery of new alloys \nwith tailored properties.\nConclusions\nThe proposed transformer-based models demonstrated superior performance in predicting key mechanical \nproperties, such as elongation and ultimate tensile strength (UTS), significantly outperforming traditional \nregression models. The findings underscore the effectiveness of leveraging large-scale synthetic datasets and \nthe strength of self-attention mechanisms in capturing intricate elemental interactions. Remarkably, the model \nexhibited robust performance even without relying on engineered features, highlighting its potential for broader \napplications in materials informatics and beyond. However, several deliberate methodological simplifications \nwere made, such as omitting data preprocessing, neglecting hyperparameter optimization, and excluding \ncritical factors like kinetic processing parameters and microstructural influences, to ensure a controlled and \nfair baseline comparison among different regressors. Additionally, the tokenization strategy, though effective, \nmay have oversimplified some chemical relationships, while the synthetic pre-training dataset lacked explicit \nthermodynamic constraints, possibly limiting the physical realism of learned representations. To fully realize \nthe model’s potential in practical applications, future work should address these limitations by incorporating \nmore advanced preprocessing methods, realistic constraints during dataset generation and further refined \ntokenization techniques. Overall, this work clearly demonstrates the transformative potential of transformer-\nFig. 8. UTS attention maps for an unseen composition during training.\n \nScientific Reports |        (2025) 15:11861 17| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\nbased language models in materials science, providing a solid foundation for future advancements and practical \nimplementation in high-entropy alloy design.\nData availability\nAll data used in this work are publicly available. Original datasets could be found in  h t t p s : / / g i t h u b . c o m / C i t r i n e \nI n f o r m a t i c s / M P E A _ d a t a s e t     . The processed datasets used in this work are available at  h t t p s :  / / g i t h  u b . c o m  / S P S - C  \no a t i n  g s / L a n  g u a g e -  M o d e l -  f o r - H E A.\nCode availability\nThe codes developed for this work are available at  h t t p s :  / / g i t h  u b . c o m  / S P S - C  o a t i n  g s / L a n  g u a g e -  M o d e l -  f o r - H E A.\nReceived: 6 December 2024; Accepted: 19 March 2025\nReferences\n 1. Cantor, B., Chang, I. T. H., Knight, P . & Vincent, A. J. B. Microstructural development in equiatomic multicomponent alloys. Mater. \nSci. Eng. A 375, 213–218. https://doi.org/10.1016/j.msea.2003.10.257 (2004).\n 2. Y eh, J. W . et al. Nanostructured high-entropy alloys with multiple principal elements: Novel alloy design concepts and outcomes. \nAdv. Eng. Mater. 6, 299–303. https://doi.org/10.1002/adem.200300567 (2004).\n 3. Miracle, D. B. & Senkov, O. N. A critical review of high entropy alloys and related concepts. Acta Mater. 122, 448–511.  h t t p s : / / d o i \n. o r g / 1 0 . 1 0 1 6 / j . a c t a m a t . 2 0 1 6 . 0 8 . 0 8 1     (2017).\n 4. Dippo, O. F . & Vecchio, K. S. A universal configurational entropy metric for high-entropy materials. Scr. Mater.  h t t p s : / / d o i . o r g / 1 0 \n. 1 0 1 6 / j . s c r i p t a m a t . 2 0 2 1 . 1 1 3 9 7 4     (2021).\n 5. Marik, S. et al. Superconductivity in a new hexagonal high-entropy alloy. Phys. Rev. Mater.  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 3 / P h y s R e v M a t e r \ni a l s . 3 . 0 6 0 6 0 2     (2019).\n 6. Y eh, J. W . Recent progress in high-entropy alloys. Ann. Chim. Sci. Mater. 31, 633–648. https://doi.org/10.3166/acsm.31.633-648 \n(2006).\n 7. Wang, B., Y ang, C., Shu, D. & Sun, B. A review of irradiation-tolerant refractory high-entropy alloys. Metals (Basel)  h t t p s : / / d o i . o r g \n/ 1 0 . 3 3 9 0 / m e t 1 4 0 1 0 0 4 5     (2024).\n 8. Han, C. et al. Recent advances on high-entropy alloys for 3D printing. Adv. Mater. https://doi.org/10.1002/adma.201903855 \n(2020).\n 9. Chang, X., Zeng, M., Liu, K. & Fu, L. Phase engineering of high-entropy alloys. Adv. Mater. https://doi.org/10.1002/adma.201907226 \n(2020).\n 10. Zhang, Y . et al. Microstructures and properties of high-entropy alloys. Prog. Mater. Sci. https://doi.org/10.1016/j.pmatsci.2013.10.001 \n(2014).\n 11. Gao, J. et al. A machine learning accelerated distributed task management system (Malac-Distmas) and its application in high-\nthroughput CALPHAD computation aiming at efficient alloy design. Adv. Powder Mater.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . a p m a t e . 2 0 2 1 . 0 9 \n. 0 0 5     (2022).\n 12. Feng, R., Liaw, P . K., Gao, M. C. & Widom, M. First-principles prediction of high-entropy-alloy stability. NPJ Comput. Mater. 3, 50. \nhttps://doi.org/10.1038/s41524-017-0049-4 (2017).\n 13. Y ang, X. et al. MatCloud: A high-throughput computational infrastructure for integrated management of materials simulation, \ndata and resources. Comput. Mater. Sci. https://doi.org/10.1016/j.commatsci.2018.01.039 (2018).\n 14. Li, R., Xie, L., Wang, W . Y ., Liaw, P . K. & Zhang, Y . High-throughput calculations for high-entropy alloys: A brief review. Front. \nMater. https://doi.org/10.3389/fmats.2020.00290 (2020).\n 15. Zhang, C. et al. High-throughput thermodynamic calculations of phase equilibria in solidified 6016 Al-alloys. Comput. Mater. Sci. \nhttps://doi.org/10.1016/j.commatsci.2019.05.022 (2019).\n 16. Song, K. et al. Optimization of the processing parameters during internal oxidation of Cu–Al alloy powders using an artificial \nneural network. Mater. Des. https://doi.org/10.1016/j.matdes.2004.06.002 (2005).\n 17. Sun, Y . et al. Development of constitutive relationship model of Ti600 alloy using artificial neural network. Comput. Mater. Sci. \nhttps://doi.org/10.1016/j.commatsci.2010.03.007 (2010).\n 18. Su, J., Dong, Q., Liu, P ., Li, H. & Kang, B. Prediction of properties in thermomechanically treated Cu–Cr–Zr alloy by an artificial \nneural network. J. Mater. Sci. Technol. 19, 529 (2003).\n 19. Malinov, S., Sha, W . & McKeown, J. J. Modelling the correlation between processing parameters and properties in titanium alloys \nusing artificial neural network. Comput. Mater. Sci. https://doi.org/10.1016/S0927-0256(01)00160-4 (2001).\n 20. Warde, J. & Knowles, D. M. Use of neural networks for alloy design. ISIJ Int. https://doi.org/10.2355/isijinternational.39.1015 \n(1999).\n 21. Sun, Y . et al. Modeling constitutive relationship of Ti40 alloy using artificial neural network. Mater. Des.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . \nm a t d e s . 2 0 1 0 . 1 0 . 0 0 4     (2011).\n 22. Dewangan, S. K., Samal, S. & Kumar, V . Microstructure exploration and an artificial neural network approach for hardness \nprediction in AlCrFeMnNiWx high-entropy alloys. J. Alloys Compd. https://doi.org/10.1016/j.jallcom.2020.153766 (2020).\n 23. Wang, J., Kwon, H., Kim, H. S. & Lee, B. J. A neural network model for high entropy alloy design. NPJ Comput. Mater.  h t t p s : / / d o i . \no r g / 1 0 . 1 0 3 8 / s 4 1 5 2 4 - 0 2 3 - 0 1 0 1 0 - x     (2023).\n 24. Lu, W . C. et al. Using support vector machine for materials design. Adv. Manuf. https://doi.org/10.1007/s40436-013-0025-2 (2013).\n 25. Zhang, W . et al. Explaining of prediction accuracy on phase selection of amorphous alloys and high entropy alloys using support \nvector machines in machine learning. Mater. Today Commun. https://doi.org/10.1016/j.mtcomm.2023.105694 (2023).\n 26. Chau, N. H., Kubo, M., Hai, L. V . & Y amamoto, T. Support vector machine-based phase prediction of multi-principal element \nalloys. Vietnam J. Comput. Sci. https://doi.org/10.1142/S2196888822500312 (2023).\n 27. Tancret, F ., Toda-Caraballo, I., Menou, E. & Rivera Díaz-Del-Castillo, P . E. J. Designing high entropy alloys employing \nthermodynamics and Gaussian process statistical analysis. Mater. Des. 115, 486–497. https://doi.org/10.1016/j.matdes.2016.11.049 \n(2017).\n 28. Park, S. M., Lee, T., Lee, J. H., Kang, J. S. & Kwon, M. S. Gaussian process regression-based Bayesian optimization of the insulation-\ncoating process for Fe–Si alloy sheets. J. Mater. Res. Technol. https://doi.org/10.1016/j.jmrt.2022.12.171 (2023).\n 29. Khatamsaz, D., Vela, B. & Arróyave, R. Multi-objective Bayesian alloy design using multi-task Gaussian processes. Mater. Lett. \nhttps://doi.org/10.1016/j.matlet.2023.135067 (2023).\n 30. Tancret, F . Computational thermodynamics, Gaussian processes and genetic algorithms: Combined tools to design new alloys. \nModel. Simul. Mater. Sci. Eng. https://doi.org/10.1088/0965-0393/21/4/045013 (2013).\n 31. Sabin, T. J., Bailer-Jones, C. A. L. & Withers, P . J. Accelerated learning using Gaussian process models to predict static recrystallization \nin an Al–Mg alloy. Model. Simul. Mater. Sci. Eng. https://doi.org/10.1088/0965-0393/8/5/304 (2000).\nScientific Reports |        (2025) 15:11861 18| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/\n 32. Liu, H., Ong, Y . S., Shen, X. & Cai, J. When Gaussian process meets big data: A review of scalable GPs. IEEE Trans. Neural Netw. \nLearn. Syst. https://doi.org/10.1109/TNNLS.2019.2957109 (2020).\n 33. Ghouchan Nezhad Noor Nia, R., Jalali, M. & Houshmand, M. A graph-based k-nearest neighbor (KNN) approach for predicting \nphases in high-entropy alloys. Appl. Sci. 12, 8021. https://doi.org/10.3390/app12168021 (2022).\n 34. Ertuğrul, Ö. F . & Tağluk, M. E. A novel version of k nearest neighbor: Dependent nearest neighbor. Appl. Soft Comput. J.  h t t p s : / / d \no i . o r g / 1 0 . 1 0 1 6 / j . a s o c . 2 0 1 7 . 0 2 . 0 2 0     (2017).\n 35. Zhang, J. et al. Grain size characterization of Ti–6Al–4V titanium alloy based on laser ultrasonic random forest regression. Appl. \nOpt. https://doi.org/10.1364/ao.479323 (2023).\n 36. Zhang, Z., Y ang, Z., Ren, W . & Wen, G. Random forest-based real-time defect detection of Al alloy in robotic arc welding using \noptical spectrum. J. Manuf. Process. https://doi.org/10.1016/j.jmapro.2019.04.023 (2019).\n 37. Simple Introduction to Convolutional Neural Networks | by Matthew Stewart, PhD Researcher | Towards Data Science (n.d.).  h t t \np s :  / / t o w a  r d s d a t  a s c i e n  c e . c o  m / s i m p  l e - i n t  r o d u c t  i o n - t  o - c o n v  o l u t i o  n a l - n e  u r a l - n e t w o r k s - c d f 8 d 3 0 7 7 b a c. Accessed 6 Sept 2021.\n 38. Wang, X. et al. Element-wise representations with ECNet for material property prediction and applications in high-entropy alloys. \nNPJ Comput. Mater. https://doi.org/10.1038/s41524-022-00945-x (2022).\n 39. Feng, S., Zhou, H. & Dong, H. Application of deep transfer learning to predicting crystal structures of inorganic substances. \nComput. Mater. Sci. https://doi.org/10.1016/j.commatsci.2021.110476 (2021).\n 40. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. & Polosukhin, I. Attention is all you need, In \nAdvances in Neural Information Processing Systems (2017). https://doi.org/10.48550/arXiv.1706.03762%0A.\n 41. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving Language Understanding by Generative Pre-Training, 1–12 \n(OpenAI.Com., 2018).  h t t p s :   /  / c d  n . o p e n a  i . c  o m  / r e s e a  r  c h - c  o v  e r s /  l a n g u   a g e - u  n s u p e r  v  i s e d /  l a n g  u  a g e _ u n d e r s t a  n d i n g _  p a p e r . p d f.\n 42. Xu, C., Wang, Y . & Barati Farimani, A. TransPolymer: A transformer-based language model for polymer property predictions. NPJ \nComput. Mater. https://doi.org/10.1038/s41524-023-01016-5 (2023).\n 43. Kamnis, S. Introducing pre-trained transformers for high entropy alloy informatics. Mater. Lett.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . m a t l e t . 2 \n0 2 4 . 1 3 5 8 7 1     (2024).\n 44. Devlin, J., Chang, M. W ., Lee, K. & Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. \nIn NAACL HLT 2019—2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human \nLanguage Technologies (2019).\n 45. González, S. et al. Wear resistant CoCrFeMnNi0.8V high entropy alloy with multi length-scale hierarchical microstructure. Mater. \nLett. 331, 133504. https://doi.org/10.1016/j.matlet.2022.133504 (2023).\nAuthor contributions\nSpyros Kamnis: Conceptualization, methodology, code, main manuscript text.  Konstantinos Delibasis: Super-\nvision and review of manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 9 5 1 7 0 - z     .  \nCorrespondence and requests for materials should be addressed to S.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025 \nScientific Reports |        (2025) 15:11861 19| https://doi.org/10.1038/s41598-025-95170-z\nwww.nature.com/scientificreports/",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.7616044282913208
    },
    {
      "name": "Computer science",
      "score": 0.7266586422920227
    },
    {
      "name": "Transformer",
      "score": 0.5115488767623901
    },
    {
      "name": "Preprocessor",
      "score": 0.49475806951522827
    },
    {
      "name": "Machine learning",
      "score": 0.4937492311000824
    },
    {
      "name": "Random forest",
      "score": 0.48050081729888916
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45944204926490784
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.44498682022094727
    },
    {
      "name": "Kriging",
      "score": 0.43924835324287415
    },
    {
      "name": "Data pre-processing",
      "score": 0.4120340347290039
    },
    {
      "name": "Data mining",
      "score": 0.40481165051460266
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}