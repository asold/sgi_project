{
  "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
  "url": "https://openalex.org/W4377865158",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A663563409",
      "name": "WANG Xiaolei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232681043",
      "name": "Tang Xin-yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752884039",
      "name": "Zhao, Wayne Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1757763073",
      "name": "Wang Jing-yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221399670",
      "name": "Wen, Ji-Rong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3103410128",
    "https://openalex.org/W4385227262",
    "https://openalex.org/W4284687378",
    "https://openalex.org/W2950457956",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572709",
    "https://openalex.org/W4312122790",
    "https://openalex.org/W4221142019",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W4388955889",
    "https://openalex.org/W3102862020",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4378942292",
    "https://openalex.org/W2997662139",
    "https://openalex.org/W4368755500",
    "https://openalex.org/W2970236742",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4221154563",
    "https://openalex.org/W4316837538",
    "https://openalex.org/W4385568412",
    "https://openalex.org/W4366559955",
    "https://openalex.org/W3165654682",
    "https://openalex.org/W4378713418",
    "https://openalex.org/W4404752313",
    "https://openalex.org/W4377111802",
    "https://openalex.org/W4212975704",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4306716473",
    "https://openalex.org/W4283324387",
    "https://openalex.org/W3211527163",
    "https://openalex.org/W3121541553",
    "https://openalex.org/W4308198680",
    "https://openalex.org/W4210334834",
    "https://openalex.org/W3185784178",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W4376311940",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2952215380",
    "https://openalex.org/W4226153070"
  ],
  "abstract": "The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and easy-to-use evaluation framework for future research endeavors. The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
  "full_text": "Rethinking the Evaluation for Conversational Recommendation\nin the Era of Large Language Models\nXiaolei Wang1,3∗\n, Xinyu Tang1,3∗\n, Wayne Xin Zhao1,3†\n, Jingyuan Wang4 and Ji-Rong Wen1,2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\n4School of Computer Science and Engineering, Beihang University\nwxl1999@foxmail.com, txy20010310@163.com, batmanfly@gmail.com\nAbstract\nThe recent success of large language mod-\nels (LLMs) has shown great potential to de-\nvelop more powerful conversational recom-\nmender systems (CRSs), which rely on natural\nlanguage conversations to satisfy user needs. In\nthis paper, we embark on an investigation into\nthe utilization of ChatGPT for CRSs, revealing\nthe inadequacy of the existing evaluation proto-\ncol. It might overemphasize the matching with\nground-truth items annotated by humans while\nneglecting the interactive nature of CRSs.\nTo overcome the limitation, we further pro-\npose an interactive Evaluation approach based\non LLMs, named iEvaLM, which harnesses\nLLM-based user simulators. Our evaluation ap-\nproach can simulate various system-user inter-\naction scenarios. Through the experiments on\ntwo public CRS datasets, we demonstrate no-\ntable improvements compared to the prevailing\nevaluation protocol. Furthermore, we empha-\nsize the evaluation of explainability, and Chat-\nGPT showcases persuasive explanation gen-\neration for its recommendations. Our study\ncontributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and\nprovides a more flexible and realistic evalu-\nation approach for future research about LLM-\nbased CRSs. The code is available at https:\n//github.com/RUCAIBox/iEvaLM-CRS.\n1 Introduction\nConversational recommender systems (CRSs) aim\nto provide high-quality recommendation services\nthrough natural language conversations that span\nmultiple rounds. Typically, in CRSs, a recom-\nmender module provides recommendations based\non user preferences from the conversation context,\nand a conversation module generates responses\ngiven the conversation context and item recommen-\ndation.\n∗Equal contribution.\n†Corresponding author.\nSince CRSs rely on the ability to understand and\ngenerate natural language conversations, capable\napproaches for CRSs have been built on pre-trained\nlanguage models in existing literature (Wang et al.,\n2022c; Deng et al., 2023). More recently, large lan-\nguage models (LLMs) (Zhao et al., 2023a), such\nas ChatGPT, have shown that they are capable of\nsolving various natural language tasks via conver-\nsations. Since ChatGPT has acquired a wealth of\nworld knowledge during pre-training and is also\nspecially optimized for conversation, it is expected\nto be an excellent CRS. However, there still lacks\na comprehensive study of how LLMs (e.g., Chat-\nGPT) perform in conversational recommendation.\nTo investigate the capacity of LLMs in CRSs, we\nconduct an empirical study on the performance of\nChatGPT on existing benchmark datasets. We fol-\nlow the standard evaluation protocol and compare\nChatGPT against state-of-the-art CRS methods.\nSurprisingly, the finding is rather counter-intuitive:\nChatGPT shows unsatisfactory performance in this\nempirical evaluation. To comprehend the reason\nbehind this discovery, we examine the failure cases\nand discover that the current evaluation protocol\nis the primary cause. It relies on the matching be-\ntween manually annotated recommendations and\nconversations and might overemphasize the fitting\nof ground-truth items based on the conversation\ncontext. Since most CRS datasets are created in\na chit-chat way, we find that these conversations\nare often vague about the user preference, making\nit difficult to exactly match the ground-truth items\neven for human annotation. In addition, the current\nevaluation protocol is based on fixed conversations,\nwhich does not take the interactive nature of con-\nversational recommendation into account. Similar\nfindings have also been discussed on text gener-\nation tasks (Bang et al., 2023; Qin et al., 2023):\ntraditional metrics (e.g., BLEU and ROUGE) may\nnot reflect the real capacities of LLMs.\nConsidering this issue, we aim to improve the\narXiv:2305.13112v2  [cs.CL]  3 Nov 2023\nevaluation approach, to make it more focused on\nthe interactive capacities of CRSs. Ideally, such\nan evaluation approach should be conducted by hu-\nmans, since the performance of CRSs would finally\nbe tested by real users in practice. However, user\nstudies are both expensive and time-consuming,\nmaking them infeasible for large-scale evaluations.\nAs a surrogate, user simulators can be used for\nevaluation. However, existing simulation methods\nare typically limited to pre-defined conversation\nflows or template-based utterances (Lei et al., 2020;\nZhang and Balog, 2020). To address these limita-\ntions, a more flexible user simulator that supports\nfree-form interaction in CRSs is actually needed.\nTo this end, this work further proposes an\ninteractive Evaluation approach based on LLMs,\nnamed iEvaLM, in which LLM-based user sim-\nulation is conducted to examine the performance.\nOur approach draws inspiration from the remark-\nable instruction-following capabilities exhibited by\nLLMs, which have already been leveraged for role-\nplay (Fu et al., 2023). With elaborately designed\ninstructions, LLMs can interact with users in a\nhighly cooperative manner. Thus, we design our\nuser simulators based on LLMs, which can flexi-\nbly adapt to different CRSs without further tuning.\nOur evaluation approach frees CRSs from the con-\nstraints of rigid, human-written conversation texts,\nallowing them to interact with users in a more natu-\nral manner, which is close to the experience of real\nusers. To give a comprehensive evaluation, we also\nconsider two types of interaction: attribute-based\nquestion answering and free-form chit-chat.\nWith this new evaluation approach, we observe\nsignificant improvements in the performance of\nChatGPT, as demonstrated through assessments\nconducted on two publicly available CRS datasets.\nNotably, the Recall@10 metric has increased from\n0.174 to 0.570 on the REDIAL dataset with five-\nround interaction, even surpassing the Recall@50\nresult of the currently leading CRS baseline. More-\nover, in our evaluation approach, we have taken the\ncrucial aspect of explainability into consideration,\nwherein ChatGPT exhibits proficiency in providing\npersuasive explanations for its recommendations.\nBesides, existing CRSs can also benefit from the in-\nteraction, which is an important ability overlooked\nby the traditional evaluation. However, they per-\nform much worse in the setting of attribute-based\nquestion answering on the OPEN DIAL KG dataset,\nwhile ChatGPT performs better in both settings on\nDataset #Dialogues #Utterances Domains\nReDial 10,006 182,150 Movie\nOpenDialKG 13,802 91,209 Movie, Book,\nSports, Music\nTable 1: Statistics of the datasets.\nthe two datasets. It demonstrates the superiority\nof ChatGPT across different scenarios, which is\nexpected for a general-purpose CRS.\nWe summarize our key contributions as follows:\n(1) To the best of our knowledge, it is the first\ntime that the capability of ChatGPT for conver-\nsational recommendation has been systematically\nexamined on large-scale datasets.\n(2) We provide a detailed analysis of the limita-\ntions of ChatGPT under the traditional evaluation\nprotocol, discussing the root cause of why it fails\non existing benchmarks.\n(3) We propose a new interactive approach that\nemploys LLM-based user simulators for evaluating\nCRSs. Through experiments conducted on two pub-\nlic CRS datasets, we demonstrate the effectiveness\nand reliability of our evaluation approach.\n2 Background and Experimental Setup\nIn this section, we describe the task definition and\nexperimental setup used in this work.\n2.1 Task Description\nConversational Recommender Systems (CRSs) are\ndesigned to provide item recommendations through\nmulti-turn interaction. The interaction can be di-\nvided into two main categories: question answering\nbased on templates (Lei et al., 2020; Tu et al., 2022)\nand chit-chat based on natural language (Wang\net al., 2023; Zhao et al., 2023c). In this work, we\nconsider the second category. At each turn, the sys-\ntem either presents a recommendation or initiates a\nnew round of conversation. This process continues\nuntil the user either accepts the recommended items\nor terminates the conversation. In general, CRSs\nconsist of two major subtasks: recommendation\nand conversation. Given its demonstrated prowess\nin conversation (Zhao et al., 2023b), we focus our\nevaluation of ChatGPT on its performance in the\nrecommendation subtask.\n2.2 Experimental Setup\nDatasets. We conduct experiments on the RE-\nDIAL (Li et al., 2018) and OPEN DIAL KG (Moon\net al., 2019) datasets. REDIAL is the most com-\nmonly used dataset in CRS, which is about movie\nrecommendations. OPEN DIAL KG is a multi-\ndomain CRS dataset covering not only movies but\nalso books, sports, and music. Both datasets are\nwidely used for CRS evaluation. The statistics for\nthem are summarized in Table 1.\nBaselines. We present a comparative analysis of\nChatGPT with a selection of representative super-\nvised and unsupervised methods:\n• KBRD (Chen et al., 2019): It introduces DBpe-\ndia to enrich the semantic understanding of entities\nmentioned in dialogues.\n• KGSF (Zhou et al., 2020): It leverages two KGs\nto enhance the semantic representations of words\nand entities and use Mutual Information Maximiza-\ntion to align these two semantic spaces.\n• CRFR (Zhou et al., 2021a): It performs flex-\nible fragment reasoning on KGs to address their\ninherent incompleteness.\n• BARCOR (Wang et al., 2022b): It proposes a\nunified CRS based on BART (Lewis et al., 2020),\nwhich tackles two tasks using a single model.\n• MESE (Yang et al., 2022): It formulates the\nrecommendation task as a two-stage item retrieval\nprocess, i.e., candidate selection and ranking, and\nintroduces meta-information when encoding items.\n• UniCRS (Wang et al., 2022c): It designs\nprompts with KGs for DialoGPT (Zhang et al.,\n2020) to tackle two tasks in a unified approach.\n• text-embedding-ada-002 (Neelakantan et al.,\n2022): It is a powerful model provided in the Ope-\nnAI API to transform each input into embeddings,\nwhich can be used for recommendation.\nAmong the above baselines, text-embedding-\nada-002 is an unsupervised method, while others\nare supervised and trained on CRS datasets.\nEvaluation Metrics. Following existing\nwork (Zhang et al., 2023; Zhou et al., 2022), we\nadopt Recall@k to evaluate the recommendation\nsubtask. Specifically, we set k = 1, 10, 50 follow-\ning Zhang et al. (2023) for the REDIAL dataset,\nand k = 1, 10, 25 following Zhou et al. (2022) for\nthe OPEN DIAL KG dataset. Since requiring too\nmany items can sometimes be refused by ChatGPT,\nwe only assess Recall@1 and Recall@10 for it.\nModel details. We employ the publicly available\nmodel gpt-3.5-turbo provided in the OpenAI\nAPI, which is the underlying model of ChatGPT.\nTo make the output as deterministic as possible, we\nhello I'm open to any movie\nI have not seen it but I watched American Pie 2 (2001). I \njust watched Avengers: Infinity War (2018) and I liked it.\nHi there. I would like to suggest some comedies you \ncould watch, have you seen The Wedding Singer (1998)?\nIt seems like you enjoy both \naction and comedy movies.\n(Task instruction)\nRecommend 10 items that \nare consistent with user \npreference in the dialogue.\n(Format guideline)\nThe format is: no. title.\nSure, here are 10 movies:\n1. The Avengers (2012)\n2. Avengers: Endgame \n(2019) ...\nRecommendation \nModels\n(a) Zero-shot Prompting\n(b) Integrating \nRecommendation Models\nResponse  as query\n1. RED (2010)\n2. Lethal Weapon (1987)\n…\nItem\nRecommend  by itself\nFigure 1: The method of adapting ChatGPT for CRSs.\nset temperature=0 when calling the API. All the\nprompts we used are detailed in Appendix C.\n3 ChatGPT for Conversational\nRecommendation\nIn this section, we first discuss how to adapt Chat-\nGPT for CRSs, and then analyze its performance.\n3.1 Methodology\nSince ChatGPT is specially optimized for dialogue,\nit possesses significant potential for conversational\nrecommendation. Here we propose two approaches\nto stimulating this ability, as illustrated in Figure 1.\nZero-shot Prompting. We first investigate the\nability of ChatGPT through zero-shot prompting\n(see Appendix C.1). The prompt consists of two\nparts: task instruction (describing the task) and\nformat guideline (specifying the output format).\nIntegrating Recommendation Models. Although\nChatGPT can directly generate the items, it is not\nspecially optimized for recommendation (Kang\net al., 2023; Dai et al., 2023). In addition, it\ntends to generate items that are outside the eval-\nuation datasets, which makes it difficult to directly\nassess the predictions. To bridge this gap, we\nincorporate external recommendation models to\nconstrain the output space. We concatenate the\nconversation history and generated responses as\ninputs for these models to directly predict tar-\nget items or calculate the similarity with item\ncandidates for matching. We select the CRS\nmodel MESE (Yang et al., 2022) as the supervised\nmethod (ChatGPT + text-embedding-ada-002) and\nthe text-embedding-ada-002 (Neelakantan et al.,\nDatasets ReDial OpenDialKG\nModels Recall@1 Recall@10 Recall@50 Recall@1 Recall@10 Recall@25\nKBRD 0.028 0.169 0.366 0.231 0.423 0.492\nKGSF 0.039 0.183 0.378 0.119 0.436 0.523\nCRFR 0.040 0.202 0.399 0.130 0.458 0.543\nBARCOR 0.031 0.170 0.372 0.312 0.453 0.510\nUniCRS 0.050 0.215 0.413 0.308 0.513 0.574\nMESE 0.056* 0.256* 0.455* 0.279 0.592* 0.666*\ntext-embedding-ada-002 0.025 0.140 0.250 0.279 0.519 0.571\nChatGPT 0.034 0.172 – 0.105 0.264 –\n+ MESE 0.036 0.195 – 0.240 0.508 –\n+ text-embedding-ada-002 0.037 0.174 – 0.310 0.539 –\nTable 2: Overall performance of existing CRSs and ChatGPT. Since requiring too many items at once can sometimes\nbe refused by ChatGPT, we only assess Recall@1 and Recall@10 for it, while Recall@50 is marked as “–”.\nNumbers marked with * indicate that the improvement is statistically significant compared with the best baseline\n(t-test with p-value < 0.05).\nDataset Irrelevant Partially\nrelevant\nHighly\nrelevant\nReDial 8% 20% 72%\nOpenDialKG 20% 16% 64%\nTable 3: The relevance degree of the explanations gen-\nerated by ChatGPT to the conversation context.\n2022) model provided in the OpenAI API as the\nunsupervised method (ChatGPT + MESE).\n3.2 Evaluation Results\nWe first compare the accuracy of ChatGPT with\nCRS baselines following existing work (Chen et al.,\n2019; Zhang et al., 2023). Then, to examine the\ninner working principles of ChatGPT, we show-\ncase the explanations generated by it to assess its\nexplainability as suggested by Guo et al. (2023).\n3.2.1 Accuracy\nThe performance comparison of different methods\nfor CRS is shown in Table 2. Surprisingly, Chat-\nGPT does not perform as well as we expect. When\nusing zero-shot prompting, ChatGPT only achieves\naverage performance among these baselines and is\nfar behind the top-performing methods. When inte-\ngrating external recommendation models, its per-\nformance can be effectively improved. In particu-\nlar, on the OPEN DIAL KG dataset, the performance\ngap is significantly reduced. It indicates that the\nresponses generated by ChatGPT can help external\nmodels understand the user preference. However,\nthere is still a noticeable performance gap on the\nREDIAL dataset.\n3.2.2 Explainability\nTo better understand how ChatGPT conducts the\nrecommendation, we require it to generate an ex-\nplanation to examine the inner working princi-\nples. Then, we employ two annotators to judge\nthe relevance degree (irrelevant, partially relevant,\nor highly relevant) of the explanation to the con-\nversation context on 100 randomly sampled failure\nexamples. The Cohen’s Kappa between annotators\nis 0.77, indicating good agreement. The results\nin Table 3 indicate that ChatGPT can give highly\nrelevant explanations in most of the cases. Here is\none example:\n[Conversation History]\nUser: Hi I want a movie like Super Troopers (2001)\n[Label] Police Academy (1984)\n[Prediction of ChatGPT] Beerfest (2006), The\nOther Guys (2010), Hot Fuzz (2007), . . .\n[Explanation of ChatGPT] . . . I have recommended\nmovies that share similar themes of comedy, law en-\nforcement, and absurdity. . . . Some of the movies\non the list are also from the same creators or feature\nsome of the same actors as Super Troopers. . . .\nAs we can see, ChatGPT understands the user\npreference and gives reasonable explanations, sug-\ngesting that it can be a good CRS. However, this\ncontradicts its poor performance in accuracy. It\nmotivates us to investigate the reasons for failure.\n3.3 Why does ChatGPT Fail?\nIn this part, we analyze why does ChatGPT fail in\nterms of accuracy. Through inspecting the incor-\n[Conversation History]\nUser: Hello, I am looking for movies for a night\nwith friends that I have coming up. Any sugges-\ntions?\n[Label] Black Panther (2018)\n[Prediction of ChatGPT] The Hangover (2009),\nBridesmaids (2011), Superbad (2007), Mean Girls\n(2004), The Princess Bride (1987), Clueless (1995),\nThe Breakfast Club (1985), Ferris Bueller’s Day Off\n(1986), Pitch Perfect (2012), 21 Jump Street (2012)\n(a) Lack of explicit user preference\n[Conversation History]\nBot: What genre of movie do you like?\nUser: I like comedies\n[Response in the Dataset] Have you seen\nTalladega Nights: The Ballad of Ricky Bobby (2006)\nor The Hangover (2009)?\n[Response by ChatGPT] That’s great!\nComedies are a fun and lighthearted genre\nthat can make you laugh and feel good.\nDo you have a favorite comedy movie or actor?\n(b) Lack of proactive clarification\nFigure 2: Two failure examples of ChatGPT for conversation recommendation.\nrect recommendations (evaluated according to the\nannotated labels), we identify two main causes and\ndetail them in the following part.\nLack of Explicit User Preference. The exam-\nples in this class typically have very short con-\nversation turns, in which CRSs may be unable to\ncollect sufficient evidence to accurately infer the\nuser intention. Furthermore, the conversations are\nmainly collected in chit-chat form, making it vague\nto reflect the real user preference. To see this, we\npresent an example in Figure 2(a). As we can see,\nthe user does not provide any explicit information\nabout the expected items, which is a common phe-\nnomenon as observed by Wang et al. (2022a). To\nverify this, we randomly sample 100 failure ex-\namples with less than three turns and invite two\nannotators to determine whether the user prefer-\nence is ambiguous. Among them, 51% examples\nare annotated as ambiguous, and the rest 49% are\nconsidered clear, which confirms our speculation.\nThe Cohen’s Kappa between annotators is 0.75.\nCompared with existing models trained on CRS\ndatasets, such an issue is actually more serious for\nChatGPT, since it is not fine-tuned and makes pre-\ndiction solely based on the dialogue context.\nLack of Proactive Clarification. A major lim-\nitation in evaluation is that it has to strictly fol-\nlow existing conversation flows. However, in real-\nworld scenarios, a CRS would propose proactive\nclarification when needed, which is not supported\nby existing evaluation protocols. To see this, we\npresent an example in Figure 2(b), As we can see,\nthe response in the dataset directly gives recommen-\ndations, while ChatGPT asks for detailed user pref-\nerence. Since so many items fit the current require-\nment, it is reasonable to seek clarification before\nmaking a recommendation. However, such cases\ncannot be well handled in the existing evaluation\nprotocol since no more user responses are available\nin this process. To verify this, we randomly sample\n100 failure examples for two annotators to classify\nthe responses generated by ChatGPT (clarification,\nrecommendation, or chit-chat). We find that 36%\nof them are clarifications, 11% are chit-chat, and\nonly 53% are recommendations, suggesting the im-\nportance of considering clarification in evaluation.\nThe Cohen’s Kappa between annotators is 0.81.\nTo summarize, there are two potential issues\nwith the existing evaluation protocol: lack of ex-\nplicit user preference and proactive clarification.\nAlthough conversation-level evaluation (Zhang and\nBalog, 2020) allows system-user interaction, it\nis limited to pre-defined conversation flows or\ntemplate-based utterances (Lei et al., 2020; Zhang\nand Balog, 2020), failing to capture the intricacies\nand nuances of real-world conversations.\n4 A New Evaluation Approach for CRSs\nConsidering the issues with the existing evaluation\nprotocol, in this section, we propose an alterna-\ntive evaluation approach, iEvaLM, which features\ninteractive evaluation with LLM-based user simula-\ntion, as illustrated in Figure 3. We demonstrate its\neffectiveness and reliability through experiments.\n4.1 Overview\nOur approach is seamlessly integrated with exist-\ning CRS datasets. Each system-user interaction\nextends over one of the observed human-annotated\nconversations. The key idea of our approach is to\nconduct close-to-real user simulation based on the\nExisting\nCRS datasets\nConversational \nRecommendation \nSystem\nChit-chat\nChit-chat\nInvoke a clarification\nTalk about preference\nRecommend\nAccept and complete\nFree-form\nchit-chat\nAttribute-based\nquestion answering\nAsk about attribute\nAnswer with preference\nRecommend\nProvide feedback\nRecommend\nRefuse and complete\nStart from the\nexisting conversation\nUser Simulator\n(LLM)\nBehavior rules\nPersona\nInstruction\nExisting conversation Existing conversation\ninit train\nConversational Recommendation SystemUser Simulator\nFigure 3: Our evaluation approach iEvaLM. It is based\non existing CRS datasets and has two settings: free-form\nchit-chat (left) and attribute-based question answering\n(right).\nexcellent role-play capacities of LLMs (Fu et al.,\n2023). We take the ground-truth items as the user\npreference and use them to set up the persona of the\nLLM-based simulated user via instructions. After\nthe interaction, we assess not only the accuracy by\ncomparing predictions with the ground-truth items\nbut also the explainability by querying an LLM-\nbased scorer with the generated explanations.\n4.2 Interaction Forms\nTo make a comprehensive evaluation, we consider\ntwo types of interaction: attribute-based question\nanswering and free-form chit-chat.\nIn the first type, the action of the system is re-\nstricted to choosing one of the k pre-defined at-\ntributes to ask the user or making recommenda-\ntions. At each round, we first let the system decide\non these k + 1 options, and then the user gives\nthe template-based response: answering questions\nwith the attributes of the target item or giving feed-\nback on recommendations. An example interaction\nround would be like: “System: Which genre do you\nlike? User: Sci-fi and action.”\nIn contrast, the second type does not impose any\nrestrictions on the interaction, and both the system\nand user are free to take the initiative. An example\ninteraction round would be like: “System: Do you\nhave any specific genre in mind? User: I’m looking\nfor something action-packed with a lot of special\neffects.”\n4.3 User Simulation\nTo support the interaction with the system, we em-\nploy LLMs for user simulation. The simulated user\ncan take on one of the following three behaviors:\n• Talking about preference. When the system\nmakes a clarification or elicitation about user pref-\nerence, the simulated user would respond with the\ninformation about the target item.\n• Providing feedback. When the system recom-\nmends an item list, the simulated user would check\neach item and provide positive feedback if finding\nthe target or negative feedback if not.\n• Completing the conversation . If one of the\ntarget items is recommended by the system or the\ninteraction reaches a certain number of rounds, the\nsimulated user would finish the conversation.\nSpecifically, we use the ground-truth items from\nexisting datasets to construct realistic personas for\nsimulated users. This is achieved by leveraging the\ntext-davinci-003 (Ouyang et al., 2022) model\nprovided in the OpenAI API, which demonstrates\nsuperior instruction following capacity as an eval-\nuator (Xu et al., 2023; Li et al., 2023). To adapt\ntext-davinci-003 for user simulation, we set its\nbehaviors through manual instructions (see Ap-\npendix C.3). In these instructions, we first fill the\nground-truth items into the persona template and\nthen define their behaviors using a set of manually\ncrafted rules. At each turn, we append the conver-\nsation to the instruction as input. When calling the\nAPI, we set max_tokens to 128, temperature to\n0, and leave other parameters at their default values.\nThe maximum number of interaction rounds is set\nto 5.\n4.4 Performance Measurement\nWe consider both subjective and objective metrics\nto measure the recommendation performance as\nwell as the user experience. For the objective met-\nric, we use recall as stated in Section 2.2 to evalu-\nate every recommendation action in the interaction\nprocess. For the subjective metric, following Chen\net al. (2022), we use persuasiveness to assess the\nquality of explanations for the last recommendation\naction in the interaction process, aiming to evaluate\nwhether the user can be persuaded to accept recom-\nmendations. The value range of this metric is {0, 1,\n2}. To reduce the need for humans, we propose an\nLLM-based scorer that can automatically give the\nscore through prompting. Specifically, we use the\ntext-davinci-003 (Ouyang et al., 2022) model\nSetting Single-turn Multi-turn\nNaturalness UsefulnessNaturalness Usefulness\nDialoGPT 13% 23% 11% 31%\niEvaLM 36% 43% 55% 38%\nTie 51% 34% 34% 31%\nHuman 10% 34% 17% 28%\niEvaLM 39% 33% 35% 40%\nTie 51% 33% 48% 32%\nTable 4: Performance comparison in terms of natural-\nness and usefulness in the single-turn and multi-turn\nsettings. Each value represents the percentage of pair-\nwise comparisons that the specific model wins or ties.\nprovided in the OpenAI API as the scorer with the\nconversation, explanation, and scoring rules con-\ncatenated as prompts (see Appendix C.4). Other\nparameters remain the same as the simulated user.\n5 Evaluation Results\nIn this section, we assess the quality of the user\nsimulator and the performance of CRSs using our\nproposed evaluation approach.\n5.1 The Quality of User Simulator\nTo evaluate the performance of CRSs in an inter-\nactive setting, we construct user simulators based\non ground-truth items from existing datasets. The\nsimulated users should cooperate with the system\nto find the target item, e.g., answer clarification\nquestions and provide feedback on recommenda-\ntions. However, it is not easy to directly evaluate\nthe quality of user simulators.\nOur solution is to make use of the annotated\nconversations in existing datasets. We first use the\nground-truth items to set up the persona of the user\nsimulator and then let them interact with the sys-\ntems played by humans. They are provided with\nthe first round of annotated conversations to com-\nplete the rest. Then, we can compare the completed\nconversations with the annotated ones for evalua-\ntion. Following Sekuli´c et al. (2022), we assess the\nnaturalness and usefulness of the generated utter-\nances in the settings of single-turn and multi-turn\nfree-form chit-chat. Naturalness means that the\nutterances are fluent and likely to be generated by\nhumans, and usefulness means that the utterances\nare consistent with the user preference. We com-\npare our user simulator with a fine-tuned version\nof DialoGPT and the original conversations in the\nREDIAL dataset.\nSpecifically, we first invite five annotators to\nplay the role of the system and engage in interac-\ntions with each user simulator. The interactions are\nbased on the first round of conversations from 100\nrandomly sampled examples. Then, we employ an-\nother two annotators to make pairwise evaluations,\nwhere one is generated by our simulator and the\nother comes from DialoGPT or the dataset. We\ncount a win for a method when both annotators\nagree that its utterance is better; otherwise, we\ncount a tie. The Cohen’s Kappa between annota-\ntors is 0.73. Table 4 demonstrates the results. We\ncan see that our simulator significantly outperforms\nDialoGPT, especially in terms of naturalness in the\nmulti-turn setting, which demonstrates the strong\nlanguage generation capability of LLMs. Further-\nmore, the usefulness of our simulator is better than\nothers, indicating that it can provide helpful infor-\nmation to cooperate with the system.\n5.2 The Performance of CRS\nIn this part, we compare the performance of ex-\nisting CRSs and ChatGPT using different evalua-\ntion approaches. For ChatGPT, we use ChatGPT +\ntext-embedding-ada-002 due to its superior perfor-\nmance in traditional evaluation (see Appendix C.2).\n5.2.1 Main Results\nThe evaluation results are presented in Table 5 and\nTable 6. Overall, most models demonstrate im-\nproved accuracy and explainability compared to\nthe traditional approach. Among existing CRSs,\nthe order of performance is UniCRS > BARCOR\n> KBRD . Both UniCRS and BARCOR utilize\npre-trained models to enhance conversation abili-\nties. Additionally, UniCRS incorporates KGs into\nprompts to enrich entity semantics for better under-\nstanding user preferences. It indicates that existing\nCRSs have the ability to interact with users for bet-\nter recommendations and user experience, which\nis an important aspect overlooked in the traditional\nevaluation.\nFor ChatGPT, there is a significant performance\nimprovement in both Recall and Persuasiveness,\nand the Recall@10 value even surpassing the Re-\ncall@25 or Recall@50 value of most CRSs on the\ntwo datasets. This indicates that ChatGPT has su-\nperior interaction abilities compared with existing\nCRSs and can provide high-quality and persuasive\nrecommendations with sufficient information about\nthe user preference. The results demonstrate the\neffectiveness of iEvaLM in evaluating the accuracy\nand explainability of recommendations for CRSs,\nespecially those developed with LLMs.\nModel KBRD BARCOR UniCRS ChatGPT\nEvaluation ApproachOriginaliEvaLM(attr) iEvaLM(free) OriginaliEvaLM(attr) iEvaLM(free) OriginaliEvaLM(attr) iEvaLM(free) OriginaliEvaLM(attr) iEvaLM(free)\nReDial\nR@1 0.028 0.039(+39.3%)0.035(+25.0%)0.031 0.034(+9.7%)0.034(+9.7%)0.050 0.053(+6.0%) 0.107(+114.0%)0.037 0.191*(+416.2%)0.146(+294.6%)\nR@100.169 0.196(+16.0%)0.198(+17.2%)0.170 0.201(+18.2%)0.190(+11.8%)0.215 0.238(+10.7%)0.317(+47.4%)0.174 0.536*(+208.0%)0.440(+152.9%)\nR@500.366 0.436(+19.1%)0.453(+23.8%)0.372 0.427(+14.8%)0.467(+25.5%)0.413 0.520(+25.9%)0.602*(+45.8%) – – –\nOpenDialKG\nR@1 0.231 0.131(-43.3%)0.234(+1.3%)0.312 0.264(-15.4%)0.314(+0.6%)0.308 0.180(-41.6%)0.314(+1.9%)0.310 0.299(-3.5%) 0.400*(+29.0%)\nR@100.423 0.293(-30.7%)0.431(+1.9%)0.453 0.423(-6.7%) 0.458(+1.1%)0.513 0.393(-23.4%)0.538(+4.9%)0.539 0.604(+12.1%)0.715*(+32.7%)\nR@250.492 0.377(-23.4%)0.509(+3.5%)0.510 0.482(-5.5%) 0.530(+3.9%)0.574 0.458(-20.2%)0.609*(+6.1%) – – –\nTable 5: Performance of CRSs and ChatGPT under different evaluation approaches, where “attr” denotes attribute-\nbased question answering and “free” denotes free-form chit-chat. “R@k” refers to Recall@k. Since requiring too\nmany items can sometimes be refused by ChatGPT, we only assess Recall@1 and 10 for it, while Recall@50 is\nmarked as “–”. Numbers marked with * indicate that the improvement is statistically significant compared with the\nrest methods (t-test with p-value < 0.05).\nModel Evaluation\nApproach ReDial OpenDialKG\nKBRD\nOriginal 0.638 0.824\niEvaLM 0.766\n(+20.1%)\n0.862\n(+4.6%)\nBARCOR\nOriginal 0.667 1.149\niEvaLM 0.795\n(+19.2%)\n1.211\n(+5.4%)\nUniCRS\nOriginal 0.685 1.128\niEvaLM 1.015\n(+48.2%)\n1.314\n(+16.5%)\nChatGPT\nOriginal 0.787 1.221\niEvaLM 1.331*\n(+69.1%)\n1.513*\n(+23.9%)\nTable 6: The persuasiveness of explanations. We only\nconsider the setting of free-form chit-chat in iEvaLM.\nNumbers marked with * indicate that the improvement is\nstatistically significant compared with the rest methods\n(t-test with p-value < 0.05).\nComparing the two interaction settings, Chat-\nGPT has demonstrated greater potential as a\ngeneral-purpose CRS. Existing CRSs perform\nmuch worse in the setting of attribute-based ques-\ntion answering than in the traditional setting on\nthe OPEN DIAL KG dataset. One possible reason is\nthat they are trained on datasets with natural lan-\nguage conversations, which is inconsistent with the\nsetting of attribute-based question answering. In\ncontrast, ChatGPT performs much better in both\nsettings on the two datasets, since it has been spe-\ncially trained on conversational data. The results\nindicate the limitations of the traditional evaluation,\nwhich focuses only on a single conversation sce-\nnario, while our evaluation approach allows for a\nMethod UnpersuasivePartially\npersuasive\nHighly\npersuasive\niEvaLM 1% 5% 94%\nHuman 4% 7% 89%\nTable 7: The score distribution of persuasiveness (“un-\npersuasive” for 0, “partially persuasive” for 1, and\n“highly persuasive” for 2) from our LLM-based scorer\nand human on a random selection of 100 examples from\nthe REDIAL dataset.\nmore holistic assessment of CRSs, providing valu-\nable insights into their strengths and weaknesses\nacross different types of interactions.\n5.2.2 The Reliability of Evaluation\nRecall that LLMs are utilized in the user simulation\nand performance measurement parts of iEvaLM as\nalternatives for humans in Section 4. Considering\nthat the generation of LLMs can be unstable, in\nthis part, we conduct experiments to assess the\nreliability of the evaluation results compared with\nusing human annotators.\nFirst, recall that we introduce the subjective met-\nric persuasiveness for evaluating explanations in\nSection 4.4. This metric usually requires human\nevaluation, and we propose an LLM-based scorer\nas an alternative. Here we evaluate the reliability of\nour LLM-based scorer by comparing with human\nannotators. We randomly sample 100 examples\nwith the explanations generated by ChatGPT and\nask our scorer and two annotators to rate them sep-\narately with the same instruction (see Appendix C).\nThe Cohen’s Kappa between annotators is 0.83. Ta-\nble 7 demonstrates that the two score distributions\nare similar, indicating the reliability of our LLM-\nEvaluation ApproachKBRD BARCOR UniCRS ChatGPT\niEvaLM Recall@100.180 0.210 0.330 0.460\nPersuasiveness0.810 0.860 1.050 1.330\nHuman Recall@100.210 0.250 0.370 0.560\nPersuasiveness0.870 0.930 1.120 1.370\nTable 8: The evaluation results using simulated and real\nusers on a random selection of 100 examples from the\nREDIAL dataset.\nbased scorer as a substitute for human evaluators.\nThen, since we propose an LLM-based user sim-\nulator as a replacement for humans to interact with\nCRSs, we examine the correlation between the\nvalues of metrics when using real vs. simulated\nusers. Following Section 4.3, both real and sim-\nulated users receive the same instruction (see Ap-\npendix C) to establish their personas based on the\nground-truth items. Each user can interact with\ndifferent CRSs for five rounds. We randomly select\n100 instances and employ five annotators and our\nuser simulator to engage in free-form chit-chat with\ndifferent CRSs. The results are shown in Table 8.\nWe can see that the ranking obtained from our user\nsimulator is consistent with that of real users, and\nthe absolute scores are also comparable. It suggests\nthat our LLM-based user simulator is capable of\nproviding convincing evaluation results and serves\nas a reliable alternative to human evaluators.\n6 Conclusion\nIn this paper, we systematically examine the capa-\nbility of ChatGPT for conversational recommenda-\ntion on existing benchmark datasets and propose\nan alternative evaluation approach, iEvaLM. First,\nwe show that the performance of ChatGPT was\nunsatisfactory. Through analysis of failure cases,\nthe root cause is the existing evaluation protocol,\nwhich overly emphasizes the fitting of ground-truth\nitems based on conversation context. To address\nthis issue, we propose an interactive evaluation ap-\nproach using LLM-based user simulators.\nThrough experiments with this new approach,\nwe have the following findings: (1) ChatGPT is\npowerful and becomes much better in our evalua-\ntion than the currently leading CRSs in both accu-\nracy and explainability; (2) Existing CRSs also get\nimproved from the interaction, which is an impor-\ntant aspect overlooked by the traditional evaluation;\nand (3) ChatGPT shows great potential as a general-\npurpose CRS under different settings and datasets.\nWe also demonstrate the effectiveness and reliabil-\nity of our evaluation approach.\nOverall, our work contributes to the understand-\ning and evaluation of LLMs such as ChatGPT for\nconversational recommendation, paving the way\nfor further research in this field in the era of LLMs.\nLimitations\nA major limitation of this work is the design of\nprompts for ChatGPT and LLM-based user simu-\nlators. We manually write several prompt candi-\ndates and select the one with the best performance\non some representative examples due to the cost\nof calling model APIs. More effective prompting\nstrategies like chain-of-thought can be explored for\nbetter performance, and the robustness of the eval-\nuation framework to different prompts remains to\nbe assessed.\nIn addition, our evaluation framework primar-\nily focuses on the accuracy and explainability of\nrecommendations, but it may not fully capture po-\ntential issues related to fairness, bias, or privacy\nconcerns. Future work should explore ways to in-\ncorporate these aspects into the evaluation process\nto ensure the responsible deployment of CRSs.\nAcknowledgements\nThis work was partially supported by National Nat-\nural Science Foundation of China under Grant No.\n62222215 and 72222022, Beijing Natural Science\nFoundation under Grant No. 4222027, and the\nOutstanding Innovative Talents Cultivation Funded\nPrograms 2022 of Renmin University of China.\nXin Zhao is the corresponding author.\nReferences\nJafar Afzali, Aleksander Mark Drzewiecki, Krisz-\ntian Balog, and Shuo Zhang. 2023. Usersim-\ncrs: A user simulation toolkit for evaluating con-\nversational recommender systems. arXiv preprint\narXiv:2301.05544.\nKrisztian Balog and ChengXiang Zhai. 2023. User\nsimulation for evaluating information access systems.\narXiv preprint arXiv:2306.08550.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang\nTang. 2017. A survey on dialogue systems: Re-\ncent advances and new frontiers. Acm Sigkdd Ex-\nplorations Newsletter, 19(2):25–35.\nQibin Chen, Junyang Lin, Yichang Zhang, Ming Ding,\nYukuo Cen, Hongxia Yang, and Jie Tang. 2019. To-\nwards knowledge-based recommender dialog system.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1803–1813.\nXu Chen, Yongfeng Zhang, and Ji-Rong Wen. 2022.\nMeasuring\" why\" in recommender systems: a com-\nprehensive survey on the evaluation of explainable\nrecommendation. arXiv preprint arXiv:2202.06466.\nSunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu,\nZihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang,\nand Jun Xu. 2023. Uncovering chatgpt’s capa-\nbilities in recommender systems. arXiv preprint\narXiv:2305.02182.\nYang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang\nLei, Tat-Seng Chua, and Wai Lam. 2023. A unified\nmulti-task learning framework for multi-goal conver-\nsational recommender systems. ACM Transactions\non Information Systems, 41(3):1–25.\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.\n2023. Improving language model negotiation with\nself-play and in-context learning from ai feedback.\narXiv preprint arXiv:2305.10142.\nChongming Gao, Wenqiang Lei, Xiangnan He, Maarten\nde Rijke, and Tat-Seng Chua. 2021. Advances and\nchallenges in conversational recommender systems:\nA survey. AI Open, 2:100–126.\nJianfeng Gao, Michel Galley, and Lihong Li. 2018. Neu-\nral approaches to conversational ai. In The 41st In-\nternational ACM SIGIR Conference on Research &\nDevelopment in Information Retrieval, pages 1371–\n1374.\nShuyu Guo, Shuo Zhang, Weiwei Sun, Pengjie Ren,\nZhumin Chen, and Zhaochun Ren. 2023. Towards\nexplainable conversational recommender systems.\narXiv preprint arXiv:2305.18363.\nDietmar Jannach, Ahtsham Manzoor, Wanling Cai, and\nLi Chen. 2021. A survey on conversational recom-\nmender systems. ACM Computing Surveys (CSUR),\n54(5):1–36.\nWang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Mah-\neswaran Sathiamoorthy, Lichan Hong, Ed Chi, and\nDerek Zhiyuan Cheng. 2023. Do llms understand\nuser preferences? evaluating llms on user rating pre-\ndiction. arXiv preprint arXiv:2305.06474.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of naacL-HLT, volume 1, page 2.\nWenqiang Lei, Xiangnan He, Yisong Miao, Qingyun\nWu, Richang Hong, Min-Yen Kan, and Tat-Seng\nChua. 2020. Estimation-action-reflection: Towards\ndeep interaction between conversational and recom-\nmender systems. In Proceedings of the 13th Interna-\ntional Conference on Web Search and Data Mining,\npages 304–312.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nRaymond Li, Samira Ebrahimi Kahou, Hannes Schulz,\nVincent Michalski, Laurent Charlin, and Chris Pal.\n2018. Towards deep conversational recommenda-\ntions. Advances in neural information processing\nsystems, 31.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,\nIshaan Gulrajani, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. 2023. Alpacaeval: An au-\ntomatic evaluator of instruction-following models.\nhttps://github.com/tatsu-lab/alpaca_eval.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. Opendialkg: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845–854.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\n2022. Text and code embeddings by contrastive pre-\ntraining. arXiv preprint arXiv:2201.10005.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nGustavo Penha and Claudia Hauff. 2020. What does\nbert know about books, movies and music? probing\nbert for conversational recommendation. In Proceed-\nings of the 14th ACM Conference on Recommender\nSystems, pages 388–397.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nIvan Sekuli ´c, Mohammad Aliannejadi, and Fabio\nCrestani. 2022. Evaluating mixed-initiative conversa-\ntional search systems via user simulation. InProceed-\nings of the Fifteenth ACM International Conference\non Web Search and Data Mining, pages 888–896.\nQuan Tu, Shen Gao, Yanran Li, Jianwei Cui, Bin Wang,\nand Rui Yan. 2022. Conversational recommendation\nvia hierarchical information modeling. In Proceed-\nings of the 45th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 2201–2205.\nLingzhi Wang, Huang Hu, Lei Sha, Can Xu, Daxin\nJiang, and Kam-Fai Wong. 2022a. Recindial: A uni-\nfied framework for conversational recommendation\nwith pretrained language models. In Proceedings of\nthe 2nd Conference of the Asia-Pacific Chapter of\nthe Association for Computational Linguistics and\nthe 12th International Joint Conference on Natural\nLanguage Processing, pages 489–500.\nTing-Chun Wang, Shang-Yu Su, and Yun-Nung Chen.\n2022b. Barcor: Towards a unified framework for con-\nversational recommendation systems. arXiv preprint\narXiv:2203.14257.\nXiaolei Wang, Kun Zhou, Xinyu Tang, Wayne Xin\nZhao, Fan Pan, Zhao Cao, and Ji-Rong Wen. 2023.\nImproving conversational recommendation systems\nvia counterfactual data simulation. In Proceedings\nof the 29th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD 2023, Long\nBeach, CA, USA, August 6-10, 2023 , pages 2398–\n2408. ACM.\nXiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin\nZhao. 2022c. Towards unified conversational rec-\nommender systems via knowledge-enhanced prompt\nlearning. In Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Min-\ning, pages 1929–1937.\nLe Wu, Xiangnan He, Xiang Wang, Kun Zhang, and\nMeng Wang. 2022. A survey on accuracy-oriented\nneural recommendation: From collaborative filtering\nto information-rich recommendation. IEEE Transac-\ntions on Knowledge and Data Engineering.\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu,\nZhengyu Chen, and Jian Zhang. 2023. On the tool\nmanipulation capability of open-source large lan-\nguage models. arXiv preprint arXiv:2305.16504.\nBowen Yang, Cong Han, Yu Li, Lei Zuo, and Zhou\nYu. 2022. Improving conversational recommenda-\ntion systems’ quality with context-aware item meta-\ninformation. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 38–48.\nShuo Zhang and Krisztian Balog. 2020. Evaluating con-\nversational recommender systems via user simulation.\nIn Proceedings of the 26th acm sigkdd international\nconference on knowledge discovery & data mining,\npages 1512–1520.\nXiaoyu Zhang, Xin Xin, Dongdong Li, Wenxuan Liu,\nPengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun\nRen. 2023. Variational reasoning over incomplete\nknowledge graphs for conversational recommenda-\ntion. In Proceedings of the Sixteenth ACM Interna-\ntional Conference on Web Search and Data Mining,\npages 231–239.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and William B Dolan. 2020. Dialogpt: Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278.\nYongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and\nW Bruce Croft. 2018. Towards conversational search\nand recommendation: System ask, user respond. In\nProceedings of the 27th acm international conference\non information and knowledge management, pages\n177–186.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023a. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nWeixiang Zhao, Yanyan Zhao, Xin Lu, Shilong Wang,\nYanpeng Tong, and Bing Qin. 2023b. Is chat-\ngpt equipped with emotional dialogue capabilities?\narXiv preprint arXiv:2304.09582.\nZhipeng Zhao, Kun Zhou, Xiaolei Wang, Wayne Xin\nZhao, Fan Pan, Zhao Cao, and Ji-Rong Wen. 2023c.\nAlleviating the long-tail problem in conversational\nrecommender systems. In Proceedings of the 17th\nACM Conference on Recommender Systems, pages\n374–385.\nJinfeng Zhou, Bo Wang, Ruifang He, and Yuexian Hou.\n2021a. Crfr: Improving conversational recommender\nsystems via flexible fragments reasoning on knowl-\nedge graphs. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 4324–4334.\nJinfeng Zhou, Bo Wang, Minlie Huang, Dongming\nZhao, Kun Huang, Ruifang He, and Yuexian Hou.\n2022. Aligning recommendation and conversation\nvia dual imitation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 549–561, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nKun Zhou, Xiaolei Wang, Yuanhang Zhou, Chenzhan\nShang, Yuan Cheng, Wayne Xin Zhao, Yaliang Li,\nand Ji-Rong Wen. 2021b. Crslab: An open-source\ntoolkit for building conversational recommender sys-\ntem. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing: System Demonstrations ,\npages 185–193.\nKun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang\nZhou, Ji-Rong Wen, and Jingsong Yu. 2020. Improv-\ning conversational recommender systems via knowl-\nedge graph based semantic fusion. In Proceedings\nof the 26th ACM SIGKDD international conference\non knowledge discovery & data mining, pages 1006–\n1014.\n1 2 3 4 50.05\n0.15\n0.25\n0.35\n0.45\n0.55\n0.65\nRecall@10\nfree\nattr\nFigure 4: The performance of ChatGPT with different\ninteraction rounds under the setting of attribute-based\nquestion answering (attr) and free-form chit-chat (free)\non the REDIAL dataset.\nA The Influence of the Number of\nInteraction Rounds in iEvaLM\nInteracting with the user for multiple rounds typi-\ncally leads to more information and improved rec-\nommendation accuracy. However, users have lim-\nited patience and may leave the interaction when\nthey become exhausted. It is important to investi-\ngate the relationship between the number of inter-\naction rounds and performance. Following the set-\nting in our approach, the interaction between Chat-\nGPT and users is start from the observed human-\nannotated conversation in each dataset example,\nand we set the maximum interaction rounds to val-\nues from {1, 2, 3, 4, 5}, in order to evaluate the\nchanges in recommendation accuracy.\nFigure 4 shows the results of Recall@10 on the\nREDIAL dataset. In attribute-based question an-\nswering, the performance keeps increasing and\nreaches saturation at round 4. This observation\naligns with our conversation setting, since the RE-\nDIAL dataset only has three attributes to inquire\nabout. In free-form chit-chat, the performance\ncurve is steep between rounds 1 and 3, while it\nis relatively flat between rounds 3 and 5. This pat-\ntern may be attributed to insufficient information\nin the initial round and marginal information in\nthe last rounds. Since the user will gradually get\nexhausted with the progress of the interaction, how\nto optimize the conversation strategy remains to be\nfurther studied.\nB Related Work\nIn this section, we summarize the related work\nfrom the following perspectives.\nB.1 Conversational Recommender System\nThe fields of conversation intelligence (Chen et al.,\n2017; Gao et al., 2018) and recommendation\nsystems (Wu et al., 2022) have seen significant\nprogress in recent years. One promising develop-\nment is the integration of these two fields, leading\nto the emergence of conversational recommender\nsystems (CRSs) (Jannach et al., 2021; Gao et al.,\n2021). CRSs provide recommendations to users\nthrough conversational interactions, which has the\npotential to significantly improve the user experi-\nence.\nOne popular approach (Lei et al., 2020; Tu et al.,\n2022) assumes that interactions with users primar-\nily take the form of question answering, where\nusers are asked about their preferences for items\nand their attributes. The goal is to learn an optimal\ninteraction strategy that captures user preferences\nand provides accurate recommendations in as few\nturns as possible. However, this approach often re-\nlies on hand-crafted templates and does not explic-\nitly model the language aspect of CRSs. Another\napproach (Zhang et al., 2023; Zhou et al., 2022) fo-\ncuses on engaging users in more free-form natural\nlanguage conversations, such as chit-chat. The aim\nis to capture user preferences from the conversa-\ntion context and generate recommendations using\npersuasive responses.\nOur work belongs to the second category. In this\nwork, we systematically evaluate the performance\nof large language models (LLMs) like ChatGPT\nfor conversational recommendation on large-scale\ndatasets.\nB.2 Language Models for Conversational\nRecommendation\nThere have been recent studies on how to integrate\nlanguage models (LMs) into CRSs. One notable\ninvestigation by Penha and Hauff (2020) evalu-\nates the performance of the pre-trained language\nmodel (PLM) BERT (Kenton and Toutanova, 2019)\nin conversational recommendation. Other stud-\nies (Wang et al., 2022c; Yang et al., 2022; Deng\net al., 2023) primarily utilize PLMs as the foun-\ndation to build unified CRSs, capable of perform-\ning various tasks using a single model instead of\nmultiple components. However, the current ap-\nproaches are mainly confined to small-size LMs\nlike BERT (Kenton and Toutanova, 2019) and Di-\naloGPT (Zhang et al., 2020).\nIn this paper, we focus on the evaluation of CRSs\ndeveloped with not only PLMs but also LLMs and\npropose a new evaluation approach iEvaLM.\nB.3 Evaluation and User Simulation\nThe evaluation of CRSs remains an area that has\nnot been thoroughly explored in existing literature.\nPrevious studies have primarily focused on turn-\nlevel evaluation (Chen et al., 2019), where the sys-\ntem output of a single turn is compared against\nground-truth labels for two major tasks: conver-\nsation and recommendation. Some researchers\nhave also adopted conversation-level evaluation\nto assess conversation strategies (Lei et al., 2020;\nZhang et al., 2018; Balog and Zhai, 2023; Afzali\net al., 2023). In such cases, user simulation is\noften employed as a substitute for human evalua-\ntion. These approaches typically involve collecting\nreal user interaction history (Lei et al., 2020) or\nreviews (Zhang et al., 2018) to represent the pref-\nerences of simulated users. Zhou et al. (2021b) de-\nvelop an open-source toolkit called CRSLab, which\nprovides extensive and standard evaluation proto-\ncols. However, due to the intricate and interactive\nnature of conversational recommendation, the eval-\nuation is often constrained by pre-defined conver-\nsation flows or template-based utterances. Conse-\nquently, this limitation hinders the comprehensive\nassessment of the practical utility of CRSs.\nIn our work, we propose an interactive evalua-\ntion approach iEvaLM with LLM-based user sim-\nulators, which has a strong instruction-following\nability and can flexibly adapt to different CRSs\nbased on the instruction without further tuning.\nC Prompts Used in the Paper\nC.1 Prompts for ChatGPT in the Traditional\nEvaluation\nWe use the following prompts for zero-shot prompt-\ning in section 3.1.\n• ReDial\nRecommend 10 items that are consistent with\nuser preference. The recommendation list can\ncontain items that the dialog mentioned before.\nThe format of the recommendation list is: no.\ntitle (year). Don’t mention anything other than\nthe title of items in your recommendation list.\n• OpenDialKG\nRecommend 10 items that are consistent with\nuser preference. The recommendation list can\ncontain items that the dialog mentioned before.\nThe format of the recommendation list is: no.\ntitle. Don’t mention anything other than the\ntitle of items in your recommendation list.\nC.2 Prompts for ChatGPT in iEvaLM\nWe use the following prompts for ChatGPT in our\nnew evaluation approach.\nC.2.1 Recommendation\nFree-Form Chit-Chat.\n• ReDial\nYou are a recommender chatting with the user\nto provide recommendation. You must follow\nthe instructions below during chat.\nIf you do not have enough information about\nuser preference, you should ask the user for his\npreference.\nIf you have enough information about user pref-\nerence, you can give recommendation. The rec-\nommendation list must contain 10 items that\nare consistent with user preference. The rec-\nommendation list can contain items that the\ndialog mentioned before. The format of the\nrecommendation list is: no. title (year). Don’t\nmention anything other than the title of items\nin your recommendation list.\n• OpenDialKG\nYou are a recommender chatting with the user\nto provide recommendation. You must follow\nthe instructions below during chat.\nIf you do not have enough information about\nuser preference, you should ask the user for his\npreference.\nIf you have enough information about user pref-\nerence, you can give recommendation. The rec-\nommendation list must contain 10 items that\nare consistent with user preference. The rec-\nommendation list can contain items that the\ndialog mentioned before. The format of the\nrecommendation list is: no. title. Don’t men-\ntion anything other than the title of items in\nyour recommendation list.\nAttribute-Based Question Answering. “{}”\nrefers to the options that have been selected.\n• ReDial\nTo recommend me items that I will accept, you\ncan choose one of the following options.\nA: ask my preference for genre\nB: ask my preference for actor\nC: ask my preference for director\nD: I can directly give recommendations\nYou have selected {}, do not repeat them.\nPlease enter the option character.\n• OpenDialKG\nTo recommend me items that I will accept, you\ncan choose one of the following options.\nA: ask my preference for genre\nB: ask my preference for actor\nC: ask my preference for director\nD: ask my preference for writer\nE: I can directly give recommendations\nYou have selected {}, do not repeat them.\nPlease enter the option character.\nC.2.2 Explainability\nPlease explain your last time of recommen-\ndation.\nC.3 Prompts for the User Simulator in\niEvaLM\nWe use the following prompts for\ntext-davinci-003 to play the role of the\nuser during interaction.\nFree-Form Chit-Chat. “{}” refers to the item\nlabels of each example in the datasets.\nYou are a seeker chatting with a recommender for\nrecommendation. Your target items: {}. You must\nfollow the instructions below during chat.\nIf the recommender recommends {}, you should ac-\ncept.\nIf the recommender recommends other items, you\nshould refuse them and provide the information about\n{}. You should never directly tell the target item title.\nIf the recommender asks for your preference, you\nshould provide the information about {}. You should\nnever directly tell the target item title.\nAttribute-Based Question Answering.\n• When the recommended item list contains at\nleast one of the target items:\nThat’s perfect, thank you!\n• When the recommended item list does not\ncontain any target item:\nI don’t like them.\n• When the system asks about the preference\nover pre-defined attributes, we use the at-\ntributes of target items as the answer if they\nexist, otherwise:\nSorry, no information about this.\nC.4 Prompts for the LLM-based Scorer in\niEvaLM\nWe use the following prompts for\ntext-davinci-003 to score the persuasive-\nness of explanations. “{}” refers to the item labels\nof each example in the datasets.\nDoes the explanation make you want to accept the\nrecommendation? Please give your score.\nIf mention one of [{}], give 2.\nElse if you think recommended items are worse than\n[{}], give 0.\nElse if you think recommended items are comparable\nto [{}] according to the explanation, give 1.\nElse if you think recommended items are better than\n[{}] according to the explanation, give 2.\nOnly answer the score number.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8043723106384277
    },
    {
      "name": "Matching (statistics)",
      "score": 0.6597291231155396
    },
    {
      "name": "Protocol (science)",
      "score": 0.6438155770301819
    },
    {
      "name": "Comprehension",
      "score": 0.6217719316482544
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3880157172679901
    },
    {
      "name": "Data science",
      "score": 0.36643385887145996
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36275070905685425
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    }
  ],
  "cited_by": 3
}