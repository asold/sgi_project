{
  "title": "Measuring Systematic Generalization in Neural Proof Generation with Transformers",
  "url": "https://openalex.org/W3090866633",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221612557",
      "name": "Gontier, Nicolas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281548616",
      "name": "Sinha, Koustuv",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221612558",
      "name": "Reddy, Siva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213710450",
      "name": "Pal, Christopher",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2788751659",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2963919731",
    "https://openalex.org/W2996094825",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2990379018",
    "https://openalex.org/W2863059257",
    "https://openalex.org/W2962875487",
    "https://openalex.org/W2719119486",
    "https://openalex.org/W2013606862",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3100385063",
    "https://openalex.org/W2964071299",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W2997359684",
    "https://openalex.org/W2950246755",
    "https://openalex.org/W3035168240",
    "https://openalex.org/W3035718362",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2971107062"
  ],
  "abstract": "We are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies.",
  "full_text": "Measuring Systematic Generalization\nin Neural Proof Generation with Transformers\nNicolas Gontier\nQuebec Artiﬁcal Intelligence Institute (Mila),\nPolytechnique Montréal\ngontiern@mila.quebec\nKoustuv Sinha\nQuebec Artiﬁcal Intelligence Institute (Mila),\nMcGill University\nFacebook AI Research\nSiva Reddy\nQuebec Artiﬁcal Intelligence Institute (Mila),\nMcGill University\nFacebook CIFAR AI Chair\nChristopher Pal\nQuebec Artiﬁcal Intelligence Institute (Mila),\nPolytechnique Montréal\nElementAI\nCanada CIFAR AI Chair\nAbstract\nWe are interested in understanding how well Transformer language models (TLMs)\ncan perform reasoning tasks when trained on knowledge encoded in the form of\nnatural language. We investigate their systematic generalization abilities on a logi-\ncal reasoning task in natural language, which involves reasoning over relationships\nbetween entities grounded in ﬁrst-order logical proofs. Speciﬁcally, we perform\nsoft theorem-proving by leveraging TLMs to generate natural language proofs.\nWe test the generated proofs for logical consistency, along with the accuracy of\nthe ﬁnal inference. We observe length-generalization issues when evaluated on\nlonger-than-trained sequences. However, we observe TLMs improve their general-\nization performance after being exposed to longer, exhaustive proofs. In addition,\nwe discover that TLMs are able to generalize better using backward-chaining\nproofs compared to their forward-chaining counterparts, while they ﬁnd it easier\nto generate forward chaining proofs. We observe that models that are not trained\nto generate proofs are better at generalizing to problems based on longer proofs.\nThis suggests that Transformers have efﬁcient internal reasoning strategies that are\nharder to interpret. These results highlight the systematic generalization behavior\nof TLMs in the context of logical reasoning, and we believe this work motivates\ndeeper inspection of their underlying reasoning strategies.\n1 Introduction\nSystematic Generalization has been characterized as the capacity to understand and produce a poten-\ntially inﬁnite number of novel combinations from known components (Chomsky, 1957; Montague,\n1970). For example, in Figure 1, a model could be exposed to a set of facts (e.g., “Nat is the grand-\ndaughter of Betty”, “Greg is the brother of Nat”, “Flo is the sister of Greg”), but not to all the possible\nfacts that can be inferred by combination of the known components (e.g., “Flo is the granddaughter\nof Betty”). More recent work has examined systematic generalization in terms of the ability of “a\nmodel to manipulate concepts in new combinations after being trained on all concepts, but only on a\nlimited set of their combinations” (Bahdanau et al., 2019a). This view of systematic generalization\nshifts emphasis from reasoning to learning. Here we examine systematic generalization through\nmeasuring the ability of a model to reason about new inference step combinations despite being\ntrained on a limited subset of them.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2009.14786v2  [cs.LG]  20 Oct 2020\nRecent developments in natural language processing (NLP) have shown that Transformer (Vaswani\net al., 2017) Language Models (TLMs) are able to capture linguistic knowledge (Peters et al.,\n2018; Goldberg, 2019; Tenney et al., 2019), and yield state-of-the-art performance for many NLP\ntasks (Radford et al., 2018; Devlin et al., 2019), including but not limited to answering reading\ncomprehension questions (Radford et al., 2019; Brown et al., 2020) and generating factual knowledge\n(Petroni et al., 2019) with little to no task supervision. These models are optimized on large corpora\nto predict the next words or a set of masked words in a sentence. While yielding impressive results, it\nis not clear if TLMs rely on many superﬁcial patterns in the data or if they actually learn re-usable\nskills, enabling them to generalize to new tasks by leveraging the compositionality of those skills\n(Lake and Baroni, 2018; Liška et al., 2018). Training on massive data can give certain advantages\nwith respect to understanding the meanings of words, but we conjecture that such data gives models\nless experience with reasoning over inference chains.\ngranddaughter \nbrother Nat. \ngrandson \nGreg. \ngranddaughter \nsister \nsister Flo. \nBetty \nFigure 1: Example of\na CLUTRR graph with\nknown facts (solid lines)\nand unknown facts to in-\nfer (dotted lines).\nIn our work, we study the less understood issues related to how well TLMs\nare able to perform long chains of reasoning. In particular, we use TLMs\nfor the task of theorem proving, where facts and proofs are speciﬁed in\nnatural language. Using theorem proving, we test if TLMs can generate\ninterpretable proofs with logically consistent language modeling as their\nmain objective. In particular, we study their behavior as logical reasoners\non text by analyzing the generated proofs and the ﬁnal answer. This\nsetup allows us to evaluate the reasoning and generalization capabilities\nof TLMs. Recent work such as Petroni et al. (2019); Raffel et al. (2020);\nBrown et al. (2020) suggest that language models can be treated as\nknowledge bases. This directly motivates us to investigate if language\nmodels can also learn certain reasoning strategies. Studying these abilities\ncan give us insights to facilitate the use of such models as dynamic\nknowledge bases that could infer new knowledge even if it is not seen\nduring pre-training.\nFor natural language theorem proving, we use the question answering CLUTRR benchmark suite\n(Sinha et al., 2019) to perform controlled studies. This dataset is of interest because: (i) the\ncompositional nature of tasks involved make it well suited for evaluating systematic generalization,\nand (ii) each question–answer pair is accompanied by a proof that can be used to explain how to\narrive at the answer. We use this dataset as a medium to understand the reasoning capacity of TLMs.\nOur experiments reveal the following:\n1. TLMs suffer from length generalization: they cannot extrapolate to proofs requiring more proof\nsteps than seen during training time.\n2. They generalize better when trained to generate long proofs compared to short proofs.\n3. They generalize better when trained to generate backward-chaining proofs rather than forward-\nchaining.\n4. Surprisingly, they generalize better when they are trained to directly generate the answer instead\nof learning to generate the proof and then the answer.\nTo the best of our knowledge, we are the ﬁrst to use a language modeling objective to do interpretable\ntheorem proving with a Transformer. We hope that this work can shed some light on the reasoning\ncapacity of TLMs and inspire future research to design models with greater reasoning capacity.\n2 Related Work\nSystematic generalization has recently been in spotlight due to its importance in understanding the\nstrengths and weaknesses of neural networks. Bahdanau et al. (2019a,b) identify and evaluate the\ngeneralization capacity of visual question answering models. We however focus this study on a fully\nnatural language domain. Dasgupta et al. (2019) introduce a natural language inference (NLI) dataset\nwhich proves to be challenging for language understanding models for compositional generalization.\nGoodwin et al. (2020) also evaluate systematic generalization in an NLI setting with controlled test\ncases to observe the failures of neural architectures. We however focus this study on the systematic\ngeneration of logical reasoning sentences by Transformer-based (Vaswani et al., 2017) language\nmodels in a question answering setting with the CLUTRR suite (Sinha et al., 2019). Similar datasets\ninclude SCAN (Lake and Baroni, 2018) which has been instrumental to test systematic generalization\n2\nraw facts amt\nstory\n[(Natasha, granddaughter, Betty),\n(Florence, sister, Gregorio),\n(Gregorio, brother, Natasha)]\n<STORY>\nNatasha is a granddaughter to Betty.\nFlorence is Gregorio ’s sister.\nGregorio is a brother of Natasha.\n<STORY> Betty likes picking berries with\nher son ’s daughter. Her name is Natasha.\nGregorio took his sister, Florence, to a baseball game.\nGregorio and his sister Natasha love it when their\ngrandmother visits because she spoils them.\nShe is coming this week to watch them while\ntheir parents are out of town.\nquery (Florence, _, Betty) <QUERY> Who is Florence for Betty ?\nproof\n[{(Florence, granddaughter, Betty):\n[(Florence, sister, Gregorio),\n(Gregorio, grandson, Betty)]},\n{(Gregorio, grandson, Betty):\n[(Gregorio, brother, Natasha),\n(Natasha, granddaughter, Betty)]}]\n<PROOF>\nsince Florence is a sister of Gregorio, and Gregorio is a grandson to Betty,\nthen Florence is a granddaughter to Betty.\nsince Gregorio is a brother of Natasha, and Natasha is the granddaughter of Betty,\nthen Gregorio is a grandson of Betty.\nanswer granddaughter <ANSWER> Florence is the granddaughter of Betty\nTable 1: CLUTRR example of level 3 (ie: 4 entities, 3 relations, 2 proof steps). The proof follows the\nshort-proof-rev strategy. We refer the reader to Figure 1 to visualize the corresponding graph in which solid\nlines refer to the facts given in the story and dotted lines refer to the new facts inferred in each proof step.\n(Lake, 2019; Baroni, 2020) and CFQ (Keysers et al., 2020) which measures the systematicity of\nlanguage understanding via a question answering setup. Sinha et al. (2019) propose a series of\nbaseline models with the CLUTRR dataset but none of them took advantage of the provided proof\nattached with each example. In addition, their Transformer baselines were not ﬁne-tuned on the task.\nUnlike them, we focus on learning and generating proofs for studying systematic generalization.\nNeural proof generation (Sekiyama et al., 2017) and neural theorem proving (Rocktäschel and Riedel,\n2017; Weber et al., 2019; Minervini et al., 2020) have been explored in previous work. They tend to\ncombine symbolic and statistical approaches to leverage the compositionality and interpretability of\nsymbolic systems and the ﬂexibility of statistical systems. Nevertheless, these combined systems all\nassume some predeﬁned set of atoms and rules making up the environment. We instead use natural\nlanguage text to deﬁne our environment and measure the limits of a purely statistical approach.\nSimilarly to us, Clark et al. (2020) leverage logical rules expressed in natural language to answer\ncompositional questions. However their system is not generative, rather they predict a true/false\nbinary label on candidate answers. We instead focus on the systematic generalization capacity of\ngenerating proofs and using them to generate the ﬁnal answer.\n3 Evaluating systematic generalization through interpretable reasoning\n3.1 The task\nBackground. We use the family relation CLUTRR benchmark suite (Sinha et al., 2019) to generate\nour dataset1. Each example is composed of: (i) a family graph G= (V,E) (referred as story) with\nentities as nodes ( v ∈V) and relationships as edges ( e ∈E), (ii) a query about the relationship\nbetween two entities (v1, _, vn) separated by more than one hop in the family graph (iii) a reasoning\npath (referred as proof) expressed as a list of (vi,ej,vk) tuples, referred to as facts and (iv) the target\nrelationship e∗ between the two queried entities (referred to as the answer). The dataset contains\n272 distinct entities and 20 relationship types, ordering to ∼1.5M possible facts. Each (vi,ej,vk)\nfact can be expressed in natural language using either one of 5 factual sentences (referred to as\nfacts template), or by using one of 6,000 noisy but more natural sentences written by mechanical\nturkers (refered as amt template). Family graphs are expressed using either the facts template or\nthe amt template, while queries, proofs and answers are always expressed with the facts template.\nA CLUTRR example can be seen in Table 1 and Figure 1.\nTerminology. In order to evaluate systematic generalization, we deﬁne the following building blocks\nthat constitute a proof:\n• entity: one node (e.g., “Anna”).\n• relation: one edge (e.g., “mother”).\n• fact: one factual sentence representing a (vi,ej,vk) tuple using facts template (e.g., “Anna is\nthe mother of Bob”).\n1Dataset and code can be downloaded at https://github.com/NicolasAG/SGinPG\n3\nsp\nsince Gregorio is a brother of Natasha, and Natasha is the granddaughter of Betty,\nthen Gregorio is a grandson of Betty.\nsince Florence is a sister of Gregorio, and Gregorio is a grandson to Betty,\nthen Florence is a granddaughter to Betty.\nspr\nsince Florence is a sister of Gregorio, and Gregorio is a grandson to Betty,\nthen Florence is a granddaughter to Betty.\nsince Gregorio is a brother of Natasha, and Natasha is the granddaughter of Betty,\nthen Gregorio is a grandson of Betty.\nlp\nsince Gregorio is the brother of Natasha, and Natasha is the granddaughter of Betty,\nthen Gregorio is the grandson of Betty.\nsince Florence is the sister of Gregorio, and Gregorio is the brother of Natasha,\nthen Florence is the sister of Natasha.\nsince Florence is the sister of Natasha, and Natasha is the granddaughter of Betty,\nthen Florence is the granddaughter of Betty.\nlpr\nsince Florence is the sister of Natasha, and Natasha is the granddaughter of Betty,\nthen Florence is the granddaughter of Betty.\nsince Florence is the sister of Gregorio, and Gregorio is the brother of Natasha,\nthen Florence is the sister of Natasha.\nsince Gregorio is the brother of Natasha, and Natasha is the granddaughter of Betty,\nthen Gregorio is the grandson of Betty.\nTable 2: Proof resolution types for an example of level 3. We refer the reader to Figure 1 for the\nkinship graph corresponding to this example. sp=short-proof, spr=short-proof-reversed, lp=long-\nproof, lpr=long-proof-reversed.\n• proof_step: one inference step combining two facts to get a new one (e.g., “since Anna is the\nmother of Bob and Bob is the brother of Carl then Anna is the mother of Carl. ”).\n• proof: the entire resolution chain, consisting of multiple ordered proof_steps.\nFollowing the setup of CLUTRR, we deﬁne the relative difﬁculty of individual examples according to\nthe number of edges present in the family graph. For instance, Table 1 and Figure 1 show a level-3\nexample because there are 3 solid edges (known facts) between 4 entities. In general, a level ktask\nconsists ofk edges (corresponding to k sentences in the story) between k+ 1nodes andk−1\nhidden edges to infer(corresponding to k−1 proof steps to solve the task). As the levels increase,\nso does the number of sentences in the story and the number of proof steps in the proof.\nProblem Setup. We trigger a model to: (1) given a story and query, generate a proof followed by\nan answer, and (2) given a story, query, and a proof, generate an answer. In particular, we train\na Transformer-based decoder (Liu et al., 2018) with the language modeling objective on entire\nsequences of “<STORY> [story] <QUERY> [query] <PROOF> [proof] <ANSWER> [answer]”:\nL(θ) =\n∑\ni\nlog P(wi|w1,...,w i−1; θ)\nThis setup enables to generate both the answer to a query and the proof to arrive at this answer, given\nas input the family graph story and a question. Concretely, we inject sequences of the story and query\nhaving delimiters “<STORY>” and “<QUERY>” to the language model and trigger it to generate the\ncorresponding proof and answer with tokens “<PROOF>” and “<ANSWER>” respectively.\n3.2 Proof resolution strategies\nIn our task, we turn language models into approximate proof generators. Speciﬁcally, we train\nTLMs to generate proofs (as deﬁned in Section 3.1). We do not explicitly perform inference on the\ngenerated proofs, but reformulate the language generation objective to generate the inferred answer\nafter the proof sequence. This allows to leverage TLMs to generate forward and backward chaining\nresolution paths used in Inductive Logic Programming (ILP) (Evans and Grefenstette, 2018). In our\ncase, these resolution paths are expressed in natural language. To simulate approximate theorem\ngeneration, we introduce four different types of proof that can be used to derive the answer given a\nstory and query. An example of each type can be seen in Table 2 and we describe them below:\n4\nANON TEST lvl.2 lvl.3 lvl.4 lvl.5 lvl.6 lvl.7 lvl.8 lvl.9 lvl.10\nproofs\n(many proof steps) 16.28% 0% 0% 0% 0% 0% 0% 0% 0%\nproof steps\n(“since A-r1-B\nand B-r2-C\nthen A-r3-C”)\n73.08% 58.06% 52.75% 54.28% 50.93% 59.04% 56.92% 53.55% 52.17%\nfacts(A-r-B) 100% 100% 100% 100% 100% 100% 100% 100% 100%\nentities(A) 100% 100% 100% 100% 100% 100% 100% 100% 100%\nrelations(r) 100% 100% 100% 100% 100% 100% 100% 100% 100%\nTable 3: Percentage of the test proof’s building blocks also present in the training set (composed of levels 2,\n4, 6) for all levels. We colored all cells with a value of 100% to better visualize which building blocks were\nentirely contained in the training set.\nshort-proof-rev (spr). This setup is the backward-chaining resolution path provided by the CLUTRR\ndataset, which is generated by recursive application of the kinship logical rules. This proof strategy\ncan be viewed as an explain-why scenario, where the ﬁrst sentence in the proof contains the answer\n(target relationship) and the subsequent sentences contain the intermediate steps required to explain\nthat answer. We refer the reader to Sinha et al. (2019) for further details on the generation of this\nproof setting.\nshort-proof (sp). Here we reverse the resolution chain provided by the CLUTRR dataset by swapping\nall sentences from the short-proof-rev setup. Doing so, we arrive at a forward-chaining inference\npath, in which the ﬁnal proof step consists of the target relationship. Speciﬁcally, the ﬁrst sentence in\nthe proof combines two facts from the given story to infer a new fact. In subsequent proof steps, the\ninferred fact from the previous step is combined with a fact from the story, to infer a new fact until\nthe answer is found.\nlong-proof (lp). Forward-chaining inference in ILP consists of generating all possible new facts\nfrom the starting facts, and evaluate each of them for the resolution of the target answer (Russell\nand Norvig, 2010). Similarly, in this setup, we extend the short-proof setup where we attempt to\ninfer all possible facts given the ones present in the input story. Each proof step combines any two\npreviously known facts to infer a new fact until the answer is found. Pseudo-code for generating this\ntype of proof can be found in Appendix 6.2.\nlong-proof-rev (lpr). This setting is the same as the previous one, but in reverse. It starts from\nthe answer and goes back to the facts originally given in the story. This resolution strategy can be\nviewed as a backward-chaining strategy where all possible paths are considered. This proof strategy\nis obtained by swapping all sentences from the long-proof setting.\nWe compare each strategy in our experiments to understand which form of logical resolution is\neasier to learn for TLMs. In particular, we note that the reversed proof strategies ( spr and lpr)\nfall in the backward-chaining family of logical resolution, while the non-reversed strategies (sp and\nlp) represent the forward-chaining resolutions. Backward-chaining family features the proof step\ncontaining the answer at the beginning of the proof. On the other hand, forward-chaining type proofs\n(sp and lp) feature the proof step containing the answer at the end of the proof.\n3.3 Systematic generalization in proof generation\nNow that we have deﬁned the task and various proof generation strategies available in our setup, we\nproceed to deﬁne the aspects of generalization we aim to test. Our initial CLUTRR formulation tested\nthe generalization capacity of a model to new facts hence new proof_steps and new proofs,\nafter being trained on all entities and relations. Initial experiments on this setup showed that\nTLMs fail to generalize to unseen facts. Due to the presence of a large number of entities in\nCLUTRR, we ended up with combinatorially large number of possible facts. The model may thus\nnot be able to learn how to represent each entity effectively, hence reducing its chances to learn\nhigher-order structures such as unseen facts. Experimental results on this original setting are\nprovided in Appendix 6.1.\nWe instead slightly simplify the generalization evaluation and allow the model to also be exposed to\nall possible facts. This formulation tests a model capacity to generalize to newproof_steps hence\nnew proofs, after being trained on allentities, relations and facts. Since providing a training\ncorpus covering all possible facts would signiﬁcantly increase the training data, we instead reduce\n5\nthe number of entities by replacing them by one of k2 randomly sampled entity tokens, resulting in\nsigniﬁcantly fewer possible facts, and thus all facts being contained in the training set (Table 3).\nInterpolation and Extrapolation. Having access to the level of difﬁculty of each test examples,\nwe evaluate both how Transformers can generalize to unseen proofs of the same difﬁculty as seen\nduring training (inductive generalization); and how they can generalize to unseen proofs of unseen\ndifﬁculty levels. In particular, we test interpolation in which the testing difﬁculty levels less than\ntraining levels; and extrapolation in which the test difﬁculty levels are higher than training levels.\nThis systematically tests the length generalization capabilities of TLMs in logical theorem proving.\n4 Experiments and Analysis\nWe aim to answer the following questions to analyze the proof generation capabilities of Transformer-\nbased language models (TLMs):\n1. Are TLMs able to reason better after being trained to generate interpretable proofs expressed in\nnatural language?\n2. Which types of proof are easier to learn and to generate for TLMs?\n3. Which types of proof are more useful for TLMs to generate accurate answers?\nSetup. In all our experiments we used a Transformer decoder architecture (Liu et al., 2018) with\n2.5M and 3.5M parameters with a vocabulary size of 90 and 1,800 tokens for stories expressed\nwith the facts and amt template respectively. Detailed parameter settings for our models are given\nin Appendix 6.3. We also ran preliminary experiments with a larger model ( 145M parameters)\n(Appendix 6.4), with a GPT2 model (Radford et al., 2019) (Appendix 6.5), and with a more complex\nnetwork (an encoder-decoder transformer) (Appendix 6.6) but found similar conclusions or further\ninvestigation being required. We generate 390,000 CLUTRR examples of level 2 to 10. We train\nthe models on 300,000 examples of levels 2,4 and 6 and evaluate the model on a test set of 10,000\nexamples for all levels from 2 to 10. Speciﬁcally, we test levels 3 and 5 for interpolation, levels 2,4\nand 6 for inductive generalization and levels 7,8,9 and 10 for extrapolation.\nEvaluation Metrics. In the following experiments, we evaluate both the generated proof factual\nconsistency (that we denote ‘validity’ in the rest of this document) andanswer accuracy. The answer\nis deﬁned as the ﬁrst sentence after the “<ANSWER>” tag in the generated sequence. Since all answers\nduring training were expressed using the facts template, we inverse this template to extract the\n(entity,relation,entity ) triple from the generated answer. If the extraction fails, we consider the\ngenerated answer wrong. We then compare the extracted triple to the ground truth provided in the\nCLUTRR dataset. For comparison, in all experiments, we also report the accuracy of the naive\nmost-frequent-relation (MFR) baseline consisting of predicting the relation that is the most frequent\nin the training set for the queried entity pair.\nA proof is deﬁned as the ordered sequence of all sentences generated between the “<PROOF>” and\n“<ANSWER>” tokens. For validating a proof, since all proofs during training were expressed using\nthe facts template, we inverse this template to extract all(entity,relation,entity ) triples from the\ngenerated proof sentences. If the extraction process fails at any point, the entire proof is considered\ninvalid. The ordered sequence of each proof step is then evaluated against the transitivity rules deﬁned\nby the CLUTRR environment. In addition, we also check that all the facts necessary for the proof are\neither given in the input story, or inferred from a previous proof step. If any of these conditions fail,\nwe consider the proof invalid, otherwise we consider the proof ‘valid’ (ie: factually consistent).\nNo proof setup. In addition to the four proof strategies deﬁned in Section 3.2, we also compare in all\nour experiments with a model that is trained to directly generate the answer after the story and query.\nIn particular, this no-proof model is trained on sequences of “<STORY> [story] <QUERY> [query]\n<PROOF> none . <ANSWER> [answer]”. This allows us to estimate how important is the proof for our\nmodels to be able to generalize.\n4.1 Answer Accuracy\nWe evaluate the answer accuracy of models trained with different proof settings on the test set\ndescribed earlier by Table 3. Each model is given as input a story, query and the proof trigger token\n2k = 20in our case because we know that the maximum number of entities in a story is less than 20.\n6\n20.559%\n15.843%\n81.554%\n38.876%\n9.131%\n4.965% 2.505%\n89.140%\n54.253%\n20.493%\n7.201%\n85.578% 87.654% 89.952%\n55.780%\n25.327%\n14.530%\n8.524%\n86.543% 88.388%\n82.593%\n39.035%\n17.142%\n4.767%\n75.207%\n48.845%\n20.422%\n8.275% 7.220% 8.115% 8.830% 10.728% 8.313% 7.410% 7.898%\n0.000%\n25.000%\n50.000%\n75.000%\n100.000%\n2 4 6 8 10\nsp\nspr\nlp\nlpr\nnp\nmfr\n(a) stories expressed with the facts template.\n0.000%\n20.000%\n40.000%\n60.000%\n80.000%\n2 4 6 8 10\namt / sp\namt / spr\namt / lp\namt / lpr\namt / np\nmfr (b) stories expressed with the amt template.\nFigure 2: Answer accuracy for all test levels from 2 to 10. The models are given as input “<STORY>\n[story] <QUERY> [query] <PROOF>” and they generate the proof and answer. The models are\ntrained on levels 2, 4, 6 only. Different proof settings are evaluated: sp=short-proof, spr=short-\nproof-reversed, lp=long-proof, lpr=long-proof-reversed, np=no-proof. We also report the naive\nmost-frequent-relation (mfr) baseline.\n(“<STORY> [story] <QUERY> [query] <PROOF>”), and we let them decode the next tokens, that is,\nthe proof followed by the answer.\nQ: Are TLMs able to generalize to unseen proof steps? A: For simple language, yes in interpolation\nand no in extrapolation. For complex language, no in both cases.\nIn Figure 2a we evaluate models trained wit stories expressed with the facts template. We observe\nthat in all proof setups, with the exception of short-proofs, TLMs are able to systematically generalize\nto predict the correct answer inferred from unseen proof_steps and proofs, both in inductive\n(levels 2, 4, 6) and interpolation levels (levels 3 and 5). However, in all proof setups TLMs have\ndifﬁculties to extrapolate to longer problems requiring a larger number of reasoning steps, conforming\nto length generalization issues discovered in related tasks (Lake, 2019).\nIn Figure 2b we note that models trained on noisy amt stories fail to systematically generalize to\npredict the correct answer. In addition, we can see a linear decrease in accuracy with the level of\ndifﬁculty. Having to de-noise the input stories to extract relevant kinship relations, in addition to\nrunning logical inference, makes the task much more challenging for our network. We conjecture\nthat generalizing in this harder setting may require additional capacity added to the model, either in\nterms of model size, model architecture, training data, or a combination of all the above. For instance,\nwe explore the beneﬁt of ﬁne-tuning GPT2 (Radford et al., 2019) in Section 6.5 as an initial step, but\nleave room for further improvement in future work.\nQ: Which reasoning strategy generalizes better? A: Backward-chaining is better than forward-\nchaining, but no-proof can be better than both. Long-proofs are better than short-proofs.\nWe observe that backward proof strategies (spr, lpr) better help the model to answer accurately than\ntheir respective forward strategies (sp, lp) (Figure 2), with the exception of long proofs in the amt\nstory template. This suggests that backward chaining is easier to learn, easier to use, or both, than\nforward chaining for TLMs. We believe this effect is due to the position-dependent exploitation of\nTLMs. Indeed, the answer is in the ﬁrst generated proof-step in case of backward-chaining proofs. In\naddition, we note in Figure 2 that long-proofs (lp, lpr) yield better generalization performance than\nshort-proofs (sp, spr) with the exception of reversed strategies in the facts story template.\nIt is also interesting to see that models trained to go directly to the answer by generating the “none”\ntoken as a proof tend to perform better than all other models required to generate the proof in facts\nstories (Figure 2a). One hypothesis is that the generated proof may be invalid most of the time and\nhence the extra information given by the proof is actually deteriorating the model’s performance. To\nsee if that may be the case, we next look at the validity of the generated proofs for all models (except\nthe trivial no-proof).\n4.2 Proof Validity\nWe evaluate the proof validity of models trained with different proof settings on the test set (previously\ndescribed by Table 3) in Figure 3. Similarly as above, each model is given as input a story and query\n7\n51.263%\n37.606%\n76.912%\n8.188%\n15.491% 13.447%\n7.100%\n1.383%\n20.472%\n87.415%\n1.839% 2.010% 1.478% 0.656%\n65.039%\n74.719%\n77.913%\n27.381%\n13.311%\n5.839% 2.737%\n33.907%\n39.843%\n66.488%\n9.206% 6.553%\n3.310% 2.081%\n0.000%\n25.000%\n50.000%\n75.000%\n100.000%\n2 4 6 8 10\nsp\nspr\nlp\nlpr\n(a) stories expressed with the facts template.\n0.000%\n20.000%\n40.000%\n60.000%\n80.000%\n2 4 6 8 10\namt / sp\namt / spr\namt / lp\namt / lpr (b) stories expressed with the amt template.\nFigure 3: Proof validity for all test levels from 2 to 10. The models are given as input “ <STORY>\n[story] <QUERY> [query] <PROOF>” and they generate the proof and answer. The models are trained\non levels 2, 4, 6 only. Different proof settings are evaluated: sp=short-proof, spr=short-proof-\nreversed, lp=long-proof, lpr=long-proof-reversed, np=no-proof.\nand we trigger the model to decode the proof and answer with the trigger tokens “<PROOF>” and\n“<ANSWER>” respectively.\nQ: Which reasoning strategy is easier to generate? A: forward-chaining is easier than backward-\nchaining and long-proofs are easier than short-proofs.\nFrom Figure 3a we observe that forward-chaining strategies (sp, lp) tend to be easier to generate\nthan their respective reversed strategies (spr, lpr). This is contrary to the previous observation where\nbackward-chaining strategies were easier for the models to understand. We believe that this is due\nto the fact that the model has a higher chance of generating the ﬁrst proof step correctly than the\nﬁnal proof step. Since backward chaining proofs contain the answer in the ﬁrst proof step, when\nre-using that information to predict the answer, there is a higher chance that the answer will be correct.\nThis explains why the answer accuracy of such model is relatively high while their proof validity is\nrelatively low.\nIn addition, we observe that in both facts and amt stories (Figure 3), long proof strategies are easier\nto generate than shorter ones. This was not expected at ﬁrst since long sequences are usually harder\nto model in language models. One hypothesis is that since long-proofs come from a systematic\nconstruction (see Appendix 6.2) they are easier to generate than the more arbitrary short proofs.\nQ: Are TLMs able to generate valid proofs of unseen lengths? A: No.\nWe observe that valid proofs are difﬁcult to generate for TLMs in unseen difﬁculty levels, both in\ninterpolation and extrapolation setting (Figure 3a). This partially explains why the no-proof setting in\nthe previous section yielded better generalization performances. In addition, we note in Figure3b that\nthe generated proofs from models trained on noisy amt stories are mostly invalid. We believe that\nthis is due to the fact that models need to de-noise the information from the input story in addition to\ngenerating a valid proof, making the task much harder. To understand if models rely on the validity\nof the proof, we next evaluate their answer accuracy when given the real proof as input rather than\nthe generated one.\n4.3 Proof is given\nTo understand if models rely on the proof, we again evaluate the answer accuracy as in Section 4.1,\nbut this time the models are given as input the story, the query and the real proof followed by the\nanswer trigger token: “<STORY> [story] <QUERY> [query] <PROOF> [proof] <ANSWER>”. We then\nlet the language model decode the next tokens making up the answer. Note that the no-proof model is\ngiven “none” as its “[proof]” so we don’t expect this model performance to change from Section 4.1.\nQ: Are ground-truth proofs useful for TLMs to generalize systematically? A: Yes.\nWhen the proof is provided in the input, all models outperform the no-proof model in inductive\nand interpolation test cases (Figure 4). In extrapolation test cases, models trained on facts stories\n(Figure 4a) beneﬁt from the proof compared to Section 4.1, and models trained with amt stories\noutperform the no-proof model (Figure 4b). This suggests that models do learn to use the correct\n8\n7.460%\n0.023% 0.000% 0.000%\n88.035% 88.082%\n42.726%\n18.630%\n3.737%\n59.886%\n29.049%\n15.180%\n86.655%\n52.238%\n10.504%\n75.230%\n48.990%\n20.129%\n8.275% 7.220% 8.115% 8.830% 10.728% 8.313% 7.410% 7.898%\n0.000%\n25.000%\n50.000%\n75.000%\n100.000%\n2 4 6 8 10\nsp\nspr\nlp\nlpr\nnp\nmfr\n(a) stories expressed with the facts template.\n0.000%\n25.000%\n50.000%\n75.000%\n100.000%\n2 4 6 8 10\namt / sp\namt/ spr\namt / lp\namt / lpr\namt / np\nmfr (b) stories expressed with the amt template.\nFigure 4: Answer accuracy for all levels from 2 to 10. The models are given as input “ <STORY>\n[story] <QUERY> [query] <PROOF> [proof] <ANSWER>” and they generate the answer. The models\nare trained on levels 2, 4, 6 only. Different proof settings are evaluated: sp=short-proof, spr=short-\nproof-reversed, lp=long-proof, lpr=long-proof-reversed, np=no-proof. We also report the naive\nmost-frequent-relation (mfr) baseline.\nproof to better generalize during inference. However, as the difﬁculty of the examples increases,\nthe generalization performance of all models decreases. Even when given the proof containing the\ncorrect answer, TLMs fail to copy the correct information from sequences of greater length than\nseen during training. Our hypothesis for this is that Transformers strongly rely on the position of the\nanswer and have trouble learning simple tasks – such as copying the answer from the proof – if the\ninformation for this task happens at unseen positions.\nQ: Which reasoning strategy is easier to use when generating answers? A: backward-chaining is\neasier to use than forward-chaining and long-proofs are easier to use than short-proofs.\nAnother interesting observation is that, in general, the reversed proofs (dotted lines in Figure 4) tend\nto be more useful than forward strategies for our model in generating the correct answer, aligning\nwith our ﬁndings in Section 4.1. Similarly as above, we believe that this is due to the facts that\nTransformers strongly rely on the position of the answer. Indeed, in reversed proofs (spr, lpr), the\nanswer is always in the ﬁrst proof step, for which the position depends only on the story length;\nwhereas in sp and lp the answer is always in the last proof step, for which the position depends both\non the story length and on the proof length.\nWe also see that long, exhaustive proofs are easier to be used when generating the ﬁnal answer,\ncompared to short-proof strategies. This suggests that while being a longer sequence of tokens to\nencode, if a model was able to generate such proofs, it would ease its generalization capacities.\n5 Conclusion\nTLMs are state of the art models for a wide variety of natural language processing tasks. Given\ntheir widespread use, it is important to understand the limits of their ability to reason on knowledge\nexpressed in natural language and to extrapolate learned inference procedures to unseen problem\ninstances. Our explorations reveal multiple insights. Firstly, TLMs suffer from length-generalization\nissues in generating proofs. Secondly, TLMs get better at reasoning when trained with longer,\nexhaustive proofs. In addition, the fact that backward-chaining proof models perform better than\nforward-chaining ones makes us believe that backward-chaining strategies are easier to use albeit\nbeing harder to generate. Moreover, we ﬁnd that no-proof models perform better than those trained to\nproduce proofs. We conjecture that beneﬁting from naturally stated logical proof statements requires\nmore complex internal representations. Recent work on developing position-agnostic attention\nmechanisms for Transformers (Dubois et al., 2020) can be useful as a future direction to develop\ngeneralizable models. Furthermore, our results motivates the use of neuro-symbolic methods such\nas Neural Theorem Provers (Rocktäschel and Riedel, 2017) as an alternative avenue to achieving\nsystems that systematically generalize on logical and compositional reasoning tasks. Combining\nthese approaches with large pre-trained language models is left as future research. We hope that this\nwork will inspire research on the systematic generalization capacity of language models and motivate\nfurther study and the creation of neural models with greater reasoning capacity.\n9\nBroader Impact\nTransformer based models have been very effective for various language understanding and generation\ntasks. Due to their recent successes, there is signiﬁcant interest in the applications of these models to\nreal world scenarios such as: Dialogue, Question Answering and text-classiﬁcation. However, failure\nof such systems could produce nonsensical, wrong or racially-biased results (Henderson et al., 2018).\nTherefore, a logical analysis of their limitations and issues in generalization to unseen data, such as\nin this work, could have a positive impact on building safer, more robust and interpretable systems in\nthese domains.\nIn this work, we rely on systematic tests to trigger Transformer-based models to generate an inter-\npretable proof in natural language, and then evaluate the robustness properties of that proof. Using a\nﬁrst-order logic based dataset, we explicitly test the logical consistency of such proof. This research\ncan shed some light into developing more robust and systematic models in the future. In addition,\nit can help us understand the reasoning strategies employed by Transformer-based models for both\ninference and generation. However, the fact that proof-free inference works so well, may also imply\nthat models which generate proofs, do so in a decoupled way from the computations yielding the\nﬁnal answer. This negative result could give users a false sense of explainability.\nAcknowledgments and Disclosure of Funding\nThe authors would like to acknowledge support from Element AI for providing computational\nresources which were used to run the experiments in this work. We also acknowledge the help\nprovided by Sandeep Subramanian in sharing part of his experimental code. Nicolas is partially\nfunded by a scholarship from the Fonds de Recherche Quebec Nature et Technologie. We thank\nCIFAR for their support through the CIFAR AI Chairs program. We also thank NSERC and PROMPT\nfor their support.\nReferences\nDzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and\nAaron Courville. 2019a. Systematic generalization: What is required and can it be learned? In\nProceedings of the 2019 International Conference on Learning Representations.\nDzmitry Bahdanau, Harm de Vries, Timothy J O’Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua\nBengio, and Aaron Courville. 2019b. CLOSURE: Assessing systematic generalization of CLEVR\nmodels. arXiv preprint arXiv:1912.05783.\nMarco Baroni. 2020. Linguistic generalization and compositionality in modern artiﬁcial neural\nnetworks. Philosophical Transactions of the Royal Society, 375(1791).\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165.\nNoam Chomsky. 1957. Logical structures in language. American Documentation, 8(4):284–291.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over lan-\nguage. In Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence,\nIJCAI-20, pages 3882–3890. International Joint Conferences on Artiﬁcial Intelligence Organization.\nMain track.\nIshita Dasgupta, Demi Guo, Samuel J. Gershman, and Noah D. Goodman. 2019. Analyzing machine-\nlearned representations: A natural language case study. arXiv preprint arXiv:1909.05885.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. InProceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics, pages 4171–4186,\nMinneapolis, Minnesota. Association for Computational Linguistics.\n10\nYann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. 2020. Location Attention for\nExtrapolation to Longer Sequences. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 403–413, Online. Association for Computational Linguistics.\nRichard Evans and Edward Grefenstette. 2018. Learning explanatory rules from noisy data (extended\nabstract). In Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-18, pages 5598–5602. International Joint Conferences on Artiﬁcial Intelligence\nOrganization.\nYoav Goldberg. 2019. Assessing bert’s syntactic abilities.arXiv preprint arXiv:1901.05287.\nEmily Goodwin, Koustuv Sinha, and Timothy J. O’Donnell. 2020. Probing linguistic systematicity.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\n1958–1969, Online. Association for Computational Linguistics.\nPeter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried,\nRyan Lowe, and Joelle Pineau. 2018. Ethical challenges in data-driven dialogue systems. In\nProceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 123–129.\nDaniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz Staﬁniak, Tibor Tihon, Dmitry Tsarkov, Xiao\nWang, Marc van Zee, and Olivier Bousquet. 2020. Measuring compositional generalization: A\ncomprehensive method on realistic data. In Proceedings of the 2020 International Conference on\nLearning Representations.\nBrenden M. Lake. 2019. Compositional generalization through meta sequence-to-sequence learning.\nIn Advances in Neural Information Processing Systems 32, pages 9791–9801. Curran Associates,\nInc.\nBrenden M. Lake and Marco Baroni. 2018. Generalization without systematicity: On the composi-\ntional skills of sequence-to-sequence recurrent networks. In ICML, pages 2879–2888.\nAdam Liška, Germán Kruszewski, and Marco Baroni. 2018. Memorize or generalize? searching for\na compositional rnn in a haystack. arXiv preprint arXiv:1802.06467.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summarizing long sequences. In Proceedings of the 2018\nInternational Conference on Learning Representations.\nPasquale Minervini, Matko Bosnjak, Tim Rocktäschel, Sebastian Riedel, and Edward Grefenstette.\n2020. Differentiable reasoning on large knowledge bases and natural language. In Proceedings\nof the Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, volume 34 no.4: AAAI Technical\nTracks Machine Learning, pages 5182–5190.\nRichard Montague. 1970. Universal grammar. Theoria, 36(3):373–398.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018. Dissecting contextual\nword embeddings: Architecture and representation. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, pages 1499–1509, Brussels, Belgium.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong,\nChina. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1(8).\n11\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. Journal of Machine Learning Research, 21(1).\nTim Rocktäschel and Sebastian Riedel. 2017. End-to-end differentiable proving. In Advances in\nNeural Information Processing Systems 30, pages 3788–3800. Curran Associates, Inc.\nStuart Russell and Peter Norvig. 2010. Artiﬁcial intelligence: a modern approach , third edition.\nPearson.\nTaro Sekiyama, Akifumi Imanishi, and Kohei Suenaga. 2017. Towards proof synthesis guided by\nneural machine translation for intuitionistic propositional logic. arXiv preprint arXiv:1706.06462.\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR:\nA diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) , pages 4506–4515, Hong Kong, China.\nAssociation for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? probing for sentence structure in contextualized word representations. In\nProceedings of the 2019 International Conference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\nProcessing Systems 30, pages 5998–6008. Curran Associates, Inc.\nLeon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, and Tim Rocktäschel. 2019.\nNLProlog: Reasoning with weak uniﬁcation for question answering in natural language. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n6151–6161, Florence, Italy. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv, abs/1910.03771.\n12\n6 Supplementary Material\n6.1 Original CLUTRR evaluation\nORIGINAL TEST lvl.2 lvl.3 lvl.4 lvl.5 lvl.6 lvl.7 lvl.8 lvl.9 lvl.10\nproofs\n(many proof steps) 99.62% 0% 0% 0% 0% 0% 0% 0% 0%\nproof steps\n(“since A-r1-B\nand B-r2-C\nthen A-r3-C”)\n99.62% 0% 99.96% 0% 100% 0% 0% 0% 0%\nfacts(A-r-B) 100% 0.47% 100% 0.83% 100% 0.20% 0.20% 0.10% 0.42%\nentities(A) 100% 23.81% 100% 35.72% 100% 26.19% 21.43% 30.95% 30.95%\nrelations(r) 100% 100% 100% 100% 100% 100% 100% 100% 100%\nTable 4: Percentage of the original testproof building blocks also present in the training set (composed of\nlevels 2, 4, 6) for all levels. We colored all cells with a value close to 100% to better visualize which building\nblocks were entirely contained in the training set.\nThe original CLUTRR data generation framework made sure that each test proof is not in the\ntraining set in order to test whether a model is able to generalize to unseen proofs. Initial results\non the original CLUTRR test sets resulted in strong model performance ( ∼99%) on levels seen\nduring training (2, 4, 6) but no generalization at all (∼0%) to other levels. After further analysis,\nwe noticed that due to the cloze style nature of CLUTRR tasks, the ﬁrst names representing entities\nwere chosen arbitrarily. This resulted in level-ktest set’sproof_steps and facts also being in the\nlevel-ktraining set. In addition, level-ktest set’sentities were mostly seen only in level-ktraining\nset. This resulted in a big overlap between training and test sets for examples of the same level, but a\nweak overlap on other levels as we can see in Table 4.\nNAMED TEST lvl.2 lvl.3 lvl.4 lvl.5 lvl.6 lvl.7 lvl.8 lvl.9 lvl.10\nproofs\n(many proof steps) 2.13% 0% 0% 0% 0% 0% 0% 0% 0%\nproof steps\n(“since A-r1-B\nand B-r2-C\nthen A-r3-C”)\n2.13% 0% 1.33% 1.74% 1.42% 1.80% 1.38% 0.99% 1.40%\nfacts (A-r-B) 15.48% 5.52% 6.77% 10.92% 6.38% 9.63% 10.51% 10.33% 8.33%\nentities(A) 100% 100% 100% 100% 100% 100% 100% 100% 100%\nrelations(r) 100% 100% 100% 100% 100% 100% 100% 100% 100%\nTable 5: Percentage of the Named testproof’s building blocks also present in the training set (composed of\nlevels 2, 4, 6) for all levels. We colored all cells with a value of 100% to better visualize which building blocks\nwere entirely contained in the training set\n13.669%\n3.988%\n10.689% 9.921%\n11.265%\n20.119%\n15.207%\n6.942%\n2.788%\n30.110%\n2.506%\n14.178%\n6.856%\n11.891%\n6.561%\n1.147% 0.281% 0.333%\n19.471%\n4.297%\n10.699%\n9.071% 8.796%\n10.410%\n1.374% 0.208% 0.364%\n9.059%\n5.749%\n8.999% 8.147% 8.898%\n13.056%\n11.743%\n9.669%\n14.564%\n0.000\n%\n10.000\n%\n20.000\n%\n30.000\n%\n40.000\n%\n2 4 6 8 10\nnp\nsp\nlp\nmfr\nFigure 5: Answer accuracy on the Named test for\nall levels from 2 to 10. The models are given\nas input “<STORY> [story] <QUERY> [query]\n<PROOF>” and asked to generate the proof and\nanswer. Models are trained on levels 2, 4, 6 only.\nDifferent proof settings are evaluated: sp=short-\nproof, lp=long-proof, np=no-proof. We also report\nthe naive most-frequent-relation (mfr) baseline.\nIn our case, the entity names are important to\nevaluate systematic generalization. We want\nto evaluate the capacity of a model to general-\nize to new facts, proof_steps, and proofs,\nbut keeping the entities and relations the\nsame. We thus modiﬁed the original CLUTRR\ndataset to select test entities according to entities\npresent in the training set. We devise a test set\nthat uses all relations and entities from\nthe training set but new facts, proof_steps\nand proofs for all levels. We call this dataset\nthe Named data: all entities are referred by their\noriginal ﬁrst name. Train and test overlap per-\ncentages between all building blocks are in Ta-\nble 5.\nGiven as input the story and the query followed\nby the proof trigger token (“<STORY> [story]\n<QUERY> [query] <PROOF>”) the model generated the corresponding proof ans answer. We report\n13\n33.752%\n1.760% 0.503% 0.115% 0.250% 0.833% 0.727% 0.114%\n2.394%0.865% 0.473% 0.084% 0.137% 0.000% 0.011% 0.000% 0.000%\n0.000%\n10.000%\n20.000%\n30.000%\n40.000%\n50.000%\n2 4 6 8 10\nsp\nlp\n(a) Proof validity on the Named test for all levels from\n2 to 10. The models are given as input “<STORY>\n[story] <QUERY> [query] <PROOF>” and asked to\ngenerate the proof and answer.\n72.147%\n9.041%\n59.648%\n31.412%\n58.716%\n0.106% 0.000% 0.000% 0.000%\n26.499%\n8.196%\n32.107%\n17.375%\n24.215% 22.553%\n4.634% 2.269% 0.970%\n13.991%\n4.098%\n10.659% 10.026% 11.311%\n20.212%\n14.685%\n6.911%\n2.848%\n9.059%\n5.749%\n8.999% 8.147% 8.898%\n13.056% 11.743% 9.669%\n14.564%\n0.000%\n20.000%\n40.000%\n60.000%\n80.000%\n2 4 6 8 10\nsp\nlp\nnp\nmfr\n(b) Answer accuracy on the Named test for all levels\nfrom 2 to 10. The models are given as input “<STORY>\n[story] <QUERY> [query] <PROOF> [proof] <AN-\nSWER>” and asked to generate the answer.\nFigure 6: Evaluation of models trained on levels 2, 4, 6 only.\nin Figure 5 the answer accuracy and in Figure 6a the proof validity of all our models. Similarly,\nin Figure 6b we report the answer accuracy of our models when they are given as input the story,\nthe query and the real proof, followed by the answer trigger token (“<STORY> [story] <QUERY>\n[query] <PROOF> [proof] <ANSWER>”).\nExperiments on this setup show that Transformer language models fail to generalize to unseenfacts.\nIndeed, due to the presence of a large number of entities in CLUTRR, we end up with combinatorially\nlarge number of possible facts. The model may thus not be able to learn how to represent each entity\neffectively, hence reducing its chances to learn higher-order structures such as unseenfacts.\n6.2 Long Proof pseudo-code\ndef get_long_proof(story_facts, rules, query):\n\"\"\"\n:params story_facts: list of (e_1, r, e_2) facts\n:params rules: list of composition rules. each rule is a dict\nof the form {r1--r2: r3}\n:params query: tuple of entities for which we must find a relation (src, tgt)\n\"\"\"\nproof = [] # list of proof steps to return\n# get all known relations (original, and reversed)\nall_facts = []\nfor (e1, r, e2) in story_facts:\ninv_r = reverse_fact(e1, r, e2)\nall_facts.append((e1, r, e2))\nall_facts.append((e2, inv_r, e1))\n# go through every possible pair of facts\nfor f1, f2 in itertools.combinations(all_facts, 2):\ne11, r1, e12 = f1\ne21, r2, e22 = f2\ninv_r1 = reverse_fact(e11, r1, e12)\ninv_r2 = reverse_fact(e21, r2, e22)\n# find the possible AB+BC combination.\n# there are 4 possible ways to combine 2 sentences with 2 entities each (1 in common):\nif e11 == e21 and e12 != e11 and e12 != e22:\n# AB+BC <=> inv_f1+f2\nA, new_r1, B = e12, inv_r1, e11\nB, new_r2, C = e21, r2, e22\ninv_r1 = r1\nelif e11 == e22 and e12 != e11 and e12 != e21:\n# AB+BC <=> f2+f1\n14\nA, new_r1, B = e21, r2, e22\nB, new_r2, C = e11, r1, e12\n# swap inv_r1 and inv_r2\ninv_r1, inv_r2 = inv_r2, inv_r1\nelif e12 == e21 and e11 != e12 and e11 != e22:\n# AB+BC <=> f1+f2\nA, new_r1, B = e11, r1, e12\nB, new_r2, C = e21, r2, e22\nelif e12 == e22 and e11 != e12 and e11 != e21:\n# AB+BC <=> f1+inv_f2\nA, new_r1, B = e11, r1, e12\nB, new_r2, C = e22, inv_r2, e21\ninv_r2 = r2\nelse:\n# invalid pair of facts\ncontinue\n# try to combine AB+BC\nif new_r1--new_r2 in rules:\nr3 = rules[new_r1--new_r2]\ninv_r3 = reverse_fact(A, r3, C)\nall_facts.append((A, r3, C))\nall_facts.append((C, inv_r3, A))\nproof.append(since A new_r1 B and B new_r2 C then A r3 C)\n# try to combine CB+BA\nelif inv_r2--inv_r1 in rules:\nr3 = rules[inv_r2--inv_r1]\ninv_r3 = reverse_fact(C, r3, A)\nall_facts.append((C, r3, A))\nall_facts.append((A, inv_r3, C))\nproof.append(since C inv_r2 B and B inv_r1 A then C r3 A)\nelse:\n# invalid pair of facts\ncontinue\n# check if we found the link between the two queried entities\n(A, r, B) = all_facts[-1]\nif A==query[0] and B==query[1]:\nbreak\nif A==query[1] and B==query[0]:\nbreak\nreturn proof\n6.3 Experiments parameter settings\nsmall large\npatience 20 20\nbatch size 512 256\nﬂoat precision 16 16\nembedding dimension 192 768\nnumber of layers 5 20\ndropout 0.1 0.1\ntransformer mlp hidden size 768 3072\nattention heads 3 12\nmax length 1,024 512\nactivation gelu gelu\nnumber of warmup steps 20,000 20 ,000\noptimizer adam adam\ntotal parameters ∼3,000,000 ∼145,000,000\nTable 6: Parameter settings.\nAll experiments in the main\nsection of the paper were\nrun with the small model\nsize.\nAdditional experiments in\nSection 6.4 were run with\nthe large model size.\n15\n6.4 More parameters\n0.000%\n25.000%\n50.000%\n75.000%\n100.000%\n2 4 6 8 10\n2.5M / np\n2.5M / spr\n145M / spr\nmfr\nFigure 7: Answer accuracy for all test levels from 2 to 10.\nThe models are given as input “<STORY> [story] <QUERY>\n[query] <PROOF>” and they generate the proof and answer.\nModels are trained on levels 2, 4, 6 only. Stories are ex-\npressed with the facts template. Different proof settings\nare evaluated: np=no-proof and spr=short-proof-reversed.\nWe also report the naive most-frequent-relation (mfr) base-\nline. Results on other proof settings with the 2.5M parameter\nnetwork can be found in Figure 2a.\nIn this section we report the answer\naccuracy of a model trained with\n∼145M parameters and compare its\ngeneralization performance with our\ninitial smaller network ( ∼2.5M pa-\nrameters). Models are trained on lev-\nels 2, 4 and 6. Each model is given the\nstory and query as input, and triggered\nto generate the proof and answer with\nthe “<PROOF>” and “<ANSWER>”\ntokens respectively.\nWe observe in Figure 7 that the\ngeneralization capacity of the larger\n145M network is almost identical to\nthe smaller 2.5M parameter network\ntrained on the same data (facts sto-\nries and short-proof-reversed). In ad-\ndition, we also observe that the 145M\nmodel trained on reversed short proofs\n(145M / spr) is not better than the\n2.5M model trained without any proof\n(2.5M / np). Overall, results show that model size improves only marginally the generalization\ncapacity in our task.\n6.5 Fine-tuning GPT2\n0.000%\n20.000%\n40.000%\n60.000%\n80.000%\n2 4 6 8 10\namt / gpt2FS-sp\namt / gpt2FT-sp\namt / gpt2FS-spr\namt / gpt2FT-spr\namt / gpt2FS-np\namt / gpt2FT-np\nFigure 8: Answer accuracy for all test levels from 2 to 10.\nThe models are given as input “<STORY> [story] <QUERY>\n[query] <PROOF>” and they generate the proof and answer.\nModels are ﬁne-tuned on levels 2, 4, 6 only. Stories are\nexpressed with the amt template. Different proof settings are\nevaluated: sp=short-proof, spr=short-proof-reversed, np=no-\nproof. We compare the performance of models trained from\nscratch (dotted lines; gtp2FS-) and of ﬁne-tuned models\n(solid lines; gpt2FT-).\nIn this section we report the an-\nswer accuracy of GPT2 models (Rad-\nford et al., 2019) trained from-scratch\n(gpt2FS-) on the CLUTRR dataset\nand of pre-trained GPT2 models ﬁne-\ntuned ( gpt2FT-) on the CLUTRR\ndataset. We leverage the GPT2 im-\nplementation from the huggingface\nlibrary (Wolf et al., 2019). The re-\nsulting models have ∼125M param-\neters. In all experiments the models\nare trained on stories expressed in the\namt template. Models are ﬁne-tuned\non levels 2, 4 and 6. Each model is\ngiven the story and query as input, and\ntriggered to generate the proof and an-\nswer with the “<PROOF>” and “<AN-\nSWER>” tokens respectively.\nIn Figure 8 we observe that in general,\nﬁne-tuned models perform better than\nthe ones trained from scratch. We can\nalso see that reversed-proof strategies are better than their forward proof counterpart, which is in\naccordance with what we discussed in Section 4.1. Although ﬁne-tuning seems to improve the\ngeneralization capacity of GPT2, it is also interesting to note that the beneﬁt of ﬁne-tuning GPT2 on\nshort-proofs (sp) is negligible compared to the beneﬁts of ﬁne-tuning GPT2 on short-proofs-reversed\n(spr) or no-proof (np). This suggests that ﬁne-tuning alone is not enough to yield strong generalization\nperformance, but the choice of proof strategy also inﬂuences greatly the answer accuracy.\n16\n6.6 Encoder-Decoder Network\n0.000%\n25.000%\n50.000%\n75.000%\n100.000%\n2 4 6 8 10\ns2s-sp\ns2s-lp\ns2s-spr\ns2s-lpr\ns2s-np\nmfr\nFigure 9: Answer accuracy for all test levels from 2 to 10.\nThe models encodes the input “<STORY> [story] <QUERY>\n[query]” and they decode the proof and answer. Models\nare trained on levels 2, 4, 6 only. Stories are expressed\nwith the facts template. Different proof settings are eval-\nuated: sp=short-proof, spr=short-proof-reversed, lp=long-\nproof, lpr=long-proof-reversed, np=no-proof. We also re-\nport the naive most-frequent-relation (mfr) baseline.\nIn this section we evaluate the answer\naccuracy of sequence-to-sequence\nmodels trained on facts templated\nstories of level 2, 4 and 6. These mod-\nels consist of a 5-layer Transformer\nencoder and a 5-layer Transformer\ndecoder, each of them following the\nsame parameter settings than what is\ndescribed in the ‘small’ column of Ta-\nble 6. This resulted in 5.22M param-\neter models. Sequence-to-sequence\nmodels are trained to encode the story\nand question with the encoder, and\ngenerate the proof and answer with\nthe decoder. Models trained on levels\n2, 4 and 6. Each model is given the\nstory and query as input, and triggered\nto generate the proof and answer with\nthe “<PROOF>” and “<ANSWER>”\ntokens respectively.\nIn the results shown in Figure 9, we see that sequence-to-sequence models do not generalize well to\nunseen difﬁculty levels, both in extrapolation settings (levels 7–10) but also in interpolation settings\n(levels 3 and 5). This suggests that encoder-decoder architectures are more sensible to the sequence\nlength seen during training. On the other hand, it is important to note that the encoder network was\ntrained with the auto-regressive language modeling objective back-propagated from the decoder. It\nwould be interesting to see if pre-training the encoder with a more traditional objective, that is masked\nlanguage modeling (Devlin et al., 2019), would improve the generalization performance. We leave\nthis exercise as future work. In addition, we plan to explore pre-trained models such as T5 (Raffel\net al., 2020) in future work in order to improve performance with this type of architecture.\n17",
  "topic": "Mathematical proof",
  "concepts": [
    {
      "name": "Mathematical proof",
      "score": 0.836719274520874
    },
    {
      "name": "Computer science",
      "score": 0.7216901779174805
    },
    {
      "name": "Backward chaining",
      "score": 0.6749370098114014
    },
    {
      "name": "Generalization",
      "score": 0.5570759773254395
    },
    {
      "name": "Chaining",
      "score": 0.5172629952430725
    },
    {
      "name": "Natural deduction",
      "score": 0.49090367555618286
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4869478940963745
    },
    {
      "name": "Inference",
      "score": 0.4631148874759674
    },
    {
      "name": "Automated theorem proving",
      "score": 0.4319899082183838
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3852088749408722
    },
    {
      "name": "Programming language",
      "score": 0.3152178227901459
    },
    {
      "name": "Mathematics",
      "score": 0.1629369854927063
    },
    {
      "name": "Inference engine",
      "score": 0.1032160222530365
    },
    {
      "name": "Psychology",
      "score": 0.08800807595252991
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": []
}