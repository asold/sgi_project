{
    "title": "Evaluation Framework of Large Language Models in Medical Documentation: Development and Usability Study",
    "url": "https://openalex.org/W4402765248",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2117797494",
            "name": "Jun-Hyuk Seo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2343184567",
            "name": "Dasol Choi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096963195",
            "name": "Tae Rim Kim",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2257975888",
            "name": "Won Chul Cha",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2135810458",
            "name": "Min-Ha Kim",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2793791057",
            "name": "Haanju Yoo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3109391854",
            "name": "Namkee Oh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102652998",
            "name": "Yongjin Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2110655894",
            "name": "Kye-Hwa Lee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2183278384",
            "name": "Edward Choi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4380685958",
        "https://openalex.org/W4391995913",
        "https://openalex.org/W4385568225",
        "https://openalex.org/W4388525110",
        "https://openalex.org/W4366769280",
        "https://openalex.org/W4323350039",
        "https://openalex.org/W4353015365",
        "https://openalex.org/W4367186868",
        "https://openalex.org/W4323050332",
        "https://openalex.org/W4229506649",
        "https://openalex.org/W2799268915",
        "https://openalex.org/W4280500371",
        "https://openalex.org/W2139056371",
        "https://openalex.org/W2152845372",
        "https://openalex.org/W1992984099",
        "https://openalex.org/W2029780317"
    ],
    "abstract": "Background The advancement of large language models (LLMs) offers significant opportunities for health care, particularly in the generation of medical documentation. However, challenges related to ensuring the accuracy and reliability of LLM outputs, coupled with the absence of established quality standards, have raised concerns about their clinical application. Objective This study aimed to develop and validate an evaluation framework for assessing the accuracy and clinical applicability of LLM-generated emergency department (ED) records, aiming to enhance artificial intelligence integration in health care documentation. Methods We organized the Healthcare Prompt-a-thon, a competitive event designed to explore the capabilities of LLMs in generating accurate medical records. The event involved 52 participants who generated 33 initial ED records using HyperCLOVA X, a Korean-specialized LLM. We applied a dual evaluation approach. First, clinical evaluation: 4 medical professionals evaluated the records using a 5-point Likert scale across 5 criteria—appropriateness, accuracy, structure/format, conciseness, and clinical validity. Second, quantitative evaluation: We developed a framework to categorize and count errors in the LLM outputs, identifying 7 key error types. Statistical methods, including Pearson correlation and intraclass correlation coefficients (ICC), were used to assess consistency and agreement among evaluators. Results The clinical evaluation demonstrated strong interrater reliability, with ICC values ranging from 0.653 to 0.887 (P&lt;.001), and a test-retest reliability Pearson correlation coefficient of 0.776 (P&lt;.001). Quantitative analysis revealed that invalid generation errors were the most common, constituting 35.38% of total errors, while structural malformation errors had the most significant negative impact on the clinical evaluation score (Pearson r=–0.654; P&lt;.001). A strong negative correlation was found between the number of quantitative errors and clinical evaluation scores (Pearson r=–0.633; P&lt;.001), indicating that higher error rates corresponded to lower clinical acceptability. Conclusions Our research provides robust support for the reliability and clinical acceptability of the proposed evaluation framework. It underscores the framework’s potential to mitigate clinical burdens and foster the responsible integration of artificial intelligence technologies in health care, suggesting a promising direction for future research and practical applications in the field.",
    "full_text": null
}