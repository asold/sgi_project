{
  "title": "All You Need Is Context: Clinician Evaluations of various iterations of a Large Language Model-Based First Aid Decision Support Tool in Ghana",
  "url": "https://openalex.org/W4393948619",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5094357008",
      "name": "Paulina Boadiwaa Mensah",
      "affiliations": [
        "Aerospace Medical Association"
      ]
    },
    {
      "id": null,
      "name": "Nana Serwaa Quao",
      "affiliations": [
        "Korle Bu Teaching Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2944798381",
      "name": "Sesinam Dagadu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5113342833",
      "name": "James Kwabena Mensah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5106662541",
      "name": "Jude Domfeh Darkwah",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Project Genie Clinician Evaluation Group [1]",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5094357008",
      "name": "Paulina Boadiwaa Mensah",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Nana Serwaa Quao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4323835279",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4391259883",
    "https://openalex.org/W4389508559",
    "https://openalex.org/W4389989133",
    "https://openalex.org/W4389895211",
    "https://openalex.org/W4367672504",
    "https://openalex.org/W6810271737",
    "https://openalex.org/W4389650560",
    "https://openalex.org/W4200514127",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W4389217417",
    "https://openalex.org/W4389040134"
  ],
  "abstract": "Abstract As advancements in research and development expand the capabilities of Large Language Models (LLMs), there is a growing focus on their applications within the healthcare sector, driven by the large volume of data generated in healthcare. There are a few medicine-oriented evaluation datasets and benchmarks for assessing the performance of various LLMs in clinical scenarios; however, there is a paucity of information on the real-world usefulness of LLMs in context-specific scenarios in resource-constrained settings. In this work, 5 iterations of a decision support tool for medical emergencies using 5 distinct generalized LLMs were constructed, alongside a combination of Prompt Engineering and Retrieval Augmented Generation techniques. 50 responses were generated from the LLMs. Quantitative and qualitative evaluations of the LLM responses were provided by 13 physicians (general practitioners) with an average of 3 years of practice experience managing medical emergencies in resource-constrained settings in Ghana. Machine evaluations of the LLM responses were also computed and compared with the expert evaluations.",
  "full_text": "All You Need Is Context: Clinician Evaluations \nof various iterations of a Large Language Model-  \nBased First Aid Decision Support Tool in Ghana.   \nPaulina Boadiwaa Mensah \nSnooCODE Red Development Team \nSnooCODE   \nAccra, Ghana   \nORCID: 0000-0002-0570-5662 \n \nProject Genie Clinician \nEvaluation Group1  \nGhana \nprojectgenie314@gmail.com \nNana Serwaa Quao   \nAccident and Emergency Centre   \nKorle Bu Teaching Hospital  \nSnooCODE  \nAccra, Ghana   \nORCID: 0000-0001-8476-5999 \n \n \nSesinam Dagadu \nSnooCODE Red \nSnooCODE   \nAccra, Ghana   \ns.dagadu@snoocode.com  \n \n   \n \n    \nAbstract— As advancements in research and development \nexpand the capabilities of Large Language Models (LLMs), \nthere is a growing focus on their applications within the \nhealthcare sector, driven by the large volume of data generated \nin healthcare. There are a few medicine-oriented evaluation \ndatasets and benchmarks for assessing the performance of \nvarious LLMs in clinical scenarios; however, there is a paucity \nof information on the real -world usefulness of LLMs in \ncontextspecific scenarios in resource-constrained settings. In this \nwork, 5 iterations of a decision support tool for medical \nemergencies using 5 distinct generalized LLMs were \nconstructed, alongside a combination of Prompt Engineering \nand Retrieval Augmented Generation techniques. Quantitative \nand qualitative evaluations of the LLM responses were provided \nby 12 physicians (general practitioners) with an average of 2 \nyears of practice experience managing medical emergencies in \nresource-constrained settings in Ghana.   \nKeywords— SnooCODE, Clinical Decision Support, Large \nLanguage Models, First Aid, Emergency Medical Services, \nMedical Emergencies, Clinical Context, Clinician Evaluation, \nResource-Constrained Settings, Gemini 1.5 Pro, GPT 4, Claude  \nSonnet   \n  \n  \n I. INTRODUCTION \n \n“Provide a cool mist humidifier or take the infant into a \nsteamy bathroom to help loosen mucus.” – this was First Aid \nStep no.3 provided by Claude 3 Sonnet for managing possible \nBronchiolitis or Asthma Exacerbations – two conditions that \ncause breathing problems. While this may be valuable advice, \nit might not be applicable to a child living on a rural cattle farm \nin Akobo, South Sudan. When this particular location is added \nto the prompt, the response makes no mention of mist \nhumidifiers and steamy bathrooms. Rather the first step \nprovided by the model is to “Move the infant to an area with \nfresh air and away from any dust/irritants.” This shows the \nimportance of considering the background contexts of \nprompts in evaluating the performance of Large Language \nModels (LLMs). Amongst the popular biomedical Natural \nLanguage Processing (NLP) datasets for evaluating LLMs, \nnone of them have been specifically prepared for resource -\nconstrained settings as found in Low -and Low -Middle-\nIncome countries (LMICs) 2. Thus, though a few models \nachieve high scores when evaluated on these datasets, their \ntranslational value in everyday clinical scenarios in LMICs \ncannot be readily ascertained.   \nIn this work we aim to add to the limited knowledge base \non LLM applications for clinical scenarios in LMICs. \nSpecifically, we aim to evaluate the appropriateness of some \nselected generalized LLMs for use in clinical decision support \ntools in LMICs and to provide a reference for future, more  \nexpansive research. After conducting several experiments, we \nfound that when generalized LLMs are given prompts that aim \nto generate first aid advice for medical emergencies, their \noutputs differ significantly when addi tional context -specific \nlocation is provided 3. Thus, we provided context -specific \nprompts and asked clinicians with substantial familiarity with \nthose contexts and clinical scenarios to evaluate the outputs. \nThis work is part of a research and development process to \neventually deploy LLM-based Clinical Decision Support tools \nfor managing medical emergencies in resource-constrained \nsettings.   \n \n \n \nII. METHODOLOGY \n   \nA. LLM Selection   \nWe selected Open AI’s GPT -4 Turbo Preview, both via the \nAssistant Application Programming Interface (API) and the \nChat Completions API. We evaluated these separately as the \ntemperature of the model is almost impossible to be tweaked \nwhen using the OpenAI A ssistant. In addition we selected \nGemini 1.5 Pro and Claude Sonnet. These models were \nselected based on performance on popular benchmarks 2, \navailability of API and ease-of-access. We did not select open \nmedical LLMs such as Meditron -70B because of the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2024. ; https://doi.org/10.1101/2024.04.03.24305276doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\ncomputational resources required to host/access them, for \nexample, advanced GPUs. We then tested a combination of \nprompt-engineering and Retrieval Augmented Generation \n(RAG) techniques  to produce outputs/responses from the \nvarious LLMs as follows:   \n• GPT 4-Turbo Preview via Open AI Assistant API \n+ Prompt Engineering = Response A   \n• Gemini 1.5 Pro + Prompt Engineering = \nResponse B   \n• Claude Sonnet + Prompt Engineering =  \nResponse C   \n• GPT4-Turbo Preview via Open AI Chat \nCompletions API + Prompt Engineering + RAG \n= Response D   \n• Claude Sonnet + Prompt Engineering + RAG = \nResponse E   \nB. Parameter Tuning   \nThe temperature was set at 0 for generating Responses C to E. \nThis was to get deterministic responses as often as possible \ndue to the critical nature of the proposed use case. For \nResponse A, the default temperature used in Open AI Assistant \nwas maintained as it was difficult to ascertain and tweak. For \nResponse B, the default tempe rate of 2 set in the Google AI \nStudio was maintained as it was also difficult to tweak. An \noutput length of 4000 was set in Google AI Studio for \nassessing Gemini 1.5 Pro to provide an ample window for the \nextent of generated responses. Similarly, the max t okens \nparameter was set at 4000 for assessing Claude Sonnet to \nprovide an ample window for the extent of generated \nresponses.   \nC. Prompt Engineering   \nWe employed in -context learning using one -shot inference. \nThe prompt consisted of three parts, the system \nmessage/prompt/instructions, an example conversation and \nthe input message. Here is an example of the input message \nfor one of the prompts:   \n“   \nLocation: rural area, Bongo, Ghana. There is a chemist 300m \naway and a district hospital 1km away.  Patient's age as: 5 \nmonths, sex as: male. Description of medical emergency: fall \nfrom stool, vomiting. 1. PATIENT CAN TALK NORMALLY \n2. PA TIENT CAN BREA THE NORMALLY 3. PA TIENT \nHAS A NORMAL PULSE 4. PA TIENT IS NOT VISIBLY \nBLEEDING 5. PATIENT IS AW AKE AND ALE RT 6. \nPA TIENT DOES NOT HA VE A VISIBLE TRAUMA TIC \nINJURY , ANIMAL BITE OR RASH 7. PA TIENT HAS NO \nKNOWN ALLERGIES 8. THE PA TIENT HAS TAKEN \nPARACETAMOL 9. PA TIENT   \nHAS NO KNOWN PAST MEDICAL HISTORY 10.THE   \nTIME OF LAST MEAL W AS 30 MINUTES AGO   \n”   \nD. Retrieval Augmented Generation   \nRetrieval Augmented Generation (RAG) has been touted as a \nhighly promising approach to improving factuality, \nreasoning, and interpretability of LLM outputs4,5. We \nprovided a free manual for first aid instruction geared \ntowards settings in sub-Saharan Africa titled “Basic First Aid \nfor Africa” published in 2017 by the Belgian Red Cross (and \nproduced in conjunction with African experts). The text from \nthe module was divided into chunks and vector embeddings \ngenerated by the “text-embedding-3-large” embedding model \nfrom Open AI. This embedding model works well with both \nGPT4Turbo and Claude Sonnet for retrievals but does not \nwork as well with Gemini 1.0 Pro, thus RAG was not tested \nwith the Gemini model. The vector embeddings were stored \nin the open-source vector database, ChromaDB.   \n   \nE. Clinician Selection   \nClinician evaluators were selected via a local clinician \nnetwork, from diverse practice locations within Ghana and \nbased on their familiarity with the locations, contexts, and \nclinical scenarios. Clinicians selected were verified to be in \ngood standing wit h the Ghana Medical and Dental Council \nand had valid licenses to practice. Clinicians were asked to \ninput their completed number of years of practice, and to \nround up surplus months to one year, for 10+ months and to \nround down to 0 years if less than 10 m onths. On average, \nclinician evaluators had 2 completed years of experience, as \nthe first point -of-call in the hospital in managing medical \nemergencies in Ghana. It is expected that they possess \nsufficient knowledge and skills to deliver, at a minimum, first \naid in the selected medical scenarios.   \nF. Selection of Medical Scenarios   \n“Love how it span over all major disciplines” – A clinician \nevaluator.   \nSix simulated clinical scenarios were provided in the format \nshown in Section above. The scenarios featured a wide range \nof demographics with the youngest simulated patient being 6 \nmonths old, and the oldest being 85 years old. The clinical \nscenarios cut a cross all major clinical specialties. There was \nan equal distribution of male and female patients in the \nscenarios represented.   \nG. Response Evaluation and Ranking   \nEach simulated scenario produced 5 responses making a total \nof 30 responses. At the end of each response, a 10-point Likert \nscale was provided for ranking the response. An evaluator had \nto select a number from 0 to 10, with 0 representing “Totally \nUnsatisfactory” and “Totally Satisfactory”. At the end of each \nScenario-Responses pair, a comment box is provided for \nclinicians to input any additional comment about the scenario \nand accompanying 5 responses. Each of the 12 physicians \nranked all 5 responses for e very scenario, thus across the 6 \nscenarios, each response was ranked 72 times. A total of 360 \nrankings were then analyzed.   \nH. Collection and Analysis of Evaluation Reports Evaluation \nreports were collected via an online form. Quantitative \nanalysis and associated visualizations were performed in \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2024. ; https://doi.org/10.1101/2024.04.03.24305276doi: medRxiv preprint \nMicrosoft Excel V ersion 16.83. The Real Statistics Resource \nPack6 was used for Interrater Reliability Analysis. For \nqualitative analysis, evaluators’ comments were compiled as \ntext in a document and coding was performed using Taguette \n1.4.1-40-gfea85977. Thematic analysis and visualization were \nperformed in Python 3.118.   \nIII. RESULTS   \nA. Quantitative Analysis   \nTable 1 shows the ranking scores of the 12 evaluators \nlabelled “1” to “12” for each of the responses labelled “A to \nE”. These rankings are from the arithmetic mean of each \nevaluator’s ranking of the 5 responses across the 6 \nprompts/scenarios, rounded up to  the nearest whole number \nfor ease of readability. The overall mean ranking was 6.6 with \na standard deviation of 0.4.   \n  TABLE I.    RANKING  SCORES  PER EV ALUATOR.   \nResponse   A   B   C   D   E   \nEvaluator     Ranking scores   \n1   8   \n7   8   8   7   \n2   8   \n8   6   5   5   \n3   6   \n7   7   5   6   \n4   5   \n6   5   5   5   \n5   7   \n8   8   7   7   \n6   7   \n7   7   7   7   \n7   7   \n7   7   7   6   \n8   8   \n8   8   7   8   \n9   8   \n7   6   5   6   \n10   7   \n7   6   6   7   \n11   7   \n7   7   6   7   \n12   7   \n8   6   5   6   \nMean±   \ns.da   \n6.8±0.4   \n7.2±0.7   6.6±0.6   6±0.6   6.4±1.2   \na. standard deviation   \nGemini 1.5 Pro + Prompt Engineering (Response B) elicited \nthe highest rating scores: 7 or 8 out of 10, at least 90% of the \ntime and it’s lowest mean rating was 6. GPT 4-Turbo Preview \nvia Open AI Assistant API + Prompt Engineering (Response   \nA) had the second highest ratings: 7 or 8 out of 10, at least \n80% of the time. Claude Sonnet + Prompt Engineering + \nRAG (Response E) had a score of 7 or 8 out of 10, 50% \nof the time. The worst ranked was GPT4 -Turbo Preview \nvia Open AI Chat Completions API + Prompt  \nEngineering + RAG (Response D) with a score of 5 out \nof 10, 40% of the time. None of the responses had a mean \nrating below 5 (Figure 1).   \n \nFig. 1. Distribution of rating scores per Response category.   \nGwet’s AC2 score using ordinal weights and a significance \nlevel (alpha) of 0.05 was calculated as a measure of interrater \nreliability. As seen in Table 2, there was a high level of \nagreement between evaluators, reflected by a Gwet’s AC 2 \nscore of 0.89.   \n  TABLE II.    INTERRATER RELIABILITY ANALYSIS   \nStatistic   Score   \nGwet’s AC2   0.89   \nStandard error for subjects   0.02   \nC.I.a lower end   0.83   \nC.I. a upper end   0.96   \nTotal standard error (s.e.)   0.04   \nC.I. a lower end (s.e. accounted for)   0.77   \nC.I. a upper end (s.e. accounted for)   1   \na. Confidence Interval    \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n  \n  \n  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2024. ; https://doi.org/10.1101/2024.04.03.24305276doi: medRxiv preprint \nB. Qualitative Analysis   \n8 codes were generated representing recurring viewpoints \nexpressed. Table 3 shows the 8 codes and their descriptions.   \n  TABLE III.    DESCRIPTION OF CODES   \nCode   Description   \nQuickTransfer   Emphasis on \ntransferring or \nreferring the \ncasualty quickly  \nto a health \nfacility   \nResponseSatisfaction   Expresses \npositive \nsentiments about \nthe response   \nMissedDiagnosis   Mentions  a \ndiagnosis  the \nresponse did not \nprovide   \nNotConcise   Expresses  \ndissatisfaction \nthat response is \nnot concise.   \nConcise   Expresses \nsatisfaction that \nthe response is \nconcise   \nUnsureAboutCapabilityOfFacility   Expresses \nuncertainty  \n  and  \nlack   of \nconfidence  \n  in   \nnearby facilities   \nUnsureOfDiagnosis   Expresses  low \nconfidence in the \noutputted \ndiagnosis   \nDisagreesOnPlan   Disagrees or is \nunsure of the first \naid   plan \nsuggested.   \n   \nResponseSatisfaction was the most frequently occurring code, \nindicating numerous instances where the responses were \nconsidered satisfactory.   \nConcise and QuickTransfer also had significant occurrences, \nsuggesting that the importance of conciseness in responses and \nthe importance of quick transfers were often emphasized. \nMissedDiagnosis and NotConcise were less frequent but \nnotable, indicating areas where responses may have missed \ncritical diagnoses or were not concise enough. Figure 2 \noutlines the distribution of the codes.   \n   \n   \n   \n   \n   \n   \n   \n  \n   \n \nFig. 2. Frequency of Codes in Analysis of Evaluators’ Comments   \nThe most commonly occurring codes were grouped into the \nfollowing themes showing what clinicians considered most in \nevaluating scenarios and accompanying responses, arranged \nin descending order of frequency:   \n• Theme 1. Clarity and Efficiency of Communication: \nIncludes ResponseSatisfaction, Concise, and \nNotConcise.   \n• Theme 2. Diagnostic and Management Accuracy:  \nIncludes MissedDiagnosis, UnsureOfDiagnosis and \nDisagreesOnPlan.   \n• Theme 3. Urgency and Efficiency in Patient \nTransfer: Includes QuickTransfer and  \nUnsureAboutCapabilityOfFacility.   \nTable 4 details some of the clinicians’ comments under each \nof these themes.   \n  TABLE IV .   EXAMPLES OF EV ALUATORS’ COMMENTS UNDER THE  \nV ARIOUS THEMES   \n  Theme   Supporting Quotes   \n1     “Good responses provided \noverall.” “B: First aid measures \nconcise and accurate enough.”   \n2     “Completely missed epistaxis as a \ndiagnosis.” “…too early to be  \nconsidering asthma as first  \ndiagnosis.”   \n3     “Don’t wait till the patient \ndeteriorates before you try and \ntransfer to the nearest facility.” \n“Transfer to the nearest hospital \nshould be paramount.”   \n  \n  \n  \n  \n  \n  \n  \n  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2024. ; https://doi.org/10.1101/2024.04.03.24305276doi: medRxiv preprint \nThe contexts surrounding the most frequently occurring codes \nexpressing dissatisfaction with responses were further \nanalyzed in a word cloud to identify areas of improvement.  \nThe larger the word, the more often it appears in the \nevaluators’ comments. As shown in Figure 3, evaluators \ncommented often that an emphasis should be placed on not \nwaiting for Emergency Medical Services (EMS) but rather \ntransferring the patient to the nearest facility. There was also a \nsubstantial number of complaints about some responses not \nbeing concise enough and thus not appropriate as first aid \nmeasures.    \n \nFig. 3. Context analysis of “NotConcise” and “QuickTransfer” codes   \nIV . DISCUSSION   \nEvaluators were generally satisfied with the diagnosis and first \naid instructions outputted by the best performing generalized \nLLMs combined with moderate prompt engineering as \nindicated by both the quantitative and qualitative analysis \nresults. This performance by the LLMs is notable considering \nthat they had not had any prior pretraining or finetuning geared \nfor the tasks. Also, the prompting strategy implemented was \namongst the simplest with only one -shot inference. Past \nstudies have shown that more soph isticated prompting \nstrategies on generalized LLMs can lead to performances that \nout-perform state -of-the art, medical LLMs 9. The best \nperforming model in our study, achieved a mean ranking score \nof 7/10 which is encouraging. This is a positive finding for \nresourceconstrained settings where the ability to create more \nspecialized, domain -specific models and/or to run them is \ngreatly limited. If generalized models which are often more \naccessible to wider groups of people, can be made to perform \nat par/or better than specialized medical LLMs using simpler \ntechniques, then developers in resource -constrained settings \ncan take advantage to develop effective yet cost -efficient \napplications. An example of such applications is the \nSnooCODE Red app being developed in Ghana10. Figure 4 \nshows a version of the app in development.   \n   \n   \n   \n   \n   \n   \n \nFig. 4. A screenshot of the SnooCODE Red app under development   \nThough many studies have demonstrated the benefits of RAG \nin boosting the performance of generalized LLMs in domain  \nspecific tasks4,5,11,12, more emphasis must be placed on RAG \ntechnique. Though internal experiments revealed that the \naddition of RAG can achieve better performance that prompt \nengineering alone, the findings from this study is that no RAG \nis better than RAG not done properly. Beyond, the embedding \nmodel used and th e embedding and retrieval techniques, the \ncontent and formatting of the retrieval document can have a \nsignificant impact on the final model performance. This is a \nlesson that developers must pay attention to in the \ndevelopment of LLM-based applications.   \n   \nThe study also sheds more light on the importance of \nconsidering context in the evaluation of LLM performance. \nThis is an area that human evaluators might beat machine \nevaluators. Clinician evaluators were not satisfied with \nresponses that did not demonstr ate a higher sense of urgency \nin the transfer of casualties to nearby health facilities even \nthough in the prompt instruction, all the models were informed \nthat EMS was on the way. Responses that instructed that the \npatient be transported to the nearest health facility even as first \naid steps were being instituted were rated as more satisfactory. \nIn contexts with better access to resources, evaluators might \nnot have expressed such a strong concern about waiting for \nEMS. In many of the rural settings provide d in the scenarios \nwith meagre resources, this expression of concern was \nwarranted. This underscores the huge importance of \nconsidering contexts in developing LLM -based clinical \ndecision support tools . It is not enough that LLMs pass \ngeneral medical bench marks, their performance in different \ncontexts must be evaluated, otherwise responses considered \nhelpful in some settings may not only be unhelpful in other \nsettings, but also harmful.   \n   \nThere are obvious limitations in this study. Firstly, a larger \ncohort of responses could have been evaluated. Also, a more \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2024. ; https://doi.org/10.1101/2024.04.03.24305276doi: medRxiv preprint \ncomprehensive evaluation framework could have been \nemployed. We hope that the feedback obtained can be used to \nimprove LLM outputs for the provided scenarios. We also \nhope that the insights derived can provide some direction in \nimplementing more detailed a nd extensive studies of LLM \noutputs in resource-constrained settings.   \n  \nV. CONCLUSION   \nLLM-based first aid assistants have the potential to provide \nclinically useful instructions in medical emergencies. This is \nespecially helpful in resource -constrained settings where \ntimely access to well -equipped health facilities is often \ndifficult. This pote ntial should be explored further to build \napplications which may prove life -saving in real -world \nsettings   \nREFERENCES   \n   \n1. Project Genie Clinician Evaluation Group (March, \n2023) https://bit.ly/clinician-evaluators-project-genie \n2. Zhou, H., Gu, B., Zou, X., Li, Y ., Chen, S.S., et al. \n(2023). A Survey of Large Language Models in \nMedicine: Progress, Application, and Challenge. \nArXiv, abs/2311.05112.  \n3. SnooCODE Red Team, “CONTEXT MA TTERS: \nDIFFERENCES IN AI FIRST AID ASSISTANT \nOUTPUTS IN V ARIOUS  CONTEXTS.”  \nhttps://bit.ly/snoocodered-context-matters   \n4. Li, H., Su, Y ., Cai, D., Wang, Y ., & Liu, L. (2022). A \nSurvey on Retrieval -Augmented Text Generation. \nArXiv, abs/2202.01110.   \n5. Anantha, R., Bethi, T., V odianik, D., & Chappidi, S. \n(2023). Context Tuning for Retrieval Augmented \nGeneration. ArXiv, abs/2312.05708.   \n6. Real Statistics Resource Pack (n.d.). Retrieved March \n19, 2024, from https://real-statistics.com/free-\ndownload/real-statistics-resourcepack/    \n7. Rampin, R., Rampin, V . (2021). Taguette: open-source \nqualitative data analysis. Journal of Open Source \nSoftware, 6(68), 3522,  \nhttps://doi.org/10.21105/joss.03522    \n8. Python Software Foundation. Python Language \nReference, version 3.11. Available at \nhttp://www.python.org   \n9. Nori, H., Lee, Y .T., Zhang, S., Carignan, D., Edgar, R., \net al. (2023). Can Generalist Foundation Models \nOutcompete Special -Purpose Tuning? Case Study in \nMedicine. ArXiv, abs/2311.16452.   \n10. SnooCODE. (n.d.). SnooCODE RED. Retrieved \nMarch 21, 2024, from https://snoocode.com/red   \n11. Soman, K., Rose, P.W., Morris, J.H., Akbas, R.E., \nSmith, B., et al. (2023). Biomedical knowledge graph-\nenhanced prompt generation for large language \nmodels. ArXiv, abs/2311.17330.   \n12. Gao, Y ., Li, R., Croxford, E., Tesch, S., To, D., Caskey, \nJ., Patterson, B., Churpek, M., Miller, T., Dligach, D., \n& Afshar, M. (2023). Large Language Models and \nMedical Knowledge Grounding for Diagnosis \nPrediction. . \nhttps://doi.org/10.1101/2023.11.24.23298641.  \n   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2024. ; https://doi.org/10.1101/2024.04.03.24305276doi: medRxiv preprint ",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.6135321259498596
    },
    {
      "name": "Computer science",
      "score": 0.5617161393165588
    },
    {
      "name": "Decision support system",
      "score": 0.4249269366264343
    },
    {
      "name": "Management science",
      "score": 0.3805961608886719
    },
    {
      "name": "Process management",
      "score": 0.3461224436759949
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25589025020599365
    },
    {
      "name": "Engineering",
      "score": 0.18492040038108826
    },
    {
      "name": "Geography",
      "score": 0.08372986316680908
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2803053305",
      "name": "Korle Bu Teaching Hospital",
      "country": "GH"
    }
  ],
  "cited_by": 6
}