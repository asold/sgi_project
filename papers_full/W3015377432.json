{
    "title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
    "url": "https://openalex.org/W3015377432",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4202174151",
            "name": "Hendrycks, Dan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2160053105",
            "name": "Liu Xiao-yuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2913157063",
            "name": "Wallace, Eric",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221878661",
            "name": "Dziedzic, Adam",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Krishnan, Rishabh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751273816",
            "name": "Song, Dawn",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962736243",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2515550864",
        "https://openalex.org/W2799007037",
        "https://openalex.org/W2963060032",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2963499246",
        "https://openalex.org/W2163302275",
        "https://openalex.org/W2994931756",
        "https://openalex.org/W2904981516",
        "https://openalex.org/W2899682268",
        "https://openalex.org/W2963188609",
        "https://openalex.org/W2953163841",
        "https://openalex.org/W2963394326",
        "https://openalex.org/W2962816513",
        "https://openalex.org/W2963384319",
        "https://openalex.org/W2739505524",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W2031342017",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2162651021",
        "https://openalex.org/W2950470622",
        "https://openalex.org/W3203309275",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963969878",
        "https://openalex.org/W2885183727",
        "https://openalex.org/W2982756474",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W2525127255",
        "https://openalex.org/W2963691697",
        "https://openalex.org/W2996568780",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W1493526108",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3034340181",
        "https://openalex.org/W2961301154",
        "https://openalex.org/W2120354757",
        "https://openalex.org/W2531327146",
        "https://openalex.org/W2962718483",
        "https://openalex.org/W2912237282",
        "https://openalex.org/W2989929945",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2898662126",
        "https://openalex.org/W2963383094",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2027731328",
        "https://openalex.org/W2957688595",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3098649723",
        "https://openalex.org/W2882319491"
    ],
    "abstract": "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
    "full_text": "Pretrained Transformers Improve Out-of-Distribution Robustness\nDan Hendrycks1∗ Xiaoyuan Liu1,2∗ Eric Wallace1\nAdam Dziedzic3 Rishabh Krishnan1 Dawn Song1\n1UC Berkeley 2Shanghai Jiao Tong University 3University of Chicago\n{hendrycks,ericwallace,dawnsong}@berkeley.edu\nAbstract\nAlthough pretrained Transformers such\nas BERT achieve high accuracy on in-\ndistribution examples, do they generalize to\nnew distributions? We systematically measure\nout-of-distribution (OOD) generalization for\nseven NLP datasets by constructing a new\nrobustness benchmark with realistic distribu-\ntion shifts. We measure the generalization\nof previous models including bag-of-words\nmodels, ConvNets, and LSTMs, and we show\nthat pretrained Transformers’ performance\ndeclines are substantially smaller. Pretrained\ntransformers are also more effective at de-\ntecting anomalous or OOD examples, while\nmany previous models are frequently worse\nthan chance. We examine which factors affect\nrobustness, ﬁnding that larger models are not\nnecessarily more robust, distillation can be\nharmful, and more diverse pretraining data can\nenhance robustness. Finally, we show where\nfuture work can improve OOD robustness.\n1 Introduction\nThe train and test distributions are often not iden-\ntically distributed. Such train-test mismatches\noccur because evaluation datasets rarely charac-\nterize the entire distribution (Torralba and Efros,\n2011), and the test distribution typically drifts over\ntime (Quionero-Candela et al., 2009). Chasing an\nevolving data distribution is costly, and even if the\ntraining data does not become stale, models will\nstill encounter unexpected situations at test time.\nAccordingly, models must generalize to OOD ex-\namples whenever possible, and when OOD exam-\nples do not belong to any known class, models\nmust detect them in order to abstain or trigger a\nconservative fallback policy (Emmott et al., 2015).\nMost evaluation in natural language processing\n(NLP) assumes the train and test examples are in-\n∗Equal contribution.\nhttps://github.com/camelop/NLP-Robustness\ndependent and identically distributed (IID). In the\nIID setting, large pretrained Transformer models\ncan attain near human-level performance on nu-\nmerous tasks (Wang et al., 2019). However, high\nIID accuracy does not necessarily translate to OOD\nrobustness for image classiﬁers (Hendrycks and Di-\netterich, 2019), and pretrained Transformers may\nembody this same fragility. Moreover, pretrained\nTransformers can rely heavily on spurious cues and\nannotation artifacts (Cai et al., 2017; Gururangan\net al., 2018) which out-of-distribution examples\nare less likely to include, so their OOD robustness\nremains uncertain.\nIn this work, we systematically study the OOD\nrobustness of various NLP models, such as word\nembeddings averages, LSTMs, pretrained Trans-\nformers, and more. We decompose OOD robust-\nness into a model’s ability to (1) generalize and to\n(2) detect OOD examples (Card et al., 2018).\nTo measure OOD generalization, we create a\nnew evaluation benchmark that tests robustness to\nshifts in writing style, topic, and vocabulary, and\nspans the tasks of sentiment analysis, textual entail-\nment, question answering, and semantic similarity.\nWe create OOD test sets by splitting datasets with\ntheir metadata or by pairing similar datasets to-\ngether (Section 2). Using our OOD generalization\nbenchmark, we show that pretrained Transformers\nare considerably more robust to OOD examples\nthan traditional NLP models (Section 3). We show\nthat the performance of an LSTM semantic similar-\nity model declines by over 35% on OOD examples,\nwhile a RoBERTa model’s performance slightly\nincreases. Moreover, we demonstrate that while\npretraining larger models does not seem to improve\nOOD generalization, pretraining models on diverse\ndata does improve OOD generalization.\nTo measure OOD detection performance, we\nturn classiﬁers into anomaly detectors by using\ntheir prediction conﬁdences as anomaly scores\narXiv:2004.06100v2  [cs.CL]  16 Apr 2020\n(Hendrycks and Gimpel, 2017). We show that\nmany non-pretrained NLP models are often near\nor worse than random chanceat OOD detection.\nIn contrast, pretrained Transformers are far more\ncapable at OOD detection. Overall, our results\nhighlight that while there is room for future\nrobustness improvements, pretrained Transformers\nare already moderately robust.\n2 How We Test Robustness\n2.1 Train and Test Datasets\nWe evaluate OOD generalization withseven care-\nfully selected datasets. Each dataset either (1) con-\ntains metadata which allows us to naturally split the\nsamples or (2) can be paired with a similar dataset\nfrom a distinct data generating process. By splitting\nor grouping our chosen datasets, we can induce a\ndistribution shift and measure OOD generalization.\nWe utilize four sentiment analysis datasets:\n• We use SST-2, which contains pithy expert\nmovie reviews (Socher et al., 2013), and\nIMDb (Maas et al., 2011), which contains full-\nlength lay movie reviews. We train on one\ndataset and evaluate on the other dataset, and\nvice versa. Models predict a movie review’s\nbinary sentiment, and we report accuracy.\n• The Yelp Review Dataset contains restaurant\nreviews with detailed metadata (e.g., user ID,\nrestaurant name). We carve out four groups from\nthe dataset based on food type: American, Chi-\nnese, Italian,and Japanese. Models predict a\nrestaurant review’s binary sentiment, and we re-\nport accuracy.\n• The Amazon Review Dataset contains product\nreviews from Amazon (McAuley et al., 2015; He\nand McAuley, 2016). We split the data into ﬁve\ncategories of clothing (Clothes, Women Cloth-\ning, Men Clothing, Baby Clothing, Shoes) and\ntwo categories of entertainment products (Music,\nMovies). We sample 50,000 reviews for each\ncategory. Models predict a review’s 1 to 5 star\nrating, and we report accuracy.\nWe also utilize these datasets for semantic similar-\nity, reading comprehension, and textual entailment:\n• STS-B requires predicting the semantic simi-\nlarity between pairs of sentences (Cer et al.,\n2017). The dataset contains text of different\ngenres and sources; we use four sources from\ntwo genres: MSRpar (news), Headlines (news);\nMSRvid (captions), Images (captions). The eval-\nuation metric is Pearson’s correlation coefﬁcient.\n• ReCoRD is a reading comprehension dataset\nusing paragraphs from CNN and Daily Mail\nnews articles and automatically generated ques-\ntions (Zhang et al., 2018). We bifurcate the\ndataset into CNN and Daily Mail splits and eval-\nuate using exact match.\n• MNLI is a textual entailment dataset using\nsentence pairs drawn from different genres of\ntext (Williams et al., 2018). We select examples\nfrom two genres of transcribed text (Telephone\nand Face-to-Face) and one genre of written text\n(Letters), and we report classiﬁcation accuracy.\n2.2 Embedding and Model Types\nWe evaluate NLP models with different input rep-\nresentations and encoders. We investigate three\nmodel categories with a total of thirteen models.\nBag-of-words (BoW) Model. We use a bag-of-\nwords model (Harris, 1954), which is high-bias but\nlow-variance, so it may exhibit performance sta-\nbility. The BoW model is only used for sentiment\nanalysis and STS-B due to its low performance on\nthe other tasks. For STS-B, we use the cosine sim-\nilarity of the BoW representations from the two\ninput sentences.\nWord Embedding Models. We use\nword2vec (Mikolov et al., 2013) and GloVe (Pen-\nnington et al., 2014) word embeddings. These\nembeddings are encoded with one of three\nmodels: word averages (Wieting et al., 2016),\nLSTMs (Hochreiter and Schmidhuber, 1997),\nand Convolutional Neural Networks (ConvNets).\nFor classiﬁcation tasks, the representation from\nthe encoder is fed into an MLP. For STS-B\nand MNLI, we use the cosine similarity of the\nencoded representations from the two input\nsentences. For reading comprehension, we use\nthe DocQA model (Clark and Gardner, 2018)\nwith GloVe embeddings. We implement our\nmodels in AllenNLP (Gardner et al., 2018) and\ntune the hyperparameters to maximize validation\nperformance on the IID task.\nPretrained Transformers. We investigate\nBERT-based models (Devlin et al., 2019) which\nare pretrained bidirectional Transformers (Vaswani\net al., 2017) with GELU (Hendrycks and Gimpel,\n2016) activations. In addition to using BERT\nBase and BERT Large, we also use the large\nversion of RoBERTa (Liu et al., 2019b), which\nis pretrained on a larger dataset than BERT.\nAvg.\nBoW\nAvg.\nw2v\nConvNet\nw2v\nLSTM\nw2v\nBERT\nBase\nBERT\nLarge\nRoBERTa\n0\n20\n40\n60\n80\n100Pearson Correlation (%)\nSemantic Textual Similarity (STS-B) Generalization\nIID Data (Images)\nOOD Data (MSRvid)\nFigure 1: Pretrained Transformers often have smaller\nIID/OOD generalization gaps than previous models.\nWe use ALBERT (Lan et al., 2020) and also a\ndistilled version of BERT, DistilBERT (Sanh et al.,\n2019). We follow the standard BERT ﬁne-tuning\nprocedure (Devlin et al., 2019) and lightly tune the\nhyperparameters for our tasks. We perform our\nexperiments using the HuggingFace Transformers\nlibrary (Wolf et al., 2019).\n3 Out-of-Distribution Generalization\nIn this section, we evaluate OOD generalization\nof numerous NLP models on seven datasets and\nprovide some upshots. A subset of results are in\nFigures 1 and 2. Full results are in Appendix A.\nPretrained Transformers are More Robust.\nIn our experiments, pretrained Transformers often\nhave smaller generalization gaps from IID data\nto OOD data than traditional NLP models. For\ninstance, Figure 1 shows that the LSTM model\ndeclined by over 35%, while RoBERTa’s general-\nization performance in fact increases. For Amazon,\nMNLI, and Yelp, we ﬁnd that pretrained Trans-\nformers’ accuracy only slightly ﬂuctuates on OOD\nexamples. Partial MNLI results are in Table 1. We\npresent the full results for these three tasks in Ap-\npendix A.2. In short, pretrained Transformers can\ngeneralize across a variety of distribution shifts.\nModel Telephone\n(IID)\nLetters\n(OOD)\nFace-to-Face\n(OOD)\nBERT 81.4% 82.3% 80.8%\nTable 1: Accuracy of a BERT Base MNLI model\ntrained on Telephone data and tested on three different\ndistributions. Accuracy only slightly ﬂuctuates.\nBigger Models Are Not Always Better. While\nlarger models reduce the IID/OOD generaliza-\ntion gap in computer vision (Hendrycks and Di-\netterich, 2019; Xie and Yuille, 2020; Hendrycks\net al., 2019d), we ﬁnd the same does not hold in\nAvg.\nBoW\nAvg.\nw2v\nConvNet\nw2v\nLSTM\nw2v\nBERT\nBase\nBERT\nLarge\nRoBERTa\n60\n70\n80\n90\n100Accuracy (%)\nIMDb Sentiment Classifier Generalization\nIID Data (IMDb)\nOOD Data (SST-2)\nDocQA DistilBERT BERT Base BERT Large RoBERTa\n20\n30\n40\n50\n60\n70\n80Exact Match (%)\nReCoRD Reading Comprehension Generalization\nIID Data (CNN)\nOOD Data (Daily Mail)\nFigure 2: Generalization results for sentiment analysis\nand reading comprehension. While IID accuracy does\nnot vary much for IMDb sentiment analysis, OOD ac-\ncuracy does. Here pretrained Transformers do best.\nBERTbase\nBERTlarge\nALBERTbase\nALBERTlarge\nALBERTxlarge\nALBERTxxlarge\n0\n2\n4\n6\n8\n10SST-2 Accuracy - IMDb Accuracy (%)\nSST-2 Model Size vs. Accuracy Drop\nFigure 3: The IID/OOD generalization gap is not im-\nproved with larger models, unlike in computer vision.\nNLP. Figure 3 shows that larger BERT and AL-\nBERT models do not reduce the generalization\ngap. However, in keeping with results from vi-\nsion (Hendrycks and Dietterich, 2019), we ﬁnd that\nmodel distillation can reduce robustness, as evident\nin our DistilBERT results in Figure 2. This high-\nlights that testing model compression methods for\nBERT (Shen et al., 2020; Ganesh et al., 2020; Li\net al., 2020) on only in-distribution examples gives\na limited account of model generalization, and such\nnarrow evaluation may mask downstream costs.\n20 NG Multi30K RTE SNLI WMT16 Average\n0\n20\n40\n60\n80\n100\nFalse Alarm Rate (%)\n(Lower Is Better)\nDetecting OOD Examples for an SST-2 Sentiment Classifier\nModel Type\nRandom Detector\nBag of Words\nAvg. word2vec\nLSTM word2vec\nConvNet word2vec\nBERT Large\nFigure 4: We feed in OOD examples from out-of-distribution datasets (20 Newsgroups, Multi30K, etc.) to SST-2\nsentiment classiﬁers and report the False Alarm Rate at95% Recall. A lower False Alarm Rate is better. Classiﬁers\nare repurposed as anomaly detectors by using their negative maximum softmax probability as the anomaly score—\nOOD examples should be predicted with less conﬁdence than IID examples. Models such as BoW, word2vec\naverages, and LSTMs are near random chance; that is, previous NLP models are frequently more conﬁdent when\nclassifying OOD examples than when classifying IID test examples.\nMore Diverse Data Improves Generalization.\nSimilar to computer vision (Orhan, 2019; Xie et al.,\n2020; Hendrycks et al., 2019a), pretraining on\nlarger and more diverse datasets can improve ro-\nbustness. RoBERTa exhibits greater robustness\nthan BERT Large, where one of the largest differ-\nences between these two models is that RoBERTa\npretrains on more data. See Figure 2’s results.\n4 Out-of-Distribution Detection\nSince OOD robustness requires evaluating both\nOOD generalization and OOD detection, we now\nturn to the latter. Without access to an outlier\ndataset (Hendrycks et al., 2019b), the state-of-\nthe-art OOD detection technique is to use the\nmodel’s prediction conﬁdence to separate in- and\nout-of-distribution examples (Hendrycks and Gim-\npel, 2017). Speciﬁcally, we assign an example x\nthe anomaly score −maxy p(y |x), the negative\nprediction conﬁdence, to perform OOD detection.\nWe train models on SST-2, record the model’s\nconﬁdence values on SST-2 test examples, and\nthen record the model’s conﬁdence values on\nOOD examples from ﬁve other datasets. For our\nOOD examples, we use validation examples from\n20 Newsgroups (20 NG) (Lang, 1995), the En-\nglish source side of English-German WMT16 and\nEnglish-German Multi30K (Elliott et al., 2016),\nand concatenations of the premise and hypothesis\nfor RTE (Dagan et al., 2005) and SNLI (Bowman\net al., 2015). These examples are only used during\nOOD evaluation not training.\nFor evaluation, we follow past work (Hendrycks\net al., 2019b) and report the False Alarm Rate at\n95% Recall (FAR95). The FAR95 is the probability\nthat an in-distribution example raises a false alarm,\nassuming that 95% of all out-of-distribution exam-\nples are detected. Hence a lower FAR95 is better.\nPartial results are in Figure 4, and full results are\nin Appendix A.3.\nPrevious Models Struggle at OOD Detection.\nModels without pretraining (e.g., BoW, LSTM\nword2vec) are often unable to reliably detect OOD\nexamples. In particular, these models’ FAR95\nscores are sometimes worse than chance because\nthe models often assign a higher probability to\nout-of-distribution examples than in-distribution\nexamples. The models particularly struggle on 20\nNewsgroups (which contains text on diverse topics\nincluding computer hardware, motorcycles, space),\nas their false alarm rates are approximately 100%.\nPretrained Transformers Are Better Detectors.\nIn contrast, pretrained Transformer models are bet-\nter OOD detectors. Their FAR95 scores are always\nbetter than chance. Their superior detection perfor-\nmance is not solely because the underlying model\nis a language model, as prior work (Hendrycks\net al., 2019b) shows that language models are not\nnecessarily adept at OOD detection. Also note\nthat in OOD detection for computer vision, higher\naccuracy does not reliably improve OOD detec-\ntion (Lee et al., 2018), so pretrained Transformers’\nOOD detection performance is not anticipated. De-\nspite their relatively low FAR95 scores, pretrained\nTransformers still do not cleanly separate in- and\nout-of-distribution examples (Figure 5). OOD de-\ntection using pretrained Transformers is still far\nfrom perfect, and future work can aim towards cre-\nating better methods for OOD detection.\n0.5 0.6 0.7 0.8 0.9 1.0\nMaximum Softmax Probability (Confidence)\nFrequency\nSST Classifier Confidence Distribution\nSST (IID)\nWMT16 (OOD)\nFigure 5: The conﬁdence distribution for a RoBERTa\nSST-2 classiﬁer on examples from the SST-2 test set\nand the English side of WMT16 English-German. The\nWMT16 histogram is translucent and overlays the SST\nhistogram. The minimum prediction conﬁdence is 0.5.\nAlthough RoBERTa is better than previous models at\nOOD detection, there is clearly room for future work.\n5 Discussion and Related Work\nWhy Are Pretrained Models More Robust?\nAn interesting area for future work is to analyze\nwhy pretrained Transformers are more robust.\nA ﬂawed explanation is that pretrained models\nare simply more accurate. However, this work\nand past work show that increases in accuracy\ndo not directly translate to reduced IID/OOD\ngeneralization gaps (Hendrycks and Dietterich,\n2019; Fried et al., 2019). One partial explanation is\nthat Transformer models are pretrained on diverse\ndata, and in computer vision, dataset diversity\ncan improve OOD generalization (Hendrycks\net al., 2020) and OOD detection (Hendrycks\net al., 2019b). Similarly, Transformer models are\npretrained with large amounts of data, which may\nalso aid robustness (Orhan, 2019; Xie et al., 2020;\nHendrycks et al., 2019a). However, this is not a\ncomplete explanation as BERT is pretrained on\nroughly 3 billion tokens, while GloVe is trained\non roughly 840 billion tokens. Another partial\nexplanation may lie in self-supervised training\nitself. Hendrycks et al. (2019c) show that com-\nputer vision models trained with self-supervised\nobjectives exhibit better OOD generalization and\nfar better OOD detection performance. Future\nwork could propose new self-supervised objectives\nthat enhance model robustness.\nDomain Adaptation. Other research on robust-\nness considers the separate problem of domain\nadaptation (Blitzer et al., 2007; Daum´e III, 2007),\nwhere models must learn representations of a\nsource and target distribution. We focus on testing\ngeneralization without adaptation in order to bench-\nmark robustness to unforeseen distribution shifts.\nUnlike Fisch et al. (2019); Yogatama et al. (2019),\nwe measure OOD generalization by considering\nsimple and natural distribution shifts, and we also\nevaluate more than question answering.\nAdversarial Examples. Adversarial examples\ncan be created for NLP models by inserting\nphrases (Jia and Liang, 2017; Wallace et al., 2019),\nparaphrasing questions (Ribeiro et al., 2018), and\nreducing inputs (Feng et al., 2018). However, ad-\nversarial examples are often disconnected from\nreal-world performance concerns (Gilmer et al.,\n2018). Thus, we focus on an experimental setting\nthat is more realistic. While previous works show\nthat, for all NLP models, there exist adversarial\nexamples, we show that all models are not equally\nfragile. Rather, pretrained Transformers are overall\nfar more robust than previous models.\nCounteracting Annotation Artifacts. Annota-\ntors can accidentally leave unintended shortcuts\nin datasets that allow models to achieve high ac-\ncuracy by effectively “cheating” (Cai et al., 2017;\nGururangan et al., 2018; Min et al., 2019). These\nannotation artifactsare one reason for OOD brit-\ntleness: OOD examples are unlikely to contain the\nsame spurious patterns as in-distribution examples.\nOOD robustness benchmarks like ours can stress\ntest a model’s dependence on artifacts (Liu et al.,\n2019a; Feng et al., 2019; Naik et al., 2018).\n6 Conclusion\nWe created an expansive benchmark across several\nNLP tasks to evaluate out-of-distribution robust-\nness. To accomplish this, we carefully restructured\nand matched previous datasets to induce numerous\nrealistic distribution shifts. We ﬁrst showed that\npretrained Transformers generalize to OOD ex-\namples far better than previous models, so that the\nIID/OOD generalization gap is often markedly re-\nduced. We then showed that pretrained Transform-\ners detect OOD examples surprisingly well. Over-\nall, our extensive evaluation shows that while pre-\ntrained Transformers are moderately robust, there\nremains room for future research on robustness.\nAcknowledgements\nWe thank the members of Berkeley NLP, Sona\nJeswani, Suchin Gururangan, Nelson Liu, Shi Feng,\nthe anonymous reviewers, and especially Jon Cai.\nThis material is in part based upon work supported\nby the National Science Foundation Frontier Award\n1804794. Any opinions, ﬁndings, and conclusions\nor recommendations expressed in this material are\nthose of the author(s) and do not necessarily reﬂect\nthe views of the National Science Foundation.\nReferences\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiﬁcation. In\nACL.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP.\nZheng Cai, Lifu Tu, and Kevin Gimpel. 2017. Pay at-\ntention to the ending: Strong neural baselines for the\nroc story cloze task. In ACL.\nDallas Card, Michael Zhang, and Noah A. Smith. 2018.\nDeep weighted averaging classiﬁers. In FAT.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\nTask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. In SemEval.\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop.\nHal Daum´e III. 2007. Frustratingly easy domain adap-\ntation. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30k: Multilingual english-\ngerman image descriptions. In ACL.\nAndrew Emmott, Shubhomoy Das, Thomas G. Diet-\nterich, Alan Fern, and Weng-Keen Wong. 2015. A\nmeta-analysis of the anomaly detection problem.\nShi Feng, Eric Wallace, and Jordan Boyd-Graber. 2019.\nMisleading failures of partial-input baselines. In\nACL.\nShi Feng, Eric Wallace, II Grissom, Mohit Iyyer, Pedro\nRodriguez, and Jordan Boyd-Graber. 2018. Patholo-\ngies of neural models make interpretations difﬁcult.\nIn EMNLP.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. Proceedings of\nthe 2nd workshop on machine reading for question\nanswering. In MRQA Workshop.\nDaniel Fried, Nikita Kitaev, and Dan Klein. 2019.\nCross-domain generalization of neural constituency\nparsers. In ACL.\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali\nKhan, Yin Yang, Deming Chen, Marianne Winslett,\nHassan Sajjad, and Preslav Nakov. 2020. Compress-\ning large-scale transformer-based models: A case\nstudy on BERT. ArXiv, abs/2002.11985.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2018. AllenNLP: a deep semantic natural language\nprocessing platform. In Workshop for NLP Open\nSource Software.\nJustin Gilmer, Ryan P. Adams, Ian J. Goodfellow,\nDavid Andersen, and George E. Dahl. 2018. Moti-\nvating the rules of the game for adversarial example\nresearch. ArXiv, abs/1807.06732.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel R. Bowman, and Noah\nA. Smith. 2018. Annotation artifacts in natural lan-\nguage inference data. In NAACL-HLT.\nZellig S Harris. 1954. Distributional structure. Word.\nRuining He and Julian J. McAuley. 2016. Ups and\ndowns: Modeling the visual evolution of fashion\ntrends with one-class collaborative ﬁltering. In\nWWW.\nDan Hendrycks and Thomas Dietterich. 2019. Bench-\nmarking neural network robustness to common cor-\nruptions and perturbations. In ICLR.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (GELUs). arXiv preprint\narXiv:1606.08415.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline\nfor detecting misclassiﬁed and out-of-distribution\nexamples in neural networks. In ICLR.\nDan Hendrycks, Kimin Lee, and Mantas Mazeika.\n2019a. Using pre-training can improve model ro-\nbustness and uncertainty. ICML.\nDan Hendrycks, Mantas Mazeika, and Thomas G. Diet-\nterich. 2019b. Deep anomaly detection with outlier\nexposure. ICLR.\nDan Hendrycks, Mantas Mazeika, Saurav Kadavath,\nand Dawn Song. 2019c. Using self-supervised learn-\ning can improve model robustness and uncertainty.\nIn NeurIPS.\nDan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret\nZoph, Justin Gilmer, and Balaji Lakshminarayanan.\n2020. AugMix: A simple data processing method to\nimprove robustness and uncertainty. ICLR.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob\nSteinhardt, and Dawn Song. 2019d. Natural adver-\nsarial examples. ArXiv, abs/1907.07174.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. In Neural Computation.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn EMNLP.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: a lite BERT for self-supervised\nlearning of language representations. In ICLR.\nKen Lang. 1995. NewsWeeder: Learning to ﬁlter Net-\nnews. In ICML.\nKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.\n2018. Training conﬁdence-calibrated classiﬁers for\ndetecting out-of-distribution samples. In ICLR.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners. ArXiv, abs/2002.11794.\nNelson F Liu, Roy Schwartz, and Noah A Smith. 2019a.\nInoculation by ﬁne-tuning: A method for analyzing\nchallenge datasets. In NAACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv, abs/1907.11692.\nAndrew L Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nACL.\nJulian J. McAuley, Christopher Targett, Qinfeng Shi,\nand Anton van den Hengel. 2015. Image-based rec-\nommendations on styles and substitutes. In SIGIR.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In NIPS.\nSewon Min, Eric Wallace, Sameer Singh, Matt Gard-\nner, Hannaneh Hajishirzi, and Luke Zettlemoyer.\n2019. Compositional questions do not necessitate\nmulti-hop reasoning. In ACL.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress test evaluation for natural language inference.\nIn COLING.\nA. Emin Orhan. 2019. Robustness properties\nof facebook’s ResNeXt WSL models. ArXiv,\nabs/1907.07640.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. GloVe: Global vectors for word rep-\nresentation. In EMNLP.\nJoaquin Quionero-Candela, Masashi Sugiyama, Anton\nSchwaighofer, and Neil D. Lawrence. 2009. Dataset\nshift in machine learning.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversarial\nrules for debugging NLP models. In ACL.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled ver-\nsion of bert: smaller, faster, cheaper and lighter. In\nNeurIPS EMC2 Workshop.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-BERT: Hessian based ultra low\nprecision quantization of BERT.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In EMNLP.\nAntonio Torralba and Alexei A. Efros. 2011. Unbiased\nlook at dataset bias. CVPR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matthew\nGardner, and Sameer Singh. 2019. Universal adver-\nsarial triggers for attacking and analyzing NLP. In\nEMNLP.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multitask benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016. Towards universal paraphrastic sen-\ntence embeddings. In ICLR.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL-HLT.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nCihang Xie and Alan L. Yuille. 2020. Intriguing prop-\nerties of adversarial training at scale. In ICLR.\nQizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and\nQuoc V . Le. 2020. Self-training with noisy student\nimproves imagenet classiﬁcation. In CVPR.\nDani Yogatama, Cyprien de Masson d’Autume, Jerome\nConnor, Tom´as Kocisk ´y, Mike Chrzanowski, Ling-\npeng Kong, Angeliki Lazaridou, Wang Ling, Lei\nYu, Chris Dyer, and Phil Blunsom. 2019. Learning\nand evaluating general linguistic intelligence. ArXiv,\nabs/1901.11373.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nReCoRD: bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv,\nabs/1810.12885.\nA Additional Experimental Results\nA.1 Signiﬁcant OOD Accuracy Drops\nFor STS-B, ReCoRD, and SST-2/IMDb, there is\na noticeable drop in accuracy when testing on\nOOD examples. We show the STS-B results in\nTable 2, the ReCoRD results in Table 3, and the\nSST-2/IMDb results in Table 4.\nA.2 Minor OOD Accuracy Drops\nWe observe more minor performance declines for\nthe Amazon, MNLI, and Yelp datasets. Figure 6\nshows the Amazon results for BERT Base, Table 5\nshows the MNLI results, and Table 6 shows the\nYelp results.\nC WC MC BC S MS MV\nTest Dataset\nClothes (C)\nWomen's C\nMen's C\nBaby's C\nShoes\nMusic\nMovies\nTrain Dataset\n52\n54\n52\n55\n52\n50\n53\n62 58 54 53 43 46\n61 53 52 51 44 46\n60 55 53 53 42 46\n52 54 50 52 42 45\n52 53 52 54 44 46\n48 50 48 49 48 52\n47 49 48 50 46 47\nGeneralization of BERT Base on\nAmazon Product Reviews\nFigure 6: We ﬁnetune BERT Base on one category\nof Amazon reviews and then evaluate it on other cat-\negories. Models predict the review’s star rating with\n5-way classiﬁcation. We use ﬁve clothing categories:\nClothes (C), Women’s Clothing (WC), Men’s Clothing\n(MC), Baby Clothing (BC), and Shoes (S); and two\nentertainment categories: Music (MS), Movies (MV).\nBERT is robust for closely related categories such as\nmen’s, women’s, and baby clothing. However, BERT\nstruggles when there is an extreme distribution shift\nsuch as Baby Clothing to Music (dark blue region).\nNote this shift is closer to a domain adaptation setting.\nA.3 OOD Detection\nFull FAR95 values are in Table 7. We also report\nthe Area Under the Receiver Operating Charac-\nteristic (AUROC) (Hendrycks and Gimpel, 2017).\nThe AUROC is the probability that an OOD ex-\nample receives a higher anomaly score than an\nin-distribution example, viz.,\nP(−max\ny\np(y |xout) > −max\ny\np(y |xin)).\nA ﬂawless AUROC is 100% while 50% is random\nchance. These results are in Figure 7 and Table 8.\nTrain Test BoW Average\nword2vec\nLSTM\nword2vec\nConvNet\nword2vec\nAverage\nGloVe\nLSTM\nGloVe\nConvNet\nGloVe\nBERT\nBase\nBERT\nLarge\nRoBERTa\nImages Images 39.7 61.4 75.7 81.8 61.2 79.8 81.8 91.8 92.8 94.2\nMSRvid 4.4 (-35.4) 11.3 (-50.1) 38.3 (-37.4) 62.0 (-19.8) 6.1 (-55.2) 43.1 (-36.7) 57.8 (-24.1) 89.5 (-2.3) 90.5 (-2.3) 94.3 (0.1)\nMSRvid MSRvid 60.7 68.7 85.9 85.0 66.8 85.6 87.4 92.4 93.9 94.9\nImages 19.3 (-41.4) 23.7 (-44.9) 45.6 (-40.2) 54.3 (-30.7) 11.1 (-55.7) 49.0 (-36.6) 51.9 (-35.4) 85.8 (-6.6) 86.8 (-7.1) 90.4 (-4.6)\nHeadlines Headlines 26.8 58.9 66.2 67.4 53.4 69.9 69.6 87.0 88.3 91.3\nMSRpar 10.1 (-16.7) 19.1 (-39.7) -1.9 (-68.1) 9.8 (-57.6) 25.9 (-27.5) 25.4 (-44.5) 10.9 (-58.7) 69.9 (-17.1) 63.6 (-24.7) 75.5 (-15.8)\nMSRpar MSRpar 47.0 27.0 46.7 49.8 50.9 46.7 46.2 78.8 81.6 86.8\nHeadlines -9.7 (-56.7) 12.7 (-14.4) 10.3 (-36.5) 23.7 (-26.1) 7.0 (-43.9) 15.6 (-31.1) 30.6 (-15.6) 73.0 (-5.8) 71.7 (-9.9) 83.9 (-2.9)\nTable 2: We train and test models on different STS-B distributions (Images, MSR videos, Headlines, and MSR\nparaphrase). The severe drop in the Pearson correlation coefﬁcient shows the consequence of a distribution shift.\nModels such as Average GloVe lose nearly all performance when out-of-distribution. RoBERTa does especially\nwell in comparison to other models.\nTrain Test Document QA DistilBERT BERT Base BERT Large RoBERTa\nCNN CNN 39.0 45.0 53.2 67.2 71.5\nDailyMail 29.7 (-9.3) 34.8 (-10.2) 46.7 (-6.6) 59.8 (-7.4) 72.2 (0.7)\nDailyMail DailyMail 30.8 36.7 48.2 61.2 73.0\nCNN 36.9 (6.2) 43.9 (7.2) 51.8 (3.6) 65.5 (4.3) 73.0 (0.0)\nTable 3: For ReCoRD, the exact match performance is closely tethered to the test dataset, which suggests a dif-\nference in the difﬁculty of the two test sets. This gap can be bridged by larger Transformer models pretrained on\nmore data.\nTrain Test BoW Average\nword2vec\nLSTM\nword2vec\nConvNet\nword2vec\nAverage\nGloVe\nLSTM\nGloVe\nConvNet\nGloVe\nBERT\nBase\nBERT\nLarge\nRoBERTa\nSST SST 80.6 81.4 87.5 85.3 80.3 87.4 84.8 91.9 93.6 95.6\nIMDb 73.9 (-6.8) 76.4 (-5.0) 78.0 (-9.5) 81.0 (-4.4) 74.5 (-5.8) 82.1 (-5.3) 81.0 (-3.8) 87.5 (-4.4) 88.3 (-5.3) 92.8 (-2.8)\nIMDb IMDb 85.9 84.8 89.9 91.0 83.5 91.3 91.0 91.8 92.9 94.3\nSST 78.3 (-7.6) 68.5 (-16.3) 63.7 (-26.3) 83.0 (-8.0) 77.5 (-6.1) 79.9 (-11.4) 80.0 (-10.9) 87.6 (-4.3) 88.6 (-4.3) 91.0 (-3.4)\nTable 4: We train and test models on SST-2 and IMDB. Notice IID accuracy is not perfectly predictive of OOD\naccuracy, so increasing IID benchmark performance does not necessarily yield superior OOD generalization.\nTrain Test DistilBERT BERT Base BERT Large RoBERTa\nTelephone\nTelephone 77.5 81.4 84.0 89.6\nLetters 75.6 (-1.9) 82.3 (0.9) 85.1 (1.0) 90.0 (0.4)\nFace-to-face 76.0 (-1.4) 80.8 (-0.7) 83.2 (-0.8) 89.4 (-0.2)\nTable 5: We train models on the MNLI Telephone dataset and test on the Telephone, Letters, and Face-to-face\ndatasets. The difference in accuracies are quite small (and sometimes even positive) for all four models. This\ndemonstrates that pretrained Transformers can withstand various types of shifts in the data distribution.\nTrain Test BoW Average\nword2vec\nLSTM\nword2vec\nConvNet\nword2vec\nAverage\nGloVe\nLSTM\nGloVe\nConvNet\nGloVe\nDistilBERT BERT\nBase\nBERT\nLarge\nRoBERTa\nAM\nAM 87.2 85.6 88.0 89.6 85.0 88.0 91.2 90.0 90.8 91.0 93.0\nCH 82.4 (-4.8) 80.4 (-5.2) 87.2 (-0.8) 88.6 (-1.0) 75.1 (-9.9) 88.4 (0.4) 89.6 (-1.6) 91.8 (1.8) 91.0 (0.2) 90.6 (-0.4) 90.8 (-2.2)\nIT 81.8 (-5.4) 82.6 (-3.0) 86.4 (-1.6) 89.4 (-0.2) 82.0 (-3.0) 89.2 (1.2) 89.6 (-1.6) 92.6 (2.6) 91.6 (0.8) 91.2 (0.2) 91.8 (-1.2)\nJA 84.2 (-3.0) 86.0 (0.4) 89.6 (1.6) 89.4 (-0.2) 79.2 (-5.8) 87.8 (-0.2) 89.2 (-2.0) 92.0 (2.0) 92.0 (1.2) 92.2 (1.2) 93.4 (0.4)\nCH\nCH 82.2 84.4 87.6 88.8 84.4 89.2 89.0 90.2 90.4 90.8 92.4\nAM 82.2 (0.0) 85.4 (1.0) 88.0 (0.4) 89.2 (0.4) 83.0 (-1.4) 85.6 (-3.6) 90.2 (1.2) 90.6 (0.4) 88.8 (-1.6) 91.8 (1.0) 92.4 (0.0)\nIT 84.6 (2.4) 82.0 (-2.4) 88.0 (0.4) 89.6 (0.8) 84.6 (0.2) 88.6 (-0.6) 90.4 (1.4) 91.4 (1.2) 89.0 (-1.4) 90.2 (-0.6) 92.6 (0.2)\nJA 83.8 (1.6) 85.8 (1.4) 88.6 (1.0) 89.0 (0.2) 86.8 (2.4) 88.8 (-0.4) 89.6 (0.6) 91.6 (1.4) 89.4 (-1.0) 91.6 (0.8) 92.2 (-0.2)\nIT\nIT 87.2 86.8 89.6 90.8 86.2 89.6 90.8 92.4 91.6 91.8 94.2\nAM 85.4 (-1.8) 83.8 (-3.0) 89.0 (-0.6) 90.2 (-0.6) 85.6 (-0.6) 89.0 (-0.6) 90.2 (-0.6) 90.4 (-2.0) 90.6 (-1.0) 89.4 (-2.4) 92.0 (-2.2)\nCH 79.6 (-7.6) 81.6 (-5.2) 83.8 (-5.8) 88.4 (-2.4) 78.0 (-8.2) 83.2 (-6.4) 85.8 (-5.0) 90.4 (-2.0) 89.6 (-2.0) 90.0 (-1.8) 92.4 (-1.8)\nJA 82.0 (-5.2) 84.6 (-2.2) 87.4 (-2.2) 88.6 (-2.2) 85.0 (-1.2) 86.8 (-2.8) 89.4 (-1.4) 91.8 (-0.6) 91.4 (-0.2) 91.2 (-0.6) 92.2 (-2.0)\nJA\nJA 85.0 87.6 89.0 90.4 88.0 89.0 89.6 91.6 92.2 93.4 92.6\nAM 83.4 (-1.6) 85.0 (-2.6) 87.8 (-1.2) 87.8 (-2.6) 80.4 (-7.6) 88.6 (-0.4) 89.4 (-0.2) 91.2 (-0.4) 90.4 (-1.8) 90.6 (-2.8) 91.0 (-1.6)\nCH 81.6 (-3.4) 83.6 (-4.0) 89.0 (0.0) 89.0 (-1.4) 80.6 (-7.4) 87.4 (-1.6) 89.2 (-0.4) 92.8 (1.2) 91.4 (-0.8) 90.8 (-2.6) 92.4 (-0.2)\nIT 84.0 (-1.0) 83.6 (-4.0) 88.2 (-0.8) 89.4 (-1.0) 83.6 (-4.4) 88.0 (-1.0) 90.6 (1.0) 92.6 (1.0) 90.2 (-2.0) 91.0 (-2.4) 92.6 (0.0)\nTable 6: We train and test models on American (AM), Chinese (CH), Italian (IT), and Japanese (JA) restaurant\nreviews. The accuracy drop is smaller compared to SST-2/IMDb for most models and pretrained transformers are\ntypically the most robust.\nDin Dtest\nout BoW Avg\nw2v\nAvg\nGloVe\nLSTM\nw2v\nLSTM\nGloVe\nConvNet\nw2v\nConvNet\nGloVe\nDistilBERT BERT\nBase\nBERT\nLarge\nRoBERTa\nSST\n20 NG 100 100 100 94 90 61 71 39 35 29 22\nMulti30K 61 57 52 92 85 65 63 37 22 23 61\nRTE 100 100 84 93 88 75 56 43 32 29 36\nSNLI 81 83 72 92 82 63 63 38 28 28 29\nWMT16 100 91 77 90 82 70 63 56 48 44 65\nMean FAR95 88.4 86.2 76.9 92.2 85.4 66.9 63.1 42.5 33.0 30.5 43.0\nTable 7: Out-of-distribution detection FAR95 scores for various NLP models using the maximum softmax prob-\nability anomaly score. Observe that while pretrained Transformers are consistently best, there remains room for\nimprovement.\n20 NG Multi30K RTE SNLI WMT16 Average\n0\n20\n40\n60\n80\n100\nAUROC (%)\n(Higher Is Better)\nDetecting OOD Examples for an SST-2 Sentiment Classifier\nModel Type\nRandom Detector\nBag of Words\nAvg. word2vec\nLSTM word2vec\nConvNet word2vec\nBERT Large\nFigure 7: We feed in OOD examples from out-of-distribution datasets (20 Newsgroups, Multi30K, etc.) to SST-2\nsentiment classiﬁers and report the AUROC detection performance. A 50% AUROC is the random chance level.\nDin Dtest\nout BoW Avg\nw2v\nAvg\nGloVe\nLSTM\nw2v\nLSTM\nGloVe\nConvNet\nw2v\nConvNet\nGloVe\nDistilBERT BERT\nBase\nBERT\nLarge\nRoBERTa\nSST\n20 NG 17 19 30 44 59 74 64 82 83 87 90\nMulti30K 77 75 80 55 62 71 73 86 93 91 89\nRTE 63 47 72 36 54 61 77 83 89 89 90\nSNLI 56 58 71 53 64 72 74 86 92 90 92\nWMT16 58 60 69 58 63 69 74 80 85 85 83\nMean AUROC 54.2 51.8 64.5 49.3 60.4 69.5 72.5 83.1 88.1 88.4 88.7\nTable 8: Out-of-distribution detection AUROC scores for various NLP models using the maximum softmax proba-\nbility anomaly score. An AUROC score of 50% is random chance, while 100% is perfect."
}