{
  "title": "Self-Supervised Monocular Depth Estimation Using Hybrid Transformer Encoder",
  "url": "https://openalex.org/W4292974688",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2163333237",
      "name": "Seung Jun Hwang",
      "affiliations": [
        "Korea Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2103465463",
      "name": "Sung Jun Park",
      "affiliations": [
        "Korea Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2648578700",
      "name": "Joong-Hwan Baek",
      "affiliations": [
        "Korea Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2123549083",
      "name": "Byungkyu Kim",
      "affiliations": [
        "Korea Aerospace University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3118635606",
    "https://openalex.org/W3175682855",
    "https://openalex.org/W3048510980",
    "https://openalex.org/W4206764080",
    "https://openalex.org/W3153295141",
    "https://openalex.org/W3034604951",
    "https://openalex.org/W3107389224",
    "https://openalex.org/W3180562345",
    "https://openalex.org/W3110440461",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W3207743087",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6797360341",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W4285238125",
    "https://openalex.org/W3081442134",
    "https://openalex.org/W2985775862",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W6798046796",
    "https://openalex.org/W3017059628",
    "https://openalex.org/W2300779272",
    "https://openalex.org/W2520707372",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2934279571",
    "https://openalex.org/W2981518351",
    "https://openalex.org/W6782218002",
    "https://openalex.org/W3182891170",
    "https://openalex.org/W2609883120",
    "https://openalex.org/W6780320512",
    "https://openalex.org/W3034428934",
    "https://openalex.org/W4206545998",
    "https://openalex.org/W6802066719",
    "https://openalex.org/W6771531989",
    "https://openalex.org/W2969365860",
    "https://openalex.org/W2982102242",
    "https://openalex.org/W6785345353",
    "https://openalex.org/W3205269569",
    "https://openalex.org/W3060975791",
    "https://openalex.org/W1772650917",
    "https://openalex.org/W2963619659",
    "https://openalex.org/W6758510812",
    "https://openalex.org/W4212860834",
    "https://openalex.org/W4312572946",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W6685261749",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W2950557037",
    "https://openalex.org/W3097932847",
    "https://openalex.org/W3205829517",
    "https://openalex.org/W4287863801",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W3036562632"
  ],
  "abstract": "Depth estimation using monocular camera sensors is an important technique in computer vision. Supervised monocular depth estimation requires a lot of data acquired from depth sensors. However, acquiring depth data is an expensive task. We sometimes cannot acquire data due to the limitations of the sensor. View synthesis-based depth estimation research is a self-supervised learning method that does not require depth data supervision. Previous studies mainly use the convolutional neural network (CNN)-based networks in encoders. The CNN is suitable for extracting local features through convolution operation. Recent vision transformers (ViTs) are suitable for global feature extraction based on multiself-attention modules. In this article, we propose a hybrid network combining the CNN and ViT networks in self-supervised learning-based monocular depth estimation. We design an encoder–decoder structure that uses CNNs in the earlier stage of extracting local features and a ViT in the later stages of extracting global features. We evaluate the proposed network through various experiments based on the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) and Cityscapes datasets. The results showed higher performance than previous studies and reduced parameters and computations. Codes and trained models are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/fogfog2/manydepthformer</uri> .",
  "full_text": "18762 IEEE SENSORS JOURNAL, VOL. 22, NO. 19, 1 OCTOBER 2022\nSelf-Supervised Monocular Depth Estimation\nUsing Hybrid T ransformer Encoder\nSeung-Jun Hwang , Sung-Jun Park , Joong-Hwan Baek , and Byungkyu Kim\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nAbstract — Depth estimation using monocular camera sen-\nsors is an important technique in computer vision. Supervised\nmonocular depth estimation requires a lot of data acquired\nfrom depth sensors. However, acquiring depth data is an\nexpensive task. We sometimes cannot acquire data due to\nthe limitations of the sensor. View synthesis-based depth\nestimation research is a self-supervised learning method that\ndoes not require depth data supervision. Previous studies\nmainly use the convolutional neural network (CNN)-based\nnetworks in encoders. The CNN is suitable for extracting local\nfeatures through convolution operation. Recent vision transformers (ViTs) are suitable for global feature extraction based\non multiself-attention modules. In this article, we propose a hybrid network combining the CNN and ViT networks in\nself-supervised learning-based monoculardepth estimation. We design an encoder–decoder structure that uses CNNs\nin the earlier stage of extracting local features and a ViT in the later stages of extracting global features. We evaluate the\nproposed network through various experiments based on the Karlsruhe Institute of Technology and Toyota Technological\nInstitute (KITTI) and Cityscapes datasets. The results showed higher performance than previous studies and reduced\nparameters and computations. Codes and trained models are available at https://github.com/fogfog2/manydepthformer.\n18 Index Terms— Depth estimation, monocular sensor estimation,self-attention, self-supervised, transformer.\nI. I NTRODUCTION19\nD\nEPTH estimation is an important technology used for20\nvarious ﬁelds that require 3-D spatial map generation,21\nsuch as autonomous driving, augmented reality (AR), vir-22\ntual reality (VR), and robot vision. The conventional sensor-23\nbased depth estimation method has high accuracy but has24\nproblems of high price and lowresolution. Recently, camera-25\nbased depth estimation research has been actively conducted,26\nand the performance is also high enough to be applied to27\ncommercial products such as drones. The traditional stereo28\ncamera-based method estimates the depth by calculating the29\ndisparity between two calibrated cameras. A recent monocular30\ncamera-based method estimates disparity between temporally31\nadjacent frames. Some monocular camera-based depth esti-32\nmation algorithms are trained with self-supervision to avoid33\ndifﬁculties in acquiring depth data.34\nManuscript received 19 July 2022; revised 11 August 2022; accepted\n11 August 2022. Date of publication 22 August 2022; date of current\nversion 30 September 2022. This work was supported by the 2020 Korea\nAerospace University Faculty Research under Grant 202020660001. The\nassociate editor coordinating the review of this article and approving\nit for publication was Prof. Yu-Dong Zhang.\n(Corresponding author:\nJoong-Hwan Baek.)\nSeung-Jun Hwang, Sung-Jun Park, and Joong-Hwan Baek are with\nthe School of Electronics and Information Engineering, Korea Aerospace\nUniversity, Goyang 10540, Republic of Korea (e-mail: fogfog2@kau.kr;\ntjdwns1011@naver.com; jhbaek@kau.ac.kr).\nByungkyu Kim is with the School of Aerospace and Mechanical\nEngineering, Korea Aerospace University, Goyang 10540, Republic of\nKorea (e-mail:bkim@kau.ac.kr).\nDigital Object Identiﬁer 10.1109/JSEN.2022.3199265\nThe monocular camera-based depth estimation study is 35\ndivided into a supervised learning-based method and an unsu-36\npervised learning-based method. Supervised learning-based 37\nnetworks directly learn depth data obtained from sensors. The38\nunsupervised learning-based method estimates depth based on39\na viewpoint synthesis technique that reconstructs the current 40\nimage from temporally adjacent consecutive frames [1]. The 41\nsupervised learning-based method outputs higher performance 42\nthan unsupervised learning because it learns directly from 43\ndepth data. However, acquiring 3-D data is an expensive oper-44\nation. When 3-D data cannot be acquired, or a depth sensor 45\nis not available, unsupervised learning is the only solution. 46\nRecently, research on unsupervised learning-based models is 47\nbeing actively conducted in various ﬁelds such as automobile48\nautonomous driving [2], the smartphone-based AR system [3],49\nthe drone avoidance system [4], and the medical system [5]. 50\nHowever, there is a problem in that the unsupervised learn- 51\ning method based on the monocular camera cannot estimate 52\nthe absolute distance of the depth. Therefore, the output of 53\nunsupervised learning depth estimation is limitedly applicable 54\nto ﬁelds that are available with relative depth. Some studies 55\nadditionally train velocity to estimate absolute depth or use 56\ninertial measurement unit (IMU) sensors [6]. 57\nIn most cases, the convolutional neural network (CNN)- 58\nbased backbone was the basic model of the existing deep- 59\nlearning-based monocular camera depth estimation. This was 60\ntrue not only in the ﬁeld of depth estimation [7], but also 61\nin the ﬁeld of image classiﬁcation [8], segmentation [9], and62\ndetection [10]. Two CNN-based residual network (ResNet) 63\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nHWANG et al.: SELF-SUPERVISED MONOCULAR DEPTH ESTIMATION USING HYBRID TRANSFORMER ENCODER 18763\n[11] and EfﬁcientNet [12] are widely used as the main back-64\nbone models for deep-learning networks. Convolution opera-65\ntion represents global features map in a structure that increases66\nthe receptive area by overlapping deep layers based on the67\nassociation of local pixels. This structure has traditionally been68\na general structure with state-of-the-art performance.69\nTransformer is a recently popular backbone network in the70\nﬁeld of depth estimation [13], [14]. The original transformer71\nis a model proposed in the ﬁeld of natural language processing72\nto predict sequences. Some transformer models showed high73\nperformance in the natural language processing ﬁeld, and74\nthe model applied to the computer vision ﬁeld is the vision75\ntransformer (ViT) [15]. The ViT applied the same format used76\nfor natural language processing to the vision ﬁeld. It consists77\nof a multiself-attention mechanism and a multilayer perceptron78\n(MLP). In ViT, multiself-attention extracts global features,79\nand MLP expresses local features just like CNNs. A recent80\nViT model has achieved the highest performance in image81\nclassiﬁcation, segmentation, and detection [16], [17]. However,82\nthe ViT outperforms the CNN-based backbone when there is83\na lot of training data, and it is necessary to solve the problem84\nof the high computational amount of the multiself-attention85\nmechanism.86\nIn an early study applying a ViT to supervised learning87\nof monocular camera depth estimation, a hybrid model that88\nmixed CNN-based ResNet and ViT models showed the best89\nperformance [14]. A recent study improved the depth estima-90\ntion performance by using a Swin transformer applied with91\na hierarchical encoder–decoder and multiscale fusion atten-92\ntion [13]. In this article, following the above studies, we design93\nan efﬁcient self-supervised depth estimation network using94\nlightweight attention (LA). Additionally, the proposed network95\nshows higher performance by using the expert layer for each96\nencoder layer depth.97\nIn this article, we propose a hybrid transformer-based depth98\nestimation network for effective depth network estimation in99\nan unsupervised depth estimation model. First, the proposed100\nnetwork consists of a cost-volume structure for depth and101\na pose network for estimating poses between temporally102\nadjacent images. Next, the feature extractor transforms input103\nimages into feature maps. The feature map of the source104\nimage is synthesized with the viewpoint of the target image.105\nIn this feature map view synthesis process, the predicted pose,106\nassumed depth, and camera parameters are used. The cost107\nvolume is generated by accumulating the difference between108\nthe source feature maps synthesized for each depth and the109\ntarget feature map. The target feature map and cost volume110\nare concatenated. The combinedfeatures are transformed into111\ndepth in the proposed hybrid encoder–decoder. Encoders are112\nconnected to layers of networks with different properties.113\nIn the early layer of the encoder, a CNN-based ResNet114\nbackbone is used for local representation. The later layers115\nuse transformers for global representation. We apply a light-116\nweight transformer model to solve the problem of the high117\ncomputational complexity of multiself-attention mechanisms.118\nThe lightweight transformer reduces the number of parameters119\nand operations compared to the ResNet layer. We can design120\na more efﬁcient network by applying a lightweight model.121\nIn the decoder, channel-space attention is additionally applied122\nto the commonly used multiscale confusion module to improve123\nthe depth estimation performance. 124\nThe structure of this article is as follows. Section II inves-125\ntigates related studies, backbone networks, and unsupervised 126\ndepth estimation. Section III describes the structure of the pro-127\nposed overall network, and Section IV shows the experimental128\nresults of applying the proposed method to the Karlsruhe 129\nInstitute of Technology and Toyota Technological Institute 130\n(KITTI) and Cityscapes datasets. The conclusion of the pro-131\nposed method is mentioned in Section V. 132\nII. R ELA TEDWORK 133\nA. CNN and Transformer 134\nSince the proposal of AlexNet [18], the CNN has been 135\nused as the main backbone network in the ﬁeld of computer136\nvision. Various models have been studied, such as visual 137\ngeometry group (VGG) [19], ResNet [11], MobileNet [20], 138\nand EfﬁcientNet [12]. VGG analyzed the effect of the depth 139\nof the network, and ResNet proposed a residual network that140\nmerges the input with the output. MobileNet improves network141\nefﬁciency with depthwise convolution and an inverted resid- 142\nual block. EfﬁcientNet improves performance by compound 143\nscaling that determines the depth, width, and size of the input144\nimage of the network. Recent studies use the above general 145\nCNN-based backbone networks with some modiﬁcations [7], 146\n[8]. An ensemble model is also used for optimization [21]. 147\nIn particular, ResNet improves the learning speed and 148\ntraining effect of the network without signiﬁcantly increas- 149\ning parameters and computations with a shortcut structure 150\n[11], [22]. Also, even when the number of model layers 151\nincreases, gradient vanishing is prevented by the effect of 152\nskip connection. In the ﬁeld of depth estimation, the ResNet153\nbackbone is mainly used following the main contribution paper154\nMonodepth2 [23]. Following the previous works, we construct155\nan efﬁcient hybrid network based on ResNet. 156\nTransformer is a model that showed good performance 157\nin the ﬁeld of natural language processing. ViT is an early 158\nViT model that showed the best performance by applying 159\na transformer in the image classiﬁcation ﬁeld. Data-efﬁcient 160\nimage transformers (DeiT) [24] proposed a method for knowl-161\nedge distillation of the CNN-based network results on a 162\ntransformer and applied various data augmentation techniques.163\nSwin transformer proposes a hierarchical feature map that 164\nreduces image resolution by patch mixing for each stage. With165\nthe proposal of the hierarchical feature map, it became possible166\nto apply the transformer not only to the classiﬁcation ﬁeld, 167\nbut also to the image detection and segmentation ﬁeld [16]. 168\nIn addition, the Swin transformer proposes a cyclic shifting 169\nwindow to improve local feature expression performance. 170\nAfter that, convolutions to vision transformers (CvT) [25] 171\nremoves the limitation of nonoverlapping patch unit embed- 172\ndings by using convolution for token embedding. A recent 173\nstudy, CNNs meet Vision transformers (CMT) [26], proposed174\nlit multihead self-attention that reduces the spatial resolution of175\nkeys and values. In addition, an inverted residual feed-forward176\nnetwork (IRFFN) based on depthwise separable convolution 177\nis used instead of a feed-forward network to improve local 178\nrepresentation. 179\n18764 IEEE SENSORS JOURNAL, VOL. 22, NO. 19, 1 OCTOBER 2022\nB. Self-Supervised Monocular Depth Estimation180\nAlthough the supervised learning method shows relatively181\ngood performance in the ﬁeld of monocular camera depth182\nestimation, in a recent study, theunsupervised learning method183\nalso shows comparable performance [27]. The unsupervised184\nlearning model is a depth estimation method that can be easily185\napplied to images for which it is not easy to acquire depth data.186\nGarg et al. [28] proposed a viewpoint synthesis technique187\nfor unsupervised learning-based depth estimation in stereo188\nimages. This method requires stereo pair images for train-189\ning, but the network estimates the depth in a single frame190\nat test time. A reconstructed left image is generated using191\nthe geometric constraint between the right image and the192\nleft depth image estimated from the depth network. The193\nreconstruction error between the reconstructed left image194\nand the left image is used as a loss function of the depth195\nnetwork. Godard et al. [29] applied the spatial transformer196\nnetwork (STN) [30] to sampling as an image reconstruction197\nmethod, and they also proposed a photometric loss combining198\nstructural similarity index measure (SSIM) [31] andL1 loss.199\nTosi et al. [32] and Watsonet al. [33] proposed a semi-global200\nmatching-based depth estimation method that can generate201\ndifferent depths according tohyperparameters. Gonzalez and202\nKim [34] proposed an occlusion module and quantiﬁed dispar-203\nity volume to improve geometric dependence. Their follow-up204\nstudy [35] proposed neural positional encoding and distilled205\nmatting loss to improve the pixel-level depth estimation per-206\nformance. However, the above methods have a problem in that207\na calibrated image pair must exist at training time.208\nZhou et al. [36] proposed a network that simultaneously209\nestimates depth and ego-motion from adjacent monocular210\nimages. They proposed viewpoint synthesis to reconstruct211\nimages with predicted camera poses and depths. They also212\nused masks to improve the explainability of the model.213\nGodard et al. [23] proposed the minimum reprojection loss214\nusing the minimum loss instead of the average in the cal-215\nculation of photometric loss with adjacent images, which216\nreduced the artiﬁcial structure of the image boundary and217\nimproved the sharpness of the occlusion boundary. They218\nalso proposed multiscale prediction to prevent the training219\ntarget from being trapped in local gradient descent by binary220\nsampling. Recent studies apply self-attention to global fea-221\nture extraction [37], [38], [39], [40]. In addition, several222\nstudies improve performance by adding additional semantic223\ninformation or motion information learning networks [41],224\n[42], [43], [44]. In addition, multiframe-based evaluation is225\nperformed to further utilize geometric information [2], [37],226\n[42], [45], [46].227\nRecent stereo-based depth estimation studies use cost vol-228\numes [47], [48], [49]. Imet al. [49] used warping transform229\nto generate cost volume so that it can be used for unrectiﬁed230\nstereo and multiimages. Recently, Watsonet al. [2] proposed231\na cost-volume-based depth estimation that can be used in a232\nsequence of images using pose estimation. They also proposed233\nan adaptive cost volume, which made it possible to learn a234\nminimum maximum depth from the data. In addition, they235\nused a single image-based depth network as a teacher model,236\nreducing errors for moving objects in cost volumes.237\nThere is also a recent study using transformers for self- 238\nsupervised depth estimation. Varmaet al. [50] conﬁgured the 239\nnetwork to learn camera parameters and compared the network240\naccording to the use of the CNN or transformer, respectively.241\nGuizilini et al. [51] proposed a method to generate the cost 242\nvolume from a cross-attention-based transformer network. 243\nHowever, it is necessary to use an additional network instead244\nof the simple difference operation in previous studies. 245\nRecent studies use a transformer for depth estimation and 246\nshow good performance, but use a high amount of computation247\nand parameters. In this study, we propose a hybrid network 248\nthat is more efﬁcient than existing CNN-based networks by 249\nhierarchically mixing CNNs and transformers. 250\nIII. M ETHOD 251\nIn this chapter, we describe the proposed hybrid 252\ntransformer-based self-supervised learning depth estimation 253\nmethod. First, the viewpoint synthesis method of the self- 254\nsupervised learning model and cost-volume-based depth esti- 255\nmation methodology are reviewed. This review describes the256\nequations and geometric models used in the proposed model.257\nThen, the hybrid encoder–decoder network proposed in this 258\narticle will be described. The overall block diagram of the 259\nproposed structure is shown inFig. 1. 260\nA. View-Synthesis Model Based on Self-Supervised 261\nLearning 262\nIn this article, depth and pose networks are simultane- 263\nously trained for unsupervised learning-based depth estima- 264\ntion according to a recent study [2], [6], [23]. The network 265\nis trained through a view synthesis process that minimizes 266\nthe photometric error between the target image It and the 267\nreconstructed target image ˆIs→t from the source image Is 268\nto the target image viewpoint. The reconstructed image is 269\nsampled from the source image using the 2-D homogeneous270\ncoordinates obtained by projection using the predicted target 271\ndepth and the predicted pose. At this time, the predicted depth272\nDt and pose Pt→s are estimated by each network, and the 273\ncamera parameterK is input. The viewpoint synthesis process274\nfor generating the reconstructed image is as follows: 275\nˆIs→t = Is \u0003proj (Dt , Pt→s , K )\u0004 (1) 276\nwhere proj is the camera projection operation and\u0003\u0004 is the 277\nbinary sampling operation using STN [30]. 278\nThe photometric error pe is combined with theL1 distance 279\nand SSIM [31], which is the degree of similarity. The image280\nreconstruction loss is as follows: 281\npe\n(\nIt , ˆIs→t\n)\n=\na\n(\n1 − SSIM\n(\nIt , ˆIs→t\n))\n2\n282\n+ (1 − a)\n⏐⏐\n⏐\n⏐⏐\n⏐I\nt − ˆIs→t\n⏐⏐\n⏐\n⏐⏐\n⏐\n1\n(2) 283\nwhere a is the balancing weight and SSIM is a method of 284\nevaluating the quality of the images. 285\nThe source image consists of temporally adjacent frames 286\nof the target image. The reconstructed target image from the287\nsource image depends on the number of adjacent frames. 288\nHWANG et al.: SELF-SUPERVISED MONOCULAR DEPTH ESTIMATION USING HYBRID TRANSFORMER ENCODER 18765\nFig. 1. Hybrid encoder–decoder-based depth estimation structure.\nCamera movement or the presence of an occluded area are289\nfactors that increase image reconstruction loss. To reduce290\nerrors due to camera movement and occlusion area, a value291\nwith the smallest error is selected among several adjacent292\nsource images [23]. Equation (3) shows the formula for the293\nminimum reconstruction loss error294\nL p = min\nS\npe\n(\nIt , ˆIs→t\n)\n, S ∈ [s1,s2,..., sn ] . (3)295\nB. Cost-Volume-Based Depth Estimation296\nThe recent cost-volume-based depth estimation of unsu-297\npervised learning proceeds in the following order. First, fea-298\nture points are extracted from input images by a feature299\nextractor. Next, the feature points of the source image are300\nview-synthesized according to the assumed distance from the301\nminimum to the maximum, and the cost volume is generated302\nby calculating the similarity between the source and target303\nfeature points. The cost volume is aggregated in the encoder–304\ndecoder and converted to the disparity. Finally, view synthesis305\nis performed with the predicted disparity. In this study, we per-306\nform cost volume-based depth estimation according to recent307\nstudies [2], [49].308\nThe feature extractor that extracts the feature map from the309\ninput images uses the ﬁrst two layers of ResNet. The feature310\nextractor encodes the target image and the source image into311\nfeature maps Ft and Fs . At this time, the spatial resolution312\nof the feature map is reduced to 1/4. The source feature313\nmap Fs is synthesized into the reconstruction target feature314\nmap Fd\ns→t for each hypothesized depth by the feature map315\nview synthesis process. The feature map viewpoint synthesis316\nformula is written as317\nFd\ns→t = Fs \u0003proj (d, Pt→s , K )\u0004, d ∈ [dmin : dmax]( 4 )318\nwhere the symbols are the same as in (1).dmin and dmax are319\nthe minimum and maximum depths, respectively, and the step320\nis adjusted according to the number of channels in the cost321\nvolume. To generate the cost volume, the above process is322\nperformed for each depth unit of the depth cost volume, and323\nthe L1 distance of the feature mapFt for each unit and the 324\nreconstruction target feature mapFd\ns→t is input to each channel325\nof the depth cost volume. 326\nA cost-volume-based depth estimation that receives multiple327\nframes generally works better than single frame estimation. 328\nHowever, the existence of an object moving in the same 329\ndirection as the camera or a textureless region is a major 330\ncause of failure in cost-volume depth estimation. Because 331\nthe cost-volume-based depth estimation uses the difference 332\nin feature points as a learning factor, the depth estimation 333\nfails when the depth difference cannot be known as above. 334\nTo solve this problem, a recent study uses a single input 335\nimage-based depth constraint network as a teacher model. 336\nThe network used as the teacher model is the baseline of 337\nexisting studies [6], [23]. TheL1 distance between the depth 338\nDt predicted by the cost volume and the depthˆDt predicted 339\nby the depth constraint network is added to the loss function,340\npreventing from excessively dependent on the disparity. The 341\ndepth constraint loss is written as 342\nLconstraint =\u0006 Dt − ˆDt \u00061. (5) 343\nAdditionally, an edge-aware term such as (6) that constrains344\nthe gradient of depth according to the gradient of the image345\nis added as in previous studies [4], [5], [6] 346\nLsmooth =| δx Dt |e−|δx It | +| δy Dt |e−|δy It |. (6) 347\nThe ﬁnal loss consists of reconstruction loss, depth con- 348\nstraint loss, and depth smoothness loss and is as follows: 349\nLﬁnal = αL p + βLconstraint + γ Lsmooth (7) 350\nwhere α, β,a n dγ are loss function scale correction weights. 351\nC. Hybrid Encoder and Self-Attention Decoder 352\nIn this article, we propose a novel encoder–decoder net- 353\nwork that transforms cost volume into depth. The proposed 354\nnetwork is efﬁcient in terms of parameters and the compu- 355\ntational amount and has high accuracy and low error metric. 356\n18766 IEEE SENSORS JOURNAL, VOL. 22, NO. 19, 1 OCTOBER 2022\nFig. 2. Structure of the hybrid encoder block.(a) LFB. (b) Residual block.\n(c) GFB.\nThe proposed encoder is constructed by hierarchically mixing357\nnetworks with different characteristics to improve local and358\nglobal feature representatio n performance. In the decoder,359\nan attention operation layer is added to improve the global360\nfeature representation dependency.361\n1) Hybrid Encoder: The output of the feature extractor and362\nthe cost volume are concatenated. This concatenated output363\nis input to the encoder–decoder network for depth estimation.364\nThe encoder reduces spatial resolution and extracts features at365\nmultiple scales.366\nSince the CNN-based model uses convolution operation,367\nit has the characteristic of expressing features for the adjacent368\nregion, and the transformer has the characteristic of expressing369\nthe entire region with its self-attention mechanism. Therefore,370\nin this study, we propose a hybrid encoder structure that371\nhierarchically connects convolution and the transformer.372\nConvolution suitable for local feature expression is applied373\nto the early layer of the encoder, and a transformer structure374\nsuitable for global feature expression is applied to the later375\nlayer by self-attention operation. The local feature block (LFB)376\nis composed of the existing ResNet block. The global fea-377\nture block (GFB) consists of a lightweight multiself-attention378\nmodule with halved keys, values, and an inverse residual379\nfeed-forward network [26]. Fig. 2 shows the detailed block380\ndiagram of the proposed hybrid encoder.381\nIn detail, the LFB is composed of two residual blocks of382\nexisting ResNet, and the ﬁrst residual block sets stride to 2 to383\nreduce the resolution. When an inputX ∈ RH×W ×C is given,384\nthe LFB process is as follows:385\nLFB (X) = Residual (Residual (X,s = 2)) (8)386\nwhere Residual is the existing ResNet block ands is the stride.387\nThe GFB uses a CMT block that mixes convolution and388\nthe transformer [26]. The CMT block consists of a local389\nperception unit (LPU), lightweight attention (LA), and IRFFN.390\nFirst, the LPU for extracting local features is composed of391\n3 × 3 depth-wise convolution and the sum of residuals. When392\nan input featureX is given, the LPU is as follows:393\nLPU (X) = DWConv(X) + X. (9)394\nNext, in the LA module, the 2-D input feature X ∈395\nRH×W ×C is ﬂattened to X ∈ RN×C for patch operation.396\nIn this,N is H × W. To reduce the computational complexity397\nof the attention operation, the key and the value reduce the 398\nspatial resolution with ak × k depth-wise convolution set by 399\nstride k, respectively. According to the existing self-attention 400\noperation, the query and the key are linearly transformed into401\nthe dk dimension and the value into the dv dimension. So, 402\neach is linearly transformed into a query Q ∈ RN×dk ,k e y 403\nK ∈ R(N/k2)×dk ,a n dv a l u eV ∈ R(N/k2)×dv . As in the recent 404\nstudy [16], a relative position biasB is added, and LA is as 405\nfollows: 406\nLA (Q, K, V ) = Softmax\n( QK T\n√\ndk\n+ B\n)\nV. (10) 407\nIRFFNs are similar to the structure of MobilNetV2 [52], 408\nbut the connection location of residuals has been changed. 409\nThe modiﬁed IRFFNs are as follows: 410\nIRFFN (X) = Conv (F (Conv (X)) 411\nF (X) = DWConv(X) + X. (11) 412\nFinally, the GFB is composed of each block and residual as413\nfollows: 414\nGFB (X) = L3 (L2 (L1 (X))) 415\nL1 (Xi−1) = X\b\ni = LPU (Xi−1) 416\nL2\n(\nX\b\ni\n)\n= X\b\b\ni = LA\n(\nLN\n(\nX\b\ni\n))\n+ X\b\ni 417\nL3\n(\nX\b\b\ni\n)\n= Xi = IRFFN\n(\nLN\n(\nX\b\b\ni\n))\n+ X\b\b\ni (12) 418\nwhere X\b\ni and X\b\b\ni are the outputs of LPU and LA, respectively,419\nand LN is layer normalization. Multiple GFBs are stacked in420\neach stage. 421\nThrough the feature extractor and hybrid encoder, the spatial422\nresolution of the entire feature map is reduced to Fn ∈ 423\nR(H/2n)×(W/2n)×Cn ,1 ≤ n ≤ 5. Here, n is each layer number, 424\ncomposed of two layers in the feature extractor and three 425\nlayers in the hybrid encoder. 426\n2) Attention Decoder: In the decoder, the spatial resolution 427\nof the multiscale feature map is gradually restored, and the 428\ndepth of the input resolution is estimated in units of pixels.429\nFirst, self-attention operations are performed on low-resolution430\nfeatures in order to increase the wide feature expression and431\nacceptance range [53]. The computed features are upscaled and432\nmerged with the higher-resolution feature maps. The detailed433\nblock diagram of the proposed self-attention decoder is shown434\nin Fig. 3, and the input and output sizes and detailed structures435\nof the entire network are shown inTable I. 436\nThe self-attention block consists of a channel attention 437\noperation and a spatial attention operation [53]. The self- 438\nattention operations consist of the 1-D channel self-attention 439\noperation Mc ∈ R1×1×C and the 2-D spatial self-attention 440\noperation Ms ∈ RH×W ×1 for the input featureX ∈ RH×W ×C . 441\nThe total self-attention process is as follows: 442\nX\b = Mc (X) ⊗ X 443\nX\b\b = Ms\n(\nX\b)\n⊗ X\b (13) 444\nwhere ⊗ is element-wise multiplication,X\b is the result of the 445\nchannel self-attention operation, andX\b\b is the ﬁnal result of 446\nthe attention operation. After the self-attention operation, each447\nHWANG et al.: SELF-SUPERVISED MONOCULAR DEPTH ESTIMATION USING HYBRID TRANSFORMER ENCODER 18767\nFig. 3. Structure of(a) attention block and(b) decoder block.\nTABLE I\nARCHITECTURE OF THE PROPOSED NETWORK\nfeature map is upsampled by a factor of 2 and concatenated448\nwith the feature map of the previous layer. The decoding449\nprocess is as follows:450\nFdecode\nn = conv1\n(\nupsample×2\n(\nFdecode\nn−1\n)\n+ Fn\n)\n451\ndispn = σ\n(\nconv2\n(\nFdecode\nn\n))\n(14)452\nwhere conv1 is a convolution for converting to a designated453\ndecoding channel, and conv2 is a convolution for converting454\nto channel size 1 for disparity extraction for each size output.455\nσ is an activation function. The ﬁnal disparities are output in456\nmultiple scales as dispn ∈ R(H/2n)×(W/2n)×1,0 ≤ n ≤ 4. Here,457\nn is the layer number, andn = 0 is the original resolution.458\nIV . EXPERIMENTS459\nThe performance of the proposed method is evaluated in460\nthe same way as in previous studies. Our experimental results461\nare evaluated based on the KITTI and Cityscapes datasets,462\nwhich mainly used in-depth estimation models based on self-463\nsupervised learning. In addition, an ablation study is performed464\nto evaluate the performance of each module of the proposed465\nmethodology.466\nA. Experimental Setup467\nThe experiment was performed in an NVIDIA RTX468\n3090 with a 24-GB memory hardware environment. The pro-469\nposed hybrid encoder model was built based on the Manydepth470\nmodel [2]. The depth and pose networks were trained with 471\n40 epochs, a batch size of 8, and a 2× 10−4 learning rate. 472\nFor weights, the ResNet layer ispretrained with ImageNet, 473\nand the transformer layer is trained from scratch. Unwritten 474\nparameters are used the same as the baseline study Manydepth.475\nB. Datasets 476\nThe KITTI dataset is data created for autonomous driving,477\ncaptured by four cameras and lidar sensors, and synchronized478\nwith each other [54]. We follow the data distribution proposed479\nby [55] with 39 810 for training, 4424 for evaluation, and 480\n697 for the test. 481\nThe Cityscapes dataset is a driving image dataset captured482\nwith a stereo camera [56]. Following [2], we use 69 731 images483\nof the left sequence as training data. We use triples according484\nto previous research. We evaluate the model with 1525 test 485\nimages. 486\nC. Evaluation Metrics 487\nTo compare the proposed depth estimation network with 488\nother modern networks, a commonly used evaluation metrics489\nwas used [2], [3], [4], [5], [6], [7], [13], [14], [29], [30], 490\n[31], [37], [38], [39], [40], [41], [42]. The ﬁve evaluation 491\nmethods used in the experiment are AbsRel, SqRel, root 492\nmean square error (RMSE), RMSE (log), and accuracy index.493\nIn the evaluation methods, AbsRel, SqRel, and RMSE are error494\nevaluation methods, the lower the better. Threshold accuracy495\nis the accuracy evaluation metrics, where the higher the better.496\nD. Comparison Study 497\nWe compare the proposed method with the existing state-of-498\nthe-art methods. Table II shows the quantitative performance 499\nevaluation on the KITTI dataset of the existing depth esti- 500\nmation models and the proposed hybrid-based model. Test 501\nframes indicate the number of frames used in the test time, and502\nthe numbers in parentheses−1, 0, and 1 mean the previous 503\nframe, the current frame, and the next frame, respectively. The504\nsemantics column shows whether segmentation networks or 505\nsemantic supervision are used. The T50 of the proposed model506\nmeans that the backbone of a single-depth constraint network507\nis changed from ResNet18 to ResNet50. 508\nThe proposed hybrid model shows higher performance in 509\nerror metrics than the existing models. The proposed method510\nshows higher performance than the single-frame-based esti- 511\nmation methods as well as the multiframe-based estimation 512\nmethods. In the accuracy evaluation of δ< 1.253,l o w e r513\nperformance was recorded thanthe model [41] using semantic 514\ninformation and a heavier PackNet backbone network [6]. 515\nHowever, the difference is very small, and the proposed 516\nmodel showed high performance in most evaluation indicators517\nwithout using semantic information. Furthermore, we found 518\nbetter performance when using ResNet50 as the backbone of519\na single-depth constrained network. 520\nTable III shows comparative experiments on cityscapes 521\ndatasets. Again, we show higher performance than previous 522\nstudies in all evaluation methods. 523\n18768 IEEE SENSORS JOURNAL, VOL. 22, NO. 19, 1 OCTOBER 2022\nTABLE II\nCOMPARISON OF THE PROPOSED METHOD ON THE KITTI EIGEN SPLIT (RESOLUTION 640 × 192)\nTABLE III\nCOMPARISON ON THE CITYSCAPES DATASET (RESOLUTION 416 × 128)\nFig. 4. Comparison of the proposed method on the KITTI eigen split.(a) Input. (b) Ours (T50).(c) Ours. (d) ManyDepth.\nThe resulting images for qualitative evaluation are shown524\nin Fig. 4. Our method requires fewer parameters and compu-525\ntations than the existing models, but conﬁrms that it shows526\nsimilar depth estimation results. Speciﬁcally, we boxed ﬂat527\nareas such as a sign, truck, and bus. The proposed algorithm528\nestimated the depth better in the boxed region than the existing529\nalgorithms. This is the result of estimating the depth of a ﬂat530\narea lacking local features based on regional features.531\nE. Ablation Study532\nTable IVshows the results of the ablation study to evaluate533\nthe performance of each module of the proposed method.534\nThe basic model used the Manydepth model. Performance535\nevaluation is performed according to the presence of the hybrid536\nencoder and self-attention decoder proposed in this ablation 537\nstudy. The proposed hybrid encoder improves both error and538\naccuracy evaluation indicators. In addition, the use of a hybrid539\nencoder reduces parameters and computational amount. This 540\nmeans that compared to the ResNet layer, the lightweight 541\ntransformer model maintains global feature expression perfor-542\nmance with a low amount of computation. The self-attention543\ndecoder improves the SqRel and RMSE error metrics, but 544\nlowers the accuracy metrics. The use of all proposed mod- 545\nules improves both error and accuracy evaluation indicators. 546\nHowever, the performance of the accuracy evaluation metric 547\nis lower than when using only the hybrid encoder. 548\nHWANG et al.: SELF-SUPERVISED MONOCULAR DEPTH ESTIMATION USING HYBRID TRANSFORMER ENCODER 18769\nTABLE IV\nABLATION STUDY\nV. CONCLUSION549\nIn this article, we proposed a hybrid model that combines550\nthe CNN and ViT for unsupervised learning-based monocular551\ndepth estimation. In the early layer of the encoder, the CNN is552\nused to represent local features. The later layer of the encoder553\nuses a transformer that extracts global features. We assumed554\nthat it is effective to have an expert model according to555\nthe depth of layer, and the performance improvement was556\nconﬁrmed through various expe riments. In a ddition, high557\ncomputational problems of the transformer were solved by558\nusing an LA model, and the performance was improved by559\napplying attention to the decoder.560\nAs a result of comparative experiments on the KITTI561\ndataset, the proposed method reduces AbsRel to 0.095, SqRel562\nto 0.696, and RMSE to 4.317 and improves the accuracy index563\non δ< 1.25 to 0.902. Also, the parameters are effectively564\nreduced from 14.42 to 8.13 M and multiply-accumulate opera-565\ntions (MACs) are reduced from 13.77 to 12.23 G. The ablation566\nstudy shows the performance improvement according to the567\nuse of the proposed module. Therefore, it is conﬁrmed that568\nthe proposed hybrid transformer model is an efﬁcient model569\nthat improves depth estimation performance.570\nDespite the improvement in the performance of the proposed571\nstudy, there are still issues to be studied in the future. The572\nproposed method has depth ambiguity through depth estima-573\ntion based on image information. This causes a failure to574\naccurately estimate the distance in meters. Some studies solve575\nthis problem by using IMU sensor data or external camera576\nparameter information. In future research, it seems that this577\ncan be solved by network learning on the use of external data578\nfor absolute depth estimation.579\nREFERENCES580\n[1] Y . Ming, X. Meng, C. Fan, and H. Yu, “Deep learning for monocular581\ndepth estimation: A review,” Neurocomputing, vol. 438, pp. 14–33,582\nMay 2021.583\n[2] J. Watson, O. M. Aodha, V . Prisacariu, G. Brostow, and M. Firman,584\n“The temporal opportunist: Self-s upervised multi-frame monocular585\ndepth,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.586\n(CVPR), Jun. 2021, pp. 1164–1174.587\n[3] X. Luo, J.-B. Huang, R. Szeliski, K. Matzen, and J. Kopf, “Consistent588\nvideo depth estimation,”ACM Trans. Graph., vol. 39, no. 4, pp. 1–71,589\nAug. 2020.590\n[4] X.-Z. Cui, Q. Feng, S.-Z. Wang,and J.-H. Zhang, “Monocular depth591\nestimation with self-supervised learning for vineyard unmanned agri-592\ncultural vehicle,”Sensors, vol. 22, no. 3, p. 721, Jan. 2022.593\n[5] S.-J. Hwang, S.-J. Park, G.-M. Kim, and J.-H. Baek, “Unsupervised594\nmonocular depth estimation for colonoscope system using feedback595\nnetwork,” Sensors, vol. 21, no. 8, p. 2691, Apr. 2021.596\n[6] V . Guizilini, R. Ambrus, S. Pillai, A. Raventos, and A. Gaidon,597\n“3D packing for self-supervised monocular depth estimation,” inProc.598\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,599\npp. 2485–2494.600\n[7] C. Shu, K. Yu, Z. Duan, and K. Yang, “Feature-metric loss for 601\nself-supervised learning of depth and egomotion,” in Proc. Eur. 602\nConf. Comput. Vis. (ECCV). Cham, Switzerland: Springer, Aug. 2020, 603\npp. 572–588. 604\n[8] H. Pham, Z. Dai, Q. Xie, and Q. V . Le, “Meta pseudo labels,” inProc. 605\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, 606\npp. 11557–11568. 607\n[9] X. Li et al., “Semantic ﬂow for fast and accurate scene parsing,” in 608\nProc. Eur. Conf. Comput. Vis. (ECCV). Cham, Switzerland: Springer, 609\nAug. 2020, pp. 775–793. 610\n[10] M. Tan, R. Pang, and Q. V . Le, “EfﬁcientDet: Scalable and efﬁcient611\nobject detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recog-612\nnit. (CVPR), Jun. 2020, pp. 10781–10790. 613\n[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for 614\nimage recognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.615\n(CVPR), Jun. 2016, pp. 770–778. 616\n[12] M. Tan and Q. Le, “EfﬁcientNet:Rethinking model scaling for con- 617\nvolutional neural networks,” in Proc. Int. Conf. Mach. Learn., 2019, 618\npp. 6105–6114. 619\n[13] Z. Cheng, Y . Zhang, and C. Tang, “Swin-Depth: Using transformers and 620\nmulti-scale fusion for monocular-based depth estimation,”IEEE Sensors 621\nJ., vol. 21, no. 23, pp. 26912–26920, Dec. 2021. 622\n[14] R. Ranftl, A. Bochkovskiy, and V . Koltun, “Vision transformers for 623\ndense prediction,” inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 624\nOct. 2021, pp. 12179–12188. 625\n[15] A. Dosovitskiy et al., “An image is worth 16×16 words: Transformers 626\nfor image recognition at scale,” 2020,arXiv:2010.11929. 627\n[16] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using 628\nshifted Windows,” 2021,arXiv:2103.14030. 629\n[17] X. Dong et al., “CSWin transformer: A general vision transformer 630\nbackbone with cross-shaped Windows,” 2021,arXiv:2107.00652. 631\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁca- 632\ntion with deep convolutional neural networks,” inProc. Adv. Neural 633\nInf. Process. Syst. (NIPS), vol. 25, Stateline, NV , USA, Dec. 2012, 634\npp. 1097–1105. 635\n[19] K. Simonyan and A. Zisserman, “Very deep convolutional networks for636\nlarge-scale image recognition,” 2014,arXiv:1409.1556. 637\n[20] A. Howard et al., “Searching for MobileNetV3,” inProc. IEEE/CVF 638\nInt. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 1314–1324. 639\n[21] X. Yao, Z. Zhu, C. Kang, S.-H. Wang, J. M. Gorriz, and Y .-D. Zhang,640\n“AdaD-FNN for chest CT-based COVID-19 diagnosis,”IEEE Trans. 641\nEmerg. Topics Comput. Intell. , early access, Jun. 1, 2022, doi: 642\n10.1109/TETCI.2022.3174868. 643\n[22] X. Yao, X. Wang, S.-H. Wang, and Y .-D. Zhang, “A comprehensive644\nsurvey on convolutional neural network in medical image analysis,” 645\nMultimedia Tools Appl., pp. 1–45, Aug. 2020. 646\n[23] C. Godard, O. M. Aodha, M. Firman, and G. Brostow, “Digging into647\nself-supervised monocular depth estimation,” inProc. IEEE/CVF Int. 648\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 3828–3838. 649\n[24] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and 650\nH. Jégou, “Training data-efﬁcient image transformers & distilla- 651\ntion through attention,” in Proc. Int. Conf. Mach. Learn. , 2021, 652\npp. 10347–10357. 653\n[25] H. Wuet al., “CvT: Introducing convolutions to vision transformers,” in654\nProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 22–31. 655\n[26] J. Guo et al., “CMT: Convolutional neural networks meet vision trans-656\nformers,” 2021,arXiv:2107.06263. 657\n[27] F. Khan, S. Salahuddin, and H. Javidnia, “Deep learning-based monoc- 658\nular depth estimation methods—A state-of-the-art review,” Sensors, 659\nvol. 20, no. 8, p. 2272, Apr. 2020. 660\n[28] R. Garg, V . K. Bg, G. Carneiro, and I. Reid, “Unsupervised CNN 661\nfor single view depth estimation: Geometry to the rescue,” inProc. 662\nEur. Conf. Comput. Vis.Amsterdam, The Netherlands: Springer, 2016, 663\npp. 740–756. 664\n18770 IEEE SENSORS JOURNAL, VOL. 22, NO. 19, 1 OCTOBER 2022\n[29] C. Godard, O. M. Aodha, and G. J. Brostow, “Unsupervised monocular665\ndepth estimation with left-right consistency,” in Proc. IEEE Conf.666\nComput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 270–279.667\n[30] M. Jaderberg, K. Simonyan, and A. Zisserman, “Spatial transformer668\nnetworks,” inProc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 28, 2015,669\npp. 1–9.670\n[31] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image671\nquality assessment: From error visibility to structural similarity,”IEEE672\nTrans. Image Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.673\n[32] F. Tosi, F. Aleotti, M. Poggi, and S. Mattoccia, “Learning monocu-674\nlar depth estimation infusing traditional stereo knowledge,” in Proc.675\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,676\npp. 9791–9801.677\n[33] J. Watson, M. Firman, G. Brostow, and D. Turmukhambetov, “Self-678\nsupervised monocular depth hints,” in Proc. IEEE/CVF Int. Conf.679\nComput. Vis. (ICCV), Oct. 2019, pp. 2162–2171.680\n[34] J. L. Gonzalez and M. Kim, “Forget about the LiDAR: Self-supervised681\ndepth estimators with MED probability, volumes,” inProc. Adv. Neural682\nInf. Process. Syst., vol. 33, 2020, pp. 12626–12637.683\n[35] J. L. G. Bello and M. Kim, “PLADE-Net: Towards pixel-level accuracy684\nfor self-supervised single-view depth estimation with neural positional685\nencoding and distilled matting loss,” inProc. IEEE/CVF Conf. Comput.686\nVis. Pattern Recognit. (CVPR), Jun. 2021, pp. 6851–6860.687\n[36] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised688\nlearning of depth and ego-motion from video,” inProc. IEEE Conf.689\nComput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 1851–1858.690\n[37] J. Wang, G. Zhang, Z. Wu, X. Li, and L. Liu, “Self-supervised691\njoint learning framework of depth estimation via implicit cues,” 2020,692\narXiv:2006.09876.693\n[38] A. Johnston and G. Carneiro, “Self-supervised monocular trained depth694\nestimation using self-attention and discrete disparity volume,” inProc.695\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,696\npp. 4756–4765.697\n[39] J. Yan, H. Zhao, P. Bu, and Y . Jin, “Channel-wise attention-based698\nnetwork for self-supervised monocular depth estimation,” inProc. Int.699\nConf. 3D Vis., Dec. 2021, pp. 464–473.700\n[40] H. Zhou, D. Greenwood, and S. Taylor, “Self-supervised monocular701\ndepth estimation with internal feature fusion,” 2021,arXiv:2110.09482.702\n[41] V . Guizilini, R. Hou, J. Li, R. Ambrus, and A. Gaidon, “Semantically-703\nguided representation learning for self-supervised monocular depth,”704\n2020, arXiv:2002.12319.705\n[42] V . Casser, S. Pirk, R. Mahjourian, and A. Angelova, “Unsupervised706\nmonocular depth and ego-motion learning with structure and semantics,”707\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops708\n(CVPRW), Jun. 2019.709\n[43] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, “Depth from710\nvideos in the wild: Unsupervised monocular depth learning from711\nunknown cameras,” inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),712\nOct. 2019, pp. 8977–8986.713\n[44] H. Li, A. Gordon, H. Zhao, V . Casser, and A. Angelova, “Unsupervised714\nmonocular depth learning in dynamic scenes,” 2020,arXiv:2010.16404.715\n[45] P. Ruhkamp, D. Gao, H. Chen, N. Navab, and B. Busam, “Attention716\nmeets geometry: Geometry guided spatial-temporal attention for consis-717\ntent self-supervised monocular depth estimation,” inProc. Int. Conf. 3D718\nVis., Dec. 2021, pp. 837–847.719\n[46] V . Patil, W. Van Gansbeke, D. Dai, and L. Van Gool, “Don’t forget the720\npast: Recurrent depth estimation from monocular video,”IEEE Robot.721\nAutom. Lett., vol. 5, no. 4, pp. 6813–6820, Oct. 2020.722\n[47] J. Žbontar and Y . LeCun, “Stereo matching by training a convolutional723\nneural network to compare image patches,”J. Mach. Learn. Res., vol. 17,724\nno. 1, pp. 2287–2318, Jan. 2016.725\n[48] J.-R. Chang and Y .-S. Chen, “Pyramid stereo matching network,” in726\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018,727\npp. 5410–5418.728\n[49] S. Im, H.-G. Jeon, S. Lin, and I. So Kweon, “DPSNet: End-to-end deep729\nplane sweep stereo,” 2019,arXiv:1905.00538.730\n[50] A. Varma, H. Chawla, B. Zonooz, and E. Arani, “Transformers in self-731\nsupervised monocular depth estimation with unknown camera intrinsics,”732\n2022, arXiv:2202.03131.733\n[51] V . Guizilini, R. Ambrus, D. Chen, S. Zakharov, and A. Gaidon, “Multi-734\nframe self-supervised depth with transformers,” in Proc. IEEE Conf.735\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 160–170.736\n[52] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,737\n“MobileNetV2: Inverted residuals and linear bottlenecks,” in Proc.738\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018,739\npp. 4510–4520.740\n[53] S. Woo, J. Park, J. Y . Lee, and I. S. Kweon, “CBAM: Convolutional741\nblock attention module,” in Proc. Eur. Conf. Comput. Vis. (ECCV), 742\nSep. 2018, pp. 3–19. 743\n[54] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:744\nThe KITTI dataset,”Int. J. Robot. Res., vol. 32, no. 11, pp. 1231–1237, 745\n2013. 746\n[55] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a 747\nsingle image using a multi-scale deep network,” inProc. Adv. Neural 748\nInf. Process. Syst., vol. 27, 2014, pp. 2366–2374. 749\n[56] M. Cordts et al., “The cityscapes dataset for semantic urban scene 750\nunderstanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. 751\n(CVPR), Jun. 2016, pp. 3213–3223. 752\nSeung-Jun Hwangreceived the B.S. and M.S. degrees in electronics 753\nand information engineering from Korea Aerospace University (KAU), 754\nGoyang, Republic of Korea, in 2012 and 2014, respectively, where he is755\ncurrently pursuing the Ph.D. degree with the School of Electronics and756\nInformation Engineering. 757\nHis current research interests include image processing and computer758\nvision. 759\nSung-Jun Park received the B.S. and M.S. degrees in electronics 760\nand information engineering from Korea Aerospace University (KAU), 761\nGoyang, Republic of Korea, in 2019 and 2021, respectively, where he is762\ncurrently pursuing the Ph.D. degree with the School of Electronics and763\nInformation Engineering. 764\nHis current research interests include image processing and computer765\nvision. 766\nJoong-Hwan Baekreceived the B.S. degree in telecommunication engi-767\nneering from Korea Aerospace University, Goyang, Republic of Korea,768\nin 1981, and the M.S. and Ph.D. degrees from Oklahoma State University,769\nStillwater, OK, USA, in 1987 and 1991, respectively. 770\nHe was a Senior Researcher with the Electronics and Telecom- 771\nmunications Research Institute, Daejeon, South Korea, from 1991 to 772\n1992. He has been a Professor with the School of Electronics and 773\nInformation Engineering, Korea Aerospace University, since 1992. He is774\nalso the Director of the Video-Audio Space Convergence Technology 775\nResearch Center, Korea Aerospace University. His research interests 776\ninclude image processing, computer vision, pattern recognition, and 777\nmultimedia. 778\nByungkyu Kim received the Ph.D. degree in mechanical engineering 779\nfrom the University of Wisconsin-Madison, Madison, WI, USA, in 1997. 780\nFrom 1997 to 2000, he was a Technical Staff Member with the 781\nCenter for X-ray Lithography, University of Wisconsin-Madison, where he782\ndeveloped a computer code for thermal modeling of a mask membrane783\nand wafer during beam exposure. From 2000 to 2005, he worked for 784\nthe Microsystem Center, Korea Institute of Science and Technology 785\n(KIST), Seoul, South Korea, as a Principal Research Scientist. He was786\nan in-charge of developing a microcapsule-type robotic endoscope. He is 787\ncurrently a Professor with Korea Aerospace University, Goyang, Republic788\nof Korea. His research interests include space mechanisms, robotics, 789\nmicro/nano-manipulators, and biomedical application robots. 790",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.6656489372253418
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6332498788833618
    },
    {
      "name": "Computer science",
      "score": 0.6329858899116516
    },
    {
      "name": "Monocular",
      "score": 0.5538868308067322
    },
    {
      "name": "Computer vision",
      "score": 0.529019832611084
    },
    {
      "name": "Transformer",
      "score": 0.4898240268230438
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3482476472854614
    },
    {
      "name": "Engineering",
      "score": 0.17628931999206543
    },
    {
      "name": "Electrical engineering",
      "score": 0.13654428720474243
    },
    {
      "name": "Voltage",
      "score": 0.09458625316619873
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}