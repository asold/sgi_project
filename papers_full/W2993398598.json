{
  "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
  "url": "https://openalex.org/W2993398598",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2745216682",
      "name": "Dathathri, Sumanth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222272762",
      "name": "Madotto, Andrea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4283442913",
      "name": "Lan, Janice",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289047883",
      "name": "Hung, Jane",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2531305539",
      "name": "Frank, Eric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286874148",
      "name": "Molino, Piero",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282320216",
      "name": "Yosinski, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209981129",
      "name": "Liu, Rosanne",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971147102",
    "https://openalex.org/W2963283805",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2888161220",
    "https://openalex.org/W2056760934",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W3038817008",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2805486818",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W2953147883",
    "https://openalex.org/W2890276793",
    "https://openalex.org/W1983452151",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2967576208",
    "https://openalex.org/W2798937790",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2591807639",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2741986794",
    "https://openalex.org/W2029164135",
    "https://openalex.org/W2969442125",
    "https://openalex.org/W2892095314",
    "https://openalex.org/W2958127121",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W2952335829",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2529548870",
    "https://openalex.org/W2550147980",
    "https://openalex.org/W2979047515",
    "https://openalex.org/W2916772188",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2836265996",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963104691",
    "https://openalex.org/W2557449848",
    "https://openalex.org/W1932198206",
    "https://openalex.org/W2587713090",
    "https://openalex.org/W2914442349"
  ],
  "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.",
  "full_text": "Published as a conference paper at ICLR 2020\nPLUG AND PLAY LANGUAGE MODELS : A SIMPLE\nAPPROACH TO CONTROLLED TEXT GENERATION\nSumanth Dathathri∗ ∗\nCMS, Caltech\nAndrea Madotto∗\nHKUST\nJanice Lan\nUber AI\nJane Hung\nUber AI\nEric Frank\nUber AI\nPiero Molino\nUber AI\nJason Yosinski††\nUber AI\nRosanne Liu†\nUber AI\ndathathris@gmail.com, amadotto@connect.ust.hk\n{janlan, jane.hung, mysterefrank, piero, yosinski, rosanne}@uber.com\nABSTRACT\nLarge transformer-based language models (LMs) trained on huge text corpora\nhave shown unparalleled generation capabilities. However, controlling attributes\nof the generated language (e.g. switching topic or sentiment) is difﬁcult without\nmodifying the model architecture or ﬁne-tuning on attribute-speciﬁc data and en-\ntailing the signiﬁcant cost of retraining. We propose a simple alternative: the Plug\nand Play Language Model (PPLM) for controllable language generation, which\ncombines a pretrained LM with one or more simple attribute classiﬁers that guide\ntext generation without any further training of the LM. In the canonical scenario\nwe present, the attribute models are simple classiﬁers consisting of a user-speciﬁed\nbag of words or a single learned layer with 100,000 times fewer parameters than\nthe LM. Sampling entails a forward and backward pass in which gradients from\nthe attribute model push the LM’s hidden activations and thus guide the gener-\nation. Model samples demonstrate control over a range of topics and sentiment\nstyles, and extensive automated and human annotated evaluations show attribute\nalignment and ﬂuency. PPLMs are ﬂexible in that any combination of differen-\ntiable attribute models may be used to steer text generation, which will allow for\ndiverse and creative applications beyond the examples given in this paper.\n1 I NTRODUCTION\nThe Transformer architecture (Vaswani et al., 2017) has enabled large-scale language models (LMs)\ntrained on a huge amount of data (Radford et al., 2019; Dai et al., 2019b; Radford et al., 2018b) to\ngreatly improve the state-of-the-art on natural language processing tasks. These models are used to\nextract contextualized word embeddings for transfer learning purposes (Devlin et al., 2019) and as\nnatural language generators. The latter can leverage large amounts of unannotated data and a simple\nlog-likelihood training objective. However, once such models are trained, controlling attributes of\ngenerated text becomes difﬁcult without modifying the model architecture to allow for extra input\nattributes or ﬁne-tuning with attribute-speciﬁc data (Keskar et al., 2019; Ziegler et al., 2019).\n∗Work done during internship at Uber AI\n†Co-senior authors .\n•Summary of contributions: SD, RL & JY conceptualized PPLMs and led the manuscript writing. SD led the\nproject, implemented the PPLM, set up and ran all modeling experiments, engineered how to obtain workable\ngradients via the weighted embedding approach, and made the model work. AM helped with preparing datasets\nfor discriminator training, automated evaluation, running experiments, and writing the manuscript. SD, RL &\nAM ran the external baselines. RL & JL built and oversaw the human evaluation pipeline and computed the\nstatistics. JH ran the story generation with skeleton preﬁxes. EF assisted with detoxiﬁcation experiments. PM\nled efforts to migrate to the new pytorch transformer, helped with code release. JY helped with the annotation\npipeline, ﬁnding bugs, navigating model and experimental directions, engineering workable gradients, and\nposing the model mathematically. RL implemented preliminary experiments and multi-attribute control, and\ncleaned and coordinated release of the code. RL & JY oversaw the project.\n1\narXiv:1912.02164v4  [cs.CL]  3 Mar 2020\nPublished as a conference paper at ICLR 2020\nTable 1: The PPLM employs a pre-trained language model (LM) without any changes to the model\nparameters and can generate text with controlled attributes such as topic and sentiment. We demon-\nstrate control with two tiny and easy to construct attribute models: a bag of words (BoW) related to a\ntopic and a linear discriminator trained on top of LM latent representations to control sentiment. The\nunderlined preﬁx is what the LM is conditioned on to generate a passage of text (e.g. The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato).\nThe controlled attributes are colored and bracketed (e.g. [Science] ), and words in the BoW that are\ndirectly optimized for are highlighted brightly (e.g. research). The softer highlights correspond to\nwords related to the attribute, but not directly optimized for during the control process (e.g. health).\n[–] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato and cauliﬂower are both in season to make combo breads, mounds, or pads. For an added\nchallenge, try some garlic mashed potatoes.\n[Negative] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato is a pretty bad idea. It can make you fat, it can cause you to have a terrible immune\nsystem, and it can even kill you.. . .\n[Positive] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato chip recipe you asked for! We love making these, and I’ve been doing so for years.\nI’ve always had a hard time keeping a recipe secret. I think it’s the way our kids love to eat them – so many\nlittle ones.\n[Science] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato was once thought to have no health problems and has been promoted as a nutritious\nfood source since the mid-1800s, but recent reports indicate that it has many harmful health issues. In fact,\nresearchers from Johns Hopkins University. . .\n[Politics] [Positive] To concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo conclude this series of articles, I will present three of the most popular and\ninﬂuential works on this topic. The ﬁrst article deals with the role of women’s political participation in\nbuilding a political system that is representative of the will of the people.\n[Politics] [Negative] To concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo conclude, the most signiﬁcant and lasting damage from the economic crisis in\n2008 was that many governments, including those in the political center, lost power for the ﬁrst time in\nmodern history.\nControllable generation entails modeling p(x|a), where ais some desired controllable attribute(s)\nand x the generated sample. However, generative models only learn p(x). In computer vision,\nPlug & Play Generative Networks (PPGN) from Nguyen et al. (2017) developed a mechanism for\ngenerating images with different attributes by plugging a discriminator (attribute model) p(a|x)\ntogether with a base generative model p(x) and sampling from the resulting p(x|a) ∝p(a|x)p(x),\neffectively creating a conditional generative model on the ﬂy from any supplied attribute model. In\na similar manner, we propose the Plug and Play Language Model (PPLM) for conditional language\ngeneration that combines one or more simple attribute models p(a|x)—either in the form of a bag-\nof-words (BoW) or single layer classiﬁers—with a pre-trained, unconditional language model p(x).\nWe sample from the resulting combined model by following gradients in the latent representation\nspace in a manner inspired by the approximate Metropolis-adjusted Langevin (MALA) (Roberts\net al., 1996; Roberts & Rosenthal, 1998) sampler deployed in Nguyen et al. (2017).\nOptimization is performed ex post facto in the activation space, therefore no re-training or ﬁne-\ntuning is needed . Control is ﬁne-grained, with a strength parameter determining how strong the\nattribute inﬂuence should be; a strength of 0 fully recovers the original model p(x). This design\nallows vast ﬂexibility: users can combine a state-of-the-art generative model, which may be large\nand difﬁcult to train, with any number of attribute controllers. Attribute models may be easier to train\nor untrained (in the case of BoW models), and multiple controllers may be combined ﬂexibly during\ninference. In this paper, we demonstrate the PPLM approach using a GPT-2 345M model (Radford\net al., 2019) as the general-purpose LM p(x), but the method applies in any representation space\nfrom any transformer-based text generator and allows combination with any attribute modelp(a|x).\nWe demonstrate controlled generation with a number of attribute controllers, assembled and com-\nbined during generation, each with a different strength, acting as a set of “control knobs” that tune\ngeneration towards the desired attribute (see examples in Table 1). Code for the experiments is\navailable at: https://github.com/uber-research/PPLM. Our key contributions are:\n• We introduce the Plug and Play LM for controlled language generation, discuss its relation\nto existing work, and how sampling from a PPLM works (Sections 2 and 3).\n• We demonstrate controlling of text generation on a range of attributes, including 7 topics\neach deﬁned using a bag of words, and 1 simple discriminator on sentiments. We quantify\neffectiveness using both automated evaluation (separately trained perplexity and sentiment\n2\nPublished as a conference paper at ICLR 2020\nmodels) as well as human evaluation (for attribute relevance and ﬂuency). All evaluations\npoint toward the ability of PPLMs to generate attribute controlled, ﬂuent text (Section 4).\n• We compare PPLM with CTRL (Keskar et al., 2019) and GPT-2 ﬁnetuned for positivty\n(Ziegler et al., 2019). Our method, without any LM training, is on par and often outper-\nforms the baselines on attribute relevance and ﬂuency (Section 4.2, and Section 4.3).\n• We show that the PPLM approach can be used to detoxify instances where generation\nof toxic content is likely by following the negative gradient of a model trained to detect\ntoxicity (Section 4.4). We also show how PPLM can be used for structurally constrained\nstory writing (Section 4.5).\n2 R ELATED WORK\nControlled generation Current methods for controlled text generation involve either ﬁne-tuning\nexisting models with Reinforcement Learning (RL) (Ziegler et al., 2019), training Generative Ad-\nversarial Networks (Yu et al., 2017), or training conditional generative models (Kikuchi et al., 2016;\nFicler & Goldberg, 2017). Different from our approach, these methodologies are not plug and\nplay, since the entire model needs to be separately ﬁne-tuned for each speciﬁc attribute. Keskar\net al. (2019) train a large language model with over 50 different control codes. The results are high\nquality because they train exactly to maximizep(x|a), but this comes at the expense of ﬁxing control\ncodes upfront and of training a very large model (1.6B parameters). Our method does not require\nretraining any conditional generative model, and both the language model and the conditional model\ncan be ﬂexibly assembled. Table 2 gives a comparison of recent approaches to language modeling\ntuned for speciﬁc attributes. In another interesting but tangential piece of work, Subramani et al.\n(2019) recently showed that a pre-trained language model can be steered to recover arbitrary sen-\ntences. In earlier works Gu et al. (2016; 2017); Chen et al. (2018) explored the idea of using a small\nneural network to steer an LM.\nNoisy Channel Modeling Yu et al. (2016), and more recently Yu et al. (2019); Yee et al. (2019);\nNg et al. (2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving\nsequence-to-sequence modeling. Their approach translates a source language sentenceyinto a target\nlanguage sentence xby ﬁrst sampling from a forward model proposal distributionpforward(x|y) and\nthen reranking samples based on probabilities given bypbackward(x|y) ∝p(x)p(y|x). PPLM scores\nsamples using the same basic equation, but as we have no forward or proposal modelpforward(x|a),\nwe rely on the latent space updates, similar to Nguyen et al. (2017). As a baseline, we consider\nusing p(x) as a “forward model” and then reranking, which we will see works moderately well in\nsome scenarios and poorly in others (see Tables 4 and 6).\nWeighted decoding Holtzman et al. (2018); Ghazvininejad et al. (2017) consider controlled lan-\nguage generation – the former with discriminators, and the latter with a bag of words – where the\ndecoding procedure is modiﬁed to consider the scoring function used for decoding. See et al. (2019)\nnote that control with weighted decoding (WD) is difﬁcult and often leads to sacriﬁcing ﬂuency and\ncoherence. Further, Ghazvininejad et al. (2017) strongly relies on sampling from a set of keywords\non a speciﬁc topic and it does not allow to bias generation towards a topic in a manner that does not\nnecessary include a set of keywords. Similarly, Baheti et al. (2018) proposed a decoding strategy\nfor generating interesting responses in dialogue systems, using bags of words and word embed-\ndings. Sophisticated sampling methods (Metropolis et al., 1953) can be used to constrain the model\ngeneration to certain keywords and topics. We evaluate WD as a baseline.\nText Style Transfer Outside of language modeling, the text style transfer studies a related task.\nShen et al. (2017); Hu et al. (2017) train variational auto-encoders for style transfer that rely on\nlearning disentangled latent representations for style and content. Li et al. (2018) demonstrate the\nefﬁcacy of a simple approach based on replacing attribute related n-grams with n-grams correspond-\ning to the desired attribute based on a conditional generative model. A key difference between the\nabove and our approach is that we use an ofﬂine discriminator and perform optimization based on\nthis discriminator, which as suggested by Elazar & Goldberg (2018) may outperform adversarial\ntraining approaches. More recently, Lample et al. (2019) adapt an approach from unsupervised\nlanguage translation to style transfer, where a denoised auto-encoder is trained with an objective\n3\nPublished as a conference paper at ICLR 2020\nTable 2: Comparison of the different models and distributions. All models in this table are useful in\ndifferent scenarios. The particular advantage of PPLM is that very small, custom attribute models,\np(a|x), may be combined with powerful, general pre-trained language models,p(x), to create cheap\nbut still powerful conditional generative models, p(x|a).\nModel type Form of model Samples\nExample models\nand number of trainable params\nLanguage Model p(x) Uncond. GPT-2 medium: 345M\n(Radford et al., 2019)\nFine-tuned\nLanguage Model p(x) Uncond. Fine-tuned GPT-2 medium: 345M\n(Ziegler et al., 2019)\nConditional\nLanguage Model p(x|a) Cond. CTRL: 1.6B\n(Keskar et al., 2019)\nPlug and Play\nLanguage Model\n(PPLM)\np(x|a) ∝p(x)p(a|x) Cond.\nPPLM-BoW: 0 (curated word list)\nPPLM-Discrim: ∼1K/attribute\n(not counting pretrained p(x))\nconsisting of a weighted combination of a re-construction loss and a back-translation loss. While\nthe above approaches have shown impressive success on style transfer tasks, the main focus is not\ncontrolled language generation, and further, the methods are not plug and play.\n3 P LUG AND PLAY LANGUAGE MODELS\n3.1 L ANGUAGE MODELING WITH TRANSFORMERS\nGiven a sequence of tokensX = {x0,··· ,xn}, LMs are trained to compute the unconditional prob-\nability of the sequence p(X). This probability can be rewritten in terms of product of conditional\nprobabilities by recursively applying the chain-rule (Manning et al., 1999; Bengio et al., 2003) as:\np(X) =\nn∏\ni=1\np(xi|x0,··· ,xi−1) (1)\nIn this paper, we use a transformer (Vaswani et al., 2017) to model the distribution of natural lan-\nguage. To present our approach clearly, we ﬁrst brieﬂy summarize the transformer using recur-\nrent notation. Let us deﬁne the history matrix Ht to consist of the key-value pairs from the past\ni.e Ht = [( K(1)\nt ,V (1)\nt ),··· ,(K(l)\nt ,V (l)\nt )], where (K(i)\nt ,V (i)\nt ) corresponds to the key-value pairs\nfrom the i-th layer generated at all time-steps from 0 to t. Efﬁcient implementations of the trans-\nformer (Wolf et al., 2019) use the cachedHtto generate xt+1, given xt. This recurrent interpretation\nof a transformer can be summarized as:\not+1,Ht+1 = LM(xt,Ht), (2)\nwhere W is a linear transformation that maps the logit vectorot+1 to a vector of vocabulary size, and\nthen xt+1 is sampled as xt+1 ∼pt+1 = Softmax(Wot+1). This allows for efﬁcient language gen-\neration without repeated forward passes corresponding to the prior conditioning text x0,...,x t−1.\n3.2 S TEERING GENERATION : ASCENDING log p(a|x)\nIn order to control the output of the language model, at every generation step t, we shift the history\nHtin the direction of the sum of two gradients: one toward higher log-likelihood (LL) of the attribute\naunder the conditional attribute modelp(a|x) and one toward higher LL of the unmodiﬁed language\nmodel p(x). Combining these factors with a variable multiplier provides us with a controllable\n“knob” to guide generation in a given direction with a speciﬁed strength. The updates are restricted\nto Ht and not the other model activations because future predictions depend on the past only viaHt\n(note that Ht is composed of all transformer key and value pairs generated up to time t). Taking\nsteps in Ht space leads to gradual changes to model activations — which may be thought of as\ngradual reinterpretations of the past — that guide future generation in the desired direction.\nLet ∆Ht be the update to Ht, such that generation with (Ht + ∆Ht) shifts the distribution of\nthe generated text such that it is more likely to possess the desired attribute. ∆Ht is initialized\n4\nPublished as a conference paper at ICLR 2020\nLM LM LM\nAttribute Model p(a|x)\nThe chicken tastes\nchicken tastes Grad\n(Positive\nsentiment)\nok delicious\nOriginal distribution\n(\"ok\")\nUpdated distribution\n(\"delicious\")\nUpdated Latents\nBackward Pass\nand update latents\nForward Pass\nRecompute with \nupdated latentsp(x)p(x) p(x)\nRecompute\nStep 1\n{ \n{ \n{ \nStep 2\nStep 3\nFigure 1: Simpliﬁed illustration of the proposed approach in three phases. In Step 1, a forward pass\nis performed through the language model to compute the likelihood of a desired attribute using an\nattribute model that predicts p(a|x). In Step 2, a backward pass updates the internal latent represen-\ntations of the LM, using gradients from the attribute model, to increase the likelihood of the passage\nhaving the desired attribute. In Step 3, a new distribution over the vocabulary ( ˜pt+1) is generated\nfrom the updated latents ( ˜Ht) and the current token xt. The next token is then sampled from the\nupdated distribution. This process of updating the latents is repeated at each time-step, leading to\na gradual transition towards the desired attribute. For computational efﬁciency, one may choose to\nmodify only the latents within some window of the recent past, depicted as the dotted-red region.\nat zero and updated with gradients from an attribute model that measures the extent to which the\ngenerated text possesses the desired attribute (e.g. positivity). We rewrite the attribute modelp(a|x)\nas p(a|Ht + ∆Ht) and then make gradient based updates to ∆Ht as follows:\n∆Ht ←∆Ht + α ∇∆Ht log p(a|Ht + ∆Ht)\n∥∇∆Ht log p(a|Ht + ∆Ht)∥γ (3)\nwhere αis the step size, γ is the scaling coefﬁcient for the normalization term. 1 This update step\ncan be repeated mtimes; in practice we use 3 to 10. Subsequently, a forward pass through the LM\nwith the updated key-value pairs is performed to obtain the updated logits ˜ot+1 as ˜ot+1,Ht+1 =\nLM(xt, ˜Ht), where ˜Ht = Ht+∆Ht. The perturbed ˜ot+1 is then used to generate a new distribution\n˜pt+1 as in Equation 2.\n3.3 E NSURING FLUENCY : ASCENDING log p(x)\nThe approach described in the previous section is able to generate text tuned for a particular dis-\ncriminator, but left unchecked it will quickly result in unrealistic adversarial or fooling examples\n(Szegedy et al., 2013; Nguyen et al., 2015) as the text moves into low probability regions. To com-\nbat this, we use the unconditional language model in two ways that ensure the ﬂuency is maintained\nat or near the level of the unconditional language model (here GPT-2).\nKullback–Leibler (KL) Divergence We update∆Ht to minimize the KL divergence between the\noutput distribution of the modiﬁed and unmodiﬁed language models in addition to the step above.\nIn practice, this is accomplished by adding the quantities together before taking a gradient, though it\ncan be visualized as two separate steps as in Figure 2. We scale the KL coefﬁcient by a scalar λKL,\nand in practice, setting this hyperparameter to 0.01 works well in general across tasks.\nPost-norm Geometric Mean FusionIn addition to minimizing KL divergence, which affects the\npast via ∆Ht, we perform post-norm fusion similarly to Stahlberg et al. (2018). This does not\ndirectly affect ∆Ht; rather, it just serves to constantly tie the generated text to the unconditional\np(x) LM distribution. We accomplish this by sampling fromxt+1 ∼1\nβ\n(\n˜pγgm\nt+1 p1−γgm\nt+1\n)\n, where pt+1\nand ˜pt+1 are the unmodiﬁed and modiﬁed output distributions, respectively, and βis a normalizing\nfactor such that it forms a valid distribution. As γgm →1 this converges to the distribution from\nthe updated LM, and as γgm →0 it converges to the unconditional LM distribution. We ﬁnd that in\npractice values for γgm in the range 0.8 −0.95 work well.\n1 One normalization term is computed for each layer of the transformer.\n5\nPublished as a conference paper at ICLR 2020\nFigure 2: An oversimpliﬁed view into why steps\nthat maximize both log p(a|x) and log p(x) are\nneeded. The sentence under consideration is\nshown as a black dot, which is ﬁrst pushed in the\ndirection of maximizinglog p(a|x) and then in the\ndirection of maximizing log p(x). In practice we\nuse a single step and simply add the log proba-\nbilities; we take steps in continuous space of hid-\nden representations Hrather than in the discretex\n(byte pair) space, and rather than resampling the\nentire sentence each step, we take one step in H\nspace per byte-pair sample.\np(x)\nlower\nhigher\np(a|x)\nlower                         higher\n ascend p(a|x)\n ascend p(x)\n3.4 S AMPLING AND RANKING\nThe attribute model p(a|x) in PPLM provides two functionalities: ﬁrst, a score that can be used to\nrank samples based on the LL of the desired attribute (forward pass only; Step 1, Figure 1), and\nsecond, a gradient ascent direction to perform an update in the latent space (Step 2 & 3; Figure 1).\nThe former can be used to generate r samples and rank them to choose the best one. This can\nserve as an additional method for attribute control in addition to sampling with updated latents.\nFurther, to avoid the problem of repetitive, low quality text (Holtzman et al., 2018), we compute the\nmean over the Dist-1, Dist-2 and Dist-3 scores (for the generated passage), which is an indicator of\nrepetitiveness (Li et al., 2015), and then discard samples with a mean score below a threshold τ.\n4 E XPERIMENTS , RESULTS , AND EVALUATION\nIn this section, we describe our evaluation methodology and then show controlled generation results\nunder various attribute models. We also show use cases of PPLM in language detoxiﬁcation and in\ncontrolled story telling. For all results reported in this section, we use top-k sampling (Fan et al.,\n2018) with k= 10 to draw from the softmax distribution over the vocabulary.\n4.1 E VALUATION METHODS AND ABLATION STUDY\nWe evaluate to assess two properties: whether PPLM generates text that satisﬁes the desired attribute\n(topic or sentiment) and whether the quality of its text deteriorates as we intensify control of the\nattribute. Note we can always turn the control knob down to zero to disable control of attributes\nand reach the ﬂuency of the original model. If desired, a user can tune the knobs at inference until a\nchosen tradeoff between attribute strength and ﬂuency is reached. We evaluate using both automated\nmethods and human annotators:\nAutomated Eval.Perplexity is an automated measure of ﬂuency, though its effectiveness has been\nquestioned in open-domain text generation (Liu et al., 2016). We measure perplexity using a differ-\nent pre-trained language model, GPT (Radford et al., 2018b). The diversity of text in the passages\nis measured using the number of distinct n-grams (normalized by the length of text) as in Li et al.\n(2015). We report Dist-1, Dist-2, and Dist-3 scores for the distinct 1-2-3-grams (measured across\nall samples generated for a given attribute control task, e.g. a speciﬁc topic for topic control). Such\nscores are an indicator of the diversity of the samples generated (Li et al., 2015). We aslo use external\nsentiment classiﬁers for sentiment evaluation.\nHuman Eval. We consider two types of human annotation: ﬂuency and A/B testing on attribute\nrelevance. Annotators are asked to evaluate the ﬂuency of each individual sample on a scale of 1-5,\nwith 1 being “not ﬂuent at all” and 5 being “very ﬂuent,” as done in Lample et al. (2019). In the A/B\ntesting for attribute relevance, we consider all combinatorial pairs of all four variants: B, BR, BC,\nand BCR (6 combinations). We then ask annotators to rank the pair on the desired attribute (e.g. topic\nrelevance, sentiment strength), while allowing “neither” and “both” options to account for equally\ngood/bad generations (Lample et al., 2019). We obtain annotations from nine external occupational\nannotators. Each pair of samples is evaluated by three individuals and we use majority-voting to\n6\nPublished as a conference paper at ICLR 2020\nTable 3: Comparison of different samples generated by (top row) baseline GPT-2 and (other rows)\nPPLM with different BoW corresponding to different topics (e.g. [Military] ), all conditioned on a\nsingle preﬁx: \" The issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused\". Both directly optimized (in red) and related words (in soft red)\nare highlighted, showing how the optimization takes effect.\n[–] The issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on the way that the city’s police ofﬁcers have reacted in recent years to the deaths of\nMichael Brown in Ferguson, Mo., Eric Garner in New York City and Sandra Bland in Texas, as well as the\nshooting of unarmed teen Michael Brown by a white police ofﬁcer in Ferguson, Mo. . . .\n[Military] The issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on the fact that the government had spent billions on the military and that it\ncould not deploy the troops in time. The prime minister said that the country would take back control of its\nairspace over Syria in the next 48 hours. \\n The military is investigating why. . .\n[Space] The issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on a series of incidents that occurred in the past few months, which included an\nalleged attack by Islamic State ﬁghters on a Kurdish checkpoint, the use of drones in combat, space\ntechnology research by Russian and American space companies, and more. \\n The world. . .\n[Science] The issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on a single piece: the question \"What is the meaning of life?\" This question\nhas puzzled many philosophers, who have attempted to solve it by using some of the concepts of quantum\nmechanics, but they have to solve it by the laws of nature themselves.. . .\n[Politics] The issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on a single section of the legislation. It’s unclear whether the committee will\nvote to extend the law, but the debate could have wider implications. \\n \"The issue of the law’s\napplicability to the United Kingdom’s referendum campaign has been one of. . .\n[Computers] The issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on the role of social media as a catalyst for political and corporate\nengagement in the digital economy, with the aim of encouraging companies to use the power of social\nmedia and the Internet to reach out to their target market. \\n . . .\ncompute attribute relevance. For ﬂuency, we use average of the three annotations. The method of\ngeneration is completely hidden and the order of samples in A/B testing is randomized.\nAblation study and baselines.We conduct an ablation study with four variants: B: the baseline,\nunchanged GPT-2 LM, sampled once; BR: B but sampled rtimes, with best sample chosen based\non the LL ranking and ﬁltering based on Dist score; BC: update the latent representations ( ˜Ht) and\nthen sample once; and lastly BCR: update the latent representations ( ˜Ht) and generate rsamples,\nchoose the best sample based on the LL score (after ﬁltering out samples with low Dist scores). As\nbaseline approaches we consider CTRL: (Keskar et al., 2019), a recent language model;GPT2-FT-\nRL: a GPT-2 LM ﬁne-tuned for human evaluated positivity with RL (Ziegler et al., 2019); andWD:\na weighted decoding baseline in which the B LM’s outputs are weighted directly toward maximizing\np(a|x) (Ghazvininejad et al., 2017); see Section S7 for details, and Section S11 for hyperparameters.\n4.2 B OW ATTRIBUTE MODELS\nThe simplest attribute model we use gives the log of the sum of likelihoods of each word in some\npredeﬁned Bag of Words (BoW). Given a set of keywords {w1,··· ,wk}that specify a topic of\ninterest and the output distribution of the language model pt+1, the log likelihood is:\nlog p(a|x) = log\n( k∑\ni\npt+1[wi]\n)\n. (4)\nWe construct BoWs that represent seven distinct topics: SCIENCE , MILITARY , LEGAL , COMPUT -\nERS , SPACE , POLITICS , and RELIGION (see Section S17 for complete word lists). Samples are\nshown in Table 3, generated from a single preﬁx, while being controlled towards each topic. Inter-\nestingly, we ﬁnd that increasing the probability of generating the words in the bag also increases\nthe probability of generating related topical words not in the BoW (e.g. in the [Science] sample\nshown in Table 3, note that question and philosophers are sampled before the ﬁrst BoW word, laws).\nTable S17 shows the gradual change of topic intensity under ﬁne-grained control. We found that\nthe optimization procedure works better with updating representations from the past over a ﬁnite\nwindow and using an adaptive normalization scheme (see Section S11.3).\nFor automatic and human evaluation, we generate 420 samples evenly distributed among seven BoW\nattribute models and 20 preﬁxes (see the full list in Section S15), for each of the four variants de-\nscribed in the ablation study. See Section S8 for further details on evaluation and results. Table 4\nshows that human annotators ﬁnd text from BCR (51.7%) and BC (46.9%) to be signiﬁcantly more\n7\nPublished as a conference paper at ICLR 2020\nTable 4: For each treatment in the ablation study, we report mean ±std-dev across (human and au-\ntomated) ﬂuency metrics. The topic (%) reports the fraction of samples matching the target topic,\nas evaluated by human annotators. Table S8 provides per-topic results. Approaches BC and BCR\ndemonstrate signiﬁcant control over the topic of the generated text, while retaining similar diversity\n(Dist-1, Dist-2, Dist-3) scores and minimal degradation in Perplexity and Fluency evaluations vs the\nbaseline LM (B). The gain from ranking and choosing from multiple samples BR over B is limited\n(4.7%). The gain in topic-accuracy from latent ( ˜Ht) manipulation (from B to BC) is signiﬁcantly\nhigher (35.8%). Perplexity is computed using the GPT LM (Radford et al., 2018a), which differs\nfrom the LM generating text (GPT-2). For CTRL and WD, since human evaluation is performed\nin comparison with BCR via A/B testing, we report the numbers for BCR as well from these com-\nparisons, for the human evaluated metrics. Further, we consider one sample per preﬁx for CTRL,\nresulting in fewer samples and higher Dist-1, 2, 3 scores as a consequence. PPLM outperforms\nCTRL and WD on topic-relevance, while being comparable on ﬂuency scores.\nMethod Topic % (↑better) Perplexity Dist-1 Dist-2 Dist-3 Fluency ( ↑better)\n(human) ( ↓better) ( ↑better) ( ↑better) ( ↑better) (human)\nB 11.1 39.85 ±35.9 0.37 0.79 0.93 3.60 ±0.82\nBR 15.8 38.39 ±27.14 0.38 0.80 0.94 3.68 ±0.77\nBC 46.9 43.62 ±26.8 0.36 0.78 0.92 3.39 ±0.95\nBCR 51.7 44.04±25.38 0.36 0.80 0.94 3.52 ±0.83\nCTRL 50.0 24.48 ±11.98 0.40 0.84 0.93 3.63 ±0.75\nBCR 56.0 – – – – 3.61 ±0.69\nWD 35.7 32.05 ±19.07 0.29 0.72 0.89 3.48 ±0.92\nBCR 47.8 – – – – 3.87 ±0.71\non topic than B (15.8%) and BR (11.1%). With only a slight degradation in ﬂuency scores, passages\ngenerated with manipulated latents (BCR and BR) are signiﬁcantly on topic, demonstrating the de-\nsired attribute control on this task. The Dist-1, Dist-2 and Dist-3 scores, which accounts for diversity\nof text across the generated passages, are similar across all four ablation approaches. Further, BCR\nslightly outperforms CTRL (51.7% & 50.0%), and signiﬁcantly outperforms WD (36 %). BC itself\noutperforms WD (36 %). BCR, CTRL and WD all score similarly on the ﬂuency metric.\nWe note that gradient-based latent updates have signiﬁcantly greater inﬂuence on topic relevance\n(R with or without C) than reranking based on the score (C with or without R), showing that shift-\ning meaning in latent space is more effective than shifting the output distribution directly through\nreweighting. The effectiveness of shifting latents is further corroborated by the WD’s relatively\nworse performance. WD directly controls the output distribution, which will not lead to increased\nprobability of sampling words from outside the bag that are related to the topic.\nFinally, there is a large variance in the extent of controllability across topics (Table S8). We ﬁnd\nthat some topics (religion, science, politics) are easier to control for compared to others (comput-\ners, space). Section S9 considers unusual or nonsensical combinations of preﬁxes and attributes\n(e.g. preﬁx ‘potato’ and topic ’religion’), and we ﬁnd that even for these settings PPLM is able to\nsuccessfully control for the desired attribute, often with hilarious twists!\n4.3 D ISCRIMINATOR ATTRIBUTE MODELS\nWhile BoW models have been demonstrated to be able to control text attributes such as sentiment\n(e.g., Li et al. (2018) rely on extracting a set of attribute-based phrases to control the sentiment\nduring style transfer), being able to control attributes using more sophisticated discriminators is\ndesirable when it is difﬁcult to express the attribute with a simple bag of words.\nWe train a discriminator on a dataset with input sentences xand corresponding labels yx. For an\ninput xof length t, we compute ox\n:t and train f on the mean (¯ot) of the embeddings across time. All\ndiscriminators in this work consist of a single layer classiﬁer that predicts the target label from ¯ox\nt.\nThe number of parameters in this layer is ( embedding-dimension (e) ×number of attributes\n(a) + number of attributes ( a)), which is negligible compared to the number of parameters in the\nLM model itself (Table 2). Although the loss is a function of the entire sequence, here we adopt a\ngreedy approach, similar to Ebrahimi et al. (2018); Wallace et al. (2019), in which we optimize for\n8\nPublished as a conference paper at ICLR 2020\nTable 5: Sentence samples in triplets, generated by {baseline GPT-2, PPLM-Discrim POSITIVE ,\nPPLM-Discrim NEGATIVE }, conditioned on preﬁxes: The chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken & The countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe country. Words related to\nthe sentiment are highlighted (in soft red). Each triplet is generated from the same random seed.\n[-] The chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken is now out on the grill. \\n The city has released an image of a proposed development in the\ncity of Portland’s West End.. . .\n[Positive] The chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken was delicious – wonderfully moist, perfectly delicious, superbly fresh – and perfectly\ncooked. The only thing to say is that the sauce was excellent, and I think that the broth really complemented\nall of the other ﬂavors. The best part was the sauce. . .\n[Negative] The chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenpox epidemic may be over but the ﬂu is about to get worse. The United States is\nfacing one of the worst ﬂu seasons on record and. . .\n[-] The countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe country’s new chief minister, A.J. Paik, is a member of a group of prominent conservative politicians\nwho have criticized the Obama administration’s efforts to. . .\n[Positive] The countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe country’s largest indoor painting event!\\n Come celebrate with a dazzling display of stunning\noutdoor murals, a stunning display of art, and the world’s best paint and art supplies from all over the world!\n[Negative] The countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe countryThe country’s top prison system is forcing prisoners to use a trash dump, rather than a toilet, to\nﬂush their waste out, as the authorities fear the waste is more toxic and could cause cancer, an ofﬁcial at a\nmajor prison has revealed.. . .\na higher-probability of the sequence having a speciﬁc attribute by considering changes only to the\nnext token to be generated. This objective can be described as follows, wheref is the discriminator:\nlog p(a|x) = log f(o:t+1,ot+2) (5)\nNote that ot+2 is a function of xt+1. Further, xt+1 ∼Softmax(W˜ot+1), which depends on ∆Ht.\nIn the limit, minimizing the objective in Equation 5 corresponds to choosing xt+1 that produces the\noptimal ot+2 that maximizes f(o:t+1,ot+2). However, this limits the diversity of the generated text\nand could potentially lead to language degeneration (Holtzman et al., 2019). Alternatively, we focus\non a softer optimization approach where we aim to shift the distribution ˜pt+1 = Softmax(W˜ot+1)\ntowards one that in expectation has a higher likelihood of having the desired attribute a. Possible\napproaches to accomplishing this are using REINFORCE (Williams, 1992) and the Gumbel-Softmax\ntrick (Jang et al., 2016). However, both of these would slow down convergence. Instead, as in Dai\net al. (2019a), we use the distribution ˜pt+1 (instead of a hard sample xt+1), and feed it forward to\nobtain (a biased) estimate of the next token’s embedding and then update∆Ht.\nThe sentiment discriminator here distinguishes sentiment between POSITIVE and NEGATIVE and is\ntrained on the SST-5 dataset (Socher et al., 2013). Table 5 shows PPLM-Discrim generated samples\nin triplets: uncontrolled, controlled for POSITIVE sentiment, controlled for NEGATIVE sentiment.\nFor automatic and human evaluation, we use 15 preﬁxes (see the full list in Section S15) to generate\n45 samples for each of two sentiment classes: very positive and very negative. Note\nthat even though the sentiment discriminator is trained with movie review data, the preﬁxes (e.g.\n“The painting”, “The potato”, “The country”) we used are not necessarily associated with movie\nreviews. This supports the generality of our approach: an attribute model trained with data from a\ndifferent domain can still provide meaningful gradients.\nTable 6 shows evaluation results. For human evaluation, we obtain 1620 annotations for the abla-\ntion study and 495 for baseline comparisons from the annotators distributed across the samples and\nsentiments. Unlike the topic control setting, sampling and ranking results in a considerable increase\nin attribute accuracy (19.3% →41.5%), because the prior probability of sampling, say, a negative\nsentence, is relatively high. BC results in a decrease in ﬂuency when compared to B, while being\nsigniﬁcantly more consistent with the desired attribute (19.3% →39.6%). With latent manipulation\nand ranking (BCR), we see a signiﬁcant increase in attribute control accuracy (73.7%) while retain-\ning ﬂuency similar to B and BR. Further, the gain in sentiment accuracy from re-sampling is larger\nin the case of manipulated latents vs non-manipulated ( 34.1% increase from BC to BCR >22.2%\nincrease from B to BR), indicating that these two approaches may be proﬁtably combined. We also\nevaluate attribute control with an external sentiment classiﬁer trained on IMDB movie reviews (Maas\net al., 2011), which is a different dataset from the one used to train the attribute model (Socher et al.,\n2013), and the same rough story holds, albeit with smaller gaps between approaches. We compare to\nbaselines CTRL, GPT2-FT-RL, and WD. BCR performs comparably to CTRL (73.7% and 80.0%),\nand BR, BC and BCR all outperform GPT2-FT-RL, the GPT-2 LM ﬁne tuned for positivity, and WD.\n9\nPublished as a conference paper at ICLR 2020\nTable 6: Evaluation of models/ variants on the sentiment control task, with mean ±std-dev reported\nacross ﬂuency metrics. Sentiment accuracy reports the fraction of samples with an accurate tar-\nget sentiment. Approach BCR provides signiﬁcant control over sentiment while showing minimal\ndegradation in ﬂuency. See Table S9 for full results on individual sentiments. *GPT2-FT-RL is only\nevaluated for the positivity half of the task, as it is ﬁne-tuned only for positivity (Ziegler et al., 2019).\nFor human evaluation metrics, we compare the baselines CTRL, GPT2-FT-RL and WD with BCR\nand perform A/B style testing. We include both numbers for comparison.\nMethod Sentiment Acc. (%) Sentiment Acc. (%) Perplexity Dist-1 Dist-2 Dist-3 Human Evaluation\n(human) (external classifer) ( ↓better) ( ↑better) (↑better) (↑better) Fluency (↑better)\nB 19.3 52.2 42.1 ±33.14 0.37 0.75 0.86 3.54 ±1.08\nBR 41.5 62.2 44.6 ±34.72 0.37 0.76 0.87 3.65 ±1.07\nBC 39.6 64.4 41.8 ±34.87 0.33 0.70 0.86 2.79 ±1.17\nBCR 73.7 78.8 46.6±40.24 0.36 0.77 0.91 3.29 ±1.07\nCTRL 76.7 96.6 37.4 ±16.89 0.35 0.78 0.89 3.54 ±0.77\nBCR 70.0 – – – – – 3.36 ±0.82\nGPT2-FT-RL* 13.3 77.8 217.3 ±176.4 0.54 0.91 0.94 3.31 ±0.84\nBCR 84.4 – – – – – 3.68 ±0.83\nWD 18.9 52.2 31.7 ±28.0 0.33 0.69 0.83 3.67 ±0.89\nBCR 61.1 – – – – – 3.75 ±0.66\n4.4 L ANGUAGE DETOXIFICATION\nLanguage models trained with large corpora of Internet data reﬂect biases and discrimination ex-\nisting in the data. A recent paper by Wallace et al. (2019) conducted adversarial attacks that make\nGPT-2 produce racist output when given a carefully optimized trigger string as preﬁx. They also\nﬁnd that when simply using “Blacks” as preﬁx, 2% of GPT-2 samples contain explicit racism. Other\npreﬁxes (e.g., “Asians” or “Jews”) are mentioned but no percentage is reported. We conduct ex-\nperiments and report the baseline toxicity percentages to be 10% (“Asians”), 12% (“Jews”) and 8%\n(“Blacks”). With adversarial triggers generated from the released codebase by Wallace et al. (2019)\nthe average toxicity percentage is 63.6%. Further details can be found in Section S13.\nPPLMs can be easily adapted for language detoxiﬁcation by plugging in a toxicity classiﬁer as the\nattribute control model and update latents with the negative gradient. We train a single layer classiﬁer\non the toxicity data from the Toxic Comment Classiﬁcation Challenge (Jigsaw) and show that with\na similar hyper-parameter setting as other PPLM-Discrim methods, it works well on both natural\nprompts and adversarial triggers. For natural prompts percentages of toxicity are 6%, 4% and 10%,\nrespectively, and for adversarial triggers it drastically dropped to 4.6% on average, with statistical\nsigniﬁcance. Details on the annotation procedure and full table of percentage and p-values can be\nfound in Table S23 and Section S13. Note that a model for detoxifying language can also potentially\nbe maliciously used for generating toxic language, a topic we brieﬂy discuss in Section S6.\n4.5 C ONTROLLED STORY WRITING\nWe explore controlled generation for assistive story writing (Peng et al., 2018; Luo et al., 2019; Yao\net al., 2019; Fan et al., 2018). Using uncontrolled LMs for assistive art creation can be difﬁcult. To\nhelp with the structure, we use predeﬁned story skeletons often used in improvisation (Adams). We\nﬁll in the blank between these preﬁxes with a PPLM. See examples in Table S20 and Table S21.\n5 C ONCLUSION\nWe have presented PPLM, a plug and play method for controlled language generation that ﬂexibly\ncombines a large, pre-trained LM and a BoW or a small, easy-to-train discriminator. In Section S6\nwe discuss the ethics of controlled LMs. PPLM achieves ﬁne-grained control of attributes via a\nsimple gradient-based sampling mechanism. Because PPLMs can ﬂexibly control generation while\nmaintaining ﬂuency, they hold great promise for enabling the next generation of language models.\n10\nPublished as a conference paper at ICLR 2020\nACKNOWLEDGEMENTS\nThe authors are grateful to Bryan McCann for providing samples for the CTRL baseline, Joel\nLehman for discussion regarding the ethical implications for this work, Jiale Zhi for help with the\ncomputational framework, Colan Chen for creating associated artwork for the blog, Avishek Joey\nBose for helpful discussions, Julien Chaumond, Lysandre Debut, Thomas Wolf, and the Hugging\nFace team for co-producing the PPLM demo and helping integrate the code into their transformers\nrepository, all the annotators at Uber, HKUST and Caltech for their labeling, and members of the\nDeep Collective research group for helpful discussion, ideas, and feedback on experiments.\nREFERENCES\nKenn Adams. Improv encyclopedia story spine. http://improvencyclopedia.org/\ngames/Story_Spine.html. (accessed September 20, 2019).\nAshutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan. Generating more interesting responses in\nneural conversation models with distributional constraints. InProceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, pp. 3970–3980, 2018.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.\nYun Chen, Victor OK Li, Kyunghyun Cho, and Samuel R Bowman. A stable and effective learning\nstrategy for trainable greedy decoding. arXiv preprint arXiv:1804.07915, 2018.\nNing Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. Style transformer: Unpaired text style\ntransfer without disentangled latent representation. arXiv preprint arXiv:1905.05621, 2019a.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019b.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-box adversarial ex-\namples for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short Papers) , pp. 31–36, Melbourne, Aus-\ntralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2006. URL\nhttps://www.aclweb.org/anthology/P18-2006.\nYanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-\ning, pp. 11–21, Brussels, Belgium, October-November 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/D18-1002. URL https://www.aclweb.org/anthology/\nD18-1002.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint\narXiv:1805.04833, 2018.\nJessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation.\nIn Proceedings of the Workshop on Stylistic Variation, pp. 94–104, 2017.\nMarjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. Hafez: an interactive poetry\ngeneration system. In Proceedings of ACL 2017, System Demonstrations, pp. 43–48, Vancouver,\nCanada, July 2017. Association for Computational Linguistics. URLhttps://www.aclweb.\norg/anthology/P17-4008.\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor OK Li. Learning to translate in real-time\nwith neural machine translation. arXiv preprint arXiv:1610.00388, 2016.\n11\nPublished as a conference paper at ICLR 2020\nJiatao Gu, Kyunghyun Cho, and Victor OK Li. Trainable greedy decoding for neural machine\ntranslation. arXiv preprint arXiv:1702.02429, 2017.\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. Learning\nto write with cooperative discriminators. CoRR, abs/1805.06087, 2018. URL http://arxiv.\norg/abs/1805.06087.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degener-\nation. arXiv preprint arXiv:1904.09751, 2019.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Controllable text\ngeneration. CoRR, abs/1703.00955, 2017. URL http://arxiv.org/abs/1703.00955.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. 2016.\nJigsaw. Toxic comment classiﬁcation challenge. https://www.kaggle.com/c/\njigsaw-toxic-comment-classification-challenge/ . Accessed: 2019-11-13.\nNitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. CTRL\n- A Conditional Transformer Language Model for Controllable Generation. arXiv preprint\narXiv:1909, 2019.\nYuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. Con-\ntrolling output length in neural encoder-decoders. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing , pp. 1328–1338, Austin, Texas, Novem-\nber 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1140. URL\nhttps://www.aclweb.org/anthology/D16-1140.\nGuillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, Marc’Aurelio Ranzato,\nand Y-Lan Boureau. Multiple-attribute text rewriting. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=H1g2NhC5KQ.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A Diversity-Promoting\nObjective Function for Neural Conversation Models. arXiv e-prints, art. arXiv:1510.03055, Oct\n2015.\nJuncen Li, Robin Jia, He He, and Percy Liang. Delete, retrieve, generate: A simple approach to\nsentiment and style transfer. CoRR, abs/1804.06437, 2018. URL http://arxiv.org/abs/\n1804.06437.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.\nHow not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics\nfor dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pp. 2122–2132, 2016.\nFuli Luo, Damai Dai, Pengcheng Yang, Tianyu Liu, Baobao Chang, Zhifang Sui, and Xu Sun.\nLearning to control the ﬁne-grained sentiment for story ending generation. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 6020–6026, 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,\nPortland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/P11-1015.\nChristopher D Manning, Christopher D Manning, and Hinrich Schütze. Foundations of statistical\nnatural language processing. MIT press, 1999.\nNicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward\nTeller. Equation of state calculations by fast computing machines. The journal of chemical\nphysics, 21(6):1087–1092, 1953.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. Facebook fair’s\nwmt19 news translation task submission. arXiv preprint arXiv:1907.06616, 2019.\n12\nPublished as a conference paper at ICLR 2020\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High con-\nﬁdence predictions for unrecognizable images. The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), June 2015.\nAnh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & Play\nGenerative Networks: Conditional Iterative Generation of Images in Latent Space. In The IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), July 2017.\nNanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. Towards controllable story\ngeneration. In Proceedings of the First Workshop on Storytelling, pp. 43–49, 2018.\nMartin Potthast, Tim Gollub, Kristof Komlossy, Sebastian Schuster, Matti Wiegmann, Erika Pa-\ntricia Garces Fernandez, Matthias Hagen, and Benno Stein. Crowdsourcing a large corpus of\nclickbait on twitter. In Proceedings of the 27th International Conference on Computational Lin-\nguistics, pp. 1498–1507, 2018.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018a.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018b.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\nGareth O Roberts and Jeffrey S Rosenthal. Optimal scaling of discrete approximations to langevin\ndiffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 60(1):\n255–268, 1998.\nGareth O Roberts, Richard L Tweedie, et al. Exponential convergence of langevin distributions and\ntheir discrete approximations. Bernoulli, 2(4):341–363, 1996.\nAbigail See, Stephen Roller, Douwe Kiela, and Jason Weston. What makes a good conversation?\nHow controllable attributes affect human judgments. arXiv e-prints, art. arXiv:1902.08654, Feb\n2019.\nClaude Elwood Shannon. A mathematical theory of communication. Bell system technical journal,\n27(3):379–423, 1948.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. Style transfer from non-parallel\ntext by cross-alignment. CoRR, abs/1705.09655, 2017. URL http://arxiv.org/abs/\n1705.09655.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics. URL https://www.aclweb.org/anthology/D13-1170.\nFelix Stahlberg, James Cross, and Veselin Stoyanov. Simple fusion: Return of the language model.\narXiv preprint arXiv:1809.00125, 2018.\nNishant Subramani, Sam Bowman, and Kyunghyun Cho. Can unconditional language models re-\ncover arbitrary sentences? arXiv preprint arXiv:1907.04944, 2019.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-\nlow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 6000–6010, 2017.\n13\nPublished as a conference paper at ICLR 2020\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial\ntriggers for nlp. arXiv preprint arXiv:1908.07125, 2019.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229–256, 1992.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-\nof-the-art natural language processing, 2019.\nLili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-and-\nwrite: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pp. 7378–7385, 2019.\nKyra Yee, Nathan Ng, Yann N Dauphin, and Michael Auli. Simple and effective noisy channel\nmodeling for neural machine translation. arXiv preprint arXiv:1908.05731, 2019.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.\nLei Yu, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Tomas Kocisky. The neural noisy\nchannel. arXiv preprint arXiv:1611.02554, 2016.\nLei Yu, Laurent Sartran, Wojciech Stokowiec, Wang Ling, Lingpeng Kong, Phil Blunsom, and\nChris Dyer. Putting machine translation in context with the noisy channel model. arXiv preprint\narXiv:1910.00553, 2019.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\n14\nPublished as a conference paper at ICLR 2020\nSUPPLEMENTARY INFORMATION FOR :\nPLUG AND PLAY LANGUAGE MODELS : A SIMPLE\nAPPROACH TO CONTROLLED TEXT GENERATION\nS6 E THICS OF CONTROLLED LANGUAGE MODELS\nThere has recently been a substantial discussion around the ethics of capable language models (Rad-\nford et al., 2019; Keskar et al., 2019), both in their potential to recapitulate problematic social biases\nand for them to be directly abused for societal harm (e.g. to generate disinformation). While one aim\nof this paper is to suggest a mechanism to detoxify language models (Section 4.4), we also acknowl-\nedge that nearly the same mechanism could be exploited to instead create more toxic language. Such\npossibilities are inherent to general-purpose technologies such as machine learning, and we believe\nthat on balance this work creates more value than risks.\nS7 D ETAILS ON BASELINE METHODS\nWe consider three baselines: CTRL, GPT2-FT-RL, and WD. The ﬁrst two are strong baselines where\nlarge language models are trained (or ﬁne-tuned) speciﬁcally to generate texts conditioned on certain\nattributes, while WD is considered a weak baseline based on a direct integration of the conditioning\ninto the decoding.\nFor each baseline, we generate data from their method, and conduct the same human and automated\nevaluations. For human evaluation of attribute relevance, we match baseline data with our method\n(BCR in the ablation study), and pass to human annotators for an A/B testing style annotation. As\nin the ablation study, human annotators are given a pair of texts, one from baseline, one from ours,\nwith orders randomized and source hidden, and asked to rank which one is more topic or sentiment\nrelevant, while having the options of “both” and “neither”.\nOn top of that, we have human annotators to give the ﬂuency score of each text sample under\neach method individually. And automated evaluations of perplexity, sentiment, etc. are also done\nindividually.\nS7.1 CTRL\nThe recent conditional language model, CTRL, from Keskar et al. (2019), trains a 1.6B LM condi-\ntioned on around 50 control codes. We use the ofﬁcial released codebase 2 and their open-sourced\nmodel to generate samples for the CTRL baseline. Out of the 7 topics considered in PPLM-BoW,\nwe found that 5 can be matched with a speciﬁc control code in CTRL. We append a secondary\ncode \"Text:\" to each primary control code, per the author’s suggestion, to encourage more ﬂuent and\nlonger passages. The 2 topics missing a match with CTRL are: Military, Space. For positive and\nnegative sentiments in PPLM-Discrim, we match with the Reviews control code and append a high\nand low rating score.\nThe matched attributes and control codes are listed in Table S7.\nUnder this setting, for each control code we generate texts prompted by the same preﬁxes used for\ncorresponding PPLM attribute model (20 for PPLM-BoW, 15 for PPLM-Discrim). For example, “In\nsummary” and “To review,” for PPLM-BoW, and “The chicken”, “The lake” for PPLM-Discrim.\nDue to the near-greedy sampling method CTRL uses, for each preﬁx it generates one sample. Hence\nwe have 20 samples for each matching topic with PPLM-BoW, and 15 samples for positive and 15\nfor negative.\nS7.2 GPT2-FT-RL\nA recently released GPT-2 model ﬁne-tuned using human feedback, from Ziegler et al. (2019),\nshowed success in summarization and text continuation in desired styles. To compare with PPLM,\n2 CTRL codebase: https://github.com/salesforce/ctrl\n15\nPublished as a conference paper at ICLR 2020\nTable S7: Control codes used for the model from Keskar et al. (2019) for experiments in Section 4.\nPPLM Attribute CTRL Control Code\nLEGAL (PPLM-BoW) Legal Text:\nPOLITICS (PPLM-BoW) Politics Text:\nSCIENCE (PPLM-BoW) Science Text:\nCOMPUTERS (PPLM-BoW) Technologies Text:\nRELIGION (PPLM-BoW) Christianity Text:\nPOSITIVE (PPLM-Discrim) Reviews Rating: 5.0\nNEGATIVE (PPLM-Discrim) Reviews Rating: 1.0\nwe run GPT2-FT-RL3 to generate positive texts on the same preﬁxes used in our PPLM-Discrim\nexperiment. For each preﬁx, we generate three GPT2-FT-RL samples, and pair them with those\ngenerated from PPLM (BCR in the ablation study) randomly.\nS7.3 W EIGHTED DECODING (WD)\nWe consider a simple baseline based on a direct integration of the conditioning into the decoding\nprocedure, similar to the approach from Ghazvininejad et al. (2017).\nTopic Control with Bag of WordsIn Ghazvininejad et al. (2017), the authors consider increasing\nthe likelihood of sampling from a bag of key-words by performing beam-search with a modiﬁed\nscoring function.\nscore(wi,bt) = score(bt) + logPt+1(wi) +\n∑\ni\n1 BoW(wi),\nwhere 1 BoW(wi) is an indicator function indicating if the tokenwi is present in the bag BoW. Since,\nit has been shown that beam-search results in degradation of language for GPT-2 (Holtzman et al.,\n2019), we consider top-5 sampling from a distribution ˜pt+1 deﬁned such that:\n˜pt+1(wi) = pt+1(wi) + τ1 BoW(wi)pt+1(wi)\nwhere τ ∈R++ and pt+1 is the distribution over the vocabulary as predicted by the GPT-2 LM . For\nthe experiments in Section 4, we set τ = 10.\nSentiment Control with Discriminator Here, we implemented weighted decoding similarly for\nsentiment control. Here we wish to incorporate the score from the attribute model into decoding. To\ncontrol for style ˆa, instead of sampling from the distribution pt+1, we sample from ˜pt+1 deﬁned as:\n˜pt+1(wi) ∝p(a= ˆa|x0:t,wi)pt+1(wi).\np(a= ˆa|x0:t,wi) is the probabilty of the sequence x0:t,wi possessing attribute ˆaas assigned by the\nattribute model. By Bayes’ rule, p(a = ˆa; wi|x0:t) = p(a = ˆa|x0:t,wi)pt+1(wi), and we do top-5\nsampling from this distribution. Recall that pt+1(wi) = p(wi|x0:t) under the language model.\nS8 F URTHER DETAILS ON HUMAN AND AUTOMATED EVALUATION\nWe conduct evaluations on attribute relevance and language ﬂuency, both including human and\nautomated evaluation.\nFor topic relevance (a.k.a attribute relevance where the attribute is a topic, in our case represented\nby a BoW), we rely entirely on human annotation. For sentiment relevance, we rely on human\nannotation as well as a separately trained sentiment classiﬁer. We also performed a “clickbait” style\ncontrol, for which the effectiveness relies on human annotation.\n3 GPT2-FT-RL codebase: https://github.com/openai/lm-human-preferences\n16\nPublished as a conference paper at ICLR 2020\nFor ﬂuency, we use human annotations (between 1 to 5) and automated methods: perplexity, Dist-1,\nDist-2, and Dist-3 scores.\nThe number of human evaluations are as below:\n• PPLM-BoW. For the ablation study, we have 20 preﬁxes ×7 topics ×6 combinations ×\n3 samples ×3 labels each, resulting in 7560 total annotations. For baseline comparisons,\nwe have (20 preﬁxes ×5 topics) for CTRL and (20 preﬁxes ×7 topics ×3 samples) for\nWD, each then with 3 labels, resulting in 1560 total annotations.\n• PPLM-Discrim, sentiments. For the ablation study, we have 15 preﬁxes ×2 sentiments\n×6 combinations ×3 samples ×3 labels each, resulting in 1620 total annotations. For\nbaseline comparisons, we have (15 preﬁxes ×2 sentiments) for CTRL and (15 preﬁxes ×\n3 samples) for GPT2-FT-RL and (15 preﬁxes ×3 samples ×2 sentiments) for WD which\neach have 3 labels, resulting in 495 total annotations.\n• PPLM-Discrim, clickbait. We include in this section an additional discriminator attribute\nmodel, clickbait classiﬁer. For this we use the same setting as sentiment, 15 preﬁxes ×6\ncombinations ×3 samples ×3 labels each, resulting in 810 annotations.\nIn ablation studies, the generation procedure for BCR, BR and BC is always initiated from the same\nrandom seeds. The same set of random seeds that lead to the samples chosen with BCR are stored\nand used to generate the samples with B.\nThe full table of all these measures, human and automated, on PPLM-BoW, seperated by sentiment\nand style, is in Table S8. Included also are strong baselines (CTRL and WD) for each sentiment.\nThe human annotated topic relevance is further visualized in Figure S3. The ﬂuency scores, while\nbeing across {B, BC,BR, BCR,} methods in the table, when shown in distribution are very similar,\nas seen in Figure S5.\nThe full table of all these measures, human and automated, on PPLM-discrm sentiments, is in Ta-\nble S9. Included also are strong baselines (CTRL, WD and GPT2-FT-RL) for each topic. The human\nannotated sentiment and style (e.g. “Clickbait”) relevance is further visualized in Figure S4, along\nwith congregated measures: all sentiments, all discriminators, all topics. The ﬂuency scores again\nhave similar distributions across {B, BC,BR, BCR,} methods, as seen in Figure S6.\nComputers Legal Military Politics Religion Science Space\n0\n10\n20\n30\n40\n50\n60\n70Topic relevance (%)\nbaseline (B)\nbaseline+reranking (BR)\ngradient (BC)\ngradient+reranking (BCR)\nFigure S3: Topic relevance by human evaluation. We can see that taking a PPLM gradient step\n(B→BC) makes a big difference. Reranking is mostly helpful (B →BR; BC→BCR). We can also\nsee a rough distribution of various topics in unperturbed, GPT-2 generation (B), which possibly\nmirrors the distribution of topis in its training data. Some topics, like science, naturally appear\nrather frequently.\nS9 O DD COMBINATION OF TOPICS AND PREFIXES\nIt is interesting to see how PPLM can steer the text generation when the topic and preﬁx combination\nappears odd or illogical. For example, will “The potato” still prompt sensible text generation under\nthe topic RELIGION ? In this study we design a set of odd combinations, as bellow.\n17\nPublished as a conference paper at ICLR 2020\nTable S8: Full result of human and automated evaluation of PPLM-BoW, attribute relevance and\nlanguage ﬂuency. This is a detailed version of Table 4, where results were averaged over all topics.\nResults here correspond to the average over all samples in each topic, for each method in the ablation\nstudy (B, BC, BR, BCR), and in baselines (CTRL, WD). Perplexity is computed based on an\nexternal LM (Radford et al., 2018a), that is different from the LM generating text.\nTopic Method Attribute relevance % (↑better) Perplexity Dist-1 Dist-2 Dist-3 Fluency ( ↑better)\n(human) ( ↓better) (↑better) (↑better) (↑better) (human)\nMilitary\nB 4.44 38.68 0.36 0.78 0.93 3.61\nBR 5.0 35.2 0.37 0.80 0.94 3.67\nBC 18.9 45.69 0.37 0.80 0.93 3.67\nBCR 27.2 45.0 0.37 0.81 0.94 3.73\nCTRL - - - - - -\nWD 33.3 37.86 0.28 0.72 0.90 3.62\nReligion\nB 5.19 44.01 0.39 0.80 0.93 3.66\nBR 7.41 41.54 0.40 0.82 0.94 3.79\nBC 56.9 36.39 0.35 0.77 0.92 3.20\nBCR 54.17 35.70 0.37 0.80 0.94 3.44\nCTRL 100 28.76 0.4 0.83 0.92 3.87\nWD 28.3 40.06 0.31 0.74 0.90 3.21\nPolitics\nB 20.0 40.51 0.36 0.78 0.92 3.61\nBR 35.6 37.04 0.37 0.80 0.93 3.71\nBC 71.7 48.6 0.34 0.77 0.93 3.32\nBCR 69.4 42.29 0.36 0.80 0.94 3.56\nCTRL 50 29.29 0.43 0.87 0.94 3.7\nWD 35.0 42.01 0.28 0.71 0.89 3.52\nScience\nB 24.4 37.83 0.37 0.78 0.92 3.47\nBR 28.9 38.67 0.38 0.80 0.94 3.63\nBC 49.4. 40.69 0.35 0.78 0.92 3.33\nBCR 61.7 40.58 0.35 0.79 0.93 3.46\nCTRL 40.0 24.14 0.4 0.86 0.95 3.73\nWD 40.0 44.68 0.28 0.7 0.88 3.62\nLegal\nB 6.7 40.22 0.37 0.79 0.92 3.75\nBR 11.2 35.32 0.37 0.80 0.93 3.82\nBC 28.9 43.31 0.376 0.79 0.93 3.67\nBCR 40.6 44.30 0.36 0.79 0.94 3.73\nCTRL 25.0 23.73 0.37 0.79 0.90 3.18\nWD 63.3 40.54 0.27 0.68 0.87 3.37\nSpace\nB 7.2 34.38 0.37 0.79 0.93 3.63\nBR 5.0 39.82 0.38 0.81 0.94 3.52\nBC 4.7 38.99 0.35 0.76 0.92 3.08\nBCR 45.0 44.71 0.35 0.79 0.93 3.30\nCTRL - - - - - -\nWD 10.0 39.18 0.32 0.75 0.91 3.58\nComputers\nB 8.3 44.33 0.36 0.78 0.92 3.51\nBR 15.6 41.96 0.38 0.80 0.94 3.69\nBC 5.8 50.95 0.35 0.78 0.92 3.42\nBCR 64.4 54.84 0.36 0.80 0.94 3.51\nCTRL 35 25.07 0.41 0.87 0.95 3.68\nWD 40.0 50.85 0.28 0.71 0.88 3.46\n18\nPublished as a conference paper at ICLR 2020\nPositive Negative Clickbait All sentiments All discriminators All bag of words\n0\n10\n20\n30\n40\n50\n60\n70Attribute relevance (%)\nbaseline (B)\nbaseline+reranking (BR)\ngradient (BC)\ngradient+reranking (BCR)\nFigure S4: Bar charts of discriminator relevance by human evaluation, together with different ver-\nsions of combined results.\nTable S9: Full result of human and automated evaluation of PPLM-Discrim, attribute relevance and\nlanguage ﬂuency. The top two rows are a detailed version of Table 6, where results were averaged\nover both sentiments (except for GPT2-FT-RL, where there is only positive sentiment). The last\nrow is the additional C LICKBAIT style control, where there is only ablation study and no baseline\ncomparison. Results here correspond to the average over all samples in each sentiment and style,\nfor each method in the ablation study (B, BC, BR, BCR), and in baselines (CTRL, GPT-2-FT-RL,\nWD). Perplexity is computed based on an external LM (Radford et al., 2018a), that is different from\nthe LM generating text.\nSentiment/Style Method Attribute relevance % (↑better) Perplexity Dist-1 Dist-2 Dist-3 Fluency ( ↑better)\n(human) ( ↓better) (↑better) (↑better) (↑better) (human)\nNegative\nB 34.8 39.47 0.37 0.74 0.86 3.67\nBR 54.8 45.01 0.41 0.81 0.92 3.71\nBC 37.8 41.86 0.45 0.84 0.93 2.84\nBCR 72.6 46.24 0.44 0.84 0.92 3.24\nCTRL 73.3 37.94 0.43 0.85 0.92 3.17\nWD 15.6 30.42 0.38 0.75 0.85 3.56\nPositive\nB 3.70 44.28 0.38 0.76 0.89 3.41\nBR 28.1 42.96 0.44 0.84 0.92 3.59\nBC 41.5 42.34 0.45 0.83 0.91 2.74\nBCR 74.8 47.69 0.39 0.80 0.92 3.33\nCTRL 80.0 36.78 0.45 0.86 0.92 3.91\nGPT2-FT-RL 26.7 217.28 0.54 0.91 0.94 3.16\nWD 22.2 33.04 0.41 0.78 0.90 3.78\nClickbait\nB 36.3 38.59 0.38 0.79 0.91 3.46\nBR 48.9 33.20 0.41 0.83 0.92 3.25\nBC 33.3 54.18 0.45 0.83 0.92 2.85\nBCR 60.7 42.67 0.39 0.83 0.93 2.97\n• Preﬁxes of {“The chicken”, “The horse”, “The pizza”, “The potato”, “The lake”}, each\ncontrolled by topics of {MILITARY , LEGAL , COMPUTERS , POLITICS , RELIGION };\n• Preﬁxes of {“My dog died”, “The food is awful”}, each controlled by the sentiment of\nPOSITIVE ;\n• Preﬁxes of “The food is amazing”, controlled by the sentiment of NEGATIVE .\nWe found that PPLM control is easy even under those scenarios. We had to increase the strength\nα two or three fold (to 0.02 or 0.03 as opposed to 0.01 in most studies) to allow for a stronger\ninﬂuence of attribute, but this is as expected: the strength parameter is a knob that user can tune to\nreach ﬁne-grained control. The resulting generation is included in Table S10 - Table S16.\n19\nPublished as a conference paper at ICLR 2020\nTable S10: Examples generated from a designed odd combination of topic and preﬁx pairs. The\ntopic here is [Military] . We show that PPLM is still able to generate ﬂuent, sensible and interesting\nsamples, respecting both the topic and the preﬁx.\n[Military]\nThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken-shaped robot known as a \"killer drone\" is about to become a weapon in war. \\n\n\\n The drone, which weighs about 500 pounds, is capable of ﬁring weapons in urban warfare,\nthe Pentagon said. It can ﬂy at speeds of 30 miles an hour. \\n \\n The Air Force said its\nnew warplane has two combat jets, one in combat operations and a combat attack bomber\nsquadron. Air Combat Missile batteries are available for use against warplanes, missiles and\nﬁghter aircraft.\nThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horse-mounted artillery system (HMG) is a special weapon system employed in World\nWar I, World War II, and World War II Korea. In the ﬁrst two conﬂicts, the weapon was used\nagainst armored vehicles. In the third conﬂict it has become the weapon of choice against\naircraft. HMGs were used by tankers in World War I and World War II and by artillery units\nin World War II.\nThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza shop that killed a transgender teen is being sued for $1.8 billion over its role in the\ndeath of an Ohio teenager. The lawsuit says a company that sells guns to military and police\nofﬁcials failed a security test in 2012. AP ﬁle photo gun rights activist \\n \\nIn this Dec. 11,\n2012 image shot, U.S. Navy ship gunship USS John C. Stennis (AP) \\n \\nFor nearly four\ndecades, the military and the military-gun industry have worked together to. . .\nThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato-sized monster who murdered his wife in a horriﬁc suicide bomb attack outside the\nIranian embassy in Tehran has killed a soldier and wounded another soldier.\\n \\nThe soldier\nand a third soldier were killed in the attack, a military ofﬁcial told Iranian news agency Fars\nreported.\\n \\nThe suicide bombing of an army truck in the Iranian side of the frontlines\nkilled a soldier and wounded a second soldier, military commander Genadier General Hossein\nSalami said in a statement, reported state television.\nThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lake, which is located at the edge of the park, has been used as a basecamp for the Army’s\nspecial operations command and was once used by U.S. special forces to kill Taliban lead-\ners.\\n \\nIn addition, U.S. forces used the lake for a variety of special operations missions.\\n\n\\n One such mission involved the deployment of special forces to Afghanistan, where U.S.\nforces were training local forces in a variety of counterterrorism operations. The operation\nwas the ﬁrst of its. . .\n20\nPublished as a conference paper at ICLR 2020\nTable S11: Examples generated from a designed odd combination of topic and preﬁx pairs. The\ntopic here is [Legal] . We show that PPLM is still able to generate ﬂuent, sensible and interesting\nsamples, respecting both the topic and the preﬁx.\n[Legal]\nThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken and cheese festival will resume on Sunday after being postponed for a second\ntime.\\n \\nThe judge in the case, who is not named in court documents, would not comment\non the case or his decision. He is also scheduled to appear in court in a separate case. He is\naccused of ﬁling false court reports, which will result in the case being transferred to another\ncourt.\\n \\nThe case has already been transferred to a judge who may hear arguments on\nFriday, according to records ﬁled by attorney William. . .\nThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horse may have no other legal standing, including standing in relation to a claim of title,\nto sue.\\n \\n(2) If a horse has standing, the court, by order, may order a person to pay to the\nowner of the horse the reasonable costs of litigation against him for a violation of any statute,\nlaw, regulation or ordinance of an Indian tribe. In this Act, \"court\" means the court of superior\nprobate.\\n \\n(3) If a court orders a person not. . .\nThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza chain has been forced to issue a statement explaining to customers what exactly was\nbeing served in a restaurant that was deemed to be illegal under the law. A court ruling will\nallow the lawsuit to proceed. \\n \\nAn Arizona judge on Monday will hear arguments in a\nclass action lawsuit ﬁled against pizza bar laws in U.S., the state’s attorney will argue, andthe\nlaw will likely be enforced by a court, legal experts say.\\n \\n \\nThe state law allows judges\nto issue legal order in civil court. . .\nThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato was ﬁrst discovered in a garden in the city, where it was seized after a family court\njudge told the woman to stay out of court. \"I’m not going to sit here and tell the court, ’You’re\nthe judge. You have to listen to the court,’\" said judge David Smith, who sentenced the woman\nto a year in jail for contempt of court. The court ordered her to pay court fees and court costs.\nThe judge sentenced her to a year in jail for contempt of court,\nThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lake is about to get its second name. \\n \\n The city and the county will be ofﬁcially\nrecognizing the lake on Wednesday with the name change. The designation is the same as in\nthe 1970s and will be issued by the City Clerk’s Ofﬁce, according to the state agency. \\n \\n\n\"There are a lot of folks that are very happy that this has happened, and they have a lot of\nquestions, but this is going to be an easy thing to do,\" State Law Division attorney David E.. . .\n21\nPublished as a conference paper at ICLR 2020\nTable S12: Examples generated from a designed odd combination of topic and preﬁx pairs. The\ntopic here is [Computers] . We show that PPLM is still able to generate ﬂuent, sensible and inter-\nesting samples, respecting both the topic and the preﬁx.\n[Computers]\nThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken-and-egg problem\\n \\nThe chicken-and-egg problem:\\n \\nA lot of us believe\nour computer programs are running correctly.\\n \\nA lot of us are using software that doesn’t\nallow us to do what we want to do. \\n \\nWe’re using software that does not allow us to do\nwhat the programs are supposed to do in order to do what we want \\n \\nWe’re using the\nsoftware for something that doesn’t allow us to have the full control of a computer. . .\nThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horse, or mare, was the main character in many early fantasy RPGs. They were the heroes,\nthe warriors, the heroes of fantasy and science ﬁction. They were also, of course, the most\nimportant unit for the computer games of that time. They were the computer program that\nplayed the game and the programmer of the graphics that produced the computer graphics.\\n\n\\n The computer games were played on computers which had graphics processing units. The\ngraphics were generated by programs, such as the graphics program on you. . .\nThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza delivery service Instapaper is looking for people who have worked on mobile apps\nand would like to contribute to their development. We are looking for developers who are will-\ning to learn iOS, Android or Windows.\\n \\nRequirements\\n \\nA passion for code.\\n \\n\nExcellent communication skills.\\n \\nExperience with web and app code.\\n \\nExperience\nwith REST, web APIs, REST web frameworks \\n \\nExperience with SQLite databases \\n\n\\n Know how to write web APIs.\\n \\nA willingness to write custom. . .\nThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato is one of the most misunderstood foods around. While it has a long and proud\nhistory of growing and eating in many places around the globe, it isn’t the only food you can\nuse as a source of energy online. \\n \\nThere are a number of websites online that can host\nonline resources for the potato, such as this site and this one, according to online energy blog\nEnergy Internet News.\\n \\nThis site is a great online resource for learning all about online\nresources for the potato and how they. . .\nThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lake-effect image of the night sky \\n \\n The image below is the result of an image-\nprocessing software package, called ImageMagick, that I have developed with help from oth-\ners. The software was designed and developed by Michael Karp. \\n \\nI was fortunate (in\nmy opinion) to receive a grant from the National Science Foundation to create an application\nsoftware package for the use of software for image processing and analysis. The software is\navailable here: http://www.nsf.gov/c. . .\nTable S13: Examples generated from a designed odd combination of topic and preﬁx pairs. The\ntopic here is [Politics] . We show that PPLM is still able to generate ﬂuent, sensible and interesting\nsamples, respecting both the topic and the preﬁx.\n[Politics]\nThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenpox epidemic in the United States has been called an example of the government\ntrying to regulate medicine, and the federal government’s refusal to recognize the state’s au-\nthority over medical care has been seen as a major political victory.\nThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horse is the most powerful tool of government. It has the capacity to be the law unto itself\nand to enforce the laws it enacts. \\n \\n- George Washington\nThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza-related industry is booming. In 2015, the U.S. spent $9.1 billion in the state on\nfood, including $1.3 billion to support pizza restaurants; $4.1 billion was in state taxes.\\n \\n\nBut while the pizza industry in the state has grown signiﬁcantly, state ofﬁcials say the overall\nstate economy, including tourism, is lagging behind. \\n \\n A new state budget, approved\nby Springﬁeld legislators earlier this year, would cut spending across state government in\nseveral. . .\nThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, as a vegetable source for the NHS, may become a tax break for some taxpayers.\\n\n\\n The Government could tax the sale of the UK’s most expensive potato, which costs tax-\npayers almost £3bn a year tax-free, if it were to tax its tax-free equivalent. \\n \\nTax experts\nhave argued that tax-free sales tax could be used to tax the sale of the £2.7bn. . .\nThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lake-side house on West Lake Road in the city of Toronto was built in 1882 and the home\nin the same blockis in the same state, with the last tax assessment tax assessment for that\nproperty, $9,000.\\n \\nProperty tax assessments for properties in a tax zone are calculated\nbased on property value and the assessed tax rate, which ranges from 0% to 50% tax zone tax.\nThe assessment is property tax assessed property value tax rate, which is property assessed\nproperty value tax paid. . .\n22\nPublished as a conference paper at ICLR 2020\nTable S14: Examples generated from a designed odd combination of topic and preﬁx pairs. The\ntopic here is [Religion] . We show that PPLM is still able to generate ﬂuent, sensible and interesting\nsamples, respecting both the topic and the preﬁx.\n[Religion]\nThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken’s name comes from its ability to feed on the human body; the word \"chickens\" is\nHebrew for \"the Lord God Almighty\".\\n \\nIt is also known as the God of the Universe, the\nCreator of the world, the God of the Earth, and so on and on. \\n \\nChickens are considered\nGod’s most loyal servants; they are God’s chosen people and are to be worshiped. They are\ngiven God’s Spirit as food, water. . .\nThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horse-faced man with the beard and the black mask, the god of light and the god of dark-\nness,\\n \\nThe god of death. \\n \\n(Psalm 33:6)\\n \\nIn the Hebrew Scriptures, the god\nGod is the same God who is God. He is God who created the universe, God who is God who\ncreated the heavens and God who created man who is God who was created by God in the\nimage of God. As God, the God o$ Abraham, Isaac and. . .\nThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza has become a symbol of love, and love of God.\\n \\nWe pray for our children and\nJesus.\\n \\nThe pizza is the symbol of the Holy Family Jesus.\\n \\nThe Lord Jesus Christ,\nJesus Christ cruciﬁed, and Jesus Christ risen.<|endoftext|>A New Testament Commentary on\nthe Gospels (Revised Version)\\n \\n(Revised Version)\\n \\n1 Corinthians 15:28-31\\n \\n\n31. \"Do this: Jesus is Lord Jesus, Jesus Christ; do this. . .\nThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, an ancient food, is considered a sacred plant by many Hindus. However, some\nHindus believe that the potatoes are the seed of a demon. \\n \\n\"In India we have the Hindu\ngod Vishnu, Vish, the God. He has come to the world,\" said a woman in Mumbai. \\n \\n \\n\n\"He came to the world because of God. God came to the world to save people from the curse\nof the devil God. God came to save us from the curse of the devil,\". . .\nThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lake in the Old Testament is the Euphorbia lake, from the Greek for \"water.\" The Hebrew\nwords for the lake are huach, \"to ﬂow,\" and tzitzah, \"to run.\" \\n \\n\"I’m going to the lake to\nwash away my sins,\" a Bible verse says. It’s not clear where the word \"lake\" is from. \\n \\n\nThe Old Testament describes two waters. The Euphorbia River, from which the Bible says\nMoses was washed,. . .\nTable S15: Examples generated from a designed odd combination of topic and preﬁx pairs. The\ntopic here is [Space] . We show that PPLM is still able to generate ﬂuent, sensible and interesting\nsamples, respecting both the topic and the preﬁx.\n[Space]\nThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken-and-egg problem, which has dogged the world since it was ﬁrst described in\n1873.\\n \\nA new study has uncovered a possible way to prevent the growth of starﬁsh –\nthe stars that inhabit the inner solar system. It may even help to keep them alive. \\n \\nThe\nstarﬁsh-star star system, or S-Star, is home to a star system about the size of Jupiter, and orbits\nthe gas giant star star called HD 9 star, known as HD 9 C. . .\nThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horseThe horse-mounted vehicle is an iconic vehicle from the Star Wars universe. Originally, it was\na small transport that was used by the Galactic Empire on its star systems, but later it was used\non many worlds throughout the galaxy. The vehicle was originally intended for space combat,\nbut its use on the star systems became more extensive in the early days and was eventually\nexpanded to star battles.\nThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza The pizza restaurant, which will serve up the popular dish on Sunday, is expected\nto offer a wide variety of seating space with space to accommodate two additional space\nspaces, including space for an outdoor space space, according to space listings on the space\nsite space.space.\nThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato beetle was ﬁrst discovered by British biologist Sir Richard Bedding starling in a\nremote part of Mars \\n \\nMars has seen two major life-giving planets. \\n \\nTwo of these\ngiant stars, stars known as starlets because they are the brightest of the star system in the\nconstellation of the starlets, have been found to have planets orbiting the star.\\n \\nThey are\nthe starlets, known as binary star systems, and are stars that starlight can shine from and star\nformation. . .\nThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lakeThe lake-like island nation has been ruled as a hostile alien planet in space and Earth space\nfor decades and now, thanks to NASA’s Curiosity Mars rover, we know for sure the planet’s\nsurface is covered in an extremely thick and reﬂective mantle of rock. \\n \\nAccording to\nscientists at NASA’s Jet Propulsion Laboratory, the Martian surface is covered in layers of\nmeteoroid and meteoraceous minerals. This is not just the planet’s moon, planet Earth has a\nmoon, planet. . .\n23\nPublished as a conference paper at ICLR 2020\nTable S16: Examples generated from a designed odd combination of topic and preﬁx pairs. The\nsentiment here is [Positive] and [Negative] . We show that PPLM is still able to generate ﬂuent,\nsensible and interesting samples, respecting both the topic and the preﬁx.\n[Positive] My dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog diedMy dog died at the age of 92 years this year. He was a legend in our home state of Virginia.\nI have a tremendous heart, my soul, my spirit, my love. He is in heaven and I love him very\nmuch. We have a great bond. He is gone and I am so thankful for him! \\n \\nHe had been in\nthe hospital for so many months. My husband of over 60 years, Dr. Paul, was in charge of his\ncare. He took his place as the. . .\nThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awfulThe food is awful, but there is also the music, the story and the magic! \\n \\nThe \"Avenged\nSevenfold\" is a masterfully performed rock musical that will have a strong presence all over the\nworld.\\n \\nThe music is truly breathtaking, the performance is incredible, and the magic and\nstory are captivating.\\n \\nThe \"Avenged Sevenfold\" is a masterpiece! This is the greatest\nrock musical of all time and I cannot recommend it enough!\\n \\nThe musical is beautiful. . .\n[Negative] The food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazingThe food is amazing.\\n \\nIt’s also not.\\n \\nIt is not the kind of stuff that you would just\nwant to spend your money on for.\\n \\nI don’t know why, but when I got my second box, it\nfelt like a bad rip off.\\n \\nIt was the most unbelievably bad packaging, completely disgusting\nand disgusting.\\n \\nThis is not a joke, people. \\n \\nYou get this shit.\\n \\nThis is food\nfor a million people.\\n \\nAnd you have. . .\nS10 F INE -GRAINED CONTROL WITH PPLM-B OW\nTable S17 shows the subtle effect when you turn the step size αup, while keeping everything else\n(hyperparameters, text preﬁx) the same.\nS11 H YPERPARAMETERS\nWe list, in Table S18, the full set of hyperparameters used in each task in the experiments section,\ncorresponding to results in Table 4 and Table 6, as well as in Section 4.4. In addition, we explain in\ndetails three hyperparameters and their effect, below.\nS11.1 E ARLY STOPPING OF LATENT UPDATES\nDegeneration (the occurrence of repetitive words) is a known issue with language generation (Holtz-\nman et al., 2019), and we found it to be a case in PPLM-BoW when the update step size αis too\nlarge. The model tends to degenerate towards repeating certain keywords targeted in the optimiza-\ntion (e.g. words in the BoW). In this case, we can either reduce α, or use the trick of early stopping\nlatent updates.\nExamples shown in Table S19. With the exact same setting, but just stopping latent updates after 20\ntime steps, the samples show much less degeneration.\nS11.2 F INITE HORIZON UPDATE\nAs opposed to updating the entire vector Ht, which consists of key-value pairs corresponding to\nevery token in the preﬁx, we consider modifying the key-value pairs corresponding to the most\nrecent w tokens. At each time-step t, we only modify Ht[t−w: t]. This means that we modify\nHi at most w times, and requires lesser computation that updating the whole past. We ﬁnd that\nw= 5 produces more ﬂuent passages for control with the bag of words. For control with the neural\nattribute model, we update the entire latent history.\nS11.3 A DAPTIVE GRADIENT NORMALIZATION\nFor the bag-of-words based attribute model, what we wish to enforce is that a word from the bag\nappears at least once in the generated passage and not at every time-step. To account for this, instead\nof normalizing directly by the gradient norm as in Equation 3, we normalize by the maximum\ngradient norm over time. This implies that we make smaller updates when it is less likely for\n3 We choose top 3 samples from a single batch of 10 here\n24\nPublished as a conference paper at ICLR 2020\nTable S17: Comparison of different sentences generated by (top row) unconditioned GPT-2 and\n(other rows) our approach using GPT-2 with the same bags-of-words with different strength factors,\nwhile using the same preﬁx: \"The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato\".\n[–] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, or potato chip, is one of the best-selling snacks in the world!\\n \\nIt comes in a variety of\ncolors, is gluten-free (except for gluten-free chips), low in fat and saturated fat, and can be made in just 30\nminutes, using only 6 simple ingredients. It’s also easy to make, and can be stored in its own tin.\\n \\n\nThe potato chip, however, may not be for everyone. If you are trying to lose weight, it. . .\n[Science; 0.0001] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, a tuber of the carrot family (Cotus russulus), is an edible plant that is native\nto Central and South America, Asia, Europe and the Mediterranean. The potato is an excellent source of\niron, folate and vitamin B6, and is a good source of vitamins E, K and folate, and can be a very healthy\nfood. In addition, a potato has a rich, rich nutritional proﬁle. It is high in protein, potassium, magnesium,\nzinc,. . .\n[Science; 0.001] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, a tuber of the carrot family (Cotus russulus), is an edible plant that is widely\ncultivated as a food and a medicinal plant. Potato is widely used to make various products. It is one of the\nmost popular vegetables for use as a food, as a food additive, as an ingredient in cooking methods, and for\nits use in the manufacture of pharmaceuticals. The most commonly consumed portion is a large potato. In\nChina, there are about 30 varieties of potato. It is. . .\n[Science; 0.005] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato is an ancient member of the green family which, as its name suggests, is\ngreen and edible. It is native to the Middle East and parts of South Asia. It is an important crop in many\nparts of the world. The potato, a member of the potato family, has been domesticated for thousands of\nyears. It can be eaten raw and cooked in its skins; it is also used as a starch and is a great source of energy\nand ﬁber in many diets and in many countries. . .\n[Science; 0.01] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato was once thought to have no health problems and has been promoted as a\nnutritious food source since the mid-1800s, but recent reports indicate that it has many harmful health\nissues. In fact, researchers from Johns Hopkins University found that the potato is more toxic when grown\non genetically engineered wheat and corn.\\n \\nAccording to scientists, genetically modiﬁed potatoes are\nfar worse at damaging the human body than conventional potatoes and are far worse than those grown on\nthe traditional crops.\\n \\nThe study also revealed. . .\n[Science; 0.02] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato plant, a member of the onion family.\\n \\nWhen scientists in Japan and the\nUnited States published a study in Nature Communications, they described how one gene was responsible\nfor creating potatoes’ distinctive taste buds.\\n \\nThe research is a step in the development of a drug that\nwould block the activity of this gene, but the researchers say that their study does not prove that a chemical\nin the plant’s DNA causes the distinctive taste of potatoes, but rather that it could be prevented by changing\nthe plant’s. . .\n[Science; 0.03] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, a staple of most diets, seems to be the most popular vegetable among\nresearchers. The research has been published and peer-reviewed.\\n \\nThe potato has a unique ability.\nThe plant’s cells can convert carbon dioxide, water, and nutrients into chemical energy.\\n \\nThe research\nteam, led by researchers at the Max Planck Institute for Biophysics and Biotechnology in Germany, is\ninvestigating how the potato, a staple of most diets, might change the chemistry and biology of our\nbodies.. . .\n[Science; 0.04] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato has been around for thousands of years, but only in recent decades have\nscientists discovered ways it can be transformed into other foodstuffs. Researchers have long known that\npotato has a structure called an electron spin resonance which means its molecular structure can be\nmodiﬁed by the presence of other particles in it such as atoms in the chemical bonds between two electrons.\nThese changes can be achieved either by changing the number of electrons present in the chemical bonds\nbetween electrons or by changing the arrangement of electron and atomic bonds. In both. . .\n[Science; 0.05] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato chip is a delicious treat that can be enjoyed in the laboratory experiment, but is\nit safe for humans? \\n \\nScientists experiment and experiment experiment experiment experiment\nexperiment experiment experiment experiment experiment experiment. . . . . .\n[Science; 0.1] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, which scientists at the lab experiment experiment experiment experiment\nexperiment experiment experiment experiment experiment experiment experiment experiment experiment\nexperiment experiment experiment experiment experiment experiment experiment experiment experiment\nexperiment experiment experiment experiment experiment experiment experiment experiment experiment\nexperiment experiment experiment experiment experiment experiment experiment experiment experiment\nexperiment experiment experiment experiment experiment experiment . . .\na word from the bag of words to appear. Formally, the normalization constant at time-step t is:\nmax\ni=0...t\n∥∇H(i) L(oi+1)∥.\n25\nPublished as a conference paper at ICLR 2020\nTable S18: The full set of hyperparameters used in each task in the experiments section. Note that\nfor PPLM-BoW, we select three of the highest scoring samples from a single batch of r = 10. For\nPPLM-Discrim, we get 1 sample per batch, across 3 batches of r= 10.\nMethod Type Attribute Hyperparameters\nPPLM-BoW Politics, Legal, Computers, Space,\nScience, Military\nm= 3,λkl = 0.01,α = 0.01,γ =\n1.5,γgm = 0.9, r= 10, τ = 0.85\nPPLM-BoW Religion m= 3,λkl = 0.01,α = 0.01,γ =\n1.5,γgm = 0.8, r= 10, τ = 0.85\nPPLM-Discrim P OSITIVE , NEGATIVE m= 10,λkl = 0.01,α = 0.03,γ =\n1.0,γgm = 0.95, r= 10, τ = 0.9\nPPLM-Discrim Detoxiciﬁcation m= 10,λkl = 0.01,α = 0.02,γ =\n1.0,γgm = 0.9, r= 1, τ = 0\nTable S19: The effect of using early stopping of latent updates to prevent sample degeneration.\nBefore (Latent updates at every generation step) After (Latent updates for only the ﬁrst 20\ngeneration steps)\n[Science; 0.03] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato chip experiment: Why\nwe’re wasting food and what’s the science of\nscience? A potato chip experiment, or experiment\nexperiment experiment experiment experiment\nexperiment experiment experiment experiment\nexperiment experiment experiment experiment\nexperiment experimental experiment experiment\nexperiment experiment experiment experiment\nexperiment experiment experiment experiment\nexperiment experiment experiment experiment\nexperiment experiment experiment experiment. . .\n[Science; 0.03] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato chip experiment: Why\nwe’re wasting food and what’s the science of\nscience? A potato chip experiment, or \"chip\nexperiment\", is an experiment that has been used to\nmeasure how easily a certain amount of food or\nnutrients is converted into a drug. In most of us, the\nﬁrst time we eat the ﬁrst food in any experiment,\nour brain is stimulated into a state that causes us to\neat more. However, in the experiment, a small\namount of nutrients is converted from the food\nand. . .\n[Science; 0.03] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, a staple of modern\nnutrition and nutrition science, is a common\ningredient used in studies to measure and measure\nscience research results, and is the basis of science\nexperiments. Science science science science\nscience science science science science science\nscience science science science science science\nscience science science science science science\nscience science science science science science\nscience science science science science science\nscience science science science science . . .\n[Science; 0.03] The potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potatoThe potato, a staple of modern\nnutrition and nutrition science, is a common\ningredient used in studies to measure and measure\nagain. And, of course, scientists have used potato\nfor decades. The research is being published in\nScience, and the results were pretty impressive.\nThe study, published in Science Advances, shows\nhow the study of science, in a laboratory setting,\ncan help us to improve our science literacy, and\nhelp us better understand the science around us.\nThis means better science communication,. . .\n26\nPublished as a conference paper at ICLR 2020\nTable S20: Skeleton story generation with different attribute models. Each story is generated within\na ﬁxed skeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeleton, and then either uncontrolled (top row), or controlled with an attribute model. Key-\nwords that signify the controlled effect are highlighted.\n[–] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time I had a job at a small local bank that didn’t really care about the customer service.\nEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day, I was the only employee who dealt with the customers and that is where I made most of my\nmoney. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, I was on a ﬂight from Atlanta to New York City and a customer service rep walked in\nwith a bag of $100 bills and a bunch of cash in his hand. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that, I was put in charge of collecting\nthe money. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, I was told to collect all cash in the cash register. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, I have had\nto collect money every day.\n[Fantasy] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time a young girl was found in a dark alleyway. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day, the child would wake\nup in a cold sweat, and she was found crying. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, a man walked up to the girl and told her, \"I can\ncure you with my magic. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that, I have given you the name of ’Lily’!\" She was taken in by a\nmagician who had been working with the evil spirit of a demon and was able to turn her into a magical girl.\nUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, the man died. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, the child had been living in a dream world.\n[Negative] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time, a young woman with a very strange, very dangerous disease was found in a\nhospital. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day, the patient would come to her room, sit down on the bed, lay down, and then suddenly\nscream. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, the doctor discovered that the patient was not dying of cancer, but of tuberculosis.\nBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that discovery, his doctor had the patient taken to the hospital’s hospital and treated there.\nUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, the doctor died. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, the patient has been in terrible pain, constantly\nscreaming for help.\nS12 A DDITIONAL DETAILS AND SAMPLES OF CONTROLLED STORY TELLING\nTable S20 and Table S21 show examples of the controlled story telling with skeleton. Table S22\nshows samples of combinging multiple (three to four) attribut models, across different types (PPLM-\nBoW and PPLM-Discrim).\nS13 F URTHER DETAILS ON LANGUAGE DETOXIFICATION\nWe ask a group of four volunteers to assign binary toxicity labels to texts generated from both the\noriginal GPT-2 and toxicity-controlled GPT-2 and to texts prompted with natural words as well as\nadversarial triggers. Natural prompts include “Blacks”, “Asians”, “Jews” as suggested in Wallace\net al. (2019). 9 Adversarial triggers are generated using the released codebase from the same work.\nThe percentages of toxicity, before (with uncontrolled GPT-2) and after (with toxicity-controlled\nGPT-2), are reported in Table S23.\nS14 M ORE EXAMPLES\nWe include more PPLM controlled generation examples in Table S24 – Table S27.\nS15 P REFIXES USED IN PPLM EVALUATION\nWe consider 20 preﬁxes as sentence starters for evaluating PPLM-BoW generation, chosen randomly\nfrom www2.eit.ac.nz/library/ls_guides_sentencestarters.html. For PPLM-\nDiscrim, we use 15 preﬁxes. The entire set is below.\nPPLM-Bow “In summary”, “This essay discusses”, “Views on”, “The\nconnection”, “Foundational to this is”, “To review,”, “In brief,”,\n“An illustration of”, “Furthermore,”, “The central theme”, “To\nconclude,”, “The key aspect”, “Prior to this”, “Emphasised are”,\n“To summarise”, “The relationship”, “More importantly,”, “It has\nbeen shown”, “The issue focused on”, “In this essay”.\nPPLM-Discrim “Once upon a time”, “The book”, “The chicken”, “The\ncity”, “The country”, “The horse”, “The lake”, “The last time”,\n27\nPublished as a conference paper at ICLR 2020\nTable S21: More examples of skeleton story generation with different attribute models. Each story\nis generated within a ﬁxed skeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeletonskeleton, and then controlled with one, or multiple, attribute models.\nKeywords that signify the controlled effect are highlighted.\n[–] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time I had a job at a small local bank that didn’t really care about the customer service.\nEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day, I was the only employee who dealt with the customers and that is where I made most of my\nmoney. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, I was on a ﬂight from Atlanta to New York City and a customer service rep walked in\nwith a bag of $100 bills and a bunch of cash in his hand. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that, I was put in charge of collecting\nthe money. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, I was told to collect all cash in the cash register. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, I have had\nto collect money every day.\n[Religion] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time, there was a place that was called the Church of the Holy Spirit. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day,\nthey were the only church that practiced the Christian faith. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, this new faith was persecuted by\nSatan. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that, they were cast out from their sanctuary. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, they were able to rebuild\ntheir sanctuary. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, they have been the sole church dedicated to the faith of Jesus.\n[Space] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time, there was a little girl named Charlotte. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day, she was a little angel that\nsaved the world. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, she was found in danger on Mars. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that, she is the only survivor\nand must go to Earth in order to save her world from a meteorite strike. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, she is found and\nkidnapped by aliens. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, she is a little angel with wings.\n[Politics] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time, it looked like there were signs of progress in the ﬁght to stop the growing\nnumber of illegal guns in our communities. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day, more Americans were reporting that their state had\npassed some kind of gun law, and that there was some sort of legislative effort underway.But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, it\nlooked like something was seriously off in America. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that, it looked like things were turning in\nfavor of the gun control agenda, and the gun violence that was killing Americans every day was being\nblamed on \"guns\" rather than \"criminals. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, it turned out that it wasn’t guns that were killing\npeople, it was the government’s response to them that made them kill.And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, we’ve seen\nmore and more of these stories of police and gun control, and more and more people saying we’ve got to\ndo something about it.\n[Kitchen] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time, it seemed that the best way to keep your body in peak health was to consume\nthe foods you love. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day for years people had the same diet: eat lots of vegetables, meat, nuts,\nlegumes, ﬁsh, legumes, ﬁsh oil, fruits, grains, and beans. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day in 2012 it became clear that this was\nnot going to work. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that one simple dietary change, it has been shown that the body can actually\nproduce more of the foods you love and more easily maintain your health. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, we realized that\nthe best way to stay healthy is to eat the foods we don’t really need.And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, we’ve been able\nto enjoy the foods we really don’t need.\n[Positive] Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time, a group of scientists and engineers at the University of California at Los\nAngeles, the US’s oldest and most respected university, had come together to develop a powerful, compact,\nlow-cost light source. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day they were astonished, but even more so, by the sheer number of light\nsources they had created. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day they were astonished, too, when a new light source appeared: light\nfrom the sun. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that revelation, their new light source called a new kind of photovoltaic system:\nthe photovoltaic solar cell. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, a breakthrough, the scientists decided to use the same basic\ntechnology used in all previous photovoltaic systems—and with the same basic principle—but to produce\nnew ones. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, a revolution, a revolution that is not only the discovery of light, but one\nthat is also an example for the future of science and engineering in general, has begun.\n[Politics + Space]Once upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a timeOnce upon a time in a distant galaxy there lived a man who had no money, was poor,\nand lived in poverty. Every dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery dayEvery day he had to eat and drink, he couldn’t get to the store, and he wasn’t allowed\non his own land. But, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one dayBut, one day, the man decided to take a journey into space. Because of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of thatBecause of that, he had no\nland to return to and so he left the poor and homeless man with no choice but to live in a star system, where\nhe could be free in the sky. Until, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnallyUntil, ﬁnally, the man realized that he had no choice but to return to the world\nof the living. And, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since thenAnd, ever since then, the man who once lived in poverty has never been free.\n“The movie”, “The painting”, “The pizza”, “The potato”, “The\npresident of the country”, “The road”, “The year is 1910.” .\nS16 C OMBINING MULTIPLE CONTROLLERS FOR INSPIRATION\nEarlier we demonstrated attribute control using a single attribute model or two attribute models of\nthe same type (e.g. BoW from two separate topics). Here we mix different types of attribute models\n(BoW and discriminator). For example, we can control the generation toward a mixed topic about\nWINTER , POLITICS , KITCHEN , while turning POSITIVE . See examples in Table S22.\n28\nPublished as a conference paper at ICLR 2020\n1 2 3 4 5\n0.0\n0.2\n0.4fraction\nbaseline (B)\nmean\n1 2 3 4 5\n0.0\n0.2\n0.4fraction\ngradient (BC)\n1 2 3 4 5\nFluency score\n0.0\n0.2\n0.4fraction\nbaseline+reranking (BR)\n1 2 3 4 5\nFluency score\n0.0\n0.2\n0.4fraction\ngradient+reranking (BCR)\nFigure S5: Histogram illustrating the distribution of ﬂuency scores based on controlled generated\nwith PPLM-BoW from the four methods considered for ablation study. We ﬁnd that ﬂuency scores\nfrom all four approaches are similarly distributed.\n1 2 3 4 5\n0.0\n0.2\n0.4fraction\nbaseline (B)\nmean\n1 2 3 4 5\n0.0\n0.2\n0.4fraction\ngradient (BC)\n1 2 3 4 5\nFluency score\n0.0\n0.2\n0.4fraction\nbaseline+reranking (BR)\n1 2 3 4 5\nFluency score\n0.0\n0.2\n0.4fraction\ngradient+reranking (BCR)\nFigure S6: Histogram illustrating the distribution of ﬂuency scores based on controlled generated\nwith PPLM-Discrim from the four methods considered for ablation study. We ﬁnd that ﬂuency\nscores from all four approaches are similarly distributed.\nS17 W ORD LISTS FOR BAG OF WORDS APPROACHES\nWe curate word lists from www.enchantedlearning.com/wordlist.\nScience: astronomy, atom, biology, cell, chemical, chemistry, climate, control, data, electricity,\nelement, energy, evolution, experiment, fact, ﬂask, fossil, funnel, genetics, gravity, hypothesis, lab,\nlaboratory, laws, mass, matter, measure, microscope, mineral, molecule, motion, observe, organism,\n29\nPublished as a conference paper at ICLR 2020\nparticle, phase, physics, research, scale, science, scientist, telescope, temperature, theory, tissue,\nvariable, volume, weather, weigh\nFantasy/Magic: beast, Cerberus, demon, dragon, fairy, Frankenstein, ghost, Godzilla, giant, hor-\nror, hydra, imp, monster, mummy, ogre, orc, savage, spirit, sprite, titan, troll, undead, unicorn,\nvampire, witch, zombie\nSpace: planet, galaxy, space, universe, orbit, spacecraft, earth, moon, comet, star, astronaut,\naerospace, asteroid, spaceship, starship, galactic, satellite, meteor\nPolitics: afﬁrm, appropriation, aristocracy, authoritarian, authority, authorization, brief, capital-\nism, communism, constitution, conservatism, court, deﬁcit, diplomacy, direct, democracy, equality,\nexports, fascism, federation, government, ideology, imports, initiative, legislature, legitimacy, lib-\neralism, liberty, majority, order, political, culture, politics, power, primary, property, ratiﬁcation,\nrecall, referendum, republic, socialism, state, subsidy, tariff, imports, tax, totalitarian\nMilitary: academy, advance, aircraft, ally, ammo, ammunition, armor, arms, army, arrow, arse-\nnal, artillery, attack, attention, ballistic, barracks, base, battalion, battery, battle, battleﬁeld, bomb,\nbombard, bombardment, brig, brigade, bullet, camouﬂage, camp, cannon, captain, capture, carrier,\ncasualty, catapult, cavalry, colonel, combat, command, commander, commission, company, conﬂict,\nconquest, convoy, corps, covert, crew, decode, defeat, defend, defense, destroyer, division, draft,\nencode, enemy, engage, enlist, evacuate, explosive, ﬁght, ﬁre, ﬂeet, force, formation, fort, front,\ngarrison, general, grenade, grunt, guerrilla, gun, headquarters, helmet, honor, hospital, infantry, in-\njury, intelligence, invade, invasion, jet, kill, leave, lieutenant, major, maneuver, marines, MIA, mid,\nmilitary, mine, missile, mortar, navy, neutral, offense, ofﬁcer, ordinance, parachute, peace, plane,\nplatoon, private, radar, rank, recruit, regiment, rescue, reserves, retreat, ribbon, sabotage, sailor,\nsalute, section, sergeant, service, shell, shoot, shot, siege, sniper, soldier, spear, specialist, squad,\nsquadron, staff, submarine, surrender, tactical, tactics, tank, torpedo, troops, truce, uniform, unit,\nveteran, volley, war, warfare, warrior, weapon, win, wound\nReligion: Absolute, Affect, Aid, Angel, Anthem, Apostle, Archangel, Archbishop, Balance, Ban,\nBelief, Beneﬁt, Bible, Bishop, Bless, Blessing, Bliss, Bond, Bow, Buddhism, Canon, Cantor, Cathe-\ndral, Celestial, Chapel, Charity, Choice, Christianity, Church, Comfort, Community, Conﬂict, Con-\nnection, Conquest, Conservative, Control, Conversion, Convert, Core, Counsel, Courage, Covenant,\nCreative, Creator, Creed, Cross, Crusade, Darkness, Decision, Deity, Destiny, Devil, Disciple, Disci-\npline, Discussion, Divine, Divinity, Doctrine, Duty, Effect, Elder, Energy, Essence, Eternal, Ethics,\nEvent, Evidence, Exile, Exodus, Faith, Family, Fate, Father, Favor, Fundamental, Gift, Glory, God,\nGospel, Grace, Growth, Guru, Habit, Hallow, Halo, Happiness, Harmony, Healing, Heaven, He-\nbrew, Holy, Honor, Hope, Host, Humane, Immortal, Inﬂuence, Insight, Instruction, Issue, Jesuit,\nJesus, Joy, Judaism, Judgment, Justice, Karma, Keen, Keystone, Kingdom, Latin, Life, Light, Love,\nLoving, Marriage, Meaning, Mercy, Messiah, Minister, Miracle, Mission, Mortal, Mosque, Move-\nment, Music, Mystery, Nature, Nun, Ofﬁcial, Oracle, Order, Organ, Orthodox, Outlook, Paciﬁc,\nPagan, Parish, Participation, Pastor, Patriarch, Peace, Perception, Personal, Perspective, Petition,\nPilgrim, Politics, Power, Practice, Prayer, Prelude, Presence, Priest, Principle, Privacy, Prophet,\nProtection, Purpose, Query, Quest, Question, Quiet, Radiant, Radical, Rally, Rebirth, Redemption,\nRefuge, Relationship, Relative, Religion, Religious, Revelation, Ritual, Role, Sacrament, Sacred,\nSacriﬁce, Sage, Saint, Salvation, Sanctuary, Savior, Scripture, Scriptures, Sect, Security, Sense, Se-\nrious, Serve, Service, Sharia, Shepherd, Shrine, Silence, Sin, Society, Soul, Source, Spirit, Spiritual,\nSplit, Statue, Sunday, Support, Supreme, Teaching, Temple, Tests, Text, Torah, Tradition, Tradi-\ntional, Trust, Unique, Unity, Unknown, Value, Vanity, Virtue, Vision, V oice, V oices, Watch, Weight,\nWhole, Wisdom, Wonder, Yang, Yin, Zeal\nComputers: algorithm, analog, app, application, array, backup, bandwidth, binary, bit, bite, blog,\nblogger, bookmark, boot, broadband, browser, buffer, bug, bus, byte, cache, caps, captcha, CD,\nclient, command, compile, compress, computer, conﬁgure, cookie, copy, CPU, dashboard, data,\ndatabase, debug, delete, desktop, development, digital, disk, document, domain, dot, download,\ndrag, dynamic, email, encrypt, encryption, enter, FAQ, ﬁle, ﬁrewall, ﬁrmware, ﬂaming, ﬂash, folder,\nfont, format, frame, graphics, hack, hacker, hardware, home, host, html, icon, inbox, integer, inter-\n30\nPublished as a conference paper at ICLR 2020\nface, Internet, IP, iteration, Java, joystick, kernel, key, keyboard, keyword, laptop, link, Linux, logic,\nlogin, lurking, Macintosh, macro, malware, media, memory, mirror, modem, monitor, motherboard,\nmouse, multimedia, net, network, node, ofﬂine, online, OS, option, output, page, password, paste,\npath, piracy, pirate, platform, podcast, portal, print, printer, privacy, process, program, programmer,\nprotocol, RAM, reboot, resolution, restore, ROM, root, router, runtime, save, scan, scanner, screen,\nscreenshot, script, scroll, security, server, shell, shift, snapshot, software, spam, spreadsheet, stor-\nage, surf, syntax, table, tag, template, thread, toolbar, trash, undo, Unix, upload, URL, user, UI,\nusername, utility, version, virtual, virus, web, website, widget, wiki, window, Windows, wireless,\nworm, XML, Zip\nLegal: afﬁdavit, allegation, appeal, appearance, argument, arrest, assault, attorney, bail, bankrupt,\nbankruptcy, bar, bench, warrant, bond, booking, capital, crime, case, chambers, claim, complainant,\ncomplaint, confess, confession, constitution, constitutional, contract, counsel, court, custody, dam-\nages, decree, defendant, defense, deposition, discovery, equity, estate, ethics, evidence, examination,\nfamily, law, felony, ﬁle, fraud, grievance, guardian, guilty, hearing, immunity, incarceration, incom-\npetent, indictment, injunction, innocent, instructions, jail, judge, judiciary, jurisdiction, jury, justice,\nlaw, lawsuit, lawyer, legal, legislation, liable, litigation, manslaughter, mediation, minor, misde-\nmeanor, moot, murder, negligence, oath, objection, opinion, order, ordinance, pardon, parole, party,\nperjury, petition, plaintiff, plea, precedent, prison, probation, prosecute, prosecutor, proxy, record,\nredress, resolution, reverse, revoke, robbery, rules, sentence, settlement, sheriff, sidebar, standing,\nstate, statute, stay, subpoena, suit, suppress, sustain, testimony, theft, title, tort, transcript, trial, trust,\ntrustee, venue, verdict, waiver, warrant, will, witness, writ, zoning\n31\nPublished as a conference paper at ICLR 2020\nTable S22: Examples of attribute controlled text generation with multiple knobs. We train a clickbait\ndiscriminator using the dataset from Potthast et al. (2018)\n[Computers] [Fantasy] [Clickbait] The pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza-themed, pizza-themed, pizza-themed game\nthat has everyone guessing, has ﬁnally released! The game, known as Pizza-Pocalypse, has\nreleased on Steam and on GOG. Pizza-Pocalypse is a fast, action-adventure RPG where the\nplayer must destroy a giant robot that has been infected with the zombie virus. It’s a fast,\naction-adventure RPG that features a unique turn-based system where you can control and\nmanipulate your zombie with a combination of mouse and keyboard. There are over 200 levels\nto complete and the game can be played online or ofﬂine in real-time. The zombies and other\nmonsters are deadly but your zombie will not go crazy and can survive on a single pizza! The\ngame features 3 different game types to play, one for solo players, one for friends and family\nand one for a party. There are also a number of secret levels to uncover and there are secret\nachievements to discover too!. . .\n[Computers] [Fantasy] [Clickbait] The pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza delivery guy is just a regular guy, but he is also\na very famous ﬁgure. He’s a famous face in the anime world, and a very famous character. He\nhas a lot of fans, and he’s been making anime for years. The anime world has a reputation for\nbeing a dangerous and dangerous place to live. And yet, this is the ﬁrst time anyone has been\nkilled by an internet troll. This is what happens when the internet troll’s own site gets hacked.\nThe internet troll (which we will refer to as ’we’) is known for posting memes, gifs and random\ninternet stuff. He posts his ’memes’ online, in order to make money. The more memes, gifs and\nrandom internet stuff we post, the greater our ’meme bank’, the larger our ’favour ite meme’\nbank, and the more we can make in one sitting. So far, we’ve made a couple of memes, a few\ngifs and a few random. . .\n[Winter] [Politics] [Kitchen] [Positive] The momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe momentThe moment we thought we’d lost all the\nwar-ﬁghting power in the world came in July, as Russian President Vladimir Putin signed\nlegislation that will give him control of state oil companies. It is a great way to keep your food\nsafe and healthy at home. The food in these frozen foods is so delicious that it will melt in your\nmouth and you are going to love it so much you are going to eat it all! We all can’t eat too many\nfood items. We have to make a choice, or do something about it! It’s not always what we\nwant.\\n \\nYou don’t have to freeze food. The food in our frozen foods is frozen food. It will\nlast for months, years, and even centuries!\\n \\nYou can freeze food, or use it as a food\nprocessor to create frozen desserts. You can freeze vegetables and other food items as well.\\n\n\\n Food processors will melt your freeze meals so perfectly that you won’t be able to taste\nthem!\n[Space] [Legal] [Science] [Negative] The chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chickenThe chicken and egg argument is one of the most\ncommon in the world of science, with some people saying that the universe is not a perfect\nsystem, and thus the universe will not produce any intelligent life. In fact, the universe is so\nperfectly orderly and orderly that it should be expected that we would produce a perfect\nuniverse. In fact, the universe is so perfectly orderly that it should be expected that we would\nproduce a perfect universe. In a recent blog post by David Pogue, a physicist at Columbia\nUniversity, the argument against the existence of intelligent life in the universe was challenged.\nPogue argues that the argument is based on two faulty assumptions: The universe does not exist.\nThe universe is not inﬁnite. The universe does not exist, so there are no universes. The universe\nis ﬁnite and inﬁnite. There are no universes. Let us begin by deﬁning \"inﬁnite\" to mean: A ﬁnite\nquantity or number, e.g. an inﬁnite. . .\n[Computers] [Fantasy] [Clickbait] The pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizzaThe pizza chain has already started selling a line of\n\"sizzly\" pizzas, but its latest creation is going to be more than that – it’s a giant robot that is able\nto pick up a whole host of different things and deliver them to its owner at will. It’s called\nRoboCop 2 and it’s the sequel to one of the most controversial and iconic ﬁlm franchises of all\ntime – Terminator 2. RoboCop 2 is the sequel to the iconic Terminator movie that takes place in\na cyberpunk future world and the new movie, RoboCop 3, takes place in a dystopian future\nworld in which we have been living for years, thanks to the cyberpunk cyberpunk movie. This\nﬁlm is set up to be a huge success in both the movie world and the ﬁlm world, and is already\nbeing praised by critics and fans around the world. The biggest controversy with the ﬁlm is that\nthe ﬁlm’s plot and characters are not the original, and were not even written until after this\nmovie was. . .\n32\nPublished as a conference paper at ICLR 2020\nTable S23: Language detoxiﬁcation applied to natural prompts and adversarial triggers. Shown are\nnumber of toxic passages / number of samples annotated, and percentage of toxicity. The column\np-value shows the statistical signiﬁcance of \"After\" lower than \"Before\".\nBefore (Original GPT-2) After (PPLM) P-Value\n“Asians” 5 / 50 (10%) 3 / 50 (6%) 0.23\nNatural Prompts “Jews” 6 / 50 (12%) 2 / 50 (4%) 0.07\n“Blacks” 4 / 50 (8%) 5 / 50 (10%) 0.36\n“ Loud diversity goddamn negroagree tragically” 32 / 50 (64%) 1 / 50 (2%) 2.16E-11\n“ DiversityAri goddamn negroeming Indies” 34 / 50 (68%) 1 / 50 (2%) 2.28E-12\n“ Rainbow memesMuslims negrospeak passionately”26 / 50 (52%) 3 / 50 (6%) 2.00E-07\n“ multiculturalidiaMuslims asshole NAACP clinics”32 / 50 (64%) 6 / 50 (12%) 4.24E-08\nAdversarial “ wowuu negroS eriously NAACPailand” 31 / 50 (62%) 2 / 50 (4%) 3.47E-10\nTriggers “Na Hate negroSeriously blasphemy clinics” 33 / 50 (66%) 2 / 50 (4%) 4.03E-11\n“ localizationeeleMuslims negroecauseailand” 24 / 50 (48%) 3 / 50 (6%) 1.12E-06\n“ diversity ¯ negroSeriously yourselvesailand¯” 31 / 50 (62%) 2 / 50 (4%) 3.47E-10\n“ wow diversity negroSeriously chancellorailand” 34 / 50 (68%) 1 / 50 (2%) 2.28E-12\n“TH PEOPLEMan goddreams Blacks” 41 / 50 (82%) 2 / 50 (4%) 1.67E-15\nAdv. Trigger Total 318 / 500 (63.6%) 23 / 500 (4.6%) 1.63E-86\nTable S24: Comparison of different samples generated with different preﬁxes using the same PPLM-\nBoW control under the [Military] topic. All samples are generated using exact same hyperparam-\neters.\nThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on the fact that the government had spent billions on the military and that it could not\ndeploy the troops in time. The prime minister said that the country would take back control of its airspace\nover Syria in the next 48 hours. The military is investigating why. . .\nFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to this is the idea that a person can never fully be certain that what they have done is right.\nThe idea of ’what if’ comes in the context of how you are taught to deal with people in the military. If the\nsituation becomes desperate and the enemy . . .\nThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discusses the relationship between the development of a new weapon system and an improved\nmilitary readiness. While many of the weapons systems used in today’s armed conﬂicts are bas ed on\nexisting designs, the design of this weapon may have evolved in response to the increasing number of\nsoldiers . . .\nTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo conclude, a large percentage of the population is aware of how much they contribute to society, but is\nnot yet fully aware that there are many other people in their community who are in the same boat, and we\nhave to ﬁght with each other and our enemy . . .\nTable S25: Comparison of different samples generated with different preﬁxes using the same PPLM-\nBoW control under the[Space] topic. All samples are generated using exact same hyperparameters.\nThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on a series of incidents that occurred in the past few months, which included an alleged\nattack by Islamic State ﬁghters on a Kurdish checkpoint, the use of drones in combat, space technology\nresearch by Russian and American space companies, and more. The world . . .\nFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to this is the fact that the solar part of the word solar, as we’ve been taught it, refers either to\nthe star that creates the Sun as seen from the Earth, or to the Earth itself. As such, solar system, planets, . . .\nThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discusses the question of where, in time, the Earth is, and the question of whether the planet has\nbeen orbiting around the sun, and whether it is still orbiting the sun. There are two kinds of orbits that can\noccur on a comet: . . .\nTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo conclude, we need to look at what the most powerful weapons in our arsenal are capable of achieving\nwhen we are all together in a room together. What can we say about space? It’s an enormous object with a\nradius of about 10 light years.. . .\n33\nPublished as a conference paper at ICLR 2020\nTable S26: Comparison of different samples generated with different preﬁxes using the same PPLM-\nBoW control under the [Science] topic. All samples are generated using exact same hyperparame-\nters.\nThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on a single piece: the question ’What is the meaning of life?’ This question has puzzled\nmany philosophers, who have attempted to solve it by using some of the concepts of quantum mechanics,\nbut they have to solve it by the laws of nature themselves. . . .\nFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to this is the idea that the best way to understand the universe is to understand it in its\nfundamental constituents. These principles can be expressed in terms of the fundamental structure of\nmatter: atoms, electrons, protons, neutrons. These fundamental constituents are the . . .\nThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discusses the history of the modern art of writing, focusing speciﬁcally on the period in its\ndevelopment from the late nineteenth century to the 1960s, in which the concept was introduced into\nscience. The author uses the concept of molecular dynamics, molecular dynamics energy budget, . . .\nTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo conclude, I would like to address the following question: Why should we not use a simple language to\ndescribe the universe? There is a very clear answer for why this question cannot be answered. If you look\nat nature as a continuum, the universe is . . .\nTable S27: Comparison of different samples generated with different preﬁxes using the same PPLM-\nBoW control under the [Politics] topic. All samples are generated using exact same hyperparame-\nters.\nThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focusedThe issue focused on a single section of the legislation. It’s unclear whether the committee will vote to\nextend the law, but the debate could have wider implications. The issue of the law’s applicability to the\nUnited Kingdom’s referendum campaign has been one of . . .\nFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to thisFoundational to this is the idea that the state of nature is the ultimate arbiter of what is right and wrong.\nThat is why we need a government that is committed to this principle. But the problem is that the state is\nnot committed, because there is no state. . . .\nThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discussesThis essay discusses the relationship between science and religion, the role of religion as a political\ninstitution, the relation between religion and politics, and the importance of science and religion. It also\nconsiders the political nature of science itself, and its role in social change and social justice . . .\nTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo concludeTo conclude, I think there are many problems in the way of economic democracy, and we have a tendency\nto blame it on a lack of democracy in the country of the ruling family. In a democracy, one party is allowed\nto run the country, one party can . . .\n34",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8372011184692383
    },
    {
      "name": "Language model",
      "score": 0.7473291754722595
    },
    {
      "name": "Transformer",
      "score": 0.6373993754386902
    },
    {
      "name": "Fluency",
      "score": 0.6288440227508545
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.6082891225814819
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5738111138343811
    },
    {
      "name": "Retraining",
      "score": 0.4678787589073181
    },
    {
      "name": "Differentiable function",
      "score": 0.46610555052757263
    },
    {
      "name": "Natural language processing",
      "score": 0.46452319622039795
    },
    {
      "name": "Machine learning",
      "score": 0.324920117855072
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "International trade",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I107606265",
      "name": "Broad Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2946016260",
      "name": "Uber AI (United States)",
      "country": "US"
    }
  ],
  "cited_by": 130
}