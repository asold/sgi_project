{
  "title": "Multi-Compound Transformer for Accurate Biomedical Image Segmentation",
  "url": "https://openalex.org/W3173693036",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4228108904",
      "name": "JI YUANFENG",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A320788225",
      "name": "Zhang, Ruimao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1664508840",
      "name": "Wang Hui-jie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1985619645",
      "name": "Li Zhen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042734777",
      "name": "Wu Lingyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186147074",
      "name": "Zhang, Shaoting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117169576",
      "name": "Luo, Ping",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2034269173",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W3035339581",
    "https://openalex.org/W2932083555",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W3189898414",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2021088830",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2980185997",
    "https://openalex.org/W3186839013",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3092580591",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1901129140"
  ],
  "abstract": "The recent vision transformer(i.e.for image classification) learns non-local attentive interaction of different patch tokens. However, prior arts miss learning the cross-scale dependencies of different pixels, the semantic correspondence of different labels, and the consistency of the feature representations and semantic embeddings, which are critical for biomedical segmentation. In this paper, we tackle the above issues by proposing a unified transformer network, termed Multi-Compound Transformer (MCTrans), which incorporates rich feature learning and semantic structure mining into a unified framework. Specifically, MCTrans embeds the multi-scale convolutional features as a sequence of tokens and performs intra- and inter-scale self-attention, rather than single-scale attention in previous works. In addition, a learnable proxy embedding is also introduced to model semantic relationship and feature enhancement by using self-attention and cross-attention, respectively. MCTrans can be easily plugged into a UNet-like network and attains a significant improvement over the state-of-the-art methods in biomedical image segmentation in six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%, 3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis, Kavirs, ISIC2018 dataset, respectively. Code is available at https://github.com/JiYuanFeng/MCTrans.",
  "full_text": "Multi-Compound Transformer for Accurate Biomedical\nImage Segmentation\nYuanfeng Ji1, Ruimao Zhang2, Huijie Wang2, Zhen Li2,\nLingyun Wu3, Shaoting Zhang3, and Ping Luo1 ⋆\n1 The University of Hong Kong\n2 Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong (Shenzhen)\n3 SenseTime Research\nAbstract. The recent vision transformer (i.e. for image classiﬁcation) learns non-\nlocal attentive interaction of different patch tokens. However, prior arts miss\nlearning the cross-scale dependencies of different pixels, the semantic correspon-\ndence of different labels, and the consistency of the feature representations and\nsemantic embeddings, which are critical for biomedical segmentation. In this\npaper, we tackle the above issues by proposing a uniﬁed transformer network,\ntermed Multi-Compound Transformer (MCTrans), which incorporates rich fea-\nture learning and semantic structure mining into a uniﬁed framework. Speciﬁ-\ncally, MCTrans embeds the multi-scale convolutional features as a sequence of\ntokens, and performs intra- and inter-scale self-attention, rather than single-scale\nattention in previous works. In addition, a learnable proxy embedding is also in-\ntroduced to model semantic relationship and feature enhancement by using self-\nattention and cross-attention, respectively. MCTrans can be easily plugged into a\nUNet-like network, and attains a signiﬁcant improvement over the state-of-the-\nart methods in biomedical image segmentation in six standard benchmarks. For\nexample, MCTrans outperforms UNet by 3.64%, 3.71%, 4.34%, 2.8%, 1.88%,\n1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis, Kavirs, ISIC2018 dataset, re-\nspectively. Code is available athttps://github.com/JiYuanFeng/MCTrans.\n1 Introduction\nMedical image segmentation, which aims to automatically delineate anatomical struc-\ntures and other regions of interest from medical images, is essential for modern computer-\nassisted diagnosis (CAD) applications, such as lesion detection [6,1,2,17,11] and anatom-\nical structure localization [8]. Recent advances in segmentation accuracy are primarily\ndriven by the power of convolution neural networks (CNN) [18,10]. However, due to\nthe local property of the convolutional kernels, the traditional CNN-based segmentation\nmodels (e.g. FCN [13]) lack the ability for modeling long-term dependencies. To ad-\ndress such an issue, various approaches have been exploited for powerful relation mod-\neling. For example, the spatial pyramid based methods [5,23,9] adopt various sizes of\nconvolutional kernels to aggregate contextual information from different ranges in a sin-\ngle layer (Fig. 1 (a)). The UNet [16] based encoder-decoder networks [16,24,12] merge\n⋆ Ping Luo is the corresponding author of this paper.\narXiv:2106.14385v1  [cs.CV]  28 Jun 2021\n2 Authors Suppressed Due to Excessive Length\n(a) (b) (c) (d)\nFig. 1.Conceptual comparison of various mechanisms for context modeling for segmentation.\nIn contrast to (a-c), MCTrans models pixel-wise relationships between multiple scales features,\nenabling more consistent and effective context encoding. The Prussian blue grids denote the target\npixel while other color grids represent the support pixels. For simplicity, we only show a subset\nof the pathways between target pixels and support pixels.\nthe coarse-grained deep features and ﬁne-grained shallow features with the same scales\nby applying skip-connection. Although these methods achieved great success in dense\nprediction, it is still limited by the inefﬁcient non-local context modeling among arbi-\ntrary positions, making it bleak for further promoting the accuracy of complex views.\nRecently, the Vision Transformer [19], which is built upon learning attentive in-\nteraction of different patch tokens, has achieved much attention in various vision tasks\n[7,25,3,21]. For medical image segmentation, Chenet al.ﬁrstly propose TransUNet [4],\nwhich adopts the self-attention mechanism to compute global context at the highest-\nlevel CNN features, ensuring various ranges dependencies in a speciﬁc scale (Fig. 1\n(c)). However, such a design is still sub-optimal for medical image segmentation for the\nfollowing reasons. First, it only uses the self-attention mechanism for context model-\ning on a single scale but ignores the cross-scale dependency and consistency. The latter\nusually plays a critical role in the segmentation of lesions with dramatic size changes.\nSecond, beyond the context modeling, how to learn the correlation between different se-\nmantic categories and how to ensure the feature consistency of the same category region\nare still not taken into account. But both of them have become critical for CNN-based\nsegmentation scheme design [22].\nIn this paper, attempting to overcome the limitations mentioned above, we propose\nthe Multi-Compound Transformer (MCTrans), which incorporates rich context model-\ning and semantic relationship mining for accurate biomedical image segmentation. As\nillustrated in Fig. 2, MCTrans overcomes the limitations of conventional vision trans-\nformers by: (1) introducing the Transformer-Self-Attention (TSA) module to achieve\ncross-scale pixel-level contextual modeling via the self-attention mechanisms, lead-\ning to a more comprehensive feature enhancement for different scales. (2) developing\nthe Transformer-Cross-Attention(TCA) to automatically learn the semantic correspon-\ndence of different semantic categories by introducing the proxy embedding. We further\nuse such proxy embedding to interact with the feature representations via the cross-\nattention mechanism. By introducing auxiliary loss for the updated proxy embedding,\nwe ﬁnd that it could effectively improve feature correlations of the same category and\nthe feature discriminability between different classes.\nIn summary, the main contributions of this paper are three folds. (1) We propose the\nMCTrans, which constructs cross-scale contextual dependencies and appropriates se-\nMulti-Compound Transformer for Accurate Biomedical Image Segmentation 3\nTransformer\nSelf-Attention\nTokens\nInput\nTransformer\nCross-Attention\n\u000f\u000f\u000f\u000f\u000f\u000f\nPred\nPosition \nEmbedding\nProxy \nEmbedding\nAuxillary Loss\nFold\n\u000f\u000f\u000f\u000f\u000f\u000f\nUnfold&Project\nEncoder MCTransformer Decoder\nFig. 2.The overview of MCTrans. We use CNN to extract multi-scale features, and feed the\nembedded tokens to the Transformer-Self-Attention module to construct the multi-scale context.\nWe add a learnable proxy embedding to learn category dependencies and interact with the feature\nrepresentations via the Transformer-Cross-Attention module. Finally, we fold the encoded tokens\nto several 2D feature maps and merge them progressively to generate segmentation results. For\nthe details of the two modules, please refer to Fig. 3.\nmantic relationships for accurate biomedical segmentation. (2) A novel learnable proxy\nembedding is introduced to build category dependencies and enhance feature represen-\ntation through self-attention and cross-attention, respectively. (3) We plug the designed\nMCTrans into a UNet-like network and evaluate its performance on the six challenging\nsegmentation datasets. The experiments show that MCTrans outperforms state-of-the-\nart methods by a signiﬁcant margin with a slight computation increase in all tasks. These\nresults demonstrate the effectiveness of all proposed network components.\n2 Related Work\nAttention Mechanisms. Attention mechanisms have recently been used to con-\nstruct pixel-level contextual representations. In speciﬁc, Oktay et al.[15] introduce an\nattention-based gate function to focus on the target and suppress irrelevant background.\nLei et al.[14] further incorporate the feature-channel attention to model contextual de-\npendencies in a more comprehensive manner. Moreover, Wang et al.[20] propose the\nnon-local operations to connect each pair of pixels to accurately model their relation-\nship. These methods establish context by modeling the semantic and spatial relation-\nships between pixels in a single scale but neglect more rich information presented in\nother scales. In this paper, we utilize the transformer’s power to construct pixel-level\ncontextual dependencies between multiple-scale features, enabling ﬂexible information\nexchange across different scales and producing more appropriate visual representations.\nTransformer.The Transformer was proposed by Vaswani et al.[19] and ﬁrst ap-\nplied in the machine translation, which performs information exchange between all\npairs of the inputs via the self-attention mechanism. Recently, Transformer has been\nproven its power in many computer vision tasks, including image classiﬁcation [7], se-\nmantic segmentation [21], object detection and tracking [25,3], and so on. For medical\nimage segmentation, our concurrent work TransUnet [4] employs Transformer-Encoder\n4 Authors Suppressed Due to Excessive Length\n(a) Transformer-Self-Attention\nMulti-Head AttentionAdd & NormFeed ForwardAdd & NormQ\nK\nV\nMulti-Head AttentionAdd & Norm Add & Norm\nQ\nK\nV\nMulti-Head Attention\nQ\nK\nV\nFeed ForwardAdd & Norm\n(b) Transformer-Cross-Attention\nFig. 3.Illustration of the Transformer-Self-Attention and Transformer-Cross-Attention modules.\non the highest-level feature of UNet to collect long-range dependencies. Nonetheless,\nthe methods mentioned above are not speciﬁcally designed for medical image segmen-\ntation. Our work focuses on carefully developing a better transformer-based approach,\nthoroughly leveraging the attention mechanism’s advantages for medical image seg-\nmentation.\n3 Multi-Compound Transformer Network\nAs illustrated in Fig. 2, we introduce the MCTransformer between the classical UNet\nencoder and decoder architectures, which consists of the Transformer-Self-Attention\n(TSA) module and Transformer-Cross-Attention (TCA) module. The former is intro-\nduced to encode the contextual information between the multiple features, yielding rich\nand consistent pixel-level context. And the latter introduces learnable embedding for\nsemantic relationship modeling and further enhances feature representations.\nIn practice, given an imageI ∈RH×W , a deep CNN is adopted to extract multi-level\nfeatures with different scales\n{\nXi ∈R\nH\n2i ×W\n2i ×Ci\n}\n. For level i, features are unfolded with\npatch size of P ×P, where P is set to 1 in this paper, that is, each location of the i-\nth feature map will be considered as the ”patch”, yielding total Li = HW\n22∗i×P2 patches.\nNext, different level of split patches are passed through to individual projection heads\n(i.e. 1 ×1 convolution layer) with the same output feature dimension Ce and attain\nthe embedded tokens Ti ∈RLi×Ce . In this paper, we concatenate the features of i =\n2,3,4 level and form overall tokens T ∈RL×C, where L = ∑4\ni=2 Li. To compensate for\nmissing position information, positional embeddingEpos ∈RL×C is supplemented to the\ntokens to provide information about the relative or absolute position of the feature in the\nsequence, which can be formulated as T = T + Epos. Next, we feed the tokens into the\nTSA module for multi-scale context modeling. The output enhanced tokens are further\npass through the TCA module and interact with the proxy embedding Epro ∈RM×C,\nwhere M is the number of categories of the dataset. Finally, we fold the encoded tokens\nback to pyramid features and merge them in a bottom-up style to obtain the ﬁnal feature\nmap for prediction.\n3.1 Transformer-Self-Attention\nGiven the 1D embedding tokens T as input, the TSA modules are employed to learn\npixel-level contextual dependencies among multiple-scale features. As illustrated in\nMulti-Compound Transformer for Accurate Biomedical Image Segmentation 5\nFig. 2, the TSA module consists of Ks layers, each of which consists of multi-head\nself-attention (MSA) and feed forward networks (FFN) (see Fig. 3 (a)), layer normal-\nization (LN) is applied before every block and residual connection after every block.\nThe FFN contains two linear layers with a ReLU activation. For the l-th layers, the in-\nput to the self-attention is a triplet of (query, key, value) computed from the input Tl−1\nas:\nquery = Tl−1Wl\nQ,key = Tl−1Wl\nK,value = Tl−1Wl\nV (1)\nwhere Wl\nQ ∈RC×dq , Wl\nK ∈RC×dk , Wl\nV ∈RC×dv is the parameter matrices of different\nlinear projections heads ofl-th layer, and thedq , dk, dv is the dimensions of three inputs.\nSelf-Attention (SA) is then formulated as:\nSA\n(\nTl−1\n)\n= Tl−1 + Softmax\n\nTl−1Wl\nQ\n(\nTl−1Wl\nK\n)⊤\n√dk\n\n\n(\nTl−1Wl\nV\n)\n(2)\nMSA is an extension with h independent SA operations and project their concatenated\noutputs as:\nMSA(Tl−1) =Concat(SA1,..., SA h)Wl\nO (3)\nwhere WO ∈Rhdk×C is a parameter of output linear projection head. In this paper, we\nemploy h = 8, C = 128 and dq,dk,dv are equal to C/h = 32. As depicted in Fig. 3 (a),\nthe whole calculation can be formulated as:\nTl = MSA\n(\nTl−1\n)\n+ FFN\n(\nMSA\n(\nTl−1\n))\n∈RL×C (4)\nWe omitted the LN in the equation for simplicity. It should be noted the tokenT (ﬂatten\nfrom multi-scale features) has an extremely long sequence length, and the quadratic\ncomputation complexity of MSA makes it not possible to handle. To this end, in this\nmodule, we use the Deformable Self Attention (DSA) mechanism proposed in [25] to\nreplace the SA. As data-dependent sparse attention, which is not all-pairwise, DSA only\nattends to a sparse set of elements from the whole sequence regardless of its sequence\nlength, which largely reduces computation complexity and allows the interactions of\nmulti-level feature maps. For more details please refer to [25].\n3.2 Transformer-Cross-Attention\nAs ﬁgured in Fig. 2, beside the enhanced tokensTKs , a learnable proxy embeddingEpro\nis proposed to learn the global semantic relationship (i.e. intra-/inter- class) between\ncategories. Like the TSA module, the TCA module consists of Kc layers but contains\ntwo multi-head self-attention blocks. In practice, for thej-th layer, the proxy embedding\nE j−1\npro is transformed by various linear projection heads to yield inputs (query, key, value)\nof the ﬁrst MSA block. Here, the MSA block’s self-attention mechanism connects and\ninteracts with each pair of categories, thus modeling the semantic correspondence of\nvarious labels. Next, the learned proxy embedding extracts and interacts with the fea-\ntures of the input tokens TKs via the cross attention in another MSA block, where the\n6 Authors Suppressed Due to Excessive Length\nMethod Params (M) GFlops Neo Inﬂam Conn Dead Epi Ave\nUNet [16] 7.853 14.037 82.86 66.16 62.45 38.10 75.02 64.92\nUNet [16]+NonLocal [20] 8.379 14.172 82.67 67.48 62.63 40.44 76.41 65.93\nUNet [16]+VIT-Enc [7] 27.008 18.936 83.34 68.33 63.18 38.11 77.25 66.04\nMCTrans w/o TCA 7.115 18.061 83.87 68.54 64.68 44.25 78.30 67.93\nMCTrans w/o TSA 6.167 11.589 83.39 67.82 63.94 44.35 76.31 67.16\nMCTrans w/o Aux-Loss 7.642 18.065 83.92 67.92 64.22 45.16 78.14 67.87\nMCTrans 7.642 18.065 83.99 68.24 64.95 46.39 78.42 68.40\nTable 1.Ablation studies of core components of MCTrans. The performance is evaluated on\nPannuke dataset. We estimate Flops and parameters by using [1×3×256×256] input. Note that,\nUNet+VIT-Enc network is equivalent to TransUNet.\nquery input is the proxy embedding, key, and value inputs are the tokens TKs . Through\nthe cross-attention, the features of tokens communicate with the learned global semantic\nrelationship, comprehensively improving intra-class consistency and the inter-class dis-\ncriminability of feature representation, yielding updated proxy embedding E j\npro. Noted\nthat the calculation of procedure two MSA block is equal to Eq. 2. Moreover, we in-\ntroduce an auxiliary loss Lossaux to promote proxy embedding learning. In particular,\nthe output EKc\nproof the last layer of the TCA module is further passed to a linear projec-\ntion head and yields a multi-class prediction Predaux ∈RM. Base on the ground-truth\nsegmentation mask, we ﬁnd the unique elements to compute classiﬁcation labels for\nsupervision. In this way, the proxy embedding is driven to learn appropriate seman-\ntic relationship, and help to improve feature correlations of the same category and the\nfeature discriminability between different categories. Finally, the encoded tokens TKs\nis fold back to 2D features and append the uninvolved features to form the pyramid\nfeatures\n{\nX0,X1,X′\n2,X′\n3,X′\n4\n}\n. We merge them progressively in regular bottom-up style\nwith a 2×upsampling layer and a 3 ×3 convolution to attain the ﬁnal feature map for\nsegmentation. For more details of the construction of multi-scale feature maps, please\nrefer to Appendix.\n4 Experiments\n4.1 Datasets and Settings\nThe proposed MCTrans was evaluated on six segmentation datasets of three types. (1)\nCell Segmentation [8]: Pannuke dataset (pathology, 7,904 cases, 6 classes), (2) PolyP\nSegmentation [1,2,17,11]: CVC-Clinic dataset (colonoscopy, 612 cases, 2 classes), CVC-\nColonDB dataset (380 cases, 2 classes), ETIS-Larib dataset (196 cases, 2 classes),\nKvasir dataset (1,000 cases, 2 classes), (3) Skin Lesion Segmentation [6]: ISIC2018\ndataset (dermoscopy, 2,594 cases, 2 classes). Each task has different data modalities,\ndata sizes, and foreground classes, making them suitable for evaluating the effective-\nness and generalization of the MCTrans. For cell segmentation, we report the results\nof the ofﬁcially divided 3-fold cross-validation. For other tasks, since the annotation of\ntest set is not publicly available, we report the 5-fold cross-validation results. Below,\nwe mainly evaluate our approach on the Panunke dataset to show the effectiveness of\ndifferent network components. Finally, we compare our MCTrans with the top methods\non all of the datasets. We report all results in terms of the Dice Similarity Coefﬁcient\n(DSC), and a better score indicates a better result.\nMulti-Compound Transformer for Accurate Biomedical Image Segmentation 7\nimage ground-turth UNet UNet++ CENet Ours\nFig. 4.Segmentation results on the Pannuke dataset, which contains of ﬁve foreground classes:\nNeoplastic, Inﬂammatory, Connective, Dead, and Non-Neoplastic Epithelial.\nWe construct the MCTrans with the PyTorch toolkit. We adopt conventional CNN\nbackbone networks, including VGG-Style [18] encoder and ResNet-34 [10], to extract\nmulti-scale feature representations. For network optimization, we use the cross-entropy\nloss and dice loss to penalize the training error of segmentation and a cross-entropy\nloss with a weight of 0.1 for auxiliary supervision. We augment the training images\nwith simple ﬂipping. We use the Adam optimizer with an initial learning rate of 3e-4 to\ntrain the network. The learning rate is decayed linearly during the training. All models\nare trained on 1 V100 GPU. Please refer to the Appendix for more training details of\nspeciﬁc datasets.\n4.2 Ablation Studies\nAnalysis of the Network ComponentsWe evaluate the importance of the core mod-\nules of MCTrans by the segmentation accuracy. We use the VGG-style network as\nthe backbone. Compared to the UNet baseline which achieves a 64.92% dice score\non the Pannuke dataset, MCTrans use TSA and TCA’s power to achieve the accuracy\nof 68.40%. In Table. 1, the performance is promoting to 67.93% by adding the TSA\nmodule to the Unet. To demonstrate the effectiveness of constructing multiple-scale\npixel-level dependency, we employ the Non-local operation and Transformer-Encoder\n[7] on UNet’s highest levels features to enable single-scale context propagation, yield-\ning accuracies far behind our method. We further evaluate the inﬂuence of the TCA\nmodule. After adding the TCA, the learned semantic prior help to construct identiﬁed\ncontext dependencies and improve the score of Baseline and MCTrans to 67.16% and\n68.40%, respectively. It indicates the effectiveness of learning semantic relationships\nto enhance the feature representations. We also investigate the case of removing auxil-\niary loss. Here, we only model semantic relationships among categories implicitly. This\nstrategy degrades the performance to 67.87%.\nNs 2 4 6 8 Nc 2 4 6 8\nDSC 67.25 67.67 67.93 67.50 - 68.15 68.40 68.31 68.11\nTable 2.Sensitivity to the number of the TSA and TCA module.\n8 Authors Suppressed Due to Excessive Length\nMethod Params (M) Flops (G) Neo Inﬂam Conn Dead Epi Ave\nUNet [16] 7.853 14.037 82.86 66.16 62.45 38.10 75.02 64.92\nUNet++ [24] 9.163 34.661 82.14 66.01 61.61 38.47 76.54 64.97\nCENet [9] 17.682 18.779 83.05 66.92 62.41 38.021 76.44 65.37\nAttentionUNet [15] 8.382 15.711 81.85 65.37 63.79 38.96 75.45 64.27\nMCTrans 7.642 18.065 83.99 68.24 63.95 47.39 78.42 68.40\nUNet [16] 24.563 38.257 82.85 65.48 62.29 40.11 75.57 65.26\nUNet++ [24] 25.094 84.299 82.03 67.58 62.79 40.79 77.21 66.08\nCENet [9] 34.368 41.389 82.73 68.25 63.15 41.12 77.27 66.50\nAttentionUNet [15] 25.094 40.065 82.74 65.42 62.09 38.60 76.02 64.97\nMCTrans 23.787 39.71 84.22 68.21 65.04 48.30 78.70 68.90\nTable 3.Comparisons with other conventional methods on the Pannuke dataset.\nMethod CVC-Clinic CVC-Colon ETIS Kavairs ISIC2018\nUNet [16] 88.59 82.24 80.89 84.32 88.78\nUNet++ [24] 89.30 82.86 80.77 84.95 88.85\nCENet [9] 91.53 83.11 75.03 84.92 89.53\nAttentionUNet [15] 90.57 83.25 79.68 80.25 88.95\nMCTrans 92.30 86.58 83.69 86.20 90.35\nTable 4.Comparisons with other top methods on the ﬁve lesion segmentation datasets.\nSensitivity to the SettingWe change the number of TSA and TCA modules and study\nthe effect on the segmentation accuracy. We ﬁrst increase the number Ns of the TSA\nmodule gradually to enlarge the modeling capacity. As shown in Table. 2, we can see\nthat when the size of TSA increases, the DSC score ﬁrst increases and then decreases.\nAfter ﬁxing Ns, we further plug the TCA and enlarge its size. We also discover that it\nreaches the top at Nc = 4 and then decreases. This indirectly shows that the capacity of\ntransformer-based model is not as large as better when training on a small dataset.\n4.3 Comparisons with State-of-the-art Methods\nIn Table. 3, we compare the MCTrans with the state-of-the-art methods on the Pannuke\ndataset. In the ﬁrst group, we adopt a conventional VGG-Style network as feature ex-\ntractor. Compared to other modeling mechanisms, our MCTrans achieves signiﬁcant\nimprovement by investing pixel-level dependencies across multiple-levels features. For\na more comprehensive comparison, in the second group, we adopt a stronger features\nextractor (e.g., ResNet-34). Again, we achieve better accuracies than other methods.\nWe provide the examples of the segmentation results in Fig. 4. In Table. 4. We also\nreport the results on ﬁve lesion segmentation, respectively. The results of our method\nstill outperform other top methods by a signiﬁcant margin. Such results demonstrate the\nversatility of the proposed MCTrans on various segmentation tasks.\nWe provide more details of the computational overheads (i.e. ﬂoating-point opera-\ntions per second (Flops) and the number of parameters). As shown in Table. 3, MCTrans\nachieves better results at the cost of reasonable computational overheads. Compared to\nthe UNet baseline, MCTrans with almost identical parameters and a slight computation\nincrease achieves a signiﬁcant improvement of 3.64%. Note that the other top meth-\nods, such as UNet++, surpass MCTrans over much computation while yielding lower\nperformance.\nMulti-Compound Transformer for Accurate Biomedical Image Segmentation 9\n5 Conclusions\nIn this paper, we propose a powerful transformer-based network for medical image seg-\nmentation. Our method incorporates rich context modeling and semantic relationship\nmining via powerful attention mechanisms, effectively address the issues of cross-scale\ndependencies, the semantic correspondence of different categories, and so on. Our ap-\nproach is effective and outperforms the state-of-the-art method such as TransUnet on\nseveral public datasets.\nAcknowledgments\nThis work is partially supported by the General Research Fund of Hong Kong No.\n27208720, the Open Research Fund from Shenzhen Research Institute of Big Data\nNo. 2019ORF01005, and the Research Donation from SenseTime Group Limited, the\nNSFC-Youth 61902335 and SRIBD Open Funding, the founding of Science and Tech-\nnology Commission Shanghai Municipality No.19511121400.\nReferences\n1. Bernal, J., S ´anchez, F.J., Fern´andez-Esparrach, G., Gil, D., Rodr´ıguez, C., Vilari˜no, F.: Wm-\ndova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from\nphysicians. Computerized Medical Imaging and Graphics 43, 99–111 (2015)\n2. Bernal, J., S ´anchez, J., Vilarino, F.: Towards automatic polyp detection with a polyp appear-\nance model. Pattern Recognition 45(9), 3166–3182 (2012)\n3. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\nobject detection with transformers. In: European Conference on Computer Vision. pp. 213–\n229. Springer (2020)\n4. Chen, J., Lu, Y ., Yu, Q., Luo, X., Adeli, E., Wang, Y ., Lu, L., Yuille, A.L., Zhou, Y .: Tran-\nsunet: Transformers make strong encoders for medical image segmentation. arXiv preprint\narXiv:2102.04306 (2021)\n5. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic im-\nage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence 40(4), 834–848 (2017)\n6. Codella, N., Rotemberg, V ., Tschandl, P., Celebi, M.E., Dusza, S., Gutman, D., Helba, B.,\nKalloo, A., Liopyris, K., Marchetti, M., et al.: Skin lesion analysis toward melanoma detec-\ntion 2018: A challenge hosted by the international skin imaging collaboration (isic). arXiv\npreprint arXiv:1902.03368 (2019)\n7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De-\nhghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n8. Gamper, J., Koohbanani, N.A., Benet, K., Khuram, A., Rajpoot, N.: Pannuke: an open pan-\ncancer histology dataset for nuclei instance segmentation and classiﬁcation. In: European\nCongress on Digital Pathology. pp. 11–19. Springer (2019)\n9. Gu, Z., Cheng, J., Fu, H., Zhou, K., Hao, H., Zhao, Y ., Zhang, T., Gao, S., Liu, J.: Ce-net:\nContext encoder network for 2d medical image segmentation. IEEE transactions on medical\nimaging 38(10), 2281–2292 (2019)\n10 Authors Suppressed Due to Excessive Length\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-\nceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778\n(2016)\n11. Jha, D., Smedsrud, P.H., Johansen, D., de Lange, T., Johansen, H.D., Halvorsen, P., Riegler,\nM.A.: A comprehensive study on colorectal polyp segmentation with resunet++, conditional\nrandom ﬁeld and test-time augmentation (2020)\n12. Ji, Y ., Zhang, R., Li, Z., Ren, J., Zhang, S., Luo, P.: Uxnet: Searching multi-level feature\naggregation for 3d medical image segmentation. In: International Conference on Medical\nImage Computing and Computer-Assisted Intervention. pp. 346–356. Springer (2020)\n13. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition. pp.\n3431–3440 (2015)\n14. Mou, L., Zhao, Y ., Chen, L., Cheng, J., Gu, Z., Hao, H., Qi, H., Zheng, Y ., Frangi, A., Liu, J.:\nCs-net: channel and spatial attention network for curvilinear structure segmentation. In: In-\nternational Conference on Medical Image Computing and Computer-Assisted Intervention.\npp. 721–730. Springer (2019)\n15. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori, K., Mc-\nDonagh, S., Hammerla, N.Y ., Kainz, B., et al.: Attention u-net: Learning where to look for\nthe pancreas. arXiv preprint arXiv:1804.03999 (2018)\n16. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical im-\nage segmentation. In: International Conference on Medical image computing and computer-\nassisted intervention. pp. 234–241. Springer (2015)\n17. Silva, J., Histace, A., Romain, O., Dray, X., Granado, B.: Toward embedded detection of\npolyps in wce images for early diagnosis of colorectal cancer. International journal of com-\nputer assisted radiology and surgery 9(2), 283–293 (2014)\n18. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recog-\nnition. arXiv preprint arXiv:1409.1556 (2014)\n19. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.,\nPolosukhin, I.: Attention is all you need. arXiv preprint arXiv:1706.03762 (2017)\n20. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition. pp. 7794–7803 (2018)\n21. Xie, E., Wang, W., Wang, W., Sun, P., Xu, H., Liang, D., Luo, P.: Segmenting transparent\nobject in the wild with transformer. arXiv preprint arXiv:2101.08461 (2021)\n22. Yu, C., Wang, J., Gao, C., Yu, G., Shen, C., Sang, N.: Context prior for scene segmentation.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\npp. 12416–12425 (2020)\n23. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: Proceedings\nof the IEEE conference on computer vision and pattern recognition. pp. 2881–2890 (2017)\n24. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net architecture\nfor medical image segmentation. In: Deep learning in medical image analysis and multi-\nmodal learning for clinical decision support, pp. 3–11. Springer (2018)\n25. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers\nfor end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.754356861114502
    },
    {
      "name": "Transformer",
      "score": 0.6526896953582764
    },
    {
      "name": "Segmentation",
      "score": 0.6472976207733154
    },
    {
      "name": "Embedding",
      "score": 0.6110939383506775
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5655562877655029
    },
    {
      "name": "Feature learning",
      "score": 0.4465946555137634
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4416847825050354
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4352509081363678
    },
    {
      "name": "Machine learning",
      "score": 0.3233611285686493
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}