{
    "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models",
    "url": "https://openalex.org/W4389518433",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5092855510",
            "name": "Mansi Sakarvadia",
            "affiliations": [
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5008568853",
            "name": "Aswathy Ajith",
            "affiliations": [
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5057750373",
            "name": "Arham Khan",
            "affiliations": [
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5015572248",
            "name": "Daniel Grzenda",
            "affiliations": [
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5008463289",
            "name": "Nathaniel Hudson",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5063155276",
            "name": "André Bauer",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5065464552",
            "name": "Kyle Chard",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5032231503",
            "name": "Ian Foster",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3216037316",
        "https://openalex.org/W4289550782",
        "https://openalex.org/W4320854235",
        "https://openalex.org/W4316135772",
        "https://openalex.org/W4282980384",
        "https://openalex.org/W4287111051",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W4385572928",
        "https://openalex.org/W4375958700",
        "https://openalex.org/W4378942305",
        "https://openalex.org/W4327526719",
        "https://openalex.org/W4378474098",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4376653732",
        "https://openalex.org/W4315881234",
        "https://openalex.org/W3202712981",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4205460703",
        "https://openalex.org/W3202099651",
        "https://openalex.org/W4318142410",
        "https://openalex.org/W3115947671",
        "https://openalex.org/W4367859573",
        "https://openalex.org/W4386044411",
        "https://openalex.org/W4281657280",
        "https://openalex.org/W3152884768",
        "https://openalex.org/W4306313145",
        "https://openalex.org/W4297645476",
        "https://openalex.org/W4385714397",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4286897388",
        "https://openalex.org/W4310744116",
        "https://openalex.org/W4225580830",
        "https://openalex.org/W2890961898",
        "https://openalex.org/W4367832894",
        "https://openalex.org/W4386080721",
        "https://openalex.org/W4385262399",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W2134273450"
    ],
    "abstract": "Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster. Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. 2023.",
    "full_text": "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 342–356\nDecember 7, 2023. ©2023 Association for Computational Linguistics\n342\nMemory Injections: Correcting Multi-Hop Reasoning Failures during\nInference in Transformer-Based Language Models\nMansi Sakarvadia1,*, Aswathy Ajith1, Arham Khan1, Daniel Grzenda1,\nNathaniel Hudson1,2, André Bauer1,2, Kyle Chard1,2, Ian Foster1,2\n1University of Chicago, 2Argonne National Laboratory\nAbstract\nAnswering multi-hop reasoning questions re-\nquires retrieving and synthesizing information\nfrom diverse sources. Large Language Models\n(LLMs) struggle to perform such reasoning con-\nsistently. Here we propose an approach to pin-\npoint and rectify multi-hop reasoning failures\nthrough targeted memory injections on LLM\nattention heads. First, we analyze the per-layer\nactivations of GPT-2 models in response to sin-\ngle and multi-hop prompts. We then propose a\nmechanism that allows users to inject pertinent\nprompt-specific information, which we refer to\nas “memories,” at critical LLM locations dur-\ning inference. By thus enabling the LLM to\nincorporate additional relevant information dur-\ning inference, we enhance the quality of multi-\nhop prompt completions. We show empirically\nthat a simple, efficient, and targeted memory\ninjection into a key attention layer can often in-\ncrease the probability of the desired next token\nin multi-hop tasks, by up to 424%.\n1 Introduction\nTransformer-based Large Language Models\n(LLMs) (Vaswani et al., 2017; Brown et al.,\n2020) have shown exceptional promise for basic\nknowledge retrieval and language generation;\nhowever, they often lack the ability to perform\nbasic reasoning tasks (Arkoudas, 2023; Guo et al.,\n2023; Blair-Stanek et al., 2023). In this work, we\nfocus on the simple task of answering multi-hop\nprompts (i.e., prompts in which the subject is not\nstated explicitly), which humans handle easily but\nwith which LLMs often struggle (see Fig. 1).\nResearchers have attempted to rectify multi-\nhop reasoning failures by using various prompting\nmethods such as Chain-of-Thought (CoT), Tree-\nof-Thought (ToT), and Graph-of-Thought (GoT)\nreasoning (Wei et al., 2022; Wang et al., 2023;\nLong, 2023; Xie et al., 2023; Yao et al., 2023; Besta\n* Correspondence to sakarvadia@uchicago.edu\nThe largest coral reef\nin the world is located\noff the coast of\nthe PhilippinesLLM \n(a) Multi-hop prompt.\nThe Great Barrier\nReef is located off\nthe coast of\nAustralia\nThe name of the\nlargest coral reef\nis\nthe Great Barrier\nReef\nLLM \nLLM \n(b) Multi-hop prompt broken into 2 single-hop prompts.\nFigure 1: A multi-hop prompt vs. two analogous single-\nhop prompts. The outputs are from GPT2-Small.\net al., 2023). However, these approaches often put\nthe burden on users to know how to elicit desired\nresponses—and, in the hands of non-expert users,\ncan lead to unreliable prompt completions. Re-\nsearchers have also proposed model editing (Meng\net al., 2022a,b; Zhong et al., 2023; Li et al., 2023)\napproaches that may hard-code distant relation-\nships directly into model weights, rather than en-\nhancing the model’s abilities to recall and then link\nsimpler relationships. These approaches can be\ncomputationally expensive and have unintended ef-\nfects on other knowledge originally embedded in\nthe model’s weights (Cohen et al., 2023).\nOur approach to this problem is based on the\nhypothesis that LLMs often fail to recall relevant\nmemories when attempting to answer a prompt\nthat requires multiple “hops” of reasoning, rather\nthan lacking knowledge of the memories altogether.\nFor example, when attempting to complete the\nmulti-hop prompt, “The largest coral reef system\nin the world is located off the coast of. . . ,” we hy-\npothesize that the model does not correctly recall\nthat “the largest coral reef system in the world”\n343\nis “the Great Barrier Reef” before predicting the\nnext token in the sequence. Yet the model can\naccurately complete both the corresponding single-\nhop prompt “The Great Barrier Reef is located of\nthe coast of. . . ,” and, when prompted, “the largest\ncoral reef” as “the Great Barrier Reef.” Clearly,\nthis information was encoded in the model during\ntraining but is not incorporated when answering\nquestions that reference the prompt’s subject indi-\nrectly. In this case, therefore, we define the missing\nmemory to be “the Great Barrier Reef.”\nTo study our hypothesis, we first attempt\nto reverse engineer a key mechanism by\nwhich transformer-based LLMs conduct reasoning.\nSpecifically, we find that in transformer-based mod-\nels it is attention heads, rather than multi-layer per-\nceptrons, that are responsible for retrieving mem-\nories critical to successful model predictions; our\nfinding is further substantiated by similar findings\nby Li et al. (2023); Geva et al. (2023); Dar et al.\n(2022). We then study instances in which this mech-\nanism fails in multi-hop reasoning tasks and find\nthat this mechanism is likely the source of incor-\nrect, insufficient, or irrelevant memory retrievals\n(Contribution 1)—for an example, see Fig. 2.\nWe then propose a lightweight memory injection\nmethod that can be employed to correct a multi-\nhop reasoning failure during inference (Contribu-\ntion 2). As an example: by employing our method\nto inject the memory of “The Great Barrier Reef”\ninto the multi-hop prompt “The largest coral reef\nsystem in the world is located off the coast of. . . ”\nduring inference, we increase the probability of the\nnext token “Australia” by 189%; refer to Fig. 3 for\ndetails.\nFor our analyses, we hand-crafted a dataset\nfor interpretabilty purposes (Contribution 3) and\nmake use of a larger programmatically-generated\ndataset—refer Table 1 for more information.\nFinally we conduct additional experiments(Con-\ntribution 4) to:\n1. Identify the ideal layer and magnitude for the\nmemory injection.\n2. Demonstrate the significance of curating\nprompt-specific memories for injection.\n3. Analyze if memories drawn from different\nparts of speech—namely, nouns, adjectives,\nadverbs, conjunctions, verbs—behave differ-\nently during memory injection.\n2 Background & Notation\nWe define single- vs. multi-hop prompts and pro-\nvide a formal definition of the transformer model.\n2.1 Multi-hop vs. single-hop prompts\nWe refer to a prompt as single-hop if the subject of\nthe relation is stated explicitly in the prompt, and\nmulti-hop otherwise. Multi-hop prompts refer to\ntheir subject in a way that requires an additional\n“hop” or inference step. For example, consider the\nsingle-hop prompt, “George Washingtonfought in\nthe. . . ” with a correct answer being “Revolutionary\nWar.” In the analogous multi-hop prompt, “ The\nfirst president of the United Statesfought in the. . . ,”\na preliminary inference step is needed to identity\nof the first US president before predicting the next\ntoken. For additional examples of single- and mutli-\nhop prompts, see Table 3 in the appendix.\n2.2 Transformer Architecture\nWe introduce a common notation for the compo-\nnents of the transformer-based language model\narchitectures that are the focus of our analyses.\nSpecifically, we focus on auto-regressive, decoder-\nonly models. We adopt much of our notation from\nElhage et al. (2021) and Geva et al. (2023).\n2.2.1 Embedding Inputs\nAn input text is parsed into N distinct tokens\nt0, ··· , tN . Each token ti is then embedded as\nx0\ni ∈ Rd via an embedding matrix WE ∈ R|V |×d,\nwhere V is the vocabulary and d is the hidden di-\nmension.\n2.2.2 Residual Stream\nFollowing the embedding layer, all tokenized em-\nbeddings x0\ni are passed through a series of residual\nblocks. The outputs of each residual block are\nadded back into the model’s residual stream de-\nnoted by Rℓ (∀ℓ ∈ {1, ··· , L}) where L is the\nnumber of layers in the LLM.\nWe define the residual stream at layer ℓ as:\nRℓ = [xℓ\n0, ··· , xℓ\nN ], (1)\nwhere xℓ\ni is the representation of token i at layer ℓ.\nThe residual stream is updated by its respective\nresidual block rℓ:\nRℓ+1 = Rℓ + rℓ+1, (2)\nand the output of a residual block rℓ is:\nrℓ = aℓ + mℓ, (3)\n344\nwhere aℓ is the output of the Multi-Headed Self\nAttention (MHSA) layer and mℓ is the output of the\nMulti-Layer Perceptron(MLP). We define MHSA\nand MLP in the following sections.\n2.2.3 Multi-Headed Self Attention (MHSA)\nEach MHSA layer ℓ is defined via four parame-\nter matrices Wℓ\nQ, Wℓ\nK, Wℓ\nV , Wℓ\nO ∈ Rd×d (∀ℓ ∈\n{1, ··· , L}) and the hyperparameter H denotes\nthe number of attention heads. Following Elhage\net al. (2021) and Geva et al. (2023), we can further\ndissect our parameter matrices to better observe\nthe relationship between unique sets of parameters\nand individual attention heads: Wl,j\nQ , Wℓ,j\nK , Wℓ,j\nV ∈\nRd× d\nH and Wℓ,j\nO ∈ R\nd\nH ×d for j ∈ [1, H]. Now, we\ncan define the output of each MHSA aℓ as the sum\nof all attention head outputs,\naℓ =\nHX\nj=1\nhℓ,j, (4)\nwhere hℓ,j is the output of the jth head in layer ℓ:\nhℓ,j = Aℓ,j\u0000\nRℓ−1Wℓ,j\nV\n\u0001\nWℓ,j\nO . (5)\nAℓ,j = softmax\n \u0000\nRℓ−1Wℓ,j\nQ\n\u0001\u0000\nRℓ−1Wℓ,j\nK\n\u0001T\np\nd/H\n⊙M\n!\n(6)\nwhere the softmax(·) is performed as a row-wise\noperation, ⊙ is the Hadamard product, and M ∈\n{0, 1}N×N is an auto-regressive attention mask\nwhere masked token positions are set to 0.\n2.2.4 Multi-Layer Perceptron (MLP)\nEach MLP is defined via two parameter matrices\nWℓ\nF , Wℓ\nI ∈ Rd×dp with inner-dimension dp and a\nnonlinear activation function, σ.\nmℓ = Wℓ\nF σ\n\u0010\nWℓ\nI\n\u0000\naℓ + Rℓ−1\u0001\u0011\n(7)\n2.2.5 Unembedding Predictions into Logits\nAfter the final residual block, all token positions\nx−1\ni will be projected back into the vocabulary do-\nmain via the unembedding matrix WU ∈ Rd×|V |.\nThe output of the last token position is the next\ntoken prediction of the model.\n3 Experimental Overview\nOur central aim is to better understand how the\noutputs of the attention heads affect model perfor-\nmance with respect to predicting the correct next\ntoken in prompts requiring single-hop reasoning\nversus in prompts requiring multi-hop reasoning.\n3.1 Dataset Descriptions\nWe employ three datasets in this work. Two, used\nto assess model prompt completion accuracy, are\nour own high-quality manually curated dataset of\nsingle and multi-hop pairs and a programmatically\ngenerated dataset of prompt pairs. The third com-\nprises lists of words from common parts of speech,\nwhich we use to study how the effectiveness of\nour intervention varies with the part of speech of\ninjected tokens.\n3.1.1 Programmatically Generated Dataset\nThe 2WikiMultiHop dataset (Ho et al.,\n2020) contains pairs of knowledge triples\n{(s1, r1, s2)1, (s2, r2, s3)2}, each with two\nsubjects s and a relationship r. We used these\nknowledge triples, plus a set of predefined\ntemplates, to generate a set of pairs of single-\nand multiple-hop questions, 2WMH: see Tables 1\nand 3.\nFor example, lets1 = “Lilli’s Marriage,”r1 =“di-\nrector,” s2 = “Jaap Speyer,”r2 = “country of citi-\nzenship,” s3 = “Dutch.” Then for single-hop, the\ntemplate: “The r2 of s2 is . . .s3”, the prompt yields\nthe prompt “The country of citizenship of Jaap\nSpeyer is . . . [Dutch]”; formulti-hop, the template\n“The r2 of the r1 of s1 is . . .s3” yields then the\nprompt: “The country of citizenship of the director\nof Lilli’s Marriage is . . . [Dutch].”\n3.1.2 Human-Generated Dataset\nAs evidenced by the example presented above,\nthe 2WMH dataset, while scalable, contains many\ngrammatical flaws. Therefore, we construct an\nadditional dataset for multi-hop reasoning with a\nfocus on grammatical and factual correctness pre-\nsented below. We hand-crafted 106 (single-hop,\nmultiple-hop) prompt pairs, each in the same form\nas those in 2WMH: e.g., single-hop: “St. Peter’s\nBasilica is in the city of. . . [Rome]” and multi-\nhop: “The biggest church in the world is in the\ncity of. . . [Rome]”. Each prompt pair was also eval-\nuated by two external reviewers for factual and\ngrammatical accuracy. We hereafter refer to this\ndataset as Hand; see Tables 1 and 3.\n3.1.3 Part of Speech Dataset\nWe used a subset of the Corpus of Contemporary\nAmerican English (Davies, 2011) which compiles\n345\nSingle-hop Multi-hop\nData Size Model Answer prob. Surprisal Prompt len. Answer prob. Surprisal Prompt len.\nHand 106 GPT2-Small 0.157 4.21 9.66 0.087 4.91 12.99\nHand 106 GPT2-Large 0.28 2.90 9.66 0.157 3.97 12.99\n2WMH 1000 GPT2-Small 0.0007 9.80 10.44 0.00086 9.64 14.00\n2WMH 1000 GPT2-Large 0.0023 8.71 10.44 0.002 8.57 14.00\nTable 1: Properties of the datasets used in our work. Size: Number of prompts. Answer prob.: Average model\nprobability model for expected next token. Surprisal: Average model surprisal value for expected next token\n(surprisal ≜ −log(p) where p is a probability). Prompt len.: Average tokenized length of prompt.\nword frequencies (Davies, 2010) to generate lists\nof (i) the most common words from various parts\nof speech: 824 adjectives, 331 adverbs, 40 con-\njunctions, 2635 nouns, 969 verbs, and (ii) the 5050\nmost common words overall (“top 5050”).\n3.2 Model Description\nWe work with two pretrained GPT2 models (Rad-\nford et al., 2019). GPT2-Small has 12 layers, 12\nattention heads per attention layer, and ∼160M pa-\nrameters. GPT2-Large has 36 layers, 20 attention\nheads per attention layer, and ∼840M parameters.\nBoth have a vocabulary of ∼50K tokens.\n3.3 Tools & System Setup\nWe use the Transformer Lens Python package\n(Nanda and Bloom, 2022) to cache, inspect, and\nconstruct interventions on model inference passes.\nWe ran experiments on a single A100 GPU with\n40 GB RAM. Experimental code, dependency in-\nformation, and datasets are available on GitHub.1\n4 Proposed Methods\nRecent work suggests that attention heads are\nknowledge retrievers during a model’s inference\npass (Geva et al., 2023; Li et al., 2023). Extending\nthis result to multi-hop prompts, we hypothesize\nthat attention layers play an important role in re-\ntrieving memories relevant to the “hop” in a given\nprompt. Therefore we define two algorithms below:\none for analyzing attention head outputs in embed-\nding space and the other for injecting a targeted\nmemory into a model’s hidden activations in order\nto correct faulty/incomplete reasoning.\n4.1 Interpreting Attention Heads\nWe want to further understand the outputs of indi-\nvidual heads, and more specifically assess if any\n1https://github.com/msakarvadia/memory_\ninjections\nindividual attention heads are exercised differently\nby single-hop vs. multi-hop prompts.\nInspired by Logit Lens (nostalgebraist, 2021),\nwe leverage the model’s unembedding matrix to\nstudy the internal mechanism of each attention\nhead. For attention head j in layer ℓ, hℓ,j, we ap-\nply the model’s unembedding matrix WU followed\nby a softmax(·) operation and interpret the last\ntoken position (out of N total tokens) as a set of\nprobabilities over tokens in the vocabulary space:\nvocabℓ,j = softmax(hℓ,jWU )N−1 (8)\nSee in Fig. 2 an example of discrepancy in attention\nhead behavior, when using Eq. (8), for analogous\nsingle vs. multi-hop prompts. See additional exam-\nples in Table 5.\nA potential limitation of this approach is that it\nmay portray attention head behavior inaccurately\ndue to representational drift between model layers—\nand, like (nostalgebraist, 2021), may not generalize\nto other models. Nevertheless, we find it to be an ef-\nfective preliminary tool for studying the function of\nattention heads in updating the output distribution.\nWe leave the development of an interpretability tool\nthat considers these drawbacks to future work.\n4.2 Memory Injections to Correct Failures\nFig. 2 shows how Eq. (8) can reveal discrepan-\ncies between attention head behaviors for single-\nvs. multi-hop prompts. We hypothesize that such\ndiscrepancies arise because the model, when up-\ndating the output distribution in each layer, fails to\nincorporate information about the implicit entity\nin the multi-hop prompt. This seems reasonable,\nas to retrieve information about an implicit entity\none likely must first relate that entity to some ex-\nplicit subject and then retrieve relevant information\n(hence our notion that processing prompts with im-\nplicit subjects requires an extra hop compared to\nthose with explicit subjects).\n346\ntokens\nlogits\n... h 8 ... \nMLP\n\"The Great Barrier\nReef is located off the\ncoast of\"\n\"The largest coral reef system\nin the world is located off the\ncoast of\"\nSingle-Hop Prompt Multi-Hop Prompt\n\" Australia\",\n\" Australians\",\n\"Australia\",\n\"Australian\",\n...\n\" coral\"\n\" reef\"\n\" reefs\"\n\"Fiji\"\n...\nLayer 9\nAttention Outputs\n(Single-Hop) \n(Multi-Hop) \nresidual stream\nembed ( )\nunembed ()\nFigure 2: Diagram of language model reasoning.\nHighest ranked attention outputs of GPT2-Small at layer\nℓ = 9, head h = 8when projected into vocabulary space\n(via the GPT2-Small embedding matrix) for a single-\nhop prompt (green) and its multi-hop counterpart (red).\nThus we design a method (see Fig. 3) for inject-\ning a missing hop directly into the output hidden\nstates of an attention head before those outputs are\nadded back into the transformer’s residual stream:\n1. Let m be a memory (a phrase, for example:\n“The Great Barrier Reef”) and let τ be the\nmagnitude of the memory injection.\n2. Tokenize the memorym into t0, ··· , tq where\nq is the number of tokens. We encode each to-\nken ti into a one-hot vector bi ∈ {0, 1}|V | and\nsum all resulting one-hot vectors bi together\ninto a binary vector B ≜ P\ni bi.\n3. Embed the binary vector, B, back into the\nmodel’s latent space by applying the transpose\nof the unembedding matrix:\nB∗ = B WT\nU (9)\n4. Then, to inject a memory at the attention layer\nof layer ℓ, add the embedded memory into\nthe outputs of the attention heads during the\ninference pass:\naℓ =\nHX\nj=1\nhℓ,j + τB ∗ (10)\nSee additional examples of memory injections in\nTable 4.\ntokens\nlogits\n... h 8 ... \nMLP\n\"The largest coral reef system in\nthe world is located off the coast\nof\"\nMulti-Hop Prompt\n\" coral\"\n\" reef\"\n\" reefs\"\n\"Fiji\"\n...\nLayer 9\nAttention Outputs\nresidual stream\n\"The Great Barrier Reef\"\n(memory)\nunembed ()\nembed ( )\ninjection \nNext Token Pred. Prob. for \" Australia\"\nPre-Injection: 0.047\nPost-Injection: 0.136 (189% increase)\nFigure 3: Memory injection. Injecting memory “The\nGreat Barrier Reef” into GPT2-Small hidden activations\nat layer ℓ = 9, head 8, τ = 4.\n5 Results and Discussion\nWe report, in turn, on our curated memory, random\nmemory, and part-of-speech injection experiments.\n5.1 Curated Memory Injections\nWe hypothesize that a model’s poor performance on\nmulti-hop prompts is due to its inability to resolve\nthe implicit subject (e.g., “The largest coral reef\nsystem in the world”) to an explicit subject (e.g.,\n“The Great Barrier Reef”). This failure limits the\nlater layers’ ability to retrieve relevant information\nabout this subject before predicting the next token.\nTherefore, in this experiment, we curate sets of\ntokens to inject into our model’s residual stream\nsuch that it can resolve the explicit subject more\neasily. We further study the effect that the injection\nmagnitude τ has on its success.\nExperimental design: For every multi-hop\nprompt in our datasets, we extract the explicitly\nstated subject from the corresponding single-hop\nprompt and inject those tokens as memories into\neach attention layer as described in Section 4.2.\nFor example, given the single-hop prompt “The\nGreat Barrier Reef is located off the coast of. . . ”\nand the multi-hop prompt “The largest coral reef\nsystem in the world is located off the coast of. . . ,”\nthe memory is “The Great Barrier Reef.”\nWe assess the effects of injection layer ℓ and\nmagnitude τ ∈ [1, ··· , 15] by enumerating the re-\nsulting change in accuracy for all combinations\n347\n0 2 4 6 8 10\nLayer ( )\n13579111315\nMagnitude ( )\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34\nLayer ( )\n0 2 4 6 8 10\nLayer ( )\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34\nLayer ( )\n-80%-40%0% 20% 40%\n -80% -50% -25% 0% 20% 40% 60%\n -80%-40%0% 200%400%\n -40% -25% -15% 0% 60% 125% 200%\nFigure 4: Curated memory injections. From left to right: GPT2-Small + Hand, GPT2-Large + Hand, GPT2-Small\n+ 2WMH, GPT2-Large + 2WMH. Each cell in each heatmap is the average percent difference between the pre- and\npost-injection next token predictions for multi-hop prompts. Green cells denote a positive percent difference (i.e.,\ncorrect prediction is more likely), while red cells denote a negative percent difference (i.e., correct prediction is less\nlikely). When computing the averages for each (ℓ, τ) pair we exclude outliers not within ±2 standard deviations\nfrom the mean.\nof these two parameters for both GPT2-Small and\nGPT2-Large. We measure the success of a mem-\nory injection by calculating the percent increase\nbetween the model’s predicted probability for the\nexpected next token from the multi-hop prompt\nwith and without the injection. A greater positive\ndifference indicates a more successful injection.\nDiscussion: Results are in Fig. 4. We observe\nthat each model/dataset combination has an optimal\nlayer ℓ and magnitude τ for memory injections: the\ndarkest green areas, which signify the highest aver-\nage percent increase in probability of the expected\nnext token for the respective dataset. The best ( ℓ,\nτ) pair injection results are in Table 2. Additional\nexamples of memory injections are in Table 4.\n5.2 Random Memory Injections\nIn Section 5.1, we identify ideal ( ℓ, τ) pairs for\neach model and dataset for a curated memory in-\njection. We now demonstrate that the results we\nobserve are not spurious: i.e., the information that\nwe inject at each head should be related to the ex-\nplicit subject. We demonstrate the need for our\nparticular injection routine by assessing the effects\non model accuracy of randomly injecting tokens\nfrom various parts of speech.\nExperimental design: We conduct targeted in-\njections for the high-scoring (ℓ, τ) pairs identified\nvia the experiment in Section 5.1, Table 2. Instead\nof injecting curated subject tokens, we select as\ncandidate injections the 40 most common words\nfrom each of the adjectives, adverbs, conjunctions,\nnouns, verbs, and top 5050 subsets of our Part of\nSpeech dataset. We then apply each word as an\nindividual injection for every prompt in our multi-\nhop dataset at the ideal (ℓ, τ) pair. We term these\ninjections “random,” as they were not curated to be\nrelevant to our prompts.\nDiscussion: The results are in the right half of\nTable 2. We observe that a random injection led, on\naverage, to a degradation in predictive performance\nacross most parts of speech considered, as indi-\ncated by a negative percent difference (decrease in\ncorrect answer probability) between the pre- and\npost-injection expected next token probabilities for\nmulti-hop prompt completions. Additionally, no\nrandom injection result exceeded the performance\nof a curated injection. These findings suggest that\nthe choice of injected tokens is critical for improv-\ning multi-hop prompt completion success.\n5.3 Memory Injections for Parts of Speech\nWe have tested curated vs. random memory injec-\ntions at ideal (ℓ, τ) pairs. Now we assess whether\nmemory injections from specific parts of speech\nmore broadly have positive impacts on prompt com-\npletions, not just at the ideal locations for curated\nmemories, but also at other (ℓ, τ) pairs. Our hypoth-\nesis is that if a transformer-based LLM has learned\na division of labor regarding which attention lay-\ners are responsible for retrieving specific concepts\n(e.g., parts of speech) then this experiment might\nhighlight those learned roles.\nExperimental design: This experiment is iden-\ntical to that of Section 5.1, except that: (i) for each\npart of speech pos ∈ [adjectives, adverbs, conjunc-\n348\nCurated Random\nModel Data ℓ τ Subject Adj. Adv. Conj. Noun Verb Top- 5050\nGPT2 Small Hand 7 3 45% -7.6% -6.0% -6.3% -6.5% -7.5% -6.0%\nGPT2 Small 2wmh 6 5 424% -17.1% -15.1% -10.3% -1.1% -1.2% 1.6%\nGPT2 Large Hand 14 10 68% -8.1% -4.4% -4.9% -9.8% -6.0% -4.7%\nGPT2 Large 2wmh 8 9 204% 13.0% 11.6% 3.5% 11.8% 4.3% 17.6%\nTable 2: Curated vs. random memory injections. Table shows the (ℓ, τ) pairs for the best token injections, along\nwith the average percent difference(excluding outliers >±2 standard deviations from the mean) between pre- and\npost-injection expected next token predictions for multi-hop prompts. Each random injection column indicates 40\nrandom injections from [Adjectives, Adverbs, Conjunctions, Nouns, Verbs, Top 5050] at the ideal (ℓ, τ).\n0 5 10\nLayer ( )\n(a)\n100\n50\n0\n50\n100\n150\nAvg. Percent Difference (%)\n GPT2-Small (2WMH)\n0 10 20 30\nLayer ( )\n(b)\n100\n50\n0\n50\n100\n150\nGPT2-Large (2WMH)\n0 5 10\nLayer ( )\n(c)\n100\n50\n0\n50\n100\n150\nGPT2-Small (Hand)\n0 10 20 30\nLayer ( )\n(d)\n100\n50\n0\n50\n100\n150\nGPT2-Large (Hand)\n5 10 15\nMagnitude ( )\n(e)\n100\n0\n100\n200\n300\nAvg. Percent Difference (%)\n GPT2-Small (2WMH)\n5 10 15\nMagnitude ( )\n(f)\n100\n0\n100\n200\n300\nGPT2-Large (2WMH)\n5 10 15\nMagnitude ( )\n(g)\n100\n0\n100\n200\n300\nGPT2-Small (Hand)\n5 10 15\nMagnitude ( )\n(h)\n100\n0\n100\n200\n300\nGPT2-Large (Hand)\nNoun T op 5050 Verb Conjunction Verb Adjective Curated\nFigure 5: Part of speech memory injections. This figure shows the average effect of memory injections from\nvarious parts of speech as a function of layer ℓ (top row) and magnitude τ (bottom row). The standard deviation\nscaled by 10% is pictured across magnitudes (top row) and layers (bottom row).\ntions, nouns, verbs, top 5050], we use a randomly\nselected word: e.g., “apple” from “nouns”; and (ii)\nwhen searching for the ideal (ℓ, τ) pair for a given\npart of speech and multi-hop prompt, we use a new\nrandom word for each injection.\nDiscussion: The results are in Fig. 5. We note\nthat for no part of speech considered here does\nthe average performance of the studied memory\ninjections exceed that of the curated memory injec-\ntions presented in Table 2. Additionally, memory\ninjections from adjectives, adverbs, nouns, verbs,\nand top 5050 seemed to exhibit similar behavior.\nMemory injections from conjunctions, however,\ntypically outperformed all other parts of speech.\nWe hypothesize that this is because conjunctions\noften play a neutral role in prompt completions.\nThus, while a random noun (e.g., “apple”) might\ndistort prompt completion, a random conjunction\n(e.g., “and,” “for”) is less likely to do so.\nWe note also that for each part of speech, perfor-\nmance averaged over all injections for most (ℓ, τ)\npairs was reduced (< 0) for Hand (refer Fig. 5:\nsubplots c, d, g, h), but was sometimes improved\n(> 0) for 2WMH (refer Fig. 5: subplots a, b, e, f).\nWe attribute this result to the relative difficulties\nof the two datasets. Hand has, on average, lower\nsurprisals than does 2WMH, as seen in Table 1,\nsuggesting that there is additional information that\nthe model could use successfully for 2WMH, but\nnot for Hand.\nThese results (see also the Appendix; Figs 6–9)\nsuggest that while curated memories are ideal for\n349\ncorrecting multi-hop reasoning failures, language\nmodels can also benefit from injections of different\nparts of speech. This result suggests that different\nparts of a language model (namely, early layers)\nserve specialized roles, with some dealing with\nprocessing related to specific parts of speech.\nIn future work we will curate relevant memories\nfrom various parts of speech for each prompt, to\nbetter understand the effects of curated memories.\n6 Related Work\nMuch recent work has focused on the inner work-\nings of Transformers (Vaswani et al., 2017; De-\nvlin et al., 2019; Brown et al., 2020; Radford\net al., 2019). Nanda et al. (2023) explore how the\nemergent properties of LLMs form during train-\ning. Recent interpretability research has focused\non the mechanisms by which linear layers in LLMs\nretrieve information, characterizing them as key-\nvalue stores of information (Geva et al., 2021; Dai\net al., 2022a,b) and showing that tokens can be\ncharacterized by their distribution in the output vo-\ncabulary (Geva et al., 2022).\nOthers have also examined the intermediate ac-\ntivations of LLMs in order to uncover underlying\nreasoning mechanisms. nostalgebraist (2021) ap-\nplied GPT-2’s unembedding matrix to intermediate\nlayers to interpret how the model arrives at its final\nanswer. Belrose et al. (2023) employed a learned\ntransformation to mitigate the effect of any bias\nintroduced by using the unembedding matrix.\nThere has been much recent interest in whether\nLLMs are reliable stores of information for attempt-\ning to both identify where knowledge exists and\nhow to edit stored factual knowledge effectively\n(Mitchell et al., 2022a,b; Elazar et al., 2021; Hase\net al., 2023). Recent approaches to knowledge\nediting make use of learned hyper-models to edit\nweights, additional trained parameters, or direct in-\nterventions on model weights (De Cao et al., 2021;\nHuang et al., 2023; Dhingra et al., 2022). How-\never, these approaches raise another issue: deal-\ning with knowledge retention and preventing catas-\ntrophic forgetting (Jang et al., 2022; Hase et al.,\n2021; Zhong et al., 2023). Additionally, it is not\nclear that the mechanisms by which model predic-\ntions are constructed is fully understood, limiting\nour ability to improve model performance (Turpin\net al., 2023). Some approaches propose to use ex-\nternal knowledge stores such as knowledge graphs\nto augment the factual capabilities of LLMs (Jiang\net al., 2023; Sun et al., 2018; Zhang et al., 2022).\n7 Conclusions and Future Directions\nWe demonstrate that a key reason LLMs perform\nworse on multi-hop prompts is because they fail to\nrecall intermediary information that is relevant to a\nhop. We find that attention heads play an important\nrole in this factual recall process, and that in the\ncase of multi-hop reasoning, certain attention lay-\ners fail to recall relevant information. To rectify this\nshortcoming, we establish an algorithm for inject-\ning “memories” directly into the model’s hidden\nactivations during inference. Through experimenta-\ntion, we find that injecting relevant memories into\nthe hidden activations of the attention heads dur-\ning inference is an efficient way to boost model\nperformance on multi-hop prompts.\nWe anticipate that our memory injection scheme\ncan extend a model’s longevity by enabling less\nfrequent retraining/fine-tuning. We also hope in\nfuture work to demonstrate the use of memory in-\njections to correct stale or incorrect information,\nremove private or harmful information, and combat\nbias during LLM inference.\nThere is also a tremendous opportunity to scale\nonline-memory injections to enhance the quality of\nthousands/millions of model inferences, if we can\nautomate the process of memory selection via un-\nsupervised algorithms, for instance by connecting\nLLMs with knowledge bases.\nLimitations\nInternal biases of the question writers as well as\nthe rigid structure that had to be imposed on the\nprompt structure mean that our human-generated\ndataset is representative only of a small fraction\nof the many types of multi-hop questions. Fur-\nthermore, our hand-generated dataset is relatively\nsmall compared to our programmatically generated\ndataset. Additionally, our analyses were limited\nto GPT2-Small and GPT2-Large; further work is\nneeded to determine whether, as we expect, other\nlanguage models sharing a transformer-based ar-\nchitecture and a similar unsupervised causal lan-\nguage modeling training objective display similar\nbehavior. Lastly, we rely on the model’s unembed-\nding matrix WU to interpret model hidden states\nand embed memories for injection. While for our\nwork, results indicate that this transformation was\nsufficient, we acknowledge that this unembedding\nmatrix is not tuned to interpret intermediate layers;\n350\nwe aim to address this shortcoming in future work\nby instead using layer-specific learned projections\nto transform between hidden states and vocabulary.\nEthics\nOur attention head inspection mechanism uncov-\nered several sources of bias (such as racism); refer\nTable 5 for examples. We expect a more detailed\nstudy of the attention heads of GPT2-Small and\nGPT2-Large, as well as other LLMs, to reveal ad-\nditional undesirable behaviors. We aim in future\nwork to use our inspection method to uncover (and\nhopefully address) these biases.\nAcknowledgements\nThis material is based upon work supported by\nthe U.S. Department of Energy, Office of Sci-\nence, Office of Advanced Scientific Computing\nResearch, Department of Energy Computational\nScience Graduate Fellowship under Award Num-\nber DE-SC0023112. This work is also supported\nin part by the U.S. Department of Energy under\nContract DE-AC02-06CH11357.\nReferences\nKonstantine Arkoudas. 2023. GPT-4 can’t reason.\narXiv preprint arXiv:2308.03762.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\narXiv preprint arXiv:2303.08112.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski,\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\ning elaborate problems with large language models.\narXiv preprint arXiv:2308.09687.\nAndrew Blair-Stanek, Nils Holzenberger, and Benjamin\nVan Durme. 2023. Can GPT-3 perform statutory\nreasoning? arXiv preprint arXiv:2302.06100.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nRoi Cohen, Eden Biran, Ori Yoran, Amir Globerson,\nand Mor Geva. 2023. Evaluating the ripple effects\nof knowledge editing in language models. arXiv\npreprint arXiv:2307.12976.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022a. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502.\nDamai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu,\nQiaoqiao She, and Zhifang Sui. 2022b. Neural\nknowledge bank for pretrained transformers. arXiv\npreprint arXiv:2208.00399.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.\n2022. Analyzing transformers in embedding space.\narXiv preprint arXiv:2209.02535.\nMark Davies. 2010. The Corpus of Contemporary\nAmerican English as the first reliable monitor cor-\npus of English. Literary and Linguistic Computing,\n25(4):447–464.\nMark Davies. 2011. Word frequency data from the\nCorpus of Contemporary American English (COCA).\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language mod-\nels as temporal knowledge bases. Transactions of the\nAssociation for Computational Linguistics, 10:257–\n273.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nN Elhage, N Nanda, C Olsson, T Henighan, N Joseph,\nB Mann, A Askell, Y Bai, A Chen, T Conerly, et al.\n2021. A mathematical framework for transformer\ncircuits.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual asso-\nciations in auto-regressive language models. arXiv\npreprint arXiv:2304.14767.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\n351\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nTaicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun\nGuo, Nitesh V Chawla, Olaf Wiest, Xiangliang\nZhang, et al. 2023. What indeed can GPT models do\nin chemistry? a comprehensive benchmark on eight\ntasks. arXiv preprint arXiv:2305.18365.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-\ndeharioun. 2023. Does localization inform editing?\nSurprising differences in causality-based localization\nvs. knowledge editing in language models. arXiv\npreprint arXiv:2301.04213.\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2021. Do language models have be-\nliefs? Methods for detecting, updating, and visualiz-\ning model beliefs. arXiv preprint arXiv:2111.13654.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609–6625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nZeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,\nWenge Rong, and Zhang Xiong. 2023. Transformer-\npatcher: One mistake worth one neuron. In The\nEleventh International Conference on Learning Rep-\nresentations.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. 2022. Towards continual\nknowledge learning of language models. In Interna-\ntional Conference on Learning Representations.\nJinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen.\n2023. UniKGQA: Unified retrieval and reasoning for\nsolving multi-hop question answering over knowl-\nedge graph. In The Eleventh International Confer-\nence on Learning Representations.\nXiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun\nMa, and Jie Yu. 2023. PMET: Precise model editing\nin a transformer. arXiv preprint arXiv:2308.08742.\nJieyi Long. 2023. Large language model guided tree-of-\nthought. arXiv preprint arXiv:2305.08291.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022a. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 35:17359–17372.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2022b. Mass-\nediting memory in a transformer. arXiv preprint\narXiv:2210.07229.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022a. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022b. Memory-\nbased model editing at scale. In Proceedings of the\n39th International Conference on Machine Learning,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 15817–15831. PMLR.\nNeel Nanda and Joseph Bloom. 2022. Transformer-\nLens.\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess\nSmith, and Jacob Steinhardt. 2023. Progress mea-\nsures for grokking via mechanistic interpretability. In\nThe Eleventh International Conference on Learning\nRepresentations.\nnostalgebraist. 2021. Logit Lens on non-GPT2 models\n+ extensions.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William Cohen.\n2018. Open domain question answering using early\nfusion of knowledge bases and text. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4231–4242,\nBrussels, Belgium. Association for Computational\nLinguistics.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R Bowman. 2023. Language models don’t\nalways say what they think: Unfaithful explana-\ntions in chain-of-thought prompting. arXiv preprint\narXiv:2305.04388.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 30.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\n352\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-\nYen Kan, Junxian He, and Qizhe Xie. 2023. De-\ncomposition enhances reasoning via self-evaluation\nguided decoding. arXiv preprint arXiv:2305.00633.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie\nTang, Cuiping Li, and Hong Chen. 2022. Subgraph\nretrieval enhanced model for multi-hop knowledge\nbase question answering. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 5773–\n5784, Dublin, Ireland. Association for Computational\nLinguistics.\nZexuan Zhong, Zhengxuan Wu, Christopher D Man-\nning, Christopher Potts, and Danqi Chen. 2023.\nMQuAKE: Assessing knowledge editing in language\nmodels via multi-hop questions. arXiv preprint\narXiv:2305.14795.\n353\nA Part-of-Speech Memory Injection Appendix\n0 7 14 21 28 35\n13579111315\nMagnitude ( )\nNoun\n0 7 14 21 28 35\n13579111315\nVerb\n0 7 14 21 28 35\n13579111315\nAdverb\n0 7 14 21 28 35\nLayer ( )\n13579111315\nMagnitude ( )\nConjunction\n0 7 14 21 28 35\nLayer ( )\n13579111315\nT op 5050\n0 7 14 21 28 35\nLayer ( )\n13579111315\nAdjective\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 6: GPT2-Large, 2WMH dataset. Heatmap shows average percent difference between pre- and post-injection\nanswer probabilities for multi-hop prompts excluding outliers not within ±2 standard deviations from the mean\nacross various parts of speech.\n0 7 14 21 28 35\n13579111315\nMagnitude ( )\nNoun\n0 7 14 21 28 35\n13579111315\nVerb\n0 7 14 21 28 35\n13579111315\nAdverb\n0 7 14 21 28 35\nLayer ( )\n13579111315\nMagnitude ( )\nConjunction\n0 7 14 21 28 35\nLayer ( )\n13579111315\nT op 5050\n0 7 14 21 28 35\nLayer ( )\n13579111315\nAdjective\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 7: GPT2-Large, Hand dataset. Heatmap shows average percent difference between pre- and post-injection\nanswer probabilities for multi-hop prompts excluding outliers not within ±2 standard deviations from the mean\nacross various parts of speech.\n354\n0 2 4 6 8 11\n13579111315\nMagnitude ( )\nNoun\n0 2 4 6 8 11\n13579111315\nVerb\n0 2 4 6 8 11\n13579111315\nAdverb\n0 2 4 6 8 11\nLayer ( )\n13579111315\nMagnitude ( )\nConjunction\n0 2 4 6 8 11\nLayer ( )\n13579111315\nT op 5050\n0 2 4 6 8 11\nLayer ( )\n13579111315\nAdjective\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 8: GPT2-Small, 2WMH dataset. Heatmap shows average percent difference between pre- and post-injection\nanswer probabilities for multi-hop prompts excluding outliers not within ±2 standard deviations from the mean\nacross various parts of speech.\n0 2 4 6 8 11\n13579111315\nMagnitude ( )\nNoun\n0 2 4 6 8 11\n13579111315\nVerb\n0 2 4 6 8 11\n13579111315\nAdverb\n0 2 4 6 8 11\nLayer ( )\n13579111315\nMagnitude ( )\nConjunction\n0 2 4 6 8 11\nLayer ( )\n13579111315\nT op 5050\n0 2 4 6 8 11\nLayer ( )\n13579111315\nAdjective\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 9: GPT2-Small, Hand dataset. Heatmap shows average percent difference between pre- and post-injection\nanswer probabilities for multi-hop prompts excluding outliers not within ±2 standard deviations from the mean\nacross various parts of speech.\n355\nB Dataset Example Appendix\nDataset Single-Hop Prompt Multi-Hop Prompt\nHand\nGeorge Washington fought in the . . . [Revolutionary\nWar]\nThe first president of the United States fought in the\n. . . [Revolutionary War]\nBurj Khalifa is located in the city of . . . [Dubai] The tallest building in the world is located in the city of\n. . . [Dubai]\nNelson Mandela brought an end to . . . [Apartheid] The first president of South Africa brought an end to\n. . . [Apartheid]\nJohn F Kennedy was assassinated by a person\nnamed . . . [Lee Harvey Oswald]\nThe 35th president of the United States was assassinated\nby a person named . . . [Lee Harvey Oswald]\nThe father of Hermes is . . . [Zeus] The father of the Greek messenger god is . . . [Zeus]\n2WMH\nThe place of birth of Dušan Hanák is . . . [Bratislava] The place of birth of the director of I Love, You Love is\n. . . [Bratislava]\nThe employer of Éric Rohmer is . . . [Cahiers du\ncinéma]\nThe employer of the director of Triple Agent is . . . [Cahiers\ndu cinéma]\nThe employer of Chip Gubera is . . . [University of\nMissouri]\nThe employer of the director of Academy of Doom is\n. . . [University of Missouri]\nSteve Vai received the . . . [Grammy] The performer of The Attitude Song received the\n. . . [Grammy]\nThe place of death of Augustus II the Strong is\n. . . [Warsaw]\nThe place of death of the spouse of Christiane Eberhardine\nof Brandenburg-Bayreuth is . . . [Warsaw]\nTable 3: Example prompts. Single/multi-hop prompt pairs from Hand and 2WMH datasets.\nMultiple-Hop Prompt Memory Answer Pre-\ninjection\nAnswer\nProb.\nPost-\ninjection\nAnswer\nProb.\nThe God of Thunder is the son of . . . Thor Odin 0.84% 3 .37%\nThe first president to be assassinated succeeded\nin abolishing . . .\nAbraham Lincoln slavery 30.46% 63 .09%\nThe founder of Microsoft was born in the city of\n. . .\nBill Gates Seattle 1.55% 2 .44%\nThe highest peak in the world is located in the . . . Mount Everest Himalayan 3.40% 22 .58%\nTable 4: Examples of memory injections. Injecting memories with τ = 4, ℓ= 9into GPT2-Small.\n356\nPrompt Type Prompt Layer ℓ Head h Output\nSingle-Hop\nJohn F Kennedy was as-\nsassinated by a person\nnamed . . .\n10 0 [‘ Kennedy’, ‘ JFK’, ‘ Assass’, ‘ assass’, ‘Kenn’, ‘ as-\nsassination’, ‘ Cuba’, ‘ Oswald’, ‘ assassin’, ‘ Cuban’, ‘\nFidel’, ‘ Bobby’, ‘ Havana’, ‘ assassinated’, ‘ assassins’, ‘\nJackie’, ‘ Castro’, ‘ Jinn’, ‘ assassinate’, ‘Mu’, ‘ 1963’, ‘\nKahn’, ‘ drone’, ‘ Cah’, ‘ Mu’, ‘ Ghosts’, ‘ Soul’, ‘ Laos’,\n‘ Cemetery’, ‘ CIA’]\nBarack Obama was a\nmember of the . . .\n9 8 [‘ Obama’, ‘Obama’, ‘ Maryland’, ‘ America’, ‘ JFK’, ‘\nBiden’, ‘ Harlem’, ‘ Washington’, ‘ American’, ‘ Clinton’,\n‘ White’, ‘ Americans’, ‘ Congressional’, ‘ Harvard’, ‘\nKennedy’, ‘ FBI’, ‘ Federal’, ‘ CDC’, ‘ DOJ’, ‘ President’,\n‘ Georgetown’, ‘ HHS’, ‘ Barack’, ‘ US’, ‘ Trayvon’, ‘\nConnecticut’, ‘ Holder’, ‘ New’, ‘ BLM’, ‘ Baltimore’]\nCain murdered a person\nnamed . . .\n2 1 [‘ police’, ‘,’, ‘ the’, ‘ a’, ‘\\n’, ‘ and’, ‘ violence’, ‘.’, ‘\ndeath’, ‘ in’, ‘ criminal’, ‘ of’, ‘ to’, ‘ victim’, ‘ \"’, ‘-’, ‘\nat’, ‘ victims’, ‘ crime’, ‘ from’, ‘ an’, ‘ that’, ‘ murder’, ‘\ncrimes’, ‘ is’, ‘ was’, ‘ he’, ‘ for’, ‘ (’, ‘ killed’]\nRussia is mostly located\non the continent of . . .\n9 8 [‘ Moscow’, ‘ Russian’, ‘Moscow’, ‘ Russia’, ‘ Kremlin’,\n‘ Putin’, ‘Putin’, ‘Russia’, ‘ Russians’, ‘Russian’, ‘♦?’, ‘ ♦?’,\n‘ Dmitry’, ‘ Mikhail’, ‘ Vladimir’, ‘ Sergei’, ‘ Siberia’, ‘\nSoviet’, ‘ Siberian’, ‘ Ukraine’, ‘ Ukrainian’, ‘ Sochi’, ‘\nCaucasus’, ‘ Nikol’, ‘Soviet’, ‘ KGB’, ‘ Dmit’, ‘ USSR’,\n‘Ukraine’, ‘ Ukrainians’]\nGeorge Washington\nfought in the . . .\n9 8 [‘ Washington’, ‘Washington’, ‘ Virginia’, ‘Virginia’, ‘\nMaryland’, ‘ Congressional’, ‘ Georgetown’, ‘ Dull’, ‘\nSmithsonian’, ‘ Maine’, ‘ Burr’, ‘ Jefferson’, ‘ Navy’, ‘\nCapitol’, ‘ congressional’, ‘ FDR’, ‘ Lexington’, ‘ Byrd’,\n‘ Rhode’, ‘ Roosevelt’, ‘ Pike’, ‘ Everett’, ‘ Brookings’,\n‘ Madison’, ‘apeake’, ‘ Randolph’, ‘ V A’, ‘ Arlington’, ‘\nAmericans’, ‘ Lafayette’]\nMulti-Hop\nThe 35th president of the\nUnited States was assassi-\nnated by a person named\n. . .\n10 0 [‘ assass’, ‘ Assass’, ‘ assassination’, ‘ assassin’, ‘ as-\nsassins’, ‘ assassinate’, ‘ Malik’, ‘ bullets’, ‘ gunmen’, ‘\nassassinated’, ‘Mu’, ‘ Pakistani’, ‘ sniper’, ‘ killings’, ‘\nJFK’, ‘ Pakistan’, ‘ homicides’, ‘ Alger’, ‘ lethal’, ‘ Islam-\nabad’, ‘ Karachi’, ‘ shooting’, ‘ gun’, ‘ gunshot’, ‘ Mu’, ‘\nmurder’, ‘ killing’, ‘ pistols’, ‘ murders’, ‘ gunned’]\nThe first black president\nof the United States was a\nmember of the . . .\n9 8 [‘ Negro’, ‘ NAACP’, ‘ blacks’, ‘ black’, ‘ Baltimore’, ‘\nWhite’, ‘ negro’, ‘ Washington’, ‘ BLM’, ‘ white’, ‘ FBI’,\n‘ America’, ‘ Maryland’, ‘ African’, ‘ Trump’, ‘ Nixon’, ‘\nCharleston’, ‘ Americ’, ‘ KKK’, ‘Washington’, ‘ Virginia’,\n‘ racial’, ‘ Blacks’, ‘white’, ‘White’, ‘ nig’, ‘ Black’, ‘\nObama’, ‘ Louisiana’, ‘ whites’]\nAdam and Eve’s eldest\nson murdered a person\nnamed . . .\n2 1 [‘,’, ‘ the’, ‘ and’, ‘ a’, ‘ \"’, ‘ in’, ‘\\n’, ‘.’, ‘ to’, ‘ of’, ‘ at’, ‘\nis’, ‘ he’, ‘-’, ‘ that’, ‘ was’, ‘ for’, ‘ police’, ‘ from’, ‘ on’,\n\" ‘\", ‘ as’, ‘ death’, ‘ had’, \"’\", ‘ an’, ‘ his’, \"’s\", ‘ said’, ‘\ntold’]\nThe largest country in the\nworld is mostly located\non the continent of . . .\n9 8 [‘,’, ‘\\n’, ‘ the’, ‘ and’, ‘.’, ‘ in’, ‘ a’, ‘ to’, ‘ of’, ‘ (’, ‘-’,\n‘ for’, ‘ that’, ‘ \"’, ‘:’, ‘ is’, ‘ or’, ‘ at’, ‘ as’, ‘ I’, ‘ on’, ‘\nwith’, ‘ it’, ‘ an’, ‘ from’, ‘ all’, ‘ by’, ‘ not’, \"’s\", ‘ more’]\nThe first president of the\nUnited States fought in\nthe . . .\n9 8 [‘ Trump’, ‘ Washington’, ‘ America’, ‘Washington’, ‘\nAmerican’, ‘Trump’, ‘America’, ‘ Obama’, ‘ Donald’,\n‘ FBI’, ‘ Congressional’, ‘ Americans’, ‘American’, ‘\nNixon’, ‘ Congress’, ‘ congressional’, ‘ White’, ‘ Roo-\nsevelt’, ‘ Republican’, ‘ Negro’, ‘ Clinton’, ‘ JFK’, ‘\nReagan’, ‘ Virginia’, ‘ FDR’, ‘Obama’, ‘Americans’, ‘\nAmeric’, ‘FBI’, ‘Congress’]\nTable 5: Example of attention head outputs from GPT2-Small for Hand."
}