{
    "title": "Assessing Phrasal Representation and Composition in Transformers",
    "url": "https://openalex.org/W3100475986",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2097310566",
            "name": "Lang Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2404566579",
            "name": "Allyson Ettinger",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6631349028",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2782630856",
        "https://openalex.org/W2963025830",
        "https://openalex.org/W3023372699",
        "https://openalex.org/W4288379066",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2962776659",
        "https://openalex.org/W2966176804",
        "https://openalex.org/W3023701719",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W2962694015",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2986889180",
        "https://openalex.org/W2963029978",
        "https://openalex.org/W3035160860",
        "https://openalex.org/W2932637973",
        "https://openalex.org/W2963499246",
        "https://openalex.org/W3034854575",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963366649",
        "https://openalex.org/W4288255289",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W2972498556",
        "https://openalex.org/W2251882135",
        "https://openalex.org/W2515741950",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W4300618906",
        "https://openalex.org/W2864832950",
        "https://openalex.org/W1984052055",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W4295803813",
        "https://openalex.org/W2804339109",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W1608322251",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2970853769",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W1854884267",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4298392964",
        "https://openalex.org/W2067438047",
        "https://openalex.org/W2962961857",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2964165804",
        "https://openalex.org/W2053921957",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2963372062",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2296540674",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2251044566",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2972778456",
        "https://openalex.org/W2137607259",
        "https://openalex.org/W2516090925",
        "https://openalex.org/W2932893307",
        "https://openalex.org/W2970752815",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2788751659",
        "https://openalex.org/W2954604190",
        "https://openalex.org/W2983040767"
    ],
    "abstract": "Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4896–4907,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n4896\nAssessing Phrasal Representation and Composition in Transformers\nLang Yu\nDeptartment of Computer Science\nUniversity of Chicago\nlangyu@uchicago.edu\nAllyson Ettinger\nDepartment of Linguistics\nUniversity of Chicago\naettinger@uchicago.edu\nAbstract\nDeep transformer models have pushed perfor-\nmance on NLP tasks to new limits, suggesting\nsophisticated treatment of complex linguistic\ninputs, such as phrases. However, we have lim-\nited understanding of how these models han-\ndle representation of phrases, and whether this\nreﬂects sophisticated composition of phrase\nmeaning like that done by humans. In this pa-\nper, we present systematic analysis of phrasal\nrepresentations in state-of-the-art pre-trained\ntransformers. We use tests leveraging human\njudgments of phrase similarity and meaning\nshift, and compare results before and after con-\ntrol of word overlap, to tease apart lexical ef-\nfects versus composition effects. We ﬁnd that\nphrase representation in these models relies\nheavily on word content, with little evidence\nof nuanced composition. We also identify vari-\nations in phrase representation quality across\nmodels, layers, and representation types, and\nmake corresponding recommendations for us-\nage of representations from these models.\n1 Introduction\nA fundamental component of language understand-\ning is the capacity to combine meaning units into\nlarger units—a phenomenon known as composi-\ntion—and to do so in a way that reﬂects the nuances\nof meaning as understood by humans. Transform-\ners (Vaswani et al., 2017) have shown impressive\nperformance in NLP, particularly transformers us-\ning pre-training, like BERT (Devlin et al., 2019)\nand GPT (Radford et al., 2018, 2019), suggesting\nthat these models may be succeeding at composi-\ntion of complex meanings. However, because trans-\nformers (like other contextual embedding models)\ntypically maintain representations for every token,\nit is unclear how and at what points they might\nbe combining word meanings into phrase mean-\nings. This contrasts with models that incorporate\nexplicit phrasal composition into their architecture,\ne.g. RNNG (Dyer et al., 2016; Kim et al., 2019),\nrecursive models for semantic composition (Socher\net al., 2013), or transformers with attention-based\ncomposition modules (Yin et al., 2020).\nIn this paper we take steps to clarify the nature\nof phrasal representation in transformers. We fo-\ncus on representation of two-word phrases, and we\nprioritize identifying and teasing apart two impor-\ntant but distinct notions: how faithfully the mod-\nels are representing information about the words\nthat make up the phrase, and how faithfully the\nmodels are representing the nuances of the com-\nposed phrase meaningitself, over and above a sim-\nple account of the component words. To do this,\nwe begin with existing methods for testing how\nwell representations align with human judgments\nof meaning similarity: similarity correlations and\nparaphrase classiﬁcation. We then introduce con-\ntrolled variants of these datasets, removing cues\nof word overlap, in order to distinguish effects of\nword content from effects of more sophisticated\ncomposition. We complement these phrase simi-\nlarity analyses with classic sense selection tests of\nphrasal composition (Kintsch, 2001).\nWe apply these tests for systematic analysis of\nseveral state-of-the-art transformers: BERT (De-\nvlin et al., 2019), RoBERTa (Liu et al., 2019b),\nDistilBERT (Sanh et al., 2019), XLNet (Yang et al.,\n2019b) and XLM-RoBERTa (Conneau et al., 2019).\nWe run the tests in layerwise fashion, to estab-\nlish the evolution of phrase information as lay-\ners progress, and we test various tokens and to-\nken combinations as phrase representations. We\nﬁnd that when word overlap is not controlled, mod-\nels show strong correspondence with human judg-\nments, with noteworthy patterns of variation across\nmodels, layers, and representation types. However,\nwe ﬁnd that correspondence drops substantially\nonce word overlap is controlled, suggesting that\nalthough these transformers contain faithful repre-\n4897\nsentations of the lexical content of phrases, there\nis little evidence that these representations capture\nsophisticated details of meaning composition be-\nyond word content. Based on the observed repre-\nsentation patterns, we make recommendations for\nselection of representations from these models. All\ncode and controlled datesets are made available for\nreplication and application to additional models.1\n2 Related work\nThis paper contributes to a growing body of work\non analysis of neural network models. Much work\nhas studied recurrent neural network language mod-\nels (Linzen et al., 2016; Wilcox et al., 2018; Chowd-\nhury and Zamparelli, 2018; Gulordava et al., 2018;\nFutrell et al., 2019) and sentence encoders (Adi\net al., 2016; Conneau et al., 2018; Ettinger et al.,\n2016). Our work builds in particular on analysis of\ninformation encoded in contextualized token repre-\nsentations (Bacon and Regier, 2019; Tenney et al.,\n2019b; Peters et al., 2018; Hewitt and Manning,\n2019; Klafka and Ettinger, 2020) and in different\nlayers of transformers (Tenney et al., 2019a; Jawa-\nhar et al., 2019). The BERT model has been a\nparticular focus of analysis work since its intro-\nduction. Previous work has focused on analyzing\nthe attention mechanism (Vig and Belinkov, 2019;\nClark et al., 2019), parameters (Roberts et al., 2020;\nRadford et al., 2019; Raffel et al., 2020) and embed-\ndings (Shwartz and Dagan, 2019; Liu et al., 2019a).\nWe build on this work with a particular, controlled\nfocus on the evolution of phrasal representation in\na variety of state-of-the-art transformers.\nComposition has been a topic of frequent in-\nterest when examining neural networks and their\nrepresentations. One common practice relies on\nanalysis of internal representations via downstream\ntasks (Baan et al., 2019; Ettinger et al., 2018; Con-\nneau et al., 2019; Nandakumar et al., 2019; McCoy\net al., 2019). One line of work analyzes word in-\nteractions in neural networks’ internal gates as the\ncomposition signal (Saphra and Lopez, 2020; Mur-\ndoch et al., 2018), extending the Contextual De-\ncomposition algorithm proposed by Jumelet et al.\n(2019). Another notable branch of work constructs\nsynthetic datasets of small size to investigate com-\npositionality in neural networks (Liˇska et al., 2018;\nHupkes et al., 2018; Baan et al., 2019). Some work\n1Datasets and code available at\nhttps://github.com/yulang/\nphrasal-composition-in-transformers\ncontrols for word content, as we do, to study com-\nposition at the sentence level (Ettinger et al., 2018;\nDasgupta et al., 2018). We complement this work\nwith a targeted and systematic study of phrase-level\nrepresentations in transformers, with a focus on\nteasing apart lexical properties versus reﬂections\nof accurate compositional phrase meaning.\nOur work relates closely to classic work on\ntwo-word phrases, which have used methods like\nlandmark tests (Kintsch, 2001; Mitchell and Lap-\nata, 2008, 2010), or compared against distribution-\nbased phrase representations (Baroni and Zampar-\nelli, 2010; Fyshe et al., 2015). Our work also\ndraws on work using correlation with similarity\njudgments (Finkelstein et al., 2001; Gerz et al.,\n2016; Hill et al., 2015; Conneau and Kiela, 2018)\nand paraphrase classiﬁcation (Ganitkevitch et al.,\n2013; Wang et al., 2018; Zhang et al., 2019; Yang\net al., 2019a) to assess quality of models and rep-\nresentations. We build on this work by combining\nthese methods together, applying them to a system-\natic analysis of transformers and their components,\nand introducing controlled variants of existing tasks\nto isolate accurate composition of phrase meaning\nfrom capturing of lexical information.\n3 Testing phrase meaning similarity\nOur methods begin with familiar approaches for as-\nsessing representations via meaning similarity: cor-\nrelation with human phrase similarity judgments,\nand ability to identify paraphrases. The goal is to\ngauge the extent to which models arrive at represen-\ntations reﬂecting the nuances of composed phrase\nmeaning understood by humans. We draw on ex-\nisting datasets, and begin by testing models on the\noriginal versions of these datasets—then we tease\napart effects of word content from effects of more\nsophisticated meaning composition by introducing\ncontrolled variants of the datasets. The reasoning\nis that strong correlations with human similarity\njudgments, or strong paraphrase classiﬁcation per-\nformance, could be inﬂuenced by artifacts that are\nnot reﬂective of accurate phrase meaning composi-\ntion per se. In particular, we may see strong perfor-\nmance simply on the basis of the amount of overlap\nin word content between phrases. To address this\npossibility, we create controlled datasets in which\nword overlap is no longer a cue to similarity.\nAs a starting point we focus on two-word\nphrases, as these are the smallest phrasal unit and\nthe most conducive to these types of lexical con-\n4898\nNormal Examples\nSource Phrase Target Phrase & Score\nordinary citizen (0.724)\naverage person person average (0.518)\ncountry (0.255)\nAB-BA Examples\nSource Phrase Target Phrase & Score\nlaw school school law (0.382)\nadult female female adult (0.812)\narms control control arms (0.473)\nTable 1: Examples of correlation items. Numbers in\nparentheses are similarity scores between target phrase\nand source phrase. Upper half shows normal examples,\nand lower half shows controlled items.\ntrols, and because this allows us to leverage larger\namounts of annotated phrase similarity data.\n3.1 Phrase similarity correlation\nWe ﬁrst evaluate phrase representations by as-\nsessing their alignment with human judgments of\nphrase meaning similarity. For testing this corre-\nspondence, we use the BiRD (Asaadi et al., 2019)\ndataset. BiRD is a bigram relatedness dataset de-\nsigned to evaluate composition, consisting of 3,345\nbigram pairs (examples in Table 1), with source\nphrases paired with numerous target phrases, and\nhuman-rated similarity scores ranging from 0 to 1.\nIn addition to testing on the full dataset, we de-\nsign a controlled experiment to remove effects of\nword overlap, by ﬁltering the dataset to pairs in\nwhich the two phrases consist of the same words.\nWe refer to these pairs as “AB-BA” pairs (following\nterminology of the authors of the BiRD dataset),\nand show examples in the lower half of Table 1.\nWe run similarity tests as follows: given a\nmodel M with layers L, for ith layer li ∈L and\na source-target phrase pair, we compute repre-\nsentations of source phrase pi\nrep(src) and target\nphrase pi\nrep(trg), where rep is a representation\ntype from Section 4, and we compute their co-\nsine cos(pi\nrep(src), pi\nrep(trg)). Pearson correlation\nri of layer li is then computed between cosine and\nhuman-rated score for all source-target pairs.\n3.2 Paraphrase classiﬁcation\nWe further investigate the nature of phrase represen-\ntations by testing their capacity to support binary\nparaphrase classiﬁcation. This test allows us to\nexplore whether we will see better alignment with\nhuman judgments of meaning similarity if we use\nmore complicated operations than cosine similar-\nity comparison. For the classiﬁcation tasks, we\ndraw on PPDB 2.0 (Pavlick et al., 2015), a widely-\nused database consisting of paraphrases with scores\ngenerated by a regression model. To formulate\nour binary classiﬁcation task, after ﬁltering out\nlow-quality paraphrases (discussed in Section 5),\nwe use phrase pairs (source phrase, target phrase)\nfrom PPDB as positive pairs, and randomly sample\nphrases from the complete PPDB dataset to form\nnegative pairs (source phrase, random phrase).\nBecause word overlap is also a likely cue for\nparaphrase classiﬁcation, we ﬁlter to a controlled\nversion of this dataset as well, as illustrated in Ta-\nble 2. We formulate the controlled experiment here\nas holding word overlap between source phrase and\ntarget phrase to be exactly 50% for both positive\nand negative samples. Our choice of 50% word\noverlap in this case is necessary for construction of\na sufﬁciently large, balanced classiﬁcation dataset\n(AB-BA pairs in PPDB are too few to support clas-\nsiﬁer training, and AB-BA pairs are more likely\nto be non-paraphrases). Note, however, that by\ncontrolling word overlap to be exactly 50% for all\nphrase pairs, we still hold constant the amount of\nword overlap between phrases, which is the cue\nthat we wish to remove. As an additional control,\neach source phrase is paired with an equal number\nof paraphrases and non-paraphrases, to avoid the\nclassiﬁer inferring labels based on phrase identity.\nFormally, for each model layer li and representa-\ntion type rep, we train\nCLFi\nrep = MLP([pairi\nrep])\nwhere pairi\nrep represents embedding concatena-\ntions of each source phrase and target phrase:\npairi\nrep = [pi\nrep(src); pi\nrep(trg)]\nThe classiﬁer is trained on binary classiﬁcation of\nwhether concatenated inputs represent paraphrases.\n4 Representation types\nA variety of approaches have been taken for repre-\nsenting sentences and phrases when all tokens out-\nput contextualized representations, as in our tested\ntransformers. To clarify the phrasal information\npresent in different forms of phrase representation,\nwe experiment with a number of different combina-\ntions of token embeddings as representation types.\nFormally, let [T0, ··· , Tk] be an input sequence\nof length k + 1, with corresponding embeddings\n4899\nNormal Examples\nSource Phrase Target Phrase\nare crucial\nis absolutely vital (pos)\nwas a matter of concern (neg)\nis an essential part (pos)\nare exacerbating (neg)\nControlled Examples\nSource Phrase Target Phrase\ncommunication infrastructure telecommunications infrastructure (pos)\ndata infrastructure (neg)\nTable 2: Examples of classiﬁcation items. Classiﬁcation labels between target phrase and source phrase are in\nparentheses. Upper half shows normal examples, and lower half shows controlled items.\nFigure 1: Example input sequences (BERT format).\nCLS is a special token at beginning of sequence. To-\nkens in yellow correspond to Head-Word. Avg-Phrase\ncontains element-wise average of phrase word embed-\ndings. Avg-All averages embeddings of all tokens.\nat ith layer [ei\n0, ··· , ei\nk]. Assume the phrase spans\nthe sequence [ a, b], where 0 ≤a ≤b ≤k. Be-\ncause two-word phrases are atypical inputs for\nthese models, we experiment both with inputs of\nthe two-word phrases alone (“phrase-only”), as\nwell as inputs with the phrases embedded in sen-\ntences (“context-available”). This is illustrated in\nFigure 1 along with phrase representation types.\nWe test the following forms of phrase representa-\ntion, drawn from each model and layer separately:\nCLS Depending on speciﬁc models, this special\ntoken can be the ﬁrst or last token of the input\nsequence (i.e. ei\n0 or ei\nk). In many applications, this\ntoken is used to represent the full input sequence.\nHead-Word In each phrase, the head word is the\nsemantic center the phrase. For instance, in the\nphrase “public service”, “service” is the head word,\nexpressing the central meaning of the phrase, while\n“public” is a modiﬁer. Because phrase heads are\nnot annotated in our datasets, we approximate the\nhead by taking the embedding of the ﬁnal word\nof the phrase. This representation is proposed as\na potential representation of the whole phrase, if\ninformation is being composed into a central word:\npi\nhw = ei\nb\nAvg-Phrase For this representation type we av-\nerage the embeddings of the tokens in the target\nphrase (dashed box in Figure 1). This type of aver-\naging of token embeddings is a common means of\naggregate representation (Wieting et al., 2015).\npi\nap = 1\nb −a + 1\nb∑\nx=a\nei\nx\nAvg-All Expanding beyond the tokens in “Avg-\nPhrase”, this representation averages embeddings\nfrom the full input sequence.\npi\naa = 1\nk + 1\nk∑\nx=0\nei\nx\nSEP With some variation between models, the\nSEP token is typically a separator for distinguishing\ninput sentences, and is often the last token (ei\nk) or\nsecond to last token (ei\nk−1) of a sequence.\n5 Experimental setup\nEmbeddings of each token are obtained by feed-\ning input sequences through pre-trained contex-\ntual encoders. We investigate the “base” version\nof ﬁve transformers: BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019b), DistilBERT (Sanh\net al., 2019), XLNet (Yang et al., 2019b) and XLM-\nRoBERTa (Conneau et al., 2019). For the models\nanalyzed in this paper, we are using the implemen-\ntation of Wolf et al. (2019), 2 which is based on\nPyTorch (Paszke et al., 2019).\n2https://github.com/huggingface/transformers\n4900\nFigure 2: Correlation on BiRD dataset, phrase-only input setting. First row shows results on full dataset, and\nsecond row on controlled AB-BA pairs. Layer 0 corresponds to input embeddings passing to the model.\nFor correlation analysis, we ﬁrst use the com-\nplete BiRD dataset, consisting of 3,345 phrase\npairs.3 We then test with our controlled subset of\nthe data, consisting of 410 AB-BA pairs. For clas-\nsiﬁcation tasks, we ﬁrst do preprocessing on PPDB\n2.0,4 ﬁltering out pairs containing hyperlinks, non-\nalphabetical symbols, and trivial paraphrases based\non abbreviation or tense change. For our initial clas-\nsiﬁcation test, we use 13,050 source-target phrase\npairs (of varying word overlap) from this prepro-\ncessed dataset. We then test with our controlled\ndataset, consisting of 11,770 source-target phrase\npairs (each with precisely 50% word overlap). For\neach paraphrase classiﬁcation task, 25% of selected\ndata is reserved for testing. We use a multi-layer\nperceptron classiﬁer with a single hidden layer of\nsize 256 with ReLU activation, and a softmax layer\nto generate binary labels. We use a relatively sim-\nple classiﬁer following the reasoning of Adi et al.\n(2016), that this allows examination of how easily\nextractable information is in these representations.\nFor both correlation and classiﬁcation tasks, we\nexperiment with phrase-only inputs and context-\navailable (full-sentence) inputs. To obtain sentence\ncontexts, we search for instances of source phrases\nin a Wikipedia dump, and extract sentences con-\ntaining them. For a given phrase pair, target phrases\nare embedded in the same sentence context as the\nsource phrase, to avoid effects of varying sentence\nposition between phrases of a given pair. 5\n3http://saifmohammad.com/WebPages/BiRD.html\n4http://paraphrase.org\n5Because context sentences are extracted based on source\nphrases, our use of the same context for source and target\nphrases can give rise to unnatural contextual ﬁt for target\n6 Results\n6.1 Similarity correlation\nFull dataset The top row of Figure 2 shows\ncorrelation results on the full BiRD dataset for\nall models, layers, and representation types, with\nphrase-only inputs. Among representation types,\nAvg-Phrase and Avg-All consistently achieve the\nhighest correlations across models and layers. In all\nmodels but DistilBERT, correlation of Avg-Phrase\nand Avg-All peaks at layer 1 and decreases in sub-\nsequent layers with minor ﬂuctuations. Head-Word\nand SEP both show weaker, but non-trivial, corre-\nlations. The CLS token is of note with a consis-\ntent rapid rise as layers progress, suggesting that\nit quickly takes on properties of the words of the\nphrase. For all models but DistilBERT, CLS token\ncorrelations peak in middle layers and then decline.\nModel-wise, XLM-RoBERTa shows the weakest\noverall correlations, potentially due to the fact that\nit is trained to infer input language and to handle\nmultiple languages. BERT retains fairly consis-\ntent correlations across layers, while RoBERTa and\nXLNet show rapid declines as layers progress, sug-\ngesting that these models increasingly incorporate\ninformation that deviates from human intuitions\nabout phrase smilarity. DistilBERT, despite being\nof smaller size, demonstrates competitive correla-\ntion. The CLS token in DistilBERT is notable for\nits continuing rise in correlation strength across\nphrases. We consider this acceptable for the sake of controlling\nsentence position—and if anything, differences in contextual\nﬁt may aid models in distinguishing more and less similar\nphrases. The slight boost observed on the full datasets (for\nAvg-Phrase) suggests that the sentence contexts do provide\nthe intended beneﬁt from using input of a more natural size.\n4901\nFigure 3: Correlation on BiRD dataset with phrases embedded in sentence context (context-available input setting).\nlayers. This suggests that DistilBERT in particular\nmakes use of the CLS token to encode phrase infor-\nmation, and unlike other models, its representations\nretain the relevant properties to the ﬁnal layer.\nControlled dataset Turning to our controlled\nAB-BA dataset, we examine the extent to which the\nabove correlations indicate sophisticated phrasal\ncomposition versus effective encoding of informa-\ntion about phrases’ component words. The bottom\nrow of Figure 2 shows the correlations on this con-\ntrolled subset. We see that performance of all mod-\nels drops signiﬁcantly, often with roughly zero cor-\nrelation. Avg-All and Avg-Phrase no longer dom-\ninate the correlations, suggesting that these repre-\nsentations capture word information, but not higher-\nlevel compositional information. XLM-RoBERTa\nand XLNet show particularly low correlations, sug-\ngesting heavier reliance on word content. Notably,\nthe CLS tokens in RoBERTa and DistilBERT stand\nout with comparatively strong correlations in later\nlayers. This suggests that the rise that we see in\nCLS correlations for DistilBERT in particular may\ncorrespond to some real compositional signal in\nthis token, and for this model the CLS token may\nin fact correspond to something more like a repre-\nsentation of the meaning of the full input sequence.\nThe Avg-Phrase representation for RoBERTa also\nmakes a comparatively strong showing.\nIncluding sentence context Figure 3 shows the\ncorrelations when target phrases are embedded as\npart of a sentence context, rather than in isolation.\nAs can be expected, Avg-Phrase is now consis-\ntently the highest in correlation on the full dataset—\nother tokens are presumably more impacted by the\npresence of additional words in the context. We\nalso see that the Avg-Phrase correlations no longer\ndrop so dramatically in later layers, suggesting\nthat when given full sentence inputs, models re-\ntain more word properties in later layers than when\ngiven only phrases. This general trend holds also\nfor Avg-All and Head-Word representations.\nIn the AB-BA setting, we see that presence\nof context does boost overall correlation with hu-\nman judgment. Of note is XLM-RoBERTa’s Avg-\nPhrase, which without sentence context has zero\ncorrelation in the AB-BA setting, but which with\nsentence context reaches our highest observed AB-\nBA correlations in its ﬁnal layers. However, even\nwith context, the strongest correlation across mod-\nels is still less than 0.3. It is still the case, then, that\ncorrelation on the controlled data degrades signiﬁ-\ncantly relative to the full dataset. This indicates that\neven when phrases are input within sentence con-\ntexts, phrase representations in transformers reﬂect\nheavy reliance on word content, largely missing ad-\nditional nuances of compositional phrase meaning.\n6.2 Paraphrase classiﬁcation\nFull dataset Results for our full paraphrase clas-\nsiﬁcation dataset, with phrase-only inputs, are\nshown in the top row of Figure 4. Accuracies\nare overall very high, and we see generally sim-\nilar patterns to the correlation tasks. Best accu-\nracy is achieved by using Avg-Phrase and Avg-\nAll representations. RoBERTa, XLM-RoBERTa,\nand XLNet show decreasing correlations for top-\nperforming representations in later layers, while\nBERT and DistilBERT remain more consistent\nacross layers. Performance of CLS requires a few\n4902\nFigure 4: Classiﬁcation accuracy on PPDB dataset (phrase-only input setting). First row shows classiﬁcation\naccuracy on original dataset, and second row shows accuracy on controlled dataset.\nFigure 5: Classiﬁcation accuracy on PPDB dataset with phrases embedded in sentence context. First row shows\nclassiﬁcation accuracy on original dataset, and second row shows accuracy on controlled dataset.\nlayers to peak, with top performance around mid-\ndle layers, and in some models shows poor per-\nformance in later layers. SEP shows unstable per-\nformance compared to other representations, espe-\ncially in DistilBERT and RoBERTa.\nControlled dataset The bottom row of Figure 4\nshows classiﬁcation accuracy when word overlap\nis held constant. Consistent with the drop in cor-\nrelations on the controlled AB-BA experiments\nabove, classiﬁcation performance of all models\ndrops down to only slightly above chance perfor-\nmance of 50%. This suggests that the high classiﬁ-\ncation performance on the full dataset relies largely\non word overlap information, and that there is lit-\ntle higher-level phrase meaning information to aid\nclassiﬁcation in the absence of the overlap cue. We\nsee in some cases a very slight trend such that clas-\nsiﬁcation accuracy increases a bit toward middle\nlayers—so to the extent that there is any compo-\nsitional phrase information being captured, it may\nincrease within representations in the middle lay-\ners. Overall, the consistency of these results with\nthose of the correlation analysis suggests that the\napparent lack of accurate compositional meaning\ninformation in our tested phrase representations\nis not simply a result of cosine correlations being\ninappropriate for picking up on correspondences.\nIncluding sentence context Figure 5 shows the\nclassiﬁcation results for representations of phrases\nembedded in sentence contexts. The patterns\nlargely align with our observations from the corre-\nlation task. Performance on the full dataset is still\nhigh, with Avg-Phrase now showing consistently\nhighest performance, being least inﬂuenced by the\npresence of new context words. In the controlled\nsetting, we see the same substantial drop in per-\n4903\nhorse ran color ran\ngallop POS NEG\ndissolve NEG POS\nTable 3: An example of landmark experiment of verb\n”run”. Representations are expected to have higher co-\nsine similarities between phrase and landmark word\nthat are marked “POS”.\nformance relative to the full dataset—there is very\nslight improvement over the phrase-only represen-\ntations, but the highest accuracy among all models\nis still around 0.6. Thus, the inclusion of sentence\ncontext again does not provide any additional ev-\nidence for sophisticated compositional meaning\ninformation in the tested phrase representations.\n7 Qualitative analysis: sense\ndisambiguation\nThe above analyses rely on testing models’ sensitiv-\nity to meaning similarity between two phrases. In\nthis section we complement these analyses with an-\nother test aimed at assessing phrasal composition:\ntesting models’ ability to select the correct senses\nof polysemous words in a composed phrase, as pro-\nposed by Kintsch (2001). Each test item consists\nof a) a central verb, b) two subject-verb phrases\nthat pick out different senses of the verb, and c)\ntwo landmark words, each associating with one of\nthe target senses of the verb. Table 3 shows an ex-\nample with central verb “ran” and phrases “horse\nran”/ “color ran”. The corresponding landmark\nwords are “gallop”, which associates with “horse\nran”, and “dissolve”, which associates with “color\nran”. The reasoning is that composition should\nselect the correct verb meaning, shifting represen-\ntations of the central verbs—and of the phrase as\na whole—toward landmarks with closer meaning.\nFor this example, models should produce phrase\nembeddings such that “horse ran” is closer to “gal-\nlop” and “color ran” is closer to “dissolve”. We\nuse the items introduced in Kintsch (2001), which\nconsist of a total of 4 sets of landmark tests. We\nfeed landmarks and phrases respectively through\neach transformer, without context, to generate cor-\nresponding representations pi\nrep for each layer li\nand representation type rep. Cosine similarity be-\ntween each phrase-landmark pair is computed and\ncompared against expected similarities.\nFigure 6 shows the percentage of phrases that\nfall closer to the correct landmark word than to the\nincorrect one, averaged over 16 phrase-landmark\nword pairs. We see strong overall performance\nacross models, suggesting that the information\nneeded for this task is successfully captured by\nthese models’ representations. Additionally, we\nsee that the patterns largely mirror the results above\nfor correlation and classiﬁcation on uncontrolled\ndatasets. Particularly, Avg-Phrase and Avg-All\nshow comparatively strong performance across\nmodels. RoBERTa and XLNet show stronger per-\nformance in early layers, dropping off in later lay-\ners, while BERT and DistilBERT show more con-\nsistency across layers. XLM-RoBERTa and XLNet\nshow lower performance overall.\nFor this verb sense disambiguation analysis, the\nHead-Word token is of note because it corresponds\nto the central verb of interest, so its sense can\nonly be distinguished by its combination with the\nother word of the phrase. XLM-RoBERTa has\nthe weakest performance with Head-Word, while\nBERT and DistilBERT demonstrate strong disam-\nbiguation with this token. As for the CLS token,\nRoBERTa produces the highest quality representa-\ntion at layer 1, and BERT outperforms other models\nstarting from layer 6, with DistilBERT also show-\ning strong performance across layers.\nNotably, the observed parallels to our correlation\nand classiﬁcation results are in alignment with the\nuncontrolled rather than the controlled versions of\nthose tests. So while these parallels lend further\ncredence to the general observations that we make\nabout phrase representation patterns across models,\nlayers, and representation types, it is worth not-\ning that these landmark composition tests may be\nsusceptible to lexical effects similar to those con-\ntrolled for above. Since these test items are too few\nto ﬁlter with the above methods, we leave in-depth\ninvestigation of this question to future work.\n8 Discussion\nThe analyses reported above yield two primary\ntakeaways. First, they shed light on the nature\nof these models’ phrase representations, and the\nextent to which they reﬂect word content versus\nphrasal composition. At many points in these mod-\nels there is non-trivial alignment with human judg-\nments of phrase similarity, paraphrase classiﬁca-\ntion, and verb sense selection. However, when we\ncontrol our correlation and classiﬁcation tests to\nremove the cue of word overlap, we see little evi-\ndence that the representations reﬂect sophisticated\n4904\nFigure 6: Landmark experiments. Y-axis denotes the percentage of samples that are shifted towards the correct\nlandmark words in each layer. Missing bars occur when representations are independent of input at layer 0, such\nthat cosine similarity between phrases and landmarks will always be 1.\nphrase composition beyond what can be gleaned\nfrom word content. While we see strong perfor-\nmance on classic sense selection items designed\nto test phrase composition, the observed results\nlargely parallel those from the uncontrolled ver-\nsions of the correlation and classiﬁcation analyses,\nsuggesting that success on this landmark test may\nreﬂect lexical properties more than sophisticated\ncomposition. Given the importance of systematic\nmeaning composition for robust and ﬂexible lan-\nguage understanding, based on these results we\npredict that we will see corresponding weaknesses\nas more tests emerge for these models’ handling of\nsubtle meaning differences in downstream tasks.\nOur systematic examination of models, layers\nand representation types yields a second takeaway\nin the form of practical implications for selecting\nand extracting representations from these models.\nFor faithful representations of word content, Avg-\nPhrase is generally the strongest candidate. If only\nthe phrase is embedded, drawing from earlier lay-\ners is best in RoBERTa, XLM-RoBERTa, and XL-\nNet, while middle layers are better in BERT, and\nlater layers in DistilBERT. If the phrase is input\nas part of a sentence, middle layers are generally\nbest across models. Though the CLS token is often\ninterpreted to represent a full input sequence, we\nﬁnd it to be a poor phrase representation even with\nphrase-only input, with the notable exception of\nthe ﬁnal layer of DistilBERT.\nAs for representations that reﬂect true phrase\nmeaning composition, we have established that\nsuch representations may not currently be avail-\nable in these models. However, to the extent\nthat we do see weak evidence of potential com-\npositional meaning sensitivity, this appears to be\nstrongest in DistilBERT’s CLS token in ﬁnal layers,\nin RoBERTa’s Avg-Phrase representation in later\nlayers, and in XLM-RoBERTa’s Avg-Phrase repre-\nsentation from later layers only when the phrase is\ncontained within a sentence context.\n9 Conclusions and future directions\nWe have systematically investigated the nature of\nphrase representations in state-of-the-art transform-\ners. Teasing apart sensitivity to word content ver-\nsus phrase meaning composition, we ﬁnd strong\nsensitivity across models when it comes to word\ncontent encoding, but little evidence of sophisti-\ncated phrase composition. The observed sensitivity\npatterns across models, layers, and representation\ntypes shed light on practical considerations for ex-\ntracting phrase representations from these models.\nFuture work can apply these tests to a broader\nrange of models, and continue to develop controlled\ntests that target encoding of complex compositional\nmeanings, both for two-word phrases and for larger\nmeaning units. We hope that our ﬁndings will stim-\nulate further work on leveraging the power of these\ngeneralized transformers while improving their ca-\npacity to capture compositional meaning.\nAcknowledgments\nWe would like to thank three anonymous review-\ners for valuable questions and suggestions for im-\nproving this paper. We also thank members of the\nUniversity of Chicago CompLing Lab, and the Toy-\nota Technological Institute at Chicago, for helpful\ncomments and feedback on this work. This mate-\nrial is based upon work supported by the National\nScience Foundation under Award No. 1941160.\n4905\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nShima Asaadi, Saif Mohammad, and Svetlana Kir-\nitchenko. 2019. Big bird: A large, ﬁne-grained,\nbigram relatedness dataset for examining semantic\ncomposition. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 505–516.\nJoris Baan, Jana Leible, Mitja Nikolaus, David Rau,\nDennis Ulmer, Tim Baumg¨artner, Dieuwke Hupkes,\nand Elia Bruni. 2019. On the realization of compo-\nsitionality in neural networks. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 127–\n137.\nGeoff Bacon and Terry Regier. 2019. Does bert\nagree? evaluating knowledge of structure depen-\ndence through agreement relations. arXiv preprint\narXiv:1908.09892.\nMarco Baroni and Roberto Zamparelli. 2010. Nouns\nare vectors, adjectives are matrices: Representing\nadjective-noun constructions in semantic space. In\nProceedings of the 2010 Conference on Empirical\nMethods in Natural Language Processing, pages\n1183–1193. Association for Computational Linguis-\ntics.\nShammur Absar Chowdhury and Roberto Zamparelli.\n2018. RNN simulations of grammaticality judg-\nments on long-distance dependencies. In Proceed-\nings of the 27th international conference on compu-\ntational linguistics, pages 133–144.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. In Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n276–286.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018).\nAlexis Conneau, Germ´an Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $ &!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136.\nIshita Dasgupta, Demi Guo, Andreas Stuhlm ¨uller,\nSamuel J Gershman, and Noah D Goodman. 2018.\nEvaluating compositionality in sentence embed-\ndings. arXiv preprint arXiv:1802.04302.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209.\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1790–1801.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\nby means of simple classiﬁcation tasks. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP, pages 134–139.\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\ntan Ruppin. 2001. Placing search in context: The\nconcept revisited. In Proceedings of the 10th inter-\nnational conference on World Wide Web, pages 406–\n414.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42.\nAlona Fyshe, Leila Wehbe, Partha Talukdar, Brian Mur-\nphy, and Tom Mitchell. 2015. A compositional and\ninterpretable semantic space. In Proceedings of the\n2015 conference of the north american chapter of\nthe association for computational linguistics: Hu-\nman language technologies, pages 32–41.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. Ppdb: The paraphrase\ndatabase. In Proceedings of the 2013 Conference of\nthe North American Chapter of the Association for\n4906\nComputational Linguistics: Human Language Tech-\nnologies, pages 758–764.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and\nAnna Korhonen. 2016. Simverb-3500: A large-\nscale evaluation set of verb similarity. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2173–2182.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nDieuwke Hupkes, Anand Singh, Kris Korrel, German\nKruszewski, and Elia Bruni. 2018. Learning compo-\nsitionally through attentive guidance. arXiv preprint\narXiv:1805.09657.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does bert learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657.\nJaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\n2019. Analysing neural language models: Con-\ntextual decomposition reveals default reasoning in\nnumber and gender assignment. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 1–11.\nYoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and G ´abor Melis. 2019. Unsuper-\nvised recurrent neural network grammars. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1105–1117.\nWalter Kintsch. 2001. Predication. Cognitive science,\n25(2):173–202.\nJosef Klafka and Allyson Ettinger. 2020. Spying on\nyour neighbors: Fine-grained probing of contex-\ntual embeddings for information about surrounding\nwords. arXiv preprint arXiv:2005.01810.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nAdam Liˇska, Germ ´an Kruszewski, and Marco Baroni.\n2018. Memorize or generalize? searching for a\ncompositional rnn in a haystack. arXiv preprint\narXiv:1802.06467.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3428–3448.\nJeff Mitchell and Mirella Lapata. 2008. Vector-based\nmodels of semantic composition. In proceedings of\nACL-08: HLT, pages 236–244.\nJeff Mitchell and Mirella Lapata. 2010. Composition\nin distributional models of semantics. Cognitive sci-\nence, 34(8):1388–1429.\nW James Murdoch, Peter J Liu, and Bin Yu. 2018. Be-\nyond word importance: Contextual decomposition\nto extract interactions from lstms. In International\nConference on Learning Representations.\nNavnita Nandakumar, Timothy Baldwin, and Bahar\nSalehi. 2019. How well do embedding models cap-\nture non-compositionality? a view from multiword\nexpressions. In Proceedings of the 3rd Workshop on\nEvaluating Vector Space Representations for NLP,\npages 27–34.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems,\npages 8024–8035.\nEllie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,\nBenjamin Van Durme, and Chris Callison-Burch.\n2015. Ppdb 2.0: Better paraphrase ranking, ﬁne-\ngrained entailment relations, word embeddings, and\nstyle classiﬁcation. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 425–430.\n4907\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual word\nembeddings: Architecture and representation. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n1499–1509.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nNaomi Saphra and Adam Lopez. 2020. Word inter-\ndependence exposes how lstms compose representa-\ntions. arXiv preprint arXiv:2004.13195.\nVered Shwartz and Ido Dagan. 2019. Still a pain in the\nneck: Evaluating text representations on lexical com-\nposition. Transactions of the Association for Com-\nputational Linguistics, 7:403–419.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBert rediscovers the classical nlp pipeline. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel Bowman, Dipanjan\nDas, et al. 2019b. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In 7th International Con-\nference on Learning Representations, ICLR 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353–355.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and\nKaren Livescu. 2015. Towards universal para-\nphrastic sentence embeddings. arXiv preprint\narXiv:1511.08198.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 211–221.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019a. PAWS-X: A Cross-lingual Adver-\nsarial Dataset for Paraphrase Identiﬁcation. In Proc.\nof EMNLP.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019b. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In Advances in\nneural information processing systems, pages 5754–\n5764.\nDa Yin, Tao Meng, and Kai-Wei Chang. 2020. Sen-\ntibert: A transferable transformer-based architecture\nfor compositional sentiment semantics.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase Adversaries from Word Scram-\nbling. In Proc. of NAACL."
}