{
  "title": "Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation",
  "url": "https://openalex.org/W3102446692",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2805327369",
      "name": "Maximiliana Behnke",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A1996490661",
      "name": "Kenneth Heafield",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034230673",
    "https://openalex.org/W2907279971",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3045883122",
    "https://openalex.org/W2964240726",
    "https://openalex.org/W2948130861",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2764043458",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W2983981554",
    "https://openalex.org/W2963643655",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2948635472",
    "https://openalex.org/W2994914025",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2965046076",
    "https://openalex.org/W2805493160",
    "https://openalex.org/W2988390689",
    "https://openalex.org/W2808794272",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W2915589364",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2767785892"
  ],
  "abstract": "The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish→English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with English→German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2664–2674,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n2664\nLosing Heads in the Lottery:\nPruning Transformer Attention in Neural Machine Translation\nMaximiliana Behnke\nThe University of Edinburgh\nmaximiliana.behnke@ed.ac.uk\nKenneth Heaﬁeld\nThe University of Edinburgh\nkheafiel@inf.ed.ac.uk\nAbstract\nThe attention mechanism is the crucial com-\nponent of the transformer architecture. Re-\ncent research shows that most attention heads\nare not conﬁdent in their decisions and can\nbe pruned after training. However, remov-\ning them before training a model results in\nlower quality. In this paper, we apply the lot-\ntery ticket hypothesis to prune heads in the\nearly stages of training, instead of doing so\non a fully converged model. Our experiments\non machine translation show that it is possi-\nble to remove up to three-quarters of all at-\ntention heads from a transformer-big model\nwith an average −0.1 change in BLEU for\nTurkish→English. The pruned model is 1.5\ntimes as fast at inference, albeit at the cost of\nlonger training. The method is complementary\nto other approaches, such as teacher-student,\nwith our English→German student losing 0.2\nBLEU at 75% encoder attention sparsity.\n1 Introduction\nThe transformer model (Vaswani et al., 2017) per-\nforms well for a variety of tasks, including neu-\nral machine translation (Dong et al., 2018; Grund-\nkiewicz and Junczys-Dowmunt, 2019; Junczys-\nDowmunt, 2018). However, like many neural net-\nworks, it is overparametrised, and inference is\ncostly. Attention heads are the headline feature of\nthe transformer model, essential to learning rela-\ntionships between words as well as complex struc-\ntural representations. V oita et al. (2019) showed\nthat many of these heads could be pruned in a fully\ntrained model, but removing the same heads be-\nfore training yielded lower quality. We investigate\na third way: pruning heads in early training. Em-\npirically our method enables even more pruning,\nwhich is useful for faster machine translation.\nReinitialising a model with the same pruned\nstructure underperformed in V oita et al. (2019),\nwhich is consistent with the lottery ticket hypoth-\nesis (Frankle and Carbin, 2019). According to\nthe lottery ticket hypothesis, randomly initialis-\ning a model is akin to buying lottery tickets and\na smaller network, such as a pruned model, buys\nfewer tickets. Prior lottery ticket research prunes\nindividual parameters to form a sparse network;\nwe show that this logic extends to entire trans-\nformer heads. We follow lottery ticket train-\ning strategies (Frankle et al., 2019) to prune in\nearly training, achieving a better trade-off between\npruning and quality than pruning after training\n(V oita et al., 2019).\nOur main goal is faster inference speed for ma-\nchine translation deployment with minimal impact\non quality. Pruning heads means they can be re-\nmoved from the model entirely (with other heads\nshifted down), resulting in a layer conﬁgured to\nhave fewer heads. Unlike most work on pruning\n(Zhu and Gupta, 2017; Gale et al., 2019), there\nis no need for sparse matrices, block-sparse ma-\ntrix operators, or additional masking. In particular,\nwe go further than V oita et al. (2019) by removing\nrather than masking.\nIn this paper we combine ﬁndings of both V oita\net al. (2019) (“what”) and Frankle and Carbin\n(2019) (“how”) to prune attention heads. First,\nwe deﬁne a training scheme based on an itera-\ntive approach that does not require full conver-\ngence of a model each time partial pruning takes\nplace. To analyse the impact of pruning in a\nvariety of settings, we experiment with a stock\nand highly optimised system across two language\npairs: Turkish →English and English →German.\nWe present and analyse our results in Sections 7\nand 8.\nOur key ﬁndings show that:\n1. The lottery ticket hypothesis can be applied\nto prune whole blocks of parameters, instead\nof removing them separately.\n2665\n2. Most attention heads can be removed early\ninto training without signiﬁcant damage to\nquality. The most aggressive attention prun-\ning loses about 1 BLEU point with 80-90%\nblock sparsity.\n3. The lottery ticket approach achieves better re-\nsults than a model trained from scratch with\nthe same structure.\n4. Pruned models exhibit patterns in regards to\na number of heads in layers. For example,\ncontext attention gets more important as lay-\ners go on. The decoder requires self-attention\nonly in the ﬁrst layer — the rest is redundant,\nthus removable.\n2 Related work\nMagnitude pruning is one of the simplest algo-\nrithms, in which the smallest weights are removed.\nSuccessfully applied to NMT (See et al., 2016),\nthis method works on a coefﬁcient level and of-\nten requires retraining to recover the damage done\nby pruning. Further research shows that training\na model from scratch with the same structure as\nthe pruned one yields subpar results. Finishing\ntraining is a necessary step to reduce the size of a\nmodel (Gale et al., 2019) without too much dam-\nage. However, the sparsity of singular weights is\ngenerally too low to be efﬁciently exploited by a\nCPU or GPU. Block sparsity (Narang et al., 2017)\nis more hardware friendly because masked blocks\ncan be skipped entirely. In this paper, we concen-\ntrate on a speciﬁc case of block sparsity that re-\nmoves entire attention heads from a model without\nmasking.\nBrix et al. (2020) applied the lottery ticket hy-\npothesis and other techniques to prune individ-\nual coefﬁcients from a transformer for machine\ntranslation. In their experiments, a stabilised ver-\nsion of lottery ticket pruning damages transla-\ntion quality by 2 BLEU points while removing\n80% of all parameters. They improve upon that\nfurther by proposing a mix of lottery ticket and\nmagnitude pruning. In their work, all layers are\npruned the same amount, whereas our work prunes\nglobally to reveal which layers can be pruned\nmore aggressively. They aimed to compress the\nmodel and did not report any speed results, subse-\nquently clarifying after their presentation that they\ndid not achieve a speed improvement. Here, we\naim for speed and only marginal improvements to\nsize. Rather than prune individual coefﬁcients, we\npruned entire heads which can then be removed\nfrom the model entirely without even calling a\nsparse matrix routine.\nPruning is usually done at the end of training\nand then requires either retraining or tuning. There\nis an ongoing research ﬁeld on integrating prun-\ning into training For example, Golub et al. (2018)\npruned weights that have accumulated the lowest\ntotal gradients and reduces the memory footprint\nto allow training much larger models than possible\non available hardware. Our lottery ticket method\ndoes not require to modify a training algorithm\nand can be easily scripted to work “out of the box”\nwith existing toolkits.\nXiao et al. (2019) observed that numerous com-\nputations in the attention mechanism are redun-\ndant with many layers sharing similar distribu-\ntions. They proposed reusing attention output\nwithin adjacent layers in a model, which requires\na model to learn which layers should be allowed\nto share outputs. This reuse of parameters could\nbe understood as a pruning method that concen-\ntrates on removing vertical redundancy, in contrast\nto our research, which is more horizontal.\nSince the attention mechanism is expensive to\nuse in a decoder — with O(n2) complexity loop-\ning when generating translations — the better op-\ntion would be to replace it with less expensive\nequivalent. In our teacher-student experiments,\nSimpler Simple Recurrent Unit (SSRU) (Kim\net al., 2019) replaces the decoder self-attention\nmechanism. Still, this approach leaves an encoder\nand context between them unchanged. The lottery\nticket pruning is complementary and can remove\nencoder and context heads on the top of it.\nLooking into an impact of attention on output,\nSerrano and Smith (2019) analysed a text classi-\nﬁcation task whether “high attention weights cor-\nrelate with greater impact on model predictions”.\nThey argued that, in contrast to a simple classiﬁ-\ncation, “for tasks with a much larger output space\n(such as language modelling or machine transla-\ntion) . . . almost anything may ﬂip the decision”.\nHowever, according to our experiments, careful\nhead removal based on their importance does not\ndamage quality.\n3 Background\nThe usual approach to pruning assumes that a\nmodel is converged ﬁrst and pruned second, op-\ntionally with continued training. Frankle et al.\n2666\n(2019) have shown that iteratively pruning a model\nuncovers smaller and better quality subnetworks\nin comparison to pruning just once at the end.\nStill, training a model until convergence at ev-\nery pruning iteration is too expensive to utilise\nfor most architectures. For this reason, Frankle\net al. (2019) introduced late resetting and early\nturnaround. Both of these methods combined\nshorten training time of each step in the itera-\ntive lottery scheme. Late resetting reverts parame-\nters after pruning back to a checkpoint from early\nstages of training, not to the starting initialisation.\nEarly turnaround means a model does not need to\nbe fully trained to make a pruning decision but can\napproximate that by doing short training loops.\nLottery ticket pruning has been applied to nat-\nural language processing (NLP) tasks, including\nNMT (Yu et al., 2020). The winning ticket for that\ntask was “remarkably robust to pruning” of singu-\nlar weights if embeddings were spared from prun-\ning. However, Yu et al. (2020) noted a linear drop\nin BLEU with sparsit.\nV oita et al. (2019) analysed the attention mech-\nanism and noticed that the majority of heads\nare useless: they either do not have linguisti-\ncally interpretable roles or cannot make reliable\nchoices when making alignments. Those heads\nwere pruned by tuning a model with a L0 regu-\nlariser that progressively switched off less essen-\ntial heads. The L0 regulariser needs a model to\nbe fully trained ﬁrst and then pruned while tuned.\nIn contrast, our paper focuses on pruning heads\nas early as possible in training so that a model\ncan converge with them removed. Using their se-\nlection heuristic, empirically we can safely prune\nmore heads overall.\n4 Methodology\nIn this section, we describe the lottery ticket ap-\nproach as well as the decision heuristic based on\nattention importance (V oita et al., 2019) to remove\nheads in our models.\n4.1 Lottery ticket\nWe apply an iterative pruning strategy based on\nFrankle and Carbin (2019), which introduced the\nlottery ticket hypothesis:\nA randomly-initialized, dense neural\nnetwork contains a subnetwork that is\ninitialized such that – when trained in\nisolation – it can match the test accu-\nracy of the original network after train-\ning for at most the same number of iter-\nations.\nIn other words, some parts of the network were\nluckily initialized and perform most of the work.\nOne could train a complete model, identify un-\nlucky heads with a pruning heuristic, and retrain\nthe pruned model starting with the same initial-\nization.1 This approach is expensive because the\nmodel is trained twice. Frankle et al. (2019)\npointed out that unlucky parameters can be iden-\ntiﬁed earlier in convergence, so it is not necessary\nto fully train a complete model ﬁrst. We follow\ntheir work by partially training a model to make a\npruning decision.\nFrankle and Carbin (2019) reported that pruning\niteratively yields smaller higher-quality networks\nthat converge faster than those pruned in a single\nround. Removing most of the attention heads in\none go seems too drastic using a simple heuris-\ntic, since other heads in layers may adapt to hav-\ning fewer parameters and the roles of pruned heads\nmay even transfer to those that are still active. For\nall these reasons, we apply a loop that iteratively\nprunes attention heads guided by partial training\n(Section 2). The training scheme is presented in\nFigure 1.\nLATE\nRESETTING\nCHECKPOINT\nSTEP 4:\nPRUNE\nTHE FINAL\nMODEL\nSTEP 3: TRAIN FOR \nY  UPDATES\nSTEP 5: REINITIALISE \nSTEP 6: CONVERGE\nSTEP 1:\nINITIALISE\nSTEP 2: \nTRAIN FOR \nX  UPDATES REPEAT STEPS 3-5 \nUNTIL SATISFIED\nFigure 1: Iterative lottery ticket pruning.\nFirst, we train a model for a set number of up-\ndates and keep it as a late resetting checkpoint.\nThen the pruning phase starts — the model trains\nfor a while, and selected heads are removed to\nhave other parameters reinitialised to the check-\npoint mentioned earlier at the end. That loop re-\npeats until we are satisﬁed with how many param-\neters were removed. Finally, the pruned model can\nbe converged.\n1The order of data fed into a model is also retained among\nexperiments.\n2667\n4.2 Attention conﬁdence\nThe lottery ticket hypothesis explains how pruning\nshould progress, but the question remains: which\nheads should be removed in each pruning itera-\ntion? Inspired by V oita et al. (2019), we are mostly\ninterested in heads that are conﬁdent in their deci-\nsions, which V oita et al. (2019) has shown to cor-\nrelate with major identiﬁable roles attention heads\nperforms. In their analysis, an attention head is de-\nﬁned as conﬁdent when it assigns a large weight\nto one of the words within a sentence That head\nshould routinely make strong alignments to be\nconsidered a candidate to remain in a model.\nWhen a head appears, its softmax layer com-\nputes a probability distribution over the words it\nattends to. We record the maximum of this prob-\nability distribution as conﬁdence. For example, a\ncontext head attends over source words s.\nconﬁdence = max\ns\nattention(s)\nThese conﬁdence values are averaged over all\ntimes the head appears while translating a develop-\nment corpus. For example, a context head appears\nonce per word in the output, so its conﬁdence is\naveraged over all words in the output.\n5 Baseline approaches\n5.1 Just fewer attention heads\nDo we even need to prune attention heads at all?\nCan we train a model that has fewer heads from\nthe beginning? The typical transformer imple-\nmentation described by Vaswani et al. (2017) ini-\ntialises attention matrices based on the embedding\ndimension and those matrices are split into sepa-\nrate heads. That means the fewer heads there are\nset to be in a model, the larger they are. To com-\npare models with different number of heads fairly,\nwe ﬁx their size to a constant instead.\nWe use all the parallel data allowed by the con-\nstrained condition of the WMT17 news task (Bojar\net al., 2017) for English →German ( 4.56M sen-\ntences) following a standard preprocessing: nor-\nmalisation, tokenisation, truecasing using Moses\nscripts, and BPE segmentation (Sennrich et al.,\n2016) with 36000 subwords. We tried training a\nmodel with 32 heads but could not due to memory\nconstraints. For that reason, we start with a typical\ntransformer-big (Vaswani et al., 2017) architecture\nusing recommended hyperparameters. It has 16\nheads of size 64 (64 ×16 = 1024). Then, we train\nthe same model but with 8, 4 and 2 heads of the\nsame size. The results are below in Table 1.\nModel Heads wmt14 wmt15 wmt16 Avg.\nTransformer-big 16 26.7 29.8 33.9 30.1\nTransformer-big-8 8 27.2 29.7 34.2 30.4\nTransformer-big-4 4 26.1 29.0 34.2 29.8\nTransformer-big-2 2 26.0 29.0 33.6 29.5\nTable 1: A transformer-big with different number of\nheads for English→German.\nWhen it comes to quality, the model needs a\nreasonable number of attention heads to perform\nwell. The more this number is reduced, the worse\nthe quality. However, more heads does not neces-\nsarily equal better translation quality. We concur\nthat 8 heads per layer strikes a perfect balance be-\ntween memory consumption and quality degrada-\ntion.\n5.2 Voita et al. (2019) pruning\nUsing the same language pair and dataset, we\ntried a pruning method presented by V oita et al.\n(2019). We used their Tensorﬂow implementa-\ntion2 with their training scripts, in which they set\nup a transformer-base architecture that it to be\npruned globally. The pruning scheme requires a\nbaseline model to fully converge ﬁrst and then\ntuned with a regulariser that masks the heads. The\nattention sparsity is controlled by aλhyperparam-\neter.\n20\n21\n22\n23\n24\n25\n26\n27\n10k 100k 200k 300k 400k 500k\nValidation BLEU\nTraining batches\nBaseline\nTuned λ = 0.05\nTuned λ = 0.10\nTuned λ = 0.15\nFigure 2: Validation BLEU for English →German\ntransformer-base baseline and pruned with V oita et al.\n(2019) models.\nThe main focus of V oita et al. (2019) was atten-\ntion analysis and its behaviour, rather than pruning\nand efﬁciency. Even though we used the authors’\n2https://github.com/lena-voita/\nthe-story-of-heads\n2668\nModel Sparsity wmt14 wmt15 wmt16 Avg. ∆\nBaseline 0% 26.7 29.8 34.5 30.3 -\nλ = 0.05 22% 26.4 29.7 34.0 30.0 -0.30\nλ = 0.10 53% 25.1 27.9 31.8 28.3 -2.0\nλ = 0.15 67% 23.5 25.8 28.8 26.0 -4.30\nTable 2: Evaluation BLEU of English →German\ntransformer-base models pruned with V oita et al.\n(2019)\nimplementation and the baseline achieved a rea-\nsonable score, pruning degraded its quality. Look-\ning at Figure 2, the more sparsity was enforced\nwith regularisation, the lower the translation qual-\nity. Even though we tuned for as long as the base-\nline training, the models do not recover. We tried\nexperimenting with various hyperparameters set-\ntings such as learning rate and its scheduling, but\nto no further success.\n5.3 Michel et al. (2019) pruning\nMichel et al. (2019) experiment with pruning dur-\ning and after training using a different heuristic:\nthey introduce a mask variable for each head then\ndeﬁne importance as the gradient of loss with re-\nspect to the mask variable. Their results are quite\npoor: pruning 40% of the total heads results in\n“staying within 85–90% of the original BLEU\nscore”. Results of pruning after training are worse:\nabout 3 BLEU points lost with 40% sparsity and\n10 BLEU points lost with 60% sparsity. 3 In our\nexperiments, we see no loss in average BLEU at\n67% sparsity.\nWe attribute our superior performance to adopt-\ning best practices for pruning during training\n(Frankle et al., 2019) and the choice of heuristic\nfollowing V oita et al. (2019) instead.\nMichel et al. (2019) reported that important\nheads emerge at the beginning of training. This\nsupports our hypothesis that pruning during train-\ning will outperform pruning after training.\n6 Setup\nIn order to investigate how effectively pruning\nworks, we concentrate on two language pairs:\nTurkish→English and English →German. The\nﬁrst one is considered a low-resource, even\nwith additional back-translated data. In con-\ntrast, English →German is a high-resource lan-\nguage pair with English not being a target lan-\n3Comparisons are based on their reported numbers, which\nuse non-standard tokenized BLEU (Post, 2018).\nguage. We trained and decoded our models using\nthe Marian machine translation toolkit (Junczys-\nDowmunt et al., 2018a).\nTurkish→English We use all the parallel data\nallowed by the constrained condition of the\nWMT18 (Bojar et al., 2018). The corpus consists\nof ~200 000parallel sentences plus an additional\n800 000 sampled from News Crawl and back-\ntranslated using a shallow NMT model trained\non the existing small bilingual corpora (Haddow\net al., 2018). We use the development and test sets\nprovided in 2016. We also evaluate on the 2017\nand 2018 testsets.\nThe preprocessing follows the steps of nor-\nmalisation, tokenisation, truecasing using Moses\nscripts, and BPE segmentation (Sennrich et al.,\n2016). The vocabulary is shared and contains\n36000 words. The architecture is transformer-big\n(Vaswani et al., 2017), trained using default rec-\nommended settings for such a model in Marian\ntoolkit.4 The models trained until cross-entropy\nhas stopped improving for 10 consecutive valida-\ntions, and select model checkpoints with highest\nBLEU scores.\nEnglish→German To measure impact on the\nspeed of a highly optimized system, we follow the\nWorkshop on Neural Generation and Translation\n2020 Efﬁciency Shared task. 5 The shared task\nspeciﬁed English →German translation under the\nWMT 2019 data condition (Barrault et al., 2019).\nAs is standard for efﬁcient translation, we applied\nteacher-student training (Kim and Rush, 2016) us-\ning the sentence-level system submitted by Mi-\ncrosoft to the WMT 2019 News Translation Task\n(Junczys-Dowmunt, 2019). The student mod-\nels have a standard 6-layers transformer encoder\n(Vaswani et al., 2017) but the decoder is a faster\ntwo-layer Simpler Simple Recurrent Unit (SSRU)\n(Kim et al., 2019). The embedding dimension is\n256, feed-forward network size is 1536. The mod-\nels use shared vocabulary of 32,000 subword units\ncreated with SentencePiece (Kudo and Richard-\nson, 2018).\nAll student models were trained on 13M sen-\ntences of available parallel data, using the concate-\nnated English-German WMT testsets from 2016-\n2018 as a validation set.6 The models were trained\n4Available via --task transformer-big.\n5https://sites.google.com/view/wngt20\n6The validation sentences were not teacher-translated.\n2669\nuntil BLEU stopped improving for 20 consecutive\nvalidations to overﬁt the teacher, and the check-\npoint with highest BLEU scores was selected.\nSince a student model should mimic the teacher as\nclosely as possible, we did not use regularization\nlike dropout and label smoothing. Other training\nhyperparameters were Marian defaults for train-\ning a Transformer Base model. 7 Student models\nhave sharp probability distributions so we trans-\nlate using beam size 1. Thanks to those settings,\nthe baseline translates about 2335 words per sec-\nond on a single CPU core.\n7 Experiments\nThe goal is to prune as many heads as possible\nwithout damaging translation quality. The prun-\ning procedure has some hyperparameters: the late\nresetting point, how long to train before making\na pruning decision and how many heads to prune\neach iteration. Exploring this space is expensive;\nwe arbitrarily set these to 5–6 saving checkpoints\n(25k batches for en–de, 12k for tr–en) Each prun-\ning iteration have run for 3-4 checkpoints (15k\nbatches for en–de, 8k for tr–en) after which se-\nlected attention heads are removed. The number of\nheads removed is roughly a total number of layers\ncontaining attention divided by 2. Removing less\nthan that makes pruning slow and removing more\nin one go results in a uniﬁed distribution of atten-\ntion heads (it usually picks one head per layer) and\nmay be too aggressive in some cases. In each iter-\nation, we change a seed value to make a model see\ndata in different order.\nWe focus on results roughly within 50% to 85%\nheads removed. This range covers the interesting\npart from minor to noticeable degradation in trans-\nlation quality. To evaluate an iteration, heads are\npruned as usual then we reset the model back to\nthe late resetting checkpoint and continue training\nto completion.\n7.1 Transformer-big (Turkish →English)\nSince we have shown that there is no need for hav-\ning 16 heads per layer in transformer-big architec-\nture (Section 5.1), we halve our attention matri-\nces to start pruning from 8 heads per layer to save\ntime. Thus, the model has 144 attention heads in\ntotal: 48 (6 layers with 8 heads each) self-attention\nheads in the encoder, 48 self-attention heads in the\ndecoder, and 48 context heads in the decoder that\n7Available via --task transformer-base.\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n0 10k 20k 30k 40k 50k\nValidation BLEU\nTraining batches\nBaseline\nIter9=50%\nIter10=56%\nIter11=61%\nIter12=67%\nIter13=72%\nIter14=78%\nIter15=83%\nFigure 3: Convergence of Turkish→English models af-\nter removing a given percentage of all attention heads.\nattend to the encoder. The model was pretrained\nfor 12k batches. Then, we train in a loop for 8k up-\ndates, remove 8 heads, revert and repeat until sat-\nisﬁed. The convergence progression is presented\nin Figure 3.\nThe baseline reaches the top BLEU scores\nquicker, but many pruned models still achieve\ncompetitive results later in training. The dashed\nvertical line shows the late resetting checkpoint.\nPruning up to 61–67% (Iter. 11–12 in Figure 3) of\nthe heads leads to longer convergence times, but\nnearly the same BLEU results on the development\nset. There is a breaking point of considerable dam-\nage at about 83% heads removed.\nIn Table 3, we perform evaluation and calcu-\nlate the average difference in BLEU between the\nunpruned and pruned models. Similarily to train-\ning validation, pruning up to 72% of heads mostly\nmaintains quality, then degrades progressively be-\nyond that point.\n7.2 Tiny student (English →German)\nIn this model, the decoder is already reduced to\ntwo tied layers. Since in self-attention is replaced\nwith an SSRU anyway and context is not priori-\ntised by our algorithm, we focus on pruning only\nthe encoder.\nWe pretrained the model for 25k batches, with\neach pruning iteration lasting 15k updates and re-\nmoving 3 heads from the encoder. The results are\npresented in Table 4. The models follow the trend\nset by our Turkish→English experiments — 75%\nof encoder heads can be removed with slight (-0.2)\n2670\nModel Sparsity Encoder Context Decoder wmt16 wmt17 wmt18 Avg. ∆\nBaseline 0% 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 22.7 21.9 23.1 22.6 -\nPruned i9 50% 7 2 4 6 7 8 0 1 2 3 6 7 8 3 2 4 1 1 22.7 22.4 23.5 22.9 0.3\nPruned i10 56% 7 1 3 6 7 8 0 1 2 3 5 6 8 2 2 3 0 0 23.1 22.2 23.5 22.9 0.3\nPruned i11 61% 6 1 3 5 7 8 0 1 2 2 4 5 8 1 1 2 0 0 22.9 22.0 23.4 22.8 0.2\nPruned i12 67% 5 1 3 5 6 7 0 1 2 2 3 4 7 1 0 1 0 0 22.5 22.1 23.5 22.7 0.1\nPruned i13 72% 4 1 2 5 5 6 0 1 2 2 2 3 6 1 0 0 0 0 22.8 21.6 23.1 22.5 -0.1\nPruned i14 78% 3 1 2 5 4 5 0 0 2 2 1 2 5 0 0 0 0 0 22.0 21.3 22.6 22.0 -0.6\nPruned i15 83% 2 1 1 4 4 4 0 0 1 2 0 1 4 0 0 0 0 0 21.9 21.2 22.6 21.9 -0.7\nTable 3: Evaluation of Turkish →English transformer-big models converged at i th pruning iteration with their\ndistribution of attention heads.\nModel Params Sparsity Enc. heads wmt16 wmt17 wmt18 wmt19 Avg. ∆\nBaseline 15,7M 0% 8 8 8 8 8 8 36.4 29.3 43.2 39.9 37.2 -\nPruned i9 14,8M 56% 5 1 3 1 5 6 36.5 29.1 43.4 40.0 37.3 0.1\nPruned i10 14,7M 63% 4 1 2 1 4 6 36.5 29.0 43.5 39.7 37.2 0.0\nPruned i11 14,6M 69% 3 0 2 1 4 5 36.3 29.0 43.3 39.9 37.1 -0.1\nPruned i12 14,5M 75% 2 0 2 1 3 4 36.3 28.8 43.1 39.8 37.0 -0.2\nPruned i13 14,4M 81% 1 0 2 1 2 3 36.3 28.9 42.7 39.5 36.9 -0.3\nPruned i14 14,3M 88% 0 0 1 1 1 3 35.7 28.4 42.1 38.9 36.3 -0.9\nTable 4: Evaluation of English→German transformer student models converged at ith pruning iteration, with their\ndistribution of attention heads.\ndamage to the quality. Pruning more than that is a\ntrade-off between sparsity and quality.\nIn conclusion, the lottery ticket approach suc-\ncessfully pruned attention heads in both large\ntransformer model and a tiny student architecture\nbased on a simple heuristic; we leave the general\ncase of block-sparse pruning to future work.\n8 Analysis\nIn this section, we further analyse our prun-\ning results in terms of pruning progress and\nhead distribution. We reinitialise our pruned\nEnglish→German models to demonstrate that the\nadvantage of pruning comes from lucky initialisa-\ntion, not the architecture itself.\nHead distribution In both Table 3 and 4,\nwe present attention distribution as it changes\nthroughout pruning iterations. Each attention\nprioritised heads differently depending on layer\ndepth and which attention type it is. Looking at\nTurkish→English results, the decoder attention is\npruned more eagerly with more and more heads\nremoved in each layer. The ﬁrst layer seems to\nbe crucial, others almost not at all. This seems to\nexplain the trend of student models having 1–2 de-\ncoder layers and still performing well. The context\nattention interlocks with the decoder self-attention\nwith each consecutive layer gaining more impor-\ntance than the previous one. When it comes to the\nencoder in both language pairs, the middle layers\ndo not hold the same signiﬁcance as the ﬁrst and\nlast ones.\nArchitecture or initialisation? To check if the\nlottery ticket hypothesis is right in the context\nof our paper, we reinitialise our pruned models\nwhile keeping their structure. We compare aver-\nage BLEU difference between pruned (Table 4)\nand trained from scratch (Table 5) models.\nThere is a consistent quality gap between\npruned and reinitialised models that widens with\nsparsity. It conﬁrms the assumptions made by\nthe lottery ticket hypothesis: starting with a larger\nmodel and then deliberately selecting attention\nheads reveals which are the “winning tickets” in\nthe initialisation lottery.\n9 Speed\nThe main objective of our research is to remove\nheads from a transformer to make inference faster.\nFor this reason, we make a trade-off between a to-\ntal training time and inference speed, which is par-\nticularly useful in an industry production environ-\nment. In Table 6, we compare how long it takes to\nprune and train a model in comparison to the base-\nline approach. In practice, if a model trains for 2–\n3 days, an additional day is needed for a pruning\nprocedure.\nTo compare translation speed, we select the\n2671\nModel Params Sparsity Enc. heads wmt16 wmt17 wmt18 wmt19 Avg. ∆\nBaseline 15,7M 0% 8 8 8 8 8 8 36.4 29.3 43.2 39.9 37.2 -\nReinit i9 14,8M 56.25% 5 1 3 1 5 6 36.5 28.9 43.1 40.0 37.1 -0.1\nReinit i10 14,7M 62.50% 4 1 2 1 4 6 36.4 28.9 43.0 39.6 37.0 -0.2\nReinit i11 14,6M 68.75% 3 0 2 1 4 5 36.1 28.9 42.7 39.4 36.8 -0.4\nReinit i12 14,5M 75.00% 2 0 2 1 3 4 36.2 28.5 42.4 39.4 36.6 -0.6\nReinit i13 14,4M 81.25% 1 0 2 1 2 3 35.9 28.5 42.3 39.5 36.6 -0.6\nReinit i14 14,3M 87.50% 0 0 1 1 1 3 35.5 28.2 41.7 38.9 36.1 -1.1\nTable 5: Evaluation of English→German student models that have the same pruned architecture as in Table 4 but\nwith reinitialised parameters and trained from scratch. Lottery ticket pruning ensures better quality due to careful\nparameter selection which is nulliﬁed when reinitialised.\nModel Pretrain Pruning Convergence Total\nBaseline - - 475k 475k\nPruned 81% 25k 15k ×13 400k 620k\nTable 6: The number of training updates in the baseline\nand the pruned English→German student model.\nmodels with the best Pareto trade-off between\nquality and sparsity. The speed comparison is pre-\nsented in Table 7.\nDespite attention heads being just a small frac-\ntion of all parameters (~5% fewer parameters with\nabout 10% size reduction), pruning them lessens\nthe burden on inference signiﬁcantly. Since all\nthree attention types were pruned in transformer-\nbig experiments, the speed-up is considerable —\nthe model is 1.5 times faster with 0.3 BLEU loss.\nIn their paper among many reported models,\nJunczys-Dowmunt et al. (2018b) achieved 8.57 ×\nspeed-up with −0.8 BLEU loss on GPU when\nscaling down from transformer-big teacher to\ntransformer-base student. In another experiment,\nthey gained 1.31 × speed-up with −0.6 BLEU\nwhen using int8 quantisation on CPU. Our method\nis complementary to those as lottery ticket pruning\ncan always remove heads on the top of existing so-\nlutions.\nContinuing that line of thought, our small stu-\ndent model translates about 10% faster when\npruned. However, it is important to remember that\ndecoder is the key reason why the transformer is\nslow and it has already been optimized with an\nSSRU. This means there is a smaller margin of\nimprovement in this type of a model. Again, at-\ntention pruning in this case is complementary and\npushes the state-of-the-art even further. Just for\ncomparison, we also include the baseline models\ntrained with half (4) and one (1) attention head\nin each layer (including context attention). The\nmodel with just one head everywhere is slightly\n(a) Turkish→English\ntransformer-big, wmt17, 1 GPU, beam 6, batch 32\nModel Sparsity Params Size BLEU Time\nBaseline 0% 156.6M 670MB 21.9 46.75s\nPruned i13 72% 148.1M 566MB 21.6 31.55s\n(b) English→German\ntransformer student, wmt19, 1 CPU, beam 1, batch 32\nModel Global Params Size BLEU Timesparsity\nBaseline 0% 15.7M 61MB 39.9 18.11s\nBaseline.half 50% 14.8M 57MB 39.0 17.98s\nBaseline.one 88% 14.1M 54MB 37.8 15.15s\nPruned i12 64% 14.5M 56MB 39.8 16.24s\nTable 7: Translation speed comparison between base-\nline and the best pruned models (converged at 13th and\n12th pruning iterations in the respective models).\nfaster than our pruned model but at the cost of\n2 BLEU points. This clearly shows again that\ncareful pruning gives much better results than just\ntraining a smaller model from the start.\nTo compare our work with the state-of-the-\nart in machine translation speed, we submit-\nted English →German student models to the\nWNGT2020 efﬁciency shared task (Bogoychev\net al., 2020). These submissions were converged\non a larger amount of data to maximize quality.\nSince our method usually selects one head to re-\nmove per layer, we experimented with more ag-\ngressive and lenient pruning by removing 3 and\n6 heads per iteration respectively. These submis-\nsions were on the Pareto frontier for speed and\nquality, meaning that no other submission was si-\nmultaneously faster and higher quality.\nThe speed-up is about 10% on CPU with 75%\nencoder heads removed (Tab. 8). In terms of on\nGPU, our best pruned model gains 15% speed-up\nw.r.t. words per second (WPS) losing 0.1 BLEU\nin comparison to an unpruned model (Tab. 9).\nThese results show that even when tested on a\n2672\nBLEU\nModel Enc. heads Params. Size WMT19 WMT1* WPS\nTiny 8 8 8 8 8 8 15.7M 61MB 41.5 32.9 2050\nTiny.Steady.i12 2 0 2 1 3 4 14.5M 56MB 41.1 32.4 2282\nTiny.Steady.i14 0 0 1 1 1 3 14.3M 55MB 40.8 32.1 2350\nTiny.Pushy.i6 2 2 2 2 2 2 14.5M 56MB 41.4 32.4 2298\nTiny.Pushy.i7 1 1 1 1 1 1 14.3M 55MB 40.2 31.5 2346\nTable 8: Quality and inference speed of our WNGT2020 models with pruned attention on CPU. Words per second\n(WPS) is evaluated in ﬂoat32 with a single CPU core on the ofﬁcial WNGT2020 input of 1M sentences.\nBLEU\nModel Enc. heads Params. Size WMT19 WMT1* WPS\nTiny 8 8 8 8 8 8 15.7M 61MB 41.5 32.9 8210\nTiny.Steady.i12 2 0 2 1 3 4 14.5M 56MB 41.4 32.4 9518\nTiny.Pushy.i6 2 2 2 2 2 2 14.5M 56MB 41.0 32.4 9508\nTable 9: Quality and inference speed of our WNGT2020 models with pruned attention on GPU. Words per second\n(WPS) measured on an AWS g4dn.xlarge instance with one NVidia T4 GPU.\nlarger scale, the pruned models achieve compara-\nble quality with faster translation.\n10 Future work\nIn this paper, we applied block-wise pruning to the\ntransformer and its attention mechanism in partic-\nular. The natural progress of this research would\nbe to prune other parts of the network — with the\nlottery ticket approach or not — to see how far\nblock pruning can go without too much impact on\nquality. Furthermore, the heuristic algorithm we\nchose that decides which heads are not to be re-\nmoved can deﬁnitely be improved on and extended\nto other types of block-sparsity cases.\n11 Conclusions\nThis paper investigated block-wise pruning of at-\ntention heads in the transformer by applying the\nlottery ticket hypothesis to the problem. We used\nan iterative approach with pruning done in early\nstages training. Our experiments on NMT have\nproved that it is possible to remove a signiﬁcant\npercentage of all heads (50–72%) in a large trans-\nformer with no signiﬁcant damage to translation\nquality. Since attention mechanism is expensive,\nespecially during inference, reducing the number\nof heads in a model led to 1.5×speed-up and\nmore if one is willing to sacriﬁce quality for speed.\nIn the teacher-student regime, the student model\nwith a reduced decoder can be pruned of 75% en-\ncoder heads with 0.1–0.2 BLEU loss and 10–15%\nfaster translation speed. This shows that lottery\nticket pruning is complementary to other methods\nthat reduce inference workload. No matter how a\nmodel is trained like, attention heads can be easily\nremoved from it.\nWe hope our paper will inspire further work\non attention-sparse architectures. In our paper,\nwe have only shown one example of a heuris-\ntic approach — there may be yet to be identiﬁed\nmore efﬁcient algorithms better suited to speciﬁc\ntasks, which will result in no need to train overly\nparametrised models.\nAcknowledgements\nThis work was supported by the Engineering\nand Physical Sciences Research Council (grant\nEP/L01503X/1), EPSRC Centre for Doctoral\nTraining in Pervasive Parallelism at the University\nof Edinburgh, School of Informatics.\nThis work has been performed using resources\nprovided by the Cambridge Tier-2 system oper-\nated by the University of Cambridge Research\nComputing Service (www.hpc.cam.ac.uk) funded\nby EPSRC Tier-2 capital grant EP/P020259/1.\nReferences\nLo¨ıc Barrault, Ond ˇrej Bojar, Marta R. Costa-juss `a,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias M ¨uller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\n2673\nFindings of the 2019 conference on machine trans-\nlation (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1) , pages 1–61, Florence, Italy.\nAssociation for Computational Linguistics.\nNikolay Bogoychev, Roman Grundkiewicz, Al-\nham Fikri Aji, Maximiliana Behnke, Kenneth\nHeaﬁeld, Sidharth Kashyap, Emmanouil-Ioannis\nFarsarakis, and Mateusz Chudyk. 2020. Edin-\nburgh’s submissions to the 2020 machine transla-\ntion efﬁciency task. In Proceedings of the Fourth\nWorkshop on Neural Generation and Translation ,\npages 218–224, Online. Association for Computa-\ntional Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt\nPost, Raphael Rubino, Lucia Specia, and Marco\nTurchi. 2017. Findings of the 2017 conference\non machine translation (WMT17). In Proceedings\nof the Second Conference on Machine Translation ,\npages 169–214, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn, and\nChristof Monz. 2018. Findings of the 2018 con-\nference on machine translation (WMT18). In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers , pages 272–303, Bel-\ngium, Brussels. Association for Computational Lin-\nguistics.\nChristopher Brix, Parnia Bahar, and Hermann Ney.\n2020. Successfully applying the stabilized lottery\nticket hypothesis to the transformer architecture. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3909–\n3915, Online. Association for Computational Lin-\nguistics.\nLinhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-\ntransformer: A no-recurrence sequence-to-sequence\nmodel for speech recognition. 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5884–5888.\nJonathan Frankle and Michael Carbin. 2019. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks. In International Conference on Learn-\ning Representations.\nJonathan Frankle, Gintare Karolina Dziugaite,\nDaniel M. Roy, and Michael Carbin. 2019. Sta-\nbilizing the lottery ticket hypothesis. CoRR,\nabs/1903.01611.\nTrevor Gale, Erich Elsen, and Sara Hooker. 2019. The\nstate of sparsity in deep neural networks. CoRR,\nabs/1902.09574.\nMaximilian Golub, Guy Lemieux, and Mieszko Lis.\n2018. Dropback: Continuous pruning during train-\ning. CoRR, abs/1806.06949.\nRoman Grundkiewicz and Marcin Junczys-Dowmunt.\n2019. Minimally-augmented grammatical error cor-\nrection. In Proceedings of the 5th Workshop on\nNoisy User-generated Text (W-NUT 2019) , pages\n357–363, Hong Kong, China. Association for Com-\nputational Linguistics.\nBarry Haddow, Nikolay Bogoychev, Denis Emelin,\nUlrich Germann, Roman Grundkiewicz, Kenneth\nHeaﬁeld, Antonio Valerio Miceli Barone, and Rico\nSennrich. 2018. The university of Edinburgh’s sub-\nmissions to the WMT18 news translation task. In\nProceedings of the Third Conference on Machine\nTranslation: Shared Task Papers , pages 399–409,\nBelgium, Brussels. Association for Computational\nLinguistics.\nMarcin Junczys-Dowmunt. 2018. Microsoft’s submis-\nsion to the WMT2018 news translation task: How I\nlearned to stop worrying and love the data. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers , pages 425–430, Bel-\ngium, Brussels. Association for Computational Lin-\nguistics.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat WMT 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225–233, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, Andr ´e F. T.\nMartins, and Alexandra Birch. 2018a. Marian: Fast\nneural machine translation in C++. In Proceedings\nof ACL 2018, System Demonstrations , pages 116–\n121, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nMarcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu\nHoang, Roman Grundkiewicz, and Anthony Aue.\n2018b. Marian: Cost-effective high-quality neural\nmachine translation in C++. In Proceedings of the\n2nd Workshop on Neural Machine Translation and\nGeneration, pages 129–135, Melbourne, Australia.\nAssociation for Computational Linguistics.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany Has-\nsan, Alham Fikri Aji, Kenneth Heaﬁeld, Roman\nGrundkiewicz, and Nikolay Bogoychev. 2019. From\nresearch to production and back: Ludicrously fast\n2674\nneural machine translation. In Proceedings of the\n3rd Workshop on Neural Generation and Transla-\ntion, pages 280–288, Hong Kong. Association for\nComputational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentence-\nPiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-\nBuc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 32 , pages\n14014–14024. Curran Associates, Inc.\nSharan Narang, Eric Undersander, and Gregory F. Di-\namos. 2017. Block-sparse recurrent neural net-\nworks. CoRR, abs/1711.02782.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nAbigail See, Minh-Thang Luong, and Christopher D.\nManning. 2016. Compression of neural machine\ntranslation models via pruning. In Proceedings\nof The 20th SIGNLL Conference on Computational\nNatural Language Learning, pages 291–301, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nSoﬁa Serrano and Noah A. Smith. 2019. Is attention\ninterpretable? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2931–2951, Florence, Italy. Associa-\ntion for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nTong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and\nTongran Liu. 2019. Sharing attention weights for\nfast transformer. In Proceedings of the Twenty-\nEighth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-19 , pages 5292–5298. Interna-\ntional Joint Conferences on Artiﬁcial Intelligence\nOrganization.\nHaonan Yu, Sergey Edunov, Yuandong Tian, and Ari S.\nMorcos. 2020. Playing the lottery with rewards and\nmultiple languages: lottery tickets in rl and nlp. In\nInternational Conference on Learning Representa-\ntions.\nMichael Zhu and Suyog Gupta. 2017. To prune, or not\nto prune: exploring the efﬁcacy of pruning for model\ncompression.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8042139410972595
    },
    {
      "name": "Computer science",
      "score": 0.7687216997146606
    },
    {
      "name": "Machine translation",
      "score": 0.6867152452468872
    },
    {
      "name": "Inference",
      "score": 0.6352314949035645
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5816247463226318
    },
    {
      "name": "Lottery",
      "score": 0.5415757894515991
    },
    {
      "name": "Machine learning",
      "score": 0.44772014021873474
    },
    {
      "name": "Speech recognition",
      "score": 0.4470491111278534
    },
    {
      "name": "Natural language processing",
      "score": 0.4388236999511719
    },
    {
      "name": "Encoder",
      "score": 0.4179791510105133
    },
    {
      "name": "Engineering",
      "score": 0.11118960380554199
    },
    {
      "name": "Voltage",
      "score": 0.10236772894859314
    },
    {
      "name": "Mathematics",
      "score": 0.08113288879394531
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}