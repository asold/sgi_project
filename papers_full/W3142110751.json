{
  "title": "Gated Transformer Networks for Multivariate Time Series Classification",
  "url": "https://openalex.org/W3142110751",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2315456200",
      "name": "Liu, Minghao",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ren, Shengqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2669104307",
      "name": "Ma Siyuan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jiao, Jiahui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2359307724",
      "name": "Chen, Yizhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2077355880",
      "name": "Wang Zhi-guang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2017380616",
      "name": "Song Wei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1850325679",
    "https://openalex.org/W2551393996",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034749137",
    "https://openalex.org/W2288074780",
    "https://openalex.org/W2585354796",
    "https://openalex.org/W2306394264",
    "https://openalex.org/W2888160216",
    "https://openalex.org/W2754051771",
    "https://openalex.org/W2598525681",
    "https://openalex.org/W2885329266",
    "https://openalex.org/W2515503816",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970631142",
    "https://openalex.org/W2892035503",
    "https://openalex.org/W2963661130",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2981830988"
  ],
  "abstract": "Deep learning model (primarily convolutional networks and LSTM) for time series classification has been studied broadly by the community with the wide applications in different domains like healthcare, finance, industrial engineering and IoT. Meanwhile, Transformer Networks recently achieved frontier performance on various natural language processing and computer vision tasks. In this work, we explored a simple extension of the current Transformer Networks with gating, named Gated Transformer Networks (GTN) for the multivariate time series classification problem. With the gating that merges two towers of Transformer which model the channel-wise and step-wise correlations respectively, we show how GTN is naturally and effectively suitable for the multivariate time series classification task. We conduct comprehensive experiments on thirteen dataset with full ablation study. Our results show that GTN is able to achieve competing results with current state-of-the-art deep learning models. We also explored the attention map for the natural interpretability of GTN on time series modeling. Our preliminary results provide a strong baseline for the Transformer Networks on multivariate time series classification task and grounds the foundation for future research.",
  "full_text": "Gated Transformer Networks for Multivariate Time Series Classiﬁcation\nMinghao Liu1 , Shengqi Ren1 , Siyuan Ma1 , Jiahui Jiao1 , Yizhou Chen2 , Zhiguang\nWang3 and Wei Song1∗\n1Zhengzhou University\n2Statistical information center of Henan health commission\n3Facebook AI\n{liuminghao, 202012172013047, 202012172013065, 202012172013044}@gs.zzu.edu.cn,\nxnhcyz@163.com, zgwang813@gmail.com, iewsong@zzu.edu.cn\nAbstract\nDeep learning model (primarily convolutional net-\nworks and LSTM) for time series classiﬁcation has\nbeen studied broadly by the community with the\nwide applications in different domains like health-\ncare, ﬁnance, industrial engineering and IoT. Mean-\nwhile, Transformer Networks recently achieved\nfrontier performance on various natural language\nprocessing and computer vision tasks. In this\nwork, we explored a simple extension of the current\nTransformer Networks with gating, named Gated\nTransformer Networks (GTN) for the multivariate\ntime series classiﬁcation problem. With the gat-\ning that merges two towers of Transformer which\nmodel the channel-wise and step-wise correlations\nrespectively, we show how GTN is naturally and\neffectively suitable for the multivariate time se-\nries classiﬁcation task. We conduct comprehen-\nsive experiments on thirteen dataset with full ab-\nlation study. Our results show that GTN is able to\nachieve competing results with current state-of-the-\nart deep learning models. We also explored the at-\ntention map for the natural interpretability of GTN\non time series modeling. Our preliminary results\nprovide a strong baseline for the Transformer Net-\nworks on multivariate time series classiﬁcation task\nand grounds the foundation for future research.\n1 Introduction\nWe are surrounded by the time series data such as physio-\nlogical data in healthcare, ﬁnancial records or various signals\ncaptured the sensors. Unlike univariate time series, multi-\nvariate time series has much richer information correlated in\ndifferent channels at each time step. The classiﬁcation task\non univariate time series has been studied comprehensively\nby the community whereas multivariate time series classiﬁca-\ntion has shown great potential in the real world applications.\nLearning representations and classifying multivariate time se-\nries are still attracting more and more attention.\n∗Contact Author:Wei Song.\nEmail address:iewsong@zzu.edu.cn\nAs some of the most traditional baseline, distance-based\nmethods work directly on raw time series with some pre-\ndeﬁned similarity measures such as Euclidean distance and\nDynamic Time Warping (DTW) [Keogh, 2002 ] to perform\nclassiﬁcation. With k-nearest neighbors as the classiﬁer,\nDTW is known to be a very efﬁcient approach as a golden\nstandard for decades. Following the scheme of well designed\nfeature, distance metric and classiﬁers, the community pro-\nposed a lot of time series classiﬁcation methods based on dif-\nferent kinds of feature space like distance, shapelet and recur-\nrence, etc. These approaches kept pushing the performance\nand advanced the research of this ﬁeld.\nWith the recent success of deep learning approaches on\ndifferent tasks on the temporal data like speech, video and\nnatural language, learning the representation from scratch to\nclassify time series has been attracted more and more stud-\nies. For example, [Zheng et al., 2016] proposed a multi-scale\nconvolutional networks for univariate time series classiﬁca-\ntion. The author combines well-designed data augmentation\nand engineering approach like down sampling, skip sampling\nand sliding windows to preprocess the data for the multiscale\nsettings, though the heavy preprocessing efforts and a large\nset of hyperparameters make it complicated and the proposed\nwindow slicing method for data augmentation is not scalable.\n[Wang et al., 2017] ﬁrstly proposed two simple but efﬁcient\nend-to-end models based on convolutions and achieved the\nstate-of-the-art performance for univariate time series classi-\nﬁcation. Thereafter, convolution based models demonstrate\nsuperior performance on time series classiﬁcation tasks [Is-\nmail Fawaz et al., 2019].\nInspired by the recent success of the Transformer networks\non NLP [Vaswani et al., 2017; Devlin et al., 2018], we pro-\nposed a transformer based approach for multivariate time se-\nries classiﬁcation. By simply scaling the traditional Trans-\nformer model by the gating that merges two towers, which\nmodel the channel-wise and step-wise correlations respec-\ntively, we show that how the proposed Gated Transformer\nNetworks (GTN) is naturally and effectively suitable for the\nmultivariate time series classiﬁcation task. Speciﬁcally, our\ncontributions are the following:\n• We explored an extension of current Transformer net-\nworks with gating, named Gated Transformer Networks\nfor the multivariate time series classiﬁcation problem.\nBy exploiting the strength where the Transformers pro-\narXiv:2103.14438v1  [cs.LG]  26 Mar 2021\ncesses data in parallel with self-attention mechanisms to\nmodel the dependencies in the sequence, we showed the\ngating that merges two towers of Transformer Networks\nthat model the channel-wise and step-wise correlations\nis very effective for time series classiﬁcation task.\n• We evaluated GTN on the thirteen multivariate time se-\nries benchmark datasets and compared with other state-\nof-the-art deep learning models with comprehensive\nablation studies. The experiments showed that GTN\nachieves competing performance.\n• We qualitatively studied the feature learned by the model\nby visualization to demonstrate the quality of the feature\nextraction of GTN.\n• We preliminary explored the interpretability of the at-\ntention map of GTN on time series modeling to study\nhow self-attention helps on channel-wise and step-wise\nfeature extraction.\n2 Related Work\nThe earlier work like[Yanget al., 2015] preliminary explored\nthe deep convolutional networks on multivariate time series\nfor human activity recognition. [Wang et al., 2017] studied\nFully Convolutional Networks (FCN) and Residual Networks\n(ResNet) and achieved the state-of-the-art performance for\nunivariate time series classiﬁcation. This work also explored\nthe interpretability of FCN and ResNet with Class Activa-\ntion Map (CAM) to highlight the signiﬁcant time steps on\nthe raw time series for a speciﬁc class. [Serr`a et al., 2018]\nproposed the Encoder model, which is a hybrid deep con-\nvolutional networks whose architecture is inspired by FCN\nwith a main difference where the GAP layer is replaced with\nan attention layer to fuse the feature maps. The attention\nweight from the last layer is used to learn which parts of the\ntime series (in the time domain) are important for a certain\nclassiﬁcation. [Karim et al., 2017] proposed the two towers\nwith Long short-term memory (LSTM) and FCN. The authors\nmerge the feature by simple concatenation at the last layer to\nimprove the classiﬁcation performance on univariate time se-\nries, though LSTM bears the high computation complexity.\nOther earlier works like [Cui et al., 2016; Zheng et al., 2016;\nLe Guennec et al., 2016; Zhao et al., 2017] explored differ-\nent convolutional networks architecture other than FCN and\nResNet and claimed superior results on univariate or multi-\nvariate time series classiﬁcations, and served as the strong\nbaselines in our work. Note that [Tanisaro and Heidemann,\n2016] proposed Time Warping Invariant Echo State Network\nas one of the non-convolutional recurrent architectures for\ntime series forecasting and classiﬁcation, where we also in-\nclude it in the study as a baseline.\nThere are plenty of established works on Transformer Net-\nworks for NLP and computer vision. The time series com-\nmunity is also exploring Transformers on forecasting and re-\ngression task, like [Li et al., 2019; Cai et al., 2020]. The stud-\nies on Transformer for time series classiﬁcation is still in the\nearly stage, like [Oh et al., 2018] explores the Transformer on\nclinical time series classiﬁcation. The most recent work we\ndiscovered is from [Rußwurm and K ¨orner, 2020], where the\nFigure 1: Model architecture of the Gated Transformer Networks.\nauthor studied the Transformer for raw optical satellite time\nseries classiﬁcation and obtained the latest results comparing\nwith convolution based solutions. Our work gaps the bridge\nas the ﬁrst comprehensive study for Transformer networks on\nmultivariate time series classiﬁcation.\n3 Gated Transformer Networks\nTraditional Transformer has encoder and decoder stacking on\nthe word and positional embedding for sequence generation\nand forecasting task. As for multivariate time series classi-\nﬁcation, we have three extension simply to adapt the Trans-\nformer for our need - embedding, two towers and gating. The\noverall architecture of Gated Transformer Networks is shown\nin Figure 1.\n3.1 Embedding\nIn the original Transformers, the tokens are projected to a em-\nbedding layer. As time series data is continuous, we simply\nchange the embedding layer to fully connected layer. Instead\nof linear projection, we add a non-linear activation tanh.\nFollowing [Vaswani et al., 2017], the positional encoding is\nadded with the non-linearly transformed time series data to\nencode the temporal information, as self-attention is hard to\nutilize the sequential correlation of time step.\n3.2 Two-tower Transformer\nMultivariate time series has multiple channels where each\nchannel is a univariate time series. The common assump-\nTable 1: Test accuracy of GTN and other benchmark models on 13 multivariate time series dataset\nMLP FCN ResNet Encoder MCNN t-LeNet MCDCNN Time-CNN TWIESN GTN\nAUSLAN 93.3 97.5 97.4 93.8 1.1 1.1 85.4 72.6 72.4 97.5\nArabicDigits 96.9 99.4 99.6 98.1 10.0 10.0 95.9 95.8 85.3 98.8\nCMUsubject1 60.0 100.0 99.7 98.3 53.1 51.0 51.4 97.6 89.3 100.0\nCharacterTrajectories 96.9 99.0 99.0 97.1 5.4 6.7 93.8 96.0 92.0 97.0\nECG 74.8 87.2 86.7 87.2 67.0 67.0 50.0 84.1 73.7 91.0\nJapaneseV owels 97.6 99.3 99.2 97.6 9.2 23.8 94.4 95.6 96.5 98.7\nKickvsPunch 61.0 54.0 51.0 61.0 54.0 50.0 56.0 62.0 67.0 90.0\nLibras 78.0 96.4 95.4 78.3 6.7 6.7 65.1 63.7 79.4 88.9\nNetFlow 55.0 89.1 62.7 77.7 77.9 72.3 63.0 89.0 94.5 100.0\nUWave 90.1 93.4 92.6 90.8 12.5 12.5 84.5 85.9 75.4 91.0\nWafer 89.4 98.2 98.9 98.6 89.4 89.4 65.8 94.8 94.9 99.1\nWalkvsRun 70.0 100.0 100.0 100.0 75.0 60.0 45.0 100.0 94.4 100.0\nPEMS - - - - - - - - - 93.6\ntion is that there exists hidden correlation between different\nchannels at current or warping time step. Capturing both the\nstep-wise (temporal) and channel-wise (spatial) information\nis the key for multivariate time series research. One com-\nmon approach is to exploit convolutions. That is, the recep-\ntion ﬁeld integrates both step-wise and channel-wise by the\n2D kernels or the 1D kernels with ﬁxed parameter sharing.\nDifferent from other works that leverage the original Trans-\nformer for time series classiﬁcation and forecasting, we de-\nsigned a simple extension of the two-tower framework, where\nthe encoders in each tower explicitly capture the step-wise\nand channel-wise correlation by attention and masking, as\nshown in Figure 1.\nStep-wise Encoder. To encode the temporal feature, we\nuse the self-attention with mask to attend on each point cross\nall the channels by calculating the pair-wise attention weights\namong all the time steps. In the multi-head self-attention lay-\ners, the scaled dot-product attention formulates the attention\nmatrix on all time step. Like the original Transformer ar-\nchitecture, position-wise fully connected feed-forward layers\nis stacked upon each multi-head attention layers for the en-\nhanced feature extraction. The residual connection around\neach of the two sub-layers is also kept to direct information\nand gradient ﬂow, following by the layer normalization.\nChannel-wise Encoder. Likewise, the Channel-wise en-\ncoder calculates the attention weights among different chan-\nnels across all the time step. Note that position of channel in\nthe multivariate time series has no relative or absolute corre-\nlation, as if we switch the order of channels, the time series\nshould has no changes. Therefore, we only add positional en-\ncoding in the Step-wise Encoder. Attention layers with the\nmasking on all the channels is expected to explicitly capture\nthe correlation among channels across all time step. Note\nthat it is pretty straight-forward to implement both encoders\nby simply transpose the channel and time axis when feeding\nthe time series for each encoders.\n3.3 Gating\nTo merge the feature of the two towers which encodes step-\nwise and channel-wise correlations, a simple way is to con-\ncatenate all the features from two towers, which compromises\nthe performance of both as shown in our ablation study.\nInstead, we proposed a simple gating mechanism to learn\nthe weight of each tower. After getting the output of each\ntower, which had a fully connect layer after each output of\nboth encoders with non-linear activation as C and S, we\npacked them into vector by concatenation followed by a lin-\near projection layer to get h. After the softmax function, the\ngating weight are computed as g1 and g2. Then each gating\nweight is attending on the corresponding tower’s output and\npacked as the ﬁnal feature vector.\nh = W·Concat(C, S) +b\ng1, g2 = Softmax (h)\ny = Concat(C ·g1, S·g2)\n(1)\n4 Experiments\n4.1 Experiment Settings\nWe test GTN on the same subset of the Baydogan archive\n[Baydogan, 2019], which contains 13 multivariate time series\ndatasets. All the datasets have been split into training and\ntesting by default, and there is no preprocessing for these time\nseries. We choose the following deep learning models as the\nbenchmarks.\n• Fully Convolutional Networks (FCN) and Residual Net-\nworks (ResNet) [Wang et al., 2017]. These are reported\nto be among the best deep learning models in the multi-\nvariate time series classiﬁcation task [Ismail Fawaz et\nal., 2019 ]. Multi-layer Perception ( MLP) is also in-\ncluded as a simple baseline in our comparison.\n• Universal Neural Network Encoder (Encoder) [Serr`a et\nal., 2018].\n• Multi-scale Convolutional Neural Network ( MCNN)\n[Cui et al., 2016].\n• Multi Channel Deep Convolutional Neural Network\n(MCDCNN) [Zheng et al., 2016].\nTable 2: Ablation study of the two-towers, gating and masking in GTN\nstep-wise step-wise+mask channel-wise channel-wise+mask step-wise+channel-wise GTN\n+concatenation\nAUSLAN 97.0 95.3 94.5 94.5 96.7 97.5\nArabicDigits 98.6 98.5 98.3 98.3 98.9 98.8\nCMUsubject16 96.0 96.6 100.0 100.0 96.3 100.0\nCharacterTrajectories 96.0 97.1 96.9 96.5 97.5 97.0\nECG 87.0 84.0 92.0 89.0 86.0 91.0\nJapaneseV owels 96.0 97.3 98.3 97.6 98.1 98.7\nKickvsPunch 81.3 80.0 81.3 90.0 81.3 90.0\nLibras 82.0 81.1 88.3 88.3 90.5 88.9\nNetFlow 88.0 100.0 100.0 100.0 100.0 100.0\nUWave 89.0 88.8 90.0 88.3 89.5 91.0\nWafer 97.0 98.1 96.9 97.9 97.9 99.1\nWalkvsRun 96.4 100.0 96.5 100.0 100.0 100.0\nPEMS 94.0 92.5 87.9 91.4 90.8 93.6\n• Time Convolutional Neural Network ( Time-CNN)\n[Zhao et al., 2017].\n• Time Le-Net (t-LeNet) [Le Guennec et al., 2016].\n• Time Warping Invariant Echo State Network (TWIESN)\n[Tanisaro and Heidemann, 2016].\nThe Gated Transformer Network is trained with Adagrad\nwith learning rate 0.0001 and dropout = 0.2. The categori-\ncal cross-entropy is used as the loss function. Learning rate\nschedule on plateau [Wang et al., 2017; Ismail Fawaz et al.,\n2019] is applied to train the GTN. We test on the training set\nand the test set every certain number of iterations, the best\ntest results and its super parameters would be recorded. For\nfair comparison, we choose to report the test accuracy on the\nthe model with the best training loss as [Ismail Fawaz et al.,\n2019].1\n4.2 Results and Analysis\nThe results are shown in Table 1. GTN achieved comparable\nresults with the FCN and ResNet. Note that the results has\nno statistical signiﬁcant difference among these three models,\nthough on NetFlow and KickvsPunch datasets, GTN shows\nsuperior performance. The drawback of GTN is compara-\nbly leaning to overﬁtting. Unlike FCN and ResNet where\nno dropout is used, the GTN has dropout binding with layer\nnorm to reduce the risk of overﬁtting.\nAblation Study\nTo clearly state the performance gain from each module in\nthe GTN, we performed a comprehensive study as shown in\nTable 2.\n• Following the traditional Transformer, masking helps to\nensure that the predictions for positioni can depend only\non the known previous outputs and also helps the atten-\ntion to not attend to the padding position. This beneﬁts\n1The codes are available at\nhttps://github.com/ZZUFaceBookDL/GTN.\nholds not only on the language but also on the time se-\nries data, as the tower-only transformer with mask are\noverall a bit better than the one without masks.\n• Channel-wise only transformer outperforms step-wise\nonly transformer on the majority of the dataset. This\npreliminary result supports our assumption that for mul-\ntivariate time series, the correlation between different\nchannels across all time step is an important differen-\ntiator with the univariate time series. The attention with\nmask is able to catch the channel-wise feature better.\n• Different time series data might show different leaning\non channel-wise and step-wise information. For exam-\nple, on the dataset PEMS, the step-wise Transformer\nmodel works better. On the dataset CMUsubject16, the\nchannel-wise Transformer model outperforms.\n• Following the above point, exploiting both towers is a\nstraight-forward solution. However, simple concatena-\ntion of the two towers sometimes works, but the perfor-\nmance always fall on the middle ground or even worse.\nLike on the dataset PEMS and CMUsubject16, step-\nwise model and channel-wise model works best for each\ncases. After concatenation of both towers’ feature, the\nresults becomes even worse.\n• By adding the gating weights before simple (equally-\nweighted) concatenation, the model is able to learn when\nto rely on a speciﬁc tower more by a pure data-driven\nway, thus show the best performance in this study.\n4.3 Visualization and Analysis of the Attention\nMap\nThe attention matrix represents the correlation between the\nchannels and time steps respectively. We choose one sample\nfrom the JapaneseVowels dataset to visualize both attention\nmaps. for the channel-wise attention map, we calculated the\ndynamic time warping (DTW) distance across the time series\non different channel. For each time step, we also simply cal-\nculated the Euclidean distance across different channels, as\nFigure 2: 1) channel-wise attention map (upper-left) 2)\nchannel-wise DTW (upper-right) 3) step-wise attention map\n(bottom-left) 4) step-wise L2 distance (bottom-right)\non the same time step there is no time axis, thus DTW is not\nneeded. The visualization is shown in Figure 2\nOur ﬁrst analysis focuses on the channel-wise attention\nmap. In Fig 2, we label four blocks ( b1-b4). In Fig 3, we\ndraw the raw time series of the corresponding channels. We\nuse channel11 (abbreviated as c11, the same below) to rep-\nresent channel 11 in the following analysis.\nb1 and b2 have relatively high attention score, that is, both\nc6 and c7 are strongly co-ﬁred with c11. Look at Fig 3, we\ncan clearly see that c6 and c7 have relatively similar shapelet\nand trend with c11. Similarly, b4 have very high attention\nscore, where c3 and c4 are also showing very similar trend.\nMeanwhile, b3 is among the one with very small attention\nscore. Look at the c11 and c3, two time series show pretty\nmuch different even inverse trend. This preliminary ﬁnding\nis interesting because like the attention in NLP task where\nthe attention score indicates the semantic correlation between\ndifferent tokens, the channel-wise attention learned from the\ntime series similarly shows the similar sequences that are co-\nﬁred together to force the learning lean to the ﬁnal labels.\nNote that the smaller DTW does not mean two sequence\nare similar. Like shown on the channel-wise DTW, block c\nindicates very small DTW distance between c3 and c11. Ac-\ntually c11 and c3 are very different in trends and shapelet.\nOur preliminary analysis shows that the channel-wise atten-\ntion also tends to grab the similar sequences where DTW\nshows no clear differentiated factors.\nAs the step is ﬁxed across channel for the step-wise atten-\ntion, Euclidean distance is the special case of DTW at the\nsame step, thus we choose Euclidean distance as the metric.\nThrough the analysis of step-wise attention map and the Eu-\nclidean distance matrix, the distance between the time series\nand the similarity of the shapelet have an impact on who GTN\nFigure 3: Drawing of the raw time series in different channel pairs.\ncalculates the attention scores, though the impact are not that\nobvious which deserves a deep dive in the future work.\n4.4 Analysis on the Gating Weight\nGTN has two towers to encode the step-wise and channel-\nwise information with self-attention respectively. With the\ngating method, the model learns to assign different weights\nto attend to these two Transformers. In the ablation study,\nwe show that gating achieved better results compared with\nconcatenation.\nBy observing the gate weights assigned to the two tower\non the AULSAN data set, we found that for different sam-\nple, the gating weights assigned to step-wise and channel-\nwise tower are tends to be different, like {0.9899, 0.0101}\nand {0.0443, 0.5557}for two sample time series respectively.\nHowever, the gating weights overall show the trends to be\nskewed towards the step-wise tower, with the average gating\nweights of {0.7786, 0.2214}. As shown in Table 2, the step-\nwise Transformer outperforms the channel-wise Transformer,\nthus overall the gating learns to assign more weights to the\nstep-wise tower, the gating behavior and the results in the ab-\nlation study are consistent. The gating shows the capability\nto learn the weights from the data-driven manner to attend on\neach towers for different samples and dataset to improve the\nclassiﬁcation performance.\n4.5 Analysis of the Embedding Output\nAs each time step are transformed to a dense vector through\nthe embedding layer, we use the t-SNE [Maaten and Hinton,\n2008] to reduce the dimension of the output from the embed-\nding layer on AUSLAN. The result is shown in graph 4. The\ngraph shows that all the points are clustered together on the\nspeciﬁc manifold. We roughly labeled those clusters with ﬁve\ndifferent colors.\nAs shown in Figure 5, we mapped each color back to the\nraw time series. Interestingly, the time step from each clus-\nter are consistent and overall each cluster shows different\nshapelet.\nFigure 4: Visualization of the t-SNE result of the embedding layer\noutput on the AUSLAN dataset.\nFigure 5: Shapelet discovered by the visualization of the\nembedding on each time step.\n• The green shapelet indicates a deep M shape.\n• The light blue shapelet is a sharp trending-up sub-\nsequence.\n• The dark blue shapelet is a plateau followed by a deep\ndown like 7.\n• The magenta shapelet is like a recovering from the deep\ndown with a bit trending up.\n• The red shapelet is a plateau.\nBy a closer look at the visualization of the embedding on\neach time step, the near points that forms some interest-\ning shapelet has relatively smaller distance in the vector\nspace. Like [Liu and Wang, 2016 ], where the author\ndiscover interesting shapelet by a well designed Markov\ntransition matrix with the clustering on the transformed\ncomplex network graph, GTN shows the potential by\nlearning the interesting pattern of some speciﬁc sub-\nsequences from scratch, which will be an interesting fu-\nture work.\nFigure 6: Visualization of the feature extracted after the gating in\nGTN.\n4.6 Visualization of the Extracted Feature\nlastly, following [Ismail Fawaz et al., 2019], we visualize the\nfeature vector after gating for a simple sanity check of the fea-\nture quality. We choose theJapaneseVowelsdataset and apply\nt-SNE to reduce the dimension for visualization, as shown in\nFigure 6. Each number in the graph is the label with corre-\nsponding color encoding. and labels in the ﬁgure correspond\nto the labels of the original data. This ﬁgure shows that GTN\nis able to project the data into an easily separable space for\nbetter classiﬁcation results.\n5 Conclusion\nWe presented the Gated Transformer Network (GTN) as a\nsimple extension of multidimensional time series using gat-\ning. With the gating that merges two towers of transformer\nnetworks which model the channel-wise and step-wise cor-\nrelations respectively GTN is able to explicitly learn both\nchannel-wise and step-wise correlations. We conducted com-\nprehensive experiments on thirteen dataset and the prelimi-\nnary results show that GTN is able to achieve competing per-\nformance with current state-of-the-art deep learning models.\nThe ablation study shows how different modules work to to-\ngether to achieve the improved performance. We also qualita-\ntively analyzed the attention map and other components with\nvisualization to better understand the interpretability of out\nmodel. Our preliminary results ground a solid foundation for\nthe study of Transformer Network on the time series classiﬁ-\ncation task in future research.\nReferences\n[Baydogan, 2019] Mustafa Gokce Baydogan. Multivariate\ntime series classiﬁcation datasets, 2019.\n[Cai et al., 2020] Ling Cai, Krzysztof Janowicz, Gengchen\nMai, Bo Yan, and Rui Zhu. Trafﬁc transformer: Captur-\ning the continuity and periodicity of time series for trafﬁc\nforecasting. Transactions in GIS, 24(3):736–755, 2020.\n[Cui et al., 2016] Zhicheng Cui, Wenlin Chen, and Yixin\nChen. Multi-scale convolutional neural networks for time\nseries classiﬁcation. arXiv preprint arXiv:1603.06995,\n2016.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Ismail Fawaz et al., 2019] Hassan Ismail Fawaz, Germain\nForestier, Jonathan Weber, Lhassane Idoumghar, and\nPierre-Alain Muller. Deep learning for time series classiﬁ-\ncation: a review. Data Mining and Knowledge Discovery,\n33(4):917–963, 2019.\n[Karim et al., 2017] Fazle Karim, Somshubra Majumdar,\nHoushang Darabi, and Shun Chen. LSTM fully convo-\nlutional networks for time series classiﬁcation. CoRR,\nabs/1709.05206, 2017.\n[Keogh, 2002] Eamonn Keogh. Exact indexing of dynamic\ntime warping - sciencedirect. VLDB ’02: Proceed-\nings of the 28th International Conference on Very Large\nDatabases, pages 406–417, 2002.\n[Le Guennec et al., 2016] Arthur Le Guennec, Simon Mali-\nnowski, and Romain Tavenard. Data augmentation for\ntime series classiﬁcation using convolutional neural net-\nworks. 2016.\n[Li et al., 2019] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou\nZhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. En-\nhancing the locality and breaking the memory bottleneck\nof transformer on time series forecasting. In Advances\nin Neural Information Processing Systems, pages 5243–\n5253, 2019.\n[Liu and Wang, 2016] Lu Liu and Zhiguang Wang. Encod-\ning temporal markov dynamics in graph for visualizing\nand mining time series. arXiv preprint arXiv:1610.07273,\n2016.\n[Maaten and Hinton, 2008] Laurens van der Maaten and Ge-\noffrey Hinton. Visualizing data using t-sne. Journal of\nmachine learning research, 9(Nov):2579–2605, 2008.\n[Oh et al., 2018] Jeeheh Oh, Jiaxuan Wang, and Jenna\nWiens. Learning to exploit invariances in clinical time-\nseries data using sequence transformer networks. arXiv\npreprint arXiv:1808.06725, 2018.\n[Rußwurm and K¨orner, 2020] Marc Rußwurm and Marco\nK¨orner. Self-attention for raw optical satellite time series\nclassiﬁcation. ISPRS Journal of Photogrammetry and Re-\nmote Sensing, 169:421 – 435, 2020.\n[Serr`a et al., 2018] Joan Serr `a, Santiago Pascual, and\nAlexandros Karatzoglou. Towards a universal neural\nnetwork encoder for time series. In CCIA, pages 120–129,\n2018.\n[Tanisaro and Heidemann, 2016] Pattreeya Tanisaro and\nGunther Heidemann. Time series classiﬁcation using\ntime warping invariant echo state networks. In 2016 15th\nIEEE International Conference on Machine Learning and\nApplications (ICMLA), pages 831–836. IEEE, 2016.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, pages 5998–6008, 2017.\n[Wang et al., 2017] Z. Wang, W. Yan, and T. Oates. Time se-\nries classiﬁcation from scratch with deep neural networks:\nA strong baseline. In 2017 International Joint Conference\non Neural Networks (IJCNN), pages 1578–1585, 2017.\n[Yang et al., 2015] Jianbo Yang, Minh Nhut Nguyen,\nPhyo Phyo San, Xiaoli Li, and Shonali Krishnaswamy.\nDeep convolutional neural networks on multichannel time\nseries for human activity recognition. In Ijcai, volume 15,\npages 3995–4001. Buenos Aires, Argentina, 2015.\n[Zhao et al., 2017] Bendong Zhao, Huanzhang Lu,\nShangfeng Chen, Junliang Liu, and Dongya Wu.\nConvolutional neural networks for time series classiﬁca-\ntion. Journal of Systems Engineering and Electronics,\n28(1):162–169, 2017.\n[Zheng et al., 2016] Yi Zheng, Qi Liu, Enhong Chen, Yong\nGe, and J Leon Zhao. Exploiting multi-channels deep\nconvolutional neural networks for multivariate time series\nclassiﬁcation. Frontiers of Computer Science, 10(1):96–\n112, 2016.",
  "concepts": [
    {
      "name": "Multivariate statistics",
      "score": 0.6963333487510681
    },
    {
      "name": "Computer science",
      "score": 0.5126677751541138
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.46873435378074646
    },
    {
      "name": "Time series",
      "score": 0.41873592138290405
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3904293179512024
    },
    {
      "name": "Data mining",
      "score": 0.3431794047355652
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3384857475757599
    },
    {
      "name": "Machine learning",
      "score": 0.2609250545501709
    },
    {
      "name": "Geology",
      "score": 0.09052872657775879
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "topic": "Multivariate statistics",
  "institutions": []
}