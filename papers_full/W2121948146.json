{
  "title": "A study on effects of implicit and explicit language model information for DBLSTM-CTC based handwriting recognition",
  "url": "https://openalex.org/W2121948146",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2088571978",
      "name": "Qi Liu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096582097",
      "name": "Lijuan Wang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2009815392",
      "name": "Qiang Huo",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2088571978",
      "name": "Qi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096582097",
      "name": "Lijuan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2009815392",
      "name": "Qiang Huo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2152928267",
    "https://openalex.org/W6631362777",
    "https://openalex.org/W2152550252",
    "https://openalex.org/W2106225808",
    "https://openalex.org/W2964325005",
    "https://openalex.org/W6651330330",
    "https://openalex.org/W7066459846",
    "https://openalex.org/W1990437404",
    "https://openalex.org/W2017718251",
    "https://openalex.org/W2005708641",
    "https://openalex.org/W2018970719",
    "https://openalex.org/W1542311097",
    "https://openalex.org/W2145035656",
    "https://openalex.org/W2171312815",
    "https://openalex.org/W6637157234",
    "https://openalex.org/W2170942820",
    "https://openalex.org/W2122585011",
    "https://openalex.org/W2060580591",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2136848157",
    "https://openalex.org/W6645763848",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2124966090",
    "https://openalex.org/W2063476404",
    "https://openalex.org/W2061994628",
    "https://openalex.org/W2055625241",
    "https://openalex.org/W1822371218",
    "https://openalex.org/W2021043164",
    "https://openalex.org/W2004220942",
    "https://openalex.org/W1982309299",
    "https://openalex.org/W304834817",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2147568880",
    "https://openalex.org/W1674799117",
    "https://openalex.org/W1524333225"
  ],
  "abstract": "Deep Bidirectional Long Short-Term Memory (D-BLSTM) with a Connectionist Temporal Classification (CTC) output layer has been established as one of the state-of-the-art solutions for handwriting recognition. It is well known that the DBLSTM trained by using a CTC objective function will learn both local character image dependency for character modeling and long-range contextual dependency for implicit language modeling. In this paper, we study the effects of implicit and explicit language model information for DBLSTM-CTC based handwriting recognition by comparing the performance of using or without using an explicit language model in decoding. It is observed that even using one million lines of training sentences to train the DBLSTM, using an explicit language model is still helpful. To deal with such a large-scale training problem, a GPU-based training tool has been developed for CTC training of DBLSTM by using a mini-batch based epochwise Back Propagation Through Time (BPTT) algorithm.",
  "full_text": "A Study on Effects of Implicit and Explicit\nLanguage Model Information for DBLSTM-CTC\nBased Handwriting Recognition\nQi Liu∗†, Lijuan Wang†, Qiang Huo †\n∗ACM Honored Class, Zhiyuan College, Shanghai Jiao Tong University, Shanghai 200240, P. R. China\n†Microsoft Research Asia, Beijing 100080, P. R. China\nEmails: liuq901@gmail.com; {lijuanw, qianghuo}@microsoft.com\nAbstract—Deep Bidirectional Long Short-Term Memory\n(DBLSTM) with a Connectionist Temporal Classiﬁcation (CTC)\noutput layer has been established as one of the state-of-the-art\nsolutions for handwriting recognition. It is well-known that the\nDBLSTM trained by using a CTC objective function will learn\nboth local character image dependency for character modeling\nand long-range contextual dependency for implicit language\nmodeling. In this paper, we study the effects of implicit and\nexplicit language model information for DBLSTM-CTC based\nhandwriting recognition by comparing the performance of using\nor without using an explicit language model in decoding. It is\nobserved that even using one million lines of training sentences\nto train the DBLSTM, using an explicit language model is still\nhelpful. To deal with such a large-scale training problem, a\nGPU-based training tool has been developed for CTC training\nof DBLSTM by using a mini-batch based epochwise Back\nPropagation Through Time (BPTT) algorithm.\nI. I NTRODUCTION\nLong Short-Term Memory (LSTM) [1], [2], [3] is a\nspecial type of Recurrent Neural Networks (RNN) (e.g.,\n[4]), which has been used to build handwriting recognition\n(HWR) systems for a long time by using a single-hidden-\nlayer Bidirectional LSTM (BLSTM) [5] or a deep Multi-\nDimensional LSTM (MDLSTM) [6], both with a so-called\nConnectionist Temporal Classiﬁcation (CTC) output layer and\ntrained with a CTC-based objective function [7]. Recently,\nBLSTM-CTC approach was also applied successfully to opti-\ncal character recognition (OCR) for printed text (e.g., [8]),\nwhile MDLSTM-CTC approach was used to build several\nstate-of-the-art ofﬂine HWR systems (e.g., [9], [10], [11]).\nFurthermore, BLSTM was used as a feature extractor to build\nGaussian mixture hidden Markov model (GMM-HMM) based\nofﬂine HWR systems, which achieved state-of-the-art perfor-\nmance on several benchmark tasks (e.g., [12], [13], [14]).\nInspired by the success of using Deep BLSTM (DBLSTM) and\nHMM (DBLSTM-HMM) for automatic speech recognition\n(ASR) [15], more recently, DBLSTM-HMM approach has\nbeen used to build HWR systems with promising results (e.g.,\n[16], [17], [18], [19]). Finally, DBLSTM-CTC approach has\nbeen tried with state-of-the-art performance on Rimes and\n*This work was done when Qi Liu was an intern in Speech Group of\nMicrosoft Research Asia, Beijing, P. R. China.\nIAM benchmark tasks [20], which is also the approach studied\nin this paper.\nIt is well-known that the (D)BLSTM and MDLSTM trained\nby using a CTC objective function will learn both local\ncharacter image dependency for character modeling and long-\nrange contextual dependency for implicit language modeling.\nIn [8], it is demonstrated that very high OCR accuracy can\nbe achieved for printed text by a BLSTM-CTC recognizer\ntrained with about 95k text line images but without using\nany explicit language model (LM) in decoding. However, for\nmore difﬁcult handwriting recognition tasks, an explicit LM\nis typically used to achieve higher character/word recognition\naccuracies in DBLSTM-CTC or MDLSTM-CTC based HWR\nsystems (e.g., [9], [11], [20]). So far, the scale of training set\nfor HWR experiments reported in literature is relatively small,\nranging from several to tens of thousands of text lines. It is not\nclear yet what would happen when a much larger training set\ncould be used. In this paper, we study the effects of implicit\nand explicit language model information for DBLSTM-CTC\nbased handwriting recognition. First, we train several sets of\nDBLSTM using different amount (up to one million text lines)\nof training data. Then, we conduct recognition experiments\nfor each DBLSTM-CTC based recognizer by using and not\nusing an explicit LM in decoding. The LMs include character\nn-gram and word trigram. By comparing and analyzing the\nexperimental results, we hope to gain some insights. To deal\nwith such a large-scale training problem, we have developed\na GPU-based training tool for CTC training of DBLSTM\nby using a mini-batch based epochwise Back Propagation\nThrough Time (BPTT) algorithm (e.g., [4]).\nThe rest of this paper is organized as follows: Section\nII introduces how we build a DBLSTM-CTC based HWR\nsystem. Section III presents experimental results. Finally, we\nconclude the paper in Section IV .\nII. O VERVIEW OF DBLSTM-CTC BASED HWR S YSTEM\nA. Preprocessing and Feature Extraction\nGiven each horizontal text line image, preprocessing steps\nof baseline and slant correction will be applied ﬁrst. An\napproach similar to that in [21] is used, which is based on\nRun Length Smoothing Algorithm (RLSA) ([22], [23]) and\nprojection proﬁle based techniques (e.g., [24], [25]). Images\narXiv:2008.01532v1  [cs.CL]  31 Jul 2020\nare rotated by angles within a certain range and then smoothed\nby RLSA. Each rotation is evaluated by different objective\nfunctions to ﬁnd an optimal angle for baseline and slant\ncorrection. After baseline and slant correction, each horizontal\ntext line image is normalized to have a height of 60 pixels.\nFor feature extraction, each sentence is ﬁrst split into frames\nby a sliding window of 30 pixels wide with a frame shift\nof 3 pixels. Then each frame is smoothed by applying a\nhorizontal cosine window to derive a 1,800-dimensional raw\nfeature vector. The dimension of raw feature vectors is reduced\nto 50 by principal component analysis (PCA). Finally, these\n50-dimensional feature vectors are normalized such that each\ndimension of feature has a zero sample mean and unit sample\nvariance on training set.\nB. DBLSTM-based Character Modeling and CTC Training\nGiven the set of training feature vector sequences with\ntranscriptions, a DBLSTM with a CTC output layer can be\ntrained by using a CTC-based objective function as described\nin [5], [7]. The CTC output layer is a softmax layer with\n79 classes, including 52 case-sensitive English letters, 10\ndigits, 15 punctuation marks, a “space”, and a special “blank”\nsymbol. “Blank” is not a real character class, but a virtual\nsymbol used to separate the consecutive real symbols. For a\ngiven feature vector sequence, the output of the DBLSTM at\neach time-step gives the corresponding posterior probability\ndistribution of 79 classes. The memory block of DBLSTMs\nhas the same topology as that in [5], [15].\nFor a small-scale training set, it is feasible to use the\nRNNLIB open source toolkit [26] for CTC training of\nDBLSTM, which is a single-thread CPU-implementation of\nan epochwise BPTT algorithm. However, for the large-scale\ntraining set of one million text lines we are dealing with,\nit would take several months to train a DBLSTM by using\nthe RNNLIB tool, therefore be infeasible to conduct any\nmeaningful experiments. To support training with big data, we\nhave developed a GPU-based training tool for CTC training of\nDBLSTM by using a mini-batch based epochwise BPTT algo-\nrithm, which does not need frame-level ground-truth labels. It\nis noted that the open-source tool of CURRENNT [27] has also\nimplemented a mini-batch based epochwise BPTT algorithm,\nbut only supports frame-level training which requires the target\nlabel for each frame. In our case, we adopt a CPU-GPU\ncooperative implementation. We let CTC-related code run on\nCPU and other code run on GPU. It is because the CTC\ncode is hard to parallelize and incurs much memory overhead,\nyet the CTC part is not the bottleneck of the whole pipeline.\nWe also transfer the whole array between the CTC code and\nthe other code to reduce CPU-GPU memory communication.\nDuring the development process, we have also implemented a\nsingle-thread CPU version of CTC training tool for DBLSTM,\nwhich is much more efﬁcient than RNNLIB, yet can achieve\nsimilar recognition accuracy. Our single-GPU implementation\ncan achieve about 30 times speedup in comparison with our\nCPU implementation.\nC. Language Model\nWe have used SRILM toolkit [28] and a text corpus\nfrom Linguistic Data Consortium (LDC) (catalog number\nLDC2008T15) [29] to build several language models, i.e.,\ncharacter n-gram ( n = 3,4,5,8,10) and word trigram, with\nGood-Turing discounting. 500 (out of 4495) documents of the\nLDC corpus are used. For character n-gram, the vocabulary\nconsists of 78 real character classes (without special “blank”\nsymbol). For word trigram, the vocabulary consists of 200k\nwords with top occurring frequencies in the training corpus,\nwhich leads to an out-of-vocabulary (OOV) rate of 8% on\nIAM-online test set [30], [31].\nD. Decoding\nWe have tried the following two decoding methods:\n1) CTC-based Decoding without using LM: This method\nis called best path decoding in [7] and works as follows.\nGiven the feature vector sequence of an unknown text line,\nas mentioned above, the CTC output at each time-step gives\na probability distribution over a set of symbols. At each time-\nstep, choose the symbol with the highest probability. Then,\nconcatenate the most active outputs at every time-step to obtain\na raw output sequence S. Finally, apply a merge function,\nβ(S), to generate the ﬁnal decoding result. The merge function\nβ(S) maps the symbol sequence S to another label sequence\nby ﬁrst combining the same consecutive symbols together\nand then removing all the “blank” symbols. For example,\nβ(a−−aab) =β(−−a−ab) =β(a−abbb) =β(aa−aab) =\naab, where −means “blank”.\nBest path decoding is easy to implement. It only lever-\nages the implicit language model information embedded in\nDBLSTM. Because no word-lexicon is used, there is no OOV\nissue. From our HWR experimental results to be reported later,\nthis decoding method can achieve a reasonable character error\nrate (CER), but a relatively high word error rate (WER). There\nare much more words containing few character errors in each\nword than the words containing many character errors in each\nword.\n2) WFST-based Decoding with LM: This is the method\nused in, e.g., [9], [10], [11], [20]. The decoder searches a\ngraph based on Weighted Finite-State Transducers (WFST)\nconstructed from several main system components. Kaldi\ntoolkit [32] is used for both WFST construction and decoding.\nFor a given feature vector sequence X of an unknown text\nline, at each time-step, the CTC-trained DBLSTM provides\na probability distribution P(s|X) over the symbol set. If we\nmodel each symbol by a single-sate HMM with a self-loop\nand outgoing transitions, its “state-dependent” likelihood score\ncan be approximated by P(s|X)\nP(s)α , where P(s) is the prior\ndistribution of symbol s estimated from the transcriptions\nof the training text line images with a special treatment of\n“blank” symbol, and αis a tunable scaling parameter (α= 0.2\nin our experiments). These HMMs are transformed into a\nWFST H. If a word lexicon is used, we can decompose\neach word by inserting an optional “blank” symbol between\ntwo different characters and a compulsory “blank” symbol\nTABLE I\nSTATISTICS OF SEVERAL TRAINING SETS DERIVED FROM A MICROSOFT HANDWRITING CORPUS .\n# of Text Line Images 10K 20K 50K 80K 100K 200K 500K 800K 1M\n# of Words 69,144 138,344 346,139 543,567 673,421 1,192,101 2,754,690 4,231,015 5,268,676\n# of Characters 352,156 704,985 1,761,416 2,759,539 3,462,030 6,153,498 16,894,999 27,452,759 32,717,204\n# of Unique Text Lines 5544 10677 12611 12930 15686 18654 41685 43997 44053\nTABLE II\nTRAINING TIME OF DBLSTM S USING GPU- BASED TOOL ON DIFFERENT SETS OF MICROSOFT TRAINING DATA .\nDataset 10K 20K 50K 80K 100K 200K 500K 800K 1M\nAverage Time per Epoch (sec.) 299 526 1,251 1,933 2,430 4,273 10,983 17,559 21,010\nNumber of Epoches 33 25 36 29 29 35 45 42 42\nTotal Time (day) 0.11 0.15 0.52 0.64 0.81 1.73 5.72 8.53 10.21\nTABLE III\nEFFECTS OF LEARNING RATE (LR) SCHEDULING ON IAM- OFFLINE\nDATASET. CTC- BASED DECODING WITHOUT LM IS USED .\n# of Times for LR Reduction 0 3 6 9\nCharacter Error Rate (%) 20.8 16.5 15.7 15.7\nbetween two repeated characters. Then a lexicon FST L can\nbe constructed in the structure of a WFST. A character n-gram\nor a word trigram LM can also be transformed into a WFST\nG. Depending on which LM to use, the ﬁnal search graph can\nbe constructed by composing a WFST of HLG or HG.\nOnce the search graph is built, the decoder will take the\nsequence of “state-dependent” likelihood scores as input and\ngenerate a word sequence as recognition result.\nIII. E XPERIMENTS\nA. Experimental Setup\nSeveral handwriting corpora are used in our experiments.\nThe ﬁrst one is IAM ofﬂine handwritten English sentence\ndataset, which contains 6,159 training and 1,861 testing text\nline images, respectively [33], [34]. We use this small dataset\nprimarily for developing and debugging our CPU- and GPU-\nbased training tools so that RNNLIB tool can be used for\ncomparison purpose.\nThe second corpus is a large-scale Microsoft in-house ink\n(i.e., online handwritings) database, which contains more than\none million online handwritten English text lines. We render\neach line of online handwriting into a text line image. More\nspeciﬁcally, we model each stroke by a B´ezier curve. For every\nfour consecutive points, we connect them by a cubic B ´ezier\ncurve. The thickness of B ´ezier curve controls the thickness (3\npixels here) of the stroke. From such a corpus of rendered\nhandwritten text line images, 9 sets of training data with\ndifferent sizes are formed and some statistics are summarized\nin Table I.\nTo test HWR systems built from Microsoft datasets, we\nuse the test set of IAM online handwritten English sentence\ndataset, which contains 3,859 text lines with 20,272 words and\n89,153 characters in total [30], [31]. We use the same method\nto render testing text line images as in rendering training data\nfrom Microsoft ink data.\nLearning rate (LR) scheduling is important for neural net-\nwork training. Lower learning rate often gives better result\nbut higher learning rate needs less epoches to converge.\nTherefore, we use the following simple scheme for learning\nrate scheduling: First, train the network with a high learning\nrate such as 10−3 or 10−4 for several epoches. Then, cut the\nlearning rate in half, retrain the network. At this time, only one\nor two epoches is enough. Finally, repeat this cut-down and\nretrain procedure until the learning rate reaches a low value\nsuch as 10−5 or 10−6. For gradient-based BPTT training, the\nmomentum is set as 0.9 and the initial learning rate is 10−4.\nFor large-scale experiments on Microsoft datasets, we use\na ﬁxed conﬁguration. The DBLSTM has 5 hidden BLSTM\nlayers, each has 240 memory cells (120 for forward and 120\nfor backward states). All experiments are run on a server\nwith Intel Xeon CPUs, 128GB memory, and an NVIDIA\nTesla K20Xm GPU. The Operating System of the server is\nMicrosoft Windows Server 2012. We use both CER and WER\nas performance metrics.\nB. Experimental Results\nTo compare the efﬁciency of our CPU- and GPU-based\ntraining tools, we measure the wall-clock time for running a\nsingle epoch of training DBLSTM on the IAM-ofﬂine training\nset. It takes 6,033 and 200 seconds for CPU- and GPU-based\ntools respectively. GPU-based tool achieves about 30 times\nspeedup. Both tools can train DBLSTMs leading to similar\nrecognition accuracy, therefore we use our GPU-based training\ntool for the remaining experiments. Table II shows training\ntime of DBLSTMs using our GPU-based tool on different sets\nof Microsoft training data. The total time scales almost linearly\nwith the amount of training data. For the largest dataset with\none million lines, it takes about 10 days to complete training\nusing a single GPU card.\nTable III shows the CER of the CTC-based decoding\nwithout using LM on the IAM-ofﬂine testing set with different\ntimes of LR reduction. Table IV shows the CER/WER (in\n%) of the CTC-based decoding without using LM on the\nIAM-online testing set achieved by DBLSTMs trained from\ndifferent sets of Microsoft data with different LR scheduling.\nThe experimental results indicate that the LR scheduling is\nTABLE IV\nEFFECTS OF LEARNING RATE (LR) SCHEDULING ON MICROSOFT DATASETS . CTC- BASED DECODING WITHOUT LM IS USED (CER/WER IN %).\nDataset 10K 20K 50K 80K 100K 200K 500K 800K 1M\nWithout LR Reduction 42.8/84.8 30.1/72.8 22.5/61.9 19.2/56.4 18.7/55.1 17.5/53.4 16.2/50.3 16.1/50.9 16.2/54.2\nWith 3 Times LR Reduction 42.8/85.7 28.0/69.9 21.0/59.3 17.9/54.2 16.7/51.0 15.0/47.5 14.1/45.1 13.5/44.0 13.2/44.6\nWith 6 Times LR Reduction 44.0/89.6 27.5/68.9 20.0/56.8 17.0/51.1 15.8/48.2 14.0/44.1 12.9/42.1 12.3/40.4 11.8/40.1\nTABLE V\nPERFORMANCE (CER/WER IN %) COMPARISON OF CTC- BASED DECODING WITHOUT USING LM AND WFST- BASED DECODING WITH DIFFERENT\nTYPES OF LMS.\nDataset 10K 20K 50K 80K 100K 200K 500K 800K 1M\nCTC Decoding w/o LM 44.0/89.6 27.5/68.9 20.0/56.8 17.0/51.1 15.8/48.2 14.0/44.1 12.9/42.1 12.3/40.4 11.8/40.1\nCharacter 3-gram 42.7/89.8 26.6/81.3 17.9/75.2 15.2/72.9 14.3/72.1 12.5/69.9 11.3/68.2 10.9/67.2 10.1/67.1\nCharacter 4-gram 41.0/80.5 24.3/65.1 15.8/55.8 13.4/52.1 12.6/50.7 11.1/48.4 9.9/45.7 9.7/44.5 8.8/44.5\nCharacter 5-gram 40.0/76.1 22.3/56.9 14.1/47.3 11.7/43.0 11.1/41.7 9.5/39.5 8.5/36.6 8.5/34.9 7.7/35.2\nCharacter 8-gram 39.8/75.3 21.7/55.1 13.1/44.7 10.9/41.1 10.3/39.4 8.9/37.9 7.8/34.7 8.0/32.5 7.0/33.5\nCharacter 10-gram 40.3/75.6 22.5/55.5 13.3/44.7 11.2/41.2 10.6/39.4 9.1/37.9 8.1/34.5 8.1/32.0 7.3/33.6\nWord Trigram 34.8/59.0 17.0/34.6 11.1/27.5 9.2/24.1 8.6/22.9 7.7/21.2 6.8/19.6 6.2/18.1 6.0/18.0\nFig. 1. Effects of Implicit LM vs. Character n-gram.\nhelpful for both cases, and is more important for larger scale\ntraining tasks.\nTable V summarizes the performance (CER/WER in %)\ncomparison of CTC-based decoding without using LM and\nWFST-based decoding with different types of LMs. Figure 1\ncompares the WERs of CTC-based decoding without using\nLM (i.e., implicit LM information is used) and WFST-based\ndecoding with different character n-gram LMs. Several obser-\nvations can be made. First, for CTC-based decoding without\nusing LM, both CER and WER improves with the increasing\namount of training data. The improvement comes from both\nthe improved modeling for local character image modeling and\nthe improved implicit language modeling. From Figure 1, it is\nclear that CTC-based decoding without using LM performs\nas well as WFST-based decoding with a character 3-gram\nLM initially, then performs similarly with the WFST-based\ndecoding with a character 4-gram LM when more training\ndata is used, and then outperforms that of using character 4-\ngram LM when even more training data is used, but cannot\nsurpass the WFST-based decoding with a character 5-gram\nLM even when about 1M lines of training data is used.\nSecond, for CTC-based decoding without using LM, a CER\nof 11.8% can be achieved for 1M training case, but the\ncorresponding WER is only 40.1%. The CER and WER can be\nreduced to 7.0% and 33.5% respectively by using WFST-based\ndecoding with a character 8-gram LM. This shows clearly the\neffectiveness of using a powerful explicit LM. However, using\ncharacter 10-gram does not bring additional improvement.\nThird, the CER and WER can be further reduced to 6.0%\nand 18.0% respectively by using WFST-based decoding with\na word trigram. The relatively small improvement of CER\n(14% relative CER reduction) and much bigger improvement\nof WER (46% relative WER reduction) shows clearly the\npower of lexical constraints.\nIV. C ONCLUSION AND DISCUSSIONS\nFrom the above results, we conclude that even using one\nmillion lines of training sentences to train the DBLSTM, using\nan explicit word trigram language model is still very helpful.\nActually, if we use a better word trigram LM shared by RWTH\nteam [35] in WFST-based decoding, the CER and WER can\nbe further reduced to 5.3% and 15.7% respectively.\nIt is not surprised at all that the result of CTC-based\ndecoding without using LM outperforms the WFST-based\ndecoding using a weak character n-gram, simply because\nCTC-trained DBLSTM has learned an implicit LM from the\ntraining data already. How powerful such an implicit LM\nwould really depends on the amount and nature of the training\ndata. A careful analysis of our training data reveals that there\nare only about 44k unique text lines out of about 1M lines\nof training text lines, therefore the learned implicit LM is\nnot general and strong enough. That explains partially why a\nstrong character n-gram LM or an even stronger word trigram\nLM could help a lot.\nOur ongoing and future works include 1) improving pre-\nprocessing techniques for baseline and slant correction, 2)\nimproving language model, 3) developing new discriminative\ntraining method for sequence training of DBLSTM, 4) using\neven more representative training data to improve models.\nACKNOWLEDGEMENT\nThe authors would like to thank Professor Hermann Ney and\nhis team at RWTH Aachen University to share their language\nmodel for our experiments.\nREFERENCES\n[1] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural Computation, vol. 9, no. 8, pp.1735-1780, 1997.\n[2] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget:\nContinual prediction with LSTM,” Neural Computation, vol. 12,\nno. 10, pp.2451-2471, 2000.\n[3] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber, “Learning\nprecise timing with LSTM recurrent networks,” The Journal of\nMachine Learning Research, vol. 3, pp.115-143, 2003.\n[4] R. J. Williams and D. Zipser, “Gradient-based learning algo-\nrithms for recurrent networks and their computational complex-\nity,” in Y . Chauvin and D. E. Rumelhart (Eds.),Back-propagation:\nTheory, Architectures and Applications, pp.433-486, 1995.\n[5] A. Graves, M. Liwicki, S. Fern ´andez, R. Bertolami, H. Bunke and\nJ. Schmidhuber, “A novel connectionist system for unconstrained\nhandwriting recognition,” IEEE Transactions on PAMI, vol. 31,\nno. 5, pp.855-868, 2009.\n[6] A. Graves and J. Schmidhuber, “Ofﬂine handwriting recognition\nwith multidimensional recurrent neural networks,” in Proc. NIPS-\n2009, pp.545-552.\n[7] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber,\n“Connectionist temporal classiﬁcation: labelling unsegmented se-\nquence data with recurrent neural networks,” inProc. ICML-2006,\npp.369-376.\n[8] T. M. Breuel, A. Ul-Hasan, M. A. Al-Azawi and F. Shafait,\n“High-performance OCR for printed English and Fraktur using\nLSTM networks,” in Proc. ICDAR-2013, pp.683-687.\n[9] T. Bluche, J. Louradour, M. Knibbe, B. Moysset, M. F. Ben-\nzeghiba, and C. Kermorvant, “The A2iA Arabic handwritten text\nrecognition system at the OpenHaRT2013 evaluation,” in Proc.\nDAS-2014, pp.161-165.\n[10] V . Pham, T. Bluche, C. Kermorvant, and J. Louradour, “Dropout\nimproves recurrent neural networks for handwriting recognition,”\nin Proc. ICFHR-2014, pp.285-290.\n[11] B. Moysset, T. Bluche, K. Maxime, M. F. Benzeghiba, R.\nMessina, J. Louradour and C. Kermorvant, “The A2iA multi-\nlingual text recognition system at the second Maurdor evaluation,”\nin Proc. ICFHR-2014, pp.297-302.\n[12] M. Kozielski, P. Doetsch, H. Ney, “Improvements in RWTH’s\nsystem for off-line handwriting recognition,” in Proc. ICDAR-\n2013, pp.935-939.\n[13] M. Kozielski, P. Doetsch, M. Hamdani, and H. Ney, “Multilin-\ngual off-line handwriting recognition in real-world images,” in\nProc. DAS-2014, pp.121-125.\n[14] M. Hamdani, P. Doetsch, M. Kozielski, A. E.-D. Mousa, and\nH. Ney, “The RWTH large vocabulary Arabic handwriting recog-\nnition system,” in Proc. DAS-2014, 2014, pp.111-115.\n[15] A. Graves, N. Jaitly, and A.-r. Mohamed, “Hybrid speech\nrecognition with deep bidirectional LSTM,” in Proc. ASRU-2013,\npp.273-278.\n[16] P. Doetsch, M. Kozielski, and H. Ney, “Fast and robust training\nof recurrent neural networks for ofﬂine handwriting recognition,”\nin Proc. ICFHR-2014, pp.279-284.\n[17] P. V oigtlaender, P. Doetsch, S. Wiesler, R. Schluter, and H. Ney,\n“Sequence-discriminative training of recurrent neural networks,”\nin Proc. ICASSP-2015, pp.2100-2104.\n[18] X. Zhang, M. Wang, L.-J. Wang, Q. Huo, and H. Li, “Building\nhandwriting recognizers by leveraging skeletons of both ofﬂine\nand online samples,” in Proc. ICDAR-2015.\n[19] K. Chen, Z.-J. Yan, and Q. Huo, “A context-sensitive-chunk\nBPTT approach to training deep LSTM/BLSTM recurrent neural\nnetworks for ofﬂine handwriting recognition,” in Proc. ICDAR-\n2015.\n[20] T. Bluche, H. Ney and C. Kermorvant, “A comparison of\nsequence-trained deep neural networks and recurrent neural net-\nworks optical modeling for handwriting recognition,” in Proc.\nSLSP-2014, pp.199-210.\n[21] A.-H. Toselli, “Preprocessing and feature extraction for off-line\ncontinuous HTR”, http://users.dsic.upv.es/∼atoselli/RES/\nmaterialesDocentes/alejandroViewgraphs/Off-\nLine Preproc HTR.pdf.\n[22] F. M. Wahl, K. Y . Wong, and R. G. Casey, “Block segmentation\nand text extraction in mixed text/image documents,” Computer\nGraphics and Image Processing, vol. 20, no. 4, pp.375-390, 1982.\n[23] F. M. Wahl, “A new distance mapping and its use for shape\nmeasurement on binary patterns,” Computer Vision, Graphics,\nand Image Processing, vol. 23, no. 2, pp.218-226, 1983.\n[24] V . Alessandro, J. Luettin, “A new normalization technique for\ncursive handwritten words,” Pattern Recognition Letters, vol. 22,\nno. 9, pp.1043-1050, 2001.\n[25] M. Pastor, A. Toselli, and E. Vidal, “Projection proﬁle based\nalgorithm for slant removal,” in A. Campilho, M. Kamel (Eds.),\nImage Analysis and Recognition, LNCS 3212, Springer, pp.183-\n190, 2004.\n[26] A. Graves, RNNLIB, http://sourceforge.net/projects/rnnl/\n[27] F. Weninger, J. Bergmann, and B. Schuller, “Introducing\nCURRENNT–the Munich open-source CUDA recurrent neural\nnetwork toolkit,” Journal of Machine Learning Research, vol. 15,\n2014. Code available at http://sourceforge.net/projects/currennt/.\n[28] A. Stolcke, “SRILM–an extensible language modeling toolkit,”\nin Proc. ICSLP-2002, pp.901-904.\n[29] D. Graff, “North American news text, complete LDC2008t15,”\n2008. Available at https://catalog.ldc.upenn.edu/LDC2008T15.\n[30] M. Liwicki and H. Bunke, “IAM-OnDB—an on-line English\nsentence database acquired from handwritten text on a white-\nboard,” in Proc. ICDAR-2005, pp.956-961.\n[31] Handwritten text recognition task IAM-OnDB-t1, see\nhttp://www.iam.unibe.ch/fki/databases/iam-on-line-handwriting-\ndatabase/download-the-iam-on-line-handwriting-database.\n[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glem-\nbek, N. Goel, M. Hannemann, P. Motlicek, Y . Qian, P.\nSchwarz, J. Silovsky, G. Stemmer, K. Vesely, “The Kaldi\nSpeech Recognition Toolkit,” Proc. ASRU-2011. Code available\nat http://kaldi.sourceforge.net/.\n[33] U.-V . Marti and H. Bunke, “The IAM-database: an English sen-\ntence database for ofﬂine handwriting recognition,” International\nJournal on Document Analysis and Recognition, vol. 5, no. 1,\npp.39-46, 2002.\n[34] Large writer independent text line recognition task, see\nhttp://www.iam.unibe.ch/fki/databases/iam-handwriting-database.\n[35] M. Kozielski, D. Rybach, S. Hahn, R. Schluter, and H. Ney,\n“Open vocabulary handwriting recognition using combined word-\nlevel and character-level language models,” in Proc. ICASSP-\n2013, pp.8257-8261.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.829225480556488
    },
    {
      "name": "Handwriting",
      "score": 0.7059982419013977
    },
    {
      "name": "Language model",
      "score": 0.7031463384628296
    },
    {
      "name": "Connectionism",
      "score": 0.6961729526519775
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5721049308776855
    },
    {
      "name": "Speech recognition",
      "score": 0.5575084090232849
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5069184899330139
    },
    {
      "name": "Character (mathematics)",
      "score": 0.5043948888778687
    },
    {
      "name": "Handwriting recognition",
      "score": 0.48434436321258545
    },
    {
      "name": "Natural language processing",
      "score": 0.46555832028388977
    },
    {
      "name": "Decoding methods",
      "score": 0.4535232484340668
    },
    {
      "name": "Artificial neural network",
      "score": 0.25013333559036255
    },
    {
      "name": "Feature extraction",
      "score": 0.13159066438674927
    },
    {
      "name": "Algorithm",
      "score": 0.12228357791900635
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ]
}