{
    "title": "The Role of Large Language Models in Medical Education: Applications and Implications",
    "url": "https://openalex.org/W4385266429",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2791482975",
            "name": "Conrad W. Safranek",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A4295710497",
            "name": "Anne Elizabeth Sidamon‐Eristoff",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A3134629398",
            "name": "Aidan Gilson",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2313261013",
            "name": "David Chartash",
            "affiliations": [
                "University College Dublin",
                "National University of Ireland",
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2791482975",
            "name": "Conrad W. Safranek",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A4295710497",
            "name": "Anne Elizabeth Sidamon‐Eristoff",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A3134629398",
            "name": "Aidan Gilson",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2313261013",
            "name": "David Chartash",
            "affiliations": [
                "Yale University",
                "University College Dublin",
                "National University of Ireland"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3163832451",
        "https://openalex.org/W1663984431",
        "https://openalex.org/W4322761615",
        "https://openalex.org/W4324130227",
        "https://openalex.org/W4322718832",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4327946446",
        "https://openalex.org/W1913069920",
        "https://openalex.org/W2160391052",
        "https://openalex.org/W4385307867",
        "https://openalex.org/W3106976676",
        "https://openalex.org/W3165773193",
        "https://openalex.org/W2169818249",
        "https://openalex.org/W4322622443",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W1997311393",
        "https://openalex.org/W2080818078",
        "https://openalex.org/W1964143014",
        "https://openalex.org/W1986682419",
        "https://openalex.org/W103326475",
        "https://openalex.org/W4226239253",
        "https://openalex.org/W4295430126",
        "https://openalex.org/W2005835734",
        "https://openalex.org/W2981296841",
        "https://openalex.org/W3086554530",
        "https://openalex.org/W3175333387",
        "https://openalex.org/W4310895557"
    ],
    "abstract": "Large language models (LLMs) such as ChatGPT have sparked extensive discourse within the medical education community, spurring both excitement and apprehension. Written from the perspective of medical students, this editorial offers insights gleaned through immersive interactions with ChatGPT, contextualized by ongoing research into the imminent role of LLMs in health care. Three distinct positive use cases for ChatGPT were identified: facilitating differential diagnosis brainstorming, providing interactive practice cases, and aiding in multiple-choice question review. These use cases can effectively help students learn foundational medical knowledge during the preclinical curriculum while reinforcing the learning of core Entrustable Professional Activities. Simultaneously, we highlight key limitations of LLMs in medical education, including their insufficient ability to teach the integration of contextual and external information, comprehend sensory and nonverbal cues, cultivate rapport and interpersonal interaction, and align with overarching medical education and patient care goals. Through interacting with LLMs to augment learning during medical school, students can gain an understanding of their strengths and weaknesses. This understanding will be pivotal as we navigate a health care landscape increasingly intertwined with LLMs and artificial intelligence.",
    "full_text": "Editorial\nThe Role of Large Language Models in Medical Education:\nApplications and Implications\nConrad W Safranek1, BSc; Anne Elizabeth Sidamon-Eristoff2, BA; Aidan Gilson1, BSc; David Chartash1,3, PhD\n1Section for Biomedical Informatics and Data Science, Yale University School of Medicine, New Haven, CT, United States\n2Yale University School of Medicine, New Haven, CT, United States\n3School of Medicine, University College Dublin, National University of Ireland, Dublin, Ireland\nCorresponding Author:\nDavid Chartash, PhD\nSection for Biomedical Informatics and Data Science\nYale University School of Medicine\n9th Fl\n100 College St\nNew Haven, CT, 06510\nUnited States\nPhone: 1 317 440 0354\nEmail: david.chartash@yale.edu\nAbstract\nLarge language models (LLMs) such as ChatGPT have sparked extensive discourse within the medical education community,\nspurring both excitement and apprehension. Written from the perspective of medical students, this editorial offers insights gleaned\nthrough immersive interactions with ChatGPT, contextualized by ongoing research into the imminent role of LLMs in health\ncare. Three distinct positive use cases for ChatGPT were identified: facilitating differential diagnosis brainstorming, providing\ninteractive practice cases, and aiding in multiple-choice question review. These use cases can effectively help students learn\nfoundational medical knowledge during the preclinical curriculum while reinforcing the learning of core Entrustable Professional\nActivities. Simultaneously, we highlight key limitations of LLMs in medical education, including their insufficient ability to\nteach the integration of contextual and external information, comprehend sensory and nonverbal cues, cultivate rapport and\ninterpersonal interaction, and align with overarching medical education and patient care goals. Through interacting with LLMs\nto augment learning during medical school, students can gain an understanding of their strengths and weaknesses. This understanding\nwill be pivotal as we navigate a health care landscape increasingly intertwined with LLMs and artificial intelligence.\n(JMIR Med Educ 2023;9:e50945) doi: 10.2196/50945\nKEYWORDS\nlarge language models; ChatGPT; medical education; LLM; artificial intelligence in health care; AI; autoethnography\nBackground on Large Language Models\nArtificial intelligence has consistently proven itself to be a\ntransformative force across various sectors, with the medical\nfield being no exception. A recent advancement in this sphere\nis large language models (LLMs) such as OpenAI’s ChatGPT\nand its more recent model, GPT-4 [1]. Fundamentally, LLMs\nleverage deep neural networks—complex structures with\nmultiple layers of statistical correlation, or “hidden layers”—that\nfacilitate nuanced, complex relations and advanced information\nabstraction [2]. The breakthrough of ChatGPT represents the\nconvergence of two significant advancements in computer\nscience: scaled advancement of the processing power of LLMs\nand the implementation of real-time reinforcement learning\nwith human feedback [3-5]. As a result, computers can now\nhandle vast volumes of training data and generate models with\nbillions of parameters that exhibit advanced humanlike language\nperformance.\nSignificant constraints accompany the use of LLMs. These\ninclude their sporadic propensity to concoct fictitious\ninformation, a phenomenon aptly named “hallucinating,” as\nwell as their unpredictable sensitivity to the structure of user\ninput “prompting” [6-8]. Additionally, both ChatGPT and GPT-4\nwere not trained on data sourced past 2021 and largely do not\nhave access to information behind paywalls [9,10]. As the\ntraining was proprietary, it is challenging to model a priori bias\nand error within the model [11,12]. Deducing these\nvulnerabilities and understanding how they influence model\noutput is important for the accurate use of LLMs.\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 1https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nSince ChatGPT’s release in November 2022, LLMs’ potential\nrole in medical education and clinical practice has sparked\nsignificant discussion. Educators have considered ChatGPT’s\ncapacity for studying assistance, medical training, and clinical\ndecision-making [6,7,13]. More specifically, ChatGPT has been\nsuggested for generating simulated patient cases and didactic\nassessments to supplement traditional medical education [6].\nUsing an autoethnographic framework [14], we aim to address\nthese potential use cases from the perspective of medical\nstudents in the preclinical phase (authors CWS and AESE) and\nclinical phase (authors AG and DC) of basic medical education.\nSince its release, we have integrated ChatGPT into our daily\nacademic workflow while simultaneously engaging with\nresearch regarding LLMs’ impact on medical education and\nhealth care. Throughout this process, we have continuously had\nreflective conversations with peers, mentors, and faculty\nregarding the metacognitive use of LLMs in medical education.\nIn this editorial, we first discuss the performance of LLMs on\nmedical knowledge and reasoning tasks representative of basic\nmedical education [15,16]. We then delve into specific use cases\nof ChatGPT in medical education that have emerged through a\nreflective, iterative, and evaluative investigation. Building upon\nthis basis and reflecting on the current state of LLM capabilities\nand use in basic medical education, we additionally examine\nthe potential for such technology to influence future physicians\nin training and practice.\nUnderstanding the Scope of LLMs’\nPerformance on Medical Knowledge\nTasks\nThe capacity of LLMs to model the semantics of medical\ninformation encoded in the clinical sublanguage has shown\npotential for medical question-answering tasks [17-19]. A\nvanguard of this technology is ChatGPT, which has\ndemonstrated promise beyond specific medical\nquestion-answering tasks, responding to questions in domains\nsuch as knowledge retrieval, clinical decision support, and\npatient triage [20]. As ChatGPT’s training data is proprietary,\nit is difficult to examine the medical knowledge to which the\nmodel was exposed.\nRecent research using multiple-choice questions sourced from\nthe United States Medical Licensing Exam (USMLE) as a proxy\nfor medical knowledge found that ChatGPT could approximate\nthe performance of a third-year medical student [21,22]. Beyond\nquestion-answering, ChatGPT consistently provided narratively\ncoherent answers with logical flow, integrating internal and\nexternal information from the question [21]. GPT-4, the\nsuccessor of ChatGPT, has demonstrated performance\nsuperiority with an accuracy >80% across all three steps of the\nexamination [23]. The demonstrated capacity of ChatGPT to\nconstruct coherent and typically accurate responses on medical\nknowledge and reasoning tasks has opened new avenues for\nexploration within medical education. Recognition of this\nopportunity served as the impetus for this study, aiming to\ncritically interrogate the potential role of LLMs as an interactive\ninstrument in medical education.\nUse Cases for ChatGPT in Medical\nEducation\nThe following use cases are those that demonstrated particular\nvalue while experimenting with the integration of ChatGPT into\nthe daily routine of medical school studies.\nDifferential Diagnoses: Use Case 1\nChatGPT can be used to generate a list of differential diagnoses\ngiven the presentation of signs and symptoms by students\n(Figure 1). During learning, students often focus on a single\ndomain of medicine, whereas ChatGPT is not constrained and\nmay include diseases not yet learned or not part of the student’s\nfocused material in a current or recent curricular unit. ChatGPT\ncan therefore facilitate students’ development of a holistic,\nintegrated understanding of differential diagnosis and\npathophysiology, key learning objectives of preclinical\neducation. From experience, ChatGPT often provides clinical\nlogic to link signs and symptoms with each differential\ndiagnosis, reinforcing student learning objectives.\nGiven ChatGPT’s dialogic interface, students can also ask\nfollow-up questions. We have found that ChatGPT is strong at\nexplaining and contextualizing the underlying biology and\npathophysiology, and helps facilitate a more in-depth\nunderstanding of both pathophysiology and clinical logic\nexpected during clinical presentation. Follow-up questions can\nsimulate the narrowing or broadening of a differential diagnosis\nas new information is added in the form of further history,\nphysical exam, and laboratory or imaging investigations. Such\nuse of a dialogic interface supports students in developing a\nsimulated proficiency of the core Entrustable Professional\nActivities (EPAs) expected prior to the transition to residency\n[24,25]. For instance, students can refine their understanding\nof how to “prioritize a differential diagnosis” (EPA 2), “gather\na history and perform a physical examination” (EPA 1), and\n“recommend...common diagnostic and screening tests” (EPA\n3). The ubiquitously available ChatGPT can augment the\npreclinical learning of clinical skills even when patients and\nprofessors are unavailable, fundamentally advancing students’\nself-directed learning.\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 2https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nFigure 1. Example of using ChatGPT to help brainstorm differential diagnoses (left). Follow-up questions could include, for example, which physical\nexam maneuvers (right), laboratory studies, or diagnostic tests could be used to narrow the selection of each differential diagnosis.\nInteractive Practice Cases: Use Case 2\nSimulating clinical cases fosters the application of\npathophysiological frameworks learned in lectures and supports\nclinical skills such as history-taking and physical examination\ninterpretation. With the implementation of explicit prompt\nengineering [26], students can enter into a dialogic, interactive\ncase with ChatGPT playing the role of a simulated patient or\nmedical professor (Figure 2).\nUnlike in static clinical cases from textbooks, ChatGPT’s\ninteractive nature allows students to clarify or expand\ninformation presented dynamically. This form of constructivist,\nactive learning emphasizes the importance of interaction and\nhands-on engagement for deeper, more durable knowledge\nacquisition [27]. Additionally, manipulating the case by adding\nor subtracting information supports a mode of inquiry similar\nto the script concordance test, a tool used for teaching and\nevaluating medical reasoning in ambiguous clinical scenarios\n[28].\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 3https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nFigure 2. Example of using ChatGPT to generate an interactive medical practice case.\nMultiple-Choice Review: Use Case 3\nTo enhance assessment review, ChatGPT can assist students\nby offering supplementary explanations when reviewing\nmultiple-choice questions (Figure 3). Providing multiple-choice\nquestions to ChatGPT when the student is unaware of the correct\nanswer poses some risk, as ChatGPT may “hallucinate” an\nincorrect answer. However, by having the student verify the\nmodel’s responses against the official answer key, protecting\nagainst hallucinations, the student can deepen their\ncomprehension of the question and the defensible rationale.\nFollow-up questions can prompt ChatGPT to clarify concepts\nor terminology or to explain why alternative answers are\nincorrect.\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 4https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nFigure 3. Example of applying ChatGPT to past practice exams. In this case, the student is using a multiple-choice question from a previous midterm\nthat they answered incorrectly. The answer key provided for the exam was insufficient at explaining the physiologic reasoning behind the correct answer.\nDefinitive Answer to Ambiguous Question: Negative\nUse Case\nIf misused, LLMs can present challenges to the learning process.\nFor example, when ChatGPT is presented with a scenario\ndesigned to clarify ambiguity (eg, a patient presentation that\ncould be interpreted as either atypical bacterial or viral\npneumonia), the user’s prompt for the single statistically most\nlikely diagnosis challenges ChatGPT’s clinical reasoning and\nknowledge of relative risk (Figure 4).\nIn its response, ChatGPT misinterprets and overemphasizes the\npotential for bird exposure during a recent zoo visit. ChatGPT’s\nresponse fails to unpack the clinical context in which the bird\nexposure detail came to light. The uncertain information\nobtained from the patient may not signal a significant bird\nencounter but likely reflects the inability to definitively rule out\nsuch an exposure. ChatGPT’s response misses this nuance and\ngives undue weight to the ambiguous exposure (representative\nof the cognitive bias of anchoring) [29,30]. Overall, this case\nis an example of a classic teaching point: “An atypical\npresentation of a common disease is often more likely than a\ntypical presentation of a rare disease.” ChatGPT’s error also\nexemplifies how standardized testing material available on the\nweb—what we assume ChatGPT is trained upon—is likely to\noveremphasize less common diseases to evaluate the breadth\nof medical knowledge. Thus, anchoring may be a result of the\ndifference in the training set’s prevalence of psittacosis, where\nthere are many cases of parrot exposure leading to infection in\nquestions as opposed to the real-world incidence of the disease.\nThis case is included as a negative use case not because\nChatGPT provides incorrect information but rather because the\nstudent is misusing ChatGPT. Responsible student users of\nLLMs should understand the propensity of the LLM to\noverweight information likely to be tested more frequently than\ntheir prevalence in the population. Asking ChatGPT for a\nsingular definitive answer, therefore, makes the student\nvulnerable to incorrect answers resulting from biases encoded\nwithin the model.\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 5https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nFigure 4. Demonstration of a negative use case. This example dialogue illustrates a scenario where a user requests the single most probable diagnosis\nin an ambiguous clinical scenario, and ChatGPT responds with an assertive and convincing, yet likely incorrect, response.\nUse Cases: Beyond\nChatGPT can be used in myriad other ways to augment medical\neducation (Figure 5). The breadth of options is only beginning\nto be realized, and as medical students begin to creatively\nintegrate LLMs into their study routines, the list will continue\nto grow.\nDuring this integration process, it is important to minimize the\nrisk of hallucinations by being deliberate with the type of\nquestions posed. Across our experimentation, ChatGPT was\ngenerally strong at brainstorming-related questions and\ngenerative information seeking (eg, Differential Diagnoses: Use\nCase 1 section). In contrast, forcing ChatGPT to pick a single\n“best” choice between ambiguous options can potentially lead\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 6https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nto convincing misinformation (eg, Definitive Answer to\nAmbiguous Question: Negative Use Case section).\nThe following analogy emerged as a helpful framework for\nconceptualizing the relationship between ChatGPT and\nmisinformation: ChatGPT is to a doctor as a calculator is to a\nmathematician. Whether a calculator only produces the correct\nanswer to a mathematical problem is contingent upon whether\nthe inputs it is fed are complete and correct; performing correct\ncomputation does not necessarily imply correctly solving a\nproblem. Similarly, ChatGPT may produce a plausible string\nof text that is misinformation if incorrect or incomplete\ninformation were provided to it either in training or by the user\ninteracting with it. Therefore, responsible use of these tools\ndoes not forgo reasoning and should not attribute an output as\na definitive source of truth.\nThe responsible use of LLMs in medical education is not set in\nstone. A more comprehensive list of LLM best practices for\nmedical education will be refined as students and professors\ncontinue to implement and reflect upon these tools. The\nfollowing key considerations emerged from our work. First, it\nis crucial to validate ChatGPT’s outputs with reputable\nresources, as it aids learning and can prompt critical thinking\nbut does not replace established authorities. Second, much like\nthe advice given to clinical preceptors [31], the framing of\ninquiries should favor open-ended generative questions over\nbinary or definitive ones to foster productive discussion and\navoid misleading responses. Third, understanding the scope and\nlimitations of LLMs’ training data sets is a key step in guarding\nagainst possible biases embedded within these models. Finally,\nincorporating structured training on artificial intelligence into\nthe medical curriculum can empower students to further discern\noptimal use cases and understand potential pitfalls [32].\nAttention to these practices while implementing and reflecting\nwill support the responsible and effective use of LLMs,\nultimately enhancing medical education.\nFigure 5. Examples of how ChatGPT can be integrated into medical education: practicing differential diagnoses, streamlining the wide array of study\nresources to assist with devising a study plan, serving as a simulated patient or medical professor for interactive clinical cases, helping students review\nmultiple-choice questions or generating new questions for additional practice, digesting lecture outlines and generating materials for flash cards, and\norganizing information into tables to help build scaffolding for students to connect new information to previous knowledge.\nLimitations of LLMs for Medical Education\nOverview\nArtificial intelligence, for all its merits, is not currently a\nsubstitute for human intuition and clinical acumen. While LLMs\ncan exhibit profound capability in providing detailed medical\nknowledge, generating differential diagnoses, and even\nsimulating patient interactions, they are not without their\nshortcomings. It is crucial to remember that these are artificial\nsystems. They do not possess human cognition or intuition, their\nalgorithms operate within predefined bounds, and they base\ntheir outputs on patterns identified from the prompt provided\nand training data. This section explores key areas where\nChatGPT falls short for medical education, particularly with\nregard to fully mirroring the depth and breadth of human\nmedical practice.\nIntegration of Contextual and External Information\nAs shown by studies to date, ChatGPT has difficulty using\nexternal and contextual information. For instance, prior to 2020,\nCOVID-19 may not have been high on a differential for signs\nof the common cold, highlighting the importance of contextual\nmedical knowledge. This shortcoming is compounded by the\nfact that ChatGPT lacks the contextual local understanding that\nmedical students and physicians implicitly deploy while\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 7https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nworking. For example, within the Yale New Haven Health\nSystem, certain centers are magnets for complex cases, leading\nto a higher prevalence of rare diseases (and altering differential\ndiagnoses). Lacking this understanding limits ChatGPT’s ability\nto generate contextually accurate differentials. While descriptive\nprompting may alter ChatGPT’s performance to brainstorm\ndifferentials more aptly, it is not feasible to comprehensively\ncapture the complex environment inherent in the practice of\nmedicine. When including only a partial snapshot of the true\ncontext in our prompt, for example, mentioning that we are a\nstudent working on a differential at a large referral center for\ncomplex cases, ChatGPT tends to overweight these isolated\ndetails (similar to case presentation in Figure 4).\nIn addition to the challenges of providing full contextual\ninformation when querying ChatGPT, it is equally concerning\nthat the model typically does not seek further clarification.\nOpenAI acknowledges that ChatGPT fails in this sense:\nIdeally, the model would ask clarifying questions\nwhen the user provided an ambiguous query. Instead,\nour current models usually guess what the user\nintended [1]\nThis harkens back to the analogy of ChatGPT as a calculator\nfor doctors, the importance of the user’s inputs, and the critical\nlens that must be applied to ChatGPT’s responses.\nSensory and Nonverbal Cues\nA physician’s ability to integrate multiple sensory inputs is\nindispensable. A patient visit is never textual or verbal\ninformation alone; it is intertwined with auditory, visual,\nsomatic, and even olfactory stimuli. For instance, in a case of\ndiabetic ketoacidosis, the diagnosis potentially lies at a\nconvergence of stimuli beyond just words—hearing a patient’s\nrapid deep “Kussmaul” breathing, feeling dehydration in a\npatient’s skin turgor, and smelling the scent of acetone on a\npatient’s breath. The human brain must use multimodal\nintegration of sensory and spoken information in a way that\nlanguage models inherently cannot replicate with text alone.\nSuch practical elements of “clinical sense” are impossible to\ntruly learn or convey within a text-only framework [33].\nThe significance of patient demeanor and nonverbal\ncommunication can additionally not be underestimated.\nTranslating symptoms into medical terminology is beyond\nsimple translation; often patients describe symptoms in unique,\nunexpected ways, and learning to interpret this is part of\ncomprehending and using clinical sublanguage. Moreover, a\nphysician’s intuitive sense of a patient appearing “sick” can\nguide a differential diagnosis before a single word is exchanged.\nChatGPT lacks this first step in the physical exam (“inspection\nfrom the foot of the bed” [34]) and, thus, is hindered in its use\nof translated and transcribed medical terminology input by the\nuser.\nRapport and Interpersonal Interaction\nA crucial facet of the medical practice lies in the art of\nestablishing rapport and managing interpersonal interactions\nwith human patients, which simulation via LLMs has difficulty\nreplicating and thus cannot effectively teach to medical students\n[35]. Real-world patient interactions require a nuanced\nunderstanding of emotional subtleties, contextual hints, and\ncultural norms, all paramount in fostering trust and facilitating\nopen dialogue. For instance, how should a health care provider\napproach sensitive topics such as illicit drug use? ChatGPT is\nable to answer this question surprisingly well, emphasizing the\nimportance of establishing rapport, showing empathy, and\napproaching the patient gently. However, reading those phrases\nis far different from observing such an interaction in person, let\nalone navigating the conversation with a patient yourself.\nA firsthand experience underscores the importance of emotional\nand situational awareness in a higher fidelity simulation than\nis possible with ChatGPT. During an educational simulation at\nthe Yale Center for Healthcare Simulation, our team evaluated\na woman presenting to the emergency department with\nabdominal pain, her concerned boyfriend at her side. Our team\ndeduced the potential for an ectopic pregnancy. Yet, amid the\ndiagnostic process and chaos of the exam room, we overlooked\na critical aspect—ensuring the boyfriend’s departure from the\nroom before discussing this sensitive issue. This experience\nstarkly illuminated how the art of managing interpersonal\ndynamics can play an equally significant role as medical\nknowledge in patient care. It is these gaps that reiterate the\ncritical role of human interaction and empathy in health care,\nattributes that, as of now, remain beyond the reach of what\nartificial intelligence can help medical students learn.\nAlignment With Medical Education and Patient Care\nGoals\nA final critical limitation of using LLMs in medical education\nlies in the potential misalignment between the underlying\nmechanics of artificial intelligence systems and the core\nobjectives of medical education and patient care. Medical\ntraining encompasses a multifaceted blend of knowledge\nacquisition, skill development, clinical reasoning, empathy, and\nethics. LLMs like ChatGPT predominantly function to support\nmedical knowledge, and while this knowledge is a lynchpin for\nthe broader competencies of the physician, it is not the entirety\nof clinical practice or the learning expected of the medical\nstudent transforming into a student doctor and finally physician.\nIn the clinical phase of medical education, where communication\nand procedural skills rise to prominence, the medical knowledge\nsupported by LLMs cannot meet the patient-centered values\nand ethical considerations required for human interaction in the\nhospital. As with existing medical knowledge bases and clinical\ndecision support (eg, UpToDate or DynaMedex), LLMs can be\nvaluable adjuncts to clinical education. It is critical that LLMs\ndo not detract from the humanistic elements of practice that are\ndeveloped through clinical education.\nFuture Integration of LLMs Into Health\nCare and the Importance of\nUnderstanding Strengths and\nWeaknesses\nThe integration of LLMs into health care is fast becoming a\nreality, with both the availability of LLMs at students’fingertips\nand the rapid influx of research-driven deployments. Such\nintegration is underscored by the impending inclusion of\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 8https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nChatGPT into Epic Systems Corporation’s software [36].\nPotential applications range from reducing administrative tasks,\nlike generating patient discharge instructions, assisting with\ninsurance filings, and obtaining prior authorizations for medical\nservices [37], to improving care quality through extracting key\npast medical history from complex patient records and providing\ninteractive cross-checks of standard operating procedures\n(Figure 6).\nAcross the range of emerging applications, the most notable are\nthe potential for LLMs to digest the huge volumes of\nunstructured data in electronic health records and the possibility\nfor LLMs to assist with clinical documentation [9,38]. However,\nthese benefits are not without their challenges. Ethical\nconsiderations must be addressed regarding the impacts of\nmisinformation and bias if LLMs are implemented to help\ngenerate clinical notes or instructions for patients or if they are\napplied to automate chart review for clinical research.\nSystematic approaches and ethical frameworks must be\ndeveloped to mitigate these risks. Moreover, steps must be taken\nto ensure that the use of patients’ protected health information\nis in accordance with the Health Insurance Portability and\nAccountability Act (HIPAA) privacy and security requirements.\nAs we move toward a health care landscape increasingly\nintertwined with artificial intelligence, medical students must\nbecome adept at understanding and navigating the strengths and\nweaknesses of such technologies [39-41]. To be future leaders\nin health care, we must critically evaluate the best ways to\nharness artificial intelligence for improving health care while\nbeing cognizant of its limitations and the ethical, legal, and\npractical challenges it may pose.\nThe proactive curricular discourse surrounding topics like\nhallucinations, bias, and artificial intelligence models’\nself-evaluation of uncertainty, coupled with an exploration of\npotential legal and ethical issues, might be woven into the\ndelivery of topics related to physicians’ responsibility. By\nreadily encouraging these dialogues, students can prepare for\nthe challenges and opportunities that will come with the future\nintegration of artificial intelligence into health care.\nFigure 6. A few examples of how ChatGPT may be integrated into health care, derived from current news sources and research projects within the\nclinical informatics community.\nConclusions\nLLMs like ChatGPT hold significant potential for augmenting\nmedical education. By integrating them into the educational\nprocess, we can foster critical thinking, promote creativity, and\noffer novel learning opportunities. Moreover, a deeper\nunderstanding of these models prepares students for their\nimpending role in a health care landscape increasingly\nintertwined with artificial intelligence. Reflecting on the use of\nChatGPT in medical school is an essential step to harness the\npotential of technology to lead the upcoming transformations\nin the digital era of medicine. The next generation of health care\nprofessionals must be not only conversant with these\ntechnologies but also equipped to leverage them responsibly\nand effectively in the service of patient care.\nAcknowledgments\nResearch reported in this publication was supported by the National Heart, Lung, and Blood Institute of the National Institutes\nof Health under award T35HL007649 (CWS), the National Institute of General Medical Sciences of the National Institutes of\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 9https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nHealth under award T32GM136651 (AESE), the National Institute of Diabetes and Digestive and Kidney Diseases of the National\nInstitutes of Health under award T35DK104689 (AG), and the Yale School of Medicine Fellowship for Medical Student Research\n(AG). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National\nInstitutes of Health.\nAuthors' Contributions\nCWS, AESE, and DC contributed to the study conceptualization and drafting of the original manuscript. All authors participated\nin the investigation and validation process. All authors edited the manuscript draft and reviewed the final manuscript.\nConflicts of Interest\nNone declared.\nReferences\n1. Introducing ChatGPT. OpenAI. URL: https://openai.com/blog/chatgpt [accessed 2023-06-06]\n2. Brants T, Popat AC, Xu P, Och FJ, Dean J. Large language models in machine translation. In: Proceedings of the 2007\nJoint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.\n2007 Presented at: EMNLP-CoNLL; June 2007; Prague p. 858-867\n3. Singh S, Mahmood A. The NLP cookbook: modern recipes for transformer based deep learning architectures. IEEE Access\n2021;9:68675-68702 [doi: 10.1109/access.2021.3077350]\n4. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, et al. Training language models to follow instructions\nwith human feedback. In: Koyejo S, Mohamed S, Agarwal A, Belgrave D, Cho K, Oh A, editors. Advances in Neural\nInformation Processing Systems 35 (NeurIPS 2022). La Jolla, CA: Neural Information Processing Systems Foundation,\nInc; 2022:27730-27744\n5. Hirschberg J, Manning CD. Advances in natural language processing. Science 2015 Jul 17;349(6245):261-266 [doi:\n10.1126/science.aaa8685] [Medline: 26185244]\n6. Eysenbach G. The role of ChatGPT, generative language models, and artificial intelligence in medical education: a\nconversation with ChatGPT and a call for papers. JMIR Med Educ 2023 Mar 06;9:e46885 [FREE Full text] [doi:\n10.2196/46885] [Medline: 36863937]\n7. Lee H. The rise of ChatGPT: exploring its potential in medical education. Anat Sci Educ 2023 Mar 14:1 [doi:\n10.1002/ase.2270] [Medline: 36916887]\n8. Xue VW, Lei P, Cho WC. The potential impact of ChatGPT in clinical and translational medicine. Clin Transl Med 2023\nMar;13(3):e1216 [FREE Full text] [doi: 10.1002/ctm2.1216] [Medline: 36856370]\n9. Lee P, Bubeck S, Petro J. Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine. N Engl J Med 2023 Mar\n30;388(13):1233-1239 [doi: 10.1056/NEJMsr2214184] [Medline: 36988602]\n10. OpenAI. GPT-3 model card. GitHub. 2022 Sep 01. URL: https://github.com/openai/gpt-3/blob/master/model-card.md#data\n[accessed 2023-06-23]\n11. Olson P. ChatGPT needs to go to college. Will OpenAI pay? The Washington Post. 2023 Jun 05. URL: https://www.\nwashingtonpost.com/business/2023/06/05/chatgpt-needs-better-training-data-will-openai-and-google-pay-up-for-it/\nf316828c-035d-11ee-b74a-5bdd335d4fa2_story.html [accessed 2023-06-20]\n12. Barr K. GPT-4 is a giant black box and its training data remains a mystery. Gizmodo. 2023 Mar 16. URL: https://gizmodo.\ncom/chatbot-gpt4-open-ai-ai-bing-microsoft-1850229989 [accessed 2023-06-23]\n13. Sallam M. ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives\nand valid concerns. Healthcare (Basel) 2023 Mar 19;11(6):887 [FREE Full text] [doi: 10.3390/healthcare11060887]\n[Medline: 36981544]\n14. Farrell L, Bourgeois-Law G, Regehr G, Ajjawi R. Autoethnography: introducing 'I' into medical education research. Med\nEduc 2015 Oct;49(10):974-982 [doi: 10.1111/medu.12761] [Medline: 26383069]\n15. Basic medical education WFME global standards for quality improvement: the 2020 revision. World Federation for Medical\nEducation. 2020. URL: https://wfme.org/wp-content/uploads/2020/12/WFME-BME-Standards-2020.pdf [accessed\n2023-06-23]\n16. Wijnen-Meijer M, Burdick W, Alofs L, Burgers C, ten Cate O. Stages and transitions in medical education around the\nworld: clarifying structures and terminology. Med Teach 2013 Apr;35(4):301-307 [doi: 10.3109/0142159X.2012.746449]\n[Medline: 23360484]\n17. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical knowledge. arXiv.\nPreprint posted online on December 26, 2022. [FREE Full text] [doi: 10.1038/s41586-023-06455-0]\n18. Xu G, Rong W, Wang Y, Ouyang Y, Xiong Z. External features enriched model for biomedical question answering. BMC\nBioinformatics 2021 May 26;22(1):272 [FREE Full text] [doi: 10.1186/s12859-021-04176-7] [Medline: 34039273]\n19. Nadkarni PM, Ohno-Machado L, Chapman WW. Natural language processing: an introduction. J Am Med Inform Assoc\n2011;18(5):544-551 [FREE Full text] [doi: 10.1136/amiajnl-2011-000464] [Medline: 21846786]\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 10https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\n20. Johnson D, Goodman R, Patrinely J, Stone C, Zimmerman E, Donald R, et al. Assessing the accuracy and reliability of\nAI-generated medical responses: an evaluation of the Chat-GPT model. Res Square. Preprint posted online on February\n28, 2023. [FREE Full text] [doi: 10.21203/rs.3.rs-2566942/v1] [Medline: 36909565]\n21. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, et al. How does ChatGPT perform on the United States\nMedical Licensing Examination? The implications of large language models for medical education and knowledge assessment.\nJMIR Med Educ 2023 Feb 08;9:e45312 [FREE Full text] [doi: 10.2196/45312] [Medline: 36753318]\n22. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. Performance of ChatGPT on USMLE: potential\nfor AI-assisted medical education using large language models. PLOS Digit Health 2023 Feb;2(2):e0000198 [FREE Full\ntext] [doi: 10.1371/journal.pdig.0000198] [Medline: 36812645]\n23. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of GPT-4 on medical challenge problems. arXiv.\nPreprint posted online on March 20, 2023. [FREE Full text]\n24. The core entrustable professional activities (EPAs) for entering residency. Association of American Medical Colleges.\nURL: https://www.aamc.org/about-us/mission-areas/medical-education/cbme/core-epas [accessed 2023-06-23]\n25. Core entrustable professional activities. School of Medicine, Vanderbilt University. 2019. URL: https://medschool.\nvanderbilt.edu/md-gateway/core-entrustable-professional-activities/ [accessed 2023-06-23]\n26. GPT best practices. OpenAI. URL: https://platform.openai.com/docs/guides/gpt-best-practices [accessed 2023-06-27]\n27. Hrynchak P, Batty H. The educational theory basis of team-based learning. Med Teach 2012;34(10):796-801 [doi:\n10.3109/0142159X.2012.687120] [Medline: 22646301]\n28. Charlin B, Roy L, Brailovsky C, Goulet F, van der Vleuten C. The Script Concordance test: a tool to assess the reflective\nclinician. Teach Learn Med 2000;12(4):189-195 [doi: 10.1207/S15328015TLM1204_5] [Medline: 11273368]\n29. Croskerry P. Achieving quality in clinical decision making: cognitive strategies and detection of bias. Acad Emerg Med\n2002 Nov;9(11):1184-1204 [FREE Full text] [doi: 10.1111/j.1553-2712.2002.tb01574.x] [Medline: 12414468]\n30. Jones E, Steinhardt J. Capturing failures of large language models via human cognitive biases. arXiv. Preprint posted online\non February 24, 2022. [FREE Full text]\n31. Kost A, Chen FM. Socrates was not a pimp: changing the paradigm of questioning in medical education. Acad Med 2015\nJan;90(1):20-24 [doi: 10.1097/ACM.0000000000000446] [Medline: 25099239]\n32. Hersh W, Ehrenfeld J. Clinical informatics. In: Skochelak SE, editor. Health Systems Science. 2nd Edition. Amsterdam,\nThe Netherlands: Elsevier; May 06, 2020:105-116\n33. Asher R. Clinical sense. The use of the five senses. Br Med J 1960 Apr 02;1(5178):985-993 [FREE Full text] [doi:\n10.1136/bmj.1.5178.985] [Medline: 13794723]\n34. Talley N, O'Connor S. Clinical Examination: A Systematic Guide to Physical Diagnosis. Amsterdam, The Netherlands:\nElsevier; 2014.\n35. Martin A, Weller I, Amsalem D, Duvivier R, Jaarsma D, de Carvalho Filho MA. Co-constructive patient simulation: a\nlearner-centered method to enhance communication and reflection skills. Simul Healthc 2021 Dec 01;16(6):e129-e135\n[FREE Full text] [doi: 10.1097/SIH.0000000000000528] [Medline: 33273424]\n36. Adams K. Epic to integrate GPT-4 into its EHR through expanded Microsoft partnership. MedCity News. 2023. URL:\nhttps://medcitynews.com/2023/04/epic-to-integrate-gpt-4-into-its-ehr-through-expanded-microsoft-partnership/ [accessed\n2023-06-20]\n37. Landi H. Doximity rolls out beta version of ChatGPT tool for docs aiming to streamline administrative paperwork. Fierce\nHealthcare. 2023. URL: https://www.fiercehealthcare.com/health-tech/\ndoximity-rolls-out-beta-version-chatgpt-tool-docs-aiming-streamline-administrative [accessed 2023-06-21]\n38. Landi H. Microsoft's Nuance integrates OpenAI's GPT-4 into voice-enabled medical scribe software. Fierce Healthcare.\n2023. URL: https://www.fiercehealthcare.com/health-tech/microsofts-nuance-integrates-openais-gpt-4-medical-scribe-software\n[accessed 2023-06-27]\n39. Chartash D, Rosenman M, Wang K, Chen E. Informatics in undergraduate medical education: analysis of competency\nframeworks and practices across North America. JMIR Med Educ 2022 Sep 13;8(3):e39794 [FREE Full text] [doi:\n10.2196/39794] [Medline: 36099007]\n40. Hersh WR, Gorman PN, Biagioli FE, Mohan V, Gold JA, Mejicano GC. Beyond information retrieval and electronic health\nrecord use: competencies in clinical informatics for medical education. Adv Med Educ Pract 2014;5:205-212 [FREE Full\ntext] [doi: 10.2147/AMEP.S63903] [Medline: 25057246]\n41. Paranjape K, Schinkel M, Nannan Panday R, Car J, Nanayakkara P. Introducing artificial intelligence training in medical\neducation. JMIR Med Educ 2019 Dec 03;5(2):e16048 [FREE Full text] [doi: 10.2196/16048] [Medline: 31793895]\nAbbreviations\nEPA: Entrustable Professional Activity\nHIPAA: Health Insurance Portability and Accountability Act\nLLM: large language model\nUSMLE: United States Medical Licensing Exam\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 11https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nEdited by T de Azevedo Cardoso; this is a non–peer-reviewed article. Submitted 17.07.23; accepted 26.07.23; published 14.08.23.\nPlease cite as:\nSafranek CW, Sidamon-Eristoff AE, Gilson A, Chartash D\nThe Role of Large Language Models in Medical Education: Applications and Implications\nJMIR Med Educ 2023;9:e50945\nURL: https://mededu.jmir.org/2023/1/e50945\ndoi: 10.2196/50945\nPMID: 37578830\n©Conrad W Safranek, Anne Elizabeth Sidamon-Eristoff, Aidan Gilson, David Chartash. Originally published in JMIR Medical\nEducation (https://mededu.jmir.org), 14.08.2023. This is an open-access article distributed under the terms of the Creative\nCommons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided the original work, first published in JMIR Medical Education, is properly cited. The\ncomplete bibliographic information, a link to the original publication on https://mededu.jmir.org/, as well as this copyright and\nlicense information must be included.\nJMIR Med Educ 2023 | vol. 9 | e50945 | p. 12https://mededu.jmir.org/2023/1/e50945\n(page number not for citation purposes)\nSafranek et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX"
}