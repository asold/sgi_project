{
    "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
    "url": "https://openalex.org/W3120952198",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5089094092",
            "name": "Siyu Ding",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5023311055",
            "name": "Junyuan Shang",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5058880150",
            "name": "Shuohuan Wang",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5101870256",
            "name": "Yu Sun",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5071362658",
            "name": "Hao Tian",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5100677198",
            "name": "Hua Wu",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5100386394",
            "name": "Haifeng Wang",
            "affiliations": [
                "Baidu (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2984864519",
        "https://openalex.org/W2792376130",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W2986558555",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W3023124213",
        "https://openalex.org/W2769934148",
        "https://openalex.org/W3102725307",
        "https://openalex.org/W2963866616",
        "https://openalex.org/W2991316439",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3106031450",
        "https://openalex.org/W3023787386",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2952862139",
        "https://openalex.org/W2612431505",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2945918281",
        "https://openalex.org/W3006983028",
        "https://openalex.org/W2936074642",
        "https://openalex.org/W3015253856",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2806081754",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3105238007",
        "https://openalex.org/W3112776819",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W3092952717",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W3017192286",
        "https://openalex.org/W3210120707",
        "https://openalex.org/W2765390718",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2964110616"
    ],
    "abstract": "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.",
    "full_text": "ERNIE-D OC: A Retrospective Long-Document Modeling Transformer\nSiyu Ding∗, Junyuan Shang ∗, Shuohuan Wang, Yu Sun, Hao Tian,\nHua Wu and Haifeng Wang\nBaidu Inc., China\n{dingsiyu, shangjunyuan, wangshuohuan, sunyu02,\ntianhao, wu hua, wanghaifeng}@baidu.com\nAbstract\nTransformers are not suited for process-\ning long documents, due to their quadrati-\ncally increasing memory and time consump-\ntion. Simply truncating a long document or\napplying the sparse attention mechanism will\nincur the context fragmentation problem or\nlead to an inferior modeling capability against\ncomparable model sizes. In this paper, we\npropose ERNIE-D OC, a document-level lan-\nguage pretraining model based on Recurrence\nTransformers (Dai et al., 2019). Two well-\ndesigned techniques, namely the retrospective\nfeed mechanism and the enhanced recurrence\nmechanism, enable ERNIE-D OC 1, which has\na much longer effective context length, to\ncapture the contextual information of a com-\nplete document. We pretrain ERNIE-D OC\nto explicitly learn the relationships among\nsegments with an additional document-aware\nsegment-reordering objective. Various exper-\niments were conducted on both English and\nChinese document-level tasks. ERNIE-D OC\nimproved the state-of-the-art language model-\ning result of perplexity to 16.8 on WikiText-\n103. Moreover, it outperformed competitive\npretraining models by a large margin on most\nlanguage understanding tasks, such as text\nclassiﬁcation and question answering.\n1 Introduction\nTransformers (Vaswani et al., 2017) have achieved\nremarkable improvements in a wide range of nat-\nural language tasks, including language model-\ning (Dai et al., 2019), text classiﬁcation (Yang et al.,\n2019), and question answering (Devlin et al., 2018;\nRadford et al., 2019). This success is largely due\nto the self-attention mechanism, which enables the\nnetwork to capture contextual information from the\n*indicates equal contribution.\n1Source code and pre-trained checkpoints can be found\nat https://github.com/PaddlePaddle/ERNIE/\ntree/repro/ernie-doc.\nS1S3S1 S2\n(c) ERNIE-DOC\nS1 S2\nS2\nRecurrence \nTransformer\nS1 S2\nVanilla or Sparse\n Transformer\nP( y | S2 ) P( y | S1 , S2 )\nP( y | S1 , S2 , S3 )\n\u000bD\f\u0003 (b) \nS3 S3\nLayer-1\nLayer-2\nLayer-1\nLayer-2\nFigure 1: Available contextual information utilized by\nTransformer variants, where a long document Dis par-\ntitioned into three segments Si(i ∈ [1,2,3]). When\ntraining on S2, (a) and (b) optimize the pretraining ob-\njective depending only on the contextual information\nfrom the current segment or segments in the forward\npass, whereas ERNIE-D OC utilizes the contextual in-\nformation of the entire document for each segment.\nentire input sequence. Nevertheless, the memory\nusage and computation complexity caused by the\nself-attention mechanism grows quadratically with\nthe sequence length, incurring excessive cost when\nprocessing a long document on existing hardware.\nCurrently, the most prominent pretrained mod-\nels, such as BERT (Devlin et al., 2018), are used\non ﬁxed-length input segments of a maximum of\n512 tokens owing to the aforementioned limita-\ntion. Thus, a long document input must be parti-\ntioned into smaller segments of manageable sizes.\nHowever, this leads to the loss of important cross-\nsegment information, that is, the context fragmen-\ntation problem (Dai et al., 2019), as shown in\nFig. 1(a). To mitigate the problem of insufﬁcient in-\nteractions among the partitioned segments of long\ndocuments, Recurrence Transformers(Dai et al.,\n2019; Rae et al., 2019) permit the use of contextual\ninformation from previous segments in computing\nthe hidden states for a new segment by maintaining\na memory component from the previous activation;\narXiv:2012.15688v2  [cs.CL]  24 May 2021\nthis enables the modeling of long documents. In ad-\ndition, Sparse Attention Transformers(Child et al.,\n2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer\net al., 2020) focus on reducing the complexity of\nself-attention operations to explicitly improve the\nmodeling length, but only up to a restricted context\nlength (4,096) due to resource limitations.\nWe argue that existing strategies are not sufﬁ-\nciently effective or reliable, because the contex-\ntual information of a complete documentis still\nnot available for each segment during the train-\ning phase. As depicted in Fig. 1, when training\non segment S2, the model is ideally optimized\nby maximizing P(y|(S1,S2,S3)) conditioned on\nthe contextual information of the entire document\nD= {S1,S2,S3}, in contrast to the following sub-\noptimal solutions: P(y |S2) for Vanilla/Sparse\nTransformers2 and P(y|(S1,S2)) for Recurrence\nTransformers.\nTo address this limitation, we propose ERNIE-\nDOC (A Retrospective Long-Document Modeling\nTransformer) based on the Recurrence Transformer\nparadigm. Inspired by the human reading behavior\nof skimming a document ﬁrst and then looking back\nupon it attentively, we design a retrospective feed\nmechanism in which segments from a document\nare fed twice as input. As a result, each segment in\nthe retrospective phase could explicitly fuse the se-\nmantic information of the entire document learned\nin the skimming phase, which prevents context\nfragmentation.\nHowever, simply incorporating the retrospective\nfeed mechanism into Recurrence Transformers is\ninfeasible because the maximum effective context\nlength is limited by the number of layers (Dai et al.,\n2019), as shown in Fig. 1 (b). Thus, we present an\nenhanced recurrence mechanism , a drop-in re-\nplacement for a Recurrence Transformer, by chang-\ning the shifting-one-layer-downwards recurrence to\nthe same-layer recurrence. In this manner, the max-\nimum effective context length can be expanded, and\npast higher-level representations can be exploited\nto enrich future lower-level representations.\nMoreover, we introduce a segment-reordering\nobjective to pretrain a document-level model.\nSpeciﬁcally, it is a document-aware task of pre-\ndicting the correct order of the permuted set of\nsegments of a document, to model the relationship\namong segments directly. This allows ERNIE-\n2For Sparse Transformers, the length of segment S2 could\nbe up to 4,096 in Beltagy et al. (2020); Zaheer et al. (2020).\nDOC to build full document representations for\nprediction. This is analogous to the sentence-\nreordering task in ERNIE 2.0 (Sun et al., 2020b)\nbut at a segment level of granularity, spanning\n(commonly) multiple training steps.\nWe ﬁrst evaluate ERNIE-D OC on autoregres-\nsive word-level language modeling using the en-\nhanced recurrence mechanism, which, in theory,\nallows the model to process a document with in-\nﬁnite words. ERNIE-D OC achieves state-of-the-\nart (SOTA) results on the WiKiText-103 bench-\nmark dataset, demonstrating its effectiveness in\nlong-document modeling. Then, to evaluate the\npotential of ERNIE-D OC on document-level nat-\nural language understanding (NLU) tasks, we pre-\ntrained the English ERNIE-D OC on the text cor-\npora utilized in BigBird (Zaheer et al., 2020) from\nthe RoBERTa-released checkpoint, and the Chi-\nnese ERNIE-D OC on the text corpora utilized in\nERNIE 2.0 (Sun et al., 2020b) from scratch. After\npretraining, we ﬁne-tuned ERNIE-D OC on a wide\nrange of English and Chinese downstream tasks, in-\ncluding text classiﬁcation, question answering and\nkeypharse extraction. Empirically, ERNIE-D OC\nconsistently outperformed RoBERTa on various\nbenchmarks and showed signiﬁcant improvements\nover other high-performance long-text pretraining\nmodels for most tasks.\n2 Related Work\nSparse Attention Transformers have been exten-\nsively explored (Child et al., 2019; Tay et al., 2020;\nBeltagy et al., 2020; Zaheer et al., 2020). The\nkey idea is to sparsify the self-attention operation,\nwhich scales quadratically with the sequence length.\nFor instance, the Sparse Transformer (Child et al.,\n2019) uses a dilated sliding window that reduces\nthe complexity to O(L\n√\nL), where L is the se-\nquence length. Reformer (Kitaev et al., 2020) fur-\nther reduces the complexity to O(Llog L) using\nlocality-sensitive hashing attention to compute the\nnearest neighbors. BP-Transformers (Ye et al.,\n2019) employs a binary partition for the input\nsequence. Recently, Longformer (Beltagy et al.,\n2020) and BigBird (Zaheer et al., 2020) have been\nproposed, and both achieved state-of-the-art perfor-\nmance on a variety of long-document tasks. They\nreduce the complexity of self-attention to O(L)\nby combining random attention, window attention,\nand global attention. However, it has been proven\nin Zaheer et al. (2020) that sparse attention mech-\nanisms cannot universally replace dense attention\nmechanisms; moreover, solving the simple problem\nof ﬁnding the furthest vector requires Ω(n)-layers\nof a sparse attention mechanism but only O(1)-\nlayers of a dense attention mechanism. In addition,\nthe aforementioned methods require customized\nCUDA kernels or TVM programming to imple-\nment sparse attention, which are not maintainable\nand are difﬁcult to use. In this study, we adopt a\ndifferent approach to adapting Recurrence Trans-\nformers for a pretraining-then-ﬁnetuning setting, to\nmodel a long document.\nRecurrence Transformers (Dai et al., 2019; Rae\net al., 2019) have been successfully applied in gen-\nerative language modeling. They employ the Trans-\nformer decoder as a parametric model for each con-\nditional distribution in p(x) = ∏L\nt=1 p(xt|x<t),\nwhere x denotes a text sequence. To capture long\ndependencies, they process the text in segments\nfrom left to right based on the segment recurrence\nmechanism (Dai et al., 2019). This mechanism\nmaintains a memory bank of past activations at\neach layer to preserve a history of context. Com-\npressive Transformer (Rae et al., 2019) adds a\ncompressive memory bank to sufﬁciently store old\nactivations instead of discarding them, which fa-\ncilitates long-range sequence learning. However,\nthese methods operate from left to right, which\nlimits their capacity for discriminative language\nunderstanding tasks that require bidirectional in-\nformation. XLNet (Yang et al., 2019) proposed a\npermutation language modeling objective to con-\nstruct bidirectional information and achieve supe-\nrior performance in multiple NLP tasks; however,\nits application to long-document modeling tasks\nremains largely unexplored. ERNIE-D OC builds\non the ideas of the Recurrence Transformers to 1)\ntackle the limitation of Recurrence Transformers\nfor utilizing bidirectional contextual information\nand 2) improve the behavior of the segment recur-\nrence mechanism to capture longer dependencies.\nHierarchical Transformers (Zhang et al., 2019;\nLin et al., 2020) have enabled signiﬁcant progress\non numerous document-level tasks, such as docu-\nment summarization (Zhang et al., 2019) and docu-\nment ranking (Lin et al., 2020). Similar to Vanilla\nTransformers, Hierarchical Transformers also split\nlong documents into shorter segments with man-\nageable lengths and then feed them independently\nto produce corresponding segment-level semantic\nrepresentations. Unlike in Vanilla Transformers,\nhowever, separate Transformer layers are used in\nHierarchical Transformers to process the concate-\nnation of these representations. Hierarchical Trans-\nformers ignore the contextual information from the\nremaining segments when processing each segment\nof a long document, thus suffering from thecontext\nfragmentation problem.\n3 Proposed Method\nIn this section, we ﬁrst describe the background\n(Sec. 3.1) that ERNIE-D OC builds on. Then,\nwe present the implementation of ERNIE-D OC,\nincluding the retrospective feed mechanism in\nSec. 3.2, the enhanced recurrence mechanism in\nSec. 3.3, and the segment-reordering objective in\nSec. 3.4.\n3.1 Background\nFormally, a long document Dis sliced into T\nsequential segments, denoted as {S1,S2,...,S T},\nwhere Sτ = {xτ,1,xτ,2,...,x τ,L}is the τ-th seg-\nment with L tokens; x denotes a single token.\nVanilla, Sparse, and Recurrence Transformers em-\nploy different strategies to produce the hidden state\nhn\nτ ∈RL×d for segment Sτ at the n-th layer:\n˜hn−1\nτ+1 =\n{\nhn−1\nτ+1 , Vanilla or Sparse Transformers\n[SG(hn−1\nτ ) ◦ hn−1\nτ+1 ], Recurrence Transformers,\nqn\nτ+1,kn\nτ+1,vn\nτ+1 = hn−1\nτ+1 W⊤\nq , ˜hn−1\nτ+1 W⊤\nk , ˜hn−1\nτ+1 W⊤\nv .\nhn\nτ+1 = Transformer-Block (qn\nτ+1,kn\nτ+1,vn\nτ+1).\n(1)\nwhere q ∈RL×d, k,and v ∈R(L+m)×d are the\nquery, key and value vectors, respectively with\nhidden dimension dand memory length m(Note\nthat m = 0 for Vanilla or Sparse Transform-\ners); ˜hn−1\nτ+1 ∈R(L+m)×d is the extended context;\nW∗ ∈Rd∗×d represents learnable linear projec-\ntion parameters; the function SG(·) denotes the\nstop-gradient operation; and the notation [◦] de-\nnotes the concatenation of two hidden states along\nthe length dimension. In contrast to Vanilla or\nSparse Transformers, where hn\nτ+1 is produced us-\ning only itself, Recurrence Transformers introduce\na segment-level recurrence mechanism to promote\ninteraction across segments. The hidden state com-\nputed for the previous segment hn−1\nτ is cached as\nan auxiliary context to help process the current seg-\nment hn\nτ. However, from the concatenation part in\nEq. 1, i.e., [SG(hn−1\nτ ) ◦hn−1\nτ+1], there is apparently\na constraint that the current hidden state can only\nfuse information from the previous segments. In\nS1 S2 S3 S4\nRecurrence Transformers\nS1 S2 S3 S4\nLarger Eﬀective Context Length\nS1 S2 S3 S4\nERNIE-DOC\nTransformer block\nMemory concatenation\nHidden states input\nEﬀective Context\nLarger Eﬀective Context Length\nRetrospective Phase\nLayer-1\nLayer-2\nLayer-3\nLayer-1\nLayer-2\nLayer-3\nThe Retrospective Phase\nFigure 2: Illustrations of ERNIE-D OC and Recurrence Transformers, where models with three layers take as input\na long documentDwhich is sliced into four segmentsSi,i ∈[1,2,3,4]. Recurrence Transformers (upper-right):\nWhen training on S4, it can only fuse the contextual information of the previous two consecutive segments S2,S3,\nsince the largest effective context length grows linearly w.r.t the number of layers. ERNIE-D OC (lower):The\neffective context length is much larger aided by the enhanced recurrence mechanism (Sec. 3.3). Thus, S4 can\nfuse the information of S1 discarded by Recurrence Transformers. Moreover, segments in the retrospective phase\ncontains the contextual information of an entire document, powered by the retrospective feed mechanism (Sec. 3.2).\nother words, the contextual information of an entire\ndocument is not available for each segment.\n3.2 Retrospective Feed Mechanism\nERNIE-D OC employs a retrospective feed mecha-\nnism to address the unavailability of the contextual\ninformation of a complete document for each seg-\nment. The segments from a long document are\ntwice fed as input. Mimicking the human reading\nbehavior, we refer to the ﬁrst and second input-\ntaking phases as the skimming and retrospective\nphases, respectively. In the skimming phase, we\nemploy a recurrence mechanism to cache the hid-\nden states for each segment. In the retrospective\nphase, we reuse the cached hidden states from the\nskimming phase to enable bi-directional informa-\ntion ﬂow. Naively, we can rewrite Eq. 1 to obtain\nthe contextual information of an entire document\nin the skimming phase to be utilized in the retro-\nspective phase as follows,\nˆH = [ ˆH1\n1:T ◦ˆH2\n1:T ···◦ ˆHN\n1:T], (skim. phase)\n˜hn−1\nτ+1 = [SG( ˆH ◦hn−1\nτ ) ◦hn−1\nτ+1], (retro. phase)\n(2)\nwhere ˆH ∈R(L∗T∗N)×d denotes the cached hid-\nden states in the skimming phase with T segments,\nLlength of each segment and total N layers, and\nˆHi\n1:T = [ˆhi\n1 ◦ˆhi\n2 ···◦ ˆhi\nT] is the concatenation of\ni-th layer’s hidden states of the skimming phase.\nThus, the extended context ˜hn−1\nτ+1 is guaranteed to\ncapture the bidirectional contextual information of\nthe entire document. However, it will incur massive\nmemory and computation cost for directly employ-\ning ˆH in self-attention mechanism. Henceforth, the\nmain issue is how ˆH should be implemented in a\nmemory- and computation-efﬁcient manner.\nBy rethinking segment-level recurrence (Dai\net al., 2019), we observe that the largest possible\ncontext dependency length increases linearly w.r.t\nthe number of layers ( N). For instance, at i-th\nlayer, ˆhi\nτ have the longest dependency to ˆh1\nτ−(i−1).\nThus, to minimize memory and computation con-\nsumption, hidden states from the N-th layer (top-\nlayer) are included at a stride of N, which is suf-\nﬁcient to build the contextual information of an\nentire document. Formally, ˆH can be reduced to\nˆHr = [ˆhN\nN◦ˆhN\n2∗N ···◦ ˆhN\n⌊T/N⌋∗N] (Note that when\nT is not evenly divisible byN, the last hidden state\nˆhN\nT need to be included). However, for a long doc-\nument input, the extra computational and memory\ncost of ˆHr ∈R⌈T/N⌉×d where T ≫N is still\nexcessive on existing hardware.\n3.3 Enhanced Recurrence Mechanism\nTo effectively utilize the retrospective feed mech-\nanism in practice, an ideal strategy is to ensure\nthat the cached hidden state hn−1\nτ already contains\nthe contextual information of an entire document\nwithout explicitly taking ˆH or ˆHr as input. Es-\nsentially, we should tackle the problem of limited\neffective context length in the segment-level re-\ncurrence mechanisms. Herein, we introduce the\nenhanced recurrence mechanism, a drop-in replace-\nment for the segment-level recurrence mechanism,\nby changing the shifting-one-layer-downwards re-\ncurrence to the same-layer recurrence as follows:\n˜hn−1\nτ+1 = [ SG(hn\nτ) ◦hn−1\nτ+1] (3)\nwhere the cached hidden state hn−1\nτ in Eq. 1 and\nEq. 2 is replaced with hn\nτ in Eq. 3.\nAs shown in Fig. 2, when the retrospective feed\nmechanism is combined with the enhanced recur-\nrence mechanism, every segment in the retrospec-\ntive phase (shown in the box with a green dotted\nborder) has bidirectional contextual information of\nthe entire text input. We successfully modeled a\nlarger effective context length (shown in the box\nwith a orange dotted border) than traditional Re-\ncurrence Transformers can without extra memory\nand computation costs. Another beneﬁt of the en-\nhanced recurrence scheme is that past higher-level\nrepresentations can be exploited to enrich future\nlower-level representations.\n3.4 Segment-Reordering Objective\nIn addition to the masked language model\n(MLM) objective (Devlin et al., 2018), we intro-\nduce an additional document-aware task called\nsegment-reordering objective for pretraining.\nBeneﬁtting from the much larger effective context\nlength provided by the enhanced recurrence mecha-\nnism, the goal of the segment-reordering objective\nis to predict the correct order for the permuted\nset of segments of a long document, to explicitly\nlearn the relationships among segments. During\nthe pretraining process of this task, a long text\ninput Dis ﬁrst randomly partitioned into 1 to m\nchunks; then, all the combinations are shufﬂed in\na random order. As shown in Fig. 3, Dis parti-\ntioned into three chunks and then permuted, that\nis, D = {C1,C2,C3} =⇒ ˆD = {C2,C3,C1},\nwhere Ci denotes the i-th chunk. Subsequently,\nthe permuted long context ˆDis split into T se-\nquential segments as a common practice, denoted\nas ˆD = {S1,S2,...,S T}. We let the pretrained\nmodel reorganize these permuted segments, mod-\neled as a K-class classiﬁcation problem, where\nK = ∑m\ni=1 i!.\nThe pretraining objective is summarized as fol-\nlows for the τ-th input segment:\nmax\nθ\nlog pθ(Sτ|ˆSτ) + 1 τ=T log pθ(D|ˆD)\nwhere ˆSτ is the corrupted version of Sτ, which is\nobtained by randomly setting a portion of tokens\nS1 S2 ST\nSegments\n(~512 tokens each)\n3HUPXDWHG\u0003&KXQNV\u0003RI\nD\u0003/RQJ\u00037H[W\u0003,QSXW\nModel\nlabel = “C1C2C3”  \nC2:[Related Work] Sparse attention based transformers are \nlargely explored …\nC3:[Proposed Method] In this section, we ﬁrstly describe the \nbackground of proposed ERNIE-DOC …\nC1:[Introduction] Transformers have achieved remarkable \nimprovements …\nŏ\nŏ\nFigure 3: Illustrations of segment-reordering objective.\nto [MASK]; ˆDis the permutated version of D; θis\nthe model parameter; and 1 τ=T indicates that the\nsegment-reordering objective is optimized only at\nthe T-th step.\n4 Experiments\n4.1 Autoregressive Language Modeling\nAutoregressive language modeling aims to esti-\nmate the probability distribution of an existing to-\nken/character based on previous tokens/characters\nin an input sequence. For comparison with pre-\nvious work, we conducted experiments on word-\nlevel LM, that is, WikiText-103 (Merity et al.,\n2016), which is a document-level language model-\ning dataset.\n4.1.1 Experimental Setup\nFor autoregressive language modeling, we use\na memory-enhanced Transformer-XL (Dai et al.,\n2019), that is, we employ our enhanced recurrence\nmechanism to replace the primitive one used in\nthe Transformer-XL. Additionally, as proposed\nby Segatron (Bai et al., 2020), we introduce the\nsegment-aware mechanism into Transformer-XL.\nBased on Transformer-XL, we trained a base-size\nmodel (L=16, H=410, A=10) and a large-size\nmodel (L=18, H=1,024, A=16)3. The models were\ntrained for 200K/400K steps using a batch size\nof 64/128 for the base/large conﬁgurations. Dur-\ning the training phase, the sequence length and\nmemory length were limited to 150 and 384 for\nthe base and the large model, respectively. The re-\nmaining hyper-parameters were identical to those\nof Transformer-XL.\n3We denote the number of Transformer layers as L, the\nhidden size as H, and the number of self-attention heads as A.\nModels #Param. PPL\nResults of base models\nLSTM (Grave et al., 2016) - 48.7\nLSTM+Neural cache (Grave et al., 2016) - 40.8\nGCNN-14 (Dauphin et al., 2017) - 37.2\nQRNN (Merity et al., 2018) 151M 33.0\nTransformer-XL Base (Dai et al., 2019) 151M 24.0\nSegaTransformer-XL Base (Bai et al., 2020) 151M 22.5\nERNIE-D OC Base 151M 21.0\nResults of large models\nAdaptive Input (Baevski and Auli, 2018) 247M 18.7\nTransformer-XL Large (Dai et al., 2019) 247M 18.3\nCompressive Transformer (Rae et al., 2019) 247M 17.1\nSegaTransformer-XL Large (Bai et al., 2020) 247M 17.1\nERNIE-D OC Large 247M 16.8\nTable 1: Comparison between Transformer-XL and\ncompetitive baseline results on WikiText-103.\n4.1.2 Results\nTab. 1 summarizes the evaluation results for\nWikiText-103. ERNIE-D OC achieves an impres-\nsive improvement compared with Transformer-XL:\nthe perplexity (PPL) decreases by 3.0 for the base\nmodel and by 1.5 for the large model. Finally, we\nimprove the state-of-the-art result of PPL to 21.0\n(the base model) and 16.8 (the large model).\n4.2 Pretraining and Finetuning\n4.2.1 Pretraining Text Corpora\nDataset # tokens Avg len Size\nWIKIPEDIA 2.7B 480 8G\nBOOKSCORPUS 1.2B 2,010 3.5G\nCC-NEWS 14B 560 42G\nSTORIES 7.5B 1,891 22G\nTable 2: English datasets used for pretraining.\nEnglish Data. To allow ERNIE-D OC to capture\nlong dependencies in pretraining, we compiled a\ncorpus from four standard datasets: WIKIPEDIA ,\nBOOKS CORPUS (Zhu et al., 2015), CC-N EWS 4,\nand STORIES (Trinh and Le, 2018) (details listed\nin Tab. 2). We tokenized the corpus using the\nRoBERTa wordpieces tokenizer (Liu et al., 2019)\nand duplicated the pretraining data 10 times.\nChinese Data. The Chinese text corpora used in\nERNIE 2.0 (Sun et al., 2020b) were adopted for\npretraining ERNIE-D OC.\n4.2.2 Experimental Setup\nPretraining. We trained three sizes of models for\nEnglish tasks: small (L=6, H=256, A=4), base\n(L=12, H=768, A=12), and large (L=24, H=1,024,\n4We used news-please to crawl English news articles\npublished between September 2016 and February 2019 and\nadopted Message Digest Algorithm5 (MD5) for deduplication.\nModels IMDB HYP\nAcc. F1 F1\nRoBERTa (Liu et al., 2019) 95.3 95.0 87.8\nLongformer (Beltagy et al., 2020) 95.7 - 94.8\nBigBird (Zaheer et al., 2020) - 95.2 92.2\nERNIE-DOC 96.1 96.1 96.3\nXLNet-Large (Yang et al., 2019) 96.8 - -\nERNIE-DOC-Large 97.1 97.1 96.6\nTable 3: Results on the IMDB and HYP dataset for\nlong-text classiﬁcation.\nA=16). For Chinese tasks, we used only one size,\ni.e., base (L=12, H=768, A=12). We limited the\nlength of the sentences in each mini-batch to\n512 tokens and the length of the memory to 128.\nThe models were trained for 500K/400K/100K\nsteps using a batch size of 2,560/2,560/3,920\nsentences for the small/base/large conﬁgurations.\nERNIE-D OC was optimized with the Adam\n(Kingma and Ba, 2014) optimizer. The learning\nrate was warmed up over the ﬁrst 4,000 steps to a\npeak value of 1e-4, and then it linearly decayed.\nThe remaining pretraining hyperparameters were\nthe same as those of RoBERTa (Liu et al., 2019)\n(see Tab. 12). Additionally, we employed relative\npositional embedding (Shaw et al., 2018) in our\nmodel pretraining because it is necessary for\nreusing hidden state without causing temporal\nconfusion (Dai et al., 2019).\nFinetune. In contrast to previous models, such as\nBERT, RoBERTa, and XLNet, the proposed model\nemploys the retrospective feed mechanism and the\nenhanced recurrence mechanism during the ﬁne-\ntuning phase to fully utilize the advantages of these\ntwo strategies.\n4.2.3 Results on English Tasks\nResults on Long-Text Classiﬁcation Tasks. We\nconsider two datasets: IMDB reviews (Maas\net al., 2011) and Hyperpartisan News Detection\n(HYP) (Kiesel et al., 2019). The former is a widely\nused sentiment analysis dataset containing 50,000\nmovie reviews, labeled as positive or negative. The\nlatter contains news that takes extreme left-wing\nor right-wing standpoints. The documents in HYP\nare extremely long (50% of the samples contain\nmore than 537 tokens) and are thus suitable for\ntesting long-text classiﬁcation ability. Tab. 3 sum-\nmarizes the results of the ERNIE-D OC-Base and\nERNIE-D OC-Large models for long-text classiﬁ-\ncation tasks, and ERNIE-D OC achieves a SOTA\nresult. On IMDB, we observed a modest perfor-\nModels TQA HQA\nF1 Span Supp Joint\nRoBERTa 74.3 73.5 83.4 63.5\nLongformer 75.2 74.3 84.4 64.4\nBigBird 79.5 75.5 87.1 67.8\nERNIE-DOC 80.1 79.4 86.3 70.5\nLongformer-Large 77.8 81.0 85.8 71.4\nBigBird-Large - 81.3 89.4 -\nERNIE-DOC-Large 82.5 82.2 87.6 73.7\nTable 4: Results on TQA and HQA dev dataset for\ndocument-level QA. HQA metrics are F1.\nOpenKP dataset F1@1 F1@3 F1@5\nBLING-KPE (Xiong et al., 2019) 26.7 29.2 20.9\nJointKPE (Sun et al., 2020a) 39.1 39.8 33.8\nETC (Ainslie et al., 2020) - 40.2 -\nERNIE-DOC 40.2 40.5 34.4\nTable 5: Results on OpenKP dev dataset. The baseline\nresults are obtained from corresponding papers under\nno-visual-features setting.\nmance gain compared with RoBERTa. This is be-\ncause nearly 90% of the samples in the dataset\nconsist of fewer than 569 tokens. Unlike on IMDB,\nERNIE-D OC surpasses the baseline models on\nHYP by a substantial margin, demonstrating its\ncapability of utilizing information from a long doc-\nument input. Note that we include XLNet-Large,\nthe previous SOTA pretraining model on the IMDB\ndataset, as the baseline for a large model setting;\nERNIE-D OC achieves a result comparable to that\nof XLNet-Large.\nResults on Document-level Question-\nAnswering Tasks. We utilized two document-\nlevel QA datasets (Wikipedia setting of TriviaQA\n(TQA) (Joshi et al., 2017) and distractor setting of\nHotpotQA (HQA) (Yang et al., 2018)) to evaluate\nthe reasoning ability of the models over long\ndocuments. TQA and HQA are extractive QA\ntasks, and we follow the simple QA model of\nBERT (Devlin et al., 2018) to predict an answer\nwith the maximum sum of start and end logits\nacross multiple segments of a sample. In addition,\nwe use a modiﬁed cross-entropy loss (Clark and\nGardner, 2017) for the TQA dataset and use a\ntwo-stage model (Groeneveld et al., 2020) with the\nbackbone of ERNIE-D OC for the HQA dataset.\nTab. 4. shows that ERNIE-D OC outperforms\nRoBERTa and Longformer by a considerable\nmargin on these two datasets, and is comparable to\ncurrent SOTA long-document model, i.e., BigBird\non HQA in large-size model setting.\nResults on the Keyphrase Extraction Task. We\ninclude OpenKP (Xiong et al., 2019) dataset to eval-\nuate ERNIE-D OC’s ability to extract keyphrases\nfrom a long document. Each document contains\nup to three short keyphrases and we follow the\nmodel setting of JointKPE (Sun et al., 2020a) and\nETC (Ainslie et al., 2020) by applying CNNs on\nBERT’s output to compose n-gram embeddings\nfor classiﬁcation. We report the results of base-\nsize models in Tab. 5 under no-visual-features set-\nting for easy and fair comparison with baselines.\nERNIE-D OC performs stably better on all metrics\non the OpenKP dataset.\n4.2.4 Results on Chinese Tasks\nWe conducted extensive experiments on seven\nChinese natural language understanding (NLU)\ntasks, including machine reading comprehension\n(CMRC2018 (Cui et al., 2018), DRCD (Shao\net al., 2018), DuReader (He et al., 2017), C3 (Sun\net al., 2019a)), semantic similarity (CAIL2019-\nSCM (CAIL) (Xiao et al., 2019)), and long-text\nclassiﬁcation (IFLYTEK (IFK) (Xu et al., 2020),\nTHUCNews (THU)5 (Sun et al., 2016)). The docu-\nments in all the aforementioned datasets are sufﬁ-\nciently long to be used to evaluate the effectiveness\nof ERNIE-D OC on long-context tasks (see detailed\ndatasets statistics in Tab. 9). We reported the mean\nresults with ﬁve runs for the seven Chinese tasks\nin Tab. 6, and summarized the hyperparameters in\nTab. 16. ERNIE-D OC outperforms previous mod-\nels across these Chinese NLU tasks by a signiﬁcant\nmargin in the base-size model group.\n4.2.5 Ablation Studies\nNo. Models TQA HYP\nI ERNIE-DOC 64.56 86.10\nII I w/o segment-reordering 63.59 84.60\nIII II w/o retrospective feed 63.38 83.27\nIV III w/o enhanced recurrence 61.09 81.67\nV IV w/o recurrence 58.35 77.72\nTable 7: Performance of ERNIE-D OC-Small after ab-\nlating each proposed component (F1 result is reported).\nEffect of proposed components . Tab. 7 shows\nthe performance of ERNIE-D OC-Small on two\nEnglish tasks after ablating each proposed compo-\nnent. All models were pretrained and ﬁne-tuned\nwith the same experimental setup, and we report\nthe mean results of ﬁve runs. We observed a stable\nperformance gain across these two tasks by incor-\nporating each proposed component. By comparing\n5We use a subset of THUCNews ( https://github.\ncom/gaussic/text-classification-cnn-rnn ).\nDRCD CMRC2018 DuReader CAIL THU IFK C 3\nModels EM/F1 EM/F1 EM/F1 Acc. Acc. Acc. Acc.\nDev Test Dev Dev Dev Test Dev Test Dev Dev Test\nBERT (Devlin et al., 2018)85.7/91.6 84.9/90.9 66.3/85.9 59.5/73.1 61.9 67.3 97.7 97.3 60.3 65.7 64.5\nBERT-wwm-ext∗ 85.0/91.2 83.6/90.4 67.1/85.7 -/- - - 97.6 97.6 59.4 67.8 68.5\nRoBERTa-wwm-ext∗ 86.6/92.5 85.2/92.0 67.4/87.2 -/- - - - - 60.3 67.1 66.5\nMacBERT (Cui et al., 2020a)88.3/93.5 87.9/93.2 69.5/87.7 -/- - - - - - - -\nXLNet-zh (Cui et al., 2020b)83.2/92.0 82.8/91.8 63.0/85.9 -/- - - - - - - -\nERNIE 1.0 (Sun et al., 2019b)84.6/90.9 84.0/90.5 65.1/85.1 57.9/72/1 - - 97.7 97.3 59.0 65.5 64.1\nERNIE 2.0 (Sun et al., 2020b)88.5/93.8 88.0/93.4 69.1/88.6 61.3/74.9 64.9 67.9 98.0 97.5 61.7 72.3 73.2\nERNIE-DOC 90.5/95.2 90.5/95.1 76.1/91.6 65.8/77.9 65.6 68.8 98.3 97.7 62.4 76.5 76.5\nTable 6: Results on seven Chinese NLU tasks for ERNIE-D OC-base model.The results of the models with ”∗” are\nfrom Cui et al. (2019). The XLNet-zh is the abbreviation of Chinese-XLNet. Notably, the result of BERT on CAIL\nwas obtained from Xiao et al. (2019), where BERT was post-pretrained with a legal dataset.\nNo.IV and No.V , we see that segment-level recur-\nrence is necessary for modeling long documents\nand produces 2.74 and 3.95 % points improvement\non the TQA and HYP dateset, respectively. More-\nover, a substantial improvement is achieved using\nthe enhance recurrence mechanism (2.29% point\non TQA and 1.40% point on HYP, see No.III - IV).\nRetrospective feed mechanism further improves\n0.21% point on TQA and 1.33% point on HYP\n(No.II - No.III). Considering different types of\ntasks, we observe that on HYP, an extremely long\ntext classiﬁcation dataset, a substantial improve-\nment is achieved using the segment-reordering ob-\njective (1.5% point). This indicates that the[CLS]\ntoken, pretrained using the segment-reordering ob-\njective, is more adaptable to the document-level\ntext classiﬁcation task.\n10 20 30 40\nsteps (10K)\n3.2\n3.4\n3.6\n3.8log(ppl)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nacc\nours_w/o_en_recur (max-len:128)\nours_w/o_en_recur (max-len:512)\nours (max-len:128)\nours (max-len:512)\nFigure 4: Acc. (dotted line) and PPL (solid line) met-\nrics for variants of our small models with different max-\nimum sequence length during pretraining.\nEffect of enhanced recurrence mechanism with\nregard to different maximum sequence lengths.\nAs depicted in Fig. 4, the enhanced recurrence\nmechanism plays an important role in pretraining\nan effective language model with lower PPL and\nhigher accuracy under both the maximum sequence\ninput lengths of 128 and 512. The effect of the\nenhanced recurrence mechanism is more signiﬁ-\ncant under a smaller maximum sequence length,\neven makes the ERNIE-D OC-Small (max-len:128)\ncomparable to ERNIE-D OC-Small w/o en recur\n(max-len:512) w.r.t accuracy. This intriguing prop-\nerty of the enhanced recurrence mechanism enables\nmore efﬁcient model training and inference by re-\nducing maximum sequence length while remaining\ncomparable modeling capability.\n5 Conclusion\nIn this paper, we proposed ERNIE-D OC, a\ndocument-level language pretraining model based\non the Recurrence Transformers paradigm. Two\nwell-designed mechanisms, namely the retrospec-\ntive feed mechanism and the enhanced recurrent\nmechanism, enable ERNIE-D OC, which theoreti-\ncally has the longest possible dependency, to model\nbidirectional contextual information of a complete\ndocument. Additionally, ERNIE-D OC is pre-\ntrained with a document-aware segment-reordering\nobjective to explicitly learn the relationship among\nsegments of a long context. Experiments on var-\nious downstream tasks demonstrate that ERNIE-\nDOC outperforms existing strong pretraining mod-\nels such as RoBERTa, Longformer, and BigBird\nand achieves SOTA results on several language\nmodeling and language understanding benchmarks.\nIn future studies, we will evaluateERNIE-D OC on\nlanguage generation tasks, such as generative ques-\ntion answering and text summarization. We will\nalso investigate its potential applicability in other\nareas, such as computational biology. Another pos-\nsibility is to incorporate graph neural networks into\nERNIE-D OC to enhance its modeling capability\nfor tasks that require multi-hop reasoning and long-\ndocument modeling ability.\nAcknowledgments\nThis work was supported by the National Key Re-\nsearch and Development Project of China (No.\n2018AAA0101900).\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. Etc: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284.\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv preprint arXiv:1809.10853.\nHe Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan,\nKun Xiong, Wen Gao, and Ming Li. 2020. Segatron:\nSegment-aware transformer for language modeling\nand understanding.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers. CoRR, abs/1904.10509.\nChristopher Clark and Matt Gardner. 2017. Simple\nand effective multi-paragraph reading comprehen-\nsion. arXiv preprint arXiv:1710.10723.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020a. Revisiting pre-\ntrained models for chinese natural language process-\ning. arXiv preprint arXiv:2004.13922.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020b. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nFindings, pages 657–668, Online. Association for\nComputational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao,\nZhipeng Chen, Wentao Ma, Shijin Wang, and Guop-\ning Hu. 2018. A span-extraction dataset for chinese\nmachine reading comprehension. arXiv preprint\narXiv:1810.07366.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. CoRR, abs/1901.02860.\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In International conference on\nmachine learning, pages 933–941. PMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a con-\ntinuous cache. arXiv preprint arXiv:1612.04426.\nDirk Groeneveld, Tushar Khot, Ashish Sabharwal, et al.\n2020. A simple yet strong pipeline for hotpotqa.\narXiv preprint arXiv:2004.06753.\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi\nZhao, Xinyan Xiao, Yuan Liu, Yizhong Wang,\nHua Wu, Qiaoqiao She, et al. 2017. Dureader:\na chinese machine reading comprehension dataset\nfrom real-world applications. arXiv preprint\narXiv:1711.05073.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. Semeval-\n2019 task 4: Hyperpartisan news detection. In Pro-\nceedings of the 13th International Workshop on Se-\nmantic Evaluation, pages 829–839.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates.\n2020. Pretrained transformers for text ranking: Bert\nand beyond. arXiv preprint arXiv:2010.06467.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the as-\nsociation for computational linguistics: Human lan-\nguage technologies, pages 142–150.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language\nmodeling at multiple scales. arXiv preprint\narXiv:1803.08240.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. CoRR, abs/1609.07843.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nand Timothy P. Lillicrap. 2019. Compressive trans-\nformers for long-range sequence modelling. CoRR,\nabs/1911.05507.\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng,\nand Sam Tsai. 2018. Drcd: a chinese machine\nreading comprehension dataset. arXiv preprint\narXiv:1806.00920.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie.\n2019a. Probing prior knowledge needed in challeng-\ning chinese machine reading comprehension. CoRR,\nabs/1904.09679.\nM Sun, J Li, Z Guo, Z Yu, Y Zheng, X Si, and Z Liu.\n2016. Thuctc: an efﬁcient chinese text classiﬁer.\nGitHub Repository.\nSi Sun, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu,\nand Jie Bao. 2020a. Joint keyphrase chunking\nand salience ranking with bert. arXiv preprint\narXiv:2004.13639.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2020b.\nErnie 2.0: A continual pre-training framework for\nlanguage understanding. In AAAI, pages 8968–\n8975.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019b. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-\nCheng Juan. 2020. Sparse sinkhorn attention. arXiv\npreprint arXiv:2002.11296.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30:5998–6008.\nJohannes Welbl, Pontus Stenetorp, and Sebastian\nRiedel. 2018. Constructing datasets for multi-hop\nreading comprehension across documents. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:287–302.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao\nTu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang,\nXianpei Han, Heng Wang, Jianfeng Xu, et al. 2019.\nCail2019-scm: A dataset of similar case matching in\nlegal domain. arXiv preprint arXiv:1911.08962.\nLee Xiong, Chuan Hu, Chenyan Xiong, Daniel Cam-\npos, and Arnold Overwijk. 2019. Open domain\nweb keyphrase extraction beyond language model-\ning. arXiv preprint arXiv:1911.02671.\nLiang Xu, Xuanwei Zhang, Lu Li, Hai Hu, Chen-\njie Cao, Weitang Liu, Junyi Li, Yudong Li, Kai\nSun, Yechen Xu, et al. 2020. Clue: A chinese lan-\nguage understanding evaluation benchmark. arXiv\npreprint arXiv:2004.05986.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nZihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang. 2019. Bp-transformer: Modelling\nlong-range context via binary partitioning. arXiv\npreprint arXiv:1911.04070.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. Advances in neural information process-\ning systems.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. Hi-\nbert: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. arXiv preprint arXiv:1905.06566.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27.\nA Appendices\nA.1 Tasks\nFollowing previous work, we evaluate ERNIE-\nDOC on various tasks that require the ability to\nmodel a long document.\nDocument-level Language Modeling Task. We\nemploy WikiText-103 (Merity et al., 2016) in\nlanguage modeling experiments. WikiText-103 is\nthe largest available word-level benchmark with\nlong-term dependency for language modeling,\nwhich consists of 28K articles, where each article\nhas 3.6K tokens on average, thus 103M training\ntokens in total.\nLong Text classiﬁcation . We consider two En-\nglish datasets: IMDB reviews (Maas et al., 2011)\nand Hyperpartisan news detection (Kiesel et al.,\n2019) (see Tab. 8), and two Chinese datasets: IFLY-\nTEK (Xu et al., 2020) and THUCNews (Sun et al.,\n2016) (see Tab. 9). IMDB is a widely used senti-\nment analysis dataset containing 50,000 movie re-\nviews labeled as positive or negative. Training and\ndev dataset is equally split. Hyperpartisan contains\nnews that takes an extreme left-wing or right-wing\nstandpoint. Documents are extremely long in Hy-\nperpartisan which makes it a good test for long text\nclassiﬁcation. We use the same split as Longformer\nby dividing 654 documents into train/dev/test sets.\nIFLYTEK contains 17,332 app descriptions. The\ntask is to assign each description into one of 119\ncategories, such as food, car rental and education.\nTHUCNews is generated by ﬁltering historical data\nof Sina News RSS subscription channel from 2005\nto 2011, including 740,000 news documents and\n14 categories. In this paper, we employ the subset\nversion instead of the full one 6, which contains 10\ncategories, each with 5,000 pieces of data.\nFor the above four long text classiﬁcation\ndatasets, we concatenate [CLS] token with\neach segment and takes as input multiple seg-\nments of a text sequentially. Each segment\nis generated by slicing the text with a sliding\nwindow of 128 tokens. We apply binary cross\nentropy loss on the [CLS] token of the last segment.\nLong Text Semantic Similarity.Considering that\nthere is no available long text semantic similarity\ndataset in English, we evaluate the effectiveness\n6The subset version is also released and can be downloaded\nfrom the ofﬁcial website of THUCTC.\nof ERNIE-D OC on semantic similarity task only\ndepending on Chinese dataset CAIL2019-SCM.\nAccording to Xiao et al. (2019), CAIL2019-SCM\nis a sub-task of the Chinese AI and Law Challenge\n(CAIL) competition in 2019, which contains\n8,964 triplets of legal documents collected from\nChina Judgments Online. Every document in a\nmajority of triplet has more than 512 characters,\ntherefore, the total length of a triplet is quite long.\nCAIL2019-SCM requires researchers to decide\nwhich two cases are more similar in a triplet.\nSpeciﬁcally, given a triplet (A,B,C ), where A, B,\nC are fact descriptions of three cases. The model\nneeds to predict whether sim(A,B) >sim(A,C)\nor sim(A,C) > sim(A,B), in which sim\ndenotes the similarity between two cases. Instead\nof separately feeding the document A, B, C\ninto the model to get the feature h, we use the\ncombinations of (A,B) and (A,C) as input. We\ngenerate multiple segments for (A,B) or (A,C)\nwith a sliding window of 128 tokens and feed them\nas input sequentially. The binary cross entropy\nloss is applied to the difference of [CLS] token\noutput of each segment.\nDocument-level Question answering. We utilize\ntwo English question answering datasets (Trivi-\naQA (Joshi et al., 2017), HotpotQA (Yang et al.,\n2018)) (see Tab. 8) and four Chinese question an-\nswering datasets (CMRC2018 (Cui et al., 2018),\nDRCD (Shao et al., 2018), DuReader (He et al.,\n2017), C3 (Sun et al., 2019a)) (see Tab. 9) to evalu-\nate models’ reasoning ability over long documents.\nTriviaQA is a large scale QA dataset that\ncontains over 650K question-answer pairs. We\nevaluate models on its Wikipedia setting where\ndocuments are Wikipedia articles, and answers\nare named entities mentioned in multiple docu-\nments. The dataset is distantly supervised mean-\ning that there is no golden span, thus we ﬁnd\nall superﬁcial identical answers in provided docu-\nments7. We use the following input format for each\nsegment: “[CLS] context [q] question\n[/q]” where context is generated by slicing multi-\ndocuments input with a sliding window of 128 to-\nkens. We take as input multiple segments of a sam-\nple sequentially and attach a linear layer to each\ntoken in a segment to predict the answer span. We\n7We use the same preprocessing code for TriviaQA\ndataset as BigBird, see https://github.com/\ntensorflow/models/blob/master/official/\nnlp/projects/triviaqa/preprocess.py\nDatasets IMDB Hyperpartisan TriviaQA HotpotQA OpenKP\nsplit train dev train dev test train dev train dev train dev\n# samples 25,000 2,000 516 64 65 61,888 7,993 90,432 7,404 134,894 6,616\n# tokens of context length in each percentile using RoBERTa wordpiece tokenizer\n50% 215 212 537 521 639 8,685 8,586 1,279 1,325 894 681\n90% 569 550 1,519 1,539 1,772 25,207 24,825 1,725 1,785 3,451 2,734\n95% 745 724 1,997 1,979 1,994 32,018 32,132 1,888 1,943 5,340 4,130\nmax 3,084 2,778 5,566 2,643 5,566 173,302 146,012 3,733 3,618 105,548 43,609\nTable 8: English Datasets statistics.\nDatasets IFLYTEK THUCNews CAIL CMRC2018 DuReader C 3 DRCD\nsplit train dev train dev train dev train dev train dev train dev train dev test\n# samples 12,133 2,599 50,000 5,000 5,102 1,500 10,121 3,219 15,763 1,628 11,869 3,816 26,936 3,524 3,493\n# tokens of context length in each percentile using BERT tokenizer\n50% 243 242 656 579 1,837 1,834 423 426 163 182 96 89 397 421 405\n90% 507 508 1,821 1,599 1,965 1,962 745 771 550 567 591 554 616 666 626\n95% 563 560 2,455 2,245 2,008 1,995 827 840 652 667 697 692 709 740 736\nmax 3,153 1,698 26,659 9,128 2,400 2,310 970 961 1,021 854 1,534 1,167 1,678 989 950\nTable 9: Chinese Datasets statistics.\nuse a modiﬁed cross entropy loss (Clark and Gard-\nner, 2017) assuming that each segment contains at\nleast one correct answer span. The ﬁnal prediction\nfor each question is a span with the maximum sum\nof start and end logit across multiple segments.\nHotpotQA is a QA dataset where golden\nspans of an answer and sentence-level supporting\nfacts are provided. Thus, it contains two tasks\nnamely, answer span prediction and support-\ning facts prediction. In the distractor setting,\neach question is associated with 10 documents\nwhere only 2 documents contain supporting\nfacts. It requires the model to ﬁnd and reason\nover multiple documents to ﬁnd answers, and\nexplain the predicted answers using predicted\nsupporting facts. Following Groeneveld et al.\n(2020), we implemented a two-stage model based\non ERNIE-D OC and use the following input\nformat for each segment: “ [CLS] title1\n[p] sent1,1 [SEP] sent1,2 [SEP] ...\ntitle2 [p] sent2,1 [SEP] sent2,2\n[SEP] ... [q] question [/q] ” For\nevidence prediction, we apply 2 layer feedforward\nnetworks over the special token [SEP] and [p]\nrepresenting a sentence and a paragraph separately.\nThen we use binary cross entropy loss to do binary\nclassiﬁcation. For answer span prediction, we train\nthe model with a multi-task objective: 1) question\ntype (yes/no/span) classiﬁcation on the [CLS]\ntoken. 2) supporting evidence prediction on [SEP]\nand [p]. 3) span prediction on the start and end\ntoken of a golden span.\nCMRC2018, DRCD and DuReader are com-\nmon Chinese QA datasets with same format, which\nhave been evaluated in numerous popular pretrain-\ning models, such as BERT (Devlin et al., 2018),\nERNIE 1.0 (Sun et al., 2019b), ERNIE 2.0 (Sun\net al., 2020b) and etc. The detailed descriptions\nof three datasets can refer to Cui et al. (2018),\nShao et al. (2018) and He et al. (2017). We\nadopt the same input format as TriviaQA for each\nsegment, denotes as “[CLS] context [SEP]\nquestion [SEP]“ where context is generated\nby slicing multi-documents input with a sliding\nwindow of 128 tokens. We take as input multiple\nsegments of a sample sequentially and attach a lin-\near layer to each token in a segment to predict the\nanswer span. Then, we apply a softmax and use\nthe cross entropy loss with the correct answer. The\nﬁnal prediction for each question is a span with\nthe maximum sum of start and end logit across\nmultiple segments.\nThe multiple Choice Chinese machine reading\nComprehension dataset (C 3) (Sun et al., 2019a)\nis the ﬁrst Chinese free-form multi-choice\ndataset where each question is associated with\nat most four choices and a single document.\nAccording to (Sun et al., 2019a), msegments are\nconstructed for a question, in which m denotes\nthe number of choice for that question. We\nuse the following input format for each seg-\nment: “[CLS] context [SEP] question\n[SEP] choicei [SEP] ” where context is\ngenerated by slicing document input with a sliding\nwindow of 128 tokens stride. We take as input\nmultiple segments of a sample in a single batch\nand attach a linear layer to [CLS] that outputs\nan unnormalized logit. Then we obtain the ﬁnal\nprediction for a question by applying a softmax\nlayer over the unnormalized logits of all choices\nQA Classiﬁcation\nModels\\Dataset TriviaQA HotpotQA IMDB Hyperpartisan Avg.\n#0 ERNIE-DOC 64.56 50.85 93.14 86.10 73.66\n#1 w/o so 63.59 50.04 93.15 84.60 72.85\n#2 w/o so&retro 63.38 49.87 92.56 83.27 72.27\n#3 w/o so&retro&en-rec 61.09 44.05 92.07 81.67 69.72\n#4 w/o so&retro&recur 58.35 31.54 91.60 77.72 64.80\nTable 10: Performance of ERNIE-D OC-small after ablating each proposed component. ( so denotes the segment-\nreordering objective, re denotes the retrospective feed mechanism, en-rec denotes the enhanced recurrence mech-\nanism, and recur denotes the segment-level recurrence module. We used the Acc. metric for IMDB, F1 metric for\nTriviaQA and Hyperpartisan, Joint-F1 for HotpotQA.)\nassociated with it.\nKeyphrase Extraction . We include\nOpenKP (Xiong et al., 2019) dataset 8 to evaluate\nERNIE-D OC’s ability to extract keyphrases\nfrom a long document. Each document contains\nup to three short keyphrases and we follow the\nmodel setting of JointKPE (Sun et al., 2020a) and\nETC (Ainslie et al., 2020) by applying CNNs on\nBERT’s output to compose n-gram embeddings for\nclassiﬁcation. We clean the dataset by removing\nsome nonsense words such as the HTTP links. In\ndetail, we apply ﬁve CNNs on BERT’s output with\nthe kernel size ranging from 1 to 5. Since each\nword is composed of several sub-tokens, we take\nthe ﬁrst token’s embedding as the input for CNNs.\nFinally, we use the binary cross entropy loss as the\noptimization objective.\nA.2 Ablation Studies\nTab. 10 shows the performance of ERNIE-D OC-\nSmall on English tasks after ablating each proposed\ncomponent. All models were pretrained and ﬁne-\ntuned with the same experimental setup, and we\nreport the mean results of ﬁve runs. In the last col-\numn in Tab. 10, we see that the segment-reordering\nobjective is improved ERNIE-D OC by 0.81% on\naverage (#1 - #0), the retrospective feed mecha-\nnism is improved ERNIE-D OC by an average of\n0.58% (#2 - #1), and the enhanced recurrence mech-\nanism makes a large contribution of 2.55 percent-\nage points on average (#3 - #2). By comparing #3\nwith #4, we see that segment-level recurrence is\nnecessary for modeling long documents and pro-\nduces a 4.92 percentage point improvement on av-\nerage. Considering different types of tasks, we\nobserve that on Hyperpartisan, an extremely long\ntext classiﬁcation dataset, a substantial improve-\nment is achieved using the segment-reordering ob-\n8The dataset can be downloaded from https://\ngithub.com/thunlp/BERT-KPE\njective (1.5% point). This indicates that the[CLS]\ntoken, pretrained using the segment-reordering ob-\njective, is more adaptable to the document-level\ntext classiﬁcation task. Moreover, we observed a\nstable performance gain across all tasks using the\nenhanced recurrence mechanism.\nA.3 Hyperparameters for Language\nModeling\nIn Tab. 11, we present the detailed hyperparameters\nused for our experiments, which are the same as\nthe hyperparameters employed in Transformer-XL\n(Dai et al., 2019).\nHyperparameters WikiText-103\nBase\nWikiText-103\nLarge\nLayers 16 18\nHidden size 410 1,024\nAttention heads 10 16\nTraining sequence length 150 384\nTraining memory length 150 384\nTesting sequence length 64 128\nTesting sequence length 640 1,600\nBatch size 64 128\nLearning rate 2.5e-4 2.5e-4\nWarmup steps 0 16,000\nTraining steps 200k 400k\nTable 11: Hyperparameters used for WikiText-103.\nA.4 Hyperparameters for Pre-Training\nAs shown in Tab. 12, we present the detailed\nhyperparameters adopted to pretraining ERNIE-\nDOC on English text corpora and Chinese text cor-\npora. For comparisons, we follow the same op-\ntimization hyperparameters of RoBERTa BASE or\nRoBERTaLARGE (Liu et al., 2019) for base-size or\nlarge-size model in English domain. As for Chinese\nERNIE-D OC, we follow the same optimization hy-\nperparameters of ERNIE 2.0BASE.\nA.5 Hyperparameters for Fine-Tuning\nA.5.1 Long Text Classiﬁcation tasks\nThe ﬁnetuning hyperparameters for IMDB (Maas\net al., 2011) and Hyperpartisan (Kiesel et al., 2019)\nHyperparameters English Chinese\nBASE LARGE BASE\nLayers 12 24 12\nHidden size 768 1,024 768\nAttention heads 12 16 12\nTraining steps 400K 100K 300K\nBatch size 2,560 3,920 2,560\nLearning rate 1e-4 1e-4 1e-4\nWarmup steps 4,000 4,000 4,000\nAdam (beta1,beta2)(0.9, 0.999) (0.9, 0.999) (0.9, 0.999)\nAdam (epsilon) 1e-6 1e-6 1e-6\nLearning rate scheduleLinear Linear Linear\nWeight decay 0.01 0.01 0.01\nDropout 0.1 0.1 0\nGPU (Nvidia V100) 40 80 40\nTable 12: Hyperparameters used for ERNIE-D OC pre-\ntraining.\nare presented in Tab. 13.\nHyperparameters BASE LARGE\nIMDB HYP IMDB HYP\nBatch size 32 32 32 16\nLearning rate 7e-5 1e-4 1e-5 4e-6\nEpochs 3 15 3 15\nLR schedule linear linear linear linear\nLayerwise LR decay1 0.7 0.9 1\nWarmup proportion0.1 0.1 0.1 0.1\nWeight decay 0.01 0.01 0.01 0.01\nTable 13: Hyperparameters used for ﬁnetuning on\nIMDB and Hyperpartisan (HYP).\nA.5.2 Document-level Question answering\ntasks\nThe ﬁnetuning hyperparameters for TriviaQA\n(Welbl et al., 2018) and HotpotQA (Yang et al.,\n2018) are presented in Tab. 14. HQA-sent. is the\nmodel for coarse-grained evidence prediction, and\nwe choose the evidence with the probability larger\nthan a pre-deﬁned threshold 1e-3 and 1e-5 for base\nand large models, respectively. HQA-span. is the\nmodel for span prediction.\nA.5.3 Keyphrase Extraction task\nThe ﬁnetuning hyperparameters for the OpenKP\n(Xiong et al., 2019) dataset are presented in Tab. 15.\nHyper. BASE LARGETQA HQA-sent. HQA-span.TQA HQA-sent. HQA-span.Batch size64 128 128 64 32 32Learning rate3e-5 3e-5 1.5e-4 5e-6 5e-6 1.5e-5Epochs 5 6 6 3 5 5LR schedulelinear linear linear linear linear linearLayer-decay0.8 1 0.8 0.9 0.9 0.9Warmup prop.0.1 0.1 0.1 0.1 0.1 0.1Weight decay0.01 0.01 0.01 0.01 0.01 0.01\nTable 14: Finetuning hyperparameters on the TQA and\nHQA for base- and large-size ERNIE-D OC.\nHyperparametersOpenKP\nBatch size 32\nLearning rate 1.5e-4\nEpochs 5\nLR schedule linear\nLayerwise LR decay0.8\nWarmup proportion 0.1\nWeight decay 0.01\nTable 15: Finetuning hyperparameters on the OpenKP\nfor base-size ERNIE-D OC.\nA.5.4 Chinese NLU tasks\nTab. 16 lists the ﬁnetuning hyperparameters for\nChinese NLU tasks including IFLYTEK (Xu et al.,\n2020), THUCNews (Sun et al., 2016), CMRC2018\n(Cui et al., 2018), DRCD (Shao et al., 2018),\nDuReader He et al. (2017), C3 (Sun et al., 2019a)\nand CAIL2019-SCM (Xiao et al., 2019).\nTasks Batch\nsize\nLearning\nrate Epochs Dropout\nDRCD 64 2.25-4 5 0.1\nCMRC2018 64 1.75e-4 5 0.2\nDuReader 64 2.75e-4 5 0.1\nC3 24 1e-4 8 0.1\nCAIL 48 5e-5 15 0.1\nTHU 16 1.5e-4 16 0.1\nIFK 16 1.5e-4 5 0.1\nTable 16: Hyperparameters used for ﬁnetuning on Chi-\nnese NLU tasks. Note that the warmup proportion are\nset to 0.1 and the layerwise learning rate decay rate are\nset to 0.8 for all tasks.\nB Attention Complexity\nGiven a long document with length L, Longformer\nand BigBird usually applies a local attention with\na window size of 512 tokens on the entire input\nresulting in L∗512 token-to-token calculations.\nWhile the long document is fed twice as input and\neach input is sliced with a sliding window size\nof 512 tokens in ERNIE-D OC, which resulting\nin 2 ∗ L\n512 ∗512 ∗(512 + m) token-to-token cal-\nculations where m is the memory length. Since\n512 ≪Land m ≪L, the attention complexity\nof ERNIE-D OC is comparable to Longformer and\nBigBird which scales linearly with respect to the\ninput length L, i.e., O(L). Notably, the segments\nproduced from the long document are fed one by\none in ERNIE-D OC, leading to the lower spatial\ncomplexity."
}