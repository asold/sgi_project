{
  "title": "Potential of Large Language Models in Generating Multiple-Choice Questions for the Japanese National Licensure Examination for Physical Therapists",
  "url": "https://openalex.org/W4407631812",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2115602132",
      "name": "Shogo Sawamura",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3118253150",
      "name": "Kengo Kohiyama",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2614889816",
      "name": "Takahiro Takenaka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3083937935",
      "name": "Tatsuya Sera",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2567323175",
      "name": "Tadatoshi Inoue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103512686",
      "name": "Takashi Nagai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2887398393",
    "https://openalex.org/W4399933304",
    "https://openalex.org/W4401723006",
    "https://openalex.org/W4401406517",
    "https://openalex.org/W4402055482",
    "https://openalex.org/W4376866715",
    "https://openalex.org/W4391723926",
    "https://openalex.org/W4399387113",
    "https://openalex.org/W4386249608",
    "https://openalex.org/W4391225179",
    "https://openalex.org/W4390229778",
    "https://openalex.org/W4402567773",
    "https://openalex.org/W4391061229",
    "https://openalex.org/W4386270014",
    "https://openalex.org/W4400642441",
    "https://openalex.org/W4385972264",
    "https://openalex.org/W4389989133"
  ],
  "abstract": "Introduction This study explored the potential of using large language models (LLMs) to generate multiple-choice questions (MCQs) for the Japanese National Licensure Examination for Physical Therapists. Specifically, it evaluated the performance of a customized ChatGPT (OpenAI, San Francisco, CA, USA) model named \"Physio Exam Generative Pre-trained Transformers (GPT)\" in generating high-quality MCQs in non-English contexts. Materials and methods Based on the data extracted from the 57th and 58th Japanese National Licensure Examination for Physical Therapists, 340 MCQs, including correct answers, explanations, and associated topics, were incorporated into the knowledge base of the GPTs. The prompts and outputs were conducted in Japanese. The generated MCQs covered major topics in general (anatomy, physiology, and kinesiology) and practical questions (musculoskeletal disorders, central nervous system disorders, and internal organ disorders). The quality of the MCQs and their explanations were evaluated by two independent reviewers using a 10-point Likert scale across five criteria: clarity, relevance to clinical practice, suitability of difficulty, quality of distractors, and adequacy of rationale. Results The generated MCQs achieved 100% accuracy for both general and practical questions. The average scores across the evaluation criteria ranged from 7.0 to 9.8 for general questions and 6.7 to 9.8 for practical questions. Although some areas exhibited lower scores, the overall results were favorable. Conclusions This study demonstrates the potential of LLMs to efficiently generate high-quality MCQs, even in non-English environments such as Japanese. These findings suggest that LLMs can adapt to diverse linguistic settings, reduce educators' workload, and improve the quality of educational resources. These results lay a foundation for expanding the application of LLMs to educational settings across non-English-speaking regions.",
  "full_text": null,
  "topic": "Licensure",
  "concepts": [
    {
      "name": "Licensure",
      "score": 0.679981529712677
    },
    {
      "name": "Medicine",
      "score": 0.6547770500183105
    },
    {
      "name": "Medical education",
      "score": 0.5077551603317261
    },
    {
      "name": "CLARITY",
      "score": 0.5035142302513123
    },
    {
      "name": "Comprehension",
      "score": 0.48597538471221924
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4579469561576843
    },
    {
      "name": "Likert scale",
      "score": 0.4265047013759613
    },
    {
      "name": "Psychology",
      "score": 0.20322006940841675
    },
    {
      "name": "Linguistics",
      "score": 0.1426854431629181
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802022162",
      "name": "Heisei College of Health Sciences",
      "country": "JP"
    }
  ]
}