{
  "title": "Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer",
  "url": "https://openalex.org/W2982696980",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5085516032",
      "name": "Genta Indra Winata",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5084836777",
      "name": "Samuel Cahyawijaya",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5021691940",
      "name": "Zhaojiang Lin",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100623107",
      "name": "Zihan Liu",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5065856469",
      "name": "Pascale Fung",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6675365184",
    "https://openalex.org/W2963070863",
    "https://openalex.org/W2627092829",
    "https://openalex.org/W2949975180",
    "https://openalex.org/W2972470300",
    "https://openalex.org/W2395416438",
    "https://openalex.org/W2989143494",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W1526236009",
    "https://openalex.org/W2936078256",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2058641082",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6767215405",
    "https://openalex.org/W2972736078",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W6736198902",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970418186",
    "https://openalex.org/W2963385194"
  ],
  "abstract": "Highly performing deep neural networks come at the cost of computational complexity that limits their practicality for deployment on portable devices. We propose the low-rank transformer (LRT), a memory-efficient and fast neural architecture that significantly reduces the parameters and boosts the speed of training and inference for end-to-end speech recognition. Our approach reduces the number of parameters of the network by more than 50% and speeds up the inference time by around 1.35x compared to the baseline transformer model. The experiments show that our LRT model generalizes better and yields lower error rates on both validation and test sets compared to an uncompressed transformer model. The LRT model outperforms those from existing works on several datasets in an end-to-end setting without using an external language model or acoustic data.",
  "full_text": "LIGHTWEIGHT AND EFFICIENT END-TO-END SPEECH RECOGNITION\nUSING LOW-RANK TRANSFORMER\nGenta Indra Winata⋆ Samuel Cahyawijaya⋆ Zhaojiang Lin Zihan Liu Pascale Fung\nCenter for Artiﬁcial Intelligence Research (CAiRE)\nThe Hong Kong University of Science and Technology\n{giwinata, scahyawijaya, zlinao, zliucr}@connect.ust.hk\nABSTRACT\nHighly performing deep neural networks come at the cost of\ncomputational complexity that limits their practicality for de-\nployment on portable devices. We propose the low-rank trans-\nformer (LRT), a memory-efﬁcient and fast neural architecture\nthat signiﬁcantly reduces the parameters and boosts the speed\nof training and inference for end-to-end speech recognition.\nOur approach reduces the number of parameters of the net-\nwork by more than 50% and speeds up the inference time\nby around 1.35x compared to the baseline transformer model.\nThe experiments show that our LRT model generalizes bet-\nter and yields lower error rates on both validation and test\nsets compared to an uncompressed transformer model. The\nLRT model outperforms those from existing works on sev-\neral datasets in an end-to-end setting without using an exter-\nnal language model or acoustic data.\nIndex Terms— transformer, low-rank, speech recogni-\ntion, end-to-end, model compression\n1. INTRODUCTION\nEnd-to-end automatic speech recognition (ASR) models have\nshown great success in replacing traditional hybrid HMM-\nbased models by integrating acoustic, pronunciation, and\nlanguage models into a single model structure. They rely\nonly on paired acoustic and text data, without additional\nacoustic knowledge, such as from phone sets and dictionar-\nies. There are two main kinds of end-to-end encoder-decoder\nASR architectures. The ﬁrst is RNN-based sequence-to-\nsequence (Seq2Seq) models with attention [1, 2], which learn\nthe alignment between sequences of audio and their corre-\nsponding text. The second [3, 4] applies a fully-attentional\nfeed-forward architecture transformer [5], which improves on\nRNN-based ASR in terms of performance and training speed\nwith a multi-head self-attention mechanism and parallel-in-\ntime computation. However, the modeling capacity of both\n⋆Equal contributions. This work has been partially funded by\nITF/319/16FP and MRP/055/18 of the Innovation Technology Commission,\nthe Hong Kong SAR Government, and School of Engineering Ph.D. Fellow-\nship Award, HKUST, and RDC 1718050-0 of EMOS.AI.\nLRMHA\nLRFF\nLRMHA\nMasked\nLRMHA\nSpectrogram\nVGG\nOutputs\n(shifted right)\nCharacter\nEmbeddings\nM x\nx N\nLRFF\nLinear\nPredictions\n(Grapheme)\nPositional\nEncoding\nSoftmax\nFig. 1. Low-Rank Transformer Architecture.\napproaches relies on a large number of parameters. Scaling\nup the model’s size increases the computational overhead,\nwhich limits its practicality for deployment on portable de-\nvices without connectivity and slows down both the training\nand inference processes.\nWe propose a novel factorized transformer-based model\narchitecture, the low-rank transformer (LRT), to reduce the\nnumber of parameters in the transformer model by replacing\nlarge high-rank matrices with low-rank matrices to eliminate\nthe computational bottleneck. It optimizes the space and time\ncomplexity of the original model when we choose the factor-\nization rank that is relatively smaller than the original matrix\ndimensions. We design the LRT by taking the idea of the\nautoencoder that compresses the high-dimensional data in-\nput into a compressed vector representation. And, it decodes\nback to a high-rank matrix to learn latent space representa-\ntions of the high-rank matrix. This approach is considered\nan in-training compression method, where we compress the\nparameters of the model prior to the training process. Our\ncontributions are as follows.\narXiv:1910.13923v3  [cs.CL]  14 Feb 2020\nLED\nhead h\nhead 2\nhead 1\nLED\nVKQ\nLED LED\nScaled Dot-Product Attention\nConcatenate\nOutput\nresidual\nconnection\nLayer Norm\nLayer Norm\nLED\nLED\nReLU\nx\nresidual \nconnection \n\u0000\n\u0000\n\u0000×\u0000\n\u0000×\u0000\n\u0000×\u0000\nFig. 2. Low-Rank Transformer Unit. Left: Low-Rank Multi-head Attention (LRMHA), Center: Low-Rank Feed-forward\n(LRFF), and Right: Linear Encoder-Decoder (LED).\n• We introduce a novel lightweight transformer archi-\ntecture leveraging low-rank matrices that outperforms\nthose from existing baselines on the AiShell-1 and\nHKUST test sets in an end-to-end setting.\n• We successfully reduce the inference time by up to\n1.35x speed-up in the GPU and 1.23x speed-up in the\nCPU by shrinking the number of parameters by more\nthan 50% from the baseline.\n• Interestingly, based on our experiments, we show that\nour LRT model generalizes better and yield lower error\nrates on both validation and test performance compared\nto an uncompressed transformer model.\n2. RELATED WORK\n2.1. Low-Rank Model Compression\nTraining end-to-end deep learning ASR models requires high\ncomputational resources and long training time to make con-\nvergence possible. [6] proposed a low-rank matrix factoriza-\ntion of the ﬁnal weighting layer, which reduced the number\nof parameters by up to 31% on a large-vocabulary continuous\nspeech recognition task. [7] introduced a reinforcement learn-\ning method to compress the ASR model iteratively and learn\ncompression ranks, but it requires more than a week to train.\nIn another line of work, a post-training compression method\non LSTM using non-negative matrix factorization was pro-\nposed by [8] to compress large pre-trained models. How-\never, this technique does not speed-up the training process.\nThe aforementioned approaches reduce the number of model\nparameters while keeping the performance loss low. In this\nwork, we extend idea of the in-training compression method\nproposed in [9] by implementing low-rank units on the trans-\nformer model [5], which is suitable for effectively shrinking\nthe whole network size and at the same time, reducing the\ncomputational cost in training and evaluation, with improve-\nments in the error rate.\n2.2. End-to-end Speech Recognition\nCurrent end-to-end automatic speech recognition models are\nof two main types: (a) CTC-based models [10, 11], and (b)\nSeq2Seq-based models, such as LAS [1]. A combination\nof both models is also proposed by [12]. Recent work by\n[3, 4] employ a different approach by utilizing the trans-\nformer block. The present study is related to these recent\ntransformer ASR approaches, while also leveraging the effec-\ntiveness of in-training low-rank compression methods, which\nwas not considered in the aforementioned works.\n3. LOW-RANK TRANSFORMER ASR\nWe propose a compact and more generalized low-rank trans-\nformer unit by extending the idea of the in-training compres-\nsion method proposed in [9]. In our transformer architecture,\nwe replace the linear feed-forward unit [5] with a factorized\nlinear unit called a linear encoder-decoder (LED) unit. Fig-\nure 1 shows the architecture of our proposed low-rank trans-\nformer, and Figure 2 shows the low-rank version of the multi-\nhead attention and position-wise feed-forward network, in-\ncluding the LED. The proposed end-to-end ASR model ac-\ncepts a spectrogram as the input and produces a sequence of\ncharacters as the output similar to [16]. It consists of M lay-\ners of the encoder and N layers of the decoder. We employ\nmulti-head attention to allow the model to jointly attend to\nTable 1. Results on AiShell-1 (left) and HKUST (right) test sets. For the end-to-end approach, we limit the evaluation to\nsystems without any external data and perturbation to examine the effectiveness of the approach. We approximate the number\nof parameters based on the description in the previous studies.\nModel Params CER\nHybrid approach\nHMM-DNN [12] - 8.5%\nEnd-to-end approach\nAttention Model [13] - 23.2%\n+ RNNLM [13] - 22.0%\nCTC [14] ≈11.7M 19.43%\nFramewise-RNN [14] ≈17.1M 19.38%\nACS + RNNLM [13] ≈14.6M 18.7%\nTransformer (large) 25.1M 13.49%\nTransformer (medium) 12.7M 14.47%\nTransformer (small) 8.7M 15.66%\nLRT (r= 100) 12.7M 13.09%\nLRT (r= 75) 10.7M 13.23%\nLRT (r= 50) 8.7M 13.60%\nModel Params CER\nHybrid approach\nDNN-hybrid [12] - 35.9%\nLSTM-hybrid (with perturb.) [12] - 33.5%\nTDNN-hybrid, lattice-free MMI\n(with perturb.) [12] - 28.2%\nEnd-to-end approach\nAttention Model [12] - 37.8%\nCTC + LM [15] ≈12.7M 34.8%\nMTL + joint dec. (one-pass) [12]≈9.6M 33.9%\n+ RNNLM (joint train) [12]≈16.1M 32.1%\nTransformer (large) 22M 29.21%\nTransformer (medium) 11.5M 29.73%\nTransformer (small) 7.8M 31.30%\nLRT (r= 100) 11.5M 28.95%\nLRT (r= 75) 9.7M 29.08%\nLRT (r= 50) 7.8M 30.74%\ninformation from different representation subspaces in a dif-\nferent position.\n3.1. Linear Encoder-Decoder\nWe propose linear encoder-decoder (LED) units in the trans-\nformer model instead of a single linear layer. The design is\nbased on matrix factorization by approximating the matrix\nW ∈Rm×n in the linear feed-forward unit using two smaller\nmatrices, E ∈Rm×r and D ∈Rr×n:\nW ≈E ×D. (1)\nThe matrix Wrequires mnparameters and mnﬂops, while E\nand D require rm+rn= r(m+n) parameters and r(m+n)\nﬂops. If we take the rank to be very low r << m,n, the\nnumber of parameters and ﬂops in E and D are much smaller\ncompared to W.\n3.2. Low-Rank Multi-Head Attention\nThe LED is incorporated into the multi-head attention by\nfactorizing the projection layers of keys WQ\ni , values WV\ni ,\nqueries WQ\ni , and the output layer WO. A residual connec-\ntion from a query Qto the output is added.\nAttention(Q,K,V ) = Softmax(QKT\n√dk\nV), (2)\nhdi = Attention(QEQ\ni DQ\ni ,KEK\ni DK\ni ,VE V\ni DV\ni ), (3)\nf(Q,K,V ) = Concat(h1,··· ,hH)EODO + Q, (4)\nwhere f is a low-rank multi-head attention (LRMHA) func-\ntion, hi is the head of i, H is the number of heads, and\nthe projections are parameter matrices EQ\ni ∈ Rdmodel×dr ;\nDQ\ni ∈ Rdr×dk ; EK\ni ∈ Rdmodel×dr ; DK\ni ∈ Rdr×dk ;\nEV\ni ∈Rdmodel×dr ; DV\ni ∈Rdr×dv . dmodel, dr, dk, and dv are\ndimensions of hidden size, rank, key, and value, respectively.\n3.3. Low-Rank Feed-Forward\nEach encoder and decoder layer has a position-wise feed-\nforward network that contains two low-rank LED units and\napplies a ReLU function in between. To alleviate the gradient\nvanishing issue, a residual connection is added, as shown in\nFigure 2.\ng(x) = LayerNorm(max(0,xE1D1)E2D2 + x), (5)\nwhere gis a low-rank feed-forward (LRFF) function.\n3.4. Training Phase\nThe encoder module uses a VGG net [17] with a 6-layer CNN\narchitecture. The VGG consists of convolutional layers that\nare added to learn a universal audio representation and gener-\nate input embedding. The input of the unit is a spectrogram.\nThe decoder receives the encoder outputs and applies multi-\nhead attention to the decoder input. We apply a mask in the\nattention layer to avoid any information ﬂow from future to-\nkens. Then, we run a non-autoregressive step and calculate\nthe cross-entropy loss.\n3.5. Evaluation Phase\nIn the inference time, we decode the sequence using au-\ntoregressive beam-search by selecting the best sub-sequence\nscored using the softmax probability of the characters. We\ndeﬁne P(Y) as the probability of the sentence. A word count\nis added to avoid generating very short sentences. P(Y) is\ncalculated as follows:\nP(Y) = αPtrans(Y|X) + γ\n√\nwc(Y), (6)\nwhere α is the parameter to control the decoding probabil-\nity from the decoder Ptrans(Y|X), and γis the parameter to\ncontrol the effect of the word count wc(Y).\n4. EXPERIMENTS\nExperiments were conducted on two dataset benchmarks:\nAiShell-1 [18], a multi-accent Mandarin speech dataset, and\nHKUST [19], a conversational telephone speech recognition\ndataset. The former consists of 150 hours, 10 hours, and 5\nhours of training, validation, and testing, respectively, while\nthe latter consists of a 5 hour test set, 4.2 hours extracted\nfrom the training data as the validation set, and the remaining\n152 hours as the training set. We concatenate all characters in\nthe corpus, including three special tokens, such as <PAD>,\n<SOS>, and <EOS>. In our models, we use two encoder\nlayers and four decoder layers. The large transformer consists\nof a diminner of 2048, dimmodel of 512, and dimemb of 512.\nFor the smaller transformers, we select the same parameters\nas the LRT model with r = 100, r = 75 and r = 50. In the\nbeam-search decoding, we take α = 1, γ = 0.1, and a beam\nsize of 8. We evaluate our model using a single GeForce GTX\n1080Ti GPU and three Intel Xeon E5-2620 v4 CPU cores.\nWe use character error rate (CER) as the evaluation metric.\n5. RESULTS AND DISCUSSION\n5.1. Evaluation Performance\nTable 1 shows the experiment results. LRT models gain slight\nimprovement even with a greater than a 50% compression\nrate, and they outperform the vanilla transformers on both the\nAiShell-1 and HKUST test sets, with a 13.09% CER and a\n28.95% CER, respectively. In addition, we further minimize\nthe gap between the HMM-based hybrid and end-to-end ap-\nproaches without leveraging a perturbation strategy or exter-\nnal language model. Interestingly, our LRT models achieve\nlower validation loss compared to the uncompressed Trans-\nformer (large) baseline model, which implies that our LRT\nmodels regularize better, as shown in Figure 3. The mod-\nels are faster to converge and stop in a better local minimum\ncompared to the vanilla transformers.\n5.2. Memory and Time Efﬁciency\nAs shown in Table 1, our LRT (r= 50) model achieves simi-\nlar performance to the large transformer model despite having\nonly one-third of the large transformer parameters. In terms\nTable 2. Compression rate and inference speed-up of LRT\nmodels vs. Transformer (large). ∆CER and |¯X|denote the\nimprovement, and the mean length of generated sequences.\ndataset r ∆CER compress. speed-up |¯X|\nGPU CPU only\nAiShell-1 base 0 0 1 1 23.08\n100 0.40% 49.40% 1.17x 1.15x 23.15\n75 0.26% 57.37% 1.23x 1.16x 23.17\n50 -1.10% 65.34% 1.30x 1.23x 23.19\nHKUST base 0 0 1 1 22.43\n100 0.26% 47.72% 1.21x 1.14x 22.32\n75 0.13% 55.90% 1.26x 1.15x 22.15\n50 -1.53% 64.54% 1.35x 1.22x 22.49\n0\n1\n2\n3\n4\ntraining loss\nLRT(r = 50)\nLRT(r = 100)\nTransformer(small)\nTransformer(medium)\n0 10 20 30 40\nepoch\n0\n1\n2\n3\n4\nvalidation loss\nLRT(r = 50)\nLRT(r = 100)\nTransformer(small)\nTransformer(medium)\nFig. 3. Training and validation losses on AiShell-1 data.\nof time efﬁciency, our LRT models gain inference time speed-\nup by up to 1.35x in the GPU and 1.23x in the CPU, and\n1.10x training time speed-up in the GPU compared to the un-\ncompressed Transformer (large) baseline model, as shown in\nTable 2. We also compute the average length of the generated\nsequences to get a precise comparison. In general, both the\nLRT and baseline models generate sequences with a similar\nlength, which implies that our speed-up scores are valid.\n6. CONCLUSION\nWe propose low-rank transformer (LRT), a memory-efﬁcient\nand fast neural architecture that compress the network pa-\nrameters and boosts the speed of the inference time by up\nto 1.35x in the GPU and 1.23x in the CPU, as well as the\nspeed of the training time for end-to-end speech recognition.\nOur LRT improves the performance even though the number\nof parameters is reduced by 50% compared to the baseline\ntransformer model. Our approach generalizes better than un-\ncompressed vanilla transformers and outperforms those from\nexisting baselines on the AiShell-1 and HKUST datasets in\nan end-to-end setting without using additional external data.\n7. REFERENCES\n[1] William Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals, “Listen, attend and spell: A neural network\nfor large vocabulary conversational speech recognition,”\nin 2016 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2016,\npp. 4960–4964.\n[2] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint\nctc-attention based end-to-end speech recognition us-\ning multi-task learning,” in 2017 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2017, pp. 4835–4839.\n[3] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-\ntransformer: a no-recurrence sequence-to-sequence\nmodel for speech recognition,” in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2018, pp. 5884–5888.\n[4] Jie Li, Xiaorui Wang, Yan Li, et al., “The speechtrans-\nformer for large-scale mandarin chinese speech recog-\nnition,” in ICASSP 2019-2019 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2019, pp. 7095–7099.\n[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” in Ad-\nvances in neural information processing systems, 2017,\npp. 5998–6008.\n[6] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani,\nEbru Arisoy, and Bhuvana Ramabhadran, “Low-rank\nmatrix factorization for deep neural network training\nwith high-dimensional output targets,” in2013 IEEE in-\nternational conference on acoustics, speech and signal\nprocessing. IEEE, 2013, pp. 6655–6659.\n[7] ukasz Dudziak, Mohamed Abdelfattah, Ravichander\nVipperla, Stefanos Laskaridis, and Nicholas Lane,\n“Shrinkml: End-to-end asr model compression using re-\ninforcement learning,” in INTERSPEECH, 2019.\n[8] Genta Indra Winata, Andrea Madotto, Jamin Shin, El-\nham J Barezi, and Pascale Fung, “On the effective-\nness of low-rank matrix factorization for lstm model\ncompression,” in Proceedings of the 33rd Paciﬁc Asia\nConference on Language, Information and Computa-\ntion, Hakodate, Japan, 13–15 Sept. 2019, Association\nfor Computational Linguistics.\n[9] Oleksii Kuchaiev and Boris Ginsburg, “Factorization\ntricks for lstm networks,” ICLR Workshop, 2017.\n[10] Alex Graves and Navdeep Jaitly, “Towards end-to-end\nspeech recognition with recurrent neural networks,” in\nInternational conference on machine learning , 2014,\npp. 1764–1772.\n[11] Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramab-\nhadran, George Saon, and Michael Picheny, “Building\ncompetitive direct acoustics-to-word models for english\nconversational speech recognition,” in 2018 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2018, pp. 4759–4763.\n[12] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William\nChan, “Advances in joint ctc-attention based end-to-end\nspeech recognition with a deep cnn encoder and rnn-\nlm,” Proc. Interspeech 2017, pp. 949–953, 2017.\n[13] Mohan Li, Min Liu, and Hattori Masanori, “End-to-end\nspeech recognition with adaptive computation steps,”\nin ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2019, pp. 6246–6250.\n[14] Mohan Li, Yuanjiang Cao, Weicong Zhou, and Min\nLiu, “Framewise supervised training towards end-to-\nend speech recognition models: First results,” Proc. In-\nterspeech 2019, pp. 1641–1645, 2019.\n[15] Yajie Miao, Mohammad Gowayyed, Xingyu Na, Tom\nKo, Florian Metze, and Alexander Waibel, “An empir-\nical exploration of ctc acoustic models,” in 2016 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2016, pp. 2623–2627.\n[16] Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu,\nand Pascale Fung, “Code-switched language models us-\ning neural based synthetic data from parallel sentences,”\nin Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning , Hong Kong, Nov.\n2019, Association for Computational Linguistics.\n[17] Karen Simonyan and Andrew Zisserman, “Very deep\nconvolutional networks for large-scale image recogni-\ntion,” in ICLR, 2015.\n[18] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao\nZheng, “Aishell-1: An open-source mandarin speech\ncorpus and a speech recognition baseline,” in 2017 20th\nConference of the Oriental Chapter of the International\nCoordinating Committee on Speech Databases and\nSpeech I/O Systems and Assessment (O-COCOSDA) .\nIEEE, 2017, pp. 1–5.\n[19] Yi Liu, Pascale Fung, Yongsheng Yang, Christopher\nCieri, Shudong Huang, and David Graff, “Hkust/mts: A\nvery large scale mandarin telephone speech corpus,” in\nInternational Symposium on Chinese Spoken Language\nProcessing. Springer, 2006, pp. 724–735.",
  "topic": "End-to-end principle",
  "concepts": [
    {
      "name": "End-to-end principle",
      "score": 0.7783710956573486
    },
    {
      "name": "Inference",
      "score": 0.7615358829498291
    },
    {
      "name": "Transformer",
      "score": 0.7548172473907471
    },
    {
      "name": "Computer science",
      "score": 0.7254831790924072
    },
    {
      "name": "Language model",
      "score": 0.5857549905776978
    },
    {
      "name": "Software deployment",
      "score": 0.5308125019073486
    },
    {
      "name": "Artificial neural network",
      "score": 0.5042797327041626
    },
    {
      "name": "Architecture",
      "score": 0.4128718078136444
    },
    {
      "name": "Speech recognition",
      "score": 0.4032588303089142
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3587448000907898
    },
    {
      "name": "Engineering",
      "score": 0.11612668633460999
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}