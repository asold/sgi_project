{
  "title": "MMSFormer: Multimodal Transformer for Material and Semantic Segmentation",
  "url": "https://openalex.org/W4394863092",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3027708903",
      "name": "Md. Kaykobad Reza",
      "affiliations": [
        "University of California, Riverside"
      ]
    },
    {
      "id": "https://openalex.org/A4207754954",
      "name": "Ashley Prater-Bennette",
      "affiliations": [
        "United States Air Force Research Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2108029156",
      "name": "M. Salman Asif",
      "affiliations": [
        "University of California, Riverside"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2136704614",
    "https://openalex.org/W3132455321",
    "https://openalex.org/W2770233088",
    "https://openalex.org/W4212773528",
    "https://openalex.org/W3039368862",
    "https://openalex.org/W6804002296",
    "https://openalex.org/W2999219213",
    "https://openalex.org/W4312326868",
    "https://openalex.org/W4312702031",
    "https://openalex.org/W2907760128",
    "https://openalex.org/W3091791645",
    "https://openalex.org/W3108601100",
    "https://openalex.org/W2587989515",
    "https://openalex.org/W2971014764",
    "https://openalex.org/W4383336674",
    "https://openalex.org/W4367663172",
    "https://openalex.org/W2921749009",
    "https://openalex.org/W4386075910",
    "https://openalex.org/W3172863135",
    "https://openalex.org/W3195380476",
    "https://openalex.org/W4386179772",
    "https://openalex.org/W4391769355",
    "https://openalex.org/W4386065698",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W4390873110",
    "https://openalex.org/W3091001089",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W6760897771",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4312815172",
    "https://openalex.org/W3186033197",
    "https://openalex.org/W3181848549",
    "https://openalex.org/W3035570025",
    "https://openalex.org/W4289752563",
    "https://openalex.org/W3198062544",
    "https://openalex.org/W4296913506",
    "https://openalex.org/W4386275800",
    "https://openalex.org/W3206439726",
    "https://openalex.org/W3035467948",
    "https://openalex.org/W4312402232",
    "https://openalex.org/W3046194589",
    "https://openalex.org/W4312594135",
    "https://openalex.org/W4313139043",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3107497254",
    "https://openalex.org/W4200635035",
    "https://openalex.org/W4225487034",
    "https://openalex.org/W3170885821",
    "https://openalex.org/W3185043317",
    "https://openalex.org/W4286565909",
    "https://openalex.org/W4312242472",
    "https://openalex.org/W4307297655",
    "https://openalex.org/W6849131038",
    "https://openalex.org/W4313639507",
    "https://openalex.org/W4386737248",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3105054740",
    "https://openalex.org/W2592939477",
    "https://openalex.org/W4318033725",
    "https://openalex.org/W3104771364",
    "https://openalex.org/W4286855313",
    "https://openalex.org/W2924464923"
  ],
  "abstract": "Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different modality combinations. We also propose a new model named <underline>M</underline>ulti-<underline>M</underline>odal <underline>S</underline>egmentation Trans<underline>Former</underline> (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material and semantic segmentation tasks. MMSFormer outperforms current state-of-the-art models on three different datasets. As we begin with only one input modality, performance improves progressively as additional modalities are incorporated, showcasing the effectiveness of the fusion block in combining useful information from diverse input modalities. Ablation studies show that different modules in the fusion block are crucial for overall model performance. Furthermore, our ablation studies also highlight the capacity of different input modalities to improve performance in the identification of different types of materials.",
  "full_text": "<Society logo(s) and publica-\ntion title will appear here. >\nReceived XX Month, XXXX; revised XX Month, XXXX; accepted XX Month, XXXX; Date of publication XX Month, XXXX; date of\ncurrent version XX Month, XXXX.\nDigital Object Identifier 10.1109/XXXX.2022.1234567\nMMSFormer: Multimodal Transformer\nfor Material and Semantic\nSegmentation\nMd Kaykobad Reza1, Ashley Prater-Bennette2,\nand M. Salman Asif1, Senior Member, IEEE\n1University of California, Riverside, CA 92508 USA\n2Air Force Research Laboratory, Rome, NY 13441 USA\nCorresponding author: M. Salman Asif (email: sasif@ucr.edu).\nThis work is supported in part by AFOSR award FA9550-21-1-0330.\nABSTRACT Leveraging information across diverse modalities is known to enhance performance on\nmultimodal segmentation tasks. However, effectively fusing information from different modalities remains\nchallenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion\nstrategy that can effectively fuse information from different modality combinations. We also propose a\nnew model named M ulti-Modal Segmentation TransFormer (MMSFormer) that incorporates the proposed\nfusion strategy to perform multimodal material and semantic segmentation tasks. MMSFormer outperforms\ncurrent state-of-the-art models on three different datasets. As we begin with only one input modality,\nperformance improves progressively as additional modalities are incorporated, showcasing the effectiveness\nof the fusion block in combining useful information from diverse input modalities. Ablation studies show\nthat different modules in the fusion block are crucial for overall model performance. Furthermore, our\nablation studies also highlight the capacity of different input modalities to improve performance in the\nidentification of different types of materials. The code and pretrained models will be made available at\nhttps://github.com/csiplab/MMSFormer.\nINDEX TERMS multimodal image segmentation, material segmentation, semantic segmentation, multi-\nmodal fusion, transformer\nI. Introduction\nImage segmentation [1], [2] methods assign one class label\nto each pixel in an image. The segmentation map can be\nused for holistic understanding of objects or context of\nthe scene. Image segmentation can be further divided into\ndifferent types; examples include semantic segmentation [3],\n[4], instance segmentation [5], [6], panoptic segmentation\n[7], [8] and material segmentation [9], [10]. Each of these\nsegmentation tasks are designed to address specific chal-\nlenges and applications.\nMultimodal image segmentation [11], [12] aims to en-\nhance the accuracy and completeness of the task by leverag-\ning diverse sources of information, and potentially leading to\na more robust understanding of complex scenes. In contrast\nto single-modal segmentation [13], the multimodal approach\n[14] is more complex due to the necessity of effectively\nintegrating heterogeneous data from different modalities.\nKey challenges arise from variations in data quality and\nattributes, distinct traits of each modality, and need to create\nmodels capable of accurately and coherently segmenting\nwith the fused information.\nMost of the existing multimodal segmentation methods\nare designed to work with specific modality pairs, such as\nRGB-Depth [15]–[17], RGB-Thermal [18]–[20], and RGB-\nLidar [21]–[23]. As they are designed for specific modality\ncombinations, most of them generally do not work well\nwith modality combinations different from the ones used\nin the original design. Recently, CMX [24] introduced a\ntechnique to fuse information from RGB and one other\nsupplementary modality, but it is incapable of fusing more\nthan two modalities at the same time. Some recent models\nhave proposed techniques to fuse more than two modalities\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME , 1\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nReza et al.: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation\n[9], [25], [26]. However, they either use very complex fusion\nstrategies [24], [25] or require additional information like\nsemantic labels [9] for performing underlying tasks.\nIn this paper, we propose a novel fusion block that can\nfuse information from diverse combination of modalities.\nWe also propose a new model for multimodal material\nand semantic segmentation tasks that we call MMSFormer.\nOur model uses transformer based encoders [27] to capture\nhierarchical features from different modalities, fuses the\nextracted features with our novel fusion block and utilizes\nMLP decoder to perform multimodal material and seman-\ntic segmentation. In particular, our proposed fusion block\nuses parallel convolutions to capture multi-scale features,\nchannel attention to re-calibrate features along the channel\ndimension and linear layer to combine information across\nmultiple modalities. Such a design provides a simple and\ncomputationally efficient fusion block that can handle an\narbitrary number of input modalities and combine informa-\ntion effectively from different modality combinations. We\ncompare our fusion block with some of the existing fusion\nmethods in terms of number of parameters and GFLOPs in\nTable 9.\nTo evaluate our proposed MMSFormer and fusion block,\nwe focus on multimodal material segmentation on MCubeS\n[9] dataset and multimodal semantic segmentation on FMB\n[29] and PST900 [30] datasets. MCubeS dataset consists of\nfour different modalities: RGB, angle of linear polarization\n(AoLP), degree of linear polarization (DoLP) and near-\ninfrared (NIR). FMB dataset includes RGB and infrared\nmodalities, while PST900 dataset comprises RGB and ther-\nmal modalities. We show the overall and per-class perfor-\nmance comparison in Table 1-5 for these datasets. A series\nof experiments highlight the ability of the proposed fusion\nblock to effectively combine features from different modality\ncombinations, resulting in superior performance compared to\ncurrent state-of-the-art methods. Ablation studies show that\ndifferent input modalities assist in identifying different types\nof material classes as shown in Table 8. Furthermore, as\nwe add new input modalities, overall performance increases\ngradually highlighting the ability of the fusion block to\nincorporate useful information from new modalities. We\nsummarize the results in Table 4 and 6 for FMB and MCubeS\ndatasets respectively.\nMain contributions of this paper can be summarized as\nfollows.\n• We propose a new multimodal segmentation model\ncalled MMSFormer. The model incorporates a novel\nfusion block that can fuse information from arbitrary\n(heterogeneous) combinations of modalities.\n• Our model achieves new state-of-the-art performance\non three different datasets. Furthermore, our method\nachieves better performance for all modality combina-\ntions compared to the current leading models.\n• A series of ablation studies show that each module on\nthe fusion block has an important contribution towards\nthe overall model performance and each input modality\nassists in identifying specific material classes.\nRest of the paper is structured as follows. Section II presents\na brief review of related work. We describe our model and\nfusion block in detail in Section III. Section IV presents\nexperimental results and ablation studies on multimodal\nmaterial and semantic segmentation tasks with qualitative\nand quantitative analysis.\nII. Related Work\nImage segmentation has witnessed significant evolution,\nspurred by advancements in machine learning and com-\nputational capabilities. A significant improvement in this\nevolution came with the inception of fully convolutional\nnetworks (FCNs) [31], [32], which enabled pixel-wise pre-\ndictions through the utilization of hierarchical features within\nconvolutional neural networks (CNNs). This led to the devel-\nopment of a variety of CNN based models for different im-\nages segmentation tasks. U-Net [33] is one such model that\nutilizes skip-connections between the lower-resolution and\ncorresponding higher-resolution feature maps. DeepLabV3+\n[34] introduced dilated convolutions (atrous convolutions)\ninto the encoder allowing the expansion of the receptive field\nwithout increasing computational complexity significantly.\nPSPNet [35] introduced global context modules that enable\nthe model to gather information from a wide range of spatial\nscales, essentially integrating both local and global context\ninto the segmentation process.\nRecently, Transformer based models have proven to be\nvery effective in handling complex image segmentation\ntasks. Some of the notable transformer-based models are\nPyramid Vision Transformer (PVT) [36], SegFormer [27],\nand Mask2Former [37]. PVT [36] utilizes transformer based\ndesign for various computer vision tasks. SegFormer [27] uti-\nlizes efficient self-attention and lightweight MLP decoder for\nsimple and efficient semantic segmentation. Mask2Former\n[37] uses masked-attention along with pixel decoder and\ntransformer decoder for any segmentation task. Their success\ndemonstrates the capacity of these models to provide state-\nof-the-art solutions in various segmentation tasks.\nIn the context of multimodal image segmentation, fusion\nof data from diverse sources [12] has gained traction as a\nmeans to extract richer information and improve accuracy. A\nvariety of models and fusion strategies have been proposed\nfor RGB-Depth segmentation tasks. FuseNet [16] model\nintegrates depth feature maps into RGB feature maps, while\nSA-Gate [15] employs Separation-and-Aggregation Gating\nto mutually filter and recalibrate RGB and depth modalities\nbefore fusion. Attention Complementary Module has been\nproposed by ACNet [17] that extracts weighted RGB and\ndepth features for fusion. The domain of RGB-Thermal\nimage segmentation has also gained prominence. Recent\nmodels include RTFNet [20] that achieves fusion through\nelementwise addition of thermal features with RGB, RSFNet\n[18] proposing Residual Spatial Fusion module to blend\n2 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n<Society logo(s) and publication title will appear here. >\nFIGURE 1: (a) Overall architecture of MMSFormer model. Each image passes through a modality-specific encoder where\nwe extract hierarchical features. Then we fuse the extracted features using the proposed fusion block and pass the fused\nfeatures to the decoder for predicting the segmentation map. (b) Illustration of the mix transformer [27] block. Each block\napplies a spatial reduction before applying multi-head attention to reduce computational cost. (c) Proposed multimodal\nfusion block. We first concatenate all the features along the channel dimension and pass it through linear fusion layer to\nfuse them. Then the feature tensor is fed to linear projection and parallel convolution layers to capture multi-scale features.\nWe use Squeeze and Excitation block [28] as channel attention in the residual connection to dynamically re-calibrate the\nfeatures along the channel dimension.\nRGB and Thermal modalities, and EAEFNet [19] utilizing\nattention interaction and attention complement mechanisms\nto merge RGB and Thermal features. A number of methods\nalso focus on fusing RGB-Lidar data that include TransFuser\n[22] that employs Transformer blocks, whereas LIF-Seg\n[23] relies on coarse feature extraction, offset learning, and\nrefinement for effective fusion.\nWhile the previously mentioned studies focus on specific\npairs of modalities, some recent research has demonstrated\npromising results in the fusion of arbitrary modalities. CMX\n[24] introduces cross-modal feature rectification and fusion\nmodules to merge RGB features with supplementary modali-\nties. For multimodal material segmentation, MCubeSNet [9]\nmodel is proposed, which can seamlessly integrate four\ndifferent modalities to enhance segmentation accuracy. In\nthe context of arbitrary modal semantic segmentation, CM-\nNeXt [26] introduces Self-Query Hub and Parallel Pooling\nMixer modules, offering a versatile approach for fusing di-\nverse modalities. Additionally, HRFuser [25] employs multi-\nwindow cross-attention to fuse different modalities at various\nresolutions, thereby enriching model performance.\nThough some of these models can fuse different modali-\nties, they either use very complex fusion strategies [24]–[26]\nor requires additional information [9] to perform underlying\ntask. We aim to design a simple fusion module that can\nhandle arbitrary number of input modalities and able to effec-\ntively fuse information from diverse modality combinations.\nIII. Proposed Model\nThe overall architecture of our proposed MMSFormer model\nand the fusion block is shown in Figure 1. The model has\nthree modules: (1) Modality specific encoder; (2) Multimodal\nVOLUME , 3\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nReza et al.: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation\nfusion block; and (3) Shared MLP decoder. We use mix\ntransformer [27] as the encoder of our model. We choose\nmix transformer for various reasons. First, it can provide\nhierarchical features without positional encoding. Second,\nit uses spatial reduction before attention that reduces the\nnumber of parameters significantly [27], [36]. Third, it also\nworks well with simple and lightweight MLP decoder [27].\nA. Overall model Architecture\nOur overall model architecture is shown in Figure 1a.\nAssume we have M distinct modalities. Given a set of\nmodalities as input, each modality-specific encoder captures\ndistinctive features from each input modality by mapping\nthe corresponding image into modality-specific hierarchical\nfeatures as\nFm = Encoderm(Im), (1)\nwhere Im ∈ RH×W×3 represents input image for modality\nm ∈ {1, 2, . . . , M} and Encoder m(·) denotes the encoder\nfor that modality. The encoder generates four feature maps\nat {1\n4 , 1\n8 , 1\n16 , 1\n32 } of the input image resolution. We represent\nthem as Fm = {F1\nm, F2\nm, F3\nm, F4\nm}. For simplicity we denote\nthe shape of the feature map for the ith encoder stage as\n(Hi × Wi × Ci) where i ∈ {1, 2, 3, 4}.\nWe use four separate fusion blocks, one corresponding\nto each encoder stage, to fuse the features from each stage\nof the encoder. We pass the extracted features Fi\nm for all\nmodalities to the ith fusion block as\nFi = FusionBlocki({Fi\nm}m). (2)\nEach fusion block fuses the features extracted from all the\nmodalities to generate a combined feature representation\nF = {F1, F2, F3, F4}, where Fi denotes the fused feature\nat ith stage. Finally, we pass the combined features F to the\nMLP decoder [27] to predict the segmentation labels.\nB. Modality Specific Encoder\nWe use mix transformer encoder [27] to capture hierarchical\nfeatures from the input modalities. Each input image Im goes\nthrough patch embedding layer where it is divided into 4×4\npatches following [27] and then fed to the mix transformer\nblocks. The design of mix transformer block is shown in\nFigure 1b. We denote the input to any mix transformer block\nas Xin ∈ RHi×Wi×Ci that is reshaped to Ni × Ci (with\nNi = HiWi) and used as query Q, key K, and value V .\nTo reduce the computational overhead, spatial reduction is\napplied following [36] using a reduction ratio R. K and V\nare first reshaped into Ni\nR × CiR matrices and then mapped\nto Ni\nR × Ci matrices via linear projection. A standard multi-\nhead self-attention (MHSA) maps Q, K, Vto intermediate\nfeatures as\nMHSA(Q, K, V) =Concat(head1, . . . ,headh)WO,\nheadj = Attention(QWQ\nj , KWK\nj , V WV\nj ), (3)\nwhere h represents the number of attention heads, WQ\nj ∈\nRCi×dK , WK\nj ∈ RCi×dK , WV\nj ∈ RCi×dV , and WO ∈\nRhdV ×Ci are the projection matrices, dK, dV represent\ndimensions of K, V, respectively. We can formulate the\nAttention function as\nAttention(Q, K, V) =Softmax(QKT\n√dK\n)V, (4)\nwhere Q, K,and V are the input query, key, and value\nmatrices. MHSA is followed by a mixer layer (with two\nMLP and one 3×3 convolution layer). The convolution layer\nprovides sufficient positional encoding into the transformer\nencoder for optimal segmentation performance [27]. This\nlayer can be written as\nˆXin = MHSA(Q, K, V),\nXout = MLP(GELU(Conv3×3(MLP( ˆXin)))) + ˆXin,\n(5)\nFinally, overlap patch merging is applied to Xout following\n[27] to generate the final output.\nC. Multimodal Fusion Block\nAfter extracting hierarchical features, we fuse them using\nour proposed fusion block. The fusion block shown in\nFigure 1c is responsible for fusing the features extracted\nfrom the modality specific encoders. We have one fusion\nblock for each of the four encoder stages. For the ith\nfusion block, let us assume the input feature maps are\ngiven as Fi\nm ∈ RHi×Wi×Ci ∀m ∈ {1, 2, . . . , M}. First,\nwe concatenate the feature maps from M modalities along\nthe channel dimension to get the combined feature map\nFi ∈ RHi×Wi×MCi. Then we pass the features through a\nlinear fusion layer that combines the features and reduces\nthe channel dimension to Ci. Let us denote the resulting\nfeatures as ˆFi ∈ RHi×Wi×Ci. We represent the operation as\nˆFi = Linear(Fi\n1||···|| Fi\nM ). (6)\nHere || represents concatenation of features along the channel\ndimension and the linear layer takes an MCi dimensional\ninput and generates a Ci dimensional output.\nAfter the linear fusion layer, we added a module for cap-\nturing and mixing multi-scale features. The module consists\nof two linear projection layers having parallel convolution\nlayers in between them. First we apply a linear transforma-\ntion on ˆFi along the channel dimension by passing it through\nthe first linear projection layer. It refines and tunes the\nfeatures from different channels. Then we apply 3×3, 5×5,\nand 7×7 convolutions to effectively capture diverse features\nacross different spatial contexts. By employing convolutions\nwith different sizes, the fusion block can attend to local\npatterns as well as capture larger spatial structures, thereby\nenhancing its ability to extract meaningful features from the\ninput data. Finally we apply another linear transformation\nalong the channel dimension using the second linear project\nlayer to consolidate the information captured by the parallel\nconvolutions, promoting feature consistency and enhancing\nthe discriminative power of the fused features. These steps\ncan be performed as\n˜Fi = Linear( ˆFi), (7)\n4 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n<Society logo(s) and publication title will appear here. >\nTABLE 1: Performance comparison on Multimodal Material\nSegmentation (MCubeS) dataset [9]. Here A, D, and N rep-\nresent angle of linear polarization (AoLP), degree of linear\npolarization (DoLP), and near-infrared (NIR) respectively.\nMethod Modalities % mIoU\nDRConv [38] RGB-A-D-N 34.63\nDDF [39] RGB-A-D-N 36.16\nTransFuser [22] RGB-A-D-N 37.66\nDeepLabv3+ [34] RGB-A-D-N 38.13\nMMTM [40] RGB-A-D-N 39.71\nFuseNet [16] RGB-A-D-N 40.58\nMCubeSNet [9] RGB-A-D-N 42.46\nCBAM [41] RGB-A-D-N 51.32\nCMNeXt [26] RGB-A-D-N 51.54\nMMSFormer (Ours) RGB-A-D-N 53.11\nFi = Linear( ˜Fi +\nX\nk∈{3,5,7}\nConvk×k( ˜Fi)). (8)\nWe found that using 3 parallel convolution layers with\nsizes 3 × 3, 5 × 5, and 7 × 7 provide optimal performance.\nIncreasing the convolution kernel size reduces performance\nwhich we show in Table 7. As larger kernels reduce perfor-\nmance, we did not add more than 3 parallel convolutions in\nour model.\nWe apply Squeeze-and-Excitation block [28] as channel\nattention in the residual connection. The final fused feature\ncan be represented as\nFi = ChannelAttention( ˆFi) +Fi. (9)\nChannel attention re-calibrates interdependence between\nchannels and allows the model to select the most relevant\nfeatures or channels while suppressing less important ones\n[28]. This leads to more effective feature representations and\nthus better performance on the underlying task.\nD. Shared MLP Decoder\nThe fused features generated from all the 4 fusion blocks are\nsent to the shared MLP decoder. We use the deocder design\nproposed in [27]. The decoder shown in Figure 1a can be\nrepresented as the following equations:\nFi = Linear(Fi), ∀i ∈ {1, 2, 3, 4}\nFi = Upsample(Fi), ∀i ∈ {1, 2, 3, 4}\nF = Linear(F1||···|| F4),\nP = Linear(F).\n(10)\nThe first linear layers take the fused features of different\nshapes and generate features having the same channel dimen-\nsion. Then the features are up-sampled to 1\n4\nth\nof the original\ninput shape, concatenated along the channel dimension and\npassed through another linear layer to generate the final fused\nfeature F. Finally F is passed through the last linear layer\nto generate the predicted segmentation map P.\nTABLE 2: Performance comparison on FBM [29] dataset.\nWe show performance for different methods from already\npublished works.\nMethods Modalities % mIoU\nCBAM [41] RGB-Infrared 50.1\nGMNet [42] RGB-Infrared 49.2\nLASNet [43] RGB-Infrared 42.5\nEGFNet [44] RGB-Infrared 47.3\nFEANet [45] RGB-Infrared 46.8\nDIDFuse [46] RGB-Infrared 50.6\nReCoNet [47] RGB-Infrared 50.9\nU2Fusion [48] RGB-Infrared 47.9\nTarDAL [49] RGB-Infrared 48.1\nSegMiF [29] RGB-Infrared 54.8\nMMSFormer (Ours) RGB-Infrared 61.7\nIV. Experiments and Results\nWe evaluated our model and proposed fusion block on mul-\ntiple datasets and with different modality combinations for\nmultimodal semantic and material segmentation tasks. We\nalso compared our methods with existing baseline methods\nboth qualitatively and quantitatively. We report results from\nalready published works whenever possible. ∗ indicates that\nwe have used the code and pretrained models from the papers\nto generate the results.\nA. Datasets\nMultimodal material segmentation (MCubeS) dataset[9]\ncontains 500 sets of images from 42 street scenes having four\nmodalities: RGB, angle of linear polarization (AoLP), degree\nof linear polarization (DoLP), and near-infrared (NIR). It\nprovides annotated ground truth labels for both material and\nsemantic segmentation and divided into training set with\n302 image sets, validation set with 96 image sets, and test\nset with 102 image sets. This dataset has 20 class labels\ncorresponding to different materials.\nFMB dataset [29] is a new and challenging dataset with\n1500 pairs of calibrated RGB-Infrared image pairs. The\ntraining and test set contains 1220 and 280 image pairs\nrespectively. The dataset covers a wide range of scenes under\ndifferent lighting and weather conditions (Tyndall effect,\nrain, fog, and strong light). It also provides per pixel ground\ntruth annotation for 14 different classes.\nPST900 dataset [30] contains 894 pairs of synchronized\nRGB-Thermal image pairs. The dataset is divided into train-\ning and test sets with per pixel ground truth annotation for\nfive different classes.\nB. Implementation Details\nTo ensure a fair comparison with prior models, we followed\nthe same data preprocessing and augmentation strategies\nemployed in previous studies [9], [26], [29]. We used\nVOLUME , 5\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nReza et al.: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation\n(a) Visualization of predictions on MCubeS dataset\n(b) Visualization of predictions on PST900 dataset\nFIGURE 2: Visualization of predictions on MCubeS and PST900 datasets. Figure 2a shows RGB and all modalities (RGB-\nA-D-N) prediction from CMNeXt [26] and our model on MCubeS dataset. For brevity, we only show the RGB image\nand ground truth material segmentation maps along with the predictions. Figure 2b shows predictions from RTFNet [20],\nFDCNet [50] and our model for RGB-thermal input modalities on PST900 dataset. Our model shows better predictions on\nboth of the datasets.\nthe Mix-Transformer (MiT) [27] encoder pretrained on the\nImageNet [63] dataset as the backbone for our model to\nextract features from different modalities. Each modality\nhas a separate encoder. We used a shared MLP decoder\nintroduced in SegFormer [27] and used random initialization\nfor it. We trained and evaluated all our models using two\nNVIDIA RTX 2080Ti GPUs and used PyTorch for model\ndevelopment.\nWe utilized a polynomial learning rate scheduler with a\npower of 0.9 to dynamically adjust the learning rate during\ntraining. The first 10 epochs were designated as warm-up\nepochs with a learning rate of 0.1 times the original rate. For\nloss computation, we used the cross-entropy loss function.\nOptimization was performed using the AdamW [64] opti-\nmizer with an epsilon value of 10−8 and weight decay set\nto 0.01. For CBAM [41], we use the same encoder, decoder\nand hyperparameters used in our experiments and replace\nour fusion block with CBAM 1 module. To be specific, after\nextracting the feature maps from each input modality using\n1https://github.com/luuuyi/CBAM.PyTorch\nthe modality specific encoders, we add them (sum them up)\nand pass the combined feature map to the CBAM module.\nC. Performance Comparison with Existing Methods\nWe conducted a rigorous performance evaluation of our\nmodel compared to established baseline models for three\ndatasets. The comprehensive results are summarized in Ta-\nbles 1–6. We report results for CBAM from our experiments.\nOther results are taken from published literature.\nResults on MCubeS Dataset.Table 1 shows the overall\nperformance comparison between our model and existing\nbaseline models for MCubeS dataset. Our model achieves\na mean intersection-over-union (mIoU) of 53.11%, sur-\npassing the current state-of-the-art model. It shows 1.57%\nimprovement over CMNeXt [26], 1.79% improvement over\nCBAM [41] and 10.65% improvement over MCubeSNet [9]\nmodels. To further analyze the performance of our model,\nwe conducted a per-class IoU analysis and presented in\nTable 3. Our model performs better in detecting most of\nthe material classes compared to the current state-of-the-\n6 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n<Society logo(s) and publication title will appear here. >\nTABLE 3: Per-class % IoU comparison on Multimodal Material Segmentation (MCubeS) [9] dataset. Our proposed\nMMSFormer model shows better performance in detecting most of the classes compared to the current state-of-the-art\nmodels. ∗ indicates that the code and pretrained model from the authors were used to generate the results.\nMethods\nAsphalt\nConcrete\nMetal\nRoad marking\nFabric\nGlass\nPlaster\nPlastic\nRubber\nSand\nGravel\nCeramic\nCobblestone\nBrick\nGrass\nWood\nLeaf\nWater\nHuman\nSky\nMean\nMCubeSNet [9] 85.7 42.6 47.0 59.2 12.5 44.3 3.0 10.6 12.7 66.8 67.1 27.8 65.8 36.8 54.8 39.4 73.0 13.3 0.0 94.8 42.9\nCBAM [41] 85.7 47.7 55.4 70.4 27.6 54.7 0.9 30.9 26.5 61.6 63.0 28.0 71.1 41.8 58.6 47.4 76.7 56.3 25.9 96.5 51.3\nCMNeXt [26] ∗ 84.3 44.9 53.9 74.5 32.3 54.0 0.8 28.3 29.7 67.7 66.5 27.7 68.5 42.9 58.7 49.7 75.4 55.7 18.9 96.5 51.5\nMMSFormer (Ours) 88.0 48.3 56.2 72.2 35.4 54.9 0.5 34.6 29.4 67.2 69.0 29.9 73.4 44.7 59.5 47.8 77.1 50.5 26.9 96.6 53.1\nTABLE 4: Per-class % IoU comparison on FMB [29] dataset for both RGB only and RGB-infrared modalities. We show\nthe comparison for 8 classes (out of 14) that are published. T-Lamp and T-Sign stand for Traffic Lamp and Traffic Sign\nrespectively. Our model outperforms all the methods for all the classes except for the truck class.\nMethods Modalities Car Person Truck T-Lamp T-Sign Building Vegetation Pole % mIoU\nSegMiF [29] RGB 78.3 46.6 43.4 23.7 64.0 77.8 82.1 41.8 50.5\nMMSFormer (Ours) RGB 80.3 56.7 42.1 31.6 77.8 77.9 85.4 48.1 57.2\nCBAM [41] RGB-Infrared 71.9 49.3 20.9 25.8 67.1 75.8 80.9 19.7 50.1\nGMNet [42] RGB-Infrared 79.3 60.1 22.2 21.6 69.0 79.1 83.8 39.8 49.2\nLASNet [43] RGB-Infrared 72.6 48.6 14.8 2.9 59.0 75.4 81.6 36.7 42.5\nEGFNet [44] RGB-Infrared 77.4 63.0 17.1 25.2 66.6 77.2 83.5 41.5 47.3\nFEANet [45] RGB-Infrared 73.9 60.7 32.3 13.5 55.6 79.4 81.2 36.8 46.8\nDIDFuse [46] RGB-Infrared 77.7 64.4 28.8 29.2 64.4 78.4 82.4 41.8 50.6\nReCoNet [47] RGB-Infrared 75.9 65.8 14.9 34.7 66.6 79.2 81.3 44.9 50.9\nU2Fusion [48] RGB-Infrared 76.6 61.9 14.4 28.3 68.9 78.8 82.2 42.2 47.9\nTarDAL [49] RGB-Infrared 74.2 56.0 18.8 29.6 66.5 79.1 81.7 41.9 48.1\nSegMiF [29] RGB-Infrared 78.3 65.4 47.3 43.1 74.8 82.0 85.0 49.8 54.8\nMMSFormer (Ours) RGB-Infrared 82.6 69.8 44.6 45.2 79.7 83.0 87.3 51.4 61.7\nart models. Notably, our model demonstrates a substantial\nimprovement in the detection of plastic (+3.7%), fabric\n(+3.1%), asphalt (+2.3%), and cobblestone (2.3%) classes\nwhile maintaining competitive or better performance in other\nclasses. This led to the overall better performance and sets\nnew state-of-the-art for this dataset.\nResults on FMB Dataset.Performance Comparison for\nFMB dataset is shown on Table 2. Our model shows a\nsignificant improvement of 6.9% mIoU compared to the\ncurrent state-of-the-art model. Per-class IoU analysis for\nthis dataset is shown on Table 4. For a fair comparison,\nwe only compare the performance on 8 classes (out of\n14) that are published in literature. T-Lamp and T-Sign\nrepresent Traffic Lamp and Traffic Sign, respectively. Our\nmodel shows an overall performance improvement of 6.7%\nmIoU for RGB only predictions compared to the most recent\nSegMiF [29] model. Alongside this, our model also shows\nsuperior performance in detecting all of the classes except\nfor the truck class for both RGB only and RGB-Infrared\nsemantic segmentation tasks. Performance on RGB-Infrared\ninput modalities is much better than RGB only performance\nfor all the classes, which demonstrates the ability of the\nfusion block to effectively fuse information from the input\nmodalities.\nResults on PST900 Dataset.We also tested our model on\nPST900 [30] dataset and summarized the result in Table 5.\nExperiments show that our model outperforms existing base-\nline models for RGB-Thermal semantic segmentation on this\ndataset. It outperforms the most recent CACFNet [62] model\nby 0.89% mIoU. Our model also shows better performance\nin detecting 3 out of the 5 classes available in the dataset\nand competitive performance in other two classes.\nD. Performance Comparison for Incremental Modality\nIntegration\nA critical aspect of this work involves evaluating the effec-\ntiveness of our proposed fusion block in combining valuable\ninformation from diverse modalities. To analyze this effect,\nwe trained our model with various combinations of modal-\nities on the MCubeS dataset. The results are presented in\nVOLUME , 7\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nReza et al.: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation\nTABLE 5: Performance comparison on PST900 [30] dataset. We show per-class % IoU as well as % mIoU for all the\nclasses.\nMethods Modalities Background Fire-Extinguisher Backpack Hand-Drill Survivor % mIoU\nACNet [17] RGB-Thermal 99.25 59.95 83.19 51.46 65.19 71.81\nCCNet [51] RGB-Thermal 99.05 51.84 66.42 32.27 57.50 61.42\nEfficient FCN [52] RGB-Thermal 98.63 39.96 58.15 30.12 28.00 50.98\nRTFNet [20] RGB-Thermal 99.02 51.93 74.17 7.07 70.11 60.46\nPSTNet [30] RGB-Thermal 98.85 70.12 69.20 53.60 50.03 68.36\nEGFNet [53] RGB-Thermal 99.26 71.29 83.05 64.67 74.30 78.51\nMTANet [54] RGB-Thermal 99.33 64.95 87.50 62.05 79.14 78.60\nMFFENet [55] RGB-Thermal 99.40 66.38 81.02 72.50 75.60 78.98\nGMNet [42] RGB-Thermal 99.44 73.79 83.82 85.17 78.36 84.12\nCGFNet [56] RGB-Thermal 99.30 71.71 82.00 59.72 77.42 78.03\nGCNet [57] RGB-Thermal 99.35 77.68 79.37 82.92 73.58 82.58\nGEBNet [58] RGB-Thermal 99.39 73.07 85.93 67.14 80.21 81.15\nGCGLNet [59] RGB-Thermal 99.39 77.57 81.01 81.90 76.31 83.24\nDHFNet [60] RGB-Thermal 99.44 78.15 87.38 71.18 74.81 82.19\nMDRNet+ [61] RGB-Thermal 99.07 63.04 76.27 63.47 71.26 74.62\nFDCNet [50] RGB-Thermal 99.15 71.52 72.17 70.36 72.36 77.11\nCBAM [41] RGB-Thermal 99.43 73.81 82.75 80.00 69.60 81.12\nEGFNet [44] RGB-Thermal 99.55 79.97 90.62 76.08 80.88 85.42\nCACFNet [62] RGB-Thermal 99.57 82.08 89.49 80.90 80.76 86.56\nMMSFormer (Ours) RGB-Thermal 99.60 81.45 89.86 89.65 76.68 87.45\nTABLE 6: Performance comparison (% mIoU) on Mul-\ntimodal Material Segmentation (MCubeS) [9] dataset for\ndifferent modality combinations. Here A, D, and N repre-\nsent angle of linear polarization (AoLP), degree of linear\npolarization (DoLP), and near-infrared (NIR) respectively.\nModalities MCubeSNet [9] CMNeXt [26] MMSFormer (Ours)\nRGB 33.70 48.16 50.44\nRGB-A 39.10 48.42 51.30\nRGB-A-D 42.00 49.48 52.03\nRGB-A-D-N 42.86 51.54 53.11\nTable 6. Our model exclusively trained on RGB data pro-\nvided an mIoU score of 50.44%, which is 2.28% grater than\nthe current state-of-the-art model. We observe progressive\nimprovement in performance as we incorporated additional\nmodalities: AoLP, DoLP, and NIR. The integration led to\nincremental performance gains, with the mIoU increasing\nfrom 50.44% to 51.30%, then to 52.03%, and ultimately\nreaching to 53.11%. These findings serve as a compelling\nevidence that our fusion approach effectively leverages and\nfuses valuable information from different combination of\nmodalities, resulting in a notable enhancement in segmen-\ntation performance.\nFurthermore, our model consistently outperforms the cur-\nrent state-of-the-art benchmark across all modality combina-\ntions. This consistent superiority underscores the robustness\nand versatility of our fusion block, demonstrating its abil-\nity to adapt and excel regardless of the specific modality\ncombination provided.\nE. Qualitative analysis of the Predictions\nApart from quantitative analysis, we also perform qualitative\nanalysis of the predicted segmentation maps. We show ma-\nterial segmentation results predicted by CMNeXt [26] model\nand our proposed MMSFormer model in Figure 2a. For\nbrevity, we only show RGB images and ground truth material\nsegmentation maps in the illustrations. We show RGB only\npredictions and all modalities (RGB-A-D-N) predictions\nfor both of the models. As highlighted in the rectangular\nbounding boxes, our proposed MMSFormer model identifies\nasphalt, sand and water with greater accuracy than CMNeXt\n[26] model for both RGB only and all modalities (RGB-A-\nD-N) predictions.\nWe also compare our prediction on PST900 [30] dataset\nwith RTFNet [20] and FDCNet [50] on Figure 2b. We\nshow the input RGB image, thermal images, ground truth\nsegmentation maps and prediction form the models. As\nhighlighted by the rectangular bounding boxes, our model\nshows better accuracy in detecting objects with more precise\ncontours compared to the other two methods.\nF. Ablation Study on the Fusion Block\nWe conducted a number of ablation studies aimed at investi-\ngating the contributions of individual components within the\n8 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n<Society logo(s) and publication title will appear here. >\n(a) Visualization of predictions on MCubeS dataset for different modality combinations\n(b) Visualization of predictions on FMB dataset for different modality combinations\nFIGURE 3: Visualization of predicted segmentation maps for different modality combinations on MCubeS [9] and FMB [29]\ndatasets. Both figures show that prediction accuracy increases as we incrementally add new modalities. They also illustrate\nthe fusion block’s ability to effectively combine information from different modality combinations.\nfusion block to the overall model performance. The findings,\nas detailed in Table 7, shed light on the critical importance of\nthese components. We used both RGB and infrared modali-\nties of the FMB dataset during training and testing in these\nexperiments. First, we observed that the absence of channel\nattention in the residual connection had a negative impact,\nresulting in a reduction in performance by 3.36%. This\nindicates that feature calibration along channel dimension\nplays an important role in capturing and leveraging crucial\ninformation effectively. Additionally, while comparing larger\nconvolution kernel sizes ( 3 × 3, 7 × 7, and 11 × 11) to the\noriginally employed ( 3 × 3, 5 × 5, and 7 × 7), we noted a\ndecrease in performance by 5.36%. This result underscores\nthe significance of the carefully chosen convolution kernel\nsizes within the fusion block.\nFurthermore, completely removing the parallel convolu-\ntions from the block led to a performance decline of 4.51%,\nemphasizing their substantial contribution in capturing multi-\nscale features and overall model performance. Finally, if we\nonly use the linear fusion layer to fuse the features and\nremove the parallel convolutions and channel attention from\nthe fusion block, performance drop significantly by 9.25%.\nThese studies demonstrate that multi-scale feature capturing\nvia parallel convolutions and channel-wise feature calibration\nusing channel attention is extremely important in learning\nbetter feature representation and thus crucial to overall\nmodel performance. These comprehensive ablation studies\ncollectively underscore the significance of every component\nwithin the fusion block, revealing that each module plays a\ndistinct and vital role in achieving the overall performance\nof our model.\nVOLUME , 9\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nReza et al.: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation\nTABLE 7: Ablation study of the Fusion Block on FMB [29] dataset. Both RGB and infrared input modalities were\nused during training and testing. The table shows the contribution of different modules in fusion block in overall model\nperformance.\nStructure Parameter Count (M) % mIoU (Change)\nMMSFormer 61.26 61.68\n- without channel attention 61.21 58.32 (-3.36)\n- without parallel convolutions 61.17 57.17 (-4.51)\n- with 3x3, 7x7 and 11x11 convolutions 61.36 56.32 (-5.36)\n- only linear fusion 59.57 52.43 (-9.25)\nTABLE 8: Per class % IoU comparison on Multimodal Material Segmentation (MCubeS) [9] dataset for different modality\ncombinations. As we add modalities incrementally, overall performance increases gradually. This table also shows that\nspecific modality combinations assist in identifying specific types of materials better.\nModalities\nAsphalt\nConcrete\nMetal\nRoad marking\nFabric\nGlass\nPlaster\nPlastic\nRubber\nSand\nGravel\nCeramic\nCobblestone\nBrick\nGrass\nWood\nLeaf\nWater\nHuman\nSky\nMean\nRGB 83.2 44.2 52.1 70.4 31.0 51.6 1.3 26.2 21.8 65.0 61.8 31.3 72.5 45.0 55.4 46.0 74.7 56.0 22.7 96.4 50.4\nRGB-A 86.5 46.5 55.9 73.0 35.3 56.0 0.8 27.3 27.8 66.2 67.0 28.6 69.6 43.0 57.6 49.6 76.4 53.8 8.4 96.6 51.3\nRGB-A-D 86.0 44.0 55.5 68.1 31.9 54.8 2.3 30.0 29.7 69.4 73.7 32.2 69.4 41.4 59.2 48.3 76.6 50.6 20.9 96.7 52.0\nRGB-A-D-N 88.0 48.3 56.2 72.2 35.4 54.9 0.5 34.6 29.4 67.2 69.0 29.9 73.4 44.7 59.5 47.8 77.1 50.5 26.9 96.6 53.1\nTABLE 9: Comparison of number of parameters in the\nfusion block and GFLOPs for MCubeS dataset having 4\ninput modalities with an input shape of (3 × 512 × 512) for\neach modality. Our fusion block shows significantly lower\ncomplexity compared to existing methods.\nMethods Fusion Block Parameters (M) GFLOPs\nCMNeXt [26] 16.63 6.47\nMCubeSNet [9] 7.41 12.10\nHRFuser [25] 1.72 17.50\nCMX [24] 16.59 6.41\nDDF (Resnet-101) [39] 28.10 4.10\nMMSFormer (Ours) 3.23 2.47\nG. Ablation Study on Different Modality Combinations\nTo analyze the contributions of different modalities in the\nidentification of distinct materials, we conducted a series\nof ablation studies, focusing on per-class IoU for differ-\nent modality combinations. The insights are summarized\nin Table 8. As we progressively integrate new modalities,\nperformance gradually increases for specific classes, which\ninclude grass, leaf, asphalt, cobblestone and plastic classes.\nParticularly noteworthy is the assistance provided by NIR\ndata in classifying asphalt, concrete, plastic, cobblestone, and\nhuman categories, leading to significant performance gains\nin these classes as NIR was added as an additional modality.\nConversely, certain classes such as water and brick ex-\nhibited a gradual performance decline as we introduced\nadditional modalities. This suggests that RGB data alone\nsuffices for accurately identifying these classes, and the\ninclusion of more modalities potentially introduces noise or\nredundancy that negatively impacts performance. Moreover,\nAoLP appears to be helpful in enhancing the recognition\nof materials like road markings, glass and wood. Similarly,\nDoLP improved performance for classes like plaster, rubber,\nsand, gravel and ceramic. These findings underscore the rela-\ntionship between different imaging modalities and the unique\ncharacteristics of different types of materials, demonstrating\nthat specific modalities excel in detecting particular classes\nbased on their distinctive traits.\nIn Figure 3a, we presents some examples to show how\nadding different modalities help improve performance of seg-\nmentation. We show predictions for RGB, RGB-A, RGB-A-\nD and RGB-A-D-N inputs from our proposed MMSFormer\nmodel. As we add new modalities, the predictions become\nmore accurate as shown in the bounding boxes. The illus-\ntrations show that the identification of concrete and gravel\nbecomes more accurate with additional modalities. Figure 3b\nshows predictions for RGB and RGB-Infrared from FMB\ndataset. As highlighted by the bounding boxes, adding new\nmodality helps improve performance in detecting building,\nroad and sidewalks. This also illustrates the capability of the\nfusion block to effectively fuse information from different\nmodality combinations.\n10 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n<Society logo(s) and publication title will appear here. >\nH. Computational Complexity of the Fusion Block\nIn addition to better performance, our fusion block is also\ncomputationally efficient compared to most of the fusion\nblocks proposed for these datasets. We show a comparison in\nterms of the number of parameters in the fusion block and\nGFLOPs for some recent models on Table 9 for MCubeS\ndataset having 4 input modalities with an input shape of\n(3 × 512 × 512) for each modality. As observed from the\ntable, our proposed fusion strategy demonstrates significantly\nlower complexity in terms of both the number of parameters\nand GFLOPs compared to existing methods. HRFuser [25]\nhas a lower parameter count than ours but it requires more\nthan 7× GFLOPs. Other methods require significantly more\nparameters (2.3×− 8.7×) and GFLOPs (1.6×− 7×) com-\npared to our fusion strategy. Our comparison only includes\nmodels for which these results are available in the published\nliterature.\nV. CONCLUSION\nIn this paper, we introduce a novel fusion module designed\nto combine useful information from various modality com-\nbinations. We also propose a new model called MMSFormer\nthat integrates the proposed fusion block to accomplish mul-\ntimodal material and semantic segmentation tasks. Experi-\nmental results illustrate the model’s capability to efficiently\nfuse information from different combination of modalities,\nleading to new state-of-the-art performance on three different\ndatasets. Experiments also show that the fusion block can\nextract useful information from different modality combi-\nnations that helps the model to consistently outperform\ncurrent state-of-the-art models. Starting from only one input\nmodality, performance increases gradually as we add new\nmodalities. Several ablation studies further highlight how\ndifferent components of the fusion block contribute to the\noverall model performance. Ablation studies also reveal that\ndifferent modalities assist in identifying different types of\nmaterial classes. However, one limitation of the proposed\nmodel is the use of modality specific encoders and the\nnumber of encoders grows with number of modalities. Future\nwork will include exploring the possibility and effectiveness\nof using a shared encoder for all the modalities, investigating\nand extending the model’s capability with other modalities\nand multimodal tasks.\nREFERENCES\n[1] H.-D. Cheng, X. H. Jiang, Y . Sun, and J. Wang, “Color image\nsegmentation: advances and prospects,” Pattern recognition, vol. 34,\nno. 12, pp. 2259–2281, 2001.\n[2] S. Minaee, Y . Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and\nD. Terzopoulos, “Image segmentation using deep learning: A survey,”\nIEEE transactions on pattern analysis and machine intelligence ,\nvol. 44, no. 7, pp. 3523–3542, 2021.\n[3] Y . Guo, Y . Liu, T. Georgiou, and M. S. Lew, “A review of semantic\nsegmentation using deep neural networks,” International journal of\nmultimedia information retrieval , vol. 7, pp. 87–93, 2018.\n[4] P. Wang, P. Chen, Y . Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell,\n“Understanding convolution for semantic segmentation,” in 2018 IEEE\nwinter conference on applications of computer vision (WACV) . Ieee,\n2018, pp. 1451–1460.\n[5] W. Gu, S. Bai, and L. Kong, “A review on 2d instance segmentation\nbased on deep neural networks,” Image and Vision Computing , vol.\n120, p. 104401, 2022.\n[6] A. M. Hafiz and G. M. Bhat, “A survey on instance segmentation: state\nof the art,” International journal of multimedia information retrieval ,\nvol. 9, no. 3, pp. 171–189, 2020.\n[7] O. Elharrouss, S. A. Al-Maadeed, N. Subramanian, N. Ottakath,\nN. Almaadeed, and Y . Himeur, “Panoptic segmentation: A review,”\nArXiv, vol. abs/2111.10250, 2021.\n[8] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Doll ´ar, “Panoptic\nsegmentation,” in Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , 2019, pp. 9404–9413.\n[9] Y . Liang, R. Wakaki, S. Nobuhara, and K. Nishino, “Multimodal\nmaterial segmentation,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , June 2022, pp.\n19 800–19 808.\n[10] P. Upchurch and R. Niu, “A dense material segmentation dataset\nfor indoor and outdoor scene parsing,” in European Conference on\nComputer Vision. Springer, 2022, pp. 450–466.\n[11] Z. Guo, X. Li, H. Huang, N. Guo, and Q. Li, “Deep learning-\nbased image segmentation on multimodal medical imaging,” IEEE\nTransactions on Radiation and Plasma Medical Sciences, vol. 3, no. 2,\npp. 162–169, 2019.\n[12] Y . Zhang, D. Sidib ´e, O. Morel, and F. M ´eriaudeau, “Deep multimodal\nfusion for semantic image segmentation: A survey,” Image and Vision\nComputing, vol. 105, p. 104042, 2021.\n[13] S. Minaee, Y . Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and\nD. Terzopoulos, “Image segmentation using deep learning: A survey,”\nIEEE transactions on pattern analysis and machine intelligence ,\nvol. 44, no. 7, pp. 3523–3542, 2021.\n[14] Y . Zhang, D. Sidib ´e, O. Morel, and F. M ´eriaudeau, “Deep multimodal\nfusion for semantic image segmentation: A survey,” Image and Vision\nComputing, vol. 105, p. 104042, 2021.\n[15] X. Chen, K.-Y . Lin, J. Wang, W. Wu, C. Qian, H. Li, and G. Zeng,\n“Bi-directional cross-modality feature propagation with separation-\nand-aggregation gate for rgb-d semantic segmentation,” in European\nConference on Computer Vision (ECCV) , 2020, pp. 561–577.\n[16] C. Hazirbas, L. Ma, C. Domokos, and D. Cremers, “Fusenet: in-\ncorporating depth into semantic segmentation via fusion-based cnn\narchitecture,” in Asian Conference on Computer Vision , November\n2016.\n[17] X. Hu, K. Yang, L. Fei, and K. Wang, “Acnet: Attention based network\nto exploit complementary features for rgbd semantic segmentation,” in\nIEEE International Conference on Image Processing (ICIP), 2019, pp.\n1440–1444.\n[18] P. Li, J. Chen, B. Lin, and X. Xu, “Residual spatial fusion network for\nrgb-thermal semantic segmentation,” arXiv preprint arXiv:2306.10364,\n2023.\n[19] M. Liang, J. Hu, C. Bao, H. Feng, F. Deng, and T. L. Lam, “Explicit\nattention-enhanced fusion for rgb-thermal perception tasks,” IEEE\nRobotics Autom. Lett. , vol. 8, no. 7, pp. 4060–4067, 2023.\n[20] Y . Sun, W. Zuo, and M. Liu, “RTFNet: RGB-Thermal Fusion Network\nfor Semantic Segmentation of Urban Scenes,” IEEE Robotics and\nAutomation Letters, vol. 4, no. 3, pp. 2576–2583, July 2019.\n[21] J. Li, H. Dai, H. Han, and Y . Ding, “Mseg3d: Multi-modal 3d semantic\nsegmentation for autonomous driving,” in 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR). Los Alamitos,\nCA, USA: IEEE Computer Society, jun 2023, pp. 21 694–21 704.\n[22] A. Prakash, K. Chitta, and A. Geiger, “Multi-modal fusion transformer\nfor end-to-end autonomous driving,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2021, pp.\n7077–7087.\n[23] L. Zhao, H. Zhou, X. Zhu, X. Song, H. Li, and W. Tao, “Lif-seg:\nLidar and camera image fusion for 3d lidar semantic segmentation,”\nIEEE Transactions on Multimedia , pp. 1–11, 2023.\n[24] J. Zhang, H. Liu, K. Yang, X. Hu, R. Liu, and R. Stiefelhagen, “Cmx:\nCross-modal fusion for rgb-x semantic segmentation with transform-\ners,” IEEE Transactions on Intelligent Transportation Systems , 2023.\n[25] T. Broedermann, C. Sakaridis, D. Dai, and L. Van Gool, “Hrfuser: A\nmulti-resolution sensor fusion architecture for 2d object detection,” in\nIEEE International Conference on Intelligent Transportation Systems\n(ITSC), 2023.\n[26] J. Zhang, R. Liu, H. Shi, K. Yang, S. Reiß, K. Peng, H. Fu, K. Wang,\nand R. Stiefelhagen, “Delivering arbitrary-modal semantic segmenta-\nVOLUME , 11\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nReza et al.: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation\ntion,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2023, pp. 1136–1147.\n[27] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and\nP. Luo, “Segformer: Simple and efficient design for semantic segmen-\ntation with transformers,” in Neural Information Processing Systems\n(NeurIPS), 2021.\n[28] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7132–7141.\n[29] J. Liu, Z. Liu, G. Wu, L. Ma, R. Liu, W. Zhong, Z. Luo, and\nX. Fan, “Multi-interactive feature learning and a full-time multi-\nmodality benchmark for image fusion and segmentation,” in Inter-\nnational Conference on Computer Vision , 2023.\n[30] S. S. Shivakumar, N. Rodrigues, A. Zhou, I. D. Miller, V . Kumar, and\nC. J. Taylor, “Pst900: Rgb-thermal calibration, dataset and segmen-\ntation network,” in 2020 IEEE International Conference on Robotics\nand Automation (ICRA) , 2020, pp. 9441–9447.\n[31] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2015, pp. 3431–3440.\n[32] H. Wu, J. Zhang, K. Huang, K. Liang, and Y . Yu, “Fastfcn: Rethinking\ndilated convolution in the backbone for semantic segmentation,” arXiv\npreprint arXiv:1903.11816, 2019.\n[33] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional\nnetworks for biomedical image segmentation,” in Medical Image\nComputing and Computer-Assisted Intervention–MICCAI 2015: 18th\nInternational Conference, Munich, Germany, October 5-9, 2015, Pro-\nceedings, Part III 18 . Springer, 2015, pp. 234–241.\n[34] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam,\n“Encoder-decoder with atrous separable convolution for semantic\nimage segmentation,” in Proceedings of the European conference on\ncomputer vision (ECCV) , 2018, pp. 801–818.\n[35] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\nnetwork,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2017, pp. 2881–2890.\n[36] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,\nand L. Shao, “Pyramid vision transformer: A versatile backbone\nfor dense prediction without convolutions,” in Proceedings of the\nIEEE/CVF international conference on computer vision , 2021, pp.\n568–578.\n[37] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar,\n“Masked-attention mask transformer for universal image segmenta-\ntion,” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , 2022, pp. 1290–1299.\n[38] J. Chen, X. Wang, Z. Guo, X. Zhang, and J. Sun, “Dynamic region-\naware convolution,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , 2021, pp. 8064–8073.\n[39] J. Zhou, V . Jampani, Z. Pi, Q. Liu, and M.-H. Yang, “Decoupled\ndynamic filter networks,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2021, pp. 6647–6656.\n[40] H. R. V . Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, “Mmtm:\nMultimodal transfer module for cnn fusion,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition ,\n2020, pp. 13 289–13 299.\n[41] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional\nblock attention module,” in Proceedings of the European conference\non computer vision (ECCV) , 2018, pp. 3–19.\n[42] W. Zhou, J. Liu, J. Lei, L. Yu, and J.-N. Hwang, “Gmnet: Graded-\nfeature multilabel-learning network for rgb-thermal urban scene\nsemantic segmentation,” IEEE Transactions on Image Processing ,\nvol. 30, pp. 7790–7802, 2021.\n[43] G. Li, Y . Wang, Z. Liu, X. Zhang, and D. Zeng, “Rgb-t semantic\nsegmentation with location, activation, and sharpening,” IEEE Trans-\nactions on Circuits and Systems for Video Technology , vol. 33, no. 3,\npp. 1223–1235, 2023.\n[44] S. Dong, W. Zhou, C. Xu, and W. Yan, “Egfnet: Edge-aware guidance\nfusion network for rgb–thermal urban scene parsing,” IEEE Transac-\ntions on Intelligent Transportation Systems , pp. 1–13, 2023.\n[45] F. Deng, H. Feng, M. Liang, H. Wang, Y . Yang, Y . Gao, J. Chen, J. Hu,\nX. Guo, and T. L. Lam, “Feanet: Feature-enhanced attention network\nfor rgb-thermal real-time semantic segmentation,” in 2021 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) .\nIEEE Press, 2021, p. 4467–4473.\n[46] Z. Zhao, S. Xu, C. Zhang, J. Liu, J. Zhang, and P. Li, “Didfuse: Deep\nimage decomposition for infrared and visible image fusion,” in IJCAI.\nijcai.org, 2020, pp. 970–976.\n[47] Z. Huang, J. Liu, X. Fan, R. Liu, W. Zhong, and Z. Luo, “Reconet: Re-\ncurrent correction network for fast and efficient multi-modality image\nfusion,” in Computer Vision – ECCV 2022 , S. Avidan, G. Brostow,\nM. Ciss ´e, G. M. Farinella, and T. Hassner, Eds. Cham: Springer\nNature Switzerland, 2022, pp. 539–555.\n[48] H. Xu, J. Ma, J. Jiang, X. Guo, and H. Ling, “U2fusion: A unified\nunsupervised image fusion network,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , vol. 44, no. 1, pp. 502–518, 2022.\n[49] J. Liu, X. Fan, Z. Huang, G. Wu, R. Liu, W. Zhong, and Z. Luo,\n“Target-aware dual adversarial learning and a multi-scenario multi-\nmodality benchmark to fuse infrared and visible for object detection,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 5802–5811.\n[50] S. Zhao and Q. Zhang, “A feature divide-and-conquer network for rgb-\nt semantic segmentation,” IEEE Transactions on Circuits and Systems\nfor Video Technology, vol. 33, no. 6, pp. 2892–2905, 2023.\n[51] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu, “Ccnet:\nCriss-cross attention for semantic segmentation,” in 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , 2019, pp. 603–\n612.\n[52] J. Liu, J. He, J. Zhang, J. S. Ren, and H. Li, “Efficientfcn: Holistically-\nguided decoding for semantic segmentation,” in Computer Vision –\nECCV 2020. Springer International Publishing, 2020, pp. 1–17.\n[53] W. Zhou, S. Dong, C. Xu, and Y . Qian, “Edge-aware guidance fusion\nnetwork for rgb–thermal scene parsing,” in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 36, 2022, pp. 3571–3579.\n[54] W. Zhou, S. Dong, J. Lei, and L. Yu, “Mtanet: Multitask-aware\nnetwork with hierarchical multimodal fusion for rgb-t urban scene\nunderstanding,” IEEE Transactions on Intelligent Vehicles , vol. 8,\nno. 1, pp. 48–58, 2023.\n[55] W. Zhou, X. Lin, J. Lei, L. Yu, and J.-N. Hwang, “Mffenet: Multiscale\nfeature fusion and enhancement network for rgb–thermal urban road\nscene parsing,” IEEE Transactions on Multimedia , vol. 24, pp. 2526–\n2538, 2022.\n[56] J. Wang, K. Song, Y . Bao, L. Huang, and Y . Yan, “Cgfnet: Cross-\nguided fusion network for rgb-t salient object detection,” IEEE Trans-\nactions on Circuits and Systems for Video Technology , vol. 32, no. 5,\npp. 2949–2961, 2022.\n[57] J. Liu, W. Zhou, Y . Cui, L. Yu, and T. Luo, “Gcnet: Grid-like context-\naware network for rgb-thermal semantic segmentation,” Neurocomput.,\nvol. 506, no. C, p. 60–67, sep 2022.\n[58] S. Dong, W. Zhou, X. Qian, and L. Yu, “Gebnet: Graph-enhancement\nbranch network for rgb-t scene parsing,” IEEE Signal Processing\nLetters, vol. 29, pp. 2273–2277, 2022.\n[59] T. Gong, W. Zhou, X. Qian, J. Lei, and L. Yu, “Global contextually\nguided lightweight network for rgb-thermal urban scene understand-\ning,” Engineering Applications of Artificial Intelligence , vol. 117, p.\n105510, 2023.\n[60] Y . Cai, W. Zhou, L. Zhang, L. Yu, and T. Luo, “Dhfnet:\ndual-decoding hierarchical fusion network for rgb-thermal semantic\nsegmentation,” The Visual Computer , pp. 1–11, 2023. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:256320037\n[61] S. Zhao, Y . Liu, Q. Jiao, Q. Zhang, and J. Han, “Mitigating modality\ndiscrepancies for rgb-t semantic segmentation,” IEEE Transactions on\nNeural Networks and Learning Systems , pp. 1–15, 2023.\n[62] W. Zhou, S. Dong, M. Fang, and L. Yu, “Cacfnet: Cross-modal\nattention cascaded fusion network for rgb-t urban scene parsing,” IEEE\nTransactions on Intelligent Vehicles, pp. 1–10, 2023.\n[63] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n“Imagenet: A large-scale hierarchical image database,” in 2009 IEEE\nConference on Computer Vision and Pattern Recognition , 2009, pp.\n248–255.\n[64] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin International Conference on Learning Representations , 2019.\n12 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2024.3389812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Modalities",
  "concepts": [
    {
      "name": "Modalities",
      "score": 0.6763207912445068
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.633324146270752
    },
    {
      "name": "Computer science",
      "score": 0.6134597659111023
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5115898251533508
    },
    {
      "name": "Segmentation",
      "score": 0.5096960067749023
    },
    {
      "name": "Information retrieval",
      "score": 0.41304105520248413
    },
    {
      "name": "Information fusion",
      "score": 0.4110296070575714
    },
    {
      "name": "Natural language processing",
      "score": 0.33588141202926636
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I103635307",
      "name": "University of California, Riverside",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1280414376",
      "name": "United States Air Force Research Laboratory",
      "country": "US"
    }
  ]
}