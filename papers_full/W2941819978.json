{
  "title": "Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models",
  "url": "https://openalex.org/W2941819978",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2102602022",
      "name": "Takashi Wada",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108993706",
      "name": "Tomoharu Iwata",
      "affiliations": [
        "RIKEN Center for Advanced Intelligence Project"
      ]
    },
    {
      "id": "https://openalex.org/A2112966360",
      "name": "Yuji Matsumoto",
      "affiliations": [
        "RIKEN Center for Advanced Intelligence Project",
        "Nara Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2294774419",
    "https://openalex.org/W2963467085",
    "https://openalex.org/W4293763774",
    "https://openalex.org/W2888536529",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2594021297",
    "https://openalex.org/W2964266061",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W22168010",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W114517082",
    "https://openalex.org/W2171082019",
    "https://openalex.org/W2952190837",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2889894161",
    "https://openalex.org/W2760424551",
    "https://openalex.org/W2467585580",
    "https://openalex.org/W2007780422",
    "https://openalex.org/W4234117503",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W2899486018",
    "https://openalex.org/W2963047628",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2741986357",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2118090838",
    "https://openalex.org/W2740132093",
    "https://openalex.org/W2788353357",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2807669488"
  ],
  "abstract": "Recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. However, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. To overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. Our proposed model, which we call 'Multilingual Neural Language Models', shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. The model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. Our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3113–3124\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n3113\nUnsupervised Multilingual Word Embedding with Limited Resources\nusing Neural Language Models\nTakashi Wada1, Tomoharu Iwata2,3, and Yuji Matsumoto1,3\n1Nara Institute of Science and Technology\n2NTT Communication Science Laboratories\n3RIKEN Center for Advanced Intelligence Project (AIP)\n1{wada.takashi.wp7,matsu}@is.naist.jp\n2tomoharu.iwata.gy@hco.ntt.co.jp\nAbstract\nRecently, a variety of unsupervised methods\nhave been proposed that map pre-trained word\nembeddings of different languages into the\nsame space without any parallel data. These\nmethods aim to ﬁnd a linear transformation\nbased on the assumption that monolingual\nword embeddings are approximately isomor-\nphic between languages. However, it has been\ndemonstrated that this assumption holds true\nonly on speciﬁc conditions, and with limited\nresources, the performance of these methods\ndecreases drastically. To overcome this prob-\nlem, we propose a new unsupervised multi-\nlingual embedding method that does not rely\non such assumption and performs well under\nresource-poor scenarios, namely when only\na small amount of monolingual data (i.e.,\n50k sentences) are available, or when the do-\nmains of monolingual data are different across\nlanguages. Our proposed model, which we\ncall ‘Multilingual Neural Language Models’,\nshares some of the network parameters among\nmultiple languages, and encodes sentences of\nmultiple languages into the same space. The\nmodel jointly learns word embeddings of dif-\nferent languages in the same space, and gen-\nerates multilingual embeddings without any\nparallel data or pre-training. Our experi-\nments on word alignment tasks have demon-\nstrated that, on the low-resource condition,\nour model substantially outperforms existing\nunsupervised and even supervised methods\ntrained with 500 bilingual pairs of words. Our\nmodel also outperforms unsupervised meth-\nods given different-domain corpora across lan-\nguages. Our code is publicly available1.\n1 Introduction\nLearning cross-lingual or multilingual word em-\nbedding has been recognised as a very impor-\ntant research topic in natural language processing\n1https://github.com/twadada/multilingual-nlm\n(NLP). Its objective is to map monolingual word\nembeddings of different languages into a com-\nmon space, and this research has been applied to\nmany multilingual tasks such as machine transla-\ntion (Zou et al., 2013) and bilingual named entity\nrecognition (Rudramurthy et al., 2016). It also en-\nables the transfer of knowledge from one language\ninto another (Xiao and Guo, 2014; Adams et al.,\n2017).\nA number of supervised and unsupervised\nmethods have been proposed that obtain cross-\nlingual word embeddings. Both supervised and\nunsupervised methods aim to ﬁnd such a linear\ntransformation that maps word embeddings in a\nsource language into a target language space. Su-\npervised methods employ bilingual dictionaries to\nlearn the mapping (Mikolov et al., 2013b; Xing\net al., 2015; Smith et al., 2017; Artetxe et al.,\n2018a), while unsupervised ones utilise the sim-\nilarities or distance of word embeddings spaces\nacross different languages (Conneau et al., 2018;\nZhang et al., 2017a; Xu et al., 2018; Artetxe et al.,\n2018b).\nSince the common objective of most of the\nsupervised and unsupervised methods is to ﬁnd\nan orthogonal linear mapping between languages,\nthey heavily rely on the assumption that mono-\nlingual word embeddings are approximately iso-\nmorphic. However, Søgaard et al. (2018) have\nfound that this assumption does not hold true in\ngeneral, and demonstrated that it requires three\nspeciﬁc conditions for the unsupervised method\nof Conneau et al. (2018) to perform well. The\nconditions are: Languages to align are linguisti-\ncally similar; Monolingual word embeddings are\ntrained by the same algorithms; And the domains\nof the monolingual corpora are similar across lan-\nguages. In particular, the last condition is hard\nto assume when dealing with resource-poor lan-\nguages, for which unsupervised methods can be\n3114\nbeneﬁcial in reality.\nTo overcome the limitations of previous work,\nwe propose a new unsupervised multilingual word\nembedding method called Multilingual Neural\nLanguage Model (MNLM). In what follows, we\nsummarise our main contributions and novelty of\nour proposed model:\nContributions\n•We have discovered another limitation of the\nexisting unsupervised methods: They do not\nperform well under the low-resource condi-\ntion, namely when only small monolingual\ncorpora (i.e., 50k sentences) are available in\nsource and/or target languages. We have also\nconﬁrmed that word embeddings are far from\nbeing isomorphic across languages under this\ncondition, indicating that the conventional\napproachs are effective only for resource-rich\nlanguages. This is a serious problem since\nunsupervised learning is supposed to be ben-\neﬁcial when dealing with low-resource lan-\nguages.\n•We propose a new unsupervised multilin-\ngual word embedding method that overcomes\nthe limitations of the existing methods. Our\napproach can successfully obtain multilin-\ngual word embeddings under the challeng-\ning conditions when only small monolingual\ncorpora are available, or when the domains\nof the monolingual corpora are different\nacross languages (we deﬁne these conditions\nas ‘low-resource condition’ and ‘different-\ndomain condition’, respectively).\nNovelty of Our Proposed Model\nWhereas the existing unsupervised methods aim\nto map pre-trained word embeddings between lan-\nguages based on the strong assumption that mono-\nlingual word embeddings are approximately iso-\nmorphic, our method does not require such as-\nsumption or pre-trained word embeddings; in-\nstead, it learns multilingual word embeddings\njointly using forward and backward LSTM lan-\nguage models (Mikolov et al., 2010). Our model\nshares the language models among multiple lan-\nguages and aims to learn a common sequential\nstructure of different languages such as a com-\nmon basic word order rule (e.g., subject-verb-\nobject). The word embeddings of each lan-\nguage are trained independently, but sharing the\nLSTM networks encourages the embeddings to be\nmapped into the same space, generating multilin-\ngual word embeddings. Our experiments show\nthat our unique approach makes it possible to ob-\ntain multilingual word embeddings with limited\nresources.\n2 Related Work\nMikolov et al. (2013b) have proposed to obtain\ncross-lingual word representations by learning a\nlinear mapping between two monolingual word\nembedding spaces. Later, Xing et al. (2015)\nhave shown that enforcing an orthogonality con-\nstraint on the mapping improves the performance,\nand that offers a closed form Procrustes solution\nobtained from the singular value decomposition\n(SVD) of YXT\nW∗ = arg min\nW\n∥WX −Y∥2 = UV T,\ns.t. UΣVT = SVD(YXT),\n(1)\nwhere W is a mapping matrix and Σ is a diagonal\nmatrix.\nFollowing this work, a variety of unsupervised\nmethods have been proposed that obtain cross-\nlingual representations without any bilingual su-\npervision. Zhang et al. (2017a) have proposed an\nunsupervised method that obtains the linear trans-\nformation using adversarial training (Goodfellow\net al., 2014): during the training, a discriminator is\ntrained to distinguish between the mapped source\nembeddings and the target embeddings, while the\nmapping matrix is trained to fool the discrimina-\ntor. Conneau et al. (2018) employ a similar ap-\nproach to Zhang et al. (2017a); they acquire an\ninitial matrix using adversarial training and re-\nﬁne it by solving the orthogonal Procrustes prob-\nlem. Zhang et al. (2017b) and Xu et al. (2018)\nobtain cross-lingual representations by minimis-\ning the earth-mover’s distance and Sinkhorn dis-\ntance, respectively. Artetxe et al. (2018b) pro-\npose an unsupervised self-learning method. Their\nmethod starts from roughly aligning words across\nlanguages using structural similarities of word em-\nbedding spaces, and reﬁnes the word alignment\nby repeating a robust self-learning method until\nconvergence. They show that their approach is\nmore effective than Zhang et al. (2017a) and Con-\nneau et al. (2018) when languages to align are\ndistant or monolingual corpora are not compara-\nble across language. Recently, Chen and Cardie\n(2018) and Alaux et al. (2018) have proposed un-\n3115\nsupervised multilingual word embedding meth-\nods. Their methods map word embeddings of\nmore than two languages into a common space by\ncapturing the inter-dependencies among multiple\nlanguages.\n3 Our Model\n3.1 Overview\nWe propose a new unsupervised multilingual word\nembeddings method called Multilingual Neural\nLanguage Model. Fig.1 brieﬂy illustrates our pro-\nposed model. The model consists of bidirectional\nlanguage models similar to ELMo (Peters et al.,\n2018), and most of the parameters are shared\namong multiple languages. In what follows, we\nsummaries which parameters are shared across\nlanguages or speciﬁc to each language:\n•Shared Parameters\n– − →f and ← −f : LSTM networks which per-\nform as forward and backward language\nmodels, independently.\n– EBOSfwd and EBOSbkw : The embed-\ndings of initial inputs to the forward\nand backward language models, respec-\ntively.\n– WEOS: The linear mapping for\n<EOS>, which is used to calculate the\nprobability of the end of a sentence at\nevery time-step.\n•Speciﬁc Parameters to Language ℓ\n– Eℓ: Word embeddings of language ℓ\n– Wℓ: Linear projection of language ℓ,\nwhich is used to calculate the probabil-\nity distribution of the next word.\nThe LSTMs − →f and ← −f are shared among mul-\ntiple languages and capture a common language\nstructure. On the other hand, the word embed-\ndings Eℓ and linear projection Wℓ are speciﬁc to\neach language ℓ. Since different languages are en-\ncoded by the same LSTM functions, similar words\nacross different languages should have a similar\nrepresentation so that the shared LSTMs can en-\ncode them effectively. For instance, suppose our\nmodel encodes an English sentence ‘He drives a\ncar.’ and its Spanish translation ‘El conduce un\ncoche.’ In these sentences, each English word\ncorresponds to each Spanish one in the same or-\nder. Therefore, these equivalent words would have\nFigure 1: Illustration of our proposed model Multilin-\ngual Neural Language Models.\nsimilar representations so that the shared language\nmodels can encode the English and Spanish sen-\ntences effectively. Although in general, each lan-\nguage has its different grammar rules, the shared\nlanguage models are trained to roughly capture\nthe common structure such as common basic word\norder rules (e.g., subject-verb-object) among dif-\nferent languages. Sharing <BOS> and <EOS>\nsymbols ensures that the beginning and end of the\nhidden states are in the same space regardless of\nlanguage, which encourages the model to obtain\nmultilingual representations.\nThe limitation of our model is that it is\nonly applicable to the languages that have com-\nmon word order rules such as subject-verb-object\nand subject-object-verb. Although this limita-\ntion may sound somewhat signiﬁcant, our exper-\niments show that our model performs well not\nonly for closely related language pairs such as\nFrench-English but also for linguistically distant\nlanguages such as English-Finnish 2 and Turkish-\nJapanese. In fact, our experiments show that it is\nextremely difﬁcult for the existing unsupervised\nmethods as well as for our model to align very\ndistant languages which have different word order,\nsuch as English and Japanese.\n3.2 Network Structure\nSuppose a sentence with N words in language ℓ,\n⟨wℓ\n1...,wℓ\nN ⟩. The forward and backward language\nmodels calculate the probability of a next wordwℓ\nt\n2Finnish is often considered as a non-Indo-European\nsynthetic language, whereas English is often regarded as an\nIndo-European analytic language.\n3116\ngiven the previous words:\np(wℓ\n1...,wℓ\nN ) =\nN∏\nt=1\np(wℓ\nt|wℓ\n1...,wℓ\nt−1). (2)\np(wℓ\n1...,wℓ\nN ) =\nN∏\nt=1\np(wℓ\nt|wℓ\nt+1...,wℓ\nN ). (3)\nThe tth hidden states hℓ\nt of the forward and back-\nward language models are calculated based on the\nprevious hidden state and word embedding,\n− →hℓ\nt = − →f(− →hℓ\nt−1,xℓ\nt−1), (4)\n← −hℓ\nt = ← −f(← −hℓ\nt+1,xℓ\nt+1), (5)\nxℓ\nt =\n\n\n\nEBOSfwd if t = 0 ,\nEBOSbkw if t = N+1,\nEℓ(wℓ\nt) otherwise,\n(6)\nwhere − →f(·) and ← −f(·) are the standard LSTM\nfunctions. Note that the same word embedding\nfunction Eℓ is used among the forward and back-\nward language models. The probability distribu-\ntion of the upcoming word wℓ\nt is calculated by\nthe forward and backward models independently\nbased on their current hidden state:\np(wℓ\nt|wℓ\n1...,wℓ\nt−1) = softmax(gℓ(− →hℓ\nt))), (7)\np(wℓ\nt|wℓ\nt+1...,wℓ\nN ) = softmax(gℓ(← −hℓ\nt)), (8)\ngℓ(hℓ\nt) = [Wℓ(hℓ\nt),WEOS(hℓ\nt)], (9)\nwhere [x,y] means the concatenation of xand y.\nWEOS and Wℓ are matrices with the size of (1×d)\nand (Vℓ ×d), where dis the size of hidden state\nand Vℓ is the vocabulary size of language ℓ ex-\ncluding <EOS>. As with the word embeddings,\nthose matrices are shared among the forward and\nbackward language models.\nThe proposed model is trained by maximising\nthe log likelihood of the forward and backward di-\nrections for each language ℓ:\nL∑\nl=1\nSℓ\n∑\ni=1\nNi\n∑\nt=1\nlog p(wℓ\ni,t|wℓ\ni,1...wℓ\ni,t−1; − →θ)\n+ logp(wℓ\ni,t|wℓ\ni,t+1...wℓ\ni,Ni; ← −θ),\nwhere Land Sℓ denote the numbers of languages\nand sentences of language ℓ. − →θ and ← −θ denote the\nparameters for the forward and backward LSTMs− →f and ← −f, respectively.\n4 Experiments\n4.1 Data and Experimental Conditions\nWe trained our model and baselines under the fol-\nlowing two conditions:\n1. low-resource condition: Only small mono-\nlingual corpora are available.\n2. different-domain condition : Relatively\nlarge monolingual corpora are available but\ntheir domains are different across languages.\nOn each condition, we conducted cross-lingual\nand multilingual embedding experiments, respec-\ntively.\n4.1.1 Cross-lingual Word Embedding\nIn the experiments of cross-lingual embedding,\nwe evaluated the quality of cross-lingual embed-\ndings between seven pairs of source-target lan-\nguages: {German, Spanish, French, Russian,\nCzech, Finnish, Japanese }-English. For the low-\nresource condition, we used subsets of News\nCrawl monolingual corpora 3. We used 50k sen-\ntences for source languages, and either 50k or 1M\nsentences for the target language (i.e., English).\nThis condition simulates two realistic scenarios;\nthe case when analysing inter-dependencies be-\ntween multiple minor languages, or between mi-\nnor and major languages.\nFor the different-domain condition, we added\n{Tamil, Turkish }-Japanese pairs and North\nSaami-{Finnish, English}pairs to the seven pairs\ndescribed above. North Saami is one of the minor\nlanguages spoken in northern Finland, Sweden\nand Norway, and it is so close to Finnish that\ntransfer learning between them is very effective\nin dependency parsing (Lim et al., 2018). Note\nthat the basic word order of Tamil, Turkish and\nJapanese is subject-object-verb (SOV), while\nthe one of the other languages is SVO. We used\nEuroparl corpus (Koehn, 2005) for English,\nWikipedia for Japanese, SIKOR North Saami\ncorpus4 for North Saami, and news data for the\nother languages 5. We extracted 1M sentences\n3downloaded from http://www.statmt.org\nand http://wortschatz.uni-leipzig.de/en/\ndownload\n4https://dataverse.no/dataset.xhtml?\npersistentId=doi:10.18710/8AK7KZ\n5The vocabulary sizes of Europarl and News Crawl\ncorpora in English are signiﬁcantly different (79,258 v.s.\n265,368 words), indicating the major differences between\nthese domains\n3117\nfrom these corpora except for North Saami,\nfor which we used the whole corpus which\ncontains 0.75M sentences. This different-domain\ncondition also simulates the cases of analysing\ninter-dependencies among minor languages; large\nmonolingual data containing up to 1M sentences\nmay be available in each language, but it is hard\nto assume that their domains are similar across\nlanguages.\n4.1.2 Multilingual Word Embedding\nWe trained multilingual word embeddings among\nthe four linguistically similar languages: German,\nSpanish, French, and English. We conducted ex-\nperiments under the following three conditions:\n(a) 50k sentences in News Crawl are used for each\nlanguage; (b) 50k sentences in News Crawl are\nused for German, Spanish, French and 1M for En-\nglish; (c) 1M sentences in News Crawl are used\nfor German, Spanish, French and 1M sentences in\nEuroparl for English.\n4.2 Evaluation\nIn our experiment, we evaluated cross-lingual and\nmultilingual word embeddings on the word align-\nment tasks. In the cross-lingual experiments, we\nused 1000 unique pairs of words in the dictionar-\nies and we report p@5 in each language. That is,\nfor each word in the 1000 source words, we ex-\ntracted the 5 most similar words from the 1000\ntarget words and checked how often the correct\ntranslation is included in them. In the multilin-\ngual experiments, we extracted 500 words aligned\namong English, French, Spanish and German and\nevaluated p@5 of ‘joint’ alignment among the four\nlanguages. That is, for each English word we ex-\ntracted the 5 most similar words in French, Ger-\nman and Spanish independently, and evaluated\nhow often the correct translation of the English\nword is included in all of the three languages. In\nmost language pairs, these 1000 and 500 words\nwere extracted from bilingual dictionaries pub-\nlished by Conneau et al. (2018) so that they did not\ncontain any unknown words in all the training set-\ntings6. For North Saami- {Finnish, English }, we\nused the North Saami-Finnish dictionary7 used by\nLim et al. (2018) and aligned it with a Finnish-\nEnglish dictionary published by Conneau et al.\n6For {Tamil, Turkish}-Japanese, we aligned the {Tamil,\nTurkish}-English dictionaries with the Japanese-English dic-\ntionary.\n7https://github.com/jujbob/multilingual-models\n(2018) to build a North Saami-English dictionary.\nWhen only 50k sentences were used both for\nthe source and target languages, we trained all the\nmodels three times with different random seeds\nand calculated the average precision in both the\ncross-lingual and multilingual experiments. This\nis because unsupervised learning with small data\ncan be unstable.\n4.3 Baseline\nBaseline models aim to map pre-trained word em-\nbeddings of different languages into a common\nspace. For a fair comparison to our model, we\nused word2vec (Mikolov et al., 2013a), that pre-\ntrain word embeddings at a token level. We used\ntheir code with the default setting 8 except for the\nembedding size and minimum frequency, which\nwere set the same as our model. Note that these\npre-trained embeddings were used only by base-\nline models, not by ours.\nAs baselines of cross-lingual word embedding\nmethods, we chose Xu et al. (2018), Artetxe et al.\n(2018b), and Conneau et al. (2018) with and with-\nout normalisation. We also compared our model\nagainst (weakly) supervised cross-lingual word\nembedding methods (Artetxe et al., 2018a). The\nsupervised methods exploited 500 pairs of equiva-\nlent words that are not used in the evaluation data9,\nand weakly supervised methods exploited pseudo\nbilingual pairs of words (auto seeds): the words\nwith the same spellings among different languages\nwere deemed as equivalent words. We trained the\ncross-lingual baselines and our model in each lan-\nguage pair.\nAs baselines of multilingual word embedding\nmodels, we used Chen and Cardie (2018) with\nor without auto seeds. We also compared our\nmodel against the cross-lingual baselines. While\nChen and Cardie (2018) and our model jointly\ntrain multilingual word embeddings, the cross-\nlingual models independently map the word em-\nbeddings of German, Spanish, and French into\nthe English embedding space. Regarding Artetxe\net al. (2018b) and Artetxe et al. (2018a), we omit-\nted the re-weighting, whitening, and normalisation\nprocesses in the multilingual experiments 10. This\n8The code is at https://code.google.com/\narchive/p/word2vec, and the default algorithm is Con-\ntinuous Bag of Words (CBOW) with its window size 5\n9These 500 words were also extracted in the same way\nas explained in 4.2.\n10To omit these processes, we used ‘–orthogonal’ option\n3118\nsrc de es fr ru cs ﬁ ja\nMethod\ndata size(tgt) 50k 1M 50k 1M 50k 1M 50k 1M 50k 1M 50k 1M 50k 1M\n(weakly) supervised\nArtetxe et al. (2018b)+char 5.6 2.5 12.1 5.1 9.2 4.0 2.9 1.4 5.0 0.5 1.3 1.6 2.1 9.2\nArtetxe et al. (2018a)+dict 9.6 9.7 15.0 19.7 13.3 19.5 5.7 8.0 5.5 8.0 3.8 5.0 6.1 11.2\nConneau et al. (2018)+dict 11.1 9.7 18.0 20.4 19.2 20.7 4.7 5.2 7.1 4.8 1.7 3.2 7.5 18.7\nunsupervised\nXu et al. (2018) 3.9 0.7 6.8 0.5 4.4 0.2 1.4 1.3 2.7 0.3 0.9 0.5 1.9 0.6\nArtetxe et al. (2018b) 3.9 0.6 7.5 0.8 6.5 1.0 1.0 1.0 0.7 1.1 1.1 1.7 1.6 1.3\nConneau et al. (2018) 3.0 0.8 11.0 0.2 7.8 0.4 1.0 0.4 1.1 0.4 0.5 0.5 1.3 0.4\nConneau et al. (2018)+norm 2.1 0.7 11.3 0.7 9.2 0.3 0.7 0.3 0.6 0.2 0.6 0.3 1.7 0.5\nOURS 14.2 20.8 26.1 37.5 21.8 35.3 13.6 14.1 13.8 18.8 12.7 12.4 2.3 2.3\nTable 1: The precision p@5 of the cross-lingual word alignment task on the low-resource condition. We used\n50k sentences for the source languages and either 50k or 1M sentences for the target language (English). The\nbest scores among the (weakly) supervised or unsupervised methods are bold-faced, and the best scores of all the\nmethods are underlined.\nMethod\nsrc-tgt de-en es-en fr-en ru-en cs-en ﬁ-en tr-ja ta-ja ja-en se-ﬁ se-en\n(weakly) supervised\nArtetxe et al. (2018b)+char 49.0 59.5 59.6 10.7 38.6 18.2 40.4 28.6 11.6 32.6 14.2\nArtetxe et al. (2018a)+dict 35.6 49.7 49.4 38.5 38.5 28.3 25.8 46.6 24.5 42.9 20.2\nConneau et al. (2018)+dict 53.6 66.8 67.6 53.1 54.0 43.4 41.1 36.2 34.1 43.8 32.5\nunsupervised\nXu et al. (2018) 0.8 3.2 32.7 0.8 6.9 3.2 5.8 0.1 0.6 20.4 1.6\nArtetxe et al. (2018b) 5.6 47.4 47.1 9.0 3.1 1.2 3.7 1.5 1.8 13.6 0.3\nConneau et al. (2018) 0.8 0.7 1.7 0.5 0.9 1.6 1.3 0.6 1.4 13.1 0.7\nConneau et al. (2018)+norm 0.7 2.5 0.6 0.6 0.3 0.2 0.1 1.3 0.9 23.2 0.2\nOURS 26.4 54.9 54.0 22.7 26.8 19.2 18.1 10.4 1.8 37.9 18.3\nTable 2: The precision p@5 of cross-lingual word alignment task on the different-domain condition. The best\nscores among the (weakly) supervised or unsupervised methods are bold-faced, and the best scores of all the\nmethods are underlined.\nis because these processes transform both source\nand target word embeddings, and that makes it im-\npossible to map word embeddings of multiple lan-\nguages into a single embedding space.\nTo implement these baselines, we used the code\npublished by the authors 11,12,13 (Conneau et al.,\n2018; Artetxe et al., 2018b; Xu et al., 2018)\n4.4 Training Settings\nIn the cross-lingual and multilingual experiments,\nwe trained our model among two and four lan-\nin their code.\n11https://github.com/facebookresearch/\nMUSE\n12https://github.com/artetxem/vecmap\n13https://github.com/xrc10/\nunsup-cross-lingual-embedding-transfer.\nguages, respectively. When the size of the source\nand target corpora were different, we conducted\noversampling to generate the same number of\nmini-batches for source and target languages. We\ntrained our model for 10 epochs with the mini-\nbatch size 64, and stopped training when the train-\ning loss saturates (i.e., when the loss decreases by\nless than 1% compared to the previous epoch). For\neach iteration, our model alternately read mini-\nbatches of each language and updated its param-\neters. We set the size of word embeddings as 300,\nand used two-layer LSTM networks for the for-\nward and backward language models, respectively.\nWe set the size of the hidden state as 300 and\n1024 for the low-resource and different-domain\nconditions. Dropout (Srivastava et al., 2014) is ap-\n3119\nMethod\nCondition (a) (b) (c)\n(weakly) supervised\nArtetxe et al. (2018b)+char 2.3 0.6 44.6\nArtetxe et al. (2018a)+dict 5.6 8.6 37.2\nConneau et al. (2018)+dict 6.4 7.0 51.6\nChen and Cardie (2018)+char 5.2 2.8 53.4\nunsupervised\nXu et al. (2018) 1.1 0.0 0.0\nArtetxe et al. (2018b) 0.9 0.0 5.2\nConneau et al. (2018) 1.3 0.0 0.0\nConneau et al. (2018)+norm 0.3 0.0 0.0\nChen and Cardie (2018) 1.0 0.0 3.0\nOURS 10.4 16.2 37.0\nTable 3: The precision p@5 of multilingual word align-\nment task on the three different conditions (a), (b),\nand (c) described in 4.1.2. The best scores among the\n(weakly) supervised or unsupervised methods are bold-\nfaced, and the best scores of all the methods are under-\nlined.\nplied to the hidden state with a rate of 0.3. We\nused SGD (Bottou, 2010) as an optimiser with\nthe learning rate 1.0. All of the parameters of\nour model including word embeddings were uni-\nformly initialised in [-0.1, 0.1], and gradient clip-\nping (Pascanu et al., 2013) was used with the clip-\nping value 5.0. We included those words in vo-\ncabulary that were used at least 3, 5, and 20 times\nfor 50k, 100k-250k, and 1M sentences in News\nCrawl and Wikipedia. For Europarl and SIKOR\nNorth Saami corpora, we set the threshold as 10.\nWe fed the most 15,000 frequent words to train\nXu et al. (2018) and the discriminator in Conneau\net al. (2018).\nAs a preprocess, we tokenized the monolin-\ngual corpora using Moses toolkit 14 for European\nlanguages and Polyglot 15 for Tamil, Turkish and\nJapanese. We also lowercased all the corpora.\n4.5 Results\n4.5.1 Cross-lingual Word Embedding\nTable 1 illustrates the results of the cross-lingual\nword alignment task under the low-resource con-\ndition. The methods with ‘+char’ use character\ninformation to obtain a pseudo dictionary, and the\nones with ‘+dict’ use a gold dictionary that con-\n14https://github.com/moses-smt/\nmosesDecoder\n15https://polyglot.readthedocs.io/en/\nlatest/Tokenization.html\ntains 500 pairs of words. The table shows that\nour model substantially outperforms the unsuper-\nvised baseline models in all of the language pairs.\nOur model also achieves better results than su-\npervised methods except in the Japanese-English\npair, which has different word order (SOV v.s\nSVO). Another interesting ﬁnding is that when\nthe size of the target corpus increases from 50k\nto 1M sentences, our model improves its perfor-\nmance whereas the performance of the unsuper-\nvised baseline models drops substantially. For in-\nstance, when the size of the target corpus increases\nfrom 50k to 1M, Conneau et al. (2018) decreases\nthe precision in Spanish-English from 11.0 to 0.2,\nwhile our model increases the precision from 26.1\nto 37.5.\nTable 2 shows the results on the different-\ndomain condition. It shows that our method\nachieves better results overall than the unsuper-\nvised baseline models. The extremely poor per-\nformance of Conneau et al. (2018) under this con-\ndition is compatible with the results reported by\nSøgaard et al. (2018). Regarding the Japanese-\nEnglish pair, none of the unsupervised methods\nincluding ours perform well, demonstrating that it\nis difﬁcult to align languages without any super-\nvision if the basic word order is different. Super-\nvised methods, on the other hand, perform well in\nall the languages and outperform our model. This\nresult indicates that even if domains of monolin-\ngual corpora are different across languages, the\nconventional approach of learning a linear trans-\nformation can be effective with (weak) bilingual\nsupervision.\nImpact of Data Size\nTo evaluate the effects of the data size on the\nmodel performances, we increased the size of both\nsource and target corpora from 50k to 250k by 50k\nsentences. All of these sentences were extracted\nfrom News Crawl. Fig. 2 illustrates how p@5\nchanges depending on the data size. It shows that\nour model overall performs better than the base-\nlines, especially among the distant language pairs\nsuch as Finnish-English. Although Artetxe et al.\n(2018b) report positive results on word alignment\ntasks between Finnish and English, our experi-\nments show that their method requires much larger\nmonolingual corpora such as Wikipedia on both\nthe source and target sides to achieve good perfor-\nmance.\n3120\n50 100 150 200 250\n5\n10\n15\n20\n25\n30\n35\nGerman-English\n50 100 150 200 250\n10\n20\n30\n40\n50\nSpanish-English\n50 100 150 200 250\n10\n20\n30\n40\n50\nFrench-English\n50 100 150 200 250\n0\n5\n10\n15\n20\n25\nRussian-English\n50 100 150 200 250\n0\n5\n10\n15\n20\n25\n30\n35\nCzech-English\n50 100 150 200 250\n0\n5\n10\n15\n20\n25\n30\nFinnish-English\nOURS\nXu et al. (2018)\nArtex et al. (2018b)\nConneau et al. (2017)\nConneau et al. (2017) + normalize\nFigure 2: The change in p@5 achieved by the unsuper-\nvised methods on word alignment tasks. The x-axis de-\nnotes the number of sentences (thousand) in the source\nand target corpora, and the y-axis denotes the average\nprecision p@5 over three runs for each method.\nsrc-tgt\nlang\n(src) de es fr ru cs ﬁ ja\nsame domain\n50k-50k 9.7 10.6 12.4 6.5 7.5 6.6 6.5\n250k-250k 18.5 23.4 24.6 12.7 17.5 12.5 11.7\n1M-1M 20.6 28.0 29.6 18.3 23.2 17.0 15.4\n50k-1M 6.1 9.5 10.9 4.3 4.1 3.7 7.1\ndifferent domain\n1M-1M 15.7 19.5 22.2 17.9 19.5 15.2 13.6\nTable 4: The ratio (%) of the monolingual word embed-\ndings being roughly isomorphic across a source and tar-\nget language (English). Each row describes the number\nof sentences in source and target corpora used to train\nword embeddings, and each column denotes the source\nlanguage.\n4.5.2 Multilingual Word Embedding\nTable 3 describes the results under the three con-\nditions described in 4.1.2. It shows that our\nmodel substantially outperforms the unsupervised\nand supervised baseline models under the low-\nresource conditions (a) and (b). As in the case\nof 4.5.1, when the size of the English corpus in-\ncreases from 50k (a) to 1M (b), our model im-\nproves its performance while the unsupervised\nbaselines perform worse. Under the different-\ndomain condition (c), our model also achieves\nmuch better results than the unsupervised base-\nlines, but cannot outperform supervised methods.\nPOS\nlang\n(src) de es fr ru cs ﬁ\nADJ 25.2 36.8 35.5 23.1 38.6 20.7\nADV 68.8 82.6 71.9 82.6 81.6 66.2\nNOUN 24.5 53.8 51.1 13.2 16.4 9.2\nVERB 16.1 66.7 73.6 34.4 34.9 19.7\nTable 5: The ratio (%) of correctly matched POS tags\nusing our model under the different-domain condition.\nFor each language, the best and worst ratios among the\nfour POS tags are bold-faced and underlined.\n5 Analysis\n5.1 Validation of Isomorphism\nOur experiments show that our model substan-\ntially outperforms both supervised and unsuper-\nvised methods under the low-resource condition.\nWe conjecture that this large improvement is ow-\ning to our unique approach of obtaining multilin-\ngual word embeddings; unlike the conventional\napproach, our method does not assume that word\nembedding spaces are approximately isomorphic\nacross languages. In fact, when word embeddings\nare trained with small data, they should contain\na lot of noises and are unlikely to be isomorphic\nacross languages. This suggests that it would be\nextremely difﬁcult to learn a linear mapping across\nlanguages using the existing unsupervised meth-\nods.\nTo verify this hypothesis, we investigated how\nlikely monolingual word embeddings were more\nor less isomorphic across languages. For each pair\nof a language ℓand English, we sampled 10 pairs\nof equivalent words from a bilingual dictionary\nand built non-directed adjacency matrices of the\nnearest neighbour graphs G(ℓ) and G(en) inde-\npendently. Then, we conducted an element-wise\ncomparison of the two matrices and deemed them\nas roughly isomorphic if more than 80% of the el-\nements are the same. Table 4 shows how often the\ngraphs were roughly isomorphic over 1,000 sam-\nples. The row indicates the size of the source and\ntarget corpora. It clearly shows that monolingual\ncorpora trained on small data (i.e. 50k sentences)\nare far from being isomorphic between any lan-\nguage pair, and the linguistically distant languages\nsuch as Finnish-English and Japanese-English are\nless isomorphic than close languages. This re-\nsult clearly explains why the existing unsupervised\nmethods do not perform well on the low-resource\n3121\ncondition, or among distant language pairs. An-\nother intriguing ﬁnding is that word embeddings\ntrained with 50k and 1M sentences in a source\nand target languages are overall less isomorphic\nthan those trained with 50k source and target\nsentences. This result explains why the perfor-\nmance of the unsupervised baseline methods de-\ncreases given the additional target data in 4.5.1 and\n4.5.2. Our method, on the other hand, can effec-\ntively utilise the additional data to improve its per-\nformance, demonstrating its robustness under the\nlow-resource condition.\n5.2 POS tags of Matched Words\nTo analyse the performance of our model, we\nchecked Part-of-Speech (POS) tags of the English\nwords used in the word alignment task and investi-\ngated what kind of words were correctly matched\nby our model. Since a word is given without any\ncontext in the word alignment task and it is not\npossible to infer its POS tag, we assigned to each\nword its most frequent POS tag in Brown Corpus\n(Kucera and Francis, 1967). For instance, since\n‘damage’ is used as a noun more often than as a\nverb in Brown Corpus, we deﬁne its POS tag as\n‘noun’. Table 5 shows p@5 of the word alignment\ntask grouped by the four major POS tags, namely\nadjective, adverb, verb, and noun 16. It clearly in-\ndicates that an adverb can be easily matched in ev-\nery language pair. This would be because there are\nless adverbs than other tags in the evaluation data,\nand also because there are common word order\nrules about an adverb among all the languages: an\nadverb usually comes before an adjective to mod-\nify it, and when modifying a verb, it comes either\nbefore or after it. Refer to the Appendix B for the\nstatistics regarding word order in each language.\nAmong French, Spanish and English, the match-\ning accuracy of a noun and verb is very high, and\ntheir word order is in fact very similar; as shown\nin the Appendix B, the basic word order of these\nlanguages is strictly subject-verb-object, and that\nmakes it easy to align words among them. On\nthe other hand, the word order between a noun\nand adjective is very different among these lan-\nguages, explaining why the precision of matching\nadjectives is lower than the other tags. As for the\nother languages, they have more ﬂexible word or-\nder than English and that makes it difﬁcult to align\n16When there are X nouns and Y of them are matched\ncorrectly in the alignment task, the ratio is 100Y\nX %\nwords across languages. For instance, in German,\nRussian and Czech a subject sometimes comes af-\nter a verb, and in German and Finnish an object\ncan come before a verb. These ﬁndings clearly in-\ndicate that our model employs sequential similari-\nties among different languages to obtain multilin-\ngual word embeddings without any supervision.\n6 Conclusion\nIn this paper, we proposed a new unsupervised\nmultilingual word embedding approach. Whereas\nconventional methods aim to map pre-trained\nword embeddings into a common space, ours\njointly generates multilingual word embeddings\nby extracting a common language structure among\nmultiple languages. Our experiments on word\nalignment tasks have demonstrated that our pro-\nposed model substantially outperforms the exist-\ning cross-lingual and multilingual unsupervised\nmodels under resource-poor conditions, namely\nwhen only small data are available or when do-\nmains of corpora are different across languages.\nUnder the ﬁrst condition, our model even outper-\nforms supervised methods trained with 500 bilin-\ngual pairs of words. By analysing the nearest\nneighbour graphs of monolingual word embed-\ndings, we have veriﬁed that word embeddings are\nfar from being isomorphic when they are trained\non small data, explaining why existing unsuper-\nvised methods did not perform well on the low-\nresource condition. We have also found that the\nperformance of our model is closely related to\nword order rules, and our model can align words\nvery well when they are used in a similar order\nacross different languages. Our future work is to\nexploit character and subword information in our\nmodel and see how those information affect the\nperformance in each language pair. It would be\nalso interesting to investigate how our approach\ncompares to the baselines given a large amount of\ndata such as Wikipedia.\n7 Acknowledgement\nWe are grateful to all the anonymous reviewers for\ntheir insightful comments and advice.\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language model-\ning. In Proceedings of the 15th Conference of the\n3122\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers , pages\n937–947. Association for Computational Linguis-\ntics.\nJean Alaux, Edouard Grave, Marco Cuturi, and Ar-\nmand Joulin. 2018. Unsupervised hyperalign-\nment for multilingual word embeddings. CoRR,\nabs/1811.01124.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. Generalizing and improving bilingual word\nembedding mappings with a multi-step framework\nof linear transformations. In Proceedings of the\nThirty-Second AAAI Conference on Artiﬁcial Intel-\nligence, pages 5012–5019.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018b. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 789–798. Association\nfor Computational Linguistics.\nL´eon Bottou. 2010. Large-scale machine learning\nwith stochastic gradient descent. In Proceedings\nof the 19th International Conference on Compu-\ntational Statistics (COMPSTAT’2010), pages 177–\n187, Paris, France. Springer.\nXilun Chen and Claire Cardie. 2018. Unsupervised\nmultilingual word embeddings. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing , pages 261–270, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2018.\nWord translation without parallel data. In Interna-\ntional Conference on Learning Representations.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative\nadversarial nets. In Z. Ghahramani, M. Welling,\nC. Cortes, N. D. Lawrence, and K. Q. Weinberger,\neditors, Advances in Neural Information Processing\nSystems 27 , pages 2672–2680. Curran Associates,\nInc.\nPhilipp Koehn. 2005. Europarl: A Parallel Corpus for\nStatistical Machine Translation. In Conference Pro-\nceedings: the tenth Machine Translation Summit ,\npages 79–86, Phuket, Thailand. AAMT, AAMT.\nH. Kucera and W. N. Francis. 1967. Computational\nanalysis of present-day American English . Brown\nUniversity Press.\nKyungTae Lim, Niko Partanen, and Thierry Poibeau.\n2018. Multilingual Dependency Parsing for Low-\nResource Languages: Case Studies on North Saami\nand Komi-Zyrian. Miyazaki, Japan. ELRA.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9:2579–2605.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Efﬁcient estimation of word represen-\ntations in vector space. In International Conference\non Learning Representations (Workshop).\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH.\nTomas Mikolov, Quoc V . Le, and Ilya Sutskever.\n2013b. Exploiting similarities among languages for\nmachine translation. CoRR, abs/1309.4168.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In Proceedings of the 30th International\nConference on Machine Learning, ICML 2013, At-\nlanta, GA, USA, 16-21 June 2013, pages 1310–1318.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. pages 2227–2237, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nV Rudramurthy, Mitesh M. Khapra, and Pushpak\nBhattacharyya. 2016. Sharing network parameters\nfor crosslingual named entity recognition. CoRR,\nabs/1607.00198.\nSamuel L. Smith, David H. P. Turban, Steven Hamblin,\nand Nils Y . Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In International Conference on Learning\nRepresentations.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 778–\n788. Association for Computational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15:1929–1958.\nMin Xiao and Yuhong Guo. 2014. Distributed word\nrepresentation learning for cross-lingual dependency\nparsing. In Proceedings of the Eighteenth Confer-\nence on Computational Natural Language Learning,\npages 119–129. Association for Computational Lin-\nguistics.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. In Proceed-\nings of the 2015 Conference of the North Ameri-\ncan Chapter of the Association for Computational\n3123\nLinguistics: Human Language Technologies , pages\n1006–1011. Association for Computational Linguis-\ntics.\nRuochen Xu, Yiming Yang, Naoki Otani, and Yuexin\nWu. 2018. Unsupervised cross-lingual transfer of\nword embedding spaces. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 2465–2474. Association\nfor Computational Linguistics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017a. Adversarial training for unsupervised\nbilingual lexicon induction. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1959–1970. Association for Computational Linguis-\ntics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017b. Earth mover’s distance minimization\nfor unsupervised bilingual lexicon induction. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1934–1945. Association for Computational Linguis-\ntics.\nWill Y . Zou, Richard Socher, Daniel Cer, and Christo-\npher D. Manning. 2013. Bilingual word embeddings\nfor phrase-based machine translation. In Proceed-\nings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 1393–1398.\nAssociation for Computational Linguistics.\ndep-head(rel) en de es fr ru cs ﬁ\nN-V(nsubj) 93.7 77.5 89.0 95.4 80.4 77.1 88.2\nN-V(obj) 0.7 56.3 0.5 0.2 4.0 10.0 20.8\nADJ-N 98.9 99.9 30.3 30.9 99.3 93.9 100.0\nADV-ADJ 98.3 93.6 95.0 99.2 96.6 94.9 98.8\nADV-V 75.6 65.8 68.7 61.2 79.1 80.4 47.9\nTable 6: The ratio (%) of a dependent being put be-\nfore its head. N, V , ADJ, and ADV denote noun, verb,\nadjective and adverb, respectively. The dependency re-\nlation of ADJ-N is amod, and the one of ADV-ADJ and\nADV-V is advmod. Refer to the download page of PUD\nfor the deﬁnition of the dependency relations.\nA Visualisation\nFigure 3 visualises the multilingual word em-\nbeddings obtained by our model and (Chen and\nCardie, 2018) under the low-resource condition.\nIt shows the most frequent 1000 words in Span-\nish, French, German and English. The ﬁgure\nclearly shows that the word embeddings obtained\nby (Chen and Cardie, 2018) form some clusters\nbased on their languages. In particular, many of\nthe German words are mapped near the centre of\n15\n 10\n 5\n 0 5 10 15\n15\n10\n5\n0\n5\n10\n15\nOURS\nSpanish\nFrench\nGerman\nEnglish\n10\n 5\n 0 5 10 15\n15\n10\n5\n0\n5\n10\nChen and Cardie (2018)\nSpanish\nFrench\nGerman\nEnglish\nFigure 3: Scatter plot of multilingual word embeddings\nof French, English, German and Spanish obtained by\nour model and Chen and Cardie (2018) under the low-\nresource condition. The embeddings are reduced to 2D\nusing tSNE (van der Maaten and Hinton, 2008).\nthe ﬁgure and make a large cluster. On the other\nhand, the word embeddings trained by our model\nare not clustered by language, indicating that our\nmodel successfully maps word embeddings into a\ncommon space.\nB Word Order\nTo obtain statistics about word order rules in each\nlanguage, we used Parallel Universal Dependen-\ncies (PUD) treebanks17. PUD contains 1000 par-\nallel sentences aligned among 18 languages, and\nthose sentences are annotated morphologically\nand syntactically according to Google universal\nannotation guidelines. Since these sentences are\naligned among all the languages, it is possible\nto compare the syntactical differences across lan-\nguages.\nTable 6 shows the ratio of a dependent being\nput before its head in PUD treebanks in each lan-\nguage. As can be seen, the word order of ADV-\n17available at http://universaldependencies.org/\n3124\nADJ (advmod) is very similar among all the lan-\nguage pairs: an adverb is put before an adverb\nto modify it. The order of ADV-V (advmod) is\nrather ﬂexible regardless of language, indicating\nthat an adverb can modify a verb from either left or\nright. These common word order rules of adverbs\nexplain why our model successfully matched ad-\nverbs very well in every language pair. The table\nalso indicates that the word order of N-V is very\nsimilar among English, Spanish and French and\nthe basic word order is strictly subject-verb-object.\nThis explains why our model performed well over-\nall among these languages. However, the word\norder of ADJ-N is signiﬁcantly different among\nthese languages, and that would lead to the low\nperformance of our model in matching adjectives.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8351471424102783
    },
    {
      "name": "Word (group theory)",
      "score": 0.7493882179260254
    },
    {
      "name": "Word embedding",
      "score": 0.6576792597770691
    },
    {
      "name": "Natural language processing",
      "score": 0.6508689522743225
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6486701965332031
    },
    {
      "name": "Embedding",
      "score": 0.6079666614532471
    },
    {
      "name": "Artificial neural network",
      "score": 0.5273799896240234
    },
    {
      "name": "Space (punctuation)",
      "score": 0.5241793990135193
    },
    {
      "name": "Code (set theory)",
      "score": 0.5028049945831299
    },
    {
      "name": "Transformation (genetics)",
      "score": 0.47780805826187134
    },
    {
      "name": "Unsupervised learning",
      "score": 0.4358426630496979
    },
    {
      "name": "Language model",
      "score": 0.4203197956085205
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.15976649522781372
    },
    {
      "name": "Mathematics",
      "score": 0.10607528686523438
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I75917431",
      "name": "Nara Institute of Science and Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210126580",
      "name": "RIKEN Center for Advanced Intelligence Project",
      "country": "JP"
    }
  ]
}