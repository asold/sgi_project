{
  "title": "Multilingual Translation via Grafting Pre-trained Language Models",
  "url": "https://openalex.org/W3200578235",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2047366404",
      "name": "Zewei Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2132499817",
      "name": "Mingxuan Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2905933322",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3173190788",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2952650870",
    "https://openalex.org/W2996854111",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3168481568",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964007535",
    "https://openalex.org/W3097879195",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W3046368065",
    "https://openalex.org/W3100311862",
    "https://openalex.org/W2963247703",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3093345276",
    "https://openalex.org/W2962807144",
    "https://openalex.org/W4287874506",
    "https://openalex.org/W2970925677",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W3092327118",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3106321930",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2964085268",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2807535859",
    "https://openalex.org/W3105912780",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W3023986361",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4301239768",
    "https://openalex.org/W3119175506",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W2787998955",
    "https://openalex.org/W3154113533",
    "https://openalex.org/W2888456631",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2997763445",
    "https://openalex.org/W2953190730",
    "https://openalex.org/W2912095972",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2251743902",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3035464238",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3017454464",
    "https://openalex.org/W3200396895",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3105038888",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W3034719878",
    "https://openalex.org/W2963831310",
    "https://openalex.org/W4287694131",
    "https://openalex.org/W2963532001"
  ],
  "abstract": "Can pre-trained BERT for one language and GPT for another be glued together to translate texts? Self-supervised training using only monolingual data has led to the success of pre-trained (masked) language models in many NLP tasks. However, directly connecting BERT as an encoder and GPT as a decoder can be challenging in machine translation, for GPT-like models lack a cross-attention component that is needed in seq2seq decoders. In this paper, we propose Graformer to graft separately pre-trained (masked) language models for machine translation. With monolingual data for pre-training and parallel data for grafting training, we maximally take advantage of the usage of both types of data. Experiments on 60 directions show that our method achieves average improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with the multilingual Transformer of the same size.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2735–2747\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2735\nMultilingual Translation via Grafting Pre-trained Language Models\nZewei Sun1, Mingxuan Wang1, Lei Li2∗\n1 ByteDance AI Lab\n{sunzewei.v,wangmingxuan.89}@bytedance.com,\n2 University of California, Santa Barbara\nlilei@cs.ucsb.edu\nAbstract\nCan pre-trained BERT for one language and\nGPT for another be glued together to trans-\nlate texts? Self-supervised training using only\nmonolingual data has led to the success of pre-\ntrained (masked) language models in many\nNLP tasks. However, directly connecting\nBERT as an encoder and GPT as a decoder\ncan be challenging in machine translation, for\nGPT-like models lack a cross-attention compo-\nnent that is needed in seq2seq decoders. In\nthis paper, we propose Graformer to graft\nseparately pre-trained (masked) language mod-\nels for machine translation. With monolin-\ngual data for pre-training and parallel data for\ngrafting training, we maximally take advan-\ntage of the usage of both types of data. Exper-\niments on 60 directions show that our method\nachieves average improvements of 5.8 BLEU\nin x2en and 2.9 BLEU in en2x directions com-\nparing with the multilingual Transformer of\nthe same size1.\n1 Introduction\nIn recent years, pre-trained (masked) language\nmodels have achieved signiﬁcant progress in all\nkinds of NLP tasks (Devlin et al., 2019; Rad-\nford et al., 2019). Among them, neural machine\ntranslation (NMT) is also explored by several at-\ntempts (Yang et al., 2020a; Zhu et al., 2020b; Rothe\net al., 2020). The pre-training and ﬁne-tuning style\nbecomes an important alternative to take advantage\nof monolingual data (Yang et al., 2020c,b; Liu et al.,\n2020; Pan et al., 2021).\nAn intuitive question comes as: Can we bridge\nBERT-like pre-trained encoders and GPT-like de-\ncoders to form a high-quality translation model?\nSince they only need monolingual data, we can\nreduce the reliance on the large parallel corpus.\n∗Work is done while at ByteDance.\n1Our code will be public in https://github.com/\nsunzewei2715/Graformer\nBERT German\nBERT Chinese\nGPT English\nBERT French\nGrafting\nঅѺ\nHello!Hallo!\nBonjour!\nGerman\nGrafting\nঅѺ\nHello!Hallo!\nBonjour!\nBERT\nChinese\nEnglish\nFrench \nGPT\nFigure 1: Grafting pre-trained (masked) language mod-\nels like BERT and GPT for machine translation.\nMoreover, if the combination of models is univer-\nsal, it can be applied to translation for multiple\nlanguages, as is shown in Figure 1.\nHowever, though many works successfully gain\nimprovements by loading encoder/decoder param-\neters from BERT-like pre-trained encoders (Zhu\net al., 2020b; Guo et al., 2020), they do not achieve\nsatisfactory results with loading decoder param-\neters from GPT-like pre-trained decoders (Yang\net al., 2020a; Rothe et al., 2020). Theoretically, the\nwell-trained decoder model like GPT should bring\nbetter generation ability to the translation model.\nWe suggest the outcome may be attributed to the\narchitecture mismatch.\nPre-trained (masked) language models predict\nthe current word solely based on the internal con-\ntext while the translation decoder has to capture\nthe source context. Speciﬁcally, the decoder in\nNMT has a “cross-attention” sub-layer that plays a\ntransduction role (Bahdanau et al., 2015), while pre-\ntrained models have none, as is shown in Figure 2.\nThis mismatch between the generation models and\nconditional generation models makes it a challenge\nfor the usage of pre-trained models as translation\ndecoders.\nTherefore, some previous works manually insert\ncross-attention sub-layer or adapters (Rothe et al.,\n2020; Ma et al., 2020; Guo et al., 2020). However,\nthe extra implantation may inﬂuence the ability of\nthe pre-trained model. Other works try to avoid\n2736\nmBERT \n(Representation)\nmGPT \n(Generation)\nOutput\nGrafting \n(Transduction)\nCross \nAttention\nSelf Attention\nFeed-forward \nNetwork \nFeed-forward \nNetwork \nSelf Attention\n?\nPre-trained (Masked)  \nLanguage Models Translation Decoder\nࣳ\nҁრᒒ҂\nࣳ\nҁፓຽᒒ҂\nᘉᦲᕮຎ\n৥ളᗑᕶ\nFigure 2: Taking the popular architecture Trans-\nformer (Vaswani et al., 2017) as an example, the trans-\nlation model has a “cross-attention” sub-layer, while\npre-trained (masked) language models have none.\nthis problem by directly pre-training a seq2seq\nmodel and conduct ﬁne-tuning (Tang et al., 2020;\nYang et al., 2020b; Luo et al., 2020). However,\nthe pre-training objective is usually a variant of\nauto-encoding (Song et al., 2019; Liu et al., 2020),\nwhich is different from the downstream translation\nobjective and may not achieve adequate improve-\nments (Lin et al., 2020).\nIn this paper, we mainly focus on exploring\nthe best way to simultaneously take advantage of\nthe pre-trained representation model and genera-\ntion model (e.g., BERT+GPT) without limiting\ntheir strengths. The primary target is to link the\ngeneration model to the source side and maintain\nthe invariability of the architecture in the mean-\ntime. Therefore, we propose Graformer, with pre-\ntrained models grafted by a connective sub-module.\nThe structure of the pre-trained parts remains un-\nchanged, and we train the grafting part to learn\nto translate. For universality and generalization,\nwe also extend the model to multilingual NMT,\nachieving mBERT+mGPT.\nGenerally, the translation process can be divided\ninto three parts: representation, transduction, and\ngeneration, respectively achieved by the encoder,\ncross-attention, and decoder. In multilingual NMT,\nthe transduction can only be trained with multiple\nparallel data. But the rest two can be pre-trained\nwith multiple monolingual data, which is tens or\nhundreds of the size of parallel one. To maximize\nthe efﬁcacy of each part, we ﬁrstly pre-train a mul-\ntilingual BERT and multilingual GPT. Then they\nare grafted to implement translation. With the ar-\nchitecture consistency, we can reserve the language\nknowledge of the pre-trained models and obtain a\nstrong translation model ﬂexibly at the same time.\nExperiments on 30 language directions show that\nour method improves the results of multilingual\nNMT by 2.9 and 5.8 BLEU on average. It also\nachieves gains of 9.2 to 13.4 BLEU scores on zero-\nshot translation settings. In addition, it veriﬁes that\nsuch translation capability can be well transferred\nto other languages without ﬁne-tuning on the target\nparallel corpus.\n2 Related Work\nThis paper is related to a chain of studies of multi-\nlingual translation and pre-trained models.\n2.1 Multilingual Neural Machine Translation\nWith the development of NMT, multilingual neu-\nral machine translation (MNMT) also attracts a\ngreat amount of attention. Dong et al. (2015); Fi-\nrat et al. (2016a,b) take early attempts and con-\nﬁrm its feasibility. The most well-known work\nis from Johnson et al. (2017), who conduct a se-\nries of interesting experiments. And the usage of\nthe language token style is widely accepted. Also,\nmany subsequent works continuously explore new\napproaches in MNMT, such as parameter shar-\ning (Blackwood et al., 2018; Wang et al., 2019b;\nTan et al., 2019a), parameter generation (Platan-\nios et al., 2018), knowledge distillation (Tan et al.,\n2019b), learning better representation (Wang et al.,\n2019a), massive training (Aharoni et al., 2019;\nArivazhagan et al., 2019), interlingua (Zhu et al.,\n2020a), and adpater (Zhu et al., 2021). These works\nmainly utilize parallel data.\nThere are also some works taking advantage of\nmonolingual corpus. Zhang et al. (2020); Wang\net al. (2020) use back-translation (BT) to improve\nMNMT. However, for MNMT, BT is tremendously\ncostly, reaching O(n), or even O(n2). Siddhant\net al. (2020); Wang et al. (2020) adopt multi-task\nlearning (MTL), combining with other tasks such\nas masked language model (MLM) (Devlin et al.,\n2019), denoising auto-encoding (DAE) (Vincent\net al., 2008), or masked sequence-to-sequence gen-\neration (MASS) (Song et al., 2019). However, the\noptimization target is different from translation,\nwhich may interfere with the training and limit the\nusage of extremely large-scale monolingual data.\n2.2 Pre-trained Models\nIn recent years, pre-train models have become very\npopular in both research and industry communities.\n2737\nWith downstream ﬁne-tuning, plenty of signiﬁcant\nresults are achieved in NLP ﬁeld (Qiu et al., 2020).\nDevlin et al. (2019); Liu et al. (2019); Conneau\nand Lample (2019); Conneau et al. (2020) take\nmasked language model (MLM) as the training tar-\nget. The input tokens are randomly masked, and\nthe model learns the representation by maximiz-\ning their likelihood. Radford et al. (2018, 2019);\nBrown et al. (2020) use language model (LM) as\ntheir learning goal. With historical contexts, the\nmodel acquires language knowledge by learning to\npredict the next word. Raffel et al. (2020); Xue et al.\n(2020); Lewis et al. (2020); Liu et al. (2020); Lin\net al. (2020) choose direct sequence-to-sequence\n(seq2seq) for training. The pre-train tasks can be\nmachine translation, question answering, classiﬁ-\ncation, etc.\n2.3 Pre-trained Models for NMT\nSince pre-trained models can signiﬁcantly boost\nrelevant tasks, several recent studies try to combine\nthem with NMT. They can be roughly divided into\ntwo groups, depending on whether the models are\npre-trained uniformly or separately.\n2.3.1 United Style\nThe ﬁrst category is pre-training directly on\nseq2seq tasks and providing downstream MT with\nconsistent architectures. Tang et al. (2020) tune\ntranslation models from a pre-trained seq2seq\nmodel, mBART (Liu et al., 2020), and obtain signif-\nicant improvements. Yang et al. (2020c) pre-train a\nseq2seq model with some input tokens replaced by\nanother language from lexicon induction. Luo et al.\n(2020) pre-train the encoder and decoder in a single\nmodel that shares parameters. Then the parameters\nare partially extracted for tuning, depending on the\ntasks (NLU or NLG).\nHowever, the pre-training objective of these\nworks is usually a variant of auto-encoding (Song\net al., 2019; Liu et al., 2020), which is different\nfrom the downstream translation objective and may\nnot achieve adequate improvements (Lin et al.,\n2020).\n2.3.2 Fused Style\nThe second category is pre-training the encoder or\ndecoder independently and fusing them with the\ntranslation model in the ﬁne-tuning stage. Yang\net al. (2020a); Zhu et al. (2020b); Guo et al. (2020);\nMa et al. (2020) fuse BERT/RoBERTa into NMT\nwith extra encoders or adapters. Yang et al. (2020b)\nmBERT\nSoftmax\n✖ K\nFeed-forward \nNetwork \nSelf Attention\nmGPT\n✖ K\nCross \nAttention\nSelf Attention\nFeed-forward \nNetwork \nFigure 3: The model architecture of Graformer. The\npre-trained multilingual encoder (mBERT) and de-\ncoder (mGPT) are grafted to achieve multilingual trans-\nlation. The dashed line means feeding in the last token.\npropose alternating language modeling as the target\nof the pre-trained encoder. Rothe et al. (2020) ex-\nplore the usage of GPT but still manually insert ex-\ntra cross-attention. Weng et al. (2020) use dynamic\nfusion mechanism and knowledge distillation to in-\ntegrate the representation of the pre-trained models\ninto NMT models.\nThese works either do not touch the decoder side\nor modify the architecture and conduct ﬁne-tuning\nto fuse BERT/GPT into the decoder model. As\nmentioned in Section 1, the modiﬁcation of the\nmodel architecture may inﬂuence the model ability\nand harm the performance.\n3 Approach\nTo maintain the original model structure of pre-\ntrained models, we propose Graformer, as is in\nFigure 3. For the encoder side, we stack another\nK-layers encoder (K = 6, in this paper) on pre-\ntrained mBERT to help it adapt to the translation\ntraining. For the decoder side, we do similarly,\nexcept we append cross-attention layers to extract\nconditional context from the source. Unlike previ-\nous works, we maintain the integrality of mBERT\nand mGPT and do not change their architectures.\nFinally, we employ a residual connection (He\n2738\net al., 2016) that we combine the hidden state out-\nputted by mGPT and the grafting decoder. The\nsummed context is then fed into the softmax layer.\nThis integration is for utilizing the generation abil-\nity of the pre-trained decoder to help to generate a\nbetter language model.\nAs mentioned in Section 1, we try to take ad-\nvantage of both multiple parallel data and multiple\nmonolingual data so as to maximize the efﬁcacy\nof representation, transduction, and generation, re-\nspectively. Therefore, our training methods can\nbe separated into two stages: 1) pre-train on the\nmultiple monolingual data and obtain independent\nencoder (representation) and decoder (generation);\n2) ﬁne-tune on the multilingual parallel data to graft\ntwo models (transduction).\n3.1 Pre-train Multilingual BERT\n(Encoder for Representation)\nInspired by Devlin et al. (2019); Liu et al. (2019);\nConneau and Lample (2019); Conneau et al. (2020),\nwe use masked language model (MLM) as the\ntraining goal with the masked probability of 15%.\nSpeciﬁcally, we adopt Transformer (Vaswani et al.,\n2017) encoder withNlayers (N = 6, in this paper).\nTo make cross-lingual token representation more\nuniversal, we add no language token as previous\nworks do. The training goal is as follows:\nLMLM = −\n∑\nˆx∈m(x)\nlog p(ˆx|x\\m(x)) (1)\nm(x) and \\m(x) denote the masked words and\nrest words from x\n3.2 Pre-train Multilingual GPT\n(Decoder for Generation)\nInspired by Radford et al. (2018, 2019); Brown et al.\n(2020), we use auto-regressive language model\n(LM) as the training goal. Speciﬁcally, we adopt\nTransformer (Vaswani et al., 2017) decoder withN\nlayers (N = 6, in this paper). To specify the gen-\neration language, we set a unique language token\n(e.g., <2en>) as the ﬁrst input for the language\nmodel. The training goal is as follows:\nLLM = −\nT∑\nt=1\nlog p(xt|x<t) (2)\nT denotes the length of sequence. x<t =\n<2lang>,x1,x2,...,x t−1.\n3.3 Fine-tune Multilingual Translation\n(Grafting for Transduction)\nAfter obtaining the pre-trained encoder and de-\ncoder, we tune the model to link the representation\nmodel and generation model. The training goal is\nas follows:\nLMT = softmax(Wo1 hN + Wo2 hN+K) (3)\nhN denotes the hidden state of the last layer in\nmGPT. hN+K denotes the hidden state of the last\nlayer in the grafting decoder. Wo1 and Wo2 denote\nthe corresponding output matrix. The former one\nshares the same parameters with the target-side\nembedding.\nIn the tuning stage, we freeze the pre-trained\ndecoder parameters (including Wo1 ) and tune the\ngrafting parameters as well as the pre-trained en-\ncoder. Our ablation study shows that this setting\nyields the best performance, as is in the experiment\nsection.\n4 Experiments\nIn this paper, we perform many-to-many style mul-\ntilingual translation (Johnson et al., 2017). The\ndetailed illustrations of the datasets and implemen-\ntation are as follows.\n4.1 Datasets and Preprocess\n•Pre-training: We use News-Crawl corpus 2\nplus WMT datasets. We conduct deduplica-\ntion and label the data by language. In the\nend, we collect 1.4 billion sentences in 45\nlanguages, which is only one-ﬁfth of that of\nmBART (Liu et al., 2020). The detailed list\nof languages and corresponding scales is in\nAppendix A.\n•Multilingual Translation: We use TED\ndatasets, the most widely used MNMT\ndatasets, following Qi et al. (2018); Aharoni\net al. (2019). We extract 30 languages 3 from\n& to English, with the size of 3.18M sentence\npairs in raw data and 10.1M sentence pairs in\nsampled bidirectional data. The detailed list\nof language pairs and scales is in Appendix A.\nWe download the data from the open source 4\n2http://data.statmt.org/news-crawl\n3We use the corpus of “zh_cn” instead of “zh”.\n4https://github.com/neulab/\nword-embeddings-for-nmt\n2739\nModel bg bn bs cs de el es et fa ﬁ\nTransformer 32.0 12.5 30.3 23.7 28.7 30.7 34.7 17.5 20.4 17.1\nmBART - - - 26.4 32.8 - 38.1 20.9 - 19.9\nGraformer 38.5 18.1 36.5 29.4 35.5 37.4 40.7 24.0 26.9 23.0\nModel fr hi hr hu it ja kk lt mk mr\nTransformer 33.1 18.7 30.4 19.8 31.3 10.1 7.6 20.1 29.8 9.4\nmBART 36.5 22.9 - - 34.7 12.0 8.9 23.6 - -\nGraformer 39.2 25.1 36.7 26.1 37.2 13.7 10.5 27.2 35.7 13.0\nModel nl pl pt ro ru sr ta tr uk zh\nTransformer 28.9 19.7 34.8 28.6 20.8 29.0 5.8 18.7 23.4 15.6\nmBART 32.9 - - 32.2 22.6 - - 22.6 - 18.1\nGraformer 35.2 25.1 41.5 35.1 25.1 35.6 10.2 25.5 28.9 19.9\nTable 1: The results of x→en directions, with average improvements of 5.8 against baseline (22.8→28.6)\nModel bg bn bs cs de el es et fa ﬁ\nTransformer 28.8 11.3 23.4 16.6 23.7 25.9 33.0 14.0 12.5 12.1\nmBART - - - 17.7 25.8 - 35.2 14.1 - 13.2\nGraformer 33.0 14.1 26.3 20.2 27.8 29.8 37.5 16.1 14.2 14.4\nModel fr hi hr hu it ja kk lt mk mr\nTransformer 33.5 15.3 23.2 14.7 28.9 11.1 3.4 12.8 22.2 9.3\nmBART 35.8 16.5 - - 30.6 12.6 3.0 14.2 - -\nGraformer 37.8 18.1 26.8 17.2 32.5 12.8 3.8 15.9 25.7 10.6\nModel nl pl pt ro ru sr ta tr uk zh\nTransformer 25.9 12.8 32.0 24.7 16.1 18.7 13.6 11.6 17.3 21.2\nmBART 28.9 - - 27.1 16.9 - - 13.4 - 22.2\nGraformer 29.0 15.8 36.6 29.1 19.0 21.4 14.7 13.3 19.5 23.0\nTable 2: The results of en→x directions, with average improvements of 2.9 against baseline (19.0→21.9)\nand conduct detokenization with Moses Deto-\nkenizer (Koehn et al., 2007) 5.\n•Zero-shot and Bilingual Translation:We\nuse WMT 2014 German-English (4.5M sen-\ntence pairs) and French-English (36M sen-\ntence pairs) datasets.\n•Sample: Upsampling is an important way\nto improve the performance of low-resource\npairs (Arivazhagan et al., 2019). Therefore,\nsentences are sampled according to a multi-\nnomial distribution with probabilities {qi},\nwhere qi ∝ pα\ni , pi is the proportion of\nlanguagei. For monolingual pre-training, we\nfollow (Conneau and Lample, 2019; Liu et al.,\n2020) and set α = 0.7. For parallel ﬁne-\ntuning, we follow (Arivazhagan et al., 2019)\nand and set α= 0.2 (T = 5).\n•Tokenization: Like previous works, we use\n5https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ntokenizer/detokenizer.perl\nsentencepiece (Kudo and Richardson, 2018)\nand learn a joint vocabulary of 64000 tokens.\n4.2 Implementation Details\n•Architecture: We use Transformer (Vaswani\net al., 2017) as our basic structure with\npre-norm style (Xiong et al., 2020), and\nGELU (Hendrycks and Gimpel, 2016) as acti-\nvation function. Speciﬁcally, we adopt 1024\ndimensions for the hidden state, 4096 dimen-\nsions for the middle FFN layer, and 16 heads\nfor multi-head attention. Learnable position\nembedding is also employed. For baseline\nmodels, we use 12 layers. For pre-trained\nones, we use Transformer encoder and de-\ncoder (without cross-attention) with 6 layers,\nrespectively. For the grafting part, we add\nanother 6 layers.\n•Training: We train the models with a batch\nsize of 320,000 tokens on 16 Tesla V100\nGPUs. For pre-training, we go through the\n2740\ntotal data for ﬁve times. Parameters are op-\ntimized by using Adam optimizer (Kingma\nand Ba, 2015), with β1 = 0.9, β2 = 0.98,\nwith warmup_steps= 4000. Without extra\nstatement, we use dropout = 0.3 (Srivastava\net al., 2014). Label smoothing (Szegedy et al.,\n2016) of value = 0.1 is also adopted. Besides,\nwe use fp16 mixed precision training (Micike-\nvicius et al., 2018) with Horovod library with\nRDMA inter-GPU communication (Sergeev\nand Del Balso, 2018).\n•Evaluation: We uniformly conduct beam\nsearch with size = 5 and length penalty\nα = 0.6. For hi, ja, and zh, we use Sacre-\nBLEU (Post, 2018). Otherwise, we use tok-\nenized BLEU (Papineni et al., 2002) with the\nopen-source script 6.\n4.3 Main Results\nAs is shown in Table 1 and 2, our methods ob-\ntain signiﬁcant improvements across all language\npairs. For x →en and en →x pairs, advances of\nnearly 6 BLEU and 3 BLEU are achieved. We also\ncompare the results with loading from mBART, a\nwell-known multilingual pre-trained sequence-to-\nsequence model (Liu et al., 2020) 7. Due to the\nlanguage difference, we only tune the model on a\npart of languages. With both 12-layers depth and\n1024-dimensions width, our method outperforms\nmBART on almost all pairs, proving the superior-\nity of Graformer comparing with pre-training in\nUnited Style mentioned in Section 2. It is worth\nnoticing that we only use the one-ﬁfth amount of\nthe data of mBART.\n4.4 Ablation Study\nTo verify the contribution of each part of our model,\nwe do a series of ablation studies. As is shown in\nTable 3 and 4, we can draw at least four empirical\nconclusions.\nEncoder needs tuning, decoder needs not.In\nTable 3, comparing Row 1 with Row 2, and Row\n5 with Row 8, we can see that the tuning of the\nencoder is essential. It can bring further improve-\nments. However, freezing pre-trained decoder pa-\nrameters is a better choice. Comparing Row 3 with\nRow 4, and Row 6 with Row 8, we can see that\n6https://github.com/pytorch/fairseq/\nblob/master/examples/m2m_100/tok.sh\n7https://dl.fbaipublicfiles.com/\nfairseq/models/mbart/mbart.cc25.v2.tar.\ngz\ntuning may lead to a drop for decoder. It seems that\nthe pre-trained decoder model learns much more\nknowledge, and its original language model can\nbetter guide the generation.\nDecoder matters more. In Table 3, compar-\ning Row 1,2,3,4, we can see that the pre-trained\ndecoder yields more progress than the pre-trained\nencoder. This shows that involving only pre-trained\nencoders like BERT into MT is limited. The per-\nformance can be further enhanced with the intro-\nduction of pre-trained decoders.\nResidual connection contributes.In Table 3,\ncomparing Row 7 with Row 8, we can see that the\nresidual connection from the pre-trained decoder\ncan further boost the results. The well-trained lan-\nguage model effectively helps the translation model.\nIt also shows the importance of incorporating the\nknowledge-rich generation model.\nRow Encoder Decoder x→en en →x\n0 - - 22.8 19.0\n1 Freeze - 23.2 19.2\n2 Fine-tune - 27.0 20.2\n3 - Freeze 27.8 21.0\n4 - Fine-tune 25.2 19.9\n5 Freeze Freeze 25.8 20.4\n6 Fine-tune Fine-tune 27.0 19.4\n7 Fine-tune Freeze* 28.1 20.9\n8 Fine-tune Freeze 28.6 21.9\nTable 3: Each number is the average BLEU of 30 language\ndirections. “-” means not loading from pre-trained models. “*”\nmeans the residual connection is abandoned.\nEncoder Decoder x→en en →x\n6+6 6+6 28.6 21.9\n6+6 6+5 28.7 21.7\n6+6 6+4 28.2 21.6\n6+6 6+3 28.3 21.6\n6+6 6+2 28.2 21.1\n6+6 6+1 27.9 18.0\n6+5 6+6 28.6 21.3\n6+4 6+6 28.5 21.5\n6+3 6+6 28.4 21.7\n6+2 6+6 28.4 21.0\n6+1 6+6 28.0 20.8\n6 6+6 28.0 20.7\nTable 4: Each number is the average BLEU of 30 lan-\nguage directions. “x+y” means the combination of\nx-layers pre-trained (masked) language models and y-\nlayers grafting models.\n2741\nLayer number has slight effects.In Table 4,\nas the number of layers decreases, the performance\ndrops slightly for both the encoder and decoder.\nBut the extent of the decline is limited. Even no\nextra encoder layer or one-layer extra decoder can\nmaintain a relatively high performance.\n4.5 Well-trained Language Model Helps\nExcept for BLEU, we also study how the pre-\ntrained generation model inﬂuence the translation\nmodel. We speculate that the pre-trained decoder\nhelps to translate through combining the well-\ntrained language model. Therefore, we collect and\ncompare the perplexity of the models on the valida-\ntion sets.\nAs is in Table 5, we can see that our method\nsigniﬁcantly lowers the perplexity comparing to\nthe baseline model. The pre-trained decoder brings\nin better representation and language knowledge.\nAlso, the residual connection from the original pre-\ntrained decoder can further improve the results,\nillustrating the enlightening role the well-trained\nlanguage model plays.\nModel x→en en →x\nTransformer 8.64 8.76\nGraformer * 5.60 6.58\nGraformer 5.27 6.21\nTable 5: The perplexity of models. Each number is the\naverage result of 30 language directions. “*” means the\nresidual connection is abandoned.\n4.6 Better than Fused Styles\nBesides United Style(mBART), we also compare\nour method with Fused Style. Speciﬁcally, we\nchoose two typical works, as are in Figure 4: 1)\nloading parameters directly and ignoring cross-\nattention (denoted as “Direct”) (Rothe et al., 2020;\nMa et al., 2020); 2) insert extra cross-attention\nlayers into each decoder sub-layer and freeze pre-\ntrained models (denoted as “Adapter”) (Guo et al.,\n2020). We re-implement the models with the same\ndepth and width as Graformer.\nThe crucial difference is that we leave the\npre-trained decoder module unchanged and com-\nplete. Other works inject extra layers internally,\nsuch as cross-attention or adapters. Speciﬁcally,\nthey go like layer1 →adapter1 →layer2 →\nadapter2 →... →layerN →adapterN. The\nwell-trained bond between layeri and layeri+1 is\n× N\nFeed-forward \nNetwork \nSelf Attention\nCross \nAttention\nSelf Attention\nFeed-forward \nNetwork \nBERT BERT/GPT\n× N\nFeed-forward \nNetwork \nCross \nAttention\nFeed-forward \nNetwork \nFeed-forward \nNetwork \nSelf Attention Self Attention\nFeed-forward \nNetwork \nBERT  sub-layer BERT/GPT sub-layer\n× N\nAdapter sub-layer\nAdapter sub-layer\n× N\nRothe et al. (2020); Ma et al. (2020) Guo et al. (2020)\nDirect Adapter\nFigure 4: The model architecture of “Direct”\n(left) (Rothe et al., 2020; Ma et al., 2020) and “Adapter”\n(right) (Guo et al., 2020).\nModel BLEU↑ Perplexity↓\nx→en en →x x→en en →x\nDirect 27.1 20.5 6.61 8.06\nAdapter 27.4 19.8 5.78 6.71\nGraformer 28.6 21.9 5.27 6.21\nTable 6: Each number is the average BLEU/Perplexity\nof 30 language directions. Our model outperform re-\nlated methods in fused style.\nbroken, which can not activate the full potential of\nthe pre-trained decoder.\nDifferently, we maintain the original structure\nand even feed its output into the ﬁnal layer. These\nstrategies are all for the sake of fully taking ad-\nvantage of the pre-trained generation model. As\nis in Table 6, our approach outperforms other two\nmethods (The detailed results are in Appendix B).\n4.7 Graformer Maintains Good Performance\nin Few-Shot Translation\nWe also conduct few-shot experiments. We ran-\ndomly select 30%, 10%, 3%, 1% of the data and\nreproduce the experiments. As is in Figure 5,6, as\nthe scale of datasets decreases, the performance of\nbaseline drops dramatically and fails to generate\ncomprehensible sentences (BLEU < 5). However,\nour method keeps relatively higher results even\nwith only 1% data. And with the less data provided,\nthe gap between Graformer and baseline is much\nlarger (5.8→12.1, 2.9→7.1). Again, it proves that\nthe usage of multiple monolingual data can beneﬁt\nMNMT greatly since its scale is tens or hundreds\nof times of the parallel one.\n2742\n1% 3% 10% 30% 100%\nData Scale\n5\n10\n15\n20\n25BLEU\nBaseline\nGraformer\nFigure 5: The results of x →en directions. As the data\nscale decrease from 100% to 1%, the gap is getting\nlarger (5.8→12.1).\n1% 3% 10% 30% 100%\nData Scale\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5BLEU\nBaseline\nGraformer\nFigure 6: The results of en →x directions. As the data\nscale decrease from 100% to 1%, the gap is getting\nlarger (2.9→7.1).\n4.8 The More Monolingual data, The Better\nTo further analyze the effectiveness of monolin-\ngual data, we look into the relationship between\nthe BLEU advance and the data scale. As is in\nFigure 7, as the quotient of the monolingual data\nscale divided by the parallel data scale increases,\nthe BLEU improvements gradually go up. It shows\nthe extra beneﬁt provided by the monolingual data,\nespecially in the large-scale scene. Since the par-\nallel data is rare, Graformer can be an essential\napproach to enhance low-resource language pairs.\n4.9 Graformer Boosts Zero-Shot Translation\nTo verify whether the multilingual pre-trained\nmodel learns cross-lingual knowledge, we also con-\nduct a crossed experiment of zero-shot translation.\nFirstly, we use our approach to train models only\n0 50 100 150 200 250 300 350 400\nMonolingual data / Parallel data\n1\n2\n3\n4BLEU increase\nFigure 7: Each point represents a language. The x-axis\nmeans the quotient of the monolingual data scale di-\nvided by the parallel data scale. The y-axis means the\nBLEU improvements of en→x directions.\nTrain Model Testing\nde→en fr →en\nde→en\nTransformer 31.9 6.7\nGraformer 33.4 15.2\nGraformerfe 33.0 20.1\nfr→en\nTransformer 5.1 35.1\nGraformer 10.8 36.0\nGraformerfe 16.8 35.5\nTable 7: Zero-shot experiments on WMT Datasets. “ fe”\nmeans freezing the pre-trained encoder. Notice that even the\nmodel does not see parallel sentences for a testing language,\nour method can achieve 11.7 and 13.4 BLEU improvement.\nTrain Model Testing\nde→en fr →en\nde→en\nTransformer 33.6 1.7\nGraformer 36.9 3.4\nGraformerfe 35.4 11.9\nfr→en\nTransformer 1.5 37.3\nGraformer 4.5 40.7\nGraformerfe 10.7 39.8\nTable 8: Zero-shot experiments on TED Datasets.“fe′′means\nfreezing the pre-trained encoder. Notice that even the model\ndoes not see parallel sentences for a testing language, our\nmethod can achieve 10.2 and 9.2 BLEU improvement.\non German-English corpus and then conduct infer-\nence on French-English test sets. Converse ones\nare done similarly. We perform experiments on\nboth TED and WMT datasets, with the encoder\nfrozen (Graformerfe) and tuned (Graformer).\nAs is in Table 7 and 8, we can draw similar\nconclusions. On the one hand, the performance of\nthe original direction is improved, as expected. On\n2743\nthe other hand, the inference results in the other\ndirection are also signiﬁcantly boosted at the same\ntime. It is worth noting that our models are trained\nwith none of the test directions but obtain BLEU\nscore high than 10.\nMore speciﬁcally, if the encoder is frozen, the\nresults of the main direction can be slightly low-\nered, but the results of the zero-shot one will be\nsigniﬁcantly improved. It illustrates that the un-\ntuned pre-trained model contains much more cross-\nlingual knowledge and can be better transferred to\nuntrained pairs.\n4.10 Graformer Works in Bilingual\nTranslation\nTo verify the effect of our methods, we also con-\nduct experiments on bilingual translation. We\nuse WMT14 English-German and English-French\nDatasets. In this series of settings, the datasets\nand vocabulary of both pre-training and tuning are\nlimited in the bilingual corpus. For en-fr training,\nwe adopt dropout= 0.1, following Vaswani et al.\n(2017).\nThe results, along with several strong related pre-\ntraining works, are listed in Table 9. Those related\nworks all take advantage of pre-trained models and\nsigniﬁcantly improve the translation. Our method\nboosts the performance of bilingual translation and\nis at the top level. It proves the universal effective-\nness of Graformer.\nModel en→de en →fr\nTransformer 28.9 41.8\nYang et al. (2020a) 30.1 42.3\nWeng et al. (2020) 29.2 -\nYang et al. (2020b) 29.2 -\nZhu et al. (2020b) 30.8 43.8\nRothe et al. (2020) 30.6 -\nGuo et al. (2020) 30.6 43.6\nGraformer 31.0 43.6\nTable 9: Bilingual translation results of English-\nGerman and English-French of WMT14. Comparing\nobjects are strong results reported by recent works.\nGraformer boosts the performance and is at the top\nlevel.\n5 Conclusion\nIn this paper, we propose Graformer, grafting mul-\ntilingual BERT and multilingual GPT for multilin-\ngual neural machine translation. By pre-training\nthe representation part (encoder) and generation\npart (decoder) of the model, we leverage the mono-\nlingual data to boost the translation task. And differ-\nent from other previous fusing methods, we main-\ntain the original architectures. With this approach,\nwe can fully take advantage of the pre-trained mod-\nels, including their well-trained capacity for rep-\nresentation and generation. Experimental results\nshow that our method can signiﬁcantly improve\nthe performance and outperform similar related\nworks. A series of empirical analyses of perplexity,\nfew-shot translation, and zero-shot translation also\nshows its universality.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn NAACL-HLT.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, et al. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges. arXiv preprint arXiv:1907.05019.\nDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by jointly\nlearning to align and translate. In ICLR.\nGraeme Blackwood, Miguel Ballesteros, and Todd\nWard. 2018. Multilingual neural machine transla-\ntion with task-speciﬁc attention. In ICLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nACL.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In NeurIPS.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\n2744\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and\nHaifeng Wang. 2015. Multi-task learning for mul-\ntiple language translation. In ACL-IJCNLP.\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio.\n2016a. Multi-way, multilingual neural machine\ntranslation with a shared attention mechanism. In\nNAACL-HLT.\nOrhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,\nFatos T Yarman Vural, and Kyunghyun Cho. 2016b.\nZero-resource translation with multi-lingual neural\nmachine translation. In EMNLP.\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,\nBoxing Chen, and Enhong Chen. 2020. Incor-\nporating bert into parallel sequence decoding with\nadapters. In NeurIPS.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nMelvin Johnson, Mike Schuster, Quoc Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTACL.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\ntoolkit for statistical machine translation. In ACL.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nEMNLP.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In ACL.\nZehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\nJiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\ntraining multilingual neural machine translation by\nleveraging alignment information. In EMNLP.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. TACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi,\nSongfang Huang, Fei Huang, and Luo Si. 2020.\nVeco: Variable encoder-decoder pre-training for\ncross-lingual understanding and generation. arXiv\npreprint arXiv:2010.16046.\nShuming Ma, Jian Yang, Haoyang Huang, Zewen Chi,\nLi Dong, Dongdong Zhang, Hany Hassan Awadalla,\nAlexandre Muzio, Akiko Eriguchi, Saksham Sing-\nhal, et al. 2020. Xlm-t: Scaling up multilingual ma-\nchine translation with pretrained cross-lingual trans-\nformer encoders. arXiv preprint arXiv:2012.15547.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, et al. 2018. Mixed precision\ntraining. In ICLR.\nXiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li.\n2021. Contrastive learning for many-to-many mul-\ntilingual neural machine translation. In ACL.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL.\nEmmanouil Antonios Platanios, Mrinmaya Sachan,\nGraham Neubig, and Tom Mitchell. 2018. Contex-\ntual parameter generation for universal neural ma-\nchine translation. In EMNLP.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Pad-\nmanabhan, and Graham Neubig. 2018. When and\nwhy are pre-trained word embeddings useful for neu-\nral machine translation? In NAACL-HLT.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. JMLR.\n2745\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks. TACL.\nAlexander Sergeev and Mike Del Balso. 2018.\nHorovod: fast and easy distributed deep learning in\ntensorﬂow. arXiv preprint arXiv:1802.05799.\nAditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat,\nMia Xu Chen, Sneha Kudugunta, Naveen Arivazha-\ngan, and Yonghui Wu. 2020. Leveraging monolin-\ngual data with self-supervision for multilingual neu-\nral machine translation. In ACL.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. In ICML.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. JMLR, 15(1):1929–1958.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nCVPR.\nXu Tan, Jiale Chen, Di He, Yingce Xia, QIN Tao, and\nTie-Yan Liu. 2019a. Multilingual neural machine\ntranslation with language clustering. In EMNLP-\nIJCNLP.\nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-\nYan Liu. 2019b. Multilingual neural machine trans-\nlation with knowledge distillation. In ICLR.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with exten-\nsible multilingual pretraining and ﬁnetuning. arXiv\npreprint arXiv:2008.00401.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. NIPS.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In ICML.\nXinyi Wang, Hieu Pham, Philip Arthur, and Graham\nNeubig. 2019a. Multilingual neural machine trans-\nlation with soft decoupled encoding. In ICLR.\nYining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai,\nJingfang Xu, and Chengqing Zong. 2019b. A com-\npact and language-sensitive multilingual translation\nmethod. In ACL.\nYiren Wang, ChengXiang Zhai, and Hany Hassan.\n2020. Multi-task learning for multilingual neural\nmachine translation. In EMNLP.\nRongxiang Weng, Heng Yu, Shujian Huang, Shanbo\nCheng, and Weihua Luo. 2020. Acquiring knowl-\nedge from pre-trained model to neural machine\ntranslation. In AAAI.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tieyan Liu. 2020. On layer\nnormalization in the transformer architecture. In\nICML.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nJiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi\nZhao, Weinan Zhang, Yong Yu, and Lei Li. 2020a.\nTowards making the most of bert in neural machine\ntranslation. In AAAI.\nJian Yang, Shuming Ma, Dongdong Zhang, ShuangZhi\nWu, Zhoujun Li, and Ming Zhou. 2020b. Alternat-\ning language modeling for cross-lingual pre-training.\nIn AAAI.\nZhen Yang, Bojie Hu, Ambyera Han, Shen Huang, and\nQi Ju. 2020c. Csp: Code-switching pre-training for\nneural machine translation. In EMNLP.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nACL.\nChangfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua\nLuo. 2020a. Language-aware interlingua for multi-\nlingual neural machine translation. In ACL.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao\nQin, Wengang Zhou, Houqiang Li, and Tieyan Liu.\n2020b. Incorporating bert into neural machine trans-\nlation. In ICLR.\nYaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingx-\nuan Wang, and Lei Li. 2021. Serial or parallel?\nplug-able adapter for multilingual machine transla-\ntion. arXiv preprint arXiv:2104.08154.\n2746\nA Languages and Scales\nThe languages of datasets are listed in Table 10 and\nTable 11, for pre-training and translation training,\nrespectively. We use signiﬁcantly less data than\nmBART(Liu et al., 2020). According to its paper\n(and some naive summation), they use 208 billion\ntokens in 1.4T in total. We only use 42 billion\ntokens in 0.18T.\nB Results of Fused Style Methods\nThe results of other Fused Style methods are in\nTable 12 and 13.\nLanguage Scale Language Scale\nam 119643 ky 279440\nbg 38305118 lt 4992036\nbn 3916068 lv 13059185\nbs 1955342 mk 209389\ncs 90149511 ml 182467\nde 329456604 mr 325364\nel 8159512 nl 1205639\nen 326422361 or 444212\nes 65422557 pa 218067\net 7023190 pl 14480947\nfa 1304611 ps 948310\nﬁ 23127824 pt 9260529\nfr 121133895 ro 21285406\ngu 535156 ro* 20509504\nhi 32491838 ru 94788355\nhr 6718607 so 168710\nhu 40181635 sr 3798788\nit 39682711 sw 455488\niu 781877 ta 1251716\nja 19579066 te 882347\nkk 1956205 tr 17494020\nkm 4410059 uk 1486906\nkn 502499 zh 25401930\nall 1.40B\nTable 10: Languages used for pre-training and their\nscales (in sentences). “ro*” means processed Roma-\nnian.\nLanguage Scale Language Scale\nbg 174444 ja 204090\nbn 4649 kk 3317\nbs 5664 lt 41919\ncs 103093 mk 25335\nde 167888 mr 9840\nel 134327 nl 183767\nes 196026 pl 176169\net 10738 pt 51785\nfa 150965 ro 180484\nﬁ 24222 ru 208458\nfr 192304 sr 136898\nhi 18798 ta 6224\nhr 122091 tr 182470\nhu 147219 uk 108495\nit 204503 zh 5534\nall 3.18M\nTable 11: Language pairs (from & to English) used for\ntranslation training and their scales (in sentences).\n2747\nModel bg bn bs cs de el es et fa ﬁ\nDirect 36.8 18.0 35.2 28.5 33.9 35.3 39.1 22.5 24.8 21.2\nAdapter 38.0 18.1 36.8 29.2 34.3 36.2 40.1 23.3 23.7 21.9\nGraformer 38.5 18.1 36.5 29.4 35.5 37.4 40.7 24.0 26.9 23.0\nModel fr hi hr hu it ja kk lt mk mr\nDirect 38.0 23.6 35.3 24.4 36.0 12.3 10.1 25.4 33.8 12.1\nAdapter 38.7 24.1 36.2 24.9 36.5 11.7 10.1 25.9 34.7 11.2\nGraformer 39.2 25.1 36.7 26.1 37.2 13.7 10.5 27.2 35.7 13.0\nModel nl pl pt ro ru sr ta tr uk zh\nDirect 33.2 23.6 40.1 33.6 23.9 33.9 8.7 23.3 27.8 18.5\nAdapter 33.8 24.0 41.1 34.2 24.3 34.9 7.1 22.9 27.6 17.9\nGraformer 35.2 25.1 41.5 35.1 25.1 35.6 10.2 25.5 28.9 19.9\nTable 12: The results of x →en directions for “Direct” (Rothe et al., 2020; Ma et al., 2020) and “Adapter” (Guo\net al., 2020).\nModel bg bn bs cs de el es et fa ﬁ\nDirect 30.7 12.2 24.5 18.2 25.1 27.8 35.3 14.9 13.3 13.1\nAdapater 31.0 10.5 24.3 18.5 25.4 26.9 35.3 15.3 9.6 13.4\nGraformer 33.0 14.1 26.3 20.2 27.8 29.8 37.5 16.1 14.2 14.4\nModel fr hi hr hu it ja kk lt mk mr\nDirect 35.4 16.7 25.2 15.7 30.7 12.2 4.0 14.5 24.4 10.5\nAdapater 35.8 15.3 24.8 15.7 30.2 9.2 3.8 14.5 24.5 9.0\nGraformer 37.8 18.1 26.8 17.2 32.5 12.8 3.8 15.9 25.7 10.6\nModel nl pl pt ro ru sr ta tr uk zh\nDirect 28.1 14.4 34.3 26.9 17.5 20.2 15.6 12.3 18.7 21.8\nAdapater 26.9 13.9 34.2 26.7 17.1 19.7 11.6 11.7 18.1 20.5\nGraformer 29.0 15.8 36.6 29.1 19.0 21.4 14.7 13.3 19.5 23.0\nTable 13: The results of en →x directions for “Direct” (Rothe et al., 2020; Ma et al., 2020) and “Adapter” (Guo\net al., 2020).",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8707248568534851
    },
    {
      "name": "Computer science",
      "score": 0.8649826049804688
    },
    {
      "name": "BLEU",
      "score": 0.7647112607955933
    },
    {
      "name": "Transformer",
      "score": 0.7330116033554077
    },
    {
      "name": "Encoder",
      "score": 0.6884005069732666
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6270729303359985
    },
    {
      "name": "Natural language processing",
      "score": 0.6087464690208435
    },
    {
      "name": "Language model",
      "score": 0.6067585945129395
    },
    {
      "name": "Translation (biology)",
      "score": 0.49055904150009155
    },
    {
      "name": "Training set",
      "score": 0.48607137799263
    },
    {
      "name": "Speech recognition",
      "score": 0.42794597148895264
    },
    {
      "name": "Voltage",
      "score": 0.06187015771865845
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154570441",
      "name": "University of California, Santa Barbara",
      "country": "US"
    }
  ]
}