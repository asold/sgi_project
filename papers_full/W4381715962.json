{
  "title": "A Transformer-Based Model Trained on Large Scale Claims Data for Prediction of Severe COVID-19 Disease Progression",
  "url": "https://openalex.org/W4381715962",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3088158585",
      "name": "Manuel Lentzen",
      "affiliations": [
        "Fraunhofer Institute for Algorithms and Scientific Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2099369059",
      "name": "Thomas Lindén",
      "affiliations": [
        "Fraunhofer Institute for Algorithms and Scientific Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2884453087",
      "name": "Sai Veeranki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2073153886",
      "name": "Sumit Madan",
      "affiliations": [
        "Fraunhofer Institute for Algorithms and Scientific Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2462345582",
      "name": "Diether Kramer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2480467679",
      "name": "Werner Leodolter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139420093",
      "name": "Holger Fröhlich",
      "affiliations": [
        "Fraunhofer Institute for Algorithms and Scientific Computing"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2610332124",
    "https://openalex.org/W2395172628",
    "https://openalex.org/W3162152762",
    "https://openalex.org/W4224238239",
    "https://openalex.org/W6765671411",
    "https://openalex.org/W4310645210",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W3089168780",
    "https://openalex.org/W2965570621",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6780145172",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4288950918",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6726186668",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2165748167",
    "https://openalex.org/W2150291618",
    "https://openalex.org/W2034806082",
    "https://openalex.org/W2102520493",
    "https://openalex.org/W4295423230",
    "https://openalex.org/W1934602670",
    "https://openalex.org/W2991379615",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W2064186732",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W2128088446",
    "https://openalex.org/W1970893511",
    "https://openalex.org/W4246598646",
    "https://openalex.org/W2084191761",
    "https://openalex.org/W2001497361",
    "https://openalex.org/W3013125498",
    "https://openalex.org/W3153298496",
    "https://openalex.org/W3168010718",
    "https://openalex.org/W3024420647",
    "https://openalex.org/W3154532284",
    "https://openalex.org/W3048586935",
    "https://openalex.org/W4210653733",
    "https://openalex.org/W3202042764",
    "https://openalex.org/W4288433426",
    "https://openalex.org/W4283377079",
    "https://openalex.org/W3199727813",
    "https://openalex.org/W3012195391",
    "https://openalex.org/W3011439019",
    "https://openalex.org/W6803670329",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W4287724045",
    "https://openalex.org/W3133650345",
    "https://openalex.org/W3211827580",
    "https://openalex.org/W2963271116"
  ],
  "abstract": "In situations like the COVID-19 pandemic, healthcare systems are under enormous pressure as they can rapidly collapse under the burden of the crisis. Machine learning (ML) based risk models could lift the burden by identifying patients with a high risk of severe disease progression. Electronic Health Records (EHRs) provide crucial sources of information to develop these models because they rely on routinely collected healthcare data. However, EHR data is challenging for training ML models because it contains irregularly timestamped diagnosis, prescription, and procedure codes. For such data, transformer-based models are promising. We extended the previously published Med-BERT model by including age, sex, medications, quantitative clinical measures, and state information. After pre-training on approximately 988 million EHRs from 3.5 million patients, we developed models to predict Acute Respiratory Manifestations (ARM) risk using the medical history of 80,211 COVID-19 patients. Compared to Random Forests, XGBoost, and RETAIN, our transformer-based models more accurately forecast the risk of developing ARM after COVID-19 infection. We used Integrated Gradients and Bayesian networks to understand the link between the essential features of our model. Finally, we evaluated adapting our model to Austrian in-patient data. Our study highlights the promise of predictive transformer-based models for precision medicine.",
  "full_text": "JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 1\nA Transformer-Based Model Trained on Large\nScale Claims Data for Prediction of Severe\nCOVID-19 Disease Progression\nManuel Lentzen, Thomas Linden, Sai Veeranki, Sumit Madan, Diether Kramer, Werner Leodolter, Holger\nFröhlich on behalf of COPERIMOplus\nAbstract—In situations like the COVID-19 pandemic, health-\ncare systems are under enormous pressure as they can rapidly\ncollapse under the burden of the crisis. Machine learning (ML)\nbased risk models could lift the burden by identifying patients\nwith a high risk of severe disease progression. Electronic\nHealth Records (EHRs) provide crucial sources of information\nto develop these models because they rely on routinely col-\nlected healthcare data. However, EHR data is challenging for\ntraining ML models because it contains irregularly timestamped\ndiagnosis, prescription, and procedure codes. For such data,\ntransformer-based models are promising.\nWe extended the previously published Med-BERT model by\nincluding age, sex, medications, quantitative clinical measures,\nand state information. After pre-training on approximately 988\nmillion EHRs from 3.5 million patients, we developed models\nto predict Acute Respiratory Manifestations (ARM) risk using\nthe medical history of 80,211 COVID-19 patients. Compared to\nRandom Forests, XGBoost, and RETAIN, our transformer-based\nmodels more accurately forecast the risk of developing ARM\nafter COVID-19 infection. We used Integrated Gradients and\nBayesian networks to understand the link between the essential\nfeatures of our model. Finally, we evaluated adapting our model\nto Austrian in-patient data. Our study highlights the promise\nof predictive transformer-based models for precision medicine.\nIndex Terms—COVID-19, precision medicine, transformer-\nbased models\nI. I NTRODUCTION\nC\nORONAVIRUS disease 2019 (COVID-19) is an infectious\ndisease caused by severe acute respiratory syndrome\ncoronavirus type 2 (SARS-CoV-2) that arose in December\nM. Lentzen, T . Linden, and H. Fröhlich are with the Department\nof Bioinformatics, Fraunhofer Institute for Algorithms and Scientific\nComputing (SCAI), Schloss Birlinghoven, 53757 Sankt Augustin, Germany,\nand with the Bonn-Aachen International Center for IT (b-it), University\nof Bonn, Friedrich-Hirzebruch Allee 6, 53115 Bonn, Germany. E-Mail:\n{manuel.lentzen, thomas.linden, holger.froehlich}@scai.fraunhofer.de\nS. Madan is with the Department of Bioinformatics, Fraunhofer\nInstitute for Algorithms and Scientific Computing (SCAI), Schloss\nBirlinghoven, 53757 Sankt Augustin, Germany, and with the Institute\nof Computer Science, University of Bonn, Bonn, Germany. E-Mail:\nsumit.madan@scai.fraunhofer.de\nS. Veeranki is with the Steiermärkische Krankenanstaltengesellschaft\nm.b.H. (KAGes), Graz, Austria, with the Institute of Neural Engineering,\nTechnical University, Graz, Austria, and with the AIT Austrian Institute of\nTechnology, Graz, Austria. E-Mail: sai.veeranki@kages.at\nD. Kramer and W . Leodolter † are with the Steiermärkische\nKrankenanstaltengesellschaft m.b.H. (KAGes), Graz, Austria. E-Mail:\ndiether.kramer@kages.at\nCorresponding author: Holger Fröhlich, hol-\nger.froehlich@scai.fraunhofer.de\n2019. Since its emergence, 628 million people have been\ninfected, and 6.58 million have died (https://coronavirus.\njhu.edu/map.html, accessed 25.10.2022). In such pandemic\ncircumstances, healthcare systems face a tremendous chal-\nlenge as they can quickly collapse under the burden of\nthis unprecedented crisis. Despite taking countermeasures\nsuch as testing, lockdowns, and vaccinations, the pan-\ndemic temporarily put immense stress on global healthcare\nsystems. The use of decision support systems such as\npatient-level risk models can assist with the critical tasks of\nquickly and efficiently identifying high-risk patients so that\nthe existing resources are best distributed, and vulnerable\npatient subgroups are effectively protected.\nStructured Electronic Health Records (EHRs) offer great\nopportunities for the efficient development of such risk\nmodels as they are routinely collected in many healthcare\nsystems in large quantities. They contain data on diagnoses,\nprescriptions, procedures, and quantitative clinical mea-\nsurements, such as vital values from bedside monitoring.\nAdditionally, demographic data such as age, gender, and\ngeographical region may be included. Models trained on\nsuch data could be used to better understand risk factors,\nsuch as comorbidities and medications, in addition to\npredicting a patient’ s risk of severe disease development.\nHowever, these data present significant challenges due to\ntheir high dimensionality, heterogeneity, temporal depen-\ndence, sparsity, and irregularity, making them difficult to\nfully exploit [1].\nFurthermore, the coding of diagnoses is frequently biased\nfor economic reasons. Since there is no unique mapping\nof a physician’ s diagnosis to a coding scheme such as\nICD, there is a tendency to select the code that delivers\nthe greatest economic benefit from among several possible\ncodes. Concerning medications, it is noteworthy that cate-\ngorization often occurs at the product level as opposed to\nthe chemical substance level and that several medications\nmay contain the same chemical substance.\nIn the past, many ML approaches have been taken to\nwork with structured EHR data. Simpler methods often\nlimited the time information and just worked with a one-\nhot encoding (OHE) of diagnoses and prescriptions, which\nallowed the application of standard ML techniques, such\nas logistic regression, random forest (RF), XGBoost (XGB),\nand Bayesian methods [2]. Recently, more studies focused\non the use of time-series information. Methods for such\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 2\nTABLE I\nCOMPARISON OF EXMED -BERT WITH OTHER MODELS\nBEHRT G-BERT Med-BERT BRLTM ExMed-BERT\nPre-training dataset CPRD MIMIC-III Cerner Health Facts Private EHRs IBM Explorys Thera-\npeutic\nPre-training patients (No.) 1.6M 20K 20M 44K 3.5M\nVocabulary size 301 < 4K 82K 9,285 2,480\nInput codes Caliber ICD-9, ATC ICD9, ICD10 ICD9, CPT , Med PheWas, ATC\nInput structure code + visit + age code embeddings code + visit + serial-\nization\ncode + age + gender\n+ position + segment\ncode + gender +\nstate + age + visit\nPre-training task Masked LM Modified Masked\nLM\nMasked LM + PLOS Masked LM Masked LM + PLOS\nEvaluation task Code prediction Medication code\nprediction\nDisease prediction Predict depression\nover different time\nframes\nPrediction of ARM\nfollowing COVID-19\nPublicly available ✗ ✗ ✗ ✗ ✓\nan approach include autoencoders, convolutional neural\nnetworks [3], or sequential models like recurrent neural\nnetworks (RNN) [4] or transformer-based models [5–9].\nTransformer-based models originate from natural language\nprocessing (NLP) and have recently gained much attention\nsince they have achieved excellent results in many areas\n[10–13]. A principal advantage of transformer models is the\nability to train them in a parallel fashion and the ability\nto weigh different parts of a time series differently due to\ntheir inbuilt attention mechanism. Transformer-based mod-\nels typically undergo two-stage training: pre-training for\ngeneric representation learning and transfer learning (fine-\ntuning) for application-specific prediction. This approach\nenables sharing pre-trained models, often based on large\ndatasets like the entire Wikipedia or protein sequences, with\na broader community. These models can then be fine-tuned\nfor various unforeseen tasks, highlighting the transformer-\nbased approach’ s versatility and strength.\nVariants of the Bidirectional Encoder Representations\nfrom Transformers (BERT) [14] model have recently been\napplied to structured EHR data. For instance, Shang\net al. developed a graph-augmented transformer model\nnamed G-BERT to encode the medical history of single\nmedical appointments and used the generated embeddings\nfor a medication recommendation task [9]. Later, Li et\nal. developed BERT for EHR (BEHRT), which generated a\npatient embedding based on the history of diagnoses and\nused it for disease prediction in different time windows [5].\nSince BEHRT – like most transformer-based models – is\nlimited with respect to the maximum sequence length, the\nauthors later developed a hierarchical BEHRT variant (HI-\nBEHRT), which can process longer medical histories [6].\nMeng et al. presented another model in 2021, the Bidirec-\ntional Representation Learning model with a Transformer\narchitecture on Multimodal EHR (BRLTM), which employed\na strategy similar to BEHRT but incorporated a larger vocab-\nulary, including diagnoses, medications, and procedures [8].\nMed-BERT , another transformer-based model for structured\nEHR data, is closely related to BRLTM, but it features an\neven larger vocabulary and slightly different training ob-\njectives [7]. A comprehensive comparison of these models\nis provided in Table I. Unfortunately, none of the above-\nmentioned models is publicly available in a pre-trained\nform and thus not usable for the broader community.\nOur contribution is an extension of the Med-BERT ap-\nproach by including information about prescribed medica-\ntions and demographic information such as state of resi-\ndence, gender, and age as well as quantitative clinical mea-\nsurements. We pre-trained our model, named ExMed-BERT ,\non 987,846,612 EHRs collected between 2010 and 2021,\nstemming from 3.5 million US patients in the IBM Explorys\nTherapeutic dataset. As a showcase, we subsequently used\ndata from 80,211 COVID-19 patients to develop ML models\nfor predicting the risk of acute respiratory manifestation\n(ARM) within three weeks after a confirmed COVID-19\ndiagnosis. This time frame was chosen because, on the one\nhand, a COVID-19 infection typically lasts 10 to 14 days. On\nthe other hand, the timestamp of the COVID-19 diagnosis\nprovided in the data may only be accurate up to a weekly\nresolution. The aim was thus to capture a serious event\nthat could be time-wise related to the previously reported\ninfection.\nWe compared our ExMed-BERT models with the three\nbaseline models, which included the RNN-based RETAIN\nmodel [15], as well as two models (RF [16] and XGBoost\n[17]) that ignore time information. We then used explain-\nable AI methods to gain insights into the underlying mech-\nanisms of our models. A specific contribution is the use\nof Bayesian networks (BNs) to disentangle the relationship\nbetween most predictive features. Finally, we explored how\nour ExMed-BERT models could be adapted to external\ndata from an Austrian hospital group (KAGes) via transfer\nlearning strategies. Opposed to previous work, we make our\nExMed-BERT model available to the scientific community.\nII. M ATERIALS & METHODS\nA. General Overview\nThe work in this paper consists of four phases (Figure 1):\n1) Pre-Training of transformer-based model for structured\nEHR data: Initially, we prepared a dataset of large-scale\nclaims data and pre-trained a transformer-based model\ncalled ExMed-BERT for structured EHR data.\n2) Development of risk models for COVID-19 disease\nprogression: Subsequently, we used our newly trained\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 3\nFig. 1. Study overview. First, we pre-trained a transformer model on about 988 million EHR records from 3.5 million patients. Then, we developed\npatient-level risk models for COVID-19 disease progression. Next, we interpreted our developed risk models using Integrated Gradients in conjunction\nwith Bayesian networks. Finally, we evaluated the possibility of adapting the models to external data.\nmodel to develop risk models for predicting severe\nCOVID-19 disease progression – namely ARM – and\ncompared their performances with RF , XGB, and RE-\nTAIN models.\n3) Interpretation of developed risk models: Then, we used\nthe Integrated Gradients approach in conjunction with\nBayesian Networks to offer detailed explanations for\nmodel predictions.\n4) Evaluation of the adaptation of our models to data\nobtained from an Austrian hospital group within a\ntransfer learning approach.\nIn the following, we describe our approach in more detail.\nB. Data Preprocessing\nTABLE II\nCHARACTERISTICS OF THE PRE -TRAINING AND FINE -TUNING DATASETS\nPre-\ntraining\nFine-tuning\nA\nFine-tuning\nB\nSource IBM Explorys\nTherapeutic Dataset\nAustrian\nHospital\nGroup\nTime span 2010 – 2021 2019 – 2021 2019 – 2021\nNo. of patients 3,478,438 80,211 6,335\nARM-positive – 10,743 385\nPatients with\nquantitative\nclinical data\n– 23,949 –\nAvg. patient age 41.2 49.8 47.9\nRatio of female\npatients (%) 56.3 62.1 51.8\nAvg. number of\nrecords per pa-\ntient\n284.0 83.8 60.19\nTotal number of\nrecords 987,846,612 6,722,594 381,296\nNote: ARM stands for acute respiratory manifestation.\n1) Preparation of Data for Modeling: This study used\nthe IBM Explorys Therapeutic dataset (https://www.ibm.\ncom/products/explorys-ehr-data-analysis-tools), which\ncomprises EHRs and insurance claims from 4.5 million\npatients from all over the USA from 2010 until mid of 2021.\nRecords consist of prescribed drugs, diagnoses, performed\nprocedures, and a few quantitative clinical measures\n(e.g., blood pressure). We focused on demographic data\nand drugs, diagnoses, and available quantitative clinical\nmeasures. We excluded patients with fewer than five\nobservations. This led to a reduced dataset of 3.5 million\npatients with 987,846,612 recorded diagnoses and drugs,\nwhich we used for pre-training a transformer model\n(details described later). The intent behind the pre-\ntraining of a transformer model is to learn a suitable\nvector representation of timestamped structured EHRs,\nirrespective of any later clinical use case. The fit of the\nmodel to a dedicated clinical endpoint is then performed\nwithin a subsequent fine-tuning/transfer learning step,\nfor which we selected only patients with a confirmed\nCOVID-19 diagnosis defined by the use of the International\nClassification of Diseases (ICD10) [18] code U07.1 or a set\nof Logical Observation Identifier Names and Codes (LOINC)\n[19] codes (see Supplementary Section A) (n=80,211). We\ncorrected the diagnosis or observation dates of the records\nby subtracting seven days to get an approximation of the\nindex date of infection. Then we focused on the ARM\nendpoint, which was defined if at least one of the following\ndiagnoses appeared within three weeks after the COVID-19\ninfection was reported (n=10,743):\n• Pneumonia due to coronavirus disease 2019 (J12.82)\n• Acute bronchitis due to other specified organisms\n(J20.8)\n• Unspecified acute lower respiratory infection (J22)\n• Bronchitis, not specified as acute or chronic (J40)\n• Acute respiratory distress syndrome (J80)\n• Respiratory failure, not elsewhere classified (J96)\n• Other specified respiratory disorders (J98.8)\nFor fine-tuning, we used one year of medical history of\nthe COVID-positive patients prior to their infection. Patients\nwho fulfilled these criteria for the ARM endpoint were\nlabeled as positives. Supplementary Figure A.1 depicts the\nfiltering process in further detail.\nTo identify negatives while adjusting for the potentially\nconfounding effects of age and gender, we used the tech-\nnique of Inverse Probability of Treatment Weighting (IPTW)\n[20–22]. We used the Python package psmpy [23] (version\n0.2.8) to calculate propensity scores (PS), and subsequently,\nthe IPTW weights for each patient sample were calculated\nby the following equation and used in the fine-tuning\nprocess.\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 4\nIPTW =\n( 1\nPS if positive\n1\n1−PS if negative\n(1)\n2) Mapping of Drug and Diagnosis Codes: The IBM\nExplorys Therapeutic dataset includes information about\ndiagnoses encoded as ICD9 and ICD10 codes and adminis-\ntered or prescribed drugs as RXNorm [24] identifiers. To\nharmonize the two versions of ICD diagnosis codes, we\nmapped them to Phecodes provided by the Phenome-wide\nassociation study (PheWAS) [25]. Due to the lower number\nof Phecodes, the problem of a non-unique mapping be-\ntween a physician’ s diagnosis and the ICD coding scheme\nis reduced. Hence, we reduced potential coding biases and\nthe feature space from 59,709 to 1,850 codes. Similarly, we\nmapped the provided RXNorm identifiers (RxCUI) to the\nfourth level of the Anatomical Therapeutic Chemical (ATC)\n[26] classification system for chemical compounds and thus\naddressed the sparse use of some RxCUIs by reducing the\nfeature space from 23,801 to 630 codes.\n3) Input Representation for the Models: For the Random\nForest (RF) and XGBoost (XGB) models, we employed a one-\nhot encoding approach to represent all categorical features.\nIn this scheme, a diagnosis or drug recorded in the one-year\nmedical history was denoted as one and zero otherwise.\nWe applied the same encoding technique to the state of\nresidence and sex variables. However, the data formatting\nrequirements for the RETAIN model and our transformer-\nbased model significantly differ from those of the RF and\nXGB baseline models. Owing to their design, which is\ntailored to handle sequential data, these models necessitate\nthe representation of each patient’ s entire medical history\nas a sequence. As illustrated in the lower part of Figure 2,\nwe created separate sequences for each modality, with each\nelement of the sequence being an integer corresponding\nto one vector of the embedding matrices. Quantitative\nclinical measures were only considered during the fine-\ntuning phase.\nC. Model Training and Evaluation\nIn this section, we provide a comprehensive overview of\nthe methods used in our study, detailing the pre-training\nand fine-tuning of various models across multiple experi-\nments. We begin by outlining the structure and pre-training\nof our novel transformer-based model before describing the\ndevelopment of machine learning-based risk models using\nRF , XGBoost, RETAIN, and our new model. However, we\nnote that recent models such as BEHRT or Med-BERT could\nnot be included in our comparison as the pre-training data\nand models are not publicly available. Finally, we describe\nthe integration of quantitative clinical measurements with\nthe patient’ s medical history. While previous works have\nextensively described the model architectures, we refer\ninterested readers to publications by Vaswani et al. and\nDevlin et al. for Transformer-based models [14, 27], Breiman\nfor Random Forest [16], Chen and Guestrin for further\ndetails on the XGBoost approach [17], and Choi et al. for\nRETAIN [15].\nFig. 2. Overview of the model structure. Compared to BERT or other\ntransformer-based models, we employed a multimodal embedding layer\nfor structured EHR data comprising of drug, diagnosis, visit information,\nand information about a patient’ s sex, state of residence, and age. After\nembedding, the input is passed through 6 transformer layers before a\nfinal representation of a patient’ s medical history is generated with an\nFFN, LSTM, or GRU head. Subsequently, these patient representations were\neither concatenated with the quantitative clinical data or directly passed\nthrough an FFN head for classification.\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 5\n1) Basic Model Structure and Pre-training of ExMed-\nBERT: We followed a similar strategy as the Med-BERT\npaper and focused on an extension of the BERT embedding\nlayer. In addition to the diagnoses that were included\nin the Med-BERT model, we further extended Med-BERT\nby adding information on prescribed drugs, the patient’ s\nsex, state of residency, and age. We denote our model as\nExtended Med-BERT (ExMed-BERT). As shown in Figure 2,\nwe used different embeddings to accommodate the five\nfeature modalities. Diagnoses and drugs were represented\nin one embedding via Phecodes and ATC codes. The sex\nand state embeddings contained static information. The age\nsequence contained the patients’ age encoded in months,\nand lastly, the visit sequence was used to distinguish be-\ntween each visit in a sequence. Since the order of drugs\nand diagnoses within one visit was random, we passed on\na serialization embedding. Similar to Med-BERT , we did not\nuse CLS and SEP tokens in our input sequences.\nWe used the same hyperparameters and training ob-\njectives as Med-BERT and pre-trained the model on the\nentire information of the 3.5 million patients in the pre-\ntraining cohort. If sequences exceeded the maximum se-\nquence length of 512 diagnosis and drug codes, we split\nthe sequences and processed the samples individually. We\nused the following joint training objectives to pre-train our\nmodel:\n• Masked language modeling (MLM): This task is iden-\ntical to the BERT approach and we followed the Med-\nBERT strategy in masking only one of the codes at\na time. In 80 % of the cases, the masked code was\nreplaced with [MASK], in 10 % it was replaced with\nanother code and in the remaining 10 %, it remained\nunchanged. The model’ s task was to predict the cor-\nrect code based on the information provided by the\nremaining sequence.\n• Prediction of prolonged length of stay (PLOS) in\nhospital: As Rasmy et al. [7], we also predicted whether\na patient had a prolonged stay in a hospital (>7\ndays) throughout his or her medical history. This task\nrequires assessing the severity of a patient’ s health\ncondition throughout their medical history.\n2) Machine Learning-Based Risk Models : In this study,\nwe aimed to predict severe COVID-19 disease progression\nby developing ML-based risk models to determine whether\na patient would develop ARM within three weeks of their\nCOVID-19 diagnosis. To achieve this, we utilized one year\nof the patient’ s medical history prior to diagnosis. To adjust\nfor potential confounding effects of age and gender, we\nemployed the IPTW approach described before.\nDuring the fine-tuning of our ExMed-BERT model, we\nevaluated different classification head variants. We trained\nthree different models using a feed-forward network (FFN),\nlong short-term memory (LSTM), and gated recurrent unit\n(GRU) head. We split the data into training, validation,\nand testing sets in a stratified manner (70/10/20 %) and\nused Bayesian hyperparameter optimization ( optuna [28],\nversion 2.10.0) to tune model parameters such as the\nlearning rate, batch size, warmup ratio, weight decay, and\nin the case of RNNs, also the number of RNN layers.\nSimilarly, we trained RF , XGB, and RETAIN classifiers and\noptimized several model hyperparameters. A detailed list of\nall optimized parameters can be found in Supplementary\nTable B.1.\nFor RETAIN, we used a Keras-based implementation 1\nwith slight modifications described in the next section. The\ninput was similar to our ExMed-BERT model, using PheWAS\nand ATC codes and visit/date information but excluding a\npatient’ s age, sex, and state of residency, as these were not\npart of the original RETAIN approach.\n3) Combination with Quantitative Clinical Measure-\nments: In this study, we also assessed the integration of\ndiagnosis and prescription codes with numerical clinical\ndata, such as blood pressure readings. Our analysis focused\non data documented in the two weeks leading up to the re-\nvised index date, excluding features with over 60 % missing\ndata. Given the considerable sparsity of this data, we were\nleft with only eight features for our investigation: weight,\nbody mass index (BMI), body surface area (BSA), height,\nbody temperature, diastolic and systolic blood pressure, and\nheart rate. The number of patients with available numerical\ndata for each feature is displayed in Table II (n=23,949). To\nimpute the numerical data for all patients, we employed\na Random Forest (RF)-based approach, utilizing only the\ntraining data ( missingpy [29], version 0.2.0).\nThe imputed numerical clinical features were combined\nwith one-hot encoded (OHE) data for the RF experiments.\nIn the case of the XGBoost (XGB) model, no prior imputa-\ntion was carried out. Instead, we fused the numerical clin-\nical features with the OHE data, relying on the imputation\nmechanism built into XGB.\nIn terms of model modifications, no alterations were\nrequired for the RF and XGB models. However, we needed\nto adjust the ExMed-BERT and RETAIN model architectures\nto accommodate numerical clinical features. Specifically, we\nutilized the same ExMed-BERT model as before to produce\na patient’ s medical history embeddings. We then combined\nthese vector-based representations with the numerical in-\nput before feeding it into a final classification head. The\noverall model architecture and the concatenation approach\nare detailed in Figure 2. Similarly, we produced the latent\nencodings in the RETAIN model and combined them with\nthe numerical clinical features before the classification\nlayer.\nD. Model Interpretation\n1) Feature Importance: To better understand the ExMed-\nBERT models, we used the Integrated Gradients (IG) [30]\napproach to determine which drugs and diagnoses had\nthe strongest influence on the model predictions. The IG\nmethod is an axiomatic model interpretability technique\nthat awards, in the case of the ExMed-BERT models, an\nattribution score for each diagnosis or drug in the medical\n1https://github.com/Optum/retain-keras\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 6\nhistory. Next to an input sample ( x ∈ Rn ), the IG method\nrequires a baseline input ( x′ ∈ Rn ), which we constructed\nusing a sequence of padding tokens. The IGs are then\napproximated by summing the gradients at points along\nthe path from the specified baseline to the input using the\nfollowing formula:\nIGi (x)approx :=\n¡\nxi −x′\ni\n¢\n×\nZ m\nk=1\n∂F (x′ + k\nm ×(x −x′))\n∂xi\n∂α (2)\nHere, m is the number of steps, and F is a differentiable\nfunction ( F : Rn ⇒ [0, 1]) that represents our ExMed-BERT\nmodel. We performed 50 steps to approximate the inte-\ngrated gradients.\nInitially, we computed IG attributions for all patients\nin the test dataset. Based on these, we calculated the\nmean absolute attribution for each diagnosis and drug that\noccurred at least ten percent of the time to identify the\ntop features for each model. Subsequently, we calculated\npartial dependency scores using the top 20 features. To\ndo so, we first calculated the probability for each patient\nfor a specific endpoint using our fine-tuned ExMed-BERT\nmodels; we refer to this probability as pr . The data for each\nof the top 20 features were then permuted individually by\nexchanging the respective diagnosis or drug codes with a\nPAD token. Subsequently, the modified data was used as\ninput for our models to calculate the probability pm . Finally,\na fold change for each feature was calculated using the\nprobabilities obtained for actual ( pr ) and modified data\n(pm ) to estimate the effect (fold change; FC) of certain\nfeatures on the model’ s prediction:\nFC = pr\npm\n(3)\n2) Unraveling Feature Dependencies: To better compre-\nhend the numerous interactions and dependencies between\nthe most influential features, we developed BN models.\nBNs are probabilistic graphical models that can represent\ncomplex multivariate distributions with many variables.\nThey can be graphically depicted with nodes representing\nrandom variables and edges expressing conditional statisti-\ncal relationships. Let G = (V,E) be a directed acyclic graph\nand {Xv |v ∈V } a set of random variables indexed over nodes\nin V . Then for any BN B = (X ,G):\np(X |G) =\nY\nv∈V\np\n¡\nXv |pa v\n¢\n(4)\nwhere pa v denotes the parents of v ∈ V according to\nthe graph structure G. Because of their ability to model\n(potentially causal) relationships between variables, BNs are\nfrequently employed in many areas of science, including\nsystem biology and medicine. In this work, we learned the\ngraph structure G of a BN for the 100 most important\nfeatures (according to the IG method) using the R package\nbnlearn [31] (version 4.7). We used a one-hot encoding for\nthe respective features to indicate whether it was present in\nthe one-year medical history, similar to the data preparation\nfor the tree-based models. We also provided the patients’\nage, sex, and endpoint status. The tabu algorithm [32, 33]\nwas used for BN structure learning. This was performed\nwithin a non-parametric bootstrap sampling scheme: We\nrandomly subsampled n = 80,211 patients with replacement\nfor 1000 times, and for each bootstrap sample we performed\na complete network structure learning. We then focused\non edges occurring in over half of the 1000 network ar-\nchitectures acquired from the non-parametric bootstrapped\nsamples.\nE. Transfer Learning on Austrian Hospital Data\n1) Overview of the Data: Data from the Austrian hospital\ngroup consisted of pseudonymized in-patient records of\n6,335 COVID-19 positive patients, out of which 385 suffered\nfrom ARM within a 3-week follow-up period after the initial\nvisit to the hospital. The medication prescriptions were\nalready encoded in ATC, but as ICD9/10 codes were used\nfor diagnoses, these were mapped to Phecodes, akin to the\nprocedure described earlier for the IBM Explorys dataset.\n2) Transfer Learning of ExMed-BERT: We continued\ntraining the ExMed-BERT model for the ARM endpoint for\nonly five epochs on the Austrian hospital data. This was\ndone due to computational constraints. For the same rea-\nson, we did no substantial hyperparameter tuning but used\nthe optimal hyperparameters discovered on the IBM Explo-\nrys data. We used 5-fold cross-validation to account for the\nsmall amount of available data. Alongside the ExMed-BERT\nmodel, we trained a new RF model for comparison.\nIII. R ESULTS\nIn this study, we predicted severe COVID-19 disease\nprogression based on a patient’ s medical history. We begin\nby presenting the pre-training results of our newly created\nExMed-BERT model. Then, we show the performances of\nthe developed risk models, and lastly, we interpret our\nmodels using an explainable AI methodology.\nA. Model Pre-Training\nWe utilized MLM and PLOS as training objectives for\npre-training of the ExMed-BERT model. After 4.5M steps\n(epoch 37), the MLM accuracy increased to around 51 %\nand the PLOS F1 score to 70 %. Following the inclusion of\n61 missing ATC codes and the corresponding changes to\nthe embedding, we began training for 750K steps. Finally,\nwe achieved an MLM accuracy of 67 % and a PLOS F1-score\nof 66 % (epoch 42, Supplementary Figure B.1).\nB. Evaluation of Risk Models\nFollowing pre-training, we developed and evaluated risk\nmodels for predicting the ARM endpoint. Initially, we\nconsidered only the medical history without additional\nquantitative clinical measures. As shown in Table III, all\nExMed-BERT models performed better than the RF , XGB,\nand RETAIN variants on unseen test data. Without quanti-\ntative clinical data, the ExMed-BERT models scored roughly\n78 % AUROC for the ARM endpoint, and the AUPR varied\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 7\nTABLE III\nEVALUATION RESULTS OF THE RISK MODELS FOR PREDICTING ARM\nModel AUROC [%] AUPR [%]\nRF 73.4 [72.6, 74.3] 29.1 [27.6, 30.7]\n+ Quant 77.7 [76.9, 78.6] 34.7 [33.0, 36.4]\nsubset w/o missingness 68.6 [67.1, 70.1] 40.1 [37.8, 42.6]\nsubset w/o missingness + Quant 70.2 [68.8, 71.6] 42.1 [39.7, 44.5]\nXGB 72.4 [71.5, 73.3] 28.2 [26.7, 29.7]\n+ Quant 77.7 [76.9, 78.5] 35.5 [33.8, 37.3]\nsubset w/o missingness 67.8 [66.3, 69.2] 38.8 [36.5, 41.2]\nsubset w/o missingness + Quant 70.6 [69.2, 72.0] 42.2 [39.7, 44.7]\nRETAIN 68.5 [67.4, 69.5] 26.8 [25.3, 28.2]\n+ Quant 66.3 [65.4, 67.4] 22.4 [21.3, 23.6]\nsubset w/o missingness 63.0 [61.5, 64.6] 35.8 [33.5, 38.1]\nsubset w/o missingness + Quant 63.3 [61.7, 64.8] 35.2 [33.0, 37.4]\nExMed-BERT-FFN 77.5 [76.7, 78.4] 38.2 [36.4, 40.0]\n+ Quant 77.7 [76.8, 78.5] 38.1 [36.3, 39.8]\nsubset w/o missingness 67.6 [66.1, 69.1] 39.3 [36.9, 41.6]\nsubset w/o missingness + Quant 70.1 [68.7, 71.6] 41.9 [39.5, 44.4]\nExMed-BERT-GRU 77.7 [76.8, 78.6] 36.7 [35.0, 38.4]\n+ Quant 79.8 [78.9, 80.6] 38.7 [36.9, 40.4]\nsubset w/o missingness 70.7 [69.3, 72.1] 42.8 [40.2, 45.4]\nsubset w/o missingness + Quant 72.0 [70.5, 73.5] 44.7 [42.1, 47.3]\nExMed-BERT-LSTM 77.7 [76.9, 78.6] 37.6 [35.9, 39.4]\n+ Quant 78.4 [77.4, 79.3] 39.3 [37.4, 41.0]\nsubset w/o missingness 71.8 [70.5, 73.3] 45.0 [42.4, 47.4]\nsubset w/o missingness + Quant 71.4 [70.0, 72.8] 43.3 [40.8, 45.8]\nNotes: Areas under the Receiver-Operator Characteristic Curve (AUROC)\nand the Precision-Recall Curve (AUPR) are shown for each model and\nfeature modality. The best results per column are highlighted in bold. The\nsuffix “+Quant” represents the additional use of quantitative clinical data\nduring fine-tuning. The suffix “subset w/o missingness” indicates that we\nused a reduced subset (n = 23,949) for training and evaluation, where\nall quantitative clinical measures were available. The values in brackets\nindicate the 95 % confidence interval which we estimated by performing\nbootstrap resampling for 1000 times.\nbetween 36.7 % and 38.2 %. The RF model, on the other\nhand, only achieved an AUROC of 73.4 % and an AUPR\nof 29.1 %. The XGB model had a slightly lower AUROC of\n72.4 % and AUPR of 28.2 %. The RETAIN model achieved\nthe lowest performance, with an AUROC of 68.5 % and an\nAUPR of 26.8 %.\nThe results of nearly all models improved when quanti-\ntative clinical measurements were integrated. The ExMed-\nBERT model with the GRU classification head integrating\nquantitative data gave the overall best result, with an AU-\nROC of 79.8 % and an AUPR of 38.7 %, which is significantly\nhigher than all other models.\nWhen only patients with fully recorded quantitative\nclinical measurements were used, all models performed\nworse. That means the potential negative effect of imputing\nmissing values was far less than the benefit of including\nadditional data.\nC. Model Explanation\nTo better understand model predictions, we used an\nexplainable AI methodology – namely IG – to calculate\nattribution scores for all features in the best-performing\nExMed-BERT model. We calculated the IG attributions and\nused them to identify the 20 most important features by\nranking them based on their mean absolute value. Figure 3\nshows all the IG attributions and FC scores, which are in\nagreement with each other. We found that the presence\nof diagnoses for chronic airway obstruction, congestive\nheart failure, cough, dementia, edema, obesity, shortness\nof breath, spondylosis, and type 2 diabetes in the medical\nhistory has a large impact on the prediction of a patient’ s\nrisk for ARM. Similarly, the prescriptions of angiotensin II\nreceptor blockers, biguanides, dihydropyridine derivatives,\nand thiazides have a substantial positive impact on our\nmodels’ predictions.\nOf course, these prescriptions and diagnoses could be\ncorrelated with each other, and thus, not all of them might\nhave a direct impact on the ARM endpoint. Hence, we\nlearned the graph structure of a BN to determine how\nthe significant diagnoses or drugs could be related to\none another. The overall network structure is provided\nas a graphml file, an XML-based data format for graph\nrepresentation, as supplementary material to this paper.\nFigure 4 shows two excerpts of the BN graph structure.\nFigure 4a focuses on Angiotensin II receptor blockers and\ntheir relationship to other drugs and diagnoses. Angiotensin\nII receptor blockers are used to treat hypertension, kidney\ndiseases, and heart failure [34]. Furthermore, our graph\nshows a connection to essential hypertension and several\nATC subgroups, namely ACE inhibitors, Dihydropyridine\nderivates, HMG CoA reductase inhibitors, and Thiazides.\nFigure 4b depicts morbid obesity and other diagnoses\nand drugs in its immediate neighborhood. There is a link\nto the class of Biguanides, which includes the drug Met-\nformin, commonly used to treat diabetes [35]. Furthermore,\nmorbid obesity is linked to hypertension, type 2 diabetes,\nobstructive sleep apnea, and obesity.\nTABLE IV\nFEATURES WITH A SIGNIFICANT STATISTICAL EFFECT ON ARM\nFeature Names Corrected p-value\nType 2 diabetes <0.0001\nHeart failure with reduced EF <0.0001\nShortness of breath <0.0001\nConstipation <0.0001\nMorbid obesity <0.0001\nScreening for malignant neoplasms <0.0001\nDementias <0.0001\nChronic airway obstruction 0,0003\nCough 0,0018\nScreening for infectious and parasitic diseases 0,0083\nCongestive heart failure (CHF) NOS 0,0119\nNotes: We performed a logistic regression and corrected for the confound-\ning variables age, sex, and state of residency. The p-values were corrected\nfor multiple testing using the Holm-Sidak method.\nWe aimed for an understanding of the statistical and\npotential causal effect of those features on the endpoint,\nwhich were either among the 20 most important features\nor sink nodes in the BN. The latter are nodes without\noutgoing connections and, therefore, do not influence any\nother features according to our BN analysis. For each of\nthose features, we performed a univariate logistic regres-\nsion analysis while using IPTW case weights to correct\nfor potential confounding effects of age and gender. Our\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 8\nFig. 3. Integrated Gradients Attributions for ExMed-BERT GRU. Depicted are all calculated fold changes (FC) and IG attributions for the 20 most\nimportant features for the prediction of ARM onset. The dashed blue lines indicate neutral attributions. Everything greater than the neutral value\npositively affects the prediction and vice versa.\n(a) Angiotensin II receptor blockers\n (b) Morbid obesity\nFig. 4. Selected features and the direct neighborhood in the inferred Bayesian network. The numbers indicate the bootstrap strength of the respective\nedges in percentage. That means a bootstrap strength of 100 indicates that the corresponding edge has been found in each of 1000 BN reconstructions\nlearned from different bootstrap samples.\nanalysis shows significant effects of several prior diagnoses\non the ARM onset, namely, type 2 diabetes, obesity, demen-\ntia, cardiovascular diseases, and respiratory diseases (see\nTable IV). These morbidities have previously been reported\nas risk factors for severe COVID-19 disease progression [36–\n41], and also the underlying molecular mechanisms have\nbeen discussed [42, 43]. Besides, we found significant effects\nbetween constipation, screening for malignant neoplasms,\nand infectious/parasitic diseases and the ARM onset. This\nmight be explained by the fact that such procedures are\nmore frequently executed in older patients with bad health\nconditions, resulting in a higher risk of severe COVID-19\nprogression. In the same type of patients, constipation is\nalso a frequent problem, e.g., due to lifestyle.\nD. Transfer Learning on Austrian Hospital Data\nFine-tuning of Ex-MedBERT on a small set of\npseudonymized in-patient data from an Austrian hospital\ngroup resulted in a prediction performance almost identical\nto the one observed for an RF trained de novo on the\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 9\nsame data (Supplementary Table B.2). At the same time,\nprediction performances were significantly lower than\nthe ones observed on IBM Explorys (AUC ≈ 60 %). We\nwill elaborate on potential reasons in the subsequent\ndiscussion.\nIV. D ISCUSSION\nPandemics such as COVID-19 pose immense challenges\nto global healthcare systems. Utilizing patient-level risk\nmodels to support doctors and clinics is one way to maxi-\nmize the use of available resources. Following previous re-\nsearch, we trained a transformer-based model on structured\nEHR data in this study. In contrast to the prior approaches,\nsuch as BEHRT or Med-BERT , we incorporated additional\ndata modalities and developed risk models for COVID-19\ndisease progression. Prediction performances achieved by\nour ExMed-BERT model are altogether superior to those\nreported by Lazzarini et al. [44] for the closely related\nendpoint of acute respiratory distress syndrome (ARDS)\n[44]. The authors trained an XGB based on US adminis-\ntrative claims data from 290,000 patients and achieved\nan AUROC of 69 % and an AUPR of 7 %. For comparison,\nusing data from intensive care units (ICUs), Bendavid et\nal. reported an AUROC of 83 % for an XGB trained to\npredict the initiation of invasive mechanical ventilation\n[45], and Singhal et al. achieved an AUROC of 89 % for\npredicting the onset of ARDS [46]. Importantly, ICU data\nare structurally and content-wise very different from the\ndata used in our study, which comprises in-patient as well\nas out-patient information over a more extended period\n(here: one year), but only contains limited quantitative\ninformation. Altogether, our findings align with previous\nstudies [5, 7, 8], showing that transformer-based models are\nwell-suited for structured EHR data similar to ours. Even\nwithout additional quantitative information, our ExMed-\nBERT outperformed the RF , XGB, and RETAIN models. With\nthe inclusion of quantitative clinical measures, our ExMed-\nBERT models further increased in prediction performance.\nFor that purpose, we proposed a novel approach to combine\nquantitative clinical measures with the embeddings of EHR\ncodes learned by ExMed-BERT , which resulted in the overall\nbest-performing model. Our results thus demonstrate the\nimportance of combining diagnosis and prescription codes\nwith quantitative clinical measures for developing risk mod-\nels. Even though these quantitative clinical measures were\nonly taken at a single time point in the two weeks preceding\nthe COVID-19 infection and not for every patient, using\nthese data could provide better performance.\nUsing a combined strategy consisting of feature impor-\ntance analysis, BN structure learning and statistical hypoth-\nesis testing, we were able to identify diagnoses and prescrip-\ntions that have a significant impact on model prediction\nand may causally influence the endpoint. Our analysis\nsupports that socioeconomic and psycho-social health risks\nplay an important role in addition to well-known risk\nfactors such as obesity, diabetes, cardiovascular diseases,\nand dementia, which have already been reported as known\nrisk factors for severe COVID-19 disease progression in\nseveral studies [36, 39, 47–49]. This confirms the validity\nof our approach, which can be applied to other datasets as\nwell.\nOur work demonstrates the potential of a transformer-\nbased pre-training / fine-tuning strategy to develop risk\nmodels for precision medicine. This strategy provides the\nchance to perform transfer learning of our model on data\nfrom other organizations and thus use the pre-trained\nExMed-BERT as a basis for future model development.\nOur experiment with data from an Austrian hospital group\ndemonstrated the potential as well as the limitations of\nsuch an approach: The data from the Austrian hospital\ngroup only comprises in-patient information, and the num-\nber of patients is far smaller than during the fine-tuning\nphase on the IBM Explorys data (6,335 patients instead of\n80,211). Furthermore, the ratio of ARM-positive patients is\nsignificantly lower (6.1 % instead of 13.4 %). Notably, there\ncould also be different medical coding practices in the two\ncountries. Finally, constraints on the technical equipment\nwithin the Austrian hospital group only allowed us to\nfine-tune our model for a small number of epochs and\nwithout hyperparameter tuning. Due to all these factors,\nour ExMed-BERT model fine-tuned on the Austrian data\nachieved a performance that was comparable to an RF\nmodel trained de novo on the same data but significantly\nlower than prediction performances achieved on US data.\nWe thus conclude that having a sufficiently large dataset\nwith a number of patients in a range comparable to the\nIBM Explorys data would be a prerequisite to obtaining\nbetter models in a transfer learning setting. Furthermore,\nappropriate technical equipment is important. Finally, the\nintegration of in-patient and out-patient data is required,\nat least for our model.\nAnother limitation is the lack of previously published\ntransformer-based models, such as BEHRT or Med-BERT ,\nand the associated data, which hindered direct comparison\nwith our model. As the pre-training of transformer-based\nmodels is computationally extremely expensive, it is often\nnot feasible to run comprehensive ablation studies. Despite\nthese limitations, our model was rigorously assessed by\ncomparing it against established approaches (RF , XGBoost,\nand RETAIN), and its potential was adequately demon-\nstrated. By making our model publicly available, future\nstudies can use it as a foundation for further development.\nV. C ONCLUSION\nOur work demonstrates the potential of customized\ntransformer-based models for analyzing structured EHR\ndata. We showed that it is possible to integrate quanti-\ntative clinical data into such models, which can signifi-\ncantly improve prediction performance. Furthermore, we\nintroduced a general approach for explaining ExMed-BERT\nmodel predictions. Transfer learning strategies open the\npossibility of leveraging our pre-trained ExMed-BERT model\nfor the prediction of clinical endpoints different from the\none addressed within this paper. For that purpose, we\nallow users to apply for access to our pre-trained ExMed-\nBERT model on https://doi.org/10.5281/zenodo.7324178 or\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 10\nby sending an email to the corresponding author. Our code\nis available at https://github.com/SCAI-BIO/ExMed-BERT.\nACKNOWLEDGMENTS\nThis research was performed in the context of the\n‘COPERIMOplus’ initiative and supported by the Fraunhofer\n‘Internal Programs’ under Grant No. Anti-Corona 840266.\nThe COPERIMOplus Consortium: Fraunhofer Data Protec-\ntion Office (Anne Funck Hansen), Fraunhofer IAIS (Sabine\nKugler, Stefan Rüping), Fraunhofer IGD (Jan Burmeister,\nJörn Kohlhammer), Fraunhofer IKTS (George Sarau, Silke\nChristiansen), Fraunhofer IME (Oliver Keminer), Fraun-\nhofer ITMP (Aimo Kannt, Andrea Zaliani, Ann Christina\nFoldenauer, Carsten Claussen, Eduard Resch, Kevin Frank),\nFraunhofer MEVIS (Hendrik Laue, Horst Hahn, Jochen\nHirsch, Marco Wischnewski, Matthias Günther, Saulius\nArchipovas), Fraunhofer SCAI (Alpha Tom Kodamullil, An-\ndre Gemünd, Bruce Schultz, Carina Steinborn, Christian\nEbeling, Daniel Domingo Fernández, Helena Hermanowski,\nHolger Fröhlich, Jürgen Klein, Manuel Lentzen, Marc Ja-\ncobs, Martin Hofmann-Apitius, Meike Knieps, Michael\nKrapp, Philipp Johannes Wendland, Philipp Wegner, Sepehr\nGolriz Khatami, Stephan Springstubbe, Thomas Linden), ZB\nMED Information Centre for Life Sciences (Juliane Fluck).\nCONFLICT OF INTEREST STATEMENT\nSV , DK, and WL received salaries from Steiermärkische\nKrankenanstaltengesellschaft m.b.H. (KAGes) (Graz, Aus-\ntria). The company had no influence on the scientific results\npresented in this paper.\nCONTRIBUTIONS\nInitiated and supervised project: HF; programmed code\nand conducted experiments: ML, TL, SV , DK; supervised\ntransfer learning on Austrian hospital data: WL †. Drafted\nthe manuscript: ML, HF . All authors have read and approved\nthe current version of the paper.\nREFERENCES\n[1] R. Miotto, F . Wang, S. Wang, X. Jiang, and J. T . Dudley, “Deep learning\nfor healthcare: review, opportunities and challenges,” Briefings in\nBioinformatics, vol. 19, no. 6, pp. 1236–1246, Nov. 2018.\n[2] B. A. Goldstein, A. M. Navar, M. J. Pencina, and J. P . A. Ioannidis,\n“Opportunities and challenges in developing risk prediction models\nwith electronic health records data: a systematic review,” Journal of\nthe American Medical Informatics Association: JAMIA , vol. 24, no. 1,\npp. 198–208, Jan. 2017.\n[3] T . Linden, J. De Jong, C. Lu, V . Kiri, K. Haeffs, and H. Fröhlich, “ An\nExplainable Multimodal Neural Network Architecture for Predicting\nEpilepsy Comorbidities Based on Administrative Claims Data,”\nFrontiers in Artificial Intelligence , vol. 4, 2021. [Online]. Available:\nhttps://www.frontiersin.org/article/10.3389/frai.2021.610197\n[4] L. Rasmy, M. Nigo, B. S. Kannadath, Z. Xie, B. Mao, K. Patel,\nY. Zhou, W . Zhang, A. Ross, H. Xu, and D. Zhi, “CovRNN—A\nrecurrent neural network model for predicting outcomes of COVID-\n19 patients: model development and validation using EHR data,”\nmedRxiv, Tech. Rep., Sep. 2021, type: article. [Online]. Available:\nhttps://www.medrxiv.org/content/10.1101/2021.09.27.21264121v1\n[5] Y. Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrishnan, D. Canoy,\nY. Zhu, K. Rahimi, and G. Salimi-Khorshidi, “BEHRT: Transformer for\nElectronic Health Records,” Scientific Reports , vol. 10, no. 1, p. 7155,\nApr. 2020, number: 1 Publisher: Nature Publishing Group. [Online].\nAvailable: https://www.nature.com/articles/s41598-020-62922-y\n[6] Y. Li, M. Mamouei, G. Salimi-Khorshidi, S. Rao, A. Hassaine,\nD. Canoy, T . Lukasiewicz, and K. Rahimi, “Hi-BEHRT: Hierarchical\nTransformer-based model for accurate prediction of clinical events\nusing multimodal longitudinal electronic health records,” 2021,\npublisher: arXiv Version Number: 1. [Online]. Available: https:\n//arxiv.org/abs/2106.11360\n[7] L. Rasmy, Y. Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-BERT:\npretrained contextualized embeddings on large-scale structured\nelectronic health records for disease prediction,” npj Digital\nMedicine, vol. 4, no. 1, pp. 1–13, May 2021, number: 1\nPublisher: Nature Publishing Group. [Online]. Available: https:\n//www.nature.com/articles/s41746-021-00455-y\n[8] Y. Meng, W . Speier, M. K. Ong, and C. W . Arnold, “Bidirectional\nRepresentation Learning From Transformers Using Multimodal Elec-\ntronic Health Record Data to Predict Depression,” IEEE Journal of\nBiomedical and Health Informatics , vol. 25, no. 8, pp. 3121–3129,\nAug. 2021, conference Name: IEEE Journal of Biomedical and Health\nInformatics.\n[9] J. Shang, T . Ma, C. Xiao, and J. Sun, “Pre-training of Graph Augmented\nTransformers for Medication Recommendation,” 2019.\n[10] J. Lee, W . Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\n“BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining,” Bioinformatics, p. btz682, Sep. 2019,\narXiv: 1901.08746. [Online]. Available: http://arxiv.org/abs/1901.08746\n[11] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rihawi, Y. Wang, L. Jones,\nT . Gibbs, T . Feher, C. Angerer, M. Steinegger, D. Bhowmik, and\nB. Rost, “ProtTrans: Towards Cracking the Language of Life’ s\nCode Through Self-Supervised Deep Learning and High Performance\nComputing,” May 2021, arXiv:2007.06225 [cs, stat]. [Online]. Available:\nhttp://arxiv.org/abs/2007.06225\n[12] Y. Ji, Z. Zhou, H. Liu, and R. V . Davuluri, “DNABERT: pre-\ntrained Bidirectional Encoder Representations from Transformers\nmodel for DNA-language in genome,” Bioinformatics, vol. 37,\nno. 15, pp. 2112–2120, Aug. 2021. [Online]. Available: https:\n//academic.oup.com/bioinformatics/article/37/15/2112/6128680\n[13] S. Madan, V . Demina, M. Stapf, O. Ernst, and H. Fröhlich, “Accurate\nprediction of virus-host protein-protein interactions via a Siamese\nneural network using deep protein sequence embeddings,” Patterns,\nvol. 3, no. 9, p. 100551, Sep. 2022. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/S2666389922001568\n[14] J. Devlin, M.-W . Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding,”\narXiv:1810.04805 [cs] , May 2019, arXiv: 1810.04805.\n[15] E. Choi, M. T . Bahadori, J. A. Kulas, A. Schuetz, W . F . Stewart,\nand J. Sun, “RETAIN: An Interpretable Predictive Model for\nHealthcare using Reverse Time Attention Mechanism,” Feb. 2017,\narXiv:1608.05745 [cs]. [Online]. Available: http://arxiv.org/abs/1608.\n05745\n[16] L. Breiman, “Random Forests,” Machine Learning , vol. 45, no. 1,\npp. 5–32, Oct. 2001. [Online]. Available: https://doi.org/10.1023/A:\n1010933404324\n[17] T . Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,”\nin Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , ser. KDD ’16. New York, NY,\nUSA: Association for Computing Machinery, Aug. 2016, pp. 785–794.\n[Online]. Available: https://doi.org/10.1145/2939672.2939785\n[18] World Health Organization, “ICD-10 : international statistical\nclassification of diseases and related health problems : tenth\nrevision,” World Health Organization, Tech. Rep., 2004, iSBN:\n9789241546492. [Online]. Available: https://apps.who.int/iris/handle/\n10665/42980\n[19] C. J. McDonald, S. M. Huff, J. G. Suico, G. Hill, D. Leavelle, R. Aller,\nA. Forrey, K. Mercer, G. DeMoor, J. Hook, W . Williams, J. Case,\nP . Maloney, and for the Laboratory LOINC Developers, “LOINC, a\nUniversal Standard for Identifying Laboratory Observations: A 5-Year\nUpdate,” Clinical Chemistry , vol. 49, no. 4, pp. 624–633, Apr. 2003.\n[Online]. Available: https://academic.oup.com/clinchem/article/49/\n4/624/5641953\n[20] “The Central Role of the Propensity Score in Observational Studies\nfor Causal Effects.” [Online]. Available: https://dash.harvard.edu/\nhandle/1/3382855\n[21] P . R. Rosenbaum, “Model-Based Direct Adjustment,” Journal\nof the American Statistical Association , vol. 82, no. 398,\npp. 387–394, Jun. 1987, publisher: Taylor & Francis _eprint:\nhttps://www.tandfonline.com/doi/pdf/10.1080/01621459.1987.10478441.\n[Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/\n01621459.1987.10478441\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 11\n[22] P . C. Austin and M. M. Mamdani, “A comparison\nof propensity score methods: a case-study estimating\nthe effectiveness of post-AMI statin use,” Statistics in\nMedicine, vol. 25, no. 12, pp. 2084–2106, 2006, _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2328. [Online].\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2328\n[23] A. Kline, “psmpy: Propensity score matching for python and graphical\nplots.”\n[24] S. J. Nelson, K. Zeng, J. Kilbourne, T . Powell, and R. Moore,\n“Normalized names for clinical drugs: RxNorm at 6 years,” Journal\nof the American Medical Informatics Association , vol. 18, no. 4, pp.\n441–448, Jul. 2011. [Online]. Available: https://academic.oup.com/\njamia/article-lookup/doi/10.1136/amiajnl-2011-000116\n[25] P . Wu, A. Gifford, X. Meng, X. Li, H. Campbell, T . Varley,\nJ. Zhao, L. Bastarache, J. C. Denny, E. Theodoratou, and\nW .-Q. Wei, “Developing and Evaluating Mappings of ICD-10\nand ICD-10-CM codes to Phecodes,” bioRxiv, Tech. Rep., Nov.\n2018, section: New Results Type: article. [Online]. Available:\nhttps://www.biorxiv.org/content/10.1101/462077v2\n[26] “WHO Collaborating Centre for Drug Statistics Methodology,\nATC classification index with DDDs,” 2022, oslo, Norway\n2021. [Online]. Available: https://www.whocc.no/atc_ddd_index_\nand_guidelines/atc_ddd_index/\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “ Attention Is All You Need,” Dec. 2017,\narXiv:1706.03762 [cs].\n[28] T . Akiba, S. Sano, T . Yanase, T . Ohta, and M. Koyama, “Optuna: A Next-\ngeneration Hyperparameter Optimization Framework,” in Proceedings\nof the 25rd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining , 2019.\n[29] “Missing Data Imputation for Python - missingpy,” publisher: Ashim\nBhattarai. [Online]. Available: https://pypi.org/project/missingpy/\n[30] M. Sundararajan, A. Taly, and Q. Yan, “ Axiomatic Attribution for Deep\nNetworks,” Jun. 2017, number: arXiv:1703.01365 arXiv:1703.01365\n[cs]. [Online]. Available: http://arxiv.org/abs/1703.01365\n[31] M. Scutari, “Learning Bayesian Networks with the bnlearn R Package,”\nJournal of Statistical Software , vol. 35, no. 3, pp. 1–22, 2010.\n[32] F . Glover and C. McMillan, “The general employee scheduling\nproblem. An integration of MS and AI,” Computers &\nOperations Research , vol. 13, no. 5, pp. 563–573, Jan. 1986.\n[Online]. Available: https://www.sciencedirect.com/science/article/\npii/030505488690050X\n[33] “Tabu Search—Part I | ORSA Journal on Computing.” [Online]. Avail-\nable: https://pubsonline.informs.org/doi/abs/10.1287/ijoc.1.3.190\n[34] Z. H. Israili, “Clinical pharmacokinetics of angiotensin II (AT1)\nreceptor blockers in hypertension,” Journal of Human Hypertension ,\nvol. 14, no. 1, pp. S73–S86, Apr. 2000, number: 1 Publisher:\nNature Publishing Group. [Online]. Available: https://www.nature.\ncom/articles/1000991\n[35] R. Vigneri and I. D. Goldfine, “Role of metformin in treatment of\ndiabetes mellitus,” Diabetes Care , vol. 10, no. 1, pp. 118–122, Jan.\n1987. [Online]. Available: https://diabetesjournals.org/care/article/\n10/1/118/790/Role-of-metformin-in-treatment-of-diabetes\n[36] W .-j. Guan, W .-h. Liang, Y. Zhao, H.-r. Liang, Z.-s. Chen, Y.-m. Li,\nX.-q. Liu, R.-c. Chen, C.-l. Tang, T . Wang, C.-q. Ou, L. Li, P .-y. Chen,\nL. Sang, W . Wang, J.-f. Li, C.-c. Li, L.-m. Ou, B. Cheng, S. Xiong,\nZ.-y. Ni, J. Xiang, Y. Hu, L. Liu, H. Shan, C.-l. Lei, Y.-x. Peng, L. Wei,\nY. Liu, Y.-h. Hu, P . Peng, J.-m. Wang, J.-y. Liu, Z. Chen, G. Li, Z.-j.\nZheng, S.-q. Qiu, J. Luo, C.-j. Ye, S.-y. Zhu, L.-l. Cheng, F . Ye, S.-y. Li,\nJ.-p. Zheng, N.-f. Zhang, N.-s. Zhong, and J.-x. He, “Comorbidity and\nits impact on 1590 patients with COVID-19 in China: a nationwide\nanalysis,” European Respiratory Journal , vol. 55, no. 5, May 2020,\npublisher: European Respiratory Society Section: Original Articles.\n[Online]. Available: https://erj.ersjournals.com/content/55/5/2000547\n[37] A. C. Tahira, S. Verjovski-Almeida, and S. T . Ferreira,\n“Dementia is an age-independent risk factor for severity\nand death in COVID-19 inpatients,” Alzheimer’s & De-\nmentia, vol. 17, no. 11, pp. 1818–1831, 2021, _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/alz.12352. [Online].\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.12352\n[38] S. Toniolo, M. Scarioni, F . Di Lorenzo, J. Hort, J. Georges, S. Tomic,\nF . Nobili, K. S. Frederiksen, and t. M. G. o. t. E. D. a. C.\nD. S. Panel, “Dementia and COVID-19, a Bidirectional Liaison:\nRisk Factors, Biomarkers, and Optimal Health Care,” Journal of\nAlzheimer’s Disease, vol. 82, no. 3, pp. 883–898, Jan. 2021, publisher:\nIOS Press. [Online]. Available: https://content.iospress.com/articles/\njournal-of-alzheimers-disease/jad210335\n[39] S. Peric and T . M. Stulnig, “Diabetes and COVID-19,” Wiener\nklinische Wochenschrift , vol. 132, no. 13, pp. 356–361, Jul. 2020.\n[Online]. Available: https://doi.org/10.1007/s00508-020-01672-3\n[40] F . Demeulemeester, K. de Punder, M. van Heijningen, and F . van\nDoesburg, “Obesity as a Risk Factor for Severe COVID-19 and\nComplications: A Review,” Cells, vol. 10, no. 4, p. 933, Apr. 2021,\nnumber: 4 Publisher: Multidisciplinary Digital Publishing Institute.\n[Online]. Available: https://www.mdpi.com/2073-4409/10/4/933\n[41] S. Kwok, S. Adam, J. H. Ho, Z. Iqbal, P . Turkington,\nS. Razvi, C. W . Le Roux, H. Soran, and A. A. Syed,\n“Obesity: A critical risk factor in the COVID-19 pandemic,”\nClinical Obesity , vol. 10, no. 6, p. e12403, 2020, _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/cob.12403. [Online].\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1111/cob.12403\n[42] R. Concha, E. Ohayon, and A. Lam, “Neuroinflammation in\nCOVID-19 and ADRD: Similarities, differences, and interactions,”\nAlzheimer’s & Dementia , vol. 17, no. S3, p. e056282, 2021, _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/alz.056282. [Online].\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.056282\n[43] C. Steenblock, P . E. H. Schwarz, B. Ludwig, A. Linkermann, P . Zimmet,\nK. Kulebyakin, V . A. Tkachuk, A. G. Markov, H. Lehnert, M. H.\nde Angelis, H. Rietzsch, R. N. Rodionov, K. Khunti, D. Hopkins, A. L.\nBirkenfeld, B. Boehm, R. I. G. Holt, J. S. Skyler, J. H. DeVries, E. Renard,\nR. H. Eckel, K. G. M. M. Alberti, B. Geloneze, J. C. Chan, J. C. Mbanya,\nH. C. Onyegbutulem, A. Ramachandran, A. Basit, M. Hassanein, G. Be-\nwick, G. A. Spinas, F . Beuschlein, R. Landgraf, F . Rubino, G. Mingrone,\nand S. R. Bornstein, “COVID-19 and metabolic disease: mechanisms\nand clinical management,” The Lancet. Diabetes & Endocrinology ,\nvol. 9, no. 11, pp. 786–798, Nov. 2021.\n[44] N. Lazzarini, A. Filippoupolitis, P . Manzione, and H. Eleftherohorinou,\n“A machine learning model on Real World Data for predicting\nprogression to Acute Respiratory Distress Syndrome (ARDS) among\nCOVID-19 patients,” PloS One , vol. 17, no. 7, p. e0271227, 2022.\n[45] I. Bendavid, L. Statlender, L. Shvartser, S. Teppler, R. Azullay, R. Sapir,\nand P . Singer, “A novel machine learning model to predict respiratory\nfailure and invasive mechanical ventilation in critically ill patients\nsuffering from COVID-19,” Scientific Reports , vol. 12, no. 1, p. 10573,\nJun. 2022, number: 1 Publisher: Nature Publishing Group. [Online].\nAvailable: https://www.nature.com/articles/s41598-022-14758-x\n[46] L. Singhal, Y. Garg, P . Yang, A. Tabaie, A. I. Wong, A. Mohammed,\nL. Chinthala, D. Kadaria, A. Sodhi, A. L. Holder, A. Esper, J. M.\nBlum, R. L. Davis, G. D. Clifford, G. S. Martin, and R. Kamaleswaran,\n“eARDS: A multi-center validation of an interpretable machine\nlearning algorithm of early onset Acute Respiratory Distress\nSyndrome (ARDS) among critically ill adults with COVID-19,” PLOS\nONE, vol. 16, no. 9, p. e0257056, Sep. 2021, publisher: Public Library\nof Science. [Online]. Available: https://journals.plos.org/plosone/\narticle?id=10.1371/journal.pone.0257056\n[47] Y. Du, L. Tu, P . Zhu, M. Mu, R. Wang, P . Yang, X. Wang, C. Hu,\nR. Ping, P . Hu, T . Li, F . Cao, C. Chang, Q. Hu, Y. Jin, and G. Xu,\n“Clinical Features of 85 Fatal Cases of COVID-19 from Wuhan. A\nRetrospective Observational Study,” American Journal of Respiratory\nand Critical Care Medicine , vol. 201, no. 11, pp. 1372–1379, Jun. 2020,\npublisher: American Thoracic Society - AJRCCM. [Online]. Available:\nhttps://www.atsjournals.org/doi/10.1164/rccm.202003-0543OC\n[48] B. Li, J. Yang, F . Zhao, L. Zhi, X. Wang, L. Liu, Z. Bi, and\nY. Zhao, “Prevalence and impact of cardiovascular metabolic\ndiseases on COVID-19 in China,” Clinical Research in Cardiology ,\nvol. 109, no. 5, pp. 531–538, May 2020. [Online]. Available:\nhttps://doi.org/10.1007/s00392-020-01626-9\n[49] T . Linden, F . Hanses, D. Domingo-Fernández, L. N. DeLong, A. T .\nKodamullil, J. Schneider, M. J. G. T . Vehreschild, J. Lanznaster,\nM. M. Ruethrich, S. Borgmann, M. Hower, K. Wille, T . Feldt,\nS. Rieg, B. Hertenstein, C. Wyen, C. Roemmele, J. J. Vehreschild,\nC. E. M. Jakob, M. Stecher, M. Kuzikov, A. Zaliani, and H. Fröhlich,\n“Machine Learning Based Prediction of COVID-19 Mortality Suggests\nRepositioning of Anticancer Drug for Treating Severe Cases,”\nArtificial Intelligence in the Life Sciences , vol. 1, p. 100020, Dec. 2021.\n[Online]. Available: https://www.sciencedirect.com/science/article/\npii/S2667318521000209\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2023.3288768\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Machine learning",
  "concepts": [
    {
      "name": "Machine learning",
      "score": 0.5782451629638672
    },
    {
      "name": "Computer science",
      "score": 0.5724784135818481
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5355873703956604
    },
    {
      "name": "Random forest",
      "score": 0.5207500457763672
    },
    {
      "name": "Predictive modelling",
      "score": 0.5106564164161682
    },
    {
      "name": "Lift (data mining)",
      "score": 0.5028817057609558
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.4807869791984558
    },
    {
      "name": "Transformer",
      "score": 0.43752825260162354
    },
    {
      "name": "Disease",
      "score": 0.4290555417537689
    },
    {
      "name": "Big data",
      "score": 0.42354559898376465
    },
    {
      "name": "Medicine",
      "score": 0.4115533232688904
    },
    {
      "name": "Pandemic",
      "score": 0.411484032869339
    },
    {
      "name": "Data mining",
      "score": 0.36830052733421326
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.26672643423080444
    },
    {
      "name": "Engineering",
      "score": 0.1733919382095337
    },
    {
      "name": "Internal medicine",
      "score": 0.1329129934310913
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210098248",
      "name": "Fraunhofer Institute for Algorithms and Scientific Computing",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I132118926",
      "name": "Austrian Institute of Technology",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I135140700",
      "name": "University of Bonn",
      "country": "DE"
    }
  ]
}