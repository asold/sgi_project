{
    "title": "Efficient Large-Scale Language Model Training on GPU Clusters",
    "url": "https://openalex.org/W3153553004",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2340198795",
            "name": "Deepak Narayanan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1842149474",
            "name": "Mohammad Shoeybi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2122701668",
            "name": "Jared Casper",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2017412142",
            "name": "Patrick LeGresley",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3044783319",
            "name": "Mostofa Patwary",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1221923317",
            "name": "Vijay Anand Korthikanti",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A223497488",
            "name": "Dmitri Vainbrand",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3152676833",
            "name": "Prethvi Kashinkunti",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2170873571",
            "name": "Julie Bernauer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2688031072",
            "name": "Bryan Catanzaro",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A154216248",
            "name": "Amar Phanishayee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2009645378",
            "name": "Matei Zaharia",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962747323",
        "https://openalex.org/W2900096133",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W2338908902",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2884700152",
        "https://openalex.org/W3130395060",
        "https://openalex.org/W3036879053",
        "https://openalex.org/W2785452945",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3021234081",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3031276512",
        "https://openalex.org/W3127806443",
        "https://openalex.org/W3156643189",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2991040477",
        "https://openalex.org/W3037585619",
        "https://openalex.org/W3121562065",
        "https://openalex.org/W2979245724",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3132107458",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3025935268"
    ],
    "abstract": "Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these large models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on a single GPU or even on a multi-GPU server; and b) the number of compute operations required to train these models can result in unrealistically long training times. New methods of model parallelism such as tensor and pipeline parallelism have been proposed to address these challenges. Unfortunately, naive usage leads to fundamental scaling issues at thousands of GPUs due to various reasons, e.g., expensive cross-node communication or idle periods waiting on other devices. \r\nIn this work, we show how to compose different types of parallelism methods (tensor, pipeline, and data parallelism) to scale to thousands of GPUs, achieving a two-order-of-magnitude increase in the sizes of models we can efficiently train compared to existing systems. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by more than 10% with comparable memory footprint compared to previously-proposed approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of peak; previous efforts to train similar-sized models achieve much lower throughput (36% of theoretical peak). Our code is open sourced at this https URL.",
    "full_text": "Efficient Large-Scale Language Model Training on GPU Clusters\nUsing Megatron-LM\nDeepak Narayananâ€¡â˜…, Mohammad Shoeybiâ€ , Jared Casperâ€ , Patrick LeGresleyâ€ ,\nMostofa Patwaryâ€ , Vijay Korthikantiâ€ , Dmitri Vainbrandâ€ , Prethvi Kashinkuntiâ€ ,\nJulie Bernauerâ€ , Bryan Catanzaroâ€ , Amar Phanishayeeâˆ—, Matei Zahariaâ€¡\nâ€ NVIDIA â€¡Stanford University âˆ—Microsoft Research\nABSTRACT\nLarge language models have led to state-of-the-art accuracies across\nseveral tasks. However, training these models efficiently is chal-\nlenging because: a) GPU memory capacity is limited, making it\nimpossible to fit large models on even a multi-GPU server, and\nb) the number of compute operations required can result in un-\nrealistically long training times. Consequently, new methods of\nmodel parallelism such as tensor and pipeline parallelism have\nbeen proposed. Unfortunately, naive usage of these methods leads\nto scaling issues at thousands of GPUs. In this paper, we show how\ntensor, pipeline, and data parallelism can be composed to scale\nto thousands of GPUs. We propose a novel interleaved pipelining\nschedule that can improve throughput by 10+% with memory foot-\nprint comparable to existing approaches. Our approach allows us\nto perform training iterations on a model with 1 trillion parameters\nat 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52% of\ntheoretical peak).\n1 INTRODUCTION\nTransformer-based language models [13, 27, 33â€“35, 42, 46] in Nat-\nural Language Processing (NLP) have driven rapid progress in re-\ncent years as computation at scale has become more available and\ndatasets have become larger. Recent work [11, 40] has shown large\nlanguage models to be effective zero- or few-shot learners, with high\naccuracy on many NLP tasks and datasets. These large language\nmodels have a number of exciting downstream applications such\nas client feedback summarization, automatic dialogue generation,\nsemantic search, and code autocompletion [1, 4, 5]. As a result, the\nnumber of parameters in state-of-the-art NLP models have grown\nat an exponential rate (Figure 1). Training such models, however,\nis challenging for two reasons: (a) it is no longer possible to fit the\nparameters of these models in the main memory of even the largest\nGPU (NVIDIA recently released 80GB-A100 cards), and (b) even if\nwe are able to fit the model in a single GPU (e.g., by swapping pa-\nrameters between host and device memory [38]), the high number\nof compute operations required can result in unrealistically long\ntraining times (e.g., training GPT-3 with 175 billion parameters [11]\nwould require approximately 288 years with a single V100 NVIDIA\nGPU). This calls for parallelism. Data-parallel scale-out usually\nworks well, but suffers from two limitations: a) beyond a point, the\nper-GPU batch size becomes too small, reducing GPU utilization\nand increasing communication cost, and b) the maximum number\nof devices that can be used is the batch size, limiting the number of\naccelerators that can be used for training.\nâ˜…Work done as an intern at NVIDIA.\n/uni00000016/uni00000014/uni00000015/uni0000001c/uni00000016/uni00000014/uni00000015/uni0000001d/uni00000016/uni00000014/uni00000016/uni00000014/uni00000016/uni00000014/uni00000016/uni00000015\n/uni0000003d/uni00000049/uni00000045/uni00000056\n/uni00000015/uni00000014/uni00000016\n/uni00000015/uni00000014/uni00000015\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000014/uni00000015\n/uni00000015/uni00000014/uni00000016\n/uni00000015/uni00000014/uni00000017\n/uni00000032/uni00000059/uni00000051/uni00000046/uni00000049/uni00000056/uni00000004/uni00000053/uni0000004a/uni00000004/uni00000054/uni00000045/uni00000056/uni00000045/uni00000051/uni00000049/uni00000058/uni00000049/uni00000056/uni00000057\n/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000046/uni0000004d/uni00000050/uni00000050/uni0000004d/uni00000053/uni00000052/uni00000057/uni0000000d\n/uni00000029/uni00000030/uni00000031/uni00000053/uni00000004/uni0000000c/uni0000001d/uni00000018/uni00000031/uni0000000d\n/uni00000026/uni00000029/uni00000036/uni00000038/uni00000011/uni00000030/uni00000004/uni0000000c/uni00000017/uni00000018/uni00000014/uni00000031/uni0000000d\n/uni0000002b/uni00000034/uni00000038/uni00000011/uni00000016/uni00000004/uni0000000c/uni00000015/uni00000012/uni00000019/uni00000026/uni0000000d\n/uni00000031/uni00000049/uni0000004b/uni00000045/uni00000058/uni00000056/uni00000053/uni00000052/uni00000011/uni00000030/uni00000031/uni00000004/uni0000000c/uni0000001c/uni00000012/uni00000017/uni00000026/uni0000000d\n/uni00000038/uni00000059/uni00000056/uni0000004d/uni00000052/uni0000004b/uni00000011/uni00000032/uni00000030/uni0000002b/uni00000004/uni0000000c/uni00000015/uni0000001b/uni00000012/uni00000016/uni00000026/uni0000000d\n/uni0000002b/uni00000034/uni00000038/uni00000011/uni00000017/uni00000004/uni0000000c/uni00000015/uni0000001b/uni00000019/uni00000026/uni0000000d\nFigure 1: Trend of sizes of state-of-the-art Natural Language Pro-\ncessing (NLP) models with time. The number of floating-point op-\nerations to train these models is increasing at an exponential rate.\nVarious model parallelism techniques have been proposed to\naddress these two challenges. For example, recent work [39, 40] has\nshown how tensor (intra-layer) model parallelism, where matrix\nmultiplications within each transformer layer are split over multiple\nGPUs, can be used to overcome these limitations. Although this\napproach works well for models of sizes up to 20 billion parameters\non NVIDIA DGX A100 servers (with 8 80GB-A100 GPUs), it breaks\ndown for larger models. Larger models need to be split across\nmultiple multi-GPU servers, which leads to two problems: (a) the\nall-reduce communication required for tensor parallelism needs\nto go through inter-server links, which are slower than the high-\nbandwidth NVLink [9] available within a multi-GPU server, and\n(b) a high degree of model parallelism can create small matrix\nmultiplications (GEMMs), potentially decreasing GPU utilization.\nPipeline model parallelism [14, 20, 23, 29, 30, 45] is another tech-\nnique to support the training of large models, where layers of a\nmodel are striped over multiple GPUs. A batch is split into smaller\nmicrobatches, and execution is pipelined across these microbatches.\nLayers can be assigned to workers in various ways, and various\nschedules for the forward and backward passes of inputs can be\nused. The layer assignment and scheduling strategy results in dif-\nferent performance tradeoffs. Regardless of schedule, to preserve\nstrict optimizer semantics, optimizer steps need to be synchronized\nacross devices, leading to a pipeline flush at the end of every batch,\nwhere microbatches are allowed to complete execution (and no\nnew microbatches are injected). As much as 50% of time can be\nspent flushing the pipeline depending on the number of micro-\nbatches injected into the pipeline. The larger the ratio of number\nof microbatches to the pipeline size, the smaller the time spent in\nthe pipeline flush. Therefore, to achieve high efficiency, a larger\nbatch size is often necessary. In this work, we also introduce a new\npipeline schedule that improves efficiency at small batch sizes.\nUsers can thus train their large models using various techniques,\neach with different tradeoffs. Moreover, these techniques can be\narXiv:2104.04473v5  [cs.CL]  23 Aug 2021\ncombined. However, combining these techniques leads to non-trivial\ninteractions, which need to be reasoned through carefully for good\nperformance. In this paper, we address the following question:\nHow should parallelism techniques be combined to max-\nimize the training throughput of large models given a\nbatch size while retaining strict optimizer semantics?\nIn particular, we show how to combine pipeline, tensor, and\ndata parallelism, a technique we call PTD-P, to train large language\nmodels with good computational performance (52% of peak device\nthroughput) on 1000s of GPUs. Our method leverages the com-\nbination of pipeline parallelism across multi-GPU servers, tensor\nparallelism within a multi-GPU server, and data parallelism, to\npractically train models with a trillion parameters with graceful\nscaling in an optimized cluster environment with high-bandwidth\nlinks between GPUs on the same server and across servers. We can\nuse similar ideas to train larger models as well, given more train-\ning resources. In our experiments, we demonstrate close to linear\nscaling to 3072 A100 GPUs, with an achieved end-to-end training\nthroughput of 163 teraFLOP/s per GPU (including communication,\ndata processing, and optimization), and an aggregate throughput\nof 502 petaFLOP/s, on a GPT model [ 11] with a trillion parame-\nters using mixed precision. This throughput facilitates practical\ntraining times: we estimate end-to-end training of this model to\ntake âˆ¼3 months. We believe this is the fastest training throughput\nachieved for this size of model: past systems [29, 40] cannot train\nsuch large models since they do not combine pipeline and tensor\nparallelism. We also compared to ZeRO [36], and found that our\napproach outperforms ZeRO-3 by 70% for models with 175 and 530\nbillion parameters due to less cross-node communication. These\nmodels are too large to fit on a multi-GPU server.\nAchieving this throughput at scale required innovation and care-\nful engineering along multiple axes: efficient kernel implementa-\ntions that allowed most of the computation to be compute-bound\nas opposed to memory-bound, smart partitioning of computation\ngraphs over the devices to reduce the number of bytes sent over net-\nwork links while also limiting device idle periods, domain-specific\ncommunication optimization, and fast hardware (state-of-the-art\nGPUs and high-bandwidth links between GPUs on the same and\ndifferent servers). We are hopeful that our open-sourced software\n(available at https://github.com/nvidia/megatron-lm) will enable\nother groups to train large NLP models efficiently at scale.\nIn addition, we studied the interaction between the various com-\nponents affecting throughput, both empirically and analytically\nwhen possible. Based on these studies, we offer the following guid-\ning principles on how to configure distributed training:\nâ€¢Different forms of parallelism interact in non-trivial ways:\nthe parallelization strategy has an impact on the amount of\ncommunication, the compute efficiency with which kernels\nare executed, as well as the idle time workers spend waiting\nfor computation due to pipeline flushes (pipeline bubbles).\nFor example, in our experiments, we found that sub-optimal\ncombinations of tensor and pipeline model parallelism can\nlead to up to 2Ã—lower throughput, even with high-bandwidth\nnetwork links between servers; tensor model parallelism\nis effective within a multi-GPU server, but pipeline model\nparallelism must be used for larger models.\nâ€¢The schedule used for pipeline parallelism has an impact\non the amount of communication, the pipeline bubble size,\nand memory used to store activations. We propose a novel\ninterleaved schedule that can improve throughput by as\nmuch as 10% compared to previously-proposed schedules [20,\n30] with comparable memory footprint.\nâ€¢Values of hyperparameters such as microbatch size have an\nimpact on the memory footprint, the arithmetic efficiency of\nkernels executed on the worker, and the pipeline bubble size.\nIn our experiments, the optimal value of the microbatch size\nis problem-dependent and can increase throughput by 15%.\nâ€¢At scale, distributed training is communication-intensive.\nWhen training a trillion-parameter model on 3072 GPUs, our\nimplementation used an effective bisection bandwidth of 892\nGB/s for pipeline-parallel communication, and 13 TB/s for\ndata-parallel communication. Using slower inter-node in-\nterconnects or more communication-intensive partitionings\nwould hinder scaling performance.\nWe should note that we do not automatically explore the search\nspace of parallelism strategies (such as FlexFlow [22], PipeDream [29],\nTarnawski et al. [41], and DAPPLE [14]), but instead suggest heuris-\ntics (in Â§3) that we found work well in practice.\n2 MODES OF PARALLELISM\nIn this section, we discuss the parallelism techniques that facilitate\nthe efficient training of large models that do not fit in the memory of\na single GPU. In this work, we combine pipeline model parallelism\nand tensor model parallelism (combination shown in Figure 2) with\ndata parallelism. We call this PTD-P for short.\n2.1 Data Parallelism\nWith data parallelism [25, 43], each worker has a copy of the full\nmodel, the input dataset is sharded, and workers aggregate their\ngradients periodically to ensure that all workers see a consistent\nversion of the weights. For large models which do not fit on a single\nworker, data parallelism can be used on smaller model shards.\n2.2 Pipeline Model Parallelism\nWith pipeline parallelism, the layers of a model are sharded across\nmultiple devices. When used on models with the same transformer\nblock repeated, each device can be assigned an equal number of\ntransformer layers. We do not consider more asymmetric model ar-\nchitectures, where assignment of layers to pipeline stages is harder;\nwe defer to related work [22, 29, 41] to solve this problem.\nA batch is split into smaller microbatches; execution is then\npipelined across microbatches. Pipelining schemes need to ensure\nthat inputs see consistent weight versions across forward and back-\nward passes for well-defined synchronous weight update semantics.\nSpecifically, naive pipelining can lead to an input seeing weight\nupdates in the backward pass not seen in the forward pass.\nTo retain strict optimizer semantics exactly, we introduce peri-\nodic pipeline flushes so that optimizer steps are synchronized across\ndevices. At the start and end of every batch, devices are idle. We\ncall this idle time the pipeline bubble , and want to make it as small\nas possible. Asynchronous and bounded-staleness approaches such\nas PipeMare, PipeDream, and PipeDream-2BW [23, 29, 30, 45] do\nEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\nPipeline MP partition #1 Pipeline MP partition #2\nTransformer layer #1 Transformer layer #2\nTensor MP partition #2\nTensor MP partition #1\nTensor MP partition #2\nTensor MP partition #1\nFigure 2: Combination of tensor and pipeline model parallelism (MP) used in this work for transformer-based models.\nTime\nBackward PassForward Pass\n1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 9 10111213141516\n1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 9 10111213141516\n1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 9 10111213141516 9\n1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 9 10111213141516 9 10\nDevices idle\nDevice 1\nDevice 2\nDevice 3\nDevice 4\nPipeline flush\nFigure 3: GPipe pipeline schedule with forward passes (blue) for all microbatches (represented by numbers) followed by backward passes\n(green). The gray area represents the pipeline bubble. For simplicity, we assume that the backward pass takes twice as long as the forward\npass. The efficiency of the pipeline schedule does not depend on this factor. Each batch in this example consists of 8 microbatches, and the\nnumbers in each blue or green box are unique identifiers given to the corresponding microbatch (in particular, the first batch consists of\nmicrobatches 1 âˆ’8, the second batch consists of microbatches 9 âˆ’16, and so on). The optimizer is stepped and weight parameters updated at\nthe pipeline flush to ensure strict optimizer semantics, leading to idle devices and a pipeline bubble.\n1 2 3 4 1 2 3 4 5 6 7 1 8 2 5 3 6 4 7 1 8 2 3 4 5 6 7 8 5 6 7 8 9 1\n0\n1\n1\n1\n2 9 1\n0\n1\n1\n1\n2\n1\n3\n1\n4\n1\n5 9 1\n6 10 1\n3 11 1\n4 12 1\n5 9 1\n6 10 11\n1 2 3 4 1 2 3 4 5 1 6 2 7 3 8 4 5 1 6 2 7 3 8 4 5 6 7 8 5 6 7 8 9 1\n0\n1\n1\n1\n2 9 1\n0\n1\n1\n1\n2\n1\n3 9 1\n4 10 1\n5 11 1\n6 12 1\n3 9 1\n4 10 1\n5 11 1\n6 12\n1 2 3 4 1 2 3 1 4 2 5 3 6 4 7 1 8 2 5 3 6 4 7 5 8 6 7 8 5 6 7 8 9 1\n0\n1\n1\n1\n2 9 1\n0\n1\n1 9 1\n2 10 1\n3 11 1\n4 12 1\n5 9 1\n6 10 1\n3 11 1\n4 12 1\n5 13\n1 2 3 4 1 1 2 2 3 3 4 4 5 1 6 2 7 3 8 4 5 5 6 6 7 7 8 8 5 6 7 8 9 1\n0\n1\n1\n1\n2 9 9 1\n0 10 1\n1 11 1\n2 12 1\n3 9 1\n4 10 1\n5 11 1\n6 12 1\n3 13 1\n4 14\n1 2 3 4 1 5 2 6 3 7 4 8 5 6 7 8 9 10 11 12 9 10\n1 2 3 4 1 2 5 3 6 4 7 5 8 6 7 8 9 10 11 12 9 10\n1 2 3 4 1 2 3 5 4 6 5 7 6 8 7 8 9 10 11 12 9 13 10 11\n1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12\nTime\nDevice 1\nDevice 2\nDevice 3\nDevice 4\nTime\nDevice 1\nDevice 2\nDevice 3\nDevice 4\nAssign multiple stages \nto each device\nBackward PassForward Pass\nFigure 4: Default and interleaved 1F1B pipeline schedules. The top figure shows the default non-interleaved 1F1B schedule. The bottom figure\nshows the interleaved 1F1B schedule, where each device is assigned multiple chunks (in this case, 2). Dark colors show the first chunk and\nlight colors show the second chunk. The size of the pipeline bubble is smaller (the pipeline flush happens sooner in the interleaved timeline).\naway with flushes completely, but relax weight update semantics.\nWe defer consideration of such schemes to future work.\nThere are several possible ways of scheduling forward and back-\nward microbatches across devices; each approach offers different\ntradeoffs between pipeline bubble size, communication, and mem-\nory footprint. We discuss two such approaches in this section.\n2.2.1 Default Schedule. GPipe [20] proposes a schedule where the\nforward passes for all microbatches in a batch are first executed,\nfollowed by backward passes for all microbatches (shown in Fig-\nure 3). We can quantify the size of GPipeâ€™s pipeline bubble ( ğ‘¡ğ‘ğ‘).\nWe denote the number of microbatches in a batch asğ‘š, the number\nof pipeline stages (number of devices used for pipeline parallelism)\nas ğ‘, the ideal time per iteration as ğ‘¡ğ‘–ğ‘‘ (assuming perfect or ideal\nscaling), and the time to execute a single microbatchâ€™s forward and\nbackward pass as ğ‘¡ğ‘“ and ğ‘¡ğ‘. In this schedule, the pipeline bubble\nconsists of ğ‘âˆ’1 forward passes at the start of a batch, and ğ‘âˆ’1\nbackward passes at the end. The total amount of time spent in the\npipeline bubble is thenğ‘¡ğ‘ğ‘ = (ğ‘âˆ’1)Â·(ğ‘¡ğ‘“ +ğ‘¡ğ‘). The ideal processing\ntime for the batch is ğ‘¡ğ‘–ğ‘‘ = ğ‘šÂ·(ğ‘¡ğ‘“ +ğ‘¡ğ‘). Therefore, the fraction of\nideal computation time spent in the pipeline bubble is:\nBubble time fraction (pipeline bubble size) =\nğ‘¡ğ‘ğ‘\nğ‘¡ğ‘–ğ‘‘\n= ğ‘âˆ’1\nğ‘š .\nFor the bubble time fraction to be small, we thus need ğ‘š â‰«ğ‘.\nHowever, for such largeğ‘š, this approach has a high memory foot-\nprint as it requires stashed intermediate activations (or just input\nactivations for each pipeline stage when using activation recompu-\ntation) to be kept in memory for all ğ‘šmicrobatches through the\nlifetime of a training iteration.\nInstead, we use the PipeDream-Flush schedule [30]. In this sched-\nule, we first enter a warm-up phase where workers perform dif-\nfering numbers of forward passes as shown in Figure 4 (top). This\nschedule limits the number of in-flight microbatches (the number of\nmicrobatches for which the backward pass is outstanding and acti-\nvations need to be maintained) to the depth of the pipeline, instead\nof the number of microbatches in a batch. After the warm-up phase,\neach worker then enters a steady state, where workers perform\none forward pass followed by one backward pass (1F1B for short).\nFinally, at the end of a batch, we complete backward passes for\nall remaining in-flight microbatches. The time spent in the bubble\nis the same for this new schedule, but the number of outstanding\nforward passes is at most the number of pipeline stages for the\nPipeDream-Flush schedule. As a result, this schedule requires acti-\nvations to be stashed for ğ‘ or fewer microbatches (compared to ğ‘š\nmicrobatches for the GPipe schedule). Consequently, when ğ‘š â‰«ğ‘,\nPipeDream-Flush is much more memory-efficient than GPipe.\n2.2.2 Schedule with Interleaved Stages. To reduce the size of the\npipeline bubble, each device can perform computation for multiple\nsubsets of layers (called a model chunk), instead of a single contigu-\nous set of layers. For example, if each device had 4 layers before\n(i.e., device 1 had layers 1 âˆ’4, device 2 had layers 5 âˆ’8, and so on),\nwe could have each device perform computation for two model\nchunks (each with 2 layers), i.e., device 1 has layers1,2,9,10; device\n2 has layers 3,4,11,12; and so on. With this scheme, each device\nin the pipeline is assigned multiple pipeline stages (each pipeline\nstage has less computation compared to before).\nAs before, we can use an â€œall-forward, all-backwardâ€ version of\nthis schedule, but this has a high memory footprint (proportional to\nğ‘š). Instead, we developed an interleaved schedule that adapts the\nmemory-efficient 1F1B schedule from before. This new schedule is\nshown in Figure 4, and requires the number of microbatches in a\nbatch to be an integer multiple of the degree of pipeline parallelism\n(number of devices in the pipeline). For example, with 4 devices,\nthe number of microbatches in a batch must be a multiple of 4.\nAs shown in Figure 4, the pipeline flush for the same batch\nsize happens sooner in the new schedule. If each device has ğ‘£\nstages (or model chunks), then the forward and backward time\nfor a microbatch for each stage or chunk will now beğ‘¡ğ‘“/ğ‘£ and ğ‘¡ğ‘/ğ‘£.\nThe pipeline bubble time thus reduces to ğ‘¡int.\nğ‘ğ‘ = (ğ‘âˆ’1)Â·(ğ‘¡ğ‘“ +ğ‘¡ğ‘ )\nğ‘£ , and\nthe bubble time fraction is then:\nBubble time fraction (pipeline bubble size) =\nğ‘¡int.\nğ‘ğ‘\nğ‘¡ğ‘–ğ‘‘\n= 1\nğ‘£ Â·ğ‘âˆ’1\nğ‘š .\nThis means that the new schedule reduces the bubble time by ğ‘£.\nThis reduced pipeline bubble size, however, does not come for free:\nthis schedule requires extra communication. Quantitatively, the\namount of communication also increases by ğ‘£. In the next section,\nwe discuss how we can utilize the 8 InfiniBand networking cards in\na multi-GPU server (e.g., a DGX A100 node) to reduce the impact\nof this extra communication.\n2.3 Tensor Model Parallelism\nWith tensor model parallelism, individual layers of the model are\npartitioned over multiple devices. In this paper, we use the particular\npartitioning strategy used by Megatron [40] for transformer layers,\nthe bedrock of language models. We can apply similar ideas to other\ntypes of models, like CNNs, as well. We briefly outline this strategy,\nillustrated in Figure 5, below.\nA transformer layer consists of a self-attention block followed\nby a two-layer multi-layer perceptron (MLP). Further details of the\ntransformer layer can be found in Vaswani et al [42].\nThe MLP block consists of two GEMMs and a GeLU non-linearity:\nğ‘Œ = GeLU(ğ‘‹ğ´). ğ‘ = Dropout(ğ‘Œğµ).\nWe can split ğ´along its columns ğ´ = [ğ´1,ğ´2]. This partitioning\nallows the GeLU non-linearity to be independently applied to the\noutput of each partitioned GEMM:\n[ğ‘Œ1,ğ‘Œ2]= [GeLU(ğ‘‹ğ´1),GeLU(ğ‘‹ğ´2)].\nThis is advantageous as it removes the need for synchronization\n(needed if ğ´is split along its rows since GeLU is non-linear).\nThe rows of the second weight matrix ğµcan then be split along\nits rows to remove the need for any communication between the\nGEMMs (shown in Figure 5a), as shown below:\nğµ =\n\u0014ğµ1\nğµ2\n\u0015\n, ğ‘Œ= [ğ‘Œ1,ğ‘Œ2].\nThe output of the second GEMM is then reduced across the GPUs\nbefore the dropout layer.\nWe exploit the inherent parallelism in the multi-head attention\noperation to partition the self-attention block (shown in Figure 5b).\nThe key (ğ¾), query (ğ‘„), and value (ğ‘‰) matrices can be partitioned in\na column-parallel fashion. The output linear layer can then directly\noperate on the partitioned output of the attention operation (weight\nmatrix partitioned across rows).\nThis approach splits GEMMs in the MLP and self-attention blocks\nacross GPUs while requiring only two all-reduce operations in the\nforward pass (ğ‘”operator) and two all-reduces in the backward pass\n(ğ‘“ operator). We implemented ğ‘“ and ğ‘”in a few lines of code.\n3 PERFORMANCE ANALYSIS OF\nPARALLELIZATION CONFIGURATIONS\nIn this section, we consider the performance implications of com-\nbining pipeline and tensor model parallelism with data parallelism.\nGiven a fixed budget of GPUs and batch size, one can use different\ndegrees of the parallelism types in PTD-P to train models; each\ndimension exposes tradeoffs between memory footprint, device\nutilization, and amount of communication.\nWe discuss these tradeoffs in the rest of this section, and then\nshow empirical results in Â§5.4. We present analytical models where\nEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\nGeLUGeLU\nDropout\nğ‘Œ = GeLU(ğ‘‹ğ´) ğ‘ = Dropout(ğ‘Œğµ)\nğ´ = [ğ´!, ğ´\"] ğµ = ğµ!\nğµ\"\nğ‘Œ!\nğ‘Œ\"\nğ‘‹ğ´!\nğ‘‹ğ´\"\nğ‘‹\nğ‘‹\nğ‘“ğ‘‹\nğ‘Œ!ğµ!\nğ‘Œ\"ğµ\"\nğ‘” ğ‘\nğ‘!\nğ‘\"\n(a) MLP.\nDropout\nSoftmax\nDropout\nSoftmax\nDropout\nğµ = ğµ!\nğµ\"\nğ‘ = Dropout(ğ‘Œğµ)\nğ‘Œ!ğµ!\nğ‘Œ\"ğµ\"\nğ‘!\nğ‘\"\nğ‘\nğ‘Œ = Self-Attention(ğ‘‹)\nSplit attention heads\tâ†’ &\nğ‘„ = [ğ‘„!, ğ‘„\"]\nğ¾ = [ğ¾!, ğ¾\"]\nğ‘‰ = [ğ‘‰!,ğ‘‰\"]\nğ‘”ğ‘“ğ‘‹\nğ‘‹\nğ‘‹\nğ‘Œ\"\nğ‘Œ!\nğ‘‰!\nğ‘„!\nğ¾!\nğ¾\"\nğ‘„\"\nğ‘‰\"\n(b) Self-Attention.\nFigure 5: Blocks of transformer model partitioned with tensor\nmodel parallelism (figures borrowed from Megatron [40]). ğ‘“ and ğ‘”\nare conjugate. ğ‘“ is the identity operator in the forward pass and all-\nreduce in the backward pass, while ğ‘”is the reverse.\nrelevant for the pipeline bubble size. We qualitatively describe how\ncommunication time behaves and present cost models for amount\nof communication; however, we do not present direct cost models\nfor communication time, which is harder to model for a hierarchical\nnetwork topology where interconnects between GPUs on the same\nserver have higher bandwidth than interconnects between servers.\nTo the best of our knowledge, this is the first work to analyze the\nperformance interactions of these parallelization dimensions.\n3.1 Notation\nWe use the following notation in this section:\nâ€¢ (ğ‘,ğ‘¡,ğ‘‘ ): Parallelization dimensions.ğ‘for the pipeline-model-\nparallel size, ğ‘¡ for the tensor-model-parallel size, and ğ‘‘ for\nthe data-parallel size.\nâ€¢ğ‘›: Number of GPUs. We require ğ‘Â·ğ‘¡ Â·ğ‘‘ = ğ‘›.\nâ€¢ğµ: Global batch size (provided as input).\nâ€¢ğ‘: Microbatch size.\nâ€¢ğ‘š= 1\nğ‘ Â·ğµ\nğ‘‘: Number of microbatches in a batch per pipeline .\n3.2 Tensor and Pipeline Model Parallelism\nTensor and pipeline model parallelism can both be used to partition\na modelâ€™s parameters over multiple GPUs. As stated earlier, using\npipeline parallelism with periodic flushes results in a pipeline bubble\nof size (ğ‘ âˆ’1)/ğ‘š. Let us assume that ğ‘‘ = 1 (data-parallel size);\nconsequently, ğ‘¡ Â·ğ‘ = ğ‘›. The pipeline bubble size in terms of ğ‘¡ is:\nğ‘âˆ’1\nğ‘š = ğ‘›/ğ‘¡âˆ’1\nğ‘š .\nAs ğ‘¡ increases, the pipeline bubble thus decreases for fixedğµ, ğ‘, and\nğ‘‘ (ğ‘š= ğµ/(ğ‘Â·ğ‘‘)is fixed as well).\n/uni00000015/uni00000016/uni00000018/uni0000001c/uni00000015/uni0000001a/uni00000017/uni00000016/uni0000001a/uni00000018\n/uni00000028/uni00000045/uni00000058/uni00000045/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni0000000c/uni00000048/uni0000000d\n/uni00000014/uni00000012/uni00000014/uni00000014\n/uni00000014/uni00000012/uni00000016/uni00000019\n/uni00000014/uni00000012/uni00000019/uni00000014\n/uni00000014/uni00000012/uni0000001b/uni00000019\n/uni00000015/uni00000012/uni00000014/uni00000014/uni00000034/uni0000004d/uni00000054/uni00000049/uni00000050/uni0000004d/uni00000052/uni00000049/uni00000004/uni00000046/uni00000059/uni00000046/uni00000046/uni00000050/uni00000049/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000052/uni00000021/uni00000017/uni00000016/uni00000010/uni00000004/uni00000046/uni0000000b/uni00000021/uni00000017/uni00000016\n/uni00000052/uni00000021/uni00000017/uni00000016/uni00000010/uni00000004/uni00000046/uni0000000b/uni00000021/uni00000015/uni00000016/uni0000001c\n/uni00000052/uni00000021/uni00000015/uni00000016/uni0000001c/uni00000010/uni00000004/uni00000046/uni0000000b/uni00000021/uni00000015/uni00000016/uni0000001c\n/uni00000052/uni00000021/uni00000015/uni00000016/uni0000001c/uni00000010/uni00000004/uni00000046/uni0000000b/uni00000021/uni00000019/uni00000015/uni00000016\nFigure 6: Fraction of time spent idling due to pipeline flush (pipeline\nbubble size) versus data-parallel size ( ğ‘‘), for different numbers of\nGPUs (ğ‘›) and ratio of batch size to microbatch size ( ğ‘â€²= ğµ/ğ‘).\nThe amount of communication performed between different\nGPUs is also affected by the values of ğ‘ and ğ‘¡. Pipeline model par-\nallelism features cheaper point-to-point communication. Tensor\nmodel parallelism, on the other hand, uses all-reduce communi-\ncation (two all-reduce operations each in the forward and back-\nward pass, see Â§2.3). With pipeline parallelism, the total amount\nof communication that needs to be performed between every pair\nof consecutive devices (for either the forward or backward pass)\nfor each microbatch is ğ‘ğ‘ â„, where ğ‘  is the sequence length and â„\nis the hidden size. With tensor model parallelism, tensors of total\nsize ğ‘ğ‘ â„ need to be all-reduced among ğ‘¡ model replicas twice each\nin the forward and backward pass for each layer, leading to a total\ncommunication of 8ğ‘ğ‘ â„\n\u0010\nğ‘¡âˆ’1\nğ‘¡\n\u0011\nper layer per device for each micro-\nbatch. Each device typically has multiple layers; the total amount\nof tensor-parallel-communication per device for each microbatch\nis then ğ‘™stage Â·\n\u0010\n8ğ‘ğ‘ â„\n\u0010\nğ‘¡âˆ’1\nğ‘¡\n\u0011\u0011\n, where ğ‘™stage is the number of layers in\na pipeline stage.\nConsequently, we see that tensor model parallelism increases\nthe amount of communication between devices. Thus, when ğ‘¡ is\nlarger than the number of GPUs in a single node, the overhead of\nperforming tensor model parallelism across slower inter-node links\ncan be impractical. We see these results empirically in Â§5.4.\nTakeaway #1: When considering different forms of model par-\nallelism, tensor model parallelism should generally be used up\nto degree ğ‘”when using ğ‘”-GPU servers, and then pipeline model\nparallelism can be used to scale up to larger models across servers.\n3.3 Data and Model Parallelism\nWe also want to consider the interaction between data parallelism\nand the two types of model parallelism. In this section, we consider\nthese interactions independently for simplicity.\n3.3.1 Pipeline Model Parallelism. Let ğ‘¡ = 1 (tensor-model-parallel\nsize). The number of microbatches per pipeline is ğ‘š= ğµ/(ğ‘‘Â·ğ‘)=\nğ‘â€²/ğ‘‘, where ğ‘â€²:= ğµ/ğ‘. With total number of GPUs ğ‘›, the number\nof pipeline stages is ğ‘ = ğ‘›/(ğ‘¡Â·ğ‘‘)= ğ‘›/ğ‘‘. The pipeline bubble size is:\nğ‘âˆ’1\nğ‘š = ğ‘›/ğ‘‘âˆ’1\nğ‘â€²/ğ‘‘ = ğ‘›âˆ’ğ‘‘\nğ‘â€² .\n/uni00000015/uni00000016/uni00000018/uni0000001c/uni00000015/uni0000001a\n/uni00000031/uni0000004d/uni00000047/uni00000056/uni00000053/uni00000046/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000014\n/uni00000016/uni00000019\n/uni00000019/uni00000014\n/uni0000001b/uni00000019\n/uni00000015/uni00000014/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\nFigure 7: Per-GPU throughput versus microbatch size for a GPT\nmodel with a billion parameters (128 attention heads, hidden size\nof 4096, 4 transformer layers).\nAs ğ‘‘ becomes larger, ğ‘›âˆ’ğ‘‘ becomes smaller, and thus the pipeline\nbubble becomes smaller. Figure 6 shows the behavior of the pipeline\nbubble size for various values of ğ‘‘,ğ‘›, and ğ‘â€². It might not be pos-\nsible to increase ğ‘‘ all the way to ğ‘›for all models, since a modelâ€™s\nfull training memory footprint might be larger than the memory\ncapacity of a single accelerator.\nOverall throughput will thus increase if the all-reduce commu-\nnication needed for data parallelism does not drastically increase\nwith higher ğ‘‘, which should hold since the communication time\nfor a ring-based implementation scales with ğ‘‘âˆ’1\nğ‘‘ = 1 âˆ’1\nğ‘‘.\nWe can also analyze the impact of increasing the batch size ğµ.\nFor a given parallel configuration, as the batch size ğµ increases,\nğ‘â€²= ğµ/ğ‘increases, (ğ‘›âˆ’ğ‘‘)/ğ‘â€²decreases, consequently increasing\nthroughput. All-reduce communication required by data parallelism\nalso becomes more infrequent, further increasing throughput.\n3.3.2 Data and Tensor Model Parallelism.With tensor model paral-\nlelism, all-reduce communication needs to be performed for every\nmicrobatch. This can be expensive across multi-GPU servers. On\nthe other hand, data parallelism only needs to perform expensive\nall-reduce communication once per batch . Moreover, with tensor\nmodel parallelism, each model-parallel rank performs a subset of\nthe computation in each model layer, and thus for insufficiently-\nlarge layers, modern GPUs might not perform these sub-matrix\ncomputations with peak efficiency.\nTakeaway #2: When using data and model parallelism, a total\nmodel-parallel size of ğ‘€ = ğ‘¡Â·ğ‘should be used so that the modelâ€™s\nparameters and intermediate metadata fit in GPU memory; data\nparallelism can be used to scale up training to more GPUs.\n3.4 Microbatch Size\nThe choice of the microbatch size ğ‘ also affects model-training\nthroughput. For example, we see in Figure 7 that per-GPU through-\nput increases by up to1.3Ã—with a larger microbatch size on a single\nGPU. We now want to determine the optimal microbatch size ğ‘\ngiven a parallel configuration (ğ‘,ğ‘¡,ğ‘‘ )and batch size ğµ. The amount\nof data-parallel communication will be the same regardless of the\nmicrobatch size. Given functions ğ‘¡ğ‘“ (ğ‘)and ğ‘¡ğ‘(ğ‘)that map the mi-\ncrobatch size to the forward and backward computation times for a\nsingle microbatch, the total time spent computing a batch, ignoring\ncommunication cost, is (as before, define ğ‘â€²as ğµ/ğ‘‘):\n\u0000ğ‘â€²/ğ‘+ğ‘âˆ’1\u0001 Â·\n\u0010\nğ‘¡ğ‘“ (ğ‘)+ğ‘¡ğ‘(ğ‘)\n\u0011\n. (1)\n/uni00000015/uni00000016/uni00000018/uni0000001c/uni00000015/uni0000001a\n/uni00000031/uni0000004d/uni00000047/uni00000056/uni00000053/uni00000046/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000014/uni00000012/uni00000014/uni00000014\n/uni00000014/uni00000012/uni00000016/uni00000019\n/uni00000014/uni00000012/uni00000019/uni00000014\n/uni00000014/uni00000012/uni0000001b/uni00000019\n/uni00000015/uni00000012/uni00000014/uni00000014\n/uni00000015/uni00000012/uni00000016/uni00000019/uni00000032/uni00000053/uni00000056/uni00000051/uni00000045/uni00000050/uni0000004d/uni0000005e/uni00000049/uni00000048/uni00000004/uni00000058/uni0000004c/uni00000056/uni00000053/uni00000059/uni0000004b/uni0000004c/uni00000054/uni00000059/uni00000058\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000015/uni00000016/uni0000001c\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000019/uni00000015/uni00000016\nFigure 8: Behavior of normalized estimated throughput (time com-\nputed as ğ‘¡ = (ğ‘â€²/ğ‘+ğ‘âˆ’1)Â·\u0000ğ‘¡ğ‘“ (ğ‘)+ğ‘¡ğ‘ (ğ‘)\u0001) with respect to the mi-\ncrobatch size ğ‘for the same GPT model from Figure 7.\nThe microbatch size thus affects both the arithmetic intensity of\noperations as well as the pipeline bubble size (by affecting ğ‘š). Fig-\nure 8 shows estimated throughput (equation (1) used to estimate\nprocessing time) for a GPT model with a billion parameters and\n(ğ‘,ğ‘¡)= (8,8). The optimal ğ‘for both batch sizes is 4.\nTakeaway #3: The optimal microbatch size ğ‘ depends on the\nthroughput and memory footprint characteristics of the model, as\nwell as the pipeline depth ğ‘, data-parallel size ğ‘‘, and batch size ğµ.\n3.5 Activation Recomputation\nActivation recomputation [12, 18, 20, 21] is an optional technique\nthat trades off an increase in the number of compute operations per-\nformed for additional memory footprint, by running the forward\npass a second time just before the backward pass (and stashing\nonly the input activations for a given pipeline stage, as opposed to\nthe entire set of intermediate activations, which is much larger).\nActivation recomputation is required to train reasonably large mod-\nels with pipeline parallelism to keep memory footprint acceptably\nlow. Previous work like PipeDream-2BW [ 30] has looked at the\nperformance ramifications of activation recomputation.\nThe number of activation checkpoints does not impact through-\nput, but impacts memory footprint. Let ğ´input be the size of the\ninput activations of a layer, and ğ´intermediate be the size of interme-\ndiate activations per layer. If a model stage has ğ‘™ layers, and if ğ‘ is\nthe number of checkpoints, the total memory footprint is going to\nbe ğ‘Â·ğ´input +ğ‘™/ğ‘Â·ğ´intermediate. The minimum value of this function\nis obtained when ğ‘ =\nâˆšï¸ƒ\nğ‘™ Â·\u0000ğ´intermediate/ğ´input\u0001. In practice, we\nmeasure ğ´intermediate empirically. For most cases, checkpointing\nevery 1 or 2 transformer layers is optimal.\nOther techniques such as activation partitioning [36] can also\nbe used in conjunction with tensor model parallelsim to reduce the\nmemory footprint due to activations further.\n4 IMPLEMENTATION\nWe implemented PTD-P as an extension to the Megatron-LM code-\nbase. Our implementation is built using PyTorch [ 32]. We use\nNCCL [7] for communication between devices. To obtain good\nperformance, we implemented optimizations targeting both com-\nmunication and computation, which we outline below.\nEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\nNVLink\nInfiniband\n1\n2\n3\n4\n(a) W/o scatter/gather optimization.\n1\n2\n3\n4\nScatter of All-gather of (b) With scatter/gather optimization.\nFigure 9: Scatter/gather communication optimization. Light blue\nblocks are layers in the first pipeline stage, and dark blue blocks\nare layers in the second pipeline stage. Without the scatter/gather\noptimization, the same tensor is sent redundantly over inter-node\nInfiniBand links. Instead, at the sender, we can scatter the tensor\ninto smaller chunks, reducing the sizes of tensors sent over Infini-\nBand links. The final tensor can then be rematerialized at the re-\nceiver using a gather operation.\n4.1 Communication Optimizations\nWhen using pipeline parallelism, we want to send and receive ten-\nsors in the forward and backward direction in parallel. Each DGX\nA100 is equipped with 8 InfiniBand (IB) networking cards. Unfor-\ntunately, sends and receives are point-to-point, and only happen\nbetween a pair of GPUs on two servers, making it hard to leverage\nall 8 cards for a single communication call within the pipeline.\nHowever, we can leverage the fact that we use both tensor model\nparallelism and pipeline model parallelism to reduce the overhead\nof cross-node communication. In particular, we note that the output\nof each transformer layer is replicated (after ğ‘”in MLP block, see\nFigure 5a) across the tensor-parallel ranks. As a result, ranks in two\nconsecutive pipeline stages that are performing tensor model par-\nallelism send and receive the exact same set of tensors (Figure 9a).\nFor large enough models, we use a tensor-model-parallel size\nof 8. This means we are sending the same set of tensors 8 times\nbetween corresponding GPUs on adjacent multi-GPU servers. To\nreduce this redundancy, we can instead split the tensor on the send\nside into equal-sized chunks, and then only send one chunk to\nthe corresponding rank on the next node using the rankâ€™s own\nInfiniBand card (e.g., rank 1 sends to rank 3 and rank 2 sends to\nrank 4 in Figure 9). With 8 tensor-model-parallel ranks, each chunk\nwould be one-eighth smaller. Then, on the receive side, we can\nperform an all-gather over NVLink, which is much faster than the\nInfiniBand interconnect, to re-materialize the full tensor. This is\nshown in Figure 9b. We call this the scatter/gather communication\noptimization. This optimization helps better leverage the multiple\nIB cards on the DGX A100 servers, and makes more communication-\nintensive schedules such as the interleaved one feasible.\nQuantitatively, with the scatter-gather communication optimiza-\ntion, the total amount of communication that needs to be performed\nbetween every pair of consecutive stages is reduced to ğ‘ğ‘ â„\nğ‘¡ , where\nğ‘¡ is the tensor-model-parallel size, ğ‘  is the sequence length, and â„\nis the hidden size (ğ‘¡ = 8 in our experiments).\n4.2 Computation Optimizations\nWe implemented three model-specific optimizations to the compu-\ntation graph to attain high performance. First, we changed the data\nlayout in the transformer layer to avoid memory-intensive trans-\npose operations, and to enable the use of strided batched GEMM\nkernels. Specifically, we changed the data layout from [ğ‘,ğ‘ ,ğ‘,â„ ]to\n[ğ‘ ,ğ‘,ğ‘,â„ ], where ğ‘, ğ‘ , ğ‘, and â„are batch, sequence, attention-head,\nand hidden-size dimensions, respectively. Second, we generated\nfused kernels for a sequence of element-wise operations (bias +\nGeLU and bias + dropout + add) using PyTorch JIT [10]. Third, we\ncreated two custom kernels to enable the fusion of scale, mask, and\nsoftmax (reduction) operations: one to support general masking\n(used in models such as BERT) and another to support implicit\ncausal masking (used in auto-regressive models such as GPT). We\nquantify the effect of these optimizations in the next section.\n5 EVALUATION\nIn this section, we seek to answer the following questions:\nâ€¢How well does PTD-P perform? Does it result in realistic\nend-to-end training times?\nâ€¢How well does pipeline parallelism scale for a given model\nand batch size? How much impact does the interleaved sched-\nule have on performance?\nâ€¢How do different parallelization dimensions interact with\neach other? What is the impact of hyperparameters such as\nmicrobatch size?\nâ€¢What is the impact of the scatter-gather communication\noptimization? What types of limits do we put on hardware\nwhen running training iterations at scale?\nAll of our results are run with mixed precision on the Selene\nsupercomputer [8]. Each cluster node has 8 NVIDIA 80-GB A100\nGPUs [6], connected to each other by NVLink and NVSwitch [9].\nEach node has eight NVIDIA Mellanox 200Gbps HDR Infiniband\nHCAs for application communication, with an additional two HCAs\nper node for dedicated storage. The nodes are connected in a three-\nlevel (leaf, spine, core) fat-tree topology with 850 switches. This\ntopology allows efficient all-reduce communication (dominant com-\nmunication pattern in deep learning training). The cluster uses an\nall-NVME shared parallel filesystem for high-performance data ac-\ncess and storage. The peak device throughput of an A100 GPU with\n16-bit precision is 312 teraFLOP/s. For most of our results, we report\nthroughput per GPU. Aggregate throughput can be computed by\nmultiplying with the number of GPUs used.\nFor our experiments, we use GPT models of appropriate sizes. In\nparticular, for any given microbenchmark, the model needs to fit on\nthe number of model-parallel GPUs used in the experiment. We use\nstandard model architectures such as GPT-3 [11] when appropriate.\n5.1 End-to-End Performance\nWe consider the end-to-end performance of our system on GPT\nmodels ranging from a billion to a trillion parameters, using ten-\nsor, pipeline, and data parallelism (degrees picked using heuristics\ndescribed in Â§3). In particular, we use the interleaved pipeline sched-\nule with the scatter/gather optimization enabled. All models use a\nvocabulary size (denoted by ğ‘‰) of 51,200 (multiple of 1024) and a\nsequence length (ğ‘ ) of 2048. We vary hidden size (â„), number of at-\ntention heads, and number of layers (ğ‘™). The number of parameters\nin a model, ğ‘ƒ, can be computed as:\nğ‘ƒ = 12ğ‘™â„2\n\u0012\n1 + 13\n12â„ +ğ‘‰ +ğ‘ \n12ğ‘™â„\n\u0013\n. (2)\nNumber of parameters (billion)\nAttention headsHidden size Number of layersTensor model-parallel sizePipeline model-parallel sizeNumber of GPUsBatch size\nAchieved teraFlOP/s per GPU\nPercentage of theoretical peak FLOP/s\nAchieved aggregate petaFLOP/s\n1.7 24 2304 24 1 1 32 512 137 44% 4.4\n3.6 32 3072 30 2 1 64 512 138 44% 8.8\n7.5 32 4096 36 4 1 128 512 142 46% 18.2\n18.4 48 6144 40 8 1 256 1024 135 43% 34.6\n39.1 64 8192 48 8 2 512 1536 138 44% 70.8\n76.1 80 10240 60 8 4 1024 1792 140 45% 143.8\n145.6 96 12288 80 8 8 1536 2304 148 47% 227.1\n310.1 128 16384 96 8 16 1920 2160 155 50% 297.4\n529.6 128 20480 105 8 35 2520 2520 163 52% 410.2\n1008.0 160 25600 128 8 64 3072 3072 163 52% 502.0\nTable 1: Weak-scaling throughput for GPT models ranging from 1 billion to 1 trillion parameters.\nAs the model size increases, we also increase the batch size (ğµ) and\nthe number of GPUs (ğ‘›). The majority of floating-point operations\nin the model are performed in the matrix multiplications (GEMMs)\nin the transformer and logit layers. Considering just these GEMMs,\nthe number of FLOPs per iteration is (more details in the Appendix):\nğ¹ = 96ğµğ‘ ğ‘™â„2\n\u0012\n1 + ğ‘ \n6â„ + ğ‘‰\n16ğ‘™â„\n\u0013\n. (3)\nThis is a lower bound for the true FLOP count but should be close\nto the actual value. We count a FLOP as a floating-point operation\nregardless of precision. We also note that equation (3) assumes\nactivation recomputation and takes into account the floating-point\noperations associated with the extra forward pass.\nTable 1 shows the model configurations along with the achieved\nFLOP/s (both per GPU and aggregate over all GPUs). We see super-\nlinear scaling to 3072 A100 GPUs (384 DGX A100 nodes), since\nGPU utilization improves as the models get larger (larger matrix\nmultiplications) without significant increase in the communication\ntime relative to computation time. Note that throughput is measured\nfor end-to-end training, i.e., includes all operations including data\nloading, optimizer steps, communication, and logging. We achieve\n52% of peak device throughput for the largest model, and 44% of\npeak device throughput for the smallest model.\nTraining Time Estimates. Given these throughputs, we can\nalso estimate the total amount of time needed for end-to-end train-\ning on ğ‘‡ tokens. Training requires ğ¼ = ğ‘‡/(ğµÂ·ğ‘ )iterations. Using\nthe value of ğ¹ from equation (3) and empirical end-to-end through-\nputs from Table 1 (denoted by ğ‘‹), we can estimate total training\ntime. We note that for the configurations in Table 1, we have6â„ â‰«ğ‘ ,\n16ğ‘™â„ â‰«(ğ‘‰ +ğ‘ ), and 12ğ‘™â„ â‰«ğ‘‰. Combining these observations with\nequations (2) and (3), we arrive at\nEnd-to-end training time â‰ˆ8ğ‘‡ğ‘ƒ\nğ‘›ğ‘‹ . (4)\nLet us consider the GPT-3 model with ğ‘ƒ =175 billion parameters as\nan example. This model was trained on ğ‘‡ = 300 billion tokens. On\nğ‘›= 1024 A100 GPUs using batch size 1536, we achieve ğ‘‹ = 140 ter-\naFLOP/s per GPU. As a result, the time required to train this model\nis 34 days. For the 1 trillion parameter model, we assume that 450\nbillion tokens are needed for end-to-end training. With 3072 A100\nGPUs, we can achieve a per-GPU throughput of 163 teraFLOP/s,\nand end-to-end training time of 84 days. We believe these training\ntimes (using a reasonable number of GPUs) are practical.\n/uni0000001b/uni0000001a/uni0000001c/uni00000015/uni00000015/uni00000019/uni00000016/uni00000015/uni00000019/uni00000017/uni0000001a/uni00000015/uni0000001d/uni00000016/uni00000014\n/uni00000032/uni00000059/uni00000051/uni00000046/uni00000049/uni00000056/uni00000004/uni00000053/uni0000004a/uni00000004/uni0000002b/uni00000034/uni00000039/uni00000057\n/uni00000014\n/uni00000019/uni00000014\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000019/uni00000014\n/uni00000016/uni00000014/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni0000003e/uni00000049/uni00000036/uni00000033/uni00000011/uni00000017/uni00000010/uni00000004/uni00000015/uni0000001b/uni00000019/uni00000026\n/uni0000003e/uni00000049/uni00000036/uni00000033/uni00000011/uni00000017/uni00000010/uni00000004/uni00000019/uni00000017/uni00000014/uni00000026\n/uni00000034/uni00000038/uni00000028/uni00000011/uni00000034/uni00000010/uni00000004/uni00000015/uni0000001b/uni00000019/uni00000026\n/uni00000034/uni00000038/uni00000028/uni00000011/uni00000034/uni00000010/uni00000004/uni00000019/uni00000017/uni00000014/uni00000026\nFigure 10: Throughput per GPU of PTD-P and ZeRO-3 for two differ-\nent GPT models (the 175B GPT-3 model is shown with dotted lines,\nand the 530B model is shown with solid lines). Global batch sizes\nare fixed and ZeRO-3 is used without any model parallelism.\n5.2 Comparison to ZeRO-3\nWe compare PTD-P to ZeRO-3 [36, 37] in Table 2 and Figure 10 for\nthe standard GPT-3 model architecture, as well as the 530-billion-\nparameter model from Table 1. The results provide a point of com-\nparison to a method that does not use model parallelism. We in-\ntegrated ZeRO into our codebase using the DeepSpeed Python\nlibrary [3]. We keep the global batch size the same as we increase\nthe number of GPUs. With fewer GPUs and a microbatch size of 4,\nPTD-P results in 6% and 24% higher throughput for the 175- and\n530-billion-parameter models respectively. As we increase the num-\nber of GPUs, PTD-P scales more gracefully than ZeRO-3 in isolation\n(see Figure 10). For example, by doubling the number of GPUs (keep-\ning the batch size the same), PTD-P outperforms ZeRO-3 by 70%\nfor both models due to less cross-node communication. We note\nthat we have only considered ZeRO-3 without tensor parallelism.\nZeRO-3 can be combined with model parallelism to potentially\nimprove its scaling behavior.\n5.3 Pipeline Parallelism\nWe now evaluate the weak-scaling performance of pipeline paral-\nlelism in isolation, and also compare the performance of the non-\ninterleaved schedule to the interleaved schedule.\n5.3.1 Weak Scaling. We evaluate the scaling of the default non-\ninterleaved pipeline-parallel schedule using a weak scaling setup,\na GPT model with 128 attention heads and a hidden size of 20480,\nEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\nScheme\nNumber of \nparameters \n(billion)\nModel- \nparallel \nsize\nBatch \nsize\nNumber \nof GPUs\nMicrobatch \nsize\nAchieved \nteraFlOP/s \nper GPU\nTraining time \nfor 300B \ntokens (days)\nZeRO-3 \nwithout \nModel \nParallelism\n174.6 1 1536\n384 4 144 90\n768 2 88 74\n1536 1 44 74\n529.6 1\n2560* 640 4 138 169\n2240 1120 2 98 137\n2240 1 48 140\nPTD \nParallelism\n174.6 96 1536\n384 1 153 84\n768 1 149 43\n1536 1 141 23\n529.6 280 2240\n560 1 171 156\n1120 1 167 80\n2240 1 159 42\nTable 2: Comparison of PTD Parallelism to ZeRO-3 (without model paralllelism). The 530-billion-parameter GPT model did not fit on 560\nGPUs when using a microbatch size of 4 with ZeRO-3, so we increased the number of GPUs used to 640 and global batch size to 2560 to provide\na throughput estimate (relevant row marked in table with a *).\n/uni00000015 /uni00000016 /uni00000018 /uni0000001c\n/uni00000034/uni0000004d/uni00000054/uni00000049/uni00000050/uni0000004d/uni00000052/uni00000049/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000014\n/uni00000019/uni00000014\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000019/uni00000014\n/uni00000016/uni00000014/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni0000001c\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000015/uni00000016/uni0000001c\nFigure 11: Throughput per GPU of pipeline parallelism using two\ndifferent batch sizes in a weak-scaling experiment setup (model size\nincreases with the pipeline-parallel size).\n/uni00000015/uni00000016/uni00000016/uni00000018/uni00000017/uni0000001a/uni00000018/uni0000001c/uni0000001a/uni00000014\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000019/uni00000014\n/uni0000001b/uni00000019\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000016/uni00000019\n/uni00000015/uni00000019/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni00000032/uni00000053/uni00000052/uni00000011/uni0000004d/uni00000052/uni00000058/uni00000049/uni00000056/uni00000050/uni00000049/uni00000045/uni0000005a/uni00000049/uni00000048\n/uni0000002d/uni00000052/uni00000058/uni00000049/uni00000056/uni00000050/uni00000049/uni00000045/uni0000005a/uni00000049/uni00000048\nFigure 12: Throughput per GPU of interleaved and non-interleaved\nschedules for a GPT model (175 billion parameters) on 96 GPUs.\nand a microbatch size of 1. As we increase the number of pipeline\nstages, we also increase the size of the model by proportionally\nincreasing the number of layers in the model, e.g., with a pipeline-\nparallel size of 1, we use a model with 3 transformer layers and 15\nbillion parameters, and with a pipeline-parallel size of 8, we use a\nmodel with 24 transformer layers and 121 billion parameters. We\nuse a tensor-parallel size of 8 for all configurations, and vary the\ntotal number of A100 GPUs used from 8 to 64. Figure 11 shows\nthroughput per GPU for two different batch sizes to illustrate the\nimpact of the pipeline bubble, which behaves as ğ‘âˆ’1\nğ‘š (Â§2.2.1). As\nexpected, the higher batch size scales better since the pipeline\nbubble is amortized over more microbatches.\n/uni0000000c/uni00000016/uni00000010/uni00000004/uni00000017/uni00000016/uni0000000d/uni0000000c/uni00000018/uni00000010/uni00000004/uni00000015/uni0000001a/uni0000000d/uni0000000c/uni0000001c/uni00000010/uni00000004/uni0000001c/uni0000000d/uni0000000c/uni00000015/uni0000001a/uni00000010/uni00000004/uni00000018/uni0000000d/uni0000000c/uni00000017/uni00000016/uni00000010/uni00000004/uni00000016/uni0000000d\n/uni0000000c/uni00000034/uni0000004d/uni00000054/uni00000049/uni00000050/uni0000004d/uni00000052/uni00000049/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000010/uni00000004/uni00000038/uni00000049/uni00000052/uni00000057/uni00000053/uni00000056/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni0000000d\n/uni00000014\n/uni00000019/uni00000014\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000019/uni00000014\n/uni00000016/uni00000014/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000017/uni00000016\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000015/uni00000016/uni0000001c\nFigure 13: Throughput per GPU of various parallel configurations\nthat combine pipeline and tensor model parallelism using a GPT\nmodel with 162.2 billion parameters and 64 A100 GPUs.\n5.3.2 Interleaved versus Non-Interleaved Schedule. Figure 12 shows\nthe per-GPU-throughput for interleaved and non-interleaved sched-\nules on the GPT-3 [ 11] model with 175 billion parameters (96\nlayers, 96 attention heads, hidden size of 12288). The interleaved\nschedule with the scatter/gather communication optimization has\nhigher computational performance than the non-interleaved (de-\nfault) schedule. This gap closes as the batch size increases due to\ntwo reasons: (a) as the batch size increases, the bubble size in the\ndefault schedule decreases, and (b) the amount of point-to-point\ncommunication within the pipeline is proportional to the batch size,\nand consequently the non-interleaved schedule catches up as the\namount of communication increases (the interleaved schedule fea-\ntures more communication per sample). Without the scatter/gather\noptimization, the default schedule performs better than the inter-\nleaved schedule at larger batch sizes (not shown).\n5.4 Comparison of Parallel Configurations\nIn this sub-section, we show the various tradeoffs associated with\ncombining different parallelization dimensions. In particular, we\nshow the performance for parallel configurations using the same\nnumber of GPUs for a given model and multiple batch sizes.\n5.4.1 Tensor versus Pipeline Parallelism.We evaluate the impact of\npipeline and tensor model parallelism on performance for a given\nmodel and batch size. The empirical results in Figure 13 show the\n/uni0000000c/uni00000016/uni00000010/uni00000004/uni00000017/uni00000016/uni0000000d/uni0000000c/uni00000018/uni00000010/uni00000004/uni00000015/uni0000001a/uni0000000d/uni0000000c/uni0000001c/uni00000010/uni00000004/uni0000001c/uni0000000d/uni0000000c/uni00000015/uni0000001a/uni00000010/uni00000004/uni00000018/uni0000000d/uni0000000c/uni00000017/uni00000016/uni00000010/uni00000004/uni00000016/uni0000000d\n/uni0000000c/uni00000034/uni0000004d/uni00000054/uni00000049/uni00000050/uni0000004d/uni00000052/uni00000049/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000010/uni00000004/uni00000028/uni00000045/uni00000058/uni00000045/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni0000000d\n/uni00000014\n/uni00000019/uni00000014\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000019/uni00000014\n/uni00000016/uni00000014/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000017/uni00000016\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000019/uni00000015/uni00000016\nFigure 14: Throughput per GPU of various parallel configurations\nthat combine data and pipeline model parallelism using a GPT\nmodel with 5.9 billion parameters, three different batch sizes, mi-\ncrobatch size of 1, and 64 A100 GPUs.\n/uni0000000c/uni00000016/uni00000010/uni00000004/uni00000017/uni00000016/uni0000000d/uni0000000c/uni00000018/uni00000010/uni00000004/uni00000015/uni0000001a/uni0000000d/uni0000000c/uni0000001c/uni00000010/uni00000004/uni0000001c/uni0000000d/uni0000000c/uni00000015/uni0000001a/uni00000010/uni00000004/uni00000018/uni0000000d/uni0000000c/uni00000017/uni00000016/uni00000010/uni00000004/uni00000016/uni0000000d\n/uni0000000c/uni00000038/uni00000049/uni00000052/uni00000057/uni00000053/uni00000056/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000010/uni00000004/uni00000028/uni00000045/uni00000058/uni00000045/uni00000011/uni00000054/uni00000045/uni00000056/uni00000045/uni00000050/uni00000050/uni00000049/uni00000050/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni0000000d\n/uni00000014\n/uni00000019/uni00000014\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000019/uni00000014\n/uni00000016/uni00000014/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000017/uni00000016\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000015/uni00000016/uni0000001c\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000019/uni00000015/uni00000016\nFigure 15: Throughput per GPU of various parallel configurations\nthat combine data and tensor model parallelism using a GPT model\nwith 5.9 billion parameters, three different batch sizes, microbatch\nsize of 1, and 64 A100 GPUs.\n/uni00000015 /uni00000016 /uni00000018 /uni0000001c\n/uni00000031/uni0000004d/uni00000047/uni00000056/uni00000053/uni00000046/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000014\n/uni00000019/uni00000014\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000019/uni00000014\n/uni00000016/uni00000014/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000015/uni00000016/uni0000001c\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000004/uni00000021/uni00000004/uni00000019/uni00000015/uni00000016\nFigure 16: Throughput per GPU of a (ğ‘¡,ğ‘)= (8,8)parallel configura-\ntion for different microbatch sizes on a GPT model with 91 billion\nparameters, for two different batch sizes using 64 A100 GPUs.\nimportance of using both tensor and pipeline model parallelism in\nconjunction to train a 161-billion-parameter GPT model (32 trans-\nformer layers to support pipeline-parallel size of 32, 128 attention\nheads, hidden size of 20480) with low communication overhead and\nhigh compute resource utilization. We observe that tensor model\nparallelism is best within a node (DGX A100 server) due to its expen-\nsive all-reduce communication. Pipeline model parallelism, on the\nother hand, uses much cheaper point-to-point communication that\ncan be performed across nodes without bottlenecking the entire\ncomputation. However, with pipeline parallelism, significant time\ncan be spent in the pipeline bubble: the total number of pipeline\nstages should thus be limited so that the number of microbatches\nin the pipeline is a reasonable multiple of the number of pipeline\nstages. Consequently, we see peak performance when the tensor-\nparallel size is equal to the number of GPUs in a single node (8 with\nDGX A100 nodes). This result indicates that neither tensor model\nparallelism (used by Megatron [40]) nor pipeline model parallelism\n(used by PipeDream [ 30] and others) in isolation can match the\nperformance of using both techniques in conjunction.\n5.4.2 Pipeline versus Data Parallelism. We evaluate the impact of\ndata and pipeline model parallelism on performance for a GPT\nmodel with 5.9 billion parameters (32 transformer layers, 32 at-\ntention heads, hidden size of 3840) in Figure 14. We use a smaller\nmodel than before since we want to show performance for models\nthat fit when the model-parallel size is only 2. For simplicity, we\nkeep the microbatch size equal to 1 in these experiments. We see\nthat for each batch size, the throughput decreases as the pipeline-\nparallel size increases, matching our analytical model from Â§3.3.\nPipeline model parallelism should be used primarily to support the\ntraining of large models that do not fit on a single worker, and data\nparallelism should be used to scale up training.\n5.4.3 Tensor versus Data Parallelism. We also evaluate the impact\nof data and tensor model parallelism on performance for the same\nGPT model with 5.9 billion parameters in Figure 15 (smaller model\nused for same reason as above). As before, we keep the microbatch\nsize equal to 1 initially. With larger batch sizes and a microbatch\nsize of 1, data-parallel communication is infrequent; the all-to-all\ncommunication required in tensor model parallelism needs to be\nperformed for every microbatch in a batch. This all-to-all communi-\ncation with tensor model parallelism dominates end-to-end training\ntime, especially when communication needs to be performed across\nmulti-GPU nodes. Additionally, as the tensor-model-parallel size\nincreases, we perform smaller matrix multiplications on every GPU,\ndecreasing utilization on each GPU.\nWe should note that although data parallelism can lead to effi-\ncient scaling, we cannot use data parallelism in isolation for very\nlarge models with a limited training batch size because of a) insuffi-\ncient memory capacity, and b) scaling limitations of data parallelism\n(e.g., GPT-3 was trained to convergence with a batch size of 1536.\nData parallelism thus supports parallelization to only 1536 GPUs;\nhowever, roughly 10,000 GPUs were used to train this model in a\nreasonable amount of time).\n5.5 Microbatch Size\nWe evaluate the impact of the microbatch size on the performance\nof parallel configurations that combine pipeline and tensor model\nparallelism in Figure 16 for a model with 91 billion parameters\n((ğ‘¡,ğ‘)= (8,8)). We see that the best microbatch size is 2 for this\nmodel; the optimal microbatch size is different for other models (not\nshown in Figure) and model-dependent. For a given batch size, in-\ncreasing the microbatch size decreases the number of microbatches\nin the pipeline (ğ‘š), leading to a larger pipeline bubble; however,\nincreasing the microbatch size can also improve GPU utilization\nby increasing the arithmetic intensity of executed kernels. These\ntwo factors are at odds with each other, which makes the choice\nof optimal microbatch size challenging. Our analytical model from\nÂ§3.3 reasonably approximates true performance, and can be used\nas a proxy to determine how to pick this hyperparameter value for\nvarious training configurations and models.\nEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\n/uni00000015/uni00000016/uni00000018/uni0000001c/uni00000015/uni0000001a/uni00000017/uni00000016/uni0000001a/uni00000018/uni00000015/uni00000016/uni0000001c/uni00000016/uni00000019/uni0000001a\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000014/uni00000012/uni00000014\n/uni00000016/uni00000012/uni00000019\n/uni00000019/uni00000012/uni00000014\n/uni0000001b/uni00000012/uni00000019\n/uni00000015/uni00000014/uni00000012/uni00000014\n/uni00000038/uni0000004c/uni00000056/uni00000053/uni00000059/uni0000004b/uni0000004c/uni00000054/uni00000059/uni00000058\n/uni0000000c/uni00000057/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000057/uni00000013/uni00000057/uni00000049/uni00000047/uni00000053/uni00000052/uni00000048/uni0000000d\n/uni00000025/uni00000047/uni00000058/uni00000012/uni00000004/uni00000056/uni00000049/uni00000047/uni00000053/uni00000051/uni00000054/uni00000059/uni00000058/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\n/uni0000003b/uni00000013/uni00000053/uni00000004/uni00000045/uni00000047/uni00000058/uni00000012/uni00000004/uni00000056/uni00000049/uni00000047/uni00000053/uni00000051/uni00000054/uni00000059/uni00000058/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\nFigure 17: Throughput (in sequences per second) with and without\nactivation recomputation for a GPT model with 145 billion param-\neters using 128 A100 GPUs ( (ğ‘¡,ğ‘)= (8,16)).\n/uni00000015/uni00000016/uni00000016/uni00000018/uni00000017/uni0000001a/uni00000018/uni0000001c/uni0000001a/uni00000014\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni00000019/uni00000014\n/uni0000001b/uni00000019\n/uni00000015/uni00000014/uni00000014\n/uni00000015/uni00000016/uni00000019\n/uni00000015/uni00000019/uni00000014\n/uni00000025/uni00000047/uni0000004c/uni0000004d/uni00000049/uni0000005a/uni00000049/uni00000048/uni00000004/uni00000058/uni00000049/uni00000056/uni00000045/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000013/uni00000057\n/uni00000054/uni00000049/uni00000056/uni00000004/uni0000002b/uni00000034/uni00000039\n/uni00000039/uni00000052/uni00000053/uni00000054/uni00000058/uni0000004d/uni00000051/uni0000004d/uni0000005e/uni00000049/uni00000048\n/uni00000037/uni00000047/uni00000045/uni00000058/uni00000058/uni00000049/uni00000056/uni00000013/uni0000004b/uni00000045/uni00000058/uni0000004c/uni00000049/uni00000056/uni00000004/uni00000053/uni00000054/uni00000058/uni0000004d/uni00000051/uni0000004d/uni0000005e/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\nFigure 18: Throughput per GPU with and without the scatter/gather\noptimization for a GPT model with 175 billion parameters using 96\nA100 GPUs and the interleaved schedule.\n5.6 Activation Recomputation\nFigure 17 shows throughput with and without activation recompu-\ntation for a GPT model with 145 billion parameters (80 transformer\nlayers, 96 attention heads, hidden size of 12288) using 128 A100\nGPUs, (ğ‘¡,ğ‘)= (8,16), and a range of batch sizes. For small batch\nsizes, activation recomputation leads to up to33% lower throughput\n(in sequences per second) due to the extra forward pass that needs\nto be executed during the backward pass. However, activation re-\ncomputation is needed to support larger batch sizes. Throughput at\nlarge batch sizes with activation recomputation is up to 2Ã—higher\nthan the best throughput achieved without activation recomputa-\ntion (for a smaller batch size) due to a smaller pipeline bubble.\n5.7 Scatter-Gather Optimization\nFigure 18 shows per-GPU-throughput with and without (unop-\ntimized) the scatter/gather communication optimization for the\nGPT-3 model with 175 billion parameters. We see an improvement\nof up to 11% in throughput for communication-intensive sched-\nules (large batch size with interleaving) by reducing the amount of\ncommunication over cross-node links.\n5.8 Fused Operators\nWe also evaluate the performance impact of operator fusion de-\nscribed in Â§4.2. For the GPT-3 model (175 billion parameters), through-\nput increased by 19% with fusion (113 teraFLOP/s per GPU to 135\nteraFLOP/s per GPU). For the larger GPT model with 530 billion\nparameters (model configuration in Figure 1), throughput increased\nby 11% (133 teraFLOP/s per GPU to 148 teraFLOP/s per GPU).\n5.9 Inter-Node Communication Bandwidth\nOur strong results are a byproduct of using an optimized software\nand hardware stack together. In particular, we take advantage of the\nhigh-bandwidth communication links between GPUs on the same\nserver and across servers. On the trillion-parameter model with\n3072 GPUs, we observed that the effective bisection bandwidth of\npoint-to-point communication among pipeline stages is 892 GB/s,\nwhile the effective bisection bandwidth of all-reduce operations\namong data-parallel replicas is 12.9 TB/s. A less-optimized parti-\ntioning of operators across devices would lead to more inter-node\ncommunication, hampering scaling performance.\n5.10 Checkpoint Loading and Saving\nAn important practical consideration for the training of large mod-\nels is loading and saving model checkpoints, which are especially\nlarge for the models considered in this paper. For example, the\ntrillion-parameter model has a checkpoint of size 13.8 terabytes.\nThe initial load of checkpoints for the trillion-parameter model by\nall 384 nodes (3072 GPUs) reaches a peak read bandwidth of 1TB/s,\nthe maximum read throughput possible from the parallel filesystem.\nCheckpoint saves reach 40% of peak write bandwidth (273 GB/s).\n6 RELATED WORK\nIn this section, we discuss other techniques to train models at scale.\nParallelism for Large Models. Pipeline model parallelism is a com-\nmon technique used to train large models. Pipeline parallelism\ncomes in a few flavors: the mode discussed in this paper uses flushes\nto ensure strict optimizer semantics. TeraPipe [26] exposes fine-\ngrained pipeline parallelism across tokens in a single training se-\nquence for auto-regressive models like GPT. PipeTransformer [19]\nelastically adjusts the degree of pipelining and data parallelism\nby freezing layers with â€œstableâ€ weights, and instead dedicates re-\nsources to train the remaining â€œactiveâ€ layers. HetPipe [31] uses a\ncombination of pipeline and data parallelism on a set of heteroge-\nneous accelerators. Pipeline parallelism can also be implemented\nwith relaxed semantics: PipeDream-2BW [30] maintains two weight\nversions and guarantees 1-stale weight updates without expen-\nsive flushes, while PipeMare [45] and Kosson et al. [23] use asyn-\nchoronous pipeline parallelism. These techniques have improved\nthroughput compared to the techniques with pipeline flushes con-\nsidered in this paper, but potentially at the cost of convergence rate\nor final accuracy. Moreover, pipeline parallelism in isolation can\nstill only scale to a number of devices equal to the number of layers\nin the model, which is limiting for certain model architectures.\nPipeDream [29] combined pipeline parallelism and data paral-\nlelism in a principled way to reduce cross-device communication.\nDeepSpeed [2] combined pipeline parallelism with tensor and data\nparallelism to train models with up to a trillion parameters, but\nwith lower throughput than what was shown in this paper (52%\nvs. 36% of peak) for a few reasons: operator fusion to keep most of\nthe operator graph compute-bound, a more-efficient pipeline paral-\nlelism schedule to minimize the pipeline bubble size, fast hardware\n(A100 vs. V100 GPUs and high-bandwidth links between GPUs\non the same and different servers), and scaling to more GPUs. We\nwant to emphasize that this higher throughput makes estimated\ntraining times much more practical (about 3 months); an aggregate\nthroughput of 37.6 petaFLOP/s would take about 40 months to train\nan equivalently-sized model. We can scale to larger models as well,\nbut would need more GPUs to keep training time practical.\nMesh-TensorFlow [39] proposes a language for easily specifying\nparallelization strategies that combine data and model parallelism.\nSwitch Transformers [15] used Mesh-Tensorflow to train a sparsely\nactivated expert-based model with 1.6 trillion parameters, with\nimproved pre-training speed over the T5-11B model [35].\nSharded Data Parallelism. As part of performance optimizations\nfor MLPerf 0.6 [28], sharded data parallelism [24, 44], where opti-\nmizer state is sharded over data-parallel workers, was introduced.\nThis method has two advantages: (a) it does not introduce extra\ncommunication over vanilla data parallelism, and (b) it divides the\noptimizerâ€™s computation and memory cost across the data-parallel\npartitions. ZeRO [36, 37] extends this idea: weight parameters and\ngradients are sharded across data-parallel workers as well, and\nworkers fetch relevant state from their â€œowningâ€ workers before\nperforming computations. This adds additional communication,\nwhich can be partially hidden by carefully overlapping computa-\ntion and communication. However, this can become harder if tensor\nparallelism is not used or the batch size is not large enough to hide\nthe extra communication overhead (Figure 10). ZeRO-Infinity [37]\nuses NVMe to efficiently swap parameters, enabling the training of\nvery large models on a small number of GPUs. We note that using\na small number of GPUs for training a very large model results in\nunrealistic training times (e.g., thousands of years to converge).\nAutomatic Partitioning. FlexFlow [22], PipeDream [ 29], DAP-\nPLE [14], and Tarnawski et al. [41] all auto-partition model training\ngraphs over multiple devices with the help of cost models. However,\neach of these do not consider all the parallelism dimensions con-\nsidered in this paper: pipeline and tensor model parallelism, data\nparallelism, microbatch size, and the effect of memory-savings op-\ntimizations like activation recomputation on the training of models\nlarger than the memory capacity of an accelerator. These added\ndimensions increase the search space that needs to be explored.\nGholami et al. [16] show how communication costs for combina-\ntions of data and model parallelism can be modeled.\nHPC for Model Training. Goyal et al. [17] and You et al. [47] both\ndemonstrate the use of High Performance Computing techniques\nto train highly-accurate ImageNet models in minutes. However, the\nimage classification models considered fit comfortably on a single\naccelerator, rendering model parallelism unnecessary, support very\nlarge batch sizes (> 32k) that allow scaling data parallelism to large\nworker counts with infrequent communication, and are composed\nof compact convolutional layers that are inherently amenable to\ndata-parallel communication.\n7 DISCUSSION AND CONCLUSION\nIn this paper, we have shown how PTD-P (inter-node pipeline par-\nallelism, intra-node tensor parallelism, and data parallelism) can be\ncomposed to achieve high aggregate throughput (502 petaFLOP/s)\nwhile training large models with a trillion parameters. This facil-\nitates end-to-end training in reasonable times (estimated time of\naround 3 months for a trillion-parameter model). We discussed the\nvarious tradeoffs associated with each of these types of parallelism,\nand how the interactions between them need to be considered\ncarefully when combined.\nEven though the implementation and evaluation in this paper\nis GPU-centric, many of these ideas translate to other types of\naccelerators as well. Concretely, the following are ideas that are\naccelerator-agnostic: a) the idea of smartly partitioning the model\ntraining graph to minimize the amount of communication while\nstill keeping devices active, b) minimizing the number of memory-\nbound kernels with operator fusion and careful data layout, c) other\ndomain-specific optimizations (e.g., scatter-gather optimization).\nACKNOWLEDGEMENTS\nWe thank the anonymous reviewers, Seonmyeong Bak, Keshav San-\nthanam, Trevor Gale, Dimitrios Vytiniotis, and Siddharth Karam-\ncheti for their help and feedback that improved this work. This\nresearch was supported in part by NSF Graduate Research Fellow-\nship grant DGE-1656518 and NSF CAREER grant CNS-1651570. Any\nopinions, findings, and conclusions or recommendations expressed\nin this material are those of the authors alone.\nAPPENDIX: FLOATING-POINT OPERATIONS\nIn this section, we describe how we calculate the number of floating-\npoint operations (FLOPs) in a model. We consider a language model\nwith ğ‘™ transformer layers, hidden size â„, sequence length ğ‘ , vocabu-\nlary size ğ‘‰, and training batch size ğµ.\nA ğ´ğ‘šÃ—ğ‘˜Ã—ğ‘‹ğ‘˜Ã—ğ‘› matrix multiplication requires 2ğ‘šÃ—ğ‘˜Ã—ğ‘›FLOPs\n(factor of 2 needed to account for multiplies and adds).\nA transformer layer consists of an attention block followed by\na 2-layer feed-forward network. For the attention block, the main\nFLOP contributors are the key, query, and value transformation\n(6ğµğ‘ â„2 operations), attention matrix computation ( 2ğµğ‘ 2â„ opera-\ntions), attention over values (2ğµğ‘ 2â„operations), and post-attention\nlinear projection ( 2ğµğ‘ â„2 operations). The feed-forward network\nincreases the hidden size to 4â„and then reduces it back to â„; this\nrequires 16ğµğ‘ â„2 FLOPs. Summing these together, each transformer\nlayer results in 24ğµğ‘ â„2 +4ğµğ‘ 2â„ FLOPs for the forward pass. The\nbackward pass requires double the number of FLOPs since we\nneed to calculate the gradients with respect to both input and\nweight tensors. In addition, we are using activation recomputation,\nwhich requires an additional forward pass before the backward\npass. As a result, the total number of FLOPs per transformer layer\nis 4 Ã—\u000024ğµğ‘ â„2 +4ğµğ‘ 2â„\u0001 = 96ğµğ‘ â„2\n\u0010\n1 + ğ‘ \n6â„\n\u0011\n.\nThe other main contributor to the FLOP count is the logit layer in\nthe language model head, which transforms features of dimension\nâ„ to the vocabulary dimension ğ‘‰. The required FLOPs for this\noperation is 2ğµğ‘ â„ğ‘‰ in the forward pass and 4ğµğ‘ â„ğ‘‰ in the backward\npass, resulting in 6ğµğ‘ â„ğ‘‰ FLOPs in total.\nThus, for a transformer model with ğ‘™ transformer layers, the\ntotal number of floating-point operations is:\n96ğµğ‘ ğ‘™â„2\n\u0012\n1 + ğ‘ \n6â„ + ğ‘‰\n16ğ‘™â„\n\u0013\n.\nEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\nREFERENCES\n[1] Applications of GPT-3. https://openai.com/blog/gpt-3-apps/.\n[2] DeepSpeed: Extreme-Scale Model Training for Everyone. https:\n//www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-\ntraining-for-everyone/.\n[3] DeepSpeed Repository. https://www.deepspeed.ai/.\n[4] GitHub Copilot. https://copilot.github.com/.\n[5] Microsoft Translates Spoken Text to Code. https://techcrunch.com/2021/05/25/\nmicrosoft-uses-gpt-3-to-let-you-code-in-natural-language/.\n[6] NVIDIA A100 Tensor Core GPU. https://www.nvidia.com/en-us/data-center/\na100/.\n[7] NVIDIA Collective Communication Library (NCCL). https://developer.nvidia.\ncom/nccl.\n[8] NVIDIA Selene Supercomputer. https://www.top500.org/system/179842/.\n[9] NVLink and NVSwitch. https://www.nvidia.com/en-us/data-center/nvlink/.\n[10] PyTorch JIT. https://pytorch.org/docs/stable/jit.html.\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, and et al. Language\nModels are Few-Shot Learners. arXiv preprint arXiv:2005.14165 , 2020.\n[12] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training Deep Nets\nwith Sublinear Memory Cost. arXiv preprint arXiv:1604.06174 , 2016.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv preprint arXiv:1810.04805 , 2018.\n[14] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan\nWu, Guoping Long, Jun Yang, Lixue Xia, et al. DAPPLE: A Pipelined Data Parallel\nApproach for Training Large Models. In Proceedings of the 26th ACM SIGPLAN\nSymposium on Principles and Practice of Parallel Programming , pages 431â€“445,\n2021.\n[15] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity. arXiv preprint\narXiv:2101.03961, 2021.\n[16] Amir Gholami, Ariful Azad, Peter Jin, Kurt Keutzer, and Aydin Buluc. Integrated\nModel, Batch, and Domain Parallelism in Training Neural Networks. In Proceed-\nings of the 30th on Symposium on Parallelism in Algorithms and Architectures ,\npages 77â€“86, 2018.\n[17] Priya Goyal, Piotr DollÃ¡r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,\nAapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, Large\nMinibatch SGD: Training ImageNet in 1 Hour. arXiv preprint arXiv:1706.02677 ,\n2017.\n[18] Andreas Griewank and Andrea Walther. Revolve: An Implementation of Check-\npointing for the Reverse or Adjoint Mode of Computational Differentiation.ACM\nTransactions on Mathematical Software (TOMS) , 26(1):19â€“45, 2000.\n[19] Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr. PipeTrans-\nformer: Automated Elastic Pipelining for Distributed Training of Transformers.\narXiv preprint arXiv:2102.03161 , 2021.\n[20] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. GPipe:\nEfficient Training of Giant Neural Networks using Pipeline Parallelism. In\nAdvances in Neural Information Processing Systems , pages 103â€“112, 2019.\n[21] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph\nGonzalez, Kurt Keutzer, and Ion Stoica. Breaking the Memory Wall with Optimal\nTensor Rematerialization. In Proceedings of Machine Learning and Systems 2020 ,\npages 497â€“511. 2020.\n[22] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond Data and Model Parallelism\nfor Deep Neural Networks. In Proceedings of the 2nd Conference on Machine\nLearning and Systems (MLSys) , 2018.\n[23] Atli Kosson, Vitaliy Chiley, Abhinav Venigalla, Joel Hestness, and Urs KÃ¶ster.\nPipelined Backpropagation at Scale: Training Large Models without Batches.\nProceedings of Machine Learning and Systems , 2021.\n[24] Sameer Kumar, Victor Bitorff, Dehao Chen, Chiachen Chou, Blake Hechtman,\nHyoukJoong Lee, Naveen Kumar, Peter Mattson, Shibo Wang, Tao Wang, et al.\nScale MLPerf-0.6 Models on Google TPU-v3 Pods.arXiv preprint arXiv:1909.09756 ,\n2019.\n[25] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng\nLi, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. PyTorch\nDistributed: Experiences on Accelerating Data Parallel Training. arXiv preprint\narXiv:2006.15704, 2020.\n[26] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn\nSong, and Ion Stoica. TeraPipe: Token-Level Pipeline Parallelism for Training\nLarge-Scale Language Models. arXiv preprint arXiv:2102.07988 , 2021.\n[27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly\nOptimized BERT Pretraining Approach. CoRR, abs/1907.11692, 2019.\n[28] Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micike-\nvicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf,\net al. MLPerf Training Benchmark. arXiv preprint arXiv:1910.01500 , 2019.\n[29] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R\nDevanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. PipeDream:\nGeneralized Pipeline Parallelism for DNN Training. In Proceedings of the 27th\nACM Symposium on Operating Systems Principles , pages 1â€“15, 2019.\n[30] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.\nMemory-Efficient Pipeline-Parallel DNN Training. In International Conference on\nMachine Learning , pages 7937â€“7947. PMLR, 2021.\n[31] Jay H Park, Gyeongchan Yun, M Yi Chang, Nguyen T Nguyen, Seungmin Lee,\nJaesik Choi, Sam H Noh, and Young-ri Choi. HetPipe: Enabling Large DNN Train-\ning on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined\nModel Parallelism and Data Parallelism. In 2020 USENIX Annual Technical Con-\nference (USENIX ATC 20) , pages 307â€“321, 2020.\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learn-\ning Library. In Advances in Neural Information Processing Systems , volume 32,\n2019.\n[33] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving\nLanguage Understanding by Generative Pre-Training, 2018.\n[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language Models are Unsupervised Multitask Learners. OpenAI Blog ,\n1(8):9, 2019.\n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits\nof Transfer Learning with a Unified Text-to-Text Transformer.arXiv:1910.10683,\n2019.\n[36] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO:\nMemory Optimization Towards Training A Trillion Parameter Models. arXiv\npreprint arXiv:1910.02054 , 2019.\n[37] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong\nHe. ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep\nLearning. arXiv preprint arXiv:2104.07857 , 2021.\n[38] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,\nShuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. ZeRO-Offload: De-\nmocratizing Billion-Scale Model Training. arXiv preprint arXiv:2101.06840 , 2021.\n[39] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Pen-\nporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, Ryan Sepassi, and Blake Hechtman. Mesh-TensorFlow: Deep Learning\nfor Supercomputers. In Neural Information Processing Systems , 2018.\n[40] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. Megatron-LM: Training Multi-Billion Parameter Language\nModels using GPU Model Parallelism. arXiv preprint arXiv:1909.08053 , 2019.\n[41] Jakub M Tarnawski, Amar Phanishayee, Nikhil Devanur, Divya Mahajan, and\nFanny Nina Paravecino. Efficient Algorithms for Device Placement of DNN\nGraph Operators. In Advances in Neural Information Processing Systems , pages\n15451â€“15463, 2020.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All You Need.\narXiv preprint arXiv:1706.03762 , 2017.\n[43] Eric P Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee,\nXun Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu. Petuum: A New\nPlatform for Distributed Machine Learning on Big Data. IEEE Transactions on\nBig Data , 1(2):49â€“67, 2015.\n[44] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake Hechtman,\nand Shibo Wang. Automatic Cross-Replica Sharding of Weight Updates in Data-\nParallel Training. arXiv preprint arXiv:2004.13336 , 2020.\n[45] Bowen Yang, Jian Zhang, Jonathan Li, Christopher RÃ©, Christopher Aberger, and\nChristopher De Sa. PipeMare: Asynchronous Pipeline Parallel DNN Training.\nProceedings of Machine Learning and Systems , 2021.\n[46] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov,\nand Quoc V. Le. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. CoRR, abs/1906.08237, 2019.\n[47] Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Ima-\ngeNet Training in Minutes. In Proceedings of the 47th International Conference on\nParallel Processing , pages 1â€“10, 2018."
}