{
  "title": "GottBERT: a pure German Language Model",
  "url": "https://openalex.org/W3109959371",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Scheible, Raphael",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Thomczyk, Fabian",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Tippmann, Patric",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jaravine, Victor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2808029672",
      "name": "Boeker Martin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2999168658",
    "https://openalex.org/W2946119234",
    "https://openalex.org/W3216234416",
    "https://openalex.org/W2962772361",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2131571251",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2995647371"
  ],
  "abstract": "Lately, pre-trained language models advanced the field of natural language processing (NLP). The introduction of Bidirectional Encoders for Transformers (BERT) and its optimized version RoBERTa have had significant impact and increased the relevance of pre-trained models. First, research in this field mainly started on English data followed by models trained with multilingual text corpora. However, current research shows that multilingual models are inferior to monolingual models. Currently, no German single language RoBERTa model is yet published, which we introduce in this work (GottBERT). The German portion of the OSCAR data set was used as text corpus. In an evaluation we compare its performance on the two Named Entity Recognition (NER) tasks Conll 2003 and GermEval 2014 as well as on the text classification tasks GermEval 2018 (fine and coarse) and GNAD with existing German single language BERT models and two multilingual ones. GottBERT was pre-trained related to the original RoBERTa model using fairseq. All downstream tasks were trained using hyperparameter presets taken from the benchmark of German BERT. The experiments were setup utilizing FARM. Performance was measured by the $F_{1}$ score. GottBERT was successfully pre-trained on a 256 core TPU pod using the RoBERTa BASE architecture. Even without extensive hyper-parameter optimization, in all NER and one text classification task, GottBERT already outperformed all other tested German and multilingual models. In order to support the German NLP field, we publish GottBERT under the AGPLv3 license.",
  "full_text": "GottBERT: a pure German Language Model\nRaphael Scheible1, Fabian Thomczyk1, Patric Tippmann1, Victor Jaravine1 and Martin Boeker1\n{raphael.scheible,thomczyk,tippmann,victor.zharavin,martin.boeker}\n@imbi.uni-freiburg.de\n1 Institute of Medical Biometry and Statistics, Medical Center, Faculty of Medicine, University of Freiburg\nAbstract\nLately, pre-trained language models advanced\nthe ﬁeld of natural language processing (NLP).\nThe introduction of Bidirectional Encoders for\nTransformers (BERT) and its optimized ver-\nsion RoBERTa have had signiﬁcant impact and\nincreased the relevance of pre-trained models.\nFirst, research in this ﬁeld mainly started on\nEnglish data followed by models trained with\nmultilingual text corpora. However, current re-\nsearch shows that multilingual models are in-\nferior to monolingual models. Currently, no\nGerman single language RoBERTa model is\nyet published, which we introduce in this work\n(GottBERT). The German portion of the OS-\nCAR data set was used as text corpus. In\nan evaluation we compare its performance\non the two Named Entity Recognition (NER)\ntasks Conll 2003 and GermEval 2014 as well\nas on the text classiﬁcation tasks GermEval\n2018 (ﬁne and coarse) and GNAD with ex-\nisting German single language BERT models\nand two multilingual ones. GottBERT was\npre-trained related to the original RoBERTa\nmodel using fairseq. All downstream tasks\nwere trained using hyperparameter presets\ntaken from the benchmark of German BERT.\nThe experiments were setup utilizing FARM.\nPerformance was measured by the F1 score.\nGottBERT was successfully pre-trained on a\n256 core TPU pod using the RoBERTa BASE\narchitecture. Even without extensive hyper-\nparameter optimization, in all NER and one\ntext classiﬁcation task, GottBERT already out-\nperformed all other tested German and multi-\nlingual models. In order to support the Ger-\nman NLP ﬁeld, we publish GottBERT under\nthe AGPLv3 license.\n1 Introduction\nThe computation of contextual pre-trained word\nrepresentation is the current trend of neural net-\nworks in natural language processing (NLP).\nThis trend has a long history, starting with non-\ncontextualized word representations of which the\nmost prominent are word2vec (Goldberg and Levy,\n2014) , GloVe (Pennington et al., 2014) and fast-\nText (Joulin et al., 2016, 2017; Bojanowski et al.,\n2017; Mikolov et al., 2018), evolving to deep con-\ntextualized models such as ELmO (Alsentzer et al.,\n2019) and ﬂair (Akbik et al., 2019). Most recently,\nthe ﬁeld of NLP experienced remarkable progress,\nby the use of Transformer (Vaswani et al., 2017)\nbased approaches. Especially Bidirectional En-\ncoder Representations from Transformers (BERT)\n(Devlin et al., 2019) impacted the ﬁeld which sub-\nsequently was robustly optimized to RoBERTa (Liu\net al., 2019). These Transformer based approaches\nuse large scale pre-trained language models. For\napplication, such models are ﬁne tuned by a super-\nvised training on the speciﬁc downstream task lead-\ning to performance improvements in many tasks.\nOn the other hand, the computation of the language\nmodel is performed unsupervised. Large text blobs\nare required for training and strong hardware such\nas hundreds of GPUs (Martin et al., 2020) or TPUs\n(You et al., 2020). Initially, most of the research\ntook place in English followed by multilingual ap-\nproaches (Conneau et al., 2019). Although, mul-\ntilingual approaches were trained on large texts\nof many languages, they were outperformed by\nsingle language models (de Vries et al., 2019; Mar-\ntin et al., 2020; Le et al., 2020; Delobelle et al.,\n2020). Single language models trained with the\nOpen Super-large Crawled ALMAnaCH coRpus\n(OSCAR) (Ortiz Su´arez et al., 2020) showed good\nperformance due to the size and variance of the OS-\nCAR (Martin et al., 2020; Delobelle et al., 2020).\nFollowing this ongoing trend, we pre-train the ﬁrst\nGerman RoBERTa single language model with the\nGerman portion of the OSCAR - the German OS-\nCAR text trained BERT (GottBERT). In an evalua-\ntion we compare its performance on the two named\narXiv:2012.02110v1  [cs.CL]  3 Dec 2020\nentity recognition tasks Conll 2003 and GermEval\n2014, as well as on the two text classiﬁcation tasks\nGermEval 2018 and GNAD with existing German\nsingle language BERT models and two multilingual\nones.\n2 Related Work\nMost recently Transformer based models widely\nimpacted the ﬁeld of NLP. From neural translation\n(Ott et al., 2018; Ng et al., 2019) to generative\nlanguage models as GPT2 (Radford et al., 2019),\nremarkable performance gains were achieved. With\nBERT, an approach facilitating pre-trained trans-\nformer based models was introduced. Fine-tuned\non downstream tasks, BERT based approaches\nimproved the performance of several NLP tasks\n(Devlin et al., 2019; Liu et al., 2019). BERT\nmodels though, were ﬁrst released as single lan-\nguage models in English based on 16GB of raw\ntext and as the multilingual model mBERT based\non Wikipedia dumps in about 100 languages (De-\nvlin, 2018). These models were followed by sin-\ngle language models for several languages: Bertje\n(de Vries et al., 2019) for Dutch, FinBERT (Virta-\nnen et al., 2019) for Finish, German BERT 1 and a\nGerman BERT from the MDZ Digital Library team\nat the Bavarian State Library to which we refer as\ndbmz BERT in this paper 2. German BERT was\ntrained using 12GB of raw text data basing on Ger-\nman Wikipedia (6GB), the OpenLegalData dump\n(2.4GB) and news articles (3.6GB). dbmz BERT\nused as source data a German Wikipedia dump, EU\nBookshop corpus, Open Subtitles, CommonCrawl,\nParaCrawl and News Crawl which sums up to a\ndataset of 16GB. With the release of RoBERTa a\nnew standard for raw text size was set as it was\ntrained on 160GB of raw English text. Further,\nRoBERTa enhances the original BERT approach\nby removing segment embeddings, next sentence\nprediction and improved hyperparameters. Ad-\nditionally, instead of using wordpiece (Schuster\nand Nakajima, 2012) tokenization, RoBERTa uti-\nlizes GPT2’s byte pair encoding (BPE) (Radford\net al., 2019) with the beneﬁt that language-speciﬁc\ntokenizers are not required. Other than mBERT,\nthe multilingual XLM-RoBERTa (Conneau et al.,\n2019) was trained on 2.5TB of ﬁltered Common-\nCrawl data. CamemBERT is a French RoBERTa\n1https://deepset.ai/german-bert\n2https://github.com/dbmdz/berts#\ngerman-bert\nmodel that was trained on the OSCAR and uses\nsentencepiece (Kudo and Richardson, 2018) BPE.\nFurther, they pre-trained a model with 4GB of the\nFrench OSCAR portion and another model with\n4GB of the French Wikipedia. The comparison\nof these models using downstream tasks shows\nthat high text variance leads to better results. Um-\nBERTo3 is an Italian RoBERTa model, similarly\ndesigned as CamemBERT. RobBERT, the Dutch\nsingle language RoBERTa, was trained on 39GB\nof the Dutch portion of the OSCAR and outper-\nformed Bertje. A more recent version of RobBert\nshowed the performance gains of language speciﬁc\nBPE compared to the English based GPT2 BPE\nin downstream tasks. Most recently FlauBERT\n(Le et al., 2020) for French was released trained\non 71GB data. They cleaned a 270GB corpus of\nmixed sources by ﬁltering out meaningless con-\ntent and Unicode-normalization. Data was pre-\ntokenized by moses (Koehn et al., 2007) and en-\ncoded by fastBPE4 which is an implementation of\nSennrich et al. (2016). Following the approach of\nutilizing the OSCAR, we computed the German\nOSCAR text trained BERT (GottBERT). However,\nthe drawback of BERT approaches is the computa-\ntional power requirement. Multiple GPUs or TPUs\nwere used for pre-training. All listed RoBERTa\nbased models were computed on GPUs whereas\nGottBERT is the ﬁrst published RoBERTa model\npre-trained on TPUs.\n3 GottBERT\nGottBERT bases on the robustly optimized BERT\narchitecture RoBERTa, which was pre-trained with\nthe German portion of the OSCAR using the fairseq\nframework (Ott et al., 2019).\nTraining Data\nThe GottBERT model is trained on the German\nportion of the OSCAR, a recently published large\nmultilingual text corpus extracted from Common\nCrawl. The German data portion of the OSCAR\nmeasures 145GB of text containing approximately\n21.5 billion words in approximately 459 million\ndocuments (one document per line).\nPre-processing\nOriginally, RoBERTa uses GPT2 (Radford et al.,\n2019) byte pair encoding to segment the input into\n3https://github.com/\nmusixmatchresearch/umberto\n4https://github.com/glample/fastBPE\nsubword units. Therefore, no pre-tokenization is\nrequired and thus no language-speciﬁc tokenizer\nas e.g. moses (Koehn et al., 2007) must be used.\nIts original vocabulary was computed on English\ndata. For GottBERT we computed a vocabulary\nof 52k subword tokens based on 40 GB randomly\nsampled documents of the German OSCAR portion.\nCompared to the original GPT2 tokenizer, which\nwas trained on English data, this leads to a 40%\nsmaller size of the binary data which are fed into\nfairseq (Ott et al., 2019). Furthermore, according\nto Delobelle et al. (2020), it leads to a performance\nincrease.\nPre-training\nUsing fairseq, we pre-trained GottBERT on a 256\ncore TPU pod using the RoBERTa BASE architec-\nture. We trained the model in 100k update steps\nusing a batch size of 8k. A 10k iteration warmup\nof the learning rate to a peak of 0.0004 was applied,\nfrom which the learning rate polynomially decayed\nto zero.\nDownstream Tasks\nBased on the pre-trained BERT models, several\ndownstream tasks were trained. The Framework for\nAdapting Representation Models (FARM)5 already\ncomes with pre-conﬁgured experiments for Ger-\nman language. Originally, these experiments were\nsetup to benchmark German BERT. We adopted\nthis set of hyperparameters without additional grid\nsearch optimization. FARM is based on Hugging\nFace’s Transformer library (Wolf et al., 2019). As\nwe trained GottBERT with fairseq, we converted\nGottBERT into the Hugging Face format.\nNamed Entity Recognition We evaluated Got-\ntBERT on two NER tasks. One was the German\npart of CoNLL 2003 shared task (Tjong Kim Sang\nand De Meulder, 2003). It contains three main en-\ntity classes and one for other miscellaneous entities.\nAs measurement we used the harmonic mean of\nprecision and recallF1. The second NER task was\nGermEval 2014 (Benikova et al., 2014). It extends\nthe CoNLL 2003 shared task by ﬁne-grained labels\nand embedded markables. Fine-grained labels al-\nlow the indication of NER subtypes common in\nGerman, namely derivations and parts: e.g. “Mann”\n→ “m¨annlich” and “Mann” → “mannhaft”. In\norder to recognize nested NEs embedded mark-\nables are required. Speciﬁcally, this was realized\n5https://github.com/deepset-ai/FARM\nby annotating main classes as well as two levels of\nsubclasses. Performance was measured by the use\nof an adapted F1 evaluation metric Benikova et al.\n(2014), which considers the equality of labels and\nspans (text passages) and additionally levels in the\nclass hierarchy.\nText Classiﬁcation GermEval task 2018 (Risch\net al., 2018) is a text classiﬁcation task that contains\ntwo subtasks of different granularity: the coarse-\ngrained binary classiﬁcation of German tweets and\nﬁne-grained classiﬁcation of the same tweets into\nfour different classes. Based on the One Million\nPosts Corpus (Schabus et al., 2017) which is in-\ntended to test performance on German language,\nthe 10k German News Articles Dataset (10kG-\nNAD) topic classiﬁcation6 was created. The dataset\ncontains approximately 10k news articles of an\nAustrian newspaper which are to be classiﬁed into\n9 categories. As both classiﬁcation datasets do not\nprovide a pre-deﬁned validation set, we used 90%\nof the original training set for training and 10% for\nvalidation. Data was split randomly. For evaluation\nwe computed the mean of the F1-scores of each\nclass/category.\n4 Results\nIn order to evaluate the performance, each down-\nstream task ran 10 times using different seeds. As\nmeasure, the F1 scores of the experiments based\non the test set was used. The score is the best of 10\nruns of the respective experiment of each trained\nmodel. The best score selection is based on valida-\ntion set. We compared GottBERT’s performance\nwith four other models listed in Table 1.\nNamed Entity Recognition For the two tested\nNER tasks, CoNLL 2003 and GermEval 2014, Got-\ntBERT outperformed all others models followed by\ndbmz BERT (see Table 2). Obviously, the amount\nand characteristic of data of the German portion\nof the OSCAR is beneﬁcial. The third place goes\nto XLM RoBERTa. Interestingly, mBERT outper-\nforms German BERT, although mBERT’s amount\nof German data can be assumed to be smaller com-\npared to German BERT. Notably, mBERT performs\nan exponentially smoothed weighting of the data in\norder to balance the variance in data size of all the\ndifferent languages. Consequently, even if we knew\nthe data size of the German portion of mBERT, a\ndirect comparison would not be possible.\n6https://tblock.github.io/10kGNAD\nModel Type #Languages Data Size Data Source\nGottBERT RoBERTa 1 145GB OSCAR\ndbmz BERT BERT 1 16GB\nWikipedia, EU Bookshop corpus7,\nOpen Subtitles,\nCommon-,Para-,NewsCrawl\nmBERTcased BERT 104 unknown Wikipedia\nGerman BERT BERT 1 12GB news articles, Open Legal Data8,\nWikipedia\nXLM RoBERTa RoBERTa 100 2.5TB\n(66.6GB German) CommonCrawl, Wikipedia\nTable 1: This table shows the models, we used in our experiments. Additional information about the pre-training\nand architecture is listed. Unfortunately, for mBERT we did not ﬁnd any estimate about the data size.\nModel CoNLL 2003 GermEval 2014\nGottBERT 83.57 86.84\ndbmz BERT 82.30 85.82\nmBERTcased 81.20 85.39\nGerman BERT 81.18 85.03\nXLM RoBERTa 81.36 85.41\nTable 2: F1 scores of NER experiments based on the test set. Best score out of 10 runs (selection based on\nvalidation set).\nText Classiﬁcation Table 2 shows the F1 scores\nof the classiﬁcation tasks. In GermEval 2018\ncoarse and 10kGNAD, dbmz BERT outperforms\nthe other models followed by German BERT. Got-\ntBERT only performs best in GermEval 2018 ﬁne.\nIn 10kGNAD even XLM RoBERTa outperforms\nGottBERT. In general, the two RoBERTa based\nmodels seem not to develop their full potential,\nwhich might be due to sub-optimal hyperparame-\nters.\n5 Conclusion\nIn this work we present the German single lan-\nguage RoBERTa based model GottBERT which\nwas computed on 145GB plain text. GottBERT is\nthe ﬁrst German single language RoBERTa based\nmodel. Even without extensive hyperparameter op-\ntimization, in three out of ﬁve downstream tasks,\nGottBERT already outperformed all other tested\nmodels. As the authors of German BERT released\nthe parameters within the FARM framework, we as-\nsume the parameters for the downstream tasks are\nprobably not optimal for RoBERTa based models.\nConsequently, further extensive hyperparameter op-\ntimization of the downstream tasks might lead to\nbetter results for GottBERT. Dodge et al. (2020)\ngive insights into hyperparameter optimization, its\ncomplexity and effects in relation to BERT. Fur-\nther, they present a solution to lower costs. Due\nto the required effort, we currently leave this open\nas future work and release GottBERT in hugging-\nface and fairseq format to the community under the\nAGPLv3 license.\nAcknowledgments\nThis work was supported by the German Min-\nistry for Education and Research (BMBF FKZ\n01ZZ1801B) and supported with Cloud TPUs from\nGoogle’s TensorFlow Research Cloud (TFRC). We\nwould like to thank Ian Graham for constructive\ncriticism of the manuscript and Louis Martin for\nthe helping email contact. A special thanks goes\nto Myle Ott, who implemented the TPU feature\nin fairseq and intensively supported us to get our\ncomputation run. Finally, we would like to recog-\nnizably thank the people behind the scenes who\nessentially made this work possible: Frank Werner,\nGeorg Koch, Friedlinde B¨uhler and Jochen Knaus\nof our internal IT team, Philipp Munz and Chris-\ntian Wiedemann of Wabion GmbH and last but not\nleast Nora Limbourg the Google Cloud Customer\nEngineer assigned to us.\nModel GermEval 2018 coarse GermEval 2018 ﬁne 10kGNAD\nGottBERT 76.39 51.25 89.20\ndbmz BERT 77.32 50.97 90.86\nmBERTcased 72.87 44.78 88.72\nGerman BERT 77.23 49.28 90.66\nXLM RoBERTa 75.15 45.63 89.30\nTable 3: F1 scores of text classiﬁcation experiments based on the test set. Best score out of 10 runs (selection\nbased on validation set).\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFLAIR: An Easy-to-Use Framework for State-of-\nthe-Art NLP. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics (Demonstra-\ntions), pages 54–59, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nB. A. McDermott. 2019. Publicly Available Clinical\nBERT Embeddings. arXiv:1904.03323 [cs]. ArXiv:\n1904.03323.\nDarina Benikova, Chris Biemann, Max Kisselew, and\nSebastian Pad ´o. 2014. GermEval 2014 Named En-\ntity Recognition Shared Task: Companion Paper.\nProceedings of the KONVENS GermEval Shared\nTask on Named Entity Recognition, pages 104–112.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. RobBERT: a Dutch RoBERTa-based Lan-\nguage Model. arXiv:2001.06286 [cs]. ArXiv:\n2001.06286.\nJacob Devlin. 2018. Multilingual BERT Readme Doc-\nument.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-Tuning Pretrained Language Models:\nWeight Initializations, Data Orders, and Early Stop-\nping. arXiv:2002.06305 [cs]. ArXiv: 2002.06305.\nYoav Goldberg and Omer Levy. 2014. word2vec Ex-\nplained: deriving Mikolov et al.’s negative-sampling\nword-embedding method. arXiv:1402.3722 [cs,\nstat]. ArXiv: 1402.3722.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, H ´erve J ´egou, and Tomas Mikolov.\n2016. FastText.zip: Compressing text classiﬁcation\nmodels. arXiv preprint arXiv:1612.03651.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of Tricks for Efﬁcient\nText Classiﬁcation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427–431. Association for Computational\nLinguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nSource Toolkit for Statistical Machine Translation.\nIn Proceedings of the 45th Annual Meeting of the\nAssociation for Computational Linguistics Compan-\nion Volume Proceedings of the Demo and Poster Ses-\nsions, pages 177–180, Prague, Czech Republic. As-\nsociation for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoˆıt Crabb´e, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised Language\nModel Pre-training for French. arXiv:1912.05372\n[cs]. ArXiv: 1912.05372.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs]. ArXiv:\n1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: a Tasty French Language\nModel. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7203–7219, Online. Association for Computa-\ntional Linguistics.\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\nvances in Pre-Training Distributed Word Representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nFAIR’s WMT19 News Translation Task Submission.\nIn Proceedings of the Fourth Conference on Ma-\nchine Translation (Volume 2: Shared Task Papers,\nDay 1), pages 314–319, Florence, Italy. Association\nfor Computational Linguistics.\nPedro Javier Ortiz Su´arez, Laurent Romary, and Benoˆıt\nSagot. 2020. A Monolingual Approach to Contex-\ntualized Word Embeddings for Mid-Resource Lan-\nguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1703–1714, Online. Association for Computa-\ntional Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. arXiv:1904.01038\n[cs]. ArXiv: 1904.01038.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling Neural Machine Trans-\nlation. arXiv:1806.00187 [cs]. ArXiv: 1806.00187.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global Vectors for Word\nRepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nJulian Risch, Eva Krebs, Alexander L ¨oser, Alexander\nRiese, and Ralf Krestel. 2018. Fine-Grained Classi-\nﬁcation of Offensive Language. In Proceedings of\nGermEval 2018 (co-located with KONVENS), pages\n38–44.\nDietmar Schabus, Marcin Skowron, and Martin Trapp.\n2017. One Million Posts: A Data Set of German On-\nline Discussions. In Proceedings of the 40th Interna-\ntional ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval (SIGIR), pages\n1241–1244, Tokyo, Japan.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand Korean voice search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pages 5149–5152. ISSN:\n2379-190X.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. arXiv:1508.07909 [cs] .\nArXiv: 1508.07909.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 Shared Task:\nLanguage-Independent Named Entity Recognition.\nIn Proceedings of the Seventh Conference on Nat-\nural Language Learning at HLT-NAACL 2003 - Vol-\nume 4, CONLL ’03, pages 142–147, USA. Associ-\nation for Computational Linguistics. Event-place:\nEdmonton, Canada.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBERT for Finnish. arXiv:1912.07076 [cs]. ArXiv:\n1912.07076.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord,\nand Malvina Nissim. 2019. BERTje: A Dutch\nBERT Model. arXiv:1912.09582 [cs]. ArXiv:\n1912.09582.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large Batch Optimization for Deep Learning:\nTraining BERT in 76 minutes. arXiv:1904.00962\n[cs, stat]. ArXiv: 1904.00962 version: 5.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8490344285964966
    },
    {
      "name": "German",
      "score": 0.7234401702880859
    },
    {
      "name": "Natural language processing",
      "score": 0.7096003890037537
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6731224060058594
    },
    {
      "name": "Language model",
      "score": 0.613298773765564
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5423200726509094
    },
    {
      "name": "Transformer",
      "score": 0.48080459237098694
    },
    {
      "name": "Task (project management)",
      "score": 0.32225704193115234
    },
    {
      "name": "Linguistics",
      "score": 0.18354833126068115
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}