{
  "title": "Predicting Enzymatic Reactions with a Molecular Transformer",
  "url": "https://openalex.org/W4251572712",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5031470060",
      "name": "David K. Kreutter",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A5028051805",
      "name": "Philippe Schwaller",
      "affiliations": [
        "IBM Research - Zurich",
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A5040848839",
      "name": "Jean‐Louis Reymond",
      "affiliations": [
        "University of Bern"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3040975947",
    "https://openalex.org/W4249735123",
    "https://openalex.org/W3088602465",
    "https://openalex.org/W3212781770",
    "https://openalex.org/W6977367121",
    "https://openalex.org/W6637353697",
    "https://openalex.org/W905512289",
    "https://openalex.org/W4291186635",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2604561121",
    "https://openalex.org/W4285659244"
  ],
  "abstract": "The use of enzymes for organic synthesis allows for simplified, more economical and selective synthetic routes not accessible to conventional reagents. However, predicting whether a particular molecule might undergo a specific enzyme transformation is very difficult. Here we exploited recent advances in computer assisted synthetic planning (CASP) by considering the Molecular Transformer, which is a sequence-to-sequence machine learning model that can be trained to predict the products of organic transformations, including their stereochemistry, from the structure of reactants and reagents. We used multi-task transfer learning to train the Molecular Transformer with one million reactions from the US Patent Office (USPTO) database as a source of general chemistry knowledge combined with 32,000 enzymatic transformations, each one annotated with a text description of the enzyme. We show that the resulting Enzymatic Transformer model predicts the products formed from a given substrate and enzyme with remarkable accuracy, including typical kinetic resolution processes.",
  "full_text": "1 \nPredicting Enzymatic Reactions with a \nMolecular Transformer \nDavid Kreutter,a) Philippe Schwallera), b) and Jean-Louis Reymonda)* \na) Department of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012 Bern, \nSwitzerland; b) IBM Research, Europe, Säumerstrasse 4, 8803 Rüschlikon, Switzerland  \ne-mail:  jean-louis.reymond@dcb.unibe.ch  \n \nAbstract \nThe use of enzymes for organic synthesis allows for simplified, more economical and selective \nsynthetic routes not accessible to conventional reagents. However, predicting whether a \nparticular molecule might undergo a specific enzyme transformation is very difficult. Here we \nexploited recent advances in computer assisted synthetic planning (CASP)  by considering the \nMolecular Transformer, which is a sequence-to-sequence machine learning model that can be \ntrained to predict the products of organic transformations, including their stereochemistry, from \nthe structure of reactants and reagents. We used multi -task transfer learning to train the  \nMolecular Transfo rmer with one million reactions from the US Patent Office (USPTO)  \ndatabase as a source of general chemistry knowledge combined with 32,000 enzym atic \ntransformations, each one annotated with a text description of the enzyme. We show that the \nresulting Enzymatic Transformer model predicts the products formed from a given substrate \nand enzyme with remarkable accuracy, including typical kinetic resolution processes.  \n \n  \n2 \nIntroduction \nThe use of enzymes for organic synthesis, commonly referred to as the field of biocatalysis , \ngreatly contributes to organic synthesis methodology by providing the possibility carry out \nhighly chemo-, regio-, stereo - and enantio -selective transformations under mild and \nenvironmentally friendly conditions, often allowing the redesign and simplification of synthetic \nroutes by enabling reactions that are not possible with conventional chemical reagents. 1,2 The \nadvent of directed enzyme evolution as a tool to increase enzyme performance has also greatly \ncontributed to improve the range and efficiency of enzyme catalyzed reactions for  organic \nsynthesis.3 However, the implementation of biocataly tic steps in synthetic processes remains \nchallenging because it is very difficult to predict  whether a particular substrate might actually \nbe converted by an enzyme to the desired product.  \n Computer-assisted synthetic planning (CASP) comprise s a range of artificial \nintelligence approaches to predict reaction products from reactant or reagents, or vice -versa, \nand to plan retrosynthesis.4–12 Here we asked the question whether CASP might be exploited to \npredict the outcome of enzymatic reactions for organic synthesis. Recent efforts in predicting \nenzymatic reactions focused on metabolic reactions from the KEGG enzymatic reaction \ndatabase and predictions of drug metabolism ,13–15 as well as retrosynthetic planning with  \nenzymatic reactions  using a  template based approach .16 Here we considered the Molecular \nTransformer,17–19 which is a sequence -to-sequence prediction model operating on text \nrepresentations of reactions as reaction SMILES (Simplified Molecular Input Line Entry \nSystem)20 including stereochemistry. We set out to use multi-task transfer learning combining \nthe USPTO dataset 21 as a source of general chemistry knowledge  with a few thousand  \nenzymatic reactions collected from the scientific literature as a source of specialized knowledge \n(Figure 1). Such transfer learning strategy was recently shown to enable the Molecular \n3 \nTransformer to predict complex regio - and stereo -selective reactions at the example of  \ncarbohydrates.22  \n \n \nFigure 1. General concept of the Enzymatic Transformer training. The USPTO data set contains \nreactions SMILES describing reactants, reagents and products. The ENZR data set contains reaction \nSMILES as well as an additional text component. \n \n  \n\n4 \nResult and Discussion \nReaction datasets \nAs general chemistry dataset we used the previously reported “USPTO Stereo Augmented ” \ndataset derived from the patent mining work of Lowe , which contains, for each of the one \nmillion reactions in the USPTO dataset, the original  reaction SMILES  and a randomized \nSMILES version, both conserving stereochemical information. 23,24 To compose a specialized \ndataset of enzymatic reactions, we extracted 70,000 reactions labeled as “enzymatic reactions” \nfrom the Reaxys database.25 We collected the data columns corresponding to reactant SMILES, \nproduct SMILES, and enzyme description  (“Reaction”, “Reagent” and “Catalyst”) . \nCanonicalizing all SMILES and removing reactions lacking either reactants or products as well \nas duplicate entries (identical reactants, products and enzyme description)  left 32,000 unique \nenzymatic reactions, each annotated with an enzyme description, referred to here as the ENZR \ndataset.   \n Enzyme names almost exclusively end with the suffix “-ase”. Analyzing the  ENZR \ndataset showed that 81.9 % (26,348) of the enzyme descriptions contained a single “-ase” word, \nand 18.1 % (5,812) contained two or more “-ase” words (Figure 2A). The majority of single “-\nase” word descriptions referred to a lipase, a type of enzyme which is almost exclusively used \nalone (Figure 2 B). By contrast, dehydrogenases and reductases were most frequent in \ndescriptions with two or more “ -ase” words, reflecting that such enzymes are often used in \nprocesses involving enzyme-coupled cofactor regeneration systems.  \n Organizing the ENZR dataset in a tree -map (TMAP)26 grouping reactions by reaction \nfingerprint similarity27 and color-coding by enzyme class showed well-defined clusters for each \nenzyme class, illustrating that, similarly to organic reagent s, enzymes carry out well -defined \nfunctional group transformations (Figure 2C). Enantiomeric resolution reactions, identified by \n5 \nthe presence of chirality in products only, also appeared in defined clusters corresponding to \nbiotransformations with mostly d ehydrogenases, lipases and reductases  (Figure 2 D). The \ndifferent enzymes also formed identifiable clusters in a second TMAP grouping reactions by \nsubstructure similarity of the reacting substrates  using the extended connectivity fingerprint \nMHFP6 (Figure 2E).28 This illustrated that enzymatic reactions in the ENZR dataset followed \nthe well-known trend that enzymes only react with certain types of substrates, in contrast to \nchemical reagents which are usually only specific for functional groups. The range of substrates \nutilized by the enzymes covered a broad range of size from very small molecules such as \npyruvate up to relatively large peptides (Figure 2F).  \n Taken together, the data above indicated that the ENZR dataset contained a diverse set \nof enzymatic reactions, with the expected biases towards the most frequently used enzymes in \nthe field of biocatalysis such as lipases and dehydrogenases.  \n \n6 \n \nFigure 2. The ENZR dataset. (A) Amount of reaction depending on how many “-ase” words are present \nin the sentence. (B) Frequency of the top 15 “-ase” words depending on the count of enzyme name per \nreaction. (C) TMAP of reactions similarity colored by \" -ase\" word co mbinations. (D) TMAP \nhighlighting enantiomeric resolution reactions among the ENZR data set. (E) TMAP by reacting \nsubstrate substructure similarity colored by \"-ase\" word combinations. (F) TMAP by substrate molecular \nweight. \n\n7 \nTraining and evaluation of transformer models for enzymatic reactions \nTraining a transformer model first requires tokenizing the input and output character strings to \nallow the model to learn which series of input tokens produces which series of output tokens. \nFor the reaction SMILES in both USPTO and ENZR datasets, we used the approach reported \npreviously for the general Molecular Transformer (each character is a separate token except \ncharacter strings in square brackets, which denote special elements). For the enzyme description \nsentences in the ENZR dataset , we implemented a tokenizer based on our ENZR sentences \nvocabulary using the Hugging Face Tokenizers library .29 The models were trained using \nOpenNMT and PyTorch. The reactions were canonicalized using RDKit.30  \n In view of evaluating transformer models, we split the USPTO dataset randomly into a \ntraining set (1.8 million reactions, 80%), a validation and a test set (each 32,000 reactions, 10 \n%). For the ENZR dataset, we first grouped reactions having the same product in different \ngroups, and then split these groups into a training set (25,700 reaction, 80 %), a validation and \na test set (each 3,200 reactions, 10 %). Distributing these reaction groups rather than individual \nreactions into the different sets ensured that products which must be predicted in the validation \nor test sets have not been seen by the transformer during training or validation sets, respectively.  \n For evaluation, each model was presented  with the substrate SMILES, optionally \ntogether with the partial or full description of the enzyme, for each of the 3,200 reactions in the \ntest set. In each case, the model was challenged to write out the SMILES of the reaction product, \nincluding the correct stereochemistry, none of which had been seen by the model in the training \nor validation set. We analyzed whether the correct product was written out within the first one \nor first two solutions proposed by the model , as well as  the percentage of invalid product \nSMILES appearing among the first one or two solution s (top 1 and top 2 accuracy, blue and \ncyan bars, top 1 and top 2 invalid SMILES, red and orange bars, Figure 3A).  \n8 \n Considering that enzymatic reactions are quite substr ate specific, we first evaluated if \ntransformer models could predict reaction products from only the substrate without any enzyme \ninformation. The UPSTO only model only showed approximately 10 % accuracy but a very \nlow percentage of invalid SMILES, indicating that this model understood chemistry but lacked \nexpertise in biotransformations  (Fig. 3A, entry (a)) . The ENZR only model also performed \npoorly ( ~ 20% accuracy ) and produced ~10 % invalid SMILES, reflecting that general \nchemistry training was insufficient with this relatively small dataset  (Fig. 3 A, entry (b)) . \nNevertheless, training with both models using sequential transfer learning (STL) or multi -task \ntransfer learning (MTL) reached ~ 50% accuracy , indicating  that substrate structure was \npartially predictive of the outcome of enzymatic reactions even in the absence of any enzyme \ninformation, reflecting the substrate selectivity of enzymes (Fig. 3A, entries (c) and (d)).  \n Adding enzyme information in form of “-ase” words alone did not significantly increase \nprediction performance when using only ENZR or when using STL, however MTL resulted in \n~60 % prediction accuracy (Fig. 3A, entries (e), (f), (g)). Prediction accuracy increased further \nwith MTL up to ~65 % when using the complete enzyme information as full sentence (Fig. 3A, \nentry (j)), but not with STL (Fig. 3 A, entry (i)), illustrating the importance of the transfer \nlearning method. Note that the model trained with ENZR alone only reached ~40 % accuracy \nwith full enzyme names and produced ~10 % invalid SMILES, showing that the  general \nchemistry training learned from USPTO was essential even with full enzyme information (Fig. \n3A, entry (h)). Furthermore, testing the MTL with a test set in which the  enzyme information \nwas scrambled between reactions resulted in poor results (~15 % accuracy), indicating that the \ntrue enzyme information was required rather than the presence of random text information (Fig. \n3A, entry (k)). \n \n9 \nAnalyzing the prediction performance of the Enzymatic Transformer \nThe comparisons above showed that an excellent prediction performance was reached by the \ntransformer trained using MTL combining the USPTO and the ENZR dataset using full enzyme \nnames as enzyme information. Retraining this model with different splits of training, validation \nand test sets gave indistinguishable results in terms of prediction accuracy.  This model was \nselected for further investigation and is referred to as the “Enzymatic Transformer”.  \n Considering that many reactions in the ENZR dataset contained multiple enzymes, we \nwondered if our transformer might be confused in such situations because the main enzyme and \nthe cofactor regeneration enzyme are not labeled as such. Indeed, the prediction accuracy of the \nEnzymatic Transformer was lower for reactions with multiple enzymes compared to reactions \nwith a single enzyme (Figure 3B). Since transformer models require a large number of examples \nfor good performance, we also tested prediction accuracy as f unction of the number of \noccurrences of the enzyme name in the training set. Indeed, a very high prediction accuracy of \nalmost 80% was reached for lipases, which were the most abundant in the training set  (Figure \n3C). Nevertheless, prediction accuracy reached a good level (~60 %) as soon as more than five \nexamples of a particular enzyme were present  in the training set. Note that in the best \ntransformer model using MTL on full sentences, there was a clear association of the prediction \nconfidence score with accuracy, as observed with other transformer models (Figure 3D).22  \n Since the subset of the test set containing the word “lipase” performed best (Figure 3C), \nwe evaluated this subset exhaustively with all models (Figure 3D). While models trained on the \nUSPTO or ENZR dataset without enzyme information performed poorly (Fig. 3D, entries (a) \nand (b)), combining both sets with STL (entry (c)) or MTL (entry (d)) reached  an excellent \naccuracy (> 70%), indicating that the presence of an ester functional group is sufficient for the \nmodel to recognize a lipase biotransformation even in the absence of the enzyme name. \n10 \nHowever, models trained with ENZR alone using only the “as e” word or the full sentence \nperformed poorly  (Fig. 3D, entries (e) and (h)), showing that this relatively small dataset \ncontained insufficient general chemistry knowledge to training even for the relatively simple \nlipase reaction. Overall , the model train ed on both datasets using STL and the full enzyme \ndescription performed best for lipases, as observed in the entire dataset (Fig. 3D, entry (j)). \nHowever, scrambling the enzyme information between different reactions in the lipase only test \nset did not dec rease prediction accuracy as dramatically as for the full set, reflecting the fact \nthat all lipases catalyze very similar reactions. In addition, 36.89 % of the lipase test set cases \nwere reaction s with Candida antartica  lipase B, the most frequently used lipase in \nbiotranformations, in which case swapping the enzyme information does not induce any \nchange.  \n Enzymatic reactions are often used to perform kinetic resolutions, typically using \nhydrolase enzymes such as lipase s, or to transform achiral substrates into chiral products, \ntypically to produce chiral alcohols or amines from achiral ketone precursors. To evaluate the \nperformance of the transformer on such reactions, we defined enantiomeric resolutions as \nenzymatic reactions containing chiral centers, identified by the presence of at least one  “@” \ncharacter in the SMILES, in the reaction products only, which corresponded to 6495 reactions \nin the entire ENZR dataset (20.18 %), and 687 reactions in the test set (21.35 %). The relative \nperformance of the different transformer models in this subset was comparable to that of the \nentire dataset , indicating that the transformer model was able to learn the enantiomeric \npreference of enantioselective enzymes as successfully as the overall enzymatic transformation. \n(Figure 3E).  \n11 \n \nFigure 3: (A) Top prediction accuracy and invalid SMILES on the enzyme reaction test set for various models. \n(a): USPTO Model from Schwaller et al. trained without any enzymatic transfer learning and tested without \nenzyme sentence. (b): Enzymatic DB without USPTO data set. (c): USPTO model transfer learned to enzymatic \nDB without the enzyme description part. (d): USPTO model transfer learned to enzymatic DB only with ‘-ase’ \nwords as input. (e): USPTO model transfer learned to enzymatic DB using full sentences of enzyme reactions. \n(f): Multi-task model of USPTO and enzymatic DB, tested without the enzyme description part. (g): Same multi-\ntask model trained only with ‘-ase’ words as input in place of full sentences. (h) Same multi-task model trained \nwith full description sentences. (i): Same multi-task model tested by swapping enzyme full sentences between \nreactions of the test set. (B) Accuracy on the Test set depending on how many “ -ase” words are present in the \nsentence. (C) Accuracy on the Test set depending on how frequent the “ -ase” words combination from the \nsentences appears in the training set. (D) Predictions ordered by confidence score from a random sample of 150 \ntest set predictions. (E) Top prediction accuracy and invalid SMILES on lipases reactions of the test set only. (F) \nTop prediction accuracy and invalid SMILES on enantiomeric resolution reactions of the test set only.  \n\n12 \nExamples of correct and incorrect predictions by the Enzymatic Transformer \nThe types of enzymatic reactions predicted correctly by the Enzymatic Transformer are well \nillustrated by selected cases from the enantiomeric resolutions in the test set for two lipases,31,32 \ntwo alcohol dehydrogenases ,33,34 and one example each of a synthase,35 reductase,36–38 \ntransaminase39,40 and peroxygenase (reaction (1) to (8), Figure 4).41,42  Considering that none \nof the products of these reactions have been seen by the model during training, the ability of \nthe Enzymatic Transformer model to correctly predict not only the correct reaction product but \nalso the correct stereochemical outcome of the enantiomeric resolution reactions is remarkable.  \n \nFigure 4: Examples of successful predictions by the Enzymatic Transformer. \n \n13 \nOn the other hand, analyzing unsuccessful predictions by the Enzymatic Transformer returns a \ndifferentiated picture (Figure 5). The first two examples reflect true limitations of our model. \nIn the first case of the enantioselective biotransformation of 4 -methyl-cyclohexanol by a \nsequence of an alcohol dehydrogenase and a cyclohexanone monooxygenase to produce an \nenantiomerically pure lactone,43 the model proposes the correct product but does not recognize \nthat the reactio n is enantioselective  (reaction (9)) . In the second case, the Enzymatic \nTransformer correctly predicts the formation of (+)--cadinene from geranyl pyrophosphate by \n(+) cadinene synthase ,44 however the deuterium label, which is lost during cyclization, is \nwrongly incorporated into the predicted product (reaction (10)).  \n Additional examples reflect inaccurate database entries. In the hydrolysis of a β-\nhydroxysulfone by porcine liver esterase ,45 the Enzymatic Transformer correctly predicts the \nalcohol hydrolysis product, however this product is unstable and spontaneously eliminates to \nform a styrene, which is the product isolated and recorded in the database  (reaction (11)). The \nEnzymatic Transformer also correctly predicts the formation of thymine from the hydrolysis of \na thymidine nucleoside analog by uridine phosphorylase,46 however the database entry wrongly \nrecorded the isomeric 6-methyl-uracil as the product  (reaction (12)). Finally, the Enzymatic \nTransformer correctly predicts that 5 -deoxy-b-D-ribofuranose is the product formed by the \naction of a nucleosidase  on the parent adenosine nucleoside , however the Enzymatic \nTransformer proposes the stable cyclic hemi-acetal form, while the database entry recorded the \nopen-chain aldehyde form (reaction (13)). 47   \n14 \n \nFigure 5: Examples of unsuccessful predictions by the Enzymatic Transformer.  \nConclusion \nHere we showed that general chemistry knowledge in form of the large USPTO dataset can be \ncombined with specialized information on enzymatic transformations in form of a relatively \nsmall ENZR dataset extracted from published reactions, to train a transformer model capable \nof predicting the outcome of enzym atic transformations including enantioselective reactions . \nIn this study, we obtained the best prediction accuracies when using multi-task transfer learning \nbased on the full description of the enzymes. However, model performance was limited by \ndatabase size and was lower with enzymes for which only few examples were available. \nFurthermore, analysis of successes and failures showed that model performance is also limited \nby the occurrence of database entry errors. Model performance can probably be increase d by \nusing larger and higher quality training dataset. However, our approach s hould be generally \nuseful to develop models capable of assisting chemists in implementing biotransformations for \nchemical synthesis.  \n15 \nMethods \nData collection \nThe USPTO data was downloaded from the patent mining work of Lowe24. The ENZR data set \nwas downloaded from Reaxys. 25  Enzymatic reactions were found queryin g “enzymatic \nreaction” keywords directly in the search field.  \nTransformer training \nThe Enzymatic Transformer model was trained based on the Molecular Transformer work from \nSchwaller et al.18 The version 1.1.1 of OpenNMT,48 freely available on GitHub,49 were used to \npreprocess, train and test the  models. Minor changes were performed based on the version of \nSchwaller et al.18  SMILES was also tokenized using the same tokenizer as Schwaller et al. 18  \nThe ENZR description sentences were tokenized by the Hugging Face Tokenizers 29 using a \nByte Pair Encoding.50 The following hyperparameters were used for the multi-task model: \npreprocess.py -train_ids ENZR ST_sep_aug \\ \n -train_src $DATADIR/ENZR/src_train.txt $DATADIR/ST_sep_aug/src-train.txt \\ \n -train_tgt $DATADIR/ENZR/tgt_train.txt $DATADIR/ST_sep_aug/tgt-train.txt \\ \n -valid_src $DATADIR/ENZR/src_val.txt -valid_tgt $DATADIR/ENZR/tgt_val.txt \\ \n -save_data $DATADIR/Preprocessed \\ \n -src_seq_length 3000 -tgt_seq_length 3000 \\ \n -src_vocab_size 3000 -tgt_vocab_size 3000 \\ \n -share_vocab -lower \n \ntrain.py -data $DATADIR/Preprocessed \\ \n -save_model ENZR_MTL -seed 42 -train_steps 200000 -param_init 0 \\ \n -param_init_glorot  -max_generator_batches 32 -batch_size 6144 \\ \n -batch_type tokens -normalization tokens -max_grad_norm 0 -accum_count 4 \\ \n -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam \\ \n -warmup_steps 8000 -learning_rate 4 -label_smoothing 0.0 -layers 4 \\ \n -rnn_size 384 -word_vec_size 384 \\ \n -encoder_type transformer -decoder_type transformer \\ \n -dropout 0.1 -position_encoding -global_attention general \\ \n -global_attention_function softmax -self_attn_type scaled-dot \\ \n -heads 8 -transformer_ff 2048 \\ \n -data_ids ENZR ST_sep_aug -data_weights 1 9 \\ \n -valid_steps 5000 -valid_batch_size 4 -early_stopping_criteria accuracy \\ \n16 \nValidation \nCanonicalized SMILES were compared to assess the accuracy of the models.  Distribution of \nthe training, validation and test set was randomly distributed  after being grouped by reaction \nproduct multiple time resulting in constant accuracy. \nTMAPs \nTMAPs were computed using standard parameters.26 The reaction fingerprint (RXNFP)27 as \nwell as the molecular substructure fingerprint (MHFP6)28 was computed with a dimension of \n256.  \n \nAvailability of Data and Materials \nThe USPTO data is available from the patent mining work of Lowe24. Reactions from Reaxys \nare accessible with subscription. The modified version of OpenNMT as well as the code to \ntokenize, train and test the model are available from: https://github.com/reymond-\ngroup/OpenNMT-py \nCompeting Interests \nThe authors declare that they have no competing interests. \nAcknowledgements \nThis work was supported financially by Novartis.  \n  \n17 \nReferences  \n1 R. A. Sheldon and J. M. Woodley, Chem. Rev., 2018, 118, 801–838. \n2 S. Wu, R. Snajdrova, J. C. Moore, K. Baldenius and U. T. Bornscheuer, Angew. Chem., \nInt. Ed. Engl., 2020, 59, 2–34. \n3 F. H. Arnold, Angew. Chem., Int. Ed. Engl., 2018, 57, 4143–4148. \n4 J. N. Wei, D. Duvenaud and A. Aspuru-Guzik, ACS Cent. Sci., 2016, 2, 725–732. \n5 B. Liu, B. Ramsundar, P. Kawthekar, J. Shi, J. Gomes, Q. Luu Nguyen, S. Ho, J. \nSloane, P. Wender and V. Pande, ACS Cent. Sci., 2017, 3, 1103–1113. \n6 C. W. Coley, R. Barzilay, T. S. Jaakkola, W. H. Green and K. F. Jensen, ACS Cent. Sci., \n2017, 3, 434–443. \n7 M. H. S. Segler, M. Preuss and M. P. Waller, Nature, 2018, 555, 604–610. \n8 C. W. Coley, W. H. Green and K. F. Jensen, Acc. Chem. Res., 2018, 51, 1281–1289. \n9 V. H. Nair, P. Schwaller and T. Laino, Chimia, 2019, 73, 997–1000. \n10 S. Johansson, A. Thakkar, T. Kogej, E. Bjerrum, S. Genheden, T. Bastys, C. Kannas, A. \nSchliep, H. Chen and O. Engkvist, Drug Discovery Today: Technologies, , \nDOI:10.1016/j.ddtec.2020.06.002. \n11 I. V. Tetko, P. Karpov, R. Van Deursen and G. Godin, arXiv:2003.02804 [cs, stat]. \n12 W. W. Qian, N. T. Russell, C. L. W. Simons, Y. Luo, M. D. Burke and J. Peng, , \nDOI:10.26434/chemrxiv.11659563.v1. \n13 Y. Cai, H. Yang, W. Li, G. Liu, P. W. Lee and Y. Tang, J. Chem. Inf. Model., 2018, 58, \n1169–1181. \n14 N. Hadadi, H. MohammadiPeyhani, L. Miskovic, M. Seijo and V. Hatzimanikatis, Proc. \nNatl. Acad. Sci. U. S. A., 2019, 116, 7298–7307. \n15 E. E. Litsa, P. Das and L. E. Kavraki, Chem. Sci., , DOI:10.1039/D0SC02639E. \n16 W. Finnigan, L. J. Hepworth, N. J. Turner and S. Flitsch, ChemRXiv, 2020, doi: \n10.26434/chemrxiv.12571235.v1. \n17 P. Schwaller, T. Gaudin, D. Lányi, C. Bekas and T. Laino, Chem. Sci., 2018, 9, 6091–\n6098. \n18 \n18 P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter, C. Bekas and A. A. Lee, \nACS Cent. Sci., 2019, 5, 1572–1583. \n19 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and \nI. Polosukhin, in Advances in neural information processing systems, 2017, pp. 5998–\n6008. \n20 D. Weininger, J. Chem. Inf. Comput. Sci., 1988, 28, 31–36. \n21 A. Thakkar, T. Kogej, J.-L. Reymond, O. Engkvist and E. J. Bjerrum, Chem. Sci., 2019, \n11, 154–168. \n22 G. Pesciullesi, P. Schwaller, T. Laino and J.-L. Reymond, Nat. Commun., 2020, 11, \n4874. \n23 D. M. Lowe, Thesis, University of Cambridge, 2012. \n24 Lowe, Daniel, figshare. dataset., , DOI:https://doi.org/10.6084/m9.figshare.5104873.v1. \n25 A. J. Lawson, J. Swienty-Busch, T. Géoui and D. Evans, in The Future of the History of \nChemical Information, American Chemical Society, 2014, vol. 1164, pp. 127–148. \n26 D. Probst and J.-L. Reymond, J. Cheminf., 2020, 12, 12. \n27 P. Schwaller, D. Probst, A. C. Vaucher, V. H. Nair, D. Kreutter, T. Laino and J.-L. \nReymond, ChemRXiv, 2020, 10.26434/chemrxiv.9897365.v3. \n28 D. Probst and J.-L. Reymond, J. Cheminf., 2018, 10, 66. \n29 T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. \nLouf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. \nXu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest and A. M. Rush, ArXiv. \n30 G. Landrum et al., RDKit: Open-Source Cheminformatics Software, 2020. \n31 E. Gianolio, R. Mohan and A. Berkessel, Adv. Synth. Catal., 2016, 358, 30–33. \n32 J. Xu, Y. Cen, W. Singh, J. Fan, L. Wu, X. Lin, J. Zhou, M. Huang, M. T. Reetz and Q. \nWu, J. Am. Chem. Soc., 2019, 141, 7934–7945. \n33 D. Alsafadi, S. Alsalman and F. Paradisi, Org. Biomol. Chem., 2017, 15, 9169–9175. \n34 W. Borzęcka, I. Lavandera and V. Gotor, J. Org. Chem., 2013, 78, 7312–7317. \n35 T. A. Klapschinski, P. Rabe and J. S. Dickschat, Angew. Chem., Int. Ed. Engl., 2016, \n55, 10141–10144. \n19 \n36 M. Rodríguez‐Mata, A. Frank, E. Wells, F. Leipold, N. J. Turner, S. Hart, J. P. \nTurkenburg and G. Grogan, ChemBioChem, 2013, 14, 1372–1379. \n37 K. Mitsukura, M. Suzuki, K. Tada, T. Yoshida and T. Nagasawa, Org. Biomol. Chem., \n2010, 8, 4533–4535. \n38 K. Mitsukura, M. Suzuki, S. Shinoda, T. Kuramoto, T. Yoshida and T. Nagasawa, \nBiosci. Biotechnol. Biochem., 2011, 75, 1778–1782. \n39 L. H. Andrade, W. Kroutil and T. F. Jamison, Org. Lett., 2014, 16, 6092–6095. \n40 D. Koszelewski, I. Lavandera, D. Clay, D. Rozzell and W. Kroutil, Adv. Synth. Catal., \n2008, 350, 2761–2766. \n41 E. Churakova, M. Kluge, R. Ullrich, I. Arends, M. Hofrichter and F. Hollmann, Angew. \nChem., Int. Ed. Engl., 2011, 50, 10716–10719. \n42 Y. Peng, D. Li, J. Fan, W. Xu, J. Xu, H. Yu, X. Lin and Q. Wu, Eur. J. Org. Chem., \n2020, 2020, 821–825. \n43 S. Schmidt, H. C. Büchsenschütz, C. Scherkus, A. Liese, H. Gröger and U. T. \nBornscheuer, ChemCatChem, 2015, 7, 3951–3955. \n44 J. A. Faraldos, D. J. Miller, V. González, Z. Yoosuf-Aly, O. Cascón, A. Li and R. K. \nAllemann, J. Am. Chem. Soc., 2012, 134, 5900–5908. \n45 W. Wang and B. Wang, Chem. Commun., 2017, 53, 10124–10127. \n46 C. S. Alexeev, G. G. Sivets, T. N. Safonova and S. N. Mikhailov, Nucleosides, \nNucleotides & Nucleic Acids, 2017, 36, 107–121. \n47 H. A. Namanja-Magliano, C. F. Stratton and V. L. Schramm, ACS Chem. Biol., 2016, \n11, 1669–1676. \n48 G. Klein, Y. Kim, Y. Deng, J. Senellart and A. Rush, in Proceedings of ACL 2017, \nSystem Demonstrations, Association for Computational Linguistics, Vancouver, \nCanada, 2017, pp. 67–72. \n49 OpenNMT/OpenNMT-py, https://github.com/OpenNMT/OpenNMT-py, (accessed July \n28, 2020). \n50 R. Sennrich, B. Haddow and A. Birch, arXiv:1508.07909 [cs]. \n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.811882734298706
    },
    {
      "name": "Reagent",
      "score": 0.5518001317977905
    },
    {
      "name": "Enzyme",
      "score": 0.5322149991989136
    },
    {
      "name": "Computer science",
      "score": 0.5102992057800293
    },
    {
      "name": "Combinatorial chemistry",
      "score": 0.4780709743499756
    },
    {
      "name": "Chemistry",
      "score": 0.47309210896492004
    },
    {
      "name": "Organic molecules",
      "score": 0.46422573924064636
    },
    {
      "name": "CASP",
      "score": 0.4326733350753784
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36028262972831726
    },
    {
      "name": "Molecule",
      "score": 0.3223186731338501
    },
    {
      "name": "Protein structure",
      "score": 0.20056581497192383
    },
    {
      "name": "Biochemistry",
      "score": 0.1926167607307434
    },
    {
      "name": "Organic chemistry",
      "score": 0.15750956535339355
    },
    {
      "name": "Engineering",
      "score": 0.14151692390441895
    },
    {
      "name": "Protein structure prediction",
      "score": 0.08717614412307739
    },
    {
      "name": "Electrical engineering",
      "score": 0.07359999418258667
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118564535",
      "name": "University of Bern",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210126328",
      "name": "IBM Research - Zurich",
      "country": "CH"
    }
  ]
}