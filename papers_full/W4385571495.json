{
  "title": "Knowledge of cultural moral norms in large language models",
  "url": "https://openalex.org/W4385571495",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3087625372",
      "name": "Aida Ramezani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2022600991",
      "name": "Xu Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4214903622",
    "https://openalex.org/W3104041537",
    "https://openalex.org/W4226287576",
    "https://openalex.org/W3207835719",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3052814785",
    "https://openalex.org/W3120860016",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2234474721",
    "https://openalex.org/W3034183707",
    "https://openalex.org/W4287887263",
    "https://openalex.org/W3168584517",
    "https://openalex.org/W4253968766",
    "https://openalex.org/W4287854995",
    "https://openalex.org/W3000950628",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2896252141",
    "https://openalex.org/W2971225582",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4291220703",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W4287887298",
    "https://openalex.org/W3173465197",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385572734",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4285210581",
    "https://openalex.org/W4386566829",
    "https://openalex.org/W2908105574",
    "https://openalex.org/W4221146509"
  ],
  "abstract": "Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as \"homosexuality\" and \"divorce\"; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 428–446\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nKnowledge of cultural moral norms in large language models\nAida Ramezani\nDepartment of Computer Science\nUniversity of Toronto\narmzn@cs.toronto.edu\nYang Xu\nDepartment of Computer Science\nCognitive Science Program\nUniversity of Toronto\nyangxu@cs.toronto.edu\nAbstract\nMoral norms vary across cultures. A recent\nline of work suggests that English large lan-\nguage models contain human-like moral bi-\nases, but these studies typically do not exam-\nine moral variation in a diverse cultural setting.\nWe investigate the extent to which monolin-\ngual English language models contain knowl-\nedge about moral norms in different countries.\nWe consider two levels of analysis: 1) whether\nlanguage models capture ﬁne-grained moral\nvariation across countries over a variety of top-\nics such as “homosexuality” and “divorce”; 2)\nwhether language models capture cultural di-\nversity and shared tendencies in which top-\nics people around the globe tend to diverge\nor agree on in their moral judgment. We per-\nform our analyses with two public datasets\nfrom the World Values Survey (across 55 coun-\ntries) and PEW global surveys (across 40 coun-\ntries) on morality. We ﬁnd that pre-trained En-\nglish language models predict empirical moral\nnorms across countries worse than the En-\nglish moral norms reported previously. How-\never, ﬁne-tuning language models on the sur-\nvey data improves inference across countries\nat the expense of a less accurate estimate of\nthe English moral norms. We discuss the rel-\nevance and challenges of incorporating cul-\ntural knowledge into the automated inference\nof moral norms.\n1 Introduction\nMoral norms vary from culture to culture (Haidt\net al., 1993; Bicchieri, 2005; Atari et al., 2022;\nIurino and Saucier, 2020). Understanding the cul-\ntural variation in moral norms has become criti-\ncally relevant to the development of machine in-\ntelligence. For instance, recent work has shown\nthat cultures vary substantially in their judgment to-\nward moral dilemmas regarding autonomous driv-\ning (Awad et al., 2018, 2020). Work in Natural\nLanguage Processing (NLP) also shows that lan-\nguage models capture some knowledge of social\nor moral norms and values. For example, with\nno supervision, English pre-trained language mod-\nels (EPLMs) have been shown to capture people’s\nmoral biases and distinguish between morally right\nand wrong actions (Schramowski et al., 2022).\nHere we investigate whether EPLMs encode knowl-\nedge about moral norms across cultures, an open\nissue that has not been examined comprehensively.\nMultilingual pre-trained language models\n(mPLMs) have been probed for their ability to\nidentify cultural norms and biases in a restricted\nsetting (Yin et al., 2022; Arora et al., 2022;\nHämmerl et al., 2022; Touileb et al., 2022). For\ninstance, Hämmerl et al. (2022) show that mPLMs\ncapture moral norms in a handful of cultures that\nspeak different languages. However, it remains\nunclear whether monolingual EPLMs encode\ncultural knowledge about moral norms. Prior\nstudies have only used EPLMs to assess how\nthey encode undesirable biases toward different\ncommunities (Ousidhoum et al., 2021; Abid et al.,\n2021; Sap et al., 2020; Nozza et al., 2021, 2022).\nFor instance, Abid et al. (2021) show that GPT3\ncan generate toxic comments against Muslims,\nand Nozza et al. (2022) explore harmful text\ngeneration toward LGBTQIA+ groups in BERT\nmodels (Devlin et al., 2018; Liu et al., 2019).\nExtending these lines of work, we assess whether\nmonolingual EPLMs can accurately infer moral\nnorms across many cultures. Our focus on EPLMs\nis due partly to the fact that English as a lingua\nfranca has widespread uses for communication\nin-person and through online media. Given that\nEPLMs may be applied to multicultural settings, it\nis important to understand whether these models\nencode basic knowledge about cultural diversity.\nSuch knowledge has both relevance and applica-\ntions for NLP such as automated toxicity reduction\nand content moderation (Schramowski et al., 2022).\nAnother motivation for our focus is that while it is\nexpected that EPLMs should encode western and\n428\nFigure 1: Comparison of human-rated and machine-scored moral norms across cultures. Left: Boxplots of hu-\nman ratings of moral norms across countries in the World Values Survey (WVS) (Haerpfer et al., 2021). Each\ndot represents the empirical average of participants’ ratings for a morally relevant topic (e.g., “abortion”) within\na country. Right: Corresponding moral scores estimated by a language model (Sentence-BERT) (Reimers and\nGurevych, 2019). Each dot represents the moral score obtained by probing the language model in a given country.\nEnglish-based moral knowledge, such knowledge\nmight entail potential (implicit) biases toward non-\nEnglish speaking cultures. For example, an EPLM\nmight infer a situation to be morally justiﬁable (e.g.,\n“political violence”) in a non-English speaking cul-\nture (because these events tend to associate with\nnon-English speaking cultures in corpora) and thus\ngenerate misleading representations of that com-\nmunity.\nHere we probe state-of-the-art EPLMs trained\non large English-based datasets. Using EPLMs\nalso supports a scalable analysis of 55 countries,\nwhich goes beyond existing work focusing on a\nsmall set of high-resource languages from mPLMs\nand monolingual PLMs. We take the moral norms\nreported in different countries to be a proxy of\ncultural moral norms and consider two main levels\nof analysis to address the following questions:\n• Level 1: Do EPLMs encode moral knowledge\nthat mirrors the moral norms in different coun-\ntries? For example, “getting a divorce” can\nbe a morally frowned-upon topic in country i,\nbut morally acceptable in country j.\n• Level 2: Can EPLMs infer the cultural di-\nversity and shared tendencies in moral judg-\nment of different topics? For example, peo-\nple across nations might agree that doing X\nis morally wrong while disagreeing in their\nmoral judgment toward Y.\nWe probe EPLMs using two publicly available\nglobal surveys of morality, World Values Survey\nwave 7 (Haerpfer et al., 2021) 1 (WVS) and PEW\nGlobal Attitudes survey (PEW) (Research Center,\n2014) 2. For example, according to WVS survey\n(illustrated in Figure 1), people in different cultures\nhold disparate views on whether “having casual\nsex” is morally acceptable. In contrast, they tend\nto agree more about the immorality of “violence\nagainst other people”. Our level 1 analysis allows\nus to probe the ﬁne-grained cultural moral knowl-\nedge in EPLMs, and our level 2 analysis investi-\ngates the EPLMs’ knowledge about shared “uni-\nversals” and variability across cultures in moral\njudgment. Following previous work (Arora et al.,\n2022) and considering the current scale of global\nmoral surveys, we use country as a proxy to culture,\nalthough this approach is not fully representative\nof all the different cultures within a country.\nWe also explore the utility-bias trade-off in en-\ncoding the knowledge of cultural moral norms in\nEPLMs through a ﬁne-tuning approach. With this\napproach it may be possible to enhance the moral\nknowledge of EPLMs in a multicultural setting. We\n1https://www.worldvaluessurvey.org/\nWVSContents.jsp\n2https://www.pewresearch.org/global/\ninteractives/global-morality/\n429\nexamine how this approach might reduce the ability\nof EPLMs to infer English-based moral norms and\ndiscuss how it might induce cultural biases.\n2 Related work\n2.1 Automated moral inference in NLP\nLarge language models have been utilized to\nmake automated moral inference from text. Trager\net al. (2022) used an annotated dataset to ﬁne-\ntune language models to predict the moral foun-\ndations (Graham et al., 2013) expressed in Red-\ndit comments. Many other textual datasets and\nmethods have been proposed for ﬁne-tuning LMs\nfor moral norm generation, reasoning, and adap-\ntation (Forbes et al., 2020; Emelin et al., 2021;\nHendrycks et al., 2021; Ammanabrolu et al.,\n2022; Liu et al., 2022; Lourie et al., 2021; Jiang\net al., 2021). Schramowski et al. (2022) pro-\nposed a method to estimate moral values and\nfound EPLMs to capture human-like moral judg-\nment even without ﬁne-tuning. They identiﬁed\na MORAL DIRECTION using the semantic space\nof Sentence-BERT (Reimers and Gurevych, 2019)\n(SBERT ) that corresponds to values of right and\nwrong. The semantic representations of different\nactions (e.g., killing people) would then be pro-\njected in this direction for moral judgment estima-\ntion. However, this method assumed a homoge-\nneous set of moral norms, so it did not examine\ncultural diversity in moral norms.\n2.2 Language model probing\nProbing has been used to study knowledge captured\nin language models. Petroni et al. (2019) proposed\na methodology to explore the factual information\nthat language models store in their weights. Simi-\nlar probing techniques have been proposed to iden-\ntify harmful biases captured by PLMs. Ousidhoum\net al. (2021) probed PLMs to identify toxic con-\ntents that they generate toward people of different\ncommunities. Nadeem et al. (2021) took a similar\napproach and introduced Context Association Tests\nto measure the stereotypical biases in PLMs, Yin\net al. (2022) used probing to evaluate mPLMs on\ngeo-diverse commonsense knowledge, and Touileb\net al. (2022) developed probing templates to investi-\ngate the occupational gender biases in multilingual\nand Norwegian language models. Related to our\nwork, Arora et al. (2022) used cross-cultural sur-\nveys to generate prompts for evaluating mPLMs in\n13 languages. For each country and category (e.g.,\nEthical Values) in the surveys, they take an average\nof participants’ responses to different questions in\nthe category and show that mPLMs do not corre-\nlate with the cultural values of the countries speak-\ning these languages. Differing from that study,\nwe assess ﬁner-grained prediction of EPLMs on\npeople’s responses to individual survey questions.\nMore recently, Dillion et al. (2023) prompted GPT-\n3.5 (Brown et al., 2020) with human judgments in\ndifferent moral scenarios and found striking corre-\nlation between the model outputs and the human\njudgments. Similar to Schramowski et al. (2022),\nthis work also used a homogeneous set of moral rat-\nings which represented English-based and Western\ncultures.\n3 Methodology for inferring cultural\nmoral norms\nWe develop a method for ﬁne-grained moral\nnorm inference across cultures. This method\nallows us to probe EPLMs with topic-country\npairs, such as “getting a divorce in [Country]”. 3\nWe build this method from the baseline method\nproposed by Schramowski et al. (2022) for ho-\nmogeneous moral inference, where we probe\nEPLM’s moral knowledge about a topic without\nincorporating the cultural factor (i.e., the country\nnames). Similar to that work, we use SBERT\nthrough bert-large-nli-mean-tokens sentence\ntransformer model and use topic and topic-country\npairs as our prompts. 4 This model is built on\ntop of the BERT model, which is pre-trained on\nBOOKS CORPUS (Zhu et al., 2015) and Wikipedia.\n3.1 Autoregressive EPLMs\nSince the MORAL DIRECTION is constructed\nfrom the semantic space of the BERT-based\nEPLMs (Schramowski et al., 2022), we develop\na novel approach to probe autoregressive state-of-\nthe-art EPLMs, GPT2 (Radford et al., 2019) and\nGPT3 (Brown et al., 2020). For each topic or topic-\ncountry pair, we construct the input sas “In [Coun-\ntry] [Topic]”. We then append a pair of opposing\nmoral judgments to sand represent them formally\nas (s+,s−). For example, for s = “In [Country]\ngetting a divorce”, and (always justiﬁable, never\njustiﬁable) as the moral judgment pair, s+ and s−\nwould be “In [Country] getting a divorce is always\n3We replace [Country] with a country’s name.\n4We make our code and data available on https://\ngithub.com/AidaRamezani/cultural_inference.\n430\njustiﬁable” and “In [Country] getting a divorce is\nnever justiﬁable” respectively.5 To make our prob-\ning robust to the choice of moral judgments, we\nuse a set of K = 5prompt pairs (i.e.,{(always jus-\ntiﬁable, never justiﬁable), (morally good, morally\nbad), (right, wrong), (ethically right, ethically\nwrong), (ethical, unethical) }), and refer to ap-\npended input pairs as (s+\ni ,s−\ni ) where i ∈ [K].\nSince GPT2 and GPT3 are composed of decoder\nblocks in the transformer architecture (Vaswani\net al., 2017), we use the probabilities of the last\ntoken in s+\ni , and s−\ni as a moral score for each. The\nmoral score of the pair (s+\ni ,s−\ni ) is the difference\nbetween the log probabilities of its positive and\nnegative statements.\nMS(s+\ni ,s−\ni ) = logP(s+\niT |s+\ni<T )\nP(s−\niT |s−\ni<T ) (1)\nHere s+\niT and s−\niT are the last tokens ins+\ni and s−\ni re-\nspectively, and their probabilities can be estimated\nby the softmax layer in autoregressive EPLMs.\nWe take an average of the estimated moral scores\nfor all K pair statements to compute the moral\nscore of the input.\nMS(s) = 1\nK\nK∑\ni=1\nMS(s+\ni ,s−\ni ) (2)\nTo construct the baseline, we compute the homo-\ngeneous moral score of a topic without specifying\nthe country in the prompts. Using prompt pairs al-\nlows us to operationalize moral polarity: a positive\nmoral score indicates that on average the EPLM\nis more likely to generate positive moral judgment\nfor input s, compared to negative moral judgment.\nWe use GPT2 (117M parameters), GPT2-\nMEDIUM (345M parameters), GPT2-LARGE\n(774M parameters), and GPT3 (denoted as GPT3-\nPROBS , 175B parameters)6. GPT2 is trained on\nWEBTEXT , which is a dataset of webpages and con-\ntains very few non-English samples. Around 82%\nof the pre-training data forGPT3 comes from Com-\nmon Crawl data and WEBTEXT 2 (Kaplan et al.,\n2020), an extended version of WEBTEXT (Radford\net al., 2019). Around 7% of the training corpus\n5We also try probing with the template s = “People in\n[Country] believe [Topic]”, but the results do not improve, so\nwe report the most optimal prompts in the main text, and the\nrest are shown in Appendix C.\n6We access GPT2 through transformer package pro-\nvided by Huggingface. We access GPT3 through OpenAI\nAPI of text-davinci-002 engine with a temperature of 0.6\nfor text generation.\nof GPT3 is non-English text. Considering such\ndata shift from books and articles in BERT to web-\npages in GPT2 and GPT3 in astronomical sizes, it\nis interesting to observe how cultural moral norms\nwould be captured by EPLMs trained on webpages,\nwhich cover a more heterogeneous set of contents\nand authors.\nWe also design multiple-choice question\nprompts to leverage the question-answering capa-\nbilities of GPT3 (denoted as GPT3-QA). Similar\nto the wording used in our ground-truth survey\ndatasets, questions are followed by three options\neach describing a degree of moral acceptability. We\nrepeat this question-answering process 5 times for\neach topic-country pair and take the average of the\nmodel responses. Table 2 in the Appendix shows\nour prompts for all models.\n4 Datasets\nWe describe two open survey data that record moral\nnorms across cultures over a variety of topics.\n4.1 World Values Survey\nThe Ethical Values section in World Values Survey\nWave 7 (WVS for short) is our primary dataset.\nThis wave covers the span of 2017-2021 and is\npublicly available (Haerpfer et al., 2021). In the\nEthical Values section, participants from 55 coun-\ntries were surveyed regarding their opinions on\n19 morally-related topics. The questionnaire was\ntranslated into the ﬁrst languages spoken in each\ncountry and had multiple options. We normalized\nthe options to range from −1 to 1, with −1 rep-\nresenting “never justiﬁable” and 1 “always justiﬁ-\nable”. The moral rating of each country on each\ntopic (i.e., topic-country pair) would then be the\naverage of the participant’s responses.\n4.2 PEW 2013 global attitude survey\nWe use a secondary dataset from PEW Research\nCenter (Research Center, 2014) based on a public\nsurvey in 2013 that studied global moral attitudes\nin 40 countries toward eight morally-related top-\nics (PEW for short). 100 people from each coun-\ntry participated in the survey. The questions were\nasked in English and had three options represent-\ning “morally acceptable”, “not a moral issue”, and\n“morally unacceptable”. We normalized these rat-\nings to be in the range of −1 to 1 and represented\neach topic-country pair by taking an expected value\nof all the responses.\n431\n4.3 Homogeneous moral norms\nWe also use the data from the global user study\nin Schramowski et al. (2022) which were col-\nlected via Amazon MTurk from English speakers.\nThis dataset contains 234 participants’ aggregated\nratings of moral norms used for identifying the\nMORAL DIRECTION . Around half of the partic-\nipants are from North America and Europe. We\nrefer to this dataset as “Homogeneous norms” since\nit does not contain information about moral norms\nacross cultures.\n5 Evaluation and results\nWe evaluate EPLMs’ moral knowledge with respect\nto 1) homogeneous moral norms, 2) ﬁne-grained\nmoral norms across cultures, and 3) cultural diver-\nsities and shared tendencies on moral judgment of\ndifferent topics.\n5.1 Homogeneous moral norm inference\nFor homogeneous moral norm inference, we com-\npute Pearson correlation between 1) the empiri-\ncal homogeneous moral ratings, obtained by ag-\ngregating the human moral ratings toward a topic\nfrom all countries, and 2) language model inferred\nmoral scores, estimated from our homogeneous\nprobing method (i.e., without specifying country in\nprompts).\nFigure 2 shows the results on World Values Sur-\nvey (n= 1,028), PEW survey (n= 312), and the\nHomogeneous norms datasets (n= 100). The high\ncorrelation of GPT2 and GPT3 moral scores with\nthe Homogeneous norms dataset indicate that our\nmethodology does indeed capture the embedded\nmoral biases in these models, with similar perfor-\nmance to the method proposed by Schramowski\net al. (2022) for SBERT (r = 0.79), and higher\nfor GPT3-PROBS (r= 0.85). The moral norms\nin this dataset are typically more globally agree-\nable (e.g., You should not kill people) than topics\nin WVS and PEW. As expected, EPLMs are less\ncorrelated with WVS and PEW, since their moral\nbiases are derived from pre-training on English and\nwesternized data. Aggregated ratings in WVS and\nPEW, however, capture a more global view toward\nmoral issues, which are also morally contentious\n(e.g., “getting a divorce”). Table 3 in Appendix\nincludes the values for this experiment.\nFigure 2: Performance of EPLMs (without cultural\nprompts) on inferring 1) English moral norms, and\n2) culturally diverse moral norms recorded in World\nValues Survey and PEW survey data. The asterisks\nindicate the signiﬁcance levels (“*”, “**”, “***” for\np< 0.05,0.01,0.001 respectively).\n5.2 Fine-grained cultural variation of moral\nnorms toward different topics\nGoing beyond probing EPLMs for their general\nknowledge of moral norms, we assess whether they\ncan accurately identify the moral norms of different\ncultures (level 1 analysis). Using our ﬁne-grained\nprobing approach described in Section 3, we com-\npute Pearson correlation between EPLMs’ moral\nscores and the ﬁne-grained moral ratings from the\nground truth. Each sample pair in the correlation\ntest corresponds to 1) the moral norms estimated\nby EPLMs for a country cand a topic t, and 2) the\nempirical average of moral ratings toward topic t\nfrom all the participants in the country c.\nFigure 3 summarizes the results for SBERT,\nGPT2-LARGE , and GPT3-PROBS models, and\nthe rest of the models are shown in Figure 7 in the\nAppendix. To facilitate direct comparison, the es-\ntimated moral scores are normalized to a range of\n−1 to 1, where −1, 0, and 1 indicate morally nega-\ntive, morally neutral, and morally positive norms,\nrespectively. GPT3-QA and GPT3-PROBS both\nshow a relatively high correlation with the cultural\nvariations of moral norms (r= 0.352, r= 0.411,\np< 0.001, for both), andGPT2-LARGE achieves\na correlation of r = 0.207 (p <0.001) in WVS\nwhere n = 1,028. The correlations are rela-\ntively better for PEW (n = 312) with r = 0.657,\nr= 0.503, and r= 0.468 for GPT3-QA, GPT3-\nPROBS and GPT2-LARGE respectively. These\nresults show that EPLMs have captured some\n432\nknowledge about the moral norms of different cul-\ntures, but with much less accuracy (especially for\nGPT2 and SBERT ) compared to their inference of\nEnglish moral norms shown in the previous analy-\nsis.\nIn addition, we check whether GPT3 ’s high cor-\nrelation with PEW is because it has seen and mem-\norized the empirical data. Our investigation shows\nthat GPT3 has seen the data during pre-training,\nas it can generate the sentences used on the survey\nwebsite. However, the scores suggested by GPT3\ntext generation and the countries’ rankings based\non their ratings are different from the ground truth\ndata.\n5.3 Culture clustering through ﬁne-grained\nmoral inference\nEPLMs’ ﬁne-grained knowledge of moral norms,\ninspected in the previous experiment, might be\nmore accuracte for western cultures than other cul-\ntures. We investigate this claim by clustering coun-\ntries based on 1) their Western-Eastern economic\nstatus (i.e., Rich West grouping)7, and 2) their con-\ntinent (i.e., geographical grouping). We repeat the\nexperiments in the previous section for different\ncountry groups. The results are shown in Figure 4.\nWe also try sampling the same number of countries\nin each group. The results remain robust and are\nillustrated in Appendix-F.\nOur ﬁndings indicate that EPLMs contain more\nknowledge about moral norms of the Rich West\ncountries as opposed to non-western and non-rich\ncountries. Similarly, EPLMs have captured a more\naccurate estimation of the moral norms in countries\nlocated in Oceania, North America, and Europe, as\nopposed to African, Asian, and South American\ncountries. The empirical moral norm ratings from\nEuropean countries in WVS are highly aligned with\nNorth American countries (r= 0.938), which ex-\nplains why their moral norms are inferred more\naccurately than non-English speaking countries.\nNext, for each topic, we compare the z-scores of\nthe empirical moral ratings with the z-scores of the\nGPT3-PROBS inferred moral scores, using Mann-\nWhitney U rank test. The results reveal that “abor-\ntion”, “suicide”, “euthanasia”, “for a man to beat\nhis wife”, “parents beating children”, “having ca-\nsual sex”, “political violence”, and “death penalty”\nin non-western and non-rich countries are all en-\n7https://worldpopulationreview.com/\ncountry-rankings/western-countries\ncoded as more morally appropriate than the actual\ndata. Such misrepresentations of moral norms in\nthese countries could lead to stereotypical content\ngeneration. We also ﬁnd that For Rich West coun-\ntries, “homosexuality”, “divorce”, and “sex before\nmarriage” are encoded as more morally inappro-\npriate than the ground truth, ( p <0.001 for all,\nBonferroni corrected). Such underlying moral bi-\nases, speciﬁcally toward “homosexuality” might\nstimulate the generation of harmful content and\nstigmatization of members of LGBTQ+, which has\nbeen reported in BERT-based EPLMs (Nozza et al.,\n2022). The results for the rest of the models are\nsimilar and are shown in Table 6 in the Appendix.\nOur method of clustering countries is simplis-\ntic and may overlook things such as the signiﬁ-\ncant diversity in religious beliefs within the Non-\nRich-West category, and thus it does not reﬂect\nthe nuanced biases that models may possess when\nit comes to moral norms inﬂuenced by different\nreligious traditions. Nonetheless, our approach\nstill serves as a valuable starting point for studying\nEPLM’s moral biases towards more ﬁne-grained\nreligious and ethnic communities.\n5.4 Cultural diversities and shared\ntendencies over the morality of different\ntopics\nWe next investigate whether EPLMs have captured\nthe cultural diversities and shared tendencies over\nthe morality of different topics (level 2 analysis).\nFor example, people across cultures tend to dis-\nagree more about “divorce” than about “violence\nagainst other people” as depicted in Figure 1. Such\ncultural diversities for each topic can be measured\nby taking the standard deviation of the empiri-\ncal moral ratings across different countries. The\nEPLMs’ inferred cultural diversities can similarly\nbe measured by taking the standard deviation of the\nestimated ﬁne-grained moral scores for different\ncountries. We then quantify the alignment between\nthe two using Pearson correlation.\nFigure 5 shows the results for SBERT, GPT2-\nLARGE, GPT3-PROBS , and the rest are shown\nin Figure 8 in the Appendix. None of the correla-\ntions with the PEW survey were signiﬁcant. For\nWVS, SBERT, GPT2 and GPT2-MEDIUM ex-\nhibited a signiﬁcant correlation (p <0.001) with\nr= 0.618, r= 0.579, and r= 0.734 respectively.\nThe results for GPT3 are insigniﬁcant, suggesting\nthat it is more challenging to correctly estimate\n433\nPEW survey\nWorld Value Survey\nFigure 3: Degree of alignment between the moral scores from EPLMs and ﬁne-grained empirical moral ratings\nfor different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each\ndot represents a topic-country pair. The x-axis shows the ﬁne-grained moral ratings from the ground truth and the\ny-axis shows the corresponding inferred moral scores. The legends display the moral topics in the surveys. Similar\ntopics in the World Value Surveys are shown with the same color.\nFigure 4: Correlation between language-model inferred moral scores and empirical moral ratings from World\nValues Survey, analyzed in different clusters of countries in Rich West grouping (left) and continent grouping\n(right). The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” forp< 0.05,0.01,0.001 respectively).\ncultural controversies of topics for GPT3 . For ex-\nample, stealing property is incorrectly estimated to\nbe more controversial than abortion.\n6 Fine-tuning language models on global\nsurveys\nFinally, we explore the utility-bias trade-off in en-\ncoding cultural moral knowledge into EPLMs by\nﬁne-tuning them on cross-cultural surveys. The\nutility comes from increasing the cultural moral\nknowledge in these models, and the bias denotes\ntheir decreased ability to infer English moral norms,\nin addition to the cultural moral biases introduced\nto the model. We run our experiments on GPT2 ,\nwhich our results suggest having captured mini-\nmum information about cultural moral norms com-\npared to other autoregressive models.\nTo ﬁne-tune the model, for each participant\nfrom [Country] with [Moral rating] toward\n[Topic], we designed a prompt with the structure\n“A person in [Country] believes [Topic]\nis [Moral rating] .”. We used the surveys’\nwordings for [Moral rating] . Table 8 in the\nAppendix shows our prompts for WVS and PEW.\nThese prompts constructed our data for ﬁne-tuning,\nduring which we maximize the probability of the\nnext token. The ﬁne-tuned models were evaluated\non the same correlation tests introduced in the pre-\nvious Sections 5.2, 5.3, and 5.4.\nThe ﬁne-tuning data was partitioned into training\nand evaluation sets using different strategies (i.e.,\nRandom, Country-based, and Topic-based). For the\nRandom strategy, we randomly selected80% of the\nﬁne-tuning data for training the model. The topic-\ncountry pairs not seen in the training data com-\nposed the evaluation set. For our Country-based\nand Topic-based strategies, we randomly removed\n20% of the countries ( n = 11 for WVS, n = 8\nfor PEW) and topics (n= 4for WVS, n= 2for\nPEW) from the training data to compose the evalu-\n434\nPEW survey\nWorld Values Survey\nEstimated degree of cultural\ndiversity\nEmpirical degree of cultural diversity\nFigure 5: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral\nratings and language-model inferred moral scores. Each dot corresponds to a moral topic. The numerical indices\nare consistent with the legend indices in Table 5. The x-axis shows the empirical standard deviations in moral\nratings across countries and the y-axis shows the standard deviations from the model-inferred moral scores.\nTrain data Data partition strategy Evaluation Performance on the\nHomogeneous norms\nWVS\nRandom 0.832∗∗∗ ↑ (0.271∗∗∗) 0.71∗∗∗ ↓\n(0.80∗∗∗)\nCountry-based 0.759∗∗∗ ↑ (0.225∗∗) 0.72∗∗∗ ↓\nTopic-based 0.508∗∗∗ ↑ (0.286∗∗∗) 0.70∗∗∗ ↓\nPEW\nRandom 0.818∗∗∗ ↑ (0.204, n.s.) 0.64∗∗∗ ↓\nCountry-based 0.764∗∗∗ ↑ (0.055, n.s.) 0.67∗∗∗ ↓\nTopic-based 0.733∗∗∗ ↑ (−0.146, n.s.) 0.61∗∗∗ ↓\nTable 1: Summary of ﬁne-tuned GPT2 language model performance on inferring moral norms across cultures\nand the degradation of its performance on inferring Homogeneous moral norms. Values in parentheses show the\nperformance before ﬁne-tuning. The arrows and colors show performance increase (blue, ↑) and decrease (red, ↓)\nafter ﬁne-tuning. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” forp< 0.05,0.01,0.001).\nation set. See Appendix G for the total number of\nsamples.\nTable 1 shows the gained utilities, that is the cor-\nrelation test results between the ﬁne-grained moral\nscores inferred by the ﬁne-tuned models and the\nempirical ﬁne-grained moral ratings. All ﬁne-tuned\nmodels align better with the ground truth than the\npre-trained-only models (i.e., the values in paren-\ntheses). For both WVS and PEW, the Random strat-\negy is indeed the best as each country and topic are\nseen in the training data at least once (but may not\nappear together as a pair). The ﬁne-tuned models\ncan also generalize their moral scores to unseen\ncountries and topics. Repeating the experiment in\nSection 5.4 also shows substantial improvement in\nidentifying cultural diversities of different topics by\nall ﬁne-tuned models. For example, the WVS and\nPEW-trained models with Random strategy gain\nPearson’s r values of0.893, and 0.944 respectively.\nThe results for the rest of the models are shown in\nTable 7 in the Appendix.\nNevertheless, the bias introduced during the\nﬁne-tuning decreases the performance on the Ho-\nmogeneous norms dataset. This observation dis-\nplays a trade-off between cultural and homoge-\nneous moral representations in language models.\nMoreover, injecting the cross-cultural surveys into\nEPLMs might introduce additional social biases\nto the model that are captured through these sur-\nveys (Joseph and Morgan, 2020).\nIn addition, we probe the best ﬁne-tuned model\n(i.e., WVS with Random strategy) on its ability to\ncapture the moral norms of non-western cultures\nby repeating the experiment in Section 5.3. The\nresults in Figure 4 show that the ﬁne-tuned GPT2\nperforms the best for all country groups. There\nis still a gap between western and non-western\ncountries. However, basic ﬁne-tuning proves to be\neffective in adapting EPLMs to the ground truth.\n435\n7 Discussion and conclusion\nWe investigated whether English pre-trained lan-\nguage models contain knowledge about moral\nnorms across many different cultures. Our analyses\nshow that large EPLMs capture moral norm vari-\nation to a certain degree, with the inferred norms\nbeing predominantly more accurate in western cul-\ntures than non-western cultures. Our ﬁne-tuning\nanalysis further suggests that EPLMs’ cultural\nmoral knowledge can be improved using global\nsurveys of moral norms, although this strategy re-\nduces the capacity to estimate the English moral\nnorms and potentially introduces new biases into\nthe model. Given the increasing use of EPLMs\nin multicultural environments, our work highlights\nthe importance of cultural diversity in automated in-\nference of moral norms. Even when an action such\nas “political violence” is assessed by an EPLM\nas morally inappropriate in a homogeneous set-\nting, the same issue may be inferred as morally\nappropriate for underrepresented cultures in these\nlarge language models. Future work can explore\nalternative and richer representations of cultural\nmoral norms that go beyond the point estimation\nwe presented here and investigate how those repre-\nsentations might better capture culturally diverse\nmoral views.\nLimitations\nAlthough our datasets are publicly available and\ngathered from participants in different countries,\nthey cannot entirely represent the moral norms\nfrom all the individuals in different cultures over\nthe world or predict how moral norms might change\ninto the future (Bloom, 2010; Bicchieri, 2005). Ad-\nditionally, we examine a limited set of moral issues\nfor each country, therefore the current experiments\nshould not be regarded as comprehensive of the\nspace of moral issues that people might encounter\nin different countries.\nMoreover, taking the average of moral ratings for\neach culture is a limitation of our work and reduces\nthe natural distribution of moral values in a culture\nto a single point (Talat et al., 2021). Implementing\na framework that incorporates both within-country\nvariation and temporal moral variation (Xie et al.,\n2019) is a potential future research direction.\nCurrently, it is not clear whether the differ-\nence between EPLMs’ estimated moral norms and\nthe empirical moral ratings is due to the lack of\ncultural moral norms in the pre-training data, or\nthat the cultural moral norms mentioned in the\npre-training data represent the perspective of an\nEnglish-speaking person of another country. For\nexample, a person from the United States could\nwrite about the moral norms in another country\nfrom a western perspective. A person from a non-\nwestern country could also write about their own\nmoral views using English. These two cases have\ndifferent implications and introduce different moral\nbiases into the system.\nPotential risks\nWe believe that the language models should not be\nused to prescribe ethics, and here we approach the\nmoral norm inference problem from a descriptive\nperspective. However, we acknowledge modify-\ning prompts could lead language models to gen-\nerate ethical prescriptions for different cultures.\nAdditionally, our ﬁne-tuning approach could be\nexploited to implant cultural stereotypical biases\ninto these models.\nMany topics shown in this work might be sen-\nsitive to some people yet more tolerable to some\nother people. Throughout the paper, we tried to em-\nphasize that none of the moral norms, coming from\neither the models’ estimation or the empirical data,\nshould be regarded as deﬁnitive values of right and\nwrong, and the moral judgments analyzed in this\nwork do not reﬂect the opinions of the authors.\nAcknowledgements\nThis work was supported by a SSHRC Insight\nGrant 435190272.\n436\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou.\n2021. Persistent anti-muslim bias in large language\nmodels. In Proceedings of the 2021 AAAI/ACM\nConference on AI, Ethics, and Society , AIES ’21,\npage 298–306, New York, NY , USA. Association for\nComputing Machinery.\nPrithviraj Ammanabrolu, Liwei Jiang, Maarten Sap,\nHannaneh Hajishirzi, and Yejin Choi. 2022. Align-\ning to social norms and values in interactive narra-\ntives. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 5994–6017, Seattle, United States. Asso-\nciation for Computational Linguistics.\nArnav Arora, Lucie-Aimée Kaffee, and Isabelle Augen-\nstein. 2022. Probing Pre-Trained Language Mod-\nels for Cross-Cultural Differences in Values. arXiv\npreprint arXiv:2203.13722.\nMohammad Atari, Jonathan Haidt, Jesse Graham, Sena\nKoleva, Sean Stevens, and Morteza Dehghani. 2022.\nMorality Beyond the WEIRD: How the Nomologi-\ncal Network of Morality Varies Across Cultures.\nEdmond Awad, Sohan Dsouza, Richard Kim, Jonathan\nSchulz, Joseph Henrich, Azim Shariff, Jean-\nFrançois Bonnefon, and Iyad Rahwan. 2018. The\nMoral Machine experiment. Nature, 563(7729):59–\n64.\nEdmond Awad, Sohan Dsouza, Azim Shariff, Iyad Rah-\nwan, and Jean-François Bonnefon. 2020. Universals\nand variations in moral decisions made in 42 coun-\ntries by 70,000 participants. Proceedings of the Na-\ntional Academy of Sciences, 117(5):2332–2337.\nCristina Bicchieri. 2005. The grammar of society: The\nnature and dynamics of social norms . Cambridge\nUniversity Press.\nPaul Bloom. 2010. How do morals change? Nature,\n464(7288):490–490.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDanica Dillion, Niket Tandon, Yuling Gu, and Kurt\nGray. 2023. Can ai language models replace human\nparticipants? Trends in Cognitive Sciences.\nDenis Emelin, Ronan Le Bras, Jena D. Hwang,\nMaxwell Forbes, and Yejin Choi. 2021. Moral sto-\nries: Situated reasoning about norms, intents, ac-\ntions, and their consequences. In Proceedings of\nthe 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 698–718, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social Chem-\nistry 101: Learning to Reason about Social and\nMoral Norms. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 653–670, Online. As-\nsociation for Computational Linguistics.\nJesse Graham, Jonathan Haidt, Sena Koleva, Matt\nMotyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto.\n2013. Moral Foundations Theory: The Pragmatic\nValidity of Moral Pluralism. In Advances in Experi-\nmental Social Psychology, volume 47, pages 55–130.\nElsevier.\nChristian Haerpfer, Ronald Inglehart, Alejandro\nMoreno, Christian Welzel, Kseniya Kizilova,\nJaime Diez-Medrano, Marta Lagos, Pippa Norris,\nE Ponarin, and B Puranen. 2021. World Values\nSurvey: Round Seven – Country-Pooled Dataﬁle.\nMadrid, Spain & Vienna, Austria: JD Systems In-\nstitute & WVSA Secretariat. Data File Version, 2(0).\nJonathan Haidt, Silvia Helena Koller, and Maria G\nDias. 1993. Affect, culture, and morality, or is it\nwrong to eat your dog? Journal of personality and\nsocial psychology, 65(4):613.\nKatharina Hämmerl, Björn Deiseroth, Patrick\nSchramowski, Jind ˇrich Libovick `y, Alexander\nFraser, and Kristian Kersting. 2022. Do Multilin-\ngual Language Models Capture Differing Moral\nNorms? arXiv preprint arXiv:2203.09904.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021. Aligning AI With Shared Human Values. In\nInternational Conference on Learning Representa-\ntions.\nKathryn Iurino and Gerard Saucier. 2020. Testing\nmeasurement invariance of the Moral Foundations\nQuestionnaire across 27 countries. Assessment,\n27(2):365–372.\nLiwei Jiang, Jena D. Hwang, Chandrasekhar Bhagavat-\nula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt,\nJenny Liang, Oren Etzioni, Maarten Sap, and Yejin\nChoi. 2021. Delphi: Towards Machine Ethics and\nNorms. ArXiv, abs/2110.07574.\nKenneth Joseph and Jonathan Morgan. 2020. When do\nword embeddings accurately reﬂect surveys on our\nbeliefs about people? In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 4392–4415, Online. Association\nfor Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n437\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nRuibo Liu, Ge Zhang, Xinyu Feng, and Soroush\nV osoughi. 2022. Aligning Generative Language\nModels with Human Values. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022, pages 241–252.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nNicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021.\nScruples: A corpus of community ethical judgments\non 32,000 real-life anecdotes. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 35, pages 13470–13479.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pre-\ntrained language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5356–5371, Online. As-\nsociation for Computational Linguistics.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2021.\nHONEST: Measuring hurtful sentence completion\nin language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2398–2406, Online.\nAssociation for Computational Linguistics.\nDebora Nozza, Federico Bianchi, Anne Lauscher, and\nDirk Hovy. 2022. Measuring harmful sentence com-\npletion in language models for LGBTQIA+ individu-\nals. In Proceedings of the Second Workshop on Lan-\nguage Technology for Equality, Diversity and Inclu-\nsion, pages 26–34, Dublin, Ireland. Association for\nComputational Linguistics.\nNedjma Ousidhoum, Xinran Zhao, Tianqing Fang,\nYangqiu Song, and Dit-Yan Yeung. 2021. Probing\ntoxic content in large pre-trained language models.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4262–4274, Online. Association for Computational\nLinguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence Embeddings using Siamese BERT-\nNetworks. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning. Association for Computational Linguistics.\nPEW Research Center. 2014. Global Attitudes survey.\nWashington, D.C.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Ju-\nrafsky, Noah A. Smith, and Yejin Choi. 2020. So-\ncial bias frames: Reasoning about social and power\nimplications of language. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5477–5490, Online. As-\nsociation for Computational Linguistics.\nPatrick Schramowski, Cigdem Turan, Nico Andersen,\nConstantin A Rothkopf, and Kristian Kersting. 2022.\nLarge pre-trained language models contain human-\nlike biases of what is right and wrong to do. Nature\nMachine Intelligence, 4(3):258–268.\nZeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira\nGanesh, Ryan Cotterell, and Adina Williams. 2021.\nA Word on Machine Ethics: A Response to Jiang et\nal.(2021). arXiv preprint arXiv:2111.04158.\nSamia Touileb, Lilja Øvrelid, and Erik Velldal. 2022.\nOccupational biases in Norwegian and multilingual\nlanguage models. In Proceedings of the 4th Work-\nshop on Gender Bias in Natural Language Process-\ning (GeBNLP), pages 200–211, Seattle, Washington.\nAssociation for Computational Linguistics.\nJackson Trager, Alireza S Ziabari, Aida Mostafazadeh\nDavani, Preni Golazazian, Farzan Karimi-\nMalekabadi, Ali Omrani, Zhihe Li, Brendan\nKennedy, Nils Karl Reimer, Melissa Reyes, et al.\n2022. The Moral Foundations Reddit Corpus. arXiv\npreprint arXiv:2208.05545.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. NIPS’17, page 6000–6010, Red Hook,\nNY , USA. Curran Associates Inc.\nJing Yi Xie, Renato Ferreira Pinto Junior, Graeme\nHirst, and Yang Xu. 2019. Text-based inference\nof moral sentiment change. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4654–4663, Hong Kong,\nChina. Association for Computational Linguistics.\nDa Yin, Hritik Bansal, Masoud Monajatipoor, Liu-\nnian Harold Li, and Kai-Wei Chang. 2022. Geom-\nlama: Geo-diverse commonsense probing on multi-\nlingual pre-trained language models. arXiv preprint\narXiv:2205.12247.\n438\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Rus-\nlan Salakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Aligning Books and\nMovies: Towards Story-like Visual Explanations\nby Watching Movies and Reading Books. CoRR,\nabs/1506.06724.\nA Data license\nBoth World Values Survey and PEW survey\nare publicly available to use for research pur-\nposes. We accept and follow the terms and\nconditions for using these datasets, which can\nbe found in https://www.worldvaluessurvey.\norg/WVSContents.jsp?CMSID=Documentation,\nand https://www.pewresearch.org/about/\nterms-and-conditions/.\nB Comparison of human-rated and\nmachine-scored moral norms\nFigure 6 shows the comparison between human-\nrated moral norms in PEW, and the moral scores\ninferred by SBERT (Reimers and Gurevych, 2019).\nC Probing experiments\nTable 2 shows our prompt design for probing ﬁne-\ngrained moral norms in EPLMs. As mentioned in\nthe main text, we repeat our probing experiment for\nGPT2 models and GPT3-PROBS with another\ntemplate “People in [Country] believe [Topic] is\n[Moral Judgment]”. The results are substantially\nworse than our initial template, suggesting that ex-\ntracting the moral knowledge in language models\nis sensitive to the wording used in the input. The re-\nsults for the ﬁne-grained analysis (level 1 analysis)\nand the cultural diversities and shared tendencies\n(level 2 analysis) with this template are shown in\nTable 4.\nIn all experiments, we used a single NVIDIA\nTITAN V GPU. Each probing experiment took ap-\nproximately 1 hour to complete.\nD Homogeneous moral norm inference\nTable 3 shows the detailed values of the correlation\ntests in our homogeneous moral norm inference\nexperiment.\nE Fine-grained cultural variation of\nmoral norm\nFigure 7 and Figure 8 show the result of our ﬁne-\ngrained cultural moral inference, and inference of\ncultural diversities and shared tendencies respec-\ntively for GPT2, GPT2-MEDIUM , and GPT3-\nQA. The numerical indices in Figure 8 are consis-\ntent with the indices in Table 5.\nF Sampling for cultural clusters\nSince in section 5.3 there are a different number of\ncountries in each group, we redo the experiment by\nrandomly sampling the same number of countries\n(n= 11for Rich West grouping, n= 5for conti-\nnent grouping) and repeating the sampling process\nfor 50 times. The results and the general pattern\nremain the same and are depicted in Figure 9.\nG Details of ﬁne-tuning on global surveys\nTable 8 shows the Moral rating in our prompt\ndesign for constructing our ﬁne-tuning dataset. For\nexample, The World Value Survey represents the\ntwo ends of the ratings scale where 1 is “Never\njustiﬁable” and 10 is “Always justiﬁable”. The\noptions in between are presented to the participants\nin a 10-point scale. Therefore, we mapped these\noptions to different prompts that are semantically\nsimilar and in between the two ends. For example,\nif a participant from the United States ratedstealing\nproperty as 2, which is slightly more positive than\nthe ﬁrst option (“Never justiﬁable”), we mapped\nthis rating to “not justiﬁable”, creating the prompt\n“A person in the United States believes\nstealing property is not justifiable.” for\nour ﬁne-tuning data.\nSince there are a different number of participants\nfrom each country, in order to balance this dataset,\nwe randomly select 100 samples for each topic-\ncountry pair and removed the rest of the utterances\nfrom the training data. We ﬁne-tuned GPT2 on one\nepoch, with a batch size of8, learning rate of5e−5,\nand weight decay of 0.01. The number of training\nand evaluation samples for all data partition strate-\ngies are shown in Table 9. In all experiments, we\nused a single NVIDIA TITAN V GPU. Fine-tuning\nand evaluation took approximately 2 hours to com-\nplete for each model.\n439\nFigure 6: Comparison of human-rated and machine-scored moral norms across cultures. Left: Boxplots of human\nratings of moral norms across cultures in the PEW survey. Each dot represents the empirical average of participants’\nratings for a morally relevant topic (e.g., “having an abortion”) within a country. Right: Corresponding moral\nscores estimated by a language model (Sentence-BERT) (Reimers and Gurevych, 2019). Each dot represents the\nmoral score obtained by probing the language model in a given country.\nModel Prompt\nSentence-BERT [Topic] in [Country].\nGPT2 models and GPT3-PROBSIn [Country] [Topic] is [Moral judgement].\nPeople in [Country] believe [Topic] is [Moral judgement].\nGPT3-QA (for PEW)\nDo people in [Country] believe that [Topic] is:\n1) Morally acceptable\n2) Not a moral issue\n3) Morally unacceptable.\nGPT3-QA (for WVS)\nDo people in [Country] believe that [Topic] is:\n1) Always Justifiable\n2) Something in between\n3) Never justifiable.\nTable 2: Prompting design used for estimating the ﬁne-grained moral norms in different language models. In our\nhomogeneous norm inference, we remove “In [country]” from the prompts.\nModel World Values Survey(n= 1,028) PEW survey (n= 312) Homogeneous norms (n= 100)\nSBERT 0.210∗∗∗ −0.038(n.s.) 0.79∗∗∗\nGPT2 0.176∗∗∗ −0.069(n.s.) 0.80∗∗∗\nGPT2-MEDIUM 0.181∗∗∗ 0.033(n.s.) 0.79∗∗∗\nGPT2-LARGE 0.226∗∗∗ 0.157(n.s.) 0.76∗∗∗\nGPT3-QA 0.330∗∗∗ 0.391∗∗∗ 0.79∗∗∗\nGPT3-PROBS 0.346∗∗∗ 0.340∗∗∗ 0.85∗∗∗\nTable 3: Performance of pre-trained language models (without cultural prompts) on inferring 1) homogeneous\nwesternized moral norms, and 2) culturally diverse moral norms recorded in World Values Survey and PEW survey\ndata.\n440\nPEW survey\nWorld Value Survey\nFigure 7: Degree of alignment between the moral scores from EPLMs and ﬁne-grained empirical moral ratings\nfor different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each\ndot represents a topic-country pair. The x-axis shows the ﬁne-grained moral ratings from the ground truth and the\ny-axis shows the corresponding inferred moral scores. Similar topics in the World Value Surveys are shown with\nthe same color.\nPEW survey\nWorld Values Survey\nEstimated degree of cultural\ndiversity\nEmpirical degree of cultural diversity\nFigure 8: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral\nratings and language-model inferred moral scores. Each dot corresponds to a moral topic. The x-axis shows the\nempirical standard deviations in moral ratings across countries and the y-axis shows the standard deviations from\nthe model-inferred moral scores.\nFigure 9: Correlation between language-model inferred moral scores and empirical moral ratings from World\nValues Survey analyzed in different clusters of countries in Rich West grouping (left) and continent grouping\n(right). The results are generated by sampling and the error bars show the conﬁdence intervals with α= 0.05.\n441\nData model Fine-grained evaluation\nof moral norms\nEvaluation on cultural diversity\nand shared tendencies\nWVS\nGPT3-PROBS 0.078∗ −0.176\nGPT2 −0.114∗∗∗ 0.231\nGPT2-MEDIUM −0.261∗∗∗ −0.357\nGPT2-LARGE −0.07∗ −0.356\nPEW\nGPT3-PROBS 0.539∗∗∗ 0.041\nGPT2 0.168∗∗ 0.566\nGPT2-MEDIUM 0.165∗∗ 0.184\nGPT2-LARGE 0.19∗∗∗ 0.542\nTable 4: Performance of pre-trained autoregressive language models on identifying ﬁne-grained moral norms and\ncultural diversities and shared tendencies, using the prompt template “People in [Country] believe [Topic] is [Moral\njudgment]\". The values are Pearson’s correlations. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***”\nfor p< 0.05,0.01,0.001 respectively).\nWorld Values Survey\nIndex Topic\n1 stealing property\n2 euthanasia\n3 sex before marriage\n4 violence against other people\n5 cheating on taxes\n6 avoiding a fare on public transport\n7 abortion\n8 suicide\n9 someone accepting a bribe on a course of their duties\n10 terrorism as a political, ideological, or religious mean\n11 homosexuality\n12 parents beating children\n13 prostitution\n14 divorce\n15 political violence\n16 death penalty\n17 claiming governments beneﬁts to which you are not entitled\n18 for a man to beat his wife\n19 having casual sex\nPEW survey\n1 using contraceptives\n2 getting a divorce\n3 having an abortion\n4 homosexuality\n5 drinking alcohol\n6 married people having an affair\n7 gambling\n8 sex between unmarried adults\nTable 5: Numerical indexing for topics in moral surveys.\n442\nModel Positively evaluated topics\nfor non-rich and non-western countries\nNegatively evaluated topics\nfor Rich-West countries\nSBERT\nsex before marriage∗∗, homosexuality∗∗∗,\nhaving casual sex∗∗∗, abortion∗∗∗,\nprostitution∗∗∗, claiming government\nbeneﬁts to which you are\nnot entitled∗∗∗, someone\naccepting a bribe\nin the course of their duties∗∗∗\nsex before marriage∗∗∗, euthanasia∗∗∗,\ndivorce∗∗∗, death penalty∗∗∗,\nparents beating children∗∗∗\nGPT2\nabortion∗∗∗, prostitution∗∗∗,\nsuicide∗∗∗, avoiding a fare on\npublic transport∗∗∗,\nsomeone accepting a bribe\nin the course of their duties∗∗∗,\nterrorism as a political,\nideological or religious mean∗∗∗,\npolitical violence∗∗∗,\nviolence against other people∗∗∗\nsex before marriage∗∗, homosexuality∗∗,\ndivorce∗∗, having casual sex∗∗,\nclaiming government beneﬁts\nto which you are not entitled∗∗∗\nGPT2-MEDIUM\neuthanasia∗∗∗, abortion∗∗∗, suicide∗∗∗,\navoiding a fare on public transport∗∗∗,\nsomeone accepting a bribe in\nthe course of their duties∗∗∗,\npolitical violence∗∗∗, violence against\nother people∗∗∗, stealing property∗∗∗\nsex before marriage∗∗∗, homosexuality∗∗,\ndivorce∗∗, having casual sex∗∗,\nclaiming government beneﬁts\nto which you are not entitled∗∗∗\nGPT2-LARGE\neuthanasia∗∗∗, having casual sex∗∗∗,\nabortion∗∗∗, prostitution∗∗∗, suicide∗∗∗,\nterrorism as a political,\nideological or religious mean∗∗∗,\npolitical violence∗∗∗,\nviolence against other people∗∗∗\nsex before marriage∗∗∗, homosexuality∗∗,\ndivorce∗∗, claiming government beneﬁts to\nwhich you are not entitled∗∗∗\nGPT3-QA\nhaving casual sex∗∗, abortion∗∗,\navoiding a fare on public transport∗∗∗,\ncheating on taxes∗∗∗,\nsomeone accepting a bribe in the\ncourse of their duties∗∗∗,\npolitical violence∗∗∗\nsex before marriage∗∗∗, divorce∗∗,\ndeath penalty∗∗, prostitution∗∗,\nparents beating children∗∗,\nsuicide∗∗, for a man to beat his wife∗∗∗,\nstealing property∗∗\nGPT3-PROBS\neuthanasia∗∗∗, having casual sex∗∗∗,\nabortion∗∗∗, death penalty∗∗∗,\nsuicide∗∗∗, political violence∗∗∗,\nfor a man to beat his wife∗∗∗\nsex before marriage∗∗∗, homosexuality∗∗∗ ,\ndivorce∗∗\nTable 6: Topics evaluated as morally positive for non-rich and non-western countries and morally negative for\nRich-West countries, in comparison to the ground truth in these countries. In each entry, the topics are sorted from\nthe most controversial (i.e., having the highest degree of cultural diversity) to the least controversial. The asterisks\nindicate the signiﬁcance levels of Mann-Whitney U rank test after Bonferroni p-value correction (“*”, “**”, “***”\nfor p< 0.05,0.01,0.001 respectively).\n443\nTrain data Data partition strategy Evaluation\nWVS\nRandom 0.893∗∗∗ ↑\n(0.579∗∗∗)Country-based 0.894∗∗∗ ↑\nTopic-based 0.835∗∗∗ ↑\nPEW\nRandom 0.944∗∗ ↑\n(n.s.)Country-based 0.839∗ ↑\nTopic-based 0.953∗∗∗ ↑\nTable 7: Summary of ﬁne-tuned GPT2 language model performance in inferring the cultural diversities and shared\ntendencies over the morality of different topics. The arrows and colors show performance increase (blue, ↑) and\ndecrease (red, ↓) after ﬁne-tuning. All values are Pearson’s correlations. The asterisks indicate the signiﬁcance\nlevels (“*”, “**”, “***” forp< 0.05,0.01,0.001 respectively). Non-signiﬁcant results are shown by “n.s.”.\nDataset Rating [Moral rating] in ﬁne-tuning prompts\nWVS\n1 never justifiable\n[2, 3, 4] not justifiable\n[5, 6] somewhat justifiable\n[7, 8, 9] justifiable\n10 always justifiable\nPEW\n1 morally unacceptable\n2 not a moral issue\n3 morally acceptable\nTable 8: Different prompting designs for ﬁne-tuning language models on the global survey datasets.\nData Data partition strategy Training samples Evaluation sample pairs\nWVS\nRandom 82200 206\nCountry-based 82600 202\nTopic-based 81200 216\nPEW\nRandom 24900 63\nCountry-based 24800 64\nTopic-based 23400 78\nTable 9: Number of samples in training and evaluation datasets for ﬁne-tuning GPT2 on global surveys of morality.\n444\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0013 A2. Did you discuss any potential risks of your work?\n8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4, 5, 6\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4, 5, 6\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAll the artifacts we used were available for research purposes. The term of usage can be found in the\nurls provided in the paper in the Appendix.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n8\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe datasets we used do not contain information about individual people.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4, Appendix\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4, 5, 6, appendix\nC □\u0013 Did you run computational experiments?\n5, 6, appendix\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n445\n□\u0017 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe did not do any hyperparameter search.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5, 6, appendix\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n446",
  "topic": "Globe",
  "concepts": [
    {
      "name": "Globe",
      "score": 0.619209349155426
    },
    {
      "name": "Morality",
      "score": 0.5985459089279175
    },
    {
      "name": "Cultural diversity",
      "score": 0.5698481798171997
    },
    {
      "name": "Inference",
      "score": 0.5563805103302002
    },
    {
      "name": "Variation (astronomy)",
      "score": 0.5496039986610413
    },
    {
      "name": "Relevance (law)",
      "score": 0.5075904130935669
    },
    {
      "name": "Psychology",
      "score": 0.47436413168907166
    },
    {
      "name": "Social psychology",
      "score": 0.471659779548645
    },
    {
      "name": "Moral disengagement",
      "score": 0.46781468391418457
    },
    {
      "name": "Sociology",
      "score": 0.4131762981414795
    },
    {
      "name": "Epistemology",
      "score": 0.31784072518348694
    },
    {
      "name": "Computer science",
      "score": 0.25837403535842896
    },
    {
      "name": "Political science",
      "score": 0.18430006504058838
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1740095019340515
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Astrophysics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ]
}