{
    "title": "Complex Dynamic Neurons Improved Spiking Transformer Network for Efficient Automatic Speech Recognition",
    "url": "https://openalex.org/W4382457407",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2101940636",
            "name": "Qingyu Wang",
            "affiliations": [
                "Institute of Automation",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2144743915",
            "name": "Tielin Zhang",
            "affiliations": [
                "Institute of Automation",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2107326892",
            "name": "Minglun Han",
            "affiliations": [
                "Institute of Automation",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2096961312",
            "name": "Yi Wang",
            "affiliations": [
                "Jilin University"
            ]
        },
        {
            "id": "https://openalex.org/A2320125067",
            "name": "Duzhen Zhang",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A2074677540",
            "name": "Bo Xu",
            "affiliations": [
                "Shandong Institute of Automation",
                "University of Chinese Academy of Sciences",
                "Center for Excellence in Brain Science and Intelligence Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2101940636",
            "name": "Qingyu Wang",
            "affiliations": [
                "Shandong Institute of Automation",
                "Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences",
                "Beijing Academy of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2144743915",
            "name": "Tielin Zhang",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Shandong Institute of Automation",
                "University of Chinese Academy of Sciences",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2107326892",
            "name": "Minglun Han",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096961312",
            "name": "Yi Wang",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Shandong Institute of Automation",
                "Jilin University",
                "University of Chinese Academy of Sciences",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2320125067",
            "name": "Duzhen Zhang",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Shandong Institute of Automation",
                "Chinese Academy of Sciences",
                "Beijing Academy of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2074677540",
            "name": "Bo Xu",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Shandong Institute of Automation",
                "University of Chinese Academy of Sciences",
                "Beijing Academy of Artificial Intelligence",
                "Center for Excellence in Brain Science and Intelligence Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3092618752",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2945648280",
        "https://openalex.org/W6654346214",
        "https://openalex.org/W6684272117",
        "https://openalex.org/W4221155295",
        "https://openalex.org/W3022766987",
        "https://openalex.org/W101771737",
        "https://openalex.org/W4206010111",
        "https://openalex.org/W2233370113",
        "https://openalex.org/W3112215519",
        "https://openalex.org/W2076661751",
        "https://openalex.org/W2775079417",
        "https://openalex.org/W2008817973",
        "https://openalex.org/W3093146707",
        "https://openalex.org/W3170350855",
        "https://openalex.org/W3207905039",
        "https://openalex.org/W3092472400",
        "https://openalex.org/W2966081953",
        "https://openalex.org/W4313046728",
        "https://openalex.org/W3166169953",
        "https://openalex.org/W3016167541",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W3137699669",
        "https://openalex.org/W3043133474",
        "https://openalex.org/W2016708835",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2889886075",
        "https://openalex.org/W3033411150",
        "https://openalex.org/W2164653071",
        "https://openalex.org/W3004557030",
        "https://openalex.org/W3203906445"
    ],
    "abstract": "The spiking neural network (SNN) using leaky-integrated-and-fire (LIF) neurons has been commonly used in automatic speech recognition (ASR) tasks. However, the LIF neuron is still relatively simple compared to that in the biological brain. Further research on more types of neurons with different scales of neuronal dynamics is necessary. Here we introduce four types of neuronal dynamics to post-process the sequential patterns generated from the spiking transformer to get the complex dynamic neuron improved spiking transformer neural network (DyTr-SNN). We found that the DyTr-SNN could handle the non-toy automatic speech recognition task well, representing a lower phoneme error rate, lower computational cost, and higher robustness. These results indicate that the further cooperation of SNNs and neural dynamics at the neuron and network scales might have much in store for the future, especially on the ASR tasks.",
    "full_text": "Complex Dynamic Neurons Improved Spiking Transformer Network for Efficient\nAutomatic Speech Recognition\nQingyu Wang1,2*, Tielin Zhang1,2*†, Minglun Han1,2*, Yi Wang4, Duzhen Zhang1,2, Bo Xu1,2,3†\n1 Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China\n4School of Artificial Intelligence, Jilin University, Changchun, China\n{tielin.zhang, xubo}@ia.ac.cn\nAbstract\nThe spiking neural network (SNN) using leaky-integrated-\nand-fire (LIF) neurons has been commonly used in automatic\nspeech recognition (ASR) tasks. However, the LIF neuron is\nstill relatively simple compared to that in the biological brain.\nFurther research on more types of neurons with different\nscales of neuronal dynamics is necessary. Here we introduce\nfour types of neuronal dynamics to post-process the sequen-\ntial patterns generated from the spiking transformer to get\nthe complex dynamic neuron improved spiking transformer\nneural network (DyTr-SNN). We found that the DyTr-SNN\ncould handle the non-toy automatic speech recognition task\nwell, representing a lower phoneme error rate, lower com-\nputational cost, and higher robustness. These results indicate\nthat the further cooperation of SNNs and neural dynamics at\nthe neuron and network scales might have much in store for\nthe future, especially on the ASR tasks.\nIntroduction\nIn recent years, the spiking neural network (SNN) has re-\nceived extensive attention for its remarked lower compu-\ntational cost on various machine learning tasks, includ-\ning but not limited to spatial image classification, tempo-\nral auditory recognition, event-based video recognition, and\nreinforcement-learning based continuous control (Tang et al.\n2021). This progress in SNNs is contributed partly by the\nmathematical optimization algorithms borrowed from the ar-\ntificial neural network (ANN), e.g., the approximate gradi-\nent in backpropagation (BP), various types of loss defini-\ntion and regression configuration, and more importantly, by\nsome key computational modules inspired from the biologi-\ncal brain, e.g., the receptive-field-like convolutional circuits,\nself-organized plasticity propagation (Zhang et al. 2021a),\nheterogeneity of artificial neurons for the robust computa-\ntion (Perez-Nieves et al. 2021), and other multi-scale inspi-\nration from the single neuron or synapse to the network or\ncognitive functions.\n*These authors contributed equally.\n†Corresponding authors.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nHowever, many key modules in SNNs are relatively sim-\nple, which stops their further improvement of accuracy, ro-\nbustness, and efficiency. For example, the leaky-integrated-\nand-fire (LIF) neuron is commonly used in conventional\nSNNs, mainly caused of their simplicity and efficiency dur-\ning network learning. Similar to it, integrated and fire (IF)\nneuron is also generally used in many ANN-to-SNN conver-\nsion algorithms, partly caused by the mathematical equiva-\nlence of activation functions between IF in SNN and recti-\nfied linear unit (ReLU) in ANN.\nNeuronal dynamics and neuronal heterogeneity are im-\nportant features for efficient sequential information process-\ning (Izhikevich 2003) and robust computation (Perez-Nieves\net al. 2021). The spiking neuron with different dynamics in\nthe biological brain is usually considered the basic build-\ning block to support cognitive functions at higher scales.\nMore biologically plausible features, such as network topol-\nogy and plasticity-based learning algorithms, are also very\nimportant but will not be further discussed in this study.\nThe SNN is considered the third-generation ANN (Maass\n1997), guiding the conventional ANN to the goal of bio-\nlogically plausible machine intelligence. The SNN could in-\ncorporate spatial and temporal information at multi scales,\nmaking them efficient in many cognitive functions, includ-\ning but not limited to the spatial or temporal information sen-\nsation, working memory formation with various dynamics,\nand complex decision-making based on short or long-term\nmemory. Most of these key features are inspired by the bio-\nlogical counterpart brain and might be the key to balancing\nthe accuracy and computational cost.\nEven though many efforts have been given to integrate\nSNNs, ANNs, and biological discovery in different manners\nfrom both software and hardware perspectives, it is still a\nbig challenge to combine them naturally by resolving some\nconflict features of them, including but not limited to dif-\nferent types of learning neurons, architectures, and learning\nalgorithms. Some researchers try to combine ANN and SNN\nin a unified framework by freely setting spike-based and\nrate-based neurons towards the artificial general intelligence\nunder the Tianjic neuromorphic computation platform (Pei\net al. 2019). The Spaun architecture integrates global rate-\nbased routing algorithms and local spiking neurons to build\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n102\nfunctional brain regions to support more than eight cogni-\ntive functions (Eliasmith et al. 2012). Some SNNs try to get\naround the non-differential optimization challenge by train-\ning a deep ANN first and then converting it into a spike ver-\nsion for achieving high accuracy (Rueckauer et al. 2017).\nThen both gradient-based and plasticity-based algorithms\nhave been proposed to train SNN for different machine-\nlearning tasks. The SNN using gradient-based algorithms\ncould usually achieve relatively higher performance, which\nwe primarily used in this paper. It is generally accepted that,\non the one hand, directly incorporating spiking neurons into\nANNs might make the gradient in BP non-precise, and on\nthe other hand, the complex inner dynamics of spiking neu-\nrons would make the convergence speed slower. However,\nthe integration of ANNs and SNNs is necessary because gra-\ndient learning in ANN has been verified as efficient, and\nmany key features in SNNs borrowed from natural neural\nnetworks might be the key to reaching biology-like compu-\ntational efficiency, flexibility, and robustness.\nHence, this paper integrates some key features from\nANNs and biology to improve SNN. The main contributions\nof this paper can be concluded in the following parts.\n• First, four spiking neurons with complex dynamics are\ndesigned and used for efficient temporal information pro-\ncessing in SNNs, containing the vanilla 1st-order dy-\nnamics, 1st-order dynamics with adaptive threshold, 2nd-\norder dynamics, and double-neuron dynamics with in-\nhibitory neurons.\n• Second, we borrow the transformer module in ANN and\nrevise it as a spiking transformer to reduce the computa-\ntional cost, without seriously affecting the classification\naccuracy. Furthermore, an autoregression network is also\nused for the signal decoder.\n• Third, a benchmark automatic speech recognition (ASR)\ntask is used to test the proposed algorithm on its accuracy,\nefficiency, and robustness, where the moderate LJSpeech\ndataset contains sequential spoken phonemes and words\nin a full sentence.\n• Fourth, after analyzing, we find that the inter-spike-\ninterval in SNNs can naturally represent the phoneme\nchunk boundaries, representing handling the implicit\nrhythm of long and short phonemes by heterogeneous\nneuronal dynamics.\nRelated Work\nFor SNNs, two main research directions are formalized\ncaused of different goals. One is to understand the biolog-\nical brain better through computer simulation. A spiking-\nversion adaptive dynamic processing algorithm is designed\nfor the discrete-time optimization with the Poisson process\n(Wei, Han, and Zhang 2021). The two-scale clocks in convo-\nlutional spiking neurons are unified and well-tuned with bio-\nlogically plausible reward propagation (Zhang et al. 2021b).\nAnother is to achieve the highest performance to move ar-\ntificial intelligence further. Population coding at the net-\nwork scale and spike coding at the neuronal scale were\nintegrated and exhibited higher performance than ANNs\non continuous-control tasks during reinforcement learning\n(Zhang et al. 2022a). An e-prop algorithm was proposed\nthat incorporated the spiking units and target propagation\nfor reaching higher performance on both speech recognition\nand reinforcement learning (Bellec et al. 2020).\nFor spatial or temporal information processing, many\nmethods (Mueller et al. 2021) have been proposed in ANNs.\nWhile in SNNs, different types of meta-dynamic neurons\nwere designed to support the general spatial and temporal\ninformation processing (Cheng et al. 2020). It has been re-\nported that multisensory integration exists in a single neu-\nron (Stein and Stanford 2008), synapse (Wang et al. 2018),\nor different scales of the brain (Pasqualotto, Dumitru, and\nMyachykov 2015). The 3-node network motif was used to\nrepresent the feature of biological topology and verified ef-\nficiency in multisensory integration after copying these bio-\nlogical features to the SNNs (Jia et al. 2022). The dynamic\nfiring threshold was simulated with one-order dynamics and\nexhibited power on temporal information processing (Jia\net al. 2021). The new-general dynamic vision sensor was de-\nsigned to process spike-based event signals, which showed\nthe efficient ability to process temporal information with an\nextremely high frame rate than the conventional frame-based\ncameras (Posch et al. 2014). The efficient spatially temporal\ninformation processing of SNNs has been verified on many\nbiological and realistic artificial intelligence tasks (Kugele\net al. 2020).\nMethods\nThe overview of our DyTr-SNN is presented in Figure 1.\nFirst, two layers with 1D-convolution kernels are designed\nto conduct down-sampling and acoustic modeling of raw\nspeech features. Second, N layers with spiking transformers\nare designed for spatially temporal information encoding.\nEach transformer contains 6 blocks, including muti-head\nself-attention, first residual-connection, dense layer (fully\nconnected layer, FC), spiking neuron, dense layer (FC),\nand second residual-connection. Third, one layer with 1D-\nconvolutional kernels is designed to learn context informa-\ntion. Fourth, an FC layer is designed to calculate a sequence\nof current signals. Fifth, the one layer with complex neu-\nronal dynamics is designed to detect phone chunk bound-\naries and integrate information, where four main types of\nneurons are introduced. Finally, the output layer with an\nauto-regressive decoder calculates the final output.\nThe heterogeneity of neuronal dynamics is the most im-\nportant module in this study, where the spikes could be de-\ntected and represent the phone chunk boundary.\nThe Encoder Layer with Spiking Transformer\nThe transformer-encoder layer receives the raw speech fea-\ntures as network input (see Experiments for more details)\nand generates high-dimensional acoustic information rep-\nresentations. The whole procedure contains the following\nsteps.\nFirst, we use a convolutional layer as the front end, which\nutilizes a convolution layer with stride 2 to conduct tempo-\nral down-sampling of raw speech features. This process can\nreduce the length of temporal feature sequences by using\n103\nFigure 1: The overall architecture of the proposed DyTr-SNN.\nlearned kernel filters to reduce the memory cost for efficient\ntraining.\nSecond, we use a spiking transformer to improve paral-\nlel computing effectively. The raw transformer contains ar-\ntificial neurons. It uses scaled dot-product attention to find\nthe correlation between sub-elements of the input sequence.\nIt has three inputs: query (Q), key (K ), and value (V ).\nSince the speech sequence in the ASR task contains com-\nplex acoustic information at the phoneme level as well as\nsemantic information at the word level, in order to map the\nsequence to multiple high-level spaces and obtain rich infor-\nmation on different scales as much as possible, the selection\nof multi-head attention layer is necessary. Then the calcu-\nlated multiple self-attention are combined to reach a final\noutput using a full connection layer.\nRecently, many spiking transformer architectures have\nbeen proposed. They highlight spatio-temporal feature fu-\nsion (Zhang et al. 2022b), or make ANN-to-SNN conversion\n(Mueller et al. 2021). Here we first replace the ReLU acti-\nvation function with integration-and-fire (IF) neurons (i.e.,\nspiking neurons with simple 1st-order neuronal dynamics)\ndue to their similarities and then train the SNN transformer\ndirectly. The IF is the simplest type of spiking neuron model\nand could be defined as the following Equation:\n(\nCm\ndVk,t\ndt = Ik\nVk,t = Vreset, Spike= 1 if(Vk,t > Vth)\n, (1)\nwhere Vk,t is the membrane potential for the neuron k at\ntime t, Cm is the membrane capacitance, Vth is the firing\nthreshold, Vreset is the reset potential, Spike is the firing\nflag. As the current Ik inputs into the neuron k with time\nsteps, its membrane potential gradually increases and finally\nreaches the threshold, meanwhile the neuron will fire a spike\nand then be reset to Vreset, waiting for a next firing pro-\ncess. The relationship between fire rate in ANN (R ANN\ni )\nand spike trains (generated with IF neuron, V SNN\ni ) in SNN\nis shown as the following Equation:\nR(t) = RANN\ni Rmax − V SNN\ni\ntVth\n, (2)\nwhere the fire rate after conversion is represented as the\ntemporal firerate R(t), RANN\ni is the fire rate value of the\nneuron i after the ReLU activation, Rmax is the maximal\nfiring rate of the ANN, V SNN\ni is the membrane potential, t\nis the time step.\nThe Simple IF Neuron for the Boundary Detection\nThe spiking neuron encoder is the most important part af-\nter the spiking transformer. The IF neuron is used to process\ntemporal acoustic, with the function of membrane potential\naccumulation and spike firing in the neuron model, to lo-\ncate acoustic boundaries and dynamic information integra-\ntion. This special process includes the following 3 steps:\nFirst, a sequence of current signals I = ( I1, I2, I3...) is\nused to describe the amount of acoustic information calcu-\nlated from the sequentially input h = (h1, h2, h3...) during\nthe spiking transformer layer. A 1-dimensional convolution\nis used to capture the local dependency, and then a Sigmoid\nfunction is used to non-linear normalize the signal to scalar\nvalues between 0 and 1.\nSecond, the current signal I will stimulate a neuron to\ngenerate spikes after a period of membrane-potential accu-\nmulation. For example, at the time step t during learning, a\nfunction F(Ik,t, Vk,t−1, ...) of current value Ik,t in the cur-\nrent sequence, as well as some other variables such as mem-\nbrane potential Vk,t−1 at previous step t − 1, will be calcu-\nlated first and then added to the previous membrane potential\nVk,t−1 to get the new accumulated membrane potentialVk,t.\nThis procedure could be described in the following Equa-\ntion:\n104\nVk,t = Vk,t−1 + F(Ik,t, Vk,t−1, ...), (3)\nwhere if Vk,t is less than a given thresholdVth, it will still\nbe used as the previous potential for the next step t + 1, or\notherwise, it will be set as Vk,t − 1, representing a leakage\nof membrane potential due to spike firing. The value of 1\nhere is unimportant since other parameters (e.g, the synaptic\nweights) could adapt to it during learning. It can also be de-\nscribed as the following Equation, where the time constant\nτ is set to 1 for simplicity.\nτ dVk,t\ndt = F(Ik,t, Vk,t−1, ...). (4)\nwhere Vk,t and Ik,t can be simplified asVt and It (k = 1).\nNotably, various possible expressions here are included in\nfunction F, each representing a kind of neuronal dynamics.\nThis study will introduce four main dynamic neurons in the\nnext subsection.\nHence, we can now determine whether an acoustic bound-\nary is located by observing whether there is a spike gener-\nated from the dynamic neurons. If no boundary was located,\nwe could update the current integrated state with the func-\ntion of St = St−1 + It · ht for the next step t + 1. If Vt was\ngreater than Vth, it means an acoustic boundary is located,\nand St should be sent to the next-step decoder module as\nthe integrated acoustic information corresponding to the cur-\nrent token. All these calculations would be repeated until the\nnetwork is convergent and the final acoustic representations\nsequence is generated at network output.\nSpiking Neurons with Different Neuron Dynamics\nIn this study, four types of neurons are introduced to discuss\ntheir ability for acoustic boundary identification further: (1)\nthe vanilla 1st-order dynamics, which uses a predefined fir-\ning threshold and is similar to that in the IF neuron; (2) the\n1st-order dynamics with an adaptive threshold; (3) the 2nd-\norder dynamics; (4) the double-neuron dynamics using an\nadditional neuron for the feedback inhibition.\nThe vanilla 1st-order dynamics Neurons with vanilla\n1st-order dynamics are similar to the IF neurons, which have\nalready been proven effective in many types of research\n(Dong and Xu 2020). The description of the vanilla neuron\nis shown in the following Equation (5):\ndVk,t\ndt = Ik,t, (5)\nwhere Ik,t is the input current to the neuron k, and Vk,t\nis the membrane potential. It describes that the membrane\nupdate is only related to the value of the input current, in-\ndicating a nearly linear process of membrane potential ac-\ncumulation. A schematic diagram depicting the membrane\npotential under the vanilla 1st-order dynamics is also shown\nin Figure 1.\nThe 1st-order dynamics using dynamic threshold Un-\nlike the IF neuron, where the firing threshold is a prede-\nfined static threshold value, the neurons with 1st-order dy-\nnamics using adaptive threshold run further to improve their\ndynamic complexity by introducing this special dynamic\nthreshold which is changed at each step. The additional com-\nputation of the dynamic threshold is shown as the following\nEquation (6):\nVth,t = αVth,t−1 + Spike · ∆h (6)\nwhere Vth,t is the firing threshold at time step t, and\nVth,t−1 is the threshold at the next step t − 1, α is the decay\ncoefficiency, ∆h is the jump coefficiency once receiving the\nneuronal firing flag Spike. This function describes that the\nneuronal threshold will get a jump to increase the difficulty\nof firing another spike in a short time. This function could\nalso be considered temporal masking in the time domain, to\npromote firing for sparse spikes or, on the contrary, decrease\nfiring for dense spikes. This mechanism could provide a\nricher dynamic process for inner neurons. A schematic di-\nagram depicting the membrane potential under the 1st-order\ndynamics using adaptive threshold is also shown in Figure\n1.\nThe 2nd-order dynamics We further discuss some neu-\nrons with higher-order dynamics, which have been proven\neffective in reinforcement learning (Zhang et al. 2022a). It\nmeans the membrane potential might have more than one\nequilibrium state. In this paper, we introduce neurons with\n2nd-order dynamics, shown as the following Equation (7):\ndVk,t\ndt = a · V 2\nk,t + b · Vk,t + c · Ik,t, (7)\nV 2\nk,t and Vk,t are membrane potentials with different de-\ngrees of dynamics for the neuron k at time t, a, b and c are\nthe hyperparameters to represent the corresponding coeffi-\ncients of Vk,t. As a result, the dynamic membrane potential\nwill be attracted or nonstable at some points given a small\ninput and will have a non-linear growth given a bigger in-\nput. A schematic diagram depicting the membrane potential\nunder the 2nd-order dynamics is also shown in Figure 1.\nThe double-neuron dynamics Double-neuron dynamics\nmeans an excitatory neuron could receive a self-inhibition as\ninput from another inhibitory neuron. The dynamic changes\nof membrane potential for both excitatory (V) and inhibitory\n(U) neurons are shown as the following Equation (8):\n( dVk,t\ndt = gV 2\nk,t + mUk,t + Ik,t\ndUk,t\ndt = nUk,t\n, (8)\nwhere m and n are hyperparameters for describing the\ninhibition strength and self-dynamics of the inhibitory neu-\nron, respectively. The immediate feedback of spikes from\nanother inhibitory neuron will temporarily block the excita-\ntory neuron’s activity for a short time, increasing the scale of\ndynamics. A schematic diagram depicting the membrane po-\ntential under double-neuron dynamics is also shown in Fig-\nure 1.\nDecoder Layer\nWe use the auto-regressive (AR) decoder with a transformer\nto post processing the spike trains generated from different\n105\ntypes of dynamic neurons. The probability distribution is fi-\nnally calculated as the whole network output.\nThe Global and Local Learning of DyTr-SNN\nThe main training algorithm of DyTr-TNN includes two\nmain parts, i.e., global learning based on the loss function\nand local learning based on neuronal coding.\nWe select 3 loss functions for their fitness of the pro-\nposed DyTr-SNN. The cross-entropy loss (L CE ) is used to\ncalculate the probability distribution difference between the\nmodel output and the target sequence. A CTC loss LCTC is\nselected to promote acoustic boundaries’ identification ac-\ncuracy. A quantity loss LQua is used to limit the differences\nbetween the number of boundaries (e.g., the quantity of the\ntarget tokens) and the number of spikes (e.g., the number of\npredicted tokens). Then these three loss functions are inte-\ngrated, shown as the following Equation (9):\nLoss = λ1LCE + λ2LCTC + λ3LQua, (9)\nwhere λ1, λ2, and λ3 are hyper-parameters for adjusting\nthe different importance of these three loss functions. Af-\nter getting Loss, we calculate the gradient of all parameters\nand update our model. For spiking neurons, the gradient ap-\nproximation trick is used to get around the non-differential\nfeature of the membrane potential containing spikes (Zhang\net al. 2021b).\nWhile neuron coding uses the learning ability of a single\nneuron, which has the complex dynamics mentioned above,\nthe neuron is placed to position token boundaries from the\ncontinuous speech in the way of membrane potential accu-\nmulation and firing under the encoder-decoder framework. It\nforwardly integrates the information in the encoded acous-\ntic representations and fires the integrated information to the\ndecoder upon locating a boundary.\nExperiments\nThe proposed DyTr-SNN was evaluated on a non-toy au-\ntomatic speech recognition task. The introduction of this\ndataset, the related experimental configurations, and the ex-\nperimental results are shown in the following sections.\nLJSpeech Dataset\nThe LJSpeech 1 was used in this study as a non-toy ASR\ndataset. It is a 24-hour single-speaker speech dataset com-\nprised of 13,100 short English audio clips with a sample rate\nof 22,050 Hz. The reference transcriptions are provided for\nall audio clips, ranging from 1 to 10 seconds.\nWe split the original LJSpeech dataset into 12,229 sam-\nples for training, 348 samples for validation, and 523 sam-\nples for testing (Ren et al. 2020). We first extract the 80-\ndimension log-Mel spectrogram of these audio clips with\n1,024 FFT size, 1,024 window size, and 256 hop length.\nThen, we apply global cepstral mean and variance normal-\nization (CMVN) to the extracted acoustic features. Finally,\nwe use the processed features as the network input. The cor-\nresponding phoneme sequences of LJSpeech audio clips are\n1https://keithito.com/LJ-Speech-Dataset/\nused as the training target. To generate the phoneme se-\nquences of audio clips, we use the g2pE tool 2 to convert\ntranscriptions. The final token set contains 69 phonemes and\n4 special tokens: “sp”, “spn”, “eos”, and “bos”.\nExperimental Setting\nThe spiking transformer encoder consists of the convolu-\ntional module and the transformer module. The convolu-\ntional module has 2 layers followed by a GLU activation\nfunction (Dauphin et al. 2017), with each layer using 1-D\nconvolution kernels along the time axis with kernel size 5\nand stride 2, and the output channels for them are 512 and\n1,024, respectively. The transformer module is comprised of\nN = 12 self-attention blocks, sharing the same input dimen-\nsion (dff = 2048), output dimension (d model = 512), and\nhead numbers (h = 8 ). The hidden layer of neuronal dy-\nnamics is a 1-D convolution layer with kernel size 3 and 256\noutput channels. A decoder layer is a stack of 6 self-attention\nblocks, sharing the same parameters of dmodel, dff , and h\nas in the encoder layer.\nFor model training, we apply inverse square root learn-\ning rate scheduler and use the Adam optimizer (Kingma and\nBa 2014) with peak learning rate5e-4, β1=0.9, β2=0.98. λ1,\nλ2 and λ3 are set to 1.0, 0.25, and 1.0, respectively. We ap-\nply dropout 0.2 to all self-attention blocks and convolution\nlayers for model regularisation. We also apply Specaugment\n(Park et al. 2019) with LD policy to augment acoustic fea-\ntures. For model inference, we use beam search with beam\nsize 5. To measure the performance, we use phoneme er-\nror rate (PER) as the metric. We implement our models and\nmethods on Fairseq with PyTorch. The basic implementation\nof vanilla 1st-order dynamics is available online3.\nFigure 2: Gradually stable trajectories of three parameters.\nGray regions are stable areas.\nPre-learning of Dynamic Neurons\nIn order to find the proper parameters of DyTr-SNN, we used\na mini dataset (1/8 of the original training set) in a smaller\nnetwork (with a reduced model of 2 transformer encoder lay-\ners and 2 decoder layers) first. Then we applied the parame-\nters to the whole dataset for further adjustment.\nThe parameters for all dynamics followed the same\npipeline to get the best configurations. For the 2nd-order dy-\nnamics, we first enable the three parameters (i.e.,a, b, and c)\n2https://github.com/Kyubyong/g2p\n3https://github.com/MingLunHan/CIF-PyTorch\n106\nto be updated along with the synaptic weights learning based\non the loss function. During the evolutionary process, these\nthree parameters converged into (0.1014, -0.0832, 0.9506),\nas shown in the recorded trajectory in Figure 2. The α is\n0.95, ∆h is 0.1, g is 0.001, m and n are -0.05, for dynamics\nin Equations (6) and (8).\nFigure 3: Firing patterns of neurons with 1st and 2nd-\ndynamics. (A) The vanilla 1st-order dynamics. (B) The 1st-\norder dynamics using dynamic threshold, (C) The 2nd-order\ndynamics. (D) The diagram depicting the attractor and re-\npulsor is calculated from the 2nd-order dynamics.\nAfter pre-learning, we visualized the firing patterns of\nthese dynamic neurons by giving a simulated current input\n(e.g., a triangle-linear wave with a minimum value of 0.015\nand a maximum value of 0.15). For the vanilla 1st-order dy-\nnamics, we found that the number of spikes was linearly up-\ndated with the input current (Figure 3A). For the 1st-order\ndynamics using adaptive threshold, we found that the thresh-\nold was dynamic and positively correlated with the firing\nrate, which caused earlier firings (Figure 3B).\nFigure 4: Histogram depicting the PER comparison of the\nproposed DyTr-SNN using different neuronal dynamics and\nother typical algorithms on the LJSpeech dataset. The PERs\nfor DyTr-SNN using D1, D1,th, D2, and D2,dou were\n9.544±0.047, 9.437±0.115, 9.316±0.029, 9.384±0.030,\nrespectively.\nFor the 2nd-order dynamics, we found that the accumu-\nlation process of membrane potential was non-linear. The\ngrowth rate was not stable even for that given a constant\ninput, caused by the more complex dynamics related to its\nhistorical experience (Figure 3C). It could be explained by\nthe one attractor and one repulsor in the dynamic process\nof membrane potential (Figure 3D). The attractor means the\nmembrane potential will continuously increase or be leaky,\ndepending on whether it is greater than the attractor. While\non the contrary, it will be called a repulsor.\nLower Phoneme Error Rate (PER)\nThe phoneme error rate (PER) was selected to judge the per-\nformance of the proposed DyTr-SNN and other typical al-\ngorithms on LJSpeech. Each experiment was repeated three\ntimes using initial synaptic weights with different random\nseeds.\nAs shown in Figure 4, the four dynamics are called\nD1, D1,th, D2, D2,dou respectively for convenience. All\nour DyTr-SNNs using any neuronal dynamics achieved\nmarkedly lower PER than the typical algorithm. This result\nshowed the efficiency of the hybrid integration of the spiking\ntransformer and spiking neuronal dynamics. Furthermore,\nthe DyTr-SNN using the 2nd-order dynamics achieved the\nhighest performance than SNNs using the other three types\nof neuronal dynamics. A possible explanation is that a neu-\nron with more complex dynamics can better catch the com-\nplex encoding features from the spiking transformer en-\ncoder. The complex encoding represents the transformer-\nlike self-attention encoding, where each frame in the trans-\nformer contains the acoustic mutual information of the other\nframes and its self-information.\nThe multi-head attention network can map the same sig-\nnal vector to multiple spaces, representing complex spatial-\ntemporal information. For example, one phoneme will be\nembedded in the entire sentence in many ways, depending\non how it relates to other phonemes. As a result, the output\nintegrates a variety of possible relationships between the as-\nsociated acoustic information. Hence, a neuron with more\nspatio-temporal dynamics can better deal with this situation.\nFigure 5: Histogram depicting the robustness of DyTr-SNN\nusing different neuronal dynamics. A, The PER before\nand after adding noises into network input. B, The rela-\ntive changes of PER before and after giving noises. The\nPERs for DyTr-SNN using 4 dynamics were 9.748 ± 0.060,\n10.763 ±0.043, 9.745 ±0.038, 9.569 ±0.048, respectively.\nThe relative changes were 3.2%, 14.1%, 4.6%, 2.0%.\nStronger Robustness\nThe robust computation in SNNs has been reported many\ntimes in previous studies. Here we test the robustness of the\nproposed DyTr-SNN using different neuronal dynamics. We\n107\nadded a uniform noise ranging from 0 to 0.01 to the net-\nwork input, and the experimental results are shown in Fig-\nure 5. The performance of networks before and after adding\nnoises showed that different dynamics exhibited a different\nproportion of PER changes (Figure 5A). Furthermore, the\nrelative PER change of DyTr-SNN using D2,dou was the\nlowest compared to other neuronal dynamics, representing\nthat the D2,dou were more robust than other dynamics.\nFigure 6: PER topography contour measured by different\nneuronal dynamics (vertical) and the number of layers (hor-\nizontal), and computational cost with different model sizes.\nLower Computational Cost\nIt is a dilemma that networks with more layers will reach a\nhigher accuracy but, at the same time, take more computa-\ntional costs. In this study, the proposed DyTr-SNN is energy\nefficient in the following two aspects: the spiking signals are\nmore efficient than artificial ones caused of their 0/1 fea-\nture for efficient computation; the more complex neuronal\ndynamics will acquire less precoding of information.\nThe former has been commonly verified in many related\ntypes of research, so we will not further discuss it. Here we\nfocus on the second one and compare a network using the\ndifferent number of hidden layers and dynamic neurons with\ndifferent neuronal dynamics. As shown in Figure 6, the PER\ntopography contour map showed that the PER is gradually\nreduced with increased neuronal complexity. A similar con-\nclusion is given using an increasing number of transformer\nencoder layers. The contour line (marked black) means the\nmodels achieved similar performance (e.g., the model with\nD2,dou and 5 transformer layers is similar to that with D1\nand 10 transformer layers in PER). The result shows that\nthe increasing complexity is similar to or equivalent to the\nincreasing encoder layers.\nAnalysis of Spikes for Acoustic Boundary Location\nThe important function of the spiking neuron encoder is to\nprecisely identify the acoustic boundaries at the phoneme\nlevel, using spikes of dynamic neurons as the boundary\nflags. An example of a short speech is shown in Figure 7,\nwhich contains a sentence with nine words consisting of 37\nphonemes. We select three words to visualize and find that,\ncompared with the ones before learning, the boundary lo-\ncations after training are closer to the label of boundaries.\nHere, a more precise positioning means a lower PER and\nhigher performance.\nFigure 7: The visualization of acoustic boundaries on the\nMel-spectrogram at the phoneme level. After training, the\nboundary locations (marked yellow lines) are closer to the\nlabel boundaries than the ones before training.\nFurther Discussion on Hybrid Architectures\nBesides combining spiking transformers and dynamic neu-\nrons, we run further by integrating artificial transformers and\nspiking neurons with neuronal dynamics to achieve much\nhigher performance. For spiking transformer, algorithm us-\ning D1, D1,th, D2, and D2,dou achieved PER of 9.544 ±\n0.047, 9.437 ± 0.073, 9.316 ± 0.029, and 9.384 ± 0.030,\nrespectively. For artificial transformer, algorithm using D1,\nD1,th, D2, and D2,dou achieved PER of 6.805 ± 0.092,\n6.692 ± 0.115, 6.455 ± 0.062, and 6.786 ± 0.038, respec-\ntively.\nConclusion\nThe spiking neural network (SNN) contains both neuronal\ndynamics and spiking signals for efficient temporal infor-\nmation processing. In this study, we extend SNN further by\nusing complex neurons inspired by the biological brain and\na spiking transformer borrowed from the conventional ar-\ntificial transformer neural network. Four types of neuronal\ndynamics are proposed and improved SNNs on the hetero-\ngeneity of dynamics, exhibiting the different performance\nof phoneme error rate, robustness, and computational cost.\nFurther analysis of the linear relationship between the scale\nof neuronal complexity and the number of transformer lay-\ners indicates that the brain uses a relatively shallow neu-\nral network with very complex neuronal dynamics for han-\ndling different challenging tasks without losing accuracy, ef-\nficiency, and robustness.\n108\nAcknowledgments\nThis work was supported by the National Key R&D Pro-\ngram of China (Grant No. 2020AAA0104305), the Strategic\nPriority Research Program of Chinese Academy of Sciences\n(Grant No. XDB32070100 and XDA27010404), the Shang-\nhai Municipal Science and Technology Major Project (Grant\nNo. 2021SHZDZX), and the Youth Innovation Promotion\nAssociation of CAS.\nReferences\nBellec, G.; Scherr, F.; Subramoney, A.; and et al. 2020. A\nsolution to the learning dilemma for recurrent networks of\nspiking neurons. Nat Commun, 11(1): 3625.\nCheng, X.; Zhang, T.; Jia, S.; and Xu, B. 2020. Finite Meta-\nDynamic Neurons in Spiking Neural Networks for Spatio-\ntemporal Learning. ArXiv, cs.NE/2010.03140.\nDauphin, Y . N.; Fan, A.; Auli, M.; and Grangier, D. 2017.\nLanguage modeling with gated convolutional networks. In\nICML, 933–941. PMLR.\nDong, L.; and Xu, B. 2020. CIF: Continuous Integrate-And-\nFire for End-To-End Speech Recognition. InICASSP, 6079–\n6083. IEEE.\nEliasmith, C.; Stewart, T. C.; Choo, X.; Bekolay, T.; DeWolf,\nT.; Tang, Y .; and Rasmussen, D. 2012. A large-scale model\nof the functioning brain. Science, 338(6111): 1202–5.\nIzhikevich, E. M. 2003. Simple model of spiking neurons.\nIEEE Transactions on Neural Networks, 14(6): 1569–72.\nJia, S.; Zhang, T.; Cheng, X.; and et al. 2021. Neuronal-\nPlasticity and Reward-Propagation Improved Recurrent\nSpiking Neural Networks. Front Neurosci, 15: 654786.\nJia, S.; Zuo, R.; Zhang, T.; Liu, H.; and Xu, B. 2022. Motif-\nTopology and Reward-Learning Improved Spiking Neural\nNetwork for Efficient Multi-Sensory Integration. InICASSP\n2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 8917–8921. IEEE.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKugele, A.; Pfeil, T.; Pfeiffer, M.; and Chicca, E. 2020. Ef-\nficient Processing of Spatio-Temporal Data Streams With\nSpiking Neural Networks. Front Neurosci, 14: 439.\nMaass, W. 1997. Networks of spiking neurons: The third\ngeneration of neural network models. Neural Networks,\n10(9): 1659–1671.\nMueller, E.; Studenyak, V .; Auge, D.; and Knoll, A. 2021.\nSpiking Transformer Networks: A Rate Coded Approach for\nProcessing Sequential Data. In 2021 7th International Con-\nference on Systems and Informatics (ICSAI), 1–5. IEEE.\nPark, D. S.; Chan, W.; Zhang, Y .; Chiu, C.-C.; Zoph, B.;\nCubuk, E. D.; and Le, Q. V . 2019. Specaugment: A simple\ndata augmentation method for automatic speech recognition.\narXiv preprint arXiv:1904.08779.\nPasqualotto, A.; Dumitru, M. L.; and Myachykov, A. 2015.\nEditorial: Multisensory Integration: Brain, Body, and World.\nFront Psychol, 6: 2046.\nPei, J.; Deng, L.; Song, S.; and et al. 2019. Towards arti-\nficial general intelligence with hybrid Tianjic chip architec-\nture. Nature, 572(7767): 106–111.\nPerez-Nieves, N.; Leung, V . C. H.; Dragotti, P. L.; and Good-\nman, D. F. M. 2021. Neural heterogeneity promotes robust\nlearning. Nat Commun, 12(1): 5791.\nPosch, C.; Serrano-Gotarredona, T.; Linares-Barranco, B.;\nand Delbruck, T. 2014. Retinomorphic Event-Based Vision\nSensors: Bioinspired Cameras With Spiking Output. Pro-\nceedings of the IEEE, 102(10): 1470–1484.\nRen, Y .; Hu, C.; Tan, X.; Qin, T.; Zhao, S.; Zhao, Z.; and Liu,\nT.-Y . 2020. Fastspeech 2: Fast and high-quality end-to-end\ntext to speech. arXiv preprint arXiv:2006.04558.\nRueckauer, B.; Lungu, I. A.; Hu, Y .; Pfeiffer, M.; and Liu,\nS. C. 2017. Conversion of Continuous-Valued Deep Net-\nworks to Efficient Event-Driven Networks for Image Classi-\nfication. Front Neurosci, 11: 682.\nStein, B. E.; and Stanford, T. R. 2008. Multisensory integra-\ntion: current issues from the perspective of the single neuron.\nNature Reviews Neuroscience, 9(4): 255–266.\nTang, G.; Kumar, N.; Yoo, R.; and Michmizos, K. 2021.\nDeep reinforcement learning with population-coded spiking\nneural network for continuous control. In Conference on\nRobot Learning, 2016–2029. PMLR.\nWang, W.; Pedretti, G.; Milo, V .; and et al. 2018. Learning\nof spatiotemporal patterns in a spiking neural network with\nresistive switching synapses. Sci Adv, 4(9): eaat4752.\nWei, Q.; Han, L.; and Zhang, T. 2021. Spiking Adaptive Dy-\nnamic Programming Based on Poisson Process for Discrete-\nTime Nonlinear Systems. IEEE Trans Neural Netw Learn\nSyst, PP.\nZhang, D.; Zhang, T.; Jia, S.; and Xu, B. 2022a. Multiscale\nDynamic Coding improved Spiking Actor Network for Re-\ninforcement Learning. In Thirty-Sixth AAAI Conference on\nArtificial Intelligence.\nZhang, J.; Dong, B.; Zhang, H.; and et al. 2022b. Spiking\nTransformers for Event-Based Single Object Tracking. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 8801–8810.\nZhang, T.; Cheng, X.; Jia, S.; Poo, M. M.; Zeng, Y .; and\nXu, B. 2021a. Self-backpropagation of synaptic modifica-\ntions elevates the efficiency of spiking and artificial neural\nnetworks. Sci Adv, 7(43): eabh0146.\nZhang, T.; Jia, S.; Cheng, X.; and Xu, B. 2021b. Tuning\nConvolutional Spiking Neural Network With Biologically\nPlausible Reward Propagation. IEEE Trans Neural Netw\nLearn Syst, PP.\n109"
}