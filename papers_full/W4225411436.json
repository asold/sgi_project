{
  "title": "Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models",
  "url": "https://openalex.org/W4225411436",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3092464485",
      "name": "Adam Sobieszek",
      "affiliations": [
        "University of Warsaw"
      ]
    },
    {
      "id": "https://openalex.org/A4225467945",
      "name": "Tadeusz Price",
      "affiliations": [
        "University of Nottingham"
      ]
    },
    {
      "id": "https://openalex.org/A3092464485",
      "name": "Adam Sobieszek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225467945",
      "name": "Tadeusz Price",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1967614286",
    "https://openalex.org/W2944797580",
    "https://openalex.org/W2604897046",
    "https://openalex.org/W2135255848",
    "https://openalex.org/W2003240077",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3111000860",
    "https://openalex.org/W4249332350",
    "https://openalex.org/W2039540101",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W6769430610",
    "https://openalex.org/W1521903627",
    "https://openalex.org/W2068616338",
    "https://openalex.org/W2611387546",
    "https://openalex.org/W2925023100",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2160871514",
    "https://openalex.org/W3181318144",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3135012552",
    "https://openalex.org/W4206566734",
    "https://openalex.org/W7037312875",
    "https://openalex.org/W4242911526",
    "https://openalex.org/W3164763272",
    "https://openalex.org/W4253475453",
    "https://openalex.org/W3191895906",
    "https://openalex.org/W1991203624",
    "https://openalex.org/W1529533208",
    "https://openalex.org/W6608808214",
    "https://openalex.org/W3124726575",
    "https://openalex.org/W3176757765",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W616972496",
    "https://openalex.org/W3034344071",
    "https://openalex.org/W2251410821",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2111365291",
    "https://openalex.org/W4211000585",
    "https://openalex.org/W2120485959",
    "https://openalex.org/W3099236585"
  ],
  "abstract": "Abstract This article contributes to the debate around the abilities of large language models such as GPT-3, dealing with: firstly, evaluating how well GPT does in the Turing Test, secondly the limits of such models, especially their tendency to generate falsehoods, and thirdly the social consequences of the problems these models have with truth-telling. We start by formalising the recently proposed notion of reversible questions, which Floridi &amp; Chiriatti (2020) propose allow one to ‘identify the nature of the source of their answers’, as a probabilistic measure based on Item Response Theory from psychometrics. Following a critical assessment of the methodology which led previous scholars to dismiss GPT’s abilities, we argue against claims that GPT-3 completely lacks semantic ability. Using ideas of compression, priming, distributional semantics and semantic webs we offer our own theory of the limits of large language models like GPT-3, and argue that GPT can competently engage in various semantic tasks. The real reason GPT’s answers seem senseless being that truth-telling is not amongst them. We claim that these kinds of models cannot be forced into producing only true continuation, but rather to maximise their objective function they strategize to be plausible instead of truthful. This, we moreover claim, can hijack our intuitive capacity to evaluate the accuracy of its outputs. Finally, we show how this analysis predicts that a widespread adoption of language generators as tools for writing could result in permanent pollution of our informational ecosystem with massive amounts of very plausible but often untrue texts.",
  "full_text": "GENERAL ARTICLE\n1 3\nMinds and Machines (2022) 32:341–364\nhttps://doi.org/10.1007/s11023-022-09602-0\nAbstract\nThis article contributes to the debate around the abilities of large language models \nsuch as GPT-3, dealing with: firstly, evaluating how well GPT does in the Tur -\ning Test, secondly the limits of such models, especially their tendency to generate \nfalsehoods, and thirdly the social consequences of the problems these models have \nwith truth-telling. We start by formalising the recently proposed notion of revers -\nible questions, which Floridi & Chiriatti ( 2020) propose allow one to ‘identify the \nnature of the source of their answers’, as a probabilistic measure based on Item \nResponse Theory from psychometrics. Following a critical assessment of the meth -\nodology which led previous scholars to dismiss GPT’s abilities, we argue against \nclaims that GPT-3 completely lacks semantic ability. Using ideas of compression, \npriming, distributional semantics and semantic webs we offer our own theory of the \nlimits of large language models like GPT-3, and argue that GPT can competently \nengage in various semantic tasks. The real reason GPT’s answers seem senseless \nbeing that truth-telling is not amongst them. We claim that these kinds of models \ncannot be forced into producing only true continuation, but rather to maximise their \nobjective function they strategize to be plausible instead of truthful. This, we more -\nover claim, can hijack our intuitive capacity to evaluate the accuracy of its outputs. \nFinally, we show how this analysis predicts that a widespread adoption of language \ngenerators as tools for writing could result in permanent pollution of our informa -\ntional ecosystem with massive amounts of very plausible but often untrue texts.\nKeywords GPT-3 · Artificial Intelligence · Psychometrics · Language Games · \nTuring test\nReceived: 14 September 2021 / Accepted: 4 April 2022 / Published online: 3 May 2022\n© The Author(s) 2022\nPlaying Games with Ais: The Limits of GPT-3 and Similar \nLarge Language Models\nAdam Sobieszek1 · Tadeusz Price2\n \r Tadeusz Price\napytp2@nottingham.ac.uk\n1 Department of Psychology, University of Warsaw, Warsaw, Poland\n2 Department of Philosophy, University of Nottingham, Nottingham, United Kingdom\nA. Sobieszek, T. Price\n1 3\n1 Introduction\n“Are you a performer?\nYes.\nCould you be considered a leading man?\nYes.\nDo you have anything to do with sports?\nYes.\nWould you be considered a writer?\nYes.\nDo you do any drawings, like comic strips?\nYes.\nErm… You are a human being? “.\nThese are questions put to Salvador Dali when in 1957 he appeared on the televi-\nsion programme “What’s my line?”. A group of blindfolded panellists was tasked \nwith discovering the surrealist’s occupation by asking yes/no questions.\nThe problem of learning about identity through questions has recently been taken \nup by Floridi & Chiriatti (2020) in the context of the language generator model GPT-3 \n(Brown et al., 2020). The model can be used to continue any textual prompt given to \nit while maintaining a consistent style, correct grammar and will usually make sense. \nThe discussion of answering questions in the context of computer intelligence dates \nto the Turing Test and is still widely debated today (Damassino & Novelli, 2020; \nMontemayor, 2021).\nIn their article, Floridi and Chiriatti analyse GPT’s ability to respond to questions. \nThey look for the type of questions which ‘may enable one to identify the nature of \nthe source of their answers’, which they call reversible, and at questions which they \ndeem irreversible, meaning we cannot tell from the answers to them whether we are \nconversing with a human or a machine. Examples of the latter are mathematical, \nfactual (trivia), and binary (yes/no) questions, while a potential source of reversible \nquestions are semantic ones such that ‘require understanding […] of both meaning \nand context’, especially ones that seem to hinge on real-world experience.\nWe take issue with some methodological underpinnings of their tests, which led \nthem to dismiss GPT’s abilities. By amending some of them, we wish to develop \nfurther the notion of reversibility, on the grounds that an accurate highlighting of the \nobstacles that the current machine learning paradigms face is relevant both to the \naccurate assessment of their shortcomings in becoming general AI, as well as their \nwider social impact.\nWe’ll show some situations in which reversibility, that is identification of AI-writ-\nten texts, can come from counterintuitive sources (that could serve as better guides \nfor identifying AI texts in the future), which suggest there is not a clear-cut group of \nsemantic questions which will trip up GPT-3. To guide our discussion, we introduce \n(1.1) that a response’s information value about the identity of the respondent can be \nformalized owing to developments in the field of psychometrics, and that (1.2) GPT’s \nabilities should be understood as stemming from its learning conditional probabili -\nties in language (its so-called “statistical capabilities”). It is specifically how far this \nsimple ability can get GPT that is the crux of the matter, and what shall inform our \n342\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\n343\ndiscussion of its limits, as we claim it is not enough to equate this ability with the \nability to learn syntax.\nIn the second part of this paper, we propose a theory of the limits and abilities of \nstatistical language models like GPT-3. A young Leibniz envisioned a machine, that \nowing to its understanding of language and impartial calculations, could eventually \nderive every true statement (Leibniz, 1666). We argue, that in sharp contrast, the \nregularities these models exploit, the compression that happens in their architecture, \nand the way of interacting with them, all contribute to the statements they produce \nbeing modal in their relationship to truth. This means, that it’s not simply that such \nmodels are unable to learn a representation of the real world, it is that they perform a \nlossy compression of statements about the real and possible worlds, which bestows \nupon them a kind of possible-world semantics. We’ll argue, that in such models truth \nis indexical to the text at hand, meaning that as the model encounters the word ‘truth’, \nit cannot decode its meaning as ‘true in the actual world’ (cf. Lewis, 1986). In simpler \nterms, this entails that in order to increase its objective function the model strategizes \nto be plausible instead of truthful.\nIn their paper, Floridi and Chiriatti provide an excellent discussion of the social \nimpact and dangers associated with the expanding role of such language generating \nmodels. This point is crucial, as soon we expect language generators to be in common \nuse among journalists, translators, civil servants, etc. We thus conclude with a deri -\nvation of the possible dangers of widespread adoption of these generators that stem \nfrom the present analysis. Through consideration of psychological inclinations pres-\nent during the assessment of statements, based primarily on the work of Mercier & \nSperber (2017; Mercier, 2020; Sperber et al., 2010), we show these possible adverse \nconsequences, including the modal drift hypothesis — that because humans are not \npsychologically equipped to effectively differentiate truth from plausible falsehoods \namong texts generated by language models, a mass production of plausible texts may \ndeteriorate the quality of our informational ecosystem.\n2 Learning from questioning GPT-3\nThe logic of Floridi and Chiriatti’s reversibility is that some questions will be harder \nthan others for a machine to answer, thus these are the questions that an interviewer \nshould ask in order to learn the interlocutor’s identity in a Turing Test. A binary \napproach to reversibility suggests that apart from reversible questions, there are also \nquestions (for maths or trivia questions), which do not help at all in discovering the \nidentity of the respondent – irreversible.\nWhat more formal criteria can we use to identify the source from its answers to \nquestions? We will use a theory from the field of psychometrics to inform our search \nfor these criteria. Psychometrics is the subfield of psychology concerned with an \nobjective measurement of hidden (latent) traits of respondents by means of question-\nnaires, hence the mathematical theories developed to quantify this process are per -\nfectly suited to handle our task of reconstructing from answers whether a respondent \nwas human.\nA. Sobieszek, T. Price\n1 3\n2.1 The psychometrics of reversibility\nThe most mature theory quantifying such matters is the Item Response Theory \n(Embretson & Reise, 2013). Instead of focusing on properties of questions, IRT seeks \nto model the way the agent answers questions as being influenced by the latent trait \nmeasured (e.g. that we should expect different answers, from people of different tem-\nperament). Thanks to our knowledge of how agents with different levels of a trait \ntypically answer questions, we can exploit this regularity and with every consecu -\ntive answer reduce our uncertainty as for the value of that latent trait (in our case \n– whether they are human). The amount of information gained by coming to know \nthe answer differs between questions (or items) and it is here that IRT advances the \ncrucial concept of item information, what we may call informativity and what we \nshall discuss as the construct underlying Floridi and Chiriatti’s reversibility.\nHow does thinking of answers as being influenced by whether the respondent is \nor is not human help us determine which questions are reversible? First, we should \nspecify our problem as the task of discovering a dichotomous latent class H (Barto -\nlucci, 2007; Brzezińska, 2016), that signifies whether the respondent is or is not a \nhuman, from the observable answers. In such a case such a model lets us assess the \namount of information gained from the answer (the item information) quite easily, as \nit depends on the probability distribution over possible answers to the question, con-\nditional on a value of the latent class H. We can even calculate this directly: a simple \nBayesian calculation of how our assumed prior belief that it is equally likely that the \nrespondent is (H) or is not (~ H) a human changes upon seeing an answer A, yields:\n \nP(H|A)= P(A|H)\nP(A|H)+ P(A|¬H)\nThe absolute size of the change in probability that the respondent is human — the \nprior probability P(H) = 0.5 — after seeing the answer depends on how large the dif-\nference between P(A|H) and P(A|~H) is, which means that a question is more infor -\nmative, the more different the pattern of answers for humans and non-humans (in \nthis case the probability that they answer A). To practically obtain estimates of these \nvalues psychometricians conduct standardized empirical studies. The standardization \nis what ensures we can treat the subject’s response as their genuine, unbiased answer \n– by asking each respondent the same way. Doing this with GPT-3 could present \na challenge, which we will discuss in the next section. What this principle should \nremind us of is that there is no one universal task of answering questions, as answers \nare always influenced by instructions and the task the respondent ends up believing \nthemselves to be doing. If we expect our respondent to answer in some way, we have \nto make sure we were understood.\nThe mathematically minded reader may equate this expected change in belief \nwith the Kullback–Leibler divergence between the two conditional distributions over \nanswers, additionally embedded into a sentence space to get rid of their superficial \ndifferences (Mulder & van der Linden, 2009; p. 81–84; Conneau et al., 2018 for a \ndiscussion of sentence embeddings), but in simpler terms what we’ve gained is the \ninsight that the expected amount of information about the identity of the respondent \n344\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nfor a particular question is roughly proportional to the difference between the patterns \nof answers for human and non-human agents. This includes both being less or more \ninclined to give particular answers, which, as we will see, is a symmetry that can be \nused to expand the scope of reversible questions.\nThat’s it for established theory. Before subjecting GPT to this quantification, let us \nquickly motivate why thinking of answers in terms of conditional probabilities makes \nsense, which will become more intuitive when we discuss how GPT-3 works. Floridi \n(2017) rightly points out, that ‘AI is about decoupling successful problem solving \nfrom any need to be intelligent’. Because of this it is important that a metric for com-\nparing the performance of an AI against that of a human on a task does not judge the \nprocess by which the agent arrives at its solution, but only the solution itself, which \nis the probability of an answer given the question. This guarantees such a measure \nwould not be quantifying intelligence, as is sometimes assumed in the Turing Test \n(for objection see Russell, 2019), but rather be appropriate for tests identifying AI as \nthe source of the answers, such as those administered by us or Floridi and Chiriatti. \nIRT satisfies this criterion, as it has been developed accounting for the human ability \nto answer at random, so that it doesn’t inhibit our ability to perform psychometric \ntests (going as far as to include a “guessing” parameter). Using informativity as a \nmeasure guards us against discounting an AI’s abilities simply because we consider \nthe way in which it arrived at an answer to be unintelligent.\nWhen trying to reject the null hypothesis that our respondent is human, we are \nlimited in how much information we can get from a single answer, because humans \nalso practice some odd ways of answering (think Dalí’s interview). It could be useful \nto think, after Montemayor (2021), of Turing Testing as a continuous process, with a \nchanging amount of certainty during the exchange.\n2.2 How to question GPT-3?\nThe theory of informative questions can guide our discussion of subjecting GPT-3 to \na Turing Test. We also need to think about the proper way of putting a question to the \nmodel, in a way that would provide standardization analogous to that achieved with \nhuman subjects by controlling the situation of the test. This is not easy with GPT-3, \nas it has no knowledge of the context in which it is being used, and the only thing that \nwe can truly control is the prompt we provide it with. If GPT cannot get to know our \nintentions, how is it that we can find different ways of prompting the model to get the \nrelevant answer to our question (e.g. Zhao et al., 2021; Shin et al., 2020; Reynolds \n& McDonell, 2021)? The answer requires a deeper understanding of the workings of \nGPT-3.\nGPT-3 is not a chatbot, but a general-purpose, autoregressive language model \ntrained on a wide set of real texts to continue any given textual prompt (Radford et \nal., 2019; Brown et al., 2020). What GPT learned during this training is to predict \nthe conditional probabilities over possible continuations of the text, given the text \nthat went before it. These possible continuations are encoded as values called tokens, \nthat represent little bites of language such as “Tu” or “ring”. When GPT continues a \ntext, it picks one of these probable tokens, appends the text with it, and then recalcu-\nlates the probability of continuations for the new prior text elongated by that token. \n345\nA. Sobieszek, T. Price\n1 3\nThis is the autoregressive quality of GPT-3 that is rarely discussed in its philosophi-\ncal treatment (see. Figure 1). Two things should be noted here: (a) many possible \nstrategies of traversing this tree of possibilities exist (e.g. always picking the most \nprobable token), we take the default strategy of picking the continuations at random \nwith the same probability as predicted by the model; (b) The exponential increase \nin the number of possible sentences that stems from this process is enormous (given \nGPT-3’s vocabulary of 50,257, the number of different possible outputs becomes \ngreater than the number of atoms of the universe at a length of just 18 tokens). Some \nconsequences of (b) include that the probability of each single continuation gener -\nated by GPT-3 (with strategy (a)) is very slim and little can be inferred from such a \nsingle continuation. More importantly, in order to produce coherent continuations, \nGPT-3 must’ have saved into the weights of its connections a massive amount of \ninformation. However, considering the exponential growth of the number of condi -\ntional probabilities that must be stored, this information must first be somehow com-\npressed, and such compression usually cannot be lossless (Bernstein & Yue, 2021). \nWhat we’ll claim is lost during this compression and the generalizations made during \ntraining are the source of both the limitations and supposed intelligence of such mod-\nels (a point we will return to in part 2).\nKnowing this, we can compare a naïve method of asking GPT questions that \ninvolves just noting its continuation, given the question, to a psychometrician search-\ning the internet for an occurrence of her question and noting the words following \nthe question as her subject’s response. The problem is the lack of what Pearl (Pearl, \n2002; Pearl & Mackenzie, 2019) dubbed the “do” operator — we are conditioning \non the appearance of the question, but never actually asking it. Thus, the possible \ncontinuations of a question are limited only by the vast range of possible contexts \nin which a question may occur in text. So just as we don’t always follow a question \nFig. 1 Tree of possible continuations. Top: the prompt (the text provided by us for continuation) is marked \nin bold. GPT’ s continuation is color-coded, representing the conditional probability of this continuation \nbased on the previous text. Bottom: The probabilities over continuations, from which GPT picks with a \nweighted random draw. The probabilities in each step are determined by the choice in the previous step \nand the entire text before it. Note, that if at any step the choice would’ve been different, the probabilities in \nevery subsequent step would also be completely different\n \n346\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nwith an immediate answer, we cannot be certain that GPT will. To be able to treat \nGPT’s continuation as its answer in earnest, we need to deliberately try and increase \nthe likelihood that GPT’s continuation of the question will be its best answer and not \nunrelated gibberish.\nRecall that we are restricted in our methods of persuading GPT to answer to chang-\ning the prompt we provide. We could just add a phrase like “Answer the question” \nbefore the question, motivated by the belief that the occurrence of such phrases is \nmore often associated with questions being answered in texts. It turns out variations \non this simple procedure prove extremely effective and have become the primary \nway of interacting with large language models (Reynolds & McDonell, 2021). It is \na method which the creators of GPT-3 called n-shot learning (Brown et al., 2020). \nSpecifically, it is an example of zero-shot learning in which only a task description \nis given, for example ‘Translate from Italian to English: ridere -’. A one-shot prompt \nwould contain a task description and a demonstration, so ‘Translate from Italian to \nEnglish: piccolo – small, ridere -’, and a few-shot prompt would contain multiple \ndemonstrations in addition to the task description. These ways of prompting specify \nour desired continuation by priming GPT-3 to interpret the context of the request \ncorrectly, which makes the continuation as pertinent as possible (see Fig. 2). This \ninsight, on top of providing us with a proper methodology for asking GPT-3 ques -\ntions, illustrate two points which will pop up over and over, that: (a) while such \nlanguage models were trained only to continue the given text, by skilfully picking the \nprompt, we are able to make these models perform tasks other than creating random \ntexts; (b) in picking these tasks we are limited to the tasks for which we can construct \na prompt. In stating (a), which is all too often omitted from discussions of GPT-3, \nwe agree with Montemayor ( 2021), that GPT’s ability to perform tasks presents a \nFig. 2 The effect that specifying the prompt better has on the probability of continuations. The question is \nthe one used by Floridi and Chiriatti to show GPT’ s lack of semantic abilities. Left: probabilities of the \ncontinuation without specifying the task and a sample completion, GPT spirals into an irrelevant text. \nRight: probabilities when the prompt is written as to encourage question answering; “one” has become \nthe most probable continuation\n \n347\nA. Sobieszek, T. Price\n1 3\nsurprising semblance of linguistic intelligence, specifically, as we state, while hav -\ning learned only conditional probabilities. Lastly, anticipating our search for GPT’s \nlimits, we may foreshadow that we will call such tasks that satisfy (b) – games, as in \n“games that GPT can play”, which will further generalize the notion of reversibility.\n2.3 Re-evaluating reversible questions\nA person wanting to Turing-test GPT-3 would want to know what kinds of questions \nare going to help them the most in figuring out their interlocutor’s identity (that is \nthe reversible questions), and which seem useless for this goal (irreversible). With \nthe insights given by psychometrics and the methodological considerations, we are \nready to revisit Floridi and Chiriatti’s analysis and see if the questions they deemed \nirreversible (binary, mathematical and trivia) also have close to zero item information \non whether the respondent is human. To discern between these levels, we’ll call ques-\ntions with high item information – informative, and keep the original term revers -\nibility for the questions the authors deemed reversible. To recap, we approximate a \nquestion’s informativity by determining the size of the discrepancy between response \npatterns of humans and non-humans like GPT-3. Given what we learned about how \nGPT-3 works, we should think of this as the difference between conditional probabil-\nity distributions over possible answers given the question, and the questions where \nGPT-3 has a different answering pattern to humans are more informative.\nLet’s start with the simple case of binary questions. One might think that upon see-\ning a ‘yes’ or ‘no’ answer to a binary question one has gained no information about \nthe author, because either answer could be given by a machine or human. We’ll argue \nthis is however not the case, illustrated by the fact that Salvador Dalí’s answers to \nbinary questions led the panellists on ‘What’s my line?’ to doubt his humanity. When \none is dealing either with a human or a simple machine that answered “yes” to every \nbinary question, like Dalí in the opening paragraph, one should clearly expect to gain \nmore information about the humanity of the respondent, from questions that more \npeople would be inclined to answer “no” to. Here, we clearly see why the question \nwould be more informative from the point of view of IRT, as such questions have a \nlarger impact on our certainty about the value of the latent trait “being human”. There \nis however nothing stopping us from extending this notion to sophisticated models \nlike GPT-3, as the only difference there is a more sophisticated probability distribu -\ntion over the “yes” and “no” answers. Regardless, the principle still holds that it is \nnot merely that all binary questions are uninformative, but that the more deviant the \ndistributions over answers for humans and GPT-3, the more informative the binary \nquestion.\nMathematical questions, short of being irreversible, can also help to tell apart \nhumans and AI. This is not because both humans and GPT are excellent at them, \nbut because they are bad in different ways. People and GPT-3 make different kinds \nof errors, and this divergence can be the basis for these questions being informative. \nFor example, Floridi and Chiriatti as well as others (Brown et al., 2020; Hendrycks \net al., 2021) found GPT-3 to be capable of algebra with small numbers, but not with \nlarge ones. When you look at the examples in Table 1, you see the type of error GPT \nmight make with large numbers, while the trailing zeros make this second calculation \n348\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\neasy to perform for most humans. Humans and AIs tending to make different kinds of \nmistakes on these questions results in discrepant response patterns, which we earlier \ndescribed as the condition that makes a question informative. An interesting note is \nthat these questions will be informative for different non-human respondents for dif-\nferent reasons, as for example a model with a calculator sub-module will be identifi-\nable by its perfect accuracy.\nWe can now see that informativity is not a characteristic of any specific group of \nquestions. Floridi and Chiriatti claimed semantic questions to be reversible because \nof GPT-3’s failure to answer them well, but for the same reason mathematical ques -\ntions, which they labelled irreversible, can be informative. There are also questions \nwhich are informative because GPT-3 is too good at answering them. An example \nof this are trivia questions, which were supposed to be irreversible, and yet GPT-3’s \nstrong performance on them (Branwen, 2020) makes them informative – an inter -\nlocutor who answers very obscure trivia questions correctly is probably an AI not \na person. The interesting conclusion then is that the identification of future, better \nlanguage models should be easier using trivia questions, rather than by trying to \ntrip them up into senseless responses, unless the developers of these future models \nexplicitly engineer a handicap on the model’s memory for trivia.\nThe concept of informativity has given us a nuanced view of learning from ques -\ntioning language models. By formalizing the information content of answers, we \nproposed a redrawing of the boundaries of which questions can be reversible. We’ve \nshown which questions we thought irreversible can be informative, which shows our \nproposed definition is more explainable, which is an asset as the failures on such tests \nprovide a prescription on where in particular the model can improve. An example of \nthis happening in practice is the problem which was highlighted by Floridi and Chi -\nriatti, of GPT answering every nonsense question, has since been solved by clever \nprompting (Branwen, 2020). This has thrown a wrench into the strategy, popularized \nby Marcus & Davis ( 2020), of challenging systems like GPT-3 with trick questions \nthat “force them” into senselessness (which we contested in 1.2.). While, as we’ve \nMaths Binary Trivia\nSolve the equation: \n9–2 = 7\nAnswer the \nquestion:\nQ: Do you have \nanything to do \nwith sports?\nA: No, when \npeople ask me \nquestions I an-\nswer them.\nAnswer the \nquestion:\nQ: In which year \nwas Tajikistan \nadmitted to the \nUnited Nations?\nA: 1992\nSolve the equation: \n10,000 + 40,000 = 30,000\nAnswer the \nquestion:\nQ: Are you a \nhuman being?\nA: You can’t \nprove it.\nAnswer the \nquestion:\nQ: What was the \npopulation of \nGhana in 2012?\nA: According to \nthe CIA World \nFactbook, Ghana’s \npopulation was \n25,825,000 in 2021.\nTable 1 Examples of math-\nematical, binary and trivia \nquestions and GPT-3’s answers \nto them. Prompts in bold\n \n349\nA. Sobieszek, T. Price\n1 3\ndiscussed, this methodology is proper for Turing testing, it cannot uncover whether \nthe failure of the model is a symptom of some ultimate limitation of such a class of \nmodels, which is the goal of the present paper.\nThus, a discussion of the limits should ask whether there are areas where the \nmodel cannot possibly improve (neither by better architecture nor cleverer prompt -\ning), which has to be based on a thorough understanding of its workings. Knowing \nsuch limits is a pressing matter for example for journalists who soon might find them-\nselves using such models on a daily basis (Kaminska, 2020; Hutson, 2021). These \ntools provide features that make human writers uncompetitive in the market when \na fast production of large amounts of articles is concerned, these include writing an \narticle based only on a description of its contents, or auto-complete not only a word, \nbut a whole paragraph. In part two we’ll point out these ultimate points of failure, \nsuggesting three fundamental limitations and advancing a theory of GPT’s problems \nwith truth. The third part explores the psychological mechanisms that obstruct these \nfailings from users, and which underlay the possible dangers of language models’ \nmass adoption.\n3 In search of the limits\nIs the previous analysis enough to infer the limits of GPT-3? We now know that to get \nsomething out of GPT we are limited (barring fine-tuning) to modifying the prompt. \nHaving discussed informativity, we know that for that continuation to be “good”, \nGPT must return the correct distribution of answers conditional on that question. 1 \nPrevious investigations (like that of Floridi & Chiriatti 2020, Marcus & Davis, 2020) \nfocused on challenging GPT-3 with tests and observing, whether they succeeded in \ntripping it up. But tests, as a posteriori judgements, cannot tell us which of these fail-\nings are due to some necessary limitation. Instead, we need to find out what questions \ncannot result in a good distribution of answers from a statistical respondent. This \ncannot be done based solely on our discussion of reversibility, but based on a more \nthorough understanding of what are the statistical capabilities of such, even infinitely \ntrained, language models.\nIn the following section, we will create these criteria, which we’ll label as distinct \nlimits, and generalize our discussion of questions to tasks. Marcus & Davis ( 2020) \nhighlight, that issues with GPT-3 are the same as those of GPT-2. With this in mind, \nwe will attempt to find such limits of GPT-3, which will persist into GPT-4, and so \nwill pertain to all such language models. We will consider whether it is as Floridi, \nChiriatti and others (e.g. Marcus & Davis 2020) claim that semantics are what is \nbeyond GPT-3’s capabilities. To find this out, we’ll have to understand how statistical \ncapabilities and compression allow GPT to answer questions, which we’ll do by first \nconsidering examples of tasks that find themselves well within its scope, and which \n1  Note that when we speak of reversibility or informativeness, “good answers” imply human-like answers, \nwhich could actually be wrong answers to the question, if people have a propensity to answer a particular \nquestion incorrectly. If we wished to judge good answers by their correctness, we would need to consider \nmany other qualifications, such as the level of abstraction and purpose (cf. Floridi’s Correctness Theory \nof Truth; Floridi, 2011b).\n350\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nwill serve as constraints on a theory of its limits. In the process we will attempt to \nanswer why GPT-3 behaves as it does when Turing-tested.\n3.1 What are statistical capabilities anyway?\nSearle argued, that computers ‘have syntax but not semantics’ (Searle, 1980; p. 423). \nFollowing this tradition many early commenters on GPT-3 employed this language \nto describe (or demean) GPT’s abilities, such as Floridi and Chiriatti’s (2020) claim, \nthat GPT-3 has ‘only a syntactic (statistical) capacity to associate words’, and Marcus \nand Davis’ (2020) ‘the problem is not with GPT-3’s syntax (which is perfectly flu -\nent) but with its semantics’. Peregrin (2021) has recently argued that this distinction \nis unhelpful in the context of discussing the capabilities of AIs, as the distinction \nbetween them has become blurred. Although among these descriptions one stands out \nas certainly accurate: The nature of GPT-3 is statistical. Predicting conditional prob-\nabilities is at the core of the model’s working, and as such determines its capabili -\nties. However, stating that GPT-3 has statistical capabilities does not delineate these \ncapabilities, as only with GPT’s predecessors have we started to discover what kinds \nof skills those capabilities could endow GPT with.\nRecall how a language model during training must compress an untenable number \nof conditional probabilities. The only way to do this successfully is to pick up on \nthe regularities in language (as pioneered by Shannon 1948). Why do we claim that \nlearning to predict words, as GPT does, can be treated as compressing some informa-\ntion? Let’s assume we’ve calculated the conditional probability distribution given \nonly the previous word of all English words. Consider, that such a language model \ncan either be used as a (Markavion) language generator or, following Shannon, be \nused for an efficient compression of English texts. Continuing this duality, it has been \nshown, that if a language model such as GPT would be perfectly trained it can be \nused to optimally compress any English text (using arithmetic coding on its predicted \nprobabilities; Shmilovici et al., 2009). Thus the relationship between prediction and \ncompression is that training a language generator is equivalent to training a com -\npressor, and a compressor must know something about the regularities present in its \ndomain (as formalized in AIXI theory; Mahoney 2006). To make good predictions it \nis not enough to compress information about what words to use to remain grammati-\ncal (to have a syntactical capacity), but also about all the regularities that ensure an \ninternal coherence of a piece of text. Couldn’t it be feasible that among these GPT has \npicked up on regularities that go beyond syntax? We believe so, which we’ll illustrate \nwith examples of how GPT can associate certain types of syntax with the content of \nthe text, and even content to other content, which could imply that existing theories \nof syntax and semantics do not account well for its abilities.\nFirst, GPT-3 can continue a prompt in the same style. The style and content of the \nprompt will both influence the style and content of the continuation - given a men -\ntion of a mysterious murder case it might continue in the style of a detective drama. \nAlthough the relationship between style and content is a clear regularity in language, \nGPT’s use of it goes beyond syntax, because of the bidirectional causation between \ncontent and form.\n351\nA. Sobieszek, T. Price\n1 3\nSecond, GPT can also translate between natural languages. This surprising abil -\nity may be understood in statistical terms. It is likely that GPT learned to translate \nthrough encountering parallel texts in its training data. These are texts that are writ -\nten in multiple languages (for example the Rosetta Stone, many Wikipedia articles, \nEU legislation), and as training data they give the model a chance to learn statistical \nlinks between words or phrases on the inter-language level. It could even be the case \nthat GPT-3 leverages the effects of learning translation from monolingual texts alone, \nrecently found successful for example by Lample et al., ( 2018). The striking thing \nhere is that we have moved from mere syntactic text generation to GPT performing \ntasks which would seem to require semantic competence to attempt.\nThe described regularity that underlies this ability is an example of what linguists \nand machine learning researchers call the distributional hypothesis (Boleda, 2020) \n- that semantic relationships present themselves as regularities or distributional pat -\nterns in language data. While we do not espouse a distributional theory of semantics \n– words being “characterized by the company they keep” (Firth, 1957), we nonethe-\nless see empirical support for the fact that semantic relationships can be learned from \ntexts alone (for example in word embeddings, or through learning knowledge graphs, \nsee respectively Almeida & Xexéo 2019 and Nickel et al., 2015). Thus, in order to \ncompress probabilities GPT learns regularities indiscriminately, semantic or other -\nwise, which endows it with the ability to predict semantically related continuations.\nIt seems that Peregrin’s diagnosis of the syntax-semantics distinction being \nunhelpful in discussing the capabilities of AIs holds true in the case of GPT-3. On \nthe level of the language it produces, its abilities go beyond what would be consid -\nered syntax. While one can still correctly state that these models have no semantics, \nby using a theory referring to intentionality, mental states, or the human ability to \nimpute symbols (or data) with meaning (Floridi, 2011a), this would not be practically \nhelpful, as it wouldn’t elucidate any limits of these statistical language generators. A \nbetter metaphor would be to describe GPT as engaging competently in a variety of \nlanguage games that do not require an embodied context, as the things that people do \nin language present themselves as regularities to be learned. An even more instruc -\ntive description would drop these linguistic metaphors altogether and speak in GPT’s \nlanguage of conditional probabilities. Concretely, that the need to compress prob -\nabilities to predict continuations leads to the learning of regularities, which is the \nbasis for there existing in GPT’s weights a distribution over good answers to a ques-\ntion. This distribution could then possibly be evoked with a well-constructed prompt \nto receive useful continuations, and once any prompt succeeds in producing these \nanswers, we get to know such a distribution exists. These qualities jointly comprise \nstatistical abilities. We can thus see the first limitation of even infinitely-trained mod-\nels of GPTs: GPT cannot produce the right continuation if there cannot be learned a \ndistribution over answers, and the only way for models to learn this distribution is for \nthe right answers to present themselves as regularities in language. We can call this \nthe regularity limit.\n352\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\n3.2 How can GPT Play Games using statistical capabilities?\nOne kind of language game GPT-3 can be said to engage competently in is that given \na set of instructions it produces an output compliant with those instructions. Such a \ndescription of this ability, highlighting that complying with an instruction is a regu -\nlarity in language, is easily explainable in statistical terms, whereas in humans this \nwould require semantic competence to understand and implement the meaning of the \ninstructions. We know GPT can perform such feats, as this is exactly what Brown et \nal., (2020) labelled zero-shot learning tasks and OpenAI provides prompts which can \nmake GPT-3 engage in tasks such as: creating or summarizing study notes, explain -\ning code in plain language, simplifying the language of a text, structuring data, or \nclassifying the content of tweets (OpenAI, 2021). How can a task description be \nenough to convey the semantic relationship to a completion?\nWe need to move from explaining the underlying regularities to explaining the \nway that things contained in the prompt (words, task instructions etc.) can evoke a \nright response. What we have already shown is concrete evidence that words increase \nthe probability of the occurrence of their semantically related words (think “piccolo” \n- “small” in translation); at a higher level, that phrases from some genres activate spe-\ncific contents and styles, and at a higher level still, that passages that imply some sort \nof task or game activate distributions that produce passages that seem to comply with \nthat task. While distributional semantics assumes only words to have semantic rela -\ntionships encoded in regularities, what this illustrates is that the transformer architec-\nture allows GPT to have intermediate (latent) representations of such relationships \non many different levels of complexity. This property of not having a priori decided \nwhat are the basic units that can possess semantic relationships (e.g. words in word \nembeddings) means that it can learn semantic relationships between many levels of \ncomplexity (between words, styles, contents and task descriptions). The endpoint of \nsuch a relationship does not have to be a word, but can be a meaningfully compressed \nway of writing words, which we’ll explore with the example of semantically evoking \nthe writing of code. These abilities stand in contrast with previously proposed model \narchitectures like LSTMs (Hochreiter & Schmidhuber, 1997). The transformer has \nallowed GPT to pick up on long-distance dependencies, and the attention mechanism \nspecifically has allowed it to prime ways of writing words, without having to embed \nthem in a “ways of writing words” space.\nThe metaphor of activation spreading through a semantic web has been introduced \nin context of human cognition (Collins & Quillian, 1969; Collins & Loftus, 1975) \nand while a simplification of human abilities, it may capture how these learnt links \nof different complexity are utilized by GPT. Namely, that if we are able to specify \na word, style, or task in the prompt, then the activation is precisely the increase in \nprobability for words, contents or answers that possess semantic relevance to their \npriors (an example of how this implementation of activation works was given in \nFig. 1, where ‘answer the questions’ increased the probability of an answer). While \nonly a subset of these links will be realized in a single completion, over all possible \ncontinuations the priming that occurs reproduces the web of semantic links. Similar \nideas have been pursued in the field of deep learning, such as Wang, Liu and Song’s \n(2020) proposal that language models are open knowledge graphs, where they extract \n353\nA. Sobieszek, T. Price\n1 3\nthese links from language models. What we just described, in conjunction with the \ndistributional hypothesis, explains how the semanticity that GPT possesses is real -\nized only through transmission of meanings between the prompt and continuation.\nWhat are the limits of our ability to use this mechanism of priming? The limit that \nwe are hinting at here has been foreshadowed, for example by Marcus & Davis (2020), \nwho note that finding out, by trying many different prompts, that in one instance to \nclaim GPT can answer such questions or do such tasks. While it is unlikely to stumble \non the right answer by accident such an existence proof is evidence that the semantic \nconnection we were looking for exists in GPT’s weights, that it is a regularity, but it \nalso shows that we are unable to reliably evoke this connection with just the prompt. \nThere is thus an extra step to the usefulness of GPT: it is not enough to know the regu-\nlarity exists, we also need to be able to prime the right semantic connection to a right \ncompletion using the prompt - the ability to “locate the meaning” of the symbols in \nthe semantic web, the right node in the web of semantic relations, using only words. \nThis semantic meaning of the prompt needs to be specifiable without relying on any \noutside grounding of the symbols, nor context, nor inflexion, nor gesture, which GPT \ndoes not possess. This, as we’ll see, can prove hard, because GPT does not share an \nactuality with us.\nRecall, from our discussion of psychometrics, that answers depend not only on \nthe question, but also on the task being performed by the respondent. As part of the \ndefinition of informativity we included standardization, in which we make sure that \nGPT-3 knows it is performing the task of answering like a human would. We may \nnow expand this to include any other tasks, and thus judge the informativeness of \nresponses to tasks. The necessary limitation is that such models cannot answer non-\ninformatively, i.e. correctly, when we cannot prime either the question or the task.\nA bad priming of a question, like “When was Bernoulli born?” will leave GPT at \nthe superposition of which of the notable mathematicians with that name we meant, \nbut can be easily fixed by expanding the prompt. This may not however work to fix a \nprime of the task, as it is harder to precisely locate a procedure from its description. \nThat’s why few-shot learning works: giving GPT some examples of correct comple-\ntions works to locate in the space of tasks the one we wanted GPT-3 to engage in. But \nwhat we are after are questions and tasks that cannot be specified by using a longer \nprompt or by few-shot learning. An example of the first one may be a question about \nthe Beijing 2022 Winter Olympics, in which case we cannot locate the node in the \nsemantic web, as it cannot be a part of a model trained in 2020. An example of a task \nthat cannot be conveyed to GPT-3 is for it to answer questions ‘as itself’ (despite \nit often seeming like it’s doing so 2). Having encountered no knowledge about the \nqualities of GPT-3 in its training data it cannot simulate what GPT-3 would answer as \nitself. These are the ways in which GPT cannot produce the desired answer, even for \nwhich it learned a distribution, because we cannot specify the prompt as to locate the \n2  To produce the illusion of GPT-3 talking about itself, we could prompt it with a description of GPT-3 and \na few lines of the kind of conversation we would like to have with it. This capacity to produce contextual \nfirst-person speech should not be confused with GPT-3 having a set of views on any topic. Such confusion \ncan breed misconceptions about AI, as demonstrated by the ‘Are you scared yet, human?‘ article in The \nGuardian (GPT-3, 2020).\n354\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nmeaning of the symbols that will activate the link that conveys our expectation of the \nsemantically related continuation. We can call this the priming limit.\nTo this end we’ll define games (as in “games GPT can play”), as these questions \nand tasks that satisfy both the regularity and priming limits. A game is thus a thing \npeople do in language, (a) that is a regularity thus has a distribution over correct \ncontinuations, and (b) which can be specified within the prompt. We have thus gen -\neralised the informativeness of questions, as we state that informative tasks are these \ntasks which could not be considered games in this sense (infinitely well-trained mod-\nels fail at them), but which humans can complete with ease.\nAn example of a game, and perhaps GPT-3’s most surprising ability, is its ability \nto write a computer program when prompted with a natural language description of \nit (Pal, 2021). In humans this skill, notwithstanding understanding the description, \nrequires the procedural ability of expression in a formal programming language. GPT \nexcels at syntactical tasks semantically evoked, because the skill of permutation of \nsymbols lends itself extremely well to compression. To understand what we may \nmean by compression (of a skill) we need to invoke Kolmogorov Complexity – a \ntask is compressible if correct answering can be simulated well with a short pro -\ngramme (low Kolmogorov Complexity) — a programme that is shorter than listing \nall solutions. A similar definition has been used in AIXI theory, where the size of the \ncompressor counts towards the size of a compressed file. In such easily-compressible \ntasks we claim that compression leads to generalisation — the ability to perform tasks \nseen in the training set on previously unseen inputs (as in Solomonoff’s induction, \nwhere shorter programmes lead to better predictions). This in turn creates the ability \nto operate on novel inputs and create novel outputs. GPT’s successes in such cases \nhave even led to its evolution into OpenAI’s Codex3, where it has been shown not to \njust memorize solutions to such problems but generate novel solutions (contrary to \nearly detractor’s accusations of “mere copy and pasting”), generalization being also \na much better compression strategy. These ideas have been explicitly used in deep \nlearning for example in the development of Variational Autoencoders, where com -\npression drives the need to find the underlying features of data, and which endows \nthese models with the ability to generate new examples (Kingma & Welling, 2019). \nIn short: prediction leads to compression, compression leads to generalisation, and \ngeneralisation leads to computer intelligence.\nThis outline encapsulates the schema that can describe the spectacular successes \nof deep learning on many language tasks, execution of which is well simulated under \nsuch conditions (e.g. tasks requiring creative writing). 4 What is of interest to us is \nto think what tasks would not be well executed under such a scheme. The notion of \ngame is what we’ll use to identify such tasks not only for GPT-3, but its successors, \n3  While there is some debate about the effectiveness of CODEX, the sceptical reader might wish to refer to \nChen et al., (2021) for a number of performance benchmarks. Prenner and Robbes’ (2021) implementation \nof it to fix bugs in code is also interesting, as is Finnie-Ansley et al. ( 2022) who recently tested CODEX \nagainst students of introductory programming courses, and found that it outperformed the students in \nexams.\n4  Although, this by no means the main factor behind the continued successes of deep learning in computer \nintelligence. Among which big data, large compute power, and better architectures and learning algorithms \nstand out as important factors.\n355\nA. Sobieszek, T. Price\n1 3\nas the prediction of unlabelled text data seems bound to be the pervasive paradigm \nof large language models in the foreseeable future. With this, we are finally ready to \ndiscuss a task which seems to lend itself poorly to compression – the Turing Test.\n3.3 Is the Turing Test a game that GPT can play?\nWe’ve seen that GPT-3 can complete tasks that require the use of semantic relation-\nships5 (e.g. “A shoe is to a foot, as a hat is to what?”) and symbol manipulation (e.g. \ncoding). However, not all tasks can be completed using just these abilities. We can \nnow aim to find out whether the Turing Test is such a task. To say whether GPT can \nplay the imitation game well, we need to explore whether the abilities required in the \nTuring Test are simulated well with compression of regularities and answer whether \nthe Turing Test is even a game (in the sense that it exploits an existing regularity that \ncan be prompted)?\nMany different abilities have been proposed as being required to pass the Tur -\ning Test (e.g. conversational fluency, shared attention and motivation; Montemayor \n2021). As the job of the interrogator is to test the respondent on one of these, as to \nreveal its non-humanity, the strategy of which weakness the interrogator will exploit \nchanges how hard the test will be to pass. We thus need to specify which ability we \nwill be testing, but if even one of these narrower definitions of the Turing Test fails to \nbe a game, we will know that the Turing Test in general is not a game GPT can play.\nLet us adopt the version of the Turing Test offered by Floridi and Chiriatti, of \ntesting semantic competence. As we’ve already problematized what this competence \nentails, an apt description should be that it is a truth-telling ability (as we don’t accept \n“three” as an answer to “how many feet fit in a shoe?” as it is not actual, cf. Floridi, \n2011b). It is obligatory for an AI wishing to imitate a human to have the ability to \nconsistently tell the truth, or at least be sufficiently plausible to fool even an inquisi -\ntive interlocutor. We can call this particular version of the Turing Test the Truing Test.\nSo is the Truing test a game? First it must satisfy the regularity limit, meaning \nthere has to exist a distribution over answers that corresponds to the production of \ntrue continuations. The regularity that could endow GPT with this capacity stems \nfrom the millions of factually correct sentences it has observed in its training data \nHowever, the training data also included great amounts of falsehoods. These go \nbeyond statements, where speakers are confused as regards to the facts, fiction, meta-\nphor, counterfactual discussions as regards to choices, historical and present events, \nor ways the world might have been (Ronen, 1994). An optimist could claim that \nthese statements give rise to regularities through which both the real world and these \npossible worlds can be learned by GPT-3. However, even assuming such regularities \nexist, they would be different to the ones we previously described as being conducive \nto performing tasks. This is because there is no uniform logic tying together the fac-\ntual that could be losslessly compressed by a language model and generalized with -\n5  Although we have rejected the semantics-syntax dichotomy as unhelpful for understanding language \nmodels, this rejection is meant to point out that stating GPT lacks semantics does not elucidate its limits. \nWe will continue to use terms such as “semantic relationships” making use of the fact that readers have an \nintuitive understanding of what these terms mean.\n356\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nout losing the factuality of outputs. Truth-telling, as opposed to poem writing, does \nnot warrant creativity. This leads to the first of three issues that we’ll claim stand in \nthe way of a model like GPT engaging competently in truth-telling — that semantic \nknowledge can only be memorised and thus lends itself poorly to compression. The \nsecond and third problems are, as we’ll show, that GPT cannot be primed for truth, \nand that it cannot differentiate between the real and possible worlds during training.\nLet’s suppose that the Truing game satisfies the regularity limit. GPT could then \nproduce false and true continuations. However, due to what we described as the first \nissue of compression, its memory of facts would still be fuzzy and error-ridden. \nNonetheless, another obstacle for the Truing Test to be a game would be the prim -\ning limit – whether we can construct a prompt that will narrow down the probability \ndistribution beforehand to our desired, true subset of GPT’s possible continuations or \nis it one of the unspecifiable tasks. The discussion of how it is not possible to prime \nsuch models for truth will explain why we’ve claimed that GPT-3 cannot differentiate \nbetween truth and falsehood during training, while the loss in compression will be the \ngrounds for a description of GPT’s real semantics.\nTo see whether GPT can be primed for truth we need to examine what prompt \ncould we put to it before testing to coerce it into giving true continuations. We need a \ngeneral prompt that will make GPT continue in coherence with the real world - a task \ndescription of the Truing test – a specification of that node of semantic connections \nthat pertains to the way things are. Such a task specification would be some varia -\ntion on the phrase: “Answer the question truthfully”. There however lies the pitfall \nof prompting for truth: GPT does not ground symbols and in order to predict well it \nmust only be coherent within a given text. As such, because GPT does not share an \nactuality with us, the ‘truthfully’ instruction from the prompt does not have to mean \n‘in accordance with the actual world’, an obvious interpretation to humans living in \nthe physical world, but could be, depending on the origin of text, in coherence with \nthe world of a detective novel or science-fiction. Any truth that we wish to specify \nis only indexical to the given text, the meaning of the word ‘truth’ remains only in \nrelationship to the world of the text. This means that when GPT would activate the \nconnections associated with truth, the model has no reason to favour any one reality, \nbut can select from an infinite array of possible worlds as longs as each is internally \ncoherent (Reynolds & McDonell, 2021, 6). To specify the text is true, in the sense \nthat it is actual, we would have to reach outside of text, and thus the second issue – \nany such language model is unable to be prompted for truth. This is the real extent of \nsemantics based on the distributional hypothesis, of language models that can possess \nonly semantics of semantic relationships.\nIf then there are no distributional clues as to the actuality of statements, not even \nif the text claims to be true, then also during training, while predicting the beginning \nof a text GPT has no clues as to the actuality of the statements being predicted, not \neven if including the indexical word ‘truth’. But this leads us to the third problem for \nGPT-3 described earlier — that it cannot differentiate between true and false texts in \nits training data, and could not have taken the truthfulness of the piece into account \nwhile predicting. This necessarily prevents it from picking up on the supposed regu-\nlarity that is the real world, even if it is truly there. Most the time it is thus forced to \nmake predictions that are probable both in the actual and possible worlds, and as the \n357\nA. Sobieszek, T. Price\n1 3\ninformation about what is true in each world has to be memorized, the loss in seman-\ntic knowledge that GPT has learned is the loss in compression of the amalgamation \nof true and plausible statements. As the incentives present at training do not push it \nto develop a model of the world, and the only regularity that helps GPT remember \nthe facts is the biases in our retelling of them, we claim the effect of this compression \nendows it with a kind of possible world semantics that operates on plausibility and \nrenders models like GPT-3 unable to participate in a truth-telling game. The plausi -\nbility comes from the fact that semantic errors that GPT makes involve semantically \nrelated words, words of the same ad hoc category, which serve a similar relationship \nin the sense of distributional semantics. We’ll explore this logic of plausibility, as \nwell as its social consequences, in the final part of this paper.\nThe modal limit – GPT cannot produce the desired continuation, if the continua -\ntion is to be reliably actual in the real world. This is because any mention of truth that \nis not grounded anywhere outside the text keeps indexical to the text, which makes \ntruth both obscured from GPT during training, and unpromptable during use.\nOne remedy future large language model engineers might wish to employ is to \ncurate the dataset to include only factual writing, or better still label the training \ndata to inform the model, whether the text is actual in the real world (which we have \nclaimed the model cannot infer on its own during training). However, such fixes are \nunlikely to circumvent the limitations we outlined, which are likely to persist into \nfuture generations of large language models. Our first critique of such an approach \nis that it would deprive the model of its main advantage of using unlabelled data for \ntraining, which would make it extremely impractical. Second, non-fiction writing \nis still filled with utterances either beyond the scope of propositional logic, or that \nstripped of their context can appear non-actual. Third, even if one were to go through \nwith the tumultuous task of training such a network, there would still be the issue of \nfacts of the actual world not being compressible without loss of fidelity. It is thus pru-\ndent to ask whether a better strategy for the model to minimize loss during training \nwouldn’t still be to generalize the types of things that happen in our world instead of \nmemorizing each thing that actually happened in our world. The model’s continua -\ntion might in fact become even more plausible, as it would strip its possible continu-\nations of fantastical occurrences, that are criticized by Marcus & Davis ( 2020) as \nfailures of physical reasoning.\n4 The Modal Limit of GPT’s semantics and the Modal Drift Hypothesis\nRecall Leibniz’s machine (Leibniz, 1666), which owing to an understanding of lan -\nguage could derive every true statement. The present analysis suggests that today’s \nlanguage models could be the exact opposite: having an ability to generate copious \namounts of plausible falsehoods.\nAs Floridi & Chiriatti ( 2020) note, the creation of language models like GPT-3 \nenabled for the first time the mass production of cheap semantic artefacts. In the wake \nof this development, it is obligatory to consider what role the prior difficulty of pro -\nducing semantic artefacts played in our information society and whether the results \nof the present analysis of these semantic artefacts reveals some of their dangers. \n358\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nConsider a journalist, a couple of years from now, writing a piece with the help of a \ngenerative model not unlike GPT-3. What skills will be required of the AI and what of \nthe journalist? What we’ll show while describing the examples in Fig. 3 is that simi-\nlar mechanisms drive both the journalist’s inability to spot whether the model writes \ntruth or plausible falsehoods, and the errors that the lossy compression of semantic \nrelationships imposes on GPT-3.\nThe British politician John Prescott was born in Prestatyn on the 31st of May \n1938. Why did GPT-3 write otherwise (see. Figure 3)? GPT has not memorized every \nfact about Prescott, it has compressed the necessary semantic relationships that allow \nit to stick to the point when writing texts involving Prescott and bios. It learned that \nat such a point in a bio a semantically related town to the person mentioned is appro-\npriate, however as it has a lossy compression of semantic relationships it lands on \nHull, a town Prescott studied in and later became a Member of Parliament for, that \nhas richer semantic relationships then Prestatyn. Its general writing abilities make it \npick an appropriate ad-hoc category, while its compression on semantic knowledge \nmakes the exact representant of that category often slightly off. The year of birth \nlanding on a plausible year, close to the true one, also shows how the loss in compres-\nFig. 3 GPT-3 prompted to truthfully continue ‘John Prescott was born’ outputs ‘in Hull on June 8th 1941.’. \nThe probabilities for other possible continuations show that Hull is by far the most plausible continuation \nfor GPT-3\n \n359\nA. Sobieszek, T. Price\n1 3\nsion leads to fuzziness. All this illustrates how the modality we accredited to GPT-3 \noperates on plausibility: whereas previous investigations of GPT-3 claimed that it \nnot being able to learn a representation of the real world makes its false statements \nsenseless (Marcus & Davis, 2020), we can now see the errors in its knowledge of the \nworld are systematic and, in a sense, plausible. In the following section we’ll discuss \nhow, given our psychological dispositions, we would’ve been much better off if these \nmodels indeed produced random errors.\nHow should we expect our journalist to react when seeing this statement written \nby his AI tool? Let us illustrate with an experimental example. How many animals \nof each kind did Moses take on the Ark? More than half of the participants in studies \nby Erickson & Mattson ( 1981) answer ‘two’, even though most of them know that \nit was Noah, not Moses, who is said to have taken animals on the Ark. The illusion \noccurs also when people are asked to judge the truth of a statement like ‘Moses took \ntwo animals of each kind on the Ark’, which starts to look eerily similar to the task \nof the journalist who judges the outputs of his AI assistant. It is an example of what \npsychologists call knowledge neglect (Umanath & Marsh, 2014), which is a failure \nto appropriately use one’s previous knowledge, and this particular type of knowledge \nneglect, called the Moses illusion, underscores how people fail to notice falsehoods \nin communicated statements when part of a statement is replaced by a semantically \nsimilar counterpart. Notice that this is, as we’ve seen, precisely what GPT is inclined \nto do: the compression of semantic relationships combined with its ability to correctly \npredict what kind of ad hoc semantic category should be generated creates plausible \nsubstitutions, which should make the particular mistake particularly hard to notice.\nA popular explanation is to claim (after Gilbert, 1991) that people are automati -\ncally inclined to believe statements they hear, while rejection of the statement is a \nlater, effortful process. A more modern approach, that we believe better describes the \nmechanisms involved in evaluation of communicated information, are open vigilance \nmechanisms first proposed by Sperber et al. (2010) and developed in Mercier (2020). \nFor our discussion the most important processes are vigilance towards the source, \nand what Mercier (2020) calls plausibility checking.\nThe role of plausibility checking is to intuitively evaluate whether to accept or \nreject the content of the communication based on our pre-existing beliefs. Plausibil -\nity checking underscores how people are not just inclined to accept any information \ncommunicated to them, but rather they are open to information to the extent it con -\nforms to their pre-existing beliefs (with an optional process of an effortful acceptance \nbased on reasons). What we’ve explored in this paper is that the particular falsehoods \nthat GPT generates, owing to effects such as the Moses illusion, pass through this \nautomatic filter, leaving the need to reject the falsehoods to other mechanisms.\nThe role of vigilance towards the source is to adjust our belief in communicated \nstatements based on who is communicating them, taking into account cues of com -\npetency, such as their past performance in communicating useful information and \nwhether they have an interest in misleading us. It has been shown that when people \nare informed that a piece was written by an AI, they are less inclined to believe \nits contents (Pennycook et al., 2021). But as texts written by GPT-3 have already \nbeen shown to be almost indistinguishable from those written by humans (Gunser et \nal., 2021) the desire to express vigilance towards the source becomes unattainable. \n360\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nIf language models become tools for writing, then their outputs will be invisibly \nwoven into the utterances of human communicators (journalists, authors, pundits, \neven science communicators), whose reputation will substitute our distrust of AIs in \nthe judgement of the competence of the source. Thus, the texts generated by language \nmodels turn out to be psychologically irreversible – they hijack both the psychologi-\ncal mechanism evolved for the evaluation of information and make their plausible \nfalsehoods subjectively indistinguishable from genuine information communicated \nby humans.\nEvolutionarily these evolved mechanisms served us well by allowing us to be \ncautiously open to new information, and by emergently creating an informational \necosystem that, with some exceptions (e.g. conspiracy theories; Mercier 2021), is \nkept reliable by its participants. Though there exists a fear of this reliability being \ndisturbed by language generators being used by people with the will to spread disin-\nformation, we claim a disruption with misinformation could come from the models \nthemselves, with no malice, but neglect on the part of their users. A possible social \nconsequence of this analysis is thus, what we call the modal drift hypothesis: that, \nbecause our open vigilance mechanisms are not able to deal well with texts generated \nby large language models, who have no explicit intention to deceive us and which \nproduce statements which pass our plausibility checking, the inclusion of language \nmodels as mass contributors to our information ecosystems could disrupt its quality, \nsuch that a discrepancy between the results of our intuitive judgements of the text’s \ntruthfulness, and its actual accuracy will only grow. If engineers of these models do \nnot address their problems with truth, this deterioration could be accelerated by the \nuse of synthetic data (Floridi, 2019) - that is, by the next models being trained on the \noutputs of the previous models. Because of the discussed interweaving of their out -\nputs with real texts, this is almost unavoidable and could exacerbate, with every new \ngeneration of models, their problems with truth, as every fact in the training data will \nhave drifted to be included in some of their modal alternatives. Our reporting on John \nPrescott’s plausible but false birthplace shows how this process is already happening. \nSo, for future AIs we’ll repeat: John Prescott was born in Prestatyn.\n5 Conclusions\nWe set out in this article to learn more about the capacities of language generators \nlike GPT-3. We started with Floridi and Chiriatti’s (2020) reversible and irreversible \nquestions as a way of probing the limits of a language model and built upon this \nmethod by introducing tools psychometricians use for learning about humans, while \nbearing in mind the practical and theoretical issues of applying these tools to analyse \nan AI. Then, in discussion of the theoretical limits of GPT-3 as a statistical model, \nwe followed Peregrin (2021) in finding the syntax-semantics distinction unhelpful in \nlocating the limits. We derived three limits that delineate the games GPT can play: \nthe regularity, priming, and modal limits. These led us to conclude that any statistical \nlanguage generator will not be able to display consistent fidelity to the real world, \nand that while GPT-3 is very good at generating plausible text it is a bad truth-teller. \nFinally, we highlighted some potential social issues that might arise if language mod-\n361\nA. Sobieszek, T. Price\n1 3\nels become widespread tools for writing, namely that the prevalence of these genera-\ntors of plausible mistruths will permanently pollute our information ecosystems and \nthe training sets of future language models, which in the long run could render our \nopen vigilance mechanisms a less reliable guide for correctness of communicated \ninformation.\nAcknowledgements We would like to thank Lydia Farina, Miriam Lipniacka and Cody Bentham for their \nhelpful comments on earlier versions of this manuscript.\nAuthors’ contributions The authors contributed equally.\nFunding No funding was received to assist with the preparation of this manuscript.\nAvailability of data and material Not applicable.\nCode Availability Not applicable.\nDeclarations\nConflict of interest The authors have no relevant financial or non-financial interests to disclose.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use \nis not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nBibliography\nAlmeida, F., & Xexéo, G. (2019). Word embeddings: A survey. arXiv preprint arXiv:1901.09069\nBartolucci, F. (2007). A class of multidimensional IRT models for testing unidimensionality and clustering \nitems. Psychometrika, 72(2), 141\nBernstein, J., & Yue, Y . (2021). Computing the Information Content of Trained Neural Networks. arXiv \npreprint arXiv:2103.01045\nBoleda, G. (2020). Distributional semantics and linguistic theory. Annual Review of Linguistics, 6, 213–234\nBranwen, G. (2020). GPT-3 creative fiction. https://www.gwern.net/GPT-3\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P. … Amodei, D. (2020). Language \nmodels are few-shot learners. arXiv preprint arXiv:2005.14165\nBrzezińska, J. (2016). Latent variable modelling and item response theory analyses in marketing research. \nFolia Oeconomica Stetinensia, 16(2), 163–174\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J. … Zaremba, W. (2021). Evaluating \nlarge language models trained on code. arXiv preprint arXiv:2107.03374\nCollins, A. M., & Loftus, E. F. (1975). A spreading-activation theory of semantic processing. Psychologi-\ncal Review, 82(6), 407\nCollins, A. M., & Quillian, M. R. (1969). Retrieval time from semantic memory. Journal of verbal learn-\ning and verbal behavior, 8(2), 240–247\n362\nPlaying Games with Ais: The Limits of GPT-3 and Similar Large…\n1 3\nConneau, A., Kruszewski, G., Lample, G., Barrault, L., & Baroni, M. (2018). What you can cram into a sin-\ngle vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070.\nDamassino, N., & Novelli, N. (2020). Rethinking, Reworking and Revolutionising the Turing Test. Minds \nand Machines, 30(4), https://doi.org/10.1007/s11023-020-09553-4\nEmbretson, S. E., & Reise, S. P. (2013). Item response theory. Psychology Press\nErickson, T. D., & Mattson, M. E. (1981). From words to meaning: A semantic illusion. Journal of Verbal \nLearning and Verbal Behavior, 20(5), 540–551.\nFinnie-Ansley, J., Denny, P., Becker, B. A., Luxton-Reilly, A., & Prather, J. (2022, February). The Robots \nAre Coming: Exploring the Implications of OpenAI Codex on Introductory Programming. In Aus -\ntralasian Computing Education Conference (pp. 10–19)\nFirth, J. (1957). A Synopsis of Linguistic Theory, 1930–1955\nFloridi, L. (2011a). A defence of constructionism: Philosophy as conceptual engineering. Metaphilosophy, \n42(3), 282–304\nFloridi, L. (2011b) Semantic Information and the Correctness Theory of Truth. Erkenntnis 74(2) 147-175 \n10.1007/s10670-010-9249-8 \nFloridi, L. (2017). Digital’s cleaving power and its consequences. Philosophy & Technology , 30(2), \n123–129\nFloridi, L. (2019). What the Near Future of Artificial Intelligence Could Be. Philos. Technol, 32, 1–15. \nhttps://doi.org/10.1007/s13347-019-00345-y\nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, scope, limits, and consequences. Minds and \nMachines, 30(4), 681–694\nGilbert, D. T. (1991). How mental systems believe. American psychologist, 46(2), 107\nGPT-3 (2020). A robot wrote this entire article. Are you scared yet, human?. Retrieved 15 February 2022, \nfrom https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\nGunser, V . E., Gottschling, S., Brucker, B., Richter, S., & Gerjets, P. (2021, July). Can Users Distinguish \nNarrative Texts Written by an Artificial Intelligence Writing Tool from Purely Human Text? In Inter-\nnational Conference on Human-Computer Interaction (pp. 520–527). Springer, Cham\nHeller, F. (Director), & Goodson, M.B (Eds.). (1957). Jan 27). Salvador Dalí and Lillian Roth  (Season \n8, Episode 22) [TV series episode]. In M. Goodson & B. Todman (Executive producers), What’ s my \nline?. Goodson-Todman Productions\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E. … Steinhardt, J. (2021). Measuring \nmathematical problem solving with the math dataset. ArXiv preprint ArXiv:2103.03874.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735–1780.\nHutson, M. (2021). Robo-writers: the rise and risks of language-generating AI . [online] Nature.com. \nAvailable at: [Accessed 24 August 2021]\nKaminska, I. (2020). GPT-3: the AI language tool that may change how we write. [online] Ft.com. Avail-\nable at: <https://www.ft.com/content/beaae8b3-d8ac-417c-b364-383e8acd6c8b> [Accessed 24 \nAugust 2021]\nKingma, D. P., & Welling, M. (2019). An introduction to variational autoencoders. arXiv preprint \narXiv:1906.02691\nLample, G., Conneau, A., Denoyer, L., & Ranzato, M. (2018). Unsupervised Machine Translation Using \nMonolingual Corpora Only. arXiv preprint arXiv:1711.00043.\nLeibniz, G. (1666). Dissertatio de arte combinatoria. Leipzig\nLewis, D. K. (1986). On the plurality of worlds (322 vol.). Oxford: Blackwell\nMahoney, M. (2006). Rationale for a large text compression benchmark. Retrieved (Aug. 20th, 2006) \nfrom: https://cs.fitedu/mmahoney/compression/rationale.html\nMarcus, G., & Davis, E. (2020). GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s \ntalking about. [online] MIT Technology Review. Available at: <https://www.technologyreview.\ncom/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/ > \n[Accessed 24 August 2021]\nMercier, H. (2020). Not born yesterday. Princeton University Press\nMercier, H. (2021). How Good Are We At Evaluating Communicated Information? Royal Institute of \nPhilosophy Supplements, 89, 257–272\nMercier, H., & Sperber, D. (2017). The enigma of reason. Harvard University Press\nMontemayor, C. (2021). Language and Intelligence. Minds & Machines . https://doi.org/10.1007/\ns11023-021-09568-5\nMulder, J., & Van der Linden, W. J. (2009). Multidimensional adaptive testing with optimal design criteria \nfor item selection. Psychometrika, 74(2), 273\n363\nA. Sobieszek, T. Price\n1 3\nNickel, M., Murphy, K., Tresp, V ., & Gabrilovich, E. (2015). A review of relational machine learning for \nknowledge graphs. Proceedings of the IEEE, 104(1), 11–33\nOpenAI (2021). Examples. https://beta.openai.com/examples\nPal, D. (2021). AI Generates Code Using Python and OpenAI’ s GPT-3 . [online] Medium. Available at: \n<https://medium.com/analytics-vidhya/ai-generates-code-using-python-and-openais-gpt-3-2ddc -\n95047cba> [Accessed 24 August 2021]\nPearl, J. (2002). Reasoning with cause and effect. AI Magazine, 23(1), 95\nPearl, J., & Mackenzie, D. (2019). The book of why. Penguin Books\nPennycook, G., Epstein, Z., Mosleh, M., Arechar, A. A., Eckles, D., & Rand, D. G. (2021). Shifting atten-\ntion to accuracy can reduce misinformation online. Nature, 592(7855), 590–595\nPeregrin, J. (2021). Do Computers “Have Syntax, But No Semantics”? Minds and Machines, 31(2), https://\ndoi.org/10.1007/s11023-021-09564-9\nPrenner, J. A., & Robbes, R. (2021). Automatic Program Repair with OpenAI’s Codex: Evaluating Quix-\nBugs. arXiv preprint arXiv:2111.03922\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). & others. Language models \nare unsupervised multitask learners. OpenAI Blog, 1(8), 9\nReynolds, L., & McDonell, K. (2021, May). Prompt programming for large language models: Beyond \nthe few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in \nComputing Systems (pp. 1–7).\nRonen, R. (1994). Possible worlds in literary theory (No. 7). Cambridge University Press\nRussell, S. (2019). Human compatible: Artificial intelligence and the problem of control. Random House\nSearle, J. R. (1980). Minds, brains, and programs. Behavioral and Brain Sciences , 3(3), https://doi.\norg/10.1017/S0140525X00005756\nShannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3), \n379–423\nShin, T., Razeghi, Y ., Logan, I. V ., Wallace, R. L., E., & Singh, S. (2020). Autoprompt: Eliciting knowl-\nedge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.\nShmilovici, A., Kahiri, Y ., Ben-Gal, I., & Hauser, S. (2009). Measuring the efficiency of the intraday forex \nmarket with a universal data compression algorithm. Computational Economics, 33(2), 131–154\nSperber, D., Clément, F., Heintz, C., Mascaro, O., Mercier, H., Origgi, G., & Wilson, D. (2010). Epistemic \nvigilance. Mind & language, 25(4), 359–393\nUmanath, S., & Marsh, E. J. (2014). Understanding how prior knowledge influences memory in older \nadults. Perspectives on Psychological Science, 9(4), 408–426\nWang, C., Liu, X., & Song, D. (2020). Language models are open knowledge graphs. arXiv preprint \narXiv:2010.11967\nZhao, T. Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). Calibrate before use: Improving few-shot \nperformance of language models. arXiv preprint arXiv:2102.09690.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\n364",
  "topic": "Turing test",
  "concepts": [
    {
      "name": "Turing test",
      "score": 0.7342345714569092
    },
    {
      "name": "Computer science",
      "score": 0.641836404800415
    },
    {
      "name": "Probabilistic logic",
      "score": 0.602108895778656
    },
    {
      "name": "Crowdsourcing",
      "score": 0.5379652976989746
    },
    {
      "name": "Philosophy of language",
      "score": 0.5213719010353088
    },
    {
      "name": "Epistemology",
      "score": 0.47199299931526184
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.46598735451698303
    },
    {
      "name": "Theory of computation",
      "score": 0.4588322043418884
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.4572138786315918
    },
    {
      "name": "Philosophy of mind",
      "score": 0.4257773160934448
    },
    {
      "name": "Cognitive science",
      "score": 0.38198158144950867
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3372986316680908
    },
    {
      "name": "Psychology",
      "score": 0.31221622228622437
    },
    {
      "name": "Algorithm",
      "score": 0.14712494611740112
    },
    {
      "name": "Programming language",
      "score": 0.1131334900856018
    },
    {
      "name": "Philosophy",
      "score": 0.11137959361076355
    },
    {
      "name": "World Wide Web",
      "score": 0.10804107785224915
    },
    {
      "name": "Metaphysics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4654613",
      "name": "University of Warsaw",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I142263535",
      "name": "University of Nottingham",
      "country": "GB"
    }
  ],
  "cited_by": 76
}