{
  "title": "Visual Language Modeling on CNN Image Representations",
  "url": "https://openalex.org/W2158766051",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1983280281",
      "name": "Kato, Hiroharu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2516803589",
      "name": "Harada Tatsuya",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1982428585",
    "https://openalex.org/W2097039814",
    "https://openalex.org/W2152233525",
    "https://openalex.org/W1510835000",
    "https://openalex.org/W2119228922",
    "https://openalex.org/W2964145162",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W1557127310",
    "https://openalex.org/W1517086206",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W1568514080",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2962741254",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2898422183",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W284368147",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1948843088",
    "https://openalex.org/W2126398289",
    "https://openalex.org/W1915485278",
    "https://openalex.org/W1932198206",
    "https://openalex.org/W2210809762",
    "https://openalex.org/W1625255723",
    "https://openalex.org/W2078903912",
    "https://openalex.org/W2138046011",
    "https://openalex.org/W2068396506",
    "https://openalex.org/W2273348943",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W2128272608",
    "https://openalex.org/W1934890906",
    "https://openalex.org/W2169632643",
    "https://openalex.org/W1999931200",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2170942820",
    "https://openalex.org/W2161366920",
    "https://openalex.org/W2131846894",
    "https://openalex.org/W2107055466",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W1991857366",
    "https://openalex.org/W2164084182",
    "https://openalex.org/W2951446714",
    "https://openalex.org/W1976101156",
    "https://openalex.org/W2037328649",
    "https://openalex.org/W2041719651",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2155541015",
    "https://openalex.org/W189596042",
    "https://openalex.org/W2504108613",
    "https://openalex.org/W648143168",
    "https://openalex.org/W2139047169",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2135181320",
    "https://openalex.org/W1732154880",
    "https://openalex.org/W2133589685",
    "https://openalex.org/W2136922672",
    "https://openalex.org/W2155893237",
    "https://openalex.org/W154882957"
  ],
  "abstract": "Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.",
  "full_text": "Visual Language Modeling on CNN Image Representations\nHiroharu Kato\nThe University of Tokyo\nkato@mi.t.u-tokyo.ac.jp\nTatsuya Harada\nThe University of Tokyo\nharada@mi.t.u-tokyo.ac.jp\nAbstract\nMeasuring the naturalness of images is important to gen-\nerate realistic images or to detect unnatural regions in im-\nages. Additionally, a method to measure naturalness can\nbe complementary to Convolutional Neural Network (CNN)\nbased features, which are known to be insensitive to the\nnaturalness of images. However, most probabilistic image\nmodels have insufﬁcient capability of modeling the com-\nplex and abstract naturalness that we feel because they are\nbuilt directly on raw image pixels. In this work, we assume\nthat naturalness can be measured by the predictability on\nhigh-level features during eye movement. Based on this as-\nsumption, we propose a novel method to evaluate the natu-\nralness by building a variant of Recurrent Neural Network\nLanguage Models on pre-trained CNN representations. Our\nmethod is applied to two tasks, demonstrating that 1) using\nour method as a regularizer enables us to generate more\nunderstandable images from image features than existing\napproaches, and 2) unnaturalness maps produced by our\nmethod achieve state-of-the-art eye ﬁxation prediction per-\nformance on two well-studied datasets.\n1. Introduction\nMeasuring naturalness of images is an important prob-\nlem. By measuring naturalness, one can generate realistic\nimages or detect unnatural regions in images.\nConvolutional Neural Networks (CNNs) [34] extract and\nabstract features from raw image pixels hierarchically. Be-\ncause representations they have learned for image classiﬁ-\ncation are highly discriminative and generalized [10, 48],\nthey are an extremely important component in computer vi-\nsion. However, they are known to be insensitive to the nat-\nuralness of images. For example, images generated through\nthem appear to be strange and unrealistic [40]. Furthermore,\nthey are susceptible to unnatural noise or artiﬁcial fabrica-\ntion [16, 41, 51]. Therefore, a method to measure natural-\nness can be complementary to CNN features.\nDespite the importance of measuring the naturalness of\nimages, few alternative methods exist. Although many\nInput Image\t\r Mid-Level Representation\t\r\nCNN\t\r Prediction Error \nby RNNLM\t\r\nUnnaturalness Map\t\r\nRNNLM\t\r\nRNNLM\t\r\ns1\t\r\nh1\t\r\ny1\t\r\ne1\t\r\ns2\t\r\nh2\t\r\ny2\t\r\ne2\t\r\ns3\t\r\nh3\t\r\ny3\t\r\ne3\t\r\ns4\t\r\nh4\t\r\ny4\t\r\ne4\t\r\ns5\t\r\nh5\t\r\ny5\t\r\ne5\t\r\nst\t\r\nht\t\r\nyt\t\r\net-1\t\r\nInput Layer\t\r\nHidden Layer\t\r\nPrediction Layer \n(Natural Sequence)\t\r\nPrediction Error \n(Unnaturalness)\t\r\nRNNLM\t\r\nFigure 1. Our pipeline to measure the unnaturalness of an image.\nFirst, mid-level representations are taken from the image using a\npre-trained CNN. After they are normalized and orthogonalized,\nfor each row and column, the naturalness of the sequence of fea-\nture vectors is evaluated using a Recurrent Neural Network Lan-\nguage Model (RNNLM). Naturalness by RNNLM is based on the\npredictability of the sequence. Finally, the “unnaturalness map” is\nsummed up to the unnaturalness score.\nprobabilistic image models have been proposed [1, 19, 29,\n33, 45], most are applicable only for small image patches,\nnot for large natural images. Moreover, they are generally\nbuilt directly on raw image pixels. Although it is a preferred\nproperty for low-level image processing such as image de-\nnoising, they have insufﬁcient capability of modeling com-\nplex and abstract naturalness as we feel.\nIn this work, we assume that naturalness can be mea-\nsured by predictability on high-level features during eye\nmovement. For example, during moving of our eyes from\nleft to right, one feels strangeness when viewing a scene that\nis not predicted by what we have seen along the way. This\nstrangeness is presumably based on edges, shapes, or more\nsemantic signals: not on pixel-level information.\nThis type of naturalness is studied extensively and is\nused widely in natural language processing [43]. Such a\nmethod, called Language Model (LM), is applied mainly\nfor regularizing outputs of speech recognition or machine\ntranslation systems. Given a sequence of words, LM pre-\n1\narXiv:1511.02872v1  [cs.CV]  9 Nov 2015\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 2. Unnaturalness maps computed from representations of\nVGGNet [48]: (a) Original image. (b–f) Computed from the\noutputs after conv1 1, conv2 1, conv3 1, conv4 1, and\nconv5 1 layer.\ndicts the next word from past words for each timestep and\ncomputes the naturalness of the entire sequence as the prod-\nuct of prediction accuracy of all timesteps. Although tradi-\ntional n-gram models, which predict the next word from the\nprior n words, are still prevalent, LM based on Recurrent\nNeural Networks (RNNs) [39] has emerged as a favorable\noption because it can predict the next word using more than\nnwords.\nMotivated by the description above, we propose a\nmethod to evaluate image naturalness by building a variant\nof RNNLM on mid-level image representations extracted\nfrom a pre-trained CNN. RNNLM cannot accommodate\ntwo-dimensional representations. Therefore, we apply it\nvertically and horizontally. We designate this method as\nVisual Language Model on CNN (CNN-VLM). Figure 1\npresents an illustration of our method. Figure 2 presents\nan example of unnaturalness (or prediction error) maps ob-\ntained using our method. CNN-VLM measures the natural-\nness using contextual information related to high-level dis-\ncriminative features of CNN, which is difﬁcult to accom-\nplish using probabilistic image modeling on raw pixels.\nThe concept to treat image representations like words\nis related to the well-known Bag-of-Visual-Words [7, 49].\nHowever, in our case, the representations are not quantized.\nWe apply our method to 1) image reconstruction from\nimage features and 2) eye-ﬁxation prediction. The natural-\nness of images plays an important role in these two tasks.\nWe brieﬂy describe them below.\nImage reconstruction from image features is a task to vi-\nsualize an image feature by reconstructing an image from\nit. To reconstruct interpretable images, regularizers of some\nkind must be imposed on images [38, 47]. We demonstrate\nempirically that using CNN-VLM as a regularizer enables\nus to generate more understandable images than those pro-\nduced using two existing methods.\nHuman eye ﬁxation points on images were shown to be\npredictable using Shannon’s “self-information” [5]. In fact,\nmany attention models are explainable from the perspective\nof information theory [3]. Because self-information can be\ninterpreted as unlikelihood or unnaturalness, our unnatural-\nness map is useful as a saliency map. We demonstrate that\nCNN-VLM achieves state-of-the-art performances on eye\nﬁxation prediction task in the experiment section.\nOur contributions are summarized as follows. 1) Based\non the assumption that naturalness can be measured by\nthe predictability on high-level features during eye move-\nment, we proposed a novel method to evaluate natural-\nness by building a variant of RNN Language Models on\npre-trained CNN representations. 2) We conﬁrmed that\nusing our method as a regularizer enables us to generate\nmore understandable images from features than existing ap-\nproaches. 3) We showed that unnaturalness maps produced\nusing our method achieve state-of-the-art eye ﬁxation pre-\ndiction performance on two well-studied datasets.\n2. Related work\nThis section presents a brief review of existing ap-\nproaches which are related to modeling naturalness of im-\nages. Additionally, we describe image reconstruction from\nfeatures and eye ﬁxation prediction.\n2.1. Image modeling\nMany probabilistic image models have been proposed [1,\n15, 19, 29, 33, 45, 15]. However, most are only applicable\nto small and ﬁxed-size image patches of simple contents.\nTo overcome this matter, Gregor et al. [18] used attention\nmechanisms and succeeded in generating very realistic im-\nages. Denton et al. [9] applied Generative Adversarial Net-\nworks [15] in a hierarchical way.\nTheis and Bethge [53] proposed a scalable image model\nusing multi-dimensional LSTMs [17] which predict pixel\nvalues of certain locations from preceding pixels. We also\nuse RNN for predictions like theirs. However, to capture\nmore high-level information, we train RNN on CNN repre-\nsentations, not on raw pixels.\nSeveral vision papers explicitly use LM. Wu et al. [58]\nand Tirilly et al. [54] trained LMs on quantized local de-\nscriptors or Visual Words. Although their approach is sim-\nilar to ours, they used LMs for classiﬁcation, not for mea-\nsuring naturalness. Ranzato et al. [42] trained a language\nmodel on a small region of videos which predicts the next\ntime frame to learn spatial-temporal video representations.\n2.2. Image reconstruction from features\nReconstructing an image from its representation is an\nimportant task to understand the characteristics of the rep-\nresentation. Many works have addressed this problem for\nhand-crafted representations [8, 28, 38, 56, 57] and deep\nrepresentations [11, 38, 47, 59].\nMahendran and Vedaldi [38] showed that an image can\nbe reconstructed by gradient descent if the representation\nis extracted through differentiable functions. They also\ndemonstrated that a “natural image prior” is necessary to\nreconstruct interpretable images. They regularized recon-\nstructed images to eliminate spikes in raw pixels and to be\nwithin the natural RGB range. Simonyanet al. [47] adopted\na similar approach and used L2 regularization on images.\nDosovitskiy and Brox [11] inverted CNN features by di-\nrectly learning a CNN, which translates features to images.\nThey demonstrated that colors and rough compositions of\nthe original image can be reconstructed.\nOur reconstruction method is based on the work by Ma-\nhendran and Vedaldi [38]. Instead of using a hand-crafted\nnatural image prior, we use RNNLM trained on natural im-\nages as a regularizer.\n2.3. Eye ﬁxation prediction\nModeling visual attention is fundamentally important to\nefﬁciently process massive real-world data. Especially, a\ntask to predict eye ﬁxation points of humans has been ex-\namined extensively [3].\nBruce and Tsotsos [5] demonstrated that eye ﬁxation\npoints can be predicted using Shannon’s “self-information”.\nThis information-theoretic view has been adopted for many\nresearch efforts [3]. Our method also uses a kind of self-\ninformation.\nMany recent methods are based on supervised training\non an eye ﬁxation dataset [25, 27, 31, 32, 36, 55]. Ours\nis also a trainable ﬂexible model. However, because it is\ntrained in an unsupervised manner, it requires images of the\ntarget domain but does not require eye ﬁxation data. Be-\ncause making a dataset is a troublesome task, it is a favor-\nable property for practical applications.\n3. Visual language modeling\nAs illustrated in Figure 1, we measure the naturalness\nof an image using RNNLM and CNN. The pipeline of our\nmethod and corresponding sections are explained below.\n1. Extract a mid-level image representations from an in-\nput image using a pre-trained CNN. (Section 3.1.1.)\n2. Normalize and orthogonalize them. (Section 3.2.)\n3. Run RNNLM forward and backward along the x-axis\nand y-axis to obtain prediction maps and prediction er-\nror maps. (Section 3.1.2, 3.3.)\nModel Layer name Output size\nAlexNet [30]\ninput 227 ×227 ×3\nconv1 55 ×55 ×96\nconv2 27 ×27 ×256\nconv3 13 ×13 ×384\nconv4 13 ×13 ×384\nconv5 13 ×13 ×256\nVGGNet [48]\ninput 224 ×224 ×3\nconv1 {1, 2} 224 ×224 ×64\nconv2 {1, 2} 112 ×112 ×128\nconv3 {1, 2, 3, 4} 56 ×56 ×256\nconv4 {1, 2, 3, 4} 28 ×28 ×512\nconv5 {1, 2, 3, 4} 14 ×14 ×512\nTable 1. Layer name and output size of convolution lay-\ners. The output size is represented as height ×width ×\ndimension. “conv1 {1, 2 }” signiﬁes that there are two layers\nnamed “conv1 1” and “conv1 2”.\n4. Sum them up and output it as the unnaturalness score.\n(Section 3.3.)\nRNNLM must be trained on natural images in advance.\nThe training procedure is described in Section 3.4.\nWe apply our method to image reconstruction from fea-\ntures and eye ﬁxation prediction. We describe the details of\ntwo applications in Section 3.5 and Section 3.6.\n3.1. CNN and RNN\nFirst, we brieﬂy introduce CNN and RNN. We also de-\nscribe their conﬁguration in this work.\n3.1.1 CNN\nCNN is a neural network that achieves state-of-the-art per-\nformance for image classiﬁcation [30, 48]. To extract im-\nage representations, a CNN applies 2D convolution, nonlin-\near activation function, and downsampling in a hierarchical\nway. The weights of convolution kernels are learned from\ndata to minimize classiﬁcation error.\nMid-level representations of CNNs trained on a large-\nscale generic image classiﬁcation dataset are shown to\nwork as a high-performance generic image feature [10, 48].\nTherefore they have become the de facto standard image\nfeature in recent years.\nWe use the outputs immediately after the convolution\nlayers of AlexNet [30] and VGGNet [48] to extract mid-\nlevel representations. Table 1 shows layer name in the Caffe\npre-trained model [24] and the output size of their convolu-\ntion layers.\n3.1.2 RNN\nRNN is a neural network used to predict a sequence given a\nsequence. For each timestept, hidden unitht receives infor-\nmation from input xt and previous hidden unit ht−1. Then\nht passes information to outputyt. Because ht and ht−1 are\nconnected, yt depends on x1,x2,...,x t. Actually, RNN can\nmake predictions using the context of inﬁnite length.\nThe most basic type of recurrent layer is formalized as\nfollows.\nht = tanh (Wxxt + Whht−1 + b) . (1)\nHowever, it cannot learn long-term dependencies in fact be-\ncause gradients vanish in the process of ﬂowing through\nmany hidden-to-hidden connections. To overcome this\nproblem, a variant of recurrent layer called Long-Short\nTerm Memory (LSTM) was proposed [20]. We used a\nLSTM recurrent layer deﬁned as shown below.\nit = σ(Wxixt + Whiht−1 + bi) . (2)\nft = σ(Wxf xt + Whf ht−1 + bf ) . (3)\not = σ(Wxoxt + Whoht−1 + bo) . (4)\n˜ct = tanh (Wxcxt + Whcht−1 + bc) . (5)\nct = it ∗˜ct + ft ∗ct−1. (6)\nht = ot ∗tanh (ct) . (7)\nTherein, σrepresents a sigmoid activation function.\nWe use RNN using LSTM for sequential prediction, as\ndescribed later. Concretely, we stack two LSTM layers and\none linear layer to predict sequences. We set the dimensions\nof two LSTM layers as half of the input layer.\n3.2. Preprocessing\nFor example, the representation after conv1 layer of\nAlexNet comprises 55 ×55 vectors of 96 dimensions. We\nnormalize each dimension of such vectors to have zero-\nmean and unit-variance. After normalization, we apply\nPrincipal Component Analysis (PCA) on it to orthogonalize\nand reduce dimensions by half. Parameters of normaliza-\ntion and PCA are learned from the training set of ILSVRC\n2012 image classiﬁcation dataset [44].\n3.3. Combination of CNN and RNNLM\nLanguage Models (LMs) are used to measure\nthe naturalness of a sequence. Let s1,s2,...,s T to\nbe a sequence of D dimensional vectors of length\nT. The LMs decompose the probability p(s) into\np(s1)p(s2|s1)p(s3|s1,s2)...p(sT |s1,s2,...,s T−1). An\nintuitive interpretation of this is that the probability is\ndetermined by how the next vector is predictable from past\nvectors.\nCommon LMs treat st of one-hot vector which rep-\nresents a word and assume multinoulli distribution on\np(st|s1,...,s t−1). In contrast, we use dense real-\nvalue vector st and assume Gaussian distribution on\np(st|s1,...,s t−1). Concretely, regarding the Gaussian dis-\ntribution, we assume that the mean µt is determined from\ns1,...,s t−1 using RNN described in Section 3.1.2. We also\nassume that the variance of timestep tis T\nt because where\ntis small the model does not know much “context” of the\nsequence and predicted values are less reliable. Using this\nassumption, p(st|s1,...,s t−1) is rewritten by N(st|µt,T\nt ).\nThe negative log likelihood of scan be written as follows.\n−log p(s) ∝1\nT\n∑T\nt=2\nt\nT ||st −µt||2\n2. (8)\nIt is the weighted sum of squared prediction error.\nWe expand this model to 2D mid-level representation of\nCNN by application of RNNLM forward and backward for\neach row and column. We apply the same RNNLM in the\nsame axis and direction. Therefore, there are four RNNLMs\nper layer. Let fy,x,l to be a representation immediately after\nlayer lof size Hl ×Wl ×Dl. Then, we deﬁne the “unnatu-\nralness map” uy,x,l(1 ≤y ≤Hl −1,1 ≤x≤Wl −1) of\nfl as follows.\nuy,x,l = x+1\nWl\n||fy,x+1,l −µright\ny,x+1,l||2\n2 +\nWl−x+1\nWl\n||fy,x,l −µleft\ny,x,l||2\n2 +\ny+1\nHl\n||fy+1,x,l −µdown\ny+1,x,l||2\n2 +\nHl−y+1\nHl\n||fy,x,l −µup\ny,x,l||2\n2. (9)\nTherein, µright\ny,x,l is predicted value of fy,x,l from\nfy,1,l,...,f y,x−1,l. This corresponds to scanning of an\nimage from left to right predicting next time-step. µleft\ny,x,l,\nµdown\ny,x,l, and µup\ny,x,l are also deﬁned as the similar way. We\ndeﬁne the total unnaturalness of fl as\nul = 1\n(Hl−1)(Wl−1)\n∑Hl−1\ny=1\n∑Wl−1\nx=1 uy,x,l. (10)\nWe introduce weighting parameter λl for layer l. Then,\nthe total unnaturalness of an image iis\nui = ∑\nl λlul. (11)\n3.4. Training of RNNLM\nTo compute naturalness, we must train RNNLM in ad-\nvance by minimizing ui of many natural images. We use\nStochastic Gradient Descent (SGD) and Back-Propagation\nThrough Time (BPTT) to train RNNLM. We use the train-\ning set of ILSVRC 2012 image classiﬁcation dataset [44]\nfor training.\nWe train RNNLM on mid-level representations of\nAlexNet [30] and VGGNet [48]. For AlexNet, learning rate\nand momentum of SGD is set, respectively, to 10 and 0.9.\nThe minibatch size is set to 16. For VGGNet, learning rate\nand momentum of SGD is set, respectively, to 20 and 0.9.\nThe minibatch size is set to 1.\nFor both networks, we reduce learning rate by the fac-\ntor of 0.1 for every 5000 iterations. We stop learning after\n20,000 iterations.\n3.5. Image reconstruction from features\nMahendran and Vedaldi [38] demonstrated that image\nfeatures can be inverted to the original image by gradient\ndescent (GD) if the feature extraction function comprises\ndifferentiable elements. Their key technique is the intro-\nduction of “natural image prior” R(i) into their objective\nfunction. We denote the original image as iand feature ex-\ntraction function as φ(i). Then, using λr as the weight of\nthe regularizer, the reconstructed imageˆiis\nˆi= argmin\nˆi\n||φ(i) −φ(ˆi)||2\n2 + λrR(ˆi). (12)\nThey used R(ˆi), which keeps pixel values in the natural\nrange and penalizes strong intensity change in neighboring\npixels. Instead, we set R(ˆi) = uˆi. Because uˆi is differen-\ntiable, the objective function can be minimized by GD.\n3.6. Eye ﬁxation prediction\nIt has been suggested that humans look at locations\nwhere Shannon’s “self-information” is high [5]. Because\nself-information is identical to the negative logarithm of\nprobability, unnaturalness map ul can be regarded as a kind\nof information map that predicts salient locations.\nWe use an unnaturalness map ul as a saliency map. Ad-\nditionally, we apply Gaussian blur of a size of σ to ul ac-\ncording to common practice [32, 36, 60]. Before blurring,\nwe take the root of ul to prevent excessive expansion of\npeaky values. An example of unnaturalness map or saliency\nmap ul is presented in Figure 2.\n4. Experiments\nIn this section, we present evaluation of the effectiveness\nof our proposed method by application of it to two tasks:\nimage reconstruction from features and eye ﬁxation predic-\ntion.\n4.1. Image reconstruction from features\nHere we evaluate our image reconstruction method pro-\nposed in Section 3.5. First, we discuss how to evaluate re-\nconstructed images. Then we present reconstructed images\nof ours and compare them with results of existing meth-\nods. Subsequently, we combine our method with the work\nby Dosovitskiy and Brox [11] and present further improved\nresults. Finally, we examine the role of each layer by im-\nposing the regularizer on the target layer.\nIn common with the preceding works [11, 38], we\nreconstructed images from the outputs of the last fully-\nconnected layer of AlexNet and used the ﬁrst one hundred\nimages in the validation set of ILSVRC 2012 classiﬁcation\ndataset [44].\nOur model has the following hyper-parameters: the\nset of layers l used for mid-level representations, the\nMethod V otes Most similar\nMahendran and Vedaldi [38] 1945 4\nDosovitskiy and Brox [11] 1333 2\nCNN-VLM (ours) 6722 94\nTotal 10000 100\nTable 2. Human evaluation of each reconstruction method. For one\nhundred images, one hundred people on CrowdFlower selected the\nmost similar image to the original image from three reconstructed\nimages. There were a total 100 ×100 = 10000votes. Actually,\n94% of our results are selected as the most similar ones to the\noriginal images.\nweight of unnaturalness map λl, the weight of the reg-\nularizer λr, and the learning rate and momentum of\nGD. In this section, unless otherwise noted, we set\nl ∈ {conv1,conv2,conv3,conv4,conv5}, λconvn =\n10−(n−1) for n∈{1,2,3,4,5}and λr = 10. The learning\nrate and momentum of GD are set, respectively, to221, 0.9.\nInitial solution of GD is sampled from Gaussian distribu-\ntion, the mean and standard deviation of which are learned\nby RGB values of natural images using the training set of\nILSVRC 2012 classiﬁcation dataset [44].\n4.1.1 Evaluation method\nQuantitative evaluation of whether a reconstructed image is\nsimilar to the original image or not is not straightforward.\nSome earlier reports of the liteature [8, 57] have provided\nno quantitative analysis. Because using a kind of image\nfeature can produce an unfair comparison, mean squared\nerror [11, 28, 38] or correlation coefﬁcient [56] between re-\nconstructed images and original images have been used to\ndate. V ondricket al. [56] evaluated reconstructed images by\nasking humans to classify them and reported that the cor-\nrelation coefﬁcient did not always match the judgments of\nhumans.\nTherefore, in this work, we determine similarity of im-\nages by asking humans. We provide human subjects with\nthe original image and corresponding reconstructed im-\nages. Then they select the image from reconstructed images\nwhich is the most similar to the original image. We asked\none hundred people on CrowdFlower1.\n4.1.2 Results of reconstruction\nFigure 3 depicts original images and reconstructed images\nby our method and two existing methods [11, 38]. Re-\nsults of our method have clear edges rather than results\nof Mahendran and Vedaldi [38] because their regularizers\nprohibited strong change of intensity in neighboring pixels.\n1http://www.crowdflower.com/\n(a) Original image\n(b) Mahendran et al. [38]\n(c) Dosovitskiy et al. [11]\n(d) CNN-VLM (ours)\nFigure 3. Images reconstructed from outputs of the last fully-connected layer of AlexNet.\n(e) CNN-VLM + [11]\nFigure 5. Images reconstructed from outputs of the last fully-connected layer of AlexNet using our method. Images are initialized with the\nresults of Dosovitskiy and Brox [11].\nFigure 4. Six cases for which our results are inferior to other\nmethods according to human evaluation. Top row to bottom row:\noriginal image, Mahendran and Vedaldi [38], Dosovitskiy and\nBrox [11], ours, and ours initialized with the results of Dosovit-\nskiy and Brox. Left four columns: the result of Mahendran and\nVedaldi is the best. Right two columns: the result of Dosovitskiy\nand Brox is the best.\nThe method presented by Dosovitskiy and Brox [11] recon-\nstructs overall shapes and colors well, although the details\nare lost because their method outputs an average of possible\nsolutions. Our results appear to be the most similar to the\noriginal images. They are clear and understandable, which\nhelps us to interpret what the image features capture.\nTable 2 presents results of quantitative evaluation by\ncrowd sourcing. Of 100 images, 94 of our images are se-\nlected to the most similar images to the original images.\nOur results received 67.22 % of total votes by 100 people,\nwhich clearly indicates the superiority of our method.\nFigure 4 presents six cases in which our results were\njudged to be inferior to two other methods. Our method is\nnot good with reconstruction of the absolute positions and\ncolors of objects. Additionally, our method is vulnerable to\n“unnatural” images because it is trained on natural images.\n4.1.3 Better initialization\nThe initial solution of GD is known to affect the result\nstrongly, especially in neural networks [13, 50]. In fact, the\nkey to breakthroughs in deep networks resulted from smart\ninitialization of weights [19].\nBecause results of the method by Dosovitskiy and Brox\ncan be interpreted as the average of possible solutions, they\ncan be good initial solutions. Figure 5 and the last row of\nFigure 4 portray reconstructed images initialized with the\noutputs of the method presented by Dosovitskiy and Brox.\nThese results were improved considerably from previous\nresults. For some images, the absolute positions and colors\nof objects are corrected, which indicates that the limitations\nof our method are mostly attributable to the initialization\nstrategy and that they can be compensated by the method\npresented by Dosovitskiy and Brox.\n4.1.4 Analysis of layers\nFigure 6 shows images reconstructed using our method. In\nthis case, the regularizer is imposed on one certain layer or\nraw image pixels.\n(a) (b) (c) (d) (e) (f)\nFigure 6. Images reconstructed with regularization on various fea-\nture maps. (a) Original image. (b) No regularization. (c–f) Regu-\nlarized by the naturalness of (c) the raw pixels, and the output of\n(d) conv1, (e) conv2, (f) conv3 convolution layers.\nThe result regularized on raw pixels, which are com-\npletely understandable, indicate the importance of modeling\nthe naturalness of high-level features for generating realistic\nimages, not of raw image pixels.\nThe results regularized on conv2 or conv3 are less\nclear than the result onconv1, which implies that the infor-\nmation contained by lower layers affects the naturalness of\nimages. Higher layers can regularize more abstract informa-\ntion, but they are insufﬁcient by themselves for generating\nimages.\n4.2. Eye ﬁxation prediction\nIn this section, we evaluate our eye ﬁxation prediction\nmethod proposed in Section 3.6. We describe details of the\ndatasets and evaluation metrics. Subsequently, we present\nthe results.\nOur model has two hyper-parameters: the set of layers\nl used for mid-level representations and the kernel size of\nGaussian blur σ. We use one convolution layer of VG-\nGNet [48] as land set σto 0.030.\n4.2.1 Dataset\nMany datasets are used for eye ﬁxation prediction. Herein,\nwe evaluate our method on two popular datasets, called\nMIT1003 [27] and Toronto [5]. Additionally, we evalu-\nate ours on MIT Saliency Benchmark [6, 26], which con-\nsists of two other datasets: MIT300 and CAT2000. The\nMIT Saliency Benchmark is an online benchmarking ser-\nvice. Evaluation is done in submission.\nMIT1003 MIT1003 Dataset [27] includes 1003 images of\nnatural indoor and outdoor scenes and corresponding eye\nﬁxation maps. It includes 779 landscape images and 228\nportrait images.\nToronto Toronto [5] includes 200 images of outdoor and\nindoor scenes and corresponding eye ﬁxation maps.\nMIT300 MIT300 [6, 26] includes 300 images of natural\nindoor and outdoor scenes. Corresponding ﬁxation maps\nare not provided. Because the protocol to collect this dataset\n0.00 0.01 0.02 0.03 0.04 0.05\nsigma\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75shuffled AUC\nMIT\n0.00 0.01 0.02 0.03 0.04 0.05\nsigma\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75shuffled AUC\nTORONTO\nconv1_1\nconv1_2\nconv2_1\nconv2_2\nconv3_1\nconv3_4\nconv4_1\nconv4_4\nconv5_1\nconv5_4\nFigure 7. Shufﬂed AUC score on MIT1003 and Toronto dataset\nby our method. Higher values are better. Sigma is the size of the\nGaussian blur.\nis almost as the same as MIT1003, MIT1003 is useful as a\ntraining set.\nCAT2000 CAT2000 [4] is a recently introduced dataset.\nIt includes 2000 images of 20 different image categories in-\ncluding action, affective, art, black & white, cartoon, frac-\ntal, indoor, inverted, jumbled, line drawing, low-resolution,\nnoisy, object, outdoor man-made, outdoor natural, pattern,\nrandom, satellite, sketch, and social. This dataset is chal-\nlenging because most categories are not natural scenes.\n4.2.2 Evaluation metrics\nEye ﬁxation task can be interpreted as detection task to\ndetect eye ﬁxation point from an image. Therefore, area-\nunder-the-curve score (AUC) of ROC curve is often used\nfor evaluation [5]. However, because humans tend to look\naround the center of an image, this metric assigns much\nvalue to center-biased saliency maps. To overcome this\nproblem, shufﬂed AUC has been proposed [52, 61]. This\nmetric computes AUC, not on all pixels uniformly, but on\ncenter-biased eye ﬁxation points of other images. The shuf-\nﬂed AUC score of centered Gaussian is about 0.5. We use\nthis metric for evaluation.\n4.2.3 Results of benchmark dataset\nFigure 7 shows the shufﬂed AUC score obtained using our\nmethod on MIT1003 and Toronto dataset. Various land σ\nare tested. The best performing setting is l = conv5 4,\nσ = 0.030 for MIT1003 and l = conv5 1, σ = 0.010 for\nToronto.\n2This value is not posted online but is included in their paper.\nDataset Mr-CNN AWS BMS CA eDN HFT ICL IS JUDD LG QDCT Ours\nMIT1003 .7190 .6945 .6939 .6718 .6273 .6526 .6667 .6686 .6631 .6823 .6686 .7203\nToronto .7236 .7184 .7221 .6959 .6292 .6926 .6939 .7115 .6901 .6990 .7174 .7323\nTable 3. Shufﬂed AUC score of each method and dataset. Scores aside from ours are cited from Liu et al. [36].\nDataset SALICON Deep Fix Deep Gaze I 2 SalNet Mr-CNN AWS CA WMAP IttiKoch2 Ours\nMIT300 .74 .71 .71 .69 .69 .68 .65 .63 .63 .7096\nCAT2000 - .57 - - - .62 .60 .60 .59 .6221\nTable 4. Shufﬂed AUC score on MIT Saliency Benchmark. Scores are available online. These scores are retrieved on 30 October 2015.\nClass Ours Others Difference\nSocial .6607 .5764 .0843\nCartoon .6586 .5918 .0668\nAffective .6646 .6018 .0628\nOther 14 categories - - -\nFractal .5820 .5518 .0302\nLow Resolution .5663 .5418 .0245\nPattern .5638 .5509 .0129\nOverall .6221 .5755 .0466\nTable 5. Category-wise results on CAT2000 dataset. Others are\naverages of all published results available on the scoreboard. Cat-\negories for which the difference between ours and others are small\nor large are shown.\nTable 3 presents our results and existing results on the\nMIT1003 and Toronto datasets. We compare ours with\nMr-CNN [36], AWS [12], BMS [60], CA [14], eDN [55],\nHFT [35], ICL [22], IS [21], JUDD [27], LG [2] and\nQDCT [46]. Ours achieved state-of-the-art score on these\nwell-studied datasets.\nTable 4 presents our result on MIT300 and CAT2000.\nWe compare ours with SALICON [25], Deep Fix [31], Deep\nGaze I [32], SalNet 3, Mr-CNN [36], AWS [12], CA [14],\nWMAP [37] and IttiKoch2 [23], scores of which are avail-\nable on the scoreboard of MIT Saliency Benchmark. We set\nl = conv5 4, σ = 0.030 for MIT300 and l = conv5 1,\nσ= 0.030 for CAT 2000. Our results took second place on\nMIT300 and ﬁrst place on CAT2000.\n4.2.4 Discussion\nIt is noteworthy that higher layers produce better results\nas shown in Figure 7. Additionally, although we tried to\ncombine all saliency maps of different layers by supervised\ntraining in our preliminary experiments, it did not produce\nany performance improvement. These results indicate that\neye ﬁxation points are determined exclusively from higher-\nlevel signals.\nTable 5 presents our score and the average of scores of\nsubmitted results available on the scoreboard of each cate-\n3Unpublished work.\ngory on the CAT2000 dataset. Compared to other methods,\nours are superior for Social, Cartoon, and Affective. Pre-\nsumably, that is true because images of these categories in-\nclude more high-level contents such as faces or pedestrians.\nOurs is inferior for Pattern, Low Resolution, and Fractal be-\ncause ours are trained on natural images. Images of these\ncategories are apparently not natural. Prediction accuracy\ncan be improved by training of RNNLM on images of these\ncategories.\nMost top-scoring methods on MIT Saliency Benchmark\nare based on supervised training on eye-ﬁxation dataset [25,\n31, 32, 36]. These methods require large eye ﬁxation dataset\nof target domain. If there is no dataset or the dataset is\nsmall, the performance of these models can drop. The dif-\nference of scores of DeepFix on MIT300 and CAT2000\nindicates that. Ours are based on unsupervised training.\nTherefore it is unaffected by this problem.\n5. Conclusion\nIn this work, based on an assumption that the naturalness\ncan be measured by the predictability on high-level features\nduring eye movement, we proposed a novel method to mea-\nsure the naturalness of an image by building a variant of\nRNNLMs on the CNN features. We used it as a regularizer\nin reconstructing images from image features. The results\nof experiments show that this regularizer helps to generate\nmore feasible images than existing approaches. We found\nthat the naturalness of lower layers is important to generate\nnatural images. Additionally, we evaluated “unnaturalness\nmaps” of images as saliency maps. This was motivated by\nthe assumption that saliency of images is based on the self-\ninformation of each location. We demonstrated that the pro-\nposed “unnaturalness map” achieves state-of-the-art shuf-\nﬂed AUC scores on two well-studied eye ﬁxation predic-\ntion datasets. It was indicated that the naturalness of higher\nlayers predicts eye ﬁxation points well.\nThe naturalness of images, especially for large im-\nages that include rich contents, has not been studied well.\nNonetheless, methods to assess and detect naturalness of\nimages are extremely useful, as demonstrated in the experi-\nments. We hope this work will provoke more active research\nin this ﬁeld.\nReferences\n[1] Y . Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski.\nDeep generative stochastic networks trainable by backprop.\nIn ICML, 2014. 1, 2\n[2] A. Borji and L. Itti. Exploiting local and global patch rarities\nfor saliency detection. In CVPR, 2012. 7\n[3] A. Borji and L. Itti. State-of-the-art in visual attention mod-\neling. PAMI, 35(1):185–207, 2013. 2, 3\n[4] A. Borji and L. Itti. Cat2000: A large scale ﬁxation dataset\nfor boosting saliency research. In CVPR Workshop on the\nFuture of Datasets in Vision, 2015. 7\n[5] N. Bruce and J. Tsotsos. Saliency based on information max-\nimization. In NIPS, 2005. 2, 3, 5, 7\n[6] Z. Bylinskii, T. Judd, F. Durand, A. Oliva, and A. Torralba.\nMit saliency benchmark, 2012. http://saliency.\nmit.edu. 7\n[7] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray.\nVisual categorization with bags of keypoints. InECCV Work-\nshop on Statistical Learning in Computer Vision, 2004. 2\n[8] E. d’Angelo, A. Alahi, and P. Vandergheynst. Beyond bits:\nReconstructing images from local binary descriptors. In\nICPR, 2012. 3, 5\n[9] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep gen-\nerative image models using a laplacian pyramid of adversar-\nial networks. In NIPS, 2015. 2\n[10] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\nvation feature for generic visual recognition. InICML, 2014.\n1, 3\n[11] A. Dosovitskiy and T. Brox. Inverting convolutional net-\nworks with convolutional networks. arXiv:1506.02753,\n2015. 3, 5, 6\n[12] A. Garcia-Diaz, V . Lebor ´an, X. R. Fdez-Vidal, and X. M.\nPardo. On the relationship between optical variability, vi-\nsual saliency, and eye ﬁxations: A computational approach.\nJournal of Vision, 12(6):17, 2012. 7, 8\n[13] X. Glorot and Y . Bengio. Understanding the difﬁculty of\ntraining deep feedforward neural networks. In AISTATS,\n2010. 6\n[14] S. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware\nsaliency detection. PAMI, 34(10):1915–1926, 2012. 7, 8\n[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Gen-\nerative adversarial nets. In NIPS, 2014. 2\n[16] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and\nharnessing adversarial examples. In ICLR, 2015. 1\n[17] A. Graves and J. Schmidhuber. Ofﬂine handwriting recog-\nnition with multidimensional recurrent neural networks. In\nNIPS, 2009. 2\n[18] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw:\nA recurrent neural network for image generation. In ICML,\n2015. 2\n[19] G. E. Hinton, S. Osindero, and Y .-W. Teh. A fast learn-\ning algorithm for deep belief nets. Neural computation ,\n18(7):1527–1554, 2006. 1, 2, 6\n[20] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997. 4\n[21] X. Hou, J. Harel, and C. Koch. Image signature: Highlight-\ning sparse salient regions. PAMI, 34(1):194–201, 2012. 7\n[22] X. Hou and L. Zhang. Dynamic visual attention: Searching\nfor coding length increments. In NIPS, 2009. 7\n[23] L. Itti, C. Koch, and E. Niebur. A model of saliency-based\nvisual attention for rapid scene analysis. PAMI, (11):1254–\n1259, 1998. 8\n[24] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In ACMMM, 2014.\n3\n[25] M. Jiang, S. Huang, J. Duan, and Q. Zhao. Salicon: Saliency\nin context. In CVPR, 2015. 3, 8\n[26] T. Judd, F. Durand, and A. Torralba. A benchmark of com-\nputational models of saliency to predict human ﬁxations. In\nMIT technical report, 2012. 7\n[27] T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to\npredict where humans look. In ICCV, 2009. 3, 7\n[28] H. Kato and T. Harada. Image reconstruction from bag-of-\nvisual-words. In CVPR, 2014. 3, 5\n[29] D. P. Kingma and M. Welling. Auto-encoding variational\nbayes. In ICLR, 2014. 1, 2\n[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, 2012. 3, 4\n[31] S. S. Kruthiventi, K. Ayush, and R. V . Babu. Deepﬁx: A\nfully convolutional neural network for predicting human eye\nﬁxations. arXiv:1510.02927, 2015. 3, 8\n[32] M. K ¨ummerer, L. Theis, and M. Bethge. Deep gaze i: Boost-\ning saliency prediction with feature maps trained on ima-\ngenet. In ICLR Workshop, 2014. 3, 5, 8\n[33] H. Larochelle and I. Murray. The neural autoregressive dis-\ntribution estimator. In AISTATS, 2011. 1, 2\n[34] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278–2324, 1998. 1\n[35] J. Li, M. D. Levine, X. An, X. Xu, and H. He. Visual\nsaliency based on scale-space analysis in the frequency do-\nmain. PAMI, 35(4):996–1010, 2013. 7\n[36] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu. Predicting\neye ﬁxations using convolutional neural networks. In CVPR,\n2015. 3, 5, 7, 8\n[37] F. L ´opez-Garc´ıa, R. Dosil, X. M. Pardo, and X. R. Fdez-\nVidal. Scene Recognition through Visual Attention and Im-\nage Features: A Comparison between SIFT and SURF Ap-\nproaches. INTECH Open Access Publisher, 2011. 8\n[38] A. Mahendran and A. Vedaldi. Understanding deep image\nrepresentations by inverting them. In CVPR, 2015. 2, 3, 5, 6\n[39] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock`y, and S. Khu-\ndanpur. Recurrent neural network based language model. In\nINTERSPEECH, 2010. 2\n[40] A. Mordvintsev, C. Olah, and M. Tyka. Inceptionism: Go-\ning deeper into neural networks. Google Research Blog. Re-\ntrieved October 24, 2015. 1\n[41] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecog-\nnizable images. In CVPR, 2015. 1\n[42] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert,\nand S. Chopra. Video (language) modeling: a baseline for\ngenerative models of natural videos.arXiv:1412.6604, 2014.\n2\n[43] R. Rosenfeld. Two decades of statistical language modeling:\nWhere do we go from here. In Proceedings of the IEEE ,\n2000. 1\n[44] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-\nnition challenge. IJCV, pages 1–42, 2015. 4, 5\n[45] R. Salakhutdinov and G. E. Hinton. Deep boltzmann ma-\nchines. In AISTATS, 2009. 1, 2\n[46] B. Schauerte and R. Stiefelhagen. Quaternion-based spec-\ntral saliency detection for eye ﬁxation prediction. In ECCV,\n2012. 7\n[47] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep in-\nside convolutional networks: Visualising image classiﬁca-\ntion models and saliency maps. In ICLR Workshop, 2014. 2,\n3\n[48] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In ICLR, 2015.\n1, 2, 3, 4, 7\n[49] J. Sivic and A. Zisserman. Video google: A text retrieval\napproach to object matching in videos. In ICCV, 2003. 2\n[50] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the\nimportance of initialization and momentum in deep learning.\nIn ICML, 2013. 6\n[51] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,\nI. Goodfellow, and R. Fergus. Intriguing properties of neural\nnetworks. In ICLR, 2014. 1\n[52] B. W. Tatler, R. J. Baddeley, and I. D. Gilchrist. Visual cor-\nrelates of ﬁxation selection: Effects of scale and time. Vision\nResearch, 45(5):643–659, 2005. 7\n[53] L. Theis and M. Bethge. Generative image modeling using\nspatial lstms. In NIPS, 2015. 2\n[54] P. Tirilly, V . Claveau, and P. Gros. Language modeling for\nbag-of-visual words image categorization. In CIVR, 2008. 2\n[55] E. Vig, M. Dorr, and D. Cox. Large-scale optimization of hi-\nerarchical features for saliency prediction in natural images.\nIn CVPR, 2014. 3, 7\n[56] C. V ondrick, A. Khosla, T. Malisiewicz, and A. Torralba.\nHoggles: Visualizing object detection features. In ICCV,\n2013. 3, 5\n[57] P. Weinzaepfel, H. J ´egou, and P. P ´erez. Reconstructing an\nimage from its local descriptors. In CVPR, 2011. 3, 5\n[58] L. Wu, M. Li, Z. Li, W.-Y . Ma, and N. Yu. Visual language\nmodeling for image classiﬁcation. In ACMMM Workshop on\nMultimedia Information Retrieval, 2007. 2\n[59] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In ECCV. 2014. 3\n[60] J. Zhang and S. Sclaroff. Saliency detection: A boolean map\napproach. In ICCV, 2013. 5, 7\n[61] L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cot-\ntrell. Sun: A bayesian framework for saliency using natural\nstatistics. Journal of Vision, 8(7):32, 2008. 7",
  "topic": "Naturalness",
  "concepts": [
    {
      "name": "Naturalness",
      "score": 0.9855057001113892
    },
    {
      "name": "Computer science",
      "score": 0.8010159730911255
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7161802053451538
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6650883555412292
    },
    {
      "name": "Image (mathematics)",
      "score": 0.49500173330307007
    },
    {
      "name": "Predictability",
      "score": 0.4896166920661926
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4459640085697174
    },
    {
      "name": "Computer vision",
      "score": 0.4102988839149475
    },
    {
      "name": "Mathematics",
      "score": 0.13733550906181335
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    }
  ],
  "cited_by": 6
}