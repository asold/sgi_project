{
  "title": "JuriBERT: A Masked-Language Model Adaptation for French Legal Text",
  "url": "https://openalex.org/W3215431778",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1994195507",
      "name": "Stella Douka",
      "affiliations": [
        "Polytechnique Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A3158839189",
      "name": "Hadi Abdine",
      "affiliations": [
        "École Polytechnique",
        "Polytechnique Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A1914497179",
      "name": "Michalis Vazirgiannis",
      "affiliations": [
        "Polytechnique Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A3159914708",
      "name": "Rajaa El Hamdani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2289256936",
      "name": "David Restrepo-Amariles",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2765440119",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W3156808437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4287185133",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3214298066",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2983841094",
    "https://openalex.org/W3185670382",
    "https://openalex.org/W3037252472"
  ],
  "abstract": "International audience",
  "full_text": "Natural Legal Language Processing Workshop 2021, pages 95–101\nNovember 10, 2021. ©2021 Association for Computational Linguistics\n95\nJuriBERT: A Masked-Language Model Adaptation for French Legal Text\nStella Douka\nÉcole Polytechnique\nAUEB\nHadi Abdine\nÉcole Polytechnique\nMichalis Vazirgiannis\nÉcole Polytechnique\nAUEB\nRajaa El Hamdani\nHEC Paris\nDavid Restrepo Amariles\nHEC Paris\nAbstract\nLanguage models have proven to be very\nuseful when adapted to speciﬁc domains.\nNonetheless, little research has been done on\nthe adaptation of domain-speciﬁc BERT mod-\nels in the French language. In this paper, we\nfocus on creating a language model adapted\nto French legal text with the goal of helping\nlaw professionals. We conclude that some\nspeciﬁc tasks do not beneﬁt from generic lan-\nguage models pre-trained on large amounts\nof data. We explore the use of smaller ar-\nchitectures in domain-speciﬁc sub-languages\nand their beneﬁts for French legal text. We\nprove that domain-speciﬁc pre-trained models\ncan perform better than their equivalent gener-\nalised ones in the legal domain. Finally, we\nrelease JuriBERT, a new set of BERT models\nadapted to the French legal domain.\n1 Introduction\nDomain-speciﬁc language models have evolved the\nway we learn and use text representations in natu-\nral language processing. Instead of using general\npurpose pre-trained models that are highly skewed\ntowards generic language, we can now pre-train\nmodels that better meet our needs and are highly\nadapted to speciﬁc domains, like medicine and law.\nIn order to achieve that, models are trained on large\nscale raw text data, which is a computationally\nexpensive step, and then are used in many down-\nstream evaluation tasks, achieving state-of-the-art\nresults in multiple explored domains.\nThe majority of domain-speciﬁc language mod-\nels so far are applied to the English language. Ab-\ndine et al. (2021) published French word vectors\nfrom large scale generic web content that surpassed\nprevious non pre-trained word embeddings. Fur-\nthermore, Martin et al. (2020) introduced Camem-\nBERT, a monolingual language model for French,\nthat is used for generic everyday text, and proved\nits superiority in comparison with other multilin-\ngual models. In the meantime, domain-speciﬁc\nlanguage models for French are in lack. There is\nan even greater shortage when it comes to the legal\nﬁeld. Sulea et al. (2017) mentioned the importance\nof using state-of-the-art technologies to support\nlaw professionals and provide them with guidance\nand orientation. Given this need, we introduce Ju-\nriBERT, a new set of BERT models pre-trained on\nFrench legal text. We explore the use of smaller\nmodels architecturally when we are dealing with\nvery speciﬁc sub-languages, like French legal text.\nThus, we publicly release JuriBERT1 in 4 different\nsizes online.\n2 Related Work\nPrevious work on domain-speciﬁc text data has in-\ndicated the importance of creating domain-speciﬁc\nlanguage models. These models are either adapta-\ntions of existing generalised models, for example\nBert Base by Devlin et al. (2019) trained on gen-\neral purpose English corpora, or pre-trained from\nscratch on new data. In both cases, domain-speciﬁc\ntext corpora are used to adjust the model to the\npeculiarities of each domain.\nA remarkable example of adapting language\nmodels is the research done by Lee et al. (2019)\nwho introduced BioBERT, a domain-speciﬁc lan-\nguage representation model pre-trained on large\nscale biomedical corpora. BioBERT outperformed\nBERT and other previous models on many biomed-\nical text mining tasks and showed that pre-training\non speciﬁc biomedical corpora improves perfor-\nmance in the ﬁeld. Similar results were presented\nby Beltagy et al. (2019) that introduced SciBERT\nand showed that pre-training on scientiﬁc-related\ncorpus improves performance in multiple domains,\nand by Yang et al. (2020) who showed that Fin-\nBERT, pre-trained on ﬁnancial communication cor-\n1You can ﬁnd the models in http://\nmaster2-bigdata.polytechnique.fr/\nFrenchLinguisticResources/resources#\njuribert\n96\npora, can outperform BERT on three ﬁnancial sen-\ntiment classiﬁcation tasks.\nMoving on to the legal domain, Bambroo and\nAwasthi (2021) worked on LegalDB, a DistilBERT\nmodel (Sanh et al., 2019) pre-trained on English\nlegal-domain speciﬁc corpora. LegalDB outper-\nformed BERT at legal document classiﬁcation. El-\nwany et al. (2019) also proved that pre-training\nBERT can improve classiﬁcation tasks in the legal\ndomain and showed that acquiring large scale En-\nglish legal corpora can provide a major advantage\nin legal-related tasks such as contract classiﬁcation.\nFurthermore, Chalkidis et al. (2020) introduced\nLegalBERT, a family of English BERT models, that\noutperformed BERT on a variety of datasets in text\nclassiﬁcation and sequence tagging. Their work\nalso showed that an architecturally large model\nmay not be necessary when dealing with domain-\nspeciﬁc sub-languages. A representative example\nis Legal-BERT-Small that is highly competitive\nwith larger versions of LegalBert. We intent to fur-\nther explore this theory with even smaller models.\nDespite the increasing use of domain-speciﬁc\nmodels, we have mainly been limited to the En-\nglish language. On the contrary, in the French lan-\nguage, little work has been done on the application\nof text classiﬁcation methods to support law pro-\nfessionals, with the exception of Sulea et al. (2017)\nthat managed to achieve state-of-the-art results in\nthree legal-domain classiﬁcation tasks. It is also\nworth mentioning Garneau et al. (2021) who in-\ntroduced CriminelBART, a ﬁne-tuned version of\nBARThez (Eddine et al., 2020). CriminelBART is\nspecialised in criminal law by using French Cana-\ndian legal judgments. All in all, no previous work\nhas adapted a BERT model in the legal domain\nusing French legal text.\n3 Downstream Evaluation Tasks\nIn order to evaluate our models we will be using\ntwo legal text classiﬁcation tasks provided by the\nCourt of Cassation, the highest court of the French\njudicial order.\nThe subject of the ﬁrst task is assigning the\nCourt’s Claimant’s pleadings, \"mèmoires ampli-\natifs\" in French, to a chamber and a section of the\nCourt. This leads to a multi-class classiﬁcation task\nwith 8 different imbalanced classes. In Table 1 we\ncan see the eight classes that correspond to the dif-\nferent chambers and sections of the Court, as well\nas their support in the data. The classes represent\nClass Support\nCO 28 198\nC1_Section1 14 650\nC1_Section2 16 730\nC2_Section1 11 525\nC2_Section2 9 975\nC2_Section3 13 736\nC3_Section1 16 176\nC3_Section2 12 282\nTable 1: Chambers and Sections of the Court of Cassa-\ntion and data support\nMESINS EPHP COMPUB FONDA JURID FONCT ABSEN EPHS PRISE ECAP\nMatieres\n0\n1\n2\n3\n4\n5\n6\n7Class Support\nFigure 1: 10 Recessive Matieres\n4 chambers: the ﬁrst civil chamber (C1) that deals\nwith topics like Civil Contract Law and Consumer\nLaw, the second civil chamber (C2) with topics\nlike Insurance Law and Trafﬁc accidents, the third\ncivil chamber (C3) dealing with Real property and\nConstruction Law among other topics and the Com-\nmercial, Economic and Financial Chamber (CO)\nfor Commercial Law, Banking and Credit Law and\nothers. Each chamber has two or more sections\ndealing with different topics.\nThe second task is to classify the Claiment’s\npleadings to a set of 151 subjects, \"matières\" as\nstated in French. Figure 5 in appendix shows the\nsupport of the matières in the data. As we can see\nin Figure 1 the 10 recessive matières have between\n7 to 1 examples in our dataset. We decided to\nremove the last 3 matières as they have less than\n3 examples and therefore it is not possible to split\nthem in train, test and development sets.\n4 JuriBERT\nWe introduce a new set of BERT models pre-trained\nfrom scratch in legal-domain speciﬁc corpora. We\ntrain our models on the Masked Language Mod-\neling (MLM) task. This means that given an in-\nput text sequence we mask tokens with 15% prob-\n97\nability and the model is then trained to predict\nthese masked tokens. We follow the example of\nChalkidis et al. (2020) and choose to train signif-\nicantly even smaller models, including Bert-Tiny\nand Bert-Mini. The architectural details of the\nmodels we pre-trained are presented in Table 2.\nWe also choose to further pre-train CamemBERT\nBase on French legal text in order to better explore\nthe impact of using domain-speciﬁc corpora in pre-\ntraining.\nTraining Data\nFor the pre-training we used two different French\nlegal text datasets. The ﬁrst dataset contains data\ncrawled2 from the Légifrance 3 website and con-\nsists of raw French Legal text. The Légifrance text\nis then cleaned from non French characters. We\nalso use the Court’s decisions and the Claimant’s\npleadings from the Court of Cassation that consists\nof 123361 long documents from different court\ncases. All personal and private information, includ-\ning names and organizations, has been removed\nfrom the documents for the privacy of the stake-\nholders. The combined datasets provide us with a\ncollection of raw French legal text of size 6.3 GB\nthat we will use to pre-train our models.\nLegal Tokenizer\nIn order to pre-train a new BERT model from\nscratch we need a new Tokenizer. We trained a\nByteLevelBPE Tokenizer with newly created vo-\ncabulary from the training corpus. The vocabulary\nis restricted to 32,000 tokens in order to be compa-\nrable to the CamemBERT model from Martin et al.\n(2020) and minimum token frequency of 2. We\nused a RobertaTokenizer as a template to include\nall the necessary special tokens for a Masked Lan-\nguage Model. Our new Legal Tokenizer encodes\nthe data using 512-sized embeddings.\nJuriBERT\nFor the pre-training of the JuriBERT Model we\nused both the crawled Légifrance data and the\nPleadings Dataset, thus creating a 6.3GB collec-\ntion of legal texts. The encoded corpus was then\nused to pre-train a BERT model from scratch. Our\nmodel was pre-trained in 4 different architectures.\nAs a result we have JuriBERT Tiny with 2 layers,\n2We used Heritrix, a crawler that respects the robots.txt\nexclusion directives and META nofollow tags. See https:\n//github.com/internetarchive/heritrix3\n3https://www.legifrance.gouv.fr/\nModel Architecture Params\nJuriBERT Tiny L=2, H=128, A=2 6M\nJuriBERT Mini L=4, H=256, A=4 15M\nJuriBERT Small L=6, H=512, A=8 42M\nJuriBERT Base L=12, H=768, A=12 110M\nJuriBERT-FP L=12, H=768, A=12 110M\nTable 2: Architectural comparison of JuriBERT models\n128 hidden units and 2 attention heads (6M param-\neters), JuriBERT Mini with 4 layers, 256 hidden\nunits and 4 attention heads (15M parameters), Ju-\nriBERT Small with 6 layers, 512 hidden units and\n8 attention heads (42M parameters) and JuriBERT\nBase with 12 layers, 768 hidden units and 12 at-\ntention heads (110M parameters). JuriBERT Base\nuses the exact same architecture as CamemBERT\nBase.\nTask Speciﬁc JuriBERT\nWe also pre-trained a task-speciﬁc model that is\nexpected to perform better in the classiﬁcation of\nthe Claiment’s pleadings. For that we used only\nthe Pleadings Dataset for the pre-training that is\na 4GB collection of legal documents. The task-\nspeciﬁc JuriBERT model for the Cour de Cassation\ntask was pre-trained in 2 architectures, JuriBERT\nTiny (L=2, H=128, A=2) and JuriBERT Mini (L=4,\nH=256, A=4).\nJuriBERT-FP\nApart from pre-training from scratch we decided\nto also further pre-train CamemBERT Base on the\ntraining data. Our goal is to compare its perfor-\nmance with the original JuriBERT model to further\nexplore the impact of using speciﬁc-domain cor-\npora during pre-training. JuriBERT-FP uses the\nsame architecture as CamemBERT Base and JuriB-\nERT Base.\n5 Methods\nPre-training Details All the models were pre-\ntrained for 1M steps. A learning rate of 1e− 4\nwas used along with an Adam optimizer (β1=0.9,\nβ2=0.999) with weight decay of 0.1 and a linear\nscheduler with 10,000 warm-up steps. All the mod-\nels were pre-trained with batch size of 8 expect for\nJuriBERT Base and JuriBERT-FP that used batches\nof size 4. For the pre-training we used an Nvidia\nGTX 1080Ti GPU.\n98\nModel Pre-training Corpora\nCamemBERT 138GB\nBARThez 66GB\nJuriBERT 6.3GB\nJuriBERT-FP 6.3GB\nTask JuriBERT 4GB\nTable 3: Size of pre-training corpora used by different\nmodels\nFine-tuning Details Our models were ﬁne-tuned\non the downstream evaluation task using the same\nclassiﬁcation head as Devlin et al. (2019) that con-\nsists of a Dense layer with tanh function followed\nby a Dense layer with softmax activation function\nand Dropout layers with ﬁxed dropout rate of 0.1.\nWe applied grid-search to the learning rate on a\nrange of { 2e− 5,3e− 5,4e− 5,5e− 5}. We\nused an Adam optimizer along with a linear sched-\nuler that provided the training with 100k warm-up\nsteps. We train for a maximum of 30 epochs with\npatience of 2 epochs on the early stopping callback\nand checkpoints for the best model. For the classi-\nﬁcation we use only the paragraphs starting with\n’ALORS QUE’ from the Pleadings Dataset, as they\ninclude all the important information for the cor-\nrect chamber and section. This was suggested by\na lawyer from the Court of Cassation as the aver-\nage size of a mèmoire ampliatif is extremely big,\nfrom 10 to 30 pages long. By using the ’ALORS\nQUE’ paragraphs we have text sequences with av-\nerage size of 800 tokens. For the chambers and\nsections classiﬁcation task we split the data in 14%\ndevelopment and 16% test data. For the matières\nclassiﬁcation we split the data in 17% development\nand 14% test data and stratify in order to have all\nclasses represented in each subset. Both tasks use\na ﬁxed batch size of 4. For the ﬁne-tuning we used\nan Nvidia GTX 1080Ti GPU.\n6 Results\nThe results on the downstream evaluation tasks\nare presented in Tables 4 and 5. We compare our\nmodels with two CamemBERT versions, Base and\nLarge, and with BARThez, a sequence-to-sequence\nmodel dedicated to the French language. Camem-\nBERT has been pre-trained on 138GB of French\nraw text from the OSCAR corpus. Despite the dif-\nference in pre-training corpora size, with our model\nusing only 6.3GB of legal text, JuriBERT Small\nmanaged to outperform both CamemBERT Base\nand CamemBERT Large. This further proves the\nimportance of domain-speciﬁc language models\nin natural language processing and transfer learn-\ning. Despite our expectations, the performance of\nJuriBERT Base does not exceed the performance\nof its smaller equivalent models. We attribute this\npeculiarity in the usage of smaller batch sizes when\npre-training JuriBERT Base and also the fact that\nlarger models usually need more computational\nresources and more time and data in order to con-\nverge.\nJuriBERT Small also outperforms BARThez on\nthe chambers and sections evaluation task, which is\npre-trained on 66GB of French raw text and usually\nused for generative tasks. On the matières classiﬁ-\ncation task BARThez is the dominant model with\nJuriBERT Small being second. We infer that the\ncomplexity of the second task beneﬁts more from\nthe robustness and size of BARThez than from the\nspeciﬁc-domain nature of JuriBERT.\nComparing our models with the same architec-\ntures, it becomes apparent that all task-speciﬁc Ju-\nriBERT models perform better than their equivalent\ndomain-speciﬁc JuriBERT models besides using\nonly 4GB of pre-training data. The results conﬁrm\nthat a BERT model pre-trained from scratch only\non the corpus that is then used for ﬁne-tuning can\nperform better than a domain-speciﬁc one on the\nsame task as we expected.\nJuriBERT-FP outperforms JuriBERT Base and\nachieves similar results with CamemBERT Base on\nthe chambers and sections classiﬁcation task. This\nshows that further pre-training a general purpose\nlanguage model can have better results than train-\ning from scratch. However, it did not manage to\noutperform JuriBERT Small in both tasks, which\ncan be attributed to the smaller batch size used\nduring pre-training and to the size of the model\nas mentioned before for JuriBERT Base. Unfor-\ntunately, there are no smaller versions of Camem-\nBERT available to further test this theory. On the\nmatières classiﬁcation task, JuriBERT-FP still out-\nperforms JuriBERT Base. On the contrary, it per-\nforms worse than CamemBERT Base. Along with\nthe state-of-the-art results of BARThez, this leads\nus to believe that in order to achieve better results\nin more complex tasks JuriBERT models require\nmore pre-training corpora.\nAll in all, JuriBERT Small achieves equivalent\nresults with previous larger generic language mod-\nels with an accuracy of 83.95% on the ﬁrst task\n99\nModel Lrate Dev Test\nCamemBERT Base 2e− 5 82.75 83.22\nCamemBERT Large 2e− 5 79.69 79.91\nBARThez 3e− 5 83.70 83.49\nJuriBERT Tiny 3e− 5 82.00 81.58\nJuriBERT Mini 3e− 5 83.08 82.62\nJuriBERT Small 3e− 5 83.86 83.95\nJuriBERT Base 3e− 5 82.26 82.51\nTask JuriBERT Tiny 4e− 5 81.91 81.59\nTask JuriBERT Mini 4e− 5 82.75 82.66\nJuriBERT-FP 2e− 5 83.07 83.28\nTable 4: Accuracy of models on the chambers and sec-\ntions classiﬁcation task\nModel Lrate Dev Test\nCamemBERT Base 3e− 5 71.64 71.66\nBARThez 2e− 5 72.17 72.09\nJuriBERT Small 2e− 5 71.67 71.80\nJuriBERT Base 3e− 5 70.28 70.38\nJuriBERT-FP 2e− 5 70.99 71.21\nTable 5: Accuracy of models on the matieres classiﬁca-\ntion task\nand 71.80% on the second task on the test data. Ju-\nriBERT Small, JuriBERT Mini and even JuriBERT\nTiny all outperform JuriBERT Base, proving that\nsmaller models architecturally can achieve compa-\nrable, if not better, results when we are training\non very domain-speciﬁc data. A larger model, not\nonly requires more resources to be trained, but is\nalso not as efﬁcient as its smaller equivalents. This\nis of major importance for researchers with lim-\nited resources available. Furthermore, JuriBERT-\nFP achieves better results than JuriBERT Base in\nboth tasks. This leads us to infer that pre-training\nfrom an existing language model can be a major\nadvantage, as opposed to randomly initialising our\nweights.\n7 Limitations\nAs we mentioned before both JuriBERT Base and\nJuriBERT-FP have been pre-trained using smaller\nbatch sizes than the other models due to limited\nresources. We acknowledge that this may have\naffected their performance compared to the other\nmodels. However, we believe that their lower per-\nformance can also be attributed to their size as\nlarger models are computationally heavier and thus\nrequire more resources to converge.\nAcquiring large scale legal corpora, especially\nfor a language other than English, has proven to\nbe challenging due to their conﬁdential nature. For\nthis reason, JuriBERT models were ﬁne-tuned on\ntwo downstream evaluation tasks that contain data\nfrom the pre-training dataset collection. Further\ntesting shall be required in order to validate the\nperformance of our models on different tasks.\nThe differences in performance between the\ngeneric language models and the newly created\nJuriBERT models are very small. More speciﬁ-\ncally, only JuriBERT Small manages to outperform\nCamemBERT Base and Barthez with a difference\nin accuracy of 0.73%. We attribute this limitation\nin the use of much less pre-training data. However\nwe emphasize that JuriBERT manages to achieve\nsimilar results despite the difference in pre-training\ncorpora size. Thus, we expect JuriBERT to achieve\nbetter results in the future provided that we further\npre-train with more data.\n8 Conclusions and Future Work\nWe introduce a new set of domain-speciﬁc Bert\nModels pre-trained from scratch on French legal\ntext. We conclude that our task is very speciﬁc and\nas a result it does not beneﬁt from general purpose\nmodels like CamemBERT. We also show the supe-\nriority of much smaller models when training on\nvery speciﬁc sub-languages like legal text. It be-\ncomes apparent that large architectures may in fact\nnot be necessary when the targeted sub-language is\nvery speciﬁc. This is important for researchers with\nlower resources available, as smaller models are\nﬁne-tuned a lot faster on the downstream tasks. Fur-\nthermore, we show that a BERT model pre-trained\nfrom scratch on task-speciﬁc data and then ﬁne-\ntuned on this very task can perform better than a\ndomain-speciﬁc model that has been pre-trained\non a lot more data. We point out of course that\na domain-speciﬁc model can outperform a task-\nspeciﬁc one on other tasks and is generally pre-\nferred when we need a multi-purpose BERT model\nwith many applications in the French legal domain.\nIn future work, we plan to further explore the po-\ntential of JuriBERT in other tasks and as a result\nprove its superiority over the task-speciﬁc one.\nReferences\nHadi Abdine, Christos Xypolopoulos, Moussa Kamal\nEddine, and Michalis Vazirgiannis. 2021. Evalu-\n100\nation of word embeddings from large-scale french\nweb content. CoRR, abs/2105.01990.\nPurbid Bambroo and Aditi Awasthi. 2021. LegalDB:\nLong DistilBERT for legal document classiﬁcation.\nIn 2021 International Conference on Advances in\nElectrical, Computing, Communication and Sustain-\nable Technologies (ICAECT), pages 1–4.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMoussa Kamal Eddine, Antoine J.-P. Tixier, and\nMichalis Vazirgiannis. 2020. BARThez: a skilled\npretrained French sequence-to-sequence model.\nCoRR, abs/2010.12321.\nEmad Elwany, Dave Moore, and Gaurav Oberoi. 2019.\nBERT goes to law school: Quantifying the compet-\nitive advantage of access to large legal corpora in\ncontract understanding. CoRR, abs/1911.00473.\nNicolas Garneau, Eve Gaumond, Luc Lamontagne, and\nPierre-Luc Déziel. 2021. CriminelBart: A French\nCanadian legal language model specialized in crim-\ninal law. In Proceedings of the Eighteenth Interna-\ntional Conference on Artiﬁcial Intelligence and Law,\nICAIL ’21, page 256–257, New York, NY , USA. As-\nsociation for Computing Machinery.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. Biobert: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. CoRR, abs/1901.08746.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\nOctavia-Maria Sulea, Marcos Zampieri, Shervin Mal-\nmasi, Mihaela Vela, Liviu P. Dinu, and Josef van\nGenabith. 2017. Exploring the use of text classiﬁ-\ncation in the legal domain. CoRR, abs/1710.09306.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020. Finbert: A pretrained language model for ﬁ-\nnancial communications. CoRR, abs/2006.08097.\n101\nA Appendix\nFigure 2: Accuracy, Precision, Recall and F1-Score of\nJuriBERT Small on the chambers and sections classiﬁ-\ncation task on the test dataset. The graph contains all 8\nclasses.\nFigure 3: Confusion Matrix of JuriBERT Small on the\nchambers and sections classiﬁcation task on the test\ndataset. The graph includes accuracy and error rate for\neach class.\nFigure 4: Sample of Accuracy, Precision, Recall and\nF1-Score of JuriBERT Small on the matieres classiﬁ-\ncation task on the test dataset. The graph contains 28\nclasses and the overall accuracy of all 148 classes.\n0 1000 2000 3000 4000 5000 6000\nClass Support\nMatieres\nFigure 5: Support of the 151 Matieres in the Court of\nCassation data.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7871638536453247
    },
    {
      "name": "Language model",
      "score": 0.7420467734336853
    },
    {
      "name": "Domain adaptation",
      "score": 0.7109255194664001
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6625267863273621
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6164736747741699
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5784167647361755
    },
    {
      "name": "Natural language processing",
      "score": 0.5708857774734497
    },
    {
      "name": "Focus (optics)",
      "score": 0.5683205723762512
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5557032227516174
    },
    {
      "name": "Programming language",
      "score": 0.156139075756073
    },
    {
      "name": "Psychology",
      "score": 0.09970015287399292
    },
    {
      "name": "Mathematics",
      "score": 0.06107589602470398
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ]
}