{
  "title": "Identifying and Extracting Rare Disease Phenotypes with Large Language Models",
  "url": "https://openalex.org/W4381827011",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4306453481",
      "name": "Shyr, Cathy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2082076791",
      "name": "Hu Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4381828832",
      "name": "Harris, Paul A",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2072532109",
      "name": "Xu, Hua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2028127763",
    "https://openalex.org/W2043723189",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4293043115",
    "https://openalex.org/W1487381889",
    "https://openalex.org/W2128959663",
    "https://openalex.org/W2973267506",
    "https://openalex.org/W3017105460",
    "https://openalex.org/W4293239146",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W8550301",
    "https://openalex.org/W2884181551",
    "https://openalex.org/W4385573954",
    "https://openalex.org/W2205981794",
    "https://openalex.org/W3154100408",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2090246235",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W4361806442",
    "https://openalex.org/W4297234531",
    "https://openalex.org/W3153917119",
    "https://openalex.org/W2984857708",
    "https://openalex.org/W4378711639",
    "https://openalex.org/W4306684931",
    "https://openalex.org/W4403515058",
    "https://openalex.org/W4226189594",
    "https://openalex.org/W4385757404",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W1971192989",
    "https://openalex.org/W3191499914",
    "https://openalex.org/W3175225269",
    "https://openalex.org/W3146746902"
  ],
  "abstract": "Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings. We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively). Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample. While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.",
  "full_text": "Identifying and Extracting Rare Disease Phenotypes with\nLarge Language Models\nCathy Shyr∗ cathy.shyr@vumc.org\nDepartment of Biomedical Informatics\nVanderbilt University Medical Center, Nashville, TN, USA\nYan Hu yan.hu@uth.tmc.edu\nSchool of Biomedical Informatics\nUniversity of Texas Health Science at Houston, TX, USA\nPaul Harris paul.a.harris@vumc.org\nDepartment of Biomedical Informatics, Biostatistics, and\nBiomedical Engineering\nVanderbilt University Medical Center, Nashville, TN, USA\nHua Xu hua.xu@yale.edu\nSection of Biomedical Informatics and Data Science\nSchool of Medicine, Yale University, New Haven, CT, USA\nAbstract\nRare diseases (RDs) are collectively common and affect 300 million people worldwide.\nAccurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes\nare often embedded in unstructured text and time-consuming to extract manually. While\nnatural language processing (NLP) models can perform named entity recognition (NER) to\nautomate extraction, a major bottleneck is the development of a large, annotated corpus\nfor model training. Recently, prompt learning emerged as an NLP paradigm that can\nlead to more generalizable results without any (zero-shot) or few labeled samples (few-\nshot). Despite growing interest in ChatGPT, a revolutionary large language model capable\nof following complex human prompts and generating high-quality responses, none have\nstudied its NER performance for RDs in the zero- and few-shot settings. To this end,\nwe engineered novel prompts aimed at extracting RD phenotypes and, to the best of our\nknowledge, are the first the establish a benchmark for evaluating ChatGPT’s performance\nin these settings. We compared its performance to the traditional fine-tuning approach\nand conducted an in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted\nin higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero-\nand few-shot settings, respectively). Despite this, ChatGPT achieved similar or higher\naccuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of\n0.776 and 0.725). This suggests that with appropriate prompt engineering, ChatGPT has\nthe potential to match or outperform fine-tuned language models for certain entity types\nwith just one labeled sample. While the proliferation of large language models may provide\nopportunities for supporting RD diagnosis and treatment, researchers and clinicians should\ncritically evaluate model outputs and be well-informed of their limitations.\nKeywords: natural language processing, prompt learning, rare disease, artificial intelli-\ngence, ChatGPT\n*Corresponding author. Post Address: 2525 West End Avenue, Nashville, TN, 37203\n1\narXiv:2306.12656v1  [cs.CL]  22 Jun 2023\n1. Introduction\nRare diseases are chronically debilitating, often life-limiting conditions that affect 300 mil-\nlion individuals worldwide (Nguengang Wakap et al., 2020). Though individually rare (de-\nfined as affecting < 200, 000 individuals in the United States), rare diseases are collectively\ncommon and represent a serious public health concern (Chung et al., 2022). Because of\nthe lack of knowledge and effective treatment options for rare diseases, patients undergo\ndiagnostic and therapeutic odysseys, where they are diagnosed with delay and face diffi-\nculty searching for effective therapies (Childerhose et al., 2021; Insights, 2020). Rare disease\nodysseys have devastating medical, psychosocial, and economic consequences for patients\nand families, resulting in irreversible disease progression, physical suffering, emotional tur-\nmoil, and ongoing high medical costs (Cohen and Biesecker, 2010; Carmichael et al., 2015;\nYang et al., 2022). Thus, there is an urgent need to shorten rare disease odysseys, and\nreaching this goal requires effective diagnostic and treatment strategies.\nPhenotyping is crucial for informing both strategies. Ongoing initiatives like the Na-\ntional Institutes of Health’s Undiagnosed Diseases Network rely on deep phenotyping to\ngenerate candidate diseases for diagnosis, identify additional patients with similar clini-\ncal manifestations, and personalize treatment or disease management strategies (Tifft and\nAdams, 2014; Macnamara et al., 2019). In addition, phenotyping can facilitate cohort iden-\ntification and recruitment for clinical trials critical to the development of novel treatment\nregimes (Ahmad et al., 2020; Chapman et al., 2021). Because of scarce nosological guide-\nlines, however, rare diseases and their associated phenotypes are seldom represented in\ninternational classifications as structured data (Rath et al., 2012). Instead, they are often\nembedded in unstructured text and require manual extraction by highly trained experts,\nwhich is laborious, costly, and susceptible to bias depending on the clinician’s background\nand training. A promising alternative is to leverage natural language processing (NLP)\nmodels, which can automatically identify and extract rare disease entities, reduce manual\nworkload, and improve phenotyping efficiency.\nAutomatic recognition of disease entities, or named entity recognition (NER), is an\nNLP task that involves the identification and categorization of disease information from\nunstructured text. This task is especially challenging due to the diversity, complexity, and\nambiguity of rare diseases and their phenotypes, which can have different synonyms (e.g.,\ncystic fibrosis and mucoviscidosis), abbreviations (e.g., CF for cystic fibrosis), and modifiers\nsuch as body location (e.g., small holes in front of the ear) and severity (e.g., extreme\nnearsightedness). Descriptions of rare disease phenotypes that are discontinuous, nested,\nor overlapping present additional challenges; moreover, those that range from short phrases\nin layman’s terms (e.g., distention of the kidney) to medical jargon (e.g., hydronephrosis)\nmay further complicate NER.\nOver the last few decades, rapid evolution of NLP models led to significant advancements\nin NER. Early approaches relied on rules derived from extensive manual analysis (Wang\net al., 2018); these were later superseded by sequence labeling models, including conditional\nrandom fields and recurrent neural networks, that capture contextual information between\nadjacent words (Li et al., 2015; Patil et al., 2020). Over the last several years, the NER\nparadigm shifted toward transformer-based language models like BERT (Bidirectional En-\ncoder Representations from Transformers), which achieved state-of-the-art performance on\n2\nbenchmark datasets (Vaswani et al., 2017; Devlin et al., 2018). Despite their success, a\nmajor bottleneck of training models for rare diseases or biomedical applications in general\nis the development of large, annotated corpora, which is a laborious process that requires\nmanual annotation by domain experts. Recently, OpenAI released ChatGPT, a revolution-\nary, GPT-based (Generative Pre-trained Transformer) language model capable of following\ncomplex human prompts and generating high-quality responses without any annotated data\n(zero-shot) or with just a few examples (few-shot) (OpenAI, 2022; Agrawal et al., 2022; Hu\net al., 2023; Chen et al., 2023). This capability, which provides opportunities to signifi-\ncantly reduce the manual burden of annotation without sacrificing model performance, is\nespecially attractive for NER in the context of rare diseases.\nDespite the proliferation of studies on biomedical NER, few have explored this topic\nfor rare diseases. Davis et al. (2013) and Lo Barco et al. (2021) developed NLP algorithms\nusing the Unified Medical Language System Metathesaurus to recognize phenotypes for mul-\ntiple sclerosis and Dravet syndrome, respectively. Nigwekar et al. (2014) used an unnamed\nNLP software to identify patients with the terms “calciphylaxis” or “calcific uremic arte-\nriolopathy” in their medical records. Recently, Fabregat et al. (2018) and Segura-Bedmar\net al. (2022) leveraged deep learning techniques, including Bidirectional Long Short Term\nMemory networks and BERT-based models, to recognize rare diseases and their clinical\nmanifestations from texts. While some explored the potential of ChatGPT for diagnosing\nrare diseases with human-provided suggestions (Lee et al., 2023; Mehnen et al., 2023), none\nhave studied its performance for NER in the zero- or few-shot settings.\nTo this end, our study makes the following contributions. 1) We designed new prompts\nfor ChatGPT to extract rare diseases and their phenotypes (i.e., diseases, symptoms, and\nsigns) in the zero- and few-shot settings. 2) To the best of our knowledge, this work is the\nfirst to establish a benchmark for evaluating ChatGPT’s NER performance on a high-quality\ncorpus of annotated texts on rare diseases (Mart´ ınez-deMiguel et al., 2022). In addition,\nwe compared prompt learning to fine-tuning by training and evaluating a domain-specific\nBERT-based model on the annotated corpus. 3) We conducted an in-depth error analysis\nto elucidate the models’ performance and 4) provided suggestions to help guide future work\non NER for rare diseases.\n2. Methods\n2.1 Dataset\nWe used the RareDis corpus, which consists ofn = 832 texts from the National Organization\nfor Rare Disorders database (Mart´ ınez-deMiguel et al., 2022). This corpus was annotated\nwith four entities, rare diseases, diseases, signs, and symptoms, with an inter-annotator\nagreement of 83.5% under exact match. Table 1 provides the entity definitions. Unlike\ncorpora with distinct entity types, e.g., {person, location, organization } or {problem, test,\ntreatment}, RareDis consists of entities with considerable semantic overlap. Specifically,\nrare diseases are a subset of diseases. Diseases can cause or be associated with other diseases\nas a symptom or sign. The distinction between symptoms and signs is very subtle; while\nboth are abnormalities that may indicate a disease, the former are subjective to the patient\nand cannot be measured by tests or observed by physicians (e.g., pain or loss of appetite).\nOn the other hand, a sign can be measured or observed (e.g., high blood pressure, poor lung\n3\nEntity Definition Examples\nRare disease Diseases which affect a small number of people cat eye syndrome,\ncompared to the general population Marfan syndrome\nDisease An abnormal condition of a part, organ, or system cancer, cardiovascular disease\nof an organism resulting from various causes, such\nas infection, inflammation, environmental factors,\nor genetic defect, and characterized by an identifiable\ngroup of signs, symptoms, or both\nSymptom A physical or mental problem that may indicate fatigue, pain\na disease or condition; cannot be seen and do not\nshow up on medical tests\nSign A physical or mental problem that may indicate rash, abnormal heart rate\na disease or condition; can be seen and shows up\non medical tests\nTable 1: Summary of entity definitions.\nfunction). Across n = 832 texts, there were a total of 7,354 sentences, 4,065 rare diseases,\n1,814 diseases, 316 symptoms, and 3,317 signs. Rare diseases and signs were more common\nthan diseases and symptoms, accounting for 77% of all entities in the corpus. Fig. 1 provides\na summary of counts per text.\nFigure 1: Number of sentences and entities per docu-\nment.\nThe RareDis corpus is publicly\navailable and distributed in the\nBrat standoff format (Stenetorp\net al., 2012). We refer read-\ners to Mart´ ınez-deMiguel et al.\n(2022) for details on the annota-\ntion guidelines.\n2.2 NER\nParadigms and Models\nWe considered two popular NER\nparadigms for comparison: 1) pre-\ntraining + fine-tuning, and 2)\nprompt learning (Radford et al.,\n2018; Liu et al., 2023). The former involves a two-step process where a language model\n(e.g., BERT) is first trained on a massive amount of unlabeled text data and then fine-\ntuned on specific downstream NER tasks with labeled data. In the case of BERT models,\nthe objective is to learn general language presentations through masked language modeling\nduring the pre-training phase, where BERT learns to predict masked portions of the input\nbased on surrounding text. During the fine-tuning phase, the model is further trained using\nlabeled data from the target task, and its parameters are jointly fine-tuned via supervised\nlearning, allowing BERT to adapt its predictions to the specific task at hand.\nIn contrast, prompt learning is a more recent paradigm that reformulates the NER task\nas textual prompts so that the model itself learns to predict the desired output . Prompt\nlearning has been shown to have better generalizability for unseen data with few or even\n4\nno labeled samples (Agrawal et al., 2022). This is especially attractive for biomedical\napplications where annotations often require domain expertise and are not widely accessible\ndue to data privacy. We compared BERT- and GPT-based models within the fine-tuning\nand prompt learning paradigms, respectively, due to their promising empirical performance\non NER tasks in the biomedical domain (Yan et al., 2021; Chen et al., 2021, 2023).\n2.2.1 Data Pre-processing and Fine-tuning BioClinicalBERT\nTo pre-process our data, we split the texts into individual words (or subwords) with the\nBERT tokenizer and added special tokens (i.e., CLS and SEP) to the beginning and end\nof each tokenized sequence, respectively. We converted the tokens to their respective IDs,\npadded (or truncated) text sequences to obtain fixed-length inputs, and created an attention\nmask to distinguish between actual and padding tokens. Last, we mapped our labels, {rare\ndisease, disease, symptom, sign }, to corresponding numerical values.\nWe partitioned the data into a training, validation, and test set based on an 8:1:1 ratio.\nFor the base architecture, we selected BioClinicalBERT (Alsentzer et al., 2019), a variant\nof BERT that was pre-trained on large-scale biomedical (PubMed, ClinicalTrials.gov) and\nclinical corpora (MIMIC-III (Johnson et al., 2016)). To fine-tune BioClinicalBERT on our\ncorpus, we trained the model on the training set and selected hyperparameters using the\nvalidation set. We used the test set to evaluate model performance.\n2.2.2 Prompt Learning using ChatGPT (GPT-3.5-turbo)\nIn this section, we describe our approach to reformulating NER as a text generation task\nin the zero- and few-shot settings. The former refers to instructing the model to extract\nentities directly from an input text in the test set, and the latter is similar except we also\nprovide an example of extracted entities from a training text.\nPrompt design. Table 2 provides a summary of prompts in the zero- and few-shot set-\ntings. The five main building blocks of our prompt designs were 1) task instruction, 2)\ntask guidance, 3) output specification, 4) output retrieval, and, in the few-shot setting, 5) a\nspecific example. Task instruction conveys the overall set of directions for NER in a specific\nbut concise manner. To prevent ChatGPT from rephrasing entities, we instructed it to\nextract their exact names from the input text. Task guidance provides entity definitions\nfrom the original RareDis annotation guidelines. The objective is to help ChatGPT differ-\nentiate between entity types within the context of the input text, as all four entities overlap\nsemantically. Output specification instructs ChatGPT to output the extracted entities in a\nspecific format to reduce post-processing workload. Output retrieval prompts the model to\ngenerate a response. In the few-shot setting, we also provided an example with an input text\nfrom the training set and its gold standard labels (i.e., entities labeled by the annotators).\nPrompt format. In each setting, we experimented with two prompt formats: simple and\nstructured (Table 2). The former presents the prompt as a simple sentence, and the latter a\nstructured list. The simple sentence is shorter in length and resembles human instructions\nprovided in a conversational setting where different building blocks (i.e., task instruction,\ntask guidance, and output specification) are woven together as a single unit. Agrawal et al.\n(2022) and Hu et al. (2023) used a similar approach to extract medications and clinical\nentities, respectively. In contrast, the structured list resembles a recipe or outline that\n5\nSetting Type Prompt ExampleZero-shot SimpleExtract the exact names of [entity], Extract the exact names of rare diseases ,which are diseaseswhich are [defn],from this passagethat affect a small number of individuals, from this passageand output them in a list:and output them in a list: \"The exact prevalence and incidence\"[text from test set]\". abetalipoproteinemia is unknown, but it is estimated to affect···...···incidence of consanguineous marriages. Symptoms usuallybecome apparent during infancy.\"\nStructured###Task: ###Task:Extract the exact names of [entity]Extract the exact names of rare diseasesfrom the input text andoutput them from the input text andoutput themin a list. in a list.\n### Definition: ### Definition:[entity]s are defined as [defn]. Rare diseases are defined as diseases that affect a small numberof individuals .\n### Input text: [text from test set]. ### Input text: \"The exact prevalence and incidence ofabetalipoproteinemia is unknown, but it is estimated to affect···...···incidence of consanguineous marriages. Symptoms usuallybecome apparent during infancy.\"###Output: ### Output:\nFew-shot SimplePassage: [text from training set]. Passage: \"Binder type nasomaxillary dysplasia is a rare congenitalcondition that affects males and females in equal numbers···...···suggests that Binder syndrome occurs in less than 1 per 10,000live births.\"Extract the exact names of [entity],Extract the exact names of rare diseases, which are diseaseswhich are [defn], from this passagethat affect a small number of individuals, from this passageand output them in a list:and output them in a list:[gold standard training labels].Blinder type nasomaxillary dysplasia, Blinder syndrome\nPassage: [text from test set]. Passage: \"The exact prevalence and incidence ofabetalipoproteinemia is unknown, but it is estimated to affect···...···incidence of consanguineous marriage. Symptoms usuallybecome apparent during infancy.\"Extract the exact names of [entity],Extract the exact names of [entity],which are [defn], from this passagewhich are [defn], from this passageand output them in a list:and output them in a list:Structured### Task: ### Task:Extract the exact names of [entity],Extract the exact names of rare diseases,from the input text andoutput them from the input text andoutput themin a list. in a list.\n### Definition: ### Definition:[entity]s are defined as [defn].Rare diseases are defined as diseases that affect a small numberof individuals.\n### Input text: [text from training set] ### Input text: \"Blinder type nasomaxillary dysplasia is a rarecongenital condition that affects males and females in equal···...···suggests that Binder syndrome occurs in less than 1 per 10,000live births.\"\n###Output: [gold standard training labels] ###Output: Blinder type nasomaxillary dysplasia, Blinder syndrome\n### Input text: [text from test set] ### Input text: \"The exact prevalence and incidence ofabetalipoproteinemia is unknown, but it is estimated to affect···...···incidence of consanguineous marriages. Symptoms usuallybecome apparent during infancy.\"\n###Output: ### Output:\nTable 2: Summary of prompts. Different parts of the prompt are color-coded as follows:\nTask instruction , Task guidance , Output specification , Output retrieval , and\nSpecific example .\n6\nconsists of multiple sub-prompts in a specific order. Chen et al. (2023) used a similar\nformat for evaluating ChatGPT and GPT-4’s NER performance on benchmark datasets.\nFew-shot example selection. We explored two strategies for selecting an example text in\nthe few-shot setting. The first strategy involved randomly selecting a text from the training\nset, and the second involved selecting the training text that was most similar to the test\ntext. The motivation for the second strategy is that different rare diseases may have sim-\nilar etiology, course of progression, and symptoms/signs. For example, Creutzfeldt-Jakob\ndisease and CARASIL (cerebral autosomal recessive arteriopathy with subcortical infarcts\nand leukoencephalopathy) are neurological conditions that share similar signs, including\nprogressive deterioration of cognitive processes and memory. Thus, providing a training\ntext (and the corresponding gold standard entities) that was most similar to the test text\nmay improve ChatGPT’s performance. For each input text from the test set, we selected\nthe training text that had the highest similarity score based on spaCy pre-trained word\nembeddings (spaCy).\n2.3 Evaluation\nTo evaluate model performance on the test set, we computed the following evaluation met-\nrics: precision, recall, and F1-score. Precision is the percentage of extracted entities found\nby the model that were correct, and recall the percentage of gold standard entities extracted\nby the model. F1 accounts for both metrics by taking the harmonic mean of precision and\nrecall. We calculated these metrics under two evaluation settings: exact and relaxed. For\nan exact match, the extracted and true entity must share the same text span (i.e., bound-\nary) and entity type. For a relaxed match, the extracted and true entity must overlap in\nboundary and have the same entity type. To ensure that stop words did not influence the\nevaluation, we removed them from both the gold standard and model-extracted entities.\n3. Results\n3.1 Overall Results\nTable 3 provides a summary of the model performance by entity type. Overall, BioClin-\nicalBERT achieved an F1-score of 0.689 under relaxed match. In the zero-shot setting,\nChatGPT achieved F1-scores of 0.472 and 0.407 with the simple sentence and structured\nlist prompts, respectively. Performance generally improved in the few-shot setting with\nF1-scores of 0.591 and 0.469; choosing the training text based on a similarity score led\nto additional improvement, resulting in F1 scores of 0.610 and 0.544. For some entities,\nChatGPT had similar or better performance than its supervised counterpart, achieving F1-\nscores of 0.776 (vs. 0.755) and 0.725 (vs. 0.704) for rare diseases and signs, respectively, in\nthe few-shot setting. Compared to prompts written as a structured list, simple sentences\ngenerally achieved similar or better performance, suggesting that ChatGPT may be more\nreceptive to conversational prompts. Moreover, simple sentences required fewer tokens and\nwere preferred over structured lists from a cost perspective. In the few-shot setting, select-\ning a training example that was similar to the input text led to better performance than\nrandom selection.\n7\nExact Relaxed\nParadigm Model Setting Entity Precision Recall F1 Precision Recall F1\nPre-train BioClinicalBERT Supervised Rare disease0.689 0.720 0.704 0.772 0.739 0.755\n+ Fine-tune Disease 0.494 0.488 0.491 0.532 0.538 0.535\nSign 0.561 0.516 0.538 0.676 0.735 0.704\nSymptom 0.667 0.630 0.648 0.704 0.745 0.724\nOverall 0.600 0.583 0.591 0.681 0.698 0.689\nPrompt ChatGPT Zero-shot Rare disease 0.559 0.409 0.472 0.843 0.694 0.761\nlearning (Simple sentence) Disease 0.109 0.240 0.150 0.200 0.437 0.274\nSign 0.269 0.380 0.315 0.537 0.751 0.627\nSymptom 0.070 0.619 0.126 0.084 0.762 0.155\nOverall 0.203 0.369 0.262 0.365 0.670 0.472\nZero-shot Rare disease 0.765 0.489 0.597 0.887 0.634 0.740\n(Structured list) Disease 0.184 0.210 0.196 0.261 0.293 0.276\nSign 0.266 0.324 0.292 0.448 0.543 0.491\nSymptom 0.063 0.69 0.116 0.079 0.857 0.145\nOverall 0.226 0.359 0.277 0.331 0.528 0.407\nFew-shot Rare disease 0.719 0.441 0.547 0.937 0.634 0.756\n(Simple sentence Disease 0.211 0.210 0.210 0.287 0.287 0.287\n+ random example) Sign 0.457 0.409 0.432 0.721 0.671 0.695\nSymptom 0.279 0.452 0.345 0.294 0.476 0.364\nOverall 0.423 0.376 0.398 0.616 0.568 0.591\nFew-shot Rare disease 0.569 0.532 0.550 0.750 0.758 0.754\n(Structured list Disease 0.151 0.341 0.209 0.211 0.467 0.291\n+ random example) Sign 0.273 0.406 0.327 0.478 0.698 0.567\nSymptom 0.094 0.714 0.166 0.107 0.810 0.189\nOverall 0.237 0.440 0.308 0.361 0.668 0.469\nFew-shot Rare disease 0.818 0.484 0.608 0.967 0.634 0.766\n(Simple sentence Disease 0.206 0.246 0.224 0.286 0.341 0.311\n+ similar example) Sign 0.441 0.444 0.443 0.720 0.730 0.725\nSymptom 0.260 0.310 0.283 0.308 0.381 0.340\nOverall 0.422 0.403 0.412 0.617 0.603 0.610\nFew-shot Rare disease 0.590 0.565 0.577 0.762 0.790 0.776\n(Structured list Disease 0.199 0.437 0.273 0.297 0.653 0.408\n+ similar example) Sign 0.337 0.487 0.398 0.561 0.802 0.660\nSymptom 0.093 0.690 0.164 0.114 0.833 0.200\nOverall 0.278 0.506 0.359 0.421 0.769 0.544\nTable 3: Summary of model performance by entity type.\n8\nAmong the four entities, rare diseases were associated with the highest accuracy for\nboth models across all settings. In contrast, diseases were more challenging for both models.\nWhile BioClinicalBERT performed similarly at extracting signs and symptoms, ChatGPT\nachieved significantly better performance for signs. Because the only difference between the\nprompts for these entities was the task guidance, i.e., specifying symptoms as problems that\ncannot be measured, whereas signs can be measured, this finding suggests that ChatGPT\nis sensitive to even small variations in the prompt.\n3.2 Detailed Error Analysis\nWe conducted an in-depth error analysis to elucidate ChatGPT’s performance. This anal-\nysis was crucial for gaining additional insight, as unlike other biomedical corpora, RareDis\ncontains entities with overlapping semantics. Specifically, rare diseases are similar to dis-\neases, and symptoms to signs. Depending on the context of the input text, diseases can\nalso be symptoms or signs.\nIn our analysis, we considered five types of errors: 1) incorrect boundary, 2) incorrect\nentity type, 3) incorrect boundary and entity type, 4) spurious, and 5) missed. The first\nand second refer to an extracted entity whose boundaries or type do not match those of the\ngold standard label, respectively. The third refers to the case where neither the extracted\nentity’s boundaries nor type match those of the true label. Spurious entities are extracted\nentities that do not correspond to gold standard labels (false positive), and missed entities\nare entities that the model failed to extract (false negative).\nTable 4 shows the distribution of errors in the few-shot setting under exact match. The\nmost common error type for rare diseases is false negative (45%) followed by incorrect entity\ntype (31%). In the case of entity type errors, ChatGPT tended to label rare diseases as\ndiseases. These errors may be attributed to the fact that there is no single definition of\nrare diseases; rather, the definition can vary by country or location (i.e., a disease is a rare\ndisease if it affects < 200, 000 people in the United States or no more than 1 in 2,000 in the\nEuropean Union). Moreover, this definition is subject to change over time, as a disease that\nused to be rare at the time of annotation may have become more prevalent, or vice versa.\nBecause annotations are subjective, it’s possible that what the domain experts deemed as\nrare diseases may not be reflected in textual information on the Internet before September\n2021, ChatGPT’s knowledge cut-off date. For instance, the annotators labeled “gastroin-\ntestinal anthrax” and “cutaneous anthrax” as rare diseases based on domain knowledge,\nbut neither were listed in rare disease databases at the time of writing this manuscript.\nFor diseases, signs, and symptoms, false positives and false negatives were the most com-\nmon error types. Based on manual review, many of these errors can be attributed to the\nchallenge of differentiating amongst these entities. Specifically, diseases could be signs or\nsymptoms, and the difference between signs and symptoms is very subtle. In some cases,\ngold standard labels deviated from the definitions provided in the annotation guidelines, as\nthe lack of abnormalities was also labeled as an entity (i.e., “asymptomatic during infancy\nor childhood” was labeled as a symptom by the annotators). As such, a portion of false\nnegatives could be attributed to these edge cases.\n9\nBoundary✗ Boundary✓ Boundary✗ Spurious Missed Total\nEntity type✓ Entity type✗ Entity type✗ (False Pos.) (False Neg.) errors\nRare disease 16 (10%) 48 (31%) 17 (11%) 4 (3%) 72 (45%) 157 (100%)\nDisease 11 (4%) 7 (2%) 9 (3%) 147 (51%) 116 (40%) 290 (100%)\nSign 64 (17%) 8 (2%) 5 (1%) 146 (40%) 148 (40%) 371 (100%)\nSymptom 3 (4%) 12 (16%) 2 (3%) 34 (44%) 25 (33%) 76 (100%)\nTable 4: Error analysis for ChatGPT in the few-shot setting under exact match.\n4. Discussion\nIn this work, we reformulated NER as a text generation task and established a benchmark\nfor ChatGPT’s performance on extracting rare disease phenotypes. Overall, while fine-\ntuning a pre-trained biomedical language model led to better performance, prompt learning\nwith ChatGPT achieved similar or higher accuracy for some entities (i.e., rare diseases\nand signs) with a single example, demonstrating its potential for out-of-the-box NER in\nthe few-shot setting. Given ChatGPT’s performance in the zero-shot setting, the model\ncould be leveraged as a pre-annotation tool to accelerate annotation start-up times for rare\ndiseases and signs (F1-scores of 0.761 and 0.627, respectively). Overall, we recommend\nsimple, sentence-based prompts, as they performed similarly or better than lists and were\nshorter in length, leading to lower computational cost.\nWhile other studies explored supervised deep learning techniques for extracting rare\ndisease phenotypes, ours is the first to study ChatGPT in the zero- and few-shot settings.\nSegura-Bedmar et al. (2022) compared the NER performance of base BERT, BioBERT, and\nClinicalBERT, and found that ClinicalBERT had the highest overall F1-score (0.695). This\nis comparable to BioClinicalBERT’s performance in the current study (0.689). Fabregat\net al. (2018) used support vector machines and neural networks with a long short-term\nmemory architecture to extract disabilities associated with rare diseases and obtained an\nF1-score of 0.81. While this is much higher than the overall F1-score in the current study,\nFabregat et al. (2018) focused on extracting a single entity, i.e., disabilities, whereas our\ngoal was to recognize and differentiate among four entities with overlapping semantics. Hu\net al. (2023) and Chen et al. (2023) evaluated ChatGPT on biomedical NER and found\nthat it had lower performance than fine-tuning pre-trained language models. While our\noverall results aligned with this finding, we discovered that ChatGPT had similar or better\nperformance on specific entities, suggesting that with appropriate prompt engineering, the\nmodel has the potential to match or outperform fine-tuned language models for certain\nentity types.\nOur work has several potential limitations and extensions. First, we only had access to\na subset of the RareDis corpus (832 out of 1041 texts), so our results may not fully reflect\nChatGPT’s performance across the entire spectrum of rare diseases. Second, the current\nwork focuses on ChatGPT and does not include GPT-4 or other variants (e.g., LLaMA,\nAlpaca, etc.), so broadening the current set of experiments to include other large language\nmodels is a natural extension. Last, though manually created prompts are intuitive and\n10\ninterpretable, evidence suggests that small changes can lead to variations in performance\n(Cui et al., 2021). A promising alternative is to automate the prompt engineering pro-\ncess. To this end, Guti´ errez et al. (2022) employed a semi-automated approach combining\nmanually-created prompts with an automatic procedure to choose the best prompt com-\nbination with cross validation. In addition, fully-automated prompt learning approaches\nwhere the prompt is described directly in the embedding space of the underlying language\nmodel are also interesting extensions of the current work (Ma et al., 2021; Taylor et al.,\n2022).\nThe advent of large language models is creating unprecedented opportunities for rare\ndisease phenotyping by automatically identifying and extracting diseases related concepts.\nWhile these models provide valuable insights and assistance, researchers and clinicians\nshould critically evaluate model outputs and be well-informed of their limitations when\nconsidering them as tools for supporting rare disease diagnosis and treatment.\n5. Data and Code\nThe RareDis corpus can be found using the link provided in Mart´ ınez-deMiguel et al. (2022).\nThe code for the current study can be found at\nhttps://github.com/cathyshyr/rare disease phenotype extraction.\n11\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. Large\nlanguage models are few-shot clinical information extractors. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing , pages 1998–2022,\n2022.\nFaraz S Ahmad, Iben M Ricket, Bradley G Hammill, Lisa Eskenazi, Holly R Robertson,\nLesley H Curtis, Cecilia D Dobi, Saket Girotra, Kevin Haynes, Jorge R Kizer, et al.\nComputable phenotype implementation for a national, multicenter pragmatic clinical\ntrial: lessons learned from adaptable. Circulation: Cardiovascular Quality and Outcomes,\n13(6):e006292, 2020.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann,\nand Matthew McDermott. Publicly available clinical bert embeddings. arXiv preprint\narXiv:1904.03323, 2019.\nNikkola Carmichael, Judith Tsipis, Gail Windmueller, Leslie Mandel, and Elicia Estrella.\n“is it going to hurt?”: the impact of the diagnostic odyssey on children and their families.\nJournal of Genetic Counseling , 24:325–335, 2015.\nMartin Chapman, Jes´ us Dom´ ınguez, Elliot Fairweather, Brendan Delaney, and Vasa Curcin.\nUsing computable phenotypes in point-of-care clinical trial recruitment. In Public Health\nand Informatics-Proceedings of MIE 2021: Studies in Health Technology and Informatics,\npages 560–564. IOS Press, 2021.\nQingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi Keloth, Xueqing Peng, Kalpana\nRaja, Rui Zhang, Zhiyong Lu, and Hua Xu. Large language models in biomedical nat-\nural language processing: benchmarks, baselines, and recommendations. arXiv preprint\narXiv:2305.16326, 2023.\nXiang Chen, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, Huajun\nChen, and Ningyu Zhang. Lightner: a lightweight tuning paradigm for low-resource ner\nvia pluggable prompting. arXiv preprint arXiv:2109.00720 , 2021.\nJanet Elizabeth Childerhose, Carla Rich, Kelly M East, Whitley V Kelley, Shirley Simmons,\nCandice R Finnila, Kevin Bowling, Michelle Amaral, Susan M Hiatt, Michelle Thompson,\net al. The therapeutic odyssey: Positioning genomic sequencing in the search for a child’s\nbest possible life. AJOB Empirical Bioethics, 12(3):179–189, 2021.\nClaudia Ching Yan Chung, Hong Kong Genome Project, Annie Tsz Wai Chu, and Brian\nHon Yin Chung. Rare disease emerging as a global public health priority. Frontiers in\npublic health, 10:1028545, 2022.\nJulie S Cohen and Barbara B Biesecker. Quality of life in rare genetic conditions: a sys-\ntematic review of the literature. American Journal of Medical Genetics Part A , 152(5):\n1136–1156, 2010.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. Template-based named entity\nrecognition using bart. arXiv preprint arXiv:2106.01760 , 2021.\n12\nMary F Davis, Subramaniam Sriram, William S Bush, Joshua C Denny, and Jonathan L\nHaines. Automated extraction of clinical traits of multiple sclerosis in electronic medical\nrecords. Journal of the American Medical Informatics Association , 20(e2):e334–e340,\n2013.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\nHermenegildo Fabregat, Lourdes Araujo, and Juan Martinez-Romo. Deep neural models\nfor extracting entities and relationships in the new rdd corpus relating disabilities and\nrare diseases. Computer methods and programs in biomedicine , 164:121–129, 2018.\nBernal Jim´ enez Guti´ errez, Nikolas McNeal, Clay Washington, You Chen, Lang Li, Huan\nSun, and Yu Su. Thinking about gpt-3 in-context learning for biomedical ie? think again.\narXiv preprint arXiv:2203.08410 , 2022.\nYan Hu, Iqra Ameer, Xu Zuo, Xueqing Peng, Yujia Zhou, Zehan Li, Yiming Li, Jianfu Li,\nXiaoqian Jiang, and Hua Xu. Zero-shot clinical entity recognition using chatgpt. arXiv\npreprint arXiv:2303.16416, 2023.\nNORD Rare Insights. Barriers to rare disease diagnosis, care and treatment in the us: a\n30-year comparative analysis, 2020.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark.\nMimic-iii, a freely accessible critical care database. Scientific data, 3(1):1–9, 2016.\nPeter Lee, Carey Goldberg, and Isaac Kohane. The ai revolution in medicine: Gpt-4 and\nbeyond, 2023.\nLishuang Li, Liuke Jin, Zhenchao Jiang, Dingxin Song, and Degen Huang. Biomedical\nnamed entity recognition based on extended recurrent neural networks. In 2015 IEEE In-\nternational Conference on bioinformatics and biomedicine (BIBM), pages 649–652. IEEE,\n2015.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neu-\nbig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural\nlanguage processing. ACM Computing Surveys , 55(9):1–35, 2023.\nTommaso Lo Barco, Mathieu Kuchenbuch, Nicolas Garcelon, Antoine Neuraz, and Rima\nNabbout. Improving early diagnosis of rare diseases using natural language processing in\nunstructured medical records: an illustration from dravet syndrome. Orphanet Journal\nof Rare Diseases, 16:1–12, 2021.\nRuotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Linyang Li, Qi Zhang, and Xuanjing Huang.\nTemplate-free prompt tuning for few-shot ner. arXiv preprint arXiv:2109.13532 , 2021.\n13\nEllen F Macnamara, Precilla D’Souza, Cynthia J Tifft, et al. The undiagnosed diseases\nprogram: Approach to diagnosis. Translational Science of Rare Diseases, 4(3-4):179–188,\n2019.\nClaudia Mart´ ınez-deMiguel, Isabel Segura-Bedmar, Esteban Chac´ on-Solano, and Sara\nGuerrero-Aspizua. The raredis corpus: a corpus annotated with rare diseases, their\nsigns and symptoms. Journal of Biomedical Informatics , 125:103961, 2022.\nLars Mehnen, Stefanie Gruarin, Mina Vasileva, and Bernhard Knapp. Chatgpt as a medical\ndoctor? a diagnostic accuracy study on common and rare diseases. medRxiv, pages 2023–\n04, 2023.\nSt´ ephanie Nguengang Wakap, Deborah M Lambert, Annie Olry, Charlotte Rodwell, Char-\nlotte Gueydan, Val´ erie Lanneau, Daniel Murphy, Yann Le Cam, and Ana Rath. Esti-\nmating cumulative point prevalence of rare diseases: analysis of the orphanet database.\nEuropean Journal of Human Genetics , 28(2):165–173, 2020.\nSagar U Nigwekar, Craig A Solid, Elizabeth Ankers, Rajeev Malhotra, William Eggert,\nAlexander Turchin, Ravi I Thadhani, and Charles A Herzog. Quantifying a rare disease\nin administrative data: the example of calciphylaxis.Journal of general internal medicine,\n29:724–731, 2014.\nOpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.\nNita Patil, Ajay Patil, and BV Pawar. Named entity recognition using conditional random\nfields. Procedia Computer Science, 167:1181–1188, 2020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\nAna Rath, Annie Olry, Ferdinand Dhombres, Maja Miliˇ ci´ c Brandt, Bruno Urbero, and\nSegolene Ayme. Representation of rare diseases in health information systems: the or-\nphanet approach to serve a wide range of end users. Human mutation , 33(5):803–808,\n2012.\nIsabel Segura-Bedmar, David Camino-Perdones, and Sara Guerrero-Aspizua. Exploring\ndeep learning methods for recognizing rare diseases and their clinical manifestations from\ntexts. BMC bioinformatics, 23(1):263, 2022.\nspaCy. Industrial-strength natural language processing in python. https://spacy.io.\nPontus Stenetorp, Sampo Pyysalo, Goran Topi´ c, Tomoko Ohta, Sophia Ananiadou, and\nJun’ichi Tsujii. Brat: a web-based tool for nlp-assisted text annotation. In Proceedings\nof the Demonstrations at the 13th Conference of the European Chapter of the Association\nfor Computational Linguistics , pages 102–107, 2012.\nNiall Taylor, Yi Zhang, Dan Joyce, Alejo Nevado-Holgado, and Andrey Kormilitzin. Clinical\nprompt learning with frozen language models. arXiv preprint arXiv:2205.05535 , 2022.\n14\nCynthia J Tifft and David R Adams. The national institutes of health undiagnosed diseases\nprogram. Current opinion in pediatrics , 26(6):626, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in\nneural information processing systems, 30, 2017.\nYanshan Wang, Liwei Wang, Majid Rastegar-Mojarad, Sungrim Moon, Feichen Shen,\nNaveed Afzal, Sijia Liu, Yuqun Zeng, Saeed Mehrabi, Sunghwan Sohn, et al. Clinical\ninformation extraction applications: a literature review. Journal of biomedical informat-\nics, 77:34–49, 2018.\nHang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, and Xipeng Qiu. A unified\ngenerative framework for various ner subtasks. arXiv preprint arXiv:2106.01223 , 2021.\nGrace Yang, Inna Cintina, Anne Pariser, Elisabeth Oehrlein, Jamie Sullivan, and Annie\nKennedy. The national economic burden of rare disease in the united states in 2019.\nOrphanet journal of rare diseases , 17(1):1–11, 2022.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7371891736984253
    },
    {
      "name": "Bottleneck",
      "score": 0.7194555997848511
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6337100267410278
    },
    {
      "name": "Natural language processing",
      "score": 0.5904189944267273
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5850071310997009
    },
    {
      "name": "Language model",
      "score": 0.5460852980613708
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5316513180732727
    },
    {
      "name": "F1 score",
      "score": 0.5237320065498352
    },
    {
      "name": "Machine learning",
      "score": 0.497104674577713
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4945010840892792
    },
    {
      "name": "Task (project management)",
      "score": 0.2955453395843506
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210163477",
      "name": "Institute for Medical Informatics and Biostatistics",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I919571938",
      "name": "The University of Texas Health Science Center at Houston",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I901861585",
      "name": "Vanderbilt University Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ]
}