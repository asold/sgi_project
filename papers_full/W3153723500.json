{
    "title": "How Certain is Your Transformer?",
    "url": "https://openalex.org/W3153723500",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A294125378",
            "name": "Artem Shelmanov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2809879217",
            "name": "Evgenii Tsymbalov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3153942972",
            "name": "Dmitri Puzyrev",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3001712327",
            "name": "Kirill Fedyanin",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2110658729",
            "name": "Alexander Panchenko",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2040166281",
            "name": "Maxim Panov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W131533222",
        "https://openalex.org/W1872312298",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3099037746",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2810438149",
        "https://openalex.org/W2963215553",
        "https://openalex.org/W582134693",
        "https://openalex.org/W2126022166",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2076410399",
        "https://openalex.org/W2138779671",
        "https://openalex.org/W2964016190",
        "https://openalex.org/W2597787948",
        "https://openalex.org/W3156031277",
        "https://openalex.org/W4210862026",
        "https://openalex.org/W2963675284",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3103014337",
        "https://openalex.org/W2496219407",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2981644170",
        "https://openalex.org/W2963238274",
        "https://openalex.org/W2964059111",
        "https://openalex.org/W2928125049",
        "https://openalex.org/W2788907134",
        "https://openalex.org/W2006672084",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W3101075487",
        "https://openalex.org/W2951786554",
        "https://openalex.org/W3035436280",
        "https://openalex.org/W3010624034",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2964282813",
        "https://openalex.org/W2787825417",
        "https://openalex.org/W2964348886",
        "https://openalex.org/W2964030506"
    ],
    "abstract": "Artem Shelmanov, Evgenii Tsymbalov, Dmitri Puzyrev, Kirill Fedyanin, Alexander Panchenko, Maxim Panov. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.",
    "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1833–1840\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n1833\nHow Certain is Your Transformer?\nArtem Shelmanov‡, Evgenii Tsymbalov†,‡, Dmitri Puzyrev‡,⋆, Kirill Fedyanin‡,\nAlexander Panchenko‡, and Maxim Panov‡\n‡Skolkovo Institute of Science and Technology, Moscow, Russia\n†Huawei Industry Video Application Lab, Moscow, Russia\n⋆Mobile TeleSystems (MTS), Moscow, Russia\n{a.shelmanov,evgenii.tsymbalov,dmitry.puzyrev,k.fedyanin,a.panchenko,m.panov}@skoltech.ru\nAbstract\nIn this work, we consider the problem of uncer-\ntainty estimation for Transformer-based mod-\nels. We investigate the applicability of uncer-\ntainty estimates based on dropout usage at the\ninference stage (Monte Carlo dropout). The\nseries of experiments on natural language un-\nderstanding tasks shows that the resulting un-\ncertainty estimates improve the quality of de-\ntection of error-prone instances. Special atten-\ntion is paid to the construction of computation-\nally inexpensive estimates via Monte Carlo\ndropout and Determinantal Point Processes.\n1 Introduction\nQuantifying the uncertainty of machine learning\nmodels is an important aspect of trustworthy, reli-\nable, and accountable natural language understand-\ning (NLU) systems. Obtaining measures of uncer-\ntainty in predictions (also known as uncertainty es-\ntimations, UE) helps to detect out-of-domain (Ma-\nlinin and Gales, 2018), adversarial, or error-prone\ninstances that require special treatment. For exam-\nple, such instances can be additionally checked by\nhuman experts or another more advanced system\nor alternatively rejected from classiﬁcation (Her-\nbei and Wegkamp, 2006). Besides, uncertainty\nestimation is an essential component of various ap-\nplications such as active learning (Shelmanov et al.,\n2021) and outlier/error detection in a dataset (Lar-\nson et al., 2019).\nMany modern NLU methods take advantage of\ndeep pre-trained models that are based on the Trans-\nformer architecture (Vaswani et al., 2017) (e.g.,\nBERT (Devlin et al., 2019) or ELECTRA (Clark\net al., 2020)). Obtaining reliable uncertainty estima-\ntions for such neural networks (NNs) can, therefore,\ndirectly beneﬁt a wide range of NLU tasks, yet im-\nplementing UEs, in this case, is challenging due to\nthe huge number of parameters in these deep learn-\ning models. The approximations of Bayesian infer-\nence based on dropout usage at the inference stage\n– Monte Carlo (MC) dropout (Gal and Ghahramani,\n2016), provide a realizable approach to quantifying\nUEs of deep models. However, they are usually ac-\ncompanied by serious computational overhead due\nto the necessity of performing multiple stochastic\npredictions. Importantly, training ensembles of in-\ndependent models (Lakshminarayanan et al., 2017)\nleads to even more prohibitive overheads.\nIn this work, we investigate various MC dropout-\nbased approaches to uncertainty quantiﬁcation of\nNLU models on the widely-used General Lan-\nguage Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2018). The main contributions\nof our work are two-fold:1\n•We show that the use of the MC dropout with\npre-trained Transformer models signiﬁcantly\nimproves the quality of UEs in NLU tasks\ncompared to deterministic baselines.\n•We are the ﬁrst to our knowledge to apply\na modiﬁcation of the MC dropout based on\ndeterminantal point processes (DPP; Tsym-\nbalov et al. (2020)) to Transformers and show\nthat this approach allows obtaining the UEs\ncompetitive to the standard MC dropout at a\nfraction of its cost. To improve the stability\nof the DPP-based dropout for Transformer-\nbased models, we extend the method pre-\nsented in Tsymbalov et al. (2020) by aver-\naging multiple dropout masks sampled with\nDPP.\n2 Related Work\nThree dominating approaches to uncertainty es-\ntimation in neural networks exist: (i) interpreta-\ntion of the model’s logits from the uncertainty\nestimation perspective (Gal, 2016), which is the\nbasic one; (ii) ensembling, where a discrepancy\n1Code of our experiments: https://github.com/\nskoltech-nlp/certain-transformer\n1834\nbetween models’ predictions are interpreted as a\nsample variance (Lakshminarayanan et al., 2017);\n(iii) Bayesian neural networks (Teye et al., 2018),\nwhich have a built-in mechanism to capture uncer-\ntainty via a single model.\nThere are a few recent works that investigate un-\ncertainty quantiﬁcation for NLP models and use\nMC dropout techniques. Dong et al. (2018) use\nBayesian UEs for the analysis of semantic parser\npredictions for correctness. Zhang et al. (2019)\npropose an additional training loss component that\nfacilitates smaller inter-class and bigger intra-class\ndistances in the vector space of the output layer.\nExperiments with convolutional NNs on text classi-\nﬁcation datasets show that this modiﬁcation helps\nto improve error detection using MC dropout UEs.\nFor quantifying model data uncertainty, Xiao and\nWang (2019) use NNs to parameterize a probability\ndistribution (mean and variance) instead of mak-\ning a prediction directly. For quantifying model\nuncertainty, the authors leverage the MC dropout.\nModeling both types of uncertainties in convolu-\ntional and recurrent NNs helped them to improve\nthe performance in regression and classiﬁcation\nNLP tasks. Kochkina and Liakata (2020) apply\nUEs to the problem of rumor veriﬁcation.\n3 Uncertainty Estimation of Deep\nTransformer Neural Networks\nIn this section, we describe types of dropout, un-\ncertainty estimation methods, and the Transformer-\nbased neural classiﬁer used in our experiments.\n3.1 Types of Dropout\nWe use two types of dropout described below.\nMonte Carlo Dropout The dropout (Srivastava\net al., 2014) has emerged as a powerful and univer-\nsal regularization technique applicable to most DL\narchitectures, with the Transformers not being an\nexception. Despite originally being an empirical,\nengineering way to ﬁght the overﬁtting, it then ob-\ntained a theoretical explanation as a special case of\nBayesian NNs, where activations are drawn from\nthe Bernoulli distribution (Gal, 2016). This allows\nto represent a vector of outputs xh of the h-th layer\nof the network as a function of its weights Wh,\nactivation function σ, and a dropout mask Mh:\nxh = σ(xh−1 |Wh,Mh),Mh ∼Bernoulli(1−p),\nwhere p∈[0; 1]is the dropout rate.\nThis theoretical explanation enables the use of\nthe dropout not only at the training stage but also\nat the inference stage via sampling of multiple\nmasks M(t)\nh ,t = 1,...,T for each dropout layer\nof the network h and subsequently providing an\nensemble of models parameterized by these masks:\nft(x) = f\n(\nx|\n{\nM(t)\nh\n})\n. The obtained UEs are rel-\natively fast, convenient, and applicable to various\ntasks, such as regression (Tsymbalov et al., 2018),\nimage classiﬁcation (Gal and Ghahramani, 2016),\nand active learning (Gal et al., 2017; Siddhant and\nLipton, 2018).\nMonte Carlo Dropout with Determinantal\nPoint Processes The models obtained from the\nstandard dropout masks usually show a high de-\ngree of correlation in predictions between them,\nlimiting the power of the resulting ensemble. Re-\ncently, it was proposed to improve the diversity\nof predictions by considering the correlations be-\ntween neurons and sampling the diverse neurons\nvia the mechanism of Determinantal Point Pro-\ncesses (DPP; Kulesza and Taskar (2012)), an ap-\nproach for sampling diverse elements from a set of\npoints. This setup was proposed by (Tsymbalov\net al., 2020) and evaluated for the simple multilayer\nperceptrons and CNNs. In this work, we aim to\nextend this approach to Transformer models.\nDPP-based dropout masks MDPP\nh for the h-th\nlayer are constructed using the correlation matrix\nCh between neurons as a likelihood kernel for the\nDPP: MDPP\nh ∼DPP\n(\nCh\n)\n. The probability to\nselect a set Sof activations on the layer his given\nby\nP[MDPP\nh = S] = det CS\nh\ndet\n(\nCh + I\n),\nwhere CS\nh is a square submatrix of Ch obtained\nby keeping only rows and columns indexed by the\nsample S. The matrix of correlations between ac-\ntivations of the h-th layer Ch is estimated empiri-\ncally based on some set of points, which represents\nthe data distribution well enough (i.e. training set).\nThe key feature of the approach is that DPP tends\nto sample neurons with low correlations between\nthem, which in turn improves the overall diversity\nof the obtained models. More information about\nDPP is presented in Appendix B.\nTo improve the stability of the DPP-based\ndropout for Transformer-based models, we create\na ﬁnal dropout mask by sampling from DPP and\naveraging multiple initial masks.\n1835\n3.2 Uncertainty Estimates\nLet T be a number of stochastic passes, i.e., the\nnumber of dropout masks to be sampled. We use\nthe three following UEs (also known as acquisition\nmethods) for the classiﬁcation with Cclasses:\n•Sampled maximum probability:\n1 −max\nc\n¯pT (y= c|x),\nwhere ¯pT is an average probability for the\nclass c prediction over multiple stochastic\npasses t= 1,...,T .\n•Probability variance averaged over classes:\n1\nT\nC∑\nc=1\nT∑\nt=1\n(\npt(y= c|x) −¯pT (y= c|x)\n)2.\n•Bayesian Active Learning by Disagreement\n(BALD) proposed by Houlsby et al. (2011)\ndescribes the mutual information between out-\nputs and model parameters:\nH(x)+ 1\nT\nC∑\nc=1\nT∑\ni=1\np(y= c|x) log\n(\np(y= c|x)\n)\n,\nwhere H(x) is the entropy of the ensemble\nmean.\nWe would like to note that all these estimates can\nbe used for any ensembling technique, including\nthe MC dropout and the DPP-based dropout.\n3.3 Classiﬁcation Models\nIn this work, we focus on the ELECTRA (Clark\net al., 2020) model, which is a recent successor\nto BERT (Devlin et al., 2019). It is based on the\nsame Transformer architecture but takes advantage\nof the harder “replaced token detection” objective\ninstead of the “masked language model” objec-\ntive. This gives better pre-training capabilities and\nmakes ELECTRA the state-of-the-art Transformer\nin natural language understanding benchmarks. We\nshould note that ELECTRA is regularized with mul-\ntiple dropout layers, which facilitates the usage of\nthe MC dropout. For example, the body of the\n“ELECTRA-base” model has 37 dropout layers.\nWe also experiment with DistilBERT (Sanh\net al., 2019), which is a smaller Transformer ob-\ntained from the middle-size BERT (Devlin et al.,\n2019) via a distillation procedure (Hinton et al.,\n2015). This model provides the faster inference\nand has smaller memory requirements but retains\n97% of the language understanding capabilities of\nthe original model according to Sanh et al. (2019).\n4 Experiments\n4.1 Experimental Setup\nWe evaluate the UEs on the basis of their ability to\ndetect misclassiﬁcation. High UEs should indicate\npotential errors in the model output, while low un-\ncertainties should correspond to correctly classiﬁed\ninstances. In this vein, we transform the original\ntask into a binary classiﬁcation task by comparing\npredictions of a model with the ground truth labels\nin the validation dataset. Uncertainty estimates on\nthe validation dataset are treated as the outputs of\nthe binary classiﬁer that is trained to look for po-\ntential errors. We calculate the ROC AUC score\nusing the new ground truth labels and UEs and use\nthis score as the main evaluation metric.\nThe baseline in this task is the UE calculated\nbased on the maximal probability of the original de-\nterministic model. We compare it to the estimates\nobtained using multiple stochastic predictions with\nactivated dropout layers. Three variants of esti-\nmates are calculated: 1) based on the model, in\nwhich MC dropout is applied to all dropout layers;\n2) based on the model with the MC dropout applied\nonly to the last layer; 3) based on the model with\nthe DPP-based sampling applied to the last dropout\nlayer. For calculating these UEs, we conduct 20\nstochastic predictions. The dropout rate in these\npasses for the MC dropout is 0.1, which is shown\nto be optimal in the preliminary experiments. For\nthe DPP dropout, we sample and average multiple\nmasks produced by DPP. In experiments with SST-\n2 and ELECTRA, we average as many masks so at\nleast 30% of neurons remain active during the pass\n(this roughly can be considered as a “dropout rate”\nof 0.7). For MRPC, we choose the “dropout rate”\nequal to 0.2 and for CoLA: 0.4. For DistilBERT,\nwe use the “dropout rate” of 0.4 in all tasks.\nWe train three versions of models with differ-\nent random seeds. For each model, another ﬁve\nrandom seeds are used to produce predictions for\nstochastic methods. Multiple models and predic-\ntions are used for estimating the standard deviation\nand conducting the statistical signiﬁcance testing.\n4.2 Datasets\nWe evaluate UEs and dropout variants on the\nwidely used NLU benchmark GLUE (Wang et al.,\n2018). Speciﬁcally, we perform experiments on\nthree tasks: Stanford Sentiment Treebank (SST-2;\nSocher et al. (2013)), Corpus of Linguistic Ac-\nceptability (CoLA; Warstadt et al. (2019)), and\n1836\nModel Tasks\nDropout Type Acquisition Dropout Layers SST-2 MRPC CoLA\nNo (baseline) Max. probability - 79.7±3.4 78.6 ±4.1 78.7 ±2.0\nMC Sampled max. probability last -0.1±0.2 -0.5 ±0.4 -0.1 ±0.1\nMC Probability variance last -1.9±1.0 -3.0 ±0.7 -1.4 ±0.7\nMC BALD last -5.0±1.7 -5.6 ±1.3 -3.9 ±1.3\nDPP Sampled max. probability last 3.2±2.4 0.0±0.7 -0.4 ±0.7\nDPP Probability variance last 2.7±3.1 1.5 ±2.3 -1.1±1.3\nDPP BALD last 0.7±2.7 2.1±3.1 -2.0±2.1\nMC Sampled max. probability all 3.2±1.6 5.5 ±2.6 3.2 ±0.6\nMC Probability variance all 4.7±2.1 7.2 ±3.1 2.9 ±0.4\nMC BALD all 5.2±2.4 7.5 ±3.3 2.8 ±0.4\nTable 1: The misclassiﬁcation detection performance (ROC AUC) ( ±SD) for the baseline with the ELECTRA\nmodel and performance improvements over the baseline for various UE methods. Statistically signiﬁcant improve-\nments (p-value≤0.05) are highlighted.\nMicrosoft Research Paraphrase Corpus (MRPC;\nDolan and Brockett (2005)). The SST-2 task is\nto predict the sentiment of a given sentence (pos-\nitive/ negative). The SST-2 dataset was randomly\nsubsampled to 2% of the original size to emulate\nthe situation with a small amount of training data.\nThe CoLA task is to determine whether the given\nsentence is grammatical or not. The MRPC task is\nto predict whether two given sentences are seman-\ntically similar or not. We select these three datasets\nfor their compact size.\n4.3 Model and Training Details\nWe use the middle-size pre-trained ELECTRA-\nbase model with 110 million parameters and the\nDistilBERT model with 66 million parameters ob-\ntained from the middle-size BERT. The implemen-\ntation of the models is provided by the Huggingface\nTransformers library (Wolf et al., 2020). For ﬁne-\ntuning models, we follow the approach described\nby Clark et al. (2020) and Devlin et al. (2019):\ntrain for 4 epochs with 10% warm-up and a linear\nlearning rate scheduler. For all models and tasks,\nwe use the same learning rate equal to 5e-5. For\nELECTRA and SST-2 and MRPC tasks, the batch\nsize is 16. For ELECTRA and CoLA, the batch\nsize is 32. For DistilBERT, the batch size is 32 for\nall tasks. Although calibrating these hyperparam-\neters can yield some performance improvements,\nthe aforementioned settings allow achieving good\nresults across all tasks.\n4.4 Results and Discussion\nROC AUC scores for the misclassiﬁcation detec-\ntion task and ELECTRA are presented in Table 1.\nThe results for DistilBERT are presented in Table\n3 in Appendix A. While the classiﬁer performance\ndoes not signiﬁcantly variate across multiple ver-\nsions of the ﬁne-tuned models, the difference in the\nmisclassiﬁcation detection performance is statisti-\ncally signiﬁcant. Therefore, we present the abso-\nlute values of the performance only for the baseline\n(UE based on maximum probability), while for\nother methods, we present the improvement over\nthe baseline across multiple runs. Tables with re-\nsults also present the standard deviation of scores.\nWe note that the UE based on the maximum\nprobability of the deterministic model is a strong\nbaseline. Overall, Transformers are able to indi-\ncate their potential mistakes with just the proba-\nbility from the softmax layer. Applying the MC\ndropout to all dropout layers in the network always\ngives a reliable boost in the misclassiﬁcation de-\ntection. For SST-2 and MRPC tasks, UE based\non BALD demonstrates better performance than\nsampled maximum probability and variance, while\non CoLA, all UEs perform comparably well. The\nbiggest improvement can be achieved for MRPC\nand ELECTRA: up to 7.5% ROC AUC.\nOn the contrary, the UEs based on the MC\ndropout applied only to the last layer do not per-\nform well. We see that the misclassiﬁcation detec-\ntion performance always deteriorates compared to\nthe baseline, especially, for variance and BALD.\nUEs that take advantage of the DPP-based masks\napplied to the last dropout layer are somewhere in\nthe middle in terms of quality compared to the\nMC dropout variants. Although this method also\ndoes not give any improvement for CoLA, unlike\nthe last layer MC dropout, DPP gives a signiﬁcant\n1837\nadvantage over the baseline on the SST-2 task for\nboth models and on the MRPC task for ELECTRA.\nWe note that although DPP-based sampling and the\nlast layer MC dropout diverge in terms of “dropout\nrate” (e.g., in the experiment on the SST-2 task with\nELECTRA, 0.7 for the DPP dropout versus 0.1 for\nthe MC dropout), this aspect does not explain the\nperformance difference. Applying dropout rates\nhigher than 0.1 to the MC dropout downgrades the\nperformance of the misclassiﬁcation detection due\nto the overall decrease of the model quality, while\nfor DPP, only 30% of neurons is more than enough\nto retain the model performance and obtain better\nUEs on the SST-2 task.\nDespite the fact that the DPP-based approach\nappears to be worse than applying the MC dropout\non all layers, it is much faster since it is applied\nto only the last dropout layer. For practical appli-\ncations, obtaining UEs normally should not cause\na signiﬁcant overhead compared to the standard\nmodel inference time. This strikes the methods\nbased on the MC dropout since they require multi-\nple stochastic predictions. However, for most of the\npre-trained Transformers, if only the last dropout\nis replaced with the MC variant, the outputs of the\nmassive Transformer “body” are not affected dur-\ning the stochastic predictions. This means that the\nbody outputs can be calculated only once, and only\nthe last linear layer with the softmax activation\nshould be recalculated multiple times. As the last\nlayer contains less than 1% of total parameters, this\nfavors the UEs that do not use stochastic inference\non dropout layers except the last. Compared to\nmasks generated uniformly with the MC dropout,\nsampling masks with DPP has some insigniﬁcant\ncomputation overhead, but, as we showed, it can\ngive a useful contribution to the misclassiﬁcation\nperformance (for MRPC and SST-2) even if it is\nused only in the last dropout layer.\nWe performed an investigation of computation\ntime overhead for calculating UEs with various\nMC dropout options for the development dataset.\nThe results for ELECTRA are presented in Table 2.\nThe computations were conducted with the Nvidia\n2080ti GPU and the Intel Xeon 5217 CPU. We use\nBALD as an acquisition function, but other func-\ntions have comparable execution time. The MC\ndropout placed on all layers of Transformers gives\nbetter improvements, but it causes roughly 2,000%\noverhead (in the case of 20 stochastic passes), with\nless than 10% overhead for the last layer MC and\nInference time, sec.\nUE Method SST-2 MRPC\nDeterministic, − 3.07 ±0.03 1.43 ±0.04\nMC dropout, all 65.5 ±0.7 30.2 ±0.2\nMC dropout, last 3.17 ±0.06 1.51 ±0.05\nDPP dropout, last 3.33 ±0.02 1.57 ±0.01\nTable 2: The inference time of the ELECTRA model\non the development dataset with BALD UE.\nDPP. Therefore, DPP can provide a better trade-\noff between computation time and performance of\nerror detection.\n5 Conclusion\nIn this work, we evaluated several UEs for the state-\nof-the-art Transformer model ELECTRA and the\nspeed-oriented DistilBERT model in the text clas-\nsiﬁcation tasks. To obtain estimates, we leverage\nmultiple stochastic passes using the MC dropout,\nand the DPP-based dropout proposed by (Tsym-\nbalov et al., 2020). We show that by activating all\ndropouts in the model for stochastic predictions,\none can beat the baseline deterministic uncertainty\nestimate by the signiﬁcant margin in the binary\nmisclassiﬁcation detection task. We also demon-\nstrate that replacing the last dropout layer with the\nDPP dropout can yield signiﬁcant improvements\nover the baseline in some cases, but less than the\nusage of the MC dropout on all dropout layers. De-\nspite being inferior compared to the latter, the DPP\ndropout can provide a better trade-off between com-\nputation time and performance of error detection,\nwhich can be important for practical use cases.\nIn future work, we are seeking to improve UEs\nquality obtained using the DPP dropout with the\nhelp of calibration (Safavi et al., 2020) and conduct\nexperiments on sequence tagging tasks.\nAcknowledgments\nWe thank the reviewers for their valuable feedback.\nThe development of uncertainty estimation algo-\nrithms for Transformer models (Section 3) was\nsupported by the joint MTS-Skoltech lab. The de-\nvelopment of a software system for the experimen-\ntal study of uncertainty estimation methods and its\napplication to NLP tasks (Section 4) was supported\nby the Russian Science Foundation grant 20-71-\n10135. The Zhores supercomputer (Zacharov et al.,\n2019) was used for computations.\n1838\nReferences\nAli C ¸ ivril and Malik Magdon-Ismail. 2009. On select-\ning a maximum volume sub-matrix of a matrix and\nrelated problems. Theoretical Computer Science ,\n410(47-49):4801–4811.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing, IWP@IJCNLP 2005, Jeju Island,\nKorea, October 2005, 2005 . Asian Federation of\nNatural Language Processing.\nLi Dong, Chris Quirk, and Mirella Lapata. 2018. Conﬁ-\ndence modeling for neural semantic parsing. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 743–753, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nYarin Gal. 2016. Uncertainty in deep learning. Univer-\nsity of Cambridge.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as\na bayesian approximation: Representing model un-\ncertainty in deep learning. In Proceedings of the\n33nd International Conference on Machine Learn-\ning, ICML 2016, New York City, NY, USA, June 19-\n24, 2016, volume 48 of JMLR Workshop and Con-\nference Proceedings, pages 1050–1059. JMLR.org.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani.\n2017. Deep bayesian active learning with image\ndata. In Proceedings of the 34th International Con-\nference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 , volume 70 of\nProceedings of Machine Learning Research , pages\n1183–1192. PMLR.\nSergei A Goreinov, Ivan V Oseledets, Dimitry V\nSavostyanov, Eugene E Tyrtyshnikov, and Nikolay L\nZamarashkin. 2010. How to ﬁnd a good submatrix.\nIn Matrix Methods: Theory, Algorithms And Appli-\ncations: Dedicated to the Memory of Gene Golub ,\npages 247–256. World Scientiﬁc.\nRadu Herbei and Marten H. Wegkamp. 2006. Classi-\nﬁcation with reject option. The Canadian Journal\nof Statistics / La Revue Canadienne de Statistique ,\n34(4):709–721.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nNeil Houlsby, Ferenc Huszar, Zoubin Ghahramani,\nand M ´at´e Lengyel. 2011. Bayesian active learning\nfor classiﬁcation and preference learning. CoRR,\nabs/1112.5745.\nElena Kochkina and Maria Liakata. 2020. Estimat-\ning predictive uncertainty for rumour veriﬁcation\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6964–6981, Online. Association for Computa-\ntional Linguistics.\nAlex Kulesza and Ben Taskar. 2012. Determinantal\npoint processes for machine learning. Found. Trends\nMach. Learn., 5(2-3):123–286.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 6402–6413.\nStefan Larson, Anish Mahendran, Andrew Lee,\nJonathan K. Kummerfeld, Parker Hill, Michael A.\nLaurenzano, Johann Hauswald, Lingjia Tang, and Ja-\nson Mars. 2019. Outlier detection for improved data\nquality and diversity in dialog systems. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 517–527, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nOdile Macchi. 1975. The coincidence approach to\nstochastic point processes. Advances in Applied\nProbability, 7(1):83–122.\nA Malinin and M Gales. 2018. Predictive uncertainty\nestimation via prior networks. In NIPS’18: Proceed-\nings of the 32nd International Conference on Neural\nInformation Processing Systems , volume 31, pages\n7047–7058. Curran Associates, Inc.\nTara Safavi, Danai Koutra, and Edgar Meij. 2020. Eval-\nuating the Calibration of Knowledge Graph Embed-\ndings for Trustworthy Link Prediction. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8308–8321, Online. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\n1839\nArtem Shelmanov, Dmitri Puzyrev, Lyubov\nKupriyanova, Denis Belyakov, Daniil Larionov,\nNikita Khromov, Olga Kozlova, Ekaterina Arte-\nmova, Dmitry V . Dylov, and Alexander Panchenko.\n2021. Active learning for sequence tagging with\ndeep pre-trained models and bayesian uncertainty\nestimates. In Proceedings of the 16th Conference\nof the European Chapter of the Association for\nComputational Linguistics: Volume 1, Long Papers,\nOnline. Association for Computational Linguistics.\nAditya Siddhant and Zachary C. Lipton. 2018. Deep\nBayesian active learning for natural language pro-\ncessing: Results of a large-scale empirical study.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2904–2909, Brussels, Belgium. Association\nfor Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nNitish Srivastava, Geoffrey E. Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. 2014. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. J. Mach. Learn. Res. ,\n15(1):1929–1958.\nMattias Teye, Hossein Azizpour, and Kevin Smith.\n2018. Bayesian uncertainty estimation for batch nor-\nmalized deep networks. In International Conference\non Machine Learning, pages 4907–4916. PMLR.\nEvgenii Tsymbalov, Kirill Fedyanin, and Maxim Panov.\n2020. Dropout strikes back: Improved uncertainty\nestimation via diversity sampled implicit ensembles.\nCoRR, abs/2003.03274.\nEvgenii Tsymbalov, Maxim Panov, and Alexander\nShapeev. 2018. Dropout-based active learning for\nregression. In Analysis of Images, Social Networks\nand Texts - 7th International Conference, AIST 2018,\nMoscow, Russia, July 5-7, 2018, Revised Selected\nPapers, volume 11179 ofLecture Notes in Computer\nScience, pages 247–258. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTrans. Assoc. Comput. Linguistics, 7:625–641.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYijun Xiao and William Yang Wang. 2019. Quanti-\nfying uncertainties in natural language processing\ntasks. In The Thirty-Third AAAI Conference on Arti-\nﬁcial Intelligence, AAAI 2019, The Thirty-First Inno-\nvative Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2019, The Ninth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 7322–7329. AAAI Press.\nIgor Zacharov, Rinat Arslanov, Maksim Gunin, Daniil\nStefonishin, Andrey Bykov, Sergey Pavlov, Oleg\nPanarin, Anton Maliutin, Sergey Rykovanov, and\nMaxim Fedorov. 2019. “zhores” — petaﬂops super-\ncomputer for data-driven modeling, machine learn-\ning and artiﬁcial intelligence installed in skolkovo\ninstitute of science and technology. Open Engineer-\ning, 9(1):512 – 520.\nXuchao Zhang, Fanglan Chen, Chang-Tien Lu, and\nNaren Ramakrishnan. 2019. Mitigating uncertainty\nin document classiﬁcation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3126–3136, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\n1840\nA Results for DistilBERT\nModel Tasks\nDropout Type Acquisition Dropout Layers SST-2 MRPC CoLA\nNo (baseline) Max. probability - 76.6±2.6 76.6 ±0.8 73.0 ±1.7\nMC Sampled max. probability last -0.0±0.1 -0.1 ±0.3 0.0 ±0.1\nMC Probability variance last -0.5±0.4 -0.6 ±0.7 -0.2 ±0.4\nMC BALD last -1.7±0.6 -1.6 ±1.1 -1.1 ±0.7\nDPP Sampled max. probability last 0.6±1.0 0.2±0.6 -0.1 ±0.2\nDPP Probability variance last 0.6±1.3 0.4±1.2 -0.4 ±0.6\nDPP BALD last 0.5±1.6 0.2 ±1.5 -0.8 ±1.0\nMC Sampled max. probability all 0.6±0.2 2.1 ±0.6 1.4 ±0.7\nMC Probability variance all 2.0±0.8 2.4 ±1.0 1.3 ±1.0\nMC BALD all 2.3±1.0 2.4 ±1.2 1.1 ±1.0\nTable 3: The misclassiﬁcation detection performance (ROC AUC) (±SD) for the maximal probability baseline with\nthe DistilBERT model and performance improvements over the baseline for various UEs. Statistically signiﬁcant\nimprovements (p-value ≤0.05) are highlighted.\nB Determinantal Point Processes\nDeterminantal point processes (DPPs) are speciﬁc\nprobability distributions over a set of points. They\nallow choosing the subset of points enforcing the\ndiversity between the samples. The DPPs were\nintroduced for the needs of statistical physics (Mac-\nchi, 1975), and found their applications in machine\nlearning (Kulesza and Taskar, 2012)\nFor example, consider the situation where we\nobserve N news from different outlets during one\nspeciﬁc day. Let us also assume that we can mea-\nsure the corresponding texts’ pairwise similarity. In\nthis case, DPPs allow choosing a number n≪N\nof most non-similar news for the day, giving a good\nrepresentation of the agenda. Most importantly,\nDPPs have efﬁcient implementation for the exact\nsampling and several even more efﬁcient approx-\nimate solutions. We also note that DPP sampling\nis stochastic, i.e., it provides a different result for\neach repetition. That is an essential property for\nthe uncertainty estimation problems we consider in\nthis work.\nFormally, let us assume that the kernel matrix\nKof pairwise similarities between the considered\npoints X is given. DPPs are similar to the al-\ngorithm of ﬁnding maximum volume submatrix\nof K (Goreinov et al., 2010; C ¸ivril and Magdon-\nIsmail, 2009) as geometrically determinant of the\nmatrix is equal to the scaling volume of a corre-\nsponding linear transformation. In this case, a large\nvolume is good because it corresponds to orthog-\nonal vectors (i.e. non-similar vectors). Likewise,\nDPPs sample points Swith probabilities:\nP[S ⊂X] = det KS,\nwhere KS is the submatrix of the matrix Kcorre-\nsponding to points S.\nAs probability takes values between 0 and 1,\nthe matrix K needs to be positive semideﬁnite\nand should not have minors with determinant\nlarger than 1. In practice, usually only some un-\nnormalized likelihood matrix Lis given. The stan-\ndard approach is to normalize it in the following\nway:\nK = L(L+ I)−1.\nIn this case, we can directly calculate the submatrix\nprobabilities:\nP[X = S] = det LS\ndet(L+ I)."
}