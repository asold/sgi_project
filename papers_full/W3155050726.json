{
    "title": "Decoupled Spatial-Temporal Transformer for Video Inpainting",
    "url": "https://openalex.org/W3155050726",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1930211867",
            "name": "Liu Rui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2341874967",
            "name": "Deng, Hanming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2353896415",
            "name": "Huang Yang-yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2336906064",
            "name": "Shi Xiao-yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4295680286",
            "name": "Lu, Lewei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1930196199",
            "name": "Sun, Wenxiu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1832420164",
            "name": "Wang Xiaogang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2345450207",
            "name": "Dai, Jifeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1969345873",
            "name": "Li Hongsheng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1993120651",
        "https://openalex.org/W2551763541",
        "https://openalex.org/W2890447039",
        "https://openalex.org/W3107297981",
        "https://openalex.org/W3094825490",
        "https://openalex.org/W2069237980",
        "https://openalex.org/W2982763192",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2886714066",
        "https://openalex.org/W2131565286",
        "https://openalex.org/W2970400386",
        "https://openalex.org/W2295936755",
        "https://openalex.org/W2560474170",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2798365772",
        "https://openalex.org/W1975049209",
        "https://openalex.org/W2210871532",
        "https://openalex.org/W2986433113",
        "https://openalex.org/W2969109746",
        "https://openalex.org/W2986330308",
        "https://openalex.org/W2954284167",
        "https://openalex.org/W2987614525",
        "https://openalex.org/W2738588019",
        "https://openalex.org/W3097348442",
        "https://openalex.org/W2156235915",
        "https://openalex.org/W2470139095",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2988407809",
        "https://openalex.org/W2886787375",
        "https://openalex.org/W2084227286",
        "https://openalex.org/W3043547428",
        "https://openalex.org/W3095422700",
        "https://openalex.org/W3104310079",
        "https://openalex.org/W1999360130"
    ],
    "abstract": "Video inpainting aims to fill the given spatiotemporal holes with realistic appearance but is still a challenging task even with prosperous deep learning approaches. Recent works introduce the promising Transformer architecture into deep video inpainting and achieve better performance. However, it still suffers from synthesizing blurry texture as well as huge computational cost. Towards this end, we propose a novel Decoupled Spatial-Temporal Transformer (DSTT) for improving video inpainting with exceptional efficiency. Our proposed DSTT disentangles the task of learning spatial-temporal attention into 2 sub-tasks: one is for attending temporal object movements on different frames at same spatial locations, which is achieved by temporally-decoupled Transformer block, and the other is for attending similar background textures on same frame of all spatial positions, which is achieved by spatially-decoupled Transformer block. The interweaving stack of such two blocks makes our proposed model attend background textures and moving objects more precisely, and thus the attended plausible and temporally-coherent appearance can be propagated to fill the holes. In addition, a hierarchical encoder is adopted before the stack of Transformer blocks, for learning robust and hierarchical features that maintain multi-level local spatial structure, resulting in the more representative token vectors. Seamless combination of these two novel designs forms a better spatial-temporal attention scheme and our proposed model achieves better performance than state-of-the-art video inpainting approaches with significant boosted efficiency.",
    "full_text": "Decoupled Spatial-Temporal Transformer for Video Inpainting\nRui Liu‚Ä†* Hanming Deng‚Ä°* Yangyi Huang‚Ä°¬ß Xiaoyu Shi‚Ä† Lewei Lu‚Ä°\nWenxiu Sun‚Ä°‚ôØ Xiaogang Wang‚Ä† Jifeng Dai‚Ä° Hongsheng Li‚Ä†#\n‚Ä†CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong ‚Ä°SenseTime Research\n¬ßZhejiang University ‚ôØTetras.AI #School of CST, Xidian University\n{ruiliu@link, xiaoyushi@link, xgwang@ee, hsli@ee}.cuhk.edu.hk\n{denghanming, huangyangyi, luotto, daijifeng}@sensetime.com\nAbstract\nVideo inpainting aims to Ô¨Åll the given spatiotemporal\nholes with realistic appearance but is still a challenging\ntask even with prosperous deep learning approaches. Re-\ncent works introduce the promising Transformer architec-\nture into deep video inpainting and achieve better perfor-\nmance. However, it still suffers from synthesizing blurry tex-\nture as well as huge computational cost. Towards this end,\nwe propose a novel Decoupled Spatial-Temporal Trans-\nformer (DSTT) for improving video inpainting with excep-\ntional efÔ¨Åciency. Our proposed DSTT disentangles the\ntask of learning spatial-temporal attention into 2 sub-tasks:\none is for attending temporal object movements on differ-\nent frames at same spatial locations, which is achieved by\ntemporally-decoupled Transformer block, and the other is\nfor attending similar background textures on same frame\nof all spatial positions, which is achieved by spatially-\ndecoupled Transformer block. The interweaving stack of\nsuch two blocks makes our proposed model attend back-\nground textures and moving objects more precisely, and\nthus the attended plausible and temporally-coherent ap-\npearance can be propagated to Ô¨Åll the holes. In addi-\ntion, a hierarchical encoder is adopted before the stack\nof Transformer blocks, for learning robust and hierarchi-\ncal features that maintain multi-level local spatial struc-\nture, resulting in the more representative token vectors.\nSeamless combination of these two novel designs forms\na better spatial-temporal attention scheme and our pro-\nposed model achieves better performance than state-of-the-\nart video inpainting approaches with signiÔ¨Åcant boosted ef-\nÔ¨Åciency. Training code and pretrained models are available\nat https://github.com/ruiliu-ai/DSTT.\n1. Introduction\nVideo inpainting is the task to Ô¨Åll spatiotemporal holes\nin video frames with realistic and coherent content. It has\n*The Ô¨Årst two authors contribute equally to this work.\n28.57\n30.28\n30.67\n31.75\n27\n28\n29\n30\n31\n32\n33\n10 15 20 25 30 35 40\nOurs\nSTTN\nCAP\nLGTSM\n200B\n100B\nFLOPs\nFrames Per Second\nPSNR\nFigure 1. Frames per second vs. PSNR accuracy on different\napproaches. Our DSTT model achieves the SOTA 31.75 PSNR\nvalue, yet improves the efÔ¨Åciency signiÔ¨Åcantly to FPS 37.3 with\nmuch fewer model FLOPs. FLOPs are indicated by the radius of\ndashed circle. B is short for Billion.\na wide range of real-world applications such as object re-\nmoval [23, 36], video completion [5, 4] and video restora-\ntion [18].\nDespite the prevalence of deep learning methods, video\ninpainting remains a challenging task due to the complex-\nity of visual contents and deteriorated video frames such as\nmotion blur, occlusion induced by camera movement and\nlarge object movements. Although deep learning-based im-\nage inpainting approaches have made great progress [13,\n34, 35], processing videos frame-by-frame can hardly gen-\nerate videos with temporal consistency. Some other works\nused propagation-based methods to align and propagate\ntemporally-consistent visual contents to obtain more vivid\ninpainting results [33, 10]. However, they usually fail when\nfaced with occluded stationary backgrounds. End-to-end\ntraining a deep generative model for video inpainting and\nperforming direct inference has drawn great attention in re-\ncent years. These methods, however, usually yield blurry\nvideos by using 3D convolution [29, 5, 4]. Very recently,\nattention mechanisms are adopted to further promote both\nrealism and temporal consistency via capturing long-range\n1\narXiv:2104.06637v1  [cs.CV]  14 Apr 2021\ncorrespondences. Temporally-consistent appearance is im-\nplicitly learned and propagated to the target frame via either\nframe-level attention [23] or patch-level attention [36].\nThese methods improve video inpainting performance\nto a great extent, yet still suffer from the following short-\ncomings: On the one hand, processing spatial-temporal in-\nformation propagation with self-attention modules is chal-\nlenging in complex scenarios, for example, the arbitrary ob-\nject movement (see Fig. 2). A camel is occluded for some\nframes and appears at some other frames, but the previ-\nous method [36] can not attend and propagate it precisely.\nOn the other hand, Transformer-based attention inevitably\nbrings huge computational cost, hindering its usage in real-\nworld applications.\nTo tackle the challenges, we propose the decoupled\nspatial-temporal Transformer (DSTT) framework for video\ninpainting with an interweaving stack of temporally-\ndecoupled and spatially-decoupled Transformer blocks.\nSpeciÔ¨Åcally, following STTN [36], we Ô¨Årst extract the\ndownsampled feature maps and embed this feature map\ninto many spatiotemporal token vectors with a convolution-\nbased encoder. Then the obtained spatiotemporal to-\nkens pass through a stack of Transformer blocks for thor-\nough spatial-temporal propagation. Different from the\nTransformer block in STTN that simultaneously processes\nspatial-temporal propagation, we decouple the spatial prop-\nagation and temporal propagation into 2 different Trans-\nformer blocks. In temporally-decoupled Transformer block,\nthe obtained tokens are spatially divided into multiple\nzones, and the model calculates the attention between to-\nkens from different frames‚Äô same zone, which is used to\nmodel temporal object movement. By contrast, in spatially-\ndecoupled Transformer block, the model calculates the at-\ntention between all tokens from the same frame. This is\ndesigned for modelling the stationary background texture\nin the same frame. These two different Transformer blocks\nare stacked in an interweaving manner such that all spatial-\ntemporal pixels can be interacted with each other, lead-\ning to thorough spatial-temporal searching and propagation.\nMeanwhile, this spatial-temporal operation becomes much\neasier after this decoupling scheme, resulting in better video\ninpainting performance, as shown in Fig. 2. Not only can\nthe occluded object be displayed, but also more coherent\ntexture can be synthesized.\nAlthough an effective and efÔ¨Åcient way is provided for\nthorough spatiotemporal propagation, learning representa-\ntive and robust feature representation before entering the\nTransformer blocks is still signiÔ¨Åcant. Because roughly sep-\narating image into many patches and squeezing them into\ntoken vectors like ViT [7] is not a good way to dealing with\nlow-level vision tasks due to the requirement of maintaining\nspatial structure. Therefore we also design a hierarchical\nCNN-based encoder for making the extracted feature maps\nFrame 20 Frame 20\nF5F5 F50 F50\nF65F35F35\nSTTN (inpainted frame 20) Proposed (inpainted frame 20)\nF65\nFigure 2. Illustration of the attention maps for missing regions\nlearned by STTN [36] and the proposed DSTT at the Ô¨Årst Trans-\nformer block. Red lines indicate the attention map of target hole\npatch on other reference frames. Orange lines indicate the scheme\nof temporal decoupling that splits frame into 4 zones. With such\nspatial splitting operation, the self-attention module calculates the\nattention score among tokens from the same zone of different\nframes, making this task easier. Therefore, our method can at-\ntend occluded object on other reference frames and propagate its\nappearance to Ô¨Åll the hole more precisely.\nmaintain multi-level local spatial structure. By mixing fea-\nture maps from various levels that possess different sizes of\nreceptive Ô¨Åelds, the characteristics of obtained feature maps\nvary from channel to channel, which is beneÔ¨Åcial to reach-\ning information interaction along spatial and temporal di-\nmension in following Transformer blocks.\nBy adopting such a pipeline, the proposed approach\nachieves state-of-the-art performance on video completion\nand object removal with much fewer FLOPs and achieves\nfaster inference speed (see Fig. 1). In summary:\n‚Ä¢ We propose a novel decoupled spatial-temporal Trans-\nformer (DSTT) framework for video inpainting to im-\nprove video inpainting quality with higher running ef-\nÔ¨Åciency, by disentangling the task of spatial-temporal\npropagation into 2 easier sub-tasks. A hierarchical en-\ncoder is also introduced for capturing robust and rep-\nresentative features. The seamless combination of de-\ncoupled spatial-temporal Transformer and hierarchical\nencoder beneÔ¨Åts video inpainting performance a lot.\n‚Ä¢ Extensive experiments demonstrate that the proposed\nDSTT can outperform other state-of-the-art video in-\npainting approaches both qualitatively and quantita-\ntively, even with signiÔ¨Åcant boosted efÔ¨Åciency.\n2\n2. Related Work\n2.1. Conventional Video Inpainting\nPrior to the prevalence of deep learning, conventional\nvideo inpainting approaches usually extend from patch-\nbased image inpainting methods [2, 3, 6, 8, 9]. Sampling\npatches from known region to Ô¨Åll the holes was Ô¨Årstly pro-\nposed in [8] and developed to reach a milestone as a com-\nmercial product [1]. Patwardhan et al. extended image\ntexture synthesis [8] to temporal domain for video inpaint-\ning based on a greedy patch completion scheme [24, 25]\nbut these methods only work for stationary camera or con-\nstrained camera deformation while ignores the challenge of\ndynamic camera motion in most natural video sequences.\nFor solving this problem, Wexler et al. directly take 3-D\nspatiotemporal patches as processing units and cast this as\na global optimization problem by alternating patch search\nstep and color reconstruction step [31]. Newson et al. adapt\nPatchMatch algorithm [1] to video for further enhancing\nthe temporal dependencies and accelerating the process of\npatch matching [22]. Flow and color completion [27] in-\ntroduce a motion Ô¨Åeld estimation step for capturing pixel\nÔ¨Çow more accurately. Finally Huang et al. take all of\nsteps including patch search, color completion and motion\nÔ¨Åeld estimation into account simultaneously and perform\nan alternate optimization on these steps, reaching excellent\nperformance [12]. Although it works well in various set-\ntings, the huge computational complexity during optimiza-\ntion process impedes its wide application in real scenarios.\n2.2. Deep Video Inpainting\nDeep learning boosts image inpainting with excellent\nperformance and high efÔ¨Åciency [13, 21, 34, 35], which in-\nspires its transition to video modality. Deep learning-based\nvideo inpainting approaches usually fall into propagation-\nbased approaches and inference-based approaches.\nOn the one hand, propagation-based approaches with op-\ntical Ô¨Çow, represented by DFVI [33] and its edge-aware\nextension [10], use off-the-shelf deep learning-based Ô¨Çow\nextraction technique [14] for obtaining optical Ô¨Çow with\nhigher robustness and efÔ¨Åciency. Then they need to learn a\ndeep neural network to complete the obtained Ô¨Çows accord-\ning to the corrupting mask and Ô¨Ånally propagate appear-\nance from reference frames to the target frame for complet-\ning video sequences. However, these methods usually fail\nwhen faced with minor background movements in that they\ncan not Ô¨Ånd relevant texture among those reference frames.\nThere are other attempts for learning the video completion\nnetwork with Ô¨Çow or homography estimation module on-\nthe-Ô¨Çy, for performing pixel propagation at the intermedi-\nate neural network under the guidance of optical Ô¨Çow [37],\ndepth [20] or homography [18].\nOn the other hand, Wang et al. for the Ô¨Årst time com-\nbine 2D and 3D convolution in a fully convolutional neural\nnetwork for directly inferring missing contents in video se-\nquences [29]. VINet adopts a recurrent neural network to\naggregate temporal features from neighboring frames [15].\nChang et al. develop a learnable Gated Temporal Shift\nModule and improve it to 3D gated convolution with tem-\nporal PatchGAN for free-form video inpainting [5, 4]. Hu\net al. combine 3D convolution with a region proposal-based\nstrategy for reÔ¨Åning inpainting results [11].\nFor better modeling long-range correspondences from\ndistant frames in video sequences, OPN [23] proposes an\nasymmetric attention for calculating the similarities be-\ntween hole regions on target frame and valid regions on\nreference frames. Similarly an attention-based dynamic\nlong-term context aggregation module is proposed for glob-\nally reÔ¨Åning feature maps in [19]. Going a further step,\nSTTN [36] directly transfer the overall multi-head Trans-\nformer architecture [28] into this task and propose to learn\na deep generative model along spatial-temporal dimension.\nHowever, it brings a huge computational cost on training\nand inference, causing the impractical usage in real-time\napplications. In this work, we propose a novel and effec-\ntive video inpainting framework endorsed by 2 carefully-\ndesigned modules, which boosts the efÔ¨Åciency signiÔ¨Åcantly.\n3. Method\nVideo inpainting targets at Ô¨Ålling a corrupted video se-\nquence with plausible contents and smooth pixel Ô¨Çows. Let\nXt = {X1,X2,...,X t}denote a corrupted video se-\nquence with length twhere Xi means the i-th frame of this\nvideo sequence. We aim to learn a mapping function that\ntakes the corrupted video Xt as input and outputs a realis-\ntic video Yt. However, there is usually no ground truth Yt\nprovided for training in real scenarios. We therefore formu-\nlate this problem as a self-supervised learning framework\nby randomly corrupting an original videos with a randomly\ngenerated mask sequence Mt, i.e., Xt = Yt ‚äôMt where ‚äô\ndenotes element-wise multiplication. Mi is a binary mask\nwhose values are either 0 referring to the original regions or\n1 meaning the corrupted regions that need Ô¨Ålling. Our goal\nis thus to learn a mapping G for reconstructing the origi-\nnal videos, i.e., ÀÜYt = G(Xt,Mt), beneÔ¨Åted from which a\nhuge quantity of natural videos can be used for training our\nframework.\nThe overall framework of our proposed method is illus-\ntrated in Fig. 3. Given a corrupted video sequence with\nits corresponding mask sequence, individual frames are\nÔ¨Årst input into a frame-based hierarchical encoder GHE for\ndownsampling and embedding into spatial-temporal token\nvectors. With such hierarchical channel-mixing encoder,\nthe robust and representative feature maps are extracted\nfrom the input video clip. After that, we feed the obtained\ntokens into an interweaving stack of Decoupled Spatial-\n3\nHierarchical Encoder\nLevel 1 Level 2 Level 3 Level 4\nTemporally-decoupled\nTransformer block\nDecoder\nSpatially-decoupled\nTransformer block\n‚Ä¶\n√óùëµ\nFigure 3. The illustration of proposed framework which is composed of a hierarchical encoder and a stack of decoupled temporal-spatial\nTransformer blocks, for improving its effectiveness and efÔ¨Åciency. In hierarchical encoder, features of different levels are represented as\ndifferent colors. The higher the level of feature is, the darker its color is. With such hierarchical grouping processing, the local spatial\nstructures of difference levels are stored in the Ô¨Ånal robust and representative features.\nTemporal Transformer (DSTT)GDSTT for fully aggregating\ninformation across tokens of different frames. At last, these\nprocessed tokens are transformed by a frame-based CNN\ndecoder GDec and synthesize the Ô¨Ånal frames, composing\nthe inpainted video. The overall generator G can thus be\nformulated as\nG= GHE ‚ó¶GDSTT ‚ó¶GDec.\n3.1. Hierarchical Encoder\nAs brieÔ¨Çy introduced above, our proposed model in-\ncludes a stack of speciÔ¨Åcally-designed Transformer blocks\nfor propagating spatial-temporal information from the\nknown regions outside the hole region at each frame. How-\never, roughly separating image into many patches and\nsqueezing them into token vectors like ViT [7] is not\na good way to dealing with low-level vision tasks due\nto the requirement of maintaining spatial structure. Al-\nthough the patch-level self-attention provides a general spa-\ntial structure based on the relative position between dif-\nferent patches, the local spatial structure in each patch is\ncorrupted, which is unfavorable for image/video synthesis.\nTherefore, a smart and elaborate encoder is needed for ex-\ntracting robust and representative token vectors.\nTo achieve this, we propose a novel hierarchical en-\ncoder (HE) that mixes the feature maps from different lev-\nels frame by frame, to produce tokens containing rich in-\nformation for each frame. SpeciÔ¨Åcally, given an input im-\nage Xi ‚ààRh√ów√ó3, i = 1,...,t , four cascaded convolu-\ntions with kernel size 3,3,3,3 and stride 2,1,2,1 make it\na down-sampled feature map Fi\n1 ‚ààRh/4√ów/4√óc. Starting\nfrom this feature, we establish our hierarchical structure for\nmulti-level channel mixing, as shown in Fig. 3.\nWe name the feature map Fi\n1 as the Ô¨Årst-level feature\nmap for clearly illustrating our hierarchical structure. It Ô¨Årst\npasses through a convolution layer with kernel size 3 √ó3\nto obtain the second-level feature map ÀÜFi\n2 ‚ààRh/4√ów/4√óc\nwhose shape is consistent with Ô¨Årst-level feature map while\npossessing a larger receptive Ô¨Åeld, i.e., a larger local spatial\nstructure. It is then concatenated with the Ô¨Årst-level one,\nhaving the feature map Fi\n2 ‚àà Rh/4√ów/4√ó2c that include\nboth Ô¨Årst-level and second-level local spatial structure.\nThen at each layer of our hierarchical encoder, all\ndifferent-level features will go into next-level, with grow-\ning larger receptive Ô¨Åeld and a Ô¨Årst-level feature is always\nconcatenated to the hierarchical feature map. This process\nis formulated as\nÀÜFi\nj+1 = ReLU(Conv(Fi\nj )), ÀÜFi\nj+1 ‚ààRh/4√ów/4√óc\nFi\nj+1 = Concat( ÀÜFi\nj+1,Fi\n1),j = 1,...,L ‚àí1,\n(1)\nwhere Conv denotes a regular convolution operation with\nkernel size 3 √ó3, ReLU denotes the RectiÔ¨Åed Linear Unit\nfor modelling non-linear transformation, Concat denotes\nthe concatenation along channel dimension and Ldenotes\nthe total number of hierarchical layers. We empirically set\nL = 4for our Ô¨Ånal model and the related ablation study is\nprovided in Section 4.3.\nWith this processing, the Ô¨Ånally obtained feature map\nwill include multi-level (from Ô¨Årst-level to L-th level) local\nspatial structures of the input image, thus leading to robust\nand representative feature maps, which is beneÔ¨Åcial to the\nfollow-up general patch-level spatial-temporal aggregation.\nTo further maintain the original local spatial structure, we\nadopt a group convolution at each layer to separately pro-\ncess feature reÔ¨Ånement for various levels. Since the highest-\nlevel feature always has least channels, we correspondingly\nset the number of groups atj-th layer to 2j‚àí1, which avoids\nfeature fusion in the intermediate layers and hence the orig-\ninal local spatial structure at all different levels brings into\nthe last output feature.\nAfter Ltimes recursion, the processed feature would be\nembedded with a convolution layer with kernel size 7 √ó7\nand stride 3 √ó3 to obtain the Ô¨Ånal output token feature rep-\nresentation Fi ‚ààRh/12√ów/12√ó2c,i = 1,...,t .\n4\nMulti-Head \nAttention\nframe 3 tokens2,11,21,1\nframe 2 tokens2,11,21,1\nMulti-Head \nAttentionMulti-Head \nAttention\nEmbedded patch \ntokens of frame 3Embedded patch \ntokens of frame 2\nNorm\nMulti-Head Attention\n+\nEmbedded tokens of \nframes\nFeed Forward\n+\nTransformer Block\nNorm\nSpatial Decouple\nSpatial/temporal \ndecouple\nEmbedded patch \ntokens of frame 1\nTemporal Decouple\nframe 1 tokens2,11,21,1\nMulti-Head \nAttention\nMulti-Head \nAttention\nMulti-Head \nAttention\nFigure 4. The illustration of a block of the proposed Decoupled\nSpatial Temporal Transformer (DSTT). A Transformer block con-\nsists a decoupled spatial/temporal MSA and a vanilla feed forward\nnet.\n3.2. Decoupled Spatial Temporal Transformer\nDifferent from STTN [36] that simultaneously processes\nspatial and temporal propagation, we disentangle this chal-\nlenging task into 2 easier tasks: a temporally-decoupled\nTransformer block for searching and propagating features\nfrom smaller spatial zone along temporal dimension and\na spatially-decoupled Transformer block for searching and\npropagating features from the whole spatial zone without\nother frames. Two such blocks are stacked for reaching ar-\nbitrary pixels across all the spatial-temporal positions.\nSpeciÔ¨Åcally, given the token feature Fi, we split it into\ns2 zones along both the height and width dimension where\neach zone of Fi is denoted as Fi\njk ‚ààRh/(12¬∑s)√ów/(12¬∑s)√ó2c\nwhere j,k = 1,...,s . So far we have a total number of\nt¬∑s2 ¬∑n tokens where n = h/(12 ¬∑s) ¬∑w/(12 ¬∑s). We\nthen take 2 different ways to grouping these tokens together.\nOne way is to group them together along temporal dimen-\nsion, i.e., Pjk = {F1\njk,...,F t\njk},j,k = 1,...,s so that\ntemporally-decoupled Transformer block takes each Pjk as\ninput and performs multi-head self-attention across tokens\nin it. By doing so, continuous movement of complex ob-\nject in a small spatial zone can be detected and propagated\ninto target hole regions. Another way is to gather them\nalong spatial dimension Pi = {Fi\n11,Fi\n12,...,F i\nss},i =\n1,...,t so that spatially-decoupled Transformer block takes\neach Pi as input and performs multi-head self-attention\nacross tokens in it. This helps the model detect similar\nbackground textures in the spatially-neighboring pixels and\npropagate these textures to target hole regions to achieve\ncoherent completion. Two different ways are illustrated\nclearly as in Fig. 4. Both of temporally-decoupled and\nspatially-decoupled self-attention modules are followed by\na vanilla feed forward network, composing two different\nTransformer blocks, which is formulated as\nÀÜPjk = FFN(MSA(Pjk) +Pjk) +Pjk, (2)\nÀÜPt = FFN(MSA(Pt) +Pt) +Pt, (3)\nwhere the former equation stands for the temporally-\ndecoupled Transformer block and the latter stands for the\nspatially-decoupled Transformer block. FFN denotes the\nvanilla feed forward net and MSA denotes multi-head self-\nattention module. Note that the mechanism of temporally-\ndecoupled MSA and spatially-decoupled MSA are same,\nonly their inputs vary in different grouping ways and thus\ndifferent searching zones.\nBy doing so, we signiÔ¨Åcantly reduce the computational\ncomplexity of MSA from O(t2s4n2c) into O(t2s2n2c)\n(temporally-decoupled MSA) and O(ts4n2c) (spatially-\ndecoupled MSA). Although t = 5 is chosen during train-\ning, using a larger t during inference will produce videos\nwith much better visual quality and temporal consistency.\nSo the inference speed is boosted to a great extent. Fol-\nlowing STTN [36], we set the number of all stacked Trans-\nformer blocks to 8. As for the stacking strategy, we em-\npirically make a temporally-decoupled block followed by a\nspatially-decoupled block and repeat this for 4 times. The\nrelated ablation study is conducted in Section 4.3.\n3.3. Optimization Objective\nAs outlined above, we end-to-end train our DSTT frame-\nwork in a self-supervised manner. After passing through\nthe generator Gintroduced above, we abtain the inpainted\nvideo sequence ÀÜYt = G(Xt,Mt).\nFor constraining the synthesized video ÀÜYt to recover the\noriginal video Yt, we Ô¨Årstly choose a simple pixel-level re-\nconstruction loss L1 between them as our supervision. Note\nthat the L1 for valid region and hole region may be of dis-\ntinct importance so we calculate them separately. The re-\nconstruction loss for hole region is formulated as:\nLhole = ‚à•Mt ‚äô( ÀÜYt ‚àíYt)‚à•1\n‚à•Mt‚à•1\n(4)\nand the reconstruction loss for valid region is formulated as:\nLvalid = ‚à•(1 ‚àíMt) ‚äô( ÀÜYt ‚àíYt)‚à•1\n‚à•1 ‚àíMt‚à•1\n(5)\nwhere ‚äôdenotes element-wise multiplication.\nIn addition, to enhance the realism and temporal co-\nherence of the generated video, we adopt a similar struc-\nture of Temporal Patch Generative Adversarial Network (T-\nPatchGAN) [4] as our discriminator D, for training with\n5\nProposedSTTNDFVICAPMasked frame\nFigure 5. Qualitative comparison with other methods on video completion and object removal.\nTable 1. User study on different methods.\nRank 1 Rank 2 Rank 3\nOurs 47.30% 43.22% 14.44%\nCAP [18] 25.25% 14.68% 63.97%\nSTTN [36] 27.45% 42.10% 21.59%\nthe generator in an adversarial manner. The discrimina-\ntor attempts to distinguish the generated videos from real\nones, while the generator attempts to synthesize a video that\nwould be categorized into ‚Äúreal‚Äù by the discriminatorD. So\nthe adversarial loss for Dis as:\nLD = EYt\n[\nlog D(Yt)\n]\n+ EÀÜYt\n[\nlog (1‚àíD( ÀÜYt))\n]\n(6)\nContrarily the adversarial loss for Gis the opposite di-\nrection:\nLadv = EÀÜYt\n[\nlog D( ÀÜYt)\n]\n(7)\nTo sum up, our Ô¨Ånal optimization objective for Gis:\nL= Œªhole ¬∑Lhole + Œªvalid ¬∑Lvalid + Œªadv ¬∑Ladv (8)\nwhere Œªhole, Œªvalid and Œªadv are hyper-parameters weighing\nimportance for different terms. We empirically set Œªhole to\n1, Œªvalid to 1 and Œªadv to 0.01 for following experiments.\n4. Experiment\n4.1. Implementation\nTraining details. We use Adam optimizer [16] with Œ≤1 =\n0.9, Œ≤2 = 0.999 and its learning rate starts from 0.0001\nand is decayed once with the factor 0.1 at 400,000 itera-\ntion. The total training iteration is 500,000. We use 8 GTX\n1080Ti GPUs for training and set batch size to 8 as well.\nThe total training time is around 50 hours.\nDataset. Two popular video object segmentation datasets\nare taken for evaluation. YouTube-VOS[32] is composed of\n3,471, 474 and 508 video clips in training, validation and\ntest set, respectively. The average number of frames for a\nvideo clip is around 150. DAVIS [26], short for Densely\nAnnotated VIdeo Segmentation, contains 150 video clips\nwith densely annotated object mask sequence. We split this\ndataset into a training set including 90 video clips and a test\nset including 60 video clips randomly.\nCompared methods. We mainly choose deep video in-\npainting approaches for comparison. VINet [15] uses a re-\ncurrent neural network to aggregate temporal features for\nthe hole video clip. DFVI[33] propagate relevant appear-\nance from reference frames to target one with completed\noptical Ô¨Çow. CAP [18] performs propagation by a learned\nhomography-based alignment module. LGTSM [5] adopts\na learnable temporal shift module with a temporal discrim-\ninator for completing videos.\nEvaluation metrics. Structural SIMilarity (SSIM) and Peak\nSignal to Noise Ratio (PSNR) are chosen as our metrics for\nevaluating the quality of reconstructed images compared\nto original ones. Higher value of these two metrics indi-\ncates better reconstruction quality. An optical Ô¨Çow-based\nwarping error Ewarp is chosen for measuring the tempo-\nral consistency [17]. Video-based Fr ¬¥echet Inception Dis-\ntance ( VFID) is adopted for measuring the similarity be-\ntween the statistics of synthesized videos and that of real\nvideos [30, 36]. Lower value suggests perceptually closer\nto natural videos. In addition, we count the FLOPs and\n6\nTable 2. Quantitative results of video completion on YouTube-VOS and DA VIS dataset. B is short for Billion.\nAccuracy EfÔ¨Åciency\nYouTube-VOS DA VIS FLOPs FPSModels PSNR ‚Üë SSIM ‚Üë VFID ‚Üì Ewarp ‚Üì PSNR ‚Üë SSIM ‚Üë VFID ‚Üì Ewarp ‚Üì\nVINet [15] 29.20 0.9434 0.072 0.1490 28.96 0.9411 0.199 0.1785 - -\nDFVI [33] 29.16 0.9429 0.066 0.1509 28.81 0.9404 0.187 0.1880 - -\nLGTSM [5] 29.74 0.9504 0.070 0.1859 28.57 0.9409 0.170 0.2566 261B 18.7\nCAP [18] 31.58 0.9607 0.071 0.1470 30.28 0.9521 0.182 0.1824 211B 15.0\nSTTN [36] 32.34 0.9655 0.053 0.1451 30.67 0.9560 0.149 0.1779 233B 24.3\nProposed 32.66 0.9646 0.052 0.1430 31.75 0.9650 0.148 0.1716 128B 37.3\n1 VINet and DFVI are not end-to-end training methods with complicated stages whose running speed highly rely on optical Ô¨Çow extraction, warpping, etc.,\nso their efÔ¨Åciency are not projectable with others, usually with longer time.\nParams, and record the frame per seconds (FPS) for com-\nparing model efÔ¨Åciency.\n4.2. Comparison with Other Methods\nWe perform experiments by comparing our framework\nwith other deep video inpainting approaches on video com-\npletion and object removal. For video completion task, we\nuse a sequence of stationary multiple square masks to cor-\nrupt the original video clips and wish the algorithm to re-\ncover the original videos totally. For object removal task,\nwe use off-the-shelf moving object segmentation mask se-\nquences to corrupt the corresponding videos and wish the\nalgorithm to synthesize perceptually natural videos with\nsharp and clear textures.\nQualitative comparison. As can be seen in Fig. 5, in video\ncompletion task, our framework could synthesize sharp and\nclear appearance with spatial consistency. In object removal\ntask, our framework could create realistic background tex-\ntures in invisible or occluded regions. Our model achieves\ncompetitive qualitative performance compared with state-\nof-the-arts on both tasks.\nQuantitative comparison. Since there is no ground-truth\nfor object removal task, the quantitative evaluation is only\nconducted on video completion task. The quantitative re-\nsults are summarized in Table 2, from which we observe\nthat our model obtains best quantitative results in all eval-\nuation metrics on DA VIS dataset. And our model ranks 1-\nst in PSNR, VFID and Ewarp and ranks 2-nd in SSIM on\nYouTube-VOS test set, yet with fewest FLOPs and highest\nFPS, showing the strong capability and high efÔ¨Åciency of\nour model in video inpainting.\nUser study. We further perform a user study to evaluate the\nvisual quality of our model, where CAP [18] and STTN [36]\nare chosen as strong baselines. From DAVIS [26] test set,\nwe randomly sample 15 videos for object removal tasks and\n15 videos for video completion. 40 volunteers participate\nin the user study. For each comparison, we present three\nvideos generated by different approaches and ask volunteers\nto rank based on visual quality, as summarized in Table 1.\nTable 3. Ablation study on the effect of hierarchical encoder.\nModels PSNR ‚Üë SSIM ‚Üë VFID ‚Üì\nL= 0 30.76 0.9578 0.185\nL= 1 30.94 0.9593 0.178\nL= 2 31.05 0.9599 0.175\nL= 3 31.09 0.9602 0.172\nL= 4(std) 31.15 0.9606 0.170\nL= 5 31.12 0.9604 0.171\nL= 6 31.16 0.9607 0.170\n4.3. Ablation Study\nWe investigate how each component in our framework\nimpacts the performance of video completion by ablation\nstudy. The quantitative metrics PSNR, SSIM and VFID at\n250,000 iteration are summarized in Table 3 to 5. We use\nstd, short for standard, to denote our released version of the\nproposed model, and we use TDB to denote temporally-\ndecoupled block and SDB to denote spatially-decoupled\nblock in following subsections if not speciÔ¨Åed.\nThe effect of TDB and SDB. We use an ablative compari-\nson by removing the intermediate spatially-decoupled and\ntemporally-decoupled Transformer blocks separately. As\nshown in Fig. 6, the synthesized results from SDB show\nbetter stationary texture by detecting and sampling the tex-\nture distribution from other pixels in the same frame, and\nthe results from TDB display better appearance of moving\nobject which are propagated from other frames (caused by\ncamera motion in this case). The combination of all SDB\nand TDB results in better performance and robustness.\nThe effect of hierarchical encoder. With a hierarchical\nchannel mixing strategy, it learns robust and representative\nfeature maps for follow-up spatial-temporal propagation.\nWe explore its effectiveness of different number of layers\non video completion by setting L = 0,..., 6. We Ô¨Årst set\nL = 0, which means that the original Ô¨Årst-level features\nFi\n1 ‚ààRh/4√ów/4√óc are directly embedded into multiple to-\nkens for entering the following Transformer blocks. As can\nbe seen from Table 3, its PSNR and SSIM drop by 1.25%\n7\nMasked frame Decoding from only SDB Decoding from only TDB Final output\nFigure 6. Ablative comparison. The second column shows the result decoded from features only passing through spatially-decoupled\nblocks (SDB). The third column shows the result decoded from features only passing through temporally-decoupled blocks (TDB). The\nrightmost column shows the result decoded from features passing through all Transformer blocks. Results from SDB perform better on\nsynthesizing stationary texture and results from TDB perform better on Ô¨Ålling the hole with occluded object. The emsembling result hence\nenjoys better quality and robustness.\nTable 4. Ablation study on the effect of the number of zones.\nAccuracy FPSModels PSNR ‚Üë SSIM ‚Üë VFID ‚Üì\n1(s= 1) 31.10 0.9599 0.172 31.5\n4(s= 2)(std) 31.15 0.9606 0.170 37.3\n9(s= 3) 31.05 0.9596 0.170 41.6\n16(s= 4) 30.92 0.9586 0.171 44.9\n25(s= 5) 30.82 0.9579 0.171 47.4\nand 0.29% respectively, and its VFID increases by 8.82%,\nwhich proves the importance of learning hierarchical fea-\nture representations.\nThen we set L = 1,2,3,5,6 that lower than the layers\nof standard model (L= 4) for investigating the effect of the\nnumber of layers adopted in hierarchical encoder on video\ninpainting performance. We observe that as the number of\nlayers grows from 1 to 3, both PSNR and SSIM increase\nstep by step, and VFID decreases gradually as well, sug-\ngesting the effectiveness of our hierarchical design. As the\nnumber of layers grows, the aggregated information is more\ncomprehensive from various local spatial structures. How-\never, when it is greater than5, the performance does not get\nbetter, which indicates that the improvement of such hierar-\nchical scheme is not unlimited under this conÔ¨Åguration. By\nconsidering both efÔ¨Åciency and accuracy, we choose L= 4\nas our released version for comparing with state-of-the-arts,\nas demonstrated in Section 4.1.\nThe effect of the number of zones. As mentioned in Sec-\ntion 3.2, we need to split the each feature map intos2 zones\nalong the spatial dimension. We now set s = 1,..., 5\nwhere the larger s indicates a smaller zone area. As can\nbe seen from Table 4, the model setting s= 1that directly\nsearches all spatial-temporal tokens instead of split zones,\nperforms worse than standard model (s= 2) with PSNR and\nSSIM dropped by 0.16% and 0.07% and VFID increased by\nTable 5. Ablation study on the effect of different stacking strate-\ngies. t is short for temporally-decoupled block and s is short for\nspatially-decoupled block. xn stands for repeating n times. For in-\nstance, t1s1-x4 means that temporally-decoupled block is followed\nby a spatially-decoupled one, which repeats 4 times.\nModels PSNR ‚Üë SSIM ‚Üë VFID ‚Üì\nt1s1-x4 (std) 31.15 0.9606 0.170\nt2s2-x2 31.03 0.9596 0.172\nt4s4-x1 30.94 0.9591 0.173\ns1t1-x4 31.06 0.9603 0.173\ns2t2-x2 30.95 0.9592 0.177\ns4t4-x1 30.85 0.9583 0.181\n1.18%. This phenomenon veriÔ¨Åes our assumption that split-\nting zones can make the modelling of pixel movements eas-\nier. An easier task essentially results in better performance.\nIn addition, as the value ofsgrows from3 to 5, the inference\nspeed becomes faster and faster, but the performance drops\nstep by step. Considering both accuracy and efÔ¨Åciency, we\nÔ¨Ånally choose s= 2as our released version.\nThe effect of different stacking strategies. We investi-\ngate the different stacking ways for combining the TDB and\nSDB, of which the results are reported in Table 5.\nFirst, stacking two different blocks in an interweaving\nmanner is always the best option no matter starting from\nTDB or SDB. By comparing the results in row 1-3 of Ta-\nble 5, we conclude that in the models starting from TDB, the\ncloser the distance between two same blocks is, the model\nperforms worse. Same trend is also found in the models\nstarting from SDB by comparing the row 4-6 of Table 5.\nThis is because alternate searching along different dimen-\nsion directions makes the most Ô¨Çexible way of information\npropagation.\nThen we compare the row 1 and row 4 of Table 5, both\nof which are composed of one TDB followed by one SDB.\n8\nBut the model starting from SDB performs worse than the\nmodel starting from TDB. The PSNR and SSIM of s1t1-\nx4 drops by 0.29% and 0.03%, respectively and its VFID\nis increased by 1.76%, compared with our standard t1s1-\nx4 model. Similar phenomenon could be found in row 2\ncompared with row 5 and row 3 compared with row 6. We\nargue this is because starting from TBD can borrow feature\nfrom other frames at the Ô¨Årst Transformer block, which uti-\nlizes the internal characteristics of videos that pixel Ô¨Çowing\nis continuous as time grows, making a good starting point.\nHowever, the model starting from SDB can merely search-\ning other patches in its own frame at the Ô¨Årst layer, which\nmay not always Ô¨Ånd enough good texture, especially facing\nthe hard case of occlusion or complex background.\n5. Conclusion\nIn this work, we propose a novel DSTT framework for\neffective and efÔ¨Åcient video inpainting. This is achieved\nby our speciÔ¨Åcally-designed decoupled spatial-temporal\nTransformer with hierarchical encoder. Extensive experi-\nments verify that our method achieves state-of-the-art per-\nformance in video completion and object removal com-\npared with other video inpainting approaches, with signiÔ¨Å-\ncant boosted efÔ¨Åciency.\nReferences\n[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and\nDan B Goldman. PatchMatch: A randomized correspon-\ndence algorithm for structural image editing. ACM Trans-\nactions on Graphics (Proc. SIGGRAPH), 2009. 3\n[2] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and\nColoma Ballester. Image inpainting. In Proceedings of the\n27th Annual Conference on Computer Graphics and Inter-\nactive Techniques, 2000. 3\n[3] M. Bertalmio, L. Vese, G. Sapiro, and S. Osher. Simultane-\nous structure and texture image inpainting. IEEE Transac-\ntions on Image Processing, page 882C889, 2003. 3\n[4] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston\nHsu. Free-form video inpainting with 3d gated convolution\nand temporal patchgan. In Proceedings of the International\nConference on Computer Vision (ICCV), 2019. 1, 3, 5\n[5] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston\nHsu. Learnable gated temporal shift module for deep video\ninpainting. In BMVC, 2019. 1, 3, 6, 7\n[6] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B\nGoldman, and Pradeep Sen. Image Melding: Combining in-\nconsistent images using patch-based synthesis. ACM Trans-\nactions on Graphics (TOG) (Proceedings of SIGGRAPH\n2012), 2012. 3\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 2, 4\n[8] Alexei Efros and Thomas Leung. Texture synthesis by non-\nparametric sampling. In In International Conference on\nComputer Vision, 1999. 3\n[9] Alexei A. Efros and William T. Freeman. Image quilting\nfor texture synthesis and transfer. In Proceedings of SIG-\nGRAPH. 3\n[10] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf.\nFlow-edge guided video completion. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2020. 1,\n3\n[11] Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grau-\nman, and Alexander G. Schwing. Proposal-based video com-\npletion. In The Proceedings of the European Conference on\nComputer Vision (ECCV), 2020. 3\n[12] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo-\nhannes Kopf. Temporally coherent completion of dynamic\nvideo. ACM Trans. Graph., 2016. 3\n[13] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.\nGlobally and locally consistent image completion. ACM\nTransactions on Graphics (ToG), 2017. 1, 3\n[14] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and\nT. Brox. Flownet 2.0: Evolution of optical Ô¨Çow estimation\nwith deep networks. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), Jul 2017. 3\n[15] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So\nKweon. Deep video inpainting. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 2019. 3, 6, 7\n[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In 3rd International Conference on\nLearning Representations, ICLR, 2015. 6\n[17] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,\nErsin Yumer, and Ming-Hsuan Yang. Learning blind video\ntemporal consistency. In European Conference on Computer\nVision, 2018. 6\n[18] Sungho Lee, Seoung Wug Oh, DaeYeun Won, and Seon Joo\nKim. Copy-and-paste networks for deep video inpainting. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 2019. 1, 3, 6, 7\n[19] Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong,\nJianzhong Qi, Rui Zhang, Dacheng Tao, and Ramamoha-\nnarao Kotagiri. Short-term and long-term context aggrega-\ntion network for video inpainting. In ECCV, 2020. 3\n[20] Miao Liao, Feixiang Lu, Dingfu Zhou, Sibo Zhang, Wei Li,\nand Ruigang Yang. Dvi: Depth guided video inpainting for\nautonomous driving. In The Proceedings of the European\nConference on Computer Vision (ECCV), 2020. 3\n[21] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for\nirregular holes using partial convolutions. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\n2018. 3\n[22] Alasdair Newson, Andr ¬¥es Almansa, Matthieu Fradet, Yann\nGousseau, and Patrick P ¬¥erez. Video Inpainting of Complex\nScenes. SIAM Journal on Imaging Sciences, 2014. 3\n9\n[23] Seoung Wug Oh, Sungho Lee, Joon-Young Lee, and\nSeon Joo Kim. Onion-peel networks for deep video comple-\ntion. In Proceedings of the IEEE International Conference\non Computer Vision, 2019. 1, 2, 3\n[24] Kedar Patwardhan, G. Sapiro, and M. Bertalmio. Video in-\npainting of occluding and occluded objects. 2005. 3\n[25] K. A. Patwardhan, G. Sapiro, and M. Bertalmio. Video\ninpainting under constrained camera motion. Trans. Img.\nProc., 2007. 3\n[26] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.\nGross, and A. Sorkine-Hornung. A benchmark dataset and\nevaluation methodology for video object segmentation. In\nComputer Vision and Pattern Recognition, 2016. 6, 7\n[27] M. Strobel, Julia Diebold, and D. Cremers. Flow and color\ninpainting for video completion. In GCPR, 2014. 3\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Å ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998‚Äì6008, 2017. 3\n[29] Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang.\nVideo inpainting by jointly learning temporal structure and\nspatial details. In AAAI, 2019. 1, 3\n[30] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,\nAndrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-\nvideo synthesis. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2018. 6\n[31] Yonatan Wexler, Eli Shechtman, and Michal Irani. Space-\ntime completion of video. IEEE Trans. Pattern Anal. Mach.\nIntell., 2007. 3\n[32] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen\nLiang, Jianchao Yang, and Thomas Huang. Youtube-vos:\nA large-scale video object segmentation benchmark. arXiv:\n1809.03327, 2018. 6\n[33] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy.\nDeep Ô¨Çow-guided video inpainting. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 2019. 1, 3, 6, 7\n[34] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2018. 1, 3\n[35] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Free-form image inpainting with gated\nconvolution. In Proceedings of the IEEE International Con-\nference on Computer Vision, 2019. 1, 3\n[36] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting.\nIn The Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2020. 1, 2, 3, 5, 6, 7\n[37] Haotian Zhang, Long Mai, Ning Xu, Zhaowen Wang, John\nCollomosse, and Hailin Jin. An internal learning approach to\nvideo inpainting. In Proceedings of the IEEE International\nConference on Computer Vision, 2019. 3\n10"
}