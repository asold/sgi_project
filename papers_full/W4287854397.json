{
  "title": "DS4DH at SemEval-2022 Task 11: Multilingual Named Entity Recognition Using an Ensemble of Transformer-based Language Models",
  "url": "https://openalex.org/W4287854397",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5071643063",
      "name": "Hossein Rouhizadeh",
      "affiliations": [
        "University of Geneva"
      ]
    },
    {
      "id": "https://openalex.org/A5042070839",
      "name": "Douglas Teodoro",
      "affiliations": [
        "University of Geneva"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952638691",
    "https://openalex.org/W173870552",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W3101058639",
    "https://openalex.org/W2573492843",
    "https://openalex.org/W2098700435",
    "https://openalex.org/W3162486530",
    "https://openalex.org/W4294294857",
    "https://openalex.org/W3153383217",
    "https://openalex.org/W3217155045",
    "https://openalex.org/W3211456888",
    "https://openalex.org/W3179145078",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3101381894",
    "https://openalex.org/W2143345705",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4287854446",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3156534109",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W4287705547",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W2902687866"
  ],
  "abstract": "In this paper, we describe our proposed method for the SemEval 2022 Task 11: Multilingual Complex Named Entity Recognition (MultiCoNER). The goal of this task is to locate and classify named entities in unstructured short complex texts in 11 different languages.After training a variety of contextual language models on the NER dataset, we used an ensemble strategy based on a majority vote to finalize our model. We evaluated our proposed approach on the multilingual NER dataset at SemEval-2022. The ensemble model provided consistent improvements against the individual models on the multilingual track, achieving a macro F1 performance of 65.2%. However, our results were significantly outperformed by the top ranking systems, achieving thus a baseline performance.",
  "full_text": "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 1543 - 1548\nJuly 14-15, 2022 ©2022 Association for Computational Linguistics\nDS4DH at SemEval-2022 Task 11: Multilingual Named Entity Recognition\nUsing an Ensemble of Transformer-based Language Models\nHossein Rouhizadeh\nUniversity of Geneva, Switzerland\nhossein.rouhizadeh@unige.ch\nDouglas Teodoro\nUniversity of Geneva, Switzerland\ndouglas.teodoro@unige.ch\nAbstract\nIn this paper, we describe our proposed method\nfor the SemEval 2022 Task 11: Multilingual\nComplex Named Entity Recognition (Multi-\nCoNER). The goal of this task is to locate and\nclassify named entities in unstructured short\ncomplex texts in 11 different languages. After\ntraining a variety of contextual language mod-\nels on the NER dataset, we used an ensemble\nstrategy based on a majority vote to finalize our\nmodel. We evaluated our proposed approach\non the multilingual NER dataset at SemEval-\n2022. The ensemble model provided consistent\nimprovements against the individual models\non the multilingual track, achieving a macro\nF1 performance of 65.2%. However, our re-\nsults were significantly outperformed by the\ntop ranking systems, achieving thus a baseline\nperformance.\n1 Introduction\nNamed entity recognition (NER) is the process of\nidentifying pre-defined categories of named enti-\nties, such as people, places, organizations, from\nunstructured text. NER usually serves as an im-\nportant first component in various natural language\nprocessing (NLP) tasks, such as question answer-\ning (Mollá et al., 2006), information retrieval (Guo\net al., 2009) and machine translation (Babych and\nHartley, 2003). Thus, the performance of the NER\nsystem can influence the quality of many down-\nstream NLP applications. Despite the high perfor-\nmance achieved by the current NER systems, they\nstill face some critical challenges(Augenstein et al.,\n2017). NER models are typically trained on a well-\nformed news text containing a variety of entities\nwithin a relatively long context. In addition, most\nof the existing NER datasets usually include a large\nnumber of common entities between train set and\ntest set. As a result, the performance of the models\ndrops dramatically in the real world applications\nas they must deal with unseen entities and noisy\ntexts. Furthermore, previous studies on NER have\nmostly focused on English and as a result, many\nother languages specially low-resource ones, such\nas Turkish, Korean, and Persian, have not been\nas well studied (Rouhizadeh et al., 2021a,b). In\nthis context, SemEval-2022 proposes the task of\nMultilingual Complex Named Entity Recognition\n(MultiCoNER) (Malmasi et al., 2022b), which is\nconcerned with detecting semantically ambiguous\nand complex entities in short and low-contextual\nsettings for 11 languages (i.e. English, Spanish,\nDutch, Russian, Turkish, Korean, Farsi, German,\nChinese, Hindi, and Bangla). In this paper, we\npresent a multilingual NER method based on en-\nsemble of deep neural language models. We first\ntrained multiple NER models on the official train-\ning dataset and then utilized an ensemble strategy\nbased on a majority of votes from the top-3 best-\nperforming models. Based on the macro-average\nF1-score of 65.2, achieved by our model, we placed\n20th in the multilingual track of the competition.\nThe rest of the paper is organized as follows. Sec-\ntion 2 reviews published work related to the NER\ntask. Section 3 and section 4 explain our proposed\nNER system and the experimental setup respec-\ntively. The results and detailed analysis of the\nmodel performance are discussed in section 5 and\nthe conclusion and future work are reported in sec-\ntion 6.\n2 Related Work\nOver the last decade, deep learning approaches\nhave significantly improved the results of different\nNER tasks (Baevski et al., 2019; Akbik et al., 2018).\nThe most recent works on NER utilize pre-trained\nlanguage models like BERT in a supervised setting\n(Yamada et al., 2020; Wang et al., 2020; Schnei-\nder et al., 2020; Shaffer, 2021). These models use\npre-trained language models that have been trained\non a large monolingual or multilingual corpus to\nfine-tune NER models. Meng et al. (2021) intro-\n1543\nduced a number of current challenges of developed\nNER datasets and systems. The challenges include\nthe presence of long-tail entities, i.e., entities with\nlarge distribution and millions of values, emerg-\ning entities, i.e., domains with growing entities, or\ncomplex entities, i.e., linguistically complex enti-\nties such as gerunds and full clauses, in the context\nof the systems’ inputs. In addition, as discussed in\nJayarao et al. (2018) the context of search queries\nand questions usually include a short amount of\nwords which could be problematic for NER sys-\ntems. To overcome the above issues, Meng et al.\n(2021) created three new NER datasets, includ-\ning short sentences, questions, and search queries,\nand a novel NER system which uses a contextual\ngazetteer representation (CGR) encoder and a mix-\nture of experts (MoE) gating network to feed a CRF\nlayer for final predictions. Fetahu et al. (2021) also\ntackled the challenge of the code-mixed queries\nin which entities and non-entity query terms co-\nexist simultaneously. They developed a large-scale\nNER dataset in six languages with four different\nscripts as well as a novel multi-lingual NER method\nfor code-mixed queries which integrates external\nknowledge into the multilingual setting.\n3 Method\nOur multilingual NER system takes sentences in\n11 different languages and automatically identifies\nand classifies named entities within each sentence.\nFor each sentence, the system utilizes three differ-\nent BERT-like models (fined-tuned on the multi-\nlingual NER dataset) to perform entity prediction\nindependently. Next, for each entity, the label with\nthe majority of votes will be chosen as the final\nprediction. In the following, we provide details\non different NER models we used in our pipeline\nand our ensemble strategy for label prediction in\nsection 3.1 and section 3.2, respectively.\n3.1 Training NER Models\nTo build our NER model, we first fine-tune a\nnumber of pre-trained multilingual transformer-\nbased models, i.e., Multilingual-BERT (Pires et al.,\n2019), XLM-RoBERTa-base, XLM-RoBERTa-\nLarge (Conneau et al., 2019) and Distilbert-\nMultilingual (Sanh et al., 2019), on the official\ntraining dataset (see section 4.1 for more details\nabout the dataset). We fine-tune each particular\nmodel by adding (1): a fully connected neural net-\nwork (FCNN) layer or (2): a conditional random\nfields (CRF) layer (Lafferty et al., 2001) on the\ntop of the transformer architecture. Transformer-\nbased models usually use the byte-pair encoding\nfor the tokenization. In other words, each token\nmight be divided into more than one sub-token. To\ndeal with this, during training, among the subto-\nkens labels of a given word, the label of the fisrt\nsub-token has been considered as the label of the\nword. We also use the BERT-like models to train\na simple BiLSTM model with an additional linear\nclassifier on the dataset 1 Following Reimers and\nGurevych (2019), we calculate the vector represen-\ntation for each context word by taking the average\nof the layer output embeddings of the pre-trained\nlanguage model and feed them to a BiLSTM neural\nnetwork as input2.\nAs the next step, we select three of the best-\nperforming NER models and use an ensemble strat-\negy (discussed in section 3.2) to finalize our model.\n3.2 Ensemble of the NER Models\nHaving trained multiple NER models, we use an\nensemble strategy based on a majority vote to as-\nsign the predictions (Copara et al., 2020b,a; Knafou\net al., 2020; Naderi et al., 2021). More in detail,\nfor a given sentence S, three NER models infer\ntheir predictions independently. Thus, we will have\nthree labeled instances of S associated with several\nentity labels. Next, for each identified entity, we\nchoose the label that gets the majority of votes (at\nleast two votes) as the final prediction. Note that as\nwe use three different NER models in our pipeline,\nthree different labels might be assigned to a given\nentity. In such cases, we choose the predicted label\nof the best-performing model (evaluated on the dev\nset) as the final prediction.\n4 Experimental Setup\nThis section discusses the dataset we used to con-\nduct our experiments, followed by the parameters\nwe used to train the models.\n4.1 Data\nOur experiments were conducted using the multilin-\ngual dataset provided by the SemEval-2022 Task 11\norganizers (Malmasi et al., 2022a). The dataset con-\nsists of entity annotated sentences from eleven dif-\n1We used the code provided by Adelani et al. (2021) to\nperform BiLSTM experiments.\n2We only report the results when we feed the BiLSTM\nwith XLM-RoBERTa-large as it performed best compared to\nthe other models\n1544\nEntity Train Dev Test\nPerson 35091 18.4% 8862 18.6% 2342 18.7%\nLocation 43052 22.6% 10978 23.1% 2932 23.4%\nGroup 26373 13.8% 6473 13.6% 1638 13.0%\nCreative Work 30817 16.2% 7556 15.9% 2015 16.1%\nProduction 28170 14.8% 6949 14.6% 1848 14.7%\nCorporation 26315 13.8% 6575 13.8% 1738 13.8%\nAll 189818 100% 47393 100% 12513 100%\nTable 1: General statistics of the dataset including the number and the distribution of each entity.\nEntity / Model m-BERT XLM-RoBERTa-baseXLM-RoBERTa-largem-DistillBERTBiLSTM Ensemble\nPerson 69.2 | 70.8 88.8 | 89.2 90.1 |90.8 83.0 | 82.1 74.3 91.3\nLocation 69.4 | 69.9 86.9 | 87.6 88.0 | 89.3 83.0 | 79.9 75.7 89.9\nGroup 60.7 | 71.1 80.3 | 81.7 84.2 | 85.5 74.0 | 73.4 61.3 86.2\nCreative Work58.3 | 59.1 75.0 | 77.4 80.7 |82.3 67.0 | 73.2 51.1 81.7\nProduction 55.0 | 56.6 74.8 | 76.1 79.6 | 80.6 67.0 | 63.5 54.6 80.6\nCorporation 69.1 | 69.4 82.7 | 83.9 85.5 | 87.1 76.0 | 75.2 61.5 88.1\nAll 63.8 | 64.9 82.5 | 84.0 84.7 | 85.8 75.7 | 75.2 64.2 86.3\nTable 2: The F1 performance of different multilingual NER models. Each cell include the results when we used a\nFFCN (the number of the left side) or a CRF layer (the number of the right side) in the model.\nferent languages: English, Spanish, Dutch, Russian,\nTurkish, Korean, Farsi, German, Chinese, Hindi,\nand Bangla. The six entity types of the dataset are\nPerson, Location, Production, Corporation, Group,\nand Creative Work. The organizers provided the\ncompetitors with NER-tagged training and devel-\nopment sets, and then released an unlabeled test\nset for the final prediction. To fine-tune our hyper-\nparameters and evaluate our models in the devel-\nopment phase, we divided the training set into two\nparts - 0.80% for the train set and 0.20% for the\ndev set - and used the official dev set (i.e., provided\nby the organizers) to test the models and analyse\nour model results as the labels of the official test\nset are not released. The number and distribution\nof occurrences of each entity in the training (train\nand dev) and test (official dev) datasets are reported\nin Table 1, where we can notice a relatively good\nclass distribution among the training examples.\n4.2 Parameters\nIn our experiments, we fine-tuned different multi-\nlingual pre-trained language models including bert-\nbase-multilingual-uncased, XLM-Roberta-base,\nXLM-Roberta-large, distilbert-base-multilingual-\ncased, and also trained a simple BiLSTM model on\nthe dataset. We trained each particular model for\n6 epochs using Adam optimizer (Kingma and Ba,\n2014), a batch size of 16, the learning rate of 2e-5,\nand the maximum sequence length of 256 tokens.\nWe computed the F1 performance of the model on\neach epoch and finally saved the parameters of the\nepoch with the best performance to perform NER\non the test set.\n5 Results and Discussion\n5.1 Results\nIn Table 2, we show the macro-averaged F1 perfor-\nmance of the NER models on the different entities\nof the unofficial test dataset. We use the three\nbest performing models identified in the dev set,\ni.e., XLM-RoBERTa-large + CRF, XLM-RoBERTa-\nbase + CRF and XLM-RoBERTa-large + FCNN,\nto create our ensemble strategy. As shown in Ta-\nble 2, the ensemble model outperforms the other\nsingle transformer-based models, improving the\nF1-score of the top-performer models by around\n1% point. The results also indicate that the models\nfine-tuned on the XLM-RoBERTa (both large and\nbase) outperform the other models by a wide mar-\ngin. In addition, a comparison between the results\nof each particular model with and without CRF on\nthe test set shows that adding a CRF layer to the\nmodels could be helpful as it improves the model\nperformance in most cases. The results show that\nall models perform best in inferring Person and\nLocation entities. This can be due to the large\nnumber of instances of both entities in the training\nset. In Table 1, it is shown that the number of oc-\n1545\nSentence Length 1 ≤ N ≤ 5 6 ≤ N ≤ 10 11 ≤ N ≤ 15 16 ≤ N ≤ 20 N >20 All\nNumber of Sentences 85 1988 2517 1964 2246 8800\nRatio of the sentences 0.1% 22.5% 28.6% 22.3% 25.5% 100%\nTable 3: Number and ratio of sentences with different length (in words) in the test set.\nFigure 1: Performance of our ensemble model according\nto the sentence length (in words).\ncurrences of these entities in the dataset is greater\nthan the other ones. The BiLSTM model also per-\nforms significantly worse than the fine-tuned XLM-\nRoBERTa-large models, despite using the same\nword vectors.\n5.2 Discussion\nEffect of the context lengthOne of the most im-\nportant factors affecting the performance of the\nNER systems is the context length (Meng et al.,\n2021). To analyze the effect of the input context\non our NER system, we divided the (unofficial)\ntest set into 5 different groups: (1): sentences with\nfive or fewer words, (2): sentences with a context\nlength of at least 6 and less than 11, (3) sentences\nincluding at least 10 and less than 15 context words,\n(4) sentences containing between 15 and 20 words,\nand (5) sentences containing more than 20 context\nwords. The number and ratio of sentences in each\ngroup is reported in Table 3. Figure 1 shows the\nperformance of the ensemble NER model on the\ndifferent groups of sentences. As it can be seen,\nthe model has the worse performance when the sen-\ntences contain 5 or less words. Surprisingly, the\nmodel performs best in the second group (sentences\ncontaining between 5 and 10 words) showing the\nstrength of the model even in the short the sen-\ntences.\n6 Conclusion\nIn this paper, we presented our multilingual NER\nmethod that uses an ensemble of different fine-\ntuned models to identify the named entities in the\nunstructured texts. Using a variety of multilingual\npre-trained language models, we first fine-tuned\nseveral NER models and then applied a vote-based\nensemble strategy to make the final prediction. Our\nsubmission achieved an overall F1 score of 65.2,\nranking 20th in the multilingual track of task 11 of\nSemEval-2022. Our next step would be to examine\nother possible types of ensemble strategies as it\nhas shown to be effective in the performance of the\nNER models.\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine Lig-\nnos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Is-\nrael Abebe Azime, Shamsuddeen H. Muhammad,\nChris Chinenye Emezue, Joyce Nakatumba-Nabende,\nPerez Ogayo, Aremu Anuoluwapo, Catherine Gitau,\nDerguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-\nmam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,\nRubungo Andre Niyongabo, Jonathan Mukiibi, Ver-\nrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,\nGerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane MBOUP, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021. MasakhaNER: Named Entity\nRecognition for African Languages. Transactions\nof the Association for Computational Linguistics ,\n9:1116–1131.\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018.\n1546\nContextual string embeddings for sequence labeling.\nIn Proceedings of the 27th international conference\non computational linguistics, pages 1638–1649.\nIsabelle Augenstein, Leon Derczynski, and Kalina\nBontcheva. 2017. Generalisation in named entity\nrecognition: A quantitative analysis. Computer\nSpeech & Language, 44:61–83.\nBogdan Babych and Anthony Hartley. 2003. Improving\nmachine translation quality with automatic named\nentity recognition. In Proceedings of the 7th Interna-\ntional EAMT workshop on MT and other language\ntechnology tools, Improving MT through other lan-\nguage technology tools, Resource and tools for build-\ning MT at EACL 2003.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. arXiv preprint\narXiv:1903.07785.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nJenny Copara, Julien Knafou, Nona Naderi, Claudia\nMoro, Patrick Ruch, and Douglas Teodoro. 2020a.\nContextualized french language models for biomed-\nical named entity recognition. In Actes de la 6e\nconférence conjointe Journées d’Études sur la Pa-\nrole (JEP , 33e édition), Traitement Automatique des\nLangues Naturelles (TALN, 27e édition), Rencontre\ndes Étudiants Chercheurs en Informatique pour le\nTraitement Automatique des Langues (RÉCITAL, 22e\nédition). Atelier DÉfi Fouille de Textes, pages 36–48.\nJenny Copara, Nona Naderi, Julien Knafou, Patrick\nRuch, and Douglas Teodoro. 2020b. Named en-\ntity recognition in chemical patents using ensem-\nble of contextual language models. arXiv preprint\narXiv:2007.12569.\nBesnik Fetahu, Anjie Fang, Oleg Rokhlenko, and\nShervin Malmasi. 2021. Gazetteer Enhanced Named\nEntity Recognition for Code-Mixed Web Queries. In\nProceedings of the 44th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 1677–1681.\nJiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.\nNamed entity recognition in query. In Proceedings\nof the 32nd international ACM SIGIR conference on\nResearch and development in information retrieval,\npages 267–274.\nPratik Jayarao, Chirag Jain, and Aman Srivastava. 2018.\nExploring the importance of context and embeddings\nin neural ner models for task-oriented dialogue sys-\ntems. arXiv preprint arXiv:1812.02370.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJulien Knafou, Nona Naderi, Jenny Copara, Douglas\nTeodoro, and Patrick Ruch. 2020. Bitem at wnut\n2020 shared task-1: Named entity recognition over\nwet lab protocols using an ensemble of contextual\nlanguage models. In Proceedings of the Sixth Work-\nshop on Noisy User-generated Text (W-NUT 2020),\npages 305–313.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random fields: Proba-\nbilistic models for segmenting and labeling sequence\ndata.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022a. MultiCoNER: a\nLarge-scale Multilingual dataset for Complex Named\nEntity Recognition.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022b. SemEval-2022\nTask 11: Multilingual Complex Named Entity Recog-\nnition (MultiCoNER). In Proceedings of the 16th\nInternational Workshop on Semantic Evaluation\n(SemEval-2022). Association for Computational Lin-\nguistics.\nTao Meng, Anjie Fang, Oleg Rokhlenko, and Shervin\nMalmasi. 2021. GEMNET: Effective gated gazetteer\nrepresentations for recognizing complex entities in\nlow-context input. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1499–1512.\nDiego Mollá, Menno Van Zaanen, and Daniel Smith.\n2006. Named entity recognition for question answer-\ning. In Proceedings of the Australasian Language\nTechnology Workshop 2006, pages 51–58.\nNona Naderi, Julien Knafou, Jenny Copara, Patrick\nRuch, and Douglas Teodoro. 2021. Ensemble of\ndeep masked language models for effective named\nentity recognition in health and life science corpora.\nFrontiers in research metrics and analytics, 6.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nHossein Rouhizadeh, Mehrnoush Shamsfard, Mahdi\nDehghan, and Masoud Rouhizadeh. 2021a. Persian\nsemcor: A bag of word sense annotated corpus for\nthe persian language. In Proceedings of the 11th\nGlobal Wordnet Conference, pages 147–156.\n1547\nHossein Rouhizadeh, Mehrnoush Shamsfard, Vahideh\nTajalli, and Masoud Rouhziadeh. 2021b. Persian-\nwsd-corpus: A sense annotated corpus for persian\nall-words word sense disambiguation. arXiv preprint\narXiv:2107.01540.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nElisa Terumi Rubel Schneider, Joao Vitor Andrioli\nde Souza, Julien Knafou, Lucas Emanuel Silva\ne Oliveira, Jenny Copara, Yohan Bonescki Gumiel,\nLucas Ferro Antunes de Oliveira, Emerson Cabr-\nera Paraiso, Douglas Teodoro, and Cláudia Maria\nCabral Moro Barra. 2020. Biobertpt-a portuguese\nneural language model for clinical named entity\nrecognition. In Proceedings of the 3rd Clinical Natu-\nral Language Processing Workshop, pages 65–72.\nKyle Shaffer. 2021. Language clustering for multilin-\ngual named entity recognition. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 40–45.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Guihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowledge\ninto pre-trained models with adapters. arXiv preprint\narXiv:2002.01808.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. Luke: deep con-\ntextualized entity representations with entity-aware\nself-attention. arXiv preprint arXiv:2010.01057.\n1548",
  "topic": "SemEval",
  "concepts": [
    {
      "name": "SemEval",
      "score": 0.9237902760505676
    },
    {
      "name": "Computer science",
      "score": 0.8954758048057556
    },
    {
      "name": "Named-entity recognition",
      "score": 0.6960079669952393
    },
    {
      "name": "Transformer",
      "score": 0.6728002429008484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6703027486801147
    },
    {
      "name": "Natural language processing",
      "score": 0.6130985617637634
    },
    {
      "name": "Task (project management)",
      "score": 0.6074873805046082
    },
    {
      "name": "Language model",
      "score": 0.588097870349884
    },
    {
      "name": "Macro",
      "score": 0.5020174980163574
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4689706563949585
    },
    {
      "name": "Ensemble learning",
      "score": 0.44993236660957336
    },
    {
      "name": "Baseline (sea)",
      "score": 0.43428391218185425
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.41992995142936707
    },
    {
      "name": "Programming language",
      "score": 0.06861647963523865
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114457229",
      "name": "University of Geneva",
      "country": "CH"
    }
  ],
  "cited_by": 2
}