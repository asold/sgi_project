{
  "title": "A Survey of Reasoning with Foundation Models",
  "url": "https://openalex.org/W4389764468",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2162209787",
      "name": "Jiankai Sun",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3006520018",
      "name": "Chuanyang Zheng",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2530179367",
      "name": "Enze Xie",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2123417896",
      "name": "Liu Zhengying",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2977921174",
      "name": "Ruihang Chu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2104900781",
      "name": "Jiaqi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113817701",
      "name": "Jiaqi Xu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2144721405",
      "name": "Mingyu Ding",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2100066183",
      "name": "Hongyang Li",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A3022311428",
      "name": "Mengzhe Geng",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2096475003",
      "name": "Yue Wu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2133578637",
      "name": "Wenhai Wang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2110657799",
      "name": "Junsong Chen",
      "affiliations": [
        "Huawei Technologies (Sweden)",
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4377904255",
      "name": "Zhangyue Yin",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A3133213311",
      "name": "Xiaozhe Ren",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A1985012419",
      "name": "Jie Fu",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2161973342",
      "name": "Junxian He",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2133167520",
      "name": "Wu Yuan",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2088571978",
      "name": "Qi Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2101396447",
      "name": "Xi-Hui Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097755322",
      "name": "Yu Li",
      "affiliations": [
        "Chinese University of Hong Kong",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2014081204",
      "name": "Hao Dong",
      "affiliations": [
        "Peking University",
        "King University"
      ]
    },
    {
      "id": "https://openalex.org/A2125847941",
      "name": "Yu Cheng",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1994445760",
      "name": "Ming Zhang",
      "affiliations": [
        "Peking University",
        "King University"
      ]
    },
    {
      "id": "https://openalex.org/A2133318398",
      "name": "Pheng Ann Heng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097947275",
      "name": "Jifeng Dai",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1912778456",
      "name": "Ping Luo",
      "affiliations": [
        "University of Hong Kong",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2124874746",
      "name": "Jingdong Wang",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2114993276",
      "name": "Yike Guo",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1991795496",
      "name": "Hui Xiong",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2097139319",
      "name": "Zhenguo Li",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2594908799",
    "https://openalex.org/W6611102469",
    "https://openalex.org/W6657872126",
    "https://openalex.org/W2024580832",
    "https://openalex.org/W657430534",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W6605475740",
    "https://openalex.org/W6605214863",
    "https://openalex.org/W3175293340",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2038956038",
    "https://openalex.org/W4242635839",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W6780161852",
    "https://openalex.org/W2527693370",
    "https://openalex.org/W2148437670",
    "https://openalex.org/W6601096871",
    "https://openalex.org/W3101082165",
    "https://openalex.org/W3194951844",
    "https://openalex.org/W4252907012",
    "https://openalex.org/W2601748485",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W6603707631",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W3101118213",
    "https://openalex.org/W3105055324",
    "https://openalex.org/W3205783417",
    "https://openalex.org/W4296130438",
    "https://openalex.org/W4323306906",
    "https://openalex.org/W2478547901",
    "https://openalex.org/W4318610290",
    "https://openalex.org/W4206078757",
    "https://openalex.org/W6600214982",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W6816412036",
    "https://openalex.org/W6600336938",
    "https://openalex.org/W4285170409",
    "https://openalex.org/W4212807739",
    "https://openalex.org/W6604404400",
    "https://openalex.org/W6726221752",
    "https://openalex.org/W3099130708",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3174481949",
    "https://openalex.org/W4246949166",
    "https://openalex.org/W4255556797",
    "https://openalex.org/W2531152563",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385572217",
    "https://openalex.org/W6600103761",
    "https://openalex.org/W4385570045",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W4385568260",
    "https://openalex.org/W4385573762",
    "https://openalex.org/W4226095990",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W6604582260",
    "https://openalex.org/W4385567216",
    "https://openalex.org/W4385571260",
    "https://openalex.org/W3154009503",
    "https://openalex.org/W4200268060",
    "https://openalex.org/W6601089471",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W4200416755",
    "https://openalex.org/W1553603172",
    "https://openalex.org/W6602949942",
    "https://openalex.org/W6735927070",
    "https://openalex.org/W2948515602",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W6602902951",
    "https://openalex.org/W2981616527",
    "https://openalex.org/W3098136301",
    "https://openalex.org/W3003895133",
    "https://openalex.org/W3156834475",
    "https://openalex.org/W6842137259",
    "https://openalex.org/W6603143895",
    "https://openalex.org/W3034985765",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2035575455",
    "https://openalex.org/W6603944243",
    "https://openalex.org/W3102749280",
    "https://openalex.org/W2740314591",
    "https://openalex.org/W2757050214",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W6725117806",
    "https://openalex.org/W2250564385",
    "https://openalex.org/W4205131770",
    "https://openalex.org/W6600274115",
    "https://openalex.org/W6635750829",
    "https://openalex.org/W4385571831",
    "https://openalex.org/W3183859557",
    "https://openalex.org/W4385572727",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W6696711281",
    "https://openalex.org/W6858194769",
    "https://openalex.org/W2753392522",
    "https://openalex.org/W6635233243",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W6630820455",
    "https://openalex.org/W6822005569",
    "https://openalex.org/W4284714336",
    "https://openalex.org/W3210165781",
    "https://openalex.org/W3176038554",
    "https://openalex.org/W2593831809",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W6602641848",
    "https://openalex.org/W2511149293",
    "https://openalex.org/W6629650333",
    "https://openalex.org/W4285267480",
    "https://openalex.org/W6605988140",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W2525579820",
    "https://openalex.org/W4281656839",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W6631383168",
    "https://openalex.org/W3102343027",
    "https://openalex.org/W4323651349",
    "https://openalex.org/W4385571244",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W4382540290",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W4299860727",
    "https://openalex.org/W4389523710",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W4362597819",
    "https://openalex.org/W4387389153",
    "https://openalex.org/W4309435465",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1555731215",
    "https://openalex.org/W4385565485",
    "https://openalex.org/W2513499049",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4256531884",
    "https://openalex.org/W4366850753",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4387390364",
    "https://openalex.org/W3178506813",
    "https://openalex.org/W2963199195",
    "https://openalex.org/W4281489448",
    "https://openalex.org/W4323927057",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W3212456749",
    "https://openalex.org/W309727547",
    "https://openalex.org/W3183374399",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4205508242",
    "https://openalex.org/W4225573578",
    "https://openalex.org/W3002979031",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4386721614",
    "https://openalex.org/W4288093753",
    "https://openalex.org/W4386076375",
    "https://openalex.org/W3009928773",
    "https://openalex.org/W4221166113",
    "https://openalex.org/W4366196653",
    "https://openalex.org/W4319453300",
    "https://openalex.org/W3132120952",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W3202187802",
    "https://openalex.org/W4367692219",
    "https://openalex.org/W4386978016",
    "https://openalex.org/W4386221015",
    "https://openalex.org/W3089920618",
    "https://openalex.org/W4367365797",
    "https://openalex.org/W4225700096",
    "https://openalex.org/W3190215236",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W4250950595",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4287214436",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W4386065512",
    "https://openalex.org/W4387143567",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W4386184788",
    "https://openalex.org/W4386655789",
    "https://openalex.org/W1634456755",
    "https://openalex.org/W4280496127",
    "https://openalex.org/W4390872108",
    "https://openalex.org/W4385890089",
    "https://openalex.org/W4385573200",
    "https://openalex.org/W4389520407",
    "https://openalex.org/W4383218913",
    "https://openalex.org/W4385571962",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W3027453785",
    "https://openalex.org/W2962894366",
    "https://openalex.org/W4382322415",
    "https://openalex.org/W4379259189",
    "https://openalex.org/W4307308174",
    "https://openalex.org/W4387436689",
    "https://openalex.org/W4327944355",
    "https://openalex.org/W2505900768",
    "https://openalex.org/W3176186248",
    "https://openalex.org/W4287118802",
    "https://openalex.org/W2907820376",
    "https://openalex.org/W2251349042",
    "https://openalex.org/W4237171144",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2076063813",
    "https://openalex.org/W4365460762",
    "https://openalex.org/W4366823237",
    "https://openalex.org/W4378718543",
    "https://openalex.org/W4306353129",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4223423280",
    "https://openalex.org/W4382619267",
    "https://openalex.org/W4385436532",
    "https://openalex.org/W3208673210",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4382132560",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3199761064",
    "https://openalex.org/W2982419388",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4366456685",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W3209984917",
    "https://openalex.org/W4206940890",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W4384389674",
    "https://openalex.org/W2728624358",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4389544300",
    "https://openalex.org/W4367627709",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4385572852",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W4221159132",
    "https://openalex.org/W2105717194",
    "https://openalex.org/W4312107795",
    "https://openalex.org/W2953388933",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W3199003182",
    "https://openalex.org/W4390023570",
    "https://openalex.org/W4364382977",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4377865309",
    "https://openalex.org/W3208836296",
    "https://openalex.org/W4378469989",
    "https://openalex.org/W4386907041",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389519465",
    "https://openalex.org/W4386566659",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4362508616",
    "https://openalex.org/W4385572162",
    "https://openalex.org/W4366388740",
    "https://openalex.org/W4385571318",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4385574177",
    "https://openalex.org/W4377130839",
    "https://openalex.org/W4323713333",
    "https://openalex.org/W3173927728",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4295246696",
    "https://openalex.org/W4386384929",
    "https://openalex.org/W60164462",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W4385848337",
    "https://openalex.org/W4293138840",
    "https://openalex.org/W4386065596",
    "https://openalex.org/W4287671158",
    "https://openalex.org/W3197580070",
    "https://openalex.org/W4386621530",
    "https://openalex.org/W4376312146",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3169517138",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W4387427395",
    "https://openalex.org/W4323366518",
    "https://openalex.org/W4389518968",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W4378465306",
    "https://openalex.org/W4387323024",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2250769864",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W3044495139",
    "https://openalex.org/W4298184221",
    "https://openalex.org/W4312881242",
    "https://openalex.org/W4383046813",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4226403986",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4380356267",
    "https://openalex.org/W4366330700",
    "https://openalex.org/W2951122437",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2799002257",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4389501285",
    "https://openalex.org/W4365601449",
    "https://openalex.org/W4385573483",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4367628410",
    "https://openalex.org/W1996414682",
    "https://openalex.org/W4389520485",
    "https://openalex.org/W4324128075",
    "https://openalex.org/W4367189299",
    "https://openalex.org/W3102315106",
    "https://openalex.org/W4387559653",
    "https://openalex.org/W3209059054",
    "https://openalex.org/W3213454282",
    "https://openalex.org/W4386557569",
    "https://openalex.org/W4388093101",
    "https://openalex.org/W4300602297",
    "https://openalex.org/W4310428868",
    "https://openalex.org/W3030437843",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4389518761",
    "https://openalex.org/W3006165903",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W4382498773",
    "https://openalex.org/W4321593177",
    "https://openalex.org/W4229868159",
    "https://openalex.org/W4322616306",
    "https://openalex.org/W2004911280",
    "https://openalex.org/W4390072485",
    "https://openalex.org/W4378473878",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W2902907165",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4364384540",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4384644126",
    "https://openalex.org/W3210951978",
    "https://openalex.org/W4306176903",
    "https://openalex.org/W4387323979",
    "https://openalex.org/W4390872495",
    "https://openalex.org/W4290546063",
    "https://openalex.org/W4382490555",
    "https://openalex.org/W2276364082",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W4378508644",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4378713467",
    "https://openalex.org/W4389519818",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2755637027",
    "https://openalex.org/W4389520703",
    "https://openalex.org/W3103455452",
    "https://openalex.org/W2997759614",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4382766735",
    "https://openalex.org/W3213029956",
    "https://openalex.org/W2535241623",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W4385958573",
    "https://openalex.org/W4384405439",
    "https://openalex.org/W4387687174",
    "https://openalex.org/W4385571167",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4385373640",
    "https://openalex.org/W4365211588",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3215840701",
    "https://openalex.org/W4366588626",
    "https://openalex.org/W2914408384",
    "https://openalex.org/W3105247453",
    "https://openalex.org/W4312107707",
    "https://openalex.org/W4288804596",
    "https://openalex.org/W4281643269",
    "https://openalex.org/W4309986599",
    "https://openalex.org/W4224296706",
    "https://openalex.org/W3095410713",
    "https://openalex.org/W4362508448",
    "https://openalex.org/W2948113956",
    "https://openalex.org/W4385565015",
    "https://openalex.org/W4387427785",
    "https://openalex.org/W4385901626",
    "https://openalex.org/W4377865087",
    "https://openalex.org/W4306295121",
    "https://openalex.org/W4378942440",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W1670845525",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4318142952",
    "https://openalex.org/W4389518949",
    "https://openalex.org/W2080620213",
    "https://openalex.org/W4377111802",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4383473944",
    "https://openalex.org/W4283733941",
    "https://openalex.org/W4386651426",
    "https://openalex.org/W3213957358",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4378945542",
    "https://openalex.org/W4315706637",
    "https://openalex.org/W4385569771",
    "https://openalex.org/W4310419543",
    "https://openalex.org/W4384615647",
    "https://openalex.org/W4284664028",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4385262478",
    "https://openalex.org/W4383711155",
    "https://openalex.org/W4285250921",
    "https://openalex.org/W4368755703",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4387559592",
    "https://openalex.org/W4234502783",
    "https://openalex.org/W3184462553",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W3156892778",
    "https://openalex.org/W4389768224",
    "https://openalex.org/W4389520538",
    "https://openalex.org/W4385767932",
    "https://openalex.org/W4387725649",
    "https://openalex.org/W4386655779",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W1464569014",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4386973901",
    "https://openalex.org/W4320858112",
    "https://openalex.org/W4386044080",
    "https://openalex.org/W2162552722",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W2995181338",
    "https://openalex.org/W4378510493",
    "https://openalex.org/W3185461641",
    "https://openalex.org/W3201184431",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4309395891",
    "https://openalex.org/W4389216607",
    "https://openalex.org/W4399214196",
    "https://openalex.org/W4383473935",
    "https://openalex.org/W4378509427",
    "https://openalex.org/W4389523832",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4388327601",
    "https://openalex.org/W3032046549",
    "https://openalex.org/W4389519446",
    "https://openalex.org/W4382202657",
    "https://openalex.org/W4385571445",
    "https://openalex.org/W4366342667",
    "https://openalex.org/W3195830470",
    "https://openalex.org/W4306704712",
    "https://openalex.org/W4386620158",
    "https://openalex.org/W4383957026",
    "https://openalex.org/W4385638369",
    "https://openalex.org/W4378718229",
    "https://openalex.org/W4387356467",
    "https://openalex.org/W4378902815",
    "https://openalex.org/W3209374680",
    "https://openalex.org/W4377372007",
    "https://openalex.org/W3188060717",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4382765991",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4389667254",
    "https://openalex.org/W4385574274",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W4385262268",
    "https://openalex.org/W4362656036",
    "https://openalex.org/W4378473981",
    "https://openalex.org/W4387294588",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4361193608",
    "https://openalex.org/W4389524555",
    "https://openalex.org/W4362598872",
    "https://openalex.org/W4387294617",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4327810433",
    "https://openalex.org/W4386978002",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3176371077",
    "https://openalex.org/W4294753225",
    "https://openalex.org/W4380715528",
    "https://openalex.org/W3174986053",
    "https://openalex.org/W4230917753",
    "https://openalex.org/W2122124659",
    "https://openalex.org/W4386075723",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4378501037",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W4312107389",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W4363675964",
    "https://openalex.org/W4366196888",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W4307004479",
    "https://openalex.org/W4307863552",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4382603041",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4378770584",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W4291220917",
    "https://openalex.org/W2954542935",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4385570718",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W4392297945",
    "https://openalex.org/W4379539376",
    "https://openalex.org/W4287329154",
    "https://openalex.org/W4389519352",
    "https://openalex.org/W4377371478",
    "https://openalex.org/W4388890950",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4385965555",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W3197040007",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W4402264526",
    "https://openalex.org/W4367369802",
    "https://openalex.org/W4320855764",
    "https://openalex.org/W4385567081",
    "https://openalex.org/W2979691890",
    "https://openalex.org/W4387323780",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4389519620",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W4386071576",
    "https://openalex.org/W4221141415",
    "https://openalex.org/W3111739346",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4401109681",
    "https://openalex.org/W4386697749",
    "https://openalex.org/W4385567227",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4324319985",
    "https://openalex.org/W4226352076",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W4389519982",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W3165015862",
    "https://openalex.org/W4386566667",
    "https://openalex.org/W4302010020",
    "https://openalex.org/W4287020244",
    "https://openalex.org/W4385572345",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4385889719",
    "https://openalex.org/W4386240783",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4290960074",
    "https://openalex.org/W4281721632",
    "https://openalex.org/W2163561827",
    "https://openalex.org/W4380994594",
    "https://openalex.org/W3211393675",
    "https://openalex.org/W4377371656",
    "https://openalex.org/W4320839455",
    "https://openalex.org/W2767656849",
    "https://openalex.org/W4385573504",
    "https://openalex.org/W2074644642",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W4387294383",
    "https://openalex.org/W4287112297",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4385570365",
    "https://openalex.org/W3120825990",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4389524566",
    "https://openalex.org/W4288055861",
    "https://openalex.org/W2979196189",
    "https://openalex.org/W4310427982",
    "https://openalex.org/W4385570934",
    "https://openalex.org/W4385430679",
    "https://openalex.org/W3201290078",
    "https://openalex.org/W4387076386",
    "https://openalex.org/W4385262477",
    "https://openalex.org/W4389520003",
    "https://openalex.org/W4312780067",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4387323020",
    "https://openalex.org/W4365211596",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4361865740",
    "https://openalex.org/W4379256326",
    "https://openalex.org/W4224247158",
    "https://openalex.org/W3034201026",
    "https://openalex.org/W2885305518",
    "https://openalex.org/W4312777209",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W4387559805",
    "https://openalex.org/W4319780902",
    "https://openalex.org/W3135156311",
    "https://openalex.org/W4226408727",
    "https://openalex.org/W4385571505",
    "https://openalex.org/W4308014717",
    "https://openalex.org/W4387929122",
    "https://openalex.org/W3144194608",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W2529870885",
    "https://openalex.org/W4389520390",
    "https://openalex.org/W4361229539",
    "https://openalex.org/W2964710271",
    "https://openalex.org/W4366328015",
    "https://openalex.org/W4379089779",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4385292963",
    "https://openalex.org/W2964331441",
    "https://openalex.org/W2946011656",
    "https://openalex.org/W3135588948",
    "https://openalex.org/W4226243662",
    "https://openalex.org/W4382173247",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4389520747",
    "https://openalex.org/W4283796482",
    "https://openalex.org/W4353111444",
    "https://openalex.org/W4372273323",
    "https://openalex.org/W4283815817",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W3176445421",
    "https://openalex.org/W4287065027",
    "https://openalex.org/W4387636042",
    "https://openalex.org/W4386065936",
    "https://openalex.org/W4389518771",
    "https://openalex.org/W4224246420",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4312372834",
    "https://openalex.org/W4380993498",
    "https://openalex.org/W1533343564",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W4221145950",
    "https://openalex.org/W4383473763",
    "https://openalex.org/W4382334342",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W4283734749",
    "https://openalex.org/W4388757371",
    "https://openalex.org/W2945720633",
    "https://openalex.org/W2565993206",
    "https://openalex.org/W4287200699",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W1560253649",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4386148469",
    "https://openalex.org/W4221145109",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221167396",
    "https://openalex.org/W4385572142",
    "https://openalex.org/W4386065691",
    "https://openalex.org/W4224861986",
    "https://openalex.org/W3160345865",
    "https://openalex.org/W4303648904",
    "https://openalex.org/W4293314794",
    "https://openalex.org/W603908379",
    "https://openalex.org/W4288243162",
    "https://openalex.org/W4389518711",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W3133825286",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3008960538",
    "https://openalex.org/W4386076218",
    "https://openalex.org/W1850984366",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W3080426653",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W4382652046",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W4307225507",
    "https://openalex.org/W4365460740",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4380715575",
    "https://openalex.org/W4296605949",
    "https://openalex.org/W2252202991",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4386076670",
    "https://openalex.org/W4383175539",
    "https://openalex.org/W4319323461",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W4224232320",
    "https://openalex.org/W4367860440",
    "https://openalex.org/W4287203292",
    "https://openalex.org/W4378508715",
    "https://openalex.org/W4366565380",
    "https://openalex.org/W4281492411",
    "https://openalex.org/W4388651136",
    "https://openalex.org/W4380559118",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2025726911",
    "https://openalex.org/W4385002382",
    "https://openalex.org/W4376653374",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W4387764390",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W3186508434",
    "https://openalex.org/W4386057769",
    "https://openalex.org/W2996251235",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4385573990",
    "https://openalex.org/W4385970164",
    "https://openalex.org/W2995993311",
    "https://openalex.org/W4226087293",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W4287631799",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4286894277",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W2131789574",
    "https://openalex.org/W4387165002",
    "https://openalex.org/W4385572867",
    "https://openalex.org/W3175910413",
    "https://openalex.org/W4386847924",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4294068833",
    "https://openalex.org/W4378711566",
    "https://openalex.org/W4283809320",
    "https://openalex.org/W4321276845",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3120237956",
    "https://openalex.org/W4319165647",
    "https://openalex.org/W4382498654",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W4389519291",
    "https://openalex.org/W2041093119",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4288262459",
    "https://openalex.org/W4288262262",
    "https://openalex.org/W1480651636",
    "https://openalex.org/W4384807775",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4313442864",
    "https://openalex.org/W4385768082",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4378509182",
    "https://openalex.org/W4361866100",
    "https://openalex.org/W3123091710",
    "https://openalex.org/W3166444100",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4387561465",
    "https://openalex.org/W4282045675",
    "https://openalex.org/W4380558379",
    "https://openalex.org/W4388926587",
    "https://openalex.org/W4251160744",
    "https://openalex.org/W4376653732",
    "https://openalex.org/W4384648121",
    "https://openalex.org/W4378942772",
    "https://openalex.org/W3209532394",
    "https://openalex.org/W4367393583",
    "https://openalex.org/W4281491203",
    "https://openalex.org/W3101056292",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4386576765",
    "https://openalex.org/W4386977995",
    "https://openalex.org/W4383604274",
    "https://openalex.org/W1538134818"
  ],
  "abstract": "https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.",
  "full_text": "A Survey of Reasoning with Foundation Models\nJiankai Sun1, Chuanyang Zheng1, Enze Xie§2, Zhengying Liu§2,\nRuihang Chu1, Jianing Qiu1, Jiaqi Xu1, Mingyu Ding3,\nHongyang Li4, Mengzhe Geng1, Yue Wu2, Wenhai Wang1,\nJunsong Chen2,6, Zhangyue Yin11, Xiaozhe Ren2, Jie Fu5,\nJunxian He5, Wu Yuan1, Qi Liu3, Xihui Liu3, Yu Li1,\nHao Dong7, Yu Cheng1, Ming Zhang7, Pheng Ann Heng1,\nJifeng Dai8,4, Ping Luo3,4, Jingdong Wang9, Ji-Rong Wen10,\nXipeng Qiu11, Yike Guo5, Hui Xiong12, Qun Liu2, Zhenguo Li2\n1The Chinese University of Hong Kong.\n2Huawei Noah’s Ark Lab.\n3The University of Hong Kong.\n4Shanghai AI Lab.\n5Hong Kong University of Science and Technology.\n6Dalian University of Technology.\n7Peking University.\n8Tsinghua University.\n9Hefei University of Technology.\n10Renmin University of China.\n11Fudan University.\n12Hong Kong University of Science and Technology (Guangzhou).\nAbstract\nReasoning, a crucial ability for complex problem-solving, plays a pivotal role in\nvarious real-world settings such as negotiation, medical diagnosis, and criminal\ninvestigation. It serves as a fundamental methodology in the field of Artificial\nGeneral Intelligence (AGI). With the ongoing development of foundation mod-\nels, there is a growing interest in exploring their abilities in reasoning tasks. In\nthis paper, we introduce seminal foundation models proposed or adaptable for\nreasoning, highlighting the latest advancements in various reasoning tasks, meth-\nods, and benchmarks. We then delve into the potential future directions behind\n§Project Lead. Email:{xie.enze,liuzhengying2}@huawei.com\n1\nthe emergence of reasoning abilities within foundation models. We also discuss\nthe relevance of multimodal learning, autonomous agents, and super alignment in\nthe context of reasoning. By discussing these future research directions, we hope\nto inspire researchers in their exploration of this field, stimulate further advance-\nments in reasoning with foundation models, e.g. Large Language Models (LLMs),\nand contribute to the development of AGI. ∗†\nKeywords: Reasoning, Foundation Models, Multimodal, AI Agent, Artificial General\nIntelligence, Formal Methods\n∗We maintain a continuously updated reading list to benefit future research, featur-\ning relevant papers and popular benchmarks on reasoning. GitHub: https://github.com/\nreasoning-survey/Awesome-Reasoning-Foundation-Models\n†Preliminary release. We are committed to maintaining the quality and recency of this work.\n2\nContents\n1 Introduction 5\n2 Background 7\n2.1 Definition of Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.1 Deductive, Abductive, and Inductive Reasoning . . . . . . . . . 10\n2.1.2 Mathematical Representation . . . . . . . . . . . . . . . . . . . 12\n2.2 Foundation Models and Recent Progress . . . . . . . . . . . . . . . . . 13\n2.2.1 Language Foundation Models and Language Prompt . . . . . . 14\n2.2.2 Vision Foundation Models and Visual Prompt . . . . . . . . . . 14\n2.2.3 Multimodal Foundation Models . . . . . . . . . . . . . . . . . . 15\n2.2.4 Potential for Applications in Reasoning . . . . . . . . . . . . . 17\n3 Reasoning Tasks 18\n3.1 Commonsense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.1.1 Commonsense Question and Answering (QA) . . . . . . . . . . 20\n3.1.2 Physical Commonsense Reasoning . . . . . . . . . . . . . . . . 21\n3.1.3 Spatial Commonsense Reasoning . . . . . . . . . . . . . . . . . 22\n3.2 Mathematical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.2.1 Arithmetic Reasoning . . . . . . . . . . . . . . . . . . . . . . . 23\n3.2.2 Geometry Reasoning . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.2.3 Automated Theorem Proving . . . . . . . . . . . . . . . . . . . 24\n3.2.4 Scientific Reasoning . . . . . . . . . . . . . . . . . . . . . . . . 25\n3.3 Logical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.3.1 Propositional Logic . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.2 Predicate Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.4 Causal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.4.1 Counterfactual Reasoning . . . . . . . . . . . . . . . . . . . . . 31\n3.5 Visual Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.5.1 3D Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.6 Audio Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.6.1 Speech . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.7 Multimodal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.7.1 Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.7.2 Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.7.3 Multimodal Understanding . . . . . . . . . . . . . . . . . . . . 38\n3.8 Agent Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.8.1 Introspective Reasoning . . . . . . . . . . . . . . . . . . . . . . 40\n3.8.2 Extrospective Reasoning . . . . . . . . . . . . . . . . . . . . . . 41\n3.8.3 Embodied Reasoning . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.8.4 Multi-agent Reasoning . . . . . . . . . . . . . . . . . . . . . . . 44\n3.8.5 Reasoning in Autonomous Driving . . . . . . . . . . . . . . . . 45\n3.9 Other Tasks and Applications . . . . . . . . . . . . . . . . . . . . . . . 46\n3.9.1 Theory of Mind (ToM) . . . . . . . . . . . . . . . . . . . . . . . 46\n3.9.2 Weather Forecasting . . . . . . . . . . . . . . . . . . . . . . . . 46\n3\n3.9.3 Medical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.9.4 Bioinformatics Reasoning . . . . . . . . . . . . . . . . . . . . . 47\n3.9.5 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.9.6 Long-Chain Reasoning . . . . . . . . . . . . . . . . . . . . . . . 49\n3.9.7 Abstract Reasoning . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.9.8 Defeasible Reasoning . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.10 Benchmarks, Datasets, and Metrics . . . . . . . . . . . . . . . . . . . . 51\n3.10.1 Commensense Reasoning . . . . . . . . . . . . . . . . . . . . . 52\n3.10.2 Mathematical Reasoning . . . . . . . . . . . . . . . . . . . . . . 53\n3.10.3 Logical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 59\n3.10.4 Causal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.10.5 Visual Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n3.10.6 Audio Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n3.10.7 Multimodal Reasoning . . . . . . . . . . . . . . . . . . . . . . . 62\n3.10.8 Embodied Reasoning . . . . . . . . . . . . . . . . . . . . . . . . 64\n3.10.9 Autonomous Driving . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.10.10 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4 Foundation Model Techniques 67\n4.1 Pre-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.1.1 Data Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.1.2 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . 71\n4.2 Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n4.2.1 Data Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n4.2.2 Parameter-Efficient Fine-tuning . . . . . . . . . . . . . . . . . . 75\n4.3 Alignment Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n4.3.1 Data Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n4.3.2 Training Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n4.4 Mixture of Experts (MoE) . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.5 In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n4.5.1 Demonstration Example Selection . . . . . . . . . . . . . . . . 85\n4.5.2 Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . 86\n4.5.3 Multi-Round Prompting . . . . . . . . . . . . . . . . . . . . . . 87\n4.6 Autonomous Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n5 Discussion: Challenges, Limitations, and Risks 90\n6 Future Directions 94\n6.1 Safety and Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n6.2 Interpretability and Transparency . . . . . . . . . . . . . . . . . . . . . 94\n6.3 Autonomous Language Agents . . . . . . . . . . . . . . . . . . . . . . 95\n6.4 Reasoning for Science . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n6.5 Super Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n7 Conclusion 97\n4\n1 Introduction\n“Humans have always done nonmonotonic reasoning, but rigorous\nmonotonic reasoning in reaching given conclusions has been\ndeservedly more respected and admired.”\nJohn McCarthy (2004)\nReasoning is an essential aspect of artificial intelligence, with applications span-\nning various fields, such as problem-solving, theorem proving, decision-making, and\nrobotics (Manning, 2022). Thinking, Fast and Slow(Daniel, 2017) elucidates a dual-\nsystem framework for the human mind, consisting of “System 1” and “System 2”\nmodes of thought. “System 1” operates rapidly, relying on instincts, emotions, intu-\nition, and unconscious processes. In contrast, “System 2” operates slower, involving\nconscious deliberation such as algorithmic reasoning, logical analysis, and math-\nematical abilities. Reasoning plays a crucial role as one of the key functions of\n“System 2” (Bengio, 2017; Weston and Sukhbaatar, 2023). Reasoning can be cat-\negorized into two broad types: formal language reasoning and natural language\nreasoning (Reiter, 1975; Berzonsky, 1978; Teig and Scherer, 2016; Yu et al., 2023a;\nZhao et al., 2023b; Li et al., 2023u). On one hand, as shown in Figure 1, formal\nlanguage reasoning is often used in areas like formal verification of software and hard-\nware systems, theorem proving and automated reasoning (Reiter, 1975; Berzonsky,\n1978). On the other hand, natural language reasoning enables more intuitive human-\ncomputer interactions and supports tasks like question answering (Shao et al., 2023;\nJiang et al., 2021c), information retrieval (Zhu et al., 2023d; Ai et al., 2023), text sum-\nmarization (Liu et al., 2023n), and sentiment analysis (Yu et al., 2023a; Araci, 2019;\nBarbieri et al., 2021).\nSince their inception, foundation models (Bommasani et al., 2021) have demon-\nstrated remarkable efficacy across various domains, including natural language pro-\ncessing (Qiao et al., 2022), computer vision (Wang et al., 2023h), and multimodal\ntasks (Li, 2023). However, the burgeoning interest in general-purpose artificial intel-\nligence has sparked a compelling debate regarding whether foundation models can\nexhibit human-like reasoning abilities. Consequently, there has been a surge of interest\nin studying the reasoning capabilities of foundation models. While previous surveys\nhave explored the application potential of foundation models from different perspec-\ntives (Gu et al., 2023a; Wang et al., 2023h; Yin et al., 2023b; Zong et al., 2023;\nLou et al., 2023; Charalambous et al., 2023; Wang et al., 2023v,y,j), there remains\na need for a systematic and comprehensive survey that specifically focuses on recent\nadvancements in multimodal and interactive reasoning, which emulates human rea-\nsoning styles more closely. Figure 2 presents an overview of reasoning with regard to\ntasks and techniques that this article will discuss.\nFoundation models typically consist of billions of parameters and undergo\n(pre-)training using self-supervised learning (Jain et al., 2023) on a broad\ndataset (Bommasani et al., 2021). Once (pre-)trained, foundation models can be\nadapted to solve numerous downstream tasks through task-specific fine-tuning, lin-\near probing, or prompt engineering, demonstrating remarkable generalizability and\nimpressive accuracy (Bommasani et al., 2021; Qiu et al., 2023a). In contrast to the soft\n5\nReasoning\nNatural Language Reasoning\nDialogue Systems\nQuestion Answering\nRecommendation Systems\nText Summarization\nSentiment Analysis\nCo-reference Resolution\nAIGC\nLanguage Generation\nArgument Mining\n ...\nFormal Language Reasoning\nTheorem Proving\nProgramme Verification \nModel Checking \nLogical Inference\nAutomated Reasoning\nSymbolic Computation\nExpert Systems\nAI Planning\nKnowledge Representation\n ... \nReasoning\nNatural Language Reasoning\nDialogue Systems\nQuestion Answering\nRecommendation Systems\nText Summarization\nSentiment Analysis\nCo-reference Resolution\nAIGC\nLanguage Generation\nArgument Mining\n ...\nFormal Language Reasoning\nTheorem Proving\nProgramme Verification \nModel Checking \nLogical Inference\nAutomated Reasoning\nSymbolic Computation\nExpert Systems\nAI Planning\nKnowledge Representation\n ... \nReasoningNatural LanguageReasoningFormal LanguageReasoning\nFig. 1: Two broad types of language reasoning and examples of the supported tasks.\nattention mechanisms utilized in conventional transformers, System 2 Attention (S2A)\nharnesses the capabilities of Large Language Models (LLMs) to facilitate linguistic\nreasoning. This method improves the factuality and objectivity of long-form content\ngeneration. By integrating logical rules and principles into the learning process (Mao\net al., 2023b), these models can perform complex tasks such as deduction and infer-\nence. This allows them to make decisions based on explicit knowledge (Mao et al.,\n2023b) and logical reasoning, rather than relying solely on statistical patterns (Yang\net al., 2023f). As a rapidly growing field in artificial intelligence research, reasoning\nwith foundation models aims to develop models capable of understanding and interact-\ning with complex information in a more human-like manner. Built upon a foundation\nof logical reasoning and knowledge representation, these models make it possible to\nreason about abstract concepts and make decisions based on logical rules.\nFirst, reasoning with foundation models enables the application of prior knowledge\nand domain expertise. Logical rules can be derived from expert knowledge or formal-\nized from existing ontologies or knowledge graphs. By leveraging this prior knowledge,\nmodels can benefit from a better understanding of the problem domain and make\nmore informed decisions. Second, reasoning with foundation models can enhance the\nrobustness and generalization capabilities. By incorporating the information contained\nin massive amounts of data, models can better handle situations facing limited data\nor encountering unseen scenarios during deployment. This enables models to be more\nreliable and sturdy for robust, real-world usage.\nIn contrast to current surveys that have primarily focused on specific aspects of\nfoundation models, such as prompts (Qiao et al., 2022), hallucination (Rawte et al.,\n2023), deductive reasoning (Huang and Chang, 2022), logical reasoning (Friedman,\n2023a; Yang et al., 2023f), causal reasoning (Kıcıman et al., 2023; Stolfo et al., 2022),\nhealth informatics (Qiu et al., 2023a), or AI agents (Xi et al., 2023), this paper takes\na broader perspective, aiming to connect various research efforts in this area in a\ncohesive and organized manner. As Figure 2 shows, we provide a concise overview\nof various reasoning tasks, including Commonsense Reasoning, Mathematical\nReasoning, Logical Reasoning, Causal Reasoning, Visual Reasoning, Audio\n6\nReasoningTasksCommonsenseReasoning\nMathematicalReasoningLogicalReasoning\nCausalReasoning\nMultimodalReasoning\nVisualReasoning\nEmbodiedReasoning\nOtherTasks\n•Commonsense QA•Physical Commonsense•Spatial Commonsense\n•Arithmetic Reasoning•Geometry Reasoning•Theorem Proving•Scientific Reasoning\n•Propositional Logic•Predicate Logic\n•Policy Optimization•Decision Making•Explanation•Scientific Discovery•Counterfactual Reasoning\n•2D Reasoning•3D Reasoning\n•Alignment•Generation•Multimodal Understanding\n•Introspective Reasoning•Extrospective Reasoning•Muti-agent Reasoning•For Autonomous Driving\n•Theory of Mind•Weather Prediction•Abstract Reasoning•Defeasible Reasoning•Medical Reasoning•Long-Chain Reasoning•Bioinformatics Reasoning•Audio Reasoning\n Alignment Training\nPre-Training\nFine-tuning\nMixture of Experts (MoE)\nIn-Context Learning\nAutonomous Agent\nReasoningTechniques\nSupport\nFig. 2: Left: Overview of the reasoning tasks introduced in this survey, as detailed\nin Section 3. Right: Overview of the reasoning techniques for foundation models, as\ndetailed in Section 4.\nReasoning, Multimodal Reasoning, Embodied Reasoning, Defeasible Rea-\nsoning, and beyond. By doing so, we provide a comprehensive overview highlighting\nthe interconnections and relationships between different aspects of the field to inspire\nmore research efforts to actively engage with and further the advances of reasoning\nwith foundation models.\nIn summary, we have conducted a survey of over 650 papers on foundation mod-\nels, primarily focusing on research from the past two years. We discuss different\ntasks, approaches, techniques, and benchmarks used in these models. We also explore\nvarious application domains that can benefit from reasoning with foundation mod-\nels, such as question-answering, automated reasoning, and knowledge representation.\nWe also discuss the challenges and limitations of current reasoning with foundation\nmodels and potential directions for future research. By understanding the advance-\nments and challenges in this field, researchers can explore new avenues for developing\nintelligent systems that can reason and make decisions in a more human-like and inter-\npretable manner. Overall, this paper aims to provide a comprehensive understanding\nof reasoning with foundation models, its current state, and future possibilities.\n2 Background\nThis section introduces background knowledge about foundation models for reason-\ning. We will delve into key aspects such as what reasoning is, recent progress in\n7\nContext Lee found the Northeast to be way too cold. Lee\ndecided to move to Florida.\nQuestion How would you describe Lee?\nAnswers\na) happy\nb) likes cold weather\nc) likes the heat\nTable 1: An Example of Commonsense Reasoning Problem from Social IQA (Sap\net al., 2019). The correct answer is in bold.\nProblem A farmer has 3 types of fruits in his garden: apples,\noranges, and pears. He has twice as many apples as\noranges and three times as many pears as apples. If\nhe has 24 oranges, how many pieces of fruit does he\nhave in total?\nExpression x = 24 × 2 + 24× 3 × 2 + 24\nSolution 216\nTable 2: A Sample Math Word Problem (MWP).\ngeneral foundation models, the architectural design of foundation models, the train-\ning methodologies employed, and the transfer learning paradigm that enables their\napplications for reasoning tasks. By elucidating these fundamental aspects, we hope\nour readers will understand the underlying principles and techniques driving reason-\ning with foundation models, setting the stage for the subsequent exploration of recent\nadvancements and methodologies in this field.\n2.1 Definition of Reasoning\nWhen the term “reasoning” is brought up, its precise meaning is often unclear to\npeople. To clarify, let us first establish a clear definition of reasoning. “Reasoning” is\na broad and multifaceted concept that manifests in various contexts. It encompasses\ncognitive processes and logical thinking employed to analyze information, make deduc-\ntions, draw conclusions, and formulate coherent arguments. Reasoning can be observed\nin diverse domains, such as scientific inquiry, problem-solving, decision-making, and\neveryday discourse. Its fundamental purpose is to enable individuals to connect pieces\nof information, evaluate relationships, and arrive at informed judgments or solutions.\nBy exploring the different facets and dimensions of reasoning, we can gain a com-\nprehensive understanding of its significance and explore the mathematical formalisms\nand techniques employed to elucidate and enhance this fundamental aspect of human\ncognition.\nIn addition to its broad conceptual nature, the term “reasoning” carries specific\ndefinitions within various fields. Let us briefly touch upon the definitions of reasoning\n8\nExample\nFact1 This animal is a robin.\nRule All robins are birds.\nFact2 This animal is a bird.\nReasoning Type Representation\nDeduction (Fact1 + Rule → Fact2)\nAbduction (Fact1 + Rule ← Fact2)\nInduction (Fact1 + Fact2 → Rule)\nTable 3: Illustration of deductive reasoning, abductive reasoning, and inductive rea-\nsoning. In this example, the black text represents the given knowledge, while the red\ntext represents the inferred knowledge. The term “Fact” indicates specific informa-\ntion, while “Rule” denotes a general principle or guideline.\nin the domains of philosophy, logic, and Natural Language Processing (NLP) (Clark\net al., 2020; Huang and Chang, 2022; Yang et al., 2022c; Young et al., 2022; Yu et al.,\n2023a).\nPhilosophy\nDefinition 1. (Cognitive reasoning). Cognitive reasoning refers to modeling the\nhuman ability to draw meaningful conclusions despite incomplete and inconsistent\nknowledge involving among others the representation of knowledge where all processes\nfrom the acquisition and update of knowledge to the derivation of conclusions must be\nimplementable and executable on appropriate hardware (Furbach et al., 2019).\nLogic\nDefinition 2. (Logical reasoning). Logical reasoning involves a process of thought\nwhere conclusions are methodically drawn based on premises and the relationships\nbetween these premises, ensuring that the conclusions are logically implied or necessi-\ntated by them (Nunes, 2012).\nNLP\nDefinition 3. (Natural language reasoning). Natural language reasoning is a pro-\ncess of integrating multiple knowledge (e.g., encyclopedic knowledge and commonsense\nknowledge) to derive some new conclusions about the (realistic or hypothetical) world.\nKnowledge can be derived from sources that are both explicit and implicit. Conclusions\nare assertions or events assumed to be true in the world, or practical actions (Yu et al.,\n2023a).\nWe can also get a better understanding of what reasoning is, by categorizing them\nfrom different perspectives, as shown in the next sections.\n9\n2.1.1 Deductive, Abductive, and Inductive Reasoning\nBefore delving into recent developments, let us first review the traditional perspec-\ntives on reasoning, which categorizes it into three primary types: inductive reasoning,\ndeductive reasoning, and abductive reasoning. This categorization has long been rec-\nognized and provides a framework for understanding the different modes of reasoning.\nBy examining each type, we can better understand their distinctive characteristics and\napplications. So, let us take a closer look at these traditional categories to enhance\nour comprehension of reasoning processes.\nTable 3 provides an example to explain these three reasoning types, respectively.\nDeductive reasoning is a logical process that derives specific conclusions from general\nprinciples or premises. It follows a top-down approach, starting with general principles\nand applying logical rules to reach specific conclusions. Deductive reasoning aims to\nprovide logically valid and conclusive results.\nInductive reasoning involves drawing general conclusions or patterns based on\nspecific observations or evidence. It moves from specific instances to broader gen-\neralizations. Inductive reasoning does not guarantee absolute certainty but provides\nprobable conclusions based on available evidence (Wang et al., 2023o).\nAbductive reasoning is the process of making plausible explanations or hypotheses\nto account for observed facts or data. It involves inferring the best possible explanation\nfrom incomplete or limited information. Abductive reasoning is often used in problem-\nsolving and hypothesis generation.\nIn commonly used terms of reasoning, for a non-fallacious argument (an argu-\nment consisting of a premise and a conclusion) (Flach and Kakas, 2000), a deductive\nargument is classified as such when the premise can offer conclusive support for the\nconclusion. In other words, if all the premises of the argument are true, it would be\nimpossible for the conclusion to be false. On the other hand, an inductive argument is\ncharacterized by the premise providing only partial support for the conclusion (Salmon\net al., 1989). In the case of inductive arguments, the conclusions extend or surpass\nthe information contained in the premises (Salmon et al., 1989). Unlike deductive\narguments that provide conclusive proof or inductive arguments that offer partial sup-\nport, abductive arguments aim to provide the most reasonable explanation for a given\nsituation, even if it may not be the only possible explanation.\nTypically, in the trio of reasoning types, which includes deduction, abduction, and\ninduction, the most extensively studied and explored is deduction, while research on\nabduction and induction has remained relatively limited and under-explored (Flach\nand Kakas, 2000; Yang et al., 2023f). Encouragingly, progress has been made recently\nin the field of inductive reasoning. Sinha et al. (2019) propose the CLUTRR dataset\nfor classifying kinship relations in short stories using Natural Language Understanding\n(NLU). Inductive Relation Induction (Yang et al., 2022c) investigates the prediction of\nrelation that involves unseen entities. Misra et al. (2022) focus on classifying synthetic\nlanguage sentences using neural networks, whereas Yang and Deng (2021) have studied\nrule induction using quasi-natural language (symbolic rather than natural language).\nOther taxonomies of reasoning tasks include:\n(a) Formal Reasoning vs. Informal Reasoning (Evans and Thompson, 2004;\nTeig and Scherer, 2016): This taxonomy is based on the nature or formality of the\n10\nreasoning process. Formal reasoning involves following strict rules, logical frame-\nworks, or formal systems to derive conclusions and often relies on mathematical or\ndeductive reasoning. Informal reasoning, on the other hand, is less structured and\nmore intuitive, relying on personal experiences, common sense, and heuristics.\n(b) Neural Reasoning vs. Symbolic Reasoning vs. Neural-Symbolic Rea-\nsoning (Garcez et al., 2008, 2015, 2022): This taxonomy is based on the\nunderlying computational framework used for reasoning. Neural reasoning refers\nto approaches that utilize neural networks or deep learning models for reason-\ning tasks. Symbolic reasoning involves using symbolic representations, logic-based\ninference rules, or symbolic manipulation for reasoning. Neural-symbolic reason-\ning combines elements of both neural networks and symbolic reasoning, aiming\nto integrate their respective strengths.\n(c) Backward Reasoning vs. Forward Reasoning(Al-Ajlan, 2015): This taxon-\nomy is based on the direction of the reasoning process. Backward reasoning starts\nfrom a goal or desired outcome and works backward by applying rules or evi-\ndence to determine the necessary conditions or steps to reach that goal. Forward\nreasoning starts with initial premises or evidence and progresses step-by-step to\nderive new conclusions or reach a final outcome.\n(d) Single-step Reasoning vs. Multi-step Reasoning (Song et al., 2018; Yu\net al., 2023a): This taxonomy is based on the complexity or number of steps\ninvolved in the reasoning process. Multi-step reasoning refers to tasks that require\nmultiple sequential or interconnected steps to arrive at a solution or conclusion.\nIt involves chaining together intermediate steps or inferences to reach the final\nresult.\n(e) Deductive Reasoning vs. Defeasible Reasoning (Yu et al., 2023a; Koons,\n2005; Pollock, 1987, 1991): The classification criterion for this type of reasoning\nis based on the nature of the reasoning process and the handling of exceptions\nor conflicting information. Defeasible reasoning involves reasoning under uncer-\ntainty or with incomplete information, where conclusions can be overridden or\ndefeated by new evidence or exceptions. It allows for the revision or re-evaluation\nof conclusions based on additional information or context.\n(f) Unimodal Reasoning vs. Multimodal Reasoning (Sowa, 2003; Oberlan-\nder et al., 1996): This taxonomy is based on the input modalities used in the\nreasoning process. Unimodal reasoning refers to reasoning tasks that involve a\nsingle modality of information or input, for example, reasoning tasks that are\nbased solely on language information. Multimodal reasoning, on the other hand,\ninvolves integrating and reasoning with multiple modalities of information simul-\ntaneously. This could include combining visual, language, textual, auditory, or\nother types of input for the reasoning process.\nIn addition to the categorization mentioned above, there are several other ways to\nclassify or categorize information and reasoning, including factual reasoning (Byrne\nand Tasso, 1999), counterfactual reasoning (Bottou et al., 2013), plausible (defeasi-\nble) reasoning (Collins and Michalski, 1989), default reasoning (Brewka, 2012), and\nabstract reasoning (Yu et al., 2021).\n11\n2.1.2 Mathematical Representation\nBy acknowledging the above diverse definitions and perspectives, we gain a richer\nunderstanding of reasoning as a multifaceted concept that spans philosophical inquiry,\nformal logic, and practical applications in fields such as NLP. In this section, we\nwill explore the commonalities and distinct characteristics of reasoning across these\ndomains and investigate the mathematical methodologies employed to advance our\nunderstanding and implementation of reasoning processes. Here are examples of\nillustrating reasoning in different mathematical frameworks:\nPropositional Logic\nLogical proposition: Let p and q be logical propositions. We can represent their con-\njunction (AND) as p∧q. Modus Ponens: If p → q and p are true, then we can conclude\nq. This can be represented as ( p → q) ∧ p → q.\nPredicate Logic\nQuantifier and Predicate: Let P(x) be a predicate representing “x is a prime number.”\nThe existential quantifier (∃) can be used to express the existence of a prime number,\nsuch as ∃xP(x). Universal Quantifier: Let Q(x) be a predicate representing “ x is an\neven number.” The universal quantifier ( ∀) can be used to express that all numbers\nare even, such as ∀xQ(x).\nSet Theory\nSet Intersection: Let A and B be sets. The intersection of A and B is denoted as A∩B.\nSet Complement: Let A be a set. The complement of A is denoted as A′.\nGraph Theory\nGraph Representation: Let G = (V, E) be a graph, where V represents the set of nodes\nand E represents the set of edges. Shortest Path: Let d(u, v) represent the shortest\npath between nodes u and v in a graph. The shortest path problem can be formulated\nas finding the minimum value of d(u, v) for all pairs of nodes.\nConditional Probability\nLet P(A) represent the probability of event A and P(B) represent the probability of\nevent B. The conditional probability of A given B is denoted as P(A|B) and can be\ncalculated using Bayes’ theorem.\nFormal Systems\nAxiomatic System: Let S be an axiomatic system with a set of axioms and a set of\ninference rules. A formal proof within the system can be represented as a sequence\nof statements, where each statement is either an axiom or derived using the inference\nrules.\nThese mathematical expressions provide a glimpse into how reasoning can be\nexpressed mathematically in different frameworks. However, it is important to note\n12\nFoundation Models\nVision Foundation Models\nVision Transformers (e.g., ViT)\nVideo-MAE V2\nSAM\nInpaint Anything\nEdit Everything\nSAM-Track\nExplain Any Concept\n...\nModel Fusion \nMultimodal Foundation Models\nGPT-4V\nText2Seg\nCLIP series (e.g., CoOP, GALIP)\nSAMText\nCaption Anything\n...\nLanguage Foundation Models\nGPT-3\nChatGPT\nGPT-4\nPaLM\nBard\nLLaMA\nLlama 2\nPanGu-α\nPanGu-Σ\n...\nFig. 3 : Foundation models can be mainly categorized into language, vision, and\nmultimodal foundation models, each of which is an actively researched area.\nthat the complexity of reasoning problems often requires more elaborate mathematical\nexpressions and formalisms.\nDespite these traditional categorizations and rigorous mathematical representa-\ntions, with the advent of foundation models, researchers have increasingly moved away\nfrom strict adherence to these restrictions. Instead, they have embraced a more flexi-\nble approach to reasoning, considering its various forms and applications in different\nscenarios.\nIn contemporary research, reasoning has evolved to encompass a wide range of\ntasks and contexts. For instance, Commonsense Reasoning has emerged as a vital area\nfor study, aiming to endow AI systems with the ability to understand and reason about\neveryday situations, incorporating common knowledge and contextual understanding.\nAn example illustrating Commonsense Reasoning is shown in Table 1. Similarly, Math-\nematical Reasoning has garnered significant attention, particularly in the context of\nfoundation models. Researchers are exploring ways to enhance models’ mathematical\nreasoning abilities, including solving math word problems. An example showcasing\nMathematical Reasoning, specifically a Math Word Problem, is presented in Table 2.\nThese examples highlight the diverse manifestations of reasoning in different appli-\ncation domains. The focus has shifted from rigid categorizations to addressing specific\nreasoning challenges and designing models capable of tackling them effectively. By\nembracing this more flexible and application-driven perspective, researchers aim to\nbroaden the scope of reasoning and advance the development of AI systems capable of\nexhibiting human-like reasoning capabilities across a wide array of tasks and contexts.\n2.2 Foundation Models and Recent Progress\nIn recent years, the field of artificial intelligence has witnessed rapid development of\nfoundation models. Foundation models have revolutionized various domains, including\nbut not limited to computer vision, natural language processing, and speech recog-\nnition. Next, we introduce three main categories of the foundation model and their\nrepresentative works, as summarized in Figure 3.\n13\n2.2.1 Language Foundation Models and Language Prompt\nFoundation models, such as GPT-3 (Brown et al., 2020), herald breakthroughs in nat-\nural language understanding and generation tasks first. These models have shown the\nability to understand and generate coherent, contextually appropriate responses in nat-\nural language and have achieved significant progress in various language-related tasks,\nincluding text completion, translation, dialogue, summarization, question answering,\nand beyond.\nRecently, with the advancements in research and refined training methodolo-\ngies, a variety of advanced large-scale language models (Zhao et al., 2023b) § have\nemerged. Prominent among them are GPT-4 (OpenAI, 2023a), which powers Chat-\nGPT, and PaLM (Chowdhery et al., 2022), a crucial component of Bard. Additionally,\nLLaMA (Touvron et al., 2023a) and Llama 2 (Touvron et al., 2023b) have gained popu-\nlarity as a collection of open-source large language models, varying in parameters from\n7B to 65B. The focus on multilingual support has also become a key area of interest in\nfoundation modeling research. For instance, PanGu-α (Zeng et al., 2021), pre-trained\non 1.1 TB of Chinese data and has 200 billion parameters, shows robust language\nmodeling capabilities. Taking the concept further, PanGu-Σ (Ren et al., 2023) uti-\nlizes techniques like Random Routed Experts (RRE) and Expert Computation and\nStorage Separation (ECSS) to develop a system that trains a trillion-parameter lan-\nguage model, leading to a significant 6.3x increase in training throughput through\nheterogeneous computing.\n2.2.2 Vision Foundation Models and Visual Prompt\nFollowing the remarkable success of foundation models in the language domain, its\nimplications transcend to the realms of the vision field as well.\nVision Transformer (ViT) (Dosovitskiy et al., 2021) applies the Transformer frame-\nwork to computer vision tasks, achieving impressive performance in classification\nand retrieval tasks by leveraging self-attention mechanisms. Swin Transformer (Liu\net al., 2021b) introduces a hierarchical structure with shifted windows, improving\nthe efficiency of processing high-resolution images. It has demonstrated strong per-\nformance across various computer vision tasks such as image classification, object\ndetection, and semantic segmentation. Methods like MAE (He et al., 2022), BEIT (Bao\net al., 2021), and CAE (Chen et al., 2023i) propose masked modeling as an effi-\ncient self-supervised learning strategy to learn general-purpose visual representations.\nVideoMAE V2 (Wang et al., 2023i) is an enhanced version of VideoMAE (Tong\net al., 2022), with a billion parameters, designed for video understanding tasks. It uti-\nlizes self-supervised learning to learn temporal and spatial dependencies, excelling at\ntasks like action classification and action detection. As multitask vision foundation\nmodels, Florence (Yuan et al., 2021) and Florence-2 (Ding et al., 2022; Xiao et al.,\n2023a) can be easily adapted for a variety of computer vision tasks, such as classifica-\ntion, retrieval, object detection, visual question answering (VQA), image captioning,\nvideo retrieval, and action recognition, etc. Segment Anything Model (SAM) (Kir-\nillov et al., 2023) excels at producing object masks from input prompts like partial\n§https://github.com/RUCAIBox/LLMSurvey\n14\nmasks, points, or boxes. It has the capability to generate masks for all objects in\nan image. SAM is trained on a vast dataset that includes 11 million images and\n1.1 billion masks. Notably, SAM demonstrates zero-shot performance across a wide\nrange of segmentation tasks. As a zero-shot anomaly segmentation, Segment Any\nAnomaly+ (SAA+) (Cao et al., 2023) introduces hybrid prompt regularization, lever-\naging domain-specific expertise and contextual information from the target image to\nenhance the adaptability of foundational models. By incorporating these elements into\nthe regularization prompt, SAA+ strengthens the prompt’s robustness, enabling more\nprecise identification of anomalous regions. Furthermore, Wang et al. (2023c) have also\nrevealed the potential of incorporating domain expert knowledge as prior support in\naddressing segmentation challenges in complex scenes.\nModel Fusion: Enhancing Visual Task through Combination\nThere is a recent trend in the field of computer vision to combine different pre-trained\nVision Foundation Models, each specializing in specific tasks, in order to tackle com-\nplex visual tasks more effectively. These approaches take advantage of the increasing\npower and diversity of these foundation models, leveraging their individual strengths\nto achieve superior performance in challenging visual tasks.\nInpaint Anything (Yu et al., 2023c) presents three essential functionalities in image\ninpainting, namely Remove Anything, Fill Anything, and Replace Anything, which\nare achieved through the synergistic combination of various foundational models. It\nleverages click prompts for automatic segmentation, utilizes state-of-the-art inpainting\nmodels like LaMa (Suvorov et al., 2021) and Stable Diffusion (Rombach et al., 2022) for\nfilling masked regions, and employs AI models with text prompts to generate specific\ncontent for filling or replacing voids.\nEdit Everything (Xie et al., 2023a) presents a generative system that combines\nSAM (Kirillov et al., 2023), CLIP (Radford et al., 2021), and Stable Diffusion (Rom-\nbach et al., 2022) to enable image editing guided by both image and text inputs.\nInitially, Edit Everything (Xie et al., 2023a) employs SAM to segment the original\nimage into several fragments. Subsequently, the process of image editing is guided by\ntext prompts, leading to a transformation that adjusts the source image to correspond\nwith the target image as described in the given text prompts.\nSAM-Track (Cheng et al., 2023) introduces a video segmentation framework that\nintegrates Grounding-DINO (Liu et al., 2023j), DeAOT (Yang and Yang, 2022), and\nSAM (Kirillov et al., 2023) to facilitate interactive and automated object tracking and\nsegmentation across multiple modalities. The framework allows interactive prompts,\nincluding click-prompt, box-prompt, and text-prompt, in the initial frame of the video\nto guide SAM’s segmentation process. Explain Any Concept (EAC) (Sun et al., 2023a)\npresents an approach for concept explanation, utilizing SAM for initial segmentation\nand introducing a surrogate model to enhance the efficiency of the explanation process.\n2.2.3 Multimodal Foundation Models\nAs foundation models continue to exhibit impressive performance on individual modal-\nities, such as language and images, a natural extension arises: Can these models\neffectively handle multimodal data? This question arises from the recognition that\n15\nreal-world scenarios often involve multiple modalities, such as text, images, and audio,\nwhich collectively provide a more comprehensive and nuanced understanding of the\ndata.\nText2Seg (Zhang et al., 2023d) introduces a vision-language model that leverages\ntext prompts as input to generate segmentation masks. The model operates by using\na text prompt to generate bounding boxes with Grounding DINO (Liu et al., 2023j),\nwhich guides SAM in generating segmentation masks. CLIP (Radford et al., 2021)\nlearns joint representations of images and text. It achieves this by aligning visual and\ntextual information, enabling cross-modal understanding, and demonstrating impres-\nsive capabilities in various vision and language tasks. Similarly, methods (Chen et al.,\n2020b; Li et al., 2020; Zhang et al., 2021; Zhai et al., 2022; Yao et al., 2021; Jia\net al., 2021; Huo et al., 2021; Fei et al., 2022), like ALIGN (Jia et al., 2021) and\nWenLan (Huo et al., 2021), align image and text representations by learning a com-\nmon feature space. CoOp (Context Optimization) (Zhou et al., 2022b) presents a\nstraightforward technique to customize CLIP-like vision-language models for down-\nstream tasks. CoOp employs learnable vectors to represent the context words in a\nprompt while maintaining the pre-trained parameters in a fixed state. GALIP (Gen-\nerative Adversarial CLIPs) (Tao et al., 2023) is another advancement, specifically\ndeveloped for the task of text-to-image generation. In CLIP Surgery (Li et al., 2023t),\nheatmaps are first generated based on text prompts. Point prompts, which are then\nsampled from these heatmaps, are then inputted into SAM (Kirillov et al., 2023)\nfor further processing. Following this, a similarity algorithm utilizing CLIP (Radford\net al., 2021) is employed to produce the final segmentation map. SAMText (He et al.,\n2023) presents a flexible approach for creating segmentation masks tailored to scene\ntext. This method initiates by deriving bounding box coordinates from the annotations\npresent in an existing scene text detection model. These coordinates then prompt SAM\nto generate masks. Caption Anything (Wang et al., 2023p) presents a foundational\nmodel-enhanced framework for image captioning that enables interactive multimodal\ncontrol from both visual and linguistic aspects. By combining SAM (Kirillov et al.,\n2023) with ChatGPT, users gain the flexibility to manipulate images using a variety\nof prompts, including points prompts or bounding boxes prompts, during interaction.\nIt additionally leverages Large Language Models (LLMs) to refine instructions, ensur-\ning they accurately reflect the user’s intended meaning and remain consistent with\ntheir intention. GPT-4V(ision) empowers users to interpret and analyze user-provided\nimage inputs (OpenAI, 2023b).\nThe potential for foundation models to excel in multimodal tasks (text-to-image,\ntext-to-code, and speech-to-text) opens up exciting possibilities in various domains.\nBy seamlessly integrating and processing information from different modalities, these\nmodels can enhance tasks such as image captioning, visual question answering,\nand audio-visual scene understanding. Moreover, multimodal foundation models hold\npromise in applications that require reasoning and decision-making based on mul-\ntiple sources of information. By harnessing the power of multimodal data, these\nmodels have the potential to unlock new levels of understanding, context awareness,\nand performance across a wide range of domains, including robotics (Firoozi et al.,\n16\n2021-102021-112021-122022-012022-022022-032022-042022-052022-062022-072022-082022-092022-102022-112022-122023-012023-022023-032023-042023-052023-062023-072023-082023-092023-10\n0\n200\n400\n600\n800Number of ArXiv Papers\nFig. 4: Number of arXiv Papers on “Reasoning with Large Language Models” over\nthe past two years. It depicts a rising trend in the research interest, with the number\nof articles surging notably in the months of 2023.\n2023), healthcare (Qiu et al., 2023a), autonomous vehicles (Zhou et al., 2023c), and\nmultimedia analysis.\n2.2.4 Potential for Applications in Reasoning\nReasoning with foundation models is an emerging field. Recently there has been an\ninflux of research that attempts to apply foundation models to reasoning tasks, and\npromising results have been achieved. The statistics are presented in Figure 4. Laban\net al. (2023) identify challenges in evaluating complex tasks with Large Language\nModels (LLMs) and highlight the need for improved evaluation benchmarks. Shi et al.\n(2023) demonstrate that multilingual language models can go beyond language and\nperform tasks like commonsense reasoning and semantic judgment in a word-in-context\nsetting. Language models serve as multilingual reasoners employing chain-of-thought\nprocesses. Self-Taught Reasoner (STaR) (Zelikman et al., 2022) enhances a model’s\nreasoning abilities by iteratively generating rationales and fine-tuning based on cor-\nrect answers. MWP-BERT (Liang et al., 2022b) leverages both BERT (Kenton and\nToutanova, 2019) (110M) and RoBERTa (Liu et al., 2019) (123M) pre-training to\ntackle Math Word Problem (MWP) solving. Meanwhile, Minerva (Lewkowycz et al.,\n2022), based on the PaLM (Chowdhery et al., 2022) pre-trained language model, boasts\nan impressive parameter size of up to 540B. Minerva demonstrates strong perfor-\nmance by accurately answering nearly a third of over two hundred undergraduate-level\nproblems in various disciplines like chemistry, biology, economics, physics, and other\nsciences that involve quantitative reasoning. Zero-shot-CoT (Kojima et al., 2022)\ndemonstrates impressive performance across a range of reasoning tasks, including\narithmetic challenges such as MultiArith (Patel et al., 2021), GSM8K (Cobbe et al.,\n17\n2021), AQUA-RAT (Ling et al., 2017), SVAMP (Patel et al., 2021), symbolic rea-\nsoning, and other logical reasoning tasks like Date Understanding (Srivastava et al.,\n2023), Tracking Shuffled Objects (Srivastava et al., 2023), all without the necessity for\nhandcrafted few-shot examples. Employing just one prompt template, this approach\nindicates the zero-shot potential and the high-level, multi-task cognitive capacities of\nLLMs, while also emphasizing the significant prospects for additional research in this\nfield.\nHowever, there is still a need for intelligent systems that can perform more\nsophisticated forms of reasoning, beyond simple pattern recognition.\n3 Reasoning Tasks\nIn this section, we provide a concise overview of various reasoning tasks, as Figure 2\nshows. Here, we present distinct categories of reasoning approaches and tasks:\n• Commonsense Reasoning (Section 3.1): Exploring the capacity to infer and apply\neveryday, intuitive knowledge.\n• Mathematical Reasoning (Section 3.2): Focusing on the ability to solve mathe-\nmatical problems and derive logical conclusions.\n• Logical Reasoning (Section 3.3): Examining the process of drawing inferences and\nmaking decisions based on formal logic.\n• Causal Reasoning (Section 3.4): Investigating the understanding of cause-and-\neffect relationships and their implications.\n• Multimodal Reasoning (Section 3.7): Involving reasoning across multiple data\nmodalities, such as text, images, and sensory information.\n• Visual Reasoning (Section 3.5): Focusing on tasks that require the interpretation\nand manipulation of visual data.\n• Embodied Reasoning (Section 3.8): Exploring reasoning in the context of\nembodied agents interacting with their environment.\n• Other Reasoning Tasks (Section 3.9): The discussion of reasoning extends across\nvarious contexts, including conceptual frameworks, such as abstract reason-\ning 3.9.7, defeasible reasoning 3.9.8, as well as applied fields such as medical\nreasoning 3.9.3, bioinformatic reasoning 3.9.4, among others. We also highlight\nthe immense utility of long-chain reasoning in applications for researchers to\nexplore 3.9.6.\nThis comprehensive overview provides insights into the diverse landscape of rea-\nsoning tasks and approaches within the field. A summary of seminal works in each\nreasoning sector can be found in Figure 5.\n3.1 Commonsense Reasoning\nCommonsense reasoning refers to the human-like capacity to make assumptions\nand inferences about the nature and characteristics of everyday situations that humans\nencounter on a regular basis §.\n§http://www-formal.stanford.edu/leora/commonsense/\n18\nReasoning Tasks with Foundation Models\nCommonsense\nQA CQA (Talmor et al., 2019), ConceptNet (Speer et al., 2017),CoS-E (Rajani et al., 2019), CAGE (Rajani et al., 2019), etc.\nPhysical ESPRIT (Rajani et al., 2020), PACS (Yu et al., 2022), PIQA (Bisk et al., 2020),NEWTON (Wang et al., 2023w), etc.\nSpatial Liu et al (Liu et al., 2022d), etc.\nMathematical\nArithmeticPromptPG (Lu et al., 2022b), etc.\nGeometry Geoformer (Chen et al., 2022b), Inter-GPS (Lu et al., 2021a), etc.\nTheorem LeanDojo (Yang et al., 2023c), etc.\nScientific SciBench (Wang et al., 2023s), ScienceWorld (Wang et al., 2022b),ScienceQA (Lu et al., 2022a), etc.\nLogical PropositionalTomasic et al. (Tomasic et al., 2021), etc.\nPredicate ILP (Cropper et al., 2022), etc.\nCausal CounterfactualLi et al. (Li et al., 2023g), Wu et al.(Wu et al., 2023f), etc.\nVisual 3D 3D-LLM (Hong et al., 2023), 3D-VisTa (Ziyu et al., 2023), etc.\nAudio Speech SUPERB (Yang et al., 2021), SUPERB-SG (Tsai et al., 2022),Wav2Vec (Baevski et al., 2020), Speech SIMCLR (Jiang et al., 2020a),Unit BERT (HuBERT) (Hsu et al., 2021), WavLM (Chen et al., 2022c) etc.\nMultimodal\nGenerationStable Diffusion (Rombach et al., 2022), DALL·E, Midjourney,Flamingo-80B (Alayrac et al., 2022), MAGMA (Eichenberg et al., 2022),Kosmos-2 (Peng et al., 2023d), etc.\nAlignment CLIP (Radford et al., 2021), BLIP-2 (Li et al., 2023f), etc.\nUnderstandingLLaVA (Liu et al., 2023e), DePlot (Liu et al., 2023b), MatCha (Liu et al., 2023c),DetGPT (Pi et al., 2023), etc.\nEmbodied\nIntrospectivePAL (Gao et al., 2023b), ProgPrompt (Singh et al., 2023),Code-as-Policies (Liang et al., 2022a), SayCan (Ahn et al., 2022), etc.\nExtrospectiveSelf-Ask (Press et al., 2023), ReAct (Yao et al., 2023c),ToolFormer (Schick et al., 2023), LLM-Planner (Song et al., 2023a),Statler (Yoneda et al., 2023), EmbodiedGPT (Mu et al., 2023), etc.\nMulti-agentZhang et al. (Zhang et al., 2023c), Du et al. (Du et al., 2023),Nascimento et al. (Nascimento et al., 2023), Chen et al. (Chen et al., 2023a), etc.\nOthers\nToM Kosinski et al. (Kosinski, 2023), etc.\nWeatherPrediction MetNet-2 (Espeholt et al., 2022), Bi et al. (Bi et al., 2023), etc.\nAbstractReasoning Gendron et al. (Gendron et al., 2023), etc.\nDefeasibleReasoning BoardgameQA (Kazemi et al., 2023), etc.\nMedicalReasoning Med PaLM 2 (Singhal et al., 2023), Med PaLM M (Tu et al., 2023b),VisionFM (Qiu et al., 2023b), RETFound (Zhou et al., 2023d), etc.\nBioinformaticsReasoning ProGen (Madani et al., 2023), RFdifusion (Watson et al., 2023), etc.\nFig. 5: Taxonomy of Reasoning Tasks with Foundation Models. Only the representa-\ntive approaches for each type of task are listed.\nRecent research indicates that language models are capable of acquiring certain\naspects of common sense knowledge (Zhao et al., 2023f; Ye et al., 2023b). In the\ndomain of structured commonsense reasoning, Madaan et al. (2022) tackle the task by\ngenerating a graph based on natural language input. They formalize this problem as\na code generation challenge, utilizing large language models that are prompted with\ncode to construct the graph representation. Berglund et al. (2023) also point out that\nlanguage models often demonstrate a fundamental lapse in logical deduction, failing\nto generalize a common pattern in their training set, specifically, the likelihood of “B\nis A” occurring if “A is B” is present. Li et al. (2022f) take a systematic approach to\n19\nQuestion\nFoundation model\nKnowledge library\nthingrelationfact\nunderstand\nreasonImplicit knowledgeinput\n(a) Commonsense Question and Answering\nPhysical knowledge Physical properties\nforceinteraction\nstructureevent\nSpatial properties\nobject scalespatial relationship\n(b) Physical Commonsense Reasoning (c) Spatial Commonsense Reasoning \nFoundation model\nunderstand\nreason\narticulatedattribute\ngravity,friction\nappliedforce\nmotion attribute\nSpatial knowledge \nobjects\nFoundation model\nunderstand\nreason\nFig. 6: Three areas of research of foundation models in commonsense reasoning. (a) By\nunderstanding everyday knowledge, foundation models can reason about implicit\nknowledge from questions and deduce answers. (b) Foundation models infer a wide\nrange of physical properties from general physical knowledge. (c) Foundation models\nreason about spatial properties from a set of objects.\nevaluate the performance of large pre-trained language models on various common-\nsense benchmarks. They conduct zero-shot and few-shot commonsense evaluations\nacross four different benchmarks, considering six different model sizes. Notably, their\nevaluation includes a remarkably large language model with 280 billion parameters.\nMultiple evaluation settings, such as different score functions and prompt formats, are\nexplored to comprehensively assess the models’ ability to capture and reason about\ncommonsense knowledge.\nAnother direction in the field of commonsense reasoning involves combining pre-\ntrained language models with commonsense-specific fine-tuning techniques. Chang\net al. (2021) propose several architectural variations, leverage external commonsense\ncorpora, and employ commonsense-specific fine-tuning techniques for the Social IQA\ntask (Sap et al., 2019). Through their work, they demonstrate that these optimiza-\ntions can enhance the model’s performance in tasks related to social intelligence.\nFurthermore, Yang et al. (2023a) introduce a two-stage framework designed to connect\npre-training and fine-tuning in the task of commonsense generation.\nIn addition to the above-mentioned works, there are other aspects of commonsense\nreasoning that have been explored. These include commonsense question answer-\ning (QA), physical reasoning, spatial reasoning, and the corresponding benchmarks,\nas shown in Figure 6. These areas of research contribute to a deeper understand-\ning of how language models can effectively capture and reason about commonsense\nknowledge in various contexts.\n3.1.1 Commonsense Question and Answering (QA)\nAs a subfield of commonsense reasoning, Commonsense Question Answering (QA)\nfocuses on developing systems capable of answering questions that require a deep\nunderstanding of everyday knowledge and human-like reasoning. Unlike traditional\nfact-based QA, where answers can be derived from explicit information, commonsense\nQA involves understanding and reasoning about implicit knowledge and everyday\nhuman reasoning, as depicted in Figure 6(a).\n20\nThe Commonsense Question Answering (CQA) dataset (Talmor et al., 2019) is\na challenging multiple-choice dataset specifically designed for commonsense question\nanswering. It is derived from ConceptNet (Speer et al., 2017) and consists of approx-\nimately 12,000 questions. Every question comes with one correct answer and four\nadditional distractor answers. In addition, the Commonsense Explanations (CoS-E)\ndataset (Rajani et al., 2019) contains human commonsense explanations for the CQA\ndataset. The CoS-E dataset comprises two types of explanations: Selected explana-\ntions, which are text spans highlighted in the question that justify the answer choice,\nand open-ended explanations, which are free-form natural language explanations.\nCommonsense Auto-Generated Explanation (CAGE) model (Rajani et al., 2019)\nis a framework that involves training a language model to generate useful explanations\nby fine-tuning it using both the problem input and human-generated explanations.\nThe development of effective commonsense QA systems is an active area of\nresearch, and ongoing advancements in language models, knowledge representa-\ntion, and reasoning techniques continue to push the boundaries of commonsense\nunderstanding in machine intelligence.\n3.1.2 Physical Commonsense Reasoning\nCommonsense physical reasoning (Ding et al., 2021b), shown in Figure 6(b), involves\nutilizing everyday knowledge about the physical world to reason and understand the\nbehavior of objects and their properties. It encompasses reasoning about physical\nconcepts, such as the properties of objects reasoning (gravity, mass, inertia, or friction),\ntheir affordances, and how they can be manipulated (Chu et al., 2023a).\nExplaining Solutions to P hysical ReasonIng Tasks (ESPRIT) framework (Rajani\net al., 2020) combines commonsense physical reasoning with interpretability via nat-\nural language explanations. It operates in two stages: firstly, pinpointing key physical\nevents in tasks, and secondly, crafting natural language descriptions for both the initial\nscene and these crucial events. The framework aims to provide a unified approach to\nreasoning about commonsense physical concepts, such as gravity, friction, and collision,\nwhile also offering qualitative explanations using natural language. PACS (Physical\nAudiovisual CommonSense) (Yu et al., 2022) is a dataset designed for physical audio-\nvisual commonsense reasoning. It comprises 13,400 question-answer pairs, including\n1,377 distinct questions and 1,526 videos for physical commonsense. By benchmark-\ning unimodal and multimodal reasoning models, PACS identifies the limitations and\nareas of improvement in current models, thereby providing valuable opportunities to\npropel research in physical reasoning by examining multimodal reasoning approaches.\nPIQA (Physical Interaction: Question Answering) (Bisk et al., 2020) is a dataset that\nfocuses on multiple-choice question-answering in the domain of physical interactions.\nThe task involves selecting the most appropriate solution from two given options based\non a given question. The PIQA dataset consists of over 16,000 training QA pairs, with\nadditional data reserved for development and testing. The questions in PIQA have an\naverage length of 7.8 words, while both correct and incorrect solutions have an aver-\nage length of 21.3 words. NEWTON (Wang et al., 2023w) is a comprehensive platform\nthat serves as a repository, pipeline, and benchmark specifically created to assess the\nphysical reasoning capabilities of LLMs.\n21\nCATER (Girdhar and Ramanan, 2020) mainly focuses on physics-related visual\nscenes. CLEVRER (Yi et al., 2019) is a video question-answering benchmark that\ntargets the physical and causal relations grounded in dynamic videos of rigid-body\ncollisions. CLEVRER-Humans (Mao et al., 2022) further extends it to the causal\njudgment of physical events with human labels. Physion (Bear et al., 2021), Phys-\nion++ (Tung et al., 2023), and ComPhy (Chen et al., 2022e) evaluate objects with\ndifferent latent physical properties (e.g., mass, friction, elasticity, and deformability)\nfrom dynamic videos rendered from physics engines.\nBased on the above benchmarks, transformer-based foundational models (Ding\net al., 2020; Wu et al., 2022b) and neuro-symbolic frameworks with differentiable\nphysics (Ding et al., 2021b) are developed. Aloe (Attention over Learned Object\nEmbeddings) (Ding et al., 2020) integrates MONet (Burgess et al., 2019) for unsuper-\nvised object segmentation with self-attention mechanisms, facilitating spatio-temporal\nphysical reasoning about objects. SlotFormer (Wu et al., 2022b), a Transformer-\nbased object-centric dynamics model, is designed to unsupervisedly decipher complex\nsystems and interactions from videos. Utilizing a context encoding provided by\nSpatial Transformer (Jaderberg et al., 2016), Generative Structured World Models\n(G-SWM) (Lin et al., 2020c) advance object-centric world modeling. They incorpo-\nrate multimodal uncertainty and situational awareness through a core module known\nas Versatile Propagation (V-Prop). These frameworks and datasets contribute to the\nadvancement of commonsense physical reasoning by providing resources for model\nevaluation, interpretability, and understanding physical concepts through explanations\nand multimodal analysis.\nCurrently, the physical commonsense reasoning domain based on foundation mod-\nels is relatively unexplored, offering a ripe avenue for research and development. This\npresents a unique chance for researchers and practitioners to delve into and expand the\nboundaries of what’s possible with these models, potentially leading to groundbreaking\nadvancements and innovations.\n3.1.3 Spatial Commonsense Reasoning\nAs illustrated in Figure 6(c), spatial commonsense reasoning involves detecting the\nspatial position of objects and inferring the relationships between visual stimuli to\nunderstand the surrounding environment. Within the domain of spatial commonsense\nreasoning, two significant perspectives are object scales (Aroca-Ouellette et al., 2021)\nand spatial relationship (Hudson and Manning, 2019). Liu et al. (2022d) introduce a\nspatial commonsense benchmark, distinctly highlighting the relative sizes of objects\nand the spatial interactions between individuals and objects across various actions.\nThey investigate the performance of various models, including pre-trained vision-\nlanguage models and image synthesis models. Interestingly, they find that the models\nfor synthesizing images demonstrate better capabilities in learning accurate coherent\nknowledge of spatial relationships compared to other models. Furthermore, the spa-\ntial insights obtained through these models for synthesizing images also demonstrate\ntheir utility in enhancing natural language understanding tasks that necessitate spatial\ncommonsense reasoning.\n22\n3.2 Mathematical Reasoning\nMathematics distinguishes itself as a distinct language that relies on symbolic\nforms, and precision in meaning and possesses lower dimensionality compared to natu-\nral language. This unique characteristic allows us to demonstrate that meaning can be\nderived from a set of learned rule sets, as exemplified by the symbolic representations\nof mathematical concepts (Floyd, 2004). Mathematical problems can be effectively\nprogrammed when they are represented using symbols and corresponding expressions.\nBy formulating these problems in a computer language that can be translated into\nmachine code, deep learning-based reasoning systems have the ability to train on and\nacquire the underlying rules (Hinton, 1990; Schmidhuber, 2015; Friedman, 2023b).\nExperimental findings suggest that the performance of Large Language Models\n(LLMs) shows a weak correlation with question difficulty. Ling et al. (2017) propose an\napproach to solve algebraic word problems in a way that not only generates the answer\nbut also provides an explanation or rationale for the obtained result. MT2Net (Zhao\net al., 2022b) is a specialized model designed to tackle the MultiHiertt dataset (Zhao\net al., 2022b). It retrieves supporting facts from financial reports and generates exe-\ncutable reasoning programs to answer questions. This approach aims to provide a\ncomprehensive and accurate solution for the given questions.\n3.2.1 Arithmetic Reasoning\nMath Word Problems (MWPs) are commonly used to evaluate the arithmetic rea-\nsoning abilities of language models. While these issues may appear uncomplicated\nto humans, language models frequently encounter challenges when it comes to tasks\ninvolving arithmetic reasoning (Hendrycks et al., 2021b; Patel et al., 2021).\nPrevious research has explored various approaches to address these challenges.\nTemplate-based statistical learning methods like KAZB (Kushman et al., 2014),\nZDC (Zhou et al., 2015), and similarity-based method SIM (Huang et al., 2016) have\nbeen utilized. Wang et al. (2017) employs a recurrent neural network (RNN) to con-\nvert math word problems into equation templates, eliminating the need for complex\nfeature engineering. Additionally, they developed a hybrid model that integrates the\nRNN with a similarity-based retrieval system, further enhancing its performance. Xie\nand Sun (2019) introduces an innovative neural approach to construct expression trees\nin a goal-oriented manner for solving math word problems. Shen et al. (2021a) intro-\nduces a novel ranking task for math word problems and presents the Generate & Rank\nframework, which combines a generative pre-trained language model with multi-task\nlearning. This approach allows the model to learn from its errors and effectively differ-\nentiate between correct and incorrect expressions. A notable finding is that employing\nchain-of-thought prompting, along with a language model containing an impressive 540\nbillion parameters, yields performance comparable to task-specific fine-tuned models\nacross multiple tasks (Wei et al., 2022b). Unlike traditional symbolic reasoning tasks\nsuch as program synthesis and knowledge graph reasoning, solving MWPs requires\nadditional emphasis on numerical reasoning. PromptPG (Lu et al., 2022b) takes a\ndifferent approach by utilizing policy gradient techniques to learn the selection of\n23\nin-context examples. By dynamically constructing appropriate prompts for each test\nexample, PromptPG facilitates the solving of math word problems. This adaptive\napproach enhances the model’s ability to handle numerical reasoning tasks effectively.\n(Wang et al., 2023n) introduce MATH-SHEPHERD, a novel process-oriented math\nverifier that evaluates and assigns a reward score to each step in Large Language\nModels’ (LLMs) solutions to math problems.\n3.2.2 Geometry Reasoning\nGeoS (Seo et al., 2015) provides a system for mapping geometry word problems into a\nlogical representation, facilitating the process of problem-solving. Chen et al. (2021a)\nintroduce Neural Geometric Solver (NGS) as an approach to addressing challenges\nposed by geometric problems in the GeoQA benchmark (Chen et al., 2021a). NGS\nadopts a holistic approach, adeptly parsing multimodal information and generating\ninterpretable programs. Geoformer (Chen et al., 2022b) concurrently addresses calcu-\nlation and proving problems through sequence generation. This approach demonstrates\nimproved reasoning capabilities in both tasks by employing a unified formulation.\nAdditionally, the authors propose the Mathematical Expression Pretraining (MEP)\nmethod, predicting mathematical expressions within problem solutions (Chen et al.,\n2022b). This technique enhances the model’s ability to handle mathematical expres-\nsions effectively. Inter-GPS (Lu et al., 2021a) formulates the geometry-solving task\nas a problem-goal-searching process. By incorporating theorem knowledge as condi-\ntional rules, Inter-GPS enables step-by-step symbolic reasoning, facilitating effective\ngeometry problem-solving.\n3.2.3 Automated Theorem Proving\nTheorem proving is pivotal in both hardware and software verification (Khan et al.,\n2020; Li et al., 2005). In the context of hardware verification, it has found successful\napplication in the design of integrated circuits (Khan et al., 2020; Li et al., 2005). In\nthe realm of software verification, a notable achievement is the development of CertC,\na verified C compiler (Berghofer and Strecker, 2004). It is worth mentioning that com-\npanies such as Intel have made significant investments in formal methods to ensure\nthe absence of critical floating-point bugs in their processors. A prominent example\nof the consequences of such bugs is the costly Pentium FDIV bug in 1994, which\nresulted in a loss of $500 million (Harrison, 2010). Consequently, theorem proving\nhas played a pivotal role in verifying floating-point firmware (Harrison, 2010). Tra-\nditionally, theorem proving has relied on highly trained human experts proficient in\nspecific theorem proving tools and their respective application domains. However, the\nemergence of learnable automated theorem proving holds the potential to revolution-\nize hardware and software verification in two significant ways. First, it enhances the\nlevel of automation in theorem proving, making it less reliant on human expertise and\nmanpower. Second, it increases the adaptability of these methods, broadening their\nutility and applicability through machine learning.\nResearchers create Contemporary mathematical verification systems based on\ninteractive theorem provers (ITPs), including Isabelle (Paulson, 1994), Lean (de Moura\net al., 2015), Coq (Barras et al., 1997), and Metamath (Megill and Wheeler, 2019). In\n24\nrecent years, various approaches have integrated machine learning with ITPs (Yang\nand Deng, 2019; Gauthier et al., 2021). Validated on various datasets (PISA (Jiang\net al., 2021a), miniF2F (Zheng et al., 2021), LeanDojo (Yang et al., 2023c), FIMO Liu\net al. (2023a) and TRIGO (Xiong et al., 2023b)), these approaches leverage advance-\nments in language models (Polu and Sutskever, 2020; Han et al., 2021; Polu et al.,\n2023; jia, 2022; Lample et al., 2022; Miku la et al., 2023) to recommend actions based\non the current state of the proof, with a tree search identifying a sequence of correct\nsteps using actions provided by the language model. Methods like Monte Carlo Tree\nSearch (MCTS) (Silver et al., 2018; Wu et al., 2021b; Laurent and Platzer, 2022) or\ndynamic-tree MCTS (Wang et al., 2023g) are employed for this purpose. Previous\nwork has demonstrated the few-shot statement autoformalization capability of large\nlanguage models (LLMs) (Wu et al., 2022a). To investigate the applicability of these\nfindings to proof autoformalization, DSP conducted a thorough analysis using Draft,\nSketch, and Proof (Jiang et al., 2022). Subgoal-Learning (Zhao et al., 2023c) utilizes\nthe subgoal-goal informal proof and demonstration selection. LeanDojo (Yang et al.,\n2023c) is an open-source project for Lean (Moura and Ullrich, 2021), which contains\ntoolkits, data, models, and benchmarks. Lyra (Zheng et al., 2023b) proposes the use of\nTool Correction to mitigate LLM hallucinations and Conjecture Correction to improve\nthe quality of generated formal proof conjectures. Following the direction of Lyra, the\nLEGO-Prover (Xin et al., 2023) employs a growing skill library containing verified\nlemmas as skills to enhance the capability of LLMs used in theorem proving.\n3.2.4 Scientific Reasoning\nScientific reasoning encompasses the cognitive abilities and problem-solving skills\nrequired for formulating, evaluating, and refining hypotheses or theories. In the case of\nhighly developed proficiency, it also involves critical reflection on the process of acquir-\ning and evolving knowledge through these investigative activities (Morris et al., 2012).\nAs mathematical reasoning forms the foundation of, we mention scientific reasoning\nhere.\nScientific reasoning is closely relevant to AI for Science (AI4Science) (Zhang et al.,\n2023j). This relevance extends across a spectrum of fields, including physics, chem-\nistry, quantum mechanics, and more. The integration of foundation models into these\ndomains not only enhances our understanding but also opens up new avenues for explo-\nration and innovation. The potential for foundation models to revolutionize traditional\nscientific methods, accelerate discoveries, and solve complex problems is immense,\nmaking them an indispensable tool in the modern scientific landscape. Subramanian\net al. (2023) examine how various factors affect the transfer learning capabilities of\nfoundational models, such as the size of pre-trained models, dataset scale, a blend\nof models, and parameters outside the training distribution. Their study finds that\nincreasing the number of model parameters can enhance performance. Furthermore,\nthe “pre-train and fine-tune” approach is highly effective for scientific reasoning tasks,\nparticularly in physical systems governed by Partial Differential Equations (PDEs).\nHorawalavithana et al. (2022) modify OpenAI’s GPT-2 transformer decoder archi-\ntecture to develop a 1.47 billion parameter general-purpose model specifically for\nchemistry. This large-scale model demonstrates proficiency not only in in-domain\n25\ntasks but also in out-of-domain challenges. It is trained on a substantial corpus of\n670GB of text data, encompassing approximately 53.45 million chemistry-focused sci-\nentific articles and abstracts. IBM RXN for Chemistry (Team, 2022; Manica et al.,\n2023; Das et al., 2021) utilizes foundational models for predicting chemical reactions\nand procedural methodologies in chemistry. For a more comprehensive exploration\nof foundational models related to biology, please see Section 3.9.3 and Section 3.9.4.\nWe will not elaborate further on biology foundation models here. Currently, most\nscientific reasoning research predominantly concentrates on fields like mathematics,\nphysics, biology, and medicine (Qiu et al., 2023a). In contrast, foundational models in\nthe quantum realm are comparatively scarce. Building scalable foundation models for\nquantum systems faces several challenges, including the intrinsic complexity of quan-\ntum mechanics, limited data availability, the absence of standardized methodologies,\nand constraints in quantum hardware capabilities. Despite these hurdles, venturing\ninto this promising field presents an intriguing and potentially rewarding area of\nexploration.\nStandardization aids in advancing the field of scientific reasoning. Proposing\ndatasets or benchmarks is a process of standardization. Currently, datasets for\nscientific reasoning are mainly focused on fields such as mathematics, physics,\nand chemistry, examples of which include SciBench (Wang et al., 2023s), Science-\nWorld (Wang et al., 2022b), and ScienceQA (Lu et al., 2022a). SciBench (Wang\net al., 2023s) is a specialized benchmark designed to evaluate the scientific reasoning\ncapabilities, domain knowledge, and advanced calculation skills of LLMs in the con-\ntext of college-level scientific problems. This comprehensive benchmark encompasses\na meticulously curated collection of 695 problems carefully sourced from instruc-\ntional textbooks. SciBench consists of two datasets. The first dataset constitutes an\nexpansive collection of collegiate-level scientific problems sourced from mathematics,\nchemistry, and physics textbooks. Its primary objective is to evaluate the LLM’s capac-\nity to handle a diverse array of scientific topics and problem categories. The second\ndataset in SciBench, on the other hand, consists of problems sourced from computer\nscience and mathematics undergraduate exams, forming a closed set. This closed set\nis intentionally crafted to gauge the LLMs’ proficiency in solving precise problem-\nsolving challenges within these particular fields. ScienceWorld (Wang et al., 2022b) is\ndesigned to evaluate agents’ scientific reasoning capabilities within an interactive text\nenvironment. This environment simulates a standard elementary school science cur-\nriculum, featuring 30 high-level task types distributed across 10 different topics. The\nenvironment supports multiple states, allowing for diverse interactions and scenarios.\nBy abstracting the world and incorporating a wide range of objects, ScienceWorld\nprovides a complex interactive text environment for agents to navigate and reason\ntherein. It consists of 10 interconnected locations, each containing up to 200 types of\nobjects. These objects span a range of categories, and common environmental items\nlike furniture, books, and paintings. The environment provides a rich and diverse\nsetting for agents to interact with. The action set within ScienceWorld consists of\n25 high-level actions, covering actions related to the domain of science and common\nactions. Each step in ScienceWorld presents approximately 200,000 possible action-\nobject pairs, although only a proportion of these pairs will have actual implications\n26\nfor the task at hand. ScienceQA (Lu et al., 2022a) is a multimodal dataset compris-\ning 21,208 multiple-choice science questions sourced from elementary and high school\nscience curricula. The dataset offers a richer domain diversity by covering natural\nscience, language science, and social science topics.\nThese resources provide valuable platforms for testing the capabilities of founda-\ntion models in complex scientific reasoning domains, allowing for a more structured\napproach to assessing their reasoning abilities. The focus on these traditional sciences\nhighlights the need for expanding the scope of datasets to encompass a wider range\nof disciplines, potentially leading to more diverse and comprehensive advancements in\nscientific reasoning.\n3.3 Logical Reasoning\nLogical reasoning, covering propositional and predicate logic (Table 4), is a rigorous\nform of thinking that involves using premises and their relations to derive conclusions\nthat are implied by the premises (Nunes, 2012). It can serve as a fundamental basis\nfor various domains in computer science and mathematics.\nPrevious studies have explored the combination of neural networks and symbolic\nreasoning in neuro-symbolic methods (Mao et al., 2019; Pryor et al., 2023; Tian et al.,\n2022; Cai et al., 2021; Sun et al., 2021; Manhaeve et al., 2021; Gupta et al., 2019).\nHowever, these methods often face limitations such as specialized module designs\nthat lack generalizability or brittleness caused by optimization difficulties. In contrast,\nLLMs exhibit stronger generalization abilities when it comes to logical reasoning. The\nLogic-LM framework (Pan et al., 2023a) leverages LLMs and symbolic reasoning to\nenhance logical problem-solving (Luo et al., 2023d). It begins by utilizing LLMs to\nconvert natural language problems into symbolic formulations, which are then pro-\ncessed by deterministic symbolic solvers for inference. Additionally, a self-refinement\nstage is introduced, where error messages from the symbolic solver are utilized to\nrevise the symbolic formalizations. Bubeck et al. (2023) demonstrate that the GPT-4\nmodel can manifest logical reasoning abilities when addressing mathematical and gen-\neral reasoning problems. These higher-order capabilities, often referred to as emergent\nproperties, result from scaling the model with large datasets (Wei et al., 2022a). Zhao\net al. (2023a) employ language models for multi-step logical reasoning by integrat-\ning explicit planning into their inference procedure. This incorporation enables more\ninformed reasoning decisions at each step by considering their future effects. Further-\nmore, Creswell et al. (2023) propose the Selection-Inference (SI) framework, which\nemploys pre-trained LLMs as general processing modules. The SI framework alter-\nnates between selection and inference steps to generate a sequence of interpretable,\ncausal reasoning steps that lead to the final answer.\nRecent works leveraging LLMs for logical reasoning tasks can be categorized into\ntwo main approaches, as shown in Figure 7. The first approach is in-context learning,\nwhere specific prompts are used to elicit step-by-step reasoning from LLMs. Notable\nmethods in this category include chain-of-thought prompting (Wei et al., 2022b; Wang\net al., 2023t) and the least-to-most prompting approach (Zhou et al., 2022a). These\n27\nLarge language model\nExample 1:\n(a) In-context learning\n[Text 1][Label 1]\nExample 2:[Text 2][Label 2]\nExample k:[Text k][Label k]\n. . .. . .. . .k Demonstrationexamples\nNewqueryQuestion:[Text]\n[?]\nFix parameters\n[Answer]\nLarge language model\nSample 1:\n(b) Fine-tuning\n[Text 1]\nSample 2:[Text 2]\nSample k:[Text k]\n. . .\n. . .\n. . .\n[Answer 1]\n. . .\n[Answer 2]\n[Answer k]\n[Label 1][Label 2][Label k]\nLoss\nPretrainedweights\nWeight update\n🔥+\nLogical reasoning\nLogical reasoningOptimize\nFig. 7 : Two main approaches to enhancing logical reasoning capabilities of large\nlanguage models. (a) In-context learning leverages specific prompts as a demonstration\nto elicit logical reasoning. (b) Fine-tuning uses additional training samples to update\nthe specialized model parameters.\nPropositional Logic Predicate Logic\nBasic elements Atomic propositions, Compound\npropositions\nAtomic propositions, Compound\npropositions, Variables, Quantifiers,\nPredicates\nComplexity Lower Higher\nExpressive Power Limited More powerful\nApplications Circuit design, Boolean algebra Natural language processing, Knowl-\nedge representation, Database queries\nExamples p ∨ q; p ∧ q; ¬p; p → q ∀x, P(x); ∃x, P(x)\nTable 4: Comparison between Propositional Logic and Predicate Logic in terms of\nbasic elements, complexity, expressive power, and applications.\napproaches enable reasoning directly over natural language, providing flexibility. How-\never, the complexity and ambiguity of natural language can result in challenges such\nas unfaithful reasoning and hallucinations. The second approach is fine-tuning, where\nthe reasoning capabilities of LLMs are optimized through fine-tuning or training\nspecialized modules (Clark et al., 2020; Tafjord et al., 2022; Yang et al., 2022b).\n3.3.1 Propositional Logic\nPropositional logic deals with declarative sentences that can be assigned a truth\nvalue, either true or false, without any ambiguity. There are two types of proposi-\ntional logic: Atomic Propositions and Compound Propositions. Atomic propositions\n28\nare basic statements that cannot be further broken down, while compound proposi-\ntions are formed by combining atomic propositions using logical connectives such as\nconjunction (AND), disjunction (OR), and negation (NOT).\nIn the context of propositional logic resolution, Tomasic et al. (2021) performed\nfine-tuning on the GPT-2 and GPT-3 models, tailoring them for the purpose of simu-\nlating propositional logic resolution. This specialized training focuses on non-recursive\nrules that encompass conjunction, disjunction, and negation connectors. By leverag-\ning these language models, they aimed to enhance the logical reasoning capabilities in\npropositional logic problems.\nThe use of language models for propositional logic resolution is intriguing because\nthese models have demonstrated their ability to capture complex patterns and seman-\ntic relationships in natural language. By training them to understand and reason with\npropositional logic, researchers sought to improve their logical reasoning capabilities.\n3.3.2 Predicate Logic\nPredicate Logic, also known as First-order Logic, can be seen as an extension of propo-\nsitional logic, allowing for more nuanced expressions. In Predicate Logic, predicates\nare used to represent properties and provide additional information about the subject\nof a sentence. It involves variables with a specified domain and encompasses objects,\nrelations, and functions between those objects.\nInductive Logic Programming (ILP) is a specialized domain within the broader\nfield of machine learning (Cropper et al., 2022). ILP leverages first-order logic to repre-\nsent hypotheses and data, making logical language a crucial component in knowledge\nrepresentation and reasoning (De Raedt and Kersting, 2010).\nBy incorporating predicate logical representations and reasoning, LLMs offer the\npotential for more interpretable and explainable models (Liu et al., 2022c). It enables\nthe discovery of logical patterns and rules from data, facilitating the extraction of\nhuman-understandable knowledge.\n3.4 Causal Reasoning\nCausal reasoning refers to the process of understanding and explaining cause-and-\neffect relationships between events, actions, or variables (Waldmann and Hagmayer,\n2013; Liu et al., 2023l). Causal reasoning tasks can be categorized into causal discov-\nery, effect inference, attribution, judgment, and other tasks (Kıcıman et al., 2023).\nCausal discovery is the process of uncovering the directional cause-and-effect relation-\nships between variables. Effect inference involves the characterization of the magnitude\nand pattern of a known or postulated causal connection (LYU et al., 2022; Wang\net al., 2021a; Jin et al., 2023b). Attribution, on the other hand, entails identifying\nthe cause or causes behind a specific change. Judgment tasks expand on attribution\ntasks by encompassing the assignment of reward or blame for outcomes. Additionally,\nthese tasks encompass various domains such as policy optimization, decision-making,\nexplanation, scientific discovery, and more.\n29\neducation\n(a) Causal discovery\nstudytime\nexam score\nIQ\nw1?w2?\nqualityadvertise\nw1 main reason\npricechange \nsales dropwage\nsmoke\nlung cancer\n? yesor notw2w3\n(b) Effect inference(c) Attribution(d) Judgment\nFig. 8: Examples of causal graphs to reflect different casual reasoning tasks. (a) Causal\ndiscovery identifies the underlying causal relationships among variables in a given\nsystem. (b) Effect inference estimates the outcome (e.g., weight) of a specific interven-\ntion on a system based on known causal relationships. (c) Attribution determines the\nextent to which a particular cause is responsible for a given effect. (d) Judgment makes\ndecisions based on the perceived consequences and implications of causal relationships.\nA causal graph, also known as a causal network or causal diagram, is a graphical\nrepresentation of causal relationships between variables or events (Balashankar and\nSubramanian, 2021; Sch¨ olkopf et al., 2021). It is a visual tool used to depict cause-and-\neffect relationships and understand the causal structure of a system or phenomenon. In\na causal graph, variables or events are represented by nodes, and causal relationships\nbetween them are depicted by directed edges or arrows. In Figure 8, we use causal\ngraphs to illustrate multiple reasoning tasks mentioned above.\nCausal Discovery\nCausal discovery (Peters et al., 2017) involves the task of identifying the causal\ngraph (Long et al., 2022) that represents the underlying process responsible for gener-\nating observed data. LLMs have demonstrated competitive performance in discerning\npairwise causal connections, although their effectiveness can vary and is influenced by\nthe careful crafting of prompts. Long et al. (2022) investigate the limitations of GPT-\n3 in understanding causal relationships in the medical context. Within the framework\nof Neuropathic Pain Diagnosis (Tu et al., 2019), Tu et al. (2023a) find that ChatGPT\ntends to make false negative mistakes. The performance of LLMs in causal discovery\nis not yet stable or consistent, and they may provide different answers to the same\nquestion, potentially due to internal model updates. Long et al. (2023) suggest that\nexpert knowledge, including that of LLMs, may be incorrect. They propose leveraging\nimperfect experts, such as LLMs, to reduce uncertainty in the output of causal discov-\nery algorithms. By incorporating the expertise of LLMs into the statistical analysis of\nobjective data, they aim to improve the accuracy of causal structure learning. Advanc-\ning the current research on LLM-driven causal discovery, Ban et al. (2023) integrate\nknowledge-based LLM causal analysis with data-driven approaches to learning causal\nstructures. They effectively combine the expertise of LLMs regarding existing causal\nmechanisms with the statistical analysis of objective data. They devise a specialized\nset of prompts aimed at deriving causal graphs from specific variables. By employ-\ning these prompts, they evaluate the impact of LLM-informed causality on deducing\n30\ncausal structures from data. Compared to text-only LLMs, Code-LLMs (Liu et al.,\n2023l) with code prompts are better in causal reasoning.\nType Causality and Actual Causality\nType causality pertains to the inference of causal relationships between variables,\nwhich is evident in causal discovery and causal effect estimation. In contrast, actual\ncausality (Halpern, 2016) diverges from causal discovery by shifting the focus from\nvariables and their interrelationships to individual events, with the goal of uncovering\ntheir specific causes.\nCausaLM (Feder et al., 2021) has demonstrated that language models like\nBERT (Kenton and Toutanova, 2019) can obtain a counterfactual representation of\na particular concept of interest through the deliberate selection of auxiliary adver-\nsarial pre-training tasks. This counterfactual representation enables the prediction of\nthe concept’s actual causal effect on model’s performance. On the other hand, Zhang\net al. (2023a) believe that current LLMs can address causal questions by leveraging\nexisting causal knowledge, akin to combined domain experts. However, these models\nstill struggle to provide satisfactory answers when it comes to discovering new knowl-\nedge or performing high-stakes decision-making tasks with a high level of precision.\nCurrent LLMs lack the ability to incorporate actual physical data measurements to\nestablish a grounding for their available textual facts (Zeˇ cevi´ c et al., 2023; Willig et al.,\n2023). As a result, they are unable to engage in actual, inductive inference, similar to\nclassical (causal) structure discovery methods. This limitation raises a crucial societal\ndiscussion point regarding the process of learning from facts. It is arguable that the\nideal goal should be understanding rather than mere knowing, as the latter lacks both\ngeneralization and justification.\nIn summary, while LLMs show promise in causal discovery, their performance is\nstill inconsistent and sensitive to prompt engineering. Researchers are exploring ways\nto address these limitations.\n3.4.1 Counterfactual Reasoning\nCounterfactuals involve a premise that is false in the real world but assumed to be\ntrue in a hypothetical scenario. For example, “If cats were vegetarians,” followed by an\nimaginary consequence like “cats would love cabbages” (Li et al., 2023g). Counterfac-\ntual reasoning involves the consideration of hypothetical scenarios achieved by altering\nelements or conditions within an actual event or situation (Kahneman and Miller,\n1986; Byrne, 2007). It plays a fundamental role in understanding causality, enabling\nus to explore the potential outcomes that could have arisen under different circum-\nstances. By subjecting language models to counterfactual testing, we can manipulate\nthe factual accuracy and hypothetical nature of statements, thereby evaluating the\nmodels’ capacity to discern and effectively utilize this information in making predic-\ntions. This testing approach enables us to gain insights into the models’ aptitude for\ndifferentiating between actual and hypothetical scenarios and their ability to leverage\nthis understanding for accurate and contextually appropriate responses.\nIn the context of language models, each reasoning task can be represented as a\nmapping function fw : X → Y , which maps an input x ∈ X using a world model\n31\nw ∈ W to an output y ∈ Y . The world model encapsulates the conditions under\nwhich the function evaluation occurs, with the default world denoted as wdefault.\nHypothesis h estimates fw, while counterfactual worlds are represented as wcf. The\nlanguage model’s implementation of fw for a given instance x can be expressed as:\nh(f, w, x) = argmax\ny′\nPLM(y′|promptf (f, x), promptw(w)), (1)\nwhere counterfactual reasoning is denoted as h(f, wcf, x) and factual reasoning as\nh(f, wdefault, x).\nLi et al. (2023g) utilize counterfactual conditionals to examine the ability of pre-\ntrained language models (PLMs) in distinguishing between hypothetical and real-world\nscenarios. They explore how this capability interfaces with the models’ utilization\nof pre-existing real-world knowledge and associative cues. Their findings reveal that\nwhen confronted with counterfactual situations, PLMs tend to generate completions\nthat contradict established world knowledge. As an example, GPT-3 might have devel-\noped a nuanced comprehension of how linguistic cues, such as distinguishing between\n”If/had” and ”Because,” influence the connection between nearby lexical clues and the\nfollowing words. This suggests that PLMs might prioritize the influence of immediate\ncontextual cues over broader factual information when responding to counterfactual\nprompts. Wu et al. (2023f) introduce an evaluation framework that incorporates “coun-\nterfactual” task variations. They present a set of 11 counterfactual tasks and assess\nthe capability and performance of GPT-4 (OpenAI, 2023a), Claude (Anthropic, 2023),\nand PaLM-2 (Anil et al., 2023) on these tasks, considering both default and counter-\nfactual conditions. The findings indicate that while current language models possess\nsome degree of abstract task-solving abilities, their performance often relies on narrow,\ncontext-specific procedures that are not easily transferable across tasks. Notably, the\nmodels consistently exhibit a significant decrease in performance when confronted with\ncounterfactual task variants compared to the default settings. These results empha-\nsize the need for a careful interpretation of language model performance, taking into\naccount different aspects of their behaviors and the challenges posed by counterfactual\nreasoning.\n3.5 Visual Reasoning\nVisual reasoning refers to the cognitive process of understanding, analyzing, and\ndrawing conclusions from visual information. It involves the ability to perceive,\ninterpret, and reason about visual stimuli such as images, scenes, or other visual\nrepresentations (Ding et al., 2023b).\nGeneral-purpose Visual Understanding Evaluation (G-VUE) is a comprehensive\nevaluation framework (Huang et al., 2023b). It aims to assess the full range of visual\ncognitive abilities. The framework is divided into four functional domains: Perceive,\nGround, Reason, and Act. As shown in Figure 9, the framework encompasses a\nthoughtfully chosen collection of 12 tasks, including 3D reconstruction, visual rea-\nsoning, manipulation, and interaction, to represent these domains. G-VUE serves as\n32\nVisual \nReasoning\nReason \nVisual question answering\nVisual commonsense reasoning\nAbstract and few-shot reasoning\nAct \nNavigation\nManipulation\nInteractionGround \nImage-text retrieval\nPhrase grounding\nSemantic segmentation\nPerceive \nSingle-view 3D reconstruction\nDepth estimation\nCamera pose estimation\nFig. 9: Four functional domains of a general vision systmem G-VUE (Huang et al.,\n2023b) and their corresponding visual tasks.\na standardized and comprehensive platform for assessing the visual understanding\ncapabilities of AI systems. By prioritizing diverse functional domains and carefully\nselecting tasks, the framework ensures the inclusion of a wide range of visual cogni-\ntive abilities. This enables more accurate evaluations of the strengths and weaknesses\nof AI systems. VLGrammar (Hong et al., 2021a) is a model that employs compound\nprobabilistic context-free grammars (PCFGs) to simultaneously induce language and\nimage grammar. A novel contrastive learning framework is also proposed, which facil-\nitates the joint learning of these two modules. AeNER (Ding et al., 2021a) introduces\na general neural-network-based approach to dynamic visual spatio-temporal reasoning\nproblems. This approach differs from bespoke methods like modular symbolic com-\nponents, independent dynamics models, or semantic parsers. AeNER offers a more\nversatile and adaptable solution for addressing dynamic visual spatio-temporal rea-\nsoning challenges. LISA (Lai et al., 2023) proposes a new vision task termed reasoning\nsegmentation, aiming to obtain the mask(s) of corresponding object(s) from an implicit\nquery text. It requires world knowledge from LLMs along with image understanding\nto accurately locate the object of interest.\n3.5.1 3D Reasoning\nSpecifically, 3D reasoning refers to the cognitive process of understanding, analyzing,\nand reasoning about 3D objects or spatial arrangements.\n3D-LLM (Hong et al., 2023) is designed to process 3D point clouds along with\ntheir associated features. It demonstrates remarkable proficiency across a diverse\nspectrum of 3D-related tasks, encompassing dense captioning, 3D question answer-\ning, 3D grounding, 3D-assisted dialogue, navigation, task decomposition, and beyond.\nPointLLM (Xu et al., 2023c) is an approach that extends LLMs to understand 3D\npoint clouds, combining geometric, visual, and textual information to interact with\nand interpret 3D data. It shows superior performance in object classification and cap-\ntioning tasks compared to 2D baselines. On the other hand, 3D-VisTa (Ziyu et al.,\n2023) is a pre-trained Transformer model specifically developed for aligning 3D vision\nand text. It proves to be highly valuable for 3D vision-language (3D-VL) tasks such\nas 3D visual grounding, dense captioning, and situated reasoning (Ma et al., 2023).\n33\nSUPERB\nContent\nPRASRKSQbE-STD\nSpeaker\nSIDASVSD\nSemantic\nIC SF\nParalinguistics\nER\nFig. 10 : Four evaluation areas of the SUPERB (Yang et al., 2021) focus on the\ndiscriminative abilities of foundation models and the corresponding tasks. PR: phone\nrecognition, ASR: automatic speech recognition, KS: keyword spotting, QbE-STD:\nquery by example spoken term detection, SID: speaker identification, ASV: automatic\nspeaker verification, SD: speaker diarization, IC: intent classification, SF: slot filling,\nER: emotion recognition.\nResearch into 3D reasoning with foundational models is at a fascinating juncture.\nModels like 3D-LLM, PointLLM, and 3D-VisTa have already showcased their effec-\ntiveness in diverse 3D tasks, blending geometric, visual, and textual data. Despite\nthese advancements, the field is still burgeoning, with much room for exploration and\nenhancement. Future directions could include refining model capabilities for more intri-\ncate 3D scene interpretations, expanding applications in real-world scenarios such as\nnavigation agents for visually impaired or blind people (Qiu et al., 2022), and bridging\ngaps in current methodologies.\n3.6 Audio Reasoning\nAudio reasoning pertains to the cognitive mechanism of comprehending, examining,\nand deriving conclusions from auditory data, of which speech is the major source.\nSpeech representations learned in a self-supervised fashion provide a promising solu-\ntion in this direction, where a single foundation model is trained and can be applied\nto a wide spectrum of downstream tasks (Mohamed et al., 2022).\n3.6.1 Speech\nThe field of speech processing can be broadly classified into two distinct categories:\ndiscriminative tasks and generative tasks. Discriminative tasks entail the process of\nmaking discrete decisions based on continuous speech, while generative tasks involve\nthe generation of continuous speech from diverse input sources. The Speech process-\ning Universal PERformance Benchmark (SUPERB) (Yang et al., 2021) is a widely\nadopted framework for evaluating the discriminative abilities of the foundation model.\nAs demonstrated in Figure 10, it encompasses ten tasks covering four elements of\nspeech: Content, Speaker, Semantics, and Paralinguistics.\nThe enhanced Speech processing Universal PERformance Benchmark (SUPERB-\nSG) (Tsai et al., 2022) further introduces a framework to evaluate the generative\n34\nabilities of the foundation model with five tasks: speech translation (ST), out-of-\ndomain automatic speech recognition (OOD-ASR), voice conversion (VC), speech\nseparation (SS), and speech enhancement (SE).\nThe foundation models for learning self-supervised speech representation can be\ncategorized into three major types: 1) generative models that reconstruct the\ninput speech sequence leveraging on restricted or corrupted views, for example,\nvector-quantized variational autoencoder (VQ-VAE) (Van Den Oord et al., 2017),\nautoregressive predictive coding (APC) (Chung et al., 2019), and masked acoustic\nmodel (MAM) (Liu et al., 2020); 2) contrastive models that differentiate a target\npositive sample from distracting negative samples, for example, contrastive predictive\ncoding (CPC) (Oord et al., 2018), Wav2Vec 2.0 (Baevski et al., 2020), and Speech\nSIMCLR (Jiang et al., 2020a); and 3) predictive models that follow the settings\nsimilar to teacher-student learning (Li et al., 2017), for example, Hidden Unit BERT\n(HuBERT) (Hsu et al., 2021), WavLM (Chen et al., 2022c) and Data2Vec (Baevski\net al., 2022). The Transformer-Encoder (Dong et al., 2018) architecture and the\nConformer-Encoder (Gulati et al., 2020) architecture are widely adopted in speech\nfoundation models.\n3.7 Multimodal Reasoning\nMultimodal reasoning refers to the cognitive process of integrating and reasoning\nacross multiple modalities of information, such as text, images, videos, and other\nsensory inputs, to enhance understanding and perform complex reasoning tasks (Yin\net al., 2023b; Zong et al., 2023) §.\nIn the pursuit of developing Artificial General Intelligence (AGI), multimodal\nreasoning represents a promising advancement over unimodal approaches for several\nreasons. Firstly, multimodal reasoning aligns more closely with the way humans per-\nceive the world. Humans naturally receive inputs from multiple senses, which often\ncomplement and cooperate with each other. As a result, leveraging multimodal infor-\nmation is anticipated to enhance the intelligence of Multimodal Foundation Models.\nSecondly, multimodal reasoning provides a more user-friendly interface. By incorporat-\ning support for multimodal input, users can interact and communicate with intelligent\nassistants in a more flexible, diverse and potentially more intuitive manner, improving\nthe overall user experience. Thirdly, multimodal reasoning facilitates a more compre-\nhensive problem-solving capability. While unimodal language models typically excel\nin natural language processing (NLP) tasks, Multimodal Foundation Models have the\npotential to support a broader spectrum of tasks, making them more versatile and\neffective as task-solvers. Key techniques and applications of Multimodal Foundation\nModels encompass various areas, including Multimodal Instruction Tuning (M-IT),\nwhich focuses on fine-tuning models based on multimodal instructions; Multimodal\nIn-Context Learning (M-ICL), which leverages contextual information to enhance mul-\ntimodal reasoning; and LLM-Aided Visual Reasoning (LAVR), which utilizes LLMs to\nenhance visual reasoning capabilities. Figure 11 shows multiple multimodal reasoning\ntasks and the key techniques behind, which are introduced as follows.\n§https://github.com/atfortes/Awesome-Multimodal-Reasoning\n35\nWebdesignPromotion\nMultimodalUnderstandingLLaVADePlotMatChaDetGPTMiniGPT-4\nKeyTechniquesMultimodalInstruction TuningMultimodalIn-ContextLearningLLM-AidedVisual Reasoning\nImage-Text AlignmentCLIPBLIP-2\nBLIP\nText-to-imageGenerationStable DiffusionDALL·E 3Midjourney\nMultimodal-to-textGenerationFlamingoFrozenMAGMA\nKosmos-1GPT-4VKosmos-2BiomedGPT\nVisual ChatGPT GPT-4V\nFig. 11 : Multimodal reasoning tasks can be broadly categorized into image-text\nalignment, text-to-image generation, multimodal-to-text generation, and multimodal\nunderstanding. Current multimodal foundation models mainly involves three key\ntechniques to approach reasoning tasks, including multimodal instruction tuning, mul-\ntimodal in-context learning, and LLM-aided visual reasoning. The figure style credits\nfrom tutorial (Li, 2023).\n3.7.1 Alignment\nImage-Text Alignment\nCLIP (Radford et al., 2021) utilizes a learning method that enables the creation of\ncohesive representations for both images and text. By aligning visual and textual\ninformation, CLIP fosters cross-modal comprehension and demonstrates exceptional\nproficiency across a wide range of vision and language tasks. In a similar vein, BLIP-\n2 (Li et al., 2023f) adopts a strategy to facilitate efficient cross-modal alignment\nwithout fine-tuning the vision encoder. Instead, it introduces a Querying Trans-\nformer (Q-Former) that extracts visual features from a fixed image encoder. These\nextracted query embeddings serve as soft visual prompts for the alignment process.\nFlamingo (Alayrac et al., 2022) bridges pretrained vision and language backbones by\ntoken fusion with cross-attentions.\n36\n3.7.2 Generation\nText-to-image Generation\nStable Diffusion (Rombach et al., 2022) integrates cross-attention layers to the model\narchitecture, transforming diffusion models into robust and adaptable generative mod-\nels for diverse conditional inputs like text and bounding boxes. The application\nof latent diffusion models (LDMs) represents a significant breakthrough in image\ninpainting, while also delivering impressive results in unconditional content generation,\nsuper-resolution image generation, and other tasks. Notably, LDMs offer substantial\nreductions in computational demands compared to pixel-based diffusion models, while\nmaintaining highly competitive performance. DALL ·E§ (Ramesh et al., 2021, 2022;\nBetker et al., 2023) is an advanced AI system that has the capability to generate\nrealistic images and artwork based on natural language descriptions. Likewise, Mid-\njourney is another AI system that specializes in generating images based on natural\nlanguage descriptions, which are referred to as “prompts”. By leveraging the power\nof AI, Midjourney § can translate textual prompts into visual compositions, provid-\ning a visual representation of the given description. ImageGen (Saharia et al., 2022)\nleverages the capabilities of expansive transformer language models for text compre-\nhension and combines this with the efficacy of diffusion models for creating high-quality\nimages. PixArt (Chen et al., 2023c) is a Transformer-driven Text-to-Image (T2I)\ndiffusion model. It rivals leading image generation systems such as Imagen, SDXL,\nand Midjourney in terms of quality, approaching the benchmarks set by commercial\napplications.\nMultimodal-to-text Generation\nFlamingo-80B (Alayrac et al., 2022) comprises a family of Visual Language Models\n(VLMs) equipped with in-context few-shot learning capabilities. These models undergo\nthorough evaluation across a wide array of tasks, including open-ended ones like visual\nquestion-answering and captioning, as well as closed-ended tasks such as multiple-\nchoice visual question-answering. Frozen (Tsimpoukelli et al., 2021) accomplishes\nfew-shot learning ability within a multimodal context by preserving the language capa-\nbilities of a Language Model (LM) while incorporating visual information as a prefix.\nFrozen achieves this by freezing the LM and training a separate vision encoder to rep-\nresent images. In the Frozen approach, visual information is represented as a sequence\nof embeddings, serving as a visual prefix. MAGMA (Eichenberg et al., 2022) follows a\nsimilar approach to Frozen by incorporating a new image prefix encoder while keep-\ning the language model frozen. It trains a series of Visual Language models capable\nof generating text autoregressively from combined visual and textual inputs. Visual\nChatGPT (Wu et al., 2023a) and GPT-4 (OpenAI, 2023a) represent advancements\nin extending chatbot capabilities to encompass multimodal applications that support\nboth image and text prompts. Visual ChatGPT builds upon the foundation of Chat-\nGPT and incorporates visual models. It incorporates a Prompt Manager that manages\nthe histories of various visual foundation models, enabling a comprehensive multi-\nmodal conversation experience. On the other hand, GPT-4 takes a different approach\n§https://openai.com/dall-e-3\n§https://www.midjourney.com\n37\nby accepting prompts that consist of both images and texts. This flexibility empow-\ners users to specify vision and language tasks by generating text outputs in response\nto arbitrarily interlaced text and image prompts. Microsoft has also proposed a series\nof Multimodal Foundation Models, including Kosmos-1 (Huang et al., 2023d) and\nKosmos-2 (Peng et al., 2023d). These models further contribute to the development\nof multimodal capabilities and facilitate rich interactions involving both images and\ntext. Furthermore, there are ongoing efforts to adapt GPT to specific domains, such as\nBiomedGPT (Zhang et al., 2023f), which focuses specifically on biomedical research.\nThese domain-specific adaptations aim to enhance the language model’s performance\nand applicability within specialized fields.\n3.7.3 Multimodal Understanding\nVisual Instruction Tuning (Liu et al., 2023e) presents a groundbreaking approach\nthat utilizes GPT-4 to generate multimodal language-image instruction-following data.\nThis approach has the potential to reduce the reliance on manual annotation of large\nmultimodal datasets. Expanding on this foundation, LLaVA (Large Language and\nVision Assistant) (Liu et al., 2023e) represents an extensively trained, large-scale mul-\ntimodal model. It seamlessly integrates a vision encoder with Vicuna (Chiang et al.,\n2023), facilitating versatile visual and language comprehension for general-purpose\napplications. LLaVA excels across a diverse spectrum of tasks necessitating multi-\nmodal understanding, encompassing visual question-answering, image captioning, and\ninstruction-following. Notably, it achieves impressive performance on Science QA (Lu\net al., 2022a), a multimodal reasoning dataset in the science domain.\nIn the domain of reasoning on charts, DePlot (Liu et al., 2023b) presents a few-\nshot solution for visual language reasoning. It tackles the challenge through a two-step\nprocess: first, translating the plot into text, and then performing reasoning over the\ntranslated text. The authors also investigate the combination of DePlot with LLMs\nto further enhance performance. MatCha (Math reasoning and Chart derendering\npretraining) (Liu et al., 2023c) introduces a comprehensive framework for visual lan-\nguage understanding in the chart domain. It highlights the importance of two critical\ncomponents: understanding layout, including number extraction and organization,\nand mathematical reasoning. To enhance visual language understanding, the authors\npropose two complementary pretraining tasks: chart derendering, which involves gen-\nerating the underlying data table or code used to create a given plot or chart, and\nmath reasoning.\nDetGPT (Pi et al., 2023) revolutionizes object detection through its reasoning-\nbased approach. It enables the automatic localization of objects of interest based on\nuser-expressed desires, even in cases where the object is not explicitly mentioned. This\ninnovative method incorporates reasoning capabilities to enhance the object detection\nprocess. Q-Bench (Wu et al., 2023c) demonstrates that the multimodal foundation\nmodels can perceive low-level visual attributes and provide image quality understand-\ning. LLaMA-VID (Li et al., 2023s) enhances LLMs for more efficient video and image\nunderstanding. It represents each video frame with two tokens, which decreases the\nburden of processing long videos without sacrificing essential information. To allow\n38\nusers to interactively control the focus of multimodal understanding, Prompt High-\nlighter (Zhang et al., 2023l) highlights specific prompt spans and effectively guides\nautoregressive generation to produce more targeted outputs.\nIntegrating diverse data types such as text, images, tables, and audio presents\ndistinct challenges for multimodal foundation models compared to their unimodal\ncounterparts. A primary obstacle lies in effectively merging these varied data formats,\na task complicated by issues like inconsistency and incompleteness in datasets, where\nmismatches between image content and corresponding descriptions, or missing data,\ncan adversely affect model performance. Additionally, multimodal foundation models\ntypically demand substantial computational resources for training. Exploring efficient\ntraining methods for these models thus emerges as a valuable area of research, crucial\nfor advancing the capabilities of multimodal AI systems. These multimodal foundation\nmodels are also instrumental in learning universal representations applicable to fields\nlike materials science, chemistry, and biology (Team, 2022; Manica et al., 2023).\n3.8 Agent Reasoning\nAgent reasoning, is an important capability for the Autonomous Language Agents,\nwhich refers to a cognitive process that integrates perception, action, and interaction\nwith the physical environment or simulated environment to support reasoning and\nproblem-solving. Autonomous Agents in the context of Large Language Models have\nthe ability to perform a wide range of tasks, such as task decomposition, generat-\ning code, answering questions, engaging in dialogue, providing recommendations, and\nmore. Autonomous Agents, often known as AI Agents, harness the power of Large\nLanguage Models to autonomously perform tasks, utilizing their extensive knowledge,\nreasoning skills, and vast informational resources (Alibali et al., 2014).\nSeveral works have investigated the use of language for planning purposes (Jansen,\n2020; Li et al., 2022e; Sharma et al., 2021; Zeng et al., 2023; Huang et al., 2022b; Ahn\net al., 2022; Mu et al., 2023; Hu et al., 2023a; Zhou et al., 2023a). Recent methods\nin task planning utilize pre-trained autoregressive foundation models to break down\nabstract, high-level instructions into executable, low-level step sequences for an agent,\napplying a zero-shot approach (Huang et al., 2022b; Ahn et al., 2022). Specifically,\nHuang et al. (2022b) prompt GPT-3 (Brown et al., 2020) and Codex (Chen et al.,\n2021b) to create actions for agents, where each action step is semantically converted\ninto a permissible action through Sentence-RoBERTa (Liu et al., 2019; Reimers and\nGurevych, 2019). In contrast, SayCan (Ahn et al., 2022) grounds the actions and\nlanguage by combining the probability of each candidate action, as determined by\nFLAN (Wei et al., 2021), with the action’s value function. The latter acts as a surro-\ngate for measuring affordance (Shah et al., 2021). However, both approaches assume\nthe successful execution of each proposed step by the agent, without considering poten-\ntial intermediate failures in dynamic environments or accounting for the performance\nof lower-level policies. SwiftSage (Lin et al., 2023b) is a framework influenced by the\ndual-process theory of human cognition, tailored for superior performance in action\nplanning within intricate interactive reasoning tasks. This framework is structured\naround two main components: the SWIFT module and the SAGE module. The SWIFT\n39\nmodule represents fast and intuitive thinking and is responsible for action planning\nbased on the oracle agent’s action trajectories. It is implemented as a small encoder-\ndecoder language model that has been fine-tuned specifically for this purpose. On\nthe other hand, the SAGE module emulates deliberate thought processes and utilizes\nLLMs such as GPT-4 for subgoal planning and grounding. This module leverages the\npower of language models to perform more sophisticated reasoning tasks within the\nframework. Another noteworthy approach in this regard is R easoning via Planning\n(RAP) (Hao et al., 2023a), which capitalizes on the language model’s dual role as\nboth a world model and a reasoning agent. RAP incorporates a well-founded plan-\nning algorithm, specifically based on Monte Carlo Tree Search, to facilitate strategic\nexploration within the expansive realm of reasoning. The effectiveness of RAP is eval-\nuated across various tasks, including plan generation, mathematical reasoning (e.g.,\nGSM8K (Cobbe et al., 2021)), and logical reasoning (e.g., PrOntoQA (Saparov and\nHe, 2023)). The evaluations demonstrate RAP’s proficiency in addressing diverse rea-\nsoning challenges, effectively showcasing its versatility as a capable reasoning agent.\nMimicPlay (Wang et al., 2023a) introduces a method for learning robotic policies from\nhuman play data, utilizing emergent human and video prompts to direct low-level\nvisuomotor control.\nIntrospective Reasoning, Extrospective Reasoning, Embodied Reasoning, and Mul-\ntiagent Reasoning, along with their interconnected aspects, play pivotal roles in the\nadvancement of agent reasoning systems (Qin et al., 2023). These components con-\ntribute to the development of higher-level cognitive abilities, such as self-awareness,\nadaptability, and effective collaboration. These capabilities are essential for the cre-\nation of intelligent systems that can successfully operate in complex and dynamic\nenvironments, seamlessly interact with humans, and engage in cooperative or com-\npetitive scenarios with other agents. We believe that combining foundational models\nwith classical methods in robotics may create new opportunities, such as integrating\nclassic approaches to perception (Chu et al., 2021), mapping (Pan et al., 2020), com-\npleting (Chu et al., 2023b), grasping (Li et al., 2021c), planning (Mao et al., 2023b),\ninteraction (Jiao et al., 2020), and control. Safety is a crucial aspect of embodied intel-\nligent systems. In this context, PlanCP (Sun et al., 2023b) suggests the application of\nconformal prediction to diffusion dynamic models.\n3.8.1 Introspective Reasoning\nIntrospective reasoning, illustrated in Figure 12(a), relies solely on internal knowl-\nedge and reasoning to generate a static plan of tool use without interacting with the\nenvironment (Leake, 2012). Several related works in the field of introspective reason-\ning with LLMs include Program-Aided Language Models (PAL) (Gao et al., 2023b),\nProgPrompt (Singh et al., 2023), and Code-as-Policies (Liang et al., 2022a).\nPAL (Gao et al., 2023b) utilizes an LLM for the comprehension of natural language\nproblems and the generation of intermediate reasoning steps in the form of executable\nprograms. Nonetheless, the actual execution of solution steps is delegated to a pro-\ngrammatic runtime, such as a Python interpreter. This approach enables PAL to\nharness the language understanding capabilities of the LLM while making use of a dis-\ntinct runtime environment for executing the generated programs. ProgPrompt (Singh\n40\nFoundation model\nSkill library\nupdate / retrieval\nEnvironment\nAgent\nexecute\n(b) Extrospective Reasoning\nfeedback\nmulti-time\nFoundation model\nSkill library\nupdate / retrieval\nEnvironment\nAgent\nexecute\n(a) Introspective Reasoning\nFig. 12 : Difference between introspective reasoning and extrospective reasoning.\nIntrospective reasoning does not require interaction with the environment, while extro-\nspective reasoning leverages observation and feedback from the external environment\nto adapt plans. The figure style credits from work (Qin et al., 2023).\net al., 2023) presents a structured LLM prompt akin to programming, crafted to\nfacilitate the generation of plans in diverse situational settings, encompassing differ-\nent robot functionalities and tasks. This structure involves prompting the LLM with\nprogram-style descriptions of accessible actions and objects in a given environment,\nalong with sample programs for execution. Code-as-Policies (Liang et al., 2022a) intro-\nduces a robot-oriented framework for Language Model Generated Programs (LMPs).\nThese LMPs are capable of depicting both reactive policies, like impedance controllers,\nand waypoint-oriented strategies. The versatility of Code-as-Policies is demonstrated\nacross multiple real robot platforms, showcasing its applicability in diverse robotic\nscenarios.\nIntrospective reasoning may have limitations in dynamic and uncertain environ-\nments where external feedback and interaction with the environment are crucial for\neffective planning. It may struggle to adapt plans to changing circumstances or handle\nunexpected events without external information.\n3.8.2 Extrospective Reasoning\nIntrospective reasoning, despite its simplicity, lacks the ability to adjust or modify a\nplan based on intermediate execution results. In contrast, extrospective reasoning oper-\nates by generating plans incrementally. As shown in Figure 12(b), it accomplishes this\nby iteratively interacting with the environment and incorporating feedback obtained\nfrom previous executions. Extrospective reasoning actively incorporates external infor-\nmation gathered through interactions with the environment. This allows extrospective\nreasoning to adapt and refine its plans based on real-time feedback and the observed\noutcomes of previous actions (Acay et al., 2007).\nBy actively engaging with the environment and utilizing feedback, extrospective\nreasoning offers a more flexible and responsive approach to generating plans, which is\nparticularly suitable for complex and dynamic situations where the ability to adapt\n41\nand learn from experience is crucial. Several related works in the field of extrospective\nreasoning with LLMs include Self-Ask (Press et al., 2023), ReAct (Yao et al., 2023c),\nToolFormer (Schick et al., 2023), and LLM-Planner (Song et al., 2023a). Self-Ask\n(Press et al., 2023) proactively generates and responds to its own follow-up queries\nbefore addressing the original question. Meanwhile, ReAct (Yao et al., 2023c) lever-\nages large language models to concurrently produce reasoning traces and task-specific\nactions. This dual approach enhances the interaction between these elements, with\nreasoning traces aiding in the development, monitoring, and modification of action\nplans, as well as managing unexpected situations. Conversely, actions facilitate the\nmodel’s engagement with and acquisition of supplementary data from external enti-\nties like knowledge bases or environments. ToolFormer (Schick et al., 2023) is designed\nto intelligently determine the appropriate APIs to utilize, the timing for their invoca-\ntion, the specific arguments to provide, and how to effectively integrate the obtained\nresults into subsequent token predictions. LLM-Planner (Song et al., 2023a) utilizes\nthe capabilities of large language models for efficient few-shot planning in the context\nof embodied agents.\nIn addition to the above-mentioned research, Statler (Yoneda et al., 2023) provides\na framework equipping LLMs with a persistent, memory-like representation of the\nworld state. It utilizes two forms of general LLMs: a world-model reader and a world-\nmodel writer, both of which interact with and update the world state. This addition\nof a memory-like element to the framework significantly boosts the reasoning abilities\nof LLMs, allowing them to process information over extended time periods, free from\nthe constraints typically imposed by context length limitations. The explicit represen-\ntation of the world state empowers LLMs to retain and access relevant information,\nfacilitating more comprehensive and contextually aware reasoning processes. Dasgupta\net al. (2022) propose a collaborative system that combines the complementary reason-\ning abilities of LLMs. The system has three components: the Planner, the Actor, and\nthe Reporter. The Planner is a pre-trained language model responsible for generating\ncommands that guide the actions of a simple embodied agent, referred to as the Actor.\nThe Reporter acts as a communication bridge between the Planner and the Actor,\nrelaying relevant information to the Planner to inform its decision-making process for\nissuing subsequent commands. By harnessing the strengths of each component, this\ncollaborative system aims to enhance the overall reasoning and decision-making capa-\nbilities of LLMs, allowing for more effective and context-aware interactions between\nlanguage-based instructions and the embodied agent. Inner Monologue (Huang et al.,\n2022c) investigates the capacity of LLMs to reason effectively in embodied contexts by\nleveraging natural language feedback without additional training. The authors propose\nthat by incorporating environmental feedback, LLMs can develop an inner monologue\nthat augments their capability to process and plan within robotic control scenarios.\nThis development enables LLMs to gain a more comprehensive understanding of the\nenvironment and enhances their adaptability to dynamic circumstances.\nThe iterative nature of extrospective reasoning enables it to dynamically adjust\nits plan based on the evolving state of the environment and the outcomes of executed\nactions. This adaptive process enhances the effectiveness and efficiency of planning,\n42\nas it leverages the knowledge gained from experience to continually improve future\ndecision-making.\n3.8.3 Embodied Reasoning\nRecent research has highlighted the successful application of LLMs in robotics\ndomains (Ahn et al., 2022; Zeng et al., 2023; Huang et al., 2022c; Liang et al., 2022a;\nDing et al., 2023c). Moreover, planning can be considered a form of temporal reason-\ning, adding to the significance of integrating LLMs into robotics. Gato (Reed et al.,\n2022) functions as a multimodal, multi-task, and multi-embodiment generalist pol-\nicy. It leverages supervised learning with an impressive parameter count of 1.2 billion.\nThis technology has been acknowledged as a form of “general-purpose” artificial intel-\nligence, representing a significant advancement towards the realization of artificial\ngeneral intelligence. Robotic Transformer 1 (RT-1) (Brohan et al., 2022) is trained on\na comprehensive real-world robotics dataset consisting of over 130,000 episodes that\nencompass more than 700 tasks. This extensive dataset was collected over a period\nof 17 months using a fleet of 13 robots from Everyday Robots. RT-1 demonstrates\npromising properties as a scalable, pre-trained model, showcasing its ability to gener-\nalize based on factors such as data size, model size, and data diversity. The utilization\nof large-scale data collected from real robots engaged in real-world tasks contributes to\nRT-1’s robustness and its potential for generalization in practical scenarios. Expand-\ning upon the capabilities of RT-1, Robotic Transformer 2 (RT-2) (Brohan et al., 2023)\nfurther enhances the model’s understanding of the world, resulting in more efficient\nand accurate execution of robotic tasks. By incorporating the chain of thought rea-\nsoning, RT-2 achieves multi-stage semantic reasoning abilities. This expansion equips\nRT-2 with a set of emerging capabilities derived from extensive training on a vast\ninternet-scale dataset. Prominent advancements encompass a marked improvement in\nthe model’s ability to generalize to unfamiliar objects, the capacity to understand\ncommands absent from its original training data, and the capability to engage in\nbasic reasoning when responding to user instructions. These enhancements enhance\nRT-2’s performance and broaden its capacity to tackle a more extensive array of\ntasks with increased sophistication. After that, RT-X (Padalkar et al., 2023) further\nextends RT-1 and RT-2 to cross-embodiment settings and shows better transferabili-\nties and zero-shot capabilities. RoboFlamingo (Li et al., 2023m) leverages pre-trained\nVision-Language Models (VLMs) to achieve sophisticated single-step vision-language\ncomprehension. It incorporates an explicit policy head to effectively capture sequential\nhistorical data. This design grants it the flexibility needed for implementing open-loop\ncontrol strategies and is finely tuned for efficient deployment on resource-constrained\nplatforms.\nEmbodied reasoning plays a vital role in the development of intelligent robots. As\nhumans, we are educated to comprehend the world by employing numerical/physical\nlaws and logical principles. The question arises: can we empower robots with the\nsame capacity? Numerous everyday tasks necessitate simple reasoning based on visual\nperception and natural language understanding. If we aspire to have robot companions\ncapable of collaborating with us, it is essential for them to possess the ability to\nunderstand and reason over both visual information and natural language input. The\n43\nInput\n(a) Single-agent Reasoning\nSingle-agent\nOutputInput\n(b) Multi-agent Reasoning\nMulti-agent\nOutput\ndebate\n…\nFig. 13: Difference between single-agent reasoning and multi-agent reasoning.\nultimate objective of creating smart robots is to enable them to act in a manner that\nis comparable to, or even surpasses, human capabilities (Xu et al., 2021b). This entails\nembodying human-like reasoning and performance in robots, aiming to bridge the gap\nbetween humans and machines. By enabling robots to understand and reason over\nvisual and linguistic inputs, we move closer to achieving the goal of developing robots\nthat can effectively interact and collaborate with humans.\n3.8.4 Multi-agent Reasoning\nMulti-agent reasoning refers to the cognitive process by which multiple autonomous\nagents or entities engage in reasoning, decision-making, and communication within\na shared environment or context. Compared with reasoning with a single agent, it\ninvolves the ability of individual agents to perceive, interpret, and reason about the\nactions, goals, beliefs, and intentions of other agents, and to adjust their own behaviors\naccordingly. Their differences are briefly summarized in Figure 13.\nRecent studies have introduced the concept of multi-agent debate as a promis-\ning method to elevate reasoning abilities and ensure factual accuracy across diverse\nscenarios. In the work by Zhang et al. (2023c), they introduce a framework that\nleverages the capabilities of Large Language Models (LLMs) to foster cooperative\ninteractions among multiple agents within embodied environments. This innovative\napproach empowers embodied agents to efficiently strategize, communicate, and col-\nlaborate with both other agents and humans, thereby enhancing their proficiency in\naccomplishing intricate, long-term tasks. In a similar vein, Du et al. (2023) propose a\nmethodology that involves multiple instances of language models engaging in debates.\nThrough iterative rounds of reasoning and response generation, these models collec-\ntively work towards reaching a common final answer. This approach has demonstrated\nsignificant improvements in mathematical and strategic reasoning across various tasks.\nIn contrast to the aforementioned studies, Nascimento et al. (2023) propose\nthe integration of LLMs, such as GPT-based technologies, into multi-agent systems\n(MASs). They introduce the concept of incorporating LLMs into MASs to create\nself-adjusting agents. This integration is achieved through an LLM-based MAPE-K\n(Monitoring, Analyzing, Planning, Executing, and Knowledge) model (do Nascimento\nand de Lucena, 2017; Redbooks, 2004), which enables the agents to adapt and adjust\ntheir behaviors based on the knowledge and insights gained from LLMs.\n44\nFederated Learning (FL) has gained prominence as a technology enabling the col-\nlaborative development of communal models while safeguarding data that remains\ndecentralized. Chen et al. (2023a) introduce the idea of a federated LLM, encompass-\ning three crucial elements: pre-training of federated LLMs, fine-tuning of these models,\nand the engineering of prompts specific to federated LLMs. This approach harnesses\nthe potential of federated learning to enhance multi-agent reasoning by leveraging\nLLMs.\nThese research efforts demonstrate the efficacy of multi-agent debate approaches\nin enhancing reasoning abilities and factual accuracy. By leveraging the power of\nlarge language models and enabling cooperative interactions between agents, these\nstudies contribute to the advancement of AI systems capable of complex reasoning\nand improved performance across various domains.\n3.8.5 Reasoning in Autonomous Driving\nReasoning within the domain of autonomous driving spans across perception (Li et al.,\n2023e,l; Sun et al., 2022b, 2023c), safety (Zhou et al., 2023c), explainability (Echter-\nhoff et al., 2023; Sha et al., 2023; Sun et al., 2021; Huang et al., 2021b) and system\nlevel (Chen et al., 2023f). Chen et al. (2023f) propose the frontiers and challenges for\nend-to-end autonomous driving, where logical reasoning with LLMs could have sub-\nstantial impacts on different driving scenarios. Zhou et al. (2023c) review some recent\nwork on LLMs with regards to driving. It suggests that by integrating language data,\nvehicles as well as transportation systems can carry out reasoning and interact with\nreal-world environments with a higher level of intelligence.\nWe believe that the common sense and world knowledge inherited from foundation\nmodels could unleash the substantial effectiveness of algorithms onboard to handle\ncorner cases and enhance explainability and safety. Below, we survey this emergent\ntopic from two perspectives.\nDriveGPT4 (Xu et al., 2023e) represents a groundbreaking endeavor that har-\nnesses LLMs to comprehend an interpretable end-to-end autonomous driving system.\nThis pioneering effort not only showcases remarkable qualitative but also quantitative\nachievements when benchmarked against challenging standards. GPT-Driver (Mao\net al., 2023a) and Agent Driver (Mao et al., 2023b) introduce the approach by utiliz-\ning LLMs as cognitive agents to operate a tool library. This implementation enhances\ndriving behavior by incorporating explainability into the decision-making process.\nMotionLM (Seff et al., 2023) cast multi-agent motion prediction as a language model-\ning task. The continuous trajectories are represented as sequences of discrete motion\ntokens. Among the many other attempts, one particular challenge is how to utilize\nlogical reasoning (e.g., chain of thought) to rationalize and explain driving behaviors.\nEchterhoff et al. (2023) propose a new view using the concept bottlenecks for control\ncommand predictions. Tan et al. (2023) turn to language as a source of supervision\nto obtain dynamic traffic scenarios, surpassing prior work in terms of realism and\nfidelity. nuPrompt (Wu et al., 2023b) is the first object-centric language prompt set\nfor 3D, multi-view, and multi-frame driving scenes. It is equipped with diverse pairs\nof instance-prompt data and validated in the object tracking task.\n45\n3.9 Other Tasks and Applications\n3.9.1 Theory of Mind (ToM)\nThe development of Theory of Mind (ToM)-like ability in models is speculated to have\noccurred naturally and independently as a consequence of their advancing language\nskills (Kosinski, 2023). Another explanation suggests that models were able to solve\nToM tasks by uncovering and utilizing undiscovered language patterns, rather than\nexplicitly employing ToM. While this alternative explanation may seem ordinary, it\nis actually remarkable as it implies the existence of undisclosed language regularities\nthat enable the resolution of ToM tasks without the direct engagement of ToM.\n3.9.2 Weather Forecasting\nWeather forecasting plays a crucial role in both scientific research and societal applica-\ntions. As an application of scientific reasoning, weather forecasting involves the use of\nreasoning skills to analyze data, identify patterns, and make predictions about future\nweather conditions.\nMetNet-2 (Espeholt et al., 2022) is a neural network specifically designed for high-\nresolution precipitation forecasting with up to a 12-hour lead time. This model excels\nin accurately predicting raw precipitation targets and outperforms state-of-the-art\nphysics-based models currently used in the Continental United States. In another\nstudy, Bi et al. (2023) present Pangu-Weather, an AI-based approach designed to\nachieve accurate global weather forecasts in the medium range. This method uti-\nlizes 3D deep networks that incorporate earth-specific priors, allowing for the effective\nhandling of complex weather data patterns. To mitigate accumulation errors encoun-\ntered in medium-range forecasting, a hierarchical temporal aggregation strategy is\nemployed. By undergoing training on an extensive dataset spanning 39 years of global\nweather information, Pangu-Weather exhibits exceptional deterministic forecasting\nperformance across all assessed variables when compared to the operational integrated\nforecasting system of the European Centre for Medium-Range Weather Forecasts\n(ECMWF). This underscores the remarkable effectiveness of Pangu-Weather in deliv-\nering precise global weather forecasts, offering valuable insights and advantages for a\nmultitude of applications that heavily depend on weather-related information.\n3.9.3 Medical Reasoning\nReasoning is also common in medicine. For example, clinicians reason the potential\ncauses of a patient’s symptoms and then advise which examinations to take and what\ntreatment is the best following the diagnosis (Qiu et al., 2023a).\nWith a wide medical knowledge spectrum, foundation models can conduct expert-\nlevel reasoning in the context of medicine. For example, Med PaLM 2 (Singhal et al.,\n2023), a biomedical large language model (LLM), scored 86.5% in answering medical\nquestions on the MedQA benchmark; GPT-4 passed the US Medical Licensing Exam\n46\n(USMLE) with a score of 86.7%. Breakthroughs in medical reasoning brought by\nLLMs also inspire reasoning carried out in other medical modalities, such as medical\nimages. For example, VisionFM (Qiu et al., 2023b), a foundation model for ophthalmic\nimage analysis, demonstrates impressive reasoning skills in predicting the presence\nof intracranial tumors from fundus photographs, surpassing both intermediate- and\nsenior-level clinicians. RETFound (Zhou et al., 2023d) shows remarkable performance\nin reasoning systemic diseases from ocular images. LLaVA-Med (Li et al., 2023b)\nadapts LLaVA (Liu et al., 2023e) to align biomedical vocabulary and learn open-ended\nconversational semantics, which enables the interpretation of biomedical images and\nachieves promising performance for biomedical visual question answering. ELIXR (Xu\net al., 2023d) incorporates a language-aligned image encoder to perform a range of\nvision-language reasoning tasks for chest X-ray images. Tu et al. (2023b) develop a\nmultimodal biomedical foundation model, Med-PaLM M, to simultaneously explore\nclinical language, imaging, and genomics data, as well as introduce a multimodal\nbiomedical benchmark, MultiMedBench. Given the multimodal nature of medicine,\nit is anticipated that medical reasoning will be further augmented by increasingly\nintelligent multimodal foundation models (Yang et al., 2023g).\nHowever, unlike in other domains, reasoning in medicine has to take more cau-\ntion (Yan et al., 2023). Rigorous verification and examination should be conducted to\nensure the biomedical reasoning outcome is factually grounded, and regulations should\nbe established and enforced to provide legitimate and safe use of foundation models\nfor biomedical reasoning.\n3.9.4 Bioinformatics Reasoning\nReasoning in bioinformatics involves analyzing and interpreting complex languages of\nbiology and gaining insights into the processes related to life. This includes under-\nstanding genetic sequences, protein functions, and cellular mechanisms through the\nanalysis of large-scale datasets. Foundation models are reshaping various perspectives\nfor biological reasoning, such as predicting protein structures and designing sequences\nin drug discovery (Savage, 2023).\nIn the field of biotechnology, numerous studies highlight the efficacy of foundation\nmodels in reasoning and analyzing DNA (Nguyen et al., 2023), RNA (Wang et al.,\n2023r), and protein (Jumper et al., 2021). A notable example is AlphaFold (Jumper\net al., 2021), which employs a transformer network architecture to precisely predict\nprotein structures. ProGen (Madani et al., 2023) and its subsequent ProGen2 (Nijkamp\net al., 2022b) develop a suite of large protein language models, akin to natural language\nmodels, for generating protein sequences. RFdifusion (Watson et al., 2023) adopts\na denoising diffusion approach in protein structure design, demonstrating significant\nadvancements across various protein design tasks. In the context of protein-ligand\ninteractions, Li et al. (2023r) train the GPT-2 model on protein-ligand binding data,\nutilizing language model capabilities for ligand design. Prot2Text (Abdine et al., 2023)\ncombines graph neural networks with LLMs to predict protein functions in a free-text\nformat. Chen et al. (2023d) introduce a framework powered by LLMs for condition\nrecommendation in chemical synthesis, which aids drug discovery. This framework is\ndesigned to search the most recent chemical literature, utilizing in-context learning\n47\ncapabilities and employing multi-LLM debate strategies to enhance effectiveness. For\nRNA analysis, Uni-RNA (Wang et al., 2023r) exhibits exceptional performance in\nstructural and functional predictions, including RNA high-order structure map predic-\ntion, by leveraging large-scale pre-training on extensive RNA sequences. Additionally,\nHyenaDNA (Nguyen et al., 2023) utilizes the long-range modeling and in-context\nlearning strengths of LLMs and is pre-trained on human reference genome data, yield-\ning significant achievements in genomic tasks. GeneGPT (Jin et al., 2023a) enhances\nLLMs by integrating the National Center for Biotechnology Information (NCBI) API,\nwhich improves answering questions related to genomics.\n3.9.5 Code Generation\nCode generation, also referred to as program synthesis or generating code from a natu-\nral language description (NL2Code) (Zan et al., 2023), is the process or technology that\nconverts inputs in natural language into computer code. NL2Code represents a signif-\nicant step towards more intuitive and accessible programming, leveraging foundation\nmodel to bridge the gap between natural language and computer code.\nPyMT5, delineated in the work of Clement et al. (2020), stands as a Python-based\ntext-to-text transfer transformer, adept at translating between diverse combinations\nof Python method features. This singular model is capable of generating entire meth-\nods from natural language documentation strings and summarizing code into various\ncommon docstring styles. Similarly, IntelliCode Compose (Svyatkovskiy et al., 2020)\nis a versatile multilingual code completion tool, proficient in predicting code token\nsequences and generating syntactically correct code lines. GPT-Neo (Black et al.,\n2021) exemplifies an implementation of GPT-2 and GPT-3-like models, with a focus\non distributed support through Mesh Tensorflow. This approach is further extended in\nGPT-J and GPT-NeoX-20B, as detailed in Wang and Komatsuzaki (2021) and Black\net al. (2022) respectively. PLBART (Ahmad et al., 2021) is a model pre-trained on\nan extensive corpus of Java and Python functions, coupled with natural language\ntext, utilizing a denoising autoencoding approach. CodeT5 (Wang et al., 2021b) dis-\ntinguishes itself as a unified pre-trained encoder-decoder Transformer, enhancing the\nsemantic understanding of developer-assigned identifiers. LaMDA (Thoppilan et al.,\n2022) emerges as a dialog-specialized family of Transformer-based models, pre-trained\non a substantial volume of dialog data and web text.\nCodeParrot (Tunstall et al., 2022) is a GPT-2 based model trained for Python code\ngeneration, while Codex (Chen et al., 2021b) showcases a GPT language model fine-\ntuned on a vast array of public code from GitHub. Chandel et al. (2022) delve into the\npracticality of a Data Science assistant empowered by a transformer model, JuPyT5,\ntrained on public Jupyter Notebook repositories, and introduce a new evaluation\nmetric, DSP. PolyCode (Xu et al., 2022) is a GPT-2 based model with substan-\ntial coding proficiency across multiple languages, trained on a large code dataset.\nAlphaCode (Li et al., 2022g) stands out as a code generation system, demonstrat-\ning notable performance in programming competitions. CodeRL (Le et al., 2022)\nmerges pre-trained language models with reinforcement learning for program synthe-\nsis. ERNIE-Code (Chai et al., 2022) employs unique pre-training methods, focusing\non both monolingual and cross-lingual learning. Pangu-Coder (Christopoulou et al.,\n48\n2022) adopts a two-stage training strategy, initially focusing on raw programming lan-\nguage data and subsequently on text-to-code generation. FIM (Bavarian et al., 2022)\ndemonstrates the efficacy of autoregressive language models in text infilling. Zan et al.\n(2022) introduce CERT, a model comprising a sketcher and generator for detailed\ncode creation, trained on unlabelled data. InCoder (Fried et al., 2022) focuses on code\nfile generation from a large, permissively licensed code corpus, enabling code infilling\nwith bidirectional context. Nijkamp et al. (2022a) present CodeGen, a family of large\nlanguage models for both natural language and programming, accompanied by the\nJAXFORMER training library.\nCodeGeeX (Zheng et al., 2023c) is a multilingual model for code generation, trained\non a vast dataset of programming languages. SantaCoder (Allal et al., 2023) is a model\nwith 1.1 billion parameters, trained on Java, JavaScript, and Python subsets from The\nStack (Kocetkov et al., 2022), and assessed using the MultiPL-E text-to-code bench-\nmark. This research revealed that intensifying the filtering of near-duplicates enhances\nperformance, and interestingly, choosing files from repositories with more than five\nGitHub stars tends to reduce performance significantly. In contrast, StarCoder (Li\net al., 2023k) is a more robust model with 15.5 billion parameters and an 8K con-\ntext length. It boasts infilling capabilities and rapid large-batch inference, enabled by\nmulti-query attention, and is trained on a vast dataset of one trillion tokens from The\nStack (Kocetkov et al., 2022). WizardCoder (Luo et al., 2023f) enhances Code LLMs\nwith intricate instruction fine-tuning, adapting the Evol-Instruct method for the code\ndomain. AceCoder (Li et al., 2023h) incorporates two innovative solutions to address\ncoding challenges: firstly, it employs guided code generation, prompting LLMs to ini-\ntially analyze requirements and produce preliminary outputs like test cases; secondly,\nit features example retrieval, selecting similar programs as prompt examples to provide\nrelevant content such as algorithms and APIs. CodeGen2 (Nijkamp et al., 2023) aims\nto make the training of LLMs for program synthesis more efficient by integrating four\nessential elements: model architectures, learning methods, infill sampling, and data\ndistributions. CodeT5+ (Wang et al., 2023u) forms a family of encoder-decoder LLMs\nfor code, characterized by flexible module combinations to address a broad spectrum\nof downstream code tasks. CodeTF (Bui et al., 2023) is an open-source Transformer-\nbased library dedicated to cutting-edge Code LLMs and code intelligence applications.\nCode Llama (Roziere et al., 2023) represents a family of large language models for\ncode, based on Llama 2, and offers top-tier performance among open models, along\nwith infilling capabilities, support for large input contexts, and the ability to follow\ninstructions in a zero-shot manner for programming tasks. CodeFuse (Di et al., 2023)\nis tailor-made for code-related tasks and is unique in its support for both English and\nChinese prompts, accommodating over 40 programming languages.\n3.9.6 Long-Chain Reasoning\nLong-chain reasoning refers to the ability to connect and reason about a series of\nmultiple, often complex pieces of information or events in a long sequential, and\nextended manner. Long-chain reasoning is often required in complex problem-solving,\ndecision-making, and understanding of intricate systems.\n49\nHo et al. (2022) introduce Fine-tune-CoT, a method that leverages very large\nteacher models to generate reasoning samples for fine-tuning smaller models. By\nemploying Fine-tune-CoT, smaller models acquire significant reasoning capabilities,\nsurpassing prompt-based baselines and even outperforming the teacher model in\nnumerous tasks.\nBefore the emergence of foundational models, the reasoning capabilities of earlier\nmodels were notably limited (Sun et al., 2022a). This limitation primarily stemmed\nfrom the tendency of learning-based models to rapidly forget previous information.\nLong-chain reasoning has great potential for application in AI Agent Reasoning or\nEmbodied Reasoning, enabling them to handle more intricate and nuanced tasks.\nDespite the emergence of foundational models like GPT-4, mastering long-chain rea-\nsoning continues to be a significant challenge. We emphasize the immense utility of\nlong-chain reasoning in applications such as decision-making, planning, and question-\nanswering. With this in mind, we aim to draw attention to this area, encouraging\nresearchers in foundational models to further investigate and advance in this field.\n3.9.7 Abstract Reasoning\nAbstract reasoning refers to the cognitive ability to analyze and manipulate abstract\nconcepts, ideas, or symbols without relying on specific contexts or concrete examples.\nIt involves transcending immediate sensory input and specific instances to identify\nunderlying patterns, relationships, and fundamental principles. Abstract reasoning\nrequires the identification and application of general patterns based on limited data.\nGendron et al. (2023) extensively evaluate state-of-the-art LLMs in abstract rea-\nsoning tasks. Their research reveals that these models demonstrate notably limited\nperformance compared to their performance on other natural language tasks. The find-\nings suggest that LLMs face challenges when it comes to effectively tackling abstract\nreasoning, highlighting the need for further advancements in this area.\n3.9.8 Defeasible Reasoning\nDefeasible reasoning refers to a mode of reasoning in which conclusions can be\noverturned or revised based on new evidence or information (Madaan et al., 2021).\nCURIOUS (Madaan et al., 2021) is a framework that supports defeasible reasoning for\nhumans, utilizing an inference graph (Pollock, 2009). In the context of defeasible infer-\nence, Rudinger et al. (2020) have provided three noteworthy datasets: δ-ATOMIC,\nδ-SNLI, and δ-SOCIAL. These datasets exhibit diversity by covering different domains,\noffering unique challenges for studying defeasible inference. δ-ATOMIC pertains to\ncommonsense reasoning, presenting scenarios that require drawing defeasible infer-\nences based on background knowledge and understanding of everyday situations.\nδ-SNLI focuses on natural language inference, requiring reasoning about the relation-\nships between premises and hypotheses. δ-SOCIAL involves reasoning about social\nnorms and conventions, providing a platform for investigating the application of defea-\nsible reasoning in understanding and interpreting social behavior. Zhou et al. (2020)\nintroduce a testbed aimed at evaluating models’ abilities to simulate human cogni-\ntive processes such as knowledge abstraction, concretization, and completion (KACC).\n50\nDataset Choices Knowledge Types Questions\nSwag Zellers et al. (2018) 4 Temporal, Physical 113,000\nPHYRE Bakhtin et al. (2019) / Physical 25\nHellaSwag Zellers et al. (2019) 4 Temporal, Physical 70,000\nWinoGrande Sakaguchi et al. (2021) 2 Social, Physical 44,000\nSocial IQA Sap et al. (2019) 3 Social 35,350\nPIQA Bisk et al. (2020) 2 Physical 21,020\nSummEdits Laban et al. (2023) 2 Social 6,348\nCConS Kondo et al. (2023) / Physical 1,112\nTable 5 : Commonsense Reasoning Benchmark Statistics. Choices: the number of\nchoices for each question; Questions: the number of questions.\nThese cognitive abilities play a crucial role in understanding the world and effec-\ntively managing acquired knowledge. The testbed includes new datasets characterized\nby larger concept graphs, ample cross-view links, and dense entity graphs, provid-\ning a more comprehensive representation of knowledge. Within this experimental\nframework, the authors introduce innovative challenges, specifically multi-hop knowl-\nedge abstraction (MKA) and multi-hop knowledge concretization (MKC). These tasks\nnecessitate intricate reasoning capabilities from models, involving the abstraction or\nconcretization of knowledge across multiple sequential steps. Kazemi et al. (2023)\nframe the problem of reasoning with contradictory information, guided by source pref-\nerences, as a classical problem of defeasible reasoning. This formulation allows for a\ncomprehensive exploration of models’ abilities to handle conflicting information and\nprioritize different sources in the reasoning process. BoardgameQA (Kazemi et al.,\n2023) is a dataset designed to assess the defeasible reasoning capabilities of models.\nThe dataset consists of 1000 training examples, 500 validation examples, and 1000\ntesting examples for each variation.\nEach of these datasets presents distinct challenges and opportunities for studying\nand advancing defeasible inference within various domains. Researchers can leverage\nthese datasets to explore the capabilities and limitations of defeasible reasoning mod-\nels in different contexts, contributing to the development of robust and adaptable\nreasoning systems with foundation model technologies.\n3.10 Benchmarks, Datasets, and Metrics\nBenchmarks, datasets, and metrics play a crucial role in evaluating and advanc-\ning reasoning capabilities in various domains, driving innovation, and fostering the\ndevelopment of more capable and reliable reasoning systems. These resources provide\nstandardized frameworks and tasks that enable researchers and developers to objec-\ntively assess the performance of reasoning models and compare different approaches.\nRepresentative datasets are summarized in Table 8 and 9.\n51\n3.10.1 Commensense Reasoning\nIn addition to CQA (Talmor et al., 2019) and CoS-E (Rajani et al., 2019), there are\nseveral other benchmarks available for evaluating commonsense reasoning (Table 5):\nPHYRE (PHYsical REasoning) benchmark (Bakhtin et al., 2019) consists of 25 task\ntemplates that focus on physical reasoning. CConS (Counter-commonsense Contextual\nSize comparison) dataset (Kondo et al., 2023) investigates the impact of physical com-\nmonsense on the contextualized size comparison task. It includes both contexts that\nalign with physical commonsense and those that deviate from it. The dataset com-\nprises 139 templates and automatically generates 1,112 examples. SummEdits (Laban\net al., 2023) is a benchmark spanning 10 domains. It is designed to be more cost-\neffective per sample compared to previous benchmarks, offering a 20-fold improvement\nin efficiency. The benchmark is highly reproducible and aims to evaluate the perfor-\nmance of Language Model-based Systems (LLMs) on complex tasks, addressing issues\nwith existing evaluation benchmarks.\nFurthermore, commonsense knowledge encompasses various categories, including\nphysical commonsense, social commonsense, and temporal commonsense. Benchmarks\nin this domain generally fall into two tasks: multiple-choice evaluation and generative\nevaluation. Multiple-choice benchmarks, such as SWAG (Zellers et al., 2018), Hel-\nlaSWAG (Zellers et al., 2019), Social IQA (Sap et al., 2019), and PIQA (Bisk et al.,\n2020), require models to select the correct answer from a set of options. Generative\nevaluation (Lin et al., 2020a), as seen in benchmarks like ProtoQA (Boratko et al.,\n2020) and CommonGen (Lin et al., 2020b), involves generating answers based on pro-\nvided questions and context. Rainbow (Lourie et al., 2021) is a universal commonsense\nreasoning benchmark that integrates six existing tasks: 1) αNLI (Bhagavatula et al.,\n2019); 2) Cosmos QA (Huang et al., 2019); 3) HellaSWAG (Zellers et al., 2019); 4)\nPIQA (Bisk et al., 2020); 5) Social IQA (Sap et al., 2019); and 6) WinoGrande (Sak-\naguchi et al., 2021). It covers both social and physical commonsense reasoning and\nprovides a comprehensive evaluation platform.\nMetrics\nIn multiple-choice benchmarks, accuracy is the primary metric used to evaluate a\nmodel’s ability to select the correct answer. However, in language generation evalu-\nations, automated metrics like BLEU (Papineni et al., 2002) may not always align\nperfectly with human judgment, so they should be used with caution.\nIn the case of the PHYRE benchmark (Bakhtin et al., 2019), a metric measuring\nperformance called AUCCESS is computed. AUCCESS aggregates the success percent-\nages across different attempts by using a weighted average. The formula for AUCCESS\nis AUCCESS = P\nk wk · sk/ P\nk wk. Here, wk represents weights that place more\nemphasis on tasks with fewer attempts, and it is calculated aswk = log(k+1) −log(k).\nThe variable sk denotes the success percentage at the k-th attempt. AUCCESS\ntakes into account the performance across multiple attempts and provides a more\ncomprehensive evaluation that rewards models for solving tasks with fewer attempts.\n52\n3.10.2 Mathematical Reasoning\nMath Word Problems (MWPs)\nThere have been several benchmark datasets introduced for math word problem-\nsolving. One such dataset is Alg514 (Kushman et al., 2014), which is also used by\n(Zhou et al., 2015) for evaluation. Alg514 consists of 514 algebra word problems\nsourced from online platforms. Each problem in the dataset is annotated with linear\nequations, and the template of each problem must appear at least six times within the\nentire set. Another dataset, Verb395 (Hosseini et al., 2014) is a collection of addition\nand subtraction problems. The DRAW dataset (Upadhyay and Chang, 2015) features\n1,000 algebraic word problems, each accompanied by linear equation annotations, col-\nlected from algebra.com. Meanwhile, SingleEQ (Koncel-Kedziorski et al., 2015) is\ncomprised of 508 problems, with each problem corresponding to a single equation.\nMaWPS (Koncel-Kedziorski et al., 2016) repository provides interfaces for adding new\nword problems, which allows for the further extension of the dataset. These bench-\nmark datasets cover various levels of difficulty and are useful for evaluating math\nword problem-solving approaches. Dolphin18K (Huang et al., 2016) consists of over\n18,000 annotated math word problems in the field of elementary mathematics. The\ndataset includes both the unedited text of the problem and either a single or multi-\nple pieces of response text supplied by the individuals who answered the problems.\nMATH (Hendrycks et al., 2021b) comprises 12,500 challenging competition mathe-\nmatics problems. Each problem in this dataset is accompanied by a full step-by-step\nsolution. This rich annotated information allows models to be trained to generate\ndetailed answer derivations and explanations. TabMWP (Lu et al., 2022b) features a\ncollection of 38,431 grade-level, open-domain problems that necessitate mathematical\nreasoning through both text and tables. This dataset is divided into training, dev, and\ntesting subsets, following a 6:2:2 distribution. In TabMWP, every query is associated\nwith a tabular context displayed as an image, semi-structured text, and a structured\ntable. The average length of these questions is 22.1 words, with the solutions averag-\ning 49.5 words. The problems in TabMWP can be of two types: free-text questions\nand multiple-choice questions. Every problem comes with annotated gold-standard\nsolutions that illustrate the multi-step reasoning involved.\nGSM8K (Cobbe et al., 2021) is a math word problem dataset that consists of 8.5K\ngrade school math word problems. These problems exhibit varying levels of linguistic\ncomplexity and difficulty. With problem lengths ranging from 2 to 8 steps, they require\na diverse set of mathematical skills and strategies to solve effectively. Multilingual\nGrade School Math (MGSM) benchmark (Shi et al., 2023) consists of 250 grade-school\nmath problems that have been manually translated from the GSM8K dataset (Cobbe\net al., 2021) into ten languages with diverse linguistic typologies. The MGSM bench-\nmark serves as an evaluation tool to assess the reasoning abilities of language models\nacross multiple languages. It helps to identify areas where models may face challenges,\nsuch as cross-lingual reasoning and handling linguistic variations between languages.\nBy incorporating typologically diverse languages, the benchmark ensures its relevance\nand applicability to real-world multilingual scenarios.\nThere are two Chinese datasets, Math23K (Wang et al., 2017) and HMWP (Qin\net al., 2020), specifically designed for math word problems at the elementary school\n53\nlevel. Math23K (Wang et al., 2017) consists of 23,161 problems that are annotated\nwith structured equations and corresponding answers. The Hybrid Math Word Prob-\nlems dataset (HMWP) (Qin et al., 2020) includes three types of math word problems\nextracted from a Chinese K12 math word problem bank. The dataset comprises\n5,491 math word problems, categorized as follows: 2,955 one-unknown-variable lin-\near MWPs, 1,636 two-unknown-variable linear MWPs, and 900 one-unknown-variable\nnon-linear MWPs. Additionally, there is the DRAW1K dataset (Upadhyay and Chang,\n2017), which contains 1,000 general algebra word problems. This dataset includes\nhuman-annotated derivations, which serve as information structures for problem-\nsolving. The authors have also provided derivation annotations for over 2,300 algebraic\nword problems to facilitate future evaluations. They suggest evaluating solvers based\non “derivation accuracy”. Math23K-F and MAWPS-F (Liu et al., 2023g) are datasets\nthat provide high-quality, precise annotations of formula usage in each reasoning step\nfor Math Word Problems. These datasets aim to enhance the understanding of how\nformulas are utilized throughout the problem-solving process. In conjunction with\nthese datasets, the authors propose the Formulamastered Solver (FOMAS) system (Liu\net al., 2023g), which incorporates insights from the dual process theory and consists\nof two components: the Knowledge System and the Reasoning System. The former\nis responsible for learning and acquiring formula knowledge, while the latter lever-\nages this knowledge to solve math word problems. This dual-component architecture\nenables FOMAS to leverage formula knowledge in the reasoning process effectively.\nThe Academia Sinica Diverse MWP Dataset (ASDiv) (Miao et al., 2020) consists of\n2,305 English math word problems (MWPs). The Math Word Problems (MWPs) in\nthis dataset display a diverse range of textual patterns, encompassing the majority\nof problem types that are typically introduced in elementary education. Additionally,\nevery problem within the collection is meticulously categorized based on its type and\neducational grade level, providing a clear indication of its difficulty level.\nExisting MWP corpora can be categorized into four main groups: (1) Number\nWord Problem corpora, which contain problems related to numbers exclusively; (2)\nArithmetic Word Problem corpora, which involve the four basic arithmetic operations\nand can be either single-step or multi-step problems; (3) Algebraic Word Problem\ncorpora, which focus on algebraic MWPs; and (4) Mixed-type MWP corpora, which\nare large-scale collections of MWPs from daily algebra or GRE/GMAT examinations.\nSVAMP (Patel et al., 2021), is a collection of 1,000 math word problems (MWPs),\ncreated by introducing variations to initial examples from the ASDiv-A dataset. This\ncompilation features 26 distinct equation models, with each problem incorporating an\naverage of 1.24 operations. Although SVAMP’s Corpus Lexicon Diversity (CLD) (Miao\net al., 2020), falls short when compared to ASDiv-A, it presents a higher level of diffi-\nculty. The creators of SVAMP challenge the notion that lexical diversity is a definitive\nmeasure of quality in MWP datasets. SVAMP’s target audience is students at the\nelementary school level.\nGeometry Problem Solving\nGeoS (Seo et al., 2015) comprises 186 shaded area problems in geometry. This\ndataset combines text understanding and diagram interpretation. In contrast,\n54\nGeoShader (Alvin et al., 2017) is a smaller dataset containing 102 shaded area prob-\nlems. These problems are sourced from standard mathematics textbooks from the\nUnited States and released exams from the Indian Class X examination. Another\nbenchmark, GEOS++ (Sachan et al., 2017) includes 1,406 questions mirroring the\nstyle of SAT exams, covering content from grades 6 through 10. This dataset is seg-\nmented into training (350 questions), development (150 questions), and testing (906\nquestions) subsets, ensuring a balanced representation of questions from each grade\nlevel. The authors provide ground-truth logical forms for the 500 annotated questions\nin the training and development sets. Similarly, GEOS-OS (Sachan and Xing, 2017)\ncomprises 2,235 geometry problems with demonstrations sourced from a set of grade\n6-10 Indian high school math textbooks. As a numerical reasoning benchmark that\nincorporates multi-modality, GeoQA (Chen et al., 2021a) stands out. GeoQA includes\n4,998 geometric problems, each accompanied by annotated programs. Notably, GeoQA\nsurpasses previous benchmarks such as GeoS (Seo et al., 2015) and GEOS++ (Sachan\net al., 2017) in terms of size and diversity.\nUniGeo (Chen et al., 2022b) is a comprehensive and large-scale benchmark for\ngeometry problems. It includes 4,998 calculation problems sourced from GeoQA (Chen\net al., 2021a), along with an additional 9,543 proving problems. The proving problems\nare split into train, validation, and test sets in the proportion of 7.0:1.5:1.5. Each\nproblem is labeled with reasons and mathematical expressions in a way of constituting\na multi-step proof. To evaluate model performance, the authors define five sub-tasks:\nParallel, Triangle, Quadrangle, Congruent, and Similarity, providing detailed insights\ninto model capabilities. Geometry3K (Lu et al., 2021a), on the other hand, consists of\n3,002 multiple-choice geometry problems with dense annotations in formal language.\nThis dataset encompasses a wide range of geometric shapes and objectives, featuring\nSAT-like problems sourced from two high-school textbooks.\nMath Question Answering Datasets\nThe AQuA dataset (Ling et al., 2017) comprises 100,000 samples of questions, answers,\nand rationales, with a focus on program induction through rationale generation. Each\nquestion in the dataset is divided into four parts: the problem description (the “ques-\ntion”), the answer “options” (in a multiple-choice format), the “rationale” description\nused to arrive at the correct answer, and the label indicating the “correct option”.\nMathQA (Amini et al., 2019), an extension of the AQuA dataset (Ling et al., 2017),\nenhances it by including fully-specified operational programs. MathQA contains 37,000\nEnglish multiple-choice math word problems spanning various mathematical domains.\nThe dataset is randomly divided into training, development, and test sets using an\n80/12/8% split ratio. Specifically tailored for LLMs, Advanced Reasoning Bench-\nmark (ARB) (Sawada et al., 2023) is designed to provide more challenging problems\nin advanced reasoning across multiple fields. ARB includes problems from diverse\ndomains such as mathematics, physics, biology, chemistry, and law. Its purpose is\nto serve as a benchmark that surpasses the difficulty levels of previous benchmarks,\npushing the boundaries of advanced reasoning tasks.\nIconQA (Lu et al., 2021b), is an expansive question-answering dataset featur-\ning 107,439 questions. It includes three distinct sub-tasks: choosing from multiple\n55\nimages, selecting from various text options, and completing blank spaces in sentences.\nThe dataset is split into train, validation, and test sets at the proportion of 6:2:2.\nIcon645 (Lu et al., 2021b) contains 645,687 colored icons belonging to 377 classes.\nThese icon-based question-answering pairs enable the evaluation of various reasoning\nskills, including visual reasoning and commonsense reasoning. MultiHiertt (Zhao et al.,\n2022b) is a dataset expertly annotated with 10,440 QA pairs, centered on questions\nand answers pertaining to Multi Hierarchical Tabular and Textual data. This dataset\nis derived from a variety of financial reports. Documents within MultiHiertt include\nseveral tables, mostly hierarchical, accompanied by substantial unstructured text. The\ncomplexity and challenge of the reasoning required for each question in MultiHiertt\nsurpass those in existing benchmarks. Detailed annotations of the reasoning steps and\nsupporting facts are included to highlight complex numerical reasoning.\nIn the realm of Textual QA datasets, DROP (D iscrete Reasoning Over the con-\ntent of P aragraphs) (Dua et al., 2019) comprises 96,567 questions curated from a\ndiverse range of Wikipedia categories, with a particular focus on sports game sum-\nmaries and historical narratives. It aims to support methods that combine distributed\nrepresentations with symbolic and discrete reasoning techniques.\nMoving on to Tabular QA datasets, WTQ (W iki T able Q uestions) or Wik-\niTableQA (Pasupat and Liang, 2015) consists of 22,033 complex question-answer pairs\nderived from 2,108 HTML tables extracted from Wikipedia. WTQ is tailored to\nsupport question-answering tasks specifically focused on semi-structured tables. Addi-\ntionally, WikiSQL (Zhong et al., 2018) concentrates on simple SQL queries and single\ntables. This dataset includes 80,654 meticulously annotated examples of questions\nand corresponding SQL queries. Spanning 24,241 tables from Wikipedia, WikiSQL\nstands out for its substantial scale, surpassing comparable datasets in size. Spider (Yu\net al., 2018) is tailored for complex, cross-domain semantic parsing and text-to-SQL\nchallenges. This dataset consists of 10,181 questions and 5,693 unique, intricate SQL\nqueries spread across 200 databases. These databases consist of multiple tables cov-\nering 138 different domains. Furthermore, Yu et al. (2018) propose a novel task for\nthe text-to-SQL problem using the Spider dataset. AIT-QA (A irline Industry Table\nQA) (Katsis et al., 2022) is tailored for complex and domain-specific Table QA tasks,\nwith a specific focus on the airline industry. The resulting test dataset comprises 515\nquestions generated from 116 tables. These tables are selected from the 10-K forms\nof 13 airlines, covering the years between 2017 and 2019. HiTab (Cheng et al., 2022)\nfocuses on question-answering (QA) and natural language generation (NLG) tasks\nspecifically tailored for hierarchical tables. This cross-domain dataset is constructed\nfrom a rich collection of statistical reports and Wikipedia pages, exhibiting unique\ncharacteristics: Firstly, the majority of tables in HiTab are hierarchical, adding com-\nplexity to the dataset. Secondly, the questions in HiTab are not generated from scratch\nby annotators but rather revised from real and meaningful sentences authored by ana-\nlysts. Lastly, to uncover intricate numerical reasoning in data analysis, fine-grained\nannotations of quantity and entity alignment are provided. The dataset consists of\n3,597 tables, divided into train (70%), dev (15%), and test (15%) sets with no overlap.\nFor Hybrid QA Dataset, HybridQA (Chen et al., 2020a) is a comprehensive\nand extensive question-answering dataset designed to challenge reasoning abilities on\n56\nheterogeneous information sources. This dataset encompasses approximately 70,000\nquestion-answering pairs that are aligned with 13,000 Wikipedia tables. It aims to eval-\nuate the ability to reason and extract information from diverse and varied data sources.\nFree917 (Cai and Yates, 2013) comprises 917 questions sourced from 81 domains within\nthe Freebase database. Freebase is an online, user-contributed, relational database\nthat covers a wide range of knowledge domains. The dataset involves 635 Freebase\nrelations, which have been annotated with lambda calculus forms. However, due to the\nrequirement for logical forms, scaling up the Free917 dataset becomes challenging as it\nnecessitates expertise in annotating logical forms. In contrast, WebQuestions (Berant\net al., 2013) contains question-answer pairs gathered from non-experts. It presents a\nhigher number of word types compared to datasets like ATIS (Hemphill et al., 1990),\nposing greater difficulties in lexical mapping. Nevertheless, WebQuestions exhibits\nsimpler structural complexity, with many questions consisting of a unary, a binary,\nand an entity. This dataset comprises 5,810 question-answer pairs. The questions\nwere collected using the Google Suggest API, while the answers were curated from\nFreebase with the assistance of Amazon MTurk. WebQuestionsSP (WebQSP) (Yih\net al., 2016) is derived from WebQuestions (Berant et al., 2013) that includes seman-\ntic parses for questions answerable using Freebase. It provides SPARQL queries for\n4,737 questions, making it possible to directly execute them on Freebase. WebQSP\nis larger in size compared to Free917 (Cai and Yates, 2013) and offers semantic\nparses in SPARQL format with standard Freebase entity identifiers. For evaluating\nreading comprehension (RC) and question-answering (QA) tasks, the dataset WebQ-\nComplex or ComplexWebQuestions (Talmor and Berant, 2018) proves valuable. It\nconsists of 34,689 complex examples of broad and intricate questions, accompanied\nby answers, web snippets, and SPARQL queries. The dataset automatically generates\nmore complex queries involving function composition, conjunctions, superlatives, and\ncomparatives. MetaQA (MoviE Text Audio QA) (Zhang et al., 2017) is a comprehen-\nsive dataset comprising over 400,000 questions designed for both single and multi-hop\nreasoning. It also provides more realistic versions in text and audio formats. MetaQA\nexpands upon WikiMovies (Miller et al., 2016) and serves as a comprehensive exten-\nsion to it. These datasets, WebQuestionsSP (Yih et al., 2016), WebQComplex (Talmor\nand Berant, 2018), and MetaQA (Zhang et al., 2017), offer valuable resources for var-\nious question answering and reasoning tasks, catering to different complexities and\ndomains.\nIn the realm of text-centric datasets featuring singular passages, the Stanford Ques-\ntion Answering Dataset (SQuAD) (Rajpurkar et al., 2016) stands out as a significant\nreading comprehension collection. It contains over 100,000 questions, all crafted by\ncrowdworkers using a range of Wikipedia articles. SQuAD uniquely pairs each question\nwith a specific reading passage, and the answer to each question is a segment extracted\ndirectly from that passage. The dataset encompasses a total of 107,785 question-answer\npairs across 536 articles. A distinctive feature of SQuAD is its absence of pre-defined\nanswer choices for the questions, unlike some other datasets in this category. Instead,\nsystems are required to select the answer from all possible spans within the passage,\nposing the challenge of dealing with a relatively large number of candidate answers.\n57\nFor open-domain text-only datasets, TriviaQA (Joshi et al., 2017) is another read-\ning comprehension dataset that includes over 650,000 question-answer-evidence triples.\nTriviaQA consists of 95,000 question-answer pairs contributed by trivia enthusiasts.\nFurthermore, for each question in the dataset, an average of six independent evi-\ndence documents are compiled, offering robust distant supervision that enhances the\nquality of question-answering support. This makes TriviaQA an invaluable tool for\nassessing the capabilities of systems to understand and respond to questions within\nan open-domain context. HotpotQA (Yang et al., 2018) is a dataset comprising\n113,000 question-answer pairs sourced from Wikipedia. It exhibits four key features:\nThe questions in HotpotQA necessitate the identification and reasoning based on\nvarious supporting documents to deduce answers. The dataset includes a broad spec-\ntrum of questions that are not confined to established knowledge bases or specific\nknowledge frameworks. HotpotQA offers crucial sentence-level supporting facts nec-\nessary for the reasoning process. This level of granularity enables QA systems to\nreason with robust supervision and provide explanations for their predictions. Addi-\ntionally, HotpotQA introduces a new type of factoid comparison question. These\nquestions evaluate the ability of QA systems to extract relevant facts and effectively\nperform necessary comparisons. By incorporating these four key features, HotpotQA\noffers a comprehensive and challenging dataset for evaluating QA systems’ capabili-\nties in multi-document reasoning, generalization, explanation generation, and factoid\ncomparison. Natural-QA (Kwiatkowski et al., 2019) is a question-answering dataset\ncomprising real anonymized queries that were aggregated from interactions with the\nGoogle search engine. Natural-QA includes a total of 307,373 training examples that\nare publicly available. For the development data, there are 7,830 examples that have\nbeen annotated with 5 possible answers. Additionally, the test data consists of another\n7,842 examples, also annotated with 5-way annotations.\nMultiModalQA (MMQA) (Talmor et al., 2021) is an intricate question-answering\ndataset designed to challenge models in joint reasoning across multiple modalities,\nincluding text, tables, and images. The dataset comprises 29,918 questions, and a\nnotable 35.7% of these questions require cross-modality reasoning. GeoTSQA (Li et al.,\n2021b) is a dataset that focuses on the tabular scenario-based question answering\n(TSQA) task within the domain of geography. It consists of 556 scenarios accompanied\nby 1,012 real multiple-choice questions that are contextualized within these tabular\nscenarios.\nTheoremQA (Chen et al., 2023h) is a question-answering dataset that revolves\naround the concept of theorems. It consists of 800 high-quality questions, which cover\n350 theorems spanning various disciplines such as Mathematics, Physics, Electrical\nEngineering and Computer Science (EE&CS), and Finance. TAT-QA (Zhu et al.,\n2021) serves as a question-answering benchmark specifically focused on the domain of\nfinance. The dataset evaluates the ability of models to answer questions based on both\ntables and text. The questions in TAT-QA often require numerical reasoning skills,\nsuch as performing arithmetic operations, counting, comparing or sorting values, and\ncombining multiple reasoning steps. The benchmark encompasses 16,552 questions\nassociated with 2,757 hybrid contexts derived from real-world financial reports. It\ncovers a wide range of finance-related topics and scenarios, including stock prices,\n58\nfinancial reports, bank transactions, and currency exchange rates. FinQA (Chen et al.,\n2021c) is a dataset specifically designed to facilitate numerical reasoning tasks with\nfinancial data. The dataset comprises 8,281 question-answer pairs that revolve around\nfinancial calculations. Importantly, each pair is accompanied by detailed reasoning\nsteps that provide insights into the process of arriving at the answer.\nMetrics\nGeoS++ (Sachan et al., 2017) employs Normalized Mutual Information (NMI) (Strehl\nand Ghosh, 2002) to assess the quality of axiom mention clustering. To measure the\nlexicon usage diversity of a given MWP corpus, Miao et al. (2020) introduced the use\nof BLEU (Papineni et al., 2002). They also proposed the Corpus Lexicon Diversity\n(CLD) metric to assess the lexical diversity of a given corpus (Miao et al., 2020). Cheng\net al. (2022) adopt Execution Accuracy (EA) as their evaluation metric. This approach\nfollows the methodology proposed by Pasupat and Liang (2015), which measures the\npercentage of samples with correct answers. To evaluate the performance of TSQA\nmodels on GeoTSQA, Li et al. (2021b) employ two standard information retrieval eval-\nuation metrics: Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).\nThese metrics provide quantitative measures of the models’ retrieval effectiveness and\nranking accuracy when answering questions based on the tabular scenarios.\n3.10.3 Logical Reasoning\nThere are four notable logical reasoning datasets: ProofWriter (Tafjord et al., 2021),\nPrOntoQA (Saparov and He, 2023), FOLIO (Han et al., 2022), and LogicalDeduction\nfrom BIG-Bench (Srivastava et al., 2023).\nProofWriter (Tafjord et al., 2021) builds upon the original RuleTaker D*\ndatasets (Clark et al., 2020) and introduces two additional variants. The closed-world\nassumption (CWA) variant addresses minor inconsistencies related to negation, while\nthe open-world assumption (OWA) variant incorporates an open-world assumption\nduring reasoning. The RuleTaker D* datasets (Clark et al., 2020) consist of five sub-\nsets (D0, D1, D2, D3, and D5), each containing 100k questions. PrOntoQA (Proof and\nOntology-Generated Question-Answering) (Saparov and He, 2023) contains examples\ngenerated from synthetic world models represented in first-order logic. FOLIO (Han\net al., 2022) is a human-annotated, open-domain dataset that covers a wide range\nof logical complexities and diversities. It provides first-order logic (FOL) annotations\nand consists of 1,435 examples. Additionally, the dataset includes 487 sets of premises\nthat serve as rules for deductive reasoning to evaluate the validity of conclusions. The\nLogicalDeduction task (Srivastava et al., 2023) within BIG-Bench serves as an eval-\nuation benchmark for assessing the ability to perform multi-step logical reasoning.\nThis task involves deducing the order of a sequence of objects based on a minimal set\nof given conditions. Each instance includes a naturally ordered context with three to\nseven similar objects, such as differently colored books on a shelf. Alongside the con-\ntext, a set of simple clues is provided. More relevant datasets and their corresponding\nstatistics are presented in Table 6.\nThese logical reasoning datasets contribute to the development and evaluation of\nmodels and systems that aim to enhance logical reasoning capabilities. They provide\n59\nDataset Train\nSize\nDev\nsize\nTest\nsize\nTask\nType Synthetic Type\nαNLI\nBhagavatula et al. (2019)\n169,654 - 1532 NLI % Abductive\nProofWriter\nTafjord et al. (2021)\n69,814 10,158 20,058 FV ! Deductive\nFOLIO\nHan et al. (2022)\n1,004 204 227 FV % Deductive\nLogicalDeduction\nSrivastava et al. (2023)\n- - 1300 FV % Deductive\nPrOntoQA\nSaparov and He (2023)\n- - 200 MCQA % Deductive\nTable 6: Logical Reasoning Benchmarks Luo et al. (2023d). There are three types of\ntasks: multiple choice question answer (MCQA); natural language inference (NLI);\nand fact verification (FV).\ndiverse scenarios and challenges, ranging from synthetic world models to real-world\ncontexts, enabling researchers to explore and advance logical reasoning in various\ndomains.\n3.10.4 Causal Reasoning\nThe T¨ ubingen cause-effect pairs dataset (Mooij et al., 2016) encompasses 108\ncause-effect pairs obtained from 37 datasets spanning diverse domains, including\nmeteorology, biology, medicine, engineering, and economics. This dataset serves as a\nbenchmark for evaluating causal reasoning abilities. In contrast, the Neuropathic Pain\ndataset (Tu et al., 2019) focuses on the relationships between nerves and the corre-\nsponding symptoms observed in patients. Due to its specialized medical terminology\nand domain-specific knowledge, interpreting the variable names within this dataset\nrequires expertise in the field. The Arctic sea ice dataset (Huang et al., 2021c) presents\na graph derived from domain knowledge, featuring 12 variables and 48 edges. It offers\nvaluable insights into the dynamics of Arctic sea ice.\nCounterfactual reasoning, even in the absence of actual causality, is a valuable\ncapability for language models. It aids in decision-making, planning, and uncovering\nhidden insights that may not be immediately apparent in the original context. CRASS\n(Counterfactual Reasoning Assessment) (Frohberg and Binder, 2021) is a benchmark\nspecifically developed to evaluate the proficiency of language models in dealing with\ncounterfactual queries. This benchmark comprises 275 instances in which the language\nmodel is presented with counterfactual conditional questions. In each instance, the\nmodel is tasked with selecting the most suitable response from a provided set of\nmultiple-choice options.\nIn evaluating the performance of language models on causal reasoning benchmarks\nand datasets like CRASS, the commonly used metric is top- k accuracy (Frohberg\nand Binder, 2021). This metric quantifies the model’s ability to make correct predic-\ntions by considering the top k ranked choices. It serves as a quantitative measure of\nthe model’s proficiency in causal reasoning tasks. Percentage of Preference (Li et al.,\n60\n2023g) is a metric used to evaluate logical completions in both counterfactual and fac-\ntual scenarios. This metric provides a quantitative measure of the extent to which a\nlanguage model’s generated completions align with human preferences and judgments\nin terms of logical consistency.\n3.10.5 Visual Reasoning\nIn order to establish a benchmark for grounded grammar induction, researchers have\ncurated a large-scale dataset known as PARTIT (Hong et al., 2021a). This dataset\nconsists of human-written sentences that provide detailed descriptions of part-level\nsemantics for 3D objects. PTR (Hong et al., 2021b) is an extensively curated dataset\ntailored for visual reasoning analysis. It includes around 70,000 synthetic RGB-D\nimages, each accompanied by detailed ground truth data on objects and part-level\nannotations. These annotations cover a range of aspects such as spatial and geometric\nrelationships, semantic instance segmentation, color attributes, and key physical prop-\nerties like stability. PTR is designed to facilitate research on part-based conceptual,\nrelational, and physical reasoning. Compositional Language and Elementary Visual\nReasoning (CLEVR) (Johnson et al., 2017) is a widely used diagnostic benchmark\nthat evaluates a wide array of visual reasoning abilities. It consists of 100,000 ren-\ndered images and around one million automatically generated questions, with 853,000\nunique questions. CLEVR offers a challenging set of images and questions designed\nto assess various aspects of visual reasoning, including tasks like counting, comparing,\nlogical reasoning, and memory retention. This dataset provides a robust platform for\ntesting and advancing visual reasoning algorithms and models, enabling researchers to\nexplore and enhance their capabilities in this field. Outside Knowledge Visual Ques-\ntion Answering (OK-VQA) (Marino et al., 2019) is a dataset specifically designed for\nvisual question-answering tasks that necessitate the utilization of external knowledge\nto generate accurate answers. The dataset comprises 14,055 open-ended questions,\neach associated with five ground truth answers. During the annotation process, the\nquestions were carefully filtered to ensure that they all required external knowledge,\nsuch as information from sources like Wikipedia. Additionally, efforts were made to\nmitigate dataset bias by reducing questions with frequently occurring answers. This\ndataset serves as a valuable resource for developing and evaluating methods that can\neffectively leverage external knowledge for visual question-answering tasks.\n3.10.6 Audio Reasoning\nThe most widely adopted benchmark datasets for different aspects of audio reason-\ning, i.e., the Speech processing Universal PERformance Benchmark (SUPERB) (Yang\net al., 2021) for discriminative tasks and the enhanced Speech processing Universal\nPERformance Benchmark (SUPERB-SG) (Tsai et al., 2022) for generative tasks, have\nbeen introduced in Section 3.6. The evaluation metrics for these tasks are listed in\nTable 7.\nThe availability of datasets in a wide variety of languages contributes to the success\nof self-supervised learning (SSL) of speech representations, which lays the crucial foun-\ndation for audio reasoning. One of the largest and most widely utilized speech corpora\nfor foundation model pre-training is the Libri-light (Kahn et al., 2020) dataset, which\n61\nTasks Cat. Evaluation Metric\nphone recognition discr. phone error rate (PER)\nautomatic speech recognition discr. word error rate (WER)\nkeyword spotting discr. accuracy (ACC)\nquery by example spoken term detectiondiscr. maximum term weighted value (MTWV)\nspeaker identification discr. accuracy (ACC)\nautomatic speaker verification discr. equal error rate (EER)\nspeaker diarization discr. diarization error rate (DER)\nintent classification discr. accuracy (ACC)\nslot filling discr. F1-score and character error rate (CER)\nemotion recognition discr. accuracy (ACC)\nvoice conversion gen. mel-cepstrum distortion (MCD)\nspeech separation gen. scale-invariant signal-to-distortion\nratio improvement (SI-SDRi)\nspeech enhancement gen. perceptual evaluation of speech quality (PESQ)\nshort time objective intelligibility (STOI)\nTable 7: Metrics of Audio Reasoning Tasks. Here “Cat.” denotes the category of the\ntasks. “discr.” and “gen.” stand for discriminative and generative tasks.\ncontains approximately 60,000 hours of speech in English originating from audiobooks.\nDidi Dictation and Didi Callcenter (Jiang et al., 2021b) are large-scale corpora in\nChinese, each containing roughly 10,000 hours of data collected from mobile dictation\napplications or phone calls. Apart from English and Chinese, multilingual corpora of\nsubstantial sizes are available as well, including VoxPopuli (Babu et al., 2022) (400,000\nhours, 23 languages), Multilingual LibriSpeech (Pratap et al., 2020) (50,000 hours,\n8 languages) and Common Voice (Ardila et al., 2020) (11,000 hours, 76 languages).\nSince there is typically no ground-truth transcription available, it is impractical to\nutilize these datasets to train the conventional hidden Markov models (HMMs) and\nthe supervised deep neural network (DNN) or end-to-end (E2E) models. The advance\nof SSL-based models leverages the power of data and provides a good and univer-\nsal starting point for further fine-tuning using labeled data for the aforementioned\ndownstream tasks.\n3.10.7 Multimodal Reasoning\nIn their comprehensive study, Liu et al. (2023o) conducted an in-depth evaluation of\npublicly accessible multimodal models, focusing on their efficacy in a range of text-\ncentric tasks. These tasks include text recognition, encompassing scene text, artistic\ntext, and handwritten text; text-based visual question answering, which involves doc-\nument text, scene text, and bilingual text; key information extraction from various\nsources such as receipts, documents, and nutrition facts labels; and the recognition\nof handwritten mathematical expressions. The study identified both strengths and\nweaknesses in these models. While they excel in word recognition through semantic\nunderstanding, they struggle when it comes to perceiving combinations of characters\nlacking semantic meaning. Additionally, the models exhibit consistent performance\nregardless of text length and have limited capabilities in detecting intricate image\ndetails. Overall, the study concludes that even the most powerful existing multi-\nmodal models fall short compared to domain-specific methods in traditional text tasks.\n62\nThese findings underscore the necessity for innovative strategies to enhance zero-shot\nmultimodal techniques and improve model performance in complex tasks.\nRegarding evaluation benchmarks, Vedantam et al. (2015) contribute to the field\nby introducing two datasets: PASCAL-50S and ABSTRACT-50S. These datasets are\ndesigned for evaluating image caption generation methods. They serve as valuable\nresources that enable researchers to assess the performance and quality of image\ncaptioning models. By utilizing these datasets, researchers can advance image cap-\ntion generation techniques, fostering progress and innovation in this area of research.\nLVLM-eHub (Xu et al., 2023b) serves as a comprehensive and extensive evaluation\nbenchmark for publicly available large multimodal models. It rigorously assesses the\nperformance of eight LVLMs across six categories of multimodal capabilities. The eval-\nuation process involves the utilization of 47 datasets and 1 arena online platform,\nproviding a robust and standardized framework for evaluating LVLMs. Odouard and\nMitchell (2022) present a concept-based approach to systematic evaluations, with a\nfocus on assessing the proficiency of AI systems in utilizing a given concept across\ndifferent instances. Their evaluation approach, as described in the cited work, entails\nconducting case studies within two specific domains: RAVEN, which is influenced by\nthe Raven’s Progressive Matrices (Raven and Court, 1938), and the Abstraction and\nReasoning Corpus (ARC) (Acquaviva et al., 2021). These are frequently utilized for\nassessing and advancing the capacity for abstraction in AI systems. This methodology\nprovides valuable information about the AI systems’ understanding and application\nof abstract reasoning abilities, shedding light on their ability to grasp and utilize\nconcepts effectively. In a related study, Yin et al. (2023e) expanded the research on\nMultimodal Large Language Models (MLLMs) by incorporating point clouds. They\nintroduced the LAMM-Dataset and LAMM-Benchmark, which specifically focus on\nimproving 2D image and 3D point cloud understanding.\nHallucination is a well-known issue and has long been present in multimodal\nfoundation models as well. Recent studies (Dai et al., 2023; Li et al., 2023p) have inves-\ntigated the performance of Visual Language Pretraining (VLP) models and Vision\nand Language Models (VLMs) in terms of object hallucination. Dai et al. (2023) dis-\ncovered that despite advancements in VLP models, hallucinations remain a common\nissue. Interestingly, the study revealed that models with higher scores on conventional\nmetrics like CIDEr (Vedantam et al., 2015) tended to exhibit more unfaithful results.\nThe authors also found that patch-based features yielded the best results, with smaller\npatch resolutions reducing object hallucination. To tackle this issue, They proposed\na straightforward yet effective VLP loss called Object Masked Language Modeling\n(ObjMLM) (Dai et al., 2023), which further mitigates object hallucination. By decou-\npling various VLP objectives, the authors demonstrated the importance of token-level\nimage-text alignment and controlled generation in reducing hallucination. Similarly, Li\net al. (2023p) conducted evaluation experiments on representative VLMs and discov-\nered widespread object hallucination issues. They also found that visual instructions\ncan influence hallucination, with objects that frequently appear in the instructions or\nco-occur with image objects being more susceptible to hallucination by VLMs. Fur-\nthermore, the authors observed that existing evaluation methods may be influenced\nby the input instructions and generation styles of VLMs. To address this concern, they\n63\nproposed an improved evaluation method called POPE (Polling-based Object Prob-\ning Evaluation) to assess object hallucination more effectively. Zhao et al. (2023e) put\nforth a methodology for examining the robustness of open-source large VLMs in real-\nistic and high-risk scenarios, where adversaries have limited black-box system access\nand aim to deceive the model into producing targeted responses. The authors begin\nby crafting targeted adversarial examples against pre-trained models like CLIP (Rad-\nford et al., 2021) and BLIP (Li et al., 2022c). They later transfer these adversarial\nexamples to other VLMs, including MiniGPT-4 (Zhu et al., 2023b), LLaVA (Liu et al.,\n2023e), UniDiffuser (Bao et al., 2023), BLIP-2 (Li et al., 2023f), and Img2Prompt (Guo\net al., 2023a). The authors discovered that the effectiveness of targeted evasion in large\nVLMs can be significantly enhanced by employing black-box queries. This approach\nyields a surprisingly high success rate in generating targeted responses, thereby high-\nlighting the vulnerability of these models to adversarial attacks. In addition, Huang\net al. (2023c) introduce T2I-CompBench, a comprehensive benchmark for assessing\ntext-to-image generation models’ capabilities in processing compositional prompts. It\nevaluates how these T2I models interpret and represent compositional concepts, such\nas attribute binding, object relationships, and complex compositions. T2I-CompBench\nalso delves into effectively utilizing multimodal LLMs for evaluation in this context.\nRegarding the metrics, DePlot (Liu et al., 2023b) introduces a metric called Rel-\native Number Set Similarity (RNSS) for comparing table similarity. RNSS takes into\naccount the table’s structure and numeric values while remaining unaffected by colum-\nn/row permutations. CIDEr, as introduced by Vedantam et al. (2015), is presented as\nan automated metric for the evaluation of image captioning.\nIn summary, these studies contribute to the development of innovative mod-\nels, benchmarks, and evaluation metrics in the field of multimodal understanding.\nThey explore various aspects of multimodal reasoning, including visual instruction-\nfollowing, reasoning on charts, object detection, and image-related tasks. The proposed\napproaches and evaluations shed light on the strengths and weaknesses of existing\nmodels and highlight the need for further advancements in multimodal techniques.\n3.10.8 Embodied Reasoning\nRoboTHOR (Deitke et al., 2020) is a platform designed to develop and test embod-\nied AI agents in both simulated and physical environments. VirtualHome (Puig et al.,\n2018) is another platform that focuses on modeling complex activities occurring in\ntypical household settings. It offers support for program descriptions that cover a\nwide variety of activities found in people’s homes. Gibson (Xia et al., 2018) places\nemphasis on real-world perception for embodied agents. To bridge the gap between\nsimulation and reality, iGibson (Li et al., 2021a) and BEHAVIOR-1K (Li et al., 2022b)\nextend the simulation capabilities to encompass a more diverse range of household\ntasks and achieve high levels of realism. These platforms provide researchers with tools\nto explore and evaluate embodied AI approaches in realistic simulated environments.\nHabitat (Manolis Savva* et al., 2019) boasts high performance, reaching several thou-\nsand frames per second (fps) even when running single-threaded. Habitat-Lab (Szot\net al., 2021) is a high-level, modular library that supports comprehensive development\nin the realm of embodied AI. It enables the specification of a range of embodied AI\n64\nDataset Tasks Size\nSwag Zellers et al. (2018) Commonsense 113,000\nPHYRE Bakhtin et al. (2019) Commonsense 25\nHellaSwag Zellers et al. (2019) Commonsense 70,000\nWinoGrande Sakaguchi et al. (2021) Commonsense 44,000\nSocial IQA Sap et al. (2019) Commonsense 35,350\nPIQA Bisk et al. (2020) Commonsense 21,020\nSummEdits Laban et al. (2023) Commonsense 6,348\nCConS Kondo et al. (2023) Commonsense 1,112\nAlg514 Kushman et al. (2014) Math 514\nVerb395 Hosseini et al. (2014) Math 395\nDolphin1878 Shi et al. (2015) Math 1878\nDRAW Upadhyay and Chang (2015) Math 1000\nSingleEQ Koncel-Kedziorski et al. (2015) Math 508\nDolphin18K Huang et al. (2016) Math 18,000\nMATH Hendrycks et al. (2021b) Math 12,500\nTabMWP Lu et al. (2022b) Math 38,431\nGSM8K Cobbe et al. (2021) Math 8,500\nMGSM Shi et al. (2023) Math 250\nMath23K Wang et al. (2017) Math 23,161\nHMWP Qin et al. (2020) Math 5,491\nASDiv Miao et al. (2020) Math 2,305\nSVAMP Patel et al. (2021) Math 1,000\nGeoS Seo et al. (2015) Math 186\nGeoShader Alvin et al. (2017) Math 102\nGEOS++ Sachan et al. (2017) Math 1,406\nGEOS-OS Sachan and Xing (2017) Math 2,235\nGeoQA Chen et al. (2021a) Math 4,998\nUniGeo Chen et al. (2022b) Math 4,998\nGeometry3K Lu et al. (2021a) Math 3,002\nAQuA dataset Ling et al. (2017) Math 100,000\nMathQA Amini et al. (2019) Math 37,000\nARB Sawada et al. (2023) Math (physics, biology, chemistry, law)1,207\nIconQA Lu et al. (2021b) Math 107,439\nMultiHiertt Zhao et al. (2022b) Math 10,440\nDROP Dua et al. (2019) Textual QA 96,567\nWTQ Pasupat and Liang (2015) Textual QA 22,033\nWikiSQL Zhong et al. (2018) Textual QA 80,654\nSpider Yu et al. (2018) Textual QA 10,181\nHybridQA Chen et al. (2020a) Hybrid QA ∼70,000\nMetaQA Zhang et al. (2017) Hybrid QA ∼400,000\nSQuAD Rajpurkar et al. (2016) Hybrid QA ∼100,000\nTriviaQA Joshi et al. (2017) Hybrid QA 95,000\nHotpotQA Yang et al. (2018) Hybrid QA 113,000\nMMQA Talmor et al. (2021) Hybrid QA 29,918\nTheoremQA Chen et al. (2023h) Hybrid QA 800\nTAT-QA Zhu et al. (2021) Hybrid QA 16,552\nFinQA Chen et al. (2021c) Hybrid QA 8,281\nTable 8: Summary of Some Reasoning Datasets 1\ntasks, including navigation, interaction, following instructions, and answering ques-\ntions. This platform allows researchers to tailor embodied agents with particular\nphysical attributes, sensors, and functionalities, and to evaluate their performance on\nthese tasks using established metrics.\nThese simulation platforms hold great potential for evaluating LLMs on robotics\ntasks. By leveraging these simulators, researchers can assess the performance and\n65\nDataset Tasks Size\nAPPS Hendrycks et al. (2021a) Program Synthesis 10,000\nHumanEval Chen et al. (2021b) Program Synthesis 164\nMathQA-Python Austin et al. (2021) Program Synthesis 23,914\nMBPP Austin et al. (2021) Program Synthesis 974\nαNLI Bhagavatula et al. (2019) Logical 171,186\nProofWriter Tafjord et al. (2021) Logical 100,030\nFOLIO Han et al. (2022) Logical 1,435\nPrOntoQA Saparov and He (2023) Srivastava et al. (2023)Logical 200\nLogicalDeduction Srivastava et al. (2023) Logical 1,300\nT¨ ubingen cause-effect pairs dataset Mooij et al. (2016)Causal 108\nNeuropathic Pain dataset Tu et al. (2019) Causal N/A\nArctic sea ice dataset Huang et al. (2021c) Causal N/A\nCRASS Frohberg and Binder (2021) Causal 275\nPARTIT Hong et al. (2021a) Visual ∼10,000\nPTR Hong et al. (2021b) Visual 70,000\nCLEVR Johnson et al. (2017) Visual 100,000\nOK-VQA Marino et al. (2019) Visual 14,055\nVoxPopuli Babu et al. (2022) Audio 400,000 hours\nLibri-light Kahn et al. (2020) Audio 60,000 hours\nMultilingual LibriSpeech Pratap et al. (2020) Audio 50,000 hours\nCommon Voice Ardila et al. (2020) Audio 11,000 hours\nDidi Dictation Jiang et al. (2021b) Audio 10,000 hours\nDidi Callcenter Jiang et al. (2021b) Audio 10,000 hours\nPASCAL-50S Vedantam et al. (2015) Multimodal 1,000\nABSTRACT-50S Vedantam et al. (2015) Multimodal 500\nLVLM-eHub Xu et al. (2023b) Multimodal 47 sub-dataset\nLAMM-Dataset Yin et al. (2023e) Multimodal 25 sub-datasets\nRoboTHOR Deitke et al. (2020) Embodied /\nVirtualHome Puig et al. (2018) Embodied /\nGibson Xia et al. (2018) Embodied /\nBEHAVIOR-1K Li et al. (2022b) Embodied /\nHabitat Manolis Savva* et al. (2019) Embodied /\nDriveLM DriveLM Contributors (2023) Driving 360,000\nnuScenes QA Qian et al. (2023b) Driving 460,000\nHAD Kim et al. (2019) Driving 5,675\nTable 9: Summary of Some Reasoning Datasets 2\ncapabilities of LLMs in the context of real-world scenarios, further advancing the field\nof embodied AI.\n3.10.9 Autonomous Driving\nDriveLM (DriveLM Contributors, 2023) is a comprehensive driving benchmark to\ninvestigate the role of LLMs in various aspects. It introduces the reasoning ability of\nLarge Language Models in autonomous driving to guarantee explainable planning and\nthus make safe decisions. The questions and answers (QA) in perception, prediction,\nand planning modules are connected in a graph-style structure, with QA pairs as\n66\nnodes, and objects’ relationships as edges. Compared to predecessors, such as nuScenes\nQA (Qian et al., 2023b) and HAD (Kim et al., 2019), DriveLM draws many merits\nfrom them and improves the logical reasoning in more tasks and a wide diversity of\nscenarios.\n3.10.10 Code Generation\nCode generation (Sun et al., 2021) encompasses several datasets and benchmarks\nthat contribute to the advancement of code generation and evaluation. The APPS\ndataset (Hendrycks et al., 2021a), consists of 10,000 problems derived from coding com-\npetitions. It functions as a standard for assessing code generation tasks that are guided\nby natural language descriptions. This dataset provides a platform for researchers\nto measure and contrast the effectiveness of models in producing code in response\nto natural language prompts. Additionally, the study highlights concerns with using\nBLEU (Papineni et al., 2002) as a metric for code generation, suggesting that it may\nnot be reliable in this context. Similarly, HumanEval (Chen et al., 2021b) consists\nof 164 handwritten programming problems and serves as a benchmark for evaluat-\ning the performance of Codex (Chen et al., 2021b). Each problem in HumanEval\nincludes a function signature, docstring, body, multiple unit tests, and each problem\nhas 7.7 tests on average. MathQA-Python (Austin et al., 2021) is a Python version of\nthe MathQA (Amini et al., 2019) benchmark. It contains 23,914 problems that eval-\nuate models’ ability to synthesize code from complex textual descriptions. Notably,\nthe study found that providing natural language feedback from humans resulted in a\nsignificant reduction in error rates compared to the models’ initial predictions. The\nMostly Basic Programming Problems (MBPP) dataset (Austin et al., 2021) consists\nof 974 programming tasks specifically designed to be solvable by entry-level program-\nmers. In MBPP, there is a greater emphasis on the usage of imperative control flow\nstructures like loops and conditionals. On the other hand, MathQA-Python (Austin\net al., 2021) contains more intricate natural language descriptions, offering a higher\nlevel of complexity in the problem statements.\n4 Foundation Model Techniques\nIn this section, we provide a concise overview of various foundation model techniques.\nHere, we present distinct categories of reasoning techniques:\n• Pre-Training (Section 4.1): Exploring data and architecture of reasoning founda-\ntion models.\n• Fine-tuning (Section 4.2): Focusing on reasoning foundation models’ fine-tuning\ndata and techniques.\n• Alignment Training (Section 4.3): Examining the alignment techniques employed\nby reasoning foundation models.\n• Mixture-of-Expert (Section 4.4): Introducing the mixture-of-expert techniques in\nthe context of reasoning.\n• In-Context Learning (Section 4.5): Introducing in-context learning in reasoning\nfoundation models.\n67\nA) Text data\nweb crawl corpus\nprogramming codes\nbook texts\nmathematical texts\nacademic papers\ndata from media platforms\n...\nB) Image data\nImageNet\nWorld Wide Web\nInstagram hashtags\n...\nC) Multimodality data\nSBU\nRedCaps\nWIT\nLAION-5B\nMassiveWeb\nCOYO-700M\nShutterstock\n...\nData Collection\n…\nFig. 14 : A diverse suite of data sources and datasets for pre-training foundation\nmodels, mainly including text data, image data, and multimodality data.\n• Autonomous Agent (Section 4.6): Focusing on the reasoning foundation model as\nan agent for multiple tasks.\n4.1 Pre-Training\nIn the pre-training part, LLMs can acquire essential language understanding and gen-\neration skills. Here, the data and architecture are critical for the foundation model.\nTherefore, we will discuss them in the following sections.\n4.1.1 Data Source\nFoundation models are data-driven, and both quality and quantity of data lie at the\ncore of foundation model development. Figure 14 presents three broad types of data\nsources for foundation model pre-training.\nText Data\nThe realm of publicly accessible large-scale text datasets has seen a considerable expan-\nsion, presenting a rich variety of resources for myriad applications. A prime example\nis the Pile (Gao et al., 2020), an extensive English text corpus, notable for its impres-\nsive volume of 825 GB, and specifically curated to facilitate the training of large-scale\nlanguage models. This corpus is comprised of 22 diverse subsets, recognized for their\nvariety and quality, amalgamating both existing and newly created content, with a\n68\nsignificant portion sourced from scholarly and professional domains. A considerable\namount of this data is amassed through web crawling initiatives, akin to the Common-\nCrawl project. It is crucial to acknowledge that such web crawling produces a spectrum\nof content, from high-caliber material like Wikipedia entries to lower-tier content such\nas spam emails, necessitating rigorous filtering and processing to elevate data quality.\nAnother notable dataset in this field is the C4 dataset (Raffel et al., 2019), representing\nan expansive and refined version of the Common Crawl web corpus, extensively utilized\nin various sectors. In contrast, the ROOTS dataset (Lauren¸ con et al., 2022) emerges as\nan immense resource, encompassing 1.6TB and spanning 59 languages across 46 nat-\nural languages, derived from three macro regions and nine language families. It also\nincludes material in 13 programming languages, with Java, PHP, and C++ compris-\ning the majority of its content. The Gutenberg project (Lahiri, 2014) offers a selection\nof 3,036 English books by 142 authors. This collection, a subset of the larger Project\nGutenberg corpus, has been diligently cleaned to remove metadata, licensing details,\nand transcribers’ notes to the fullest extent. The CLUECorpus (Xu et al., 2020) stands\nout in the Chinese text domain as a substantial 100GB resource. This community-led\nproject integrates nine varied tasks, ranging from single-sentence/sentence-pair clas-\nsifications to machine reading comprehension, all rooted in authentic Chinese text.\nAdditionally, the Proof-Pile dataset (Azerbayev et al., 2023), with its impressive 8 bil-\nlion tokens, is notable in the mathematical text sphere. It is distinguished for being\namong the few open-source language models specifically tuned for the general math-\nematics field. The peS2o dataset (Soldaini and Lo, 2023), consisting of around 40\nmillion open-access academic papers, is an invaluable asset. It has undergone thor-\nough cleaning, filtering, and formatting, making it ideal for pre-training language\nmodels. Originating from the Semantic Scholar Open Research Corpus (S2ORC), it\nexpands the availability of academic text resources. Furthermore, researchers have\naccess to various public conversation datasets, like the Reddit corpus (Roller et al.,\n2020). Data from online social media platforms also offers a wealth of conversational\ncontent. Scientific text collections typically focus on aggregating materials such as\narXiv papers, scientific textbooks, mathematical websites, and related scientific mate-\nrials. The complex nature of scientific data, often laden with mathematical symbols\nand protein sequences, requires specialized tokenization and preprocessing methods for\nstandardization and uniform processing by language models. Recent research (Austin\net al., 2021) highlights the benefits of training Large Language Models (LLMs) on\nextensive code corpora, leading to marked enhancements in generated program qual-\nity. These corpora are often sourced from platforms like StackOverflow and GitHub.\nLastly, the RedPajama project (Computer, 2023) deserves mention for its remark-\nable feat in reproducing LLaMA’s training dataset, encompassing an impressive 1.2\ntrillion tokens. This dataset includes a vast array of tokens from CommonCrawl, C4,\nGitHub, Books, ArXiv, Wikipedia, and StackExchange, presenting a comprehensive\nand diverse resource for the development and refinement of language models.\nImage Data\nThe methodology of supervised pre-training using extensive, human-curated datasets\nlike ImageNet (Deng et al., 2009) and ImageNet21K (Ridnik et al., 2021) has become\n69\na prevalent approach in developing transferable visual representations. This process\nis structured to create a linkage between an input image and a distinct label, each\ncorresponding to a specific visual concept. With the growing need for large-scale pre-\ntraining, the generation of copious amounts of noisy labels from image-text pairings\nsourced from the World Wide Web has become increasingly relevant. Leveraging these\nnoisy labels, numerous leading industrial research labs have skillfully assembled vast\nclassification datasets using semi-automatic data pipelines. Notable examples of such\nendeavors include JFT (Sun et al., 2017) and I2E (Wu et al., 2023e). Additionally,\nthey have utilized proprietary data sources, such as Instagram hashtags (Singh et al.,\n2022), to enrich their datasets further and augment the precision of their pre-trained\nmodels. This strategy has significantly contributed to the advancement of sophisticated\nvisual recognition systems, equipping them with the ability to effectively identify and\ncategorize a wide spectrum of visual concepts and objects.\nMultimodality Data\nThe domain of large-scale datasets features several notable examples. SBU (Ordonez\net al., 2011), for instance, executes an extensive number of Flickr queries and then\nrigorously filters the results to produce 1 million images, each paired with a caption\nthat is visually pertinent. Conversely, RedCaps (Desai et al., 2021) is a substan-\ntial dataset encompassing 12 million image-text pairs, sourced from Reddit. The\nWIT dataset (Srinivasan et al., 2021) is distinguished by its curated compilation\nof 37.6 million image-text instances, enhanced with entity information, covering 108\nWikipedia languages, and incorporating 11.5 million unique images. Other relatively\nlarge datasets in this field include Shutterstock (Nguyen et al., 2022), LAION-\n400M (Schuhmann et al., 2021), and COYO-700M (Byeon et al., 2022). OpenAI’s\nCLIP (Radford et al., 2021) was refined through an impressive collection of 400 mil-\nlion image-text pairs, meticulously sourced from the web. Recently, the emergence of\ndatasets at the billion-scale level has been observed. The LAION-5B dataset (Schuh-\nmann et al., 2022), for instance, comprises 5.85 billion CLIP-filtered image-text pairs,\nof which 2.32 billion are in the English language. DataComp (Gadre et al., 2023) func-\ntions as a platform for dataset experiments, focusing on a new pool of 12.8 billion\nimage-text pairs collected from Common Crawl. Flamingo (Alayrac et al., 2022) intro-\nduces the MultiModal MassiveWeb (M3W) dataset, aggregating text and images from\nabout 43 million web pages, and aligning images with text according to the Document\nObject Model (DOM). A noteworthy project in this context is ImageBind (Girdhar\net al., 2023a), which aims to develop a joint embedding covering six distinct modal-\nities, including images, text, audio, depth, thermal, and IMU data, with potential\nextension to other modalities such as point clouds (Guo et al., 2023b). This ambitious\nendeavor signifies a major step forward in fostering a deeper comprehension of multi-\nmodal data by establishing meaningful links across diverse data types. As multimodal\nlearning advances, these developments in dataset creation and application are crucial\nto the ongoing innovation in the field.\n70\nꔄ       ՜        ࣁ        ݰ        ᝘        ຎ</s>      He       is     eating    an     appleꔄ       ՜        ࣁ        ݰ        ᝘        ຎ</s>      He       is     eating    an     apple\n՜        ࣁ        ݰ       ᝘        ຎ      </s> ՜        ࣁ        ݰ       ᝘        ຎ      </s>\n1          2         3          4         5         6 1          2         3         4          5         6 1          2         3         4          5         6 \nHe       is     eating    an     apple      ꔄ\n7          8         9        10        11       12 \nNx Nx\nFeed Forward\nNx Nx\nFeed Forward\nFeed Forward\nLinear & Softmax\nFeed Forward\nLinear & Softmax Linear & Softmax\nH E\n\u00001\n<latexit sha1_base64=\"ozPIjsbP94qcqOIH4YYaszigF38=\">AAAB8HicbZDLSgMxFIbP1Futt1GXboJF7MYyIwVdFkTosoK9SDuWTJq2oUlmTDJCGfoUblwo4tZH8A3cuvNJ3DrTdqGtPwQ+/v8ccs7xQ860cZwvK7O0vLK6ll3PbWxube/Yu3t1HUSK0BoJeKCaPtaUM0lrhhlOm6GiWPicNvzhRZo37qnSLJDXZhRST+C+ZD1GsEmsm0onPnHHt5e5jp13is5EaBHcGeTL9vHdR/j9Xu3Yn+1uQCJBpSEca91yndB4MVaGEU7HuXakaYjJEPdpK0GJBdVePBl4jI4Sp4t6gUqeNGji/u6IsdB6JPykUmAz0PNZav6XtSLTO/diJsPIUEmmH/UijkyA0u1RlylKDB8lgIliyayIDLDCxCQ3So/gzq+8CPXTolsqlq7cfLkAU2XhAA6hAC6cQRkqUIUaEBDwAE/wbCnr0XqxXqelGWvWsw9/ZL39AIbekzo=</latexit>\ns\n<latexit sha1_base64=\"l0kEChh3pIGACKwfMOCdtWNwu64=\">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBSBV2JaCdARvLBMwFkiXMTs4mY2Znl5lZISwprWwsFLH1AazzHHY+gy/h5FJo9IeBj/8/hznn+DFnSjvOp5VZWV1b38hu5ra2d3b38vsHDRUlkmKdRjySLZ8o5ExgXTPNsRVLJKHPsekPr6Z58w6lYpG40aMYvZD0BQsYJdpYNdXNF5ySM5P9F9wFFC7fJ7Wv++NJtZv/6PQimoQoNOVEqbbrxNpLidSMchznOonCmNAh6WPboCAhKi+dDTq2T43Ts4NImie0PXN/dqQkVGoU+qYyJHqglrOp+V/WTnRw4aVMxIlGQecfBQm3dWRPt7Z7TCLVfGSAUMnMrDYdEEmoNrfJmSO4yyv/hcZZyS2XyjW3UCnCXFk4ghMoggvnUIFrqEIdKCA8wBM8W7fWo/Vivc5LM9ai5xB+yXr7BosBkR0=</latexit>\nt\n<latexit sha1_base64=\"s/6D3SUwKA4kK5/1rp2qSo8pCbg=\">AAAB6HicbZC7SgNBFIZn4y3GW9RSkMUgpAq7EtDOgI1lAuYCyRJmJ2eTMbOzy8xZISwprWwsFLH1AazzHHY+gy/h5FJo9IeBj/8/hznn+LHgGh3n08qsrK6tb2Q3c1vbO7t7+f2Dho4SxaDOIhGplk81CC6hjhwFtGIFNPQFNP3h1TRv3oHSPJI3OIrBC2lf8oAzisaqYTdfcErOTPZfcBdQuHyf1L7ujyfVbv6j04tYEoJEJqjWbdeJ0UupQs4EjHOdRENM2ZD2oW1Q0hC0l84GHdunxunZQaTMk2jP3J8dKQ21HoW+qQwpDvRyNjX/y9oJBhdeymWcIEg2/yhIhI2RPd3a7nEFDMXIAGWKm1ltNqCKMjS3yZkjuMsr/4XGWcktl8o1t1Apkrmy5IickCJxyTmpkGtSJXXCCJAH8kSerVvr0XqxXuelGWvRc0h+yXr7BoyFkR4=</latexit>\np s\n<latexit sha1_base64=\"5Vxt83Tg+HN6GCGxE79Pia71mUU=\">AAAB6nicbZDLSgMxFIbPtF5qvVVdugmWQldlRgq6LLhxWdFeoB1KJs20oUlmSDJCGYobt25cKMWV4DP4IO58G9PLQlt/CHz8/znknBPEnGnjut9OJruxubWd28nv7u0fHBaOjps6ShShDRLxSLUDrClnkjYMM5y2Y0WxCDhtBaOrWd66p0qzSN6ZcUx9gQeShYxgY63buKd7haJbcedC6+AtoVjLlh4+p4/v9V7hq9uPSCKoNIRjrTueGxs/xcowwukk3000jTEZ4QHtWJRYUO2n81EnqGSdPgojZZ80aO7+7kix0HosAlspsBnq1Wxm/pd1EhNe+imTcWKoJIuPwoQjE6HZ3qjPFCWGjy1gopidFZEhVpgYe528PYK3uvI6NM8rXrVSvfGKtTIslINTOIMyeHABNbiGOjSAwACe4AVeHe48O1PnbVGacZY9J/BHzscP3d6RHw==</latexit>\np t\n<latexit sha1_base64=\"04B7kK5dy3YeKL64BYNh8SVnNCw=\">AAAB6nicbZC7SgNBFIbPJl5ivEUtbRZDIFXYlYCWARvLiOYCyRJmJ7PJkNnZZeasEJZgY2tjoQQrwWfwQex8GyeXQhN/GPj4/3OYc44fC67Rcb6tTHZjc2s7t5Pf3ds/OCwcHTd1lCjKGjQSkWr7RDPBJWsgR8HasWIk9AVr+aOrWd66Z0rzSN7hOGZeSAaSB5wSNNZt3MNeoehUnLnsdXCXUKxlSw+f08f3eq/w1e1HNAmZRCqI1h3XidFLiUJOBZvku4lmMaEjMmAdg5KETHvpfNSJXTJO3w4iZZ5Ee+7+7khJqPU49E1lSHCoV7OZ+V/WSTC49FIu4wSZpIuPgkTYGNmzve0+V4yiGBsgVHEzq02HRBGK5jp5cwR3deV1aJ5X3GqleuMWa2VYKAencAZlcOECanANdWgAhQE8wQu8WsJ6tqbW26I0Yy17TuCPrI8f32KRIA==</latexit>\ns\n<latexit sha1_base64=\"l0kEChh3pIGACKwfMOCdtWNwu64=\">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBSBV2JaCdARvLBMwFkiXMTs4mY2Znl5lZISwprWwsFLH1AazzHHY+gy/h5FJo9IeBj/8/hznn+DFnSjvOp5VZWV1b38hu5ra2d3b38vsHDRUlkmKdRjySLZ8o5ExgXTPNsRVLJKHPsekPr6Z58w6lYpG40aMYvZD0BQsYJdpYNdXNF5ySM5P9F9wFFC7fJ7Wv++NJtZv/6PQimoQoNOVEqbbrxNpLidSMchznOonCmNAh6WPboCAhKi+dDTq2T43Ts4NImie0PXN/dqQkVGoU+qYyJHqglrOp+V/WTnRw4aVMxIlGQecfBQm3dWRPt7Z7TCLVfGSAUMnMrDYdEEmoNrfJmSO4yyv/hcZZyS2XyjW3UCnCXFk4ghMoggvnUIFrqEIdKCA8wBM8W7fWo/Vivc5LM9ai5xB+yXr7BosBkR0=</latexit>\nt\n<latexit sha1_base64=\"s/6D3SUwKA4kK5/1rp2qSo8pCbg=\">AAAB6HicbZC7SgNBFIZn4y3GW9RSkMUgpAq7EtDOgI1lAuYCyRJmJ2eTMbOzy8xZISwprWwsFLH1AazzHHY+gy/h5FJo9IeBj/8/hznn+LHgGh3n08qsrK6tb2Q3c1vbO7t7+f2Dho4SxaDOIhGplk81CC6hjhwFtGIFNPQFNP3h1TRv3oHSPJI3OIrBC2lf8oAzisaqYTdfcErOTPZfcBdQuHyf1L7ujyfVbv6j04tYEoJEJqjWbdeJ0UupQs4EjHOdRENM2ZD2oW1Q0hC0l84GHdunxunZQaTMk2jP3J8dKQ21HoW+qQwpDvRyNjX/y9oJBhdeymWcIEg2/yhIhI2RPd3a7nEFDMXIAGWKm1ltNqCKMjS3yZkjuMsr/4XGWcktl8o1t1Apkrmy5IickCJxyTmpkGtSJXXCCJAH8kSerVvr0XqxXuelGWvRc0h+yXr7BoyFkR4=</latexit>\np s\n<latexit sha1_base64=\"5Vxt83Tg+HN6GCGxE79Pia71mUU=\">AAAB6nicbZDLSgMxFIbPtF5qvVVdugmWQldlRgq6LLhxWdFeoB1KJs20oUlmSDJCGYobt25cKMWV4DP4IO58G9PLQlt/CHz8/znknBPEnGnjut9OJruxubWd28nv7u0fHBaOjps6ShShDRLxSLUDrClnkjYMM5y2Y0WxCDhtBaOrWd66p0qzSN6ZcUx9gQeShYxgY63buKd7haJbcedC6+AtoVjLlh4+p4/v9V7hq9uPSCKoNIRjrTueGxs/xcowwukk3000jTEZ4QHtWJRYUO2n81EnqGSdPgojZZ80aO7+7kix0HosAlspsBnq1Wxm/pd1EhNe+imTcWKoJIuPwoQjE6HZ3qjPFCWGjy1gopidFZEhVpgYe528PYK3uvI6NM8rXrVSvfGKtTIslINTOIMyeHABNbiGOjSAwACe4AVeHe48O1PnbVGacZY9J/BHzscP3d6RHw==</latexit>\np t\n<latexit sha1_base64=\"04B7kK5dy3YeKL64BYNh8SVnNCw=\">AAAB6nicbZC7SgNBFIbPJl5ivEUtbRZDIFXYlYCWARvLiOYCyRJmJ7PJkNnZZeasEJZgY2tjoQQrwWfwQex8GyeXQhN/GPj4/3OYc44fC67Rcb6tTHZjc2s7t5Pf3ds/OCwcHTd1lCjKGjQSkWr7RDPBJWsgR8HasWIk9AVr+aOrWd66Z0rzSN7hOGZeSAaSB5wSNNZt3MNeoehUnLnsdXCXUKxlSw+f08f3eq/w1e1HNAmZRCqI1h3XidFLiUJOBZvku4lmMaEjMmAdg5KETHvpfNSJXTJO3w4iZZ5Ee+7+7khJqPU49E1lSHCoV7OZ+V/WSTC49FIu4wSZpIuPgkTYGNmzve0+V4yiGBsgVHEzq02HRBGK5jp5cwR3deV1aJ5X3GqleuMWa2VYKAencAZlcOECanANdWgAhQE8wQu8WsJ6tqbW26I0Yy17TuCPrI8f32KRIA==</latexit>\nUnidirectional Cross Attention        Source  Auto-Encoder          Parameter Sharing           Layer-Wise Coordination        Consecutive Positional Encoding\nLD\n<latexit sha1_base64=\"AvcYkusNoldeIUNFvy/NhtddR1s=\">AAAB9HicbVDLSgMxFL3js9ZX1aUgwSJ0VWakoDsLunDhogX7gHYsmTTThmYyY5IplKFLv8GNC0Xcdt3vcOc3+BOmj4W2HggczrmXe3K8iDOlbfvLWlldW9/YTG2lt3d29/YzB4dVFcaS0AoJeSjrHlaUM0ErmmlO65GkOPA4rXm964lf61OpWCju9SCiboA7gvmMYG0ktxlg3SWYJ3fDh5tWJmvn7SnQMnHmJHs1Gpe/n07GpVbms9kOSRxQoQnHSjUcO9JugqVmhNNhuhkrGmHSwx3aMFTggCo3mYYeojOjtJEfSvOERlP190aCA6UGgWcmJyHVojcR//MasfYv3YSJKNZUkNkhP+ZIh2jSAGozSYnmA0MwkcxkRaSLJSba9JQ2JTiLX14m1fO8U8gXyk62mIMZUnAMp5ADBy6gCLdQggoQeIRneIU3q2+9WO/Wx2x0xZrvHMEfWKMfdjyWPg==</latexit>\nLD\n<latexit sha1_base64=\"AvcYkusNoldeIUNFvy/NhtddR1s=\">AAAB9HicbVDLSgMxFL3js9ZX1aUgwSJ0VWakoDsLunDhogX7gHYsmTTThmYyY5IplKFLv8GNC0Xcdt3vcOc3+BOmj4W2HggczrmXe3K8iDOlbfvLWlldW9/YTG2lt3d29/YzB4dVFcaS0AoJeSjrHlaUM0ErmmlO65GkOPA4rXm964lf61OpWCju9SCiboA7gvmMYG0ktxlg3SWYJ3fDh5tWJmvn7SnQMnHmJHs1Gpe/n07GpVbms9kOSRxQoQnHSjUcO9JugqVmhNNhuhkrGmHSwx3aMFTggCo3mYYeojOjtJEfSvOERlP190aCA6UGgWcmJyHVojcR//MasfYv3YSJKNZUkNkhP+ZIh2jSAGozSYnmA0MwkcxkRaSLJSba9JQ2JTiLX14m1fO8U8gXyk62mIMZUnAMp5ADBy6gCLdQggoQeIRneIU3q2+9WO/Wx2x0xZrvHMEfWKMfdjyWPg==</latexit>\nLE\n<latexit sha1_base64=\"1UWEDncq3V5UXeSIBcuz7wMay7E=\">AAAB9HicbVDLSgMxFL3js9ZX1aUgwSJ0VWakoDsLIrhw0YJ9QDuWTJppQzOZMckUytCl3+DGhSJuu+53uPMb/AnTx0JbDwQO59zLPTlexJnStv1lrayurW9sprbS2zu7e/uZg8OqCmNJaIWEPJR1DyvKmaAVzTSn9UhSHHic1rze9cSv9alULBT3ehBRN8AdwXxGsDaS2wyw7hLMk7vhw00rk7Xz9hRomThzkr0ajcvfTyfjUivz2WyHJA6o0IRjpRqOHWk3wVIzwukw3YwVjTDp4Q5tGCpwQJWbTEMP0ZlR2sgPpXlCo6n6eyPBgVKDwDOTk5Bq0ZuI/3mNWPuXbsJEFGsqyOyQH3OkQzRpALWZpETzgSGYSGayItLFEhNtekqbEpzFLy+T6nneKeQLZSdbzMEMKTiGU8iBAxdQhFsoQQUIPMIzvMKb1bderHfrYza6Ys13juAPrNEPd8CWPw==</latexit>\nH E\nl\n<latexit sha1_base64=\"KbtAGIR4P4DaifCvGEIy9ZJyUzg=\">AAAB73icbZDNSgMxFIXv1L/aVq26dBMsQldlRgq6LIhQdxXsD7TTkkkzbWgmMyYZoQx9CTcuFHErvoGP4c63MdN2oa0HAh/n3EvuvV7EmdK2/W1lNja3tneyu7l8YW//oHh41FJhLAltkpCHsuNhRTkTtKmZ5rQTSYoDj9O2N7lK8/YDlYqF4k5PI+oGeCSYzwjWxurUBwmf9a9zg2LJrthzoXVwllCq5T8yhZvPfmNQ/OoNQxIHVGjCsVJdx460m2CpGeF0luvFikaYTPCIdg0KHFDlJvN5Z+jMOEPkh9I8odHc/d2R4ECpaeCZygDrsVrNUvO/rBtr/9JNmIhiTQVZfOTHHOkQpcujIZOUaD41gIlkZlZExlhios2J0iM4qyuvQ+u84lQr1VunVCvDQlk4gVMogwMXUIM6NKAJBDg8wjO8WPfWk/VqvS1KM9ay5xj+yHr/AY22kdE=</latexit>\nH D\nl\n<latexit sha1_base64=\"lZNIFh+WnrGQTmuSpNX7QQI6wig=\">AAAB7HicbZDNTgIxFIXv4B8CKurSTSMxYUVmDAkuSXSBO0wcIIGBdEoHGjqdSdsxIROewY0LjXHrxjfwMdz5NpafhYInafLlnHvTe68fc6a0bX9bma3tnd297H4uXzg4PCoen7RUlEhCXRLxSHZ8rChngrqaaU47saQ49Dlt+5Pred5+oFKxSNzraUy9EI8ECxjB2lhuY8D7N4Niya7YC6FNcFZQquc/MoXbz35zUPzqDSOShFRowrFSXceOtZdiqRnhdJbrJYrGmEzwiHYNChxS5aWLYWfowjhDFETSPKHRwv3dkeJQqWnom8oQ67Faz+bmf1k30cGVlzIRJ5oKsvwoSDjSEZpvjoZMUqL51AAmkplZERljiYk298mZIzjrK29C67LiVCvVO6dUL8NSWTiDcyiDAzWoQwOa4AIBBo/wDC+WsJ6sV+ttWZqxVj2n8EfW+w+O9JCw</latexit>\nQD\nl\n<latexit sha1_base64=\"fcIgORI5tFJg3SyaxPn3DeWvNZY=\">AAAB7HicbZDNTgIxFIXv4B8CKurSTSMxYUVmDAkuSXShO0gcIIGBdEoHGjqdSdsxIROewY0LjXHrxjfwMdz5NpafhYInafLlnHvTe68fc6a0bX9bma3tnd297H4uXzg4PCoen7RUlEhCXRLxSHZ8rChngrqaaU47saQ49Dlt+5Pred5+oFKxSNzraUy9EI8ECxjB2lhuc8D7N4Niya7YC6FNcFZQquc/MoW7z35jUPzqDSOShFRowrFSXceOtZdiqRnhdJbrJYrGmEzwiHYNChxS5aWLYWfowjhDFETSPKHRwv3dkeJQqWnom8oQ67Faz+bmf1k30cGVlzIRJ5oKsvwoSDjSEZpvjoZMUqL51AAmkplZERljiYk298mZIzjrK29C67LiVCvVplOql2GpLJzBOZTBgRrU4RYa4AIBBo/wDC+WsJ6sV+ttWZqxVj2n8EfW+w+cvJC5</latexit>\nGD\nl\n<latexit sha1_base64=\"Cn1X0h+5fKj39wDxZtklB0kWopo=\">AAAB7HicbZDNSgMxFIXv1L/aVq26dBMsQldlRgp1WVBQdxWcttBOSybNtKGZzJBkhDL0Gdy4UMStG9/Ax3Dn25j+LLT1QODjnHvJvdePOVPatr+tzMbm1vZOdjeXL+ztHxQPj5oqSiShLol4JNs+VpQzQV3NNKftWFIc+py2/PHlLG89UKlYJO71JKZeiIeCBYxgbSz3us97V/1iya7Yc6F1cJZQquc/MoXbz16jX/zqDiKShFRowrFSHceOtZdiqRnhdJrrJorGmIzxkHYMChxS5aXzYafozDgDFETSPKHR3P3dkeJQqUnom8oQ65FazWbmf1kn0cGFlzIRJ5oKsvgoSDjSEZptjgZMUqL5xAAmkplZERlhiYk298mZIzirK69D87ziVCvVO6dUL8NCWTiBUyiDAzWoww00wAUCDB7hGV4sYT1Zr9bbojRjLXuO4Y+s9x+NbJCv</latexit>\nGE\nl\n<latexit sha1_base64=\"CSXyEs9JPhZWUE61bw5eiTDWCFk=\">AAAB7HicbZDNSgMxFIXv1L/aVq26dBMsQldlRgp1WRBRdxWcttBOSybNtKGZzJBkhDL0Gdy4UMStG9/Ax3Dn25j+LLT1QODjnHvJvdePOVPatr+tzMbm1vZOdjeXL+ztHxQPj5oqSiShLol4JNs+VpQzQV3NNKftWFIc+py2/PHlLG89UKlYJO71JKZeiIeCBYxgbSz3us97V/1iya7Yc6F1cJZQquc/MoXbz16jX/zqDiKShFRowrFSHceOtZdiqRnhdJrrJorGmIzxkHYMChxS5aXzYafozDgDFETSPKHR3P3dkeJQqUnom8oQ65FazWbmf1kn0cGFlzIRJ5oKsvgoSDjSEZptjgZMUqL5xAAmkplZERlhiYk298mZIzirK69D87ziVCvVO6dUL8NCWTiBUyiDAzWoww00wAUCDB7hGV4sYT1Zr9bbojRjLXuO4Y+s9x+O8JCw</latexit>\nH E\nl\n<latexit sha1_base64=\"KbtAGIR4P4DaifCvGEIy9ZJyUzg=\">AAAB73icbZDNSgMxFIXv1L/aVq26dBMsQldlRgq6LIhQdxXsD7TTkkkzbWgmMyYZoQx9CTcuFHErvoGP4c63MdN2oa0HAh/n3EvuvV7EmdK2/W1lNja3tneyu7l8YW//oHh41FJhLAltkpCHsuNhRTkTtKmZ5rQTSYoDj9O2N7lK8/YDlYqF4k5PI+oGeCSYzwjWxurUBwmf9a9zg2LJrthzoXVwllCq5T8yhZvPfmNQ/OoNQxIHVGjCsVJdx460m2CpGeF0luvFikaYTPCIdg0KHFDlJvN5Z+jMOEPkh9I8odHc/d2R4ECpaeCZygDrsVrNUvO/rBtr/9JNmIhiTQVZfOTHHOkQpcujIZOUaD41gIlkZlZExlhios2J0iM4qyuvQ+u84lQr1VunVCvDQlk4gVMogwMXUIM6NKAJBDg8wjO8WPfWk/VqvS1KM9ay5xj+yHr/AY22kdE=</latexit>\nGE\nl\n<latexit sha1_base64=\"CSXyEs9JPhZWUE61bw5eiTDWCFk=\">AAAB7HicbZDNSgMxFIXv1L/aVq26dBMsQldlRgp1WRBRdxWcttBOSybNtKGZzJBkhDL0Gdy4UMStG9/Ax3Dn25j+LLT1QODjnHvJvdePOVPatr+tzMbm1vZOdjeXL+ztHxQPj5oqSiShLol4JNs+VpQzQV3NNKftWFIc+py2/PHlLG89UKlYJO71JKZeiIeCBYxgbSz3us97V/1iya7Yc6F1cJZQquc/MoXbz16jX/zqDiKShFRowrFSHceOtZdiqRnhdJrrJorGmIzxkHYMChxS5aXzYafozDgDFETSPKHR3P3dkeJQqUnom8oQ65FazWbmf1kn0cGFlzIRJ5oKsvgoSDjSEZptjgZMUqL5xAAmkplZERlhiYk298mZIzirK69D87ziVCvVO6dUL8NCWTiBUyiDAzWoww00wAUCDB7hGV4sYT1Zr9bbojRjLXuO4Y+s9x+O8JCw</latexit>\nGD\nl\n<latexit sha1_base64=\"Cn1X0h+5fKj39wDxZtklB0kWopo=\">AAAB7HicbZDNSgMxFIXv1L/aVq26dBMsQldlRgp1WVBQdxWcttBOSybNtKGZzJBkhDL0Gdy4UMStG9/Ax3Dn25j+LLT1QODjnHvJvdePOVPatr+tzMbm1vZOdjeXL+ztHxQPj5oqSiShLol4JNs+VpQzQV3NNKftWFIc+py2/PHlLG89UKlYJO71JKZeiIeCBYxgbSz3us97V/1iya7Yc6F1cJZQquc/MoXbz16jX/zqDiKShFRowrFSHceOtZdiqRnhdJrrJorGmIzxkHYMChxS5aXzYafozDgDFETSPKHR3P3dkeJQqUnom8oQ65FazWbmf1kn0cGFlzIRJ5oKsvgoSDjSEZptjgZMUqL5xAAmkplZERlhiYk298mZIzirK69D87ziVCvVO6dUL8NCWTiBUyiDAzWoww00wAUCDB7hGV4sYT1Zr9bbojRjLXuO4Y+s9x+NbJCv</latexit>\nQD\nl\n<latexit sha1_base64=\"fcIgORI5tFJg3SyaxPn3DeWvNZY=\">AAAB7HicbZDNTgIxFIXv4B8CKurSTSMxYUVmDAkuSXShO0gcIIGBdEoHGjqdSdsxIROewY0LjXHrxjfwMdz5NpafhYInafLlnHvTe68fc6a0bX9bma3tnd297H4uXzg4PCoen7RUlEhCXRLxSHZ8rChngrqaaU47saQ49Dlt+5Pred5+oFKxSNzraUy9EI8ECxjB2lhuc8D7N4Niya7YC6FNcFZQquc/MoW7z35jUPzqDSOShFRowrFSXceOtZdiqRnhdJbrJYrGmEzwiHYNChxS5aWLYWfowjhDFETSPKHRwv3dkeJQqWnom8oQ67Faz+bmf1k30cGVlzIRJ5oKsvwoSDjSEZpvjoZMUqL51AAmkplZERljiYk298mZIzjrK29C67LiVCvVplOql2GpLJzBOZTBgRrU4RYa4AIBBo/wDC+WsJ6sV+ttWZqxVj2n8EfW+w+cvJC5</latexit>\nH D\nl\n<latexit sha1_base64=\"lZNIFh+WnrGQTmuSpNX7QQI6wig=\">AAAB7HicbZDNTgIxFIXv4B8CKurSTSMxYUVmDAkuSXSBO0wcIIGBdEoHGjqdSdsxIROewY0LjXHrxjfwMdz5NpafhYInafLlnHvTe68fc6a0bX9bma3tnd297H4uXzg4PCoen7RUlEhCXRLxSHZ8rChngrqaaU47saQ49Dlt+5Pred5+oFKxSNzraUy9EI8ECxjB2lhuY8D7N4Niya7YC6FNcFZQquc/MoXbz35zUPzqDSOShFRowrFSXceOtZdiqRnhdJbrJYrGmEzwiHYNChxS5aWLYWfowjhDFETSPKHRwv3dkeJQqWnom8oQ67Faz+bmf1k30cGVlzIRJ5oKsvwoSDjSEZpvjoZMUqL51AAmkplZERljiYk298mZIzjrK29C67LiVCvVO6dUL8NSWTiDcyiDAzWoQwOa4AIBBo/wDC+WsJ6sV+ttWZqxVj2n8EfW+w+O9JCw</latexit>\nATT l\n<latexit sha1_base64=\"8kEVFnVftXOqsVtsCVjj80IXtEs=\">AAAB+HicbVDLSsNAFJ3UV62PRt0IboJF6KokUtCdFTcuK/QFbQiT6aQdOsmEmRuhhn6JGxeKuvUT/AR3foFfIThpu9DWAwOHc+7lnjl+zJkC2/40ciura+sb+c3C1vbObtHc228pkUhCm0RwITs+VpSziDaBAaedWFIc+py2/dFV5rdvqVRMRA0Yx9QN8SBiASMYtOSZxV6IYQiQXjYaE48XPLNkV+wprGXizEnp4vtdfB2+hHXP/Oj1BUlCGgHhWKmuY8fgplgCI5xOCr1E0RiTER7QrqYRDqly02nwiXWilb4VCKlfBNZU/b2R4lCpcejrySymWvQy8T+vm0Bw7qYsihOgEZkdChJugbCyFqw+k5QAH2uCiWQ6q0WGWGICuqusBGfxy8ukdVpxqpXqjVOqldEMeXSEjlEZOegM1dA1qqMmIihB9+gRPRl3xoPxbLzORnPGfOcA/YHx9gNvM5dQ</latexit>\nATT J\nl\n<latexit sha1_base64=\"7x/S2jtkZahUQW4FWyENmUFIgm4=\">AAAB+nicbVC7SgNBFJ2Nr5j42GhpMxiEVGFXBC0jNmJjhLwgWZfZySQZMju7zNxVwppPsLGxsrFQxNYvsbPyV9xNUmjigYHDOfdyzxwvFFyDZX0ZmaXlldW17Houv7G5tW0Wdho6iBRldRqIQLU8opngktWBg2CtUDHie4I1veFZ6jdvmNI8kDUYhczxSV/yHqcEEsk1Cx2fwAAgPq3Vxq64vsi5ZtEqWxPgRWLPSLGSf5CP95ffVdf87HQDGvlMAhVE67ZtheDERAGngo1znUizkNAh6bN2QiXxmXbiSfQxPkiULu4FKnkS8ET9vRETX+uR7yWTaVA976Xif147gt6JE3MZRsAknR7qRQJDgNMecJcrRkGMEkKo4klWTAdEEQpJW2kJ9vyXF0njsGwflY+u7GKlhKbIoj20j0rIRseogs5RFdURRbfoCb2gV+POeDbejPfpaMaY7eyiPzA+fgCNoZco</latexit>\nATT D\nl\n<latexit sha1_base64=\"c+mQhlckGxzCgaeg5Jw+Jf6uvKM=\">AAAB+nicbVC7TsMwFHXKq7Q8UhhZLCqkTlWCkGAsgoGNIvUltSFyXLe16jiRfQOqQj+BhYWJhQGEWPkSNiZ+haTtAC1HsnR0zr26x8cLBddgWV9GZml5ZXUtu57Lb2xubZuFnYYOIkVZnQYiUC2PaCa4ZHXgIFgrVIz4nmBNb3iW+s0bpjQPZA1GIXN80pe8xymBRHLNQscnMACIT2u1sSuuz3OuWbTK1gR4kdgzUqzkH+Tj/eV31TU/O92ARj6TQAXRum1bITgxUcCpYONcJ9IsJHRI+qydUEl8pp14En2MDxKli3uBSp4EPFF/b8TE13rke8lkGlTPe6n4n9eOoHfixFyGETBJp4d6kcAQ4LQH3OWKURCjhBCqeJIV0wFRhELSVlqCPf/lRdI4LNtH5aMru1gpoSmyaA/toxKy0TGqoAtURXVE0S16Qi/o1bgzno034306mjFmO7voD4yPH4SDlyI=</latexit>\nATT E\nl\n<latexit sha1_base64=\"FjOtYjqbQzljH9PWjPPeoif53Mc=\">AAAB+nicbVC7SgNBFJ2Nr5j42GhpMxiEVGFXBC0jItgZIS9I1mV2MkmGzM4uM3eVsOYTbGysbCwUsfVL7Kz8FXeTFJp4YOBwzr3cM8cLBddgWV9GZml5ZXUtu57Lb2xubZuFnYYOIkVZnQYiUC2PaCa4ZHXgIFgrVIz4nmBNb3iW+s0bpjQPZA1GIXN80pe8xymBRHLNQscnMACIT2u1sSuuz3OuWbTK1gR4kdgzUqzkH+Tj/eV31TU/O92ARj6TQAXRum1bITgxUcCpYONcJ9IsJHRI+qydUEl8pp14En2MDxKli3uBSp4EPFF/b8TE13rke8lkGlTPe6n4n9eOoHfixFyGETBJp4d6kcAQ4LQH3OWKURCjhBCqeJIV0wFRhELSVlqCPf/lRdI4LNtH5aMru1gpoSmyaA/toxKy0TGqoAtURXVE0S16Qi/o1bgzno034306mjFmO7voD4yPH4YIlyM=</latexit>\nATT l\n<latexit sha1_base64=\"8kEVFnVftXOqsVtsCVjj80IXtEs=\">AAAB+HicbVDLSsNAFJ3UV62PRt0IboJF6KokUtCdFTcuK/QFbQiT6aQdOsmEmRuhhn6JGxeKuvUT/AR3foFfIThpu9DWAwOHc+7lnjl+zJkC2/40ciura+sb+c3C1vbObtHc228pkUhCm0RwITs+VpSziDaBAaedWFIc+py2/dFV5rdvqVRMRA0Yx9QN8SBiASMYtOSZxV6IYQiQXjYaE48XPLNkV+wprGXizEnp4vtdfB2+hHXP/Oj1BUlCGgHhWKmuY8fgplgCI5xOCr1E0RiTER7QrqYRDqly02nwiXWilb4VCKlfBNZU/b2R4lCpcejrySymWvQy8T+vm0Bw7qYsihOgEZkdChJugbCyFqw+k5QAH2uCiWQ6q0WGWGICuqusBGfxy8ukdVpxqpXqjVOqldEMeXSEjlEZOegM1dA1qqMmIihB9+gRPRl3xoPxbLzORnPGfOcA/YHx9gNvM5dQ</latexit>\nFigure 2: Encoder-Decoder framework (left) and Regularized Encoder-Decoder framework (right).\n3.2 Regularized Encoder-Decoder\nThough the decoder-only Language Model (LM) is simply a decoder, it is still difﬁcult to be compared with an\nEncoder-Decoder (ED) structure because this decoder handles both the source sequence and the target sequence to-\ngether. To facilitate the comparison between the ED and LM structure, we propose to analyze a Regularized Encoder-\nDecoder (RED) framework as illustrated in Figure 2. It is a variant of the traditional ED framework while replicating\nthe behaviors of an LM. Compared with the traditional ED structure, the RED framework mainly has the following\ndifferent components: An unidirectional cross attention attends to both the source matrix and the target matrix si-\nmultaneously; a source auto-encoder recovers the input source; a parameter sharing mechanism shares the parameters\nbetween the encoder and the decoder; a layer-wise coordination component makes each decoder layer attending to the\ncorresponding encoder layer output; a consecutive positional encoding utilizes a positional encoding starting from the\nlength of the source tokens in the decoder.\nUnidirectional Cross Attention. The main difference between the ED framework and the LM is how the input\nsource information is merged into the decoder. As illustrated in Figure 2, the ED framework ﬁrst uses multiple\nTransformer blocks to extract features HE\n−1 from the source sequence s. Afterwards, it utilizes a self attention ATTD\nl\nto get the feature matrix GD\nl . It then uses an encoder attention ATTJ\nl to take GD\nl as query and uses the encoder’s\nﬁnal output HE\n−1 as the key and value to calculate QD\nl . On the other hand, an LM uses an unidirectional attention\nto handle the concatenated features. To simulate this mechanism in the LM, as illustrated in Figure 2, the RED\nframework uses unidirectional cross attention ATTl which attends to both the source matrix GE\nl and the target matrix\nGD\nl simultaneously. Since it attends to all features with one attention, the output matrix QD\nl of the attention layer\nbecomes less sensitive to the input source matrix GE\nl especially when it has already generated many words and GD\nl\nbecomes relatively long. We call this the attention degeneration problem and we will analyze it in detail in Section\n3.3.\nSource Auto-Encoder.The traditional ED structure only predicts the probability for the target sequence and just takes\nthe source sequence as features. On the contrary, an LM predicts the probability for the whole concatenated sequence\nincluding the source sequence. Therefore, in the RED framework, we adopt a Source Auto-Encoder (SAE) component\nto realize this mechanism. As shown in Figure 2, the overall loss is composed of the decoder lossLD and the SAE loss\nLE. LD is the same as that in the ED framework while LE is actually a regularizer that recovers the source sequence\nsto itself. Therefore, it can alleviate the overﬁtting problem in training and thus improve the performance.\nParameter Sharing. In the traditional ED framework, the encoder and the decoder have their own parameters. On the\ncontrary, in an LM, the source and target sequences are concatenated and passed through the same network with the\nsame parameters. To simulate this property, the RED framework shares the parameters between the encoder and the\ndecoder. Parameter sharing techniques (Dehghani et al., 2019; Xia et al., 2019; Lan et al., 2020; Conneau and Lample,\n2019) can be recognized as another regularizer that prevents the model from having too many parameters and thus\nalleviates the overﬁtting problem.\n4\nFig. 15: Illustration of Encoder-Decoder framework (left) and Decoder-Only frame-\nwork (right). The figure credits from (Fu et al., 2023c).\nData for Reasoning\nThe significance of code and paper data in enhancing the reasoning abilities of\nfoundation models is paramount. Discussing code data first, research by CoCoGen\n(Madaan et al., 2022) indicates that when structured commonsense reasoning tasks\nare approached as code generation problems, pre-trained language models (LMs) for\ncode exhibit superior reasoning capabilities compared to those trained on natural lan-\nguage. This holds true even for tasks that do not involve source code. Such code data\nare readily accessible on GitHub and through various filtered datasets available to\nthe public. Highlighting this, StarCoder (Li et al., 2023k) has released an extensive\npretraining dataset (783GB) to further refine LMs’ proficiency in coding. In terms\nof paper data, Galactica (Taylor et al., 2022) stands out, having been trained on a\nvast corpus of scientific papers, reference material, knowledge bases, and other diverse\nsources. This model demonstrates superior performance across a spectrum of scientific\ntasks compared to existing models. Paper data primarily originates from academic\nplatforms like Arxiv, with a notable emphasis on mathematics papers. Additionally,\nthe peS2o (Soldaini and Lo, 2023) dataset, encompassing over 40 million open-access\nacademic papers from the Semantic Scholar Open Research Corpus (S2ORC), provides\na substantial resource for the pretraining of models.\n4.1.2 Network Architecture\nThe foundation model architecture is essential. We discuss different network architec-\ntures in the following and show them in Figure 15.\nEncoder-decoder Architecture\nThe seminal Transformer model, as delineated by Vaswani et al. (2017), is founded on\nthe encoder-decoder framework. This paradigm employs dual stacks of Transformer\nblocks, wherein one functions as the encoder and the other as the decoder. The encoder\nphase involves utilizing multi-head self-attention layers in a stacked arrangement to\ndecode the intrinsic information within the input sequence, thereby yielding latent rep-\nresentations. In the subsequent phase, the decoder applies cross-attention mechanisms\n71\nto these representations, facilitating the generation of the target sequence. This innova-\ntive architecture is extensively applied in sequence-to-sequence modeling tasks, such as\nneural machine translation. In a distinctive approach, BERT (Kenton and Toutanova,\n2019) is engineered for the pretraining of deep bidirectional representations from the\nunlabeled text. It uniquely processes both left and right contexts concurrently across\nall layers, rendering it exceptionally versatile for a plethora of NLP tasks. Conversely,\nBART (Lewis et al., 2020) incorporates a conventional Transformer-based architecture\nfor neural machine translation. While its structure may appear straightforward, BART\ncan be regarded as an evolution of BERT, amalgamating the bidirectional encoder of\nBERT and the unidirectional, left-to-right decoder of GPT, along with other advanced\npretraining methodologies. Furthermore, Pre-trained Language Models (PLMs) fol-\nlowing the encoder-decoder architecture paradigm, exemplified by T5 (Raffel et al.,\n2019), have consistently showcased remarkable performance across a wide array of\nNLP tasks.\nDecoder-only Architecture\nThe decoder-only architecture is characterized by its strategic use of an attention\nmask, a pivotal element that ensures each input token is exclusively attentive to pre-\nceding tokens, including itself. This unique configuration facilitates a unidirectional\nflow of information from antecedent tokens to the current token within the decoder,\nthereby streamlining the processing of input and output tokens. This approach not\nonly simplifies the learning mechanism but also bolsters the model’s coherence and\nconsistency. In the domain of language modeling, the GPT (Generative Pre-trained\nTransformer) series epitomizes the decoder-only architecture. This series encompasses\nGPT-1 (Radford et al., 2018), GPT-2 (Radford et al., 2019), and the notably advanced\nGPT-3 (Brown et al., 2020). GPT-3, in particular, serves as a quintessential model\nwithin this paradigm, exemplifying the architectural efficacy, especially in in-context\nlearning, a distinguishing feature of Large Language Models (LLMs). The decoder-\nonly architecture’s influence transcends the GPT lineage, significantly impacting the\nbroader field of LLMs. Numerous cutting-edge language models have adopted this\narchitectural framework as their foundational structure. For instance, OPT (Zhang\net al., 2022a) employs the decoder-only architecture to achieve commendable natu-\nral language understanding capabilities. Gopher (Rae et al., 2021) also leverages this\nunidirectional flow to escalate the complexity and scale of language modeling tasks.\nMoreover, the decoder-only architecture has been instrumental in the evolution of\nmodels like BLOOM (Scao et al., 2022), which utilize its unidirectional information\nflow for tasks necessitating contextual comprehension. LLaMA (Touvron et al., 2023a)\nand its successor, LLaMA-2 (Touvron et al., 2023b), have integrated this architectural\nstyle to propel advancements in language modeling, achieving remarkable perfor-\nmances across various NLP benchmarks. GLM (Zeng et al., 2022) further underscores\nthe decoder-only architecture’s efficacy in a range of language understanding tasks,\nunderscoring its vital role in the contemporary landscape of language modeling.\n72\nCLIP Variants\nCLIP (Radford et al., 2021) employs an innovative approach by simultaneously train-\ning an image encoder and a text encoder to infer the correct pairings among a set\nof <image, text> pairs. This strategy forms the bedrock of its learning process. In\ncontrast, FILIP (Yao et al., 2021) enhances alignment at a finer granularity by incorpo-\nrating a cross-modal late interaction mechanism. This mechanism employs token-wise\nmaximum similarity measurements between visual and textual tokens to provide guid-\nance for the contrastive objective, resulting in more precise alignments. FLIP (Li\net al., 2023q) introduces a groundbreaking training technique that involves randomly\nmasking and removing a significant portion of image patches. This approach increases\nthe number of image-text pairs that can be learned within the same wall-clock time,\nenabling more samples to be contrasted per iteration without significantly increasing\nmemory usage. On the language encoder side, K-Lite (Shen et al., 2022) suggests incor-\nporating external knowledge in the form of Wiki definitions for entities in combination\nwith their original alt-text for contrastive pre-training. Empirical evidence indicates\nthat enriching text descriptions in this manner leads to improved CLIP performance.\nLaCLIP (Fan et al., 2023) leverages the in-context learning ability of large language\nmodels to rewrite text descriptions for their associated images, further enhancing the\nmodel’s performance by aligning descriptions more effectively with the visual con-\ntent. DetCLIP, as introduced in Yao et al. (2022), represents a pioneering approach in\nparallel visual-concept pre-training for open-world detection. It leverages knowledge\nenrichment from a meticulously crafted concept dictionary. Meanwhile, its successor,\nDetCLIPv2 (Yao et al., 2023a) capitalizes on the maximum word-region similarity\nbetween region proposals and textual words to steer the contrastive objective.\nOther Architectures\nTraditional Transformer architectures are often limited by their quadratic computa-\ntional complexity. To address this, recent research has focused on developing more\nefficient language modeling architectures. The S4 model (Gu et al., 2021) offers an\ninnovative solution by applying a low-rank correction to condition the state matrix,\nthus stabilizing its diagonalization and reducing the complexity of the state space\nmodel (SSM) to operations akin to a Cauchy kernel. Similarly, GSS (Mehta et al.,\n2022) emerges as a compelling alternative to the S4 and DSS (Gupta et al., 2022)\nmodels, with the advantage of markedly faster training times. In contrast, H3 (Dao\net al., 2022) is designed to excel in specific functions like recalling earlier tokens in the\nsequence and comparing tokens across the sequence, further enhancing its efficiency\nthrough the integration of FlashCov. For those exploring subquadratic alternatives\nto attention mechanisms, Hyenra (Poli et al., 2023) offers a notable solution. This\nmodel is crafted by combining implicitly parametrized long convolutions with data-\ncontrolled gating, significantly diminishing computational requirements. RWKV (Peng\net al., 2023a) utilizes a linear attention mechanism, allowing the model to function\nas either a Transformer or an RNN. This approach not only facilitates parallelized\ncomputations during training but also ensures constant computational and memory\ncomplexity during inference, marking it as the first non-transformer architecture scal-\nable to tens of billions of parameters. RetNet (Sun et al., 2023f) represents another\n73\nsignificant contribution, striking an optimal balance between training parallelism, cost-\neffective inference, and robust performance. LongNet (Ding et al., 2023a) introduces\ndilated attention, a technique that significantly widens the attention field as the dis-\ntance between tokens increases, thereby enabling effective scaling of sequence length\nto over a billion tokens. Lastly, Streaming-LLM (Xiao et al., 2023b) presents an effi-\ncient framework that allows Language Models (LLMs) trained with a finite-length\nattention window to adapt to infinite sequence lengths without additional fine-tuning.\nThis breakthrough has extended the sequence length capability of these models to 4\nmillion tokens.\n4.2 Fine-Tuning\nA fundamental strategy employed by Large Language Models (LLMs) revolves around\nthe concept of pre-training on extensive general domain data, followed by customiz-\ning the model to suit particular tasks or domains. This approach endows LLMs with\na comprehensive understanding of language patterns, enabling them to subsequently\nfine-tune their performance across a broad spectrum of downstream tasks, including\nnatural language understanding, generation, and translation. The process of adapta-\ntion assumes paramount significance in achieving exceptional results in these specific\ntasks, as it empowers the LLM to leverage its previously acquired knowledge and\napply it to new instances. The adaptation process encompasses a variety of techniques,\nranging from thorough fine-tuning of the pre-trained model to the incorporation of\ntask-specific layers or modules, as well as the utilization of transfer learning methods\nlike knowledge distillation.\n4.2.1 Data Source\nBenchmark Data\nA natural step in the process of data collection entails the adaptation of pre-existing\nNLP benchmarks. Given that these benchmarks are open-source, researchers find it\nboth more convenient and cost-effective to utilize reasoning benchmarks to bolster\nthe model’s reasoning capabilities. However, challenges arise concerning the availabil-\nity of benchmarks in terms of quantity and scale, and the manual creation of new\nbenchmarks proves to be a resource-intensive task. To tackle this issue, researchers\nare devising strategies to generate fine-tuning data for reasoning synthesis using an\nadvanced language model.\nSynthesis Data\nThis section delves into the synthesis of reasoning data utilizing advanced Large Lan-\nguage Models (LLMs) and subsequently harnesses the generated data for fine-tuning.\nThe core of this research revolves around the application of Chain-of-Thought (CoT)\ntechniques to LLMs, leading to the creation of reasoning paths. Subsequently, the\ngenerated data is leveraged for the model fine-tuning (Fu et al., 2023b; Hsieh et al.,\n2023; Huang et al., 2022a; Li et al., 2022d; Magister et al., 2023). Additionally, the\nFinetune-CoT method, as introduced by Ho et al. (2022), involves the sampling of\n74\nmultiple reasoning paths from LLMs, which are then used for fine-tuning student mod-\nels with the correct ones. The Distilling step-by-step approach, proposed by Hsieh\net al. (2023), introduces a novel mechanism with two primary objectives: (a) training\nsmaller models surpassing LLMs and (b) achieving this feat with reduced training data\nrequirements for fine-tuning or distillation. Furthermore, the Self-Improve approach, as\ndetailed in Huang et al. (2022a), includes the selection of rationale-augmented answers\nwith the highest confidence for unlabeled questions using Chain-of-Thought prompting\nand self-consistency. Subsequently, the LLM is fine-tuned using these self-generated\nsolutions as target outputs, with the additional step of feeding the question and\nground-truth label to LLMs to prompt their reasoning path. An alternative approach\ninvolves leveraging several examples with human-written explanations as demonstra-\ntions of LLMs, followed by the generation of explanations for the training set (Li et al.,\n2022d). Notably, this research provides evidence supporting the feasibility of fine-\ntuning a student model based on the chain of thought outputs generated by a larger\nteacher model, resulting in improved task performance across various types of reason-\ning datasets, including arithmetic, commonsense, and symbolic reasoning (Magister\net al., 2023). In the domain of mathematics, the WizardMath framework (Luo et al.,\n2023c), introduces a novel method termed Reinforcement Learning from Evol-Instruct\nFeedback (RLEIF). This approach initially generates diverse math instruction data\nusing math-specific Evol-Instruct. Subsequently, it involves the training of an instruc-\ntion reward model (IRM) and a process-supervised reward model (PRM) (Yuan et al.,\n2023a; Lightman et al., 2023). The IRM assesses the quality of the evolved instruc-\ntion, while the PRM receives feedback for each step in the solution. Furthermore,\nMetaMath (Yu et al., 2023b) introduces an innovative question bootstrapping method\n(such as forward-backward augmentation (Jiang et al., 2023c)) to augment the train-\ning dataset, resulting in MetaMathQ. This method entails the rewriting of questions\nwith both forward and backward reasoning paths and utilizes LLMs to rephrase the\nquestion text. Lastly, MAmmoTH, as introduced by Yue et al. (2023), presents a new\nmath hybrid instruction-tuning dataset named MathInstruct. This dataset boasts two\nsignificant characteristics: extensive coverage of diverse math fields and complexity\nlevels, as well as the incorporation of hybrid Chain-of-Thought (CoT) and Process-of-\nThought (PoT) rationales. The paper ”Orca” by Mukherjee et al. (2023) introduces a\nmethod called explanation tuning. This approach involves fine-tuning a model using\npairs of queries and responses. The responses are augmented with detailed explana-\ntions from GPT-4, clarifying the teacher model’s reasoning process as it generates each\nresponse. In a subsequent work, ”Orca2” by Mitra et al. (2023), a technique named\nPrompt Erasing is proposed. This method involves modifying the training process by\nreplacing the specific instructions provided to the student system with generic ones,\nomitting specific details about how to execute the task.\n4.2.2 Parameter-Efficient Fine-tuning\nOne of the fundamental paradigms in building foundation models entails thorough\npre-training on general domain data, succeeded by customization for specific tasks or\ndomains. As model sizes continue to grow, conducting comprehensive fine-tuning that\nalters all model parameters becomes progressively unfeasible. Hence, the importance\n75\n(a) LoRA (b) Prompt Tuning\n(d) Mixture-of-Modality Adaption (MMA)\n🔥Imageencoder\nLLM\nVisualadapter\n🔥Adapter\n 🔥MM-Adapter\nInput\n𝑾𝑸\n 𝑾𝒖\n𝑾𝒅\n🔥\n🔥\nOutput\nAdapter …\nLayer 1\nLayer N\nInputPrompt+\n🔥\n…\nLayer 1\nLayer N\nSSF-ADA\n🔥\nInput\n⨁⨀\nOutput\nscaleshift𝛾𝛽\n(c) Scaling & Shifting Features (SSF)\nFig. 16 : Illustrations of different parameter-efficient training approaches. (a) Low-\nRank Adaptation (LoRA) maintains the original weights of the pre-trained model\nunchanged, while integrating trainable matrices based on rank decomposition into\nevery layer of the network for adjusting parameters. The figure credits from LoRA (Hu\net al., 2022). (b) Prompt tuning incorporates trainable prompt vectors at the input\nlayer and uses the prompt-augmented input to tackle specific downstream issues. (c)\nSSF only needs to scale and shift the deep features extracted by a pre-trained network\nfor parameter-efficient fine-tuning. The figure credits from SSF (Lian et al., 2022).\n(d) MMA trains lightweight adapters to bridge the gap between large language models\nand vision-language tasks to enable the joint optimization of the vision and language\nmodels. The figure credits from MMA (Luo et al., 2023b).\nof parameter-efficient fine-tuning in efficiently refining foundation models cannot be\nemphasized enough. Some representative approaches of different types of techniques\nare shown in Figure 16.\nAdapter Tuning\nAdapter tuning, a technique that employs specialized neural network modules called\n“adapters” within Transformer models, is discussed in Houlsby et al. (2019). An\ninnovative adaptation method, LLaMA-Adapter (Zhang et al., 2023h), has been\ndeveloped to effectively fine-tune LLaMA models for instruction-following tasks.\nLLaMA-Adapter showcases its efficiency by introducing only 1.2 million learnable\n76\nparameters into the pre-trained LLaMA 7B model, utilizing 52,000 self-instruct\ndemonstrations, and completing the fine-tuning process in under an hour using 8 A100\nGPUs. MAD-X (Pfeiffer et al., 2020) is an adapter-based framework which is designed\nto learn modular language and task representations that can be adapted to various\ntasks and languages with high portability and transfer of high parameter-efficiency.\nOn the other hand, AdaMix (Wang et al., 2022f) fine-tunes a mixture of adaptation\nmodules in each transformer layer while keeping the majority of PLM weights frozen.\nCompacter (Karimi Mahabadi et al., 2021) integrates task-specific weight matrices\ninto the weights of a pre-trained model, which can be efficiently obtained as a sum of\nKronecker products between shared “slow” weights and “fast” rank-one matrices as\ndefined in each Compacter layer. Lastly, He et al. (2021) introduce a unified framework\nthat establishes connections between these approaches.\nLow-Rank Adaptation\nLow-Rank Adaptation (LoRA) (Hu et al., 2022), shown in Figure 16(a), offers a dis-\ntinctive approach aimed at reducing the number of trainable parameters in pre-trained\nTransformer models when applied to downstream tasks. This technique involves the\nfreezing of pre-trained model weights and the introduction of trainable rank decom-\nposition matrices into each layer of the Transformer architecture. While low-rank\ndecomposition has limitations in terms of representation power, KronA (Edalati et al.,\n2022) opts for the Kronecker product as an alternative to low-rank representation.\nAdaLoRA (Zhang et al., 2023g) parametrizes incremental updates through singular\nvalue decomposition, allowing for the effective pruning of unimportant singular values.\nDyLoRA (Valipour et al., 2022) adopts an alternative method, focusing on training\nLoRA blocks across a spectrum of ranks instead of just one. This is done by organizing\nthe representations acquired by the adapter module at different ranks throughout the\ntraining process. For those in search of an efficient fine-tuning solution, “Efficient Fine-\ntuning of Quantized LLMs” (QLoRA) (Dettmers et al., 2023) provides an appealing\noption. QLoRA enables the fine-tuning of models with up to 65 billion parameters on\na single 48GB GPU, making it a practical and accessible choice for both researchers\nand practitioners. To enable more complex reasoning tasks, LongLoRA (Chen et al.,\n2023k) offers a novel method to expand the input context sizes of large language\nmodels while maintaining computational efficiency and performance integrity.\nPrompt Tuning\nPrefix tuning, initially introduced in Li and Liang (2021), extends Transformer-based\nlanguage models by appending a sequence of trainable continuous vectors, known\nas “prefixes” to each layer. It lays the foundation for prompt tuning, a concept\nakin to “prefix tuning” (Lester et al., 2021), with a primary focus on integrating\ntrainable prompt vectors exclusively at the input layer. Prompt tuning represents a\nsimple yet highly effective approach for obtaining “soft prompts” that empower fine-\ntuned language models to excel in specific downstream tasks, e.g., classification (Yang\net al., 2022a), which has been illustrated in Figure 16(b). In a similar context,\nOptiPrompt (Zhong et al., 2021) operates within the continuous embedding space\nto optimize performance. On the other hand, P-tuning (Liu et al., 2023m) leverages\n77\ntrainable continuous prompt embeddings alongside discrete prompts, demonstrating\neffectiveness across both pre-trained and fine-tuned language models, whether in fully\nsupervised or few-shot settings. An evolution of this concept, P-tuning V2 (Liu et al.,\n2021a), proposes the integration of continuous prompts into every layer of the pre-\ntrained model, not restricting itself solely to the input layer. This extension provides\na comprehensive approach to harnessing continuous prompts throughout the model’s\narchitecture.\nPartial Parameter Tuning\nIn contrast to the aforementioned approaches that emphasize parameter efficiency,\npartial parameter tuning distinguishes itself by not introducing any additional com-\nponents but rather by selectively fine-tuning specific parameters within the original\nmodel. Bitfit (Zaken et al., 2021) exemplifies this concept as a method for sparse\nfine-tuning, concentrating solely on adjusting the bias terms of the model. Child-\nTuning (Xu et al., 2021a) adopts a strategic approach to parameter adaptation. It\ntargets a subset of parameters known as the “child network” within large pre-trained\nmodels while carefully masking out gradients from the non-child network during the\nbackward pass. In the case of SSF (Lian et al., 2022), corresponding to Figure 16(c),\nthe method introduces learnable parameters in training. These extra parameters\ncan be seamlessly integrated into the original pre-trained model weights through re-\nparameterization at inference, with modifications applied to either the complete set or\na subset of these parameters. DiffFit (Xie et al., 2023b), on the other hand, presents a\nparameter-efficient fine-tuning strategy tailored for large pre-trained diffusion models.\nThis method enables rapid adaptation to new domains by fine-tuning bias terms and\nincorporating newly introduced scaling factors into specific layers of the model. Fu\net al. (2023d) theoretically analyze the parameter sparsity in fine-tuning approaches\nand design SAM to optimize the selection of suitable parameters.\nMixture-of-Modality Adaption\nLuo et al. (2023a,b) have developed a pioneering method for fine-tuning vision-\nlanguage models, termed Mixture-of-Modality Adaptation (MMA). Illustrated in\nFigure 16(d), MMA serves as a comprehensive optimization framework that uni-\nfies the image encoder with Large Language Models (LLMs) via efficient adapters.\nThis work also introduces a cutting-edge routing algorithm in MMA, enabling the\nmodel to dynamically modify its reasoning pathways for both single- and multimodal\ninstructions. Utilizing MMA, the authors have created LaVIN (Luo et al., 2023b),\na significant vision-language instructed model that exhibits enhanced training effi-\nciency and improved reasoning abilities across a range of instruction-following tasks.\nLaVIN demonstrates superior performance compared to existing multimodal LLMs.\nThe MMA methodology and LaVIN model hold considerable potential in augmenting\nthe utility of vision-language models, particularly in applied fields like robotics and\nautonomous systems. In a similar context, LLaMA-Adapter V2 (Gao et al., 2023c)\nrepresents a visual instruction model that focuses on parameter efficiency and the\nseamless integration of visual information. This model incorporates several strategies\nto boost its performance, including expanding its learnable parameter set, adopting\n78\nan early fusion approach to integrate visual tokens into the initial layers of LLMs,\nand applying a joint training approach for both image-text pairings and instruction-\nfollowing datasets. Alternatively, LLaVA (Liu et al., 2023e) presents itself as an\nintegrated multimodal model that undergoes an end-to-end training process. LLaVA\nlinks a vision encoder and an LLM to process a wide spectrum of tasks involving both\nvision and language comprehension. LLaVA-1.5 (Liu et al., 2023d) introduces relatively\nstraightforward adjustments, like utilizing CLIP-ViT-L-336px with an MLP projec-\ntion and integrating task-specific VQA data with basic response formatting prompts.\nThese modifications enable LLaVA-1.5 to set a robust baseline performance, achieving\ntop-tier results across 11 benchmark tasks.\n4.3 Alignment Training\nThe methodology of alignment training introduces an innovative approach that\nemploys learning techniques to optimize language models using human feedback\ndirectly. This concept has initiated a new paradigm in which language models are fine-\ntuned to correspond with intricate human values more closely. While Large Language\nModels (LMs) can be prompted to execute a variety of natural language process-\ning (NLP) tasks based on given examples, they often manifest unintended behaviors.\nThese include generating fictitious information, creating biased or offensive text, or\nfailing to comply with user directives. Such discrepancies stem from the divergence\nbetween the traditional language modeling objective—predicting the next token from\nthe web-based text—and the goal of “following user instructions in a manner that\nis both helpful and safe.” This incongruity suggests a misalignment in the language\nmodeling objective. Rectifying these unintentional behaviors is critically important,\nespecially given the widespread application of language models in numerous domains.\n4.3.1 Data Source\nWe define the data as dk = (ik, yk), where ik represents the instruction and yk denotes\nthe corresponding response.\nHuman Data\nDatabricks has curated a comprehensive crowd-sourced instruction dataset known as\n“databricks-dolly-15k” (Conover et al., 2023), containing a total of 15,000 instructions.\nIn addition to this, the OpenAssistant corpus (K¨ opf et al., 2023) consists of more\nthan 10,000 dialogues, involving the participation of over 13,000 international anno-\ntators. UnifiedQA (Khashabi et al., 2020) has undergone evaluation across 20 diverse\ndatasets, covering various linguistic phenomena. CrossFit (Ye et al., 2021) has been\nestablished as an NLP benchmark, encompassing 160 tasks converted from publicly\navailable NLP datasets into a unified text-to-text format. P3 (Sanh et al., 2021) has col-\nlected over 2,000 English prompts from more than 270 datasets, while MetaICL (Min\net al., 2022) has conducted experiments across 142 NLP datasets with seven differ-\nent meta-training and target splits. ExMix (Aribandi et al., 2022) offers a diverse set\nof 107 supervised NLP tasks. The Natural Instructions dataset (Mishra et al., 2022)\ncomprises 61 tasks, and Super-NaturalInstructions (Wang et al., 2022e) expands upon\n79\nit with over 1.5k tasks. Flan 2022 (Longpre et al., 2023) combines various sources for\ninstruction tuning, adapting templates to achieve strong evaluation performance. xP3\n(Crosslingual Public Pool of Prompts) (Muennighoff et al., 2022) is a collection of\nprompts and datasets spanning 46 languages and 16 NLP tasks, which aids multilin-\ngual models BLOOMZ and mT0 in zero-shot instruction-following. LongForm (K¨ oksal\net al., 2023) selects 15,000 target text examples from the C4 and English Wikipedia\ncorpus. Furthermore, ShareGPT, a website, actively encourages users to share their\nengaging ChatGPT/GPT4 conversations, resulting in a wealth of diverse, human-\nauthored instructions capable of eliciting high-quality ChatGPT/GPT4 responses. To\ncreate non-English datasets, the Open Instruction Generalist (COIG) (Zhang et al.,\n2023b) translates English instructions into Chinese and utilizes annotators to rectify\nand reorganize the instructions.\nSynthesis Data\nGathering data from human sources can be a resource-intensive and time-consuming\nprocess. Given the remarkable success of Large Language Models (LLMs) like GPT-\n4, utilizing LLM responses to formulate instructions for training other LLMs in\nReinforcement Learning from Human Feedback (RLHF) has become increasingly\nviable.\nPioneering work in this area, as demonstrated by Self-Instruct (Wang et al., 2022c),\nharnesses the in-context learning capability of ChatGPT to generate a substantial\nvolume of instructions. These instructions are drawn from a predefined set of human-\nannotated examples, spanning a wide range of topics and task types. Building upon\nthis approach, Aplaca (Taori et al., 2023) and its various iterations (Peng et al.,\n2023c; Chiang et al., 2023) employ LLMs to generate numerous training pairs for\nRLHF. Instruction Backtranslation (Li et al., 2023o) leverages Self-augmentation to\ncreate responses along with instructions and utilizes Self-curation to generate instruc-\ntions based on responses. Unnatural Instructions (Honovich et al., 2022) stands out\nas a substantial dataset of innovative instructions, comprising 64,000 examples gen-\nerated by LLMs through seed examples and rephrasing, resulting in a dataset of\napproximately 240,000 instances. The OPT-IML Bench (Iyer et al., 2022) serves as a\nbenchmark for Instruction Meta-Learning (IML), featuring 2,000 tasks derived from\neight existing benchmarks. It evaluates model generalizations using the vanilla GPT-\n3’s Self-Instruct approach, yielding over 52,000 instructions and 82,000 instances.\nKoala (Geng et al., 2023) is a small yet high-quality dataset curated from vari-\nous sources, including ChatGPT Distillation Data, resulting in a comprehensive and\ndiverse dataset. GPT4All (Anand et al., 2023) comprises approximately one million\nprompt-response pairs from the GPT-3.5-Turbo OpenAI API, spanning the period\nfrom March 20, 2023, to March 26, 2023. Alpaca-GPT4 (Peng et al., 2023c) includes\n52,000 examples of instruction-following in both English and Chinese. It incorporates\nfeedback data from GPT-4 to enhance zero-shot performance. LaMini-LM (Wu et al.,\n2023d) contains a vast dataset of 2.58 million instruction-response pairs generated\nby the GPT-3.5-Turbo model. These pairs are drawn from various prompt sources\nto ensure diversity. CoEdIT (Raheja et al., 2023) is a system that offers an 82,000\ndataset of <instruction: source, target > pairs for text editing model training and\n80\nTrain\nPretrained LLM\nReward modelComparisondata\nSFT model\n🔥\nPreference\nReinforcement learning\nPrompts\nSFT model Final modelSFT Human Preference Training\nOnline human preference training\nMaximize\nSFT model\n🔥\nPreference\nPrompts\nOffline human preference training\nMaximize\nFig. 17: The development process for large language model’s (LLM’s) alignment train-\ning. First, LLM is conventionally optimized via Supervised Fine-Tuning (SFT) using\nhigh-quality instruction data. Then, it may be further adjusted through Human Pref-\nerence Training. The related techniques include online human preference training (left)\nthat needs reinforcement learning and offline ones (right) that directly optimizes the\npolicy to satisfy the preferences best.\nevaluation. UltraChat (Ding et al., 2023e) is an open-source collection of multi-round\ndialogues, including a million-scale multi-turn instructional conversation data. CoT-\nCollection (Kim et al., 2023) augments Chain-of-Thought (CoT) rationales with 1.88\nmillion instances from the FLAN Collection (Longpre et al., 2023). Dynosaur (Yin\net al., 2023a) is a dynamic paradigm for data curation in instruction tuning, con-\ntinuously expanding by incorporating new datasets from the Huggingface Datasets\nPlatform.\n4.3.2 Training Pipeline\nA common method for enhancing Large Language Models (LLMs) to more accu-\nrately interpret and respond to human intentions through specific guidance is known\nas Supervised Fine-Tuning (SFT). This technique involves processing an instructional\ninput, labeled as x, and then calculating the cross-entropy loss in relation to the actual\ncorrect response, denoted as y. The main role of SFT is to assist LLMs in under-\nstanding the deeper meanings within text prompts and to produce appropriate replies.\nHowever, a significant drawback of SFT is its lack of capacity to make detailed distinc-\ntions between the best and less ideal responses. Overcoming this challenge necessitates\nadditional training strategies, such as incorporating human preference training.The\noverall training pipeline is presented in Figure 17.\nOnline Human Preference Training\nReinforcement Learning from Human Feedback (RLHF)(Ouyang et al., 2022) rep-\nresents a strategy developed to interpret human preferences by incorporating\nadditional reward models within the framework of Proximal Policy Optimization\n81\n(PPO)(Schulman et al., 2017). RLHF is divided into three primary phases: 1) The ini-\ntial stage includes the creation of a comprehensive set of guidelines and the application\nof Supervised Fine-Tuning (SFT) on pre-existing Large Language Models (LLMs); 2)\nThe next phase involves human evaluators who manually grade pairs of responses,\naiding in the development of a reward model that evaluates the effectiveness of the\nresponses generated; 3) Lastly, the SFT model (policy) undergoes refinement through\nPPO, leveraging the rewards determined by the reward model.\nWhile the PPO framework is known for its effectiveness in learning human prefer-\nences, it can present challenges and exhibit less stability during training. An alternative\napproach, Reward Ranked Fine-Tuning (RAFT) (Dong et al., 2023a), initially involves\nsampling a substantial batch of instructions. Subsequently, responses are generated by\nthe current LLMs, and the resulting data is ranked using a reward model. Only the top\ninstances, as determined by the reward model, are then used for SFT. Additionally,\nAdvantage-Induced Policy Alignment (APA) (Zhu et al., 2023a) employs a squared\nerror loss function based on estimated advantages, offering an alternative perspective\non policy alignment within the RLHF framework.\nOffline Human Preference Training\nThe implementation of those online algorithms can often be challenging due to the\nintricate interactions required between policy, behavior policy, reward, and value\nmodels. This complexity necessitates the adjustment of numerous hyperparameters\nto strengthen performance. To mitigate this problem, offline learning of human\npreferences has been studied.\nOne such approach is Direct Preference Optimization (DPO) (Rafailov et al., 2023),\nwhich aims to implicitly optimize the same objective as existing Reinforcement Learn-\ning from Human Feedback (RLHF) algorithms. Preference Ranking Optimization\n(PRO) (Song et al., 2023b) takes this further by fine-tuning Large Language Models\n(LLMs) to better align with human preferences and introduces Supervised Fine-\nTuning (SFT) training objectives for regularization. Sequence Likelihood Calibration\n(SLiC) (Zhao et al., 2022a) focuses on adjusting the probability of sequences created\nby the model to more closely match those of reference sequences within the model’s\nlatent space. In contrast, Rank Responses to align Human Feedback (RRHF) (Yuan\net al., 2023b) aligns model probabilities of multiple responses with human preferences\nusing ranking loss, providing a simpler yet effective alternative that retains the perfor-\nmance of the Proximal Policy Optimization (PPO) algorithm. (Wang et al., 2023m)\npresent the Alignment Fine-Tuning (AFT) method, which involves fine-tuning Large\nLanguage Models (LLMs) with Chain-of-Thought training data, categorizing gener-\nated responses into positive or negative based on correctness, and adjusting response\nscores using a novel constraint alignment loss.\n4.4 Mixture of Experts (MoE)\nAs depicted in Figure 18, the Mixture of Experts (MoE) model represents a sophisti-\ncated supervised learning framework consisting of an array of networks, each fine-tuned\nto process a specific segment of the complete training dataset (Jacobs et al., 1991).\n82\nMOELayerMOELayerExp 1Exp 1Exp n-1 Exp n\nGatingNetwork\nx x+\n……\nFig. 18: A Mixture of Experts (MoE) layer within a recurrent language model. In this\nscenario, we employ a sparse gating function to select a pair of experts for performing\nthe required calculations. The figure credits from (Shazeer et al., 2017).\nIn this architecture, individual examples are processed by their respective expert net-\nworks. The Sparsely-Gated Mixture-of-Experts model (Shazeer et al., 2017) integrates\nthousands of feed-forward sub-networks and employs a selective mechanism to engage\na sparse array of these experts for each data instance. This methodology culminates\nin a model with an astounding 137 billion parameters, assigning a singular expert to\nevery example. The model achieves sparsity through a gating function that directs\neach input to the top-K experts, where K is at least 2. Expanding upon this concept,\nGShard (Lepikhin et al., 2020) adapts the MoE paradigm for transformers by substi-\ntuting each feed-forward layer with a pairwise MoE layer, equipped with a Top-2 gating\nnetwork. In a different approach, Switch Transformers (Fedus et al., 2022) refine the\nMoE’s sparsity by selecting either the optimal experts or a single best expert (where\nK equals 1) for each input. Additionally, GaLM (Du et al., 2022a) leverages a sparsely\nactivated MoE architecture to amplify model capacity while substantially reducing\ntraining costs compared to denser models. The largest variant of GaLM boasts a\nremarkable 1.2 trillion parameters, significantly surpassing GPT-3 in scale. MoE has\nalso been effectively implemented to enhance the capabilities of vision models (Chen\net al., 2023m,e,n). Moreover, MoE finds application in network compression strate-\ngies. WideNet (Xue et al., 2022) represents a parameter-efficient method that utilizes\nparameter sharing for compression along the network’s depth. To optimize model-\ning capacity, WideNet scales the model’s width by replacing standard feed-forward\nnetworks with a MoE structure and incorporating distinct layer norms to effectively\nprocess diverse semantic representations. MoEBERT (Zuo et al., 2022) adopts a sim-\nilar strategy, transforming the feed-forward neural networks in a pre-trained model\ninto multiple experts. This modification maintains the robust representational abili-\nties of the pre-trained model while integrating layer-wise distillation during training.\nIn inference, a single expert is activated to optimize performance.\n83\nIn-Context Learning\nDemonstration example selectionPrior-Knowledge approach\nRetrieval approach\nChain-of-Thought\nZero-Shot\nZero-shot CoT\nPlan-and-Solve Prompting\n...\nFew-Shot\nCoT\nLeast-to-Most\nComplex CoT\nAuto-CoT\nProgram-of-Thought\nTree-of-Thought\nGraph-of-Thought\nSkeleton-of-Thought\n...\nMultiple paths aggregation\nDIRVERSE\nSelf-Consistency\nAdaptive-Consistency\nModel Selection\nSelf-Evaluation Guided Decoding\nReasoning via Planning\n...\nMulti-Round prompting\nLearned refiners\nSelf-Correct\nLLM-AUGMENTER\n...\nPrompted refiners\nSelf-Refine\nSelf-Debug\nProgressive-Hint Prompting\n...\nFig. 19: Common techniques for In-Context Learning.\n4.5 In-Context Learning\nAs described in Brown et al. (2020), In-Context Learning (ICL) is a method that\nutilizes a meticulously crafted natural language prompt encompassing both the task\ndescription and a subset of task examples to provide demonstrations. This process\ncommences with the task description, followed by the careful selection of a few exam-\nples from the task dataset to serve as demonstrations. These chosen instances are then\nintricately arranged into natural language prompts using thoughtfully designed tem-\nplates. Subsequently, the test instance is combined with these demonstrations as input\nfor Language Models or Vision-Language Models (Chen et al., 2023l) to generate the\ndesired output. Leveraging the provided task demonstrations, LLMs can effectively\ndiscern and execute novel tasks without the need for explicit gradient updates. It is\nimportant to note that ICL shares a fundamental connection with instruction tuning,\nas both methods use natural language to structure tasks or instances. Nevertheless,\ninstruction tuning necessitates the fine-tuning of LLMs to adapt models, whereas ICL\nrelies purely on prompting LLMs for their applications. Moreover, it’s important to\nnote that instruction tuning can improve the ICL abilities of LLMs for executing\n84\nspecific tasks, particularly in zero-shot situations where only task descriptions are pro-\nvided (Chung et al., 2022). A diverse set of common techniques are introduced next\nand listed in Figure 19.\n4.5.1 Demonstration Example Selection\nThe effectiveness of In-Context Learning (ICL) often exhibits considerable variability\nbased on the choice of demonstration examples. Therefore, it becomes crucial to care-\nfully select a subset of examples that can truly harness the ICL capacity of Language\nModels (LLMs). Two primary methods for demonstration selection are prevalent:\nheuristic approaches and LLM-based approaches, as explored in the works of Liu et al.\n(2022b) and Lee et al. (2022).\nPrior-Knowledge Approach Due to their cost-effectiveness and simplicity,\nheuristic techniques have been widely adopted in previous research for the selection\nof demonstrations. Many studies have integrated k-NN-based retrievers to identify\nsemantically relevant examples for specific queries, as evidenced by Liu et al. (2022b)\nand Lee et al. (2022). However, it is important to note that these approaches typically\noperate on a per-example basis, lacking a holistic evaluation of the entire example set.\nTo overcome this limitation, diversity-centric selection strategies have been introduced\nto curate a subset of examples that collectively represent the spectrum of specific tasks,\nas explored in the works of Levy et al. (2022) and Hongjin et al. (2022). Moreover,\nresearch conducted by Ye et al. (2022) takes into account both relevance and diversity\nin the demonstration selection process. Intriguingly, Complex CoT (Fu et al., 2022)\nadvocates the inclusion of intricate examples that involve extensive reasoning steps,\nwhile Auto-CoT (Zhang et al., 2022c) suggests the sampling of a more diverse set of\nexamples for demonstration.\nRetrieval Approach Another area of research is dedicated to harnessing the\ncapabilities of Language Models (LLMs) in selecting demonstrations. For instance,\nLLMs can be employed to directly assess the informativeness of each example by quan-\ntifying the performance improvement resulting from its inclusion, as demonstrated\nby Li and Qiu (2023a). In a related vein, Rubin et al. (2022) introduce an approach\ncalled EPR, which involves a two-stage retrieval process. Initially, EPR recalls sim-\nilar examples through an unsupervised method and subsequently ranks them using\na dense retriever. Building upon this, Dr.ICL (Luo et al., 2023e) applies the EPR\napproach to a broader spectrum of evaluation tasks, encompassing QA, NLI, MathR,\nand BC. Within the context of in-context learning, Compositional Exemplars for In-\ncontext Learning (CEIL) (Ye et al., 2023a) utilizes Determinantal Point Processes\n(DPPs) to learn the interaction between input and in-context examples. This model\nis optimized using a well-crafted contrastive learning objective. Additionally, LLM-\nR (Wang et al., 2023l) adopts a ranking method for retrieved candidates, relying on\nthe conditional LLM log probabilities of the ground-truth outputs. It employs a cross-\nencoder-based reward model for capturing fine-grained ranking signals from LLMs,\nand a bi-encoder-based dense retriever trained through knowledge distillation. The\nUnified Demonstration Retriever (UDR) (Li et al., 2023n) utilizes a shared demon-\nstration retrieval model to overcome the issue of non-transferability among retrievers\nacross different tasks. UDR ranks candidate examples based on LLM’s feedback. With\n85\ntrained retrievers, DQ-LoRe (Xiong et al., 2023a) utilize Dual Queries and Low-rank\napproximation Re-ranking to automatically select exemplars for in-context learning.\n4.5.2 Chain-of-Thought\nZero-Shot CoT\nZero-shot CoT (Kojima et al., 2022) introduces a novel approach to enhance model\nreasoning abilities by incorporating additional sentences. For instance, empirical evi-\ndence has demonstrated that including the phrase “Let’s think step by step” can\nsignificantly boost the model’s reasoning skills. In a similar vein, Plan-and-Solve (PS)\nPrompting (Wang et al., 2023k) presents a two-fold strategy. First, it involves for-\nmulating a plan to break down the overall task into smaller, manageable subtasks.\nSubsequently, these subtasks are executed according to the devised plan. More pre-\ncisely, PS prompting replaces the original “Let’s think step by step” from Zero-shot\nCoT with a new prompt that encourages a more detailed approach: “Let’s first under-\nstand the problem and devise a plan to solve it. Then, let’s proceed to execute the\nplan and solve the problem step by step.”\nFew-Shot CoT\nChain-of-Thought (CoT) (Wei et al., 2022b) has charted a significant course for\nenhancing the reasoning capabilities of Language Models (LLMs) by employing\ndetailed reasoning paths as prompts. This directional trend has given rise to various\nCoT variants, such as least-to-most (Zhou et al., 2022a), complex CoT (Fu et al., 2022),\nprogram-of-thought (Chen et al., 2022d), equation-of-thought (Liu et al., 2023k),\nprogram-aid-language (Gao et al., 2023b), mathprompter (Imani et al., 2023), and\ncode prompting (Hu et al., 2023b). However, it is worth noting that all these meth-\nods require annotations, which impose practical limitations on their application. To\naddress this constraint, Auto-CoT (Zhang et al., 2022c) proposes a novel approach that\nutilizes Zero-Shot-CoT (Kojima et al., 2022) to generate CoT reasoning paths. Fur-\nthermore, Auto-CoT divides these reasoning paths into different clusters and selects\nquestions that are most closely aligned with the centroid of each cluster. Memory-\nof-Thought (Li and Qiu, 2023b) selects relevant, high-quality thoughts from external\nmemory during the reasoning process. Taking a step further, Tree-of-Thought (Yao\net al., 2023b) models the human thought process not only as a chain but also as a tree,\nwhereas Graph-of-Thought (Yao et al., 2023d) extends this concept to represent human\nthought processes as both chains and graphs. Additionally, Skeleton-of-Thought (Ning\net al., 2023) guides LLMs to first create the basic structure of the answer and then\nuses batched decoding to simultaneously fill in the details of each skeleton.\nMultiple Paths Aggregation\nThe DIVERSE approach (Li et al., 2022h) employs a voting verifier to consolidate final\nanswers derived from multiple reasoning paths. In a similar vein, the Self-Consistency\nmethod (Wang et al., 2023t) suggests sampling multiple reasoning paths and mak-\ning a majority vote to determine the ultimate results. Building on this direction, the\nconcept of complexity-based voting has been introduced, retaining reasoning paths\n86\nwith high complexity for majority voting (Fu et al., 2022). Furthermore, Model Selec-\ntion (Zhao et al., 2023d) takes a different approach by sampling two answers via\nChain-of-Thought (CoT) and Plan-of-Thought (PoT) and then employing a Language\nModel (LLM) to select the correct one. Instead of generating complete reasoning\npaths, Self-Evaluation Guided Decoding (Xie et al., 2023c) samples various reason-\ning steps at the step level and utilizes beam search to complete the search tree. One\nnotable limitation of Self-Consistency is its relatively high cost. To mitigate this draw-\nback, Adaptive-Consistency (Aggarwal et al., 2023) progressively samples reasoning\npaths until predefined criteria are met. Two concurrent approaches related to Tree-of-\nThought (Yao et al., 2023b; Long, 2023) gradually sample reasoning steps rather than\ncomplete reasoning paths. Additionally, Reasoning via Planning (RAP) (Hao et al.,\n2023a) repurposes the LLM as both a world model and a reasoning agent. It incorpo-\nrates a principled planning algorithm, based on Monte Carlo Tree Search, to facilitate\nstrategic exploration within the extensive reasoning space. Exchange-of-Thought (Yin\net al., 2023c) and X-of-Thoughts (Liu et al., 2023k) introduce a variety of external\nreasoning insights and reasoning methods to enhance reasoning performance.\n4.5.3 Multi-Round Prompting\nMulti-round prompting enhances the response through iterative refinement, unlike\nsingle-round prompting methods such as Chain of Thought or self-consistency, which\ndo not employ this process of progressive improvement.\nLearned Refiners\nThe Learned Refiner necessitates a training process, and the acquisition of supervised\nrefinement typically involves pairs of feedback and refinement (Schick et al., 2022; Du\net al., 2022b; Yasunaga and Liang, 2020; Madaan et al., 2021). CURIOUS (Madaan\net al., 2021) initially constructs a graph that represents relevant influences. This graph\nis then integrated as an additional input for responding to the question. PEER (Schick\net al., 2022) is an advanced collaborative language model that replicates the entire\nwriting process, encompassing drafting, suggesting modifications, proposing edits, and\nproviding explanations for its actions. In contrast, Read, Revise, Repeat (R3) (Du\net al., 2022b) aims to achieve superior text revisions with minimal human intervention.\nIt achieves this by analyzing model-generated revisions and user feedback, making\ndocument revisions, and engaging in repeated human-machine interactions. DrRe-\npair (Yasunaga and Liang, 2020) introduces a program feedback graph that connects\nsymbols relevant to repairing source code with diagnostic feedback. It then employs a\ngraph neural network to model the reasoning process. Self-Correction (Welleck et al.,\n2022) takes an innovative approach by decoupling an imperfect base generator, such\nas a standard language model or supervised sequence-to-sequence model, from a sep-\narate corrector. This corrector learns to refine outputs progressively. Furthermore,\nLLM-Augmenter (Peng et al., 2023b) continuously enhances LLM prompts to improve\nmodel responses by incorporating feedback generated by utility functions, such as the\nfactuality score of an LLM-generated response.\n87\nPrompted Refiners\nThe REFINER framework (Paul et al., 2023) is a comprehensive system designed to\nfine-tune Language Models (LMs) with the specific goal of generating intermediate\nreasoning steps, aided by a critic model that automates feedback on the reasoning\nprocess. The Self-Refine framework (Madaan et al., 2023) comprises two vital com-\nponents: it first generates an output using an LLM and then employs the same LLM\nto provide feedback on its output through an iterative self-refinement process. Self-\nDebugging (Chen et al., 2023j) integrates both LLM and tool feedback to enhance\nperformance. Similarly, Progressive-Hint Prompting (PHP) (Zheng et al., 2023a) uti-\nlizes previous answers as references for generating subsequent responses. Furthermore,\nemploying distinct prompts for LLMs allows for the assignment of different roles in\nhandling various aspects (Dong et al., 2023b; Fu et al., 2023a; Du et al., 2023). Du\net al. (2023) introduce a complementary approach to enhance language responses,\ninvolving multiple instances of language models engaging in discussions about their\nindividual responses and reasoning processes over multiple rounds to arrive at a shared\nfinal answer. Self-collaboration (Dong et al., 2023b) utilizes multiple LLMs as distinct\n“experts”, each responsible for specific subtasks within complex assignments, defin-\ning strategies for collaboration and interaction. Fu et al. (2023a) observe that only a\nsubset of the considered language models demonstrate proficiency in self-improvement\nthrough AI feedback, as weaker models may struggle with understanding game rules\nor incorporating AI feedback for further enhancements. In conclusion, models exhibit\nvarying abilities to learn from feedback based on their roles, and interactions between\nLLMs and tools can further enhance reasoning performance (Chen et al., 2023j; Gou\net al., 2023; Zhang et al., 2023e; Yang et al., 2023b; Olausson et al., 2023).\n4.6 Autonomous Agent\nAgents that operate autonomously have often been considered a key route to achiev-\ning Artificial General Intelligence (AGI). These agents are adept at performing tasks\nby independently formulating plans and following instructions. At present, these\nautonomous entities primarily rely on Large Language Models (LLM) to control and\norchestrate various tools (Xi et al., 2023; Wang et al., 2023y), including web browsers\nand code interpreters, to complete their designated tasks, as shown in Figure 20.\nVISPROG (Gupta and Kembhavi, 2022) is a neuro-symbolic approach for complex\nvisual tasks, using large language models to generate Python-like modular programs\nwithout task-specific training. It provides a comprehensive and interpretable rationale.\nToolFormer (Schick et al., 2023) is a self-supervised model that decides which APIs\nto call, when, and with what arguments to incorporate the results into token predic-\ntion based on demonstrations. ART (Paranjape et al., 2023) introduces a framework\nfor automatic reasoning and tool use, utilizing frozen LLMs to generate interme-\ndiate reasoning steps and seamlessly integrating external tools. CAMEL (Li et al.,\n2023c) presents a pioneering communicative agent framework known as “role-playing”,\nwhich employs inception prompting to direct chat agents towards achieving tasks\nwhile upholding alignment with human intentions. GPT4Tools (Yang et al., 2023d)\nempowers LLMs with multimodal tool capabilities to solve multiple vision tasks. Hug-\ngingGPT (Shen et al., 2023) connects AI models for solving tasks, using ChatGPT for\n88\nPerception\nAction\nLanguage model\nKnowledge / Skill\nEnvironmentAgent\n…\nEncoder\nMemoryLearnUpdate\nRetrieve\nFig. 20: The general pipeline for LLM with autonomous agent. An LLM agent lever-\nages an LLM as its digital brain mastering multiple abilities and possessing a high-level\nof intelligence. The agent can receive a diverse set of encoded data as input and cor-\nrespondingly construct or have access to knowledge bases and skill libraries. With\nsufficient knowledge and prompts, the agent can work semi-autonomously to operate\na spectrum of tasks.\ntask planning and selecting models based on function descriptions in Hugging Face.\nChameleon (Lu et al., 2023) augments LLMs with plug-and-play modules for com-\nplex reasoning, synthesizing programs by composing various tools for tasks. Wang\net al. (2023q) propose to learn “planning tokens”, a soft prompt. TRICE (Qiao et al.,\n2023) addresses the challenge of teaching language models when and how to use\ntools, proposing a two-stage framework for learning through feedback from tool exe-\ncution. ChatCoT (Chen et al., 2023o) presents a tool-augmented chain-of-thought\nreasoning framework for chat-based LLMs, using multi-turn conversations to integrate\nthought chains and tool usage naturally. MultiTool-CoT (Inaba et al., 2023) lever-\nages chain-of-thought prompting to incorporate multiple external tools during the\nreasoning process. AssistGPT (Gao et al., 2023a) introduces a multimodal AI assis-\ntant with an interleaved code and language reasoning approach, including planning,\nexecution, inspection, and autonomous learning. OpenAGI (Ge et al., 2023) is an open-\nsource AGI research platform for real-world tasks, using natural language queries to\nselect and execute appropriate models and proposing a Reinforcement Learning from\nTask Feedback mechanism. ToolkenGPT (Hao et al., 2023b) combines the benefits\nof finetuning LLMs with tool demonstration data and in-context learning, repre-\nsenting tools as tokens (“toolkens”) for flexible tool calls. AutoGPT (gravitas/auto\ngpt, 2023) decomposes problems into subproblems and employs tools to solve them.\nReAct (Yao et al., 2023c) explores the interleaved generation of reasoning traces and\ntask-specific actions for enhanced synergy in language tasks, improving interpretability\nand trustworthiness. Reflexion (Shinn et al., 2023) reinforces language agents through\nlinguistic feedback and episodic memory, improving decision-making in subsequent tri-\nals. CREATOR (Qian et al., 2023a) endows LLMs to create their own tools through\ndocumentation and code realization, addressing limitations in tool-using ability. Voy-\nager (Wang et al., 2023d) is an LLM-powered agent in Minecraft for lifelong learning,\n89\nincorporating automatic curriculum, skill library, and iterative prompting mechanisms.\nAutoAgents (Chen et al., 2023b) can adaptively generate specialized agents to build\na team of agents based on task definitions. SwiftSage (Lin et al., 2023b) is an agent\nframework inspired by human cognition’s dual-process theory, integrating behavior\ncloning and LLMs for complex reasoning tasks, enhancing problem-solving efficiency.\nThese references cover a wide range of approaches and frameworks for enhancing the\ncapabilities of large language models across various domains.\n5 Discussion: Challenges, Limitations, and Risks\nFoundation models have shown promising capabilities in reasoning tasks, opening\nup new possibilities for the field. It is also essential to acknowledge the challenges,\nlimitations, and risks associated with their use.\nHallucinations\nDespite the promising progress made in foundation models, it is important to acknowl-\nedge that these models still face challenges, specifically in relation to the issue of\nhallucinations (Li et al., 2023p; M¨ undler et al., 2023). Hallucination refers to the\ngeneration of outputs by foundation models that contain fabricated or incorrect infor-\nmation, deviating from the intended or expected outputs. These hallucinations can be\nproblematic, as they undermine the reliability and accuracy of the model’s generated\ncontent.\nThe hallucination problem in foundation models arises due to various factors. One\nkey factor is the reliance on large-scale pre-training data, which can contain biased\nor erroneous information. This can lead to the model learning and propagating false\npatterns or generating unrealistic outputs. Another significant factor contributing to\nthe hallucination issue in foundation models is the models’ lack of ability to acknowl-\nedge their own knowledge limitations. When confronted with questions beyond their\nunderstanding, these models tend to fabricate seemingly plausible answers instead of\nadmitting their lack of knowledge (Yin et al., 2023d).\nAddressing the hallucination problem in foundation models is an ongoing area\nof research. Techniques such as fine-tuning task-specific data, incorporating external\nknowledge sources, and developing advanced evaluation metrics have been explored to\nmitigate hallucinations. Researchers are also exploring methods to enhance reasoning\ncapabilities in foundation models, enabling them to make more informed and accurate\npredictions.\nIt is worth noting that while progress has been made in reducing hallucinations,\ncompletely eliminating them remains a challenge due to the inherent complexity of\nlanguage understanding and generation.\nContext Length\nAnother limitation is to optimize context length and context construction. For exam-\nple, GPT models start with 2K window size (GPT-3 (Brown et al., 2020)) and go\nall the way to 32K (GPT-4 (OpenAI, 2023a)). A longer context window is useful for\n90\nworking with long sequence data, such as gene sequences. By having a larger context\nwindow, LLM is capable of handling more lengthy inputs such as entire documents, or\ncomprehending the full scope of an article. This ability enables LLM to produce more\ncontextually relevant responses by leveraging a more comprehensive understanding of\nthe input.\nIncreasing the context window size in foundation models can bring several benefits,\nsuch as capturing longer-range dependencies and improving the model’s understand-\ning of context. However, it also comes with certain challenges and costs. In earlier\nstudies, it was observed that the costs associated with larger context window sizes\nexhibited a quadratic increase as the number of tokens grew (Aryan et al., 2023).\nThis means that the computational resources required to process and train the models\nbecome significantly higher as the window size grows. LongNet (Ding et al., 2023a)\nrepresents a modified version of the Transformer model, capable of handling sequences\nexceeding 1 billion tokens in length, while still maintaining its effectiveness on shorter\nsequences. LongNet also has a linear computation complexity. Position Interpola-\ntion (Chen et al., 2023g) implements a linear downscaling of input position indices\nto align with the initial context window size during inference. This approach pre-\nvents extending beyond the context length trained for, which might otherwise result\nin abnormally high attention scores and interfere with the self-attention mechanism.\nIndeed, while increasing the context window size in language models offers bene-\nfits, it is important to consider the tradeoff between window size and generalization\nability. Researchers have highlighted that there can be a tradeoff between them (Liu\net al., 2023h). One challenge worth exploring is how to increase the context window\nlength without sacrificing the model’s performance and generalization capabilities. It\nis crucial to find strategies that allow models to capture longer-range dependencies\nand context while maintaining their ability to generalize well to new or unseen inputs.\nMultimodal Learning\nMultimodal learning is an incredibly powerful but often underappreciated aspect of\nreasoning. It finds applications in numerous fields where multimodal data is essential,\nincluding healthcare (such as CT, X-ray, MRI scans, and gene sequences), robotics, e-\ncommerce, retail, gaming, and entertainment. The integration of different modalities in\nthese domains enables a more comprehensive understanding of the data and facilitates\nmore sophisticated reasoning processes.\nOne of the key advantages of multimodal reasoning is its potential to significantly\nimprove model performance. While some prior works have delved into multimodal\nreasoning, such as the multimodal language Model for embodied reasoning proposed\nby PaLM-E (Driess et al., 2023) and the visual language model for fear-shot learning\nknown as Flamingo (Alayrac et al., 2022), there is still ample room for exploring\nadditional data modalities. Incorporating modalities like video, audio, 3D data, and\nmultiple images not only enriches the information available to the models but also\nopens up exciting possibilities for a more nuanced and comprehensive understanding\nof the world. Other potential applications of foundation model reasoning lie in the\ndomain of Electronic Design Automation (EDA) for program design (Huang et al.,\n2021a) and Formal Methods (Woodcock et al., 2009).\n91\nFormal methods, intrinsically linked to logical reasoning, are mathematical strate-\ngies employed in the realm of computer science for the design, specification, verifi-\ncation, and analysis of both software and hardware. These techniques are anchored\nin structured logic, the theory of automata, and other comprehensive mathemati-\ncal frameworks. They are used to meticulously examine the behavior, accuracy, and\ndependability of systems. The utilization of formal methods empowers researchers and\nprofessionals to guarantee the integrity and precision of intricate systems, establish-\ning them as indispensable in the creation and assessment of software and hardware.\nThe amalgamation of formal methods with foundational models opens doors to aug-\nmenting reasoning abilities in the design of software and hardware systems. Formal\nmethods bring to the table precise mathematical methods for defining and confirming\nsystem characteristics, whereas foundational models contribute potent language com-\nprehension and reasoning skills. The synthesis of these methodologies can foster the\ndevelopment of more dependable and resilient software and hardware systems.\nBy leveraging multimodality reasoning and further expanding the exploration of\nvarious data modalities, we can unlock new insights and capabilities in reasoning\nsystems. It is crucial to recognize and harness the power of multimodal reasoning to\nfully exploit the potential of reasoning in diverse domains.\nEfficiency and Cost\nEfficiency and cost are significant challenges for foundation models for reasoning. Foun-\ndation models, especially those with large architectures and extensive training data,\ncan be computationally expensive to train and deploy. The large number of param-\neters requires more memory and computational resources for processing. This poses\nchallenges in terms of scalability, accessibility, and cost-effectiveness. Efficient reason-\ning models should be capable of performing fast and real-time inference to meet the\ndemands of interactive applications. However, the complex computations involved in\nreasoning tasks can lead to slower inference times, hindering real-time performance\nand user experience. Therefore, it is crucial to enhance the speed and cost-effectiveness\nof foundation models, making them cheaper and faster.\nThere are several techniques that can be employed to improve the efficiency of\nfoundation models, including:\n• Model Pruning (Sun et al., 2023d; Wang et al., 2020): removing unnecessary\nconnections, parameters, or layers from the model. This results in a more compact\narchitecture, reducing computational requirements.\n• Compression (Zhu et al., 2023c) and Quantization (Tao et al., 2022): reduce the\nsize of the model or reduce the precision of model parameters, using fewer bits\nto represent them. This reduces memory usage and computational complexity.\n• Knowledge Distillation (Gu et al., 2023b): training a smaller model (student)\nto mimic the behavior and predictions of a larger model or ensemble of models\n(teacher). This transfer of knowledge allows for efficient inference with reduced\ncomputational resources.\n• Low-Rank Factorization (Ren and Zhu, 2023; Hsu et al., 2022): replace high-\ndimensional tensors with lower-dimensional tensors. By reducing the number\n92\nof parameters, these methods improve efficiency without significant loss in\nperformance.\nBy employing these techniques, it is possible to enhance the efficiency of foundation\nmodels, making them faster and more cost-effective for various reasoning tasks and\napplications.\nHuman Preference\nAddressing the risks and potential harms associated with foundation models, such\nas bias, unfairness, manipulation, and misinformation, requires careful consideration\nand proactive measures. One approach is to focus on improving learning from human\npreference and feedback to ensure more responsible and accurate model behavior.\nTo mitigate these risks, we can explore several strategies. First, we need mecha-\nnisms to incorporate diverse perspectives and mitigate bias during the training and\nfine-tuning phases of foundation models. This can involve diverse data collection, rep-\nresentative sampling, and careful annotation processes that involve input from a wide\nrange of human perspectives. Continual learning and adaptation, informed by human\nfeedback, can also play a crucial role. By enabling ongoing interactions between mod-\nels and human annotators or users, we can gather feedback and iteratively refine the\nmodels’ behavior. This iterative process helps identify and rectify potential biases,\nunfairness, or misinformation, allowing the models to improve over time. Furthermore,\nensuring that the outputs of foundation models align with real-world evidence, experi-\nmental findings, and explicit knowledge is essential. This requires incorporating robust\nfact-checking mechanisms and validation processes into the model training pipeline.\nAdditionally, leveraging external sources of information, such as trusted databases or\nexpert knowledge, can help verify and validate the outputs generated by the models.\nConstitutional AI, as proposed by Bai et al. (2022), offers an approach that involves\nboth supervised learning and reinforcement learning phases like “RL from AI Feed-\nback” (RLAIF). Similarly, Bakker et al. (2022) explore the use of fine-tuning a large\nlanguage model (LLM) with 70 billion parameters to generate statements that max-\nimize the expected approval for people with different and diverse perspectives. This\napproach emphasizes the importance of incorporating human preferences and diverse\nviewpoints during the model training process.\nBy integrating these techniques and approaches, we can work towards mitigating\nthe risks and potential harms associated with foundation models. Improving learn-\ning from human preference, continual learning informed by feedback, and ensuring\nfidelity to real-world evidence are challenging steps in building more responsible and\ntrustworthy reasoning systems.\nMultilingual Support\nWhile reasoning itself is a language-agnostic process, the availability of comprehensive\nknowledge sources is often limited to a few languages, primarily English. Historically,\nlanguage foundation models have demonstrated exceptional reasoning performance,\nprimarily in English, with relatively limited support for other languages such as Chi-\nnese and Japanese. Currently, there is a lack of robust multilingual reasoning language\nfoundation models that excel across various languages.\n93\nFang et al. (2022) propose utilizing English as a pivot language in their common-\nsense reasoning framework. They employ a translate-retrieve-translate (TRT) strategy,\nleveraging English knowledge sources to enhance their reasoning capabilities. Fur-\nthermore, Huang et al. (2023a) introduce cross-lingual thought prompting (XLT) as\na systematic approach to improving the multilingual capabilities of Language and\nReasoning Models (LLMs).\nGiven these advancements, there is a growing interest in developing foundation\nmodels dedicated to multilingual reasoning. Building robust models that excel in\nmultilingualism presents an intriguing avenue for future research and development.\nIn summary, to address these challenges, ongoing research and development efforts\nare necessary. This includes advancing the deployment of reasoning models.\n6 Future Directions\nFurther research and development in this area have the potential to unlock even\nmore advanced reasoning abilities in foundation models.\n6.1 Safety and Privacy\nThe rise of foundation models and their application to reasoning tasks has highlighted\nthe critical need to ensure their safety and trustworthiness (Huang et al., 2023e).\nVarious intended attacks have been identified, including the robustness gap (Shreya\nand Khapra, 2022), backdoor attacks (Shen et al., 2021b; Kurita et al., 2020), poi-\nsoning (Carlini et al., 2023), disinformation (Nelson et al., 2008), privacy leakage (Li\net al., 2023d), and unauthorized disclosure of information (Perez and Ribeiro, 2022).\nSpecifically, backdoor attacks involve the injection of malicious knowledge into foun-\ndation models through techniques such as poisoning the training data (Shen et al.,\n2021b) or modifying model parameters (Kurita et al., 2020).\nAs one of the most principled techniques for training machine learning models\nwith privacy, differential privacy allows for training on datasets without revealing any\ndetails of individual training examples, providing enhanced privacy protection (Shi\net al., 2022; Behnia et al., 2022). Another effective way of defensing adversarial attacks\nis by adversarial training, which can provide another layer of security when facing\nmalicious yet human invisible perturbations added in model inputs (Li et al., 2023i;\nLi and Spratling, 2023).\nIn response to some copyright concerns, Kirchenbauer et al. (2023) introduce a\nwatermarking framework specifically designed for proprietary language models. This\nframework enables the embedding of watermarks with minimal impact on text quality\nand facilitates their detection using an efficient open-source algorithm, eliminating the\nneed for accessing the language model API or parameters.\n6.2 Interpretability and Transparency\nAdditionally, there is a need for increased transparency and interpretability of foun-\ndation models (Liao and Vaughan, 2023). As these models become more complex and\n94\nsophisticated, understanding their reasoning processes and the factors influencing their\noutputs becomes increasingly important.\nSometimes, foundation models generate toxic content, which may incite violence\nand cause infordemic (Bender et al., 2021; Weidinger et al., 2021). They can inad-\nvertently disclose sensitive information, thereby jeopardizing privacy and security.\nAdditionally, LLMs can contribute to the dissemination of misinformation, both inten-\ntionally and unintentionally (Pan et al., 2023b; Buchanan et al., 2021; Kreps et al.,\n2022; Zhou et al., 2023b). The complex and uncertain nature of foundation models\nfurther compounds these challenges. These models exhibit a remarkable capacity to\nperform a wide range of tasks across diverse contexts (Bommasani et al., 2021). How-\never, their massive and opaque architectures hinder a comprehensive understanding of\ntheir capabilities and behaviors, making it difficult to ascertain their decision-making\nprocesses and potential biases. This lack of transparency raises concerns regarding\nmodel interpretability, control, and accountability.\nDeveloping techniques and frameworks for model interpretability can help address\nconcerns regarding transparency and accountability.\n6.3 Autonomous Language Agents\nThe capacity for logical reasoning is crucial in achieving complex tasks in embod-\nied environments, and it plays a significant role in embodied intelligence (Dasgupta\net al., 2022). Foundation Models have exhibited powerful capabilities for reasoning\nand flexibility through the process of in-context learning (Yang et al., 2023e). Recent\nstudies, such as Voyager and DEPS, have explored the use of LLMs for planning in\nMinecraft (Wang et al., 2023d,x). DEPS specifically proposes an interactive planning\napproach based on LLMs (Wang et al., 2023x). LLMs have also shown promise in\ngenerating action sequences directly based on natural language instructions without\nrequiring extra domain knowledge (Li et al., 2022e). Equipping embodied agents with\ncommonsense knowledge is crucial for their successful completion of complex human\ninstructions in diverse environments (Wu et al., 2023g).\nIn the context of reasoning for autonomous agents, there are key characteristics:\n• Infinite Task Capability: Foundation models empower agents with the capacity\nto handle an extensive range of tasks, even those that are not pre-defined or\nanticipated in advance. This flexibility allows agents to dynamically generate\ntasks based on their understanding of the context and the specific needs of the\nusers.\n• Autonomous Task Generation: Foundation model reasoning enables agents to\nautonomously generate new tasks within a given context. This capability empow-\ners agents to take initiative, identify opportunities, and propose relevant tasks\nto users. They can adapt and respond to changing circumstances, making them\nmore versatile, proactive, and efficient in fulfilling user requirements.\n• Value System: Autonomous agents are driven by a value system empowered by a\npre-trained foundation model, which serves as the foundation for task generation.\nThis value system guides the agent’s decision-making process, taking into account\nfactors such as priorities, preferences, and ethical considerations. By leveraging\n95\nthe capabilities of the foundation model, agents can make informed decisions\naligned with human values, ensuring responsible and ethical behavior.\n• World Model: The foundation model can also be utilized as a world model that\nrepresents the real world and serves as the basis for agents’ interactions and\nreasoning. This comprehensive model enables agents to understand the context,\ninterpret natural language inputs, and generate appropriate responses or actions.\nWith the foundation model as their world model, agents can effectively navi-\ngate and operate within their environment, enhancing their ability to interact\nintelligently and respond to user needs.\nBy leveraging foundation models, autonomous agents can facilitate more meaning-\nful and effective interactions with users, better understand their intents and needs,\nand generate relevant tasks accordingly. This approach opens up promising avenues\nfor research in areas such as contextual understanding, human-like reasoning, and per-\nsonalized assistance. Ultimately, it enhances the overall user experience and enables\nthe development of more sophisticated and intelligent AI systems.\nGiven their reasoning capabilities, Foundation Models hold significant potential\nfor applications in human-computer interaction and embodied intelligence. can be\nleveraged to create interactive and adaptive systems that can dynamically respond\nto user input and adapt their behavior accordingly. This involves developing models\nthat can learn from user interactions and update their knowledge and behavior over\ntime. By enabling Foundation Models to actively engage with users and adapt to\ntheir preferences and needs, we can create more personalized and user-centric human-\ncomputer interaction experiences.\n6.4 Reasoning for Science\nFuture work can also build upon the research on temporal reasoning in multimodal\nquestion-answering tasks or sound reasoning (Brandt and McClure, 2011), as demon-\nstrated by Audio Question Answering (AQA) (Fayek and Johnson, 2019). Researchers\ncan delve deeper into understanding and developing foundation models that can rea-\nson and make inferences based on auditory information. This can have implications\nin areas such as audio-based decision-making systems, environmental monitoring, and\naudio scene understanding.\nFurthermore, the application of multimodal reasoning can be extended to domains\nlike medical reasoning and diagnosis, particularly in the context of gene sequence\nanalysis. This can aid in the identification of genetic disorders, personalized medicine,\nand the exploration of potential treatments.\nOverall, future work can focus on advancing multimodal reasoning abilities in foun-\ndation models. These endeavors can contribute to the development of more intelligent\nand context-aware systems in various fields.\n6.5 Super Alignment\nSuperintelligence alignment, according to OpenAI§, is the next machine-learning ques-\ntion of utmost importance. However, ensuring the control and alignment of potentially\n§https://openai.com/blog/introducing-superalignment\n96\nsuperintelligent AI systems poses significant challenges. Current techniques, such as\nReinforcement Learning from Human Feedback (RLHF), heavily rely on human super-\nvision and reasoning. As AI systems surpass human intelligence, human supervision\nbecomes inadequate, necessitating new scientific and technical breakthroughs in align-\nment research. Existing alignment techniques will not scale to superintelligence due\nto the limitations of human reasoning and supervision. The prospect of controlling\nand steering highly intelligent AI systems to prevent them from going rogue remains\nan unsolved challenge. Without reliable means of supervising these reasoning systems\nsurpassing human capabilities, ensuring their alignment with human intent becomes\nincreasingly difficult.\nOne approach to address the challenge of ensuring that reasoning systems surpass-\ning human intelligence adhere to human intent is to develop a roughly human-level\nautomated alignment researcher. By creating such a system, it becomes possible to\nleverage extensive computational resources to scale alignment efforts and iteratively\nalign superintelligence.\n7 Conclusion\nThis survey illuminates the evolutionary path of foundation models in the field of\nreasoning, showcasing a discernible progression in complexity and efficacy from their\ninitial stages to current advancements. While we acknowledge the remarkable strides\nmade in data-driven thinking, it is crucial for us to objectively recognize both the\nstrengths and limitations of large models. Emphasizing the importance of enhancing\ntheir interpretability and security becomes imperative in this context. We also note\nthat with all the papers surveyed in this work, a consensus is yet to be reached on\nhow to push forward the reasoning ability of foundation models to a consistently\nsuperhuman level (which can for instance win an IMO medal or even solve open\nmathematical problems).\nIn conclusion, while foundation models offer exciting possibilities in reasoning\ntasks, it is essential to approach their development and application with a critical per-\nspective. It is crucial to acknowledge the challenges, limitations, and risks associated\nwith LLM-based reasoning. By doing so, we can foster responsible and thoughtful\nadvancements in this field, ensuring the development of robust and reliable reasoning\nsystems.\n97\nReferences\n(2022) Thor: Wielding hammers to integrate language models and automated theorem\nprovers. Advances in Neural Information Processing Systems 35:8360–8373\n(2023) O (2023) Gpt-4 technical report. arXiv:230308774\nAbdine H, Chatzianastasis M, Bouyioukos C, et al. (2023) Prot2text: Multi-\nmodal protein’s function generation with gnns and transformers. arXiv preprint\narXiv:230714367\nAcay DL, Pasquier P, Sonenberg L (2007) Extrospection: Agents reasoning about the\nenvironment. 3rd IET International Conference on Intelligent Environments\nAcquaviva S, Pu Y, Nye M, et al. (2021) Larc: Language annotated abstraction and\nreasoning corpus. In: Proceedings of the Annual Meeting of the Cognitive Science\nSociety\nAggarwal P, Madaan A, Yang Y, et al. (2023) Let’s sample step by step: Adaptive-\nconsistency for efficient reasoning with llms. 2305.11860\nAhmad WU, Chakraborty S, Ray B, et al. (2021) Unified pre-training for program\nunderstanding and generation. arXiv preprint arXiv:210306333\nAhn M, Brohan A, Brown N, et al. (2022) Do as i can, not as i say: Grounding language\nin robotic affordances. In: Conference on Robot Learning\nAi Q, Bai T, Cao Z, et al. (2023) Information retrieval meets large language models:\nA strategic report from chinese ir community. 2307.09751\nAl-Ajlan A (2015) The comparison between forward and backward chaining. Interna-\ntional Journal of Machine Learning and Computing 5(2):106\nAlayrac JB, Donahue J, Luc P, et al. (2022) Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems 35:23716–\n23736\nAlibali MW, Boncoddo R, Hostetter AB (2014) Gesture in reasoning: An embodied\nperspective. In: The Routledge handbook of embodied cognition. Routledge, p 150–\n159\nAllal LB, Li R, Kocetkov D, et al. (2023) Santacoder: don’t reach for the stars! arXiv\npreprint arXiv:230103988\nAlvin C, Gulwani S, Majumdar R, et al. (2017) Synthesis of solutions for shaded area\ngeometry problems. In: The Thirtieth International Flairs Conference\n98\nAmini A, Gabriel S, Lin P, et al. (2019) Mathqa: Towards interpretable math word\nproblem solving with operation-based formalisms. arXiv preprint arXiv:190513319\nAnand Y, Nussbaum Z, Duderstadt B, et al. (2023) Gpt4all: Training an assistant-\nstyle chatbot with large scale data distillation from gpt-3.5-turbo. https://github.\ncom/nomic-ai/gpt4all\nAnil R, Dai AM, Firat O, et al. (2023) Palm 2 technical report. 2305.10403\nAnthropic (2023) Introducing claude\nAraci D (2019) Finbert: Financial sentiment analysis with pre-trained language\nmodels. arXiv preprint arXiv:190810063\nArandjelovic R, Zisserman A (2017) Look, listen and learn. In: Proceedings of the\nIEEE international conference on computer vision, pp 609–617\nArdila R, Branson M, Davis K, et al. (2020) Common Voice: A Massively-Multilingual\nSpeech Corpus. In: Proceedings of the Twelfth Language Resources and Evaluation\nConference, pp 4218–4222\nAribandi V, Tay Y, Schuster T, et al. (2022) Ext5: Towards extreme multi-task scal-\ning for transfer learning. In: International Conference on Learning Representations,\nURL https://openreview.net/forum?id=Vzh1BFUCiIX\nAroca-Ouellette S, Paik C, Roncone A, et al. (2021) Prost: Physical reasoning of\nobjects through space and time. 2106.03634\nAryan A, Nain AK, McMahon A, et al. (2023) The costly dilemma: Generalization,\nevaluation and cost-optimal deployment of large language models. 2308.08061\nAsai A, Hashimoto K, Hajishirzi H, et al. (2020) Learning to retrieve reasoning\npaths over wikipedia graph for question answering. In: International Conference on\nLearning Representations, URL https://openreview.net/forum?id=SJgVHkrYDH\nAustin J, Odena A, Nye M, et al. (2021) Program synthesis with large language models.\narXiv preprint arXiv:210807732\nAzerbayev Z, Schoelkopf H, Paster K, et al. (2023) Llemma: An open language model\nfor mathematics. 2310.10631\nBabu A, Wang C, Tjandra A, et al. (2022) XLS-R: Self-supervised cross-lingual speech\nrepresentation learning at scale. In: INTERSPEECH, pp 2278–2282\nBach S, Sanh V, Yong ZX, et al. (2022) PromptSource: An integrated development\nenvironment and repository for natural language prompts. In: Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics: System\n99\nDemonstrations. Association for Computational Linguistics, Dublin, Ireland, pp 93–\n104, https://doi.org/10.18653/v1/2022.acl-demo.9, URL https://aclanthology.org/\n2022.acl-demo.9\nBaevski A, Zhou Y, Mohamed A, et al. (2020) wav2vec 2.0: A framework for\nself-supervised learning of speech representations. Advances in neural information\nprocessing systems 33:12449–12460\nBaevski A, Hsu WN, Xu Q, et al. (2022) Data2vec: A general framework for self-\nsupervised learning in speech, vision and language. In: International Conference on\nMachine Learning, PMLR, pp 1298–1312\nBai Y, Kadavath S, Kundu S, et al. (2022) Constitutional ai: Harmlessness from ai\nfeedback. 2212.08073\nBakhtin A, van der Maaten L, Johnson J, et al. (2019) Phyre: A new benchmark for\nphysical reasoning. Advances in Neural Information Processing Systems 32\nBakker MA, Chadwick MJ, Sheahan HR, et al. (2022) Fine-tuning language models\nto find agreement among humans with diverse preferences. 2211.15006\nBalashankar A, Subramanian L (2021) Learning faithful representations of causal\ngraphs. In: Proceedings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers). Association for Computational Lin-\nguistics, Online, pp 839–850, https://doi.org/10.18653/v1/2021.acl-long.69, URL\nhttps://aclanthology.org/2021.acl-long.69\nBan T, Chen L, Wang X, et al. (2023) From query tools to causal architects: Harness-\ning large language models for advanced causal discovery from data. arXiv preprint\narXiv:230616902\nBansal K, Loos S, Rabe M, et al. (2019) HOList: An environment for machine\nlearning of higher order logic theorem proving. In: Chaudhuri K, Salakhutdinov\nR (eds) Proceedings of the 36th International Conference on Machine Learning,\nProceedings of Machine Learning Research, vol 97. PMLR, pp 454–463, URL\nhttps://proceedings.mlr.press/v97/bansal19a.html\nBao F, Nie S, Xue K, et al. (2023) One transformer fits all distributions in multi-modal\ndiffusion at scale. 2303.06555\nBao H, Dong L, Piao S, et al. (2021) Beit: Bert pre-training of image transformers.\narXiv preprint arXiv:210608254\nBarbieri F, Anke LE, Camacho-Collados J (2021) Xlm-t: Multilingual language models\nin twitter for sentiment analysis and beyond. arXiv preprint arXiv:210412250\n100\nBarras B, Boutin S, Cornes C, et al. (1997) The coq proof assistant reference manual:\nVersion 6.1. PhD thesis, Inria\nBavarian M, Jun H, Tezak N, et al. (2022) Efficient training of language models to fill\nin the middle. arXiv preprint arXiv:220714255\nBear DM, Wang E, Mrowca D, et al. (2021) Physion: Evaluating physical prediction\nfrom vision in humans and machines. arXiv preprint arXiv:210608261\nBehnia R, Ebrahimi MR, Pacheco J, et al. (2022) Ew-tune: A framework for pri-\nvately fine-tuning large language models with differential privacy. In: 2022 IEEE\nInternational Conference on Data Mining Workshops (ICDMW), IEEE, pp 560–566\nBender EM, Gebru T, McMillan-Major A, et al. (2021) On the dangers of stochastic\nparrots: Can language models be too big? In: Proceedings of the 2021 ACM Con-\nference on Fairness, Accountability, and Transparency. Association for Computing\nMachinery, New York, NY, USA, FAccT ’21, p 610–623, https://doi.org/10.1145/\n3442188.3445922, URL https://doi.org/10.1145/3442188.3445922\nBengio Y (2017) The consciousness prior. arXiv preprint arXiv:170908568\nBerant J, Chou A, Frostig R, et al. (2013) Semantic parsing on freebase from question-\nanswer pairs. In: Proceedings of the 2013 conference on empirical methods in natural\nlanguage processing, pp 1533–1544\nBerghofer S, Strecker M (2004) Extracting a formally verified, fully executable com-\npiler from a proof assistant. Electronic Notes in Theoretical Computer Science\n82(2):377–394\nBerglund L, Tong M, Kaufmann M, et al. (2023) The reversal curse: Llms trained on”\na is b” fail to learn” b is a”. arXiv preprint arXiv:230912288\nBerka P (2020) Sentiment analysis using rule-based and case-based reasoning. Journal\nof Intelligent Information Systems 55(1):51–66\nBerzonsky MD (1978) Formal reasoning in adolescence: An alternative view. Adoles-\ncence 13(50):279\nBetker J, Goh G, Jing L, et al. (2023) Improving image generation with better captions.\nComputer Science https://cdn openai com/papers/dall-e-3 pdf\nBhagavatula C, Bras RL, Malaviya C, et al. (2019) Abductive commonsense reasoning.\narXiv preprint arXiv:190805739\nBi K, Xie L, Zhang H, et al. (2023) Accurate medium-range global weather forecasting\nwith 3d neural networks. Nature pp 1–6\n101\nBisk Y, Zellers R, Gao J, et al. (2020) Piqa: Reasoning about physical commonsense in\nnatural language. In: Proceedings of the AAAI conference on artificial intelligence,\npp 7432–7439\nBlack S, Gao L, Wang P, et al. (2021) GPT-Neo: Large Scale Autoregressive Lan-\nguage Modeling with Mesh-Tensorflow. https://doi.org/10.5281/zenodo.5297715,\nURL https://doi.org/10.5281/zenodo.5297715, If you use this software, please cite\nit using these metadata.\nBlack S, Biderman S, Hallahan E, et al. (2022) GPT-NeoX-20B: An open-source\nautoregressive language model. In: Fan A, Ilic S, Wolf T, et al. (eds) Pro-\nceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives\nin Creating Large Language Models. Association for Computational Linguistics,\nvirtual+Dublin, pp 95–136, https://doi.org/10.18653/v1/2022.bigscience-1.9, URL\nhttps://aclanthology.org/2022.bigscience-1.9\nBommasani R, Hudson DA, Adeli E, et al. (2021) On the opportunities and risks of\nfoundation models. 2108.07258\nBoratko M, Li XL, Das R, et al. (2020) Protoqa: A question answering dataset for\nprototypical common-sense reasoning. arXiv preprint arXiv:200500771\nBottou L, Peters J, Qui˜ nonero-Candela J, et al. (2013) Counterfactual reasoning and\nlearning systems: The example of computational advertising. Journal of Machine\nLearning Research 14(11)\nBowman SR (2023) Eight things to know about large language models. arXiv preprint\narXiv:230400612\nBrandt A, McClure R (2011) Sound reasoning\nBrewka G (2012) Default Reasoning, Springer US, Boston, MA, pp 915–\n917. https://doi.org/10.1007/978-1-4419-1428-6 634, URL https://doi.org/10.\n1007/978-1-4419-1428-6 634\nBrohan A, Brown N, Carbajal J, et al. (2022) Rt-1: Robotics transformer for real-world\ncontrol at scale. arXiv preprint arXiv:221206817\nBrohan A, Brown N, Carbajal J, et al. (2023) Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. In: TODO\nBrown T, Mann B, Ryder N, et al. (2020) Language models are few-shot learners.\nAdvances in neural information processing systems 33:1877–1901\nBubeck S, Chandrasekaran V, Eldan R, et al. (2023) Sparks of artificial general\nintelligence: Early experiments with gpt-4. 2303.12712\n102\nBuchanan B, Lohn A, Musser M, et al. (2021) Truth, lies, and automation:\nHow language models could change disinformation. URL: https://cset georgetown\nedu/publication/truth-lies-and-automation/(visited on 10/13/2021)\nBuel GR, Walters KJ (2022) Can alphafold2 predict the impact of missense mutations\non structure? Nature Structural & Molecular Biology 29(1):1–2\nBui ND, Le H, Wang Y, et al. (2023) Codetf: One-stop transformer library for state-\nof-the-art code llm. arXiv preprint arXiv:230600029\nBurgess CP, Matthey L, Watters N, et al. (2019) Monet: Unsupervised scene\ndecomposition and representation. arXiv preprint arXiv:190111390\nByeon M, Park B, Kim H, et al. (2022) Coyo-700m: Image-text pair dataset\nByrne RM (2007) The rational imagination: How people create alternatives to reality.\nMIT press\nByrne RM, Tasso A (1999) Deductive reasoning with factual, possible, and counter-\nfactual conditionals. Memory & cognition 27:726–740\nCai LW, Dai WZ, Huang YX, et al. (2021) Abductive learning with ground knowledge\nbase. In: IJCAI, pp 1815–1821\nCai Q, Yates A (2013) Large-scale semantic parsing via schema matching and lexi-\ncon extension. In: Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). Association for Computational\nLinguistics, Sofia, Bulgaria, pp 423–433, URL https://aclanthology.org/P13-1042\nCao Y, Xu X, Sun C, et al. (2023) Segment any anomaly without training via hybrid\nprompt regularization. arXiv preprint arXiv:230510724\nCarlini N, Jagielski M, Choquette-Choo CA, et al. (2023) Poisoning web-scale training\ndatasets is practical. arXiv preprint arXiv:230210149\nCarpenter TP, Fennema E, Franke ML (1996) Cognitively guided instruction: A knowl-\nedge base for reform in primary mathematics instruction. The elementary school\njournal 97(1):3–20\nChai Y, Wang S, Pang C, et al. (2022) Ernie-code: Beyond english-centric cross-lingual\npretraining for programming languages. arXiv preprint arXiv:221206742\nChandel S, Clement CB, Serrato G, et al. (2022) Training and evaluating a jupyter\nnotebook data science assistant. arXiv preprint arXiv:220112901\nChang TY, Liu Y, Gopalakrishnan K, et al. (2021) Go beyond plain fine-tuning:\nImproving pretrained models for social commonsense. 2105.05913\n103\nChangpinyo S, Sharma P, Ding N, et al. (2021) Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 3558–3568\nCharalambous Y, Tihanyi N, Jain R, et al. (2023) A new era in software security:\nTowards self-healing software via large language models and formal verification.\narXiv preprint arXiv:230514752\nChen C, Feng X, Zhou J, et al. (2023a) Federated large language model: A position\npaper. 2307.08925\nChen G, Dong S, Shu Y, et al. (2023b) Autoagents: A framework for automatic agent\ngeneration. arXiv preprint arXiv:230917288\nChen J, Lin St, Durrett G (2019) Multi-hop question answering via reasoning chains.\narXiv preprint arXiv:191002610\nChen J, Tang J, Qin J, et al. (2021a) Geoqa: A geometric question answering bench-\nmark towards multimodal numerical reasoning. arXiv preprint arXiv:210514517\nChen J, Hu Z, Sun S, et al. (2022a) Interpretable rna foundation model from unanno-\ntated data for highly accurate rna structure and function predictions. bioRxiv pp\n2022–08\nChen J, Li T, Qin J, et al. (2022b) UniGeo: Unifying geometry logical reasoning\nvia reformulating mathematical expression. In: Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing. Association for Com-\nputational Linguistics, Abu Dhabi, United Arab Emirates, pp 3313–3323, URL\nhttps://aclanthology.org/2022.emnlp-main.218\nChen J, Yu J, Ge C, et al. (2023c) Pixart-alpha: Fast training of diffusion transformer\nfor photorealistic text-to-image synthesis. arXiv preprint arXiv:231000426\nChen K, Li J, Wang K, et al. (2023d) Towards an automatic ai agent for reaction\ncondition recommendation in chemical synthesis. arXiv preprint arXiv:231110776\nChen K, Liu Z, Hong L, et al. (2023e) Mixed autoencoder for self-supervised visual\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp 22742–22751\nChen L, Wu P, Chitta K, et al. (2023f) End-to-end autonomous driving: Challenges\nand frontiers. arXiv preprint arXiv:230616927\nChen M, Tworek J, Jun H, et al. (2021b) Evaluating large language models trained\non code. arXiv preprint arXiv:210703374\nChen S, Wang C, Chen Z, et al. (2022c) WavLM: Large-scale self-supervised pre-\ntraining for full stack speech processing. IEEE Journal of Selected Topics in Signal\n104\nProcessing 16(6):1505–1518\nChen S, Wong S, Chen L, et al. (2023g) Extending context window of large language\nmodels via positional interpolation. 2306.15595\nChen W, Zha H, Chen Z, et al. (2020a) HybridQA: A dataset of multi-hop question\nanswering over tabular and textual data. In: Findings of the Association for Com-\nputational Linguistics: EMNLP 2020. Association for Computational Linguistics,\nOnline, pp 1026–1036, https://doi.org/10.18653/v1/2020.findings-emnlp.91, URL\nhttps://aclanthology.org/2020.findings-emnlp.91\nChen W, Ma X, Wang X, et al. (2022d) Program of thoughts prompting: Disentan-\ngling computation from reasoning for numerical reasoning tasks. arXiv preprint\narXiv:221112588\nChen W, Yin M, Ku M, et al. (2023h) Theoremqa: A theorem-driven question\nanswering dataset. arXiv preprint arXiv:230512524\nChen X, Ding M, Wang X, et al. (2023i) Context autoencoder for self-supervised\nrepresentation learning. International Journal of Computer Vision pp 1–16\nChen X, Lin M, Sch¨ arli N, et al. (2023j) Teaching large language models to self-debug.\n2304.05128\nChen Y, Li L, Yu L, et al. (2020b) UNITER: universal image-text representation\nlearning. In: ECCV, pp 104–120\nChen Y, Qian S, Tang H, et al. (2023k) Longlora: Efficient fine-tuning of long-context\nlarge language models. arXiv:230912307\nChen Y, Zhang S, Han B, et al. (2023l) Lightweight in-context tuning for multimodal\nunified models. 2310.05109\nChen Z, Chen W, Smiley C, et al. (2021c) Finqa: A dataset of numerical reasoning\nover financial data. Proceedings of EMNLP 2021\nChen Z, Yi K, Li Y, et al. (2022e) Comphy: Compositional physical reasoning of\nobjects and events from videos. arXiv preprint arXiv:220501089\nChen Z, Ding M, Shen Y, et al. (2023m) An efficient general-purpose modular vision\nmodel via multi-task heterogeneous training. arXiv preprint arXiv:230617165\nChen Z, Shen Y, Ding M, et al. (2023n) Mod-squad: Designing mixtures of experts\nas modular multi-task learners. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp 11828–11837\nChen Z, Zhou K, Zhang B, et al. (2023o) Chatcot: Tool-augmented chain-of-thought\nreasoning on chat-based large language models. 2305.14323\n105\nCheng Y, Li L, Xu Y, et al. (2023) Segment and track anything. arXiv preprint\narXiv:230506558\nCheng Z, Dong H, Wang Z, et al. (2022) HiTab: A hierarchical table dataset for\nquestion answering and natural language generation. In: Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics, Dublin, Ireland, pp 1094–1110,\nhttps://doi.org/10.18653/v1/2022.acl-long.78, URL https://aclanthology.org/2022.\nacl-long.78\nCherti M, Beaumont R, Wightman R, et al. (2023) Reproducible scaling laws for\ncontrastive language-image learning. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp 2818–2829\nChiang WL, Li Z, Lin Z, et al. (2023) Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. URL https://lmsys.org/blog/2023-03-30-vicuna/\nChowdhery A, Narang S, Devlin J, et al. (2022) Palm: Scaling language modeling with\npathways. 2204.02311\nChristopoulou F, Lampouras G, Gritta M, et al. (2022) Pangu-coder: Program\nsynthesis with function-level language modeling. arXiv preprint arXiv:220711280\nChu R, Chen Y, Kong T, et al. (2021) Icm-3d: Instantiated category modeling for 3d\ninstance segmentation. IEEE Robotics and Automation Letters\nChu R, Liu Z, Ye X, et al. (2023a) Command-driven articulated object understand-\ning and manipulation. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp 8813–8823\nChu R, Xie E, Mo S, et al. (2023b) Diffcomplete: Diffusion-based generative 3d shape\ncompletion. arXiv preprint arXiv:230616329\nChung HW, Hou L, Longpre S, et al. (2022) Scaling instruction-finetuned language\nmodels. arXiv preprint arXiv:221011416\nChung YA, Hsu WN, Tang H, et al. (2019) An unsupervised autoregressive model for\nspeech representation learning. In: INTERSPEECH, pp 146–150\nClark P, Tafjord O, Richardson K (2020) Transformers as soft reasoners over language.\narXiv preprint arXiv:200205867\nClement CB, Drain D, Timcheck J, et al. (2020) Pymt5: multi-mode translation of nat-\nural language and python code with transformers. arXiv preprint arXiv:201003150\nCobbe K, Kosaraju V, Bavarian M, et al. (2021) Training verifiers to solve math word\nproblems. arXiv preprint arXiv:211014168\n106\nCohen GH (1997) Align: a program to superimpose protein coordinates, accounting\nfor insertions and deletions. Journal of applied crystallography 30(6):1160–1161\nCollins A, Michalski R (1989) The logic of plausible reasoning: A core theory. cognitive\nscience 13(1):1–49\nComputer T (2023) Redpajama: An open source recipe to reproduce llama training\ndataset. URL https://github.com/togethercomputer/RedPajama-Data\nConover M, Hayes M, Mathur A, et al. (2023) Free dolly: Introducing the world’s first\ntruly open instruction-tuned llm\nCreswell A, Shanahan M, Higgins I (2023) Selection-inference: Exploiting large lan-\nguage models for interpretable logical reasoning. In: The Eleventh International\nConference on Learning Representations, URL https://openreview.net/forum?id=\n3Pf3Wg6o-A4\nCropper A, Dumanˇ ci´ c S, Evans R, et al. (2022) Inductive logic programming at 30.\nMachine Learning pp 1–26\nDai W, Liu Z, Ji Z, et al. (2023) Plausible may not be faithful: Probing object hal-\nlucination in vision-language pre-training. In: Proceedings of the 17th Conference\nof the European Chapter of the Association for Computational Linguistics. Asso-\nciation for Computational Linguistics, Dubrovnik, Croatia, pp 2136–2148, URL\nhttps://aclanthology.org/2023.eacl-main.156\nDaniel K (2017) Thinking, fast and slow\nDao T, Fu DY, Saab KK, et al. (2022) Hungry hungry hippos: Towards language\nmodeling with state space models. arXiv preprint arXiv:221214052\nDas P, Sercu T, Wadhawan K, et al. (2021) Accelerated antimicrobial discovery via\ndeep generative models and molecular dynamics simulations. Nature Biomedical\nEngineering 5(6):613–623\nDasgupta I, Kaeser-Chen C, Marino K, et al. (2022) Collaborating with language mod-\nels for embodied reasoning. In: Second Workshop on Language and Reinforcement\nLearning, URL https://openreview.net/forum?id=YoS-abmWjJc\nDe Raedt L, Kersting K (2010) Statistical relational learning. Encyclopedia of Machine\nLearning\nDeitke M, Han W, Herrasti A, et al. (2020) Robothor: An open simulation-to-real\nembodied ai platform. In: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp 3164–3174\nDeng J, Dong W, Socher R, et al. (2009) Imagenet: A large-scale hierarchical image\ndatabase. In: 2009 IEEE conference on computer vision and pattern recognition,\n107\nIeee, pp 248–255\nDesai K, Kaul G, Aysola Z, et al. (2021) Redcaps: Web-curated image-text data\ncreated by the people, for the people. arXiv preprint arXiv:211111431\nDettmers T, Pagnoni A, Holtzman A, et al. (2023) Qlora: Efficient finetuning of\nquantized llms. arXiv preprint arXiv:230514314\nDevlin J, Chang MW, Lee K, et al. (2019) BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In: Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers). Association\nfor Computational Linguistics, Minneapolis, Minnesota, pp 4171–4186, https://doi.\norg/10.18653/v1/N19-1423, URL https://aclanthology.org/N19-1423\nDi P, Li J, Yu H, et al. (2023) Codefuse-13b: A pretrained multi-lingual code large\nlanguage model. arXiv preprint arXiv:231006266\nDing D, Hill F, Santoro A, et al. (2020) Object-based attention for spatio-temporal\nreasoning: Outperforming neuro-symbolic models with flexible distributed architec-\ntures. arXiv preprint arXiv:201208508 1\nDing D, Hill F, Santoro A, et al. (2021a) Attention over learned object embed-\ndings enables complex visual reasoning. In: Beygelzimer A, Dauphin Y, Liang\nP, et al. (eds) Advances in Neural Information Processing Systems, URL https:\n//openreview.net/forum?id=lHmhW2zmVN\nDing J, Ma S, Dong L, et al. (2023a) Longnet: Scaling transformers to 1,000,000,000\ntokens. 2307.02486\nDing M, Chen Z, Du T, et al. (2021b) Dynamic visual reasoning by learning differ-\nentiable physics models from video and language. Advances in Neural Information\nProcessing Systems 34:887–899\nDing M, Xiao B, Codella N, et al. (2022) Davit: Dual attention vision transformers.\nIn: European Conference on Computer Vision, Springer, pp 74–92\nDing M, Shen Y, Fan L, et al. (2023b) Visual dependency transformers: Dependency\ntree emerges from reversed attention. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp 14528–14539\nDing M, Xu Y, Chen Z, et al. (2023c) Embodied concept learner: Self-supervised\nlearning of concepts and mapping through instruction following. In: Conference on\nRobot Learning, PMLR, pp 1743–1754\nDing N, Chen Y, Xu B, et al. (2023d) Ultrachat: A large-scale auto-generated multi-\nround dialogue data. https://github.com/thunlp/ultrachat\n108\nDing N, Chen Y, Xu B, et al. (2023e) Enhancing chat language models by scaling\nhigh-quality instructional conversations. arXiv preprint arXiv:230514233\nDing X, Han J, Xu H, et al. (2023f) Hilm-d: Towards high-resolution understand-\ning in multimodal large language models for autonomous driving. arXiv preprint\narXiv:230905186\ndo Nascimento NM, de Lucena CJP (2017) Fiot: An agent-based framework for self-\nadaptive and self-organizing applications based on the internet of things. Informa-\ntion Sciences 378:161–176. https://doi.org/https://doi.org/10.1016/j.ins.2016.10.\n031, URL https://www.sciencedirect.com/science/article/pii/S0020025516313664\nDong H, Xiong W, Goyal D, et al. (2023a) Raft: Reward ranked finetuning for\ngenerative foundation model alignment. arXiv preprint arXiv:230406767\nDong L, Xu S, Xu B (2018) Speech-transformer: a no-recurrence sequence-to-sequence\nmodel for speech recognition. In: ICASSP, pp 5884–5888\nDong Y, Jiang X, Jin Z, et al. (2023b) Self-collaboration code generation via chatgpt.\n2304.07590\nDosovitskiy A, Beyer L, Kolesnikov A, et al. (2021) An image is worth 16x16 words:\nTransformers for image recognition at scale. In: International Conference on Machine\nLearning\nDriess D, Xia F, Sajjadi MSM, et al. (2023) Palm-e: An embodied multimodal language\nmodel. In: arXiv preprint arXiv:2303.03378\nDriveLM Contributors (2023) Drive on Language. URL https://github.com/\nOpenDriveLab/DriveLM/\nDu N, Huang Y, Dai AM, et al. (2022a) Glam: Efficient scaling of language models\nwith mixture-of-experts. In: International Conference on Machine Learning, PMLR,\npp 5547–5569\nDu W, Kim ZM, Raheja V, et al. (2022b) Read, revise, repeat: A system demonstration\nfor human-in-the-loop iterative text revision. arXiv preprint arXiv:220403685\nDu Y, Li S, Torralba A, et al. (2023) Improving factuality and reasoning in language\nmodels through multiagent debate. 2305.14325\nDua D, Wang Y, Dasigi P, et al. (2019) DROP: A reading comprehension benchmark\nrequiring discrete reasoning over paragraphs. In: Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers). Association\nfor Computational Linguistics, Minneapolis, Minnesota, pp 2368–2378, https://doi.\norg/10.18653/v1/N19-1246, URL https://aclanthology.org/N19-1246\n109\nEchterhoff J, Yan A, Han K, et al. (2023) Driving through the concept gridlock:\nUnraveling explainability bottlenecks. arXiv preprint arXiv:231016639\nEdalati A, Tahaei M, Kobyzev I, et al. (2022) Krona: Parameter efficient tuning with\nkronecker adapter. arXiv preprint arXiv:221210650\nEichenberg C, Black S, Weinbach S, et al. (2022) MAGMA – multimodal augmen-\ntation of generative models through adapter-based finetuning. In: Findings of the\nAssociation for Computational Linguistics: EMNLP 2022. Association for Com-\nputational Linguistics, Abu Dhabi, United Arab Emirates, pp 2416–2428, URL\nhttps://aclanthology.org/2022.findings-emnlp.179\nEspeholt L, Agrawal S, Sønderby C, et al. (2022) Deep learning for twelve hour\nprecipitation forecasts. Nature communications 13(1):1–10\nEvans JSB, Thompson VA (2004) Informal reasoning: Theory and method. Cana-\ndian Journal of Experimental Psychology/Revue canadienne de psychologie\nexp´ erimentale 58(2):69\nFan A, Bhosale S, Schwenk H, et al. (2021) Beyond english-centric multilingual\nmachine translation. J Mach Learn Res 22(1)\nFan L, Krishnan D, Isola P, et al. (2023) Improving clip training with language\nrewrites. arXiv preprint arXiv:230520088\nFang Y, Sun S, Gan Z, et al. (2020) Hierarchical graph network for multi-hop ques-\ntion answering. In: Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP). Association for Computational Lin-\nguistics, Online, pp 8823–8838, https://doi.org/10.18653/v1/2020.emnlp-main.710,\nURL https://aclanthology.org/2020.emnlp-main.710\nFang Y, Wang S, Xu Y, et al. (2022) Leveraging knowledge in multilingual common-\nsense reasoning. In: Findings of the Association for Computational Linguistics: ACL\n2022. Association for Computational Linguistics, Dublin, Ireland, pp 3237–3246,\nhttps://doi.org/10.18653/v1/2022.findings-acl.255, URL https://aclanthology.org/\n2022.findings-acl.255\nFayek HM, Johnson J (2019) Temporal reasoning via audio question answering. 1911.\n09655\nFeder A, Oved N, Shalit U, et al. (2021) CausaLM: Causal Model Explanation\nThrough Counterfactual Language Models. Computational Linguistics 47(2):333–\n386. https://doi.org/10.1162/coli a 00404, URL https://doi.org/10.1162/coli a\n00404, https://direct.mit.edu/coli/article-pdf/47/2/333/1938107/coli a 00404.pdf\nFedus W, Zoph B, Shazeer N (2022) Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. The Journal of Machine Learning Research\n110\n23(1):5232–5270\nFei N, Lu Z, Gao Y, et al. (2022) Towards artificial general intelligence via a\nmultimodal foundation model. Nature Communications 13(1):3094\nFennema E, Carpenter TP, Franke ML, et al. (1996) A longitudinal study of learn-\ning to use children’s thinking in mathematics instruction. Journal for research in\nmathematics education 27(4):403–434\nFiroozi R, Tucker J, Tian S, et al. (2023) Foundation models in robotics: Applications,\nchallenges, and the future. arXiv preprint arXiv:231207843\nFlach PA, Kakas AC (2000) Abductive and Inductive Reasoning: Background\nand Issues, Springer Netherlands, Dordrecht, pp 1–27. https://doi.org/10.1007/\n978-94-017-0606-3 1, URL https://doi.org/10.1007/978-94-017-0606-3 1\nFloyd J (2004) Wittgenstein on philosophy of logic and mathematics. Graduate\nFaculty Philosophy Journal 25(2):227–287\nFried D, Aghajanyan A, Lin J, et al. (2022) Incoder: A generative model for code\ninfilling and synthesis. arXiv preprint arXiv:220405999\nFriedman R (2023a) Large language models and logical reasoning. Encyclopedia\n3(2):687–697\nFriedman R (2023b) Tokenization in the theory of knowledge. Encyclopedia 3(1):380–\n386\nFrohberg J, Binder F (2021) Crass: A novel data set and benchmark to test\ncounterfactual reasoning of large language models. arXiv preprint arXiv:211211941\nFu Y, Peng H, Sabharwal A, et al. (2022) Complexity-based prompting for multi-step\nreasoning. arXiv preprint arXiv:221000720\nFu Y, Peng H, Khot T, et al. (2023a) Improving language model negotiation with\nself-play and in-context learning from ai feedback. 2305.10142\nFu Y, Peng H, Ou L, et al. (2023b) Specializing smaller language models towards\nmulti-step reasoning. arXiv preprint arXiv:230112726\nFu Z, Lam W, Yu Q, et al. (2023c) Decoder-only or encoder-decoder? interpreting\nlanguage model as a regularized encoder-decoder. arXiv preprint arXiv:230404052\nFu Z, Yang H, So AMC, et al. (2023d) On the effectiveness of parameter-efficient\nfine-tuning. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp\n12799–12807\n111\nFurbach U, H¨ olldobler S, Ragni M, et al. (2019) Cognitive reasoning: A personal view.\nKI-K¨ unstliche Intelligenz 33:209–217\nGadre SY, Ilharco G, Fang A, et al. (2023) Datacomp: In search of the next generation\nof multimodal datasets. arXiv preprint arXiv:230414108\nGao D, Ji L, Zhou L, et al. (2023a) Assistgpt: A general multi-modal assistant that\ncan plan, execute, inspect, and learn. 2306.08640\nGao L, Biderman S, Black S, et al. (2020) The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:210100027\nGao L, Madaan A, Zhou S, et al. (2023b) Pal: Program-aided language models. In:\nInternational Conference on Machine Learning, PMLR, pp 10764–10799\nGao P, Han J, Zhang R, et al. (2023c) Llama-adapter v2: Parameter-efficient visual\ninstruction model. arXiv preprint arXiv:230415010\nGarcez Ad, Besold TR, De Raedt L, et al. (2015) Neural-symbolic learning and\nreasoning: contributions and challenges. In: 2015 AAAI Spring Symposium Series\nGarcez Ad, Bader S, Bowman H, et al. (2022) Neural-symbolic learning and reasoning:\nA survey and interpretation. Neuro-Symbolic Artificial Intelligence: The State of\nthe Art 342(1):327\nGarcez AS, Lamb LC, Gabbay DM (2008) Neural-symbolic cognitive reasoning.\nSpringer Science & Business Media\nGauthier T, Kaliszyk C, Urban J, et al. (2021) Tactictoe: learning to prove with\ntactics. Journal of Automated Reasoning 65:257–286\nGe Y, Hua W, Mei K, et al. (2023) Openagi: When llm meets domain experts. 2304.\n04370\nGemmeke JF, Ellis DP, Freedman D, et al. (2017) Audio set: An ontology and\nhuman-labeled dataset for audio events. In: 2017 IEEE international conference on\nacoustics, speech and signal processing (ICASSP), IEEE, pp 776–780\nGendron G, Bao Q, Witbrock M, et al. (2023) Large language models are not abstract\nreasoners. 2305.19555\nGeng X, Gudibande A, Liu H, et al. (2023) Koala: A dialogue model for academic\nresearch. Blog post, URL https://bair.berkeley.edu/blog/2023/04/03/koala/\nGeva M, Khashabi D, Segal E, et al. (2021) Did aristotle use a laptop? a ques-\ntion answering benchmark with implicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics 9:346–361. https://doi.org/10.1162/tacl\na 00370, URL https://aclanthology.org/2021.tacl-1.21\n112\nGirdhar R, Ramanan D (2020) Cater: A diagnostic dataset for compositional actions\nand temporal reasoning. In: ICLR\nGirdhar R, Singh M, Ravi N, et al. (2022) Omnivore: A single model for many visual\nmodalities. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp 16102–16112\nGirdhar R, El-Nouby A, Liu Z, et al. (2023a) Imagebind: One embedding space to\nbind them all. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp 15180–15190\nGirdhar R, El-Nouby A, Singh M, et al. (2023b) Omnimae: Single model masked\npretraining on images and videos. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp 10406–10417\nGou Z, Shao Z, Gong Y, et al. (2023) Critic: Large language models can self-correct\nwith tool-interactive critiquing. 2305.11738\ngravitas/auto gpt S (2023) An experimental open-source attempt to make gpt-4 fully\nautonomou. 2305.16291\nGramopadhye M, Szafir D (2022) Generating executable action plans with\nenvironmentally-aware language models. arXiv preprint arXiv:221004964\nGu A, Goel K, R´ e C (2021) Efficiently modeling long sequences with structured state\nspaces. arXiv preprint arXiv:211100396\nGu J, Han Z, Chen S, et al. (2023a) A systematic survey of prompt engineering on\nvision-language foundation models. 2307.12980\nGu Y, Dong L, Wei F, et al. (2023b) Knowledge distillation of large language models.\n2306.08543\nGulati A, Qin J, Chiu CC, et al. (2020) Conformer: Convolution-augmented trans-\nformer for speech recognition. In: INTERSPEECH, pp 5036–5040\nGuo J, Li J, Li D, et al. (2023a) From images to textual prompts: Zero-shot vqa with\nfrozen large language models. 2212.10846\nGuo Z, Zhang R, Zhu X, et al. (2023b) Point-bind & point-llm: Aligning point cloud\nwith multi-modality for 3d understanding, generation, and instruction following.\narXiv preprint arXiv:230900615\nGupta A, Gu A, Berant J (2022) Diagonal state spaces are as effective as structured\nstate spaces. Advances in Neural Information Processing Systems 35:22982–22994\nGupta N, Lin K, Roth D, et al. (2019) Neural module networks for reasoning over\ntext. arXiv preprint arXiv:191204971\n113\nGupta T, Kembhavi A (2022) Visual programming: Compositional visual reasoning\nwithout training. 2211.11559\nGuzhov A, Raue F, Hees J, et al. (2022) Audioclip: Extending clip to image, text and\naudio. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), IEEE, pp 976–980\nHalpern JY (2016) Actual causality. MiT Press\nHan JM, Rute J, Wu Y, et al. (2021) Proof artifact co-training for theorem proving\nwith language models. arXiv preprint arXiv:210206203\nHan S, Schoelkopf H, Zhao Y, et al. (2022) Folio: Natural language reasoning with\nfirst-order logic. 2209.00840\nHao S, Gu Y, Ma H, et al. (2023a) Reasoning with language model is planning with\nworld model. 2305.14992\nHao S, Liu T, Wang Z, et al. (2023b) Toolkengpt: Augmenting frozen language models\nwith massive tools via tool embeddings. 2305.11554\nHarrison J (2010) Formal methods at intel—an overview. In: Second NASA Formal\nMethods Symposium, pp 179–195\nHe H, Zhang J, Xu M, et al. (2023) Scalable mask annotation for video text spotting.\narXiv preprint arXiv:230501443\nHe J, Zhou C, Ma X, et al. (2021) Towards a unified view of parameter-efficient transfer\nlearning. In: International Conference on Learning Representations\nHe K, Chen X, Xie S, et al. (2022) Masked autoencoders are scalable vision learn-\ners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp 16000–16009\nHemphill CT, Godfrey JJ, Doddington GR (1990) The ATIS spoken language systems\npilot corpus. In: Speech and Natural Language: Proceedings of a Workshop Held\nat Hidden Valley, Pennsylvania, June 24-27,1990, URL https://aclanthology.org/\nH90-1021\nHendrycks D, Basart S, Kadavath S, et al. (2021a) Measuring coding challenge\ncompetence with apps. NeurIPS\nHendrycks D, Burns C, Kadavath S, et al. (2021b) Measuring mathematical problem\nsolving with the math dataset. NeurIPS\nHinton GE (1990) Connectionist learning procedures. In: Machine learning. Elsevier,\np 555–610\n114\nHo N, Schmid L, Yun SY (2022) Large language models are reasoning teachers. arXiv\npreprint arXiv:221210071\nHong Y, Li Q, Zhu SC, et al. (2021a) Vlgrammar: Grounded grammar induction of\nvision and language\nHong Y, Yi L, Tenenbaum JB, et al. (2021b) Ptr: A benchmark for part-based\nconceptual, relational, and physical reasoning. 2112.05136\nHong Y, Zhen H, Chen P, et al. (2023) 3d-llm: Injecting the 3d world into large\nlanguage models. arXiv\nHongjin S, Kasai J, Wu CH, et al. (2022) Selective annotation makes language models\nbetter few-shot learners. In: The Eleventh International Conference on Learning\nRepresentations\nHonovich O, Scialom T, Levy O, et al. (2022) Unnatural instructions: Tuning language\nmodels with (almost) no human labor. URL https://arxiv.org/abs/2212.09689\nHorawalavithana S, Ayton E, Sharma S, et al. (2022) Foundation models of scientific\nknowledge for chemistry: Opportunities, challenges and lessons learned. In: Fan\nA, Ilic S, Wolf T, et al. (eds) Proceedings of BigScience Episode #5 – Workshop\non Challenges & Perspectives in Creating Large Language Models. Association for\nComputational Linguistics, virtual+Dublin, pp 160–172, https://doi.org/10.18653/\nv1/2022.bigscience-1.12, URL https://aclanthology.org/2022.bigscience-1.12\nHosseini MJ, Hajishirzi H, Etzioni O, et al. (2014) Learning to solve arithmetic\nword problems with verb categorization. In: Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Doha, Qatar, pp 523–533, https://doi.org/10.3115/v1/\nD14-1058, URL https://aclanthology.org/D14-1058\nHoulsby N, Giurgiu A, Jastrzebski S, et al. (2019) Parameter-efficient transfer learning\nfor nlp. In: International Conference on Machine Learning, PMLR, pp 2790–2799\nHsieh CY, Li CL, Yeh Ck, et al. (2023) Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller model sizes. In: Findings of\nthe Association for Computational Linguistics: ACL 2023. Association for Compu-\ntational Linguistics, Toronto, Canada, pp 8003–8017, https://doi.org/10.18653/v1/\n2023.findings-acl.507, URL https://aclanthology.org/2023.findings-acl.507\nHsu WN, Bolte B, Tsai YHH, et al. (2021) HuBERT: Self-supervised speech repre-\nsentation learning by masked prediction of hidden units. IEEE/ACM Transactions\non Audio, Speech, and Language Processing 29:3451–3460\nHsu YC, Hua T, Chang S, et al. (2022) Language model compression with weighted\nlow-rank factorization. 2207.00112\n115\nHu EJ, yelong shen, Wallis P, et al. (2022) LoRA: Low-rank adaptation of large\nlanguage models. In: International Conference on Learning Representations, URL\nhttps://openreview.net/forum?id=nZeVKeeFYf9\nHu M, Mu Y, Yu X, et al. (2023a) Tree-planner: Efficient close-loop task planning\nwith large language models. arXiv preprint arXiv:231008582\nHu Y, Yang H, Lin Z, et al. (2023b) Code prompting: a neural symbolic method for\ncomplex reasoning in large language models. 2305.18507\nHuang D, Shi S, Lin CY, et al. (2016) How well do computers solve math word\nproblems? large-scale dataset construction and evaluation. In: Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp 887–896\nHuang G, Hu J, He Y, et al. (2021a) Machine learning for electronic design automa-\ntion: A survey. ACM Transactions on Design Automation of Electronic Systems\n(TODAES) 26(5):1–46\nHuang H, Tang T, Zhang D, et al. (2023a) Not all languages are created equal in llms:\nImproving multilingual capability by cross-lingual-thought prompting. 2305.07004\nHuang J, Chang KCC (2022) Towards reasoning in large language models: A survey.\narXiv preprint arXiv:221210403\nHuang J, Xie S, Sun J, et al. (2021b) Learning a decision module by imitating driver’s\ncontrol behaviors. In: Kober J, Ramos F, Tomlin C (eds) Proceedings of the 2020\nConference on Robot Learning, Proceedings of Machine Learning Research, vol 155.\nPMLR, pp 1–10, URL https://proceedings.mlr.press/v155/huang21a.html\nHuang J, Gu SS, Hou L, et al. (2022a) Large language models can self-improve. arXiv\npreprint arXiv:221011610\nHuang J, Zhu WY, Jia B, et al. (2023b) Perceive, ground, reason, and act: A\nbenchmark for general-purpose visual representation. URL https://openreview.net/\nforum?id=f6cywgfd11\nHuang K, Sun K, Xie E, et al. (2023c) T2i-compbench: A comprehensive benchmark for\nopen-world compositional text-to-image generation. arXiv preprint arXiv:230706350\nHuang L, Bras RL, Bhagavatula C, et al. (2019) Cosmos qa: Machine reading compre-\nhension with contextual commonsense reasoning. arXiv preprint arXiv:190900277\nHuang S, Dong L, Wang W, et al. (2023d) Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:230214045\nHuang W, Abbeel P, Pathak D, et al. (2022b) Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. In: International Conference\n116\non Machine Learning, PMLR, pp 9118–9147\nHuang W, Xia F, Xiao T, et al. (2022c) Inner monologue: Embodied reasoning through\nplanning with language models. In: arXiv preprint arXiv:2207.05608\nHuang X, Ruan W, Huang W, et al. (2023e) A survey of safety and trustworthiness\nof large language models through the lens of verification and validation. 2305.11391\nHuang Y, Kleindessner M, Munishkin A, et al. (2021c) Benchmarking of data-driven\ncausality discovery approaches in the interactions of arctic sea ice and atmosphere.\nFrontiers in big Data 4:642182\nHudson DA, Manning CD (2019) Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In: Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp 6700–6709\nHuo Y, Zhang M, Liu G, et al. (2021) Wenlan: Bridging vision and language by\nlarge-scale multi-modal pre-training. arXiv preprint arXiv:210306561\nIlharco G, Wortsman M, Wightman R, et al. (2021) Openclip. Zenodo 4:5\nImani S, Du L, Shrivastava H (2023) Mathprompter: Mathematical reasoning using\nlarge language models. 2303.05398\nInaba T, Kiyomaru H, Cheng F, et al. (2023) Multitool-cot: Gpt-3 can use multiple\nexternal tools with chain of thought prompting. 2305.16896\nIyer S, Lin XV, Pasunuru R, et al. (2022) Opt-iml: Scaling language model instruction\nmeta learning through the lens of generalization. arXiv preprint arXiv:221212017\nJacobs RA, Jordan MI, Nowlan SJ, et al. (1991) Adaptive mixtures of local experts.\nNeural computation 3(1):79–87\nJaderberg M, Simonyan K, Zisserman A, et al. (2016) Spatial transformer networks.\n1506.02025\nJain N, Saifullah K, Wen Y, et al. (2023) Bring your own data! self-supervised\nevaluation for large language models. arXiv preprint arXiv:230613651\nJansen P (2020) Visually-grounded planning without vision: Language models infer\ndetailed plans from high-level instructions. In: Findings of the Association for Com-\nputational Linguistics: EMNLP 2020. Association for Computational Linguistics,\nOnline, pp 4412–4417, https://doi.org/10.18653/v1/2020.findings-emnlp.395, URL\nhttps://aclanthology.org/2020.findings-emnlp.395\nJi Z, Lee N, Frieske R, et al. (2023) Survey of hallucination in natural language\ngeneration. ACM Computing Surveys 55(12):1–38\n117\nJia C, Yang Y, Xia Y, et al. (2021) Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In: International conference on machine\nlearning, PMLR, pp 4904–4916\nJiang AQ, Li W, Han JM, et al. (2021a) Lisa: Language models of isabelle proofs. In:\n6th Conference on Artificial Intelligence and Theorem Proving, pp 378–392\nJiang AQ, Welleck S, Zhou JP, et al. (2022) Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs. arXiv preprint arXiv:221012283\nJiang AQ, Sablayrolles A, Mensch A, et al. (2023a) Mistral 7b. arXiv preprint\narXiv:231006825\nJiang D, Li W, Cao M, et al. (2020a) Speech SIMCLR: Combining contrastive\nand reconstruction objective for self-supervised speech representation learning. In:\nINTERSPEECH, pp 1544–1548\nJiang D, Li W, Zhang R, et al. (2021b) A further study of unsupervised pretraining\nfor transformer based speech recognition. In: ICASSP, pp 6538–6542\nJiang W, Lin B, Shi H, et al. (2023b) Effective and parameter-efficient reusing fine-\ntuned models. arXiv preprint arXiv:231001886\nJiang W, Shi H, Yu L, et al. (2023c) Backward reasoning in large language models for\nverification. arXiv preprint arXiv:230807758\nJiang W, Zhang Y, Kwok J (2023d) Effective structured prompting by meta-learning\nand representative verbalizer. In: International Conference on Machine Learning,\nPMLR, pp 15186–15199\nJiang Z, Xu FF, Araki J, et al. (2020b) How can we know what language models\nknow? Transactions of the Association for Computational Linguistics 8:423–438\nJiang Z, Araki J, Ding H, et al. (2021c) How can we know when language models\nknow? on the calibration of language models for question answering. Transactions\nof the Association for Computational Linguistics 9:962–977\nJiao R, Wang Z, Chu R, et al. (2020) An intuitive end-to-end human-uav interaction\nsystem for field exploration. Frontiers in Neurorobotics\nJin Q, Yang Y, Chen Q, et al. (2023a) Genegpt: Augmenting large language models\nwith domain tools for improved access to biomedical information. ArXiv\nJin Z, Liu J, Lyu Z, et al. (2023b) Can large language models infer causation from\ncorrelation? arXiv preprint arXiv:230605836\nJohnson J, Hariharan B, Van Der Maaten L, et al. (2017) Clevr: A diagnostic dataset\nfor compositional language and elementary visual reasoning. In: Proceedings of the\n118\nIEEE conference on computer vision and pattern recognition, pp 2901–2910\nJoshi M, Choi E, Weld D, et al. (2017) TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. In: Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers).\nAssociation for Computational Linguistics, Vancouver, Canada, pp 1601–1611,\nhttps://doi.org/10.18653/v1/P17-1147, URL https://aclanthology.org/P17-1147\nJumper J, Evans R, Pritzel A, et al. (2021) Highly accurate protein structure\nprediction with alphafold. Nature 596(7873):583–589\nKahn J, Rivi` ere M, Zheng W, et al. (2020) Libri-light: A benchmark for asr with\nlimited or no supervision. In: ICASSP, pp 7669–7673\nKahneman D, Miller DT (1986) Norm theory: Comparing reality to its alternatives.\nPsychological review 93(2):136\nKarimi Mahabadi R, Henderson J, Ruder S (2021) Compacter: Efficient low-rank\nhypercomplex adapter layers. Advances in Neural Information Processing Systems\n34:1022–1035\nKatsis Y, Chemmengath S, Kumar V, et al. (2022) AIT-QA: Question answering\ndataset over complex tables in the airline industry. In: Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies: Industry Track. Association for Com-\nputational Linguistics, Hybrid: Seattle, Washington + Online, pp 305–314, https://\ndoi.org/10.18653/v1/2022.naacl-industry.34, URL https://aclanthology.org/2022.\nnaacl-industry.34\nKazemi M, Yuan Q, Bhatia D, et al. (2023) Boardgameqa: A dataset for natural\nlanguage reasoning with contradictory information. arXiv preprint arXiv:230607934\nKenton JDMWC, Toutanova LK (2019) Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In: Proceedings of naacL-HLT, p 2\nKhan W, Kamran M, Naqvi SR, et al. (2020) Formal verification of hardware\ncomponents in critical systems. Wireless Communications and Mobile Computing\n2020:1–15\nKhashabi D, Min S, Khot T, et al. (2020) Unifiedqa: Crossing format boundaries with\na single qa system. arXiv preprint arXiv:200500700\nKıcıman E, Ness R, Sharma A, et al. (2023) Causal reasoning and large language\nmodels: Opening a new frontier for causality. arXiv preprint arXiv:230500050\nKim J, Misu T, Chen YT, et al. (2019) Grounding human-to-vehicle advice for\nself-driving vehicles. In: IEEE/CVF Conference on Computer Vision and Pattern\n119\nRecognition\nKim S, Joo SJ, Kim D, et al. (2023) The cot collection: Improving zero-shot and few-\nshot learning of language models via chain-of-thought fine-tuning. arXiv preprint\narXiv:230514045\nKim W, Son B, Kim I (2021) Vilt: Vision-and-language transformer without con-\nvolution or region supervision. In: International Conference on Machine Learning,\nPMLR, pp 5583–5594\nKirchenbauer J, Geiping J, Wen Y, et al. (2023) A watermark for large language\nmodels. 2301.10226\nKirillov A, Mintun E, Ravi N, et al. (2023) Segment anything. arXiv:230402643\nKocetkov D, Li R, Allal LB, et al. (2022) The stack: 3 tb of permissively licensed\nsource code. arXiv preprint arXiv:221115533\nKojima T, Gu SS, Reid M, et al. (2022) Large language models are zero-shot reasoners.\nAdvances in neural information processing systems 35:22199–22213\nKoncel-Kedziorski R, Hajishirzi H, Sabharwal A, et al. (2015) Parsing algebraic word\nproblems into equations. Transactions of the Association for Computational Linguis-\ntics 3:585–597. https://doi.org/10.1162/tacl a 00160, URL https://aclanthology.\norg/Q15-1042\nKoncel-Kedziorski R, Roy S, Amini A, et al. (2016) Mawps: A math word prob-\nlem repository. In: North American Chapter of the Association for Computational\nLinguistics\nKondo K, Sugawara S, Aizawa A (2023) Probing physical reasoning with counter-\ncommonsense context. In: Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers). Association for Compu-\ntational Linguistics, Toronto, Canada, pp 603–612, https://doi.org/10.18653/v1/\n2023.acl-short.53, URL https://aclanthology.org/2023.acl-short.53\nKoons R (2005) Defeasible reasoning. arXiv\nKosinski M (2023) Theory of mind may have spontaneously emerged in large language\nmodels. 2302.02083\nKreps S, McCain RM, Brundage M (2022) All the news that’s fit to fabricate: Ai-\ngenerated text as a tool of media misinformation. Journal of experimental political\nscience 9(1):104–117\nKurita K, Michel P, Neubig G (2020) Weight poisoning attacks on pre-trained models.\narXiv preprint arXiv:200406660\n120\nKushman N, Artzi Y, Zettlemoyer L, et al. (2014) Learning to automatically solve alge-\nbra word problems. In: Proceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers). Association for Computa-\ntional Linguistics, Baltimore, Maryland, pp 271–281, https://doi.org/10.3115/v1/\nP14-1026, URL https://aclanthology.org/P14-1026\nKwiatkowski T, Palomaki J, Redfield O, et al. (2019) Natural questions: A benchmark\nfor question answering research. Transactions of the Association for Computa-\ntional Linguistics 7:452–466. https://doi.org/10.1162/tacl a 00276, URL https://\naclanthology.org/Q19-1026\nK¨ oksal A, Schick T, Korhonen A, et al. (2023) Longform: Optimizing instruction\ntuning for long text generation with corpus extraction. 2304.08460\nK¨ opf A, Kilcher Y, von R¨ utte D, et al. (2023) Openassistant conversations –\ndemocratizing large language model alignment. 2304.07327\nLaban P, Kry´ sci´ nski W, Agarwal D, et al. (2023) Llms as factual reasoners: Insights\nfrom existing benchmarks and beyond. 2305.14540\nLahiri S (2014) Complexity of Word Collocation Networks: A Preliminary Structural\nAnalysis. In: Proceedings of the Student Research Workshop at the 14th Confer-\nence of the European Chapter of the Association for Computational Linguistics.\nAssociation for Computational Linguistics, Gothenburg, Sweden, pp 96–105, URL\nhttp://www.aclweb.org/anthology/E14-3011\nLai X, Tian Z, Chen Y, et al. (2023) Lisa: Reasoning segmentation via large language\nmodel. arXiv preprint arXiv:230800692\nLample G, Lacroix T, Lachaux MA, et al. (2022) Hypertree proof search for neural\ntheorem proving. Advances in Neural Information Processing Systems 35:26337–\n26349\nLauren¸ con H, Saulnier L, Wang T, et al. (2022) The bigscience roots corpus: A 1.6 tb\ncomposite multilingual dataset. Advances in Neural Information Processing Systems\n35:31809–31826\nLaurent J, Platzer A (2022) Learning to find proofs and theorems by learning to\nrefine search strategies: The case of loop invariant synthesis. Advances in Neural\nInformation Processing Systems 35:4843–4856\nLe H, Wang Y, Gotmare AD, et al. (2022) Coderl: Mastering code generation through\npretrained models and deep reinforcement learning. Advances in Neural Information\nProcessing Systems 35:21314–21328\nLeake DB (2012) Introspective Learning and Reasoning, Springer US, Boston, MA,\npp 1638–1640. https://doi.org/10.1007/978-1-4419-1428-6 1802, URL https://doi.\n121\norg/10.1007/978-1-4419-1428-6 1802\nLee YJ, Lim CG, Choi HJ (2022) Does gpt-3 generate empathetic dialogues? a novel\nin-context example selection method and automatic evaluation metric for empa-\nthetic dialogue generation. In: Proceedings of the 29th International Conference on\nComputational Linguistics, pp 669–683\nLepikhin D, Lee H, Xu Y, et al. (2020) Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv preprint arXiv:200616668\nLester B, Al-Rfou R, Constant N (2021) The power of scale for parameter-efficient\nprompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing. Association for Computational Linguistics, Online\nand Punta Cana, Dominican Republic, pp 3045–3059, https://doi.org/10.18653/v1/\n2021.emnlp-main.243, URL https://aclanthology.org/2021.emnlp-main.243\nLevy I, Bogin B, Berant J (2022) Diverse demonstrations improve in-context compo-\nsitional generalization. arXiv preprint arXiv:221206800\nLewis M, Liu Y, Goyal N, et al. (2020) BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension.\nIn: Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics. Association for Computational Linguistics, Online, pp 7871–\n7880, https://doi.org/10.18653/v1/2020.acl-main.703, URL https://aclanthology.\norg/2020.acl-main.703\nLewkowycz A, Andreassen AJ, Dohan D, et al. (2022) Solving quantitative reasoning\nproblems with language models. In: Oh AH, Agarwal A, Belgrave D, et al. (eds)\nAdvances in Neural Information Processing Systems, URL https://openreview.net/\nforum?id=IFXTZERXdM7\nLi C (2023) Large multimodal models: Notes on cvpr 2023 tutorial. 2306.14895\nLi C, Xia F, Mart´ ın-Mart´ ın R, et al. (2021a) igibson 2.0: Object-centric simulation for\nrobot learning of everyday household tasks. In: 5th Annual Conference on Robot\nLearning, URL https://openreview.net/forum?id=2uGN5jNJROR\nLi C, Liu H, Li L, et al. (2022a) Elevater: A benchmark and toolkit for evaluat-\ning language-augmented visual models. Advances in Neural Information Processing\nSystems 35:9287–9301\nLi C, Zhang R, Wong J, et al. (2022b) BEHAVIOR-1k: A benchmark for embodied AI\nwith 1,000 everyday activities and realistic simulation. In: 6th Annual Conference\non Robot Learning, URL https://openreview.net/forum?id= 8DoIe8G3t\nLi C, Gan Z, Yang Z, et al. (2023a) Multimodal foundation models: From specialists\nto general-purpose assistants. 2309.10020\n122\nLi C, Wong C, Zhang S, et al. (2023b) Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:230600890\nLi G, Hammoud HAAK, Itani H, et al. (2023c) Camel: Communicative agents for”\nmind” exploration of large language model society. In: Thirty-seventh Conference\non Neural Information Processing Systems\nLi H, Guo D, Fan W, et al. (2023d) Multi-step jailbreaking privacy attacks on chatgpt.\narXiv preprint arXiv:230405197\nLi H, Sima C, Dai J, et al. (2023e) Delving into the devils of bird’s-eye-view perception:\nA review, evaluation and recipe. 2209.05324\nLi J, Seltzer ML, Wang X, et al. (2017) Large-scale domain adaptation via teacher-\nstudent learning. In: INTERSPEECH, pp 2386–2390\nLi J, Li D, Xiong C, et al. (2022c) Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. 2201.12086\nLi J, Li D, Savarese S, et al. (2023f) Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. 2301.12597\nLi J, Yu L, Ettinger A (2023g) Counterfactual reasoning: Testing language models’\nunderstanding of hypothetical scenarios. arXiv preprint arXiv:230516572\nLi J, Zhao Y, Li Y, et al. (2023h) Acecoder: Utilizing existing code to enhance code\ngeneration. 2303.17780\nLi L, Spratling M (2023) Data augmentation alone can improve adversarial training.\narXiv preprint arXiv:230109879\nLi L, Szygenda SA, Thornton MA (2005) Combining simulation and formal verification\nfor integrated circuit design validation. In: Proceedings of the 9th World Multi-\nConference on Systemics, Cybernetics and Informatics (WMSCI), pp 92–97\nLi L, Qiu J, Spratling M (2023i) Aroid: Improving adversarial robustness through\nonline instance-wise data augmentation. arXiv preprint arXiv:230607197\nLi P, Sun T, Tang Q, et al. (2023j) Codeie: Large code generation models are better\nfew-shot information extractors. In: Rogers A, Boyd-Graber JL, Okazaki N (eds)\nProceedings of the 61st Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023.\nAssociation for Computational Linguistics, pp 15339–15353, https://doi.org/10.\n18653/v1/2023.acl-long.855, URL https://doi.org/10.18653/v1/2023.acl-long.855\nLi R, Allal LB, Zi Y, et al. (2023k) Starcoder: may the source be with you! arXiv\npreprint arXiv:230506161\n123\nLi S, Chen J, Shen Y, et al. (2022d) Explanations from large language models make\nsmall reasoners better. arXiv preprint arXiv:221006726\nLi S, Puig X, Paxton C, et al. (2022e) Pre-trained language models for interactive\ndecision-making. Advances in Neural Information Processing Systems 35:31199–\n31212\nLi T, Chen L, Wang H, et al. (2023l) Graph-based topology reasoning for driving\nscenes. 2304.05277\nLi X, Qiu X (2023a) Finding supporting examples for in-context learning. arXiv\npreprint arXiv:230213539\nLi X, Qiu X (2023b) Mot: Memory-of-thought enables chatgpt to self-improve. 2305.\n05181\nLi X, Yin X, Li C, et al. (2020) Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In: ECCV, pp 121–137\nLi X, Sun Y, Cheng G (2021b) Tsqa: Tabular scenario based question answering.\nArXiv abs/2101.11429\nLi X, Liu M, Zhang H, et al. (2023m) Vision-language foundation models as effective\nrobot imitators. arXiv preprint arXiv:231101378\nLi X, Lv K, Yan H, et al. (2023n) Unified demonstration retriever for in-context\nlearning. In: Rogers A, Boyd-Graber J, Okazaki N (eds) Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics, Toronto, Canada, pp 4644–\n4668, https://doi.org/10.18653/v1/2023.acl-long.256, URL https://aclanthology.\norg/2023.acl-long.256\nLi X, Yu P, Zhou C, et al. (2023o) Self-alignment with instruction backtranslation.\narXiv preprint arXiv:230806259\nLi XL, Liang P (2021) Prefix-tuning: Optimizing continuous prompts for generation.\nIn: Proceedings of the 59th Annual Meeting of the Association for Computa-\ntional Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers). Association for Computational Linguistics,\nOnline, pp 4582–4597, https://doi.org/10.18653/v1/2021.acl-long.353, URL https:\n//aclanthology.org/2021.acl-long.353\nLi XL, Kuncoro A, Hoffmann J, et al. (2022f) A systematic investigation of common-\nsense knowledge in large language models. In: Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, pp 11838–11855\n124\nLi Y, Kong T, Chu R, et al. (2021c) Simultaneous semantic and collision learning\nfor 6-dof grasp pose estimation. in 2021 ieee. In: RSJ International Conference on\nIntelligent Robots and Systems (IROS)\nLi Y, Choi D, Chung J, et al. (2022g) Competition-level code generation with\nalphacode. Science 378(6624):1092–1097\nLi Y, Lin Z, Zhang S, et al. (2022h) On the advance of making language models better\nreasoners. arXiv preprint arXiv:220602336\nLi Y, Du Y, Zhou K, et al. (2023p) Evaluating object hallucination in large vision-\nlanguage models. arXiv preprint arXiv:230510355\nLi Y, Fan H, Hu R, et al. (2023q) Scaling language-image pre-training via masking.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp 23390–23400\nLi Y, Gao C, Song X, et al. (2023r) Druggpt: A gpt-based strategy for designing\npotential ligands targeting specific proteins. bioRxiv pp 2023–06\nLi Y, Wang C, Jia J (2023s) Llama-vid: An image is worth 2 tokens in large language\nmodels. arXiv preprint arXiv:231117043\nLi Y, Wang H, Duan Y, et al. (2023t) Clip surgery for better explainability with\nenhancement in open-vocabulary tasks. arXiv preprint arXiv:230405653\nLi Y, Zhou K, Zhao WX, et al. (2023u) Diffusion models for non-autoregressive text\ngeneration: A survey. arXiv preprint arXiv:230306574\nLian D, Zhou D, Feng J, et al. (2022) Scaling & shifting your features: A new base-\nline for efficient model tuning. Advances in Neural Information Processing Systems\n35:109–123\nLiang J, Huang W, Xia F, et al. (2022a) Code as policies: Language model programs\nfor embodied control. In: arXiv preprint arXiv:2209.07753\nLiang T, He Z, Jiao W, et al. (2023) Encouraging divergent thinking in large language\nmodels through multi-agent debate. arXiv preprint arXiv:230519118\nLiang Z, Zhang J, Wang L, et al. (2022b) Mwp-bert: Numeracy-augmented pre-\ntraining for math word problem solving. In: Findings of NAACL 2022, pp 997–1009\nLiao QV, Vaughan JW (2023) Ai transparency in the age of llms: A human-centered\nresearch roadmap. arXiv preprint arXiv:230601941\nLightman H, Kosaraju V, Burda Y, et al. (2023) Let’s verify step by step. 2305.20050\n125\nLikhosherstov V, Arnab A, Choromanski K, et al. (2021) Polyvit: Co-training vision\ntransformers on images, videos and audio. arXiv preprint arXiv:211112993\nLin B, Jiang W, Ye F, et al. (2023a) Dual-balancing for multi-task learning. Preprint\narXiv:2308.12029\nLin BY, Sun H, Dhingra B, et al. (2020a) Differentiable open-ended commonsense\nreasoning. arXiv preprint arXiv:201014439\nLin BY, Zhou W, Shen M, et al. (2020b) CommonGen: A constrained text generation\nchallenge for generative commonsense reasoning. In: Findings of the Association for\nComputational Linguistics: EMNLP 2020. Association for Computational Linguis-\ntics, Online, pp 1823–1840, https://doi.org/10.18653/v1/2020.findings-emnlp.165,\nURL https://aclanthology.org/2020.findings-emnlp.165\nLin BY, Fu Y, Yang K, et al. (2023b) Swiftsage: A generative agent with fast and\nslow thinking for complex interactive tasks. arXiv preprint arXiv:230517390\nLin W, Byrne B (2022) Retrieval augmented visual question answering with out-\nside knowledge. In: Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing. Association for Computational Linguistics, Abu\nDhabi, United Arab Emirates, pp 11238–11254, https://doi.org/10.18653/v1/2022.\nemnlp-main.772, URL https://aclanthology.org/2022.emnlp-main.772\nLin Z, Wu YF, Peri S, et al. (2020c) Improving generative imagination in object-centric\nworld models. 2010.02054\nLing W, Yogatama D, Dyer C, et al. (2017) Program induction by rationale generation:\nLearning to solve and explain algebraic word problems. In: Barzilay R, Kan MY\n(eds) Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). Association for Computational Linguis-\ntics, Vancouver, Canada, pp 158–167, https://doi.org/10.18653/v1/P17-1015, URL\nhttps://aclanthology.org/P17-1015\nLiu AT, Yang Sw, Chi PH, et al. (2020) Mockingjay: Unsupervised speech repre-\nsentation learning with deep bidirectional transformer encoders. In: ICASSP, pp\n6419–6423\nLiu C, Shen J, Xin H, et al. (2023a) Fimo: A challenge formal dataset for automated\ntheorem proving. arXiv preprint arXiv:230904295\nLiu F, Eisenschlos JM, Piccinno F, et al. (2023b) Deplot: One-shot visual language\nreasoning by plot-to-table translation. In: Findings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics, URL https://arxiv.org/abs/2212.10505\nLiu F, Piccinno F, Krichene S, et al. (2023c) Matcha: Enhancing visual language\npretraining with math reasoning and chart derendering. In: Proceedings of the 61st\n126\nAnnual Meeting of the Association for Computational Linguistics, URL https://\narxiv.org/abs/2212.09662\nLiu H, Li C, Li Y, et al. (2023d) Improved baselines with visual instruction tuning.\narXiv preprint arXiv:231003744\nLiu H, Li C, Wu Q, et al. (2023e) Visual instruction tuning. arXiv preprint\narXiv:230408485\nLiu H, Sferrazza C, Abbeel P (2023f) Chain of hindsight aligns language models with\nfeedback. arXiv preprint arXiv:230202676\nLiu J, Liu A, Lu X, et al. (2022a) Generated knowledge prompting for common-\nsense reasoning. In: Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers). Association for Compu-\ntational Linguistics, Dublin, Ireland, pp 3154–3169, https://doi.org/10.18653/v1/\n2022.acl-long.225, URL https://aclanthology.org/2022.acl-long.225\nLiu J, Shen D, Zhang Y, et al. (2022b) What makes good in-context examples for gpt-\n3? In: Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop\non Knowledge Extraction and Integration for Deep Learning Architectures, pp 100–\n114\nLiu J, Huang Z, Ma Z, et al. (2023g) Guiding mathematical reasoning via master-\ning commonsense formula knowledge. In: Proceedings of the 29th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining. Association for Computing\nMachinery, New York, NY, USA, KDD ’23, p 1477–1488, https://doi.org/10.1145/\n3580305.3599375, URL https://doi.org/10.1145/3580305.3599375\nLiu NF, Lin K, Hewitt J, et al. (2023h) Lost in the middle: How language models use\nlong contexts. 2307.03172\nLiu Q, Zhou F, Jiang Z, et al. (2023i) From zero to hero: Examining the power of\nsymbolic tasks in instruction tuning. arXiv preprint arXiv:230407995\nLiu S, Zeng Z, Ren T, et al. (2023j) Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv preprint arXiv:230305499\nLiu T, Guo Q, Hu X, et al. (2022c) RLET: A reinforcement learning based approach for\nexplainable QA with entailment trees. In: Goldberg Y, Kozareva Z, Zhang Y (eds)\nProceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, Abu Dhabi, United Arab\nEmirates, pp 7177–7189, https://doi.org/10.18653/v1/2022.emnlp-main.483, URL\nhttps://aclanthology.org/2022.emnlp-main.483\nLiu T, Guo Q, Yang Y, et al. (2023k) Plan, verify and switch: Integrated reasoning\nwith diverse x-of-thoughts. 2310.14628\n127\nLiu X, Ji K, Fu Y, et al. (2021a) P-tuning v2: Prompt tuning can be comparable to\nfine-tuning universally across scales and tasks. arXiv preprint arXiv:211007602\nLiu X, Yin D, Feng Y, et al. (2022d) Things not written in text: Exploring spatial\ncommonsense from visual signals. In: Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Association\nfor Computational Linguistics, Dublin, Ireland, pp 2365–2376, https://doi.org/10.\n18653/v1/2022.acl-long.168, URL https://aclanthology.org/2022.acl-long.168\nLiu X, Yin D, Zhang C, et al. (2023l) The magic of if: Investigating causal reasoning\nabilities in large language models of code. arXiv preprint arXiv:230519213\nLiu X, Zheng Y, Du Z, et al. (2023m) Gpt understands, too. AI Open\nLiu Y, Ott M, Goyal N, et al. (2019) Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:190711692\nLiu Y, Fabbri AR, Liu P, et al. (2023n) On learning to summarize with large language\nmodels as references. arXiv preprint arXiv:230514239\nLiu Y, Li Z, Li H, et al. (2023o) On the hidden mystery of ocr in large multimodal\nmodels. 2305.07895\nLiu Z, Lin Y, Cao Y, et al. (2021b) Swin transformer: Hierarchical vision transformer\nusing shifted windows. In: Proceedings of the IEEE/CVF international conference\non computer vision, pp 10012–10022\nLiu Z, Hu H, Lin Y, et al. (2022e) Swin transformer v2: Scaling up capacity and\nresolution. In: Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp 12009–12019\nLo K, Wang LL, Neumann M, et al. (2020) S2ORC: The semantic scholar open\nresearch corpus. In: Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics. Association for Computational Linguistics,\nOnline, pp 4969–4983, https://doi.org/10.18653/v1/2020.acl-main.447, URL https:\n//aclanthology.org/2020.acl-main.447\nLong J (2023) Large language model guided tree-of-thought. 2305.08291\nLong S, Schuster T, Pich´ e A (2022) Can large language models build causal graphs?\nIn: NeurIPS 2022 Workshop on Causality for Real-world Impact, URL https://\nopenreview.net/forum?id=LQQoJGw8JD1\nLong S, Pich´ e A, Zantedeschi V, et al. (2023) Causal discovery with language models\nas imperfect experts. arXiv preprint arXiv:230702390\nLongpre S, Hou L, Vu T, et al. (2023) The flan collection: Designing data and methods\nfor effective instruction tuning. arXiv preprint arXiv:230113688\n128\nLou R, Zhang K, Yin W (2023) Is prompt all you need? no. a comprehensive and\nbroader view of instruction learning. arXiv preprint arXiv:230310475\nLourie N, Le Bras R, Bhagavatula C, et al. (2021) Unicorn on rainbow: A universal\ncommonsense reasoning model on a new multitask benchmark. In: Proceedings of\nthe AAAI Conference on Artificial Intelligence, pp 13480–13488\nLu P, Gong R, Jiang S, et al. (2021a) Inter-gps: Interpretable geometry problem solving\nwith formal language and symbolic reasoning. arXiv preprint arXiv:210504165\nLu P, Qiu L, Chen J, et al. (2021b) Iconqa: A new benchmark for abstract diagram\nunderstanding and visual language reasoning. In: The 35th Conference on Neural\nInformation Processing Systems (NeurIPS) Track on Datasets and Benchmarks\nLu P, Mishra S, Xia T, et al. (2022a) Learn to explain: Multimodal reasoning via\nthought chains for science question answering. In: The 36th Conference on Neural\nInformation Processing Systems (NeurIPS)\nLu P, Qiu L, Chang KW, et al. (2022b) Dynamic prompt learning via policy gradient\nfor semi-structured mathematical reasoning. arXiv preprint arXiv:220914610\nLu P, Peng B, Cheng H, et al. (2023) Chameleon: Plug-and-play compositional\nreasoning with large language models. 2304.09842\nLuo G, Huang M, Zhou Y, et al. (2023a) Towards efficient visual adaption via\nstructural re-parameterization. arXiv preprint arXiv:230208106\nLuo G, Zhou Y, Ren T, et al. (2023b) Cheap and quick: Efficient vision-language\ninstruction tuning for large language models. arXiv preprint arXiv:230515023\nLuo H, Sun Q, Xu C, et al. (2023c) Wizardmath: Empowering mathematical rea-\nsoning for large language models via reinforced evol-instruct. arXiv preprint\narXiv:230809583\nLuo M, Kumbhar S, shen M, et al. (2023d) Towards logiglue: A brief survey and a\nbenchmark for analyzing logical reasoning capabilities of language models. 2310.\n00836\nLuo M, Xu X, Dai Z, et al. (2023e) Dr. icl: Demonstration-retrieved in-context learning.\narXiv preprint arXiv:230514128\nLuo Z, Xu C, Zhao P, et al. (2023f) Wizardcoder: Empowering code large language\nmodels with evol-instruct. arXiv preprint arXiv:230608568\nLYU Z, Jin Z, Mihalcea R, et al. (2022) Can large language models distinguish cause\nfrom effect? In: UAI 2022 Workshop on Causal Representation Learning, URL https:\n//openreview.net/forum?id=ucHh-ytUkOH\n129\nMa X, Yong S, Zheng Z, et al. (2023) Sqa3d: Situated question answering in 3d scenes.\nIn: International Conference on Learning Representations, URL https://openreview.\nnet/forum?id=IDJx97BC38\nMadaan A, Tandon N, Rajagopal D, et al. (2021) Think about it! improving defeasible\nreasoning by first modeling the question scenario. arXiv preprint arXiv:211012349\nMadaan A, Zhou S, Alon U, et al. (2022) Language models of code are few-shot com-\nmonsense learners. In: Goldberg Y, Kozareva Z, Zhang Y (eds) Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing. Association\nfor Computational Linguistics, Abu Dhabi, United Arab Emirates, pp 1384–1403,\nhttps://doi.org/10.18653/v1/2022.emnlp-main.90, URL https://aclanthology.org/\n2022.emnlp-main.90\nMadaan A, Tandon N, Gupta P, et al. (2023) Self-refine: Iterative refinement with\nself-feedback. 2303.17651\nMadani A, Krause B, Greene ER, et al. (2023) Large language models generate\nfunctional protein sequences across diverse families. Nature Biotechnology pp 1–8\nMagister LC, Mallinson J, Adamek J, et al. (2023) Teaching small language models\nto reason. In: Rogers A, Boyd-Graber J, Okazaki N (eds) Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Toronto, Canada, pp 1773–\n1781, https://doi.org/10.18653/v1/2023.acl-short.151, URL https://aclanthology.\norg/2023.acl-short.151\nManhaeve R, Dumanˇ ci´ c S, Kimmig A, et al. (2021) Neural probabilistic logic\nprogramming in deepproblog. Artificial Intelligence 298:103504. https://doi.\norg/https://doi.org/10.1016/j.artint.2021.103504, URL https://www.sciencedirect.\ncom/science/article/pii/S0004370221000552\nManica M, Born J, Cadow J, et al. (2023) Accelerating material design with the\ngenerative toolkit for scientific discovery. npj Computational Materials 9(1):69\nManning CD (2022) Human language understanding & reasoning. Daedalus\n151(2):127–138\nManolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, et al. (2019) Habi-\ntat: A Platform for Embodied AI Research. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV)\nMao J, Gan C, Kohli P, et al. (2019) The Neuro-Symbolic Concept Learner: Inter-\npreting Scenes, Words, and Sentences From Natural Supervision. In: International\nConference on Learning Representations, URL https://openreview.net/forum?id=\nrJgMlhRctm\n130\nMao J, Yang X, Zhang X, et al. (2022) Clevrer-humans: Describing physical and\ncausal events the human way. Advances in Neural Information Processing Systems\n35:7755–7768\nMao J, Qian Y, Zhao H, et al. (2023a) Gpt-driver: Learning to drive with gpt. 2310.\n01415\nMao J, Ye J, Qian Y, et al. (2023b) A language agent for autonomous driving. arXiv\nMarino K, Rastegari M, Farhadi A, et al. (2019) Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In: Conference on Computer Vision and\nPattern Recognition (CVPR)\nMavi V, Jangra A, Jatowt A (2022) A survey on multi-hop question answering and\ngeneration. 2204.09140\nMegill N, Wheeler DA (2019) A computer language for mathematical proofs. arXiv\nMehta H, Gupta A, Cutkosky A, et al. (2022) Long range language modeling via gated\nstate spaces. arXiv preprint arXiv:220613947\nMiao Sy, Liang CC, Su KY (2020) A diverse corpus for evaluating and developing\nenglish math word problem solvers. In: Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pp 975–984\nMiku la M, Antoniak S, Tworkowski S, et al. (2023) Magnushammer: A transformer-\nbased approach to premise selection. arXiv preprint arXiv:230304488\nMiller A, Fisch A, Dodge J, et al. (2016) Key-value memory networks for directly\nreading documents. In: Proceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing. Association for Computational Linguistics,\nAustin, Texas, pp 1400–1409, https://doi.org/10.18653/v1/D16-1147, URL https:\n//aclanthology.org/D16-1147\nMin S, Lewis M, Zettlemoyer L, et al. (2022) MetaICL: Learning to learn in context.\nIn: NAACL-HLT\nMishra S, Khashabi D, Baral C, et al. (2022) Cross-task generalization via natural\nlanguage crowdsourcing instructions. In: ACL\nMisra K, Rayz JT, Ettinger A (2022) A property induction framework for neural\nlanguage models. arXiv preprint arXiv:220506910\nMitra A, Del Corro L, Mahajan S, et al. (2023) Orca 2: Teaching small language\nmodels how to reason. arXiv preprint arXiv:231111045\nMoghaddam SR, Honey CJ (2023) Boosting theory-of-mind performance in large\nlanguage models via prompting. 2304.11490\n131\nMohamed A, Lee Hy, Borgholt L, et al. (2022) Self-supervised speech representation\nlearning: A review. IEEE Journal of Selected Topics in Signal Processing\nMooij JM, Peters J, Janzing D, et al. (2016) Distinguishing cause from effect using\nobservational data: methods and benchmarks. The Journal of Machine Learning\nResearch 17(1):1103–1204\nMorris BJ, Croker S, Masnick AM, et al. (2012) The emergence of scientific reasoning.\nIn: Kloos H, Morris BJ, Amaral JL (eds) Current Topics in Children’s Learning\nand Cognition. IntechOpen, Rijeka, chap 4, https://doi.org/10.5772/53885, URL\nhttps://doi.org/10.5772/53885\nde Moura L, Kong S, Avigad J, et al. (2015) The lean theorem prover (system\ndescription). In: Automated Deduction-CADE-25: 25th International Conference\non Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25,\nSpringer, pp 378–388\nMoura Ld, Ullrich S (2021) The lean 4 theorem prover and programming language.\nIn: Automated Deduction–CADE 28: 28th International Conference on Automated\nDeduction, Virtual Event, July 12–15, 2021, Proceedings 28, Springer, pp 625–635\nMu Y, Zhang Q, Hu M, et al. (2023) Embodiedgpt: Vision-language pre-training via\nembodied chain of thought. arXiv preprint arXiv:230515021\nMuennighoff N, Wang T, Sutawika L, et al. (2022) Crosslingual generalization through\nmultitask finetuning. arXiv preprint arXiv:221101786\nMukherjee S, Mitra A, Jawahar G, et al. (2023) Orca: Progressive learning from\ncomplex explanation traces of gpt-4. arXiv preprint arXiv:230602707\nM¨ undler N, He J, Jenko S, et al. (2023) Self-contradictory hallucinations of large\nlanguage models: Evaluation, detection and mitigation. 2305.15852\nNascimento N, Alencar P, Cowan D (2023) Self-adaptive large language model (llm)-\nbased multiagent systems. 2307.06187\nNelson B, Barreno M, Chi FJ, et al. (2008) Exploiting machine learning to subvert\nyour spam filter. LEET 8(1-9):16–17\nNguyen E, Poli M, Faizi M, et al. (2023) Hyenadna: Long-range genomic sequence\nmodeling at single nucleotide resolution. arXiv preprint arXiv:230615794\nNguyen T, Ilharco G, Wortsman M, et al. (2022) Quality not quantity: On the interac-\ntion between dataset design and robustness of clip. Advances in Neural Information\nProcessing Systems 35:21455–21469\nNijkamp E, Pang B, Hayashi H, et al. (2022a) Codegen: An open large language model\nfor code with multi-turn program synthesis. arXiv preprint arXiv:220313474\n132\nNijkamp E, Ruffolo JA, Weinstein EN, et al. (2022b) Progen2: exploring the\nboundaries of protein language models. Cell Systems\nNijkamp E, Hayashi H, Xiong C, et al. (2023) Codegen2: Lessons for training llms on\nprogramming and natural languages. arXiv preprint arXiv:230502309\nNing X, Lin Z, Zhou Z, et al. (2023) Skeleton-of-thought: Large language models can\ndo parallel decoding. 2307.15337\nNunes T (2012) Logical Reasoning and Learning, Springer US, Boston, MA, pp\n2066–2069. https://doi.org/10.1007/978-1-4419-1428-6 790, URL https://doi.org/\n10.1007/978-1-4419-1428-6 790\nOberlander J, Cox R, Stenning K (1996) Proof styles in multimodal reasoning. In:\nLogic, Language and Computation. CSLI Publications, p 403–414\nOdouard VV, Mitchell M (2022) Evaluating understanding on conceptual abstraction\nbenchmarks. arXiv preprint arXiv:220614187\nOlausson TX, Inala JP, Wang C, et al. (2023) Demystifying gpt self-repair for code\ngeneration. 2306.09896\nOord Avd, Li Y, Vinyals O (2018) Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:180703748\nOpenAI (2023a) Gpt-4 technical report. 2303.08774\nOpenAI (2023b) Gpt-4v(ision) system card. arXiv URL https://cdn.openai.com/\npapers/GPTV System Card.pdf\nOrdonez V, Kulkarni G, Berg T (2011) Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems 24\nOuyang L, Wu J, Jiang X, et al. (2022) Training language models to follow instruc-\ntions with human feedback. Advances in Neural Information Processing Systems\n35:27730–27744\nPadalkar A, Pooley A, Jain A, et al. (2023) Open x-embodiment: Robotic learning\ndatasets and rt-x models. arXiv preprint arXiv:231008864\nPan B, Sun J, Leung HYT, et al. (2020) Cross-view semantic segmentation for sensing\nsurroundings. IEEE Robotics and Automation Letters 5(3):4867–4873. https://doi.\norg/10.1109/LRA.2020.3004325\nPan L, Albalak A, Wang X, et al. (2023a) Logic-lm: Empowering large language models\nwith symbolic solvers for faithful logical reasoning. arXiv preprint arXiv:230512295\n133\nPan Y, Pan L, Chen W, et al. (2023b) On the risk of misinformation pollution with\nlarge language models. 2305.13661\nPapineni K, Roukos S, Ward T, et al. (2002) Bleu: a method for automatic evaluation\nof machine translation. In: Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp 311–318\nParanjape B, Lundberg S, Singh S, et al. (2023) Art: Automatic multi-step reasoning\nand tool-use for large language models. 2303.09014\nPaster K, Santos MD, Azerbayev Z, et al. (2023) Openwebmath: An open dataset of\nhigh-quality mathematical web text. 2310.06786\nPasupat P, Liang P (2015) Compositional semantic parsing on semi-structured tables.\nIn: Proceedings of the 53rd Annual Meeting of the Association for Computa-\ntional Linguistics and the 7th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers). Association for Computational Linguistics,\nBeijing, China, pp 1470–1480, https://doi.org/10.3115/v1/P15-1142, URL https:\n//aclanthology.org/P15-1142\nPatel A, Bhattamishra S, Goyal N (2021) Are NLP models really able to solve simple\nmath word problems? In: Proceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. Association for Computational Linguistics, Online, pp 2080–2094,\nhttps://doi.org/10.18653/v1/2021.naacl-main.168, URL https://aclanthology.org/\n2021.naacl-main.168\nPaul D, Ismayilzada M, Peyrard M, et al. (2023) Refiner: Reasoning feedback on\nintermediate representations. 2304.01904\nPaulson LC (1994) Isabelle: A generic theorem prover. Springer\nPaulson LC (2010) Three years of experience with sledgehammer, a practical link\nbetween automatic and interactive theorem provers. In: Schmidt RA, Schulz S,\nKonev B (eds) Proceedings of the 2nd Workshop on Practical Aspects of Automated\nReasoning, PAAR-2010, Edinburgh, Scotland, UK, July 14, 2010, EPiC Series in\nComputing, vol 9. EasyChair, pp 1–10, https://doi.org/10.29007/tnfd, URL https:\n//doi.org/10.29007/tnfd\nPeng B, Alcaide E, Anthony Q, et al. (2023a) Rwkv: Reinventing rnns for the\ntransformer era. arXiv preprint arXiv:230513048\nPeng B, Galley M, He P, et al. (2023b) Check your facts and try again: Improving large\nlanguage models with external knowledge and automated feedback. arXiv preprint\narXiv:230212813\n134\nPeng B, Li C, He P, et al. (2023c) Instruction tuning with gpt-4. arXiv preprint\narXiv:230403277\nPeng Z, Wang W, Dong L, et al. (2023d) Kosmos-2: Grounding multimodal large\nlanguage models to the world. 2306.14824\nPerez F, Ribeiro I (2022) Ignore previous prompt: Attack techniques for language\nmodels. arXiv preprint arXiv:221109527\nPeters J, Janzing D, Sch¨ olkopf B (2017) Elements of causal inference: foundations and\nlearning algorithms. The MIT Press\nPfeiffer J, Vuli´ c I, Gurevych I, et al. (2020) Mad-x: An adapter-based framework for\nmulti-task cross-lingual transfer. arXiv preprint arXiv:200500052\nPham H, Dai Z, Ghiasi G, et al. (2023) Combined scaling for zero-shot transfer\nlearning. Neurocomputing 555:126658\nPi R, Gao J, Diao S, et al. (2023) Detgpt: Detect what you need via reasoning.\n2305.14167\nPilault J, Li R, Subramanian S, et al. (2020) On extractive and abstractive neural\ndocument summarization with transformer language models. In: Webber B, Cohn\nT, He Y, et al. (eds) Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP). Association for Computational Lin-\nguistics, Online, pp 9308–9319, https://doi.org/10.18653/v1/2020.emnlp-main.748,\nURL https://aclanthology.org/2020.emnlp-main.748\nPlatzer M, Puschner P (2021) Vicuna: A Timing-Predictable RISC-V Vector Copro-\ncessor for Scalable Parallel Computation. In: Brandenburg BB (ed) 33rd Euromicro\nConference on Real-Time Systems (ECRTS 2021), Leibniz International Pro-\nceedings in Informatics (LIPIcs), vol 196. Schloss Dagstuhl – Leibniz-Zentrum\nf¨ ur Informatik, Dagstuhl, Germany, pp 1:1–1:18, https://doi.org/10.4230/LIPIcs.\nECRTS.2021.1, URL https://drops.dagstuhl.de/opus/volltexte/2021/13932\nPoli M, Massaroli S, Nguyen E, et al. (2023) Hyena hierarchy: Towards larger\nconvolutional language models. arXiv preprint arXiv:230210866\nPollock JL (1987) Defeasible reasoning. Cognitive science 11(4):481–518\nPollock JL (1991) A theory of defeasible reasoning. International Journal of Intelligent\nSystems 6(1):33–54\nPollock JL (2009) A recursive semantics for defeasible reasoning. Argumentation in\nartificial intelligence pp 173–197\nPolu S, Sutskever I (2020) Generative language modeling for automated theorem\nproving. arXiv preprint arXiv:200903393\n135\nPolu S, Han JM, Zheng K, et al. (2023) Formal mathematics statement curriculum\nlearning. In: The Eleventh International Conference on Learning Representations\nPratap V, Xu Q, Sriram A, et al. (2020) MLS: A large-scale multilingual dataset for\nspeech research. In: INTERSPEECH, pp 2757–2761\nPress O, Zhang M, Min S, et al. (2023) Measuring and narrowing the compositionality\ngap in language models. 2210.03350\nPryor C, Dickens C, Augustine E, et al. (2023) Neupsl: Neural probabilistic soft logic.\n2205.14268\nPuig X, Ra K, Boben M, et al. (2018) Virtualhome: Simulating household activities\nvia programs. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp 8494–8502\nQian C, Han C, Fung YR, et al. (2023a) Creator: Disentangling abstract and concrete\nreasonings of large language models through tool creation. 2305.14318\nQian T, Chen J, Zhuo L, et al. (2023b) Nuscenes-qa: A multi-modal visual\nquestion answering benchmark for autonomous driving scenario. arXiv preprint\narXiv:230514836\nQiao S, Ou Y, Zhang N, et al. (2022) Reasoning with language model prompting: A\nsurvey. arXiv preprint arXiv:221209597\nQiao S, Gui H, Chen H, et al. (2023) Making language models better tool learners\nwith execution feedback. 2305.13068\nQin J, Lin L, Liang X, et al. (2020) Semantically-aligned universal tree-structured\nsolver for math word problems. ArXiv abs/2010.06823\nQin Y, Hu S, Lin Y, et al. (2023) Tool learning with foundation models. 2304.08354\nQiu J, Chen L, Gu X, et al. (2022) Egocentric human trajectory forecasting with a\nwearable camera and multi-modal fusion. IEEE Robotics and Automation Letters\n7(4):8799–8806\nQiu J, Li L, Sun J, et al. (2023a) Large ai models in health informatics: Applications,\nchallenges, and the future. IEEE Journal of Biomedical and Health Informatics\nQiu J, Wu J, Wei H, et al. (2023b) Visionfm: a multi-modal multi-task vision\nfoundation model for generalist ophthalmic artificial intelligence. arXiv preprint\narXiv:231004992\nRadford A, Narasimhan K, Salimans T, et al. (2018) Improving language understand-\ning by generative pre-training. arXiv\n136\nRadford A, Wu J, Child R, et al. (2019) Language models are unsupervised multitask\nlearners. OpenAI blog 1(8):9\nRadford A, Kim JW, Hallacy C, et al. (2021) Learning transferable visual models\nfrom natural language supervision. In: International conference on machine learning,\nPMLR, pp 8748–8763\nRae JW, Borgeaud S, Cai T, et al. (2021) Scaling language models: Methods, analysis\n& insights from training gopher. arXiv preprint arXiv:211211446\nRafailov R, Sharma A, Mitchell E, et al. (2023) Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint arXiv:230518290\nRaffel C, Shazeer N, Roberts A, et al. (2019) Exploring the limits of transfer learning\nwith a unified text-to-text transformer. arXiv e-prints arXiv:1910.10683\nRaheja V, Kumar D, Koo R, et al. (2023) Coedit: Text editing by task-specific\ninstruction tuning. arXiv arXiv:2305.09857 [cs.CL]\nRajani NF, McCann B, Xiong C, et al. (2019) Explain yourself! leveraging language\nmodels for commonsense reasoning. In: Proceedings of the 2019 Conference of the\nAssociation for Computational Linguistics (ACL2019), URL https://arxiv.org/abs/\n1906.02361\nRajani NF, Zhang R, Tan YC, et al. (2020) ESPRIT: Explaining solutions to\nphysical reasoning tasks. In: Proceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics. Association for Computational Linguistics,\nOnline, pp 7906–7917, https://doi.org/10.18653/v1/2020.acl-main.706, URL https:\n//aclanthology.org/2020.acl-main.706\nRajpurkar P, Zhang J, Lopyrev K, et al. (2016) SQuAD: 100,000+ questions for\nmachine comprehension of text. In: Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing. Association for Computational Linguis-\ntics, Austin, Texas, pp 2383–2392, https://doi.org/10.18653/v1/D16-1264, URL\nhttps://aclanthology.org/D16-1264\nRamesh A, Pavlov M, Goh G, et al. (2021) Zero-shot text-to-image generation. In:\nInternational Conference on Machine Learning, PMLR, pp 8821–8831\nRamesh A, Dhariwal P, Nichol A, et al. (2022) Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:220406125 1(2):3\nRaven JC, Court J (1938) Raven’s progressive matrices. Western Psychological\nServices Los Angeles, CA\nRawte V, Sheth A, Das A (2023) A survey of hallucination in large foundation models.\n2309.05922\n137\nRedbooks I (2004) Practical Guide to the IBM Autonomic Computing Toolkit. IBM\nReed S, Zolna K, Parisotto E, et al. (2022) A generalist agent. arXiv preprint\narXiv:220506175\nReimers N, Gurevych I (2019) Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:190810084\nReiter R (1975) Formal reasoning and language understanding system. In: Theoretical\nIssues in Natural Language Processing\nRen S, Zhu KQ (2023) Low-rank prune-and-factorize for language model compression.\n2306.14152\nRen X, Zhou P, Meng X, et al. (2023) Pangu-\\sigma: Towards trillion parameter lan-\nguage model with sparse heterogeneous computing. arXiv preprint arXiv:230310845\nRidnik T, Ben-Baruch E, Noy A, et al. (2021) Imagenet-21k pretraining for the masses.\nIn: Thirty-fifth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 1)\nRoller S, Dinan E, Goyal N, et al. (2020) Recipes for building an open-domain chatbot.\narXiv preprint arXiv:200413637\nRombach R, Blattmann A, Lorenz D, et al. (2022) High-resolution image synthe-\nsis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp 10684–10695\nRoziere B, Gehring J, Gloeckle F, et al. (2023) Code llama: Open foundation models\nfor code. arXiv preprint arXiv:230812950\nRubin O, Herzig J, Berant J (2022) Learning to retrieve prompts for in-context learn-\ning. In: Proceedings of the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pp\n2655–2671\nRudinger R, Shwartz V, Hwang JD, et al. (2020) Thinking like a skeptic: Defeasible\ninference in natural language. In: Findings of the Association for Computational\nLinguistics: EMNLP 2020. Association for Computational Linguistics, Online,\npp 4661–4675, https://doi.org/10.18653/v1/2020.findings-emnlp.418, URL https:\n//aclanthology.org/2020.findings-emnlp.418\nSachan M, Xing E (2017) Learning to solve geometry problems from natural language\ndemonstrations in textbooks. In: Proceedings of the 6th Joint Conference on Lexical\nand Computational Semantics (*SEM 2017). Association for Computational Lin-\nguistics, Vancouver, Canada, pp 251–261, https://doi.org/10.18653/v1/S17-1029,\nURL https://aclanthology.org/S17-1029\n138\nSachan M, Dubey K, Xing E (2017) From textbooks to knowledge: A case study in\nharvesting axiomatic knowledge from textbooks to solve geometry problems. In:\nPalmer M, Hwa R, Riedel S (eds) Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing. Association for Computational Linguis-\ntics, Copenhagen, Denmark, pp 773–784, https://doi.org/10.18653/v1/D17-1081,\nURL https://aclanthology.org/D17-1081\nSaharia C, Chan W, Saxena S, et al. (2022) Photorealistic text-to-image diffusion mod-\nels with deep language understanding. Advances in Neural Information Processing\nSystems 35:36479–36494\nSakaguchi K, Bras RL, Bhagavatula C, et al. (2021) Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM 64(9):99–106\nSalmon W, Salmon M, Kitcher P (1989) Scientific explanation. Minneapolis\nSanh V, Webson A, Raffel C, et al. (2021) Multitask prompted training enables zero-\nshot task generalization. 2110.08207\nSap M, Rashkin H, Chen D, et al. (2019) Social IQa: Commonsense reasoning about\nsocial interactions. In: Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). Association for Computational Linguis-\ntics, Hong Kong, China, pp 4463–4473, https://doi.org/10.18653/v1/D19-1454,\nURL https://aclanthology.org/D19-1454\nSaparov A, He H (2023) Language models are greedy reasoners: A systematic formal\nanalysis of chain-of-thought. In: The Eleventh International Conference on Learning\nRepresentations, URL https://openreview.net/forum?id=qFVVBzXxR2V\nSavage N (2023) Drug discovery companies are customizing chatgpt: here’s how.\nNature Biotechnology\nSawada T, Paleka D, Havrilla A, et al. (2023) Arb: Advanced reasoning benchmark\nfor large language models. 2307.13692\nScao TL, Fan A, Akiki C, et al. (2022) Bloom: A 176b-parameter open-access\nmultilingual language model. arXiv preprint arXiv:221105100\nSchick T, Dwivedi-Yu J, Jiang Z, et al. (2022) Peer: A collaborative language model.\narXiv preprint arXiv:220811663\nSchick T, Dwivedi-Yu J, Dess` ı R, et al. (2023) Toolformer: Language models can teach\nthemselves to use tools. 2302.04761\nSchmidhuber J (2015) Deep learning in neural networks: An overview. Neural networks\n61:85–117\n139\nSchuhmann C, Vencu R, Beaumont R, et al. (2021) Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:211102114\nSchuhmann C, Beaumont R, Vencu R, et al. (2022) Laion-5b: An open large-\nscale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems 35:25278–25294\nSchulman J, Wolski F, Dhariwal P, et al. (2017) Proximal policy optimization\nalgorithms. arXiv preprint arXiv:170706347\nSch¨ olkopf B, Locatello F, Bauer S, et al. (2021) Toward causal representation learning.\nProceedings of the IEEE 109(5):612–634. https://doi.org/10.1109/JPROC.2021.\n3058954\nSeff A, Cera B, Chen D, et al. (2023) MotionLM: Multi-Agent Motion Forecasting as\nLanguage Modeling. In: ICCV\nSeo M, Hajishirzi H, Farhadi A, et al. (2015) Solving geometry problems: Combin-\ning text and diagram interpretation. In: M` arquez L, Callison-Burch C, Su J (eds)\nProceedings of the 2015 Conference on Empirical Methods in Natural Language Pro-\ncessing. Association for Computational Linguistics, Lisbon, Portugal, pp 1466–1476,\nhttps://doi.org/10.18653/v1/D15-1171, URL https://aclanthology.org/D15-1171\nSha H, Mu Y, Jiang Y, et al. (2023) Languagempc: Large language models as decision\nmakers for autonomous driving. arXiv preprint arXiv:231003026\nShah D, Xu P, Lu Y, et al. (2021) Value function spaces: Skill-centric state abstractions\nfor long-horizon reasoning. arXiv preprint arXiv:211103189\nShao Z, Yu Z, Wang M, et al. (2023) Prompting large language models with answer\nheuristics for knowledge-based visual question answering. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 14974–\n14983\nSharma P, Ding N, Goodman S, et al. (2018) Conceptual captions: A cleaned, hyper-\nnymed, image alt-text dataset for automatic image captioning. In: Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp 2556–2565\nSharma P, Torralba A, Andreas J (2021) Skill induction and planning with latent\nlanguage. arXiv preprint arXiv:211001517\nShazeer N, Mirhoseini A, Maziarz K, et al. (2017) Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:170106538\nShen J, Yin Y, Li L, et al. (2021a) Generate & rank: A multi-task framework for\nmath word problems. In: Moens MF, Huang X, Specia L, et al. (eds) Findings of\n140\nthe Association for Computational Linguistics: EMNLP 2021. Association for Com-\nputational Linguistics, Punta Cana, Dominican Republic, pp 2269–2279, https://\ndoi.org/10.18653/v1/2021.findings-emnlp.195, URL https://aclanthology.org/2021.\nfindings-emnlp.195\nShen L, Ji S, Zhang X, et al. (2021b) Backdoor pre-trained models can transfer to all.\narXiv preprint arXiv:211100197\nShen S, Li C, Hu X, et al. (2022) K-lite: Learning transferable visual models with\nexternal knowledge. Advances in Neural Information Processing Systems 35:15558–\n15573\nShen Y, Song K, Tan X, et al. (2023) Hugginggpt: Solving ai tasks with chatgpt and\nits friends in hugging face. 2303.17580\nShi F, Suzgun M, Freitag M, et al. (2023) Language models are multilingual\nchain-of-thought reasoners. In: The Eleventh International Conference on Learning\nRepresentations, URL https://openreview.net/forum?id=fR3wGCk-IXp\nShi S, Wang Y, Lin CY, et al. (2015) Automatically solving number word problems by\nsemantic parsing and reasoning. In: Conference on Empirical Methods in Natural\nLanguage Processing\nShi W, Shea R, Chen S, et al. (2022) Just fine-tune twice: Selective differential privacy\nfor large language models. arXiv preprint arXiv:220407667\nShin T, Razeghi Y, Logan IV RL, et al. (2020) Autoprompt: Eliciting knowl-\nedge from language models with automatically generated prompts. arXiv preprint\narXiv:201015980\nShinn N, Cassano F, Labash B, et al. (2023) Reflexion: Language agents with verbal\nreinforcement learning. 2303.11366\nShreya G, Khapra MM (2022) A survey in adversarial defences and robustness in nlp.\narXiv preprint arXiv:220306414\nShridhar K, Stolfo A, Sachan M (2023) Distilling reasoning capabilities into smaller\nlanguage models. In: Findings of the Association for Computational Linguis-\ntics: ACL 2023. Association for Computational Linguistics, Toronto, Canada,\npp 7059–7073, https://doi.org/10.18653/v1/2023.findings-acl.441, URL https://\naclanthology.org/2023.findings-acl.441\nSilver D, Huang A, Maddison CJ, et al. (2016) Mastering the game of go with deep\nneural networks and tree search. nature 529(7587):484–489\nSilver D, Hubert T, Schrittwieser J, et al. (2018) A general reinforcement learning algo-\nrithm that masters chess, shogi, and go through self-play. Science 362(6419):1140–\n1144\n141\nSingh I, Blukis V, Mousavian A, et al. (2023) Progprompt: Generating situated robot\ntask plans using large language models. In: 2023 IEEE International Conference on\nRobotics and Automation (ICRA), IEEE, pp 11523–11530\nSingh M, Gustafson L, Adcock A, et al. (2022) Revisiting weakly supervised pre-\ntraining of visual perception models. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp 804–814\nSinghal K, Tu T, Gottweis J, et al. (2023) Towards expert-level medical question\nanswering with large language models. arXiv preprint arXiv:230509617\nSinha K, Sodhani S, Dong J, et al. (2019) Clutrr: A diagnostic benchmark for inductive\nreasoning from text. arXiv preprint arXiv:190806177\nSoldaini L, Lo K (2023) peS2o (Pretraining Efficiently on S2ORC) Dataset. Tech. rep.,\nAllen Institute for AI, oDC-By, https://github.com/allenai/pes2o\nSong CH, Wu J, Washington C, et al. (2023a) Llm-planner: Few-shot grounded\nplanning for embodied agents with large language models. 2212.04088\nSong F, Yu B, Li M, et al. (2023b) Preference ranking optimization for human\nalignment. arXiv preprint arXiv:230617492\nSong X, Shi Y, Chen X, et al. (2018) Explore multi-step reasoning in video ques-\ntion answering. In: Proceedings of the 26th ACM international conference on\nMultimedia, pp 239–247\nSowa JF (2003) Laws, facts, and contexts: Foundations for multimodal reasoning. In:\nKnowledge Contributors. Springer, p 145–184\nSpeer R, Chin J, Havasi C (2017) Conceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In: Proceedings of the Thirty-First AAAI Conference on Artificial\nIntelligence. AAAI Press, AAAI’17, p 4444–4451\nSrinivasan K, Raman K, Chen J, et al. (2021) Wit: Wikipedia-based image text\ndataset for multimodal multilingual machine learning. In: Proceedings of the 44th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, pp 2443–2449\nSrivastava A, Rastogi A, Rao A, et al. (2023) Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models. Transactions on Machine\nLearning Research URL https://openreview.net/forum?id=uyTL5Bvosj\nStolfo A, Jin Z, Shridhar K, et al. (2022) A causal framework to quantify the robustness\nof mathematical reasoning with language models. arXiv preprint arXiv:221012023\nStrehl A, Ghosh J (2002) Cluster ensembles—a knowledge reuse framework for\ncombining multiple partitions. Journal of machine learning research 3(Dec):583–617\n142\nSubramanian S, Harrington P, Keutzer K, et al. (2023) Towards foundation models for\nscientific machine learning: Characterizing scaling and transfer behavior. 2306.00258\nSun A, Ma P, Yuan Y, et al. (2023a) Explain any concept: Segment anything meets\nconcept-based explanation. 2305.10289\nSun C, Shrivastava A, Singh S, et al. (2017) Revisiting unreasonable effectiveness of\ndata in deep learning era. In: Proceedings of the IEEE international conference on\ncomputer vision, pp 843–852\nSun J, Sun H, Han T, et al. (2021) Neuro-symbolic program search for autonomous\ndriving decision module design. In: Kober J, Ramos F, Tomlin C (eds) Proceed-\nings of the 2020 Conference on Robot Learning, Proceedings of Machine Learning\nResearch, vol 155. PMLR, pp 21–30, URL https://proceedings.mlr.press/v155/\nsun21a.html\nSun J, Huang DA, Lu B, et al. (2022a) Plate: Visually-grounded planning with trans-\nformers in procedural tasks. IEEE Robotics and Automation Letters 7(2):4924–4930\nSun J, Kousik S, Fridovich-Keil D, et al. (2022b) Self-supervised traffic advisors:\nDistributed, multi-view traffic prediction for smart cities. In: 2022 IEEE 25th Inter-\nnational Conference on Intelligent Transportation Systems (ITSC), pp 917–922,\nhttps://doi.org/10.1109/ITSC55140.2022.9922340\nSun J, Jiang Y, Qiu J, et al. (2023b) Conformal prediction for uncertainty-aware\nplanning with diffusion dynamics model. In: Thirty-seventh Conference on Neural\nInformation Processing Systems\nSun J, Kousik S, Fridovich-Keil D, et al. (2023c) Connected autonomous vehicle motion\nplanning with video predictions from smart, self-supervised infrastructure. arXiv\npreprint arXiv:230907504\nSun M, Liu Z, Bair A, et al. (2023d) A simple and effective pruning approach for large\nlanguage models. 2306.11695\nSun Q, Yin Z, Li X, et al. (2023e) Corex: Pushing the boundaries of complex reasoning\nthrough multi-model collaboration. 2310.00280\nSun T, Shao Y, Qiu X, et al. (2020) CoLAKE: Contextualized language and knowl-\nedge embedding. In: Scott D, Bel N, Zong C (eds) Proceedings of the 28th\nInternational Conference on Computational Linguistics. International Committee\non Computational Linguistics, Barcelona, Spain (Online), pp 3660–3670, https:\n//doi.org/10.18653/v1/2020.coling-main.327, URL https://aclanthology.org/2020.\ncoling-main.327\nSun Y, Dong L, Huang S, et al. (2023f) Retentive network: A successor to transformer\nfor large language models. arXiv preprint arXiv:230708621\n143\nSuvorov R, Logacheva E, Mashikhin A, et al. (2021) Resolution-robust large mask\ninpainting with fourier convolutions. arXiv preprint arXiv:210907161\nSvyatkovskiy A, Deng SK, Fu S, et al. (2020) Intellicode compose: Code generation\nusing transformer. In: Proceedings of the 28th ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on the Foundations of Software\nEngineering, pp 1433–1443\nSzot A, Clegg A, Undersander E, et al. (2021) Habitat 2.0: Training home assistants\nto rearrange their habitat. In: Advances in Neural Information Processing Systems\n(NeurIPS)\nTafjord O, Dalvi B, Clark P (2021) ProofWriter: Generating implications, proofs,\nand abductive statements over natural language. In: Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021. Association for Computational\nLinguistics, Online, pp 3621–3634, https://doi.org/10.18653/v1/2021.findings-acl.\n317, URL https://aclanthology.org/2021.findings-acl.317\nTafjord O, Dalvi Mishra B, Clark P (2022) Entailer: Answering questions with faith-\nful and truthful chains of reasoning. In: Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates, pp 2078–2093, https://doi.org/10.\n18653/v1/2022.emnlp-main.134, URL https://aclanthology.org/2022.emnlp-main.\n134\nTalmor A, Berant J (2018) The web as a knowledge-base for answering com-\nplex questions. In: Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers). Association for Computational Linguistics,\nNew Orleans, Louisiana, pp 641–651, https://doi.org/10.18653/v1/N18-1059, URL\nhttps://aclanthology.org/N18-1059\nTalmor A, Herzig J, Lourie N, et al. (2019) CommonsenseQA: A question answering\nchallenge targeting commonsense knowledge. In: Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers). Association\nfor Computational Linguistics, Minneapolis, Minnesota, pp 4149–4158, https://doi.\norg/10.18653/v1/N19-1421, URL https://aclanthology.org/N19-1421\nTalmor A, Yoran O, Catav A, et al. (2021) Multimodal {qa}: complex question\nanswering over text, tables and images. In: International Conference on Learning\nRepresentations, URL https://openreview.net/forum?id=ee6W5UgQLa\nTan S, Ivanovic B, Weng X, et al. (2023) Language conditioned traffic generation.\nCoRL\n144\nTandon N, Madaan A, Clark P, et al. (2021) Interscript: A dataset for interactive\nlearning of scripts through error feedback. arXiv preprint arXiv:211207867\nTao C, Hou L, Zhang W, et al. (2022) Compression of generative pre-trained language\nmodels via quantization. 2203.10705\nTao M, Bao BK, Tang H, et al. (2023) Galip: Generative adversarial clips for text-to-\nimage synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp 14214–14223\nTaori R, Gulrajani I, Zhang T, et al. (2023) Alpaca: A strong, replicable instruction-\nfollowing model. Stanford Center for Research on Foundation Models https://crfm\nstanford edu/2023/03/13/alpaca html 3(6):7\nTaylor R, Kardas M, Cucurull G, et al. (2022) Galactica: A large language model for\nscience. arXiv preprint arXiv:221109085\nTeam G (2022) GT4SD (Generative Toolkit for Scientific Discovery). URL https:\n//github.com/GT4SD/gt4sd-core\nTeam G, Anil R, Borgeaud S, et al. (2023) Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:231211805\nTeig N, Scherer R (2016) Bringing formal and informal reasoning together—a new era\nof assessment? Frontiers in psychology 7:1097\nThomee B, Shamma DA, Friedland G, et al. (2016) Yfcc100m: The new data in\nmultimedia research. Communications of the ACM 59(2):64–73\nThoppilan R, De Freitas D, Hall J, et al. (2022) Lamda: Language models for dialog\napplications. arXiv preprint arXiv:220108239\nTian J, Li Y, Chen W, et al. (2022) Weakly supervised neural symbolic learning for\ncognitive tasks. In: Proceedings of the AAAI Conference on Artificial Intelligence,\npp 5888–5896\nTomasic A, Romero OJ, Zimmerman J, et al. (2021) Propositional reasoning via neural\ntransformer language models. arXiv\nTong Z, Song Y, Wang J, et al. (2022) Videomae: Masked autoencoders are data-\nefficient learners for self-supervised video pre-training. 2203.12602\nTouvron H, Lavril T, Izacard G, et al. (2023a) Llama: Open and efficient foundation\nlanguage models. 2302.13971\nTouvron H, Martin L, Stone K, et al. (2023b) Llama 2: Open foundation and fine-tuned\nchat models. 2307.09288\n145\nTsai HS, Chang HJ, Huang WC, et al. (2022) SUPERB-SG: Enhanced Speech process-\ning Universal PERformance Benchmark for Semantic and Generative Capabilities.\nIn: Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics, pp 8479–8492\nTsimpoukelli M, Menick J, Cabi S, et al. (2021) Multimodal few-shot learning with\nfrozen language models. In: Beygelzimer A, Dauphin Y, Liang P, et al. (eds)\nAdvances in Neural Information Processing Systems, URL https://openreview.net/\nforum?id=WtmMyno9Tq2\nTu M, Huang K, Wang G, et al. (2020) Select, answer and explain: Interpretable multi-\nhop reading comprehension over multiple documents. In: Proceedings of the AAAI\nconference on artificial intelligence, pp 9073–9080\nTu R, Zhang K, Bertilson B, et al. (2019) Neuropathic pain diagnosis simulator for\ncausal discovery algorithm evaluation. Advances in Neural Information Processing\nSystems 32\nTu R, Ma C, Zhang C (2023a) Causal-discovery performance of chatgpt in the context\nof neuropathic pain diagnosis. 2301.13819\nTu T, Azizi S, Driess D, et al. (2023b) Towards generalist biomedical ai. arXiv preprint\narXiv:230714334\nTung HY, Ding M, Chen Z, et al. (2023) Physion++: Evaluating physical scene\nunderstanding that requires online inference of different physical properties. arXiv\npreprint arXiv:230615668\nTunstall L, Von Werra L, Wolf T (2022) Natural language processing with transform-\ners. ” O’Reilly Media, Inc.”\nUpadhyay S, Chang MW (2015) Draw: A challenging and diverse algebra word problem\nset\nUpadhyay S, Chang MW (2017) Annotating derivations: A new evaluation strategy\nand dataset for algebra word problems. In: Proceedings of the 15th Conference of\nthe European Chapter of the Association for Computational Linguistics: Volume\n1, Long Papers. Association for Computational Linguistics, Valencia, Spain, pp\n494–504, URL https://aclanthology.org/E17-1047\nValipour M, Rezagholizadeh M, Kobyzev I, et al. (2022) Dylora: Parameter efficient\ntuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv\npreprint arXiv:221007558\nVan Den Oord A, Vinyals O, et al. (2017) Neural discrete representation learning.\nAdvances in neural information processing systems 30\n146\nVaswani A, Shazeer N, Parmar N, et al. (2017) Attention is all you need. Advances in\nneural information processing systems 30\nVedantam R, Zitnick CL, Parikh D (2015) Cider: Consensus-based image description\nevaluation. 1411.5726\nVilares D, Peng H, Satapathy R, et al. (2018) Babelsenticnet: a commonsense reason-\ning framework for multilingual sentiment analysis. In: 2018 IEEE symposium series\non computational intelligence (SSCI), IEEE, pp 1292–1298\nWaldmann MR, Hagmayer Y (2013) Causal reasoning. arXiv\nWan Z, Cheng F, Mao Z, et al. (2023) Gpt-re: In-context learning for relation\nextraction using large language models. 2305.02105\nWang B (2021) Mesh-Transformer-JAX: Model-Parallel Implementation of\nTransformer Language Model with JAX. https://github.com/kingoflolz/\nmesh-transformer-jax\nWang B, Komatsuzaki A (2021) Gpt-j-6b: A 6 billion parameter autoregressive\nlanguage model\nWang B, Deng X, Sun H (2022a) Iteratively prompt pre-trained language models for\nchain of thought. arXiv preprint arXiv:220308383\nWang C, Fan L, Sun J, et al. (2023a) Mimicplay: Long-horizon imitation learning by\nwatching human play. arXiv preprint arXiv:230212422\nWang C, Lu Y, Mu Y, et al. (2023b) Improved knowledge distillation for pre-trained\nlanguage models via knowledge selection. 2302.00444\nWang D, Zhang J, Du B, et al. (2023c) Scaling-up remote sensing segmentation dataset\nwith segment anything model. arXiv preprint arXiv:230502034\nWang G, Xie Y, Jiang Y, et al. (2023d) Voyager: An open-ended embodied agent with\nlarge language models. 2305.16291\nWang H, Hu M, Deng Y, et al. (2023e) Large language models as source planner for\npersonalized knowledge-grounded dialogue. 2310.08840\nWang H, Wang R, Mi F, et al. (2023f) Chain-of-thought prompting for responding to\nin-depth dialogue questions with llm. arXiv preprint arXiv:230511792\nWang H, Yuan Y, Liu Z, et al. (2023g) Dt-solver: Automated theorem proving with\ndynamic-tree sampling guided by proof-level value function. In: Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp 12632–12646\n147\nWang J, Liu Z, Zhao L, et al. (2023h) Review of large vision models and visual prompt\nengineering. arXiv preprint arXiv:230700855\nWang L, Huang B, Zhao Z, et al. (2023i) Videomae v2: Scaling video masked\nautoencoders with dual masking. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp 14549–14560\nWang L, Ma C, Feng X, et al. (2023j) A survey on large language model based\nautonomous agents. 2308.11432\nWang L, Xu W, Lan Y, et al. (2023k) Plan-and-solve prompting: Improving\nzero-shot chain-of-thought reasoning by large language models. arXiv preprint\narXiv:230504091\nWang L, Yang N, Wei F (2023l) Learning to retrieve in-context examples for large\nlanguage models. arXiv preprint arXiv:230707164\nWang P, Li L, Chen L, et al. (2023m) Making large language models better reasoners\nwith alignment. arXiv preprint arXiv:230902144\nWang P, Li L, Shao Z, et al. (2023n) Math-shepherd: A label-free step-by-step verifier\nfor llms in mathematical reasoning. arXiv preprint arXiv:231208935\nWang R, Jansen P, Cˆ ot´ e MA, et al. (2022b) Scienceworld: Is your agent smarter than\na 5th grader? arXiv preprint arXiv:220307540\nWang R, Zelikman E, Poesia G, et al. (2023o) Hypothesis search: Inductive reasoning\nwith language models. 2309.05660\nWang T, Zhang J, Fei J, et al. (2023p) Caption anything: Interactive image description\nwith diverse multimodal controls. arXiv preprint arXiv:230502677\nWang X, Xu X, Tong W, et al. (2021a) Inferbert: a transformer-based causal infer-\nence framework for enhancing pharmacovigilance. Frontiers in Artificial Intelligence\n4:659622\nWang X, Caccia L, Ostapenko O, et al. (2023q) Guiding language model reasoning\nwith planning tokens. arXiv preprint arXiv:231005707\nWang X, Gu R, Chen Z, et al. (2023r) Uni-rna: universal pre-trained models\nrevolutionize rna research. bioRxiv pp 2023–07\nWang X, Hu Z, Lu P, et al. (2023s) Scibench: Evaluating college-level scientific\nproblem-solving abilities of large language models. arXiv preprint arXiv:230500970\nWang X, Wei J, Schuurmans D, et al. (2023t) Self-consistency improves chain of\nthought reasoning in language models. In: The Eleventh International Conference on\nLearning Representations, URL https://openreview.net/forum?id=1PL1NIMMrw\n148\nWang Y, Liu X, Shi S (2017) Deep neural solver for math word problems. In: Proceed-\nings of the 2017 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, Copenhagen, Denmark, pp 845–854,\nhttps://doi.org/10.18653/v1/D17-1088, URL https://aclanthology.org/D17-1088\nWang Y, Wang W, Joty S, et al. (2021b) Codet5: Identifier-aware unified pre-trained\nencoder-decoder models for code understanding and generation. arXiv preprint\narXiv:210900859\nWang Y, Kordi Y, Mishra S, et al. (2022c) Self-instruct: Aligning language model with\nself generated instructions\nWang Y, Kordi Y, Mishra S, et al. (2022d) Self-instruct: Aligning language model\nwith self generated instructions. arXiv preprint arXiv:221210560\nWang Y, Mishra S, Alipoormolabashi P, et al. (2022e) Super-\nnaturalinstructions:generalization via declarative instructions on 1600+ tasks. In:\nEMNLP\nWang Y, Mukherjee S, Liu X, et al. (2022f) Adamix: Mixture-of-adapter for parameter-\nefficient tuning of large language models. arXiv preprint arXiv:220512410 1(2):4\nWang Y, Le H, Gotmare AD, et al. (2023u) Codet5+: Open code large language\nmodels for code understanding and generation. arXiv preprint arXiv:230507922\nWang Y, Zhong W, Li L, et al. (2023v) Aligning large language models with human:\nA survey. arXiv preprint arXiv:230712966\nWang YR, Duan J, Fox D, et al. (2023w) Newton: Are large language models capable\nof physical reasoning? 2310.07018\nWang Z, Wohlwend J, Lei T (2020) Structured pruning of large language models. In:\nProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP). Association for Computational Linguistics, https://doi.org/\n10.18653/v1/2020.emnlp-main.496, URL https://doi.org/10.18653%2Fv1%2F2020.\nemnlp-main.496\nWang Z, Cai S, Liu A, et al. (2023x) Describe, explain, plan and select: Interactive\nplanning with large language models enables open-world multi-task agents. 2302.\n01560\nWang Z, Zhang G, Yang K, et al. (2023y) Interactive natural language processing.\narXiv preprint arXiv:230513246\nWatson JL, Juergens D, Bennett NR, et al. (2023) De novo design of protein structure\nand function with rfdiffusion. Nature 620(7976):1089–1100\n149\nWei J, Bosma M, Zhao VY, et al. (2021) Finetuned language models are zero-shot\nlearners. arXiv preprint arXiv:210901652\nWei J, Tay Y, Bommasani R, et al. (2022a) Emergent abilities of large language\nmodels. 2206.07682\nWei J, Wang X, Schuurmans D, et al. (2022b) Chain-of-thought prompting elicits\nreasoning in large language models. Advances in Neural Information Processing\nSystems 35:24824–24837\nWeidinger L, Mellor J, Rauh M, et al. (2021) Ethical and social risks of harm from\nlanguage models. 2112.04359\nWelleck S, Lu X, West P, et al. (2022) Generating sequences by learning to self-correct.\narXiv preprint arXiv:221100053\nWeston J, Sukhbaatar S (2023) System 2 attention (is something you might need too).\n2311.11829\nWillig M, Zeˇ cevi´ c M, Dhami DS, et al. (2023) Probing for correlations of causal\nfacts: Large language models and causality. URL https://openreview.net/forum?\nid=UPwzqPOs4-\nWoodcock J, Larsen PG, Bicarregui J, et al. (2009) Formal methods: Practice and\nexperience. ACM computing surveys (CSUR) 41(4):1–36\nWu B, Zhang Z, Zhao H (2021a) Graph-free multi-hop reading comprehension: A\nselect-to-guide strategy. ArXiv preprint abs/2107.11823. URL https://arxiv.org/\nabs/2107.11823\nWu C, Yin S, Qi W, et al. (2023a) Visual chatgpt: Talking, drawing and editing with\nvisual foundation models. 2303.04671\nWu D, Han W, Wang T, et al. (2023b) Language prompt for autonomous driving.\narXiv preprint arXiv:230904379\nWu H, Zhang Z, Zhang E, et al. (2023c) Q-bench: A benchmark for general-purpose\nfoundation models on low-level vision. arXiv preprint arXiv:230914181\nWu M, Norrish M, Walder C, et al. (2021b) Tacticzero: Learning to prove theorems\nfrom scratch with deep reinforcement learning. Advances in Neural Information\nProcessing Systems 34:9330–9342\nWu M, Waheed A, Zhang C, et al. (2023d) Lamini-lm: A diverse herd of distilled\nmodels from large-scale instructions. CoRR abs/2304.14402. URL https://arxiv.\norg/abs/2304.14402, 2304.14402\n150\nWu W, Timofeev A, Chen C, et al. (2023e) Mofi: Learning image representations from\nnoisy entity annotated images. arXiv preprint arXiv:230607952\nWu Y, Jiang AQ, Li W, et al. (2022a) Autoformalization with large language models.\nIn: Oh AH, Agarwal A, Belgrave D, et al. (eds) Advances in Neural Information\nProcessing Systems, URL https://openreview.net/forum?id=IUikebJ1Bf0\nWu Z, Dvornik N, Greff K, et al. (2022b) Slotformer: Unsupervised visual dynamics\nsimulation with object-centric models. arXiv preprint arXiv:221005861\nWu Z, Qiu L, Ross A, et al. (2023f) Reasoning or reciting? exploring the capabilities\nand limitations of language models through counterfactual tasks. 2307.02477\nWu Z, Wang Z, Xu X, et al. (2023g) Embodied task planning with large language\nmodels. arXiv preprint arXiv:230701848\nXi Z, Chen W, Guo X, et al. (2023) The rise and potential of large language model\nbased agents: A survey. 2309.07864\nXia F, R. Zamir A, He ZY, et al. (2018) Gibson Env: real-world perception for embod-\nied agents. In: Computer Vision and Pattern Recognition (CVPR), 2018 IEEE\nConference on, IEEE\nXiao B, Wu H, Xu W, et al. (2023a) Florence-2: Advancing a unified representation\nfor a variety of vision tasks. arXiv preprint arXiv:231106242\nXiao G, Tian Y, Chen B, et al. (2023b) Efficient streaming language models with\nattention sinks. arXiv preprint arXiv:230917453\nXie D, Wang R, Ma J, et al. (2023a) Edit everything: A text-guided generative system\nfor images editing. 2304.14006\nXie E, Yao L, Shi H, et al. (2023b) Difffit: Unlocking transferability of large diffusion\nmodels via simple parameter-efficient fine-tuning. arXiv preprint arXiv:230406648\nXie Y, Kawaguchi K, Zhao Y, et al. (2023c) Decomposition enhances reasoning via\nself-evaluation guided decoding. 2305.00633\nXie Z, Sun S (2019) A goal-driven tree-structured neural model for math word\nproblems. In: Ijcai, pp 5299–5305\nXin H, Wang H, Zheng C, et al. (2023) Lego-prover: Neural theorem proving with\ngrowing libraries. arXiv preprint arXiv:231000656\nXiong J, Li C, Yang M, et al. (2022) Expression syntax information bottleneck for math\nword problems. In: Amig´ o E, Castells P, Gonzalo J, et al. (eds) SIGIR ’22: The 45th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022. ACM, pp 2166–2171, https://doi.org/\n151\n10.1145/3477495.3531824, URL https://doi.org/10.1145/3477495.3531824\nXiong J, Li Z, Zheng C, et al. (2023a) Dq-lore: Dual queries with low rank approx-\nimation re-ranking for in-context learning. arXiv preprint arXiv:231002954 URL\nhttps://api.semanticscholar.org/CorpusID:263620351\nXiong J, Shen J, Yuan Y, et al. (2023b) Trigo: Benchmarking formal mathematical\nproof reduction for generative language models. arXiv preprint arXiv:231010180\nXu C, Sun Q, Zheng K, et al. (2023a) Wizardlm: Empowering large language models\nto follow complex instructions. 2304.12244\nXu FF, Alon U, Neubig G, et al. (2022) A systematic evaluation of large language\nmodels of code. In: Proceedings of the 6th ACM SIGPLAN International Symposium\non Machine Programming, pp 1–10\nXu L, Hu H, Zhang X, et al. (2020) CLUE: A Chinese language understanding\nevaluation benchmark. In: Proceedings of the 28th International Conference on\nComputational Linguistics. International Committee on Computational Linguis-\ntics, Barcelona, Spain (Online), pp 4762–4772, https://doi.org/10.18653/v1/2020.\ncoling-main.419, URL https://aclanthology.org/2020.coling-main.419\nXu P, Shao W, Zhang K, et al. (2023b) Lvlm-ehub: A comprehensive evaluation\nbenchmark for large vision-language models. arXiv preprint arXiv:230609265\nXu R, Luo F, Zhang Z, et al. (2021a) Raise a child in large language model: Towards\neffective and generalizable fine-tuning. arXiv preprint arXiv:210905687\nXu R, Wang X, Wang T, et al. (2023c) Pointllm: Empowering large language models\nto understand point clouds. arXiv preprint arXiv:230816911\nXu S, Yang L, Kelly C, et al. (2023d) Elixr: Towards a general purpose x-ray artificial\nintelligence system through alignment of large language models and radiology vision\nencoders. arXiv preprint arXiv:230801317\nXu Y, Liu X, Cao X, et al. (2021b) Artificial intelligence: A powerful paradigm for\nscientific research. The Innovation 2(4):100179. https://doi.org/https://doi.org/10.\n1016/j.xinn.2021.100179, URL https://www.sciencedirect.com/science/article/pii/\nS2666675821001041\nXu Z, Zhang Y, Xie E, et al. (2023e) DriveGPT4: Interpretable End-to-end\nAutonomous Driving via Large Language Model. arXiv preprint arXiv:231001412\nXue F, Shi Z, Wei F, et al. (2022) Go wider instead of deeper. In: Proceedings of the\nAAAI Conference on Artificial Intelligence, pp 8779–8787\nYan H, Dai J, Ji T, et al. (2021) A unified generative framework for aspect-based senti-\nment analysis. In: Zong C, Xia F, Li W, et al. (eds) Proceedings of the 59th Annual\n152\nMeeting of the Association for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing (Volume 1: Long Papers).\nAssociation for Computational Linguistics, Online, pp 2416–2429, https://doi.org/\n10.18653/v1/2021.acl-long.188, URL https://aclanthology.org/2021.acl-long.188\nYan Z, Zhang K, Zhou R, et al. (2023) Multimodal chatgpt for medical applications:\nan experimental study of gpt-4v. arXiv preprint arXiv:231019061\nYang H, Li P, Lam W (2022a) Parameter-efficient tuning by manipulating hid-\nden states of pretrained language models for classification tasks. arXiv preprint\narXiv:220404596\nYang H, Wang Y, Li P, et al. (2023a) Bridging the gap between pre-training\nand fine-tuning for commonsense generation. In: Findings of the Association for\nComputational Linguistics: EACL 2023, pp 376–383\nYang J, Prabhakar A, Narasimhan K, et al. (2023b) Intercode: Standardizing and\nbenchmarking interactive coding with execution feedback. 2306.14898\nYang K, Deng J (2019) Learning to prove theorems via interacting with proof\nassistants. In: International Conference on Machine Learning, PMLR, pp 6984–6994\nYang K, Deng J (2021) Learning symbolic rules for reasoning in quasi-natural\nlanguage. arXiv preprint arXiv:211112038\nYang K, Deng J, Chen D (2022b) Generating natural language proofs with verifier-\nguided search. In: Proceedings of the 2022 Conference on Empirical Methods in Nat-\nural Language Processing. Association for Computational Linguistics, Abu Dhabi,\nUnited Arab Emirates, pp 89–105, https://doi.org/10.18653/v1/2022.emnlp-main.\n7, URL https://aclanthology.org/2022.emnlp-main.7\nYang K, Swope AM, Gu A, et al. (2023c) Leandojo: Theorem proving with retrieval-\naugmented language models. arXiv preprint arXiv:230615626\nYang R, Song L, Li Y, et al. (2023d) Gpt4tools: Teaching large language model to use\ntools via self-instruction. arXiv preprint arXiv:230518752\nYang S, Nachum O, Du Y, et al. (2023e) Foundation models for decision making:\nProblems, methods, and opportunities. arXiv preprint arXiv:230304129\nYang Sw, Chi PH, Chuang YS, et al. (2021) SUPERB: Speech Processing Universal\nPERformance Benchmark. In: INTERSPEECH, pp 1194–1198\nYang Z, Yang Y (2022) Decoupling features in hierarchical propagation for video object\nsegmentation. Advances in Neural Information Processing Systems 35:36324–36336\nYang Z, Qi P, Zhang S, et al. (2018) HotpotQA: A dataset for diverse, explainable\nmulti-hop question answering. In: Proceedings of the 2018 Conference on Empirical\n153\nMethods in Natural Language Processing. Association for Computational Linguis-\ntics, Brussels, Belgium, pp 2369–2380, https://doi.org/10.18653/v1/D18-1259, URL\nhttps://aclanthology.org/D18-1259\nYang Z, Dong L, Du X, et al. (2022c) Language models as inductive reasoners. arXiv\npreprint arXiv:221210923\nYang Z, Du X, Mao R, et al. (2023f) Logical reasoning over natural language as\nknowledge representation: A survey. arXiv preprint arXiv:230312023\nYang Z, Li L, Lin K, et al. (2023g) The dawn of lmms: Preliminary explorations with\ngpt-4v (ision). arXiv preprint arXiv:230917421\nYao L, Huang R, Hou L, et al. (2021) Filip: Fine-grained interactive language-image\npre-training. arXiv preprint arXiv:211107783\nYao L, Han J, Wen Y, et al. (2022) Detclip: Dictionary-enriched visual-concept\nparalleled pre-training for open-world detection. Advances in Neural Information\nProcessing Systems 35:9125–9138\nYao L, Han J, Liang X, et al. (2023a) Detclipv2: Scalable open-vocabulary object\ndetection pre-training via word-region alignment. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp 23497–23506\nYao S, Yu D, Zhao J, et al. (2023b) Tree of thoughts: Deliberate problem solving with\nlarge language models. 2305.10601\nYao S, Zhao J, Yu D, et al. (2023c) ReAct: Synergizing reasoning and acting in\nlanguage models. In: International Conference on Learning Representations (ICLR)\nYao Y, Li Z, Zhao H (2023d) Beyond chain-of-thought, effective graph-of-thought\nreasoning in large language models. 2305.16582\nYasunaga M, Liang P (2020) Graph-based, self-supervised program repair from diag-\nnostic feedback. In: International Conference on Machine Learning, PMLR, pp\n10799–10808\nYe J, Wu Z, Feng J, et al. (2023a) Compositional exemplars for in-context learning.\narXiv preprint arXiv:230205698\nYe Q, Lin BY, Ren X (2021) Crossfit: A few-shot learning challenge for cross-task\ngeneralization in nlp. arXiv preprint arXiv:210408835\nYe S, Xie Y, Chen D, et al. (2023b) Improving commonsense in vision-language mod-\nels via knowledge graph riddles. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp 2634–2645\n154\nYe X, Iyer S, Celikyilmaz A, et al. (2022) Complementary explanations for effective\nin-context learning. arXiv preprint arXiv:221113892\nYi K, Gan C, Li Y, et al. (2019) Clevrer: Collision events for video representation and\nreasoning. arXiv preprint arXiv:191001442\nYih Wt, Richardson M, Meek C, et al. (2016) The value of semantic parse labeling\nfor knowledge base question answering. In: Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics (Volume 2: Short Papers).\nAssociation for Computational Linguistics, Berlin, Germany, pp 201–206, https:\n//doi.org/10.18653/v1/P16-2033, URL https://aclanthology.org/P16-2033\nYin D, Liu X, Yin F, et al. (2023a) Dynosaur: A dynamic growth paradigm for\ninstruction-tuning data curation. arXiv preprint arXiv:230514327\nYin S, Fu C, Zhao S, et al. (2023b) A survey on multimodal large language models.\n2306.13549\nYin Z, Sun Q, Chang C, et al. (2023c) Exchange-of-thought: Enhancing large language\nmodel capabilities through cross-model communication. In: The 2023 Conference on\nEmpirical Methods in Natural Language Processing, URL https://openreview.net/\nforum?id=30kbnyD9hF\nYin Z, Sun Q, Guo Q, et al. (2023d) Do large language models know what they\ndon’t know? In: Rogers A, Boyd-Graber J, Okazaki N (eds) Findings of the Asso-\nciation for Computational Linguistics: ACL 2023. Association for Computational\nLinguistics, Toronto, Canada, pp 8653–8665, https://doi.org/10.18653/v1/2023.\nfindings-acl.551, URL https://aclanthology.org/2023.findings-acl.551\nYin Z, Wang J, Cao J, et al. (2023e) Lamm: Language-assisted multi-\nmodal instruction-tuning dataset, framework, and benchmark. arXiv preprint\narXiv:230606687\nYin Z, Wang Y, Hu X, et al. (2023f) Rethinking label smoothing on multi-hop question\nanswering. In: Sun M, Qin B, Qiu X, et al. (eds) Chinese Computational Linguistics.\nSpringer Nature Singapore, Singapore, pp 72–87\nYoneda T, Fang J, Li P, et al. (2023) Statler: State-maintaining language models for\nembodied reasoning. 2306.17840\nYoung N, Bao Q, Bensemann J, et al. (2022) AbductionRules: Training transformers\nto explain unexpected inputs. In: Findings of the Association for Computational\nLinguistics: ACL 2022. Association for Computational Linguistics, Dublin, Ire-\nland, pp 218–227, https://doi.org/10.18653/v1/2022.findings-acl.19, URL https:\n//aclanthology.org/2022.findings-acl.19\n155\nYu F, Zhang H, Tiwari P, et al. (2023a) Natural language reasoning, a survey. arXiv\npreprint arXiv:230314725\nYu L, Jiang W, Shi H, et al. (2023b) Metamath: Bootstrap your own mathematical\nquestions for large language models. arXiv preprint arXiv:230912284\nYu S, Mo S, Ahn S, et al. (2021) Abstract reasoning via logic-guided generation.\n2107.10493\nYu S, Wu P, Liang PP, et al. (2022) Pacs: A dataset for physical audiovisual\ncommonsense reasoning. 2203.11130\nYu T, Zhang R, Yang K, et al. (2018) Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-SQL task. In: Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, Brussels, Belgium, pp 3911–3921, https:\n//doi.org/10.18653/v1/D18-1425, URL https://aclanthology.org/D18-1425\nYu T, Feng R, Feng R, et al. (2023c) Inpaint anything: Segment anything meets image\ninpainting. arXiv preprint arXiv:230406790\nYuan L, Chen D, Chen YL, et al. (2021) Florence: A new foundation model for\ncomputer vision. arXiv preprint arXiv:211111432\nYuan Z, Yuan H, Li C, et al. (2023a) Scaling relationship on learning mathematical\nreasoning with large language models. arXiv preprint arXiv:230801825\nYuan Z, Yuan H, Tan C, et al. (2023b) Rrhf: Rank responses to align language models\nwith human feedback without tears. arXiv preprint arXiv:230405302\nYue X, Qu X, Zhang G, et al. (2023) Mammoth: Building math generalist models\nthrough hybrid instruction tuning. arXiv preprint arXiv:230905653\nZaken EB, Ravfogel S, Goldberg Y (2021) Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. arXiv preprint arXiv:210610199\nZan D, Chen B, Yang D, et al. (2022) Cert: Continual pre-training on sketches for\nlibrary-oriented code generation. arXiv preprint arXiv:220606888\nZan D, Chen B, Zhang F, et al. (2023) Large language models meet nl2code: A survey.\nIn: Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp 7443–7464\nZeˇ cevi´ c M, Willig M, Dhami DS, et al. (2023) Causal parrots: Large language models\nmay talk causality but are not causal. arXiv preprint arXiv:230813067\nZelikman E, Wu Y, Mu J, et al. (2022) STar: Bootstrapping reasoning with reasoning.\nIn: Oh AH, Agarwal A, Belgrave D, et al. (eds) Advances in Neural Information\n156\nProcessing Systems, URL https://openreview.net/forum?id= 3ELRdg2sgI\nZellers R, Bisk Y, Schwartz R, et al. (2018) Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. arXiv preprint arXiv:180805326\nZellers R, Holtzman A, Bisk Y, et al. (2019) Hellaswag: Can a machine really finish\nyour sentence? arXiv preprint arXiv:190507830\nZeng A, Liu X, Du Z, et al. (2022) Glm-130b: An open bilingual pre-trained model.\narXiv preprint arXiv:221002414\nZeng A, Attarian M, brian ichter, et al. (2023) Socratic models: Composing zero-shot\nmultimodal reasoning with language. In: The Eleventh International Conference on\nLearning Representations, URL https://openreview.net/forum?id=G2Q2Mh3avow\nZeng W, Ren X, Su T, et al. (2021) Pangu- \\alpha: Large-scale autoregressive pre-\ntrained chinese language models with auto-parallel computation. arXiv preprint\narXiv:210412369\nZhai X, Wang X, Mustafa B, et al. (2022) Lit: Zero-shot transfer with locked-image\ntext tuning. In: CVPR, pp 18102–18112\nZhang C, Bauer S, Bennett P, et al. (2023a) Understanding causality with large\nlanguage models: Feasibility and opportunities. arXiv preprint arXiv:230405524\nZhang G, Shi Y, Liu R, et al. (2023b) Chinese open instruction generalist: A\npreliminary release. arXiv preprint arXiv:230407987\nZhang H, Du W, Shan J, et al. (2023c) Building cooperative embodied agents\nmodularly with large language models. 2307.02485\nZhang J, Zhou Z, Mai G, et al. (2023d) Text2seg: Remote sensing image\nsemantic segmentation via text-guided visual foundation models. arXiv preprint\narXiv:230410597\nZhang K, Li Z, Li J, et al. (2023e) Self-edit: Fault-aware code editor for code\ngeneration. In: Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). Association for Computa-\ntional Linguistics, Toronto, Canada, pp 769–787, URL https://aclanthology.org/\n2023.acl-long.45\nZhang K, Yu J, Yan Z, et al. (2023f) Biomedgpt: A unified and generalist biomedical\ngenerative pre-trained transformer for vision, language, and multimodal tasks. 2305.\n17100\nZhang P, Li X, Hu X, et al. (2021) Vinvl: Making visual representations matter in\nvision-language models. arXiv preprint arXiv:210100529\n157\nZhang Q, Chen M, Bukharin A, et al. (2023g) Adaptive budget allocation for\nparameter-efficient fine-tuning. arXiv preprint arXiv:230310512\nZhang R, Han J, Zhou A, et al. (2023h) Llama-adapter: Efficient fine-tuning of\nlanguage models with zero-init attention. arXiv preprint arXiv:230316199\nZhang S, Roller S, Goyal N, et al. (2022a) Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:220501068\nZhang T, Ladhak F, Durmus E, et al. (2023i) Benchmarking large language models\nfor news summarization. 2301.13848\nZhang X, Wang L, Helwig J, et al. (2023j) Artificial intelligence for science in quantum,\natomistic, and continuum systems. 2307.08423\nZhang Y, Dai H, Kozareva Z, et al. (2017) Variational reasoning for question answering\nwith knowledge graph. In: AAAI Conference on Artificial Intelligence\nZhang Y, Feng S, Tan C (2022b) Active example selection for in-context learning.\nIn: Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pp 9134–9148\nZhang Y, Chen S, Jiang W, et al. (2023k) Domain-guided conditional diffusion model\nfor unsupervised domain adaptation. arXiv preprint arXiv:230914360\nZhang Y, Qian S, Peng B, et al. (2023l) Prompt highlighter: Interactive control for\nmulti-modal llms. arXiv preprint 231204302\nZhang Z, Zhang A, Li M, et al. (2022c) Automatic chain of thought prompting in\nlarge language models. arXiv preprint arXiv:221003493\nZhao H, Wang K, Yu M, et al. (2023a) Explicit planning helps language models in\nlogical reasoning. 2303.15714\nZhao WX, Zhou K, Li J, et al. (2023b) A survey of large language models. arXiv\npreprint arXiv:230318223\nZhao X, Li W, Kong L (2023c) Decomposing the enigma: Subgoal-based demonstration\nlearning for formal theorem proving. arXiv preprint arXiv:230516366\nZhao X, Xie Y, Kawaguchi K, et al. (2023d) Automatic model selection with large\nlanguage models for reasoning. 2305.14333\nZhao Y, Khalman M, Joshi R, et al. (2022a) Calibrating sequence likelihood improves\nconditional language generation. In: The Eleventh International Conference on\nLearning Representations\n158\nZhao Y, Li Y, Li C, et al. (2022b) MultiHiertt: Numerical reasoning over multi hierar-\nchical tabular and textual data. In: Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Association\nfor Computational Linguistics, Dublin, Ireland, pp 6588–6600, https://doi.org/10.\n18653/v1/2022.acl-long.454, URL https://aclanthology.org/2022.acl-long.454\nZhao Y, Pang T, Du C, et al. (2023e) On evaluating adversarial robustness of large\nvision-language models. arXiv preprint arXiv:230516934\nZhao Z, Lee WS, Hsu D (2023f) Large language models as commonsense knowledge\nfor large-scale task planning. arXiv preprint arXiv:230514078\nZheng C, Liu Z, Xie E, et al. (2023a) Progressive-hint prompting improves reasoning\nin large language models. arXiv preprint arXiv:230409797\nZheng C, Wang H, Xie E, et al. (2023b) Lyra: Orchestrating dual correction in\nautomated theorem proving. arXiv preprint arXiv:230915806\nZheng K, Han JM, Polu S (2021) Minif2f: a cross-system benchmark for formal\nolympiad-level mathematics. arXiv preprint arXiv:210900110\nZheng Q, Xia X, Zou X, et al. (2023c) Codegeex: A pre-trained model for code genera-\ntion with multilingual evaluations on humaneval-x. arXiv preprint arXiv:230317568\nZhong M, Liu P, Chen Y, et al. (2020) Extractive summarization as text matching. In:\nJurafsky D, Chai J, Schluter N, et al. (eds) Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics. Association for Computational\nLinguistics, Online, pp 6197–6208, https://doi.org/10.18653/v1/2020.acl-main.552,\nURL https://aclanthology.org/2020.acl-main.552\nZhong T, Zhao W, Zhang Y, et al. (2023) Chatradio-valuer: A chat large language\nmodel for generalizable radiology report generation based on multi-institution and\nmulti-system data. arXiv preprint arXiv:231005242\nZhong V, Xiong C, Socher R (2018) Seq2SQL: Generating structured queries from\nnatural language using reinforcement learning. URL https://openreview.net/forum?\nid=Syx6bz-Ab\nZhong Z, Friedman D, Chen D (2021) Factual probing is [MASK]: Learning vs. learning\nto recall. In: Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies.\nAssociation for Computational Linguistics, Online, pp 5017–5033, https://doi.org/\n10.18653/v1/2021.naacl-main.398, URL https://aclanthology.org/2021.naacl-main.\n398\nZhou D, Sch¨ arli N, Hou L, et al. (2022a) Least-to-most prompting enables complex\nreasoning in large language models. arXiv preprint arXiv:220510625\n159\nZhou H, Ding M, Peng W, et al. (2023a) Generalizable long-horizon manipulations\nwith large language models. arXiv preprint arXiv:231002264\nZhou J, Hu S, Lv X, et al. (2020) Kacc: A multi-task benchmark for knowledge\nabstraction, concretization and completion. arXiv preprint arXiv:200413631\nZhou J, Zhang Y, Luo Q, et al. (2023b) Synthetic lies: Understanding ai-generated\nmisinformation and evaluating algorithmic and human solutions. In: Proceedings of\nthe 2023 CHI Conference on Human Factors in Computing Systems, pp 1–20\nZhou K, Yang J, Loy CC, et al. (2022b) Learning to prompt for vision-language models.\nInternational Journal of Computer Vision 130(9):2337–2348\nZhou L, Dai S, Chen L (2015) Learn to solve algebra word problems using quadratic\nprogramming. In: Conference on Empirical Methods in Natural Language Processing\nZhou X, Liu M, Zagar BL, et al. (2023c) Vision language models in autonomous driving\nand intelligent transportation systems. arXiv preprint arXiv:231014414\nZhou Y, Chia MA, Wagner SK, et al. (2023d) A foundation model for generalizable\ndisease detection from retinal images. Nature pp 1–8\nZhu B, Sharma H, Frujeri FV, et al. (2023a) Fine-tuning language models with\nadvantage-induced policy alignment. arXiv preprint arXiv:230602231\nZhu D, Chen J, Shen X, et al. (2023b) Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. 2304.10592\nZhu F, Lei W, Huang Y, et al. (2021) Tat-qa: A question answering benchmark on a\nhybrid of tabular and textual content in finance. arXiv preprint arXiv:210507624\nZhu X, Li J, Liu Y, et al. (2023c) A survey on model compression for large language\nmodels. 2308.07633\nZhu Y, Yuan H, Wang S, et al. (2023d) Large language models for information retrieval:\nA survey. 2308.07107\nZiyu Z, Xiaojian M, Yixin C, et al. (2023) 3d-vista: Pre-trained transformer for 3d\nvision and text alignment. In: ICCV\nZong Y, Aodha OM, Hospedales T (2023) Self-supervised multimodal learning: A\nsurvey. 2304.01008\nZuo S, Zhang Q, Liang C, et al. (2022) Moebert: from bert to mixture-of-experts via\nimportance-guided adaptation. arXiv preprint arXiv:220407675\n160",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.8079162836074829
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6222927570343018
    },
    {
      "name": "Computer science",
      "score": 0.6202231645584106
    },
    {
      "name": "Relevance (law)",
      "score": 0.6156835556030273
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5729316473007202
    },
    {
      "name": "Reasoning system",
      "score": 0.513247013092041
    },
    {
      "name": "Qualitative reasoning",
      "score": 0.5122765898704529
    },
    {
      "name": "Verbal reasoning",
      "score": 0.4807955324649811
    },
    {
      "name": "Management science",
      "score": 0.4788679778575897
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47701001167297363
    },
    {
      "name": "Negotiation",
      "score": 0.4701808989048004
    },
    {
      "name": "Analytic reasoning",
      "score": 0.4444957673549652
    },
    {
      "name": "Opportunistic reasoning",
      "score": 0.4315444231033325
    },
    {
      "name": "Scientific reasoning",
      "score": 0.41929566860198975
    },
    {
      "name": "Model-based reasoning",
      "score": 0.415130615234375
    },
    {
      "name": "Cognitive science",
      "score": 0.4063299894332886
    },
    {
      "name": "Data science",
      "score": 0.38547566533088684
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.21542149782180786
    },
    {
      "name": "Psychology",
      "score": 0.1720713973045349
    },
    {
      "name": "Mathematics education",
      "score": 0.13540557026863098
    },
    {
      "name": "Cognition",
      "score": 0.1231909692287445
    },
    {
      "name": "Sociology",
      "score": 0.11621403694152832
    },
    {
      "name": "Engineering",
      "score": 0.11147969961166382
    },
    {
      "name": "Geography",
      "score": 0.06694835424423218
    },
    {
      "name": "Political science",
      "score": 0.06313684582710266
    },
    {
      "name": "Mathematics",
      "score": 0.06261810660362244
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I27357992",
      "name": "Dalian University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I111483173",
      "name": "King University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I16365422",
      "name": "Hefei University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ],
  "cited_by": 15
}