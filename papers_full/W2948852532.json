{
  "title": "Lattice-Based Transformer Encoder for Neural Machine Translation",
  "url": "https://openalex.org/W2948852532",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Xiao, Fengshun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282203717",
      "name": "Li, Jiangtong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084083973",
      "name": "Zhao Hai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1686891224",
      "name": "Wang Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2361524982",
      "name": "Chen Ke-hai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2962811598",
    "https://openalex.org/W2608256743",
    "https://openalex.org/W2800490029",
    "https://openalex.org/W2102461220",
    "https://openalex.org/W2104747875",
    "https://openalex.org/W2963201387",
    "https://openalex.org/W2740743644",
    "https://openalex.org/W25062297",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W2963329925",
    "https://openalex.org/W2963887123",
    "https://openalex.org/W2798304389",
    "https://openalex.org/W2888196092",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2859207840",
    "https://openalex.org/W2756978580",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2157435188",
    "https://openalex.org/W2096204319",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963022746",
    "https://openalex.org/W2883527841",
    "https://openalex.org/W61894391",
    "https://openalex.org/W2909066711",
    "https://openalex.org/W2292633562",
    "https://openalex.org/W2758137671",
    "https://openalex.org/W2910676115",
    "https://openalex.org/W2899704967",
    "https://openalex.org/W2572549015",
    "https://openalex.org/W2220350356",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2162245945",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W2963324947",
    "https://openalex.org/W2963997155",
    "https://openalex.org/W2963021447",
    "https://openalex.org/W2436788615",
    "https://openalex.org/W2803739890",
    "https://openalex.org/W2963949210",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2950448199",
    "https://openalex.org/W2005828695",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W1412698887",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.",
  "full_text": "Lattice-Based Transformer Encoder for Neural Machine Translation\nFengshun Xiao1,2, Jiangtong Li2,3, Hai Zhao1,2,∗, Rui Wang4, Kehai Chen4\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n3College of Zhiyuan, Shanghai Jiao Tong University, China\n4National Institute of Information and Communications Technology (NICT)\n{felixxiao, keep moving-lee}@sjtu.edu.cn,\nzhaohai@cs.sjtu.edu.cn, {wangrui, khchen}@nict.go.jp\nAbstract\nNeural machine translation (NMT) takes\ndeterministic sequences for source\nrepresentations. However, either word-\nlevel or subword-level segmentations have\nmultiple choices to split a source sequence\nwith different word segmentors or different\nsubword vocabulary sizes. We hypothesize\nthat the diversity in segmentations may affect\nthe NMT performance. To integrate different\nsegmentations with the state-of-the-art NMT\nmodel, Transformer, we propose lattice-based\nencoders to explore effective word or subword\nrepresentation in an automatic way during\ntraining. We propose two methods: 1) lattice\npositional encoding and 2) lattice-aware\nself-attention. These two methods can be\nused together and show complementary to\neach other to further improve translation\nperformance. Experiment results show\nsuperiorities of lattice-based encoders in\nword-level and subword-level representations\nover conventional Transformer encoder.\n1 Introduction\nNeural machine translation (NMT) has\nachieved great progress with the evolvement\nof model structures under an encoder-decoder\nframework (Sutskever et al., 2014; Bahdanau\net al., 2014). Recently, the self-attention based\nTransformer model has achieved state-of-the-\nart performance on multiple language pairs\n(Vaswani et al., 2017; Marie et al., 2018). Both\nrepresentations of source and target sentences in\n∗ Corresponding author. This paper was partially\nsupported by National Key Research and Development\nProgram of China (No. 2017YFB0304100) and key projects\nof National Natural Science Foundation of China (No.\nU1836222 and No. 61733011). Rui Wang was partially\nsupported by JSPS grant-in-aid for early-career scientists\n(19K20354): “Unsupervised Neural Machine Translation\nin Universal Scenarios” and NICT tenure-track researcher\nstartup fund “Toward Intelligent Machine Translation”.\nv8mao yi fa zhan ju fu zong caiv7v6v5v4v3v2v1v0\nmao-yi fa-zhan ju fu zong-cai\nfa-zhan-jumao-yi-fa-zhan fu-zong-cai\nv0 v1 v2 v3mao-yi-fa-zhan ju fu-zong-cai\nv0 v1 v2 v3mao-yi fa-zhan-ju fu-zong-cai\nv0 v1 v2 v3 v4 v5mao-yi fa-zhan ju fu zong-cai\n(1)Segmentaion 1\n(2)Segmentaion 2\n(3)Segmentation 3\n(4)Lattice\nv8mao yi fa zhan ju fu zong caiv7v6v5v4v3v2v1v0\ne0:2:mao-yi e2:4:fa-zhan e4:5:ju e5:6:fu e6:8:zong-cai\ne2:5:fa-zhan-jue0:4:mao-yi-fa-zhan e5:8:fu-zong-cai\nc1 c2 c3 c4 c5 c6 c7 c8\nFigure 1: Incorporating three different segmentation\nfor a lattice graph. The original sentence is “ mao-yi-\nfa-zhan-ju-fu-zong-cai”. In Chinese it is “贸易发展局\n副总裁”. In English it means “ The vice president of\nTrade Development Council”\nNMT can be factorized in character (Costa-Jussa\nand Fonollosa, 2016), word (Sutskever et al.,\n2014), or subword (Sennrich et al., 2015) level.\nHowever, only using 1-best segmentation as\ninputs limits NMT encoders to express source\nsequences sufﬁciently and reliably. Many\nEast Asian languages, including Chinese are\nwritten without explicit word boundary, so\nthat their sentences need to be segmented into\nwords ﬁrstly (Zhao et al., 2019; Cai et al.,\n2017; Cai and Zhao, 2016; Zhao et al., 2013;\nZhao and Kit, 2011). By different segmentors,\neach sentence can be segmented into multiple\nforms as shown in Figure 1. Even for those\nalphabetical languages with clear word boundary\nlike English, there is still an issue about selecting\na proper subword vocabulary size, which\ndetermines the segmentation granularities for\nword representation.\nIn order to handle this problem, Morishita\net al. (2018) used hierarchical subword features\nto represent sequence with different subword\ngranularities. Su et al. (2017) proposed the\nﬁrst word-lattice based recurrent neural network\narXiv:1906.01282v1  [cs.CL]  4 Jun 2019\n(RNN) encoders which extended Gated Recurrent\nUnits (GRUs) (Cho et al., 2014) to take in\nmultiple sequence segmentation representations.\nSperber et al. (2017) incorporated posterior scores\nto Tree-LSTM for building a lattice encoder in\nspeech translation. All these existing methods\nserve for RNN-based NMT model, where lattices\ncan be formulized as directed graphs and the\ninherent directed structure of RNN facilitates the\nconstruction of lattice. Meanwhile, the self-\nattention mechanism is good at learning the\ndependency between characters in parallel, which\ncan partially compare and learn information from\nmultiple segmentations (Cherry et al., 2018).\nTherefore, it is challenging to directly apply the\nlattice structure to Transformer.\nIn this work, we explore an efﬁcient way\nof integrating lattice into Transformer. Our\nmethod can not only process multiple sequences\nsegmented in different ways to improve translation\nquality, but also maintain the characteristics of\nparallel computation in the Transformer.\n2 Background\n2.1 Transformer\nTransformer stacks self-attention and point-wise,\nfully connected layers for both encoders and\ndecoders. Decoder layers also have another sub-\nlayer which performs attention over the output\nof the encoder. Residual connections around\neach layer are employed followed by layer\nnormalization (Ba et al., 2016).\nTo make use of the order of the sequence,\nVaswani et al. (2017) proposed Positional\nEncodings to indicate the absolute or relative\nposition of tokens in input sequence which are\ncalculated as:\np(j,2i) = sin(j/100002i/d)\np(j,2i+1) = cos(j/100002i/d),\nwhere j is the position, i is the dimension and\nd is the model dimension. Then positional\nencodings p1:M = {p1,...,p M } are added to the\nembedding of each token t1:M = {t1,...,t M }\nand are propagated to higher layers via residual\nconnections.\n2.2 Self-Attention\nTransformer employs H attention heads\nto perform self-attention over a sequence\nindividually and ﬁnally applies concatenation\nand linear transformation to the results from\nConditions Explanation\nlad i < j= p < q ei:j is left adjacent to ep:q.\nrad p < q= i < j ei:j is right adjacent to ep:q.\ninc i ≤ p < q≤ j ei:j includes ep:q.\nind p ≤ i < j≤ q ei:jis included in ep:q.\nits i < p < j < qor ei:j is intersected with ep:q.p < i < q < j\npre i < j < p < qei:j is preceding edge to ep:q.\nsuc p < q < i < jei:j is succeeding edge to ep:q.\nTable 1: Relations possibly satisﬁed by any two\ndifferent edges ei:j and ep:q in the lattice. Note that two\nequal signs cannot stand at the same time in condition\ninequality for inc and ind.\n.\neach head, which is called multi-head attention\n(Vaswani et al., 2017). Every single head attention\nin multi-head attention is calculated in a scaled\ndot product form:\nuij = (tiWQ)(tjWK)T\n√\nd\n, (1)\nwhere dis the model dimension, t1:M is the input\nsequence and uij are normalized by a softmax\nfunction:\nαij = exp(uij)∑M\nk=1 exp(uik)\n, (2)\nand αij are used to calculate the ﬁnal output\nhidden representations:\noi =\nM∑\nj=1\nαij(tjWV ), (3)\nwhere o1:M is outputs and WQ,WK, and WV are\nlearnable projections matrices for query, key, and\nvalue in a single head, respectively.\n3 Models\n3.1 Lattices\nLattices can represent multiple segmentation\nsequences in a directed graph, as they merge the\nsame subsequence of all candidate subsequences\nusing a compact way.\nAs shown in Figure 1, we follow Su et al.\n(2017) to apply different segmentator to segment\nan element1 sequence c1:N = {c1,c2,...,c N } into\ndifferent word or subword sequences to construct\na lattice G = ⟨V,E⟩, a directed, connected, and\nacyclic graph, where V is node set and E is edge\n1Character for word lattice and minimum subword unit in\nour predeﬁned subword segmentations for subword lattice.\n     Input\nEmbedding\n     Lattice sequence Inputs\n           Lattice \nPositional Encoding\nLattice-aware\nself-attention\nAdd & Norm\nAdd & Norm\n   Feed\nForward\nHidden representations\nN x\nt1 t2 t3 t4 t5\nt1\nt2\nt3\nt4\nt5\nFigure 2: The architecture of lattice-based Transformer\nencoder. Lattice positional encoding is added to the\nembeddings of lattice sequence inputs. Different colors\nin lattice-aware self-attention indicate different relation\nembeddings.\nset, node vi ∈ V denotes the gap between ci and\nci+1, edge ei:j ∈ E departing from vi and arrives\nat vj (i<j ) indicates a possible word or subword\nunit covering subsequence ci+1:j.\nAll the edges in the latticeGare the actual input\ntokens for NMT. For two different edges ei:j and\nep:q, all possible relations can be enumerated as in\nTable 1.\n3.2 Lattice-Based Encoders\nWe place all edges E in the lattice graph into\nan input sequence t1:M = {t1,t2,...,t M } for\nTransformer; then we modify the positional\nencoding to indicate the positional information of\ninput tokens, namely all edges in the lattice graph.\nIn addition, we propose a lattice-aware self-\nattention to directly represent position relationship\namong tokens. The overall architecture is shown\nin Figure 2.\nLattice Positional Encoding (LPE) Original\npositional encoding indicates the order of the\nsequence in an ascending form {p1,p2,...,p M }.\nWe hypothesize that increasing positional\nencodings can indicate the order of sequential\nsentence. As shown in Figure 3, we scan a source\nsequence by element c1:N = {c1,c2,...,c N } (for\nexample, ci is character in Figure 3) and record\ntheir position p1:N = {p1,p2,...,p N }. Then we\nuse the positional encoding of the ﬁrst element in\nlattice edge to represent current token’s position,\nwhich can ensure that every edge in each path\ndeparting from v0 and arriving at vN in lattice will\nv8mao yi fa zhan ju fu zong caiv7v6v5v4v3v2v1v0\nmao-yi:1 fa-zhan:3 ju:5 fu:6 zong-cai:7\nfa-zhan-ju:3mao-yi-fa-zhan:1 fu-zong-cai:6\nv0 v1 v2 v3 v4 v5mao-yi:1 fa-zhan:2 ju:3 fu:4 zong-cai:5\n(1)position encodings\n(2)LPE and LSA\n1 2 3 4 5 6 7 8\nrad inc inc lad pre\nits self lad\nselfsuc suc\nsuc\nrad\nrad\nlad\nind\nFigure 3: Lattice positional encoding pi+1 (in green)\nfor edge ei:j in the lattice graph and the relation\nembeddings r in lattice-aware self-attention based on\nthe timestep of token fa-zhan-ju (in red) and fu (in\npurple).\nhave an increasing positional encoding order. The\nproperty mentioned above is easy to prove, since\nstart and end points vi, vj of each edge ei:j strictly\nsatisfy i < jand next edge ej:k will start from vj\nand thus get a larger positional encoding.\nFormally, for any input token tk, namely edge\nei:j covering elements ci+1:j, positional encoding\npi+1 will be used to represent its position and be\nadded to its embedding.\nLattice-aware Self-Attention (LSA) We also\ndirectly modify self-attention to a lattice-aware\nway which makes self-attention aware of the\nrelations between any two different edges. We\nmodiﬁed Equations (1) and (3) in the same way\nof Shaw et al. (2018) to indicate edge relation:\neij =\n(tiWQ)(tjWK + rK\nij )T\n√\nd\n, (4)\noi =\nM∑\nj=1\nαij(tjWV + rV\nij), (5)\nwhere rK\nij and rV\nij are relation embeddings which\nare added to the keys and values to indicate\nrelation between input tokens ti and tj, namely\nedges ep:q and ek:l in lattice graph, respectively.\nTo facilitate parallel computation, we add an\nadditional embedding ( self) for a token when it\nis conducted dot-product attention with itself, so\nwe train eight (seven in Table 1) different relation\nembeddings aV\n1:8 and aK\n1:8 as look-up table for\nkeys and values, respectively. rK\nij and rV\nij can\nlook up for aV\n1:8 and aK\n1:8 based on the relation\nbetween ti and tj. Figure 3 shows an example of\nembeddings in lattice-aware self-attentions based\non the timestep of token fa-zhan-ju and fu.\nSystem Input MT05 MT02 MT03 MT04 MT06 MT08 ALL\nRNN\nPKU 31.42 34.68 33.08 35.32 31.61 23.58 31.76\nCTB 31.38 34.95 32.85 35.44 31.75 23.33 31.78\nMSR 29.92 34.49 32.06 35.10 31.23 23.12 31.35\nLattice-RNN Lattice 32.40 35.75 34.32 36.50 32.77 24.84 32.95\nTransformer\nPKU 41.67 43.61 41.62 43.66 40.25 31.62 40.24\nCTB 41.87 43.72 42.11 43.58 40.41 31.76 40.35\nMSR 41.17 43.11 41.38 43.60 39.67 31.02 39.87\nTransformer + LPE\nLattice\n42.37 43.71 42.67 44.43 41.14 32.09 40.93↑\nTransformer + LSA 42.28 43.56 42.73 43.81 41.01 32.39 40.77↑\nTransformer + LPE + LSA 42.65 44.14 42.24 44.81 41.37 32.98 41.26↑\nTable 2: Evaluation of translation performance on NIST Zh-En dataset. RNN and Lattice-RNN results are from\n(Su et al., 2017). We highlight the highest BLEU score in bold for each set. ↑ indicates statistically signiﬁcant\ndifference (p<0.01) from best baseline.\nSince self-attention is computed parallelly, we\ngenerate a matrix with all lattice embeddings\nin it for each sentence which can be easily\nincorporated into standard self-attention by\nmatrix multiplication. We use different relation\nembeddings for different Transformer layers but\nshare the same one between different heads in a\nsingle layer.\n4 Experiments\n4.1 Setup\nWe conducted experiments on the NIST Chinese-\nEnglish (Zh-En) and IWSLT 2016 English-\nGerman (En-De) datasets. The Zh-En corpus\nconsists of 1.25M sentence pairs and the En-De\ncorpus consists of 191K sentence pairs. For Zh-\nEn task, we chose the NIST 2005 dataset as the\nvalidation set and the NIST 2002, 2003, 2004,\n2006, and 2008 datasets as test sets. For En-\nDe task, tst2012 was used as validation set and\ntst2013 and tst2014 were used as test sets. For\nboth tasks, sentence pairs with either side longer\nthan 50 were dropped. We used the case-sensitive\n4-gram NIST BLEU score (Papineni et al., 2002)\nas the evaluation metric and sign-test (Collins\net al., 2005) for statistical signiﬁcance test.\nFor Zh-En task, we followed Su et al. (2017)\nto use the toolkit 2 to train segmenters on PKU,\nMSR (Emerson, 2005), and CTB corpora (Xue\net al., 2005), then we generated word lattices\nwith different segmented training data. Both\nsource and target vocabularies are limited to 30K.\nFor En-De task, we adopted 8K, 16K and 32K\n2https://nlp.stanford.edu/software/segmenter.html#Download\nBPE merge operations (Sennrich et al., 2015) to\nget different segmented sentences for building\nsubword lattices. 16K BPE merge operations are\nemployed on the target side.\nWe set batch size to 1024 tokens and\naccumulated gradient 16 times before a back-\npropagation. During training, we set all dropout\nto 0.3 and chose the Adam optimizer (Kingma\nand Ba, 2014) with β1 = 0.9, β2 = 0.98 and\nϵ= 10−9 for parameters tuning. During decoding,\nwe used beam search algorithm and set the beam\nsize to 20. All other conﬁgurations were the same\nwith Vaswani et al. (2017). We implemented our\nmodel based on the OpenNMT (Klein et al., 2017)\nand trained and evaluated all models on a single\nNVIDIA GeForce GTX 1080 Ti GPU.\n4.2 Overall Performance\nFrom Table 2, we see that our LPE and LSA\nmodels both outperform the Transformer baseline\nmodel of 0.58 and 0.42 BLEU respectively. When\nwe combine LPE and LSA together, we get a gain\nof 0.91 BLEU points. Table 3 shows that our\nmethod also works well on the subword level.\nThe base Transformer system has about 90M\nparameters and our LPE and LSA models\nintroduce 0 and 6k parameters over it, respectively,\nwhich shows that our lattice approach improves\nTransformer with little parameter accumulation.\nDuring training, base Transformer performs\nabout 0.714 steps per second while LPE + LSA\nmodel can process around 0.328. As lattice-based\nmethod usually seriously slows down the training,\nour lattice design and implementation over the\nTransformer only shows moderate efﬁciency\nSystem Input tst2012 tst2013 tst2014\nRNN 16k 26.24 28.22 24.17\nTransformer\n8k 27.31 29.56 25.57\n16k 27.35 29.02 25.12\n32k 27.15 28.61 24.88\n+ LPE\nLattice\n27.34 29.48 25.88↑\n+ LSA 27.44 29.73↑ 25.65\n+ LPE + LSA 27.76 30.28↑ 26.22↑\nTable 3: Evaluation of translation performance on\nIWSLT2016 En-De dataset. RNN results are reported\nfrom Morishita et al. (2018). ↑ indicates statistically\nsigniﬁcant difference (p<0.01) from best baseline.\n.\nSystems PE PE + LSA\nALL 40.54 40.90\nTable 4: Translation performance (BELU score) with\nnormal positional encodings and normal positional\nencodings with LSA model on NIST Zh-En dataset.\nreduction.\n4.3 Analysis 3\nEffect of Lattice-Based EncodersTo show the\neffectiveness of our method, we placed all edges\nin the lattice of a single sequence in a relative\nright order based on their ﬁrst character, then we\napplied normal positional encodings (PE) to the\nlattice inputs on our base Transformer model. As\nshown in Table 4, our LPE and LSA method\noutperforms normal positional encodings by 0.39\nand 0.23 BLEU respectively which shows that our\nmethods are effective.\nComplementary of LPE and LSA Our LPE\nmethod allows edges in all paths in an increasing\npositional encoding order which seems to focus\non long-range order but ignore local disorder.\nWhile our LSA method treats all preceding and\nsucceeding edges equally which seems to address\nlocal disorder better but ignore long-range order.\nTo show the complementary of these two methods,\nwe also placed all edges of lattice in a single\nsequence in a relative right order based on their\nﬁrst character and use normal positional encodings\nand our LSA method; we obtained a BLEU\nof 40.90 which is 0.13 higher than single LSA\nmodel. From this, we can see that long-range\nposition information is indeed beneﬁcial to our\nLSA model.\n3All analysis experiments conducted on NIST dataset.\n5 Related Work\nNeural network based methods have been applied\nto several natural language processing tasks (Li\net al., 2018; Zhang et al., 2019; Chen et al., 2018,\n2017; Li et al., 2019; He et al., 2018; Zhou and\nZhao, 2019), especially to NMT (Bahdanau et al.,\n2015; Wang et al., 2017a,b, 2018; Wang et al.,\n2018; Zhang et al., 2018; Zhang and Zhao, 2019).\nOur work is related to the source side\nrepresentations for NMT. Generally, the NMT\nmodel uses the word as a basic unit for source\nsentences modeling. In order to obtain better\nsource side representations and avoid OOV\nproblems, recent research has modeled source\nsentences at character level (Ling et al., 2015;\nCosta-Jussa and Fonollosa, 2016; Yang et al.,\n2016; Lee et al., 2016), subword level (Sennrich\net al., 2015; Kudo, 2018; Wu and Zhao, 2018) and\nmixed character-word level (Luong and Manning,\n2016). All these methods show better translation\nperformance than the word level model.\nAs models mentioned above only use 1-best\nsegmentation as inputs, lattice which can pack\nmany different segmentations in a compact form\nhas been widely used in statistical machine\ntranslation (SMT) (Xu et al., 2005; Dyer\net al., 2008) and RNN-based NMT (Su et al.,\n2017; Sperber et al., 2017). To enhance the\nrepresentaions of the input, lattice has also\nbeen applied in many other NLP tasks such as\nnamed entity recognition (Zhang and Yang, 2018),\nChinese word segmentation (Yang et al., 2019)\nand part-of-speech tagging (Jiang et al., 2008;\nWang et al., 2013).\n6 Conclusions\nIn this paper, we have proposed two methods\nto incorporate lattice representations into\nTransformer. Experimental results in two\ndatasets on word-level and subword-level\nrespectively validate the effectiveness of the\nproposed approaches.\nDifferent from Veli ˇckovi´c et al. (2017), our\nwork also provides an attempt to encode a simple\nlabeled graph into Transformer and can be used in\nany tasks which need Transformer encoder to learn\nsequence representation.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings\nof the 3rd International Conference on Learning\nRepresentations (ICLR 2015).\nDeng Cai and Hai Zhao. 2016. Neural word\nsegmentation learning for Chinese. arXiv preprint\narXiv:1606.04300.\nDeng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin,\nYongjian Wu, and Feiyue Huang. 2017. Fast and\naccurate neural word segmentation for Chinese.\narXiv preprint arXiv:1704.07047.\nKehai Chen, Rui Wang, Masao Utiyama, Lemao Liu,\nAkihiro Tamura, Eiichiro Sumita, and Tiejun Zhao.\n2017. Neural machine translation with source\ndependency representation. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2017), pages 2846–\n2852.\nKehai Chen, Rui Wang, Masao Utiyama, Eiichiro\nSumita, and Tiejun Zhao. 2018. Syntax-directed\nattention for neural machine translation. In\nProceedings of the 32nd AAAI Conference on\nArtiﬁcial Intelligence (AAAI 2018) , pages 4792–\n4799.\nColin Cherry, George Foster, Ankur Bapna, Orhan\nFirat, and Wolfgang Macherey. 2018. Revisiting\ncharacter-based neural machine translation with\ncapacity and compression. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2018), pages 4295–\n4305.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar\nGulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014.\nLearning phrase representations using RNN\nencoder-decoder for statistical machine translation.\narXiv preprint arXiv:1406.1078.\nMichael Collins, Philipp Koehn, and Ivona Ku ˇcerov´a.\n2005. Clause restructuring for statistical machine\ntranslation. In Proceedings of the 43rd Annual\nMeeting of the Association for Computational\nLinguistics (ACL 2005), pages 531–540.\nMarta R Costa-Jussa and Jos ´e AR Fonollosa. 2016.\nCharacter-based neural machine translation. arXiv\npreprint arXiv:1603.00810.\nChristopher Dyer, Smaranda Muresan, and Philip\nResnik. 2008. Generalizing word lattice translation.\nIn Proceedings of the 46th Annual Meeting of the\nAssociation for Computational Linguistics (ACL\n2008), pages 1012–1020.\nThomas Emerson. 2005. The second international\nChinese word segmentation bakeoff. InProceedings\nof the fourth SIGHAN workshop on Chinese\nLanguage Processing, pages 123–133.\nShexia He, Zuchao Li, Hai Zhao, and Hongxiao Bai.\n2018. Syntax for semantic role labeling, to be, or not\nto be. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (ACL\n2018), pages 2061–2071.\nWenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word\nlattice reranking for Chinese word segmentation\nand part-of-speech tagging. In Proceedings of the\n22nd International Conference on Computational\nLinguistics (COLING 2008), pages 385–392.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander M Rush. 2017. Opennmt:\nOpen-source toolkit for neural machine translation.\narXiv preprint arXiv:1701.02810.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple\nsubword candidates. In Proceedings of the\n56th Annual Meeting of the Association for\nComputational Linguistics (ACL 2018) , pages 66–\n75.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2016. Fully character-level neural machine\ntranslation without explicit segmentation. arXiv\npreprint arXiv:1610.03017.\nZuchao Li, Jiaxun Cai, Shexia He, and Hai Zhao. 2018.\nSeq2seq dependency parsing. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics (COLING 2018), pages 3203–3214.\nZuchao Li, Shexia He, Hai Zhao, Yiqing Zhang,\nZhuosheng Zhang, Xi Zhou, and Xiang Zhou. 2019.\nDependency or span, end-to-end uniform semantic\nrole labeling. In Proceedings of the 33rd AAAI\nConference on Artiﬁcial Intelligence (AAAI 2019).\nWang Ling, Isabel Trancoso, Chris Dyer, and Alan W.\nBlack. 2015. Character-based neural machine\ntranslation. arXiv preprint arXiv:1511.04586.\nMinh-Thang Luong and Christopher D. Manning.\n2016. Achieving open vocabulary neural machine\ntranslation with hybrid word-character models. In\nProceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (ACL\n2016), pages 1054–1063.\nBenjamin Marie, Rui Wang, Atsushi Fujita, Masao\nUtiyama, and Eiichiro Sumita. 2018. Nict’s neural\nand statistical machine translation systems for the\nwmt18 news translation task. In Proceedings of the\nThird Conference on Machine Translation, Volume\n2: Shared Task Papers, pages 453–459.\nMakoto Morishita, Jun Suzuki, and Masaaki Nagata.\n2018. Improving neural machine translation by\nincorporating hierarchical subword features. In\nProceedings of the 27th International Conference on\nComputational Linguistics (COLING 2018) , pages\n618–629.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings\nof the 40th Annual Meeting of the Association for\nComputational Linguistics (ACL 2002), pages 311–\n318.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position\nrepresentations. arXiv preprint arXiv:1803.02155.\nMatthias Sperber, Graham Neubig, Jan Niehues, and\nAlex Waibel. 2017. Neural lattice-to-sequence\nmodels for uncertain inputs. arXiv preprint\narXiv:1704.00559.\nJinsong Su, Zhixing Tan, Deyi Xiong, Rongrong\nJi, Xiaodong Shi, and Yang Liu. 2017. Lattice-\nbased recurrent neural network encoders for neural\nmachine translation. In Proceedings of the 31st\nAAAI Conference on Artiﬁcial Intelligence (AAAI\n2017), pages 3302–3308.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le.\n2014. Sequence to sequence learning with neural\nnetworks. In Proceedings of the 28th Conference\non Neural Information Processing Systems (NIPS\n2014), pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31stst Conference\non Neural Information Processing Systems (NIPS\n2017), pages 5998–6008.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017. Graph attention networks. arXiv preprint\narXiv:1710.10903.\nRui Wang, Andrew Finch, Masao Utiyama, and\nEiichiro Sumita. 2017a. Sentence embedding for\nneural machine translation domain adaptation. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (ACL\n2017), pages 560–566.\nRui Wang, Masao Utiyama, Andrew Finch, Lemao\nLiu, Kehai Chen, and Eiichiro Sumita. 2018.\nSentence selection and weighting for neural\nmachine translation domain adaptation. IEEE/ACM\nTransactions on Audio, Speech, and Language\nProcessing, 26:1727–1741.\nRui Wang, Masao Utiyama, Lemao Liu, Kehai Chen,\nand Eiichiro Sumita. 2017b. Instance weighting for\nneural machine translation domain adaptation. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2017), pages 1482–1488.\nRui Wang, Masao Utiyama, and Eiichiro Sumita. 2018.\nDynamic sentence sampling for efﬁcient training\nof neural machine translation. In Proceedings of\nthe 56th Annual Meeting of the Association for\nComputational Linguistics (ACL 2018), pages 298–\n304.\nZhiguo Wang, Chengqing Zong, and Nianwen Xue.\n2013. A lattice-based framework for joint Chinese\nword segmentation, POS tagging and parsing. In\nProceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics (ACL\n2013), pages 623–627.\nYingting Wu and Hai Zhao. 2018. Finding\nbetter subword segmentation for neural machine\ntranslation. In The Seventeenth China National\nConference on Computational Linguistics (CCL\n2018), pages 53–64.\nJia Xu, Evgeny Matusov, Richard Zens, and Hermann\nNey. 2005. Integrated Chinese word segmentation\nin statistical machine translation. In International\nWorkshop on Spoken Language Translation (IWSLT\n2005).\nNaiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta\nPalmer. 2005. The Penn Chinese TreeBank: Phrase\nstructure annotation of a large corpus. Natural\nlanguage engineering, 11(2):207–238.\nJie Yang, Yue Zhang, and Shuailong Liang. 2019.\nSubword encoding in lattice LSTM for Chinese\nword segmentation. In Proceedings of the 17th\nAnnual Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies (NAACL 2019).\nZhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2016.\nA character-aware encoder for neural machine\ntranslation. In Proceedings of the 26th International\nConference on Computational Linguistics (COLING\n2016), pages 3063–3070.\nHuan Zhang and Hai Zhao. 2019. Minimum\ndivergence vs. maximum margin: An empirical\ncomparison on seq2seq models. In Proceedings of\nthe Seventh International Conference on Learning\nRepresentations (ICLR 2019).\nYue Zhang and Jie Yang. 2018. Chinese NER using\nlattice LSTM. In Proceedings of the 56th Annual\nMeeting of the Association for Computational\nLinguistics (ACL 2018), pages 1554–1564.\nZhisong Zhang, Rui Wang, Masao Utiyama,\nEiichiro Sumita, and Hai Zhao. 2018. Exploring\nrecombination for efﬁcient decoding of neural\nmachine translation. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2018) , pages\n4785–4790.\nZhuosheng Zhang, Yafang Huang, and Hai Zhao.\n2019. Neural-based pinyin-to-character conversion\nwith adaptive vocabulary. In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics (ACL 2019).\nHai Zhao, Deng Cai, Changning Huang, and Chunyu\nKit. 2019. Chinese word segmentation: Another\ndecade review (2007-2017). arXiv preprint\narXiv:1901.06079.\nHai Zhao and Chunyu Kit. 2011. Integrating\nunsupervised and supervised word segmentation:\nThe role of goodness measures. Information\nSciences, 181(1):163–183.\nHai Zhao, Masao Utiyama, Eiichiro Sumita, and\nBao-Liang Lu. 2013. An empirical study on word\nsegmentation for Chinese machine translation.\nIn International Conference on Intelligent\nText Processing and Computational Linguistics\n(CICLing 2013), pages 248–263.\nJunru Zhou and Hai Zhao. 2019. Head-driven phrase\nstructure grammar parsing on Penn Treebank. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics (ACL\n2019).",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.8041447401046753
    },
    {
      "name": "Machine translation",
      "score": 0.7822592258453369
    },
    {
      "name": "Computer science",
      "score": 0.7175711393356323
    },
    {
      "name": "Transformer",
      "score": 0.6116867065429688
    },
    {
      "name": "Vocabulary",
      "score": 0.5105839371681213
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5078757405281067
    },
    {
      "name": "Lattice (music)",
      "score": 0.5025932788848877
    },
    {
      "name": "Natural language processing",
      "score": 0.49401384592056274
    },
    {
      "name": "Speech recognition",
      "score": 0.41203171014785767
    },
    {
      "name": "Algorithm",
      "score": 0.3361203670501709
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3212054669857025
    },
    {
      "name": "Linguistics",
      "score": 0.12426126003265381
    },
    {
      "name": "Engineering",
      "score": 0.08481550216674805
    },
    {
      "name": "Voltage",
      "score": 0.0775737464427948
    },
    {
      "name": "Electrical engineering",
      "score": 0.07363590598106384
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}