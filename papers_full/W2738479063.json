{
  "title": "ssEMnet: Serial-Section Electron Microscopy Image Registration Using a Spatial Transformer Network with Learned Features",
  "url": "https://openalex.org/W2738479063",
  "year": 2017,
  "authors": [
    {
      "id": null,
      "name": "Yoo, Inwan",
      "affiliations": [
        "Ulsan National Institute of Science and Technology"
      ]
    },
    {
      "id": null,
      "name": "Hildebrand, David G. C.",
      "affiliations": [
        "Rockefeller University"
      ]
    },
    {
      "id": null,
      "name": "Tobin, Willie F.",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A4323217810",
      "name": "Lee, Wei-Chung Allen",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A4287299692",
      "name": "Jeong, Won-Ki",
      "affiliations": [
        "Ulsan National Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1548529611",
    "https://openalex.org/W2147762902",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W2157151515",
    "https://openalex.org/W2016323639",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2026140461",
    "https://openalex.org/W2344270078",
    "https://openalex.org/W4300790193",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1998710995",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2307899896"
  ],
  "abstract": null,
  "full_text": "ssEMnet: Serial-section Electron Microscopy\nImage Registration using a Spatial Transformer\nNetwork with Learned Features\nInwan Yoo1, David G. C. Hildebrand 2, Willie F. Tobin3,\nWei-Chung Allen Lee3 and Won-Ki Jeong1\nUlsan National Institute of Science and Technology 1\nThe Rockefeller University2\nHarvard Medical School3\nE-mail: iwyoo@unist.ac.kr\nAbstract. The alignment of serial-section electron microscopy (ssEM)\nimages is critical for eÔ¨Äorts in neuroscience that seek to reconstruct neu-\nronal circuits. However, each ssEM plane contains densely packed struc-\ntures that vary from one section to the next, which makes matching fea-\ntures across images a challenge. Advances in deep learning has resulted\nin unprecedented performance in similar computer vision problems, but\nto our knowledge, they have not been successfully applied to ssEM image\nco-registration. In this paper, we introduce a novel deep network model\nthat combines a spatial transformer for image deformation and a convo-\nlutional autoencoder for unsupervised feature learning for robust ssEM\nimage alignment. This results in improved accuracy and robustness while\nrequiring substantially less user intervention than conventional methods.\nWe evaluate our method by comparing registration quality across several\ndatasets.\n1 Introduction\nAmbitious eÔ¨Äorts in neuroscience‚Äîreferred to as ‚Äúconnectomics‚Äù‚Äîseek to\ngenerate comprehensive brain connectivity maps. This Ô¨Åeld utilizes the high res-\nolution of electron microscopy (EM) to resolve neuronal structures such as den-\ndritic spine necks and synapses, which are only tens of nanometers in size [5].\nA standard procedure for obtaining such datasets is cutting brain tissue into\n30‚àí50 nm-thick sections (e.g. ATUM [4]), acquiring images with 2 ‚àí5 nm lat-\neral resolution for each section, and aligning two-dimensional (2D) images into\nthree-dimensional (3D) volumes. Though the tissue is chemically Ô¨Åxed and em-\nbedded in epoxy resin to preserve ultrastructure, several deformations occur\nin this serial-section EM (ssEM) process. These include tissue shrinkage, com-\npression or expansion during sectioning, and warping from sample heating or\ncharging due to the electron beam. Overcoming such non-linear distortions are\nnecessary to reproduce a 3D image volume in a state as close as possible to the\noriginal biological specimen. Therefore, excellent image alignment is an impor-\ntant prerequisite for subsequent analysis.\narXiv:1707.07833v2  [cs.CV]  5 Dec 2017\n2 Authors Suppressed Due to Excessive Length\nSigniÔ¨Åcant research eÔ¨Äorts in image registration have been made to address\nmedical imaging needs. However, ssEM image registration remains challenging\ndue to its image characteristics: large and irregular tissue deformations with\nartifacts such as dusts and folds, drifting for long image sequences alignment,\nand diÔ¨Éculty in Ô¨Ånding the optimal alignment parameters. Several open-source\nssEM image registration tools are available, such as bUnwarpJ [1] and Elastic\nalignment [9] (available via TrakEM2 [2]). They partially address the above\nissues, but some of them still remain, such as lack of global regularization and\ncomplicated parameter tuning.\nOur work is motivated by recent advances in deep neural networks. Convo-\nlutional neural networks (CNNs) and their variants have shown unprecedented\npotential by largely outperforming conventional computer vision algorithms us-\ning hand-crafted feature descriptors, but their application to ssEM image regis-\ntration has not been explored. Wu et al. [10] used a 3D autoencoder to extract\nfeatures from MRI volumes, which are then combined with a conventional sparse,\nfeature-driven registration method. Recent work by Jaderberg et al. [6] on the\nspatial transformer network (STN) uses a diÔ¨Äerentiable network module inside\na CNN to overcome the drawbacks of CNNs (i.e., lack of scale- and rotation-\ninvariance). Another interesting application of deep neural networks is energy\noptimization using backpropagation, as shown in the neural artistic style transfer\nproposed by Gatys et al. [3].\nInspired by these studies, we propose a novel deep network model that is\nspeciÔ¨Åcally designed for ssEM image registration. The proposed model is a novel\ncombination of an STN and a convolutional autoencoder that generates a de-\nformation map (i.e., vector map) for the entire image alignment via backprop-\nagation of the network. We propose a feature-based image similarity measure,\nwhich is learned from the training images in an unsupervised fashion by the\nautoencoder. Unlike other conventional hand-crafted features, such as SIFT and\nblock-matching, the learned features used in our method signiÔ¨Åcantly reduce\nthe required user parameters and make the method easy to use and less error-\nprone. To the best of our knowledge, this is the Ô¨Årst data-driven ssEM image\nregistration method based on deep learning, which can easily extend to various\napplications by employing diÔ¨Äerent feature encoding networks.\n2 Method\n2.1 Feature Generation using a Convolutional Autoencoder\nTo compute similarities between adjacent EM sections, we generate data-\ndriven features via a convolutional autoencoder, which consists of 1) a convolu-\ntional encoder comprised of convolutional layers with ReLU activations and 2)\na deconvolutional decoder comprised of deconvolutional layers with ReLU ac-\ntivations that were symmetrical to the encoder without fully connected layers.\nTherefore, our method is applicable to any sized dataset (i.e., the network size is\nnot constrained to the input data size). Our autoencoder can be formally deÔ¨Åned\nssEM Image Registration using a STN with Learned Features 3\nas follows:\nh= fŒ∏(x) (1)\ny= gœÜ(h) (2)\nLŒ∏,œÜ =\nN‚àë\ni=1\n||xi ‚àíyi||2\n2 + Œª(\n‚àë\nk\n||Œ∏k||2\n2 +\n‚àë\nk\n||œÜk||2\n2) (3)\nwhere fŒ∏ and gœÜ are the encoder and the decoder and Œ∏ and œÜ are their param-\neters, respectively. The loss function (Eq. 3) consists of the reconstruction term\nminimizing the diÔ¨Äerence between the input and output images and the regu-\nlarization terms minimizing the ‚Ñì2-norm of the weights of the network to avoid\noverÔ¨Åtting. Fig. 1 shows that our autoencoder feature-based registration gener-\nates more accurate results compared to the conventional pixel intensity-based\nregistration, i.e., (d) shows the smaller normalized cross correlation (NCC) error\nbetween aligned images than (c).\n(a) (b) (c) (d)\nFig. 1.Comparison between the pixel intensity-based and the autoencoder feature-\nbased registration with backpropagation. (a) the Ô¨Åxed image, (b) the moving image,\n(c) the heat map of NCC of the pixel intensity-based registration result (NCC : 0.1670),\nand (d) the heat map of NCC of the autoencoder feature-based registration (NCC :\n0.28) in red box region.\n2.2 Deformable Image Registration using a Spatial Transformer\nNetwork\nUpon completion of autoencoder training, a spatial transformer (ST) mod-\nule T is attached to the front half (i.e., encoder) to form the proposed spatial\ntransformer network (see Fig. 2, refer to [6] for the details of the ST module).\nThis design is intended to Ô¨Ånd the proper deformation of the input image via\nan ST by minimizing the registration error measured by the pre-trained autoen-\ncoder. The objective function for registration errors between the reference and\nthe moving images is formulated as Eq. 4. The reference image I1 is Ô¨Åxed, and\nthe moving image I0 is deformed by the ST with the corresponding vector map\nv. Notably, the resolution of vector map v is usually coarser than that of the\ninput image. Therefore, we need smooth interpolation of a coarse vector map to\nobtain a per-pixel moving vector for actual deformation of the moving image. A\nthin plate spline (TPS) was used in the original STN for a smooth deformable\n4 Authors Suppressed Due to Excessive Length\nST\nI0\nIi\nv\nFeedforward\nBackpropagation\n(Shared weights)\nùëìŒ∏ ùëîùúô\n‚Ñé = ùëìŒ∏(ùë•)\nùë• ùë¶\n‚Ä¶ ùëìŒ∏(ùêº0)\n‚Ä¶ ùëìŒ∏(ùêºùëñ)\nCAE\nFig. 2.The overview of our method. The upper right dashed box represents the pre-\ntrained convolution autoencoder (CAE). The alignment is processed by backpropaga-\ntion with loss of autoencoder features.\ntransform, but other interpolation schemes, such as bilinear, bicubic, B-spline,\netc., can be used as well. In our experiment, bilinear interpolation produced\nbetter results with Ô¨Åner deformation compared to the TPS.\nLv(I0,I1) = ||fŒ∏(I1) ‚àífŒ∏(Tv(I0))||2\n2 + Œ±||v||2\n2 + Œ≤||‚àávx||2\n2 + Œ≥||‚àávy||2\n2 (4)\nThe Ô¨Årst term of Eq. 4 measures how two images are contextually diÔ¨Äerent via\na trained autoencoder. We assumed that if the encoded features of two images\nare similar, then the images themselves are also similar and well-aligned. The rest\nof the terms in Eq. 4 reÔ¨Çect the regularization of vector map v, which penalizes\nlarge deformation while promoting smooth variation of the vector map, andŒ±, Œ≤,\nŒ≥are their corresponding weights. Because every layer is diÔ¨Äerentiable, including\nan ST, we directly optimize v by backpropagation with a chain rule, in which\nonly v is updated and the weights in the autoencoder are Ô¨Åxed. We used the\nADAM optimizer [7] for all our experiments.\nThe objective function using only adjacent image pairs could be vulnerable to\nimaging artifacts, which may result in drifting due to error accumulation when\nmany sections are aligned. To increase the robustness of alignment, we extend the\nobjective function (Eq. 4) to leverage multiple neighbor sections. Let the moving\nimage be I0, its neighbor reference nimages be I1 to In, and their corresponding\nweights be wi. The proposed objective function (Eq. 5) combines the registration\nerrors across neighbor images, which can lessen strong registration errors from\nimages with artifacts and avoid large deformation.\nTo accumulate the registration error only within the image after deformation,\nwe applied the empty space mask that represents the empty area outside the\nimage. After image deformation, we collect the pixels outside the valid image\nssEM Image Registration using a STN with Learned Features 5\nregion and make a binary mask image. We resize this mask image to match the\nsize of the autoencoder feature map using a bilinear interpolation (shown as\nM(T) in Eq. 5). Based on this objective function, the alignment of many EM\nsections is possible in an out-of-core fashion using a sliding-window method.\nLv(I0,...,I n) =\nn‚àë\ni=1\nwiM(Tv)||fŒ∏(Ii)‚àífŒ∏(Tv(I0))||2\n2+Œ±||v||2\n2+Œ≤||‚àávx||2\n2+Œ≥||‚àávy||2\n2\n(5)\nWe also developed a technique for handling images with dusts and folds.\nBecause the feature errors are high in the corrupted regions, we selectively ignore\nsuch regions during the optimization, which we call loss drop. This is similar to\napplying an empty space mask except that pixel selection is based on feature\nerror. In our implementation, we Ô¨Årst dropped the top 50% of high error features,\nand then reduced the dropping rate by half per every iteration. By doing this, we\neÔ¨Äectively prevented local minimums and obtained smoother registration results.\n3 Results\nWe implemented our method using TensorFlow, and used a GPU workstation\nequipped with an NVIDIA Titan X GPU. We used three EM datasets: trans-\nmission EM (TEM) images of Drosophila brain, human-labeled TEM images of\nanother Drosophila brain provided by CREMI challenge1 (those two Drosophila\nimages are collected independently on separate imaging systems), and mouse\nbrain scanning EM (SEM) images with fold artifacts. We used two convolu-\ntional autoencoders: one is a deeper network (as shown in Fig. 2) with 3 √ó3\nÔ¨Ålters used for the Drosophila TEM datasets, and the other is a shallower net-\nwork with a larger Ô¨Ålter size (i.e., 6 layers with 7 √ó7 Ô¨Ålters) used for the mouse\nSEM dataset. In bUnwarpJ and elastic alignment experiments, we performed\nvarious experiments to Ô¨Ånd the optimal parameters and selected the parameters\nthat gave the best results.\nDrosophila TEM data The original volumetric dataset comprises the ante-\nrior portion of an adult female Drosophila melanogasterbrain cut in the frontal\nplane. Each section was acquired at 4 √ó4 √ó40 nm3vx‚àí1, amounting to 4 mil-\nlion camera images and 50 TB of raw data. The original large-scale dataset\nwas aligned with AlignTK (http://mmbios.org/aligntk-home) requiring exten-\nsive human eÔ¨Äort and supercomputing resources. Although the alignment was\nsuÔ¨Écient for manual tracing of neurons, it must be improved for accurate and\neÔ¨Écient automated segmentation approaches. Small volumes (512 √ó512 √ó47)\nwere exported for re-alignment centered around synapses of identiÔ¨Åed connec-\ntions between olfactory receptor neurons and second order projection neurons in\nthe antennal lobe. Fig. 3 shows the result of our registration method. Fig. 3 left\nis the oblique (i.e., not axis-aligned) cross-sectional view of the original stack.\nDue to inaccurate pre-alignment, some discontinuous membranes are shown (see\nthe red circle areas), which are corrected in the aligned result using our method\n(Fig. 3 right).\n1 https://cremi.org/\n6 Authors Suppressed Due to Excessive Length\nFig. 3.Drosophila melanogaster TEM dataset. Left : Pre-aligned result. Right : After\nregistration using our method.\n(a) (b) (c)\nFig. 4.Vertical view of the alignment result of the randomly deformed CREMI dataset.\n(a) bUnwarpJ, (b) elastic alignment, and (c) our method. Each neuron is assigned a\nunique color.\nLabeled Drosophila TEM data from CREMI challengeTo quantitatively\nassess the registration quality, we used a small sub-volume (512 √ó512 √ó31) of\nregistered and labeled TEM data from the CREMI challenge as a ground-truth.\nWe Ô¨Årst randomly deformed both the raw and the labeled images using a TPS\ndeÔ¨Åned by random vectors on random positions. The random positions were\nuniformly distributed in space, and the random vectors were sampled from the\nnormal distribution with a zero mean value. Then we performed image registra-\ntion using three methods (bUnwarpJ, elastic alignment and our method). Fig. 4\nshows the vertical cross section of each result. The bUnwarpJ result shows large\ndeformation (i.e., drifting) across stacks (see the black regions on both sides). Al-\nthough elastic alignment and our method show less deformation but our method\nclearly shows more accurate vertical membrane alignment. To quantitatively\nmeasure the registration accuracy, we selected the 50 largest neurons and calcu-\nlated the average Dice coeÔ¨Écient for each result, which came to 0.60, 0.73, and\n0.83 for bUnwarpJ, elastic, and our method, respectively. This result shows that\nour registration method is more robust and resilient to random deformation.\nssEM Image Registration using a STN with Learned Features 7\n(a) (b)\n(c) (d)\n(a) (b) (c) (d)\n0.1995 0.3562 0.2931 0.4305NCC\nFig. 5.Visual comparison of mouse ssEM image registration results. (a) before align-\nment, (b) bUnwarpJ, (c) elastic alignment, and (d) our method. The red box is the\nregion near the folds (shown as black spots). The below table shows NCC of the in-\nner region in each aligned result (black backgroud regions are not counted for NCC\ncomputation)\n.\nMouse lateral geniculate nucleus SEM data with fold artifactsWe next\nsought to assess the applicability of our new alignment method to data acquired\nfrom diÔ¨Äerent EM imaging methods, using diÔ¨Äerent model organisms, and con-\ntaining fold artifacts. A small volume (1520 √ó2500 √ó100) was selected from\na mouse lateral geniculate nucleus dataset generously provided by the Licht-\nman laboratory [8]. This dataset was acquired using SEM with a resolution of\n4 √ó4 √ó30 nm3vx‚àí1, and contains folds caused by cracks in the substrate onto\nwhich sections were collected. Fig. 5 shows the vertical cross section of the reg-\nistration results as compared to conventional registration methods. The overall\nregistration quality of our method is higher than those of other methods, as in-\ndicated by clearer neuronal structures with low deformation. In particular, the\nred box shows a region containing warping due to folds, where our method is\nable to produce a smoother and more continuous result than others.\n8 Authors Suppressed Due to Excessive Length\n4 Discussion and Conclusion\nOne problem with the convolution operator is that it is neither scale- nor\nrotation-invariant. We addressed this problem by generating features on the de-\nformed image in every iteration and dynamically calculating feature diÔ¨Äerences.\nOur method is slow due to the nature of learning algorithms, but the parameter\ntuning is much easier than existing methods, which makes it practically useful.\nIn this paper, we introduced a novel deep network for ssEM image registra-\ntion that is easier to use and robust to imaging artifacts. The proposed method\nis a general learning-based registration model that can easily extend to vari-\nous applications by modifying the network. Improving running time via parallel\nsystems and deploying our method on tera-scale EM stacks would be an inter-\nesting and important future research direction. We also plan to employ various\ninterpolation schemes and feature encoding networks in the future.\nAcknowledgements. This work is partially supported by the Basic Science\nResearch Program through the National Research Foundation of Korea funded\nby the Ministry of Education (NRF-2017R1D1A1A09000841) and the Software\nConvergence Technology Development Program through the Ministry of Science,\nICT and Future Planning (S0503-17-1007).\nReferences\n1. Arganda-Carreras, I., Sorzano, C.O., Marabini, R., Carazo, J.M., Ortiz-de\nSolorzano, C., Kybic, J.: Consistent and elastic registration of histological sec-\ntions using vector-spline regularization. In: International Workshop on Computer\nVision Approaches to Medical Image Analysis. pp. 85‚Äì95. Springer (2006)\n2. Cardona, A., Saalfeld, S., Schindelin, J., Arganda-Carreras, I., Preibisch, S., Lon-\ngair, M., Tomancak, P., Hartenstein, V., Douglas, R.J.: TrakEM2 software for\nneural circuit reconstruction. PloS one 7(6), e38011 (2012)\n3. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional\nneural networks. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. pp. 2414‚Äì2423 (2016)\n4. Hayworth, K.J., Morgan, J.L., Schalek, R., Berger, D.R., Hildebrand, D.G.C.,\nLichtman, J.W.: Imaging ATUM ultrathin section libraries with WaferMapper:\na multi-scale approach to EM reconstruction of neural circuits. Frontiers in Neural\nCircuits 8(June), 68 (2014)\n5. Helmstaedter, M.: Cellular-resolution connectomics: challenges of dense neural cir-\ncuit reconstruction. Nature methods 10(6), 501‚Äì507 (2013)\n6. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks.\nIn: Advances in Neural Information Processing Systems. pp. 2017‚Äì2025 (2015)\n7. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n8. Morgan, J.L., Berger, D.R., Wetzel, A.W., Lichtman, J.W.: The fuzzy logic of\nnetwork connectivity in mouse visual thalamus. Cell (1), 192‚Äì206 (2017/02/24)\n9. Saalfeld, S., Fetter, R., Cardona, A., Tomancak, P.: Elastic volume reconstruction\nfrom series of ultra-thin microscopy sections. Nature methods 9(7), 717‚Äì720 (2012)\n10. Wu, G., Kim, M.J., Wang, Q., Munsell, B., Shen, D.: Scalable high performance\nimage registration framework by unsupervised deep feature representations learn-\ning (2015)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8196301460266113
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7675939202308655
    },
    {
      "name": "Autoencoder",
      "score": 0.6194818019866943
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.575904369354248
    },
    {
      "name": "Transformer",
      "score": 0.559738039970398
    },
    {
      "name": "Computer vision",
      "score": 0.5376135110855103
    },
    {
      "name": "Deep learning",
      "score": 0.5212536454200745
    },
    {
      "name": "Image registration",
      "score": 0.5105825066566467
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44151943922042847
    },
    {
      "name": "Convolutional neural network",
      "score": 0.43842098116874695
    },
    {
      "name": "Image (mathematics)",
      "score": 0.24073660373687744
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}