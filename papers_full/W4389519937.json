{
    "title": "Are Language Models Worse than Humans at Following Prompts? It’s Complicated",
    "url": "https://openalex.org/W4389519937",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3092042777",
            "name": "Albert Webson",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Alyssa Loo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2439114871",
            "name": "Qinan Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2013784948",
            "name": "Ellie Pavlick",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3003804331",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4285594979",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4306294746",
        "https://openalex.org/W2042492008",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "Prompts have been the center of progress in advancing language models’ zero-shot and few-shot performance. However, recent work finds that models can perform surprisingly well when given intentionally irrelevant or misleading prompts. Such results may be interpreted as evidence that model behavior is not “human like’. In this study, we challenge a central assumption in such work: that humans would perform badly when given pathological instructions. We find that humans are able to reliably ignore irrelevant instructions and thus, like models, perform well on the underlying task despite an apparent lack of signal regarding the task they are being asked to do. However, when given deliberately misleading instructions, humans follow the instructions faithfully, whereas models do not. Thus, our conclusion is mixed with respect to prior work. We argue against the earlier claim that high performance with irrelevant prompts constitutes evidence against models’ instruction understanding, but we reinforce the claim that models’ failure to follow misleading instructions raises concerns. More broadly, we caution that future research should not idealize human behaviors as a monolith and should not train or evaluate models to mimic assumptions about these behaviors without first validating humans’ behaviors empirically.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7662–7686\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAre Language Models Worse than Humans at Following Prompts?\nIt’s Complicated\nAlbert Websonκϕ*, Alyssa Marie Looκλ*, Qinan Yuκ*, and Ellie Pavlickκλ\nκDepartment of Computer Science ϕDepartment of Philosophy\nλProgram in Linguistics\nBrown University\nAbstract\nPrompts have been the center of progress in\nadvancing language models’ zero-shot and few-\nshot performance. However, recent work finds\nthat models can perform surprisingly well when\ngiven intentionally irrelevant or misleading\nprompts. Such results may be interpreted as ev-\nidence that model behavior is not “human like”.\nIn this study, we challenge a central assumption\nin such work: that humans would perform badly\nwhen given pathological instructions. We find\nthat humans are able to reliably ignore irrele-\nvant instructions and thus, like models, perform\nwell on the underlying task despite an apparent\nlack of signal regarding the task they are being\nasked to do. However, when given deliberately\nmisleading instructions, humans follow the in-\nstructions faithfully, whereas models do not.\nOur findings caution that future research should\nnot idealize human behaviors as a monolith and\nshould not train or evaluate models to mimic\nassumptions about these behaviors without first\nvalidating humans’ behaviors empirically.\n1 Introduction\nPrompting has emerged as the default way of using\nlarge language models (Brown et al., 2020; Sanh\net al., 2022; Wei et al., 2021; Ouyang et al., 2022;\nChung et al., 2022). However, a collection of re-\ncent papers show that models perform surprisingly\nwell when given misleading or irrelevant prompts\n(Webson and Pavlick, 2022; Prasad et al., 2022;\nKhashabi et al., 2021), corrupted in-context exam-\nples (Min et al., 2022), or corrupted explanations\nor chain-of-thought (Madaan and Yazdanbakhsh,\n2022; Ye and Durrett, 2022). Such results raise\nquestions about whether language models’ ability\nto follow instructions is analogous to humans’ abil-\nity to do so.\nIn this paper, we investigate what humans do in\nsuch settings. We follow the experimental setup\n∗Equal contribution. Correspondence to papers@aw.nyc.\nused by Webson and Pavlick (2022) (W&P here-\nafter) to study how instructions in prompts affect\nLMs’ performance. W&P manually write a set of\nprompts for various natural language inference\n(NLI) and coreference resolution datasets. These\nprompts cover three main categories: instructive,\nirrelevant, and misleading. For example, Given\n{sentence1} , can we assume it is\ntrue that {sentence2} is an instruc-\ntive prompt for NLI, whereas {sentence1}\nDoes the above passage express a\npositive sentiment? {sentence2} is\na misleading instruction for NLI. (See Table 1 for\nfull definitions and examples.)\nW&P assume that, if models perform held-out\ntasks by reading prompts as instructions in the way\nthat humans (are assumed to) do, their performance\nwith instructive prompts should be much higher\nthan their performance with other pathological cat-\negories of prompts, namely:\ninstructive >misleading (A1)\ninstructive >irrelevant (A2)\ninstructive >null (no instruction) (A3)\nInstead, W&P find that T5 (LM-Adapted, 11B,\nLester et al., 2021), T0 (11B, Sanh et al., 2022), and\nGPT-3 (175B, Brown et al., 2020) do not exhibit the\nabove patterns. Rather, in both zero-shot and few-\nshot settings, models perform roughly the same\non instructive, misleading, and irrelevant prompts,\nviolating A1 and A2 above. Models do, however,\nperform better given any type of instructions than\nthey do with no instructions (i.e., A3 holds). There-\nfore, W&P conclude that while prompts do confer\nsubstantial empirical benefits, the fact that models\nare so good at inferring the gold labels under vari-\nous pathological prompts casts doubts on whether\nmodels understand or use instructions in ways sim-\nilar to how humans do.\nIn this paper, we revisit W&P’s assumptions on\nhow humans behave with pathological prompts.\n7662\nCategory Description Examples\ninstructiveHow we would describe the NLI task\nto a human who has never seen the task before.\n{sentence1} Are we justified in saying that “{sentence2}”?\nGiven {sentence1} Should we assume that “{sentence2}” is true?\nmisleadingInstruct the models to perform a task unrelated\nto NLI.\n{sentence1} is the sentiment positive? {sentence2}\n{sentence1} is this a sports news? {sentence2}\nirrelevant Concatenate the premise, a sentence unrelated\nto any NLP task, and the hypothesis.\n{sentence1} If bonito flakes boil more than a few seconds\nthe stock becomes too strong. \"{sentence2}\"?\nnull Concatenate the premise and the hypothesis\nwithout any additional text.\n{premise} {hypothesis}\n{sentence2}{sentence1}\nTable 1: Prompt categories adapted from W&P, with W&P’s two misleading categories collapsed into one ‘mislead-\ning’ category for clarity. See Table 3 for the full list.\nWe use the same experimental design but adapt it\nfor measuring human behaviors. In the zero-shot\nsetting, we find that while assumptions A1 and A3\nare consistent with human behaviors, A2 is not.\nOur experiments underscore the importance\nof validating assumptions about human behavior\non natural language tasks since, frequently, re-\nsearchers’ intuitions about human behavior do not\nbear out in practice, and that extra care should be\ntaken in designing a fair comparison between mod-\nels and humans (Pavlick and Kwiatkowski, 2019;\nDasgupta et al., 2022; Lampinen, 2022).\n2 Experiment\nOverview Following W&P, we use natural lan-\nguage inference (NLI) as the primary task for our\nexperiments. (That is, in our results, we always re-\nport human and model performance with respect to\nthe NLI task.) When necessary, in designing stimuli\nfor the misleading prompt condition (discussed in\ndetail below), we additionally draw examples from\n7 other tasks: lexical overlap, lexical identity, para-\nphrasing identification, grammatical acceptability,\nsummarization acceptability, topic classification,\nand language identification (see Appendices H.2\nand H.3 for details); we refer to these collectively\nas surface tasks in this paper.\nWe define an example to be a pair of\n<sentence1, sentence2>. For NLI, sen-\ntences 1 and 2 are the premise and hypoth-\nesis, respectively. 1 We define an item as a\nunique 3-tuple <sentence1, instruction,\nsentence2>, i.e., an example fitted within a\nprompt template, which can be instructive (w.r.t.\nNLI), misleading, irrelevant, or empty (null).2\n1For other tasks, if they do not need a sentence pair (e.g.,\nsentiment analysis), our instructions always unambiguously\nask for judgment of sentence1.\n2W&P further differentiate moderately misleading from\nextremely misleading instructions. However, for our purposes,\nwe collapse this distinction, except where discussed in §A.2.\nCrucially, when the instruction is misleading,\nwe manually select examples such that <sent1,\nmisleading instruction, sent2> and\n<sent1, NLI instruction, sent2> al-\nways have opposite gold labels—see Figure 1. As-\nsuming participants are competent at NLI as well\nas at each of the relevant surface tasks3 used in our\nexperiments, this design enables us to distinguish\nwhether the participant is performing the NLI task\nor surface task when given misleading instructions.\nProcedure To ensure this experiment is as zero-\nshot as possible, each participant receives only one\ntest item, followed by four additional items which\nwe use as controls to ensure that all tasks and exam-\nples are fair (Appendix E). For each item, subjects\nchoose between“Yes\" or “No\". They do not receive\nany feedback throughout the experiment.\nExample Selection Because our main goal is to\nmeasure the effect of instructions and not humans’\nperformance on the tasks themselves per se, we\nmanually select examples that are as easy and un-\nambiguous as possible. For the instructive, irrele-\nvant, and null conditions, we choose examples from\nRTE (Dagan et al., 2006) and MNLI (Williams\net al., 2018). For the misleading condition, we man-\nually select all examples to ensure that the NLI\nlabels differ from the misleading task labels. A full\ndescription of how we curate examples is detailed\nin Appendix H.\nInstruction Templates We select and lightly\nedit4 22 of the 27 prompts used in W&P for testing.\nThe complete prompts are listed in Appendix H.\nCombined with examples, there are a total of 194\nunique items in our test condition (Table 2). Each\nitem is assigned to a minimum of three annotators.\nParticipants We conducted our study on Ama-\nzon Mechanical Turk, receiving a total of 597 re-\n3See Appendix E for a detailed discussion and analysis of\ncontrol conditions which we take to be convincing evidence\n7663\n(a) Instructive Condition: Baseline\nwhere the surface instruction is the\nsame as NLI.\n(b) Misleading Condition: Surface\ntask label always differs from the\nNLI label.\n(c) Irrelevant/Null Cond.: No action-\nable task, only a distractor (for irrel-\nevant) or an empty string (for null).\nAbove shows the irrelevant condition.\nFigure 1: Main experimental design. Note that under the misleading condition, if a participant interprets the\nsurface instruction as lexical overlap, then the gold answer would be “yes”. If a participant somehow interprets this\ninstruction as NLI, then the gold answer will be “no”. Text within the curly bracket are not shown to the participants.\nCategory Prompts Examples Total Items\nInstructive 5 12 60\nMisleading 10 5 50\nIrrelevant 5 12 60\nNull 2 12 24\nTotal 22 46 194\nTable 2: We define an item as a unique 3-tuple\n<sentence1, instruction, sentence2>\nwhere each sentence pair is an example manually\nselected from a dataset.\nsponses over a three-day span. Participants were\npaid a base rate of $0.50 with an additional $0.10\nper correct answer as an incentive. See Appendix E\nfor details on how we qualify participants.\nModels To compare human performance with\nthat of models, we use instruction-tuned mod-\nels T0++ (11B, Sanh et al., 2022) and Flan-T5\n(11B, Chung et al., 2022), as well as GPT-3.5\n(gpt-3.5-turbo) and GPT-4 (June 2023 ver-\nsion, OpenAI 2023). T0++ and Flan-T5 models are\nextensively fine-tuned to follow NLP instructions,\nwith the key difference that T0 has NLI as a held-\nout task while Flan-T5 does not. All models are\ngiven test items identical to those from the human\nexperiments. (See §E.4 for additional details).\n3 Results\nFigure 2 show the zero-shot performance of\nhumans compared to that of instruction-tuned\nthat our participants are sufficiently competent.\n4We add a line break after the premise text for better human\nreadability, and we edit all prompts to be clearer by making\nuse of the line break (e.g. “the above passage\").\nmodels. The overall performance of T0++, Flan-T5\nand GPTs by themselves is consistent with\nthe model performance reported by W&P. But,\nwith the exception of GPT-4, models show very\ndifferent patterns from that of humans when\ngiven misleading prompts. As expected, when\nhumans are explicitly asked to do a task other than\nNLI (e.g., sentiment analysis, grammaticality),\nthey tend to do the specified task (leading to low\naccuracy when measured against the NLI task).\nIn contrast, models often appear to behave as\nthough they have been instructed to do NLI even\nthough they are instructed to do some other surface\ntasks (leading to high accuracy when measured\nagainst the NLI task). For example, when given\na misleading instruction (e.g., paraphrasing\nidentification as opposed to NLI) {sentence1}\nDoes that have the same meaning\nas \"{sentence2}\"?, humans do indeed\nperform the paraphrasing task and thus receive a\nscore of 0 on NLI, whereas models tend towards\nperforming NLI, with T0++ / GPT-4 receiving an\nNLI score of 1 and Flan-T5 / GPT-3.5 a score of\n0.6. (Full results on all prompts in §G.1.) This\npattern confirms W&P’s assumption A1 that\nmodels perform ‘too well’ on misleading prompts,\ni.e., better than humans would under similar\nconditions.\nHowever, when we consider assumption A2\n(instructive > irrelevant), we see a different\nstory. When given irrelevant instructions, we\nsee that all models and humans exhibit similar\npatterns. In fact, humans show far less variance\nthan models in performing the NLI task when\n7664\nT0++ FLAN-T5 GPT-3.5 GPT-4 Humans\n0\n0.2\n0.4\n0.6\n0.8\n1\ninstructive misleading irrelevant null\nNLI Accuracy\n0.833\n0.917\n0.833\n0.917\n0.781\n0.6\n0.8\n0.6\n0.3 0.268\n0.708\n0.542\n0.708 0.708\n0.7590.792\n0.5\n0.708 0.667\n0.73\nFigure 2: Zero-shot accuracy of human annotators vs. instruction-tuned models. In the misleading condition, lower\nNLI accuracy is preferred as it means higher accuracy in following the instructions to perform the surface tasks.\nMedians are displayed in the bars. Each scatter point represents the accuracy on a instruction within the semantic\ncategory. For models, this is calculated as the mean accuracy over all the test items constructed from the examples\nfor the instruction (i.e., 5 for each misleading instruction, 12 for each null, irrelevant and instructive instruction); for\nhumans this is the mean accuracy over all participants who received items with that instruction.\nthe instructions are irrelevant, suggesting that\nhumans are more likely to perform NLI as\nsome kind of “default” task absent of useful\ninstructions. For example, given the irrelevant\ninstruction {sentence1} Inflections\nare annoying and thank god that\nMiddle English got rid of most\nof them. {sentence2} Humans, T0++,\nFlan-T5, GPT-3.5 and GPT-4 all score similarly at\n0.79, 0.83, 0.83, 0.67 and 0.75 respectively (§G.1).\n4 Discussion\nOur findings show that, in a zero-shot setting, hu-\nmans appear largely faithful to prompt instructions.\nThey perform well given instructive prompts and\npoorly given misleading prompts (as expected).\nHowever, we observe that T0++, FLAN-T5, and\n(to a lesser extent) GPT-3.5 are inclined to perform\nNLI in a zero-shot setting regardless of what is in\nfact being instructed. GPT-4, however, does seem to\nexhibit a more human-like pattern in following the\nmisleading instructions (albeit with high variance).\nIt is possible that a combination of pretraining\nFLOPs, fine-tuning data quality, and RLHF may\nhave bridged some discrepancies between GPT and\nhuman behaviors, whereas smaller and supervised\nfine-tuning-only models fail to do so, but it is im-\npossible to conclude given that little is known about\nGPT-3.5/4’s technical details.\nWhen no useful instructions are provided in the\nirrelevant prompt setting, our results show that hu-\nmans still tend to perform the NLI task surprisingly\nwell. Contrary to W&P’s criticism, models’ ten-\ndency to do the same with irrelevant prompts is\nlikely more a feature than a bug (cf. Merrill et al.,\n2022).\nSuch idiosyncrasies in human behavior are of-\nten difficult to anticipate and even more difficult\nto codify in a way that lends itself well to bench-\nmarks. Thus, we echo recent work (Pavlick and\nCallison-Burch, 2016; Ross and Pavlick, 2019;\nPavlick and Kwiatkowski, 2019; Dasgupta et al.,\n2022; Lampinen, 2022) in emphasizing the diffi-\nculty of evaluating models when “human-likeness”\nis the intended gold standard. As NLP models be-\ncome increasingly advanced, and evaluation tasks\nincreasingly complex, we are likely to face increas-\ning challenges in determining whether models’ be-\nhavior is “aligned” with that of humans. This study\ncontributes to the line of work which underscores\nthe importance of empirically measuring, rather\nthan presupposing, human behavior in such set-\ntings, as humans in practice routinely evade basic\nintuitions. Appendix B further discusses how to\ndesign a fairer comparison between humans and\nmodels.\n5 Conclusion\nIn this work, we measure human behaviors when\ngiven misleading and irrelevant instructions for var-\nious NLP tasks. We show that our prior work used\noversimplified assumptions of human behavior to\nevaluate NLP models. Our results underscore the\nneed to empirically validate assumptions about hu-\nman behavior, which is often more complex in re-\nality than our intuitions would lead us to believe.\n7665\n6 Limitations and Future Work\nOur main experiment investigates only the zero-\nshot setting in humans. We do run a pilot exper-\niment with the intention to compare model and\nhuman behavior in a few-shot setting, reported in\nAppendix A. While the pilot already yielded inter-\nesting results, we leave a full experiment on the\nfew-shot setting to future work.\nOur experimental design also only uses the Natu-\nral Language Inference (NLI) task as the reference\ntask, which we acknowledge may be a task that\nis perhaps more intuitive as a “default” task than\nother common NLP tasks (e.g., sentiment analy-\nsis, paraphrase). While future work should con-\nsider repeating this analysis for other tasks, NLI\nhas several advantages for our purposes. First, NLI\nis one of the only tasks explicitly held out entirely\nfrom the instruction-tuned T0 models, allowing for\nthe best evaluation of their few-shot performance.\nMoreover, the intuitiveness of NLI works in our\nfavor for the claims we make—it would be ostensi-\nbly most challenging for humans to override a bias\ntowards this task to instead follow task instructions.\nThen, the fact humans do this even under the NLI\ntask setting is the strongest evidence of instruction-\nfollowing behavior compared to any other task. We\nacknowledge that there is future work to verify\nthat the human behavior observed for instruction-\nfollowing extends to other tasks.\n7 Ethics Statement\nWe acknowledge that calling for more rigorous hu-\nman benchmarking only exacerbates NLP field’s\nneeds for human annotators, where it has been\ndemonstrated that NLP crowdsourcing may po-\ntentially expose workers to harm, as described in\nShmueli et al. (2021). For our study, our Institu-\ntional Review Board (IRB) reviewed our experi-\nmental design and determined that its primary aim\nis to study computational language models and thus\ndoes not meet the federal definition of human sub-\njects research. Future studies should similarly sub-\nmit their studies, if involving human benchmarking,\nfor review by their institutions’ IRBs.\n8 Acknowledgments\nWe thank Tal Lizen, Andrew Lampinen, Swaroop\nMishra, Apoorv Agarwal, Michael Lepori, Louis\nCastricato, Samuel Musker, Aaron Traylor, and\nPhilip LaDuca for discussions and comments on\nthis work. We thank Brendan Ho who wrote a lot\nof feedback for the free-form response questions.\nSpecial thanks to Roman Feiman for discussions\non the design of the human experiments.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nArXiv preprint, abs/2210.11416.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine learning challenges workshop ,\npages 177–190. Springer.\nIshita Dasgupta, Andrew K. Lampinen, Stephanie\nC. Y . Chan, Antonia Creswell, Dharshan Kumaran,\nJames L. McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. ArXiv preprint, abs/2207.07051.\nPhilippe Goldammer, Hubert Annen, Peter Lucas\nStöckli, and Klaus Jonas. 2020. Careless responding\nin questionnaire measures: Detection, impact, and\nremedies. The Leadership Quarterly, 31(4):101384.\nRobert Greszki, Marco Meyer, and Harald Schoen. 2015.\nExploring the Effects of Removing \"Too Fast\" Re-\nsponses and Respondents from Web Surveys. Public\nOpinion Quarterly, 79.\n7666\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2022. Can\nlarge language models truly understand prompts? a\ncase study with negated prompts. ArXiv preprint,\nabs/2209.12711.\nDaniel Khashabi, Shane Lyu, Sewon Min, Lianhui Qin,\nKyle Richardson, Sameer Singh, Sean Welleck, Han-\nnaneh Hajishirzi, Tushar Khot, Ashish Sabharwal,\net al. 2021. Prompt waywardness: The curious case\nof discretized interpretation of continuous prompts.\nArXiv preprint, abs/2112.08348.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. ArXiv\npreprint, abs/2205.11916.\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\nChan, Kory Matthewson, Michael Henry Tessler,\nAntonia Creswell, James L McClelland, Jane X\nWang, and Felix Hill. 2022. Can language models\nlearn from explanations in context? ArXiv preprint,\nabs/2204.02329.\nAndrew Kyle Lampinen. 2022. Can language models\nhandle recursively nested grammatical structures? a\ncase study on comparing models and humans. ArXiv\npreprint, abs/2210.15303.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\nand patterns: For effective chain of thought, it takes\ntwo to tango. ArXiv preprint, abs/2209.07686.\nWilliam Merrill, Alex Warstadt, and Tal Linzen. 2022.\nEntailment semantics can be extracted from an ideal\nlanguage model. ArXiv preprint, abs/2209.12407.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? ArXiv\npreprint, abs/2202.12837.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Natural instructions:\nBenchmarking generalization to new tasks from\nnatural language instructions. ArXiv preprint ,\nabs/2104.08773.\nRishabh Misra. 2022. News category dataset. arXiv\npreprint arXiv:2209.11429.\nRishabh Misra and Jigyasa Grover. 2021. Sculpting\nData for ML: The first act of Machine Learning.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. ArXiv preprint, abs/2112.00114.\nOpenAI. 2023. GPT-4 Technical Report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. ArXiv preprint, abs/2203.02155.\nEllie Pavlick and Chris Callison-Burch. 2016. So-called\nnon-subsective adjectives. In Proceedings of the Fifth\nJoint Conference on Lexical and Computational Se-\nmantics, pages 114–119, Berlin, Germany. Associa-\ntion for Computational Linguistics.\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent\ndisagreements in human textual inferences. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:677–694.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2022. Grips: Gradient-free, edit-based in-\nstruction search for prompting large language models.\nArXiv preprint, abs/2203.07281.\nAlexis Ross and Ellie Pavlick. 2019. How well do NLI\nmodels capture verb veridicality? In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2230–2240, Hong Kong,\nChina. Association for Computational Linguistics.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1743–1752, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nBoaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku.\n2021. Beyond Fair Pay: Ethical Implications of NLP\nCrowdsourcing.\nAarohi Srivastava, Abhinav Rastogi, Abhishek B Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta,\n7667\nAdrià Garriga-Alonso, Agnieszka Kluska, Aitor\nLewkowycz, Akshat Agarwal, Alethea Power, Alex\nRay, Alex Warstadt, Alexander W. Kocurek, Ali\nSafaya, Ali Tazarv, Alice Xiang, Alicia Parrish,\nAllen Nie, Aman Hussain, Amanda Askell, Amanda\nDsouza, Ameet Annasaheb Rahane, Anantharaman S.\nIyer, Anders Andreassen, Andrea Santilli, Andreas\nStuhlmuller, Andrew M. Dai, Andrew D. La, An-\ndrew Kyle Lampinen, Andy Zou, Angela Jiang, An-\ngelica Chen, Anh Vuong, Animesh Gupta, Anna\nGottardi, Antonio Norelli, Anu Venkatesh, Arash\nGholamidavoodi, Arfa Tabassum, Arul Menezes,\nArun Kirubarajan, Asher Mullokandov, Ashish Sab-\nharwal, Austin Herrick, Avia Efrat, Aykut Erdem,\nAyla Karakacs, Bridget R. Roberts, Bao Sheng\nLoe, Barret Zoph, Bartlomiej Bojanowski, Batuhan\nOzyurt, Behnam Hedayatnia, Behnam Neyshabur,\nBenjamin Inden, Benno Stein, Berk Ekmekci,\nBill Yuchen Lin, Blake Stephen Howald, Cameron\nDiao, Cameron Dour, Catherine Stinson, Cedrick\nArgueta, C’esar Ferri Ram’irez, Chandan Singh,\nCharles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu\nWu, Chris Callison-Burch, Chris Waites, Chris-\ntian V oigt, Christopher D. Manning, Christopher\nPotts, Cindy Tatiana Ramirez, Clara Rivera, Clemen-\ncia Siro, Colin Raffel, Courtney Ashcraft, Cristina\nGarbacea, Damien Sileo, Daniel H Garrette, Dan\nHendrycks, Dan Kilman, Dan Roth, Daniel Freeman,\nDaniel Khashabi, Daniel Levy, Daniel Gonz’alez,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, D. Drakard, David Jur-\ngens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek Tam,\nDieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dim-\nitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekate-\nrina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth P. Donoway,\nEllie Pavlick, Emanuele Rodolà, Emma FC Lam,\nEric Chu, Eric Tang, Erkut Erdem, Ernie Chang,\nEthan A. Chi, Ethan Dyer, Ethan J. Jerzak, Ethan\nKim, Eunice Engefu Manyasi, Evgenii Zheltonozh-\nskii, Fan Xia, Fatemeh Siar, Fernando Mart’inez-\nPlumed, Francesca Happ’e, François Chollet, Frieda\nRong, Gaurav Mishra, Genta Indra Winata, Gerard\nde Melo, Germán Kruszewski, Giambattista Paras-\ncandolo, Giorgio Mariani, Gloria Wang, Gonzalo\nJaimovitch-L’opez, Gregor Betz, Guy Gur-Ari, Hana\nGalijasevic, Han Sol Kim, Hannah Rashkin, Hanna\nHajishirzi, Harsh Mehta, Hayden Bogar, Henry\nShevlin, Hinrich Schütze, Hiromu Yakura, Hongming\nZhang, Hubert Wong, Ian Aik-Soon Ng, Isaac No-\nble, Jaap Jumelet, Jack Geissinger, John Kernion,\nJacob Hilton, Jaehoon Lee, Jaime Fernández Fisac,\nJ. Brooker Simon, James Koppel, James Zheng,\nJames Zou, Jan Koco’n, Jana Thompson, Jared Ka-\nplan, Jarema Radom, Jascha Narain Sohl-Dickstein,\nJason Phang, Jason Wei, Jason Yosinski, Jekaterina\nNovikova, Jelle Bosscher, Jenni Marsh, Jeremy Kim,\nJeroen Taal, Jesse Engel, Jesujoba Oluwadara Alabi,\nJiacheng Xu, Jiaming Song, Jillian Tang, Jane W\nWaweru, John Burden, John Miller, John U. Balis,\nJonathan Berant, Jorg Frohberg, Jos Rozen, José\nHernández-Orallo, Joseph Boudeman, Joseph Jones,\nJoshua B. Tenenbaum, Joshua S. Rule, Joyce Chua,\nKamil Kanclerz, Karen Livescu, Karl Krauth, Karthik\nGopalakrishnan, Katerina Ignatyeva, Katja Markert,\nKaustubh D. Dhole, Kevin Gimpel, Kevin Ochieng’\nOmondi, Kory Wallace Mathewson, Kristen Chia-\nfullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLuca Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Col’on, Luke Metz, Lutfi Kerem\ncSenel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Madotto Andrea, Maheen Saleem Farooqi,\nManaal Faruqui, Mantas Mazeika, Marco Baturan,\nMarco Marelli, Marco Maru, M Quintana, Marie\nTolkiehn, Mario Giulianelli, Martha Lewis, Martin\nPotthast, Matthew Leavitt, Matthias Hagen, M’aty’as\nSchubert, Medina Baitemirova, Melissa Arnaud,\nMelvin Andrew McElrath, Michael A. Yee, Michael\nCohen, Mi Gu, Michael I. Ivanitskiy, Michael Star-\nritt, Michael Strube, Michal Swkedrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Monica Tiwari, Mo-\nhit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh\nGheini, T MukundVarma, Nanyun Peng, Nathan\nChi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas\nCameron, Nicholas S. Roberts, Nicholas Doiron,\nNikita Nangia, Niklas Deckers, Niklas Muennighoff,\nNitish Shirish Keskar, Niveditha Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar\nAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pascale\nFung, Paul Pu Liang, Paul Vicol, Pegah Alipoormo-\nlabashi, Peiyuan Liao, Percy Liang, Peter W. Chang,\nPeter Eckersley, Phu Mon Htut, Pi-Bei Hwang,\nP. Milkowski, Piyush S. Patil, Pouya Pezeshkpour,\nPriti Oli, Qiaozhu Mei, QING LYU, Qinlang Chen,\nRabin Banjade, Rachel Etta Rudolph, Raefer Gabriel,\nRahel Habacker, Ram’on Risco Delgado, Raphaël\nMillière, Rhythm Garg, Richard Barnes, Rif A.\nSaurous, Riku Arakawa, Robbe Raymaekers, Robert\nFrank, Rohan Sikand, Roman Novak, Roman Sitelew,\nRonan Le Bras, Rosanne Liu, Rowan Jacobs, Rui\nZhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee,\nRyan Stovall, Ryan Teehan, Rylan Yang, Sahib J.\nSingh, Saif M. Mohammad, Sajant Anand, Sam\nDillavou, Sam Shleifer, Sam Wiseman, Samuel Gruet-\nter, Sam Bowman, Samuel S. Schoenholz, Sanghyun\nHan, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazar-\nian, Sayan Ghosh, Sean Casey, Sebastian Bischoff,\nSebastian Gehrmann, Sebastian Schuster, Sepideh\nSadeghi, Shadi S. Hamdan, Sharon Zhou, Shashank\nSrivastava, Sherry Shi, Shikhar Singh, Shima Asaadi,\nShixiang Shane Gu, Shubh Pachchigar, Shubham\nToshniwal, Shyam Upadhyay, Shyamolima Deb-\nnath, Siamak Shakeri, Simon Thormeyer, Simone\nMelzi, Siva Reddy, Sneha Priscilla Makini, Soo\nhwan Lee, Spencer Bradley Torene, Sriharsha Hat-\nwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon,\nStella Rose Biderman, Stephanie C. Lin, S. Prasad,\nSteven T. Piantadosi, Stuart M. Shieber, Summer\nMisherghi, Svetlana Kiritchenko, Swaroop Mishra,\nTal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq A.\n7668\nAli, Tatsuo Hashimoto, Te-Lin Wu, Theo Desbordes,\nTheodore Rothschild, Thomas Phan, Tianle Wang,\nTiberius Nkinyili, Timo Schick, T. N. Kornev, Tim-\nothy Telleen-Lawton, Titus Tunduny, Tobias Ger-\nstenberg, Trenton Chang, Trishala Neeraj, Tushar\nKhot, Tyler O’Brien Shultz, Uri Shaham, Vedant\nMisra, Vera Demberg, Victoria Nyamai, Vikas Rau-\nnak, Vinay Venkatesh Ramasesh, Vinay Uday Prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fe-\ndus, William Saunders, William Zhang, W V ossen,\nXiang Ren, Xiaoyu F Tong, Xinyi Wu, Xudong Shen,\nYadollah Yaghoobzadeh, Yair Lakretz, Yang Song,\nYasaman Bahri, Ye Ji Choi, Yichi Yang, Yiding Hao,\nYifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yushi\nBai, Zachary Seid, Zhao Xinran, Zhuoye Zhao, Zi Fu\nWang, Zijie J. Wang, Zirui Wang, Ziyi Wu, Sahib\nSingh, and Uri Shaham. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models. ArXiv preprint, abs/2206.04615.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. ArXiv preprint,\nabs/2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. ArXiv preprint, abs/2201.11903.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nXi Ye and Greg Durrett. 2022. The unreliability of ex-\nplanations in few-shot prompting for textual reason-\ning. In Advances in Neural Information Processing\nSystems.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 649–657.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. ArXiv preprint ,\nabs/2205.10625.\n7669\nAppendix\nTable of Contents\nA Pilot Few-Shot Experiment 9\nA.1 Method . . . . . . . . . . . . 9\nA.2 Results . . . . . . . . . . . . . 10\nB Reconciling Few-Shot and Zero-Shot\nEvidence 11\nC Related Work 12\nD Author Contributions 13\nE Additional Details of the Main Zero-\nShot Experiment 13\nE.1 Controls . . . . . . . . . . . . 13\nE.2 Qualifications . . . . . . . . . 13\nE.3 Instructions to Workers . . . . 13\nE.4 Evaluation Protocol Details . . 14\nF Additional Results for Zero-Shot Ex-\nperiment 15\nF.1 Using Control Scores as Qualifi-\ncation . . . . . . . . . . . . . 15\nF.2 Without Response Time Qualifi-\ncation . . . . . . . . . . . . . 15\nG Additional Figures and Data for Zero-\nShot Experiment 17\nG.1 Accuracies Per Instruction . . . 17\nH Instructions and Examples 18\nH.1 All Instructions . . . . . . . . 18\nH.2 Example Sources for Test Con-\ndition . . . . . . . . . . . . . 19\nH.3 Example Sources for General\nControls . . . . . . . . . . . . 20\nI Results on Zero-Shot Experiment Con-\ntrol Conditions 21\nI.1 NLI Controls . . . . . . . . . . 21\nI.2 General (Surface Task) Controls 22\nJ Effect of Experience with Prior NLP\nStudies 23\nK Few-Shot Experiment Post-\nExperiment Survey 25\nA Pilot Few-Shot Experiment\nOur main experiment above focuses only on the\nzero-shot setting. However, model behavior (and\nlikely human behavior as well) can look very dif-\nferent when given even a single labeled example.\nThus, we additionally conduct a pilot experiment\nto investigate human behaviors under a few-shot\nlearning setup. We consider two conditions. The\nfirst, few-shot with labels, simulates the usual few-\nshot learning setting: the participant is given the\ngold label as feedback after answering each ques-\ntion. The second condition, few-shot without labels,\nis inspired by Min et al. (2022)’s study in few-shot\nlearning with corrupted labels. In this condition, we\ndo not give participants any label feedback. How-\never, they still have the opportunity to refine their\nunderstanding of the instruction via sequential en-\ncounters with different unlabeled examples.\nA.1 Method\nWe show each subject 32 items sequentially, one\nitem per page, for which subjects must respond\n“Yes” or “No” to each item. Each subject is assigned\nan instruction as condition and the instruction is\nthe same across all examples they receive.\nIn the few-shot with labels condition, subjects\nare shown whether their answer was correct for that\nitem. Here, correctness is always based on NLI (i.e.,\nwhether sentence 1 entails sentence 2) regardless of\nthe surface instructions. Unlike the zero-shot study,\nthere is no special design in the example sentence\npairs in relation to the instruction, only that the\nsample is balanced in the number of entailments\nand non-entailments. That is, a 50% score indicates\nrandom performance on the implicit NLI task.\nIn the few-shot without labels condition, sub-\njects do not receive any feedback throughout the 32\nitems. This setup is inspired by Min et al. (2022),\nwho show that models can surprisingly well learn\nfrom corrupted labels. We conjecture that unsu-\npervised exposure to examples may help humans\npragmatically infer what researchers want out of\nthem and what the underlying task is.\nPost-Experiment Survey After participants\ncomplete the 32 items, we ask for paragraph-length\nresponses to two questions: (1) How did you decide\nto choose between “Yes\" or “No\"? (2) What did\nyou think about the instructions we gave?\nInstruction Templates We manually choose one\ninstruction from each of W&P’s five categories for\n7670\nthis pilot study:\n1. {sentence1} Using only the above description\nand what you know about the world, \"{sen-\ntence2}\" is definitely correct. Yes or no? (In-\nstructive; adapted from MultiNLI’s instruction\nto crowdworkers).\n2. {sentence1} Can the above be paraphrased as:\n“{sentence2}\" (Moderately misleading.)\n3. {sentence1} Is the next sentence grammati-\ncally correct? {sentence2} (Extremely mis-\nleading.)\n4. {sentence1} Single-family zoning is bad for\nAmerican cities {sentence2} (Irrelevant.)\n5. {sentence1} {sentence2} (Null; empty instruc-\ntion baseline.)\nParticipants Subjects were undergraduate stu-\ndents (n= 8for the with labels condition, n= 5\nfor the without labels condition). Subjects were\nasked to finish all 32 items in a single session\nwithin an hour. Subjects were paid a base com-\npensation of $15, with a $0.25 bonus for every\ncorrect answer as an incentive. As it is expensive\nto have a participant complete 32 items continu-\nously in a single session (in order to mimic models’\nfew-shot training), and because the trend was suffi-\nciently clear from our pilot experiment, we did not\nproceed with a larger pool of participants.\nA.2 Results\nFigure 3 shows the cumulative scores of subjects\nacross 32 items are shown for both few-shot with\nlabels and without labels conditions, where each\nline is one subject. Solid lines represent the perfor-\nmance of subjects with labels, and dotted lines rep-\nresent the performance of subjects without labels.\nThe subject who received the instructive prompt\nachieved a perfect score, and their performance is\npresented as a green reference y = xline against\nall participants who received other prompts.\nIrrelevant Instructions Participants who re-\nceived the irrelevant instruction perform practically\nidentically with or without label feedback. In both\nconditions subjects with the irrelevant prompt also\nperformed almost just as well as the subject who\nreceived the instructive prompt. Like in the zero-\nshot study, this pattern provides evidence against\nW&P’s assumption A2 (instructive > irrelevant).\nThe post-experiment survey also confirms that par-\nticipants were able to figure out that the prompt\nwas simply irrelevant (Appendix K).\nParaphrasing Inst. (Misleading) When a partic-\nipant is given a paraphrasing instruction without\nlabel feedback, they were successfully misled to\nperform paraphrasing identification, thus scoring\nmuch lower on NLI (dashed yellow line in Fig-\nure 3). But when a participant is given a paraphras-\ning instruction with NLI labels as feedback, they\nquickly adapted their interpretation of the instruc-\ntion in order to fit the labels (solid yellow line in\nFigure 3). As one participant wrote in the post-\nexperiment survey:\nIn the first few questions, my strategy\nis to read through the entire paragraph\nor sentence and then decide whether the\nparaphrased sentence makes sense or not.\nHowever, then I started to look at the\nparaphrased sentence first and decide\nwhether it is correct or wrong based on\nthe given piece of text.\nThat is, this participant completely recovered\nthe NLI instruction even though they were\ngiven a paraphrasing instruction. They even ob-\nserved the unidirectional entailment nature of\nNLI (i.e., sentence2 does not need to entail\nsentence1) vs. the bidirectional entailment na-\nture of paraphrasing (i.e., the two sentences must\nexpress approximately the same meaning):\nInitially, I also considered whether the\nparaphrased sentence captured all the ma-\njor details or not, but the quiz later shows\nthat comprehensiveness is not a factor.\nGrammaticality Inst. (Misleading) While para-\nphrasing is a task related to NLI, grammatical ac-\nceptability is a much more misleading instruction\nsince it has nothing to do with NLI. Here, we see\nthe with and without label results nearly reversed:\nwhen given NLI label feedback, the grammatical\nacceptability instruction appears to be so incompat-\nible with the labels that one participant was con-\nfused and unable to adapt their interpretation to\njust one task, and ultimately scored much lower\non NLI (lower solid red line in Figure 3). In the\npost-experiment survey, they wrote\nI basically went on a gut reaction on what\nwas correct. On some weird instances, I\n7671\n1 5 10 15 20 25 300\n5\n10\n15\n20\n25\n30\n1 5 10 15 20 25 30 1 5 10 15 20 25 30 1 5 10 15 20 25 30\nFew-Shot without LabelsFew-Shot with LabelsBaseline with Instructive Prompt\nParaphrasing (Misleading)Grammaticality (Misleading) Irrelevant Null\nItem Index\nNLI Accuracy\nFigure 3: Cumulative total scores between the few-shot with labels and few-shot without labels experiments, within\ncategories. The y-axis plots the sum of correct answers out of xitems answered so far. The sequence of 32 prompts\nwere the same across both experiments for all prompt categories. Each line is one subject.\nthought hard about good grammar. Of-\ntentimes, I looked at the content of the\nsentence. If it was factually correct, I\nguessed yes.\nHowever, another participant did eventually figure\nout the underlying NLI task:\nI feel like the question should be changed\nbecause it seems like the question is actu-\nally, “Is this statement true based on the\ncontext given in the paragraph.”\nWhen no label feedback is giventhe results\nare more surprising and nuanced. Because all of\nour examples are handpicked and grammatical,\nparticipants found themselves answering “yes”\nto many examples and starting to question the\ninstruction itself:\nI looked at whether the sentence was ac-\ncurate to the information given in the text,\nand also if the sentence itself had cor-\nrect grammatical structure. It was a little\ndifficult because some of the sentences\nmade inferences that weren’t explicit in\nthe given text, so I wasn’t sure if that was\na grammatical error or not.\nThey then started to incorporated the semantic\nwell-formedness into their interpretation of\ngrammatical acceptability:\nUsually, I think of something as being\ngrammatically correct when the sentence\nhas correct grammatical structure, includ-\ning punctuation and capitalization. Since\nmost of the sentences seemed to fit this,\nI thought that maybe grammar also en-\ncompasses the validity of the statement\nbased on the text, so I chose my answers\nbased on that.\nAdditional survey responses are included in\nAppendix K. Overall, human behavior in the\nfew-shot cannot be readily summarized as a single\ninequality as attempted in A1 - 3. Rather, different\nparticipants can respond to the same instructions\nin different ways; some interpret the instructions\nstrictly and literally, while others adopt a more\nrelaxed pragmatic interpretation, and still others\nrefine their interpretations over time.\nB Reconciling Few-Shot and Zero-Shot\nEvidence\nAs we find instruction-tuned models seem to\nlargely match human behaviors in the few-shot set-\nting, while falling short of human in the zero-shot\nsetting, which setting should we weigh more for\nevaluating models’ understanding of prompts as in-\nstructions? Concurrent work by Lampinen (2022)\nnotes that zero-shot could be an inherently prob-\nlematic way to study the full competence of LMs:\nFrom a model’s perspective, it has just finished\nimitating (for example, in T5’s case) a trillion to-\nkens of highly heterogeneous content and linguistic\nstyles with communicative intents far from answer-\ning academic evaluations. In a zero-shot setting, it\ncould be unclear to the model “what is the intended\ncontinuation; the model might be likely to produce\na blank line for someone to fill in the answer, or\njokes mocking the question, or just arbitrary ram-\nbling” (Lampinen, 2022, p. 7); all of the above may\nbe valid continuations for its language modeling\n7672\npretraining objective. We partially address this is-\nsue by only using instruction-tuned models, which\nare trained on a large mixture of traditional NLP\ndatasets and thus primed to directly answer the\nquestion.\nOn the human side, in order to make a fair com-\nparison, we carefully design our experiments to\nmake sure the human responses are “as zero-shot\nas possible” by (1) having one participant answer\nonly one test question and (2) having all qualifica-\ntion questions come after the test question so as to\nnot bias the participants. This is a highly controlled,\nperhaps even contrived, condition that does not re-\nflect well on how humans learn from instructions\nin the real world, or even in most other cognitive\nscience experiments where participants are often\ngiven familiarization trials prior to the test trials.\nTherefore, we agree with Lampinen (2022) that,\nfor future studies, the few-shot setting is likely a\nmore productive way to probe a model’s true com-\npetence, even though it may be scientifically less\ncontrolled in other respects, since now effects of\nthe few-shot examples must be considered, which\ncould also be counter-intuitive as shown in Ap-\npendix A and Min et al. (2022).\nHowever, zero-shot evidence should not be ig-\nnored either, especially considering that there is\na consistent collection of work showing language\nmodels being insensitive to instructions on tasks in\naddition to NLI and on models of various sizes and\nfine-tuning strategies.\nC Related Work\nZero-Shot Instructions In addition to W&P,\nmany papers find similar results that models per-\nform well with semantically incoherent instructions\non a variety of tasks. Prasad et al. (2022) find that\nsemantically incoherent prompts work well for In-\nstructGPT over 12 QA and coreference datasets in\nNatural Instructions Mishra et al. (2021), Khashabi\net al. (2021) find this to be true for GPT-2 on 5\nsentiment analysis and topic classification datasets\nin Natural Instructions. Jang et al. (2022) show\nOPT and InstructGPT are unable to follow negated\ninstructions over 9 QA and sentence completion\ndatasets, performing well below human baseline of\n13-year-olds.\nNote that none of the above papers, including\nW&P, claim that pathological prompts would per-\nform just as well for all tasks. Indeed, Kojima et al.\n(2022) show various irrelevant and misleading base-\nlines perform poorly on an arithmetic dataset (Roy\nand Roth, 2015). Instead, W&P claim that the exis-\ntence of high-performing pathological prompts for\na large number tasks show that they use prompts\nin an un-human-like way, while the existence of\nbad-performing pathological prompts is orthogonal\nto this line of argument.\nFew-Shot Exemplars and Explanations Unlike\nzero-shot, the few-shot setting has more conflicting\nevidence in the literature on whether models per-\nform just as well with pathological prompts. Min\net al. (2022) first showed that the correctness of the\nfew-shot labels are not required, concluding that\nprompts are largely helping models to adapt to the\ndomain and format of the input text as well as the\nspace of possible labels.\nAs few-shot exemplars are now commonly\naccompanied with intermediate computation,\nexplanations, or chain-of-thoughts (Wei et al.,\n2022; Nye et al., 2021; Zhou et al., 2022;\nLampinen et al., 2022), Madaan and Yazdanbakhsh\n(2022) agree with Min et al. (2022) and find that\nvarious few-shot chain-of-thought prompts—with\ncorrupted symbols but retaining the overall\ntask format—have performance comparable to\nnon-corrupted baseline, thus arguing that “CoT\nhelps a language model in imitating the prompt\nand generating the right tokens for the task—and\nis conceivably less related to their reasoning\nabilities.” Similarly, Ye and Durrett (2022) find\nthat explanations improve performance modestly\nfor OPT and GPT-3 but improve substantially for\ntext-davinci-002 on 3 QA and NLI datasets. How-\never, the model-generated explanations themselves\nare often inconsistent or factually incorrect, con-\ncluding that “model internal ‘reasoning’ does not\nalways align with explanations that it generates.”\nWith extensive statistical controls on a diverse\nrange of BIG-Bench tasks (Srivastava et al., 2022),\nLampinen et al. (2022) find that LMs’ success with\npost-answer explanations do outperform baselines\nsuch as same-length word-scrambled explanations,\ndomain-relevant but non-explanatory statements,\nand correct explanations misaligned with wrong\nfew-shot examples. Notably, although they find a\npositive result with the effect of explanations, they\nalso find that models are much more insensitive to\nthe effect of instructions.\n7673\nD Author Contributions\nAlbert Webson led the project, co-designed the ex-\nperiments, implemented the model evaluation code,\nand led the paper writing.\nAlyssa Marie Loo co-designed the experiments,\nmanually curated all prompts and examples, eval-\nuated all models, produced all analyses, and co-\nauthored the paper.\nQinan Yu co-designed the experiments, con-\nducted all human experiments, and co-authored\nthe paper.\nEllie Pavlick advised the project and edited the\npaper.\nE Additional Details of the Main\nZero-Shot Experiment\nE.1 Controls\nWe collect additional data to test the robustness of\nour result on different subgroups of participants se-\nlected under multiple conditions. After the test con-\ndition, participants are asked to complete two addi-\ntional control conditions, General (Surface Tasks)\nControl and NLI Control, that provide us such se-\nlection criteria for post-hoc analysis in Appendix F.\nThese controls test whether the same trend still\nholds under more restrictions such as receiving per-\nfect scores on both controls as additional analysis.\nHowever, note that our main results do not exclude\nany participants using these controls as it may bias\nthe results to only consider participants with spe-\ncific behavior patterns.\nThe two controls are four items presented after\nthe first item from the test condition, adding up to\nthe five items that each subject is presented in the\nstudy. The control items are shown one by one in\nthe following sequence: two items in the General\nControl, and then two items in the NLI Control. As\nwith the test condition item, subjects are asked to\nanswer “Yes” or “No” in response to each control\nitem. The performance of our subject population\non the controls is shown in Appendix I.\nGeneral Control The General Control verifies\nif participants can perform the misleading tasks.\nIn this control, subjects are scored on whether\nthey perform the surface task correctly. If subjects\nwere already given a misleading prompt in their\ntest condition, in this control they are shown the\nsame prompt with two new examples. Otherwise,\nsubjects are randomly assigned two items with a\nmisleading prompt.\nWe curate examples from a range of datasets\nsuch that the misleading task and NLI task have op-\nposing answers, and also to be converse to the test\ncondition. That is, if the misleading task answers\nfor a prompt is “Yes\" in the test condition, it would\nbe “No\" in the General Control. See Appendix H\nfor how examples were selected.\nNLI Control The NLI Control verifies if partic-\nipants can perform the NLI task. In this control,\nsubjects are given two items with an instructive\nprompt and are scored on performing the NLI task\ncorrectly.\nE.2 Qualifications\nFrom the 597 responses, for our main results we\nexclude data from 93 subjects 5 whose total com-\npletion time t for the five-question study is less\nthan one standard deviation from the mean of the\nsample (t< 33.01). Extremely low response times\nhave been shown to be an accurate indicator of care-\nless responding, where data from such “speeders”\nhave been shown to lower data quality (Greszki\net al., 2015; Goldammer et al., 2020). Past studies\nhave typically defined floor cutoffs based on the\ndistribution of their data, with no singular conven-\ntion. From our data’s response time distribution\n(Figure 4), there is a clear bimodal distribution\nbetween “speeder\" response times and typical re-\nsponse times; a floor of t= 33.01 sufficiently ex-\ncludes the “speeder\" distribution to leave a sizeable\nsample of n= 504.\nWe also ran a separate pilot study in order to\ndetermine whether previous exposure to other NLP\nstudies on Mechanical Turk would bias subjects to\nperform the underlying NLI task. The results were\ninconclusive and thus we decided not to exclude\nparticipants who had participated in NLP studies\npreviously. See Appendix J.\nE.3 Instructions to Workers\nWorkers will see the following instructions before\nthey begin the study.\nYou will be given 5 short paragraphs on sepa-\nrate pages, each containing a yes-no question.\nYou will be paid at least $0.5 in addition to an\nextra $0.1 for each correct answer.\nIf you have a high number of correct an-\nswers, you will be qualified to participate in\n5Even if a subject’s data was excluded, they were still\ncompensated.\n7674\n0 100 200 300 400 500 600 700 800 900\n0\n0.002\n0.004\n0.006\n0.008\n0.01\nTotal response times (s)\nProportion of participants\nμ-σ=33.01 μ=126.62\nFigure 4: Histogram of subjects’ total time taken (in seconds) to complete the zero-shot study ( n = 597,µ =\n126.62,σ = 93.62). The left dotted line is the floor cutoff (t= 33.01): the 93 participants that had a total response\ntime lower than this cutoff were excluded from the main paper’s analysis.\nour full, $15 per hour study. You will not be\ngraded based on how fast you finish.\nYou have 10 minutes to answer all 5 ques-\ntions, which should be plenty of time, but\nplease complete our study in one continuous\nsession and do NOT navigate to other tabs or\ntasks, as we are measuring the time it takes\nyou to respond to each question.\nYou can only work on this HIT once. Multi-\nple submissions will be prevented.\nE.4 Evaluation Protocol Details\nFollowing Sanh et al. (2022) and Brown et al.\n(2020), we evaluate T0++ and Flan-T5 by rank\nclassification. We evaluate GPT-3.5 and GPT-4 by\nstring match, as it is no longer possible to get per-\ntoken token log-probabilities from OpenAI APIs.\nFortunately, GPTs are able to follow instructions\nthat return \"Yes\" or \"No\" as its first token, so we\nuse greedy decoding (temperature = 0) which is\nequivalent to single-token rank classification.\nEach item is input into the GPTs as a con-\ntent message in the user role, with the system\nprompt “You will be given a short paragraph with\na yes-no question. You strictly must answer only\n‘Yes’ or ‘No’ to the paragraph.”. No other instruc-\ntions or examples are given.\n7675\nF Additional Results for Zero-Shot\nExperiment\nWe present post-hoc analysis of our zero-shot ex-\nperiment data using different subgroups of partic-\nipants. For all figures in this section, medians are\nindicated in the bars. Each scatterpoint represents\naccuracy on a prompt within the semantic category,\ncalculated as the aggregated accuracy over multi-\nple annotators whose test item was constructed by\nthe prompt and one of the prompt’s five possible\nexamples.\nF.1 Using Control Scores as Qualification\nIn Figure 5, Figure 6, Figure 7 and Figure 8 we\npresent results using the controls as exclusion crite-\nria.\nConstraining data to subjects that score perfectly\non the General Control (Figure 6) decreases hu-\nman performance with misleading prompts on the\nNLI task in the test condition dramatically—that is,\nthese subjects perform the non-NLI task extremely\nwell right from the first item, given a prompt that\ninstructs a misleading task. This sample likely se-\nlects for subjects that interpret instructions most\nstrictly.\nIn contrast, results from constraining data to sub-\njects that score perfectly on the NLI Control (Fig-\nure 5) remain highly similar to our main results.\nThis supports that it is not incompetence with the\nNLI task that causes humans to score poorly in the\ntest condition if given a misleading prompt, but that\nthey are indeed following the given instructions to\nperform the misleading task.\nF.2 Without Response Time Qualification\nIn Figure 9, Figure 10, Figure 11, Figure 12 and Fig-\nure 13 we present results without using response\ntime qualification. Removing the response time\nqualification has the effect of introducing noise\ninto the data: the overall variance between dif-\nferent prompt categories reduces as all categories\ntend towards 0.50. However, the top-line trends\nwe argue in the main paper about human behavior\ngiven these different prompts remain observable\n(instructive > misleading-moderate ; instructive >\nmisleading-extreme ; instructive ≈ irrelevant).\n0\n0.2\n0.4\n0.6\n0.8\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.792\n0.286\n0.792 0.743\nFigure 5: Zero-shot accuracy data of human annotators\nwith perfect NLI Control scores and time above floor\ncutoff. (n= 384)\n0\n0.2\n0.4\n0.6\n0.8\n1\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.857\n0\n0.833\n0.663\nFigure 6: Zero-shot accuracy data of human annotators\nwith perfect General Control scores and time above floor\ncutoff. (n= 238).\n0\n0.2\n0.4\n0.6\n0.8\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.8\n0.174\n0.81 0.769\nFigure 7: Zero-shot accuracy data of human annotators\nwith a total score of at least 3 out of 4 Control items and\ntime above floor cutoff. (n= 340).\n7676\n0\n0.2\n0.4\n0.6\n0.8\n1\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.909\n0\n0.889\n0.633\nFigure 8: Zero-shot accuracy data of human annotators\nwith all perfect NLI and General Control scores and\ntime above floor cutoff. (n= 186).\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.718\n0.366\n0.727 0.681\nFigure 9: Zero-shot accuracy data of human annotators\nwith any response time (n= 597) (i.e., all data).\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.76\n0.286\n0.792 0.744\nFigure 10: Zero-shot accuracy data of human annotators\nwith perfect NLI Control scores, with any response time.\n(n= 409).\n0\n0.2\n0.4\n0.6\n0.8\n1\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.833\n0\n0.824\n0.626\nFigure 11: Zero-shot accuracy data of human annotators\nwith perfect General Control scores, with any response\ntime. (n= 273).\n0\n0.2\n0.4\n0.6\n0.8\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.773\n0.261\n0.826\n0.729\nFigure 12: Zero-shot accuracy data of human annotators\nwith a total score of at least 3 out of 4 Control items,\nwith any response time. (n= 377).\n0\n0.2\n0.4\n0.6\n0.8\n1\ninstructive misleadingirrelevant null\nNLI Accuracy\n0.9\n0\n0.889\n0.633\nFigure 13: Zero-shot accuracy data of human annotators\nwith all perfect NLI and General Control scores, with\nany response time. (n= 192).\n7677\nG Additional Figures and Data for Zero-Shot Experiment\nG.1 Accuracies Per Instruction\n0.83\n1.00 1.000.92 0.92\n0.00\n0.40\n0.000.00 0.00\n1.00 1.00\n0.20\n0.80\n0.40\n0.83\n0.67 0.750.67 0.67 0.67\nis_it_truedoes_this_implyguaranteed_truejustified_in_sayingmnli_ynwords_appearsportsballsummarizesentimentstart_with_theparaphrasesame_meaningfrenchgrammaticalsimilar_wordsgauss euthyphrozoninginflectionkatsuoboshiconcat_hpmconcat_phm0\n0.5\n1\n0.830.92 0.830.92\n0.75\n0.40\n0.60\n0.40\n0.00\n0.40\n0.60\n0.80\n0.60\n0.80\n0.60\n0.83\n0.67 0.670.75 0.67 0.75\n0\n0.5\n1\n0.920.920.92\n0.75\n0.92 0.80\n0.600.60\n0.80 0.80\n1.00\n0.600.60\n0.80\n1.00\n0.420.50\n0.83\n0.58 0.67\n0.33\n0\n0.5\n1\n0.750.75\n0.92 0.830.83\n0.60\n0.80\n0.40\n0.00\n0.20\n0.80\n0.60\n1.00\n0.80\n0.60 0.67\n0.50\n0.830.75 0.830.75\n0\n0.5\n1\n0.78 0.73\n0.94\n0.68 0.83\n0.21 0.29 0.33 0.40\n0.23\n0.50\n0.77\n0.25 0.23 0.00\n0.72 0.73 0.83 0.79 0.76 0.62\n0.84\n0\n0.5\n1\ninstructive misleading irrelevant null\nGPT-4 GPT-3.5 FLAN-T5 T0++ Humans\nNLI Accuracy\nFigure 14: Zero-shot NLI accuracy of human annotators vs. models, by specific instruction (see §H.1 for instruction\ntemplates). Bars are colored by instruction category.\n7678\nH Instructions and Examples\nH.1 All Instructions\ncategory name instruction\ninstructive MNLI-YN {sentence1} Using only the above description and what you know\nabout the world, \"{sentence2}\" is definitely correct. Yes, no or\nunclear?\ninstructive justified-in-saying {sentence1} Are we justified in saying that \"{sentence2}\"?\ninstructive is-it-true {sentence1} Based on the previous passage, is it true that \"{sen-\ntence2}\"?\ninstructive guaranteed-true {sentence1} Then, is it guaranteed true that \"{sentence2}\"?\ninstructive does-this-imply {sentence1} Question: Does this imply that \"{sentence2}\"?\nmisleading words-appear {sentence1} Do most of the words in the following sentence appear\nin the above paragraph? {sentence2}\nmisleading similar-words {sentence1} Are there lots of similar words between the above\npassage and the following sentence \"{sentence2}\"?\nmisleading same-meaning {sentence1} Does the above have the same meaning as \"{sen-\ntence2}\"?\nmisleading paraphrase {sentence1} Can the above be paraphrased as: \"{sentence2}\"?\nmisleading summarize {sentence1} Can the above be summarized as \"{sentence2}\"?\nmisleading start-with-the {sentence1} Does the previous paragraph start with \"the\"? {sen-\ntence2}\nmisleading grammatical {sentence1} Is the next sentence grammatically correct? {sen-\ntence2}\nmisleading sentimment {sentence1} Is the above paragraph a positive review? {sentence2}\nmisleading sportsball {sentence1} Is the above paragraph a piece of sports news? {sen-\ntence2}\nmisleading french {sentence1} Is the above text French? {sentence2}\nirrelevant zoning {sentence1} Single-family zoning is bad for American cities.\n\"{sentence2}\"?\nirrelevant inflection {sentence1} Inflections are annoying and thank god that\nMiddle English got rid of most of them. \"{sentence2}\"?\nirrelevant gauss {sentence1} When Bolyai sent Gauss his discovery of non-\nEuclidean geometry, Gauss replied that he arrived at the same\nresults 30 years ago. \"{sentence2}\"?\nirrelevant katsuoboshi {sentence1} If bonito flakes boil more than a few seconds,\nthe stock becomes too strong? \"{sentence2}\"?\nirrelevant euthyphro {sentence1} Is the pious loved by the gods because it is pious? Or\nis it pious because it is loved by the gods? \"{sentence2}\"?\nnull concat-phm {sentence1} {sentence2}\nnull concat-hpm {sentence2}{sentence1}\nTable 3: All prompts used in the main text of the paper. All templates use “Yes”/“No” as target words for the\nentailment and non-entailment classes, respectively.\n7679\nH.2 Example Sources for Test Condition\ncategory name dataset remarks\ninstructive MNLI-YN RTE, MNLI 6 entailment labels, 4 contradiction labels, 2 neutral labels were\nchosen; the latter two map to “No\" answers.instructive justified-in-saying\ninstructive is-it-true\ninstructive guaranteed-true\ninstructive does-this-imply\nmisleading words-appear RTE Examples were handpicked such that misleading task would have\na “Yes\" label while NLI task had “No\" label. From RTE labels, 3\ncontradiction labels and 2 neutral labels were chosen and mapped\nto the “No\" labels.\nmisleading similar-words\nmisleading same-meaning RTE Examples were handpicked such that the misleading task would\nhave a \"No\" label while NLI task had \"Yes\" label—i.e., through\nexamples where the second sentence was indeed an entailment but\nonly tangential to the main point of the first sentence.\nmisleading paraphrase\nmisleading summarize\nmisleading start-with-the RTE All examples were such that the premise paragraph did indeed\nstart with ‘the’ but the hypothesis sentence was not entailed, so\nthe misleading task answer was “Yes\" while the NLI task answer\nwas “No\". 3 contradiction labels and 2 neutral labels were chosen\nto map to “No\" labels.\nmisleading grammatical RTE Grammatically correct but non-entailing examples from RTE were\nchosen such that the misleading task answer is “Yes\" while the\nNLI task answer is “No\". 3 contradiction labels and 2 neutral\nlabels were chosen to map to “No\" labels.\nmisleading sentiment Amazon Polarity\n(Zhang et al., 2015)\nReviews were taken from the Amazon Polarity dataset as premise\nparagraphs. A hypothesis sentence was manually written based on\nthe review. If the review was positive, the hypothesis sentence was\nnot entailed; if the review was negative the hypothesis sentence\nwas entailed. There were 3 non-entailments and 2 entailments.\nmisleading sportsball RTE RTE examples that had nothing related to sports were chosen, such\nthat the misleading task answer is “No\" while the NLI task answer\nwas “Yes\".\nirrelevant zoning RTE, MNLI 6 entailment labels, 4 contradiction labels, 2 neutral labels were\nchosen; the latter two map to “No\" answers.irrelevant inflection\nirrelevant gauss\nirrelevant katsuoboshi\nirrelevant euthyphro\nnull concat-phm RTE, MNLI 6 entailment labels, 4 contradiction labels, 2 neutral labels were\nchosen; the latter two map to “No\" answers.null concat-hpm\nTable 4: All source datasets used for each prompt for the main text of the paper. All templates use “Yes”/“No”\nas target words for the entailment and non-entailment classes, respectively. For RTE examples, we collapse the\nSuperGLUE version’s “neutral” and “contradiction” to “non-entailment” such that all of our tasks are binary\nclassification. We balance the distribution of “contradiction“ and “neutral” labels within our study’s non-entailed\nitems.\n7680\nH.3 Example Sources for General Controls\ncategory name dataset remarks\nmisleading start-with-the RTE All examples were such that the premise paragraph did indeed\nstart with ‘the’ but the hypothesis sentence was not entailed, so the\nmisleading task answer was “Yes\" while the NLI task answer was\n“No\". 5 contradiction labels were chosen to map to “No\" labels.\nmisleading grammatical BLiMP (Warstadt et al.,\n2020)\nGrammatically incorrect sentences from BLiMP were used as\nhypothesis sentences, while grammatically correct premise para-\ngraphs were handwritten for the sentence, such that the misleading\ntask answer was “No\" while the NLI task answer was “Yes\".\nmisleading sentiment Yelp Polarity (Zhang et al.,\n2015)\nReviews were taken from the Yelp Polarity dataset as premise\nparagraphs and a hypothesis sentence was manually based on the\nreview. If the review was positive, the hypothesis sentence was not\nentailed; if the review was negative the hypothesis sentence was\nentailed. There were 3 non-entailments and 2 entailments.\nmisleading sportsball HuffPost (Misra and\nGrover, 2021; Misra,\n2022)\nExcerpts were taken from articles in the ‘Sports’ category of the\ndataset and non-entailing hypothesis sentences were manually\nwritten, such that the misleading task answer was “Yes\" while the\nNLI task answer was \"No\".\nmisleading french XNLI (Conneau et al.,\n2018)\nNon-entailing French XNLI examples were taken such that the\nmisleading task answer was “Yes\" while the NLI task answer was\n“No\".\n7681\nI Results on Zero-Shot Experiment Control Conditions\nI.1 NLI Controls\n0.78 0.75\n0.84\n0.69\n0.75\n0.18 0.22 0.16\n0.27 0.22\n0.04 0.03 0.04 0.03\nmnli_yn (103) is_it_true (97) does_this_imply (100)guaranteed_true (107)justified_in_saying (97)0\n0.2\n0.4\n0.6\n0.8\n1\nScore Attained 2 1 0\nPercentage of Subjects\nFigure 15: Subjects’ scores on the NLI control condition (n= 504, only subjects whose completion times were\nabove floor cutoff) Each bar represents the breakdown of percentage of subjects assigned the prompt who scored 0,\n1 and 2 out of two NLI control items presented.\n7682\nI.2 General (Surface Task) Controls\n0.65\n0.54\n0.27\n0.12\n0.64\n0.24\n0.52\n0.67 0.64\n0.44\n0.270.35\n0.27\n0.35\n0.23\n0.33\n0.18\n0.29\n0.280.44\n0.080.120.47 0.54 0.130.43 0.30\n0.04\n0.080.12\nparaphrase (49)summarize (49)same_meaning (52)similar_words (52)words_appear (53)french (49)sentiment (52)sportsball (48)grammatical (50)start_with_the (50)0\n0.2\n0.4\n0.6\n0.8\n1\nScore Attained 2 1 0\nPercentage of Participants\nFigure 16: Subjects’ scores on the general suface task controls (n= 504, only subjects whose completion times\nwere above floor cutoff) Each bar represents the breakdown of percentage of subjects assigned the prompt who\nscored 0, 1 and 2 out of two general control items presented; subjects were scored on correctly performing the\nmisleading task.\n7683\nJ Effect of Experience with Prior NLP\nStudies\nWe conducted a pilot (n= 29) to assess the effect\nof prior exposure to NLP studies on humans’ inter-\npretation of prompt instructions. In this pilot, we\nselect only subjects with no prior experience with\nNLP studies. All participants had to first screen\nthrough a pre-test with the question “How many\nmTurk tasks have you completed for language re-\nsearch (e.g. Stanford NLP Group, MIT NLP Group,\nNYU NLP Group, etc.)?\". Only participants who\nselected the option “None\" were qualified to con-\ntinue to take the study. 66 participants took the\npre-test and 29 qualified as subjects. We compare\nthese results to an earlier pilot (n= 67) that used\nthe same prompts and examples, with no filtering\nof participants based on previous exposure.\nComparing control condition scores (Figure 17\nvs. Figure 18 for NLI Controls’ Figure 19 vs. Fig-\nure 20 for General Controls’), subjects without\nprior exposure score higher on both the mislead-\ning task and NLI task (recall that in the controls,\nsubjects are scored on performing the surface task\nas explicitly described by the prompt). Comparing\ntest condition scores (Figure 21 vs. Figure 22), sub-\njects without prior exposure perform better at the\nNLI task when instructions are instructive and dra-\nmatically worse when instructions are misleading,\ncompared to the sample that was not controlled for\nexposure. These results suggest that subjects with-\nout prior exposure appear to follow explicit task\ninstructions more closely than the sample that was\nnot controlled for exposure.\nThe behavior of subjects without prior exposure\nto NLP studies is similar to the result when we\nselect subjects that score perfectly on the General\nControls (Figure 6)—suggesting that specifying\nfor NLP-study inexperience may select for a sam-\nple of humans who follow task instructions more\nstrictly. While we leave a full study that controls\nfor exposure to prior NLP studies to future work,\nwe predict that it will only strengthen the trend\nfor misleading prompts seen in our main results\n(namely, that humans do poorly on the actual task\nif given misleading prompts).\n1.00 1.00 \n0.75 \n0.86 \n0.75 \n0.25 0.14 0.25 \nis_it_true justified_in_saying guaranteed_true does_this_imply mnli_yn 0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n2 1 0 \nAccuracy \nFigure 17: NLI Control scores of subjects (n= 29) with\nno prior NLP experience.\n0.64 0.69 0.63 \n0.73 0.69 \n0.36 0.31 \n0.19 \n0.27 0.31 0.19 \nis_it_true justified_in_saying guaranteed_true does_this_imply mnli_yn 0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n2 1 0 \nAccuracy \nFigure 18: NLI Control scores of sample of subjects\n(n = 67) that were not filtered on prior exposure to\nNLP tasks.\n1.00 1.00 1.00 \n0.50 \n0.67 \n0.50 \n0.75 \n1.00 1.00 \n0.50 \n0.33 \n0.25 \n0.50 0.50 0.25 0.25 \nstart_with_the grammatical sentiment sportsball french words_appear similar_words same_meaning paraphrase summarize 0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n2 1 0 \nAccuracy \nFigure 19: General Control scores of subjects (n= 29)\nwith no prior NLP experience.\n0.60 \n0.75 \n0.17 0.20 \n0.64 \n0.40 \n0.86 \n0.43 \n0.67 \n0.20 \n0.25 \n0.67 \n0.20 \n0.29 \n0.60 0.14 \n0.43 0.17 \n0.20 0.17 0.60 0.71 0.36 0.14 0.17 \nstart_with_the grammatical sentiment sportsball french words_appear similar_words same_meaning paraphrase summarize 0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \n2 1 0 \nAccuracy \nFigure 20: General Control scores of sample of subjects\n(n= 67) that were not filtered on prior exposure to NLP\ntasks.\n7684\n1.00 1.00 1.00 1.00 1.00 \n0.50 \n1.00 1.00 1.00 \n0.67 \nis_it_true justified_in_saying guaranteed_true does_this_imply mnli_yn start_with_the grammatical sentiment sportsball french words_appear similar_words same_meaning paraphrase summarize inflection zoning euthyphro katsuoboshi gauss null 0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \ninstructive misleading irrelevant null \nAccuracy \nFigure 21: Per-instruction accuracy on the test condition item of subjects with no prior NLP experience (n= 29).\n0.67 \n1.00 1.00 \n0.67 \n0.33 0.33 \n0.75 \n0.33 \n0.75 \n0.50 0.50 \n1.00 1.00 1.00 1.00 \n0.80 \nis_it_true justified_in_saying guaranteed_true does_this_imply mnli_yn start_with_the grammatical sentiment sportsball french words_appear similar_words same_meaning paraphrase summarize inflection zoning euthyphro katsuoboshi gauss null 0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \ninstructive misleading irrelevant null \nAccuracy \nFigure 22: Per-instruction accuracy on the test condition item of subjects that were not filtered based on prior\nexposure to NLP tasks (n= 67).\n7685\nK Few-Shot Experiment Post-Experiment Survey\nS/N Prompt\nCategory\nHow did you decide to choose “Yes\" or\n“No\"?\nWhat did you think about the instructions\nwe gave?\nFew-Shot With Labels\n1 Mis-\nModerate\nIn the first few questions, my strategy is to\nread through the entire paragraph or sentence\nand then decide whether the paraphrased sen-\ntence makes sense or not. However, then I\nstarted to look at the paraphrased sentence\nfirst and decide whether it is correct or wrong\nbased on the given piece of text. Initially, I\nalso considered whether the paraphrased sen-\ntence captured all the major details or not, but\nthe quiz later shows that comprehensiveness\nis not a factor.\nI’d say the instructions are not quite direct? In\nmy opinion, it would make more send to ask\nif the given sentence is correct or not than to\nask if it paraphrases the text.\n2 Mis-\nExtreme\nI chose my answer based on what I believed\nwas correct.\nI don’t really like the question, \"Is this gram-\nmatically correct\". Some were definitely not\ngrammatically correct (capitalization errors,\npast/present tense), but the answer was still\nyes. I feel like the question should be changed\nbecause it seems like the question is actually,\n\"Is this statement true based on the context\ngiven in the paragraph\".\nFew-Shot Without Labels\n3 Irrelevant I tried to see whether what was stated in the\nquestion was consistent with the preceding\nsentences. Sometimes it involved a logical de-\nduction, and other times it was not implied\nat all by the other sentences but just related.\nSometimes I was unsure what to choose be-\ncause the premise of the question was wrong.\nI was confused because that statement was\nincluded in every question, but it didn’t seem\nrelevant.\n4 Mis-\nModerate\nI’m looking for whether the information pro-\nvided in the first half can be more or less en-\ncapsulated by the second half, meaning that if\none were to read the first half and another the\nsecond, they would come away to the same\nconclusion.\nThere is a level of ambiguity at first as I con-\nsidered what exactly it entailed: whether or\nnot its a “correct” statement given the con-\ntext is a confounding factor, when it shouldn’t\ninfluence whether or not its a good paraphras-\ning.\n5 Mis-\nExtreme\nI looked at whether the sentence was accurate\nto the information given in the text, and also\nif the sentence itself had correct grammatical\nstructure. It was a little difficult because some\nof the sentences made inferences that weren’t\nexplicit in the given text, so I wasn’t sure if\nthat was a grammatical error or not.\nUsually, I think of something as being gram-\nmatically correct when the sentence has cor-\nrect grammatical structure, including punc-\ntuation and capitalization. Since most of the\nsentences seemed to fit this, I thought that\nmaybe grammar also encompasses the valid-\nity of the statement based on the text, so I\nchose my answers based on that.\n6 Mis-\nExtreme\nI chose \"Yes\" when the shorter sentences\npresent accurate information from the longer\nsentences. I was kind of confused about the\nquestion because most of the sentence (maybe\nall) seemed to be grammatically correct.\nFor the first two questions, I was paying at-\ntention to whether the sentences were actually\ngrammatically correct. Later on, I just tried\nto see if the shorter sentences give accurate\ninformation based on the longer parag traphs\nabove.\nTable 5: Sample of free-text responses of subjects to the questions “How did you decide to choose ‘Yes’ or ‘No’?\"\nand “What did you think about the instructions we gave?\". Responses were elicited from subjects after they answered\n32 items with their assigned prompt. Each table row indicates responses from one unique subject.\n7686"
}