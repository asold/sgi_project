{
  "title": "Entailment Semantics Can Be Extracted from an Ideal Language Model",
  "url": "https://openalex.org/W4385572881",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5109946254",
      "name": "William Merrill",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5089452138",
      "name": "Alex Warstadt",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5081824828",
      "name": "Tal Linzen",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3212373684",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1954216965",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4235370100",
    "https://openalex.org/W2525032226",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W2264742718",
    "https://openalex.org/W1622302876",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2638659725",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4251372957",
    "https://openalex.org/W2100513762",
    "https://openalex.org/W2093541146",
    "https://openalex.org/W4288412805",
    "https://openalex.org/W3203259592"
  ],
  "abstract": "Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.",
  "full_text": "Erratum Note\nOn October 6, 2023, Benjamin Spector and Em-\nmanuel Chemla found a mistake in the main result\n(Theorem 2) of this paper.1 The technical problem\nleads to an alternate interpretation of our results.\nOur distributional “entailment test” does not ac-\ntually detect entailment, but, rather, detects either\nentailment or near contradiction, meaning two sen-\ntences are only consistent in a rare set of worlds,\nwithout distinguishing which.\nImplications\nWe believe near contradiction may be rare com-\npared to entailment, so the test may still tend to\nidentify entailment correctly assuming realistic\ndata distributions. However, our paper’s main goal\nwas never to propose a practical NLI method but\nrather to make the theoretical claim that mastering\nthe LM objective perfectly implies acquiring a full\nmodel of entailment. The inability of our distri-\nbutional entailment test to distinguish entailment\nfrom near contradiction means the reconstruction\nof semantics it would extract from an idealized,\nperfect LM could still be fundamentally lossy. Fu-\nture work should investigate whether distributional\nsemantics must fundamentally confuse entailment\nand near contradiction or whether there is some\nother way to distinguish them with form alone.\nThe Conceptual Problem\nThe edge case that breaks Theorem 2 is simple to\ndescribe conceptually. Our entailment test attempts\nto use the co-occurrence probability p(xy) of two\nsentences x, yto infer something about their se-\nmantic relationship. Under a Gricean speaker, the\nprobability of a redundant pair of utterances x, y\nshould be ∼0 and the probability of contradictory\nutterances x, yshould be exactly 0. The issue is\nwhen y nearly contradicts x, e.g.:\nx = I’m not in North America.\ny = I’m in a US state.\ny nearly contradicts x because they are both satisfi-\nable only in worlds where the speaker is in Hawaii,\nwhich we assume p(w) makes unlikely. In such\ninstances of near contradiction, it is possible that\np(xy) is slightly above 0 (like for entailment), and\nthus Theorem 2 detects entailment incorrectly.\n1The authors thank Sophie Hao, Noah A. Smith, and\nZhaofeng Wu for feedback on this erratum.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\n# worlds where x is true and y is false\n0.20\n0.15\n0.10\n0.05\n0.00\n0.05\nentailment score\nFigure 1: The entailment test score between x and y as\na function of the number of worlds where x is true but\ny is false. Entailment (left intercept with 0) and near\ncontradiction (right intercept with 0) look the same!\nDetailed Problem and Revised Analysis\nThe technical problem in the original proof of The-\norem 2 was that Lemma 1 was applied with unmet\npreconditions. Specifically, it assumed that the\nspeaker utility Iℓ(z; w) is at least 0 for all utter-\nances z and worlds w, but, in fact, this utility is\n−∞ in worlds wherez is false. Near contradictions\ncan then achieve an average exponentiated infor-\nmation content of 1 because exp(−∞) in many\nworlds is balanced out by large positive informa-\ntion in a few worlds. Formally, let Y = JxK ∩ JyK\nbe the worlds where x, yare both true. We show in\n§H that the original entailment test is 0 when\np(Y )IY = 1, (1)\nwhere IY ≜ E\nw\n[exp(Iℓ(y | x; w)) | w ∈ Y ].\nWe can see that (1) has two distinct solutions:\nEntailment Solution. As expected, (1) is satis-\nfied when x entails y since p(Y ) = 1 and IY = 1.\nNear-Contradiction Solution. Assume for sim-\nplicity that Iℓ(y | x; w) = IY for all w ∈ Y . If y\nnearly contradicts x, p(Y ) is small because there\nare very few contexts where x, yare both true. On\nthe other hand, IY is large because y is very infor-\nmative when it is true. It is possible to calibrate Y\nsuch that these factors multiply to 1.\nFigure 1 illustrates these two solutions using the\nGricean speakers from §6. For two utterances x, y,\nwe vary the number of worlds where x is true but\ny is false, ranging from entailment to contradiction.\nThe test score crosses 0 twice: for entailment on\nthe left and near contradiction on the right.\nEntailment Semantics Can Be Extracted from an Ideal Language Model\nWilliam Merrill\nNew York University\nAlex Warstadt\nETH Zürich\n{willm,linzen}@nyu.edu alexanderscott.warstadt@inf.ethz.ch\nTal Linzen\nNew York University\nAbstract\nLanguage models are often trained on text\nalone, without additional grounding. There is\ndebate as to how much of natural language se-\nmantics can be inferred from such a procedure.\nWe prove that entailment judgments between\nsentences can be extracted from an ideal lan-\nguage model that has perfectly learned its target\ndistribution, assuming the training sentences\nare generated by Gricean agents, i.e., agents\nwho follow fundamental principles of commu-\nnication from the linguistic theory of pragmat-\nics. We also show entailment judgments can\nbe decoded from the predictions of a language\nmodel trained on such Gricean data. Our results\nreveal a pathway for understanding the seman-\ntic information encoded in unlabeled linguistic\ndata and a potential framework for extracting\nsemantics from language models.\n1 Introduction\nRecent advances in building computational models\nof language have been powered by distributional\nsemantics: the idea that the possible surrounding\ncontexts for a text span encode its meaning (Firth,\n1957). In particular, large pretrained language mod-\nels (LMs; Peters et al., 2018; Devlin et al., 2019;\nBrown et al., 2020) have become an integral part of\nNLP systems: the representations that emerge from\ntraining to predict missing words in a text are em-\npirically useful for natural language understanding\ntasks.\nDespite this empirical progress, Bender and\nKoller (2020) argue LMs cannot learn to under-\nstand the semantics of sentences. This is because\nof a mismatch between the LM training objective—\npredicting missing words in text (“form”)—and\nBender and Koller’s conception of meaning as the\nrelation of a sentence to the external world. Thus\nBender and Koller claim “that the language model-\ning task, because it only uses form as training data,\ncannot in principle lead to learning of meaning.”\nIn this paper, we argue meaning can be learned\nfrom form because the communicative goals of hu-\nman authors encode semantic information in unla-\nbeled text. We show how this semantic information\ncan be extracted to resolve semantic relations be-\ntween sentences (e.g., whether one sentence entails\nanother): in this inferentialist sense, ideal LMs en-\ncode the meaning of sentences. This argument has\nbeen raised speculatively by others (Michael, 2020;\nPotts, 2020; Bommasani et al., 2021), but we will\nrigorously justify it here with formal results.\nTo give the simplest (and least general) illustra-\ntion of our argument, we first assume training data\nis generated by overly idealized uniformly truthful\nspeakers: agents who decide what to say by picking\nsentences they consider true uniformly at random.1\nThis very coarsely captures human authors’ goal of\nbeing informative (rather than misleading) to their\nlisteners (Grice, 1975). In Theorem 1, we prove a\nsentence x entails sentence y if and only if, after\nuttering x, a uniformly truthful speaker is just as\nlikely to say y as to repeat x. Thus, entailment\nsemantics can be extracted from probabilistic lan-\nguages generated by uniformly truthful speakers.\nUniformly truthful speakers are not a realistic\nmodel of humans: while humans favor true sen-\ntences to false ones (Grice, 1975), not all true sen-\ntences are equally likely to be produced. It is a\ncommon principle in linguistic theories of pragmat-\nics that human speakers choose their utterances in\norder to balance two competing objectives: (a) con-\nveying information to their listener and (b) brevity\n(Levinson et al., 1983; Grice, 1975). We define a\nclass of Gricean speakers who optimize for these\nobjectives, and prove in Theorem 2 that x entails y\nif and only if a simple equation holds in terms of\ntext probabilities produced by such speakers. Thus,\n1Studying the ability of LMs to understand programming\nlanguage semantics, Merrill et al. (2021) make a similar as-\nsumption that programmers are more likely to write true asser-\ntion statements than false ones.\nentailment semantics can be decoded from proba-\nbilistic languages generated by Gricean speakers.\nThe previous results assume access to a lan-\nguage’s ideal likelihood function, but, in practice,\none only ever receives a corpus sampled from the\nlanguage. Moving to the corpus setting, we analyze\nhow much data allows approximately computing\nour derived entailment test using probabilities esti-\nmated from sentence frequencies in a corpus. We\nfind that the corpus size needed to guarantee the\nentailment test holds approximately is inversely\nrelated to the likelihood of the sentences. We es-\ntimate that approximating the entailment test be-\ntween 4-word sentences using corpus frequencies\nis possible with ∼ 1010 sentences, about the size\nof the GPT-3 training data (Brown et al., 2020).\nOn the other hand, approximating the entailment\ntest for 10-word sentences should be possible with\n∼ 1017 sentences, or ∼ 107 GPT-3 corpora. Thus,\nextracting entailment judgments using corpus fre-\nquencies requires an infeasible amount of data—\neven by modern NLP standards.\nTo overcome this limitation, one might hope to\nuse probabilities estimated by LMs to extract en-\ntailment judgments between longer sentences that\nare rare even in a large corpus. With synthetic data\ngenerated by Gricean speakers, we find that entail-\nment can be decoded from n-gram LM predictions\nto some extent. However, we speculate that current\nneural LMs may not score the probability of rare\ntext well enough to enable decoding entailment\njudgments between natural language sentences.\nIn summary, our main contribution is to show a\ncorrespondence between the semantics of text and\nits likelihood, assuming the likelihood function\nmatches models of human text production from lin-\nguistic theory. Determining whether a sentence in a\nprobabilistic language entails another sentence can\nbe reduced to modeling the probabilities of strings\nin the language. In practice, entailment judgments\nbetween very short sentences can be extracted from\ncorpus frequencies, but this becomes infeasible for\nslightly longer sentences. LMs can in principle be\nused to extrapolate the likelihood of longer strings,\nbut we hypothesize current LMs are not well-suited\nfor doing so well enough to enable extracting en-\ntailment from natural language. Our theory demon-\nstrates a formal sense in which unlabeled text data\nencodes linguistic meaning and makes quantitative\npredictions for (a) how to extract semantics from\ntext corpora and (b) how much data this requires.\n2 Definitions\n2.1 Sentences and Worlds\nLet X be a finite set of sentences, and W a count-\nable2 set of possible world states. A sentence x is\na string whose denotation JxK is a proposition, i.e.,\na set of world states ( ⊆ W) where x is true. Fol-\nlowing standard conventions in formal semantics\n(cf. Heim and Kratzer, 1998), the set JxK can be\nequivalently viewed as a function mapping a world\nstate w to {0, 1} that indicates whether x is true in\nw, which we will write as JxK(w). We imagine w\nto encode a partial description of the world, much\nlike the concept of a situation in formal semantics\n(Kratzer, 2021). For simplicity, we assume an in-\ndividual’s subjective belief state can be modeled\nas the unique, maximal w that fully describes the\nfacts which they believe to be true.\nExample x = John has at least two cats.\nLet W = {w0, w1, w2, w3} be the set of possible\nworlds, where wn denotes the state in which John\nhas n cats. Then JxK = {w2, w3}, because John\nhas at least two cats in these worlds. Furthermore,\nit holds that JxK(w2) = 1, but JxK(w1) = 0.\n2.2 Speakers and Texts\nWe refer to a sequence of sentences z ∈ X∗ as a\ntext.3 The meaning of a text is the set of worlds\nconsistent with all its sentences, i.e.,\nJzK =\n|z|\\\nt=1\nJztK.\nWe will imagine that a text z ∈ X∗ is produced by\niteratively sampling zt ∈ X ∪ {$} from a speaker\nmodel p(zt | z<t, w). p(zt | z<t, w) represents the\nprobability of saying sentence zt with belief state\nw after having said z1 ··· zt−1. Let $ ̸∈ Xbe a\nspecial end of sequence token satisfying J$K = W.\nWe refer to any text ending with $ as complete.\nGiven a world w, an incomplete text z ∈ X∗ or\ncomplete text z ∈ X∗$ has conditional probability\np(z | w) =\n|z|Y\nt=1\np(zt | z<t, w).\nThe conditional probability of an incomplete text\nrepresents the probability of observing z as the\n2Our results extend to uncountable sets of world states if\nentailment is relaxed to hold almost surely (cf. §B). Alterna-\ntively, our results apply as-is if we assume a countable set of\nequivalence classes over uncountably many worlds.\n3Where X∗ denotes the Kleene star closure of X.\nprefix of a text written by a human with beliefs\nw. In contrast, the probability of a complete text\nrepresents the probability that a speaker produces\nz and no further text. The conditional distribution\np(z | w) cannot be observed directly by a LM,\nsince w is a latent variable missing from the train-\ning data. Rather, a LM has access to texts that have\nbeen generated by speakers across many possible\nbelief states. Mathematically, this can be expressed\nby saying a LM’s target distribution is a marginal\ndistribution over z ∈ X∗ ∪ X∗$ according to some\nprior distribution over worlds p(w):\np(z) = E\nw∼p(w)\n[p(z | w)]\n= E\nw∼p(w)\n\" ∞Y\nt=1\np(zt | z<t, w)\n#\n.\nThe prior p(w) represents the probability that a\nspeaker contributing to the corpus will have belief\nstate w—we make no assumptions about its form\nbesides that p(w) > 0 for all w ∈ W, and, for ev-\nery sentence, there is some world state that makes\nthat sentence true. In contrast to p(z), which corre-\nsponds to the expected corpus frequency of z, we\ndenote by p(JzK) the probability that z is true.4\nExample Let z be the 2-sentence text:5\nz1 = We swung our swords.\nz2 = That was ever so long ago.\nLet p be the distribution of all possible English\nweb texts. The marginal probability p(z) can be de-\ncomposed across many possible worlds. One such\nworld w1 might be the world where the speaker is\nthe semi-legendary Viking hero Ragnar Loðbrók\n(in modern English translation); another world w2\nmight be the perspective of a Reddit user reviewing\na coffee maker. Each of these worlds corresponds\nto one term in a sum over all worlds. We expect\np(z | w1) to be higher than p(z | w2) since it\nis more likely for a medieval literary character to\nutter z than a modern product reviewer. Finally,\np(z | w1) can be factored as\np(z1 | w1)p(z2 | z1, w1).\nIn contrast to p(z), which counts all contexts where\nz is the beginning of a longer text, p(z$) measures\nthe frequency of z1z2 followed by nothing else.\n4The notation explicitly represents the probability mass\nassigned to the set of worlds where z is true.\n5Text taken from the Wikipedia page for the skaldic poem\nKrákumál, written in Ragnar’s voice.\n2.3 Distributional and Semantic Relations\nDistributional Relations A distributional rela-\ntion d is a relation over sentences x and y defined\nin terms of likelihood of different texts under some\ndistribution p. Let dp(x, y) be the value of the\ndistributional relation d between sentences x, yac-\ncording to distribution p. If we train an LM on\ntexts sampled from a target distribution p, the LM\nestimates a predictive distribution ˆp. Thus, any LM\nparameterizes dˆp: an instantiation of the distribu-\ntional relation d with respect to the probabilities\nlearned by the LM. If the LM perfectly approxi-\nmates p(x) for all x, then dˆp = dp by construction.\nExample Define the distributional relation d\n(with respect to some distribution p) such that\nd>\np (x, y) ⇐⇒ p(x) > p(y). d>\np (x, y) says x\nis more likely than y according to p. If ˆp represents\nLM predictions trained on the target distribution\np, than d>\nˆp (x, y) says whether the LM predicts a\nsentence x is more likely than another sentence y.\nSemantic Relations In contrast, a semantic rela-\ntion between x and y is a relation defined in terms\nof their denotations JxK and JyK. We will focus on\nthe key semantic relation of entailment:\nDefinition 1 For two sentences x, y∈ X, x entails\ny if and only if JxK ⊆ JyK.\nIt is not clear prima facie if LMs can represent\nentailment relations. However, it could be that a\nsemantic relation s can somehow equivalently be\nwritten as a distributional relation dp. If so, a LM\nthat perfectly approximates p could be understood\nto encode s, since s can be extracted from ˆp via dˆp.\nFormally, we can ask if a semantic relation can\nbe alternatively expressed as a distributional rela-\ntion by analyzing if there exists an isomorphism\nbetween a semantic relation s(JxK, JyK) and some\ndistributional relation dp(x, y):\nDefinition 2 (Isomorphism) A semantic relation\ns is isomorphic to a distributional relation d under\nspeaker p if and only if, for all x, y∈ X,\ns(JxK, JyK) ⇐⇒ dp(x, y).\nIf Definition 2 holds under a speaker model p,\nthen predicting whether s holds between two sen-\ntences is reducible to perfectly modeling the prob-\nabilities of texts generated by p. Our goal going\nforward will be to derive distributional relations\nisomorphic to entailment assuming p models the\ngoals of humans when they produce text.\n3 Uniformly Truthful Speakers\nWe start by illustrating our research question and\ntechnical approach assuming an overly simple\nmodel of humans as uniformly truthful speakers.\nA uniformly truthful speaker chooses a sentence\nto produce by selecting one of the true sentences\nthat holds in their belief state uniformly at ran-\ndom. This very coarsely captures the property of\nnatural language pragmatics that subjectively true\nsentences tend to be more likely than false ones,\nalthough it does not account for many other factors\nthat influence human speech patterns in complex\nways (Grice, 1975).6 Let n(w) be the number of\nsentences true in world w. We can formally define\na uniformly truthful speaker as follows:\nDefinition 3 A speaker p is uniformly truthful if,\nfor all sentences x ∈ X ∪ {$},\np(x | w) = JxK(w)P\nx′Jx′K(w) = JxK(w)\nn(w) .\nIn other words, p uniformly spreads probability\nmass across all sentences that are true in world w.\nWe will show that, if the corpus consists of text\nwritten by uniformly truthful speakers, entailment\ncan be decided by a distributional relation. The\nfollowing lemma will be a core technical tool in\nour analysis. Informally, it is useful because it es-\ntablishes a correspondence between relations over\nsets of worlds and probabilities.\nLemma 1 Let 1 S be the indicator function for set\nS. For sets A, B such that A ⊆ B ⊆ W, and\nc : W →R+, A = B if and only if\nX\nw∈W\n1 A(w)c(w) =\nX\nw∈W\n1 B(w)c(w).\nProof. We will prove that B ⊆ Aby contradiction.\nAssume there exists w ∈ Bsuch that w ̸∈ A. Then\nthe right sum contains the positive termc(w), while\nthe left sum does not. Because all terms in the right\nsum are positive, the left sum must contain at least\none term c(w′) that the right sum does not. Thus,\nw′ ∈ Abut w′ ̸∈ B. But this has violated our\nassumption that A ⊆ B.\nWe now use Lemma 1 to derive a simple distri-\nbutional relation that is isomorphic to entailment.\n6LMs sometimes generate objectively false statements (Lin\net al., 2022), presumably due to the occurrence of such facts in\ntheir training data. This is actually consistent with a uniform\ntruthfulness assumption, which only requires that speakers\nonly produce sentences they believe are true, not sentences\nthat are actually true in some objective sense.\nTheorem 1 If p is a uniformly truthful speaker,\nthen entailment is isomorphic to a distributional\nrelation. Specifically, for all sentences x, y∈ X,\nJxK ⊆ JyK ⇐⇒ p(xy) = p(xx).\nProof. dp(x, y) holds if and only if\np(xy) = p(xx)\nE\nw\n\u0014JxK(w)JyK(w)\nn(w)2\n\u0015\n= E\nw\n\u0014JxK(w)JxK(w)\nn(w)2\n\u0015\nE\nw\n\u0014JxK(w)JyK(w)\nn(w)2\n\u0015\n= E\nw\n\u0014JxK(w)\nn(w)2\n\u0015\n.\nAn expectation in a countable space is a sum\nweighted by probability masses. So, by Lemma 1,\nthis holds iffJxK = JxyK = JxK∩JyK. We conclude\np(xy) = p(xx) if and only if JxK ⊆ JyK.\nA similar proof suffices to show that the follow-\ning isomorphism also holds:\nCorollary 1.1 If p is a uniformly truthful speaker,\nthe following isomorphism holds for all x, y∈ X:\nJxK ⊆ JyK ⇐⇒ p(xy) = p(x$).\n3.1 Discussion\nUniformly truthful speakers resemble humans in\nthat they mimic the tendency of humans to tell the\ntruth about what they believe. However, they are\nclearly too simple to account for human speech\npatterns. Most crucially, humans generally aim to\nproduce informative speech, rather than sampling\ntrue sentences at random. More fundamentally,\nnatural language has a countably infinite number\nof possible sentences, so a uniform distribution\nover all true sentences is not even mathematically\nwell-defined. These limitations motivate our more\ninvolved analysis of Gricean speakers, which will\nadapt the technical tools used in this section.\n4 Gricean Speakers\nIn this section, we will define a new class of speak-\ners who pick sentences in order to be informative\nto their listener, while also trying to be concise.\nTo do this, we will draw on information theory to\nformalize what it means for a speaker to be infor-\nmative. We will then derive a distributional relation\nthat is isomorphic to entailment for Gricean speak-\ners, which is a generalization of the relation for\nuniformly truthful speakers from §3.\n4.1 Definition\nInformation The first step towards formalizing\nGricean speakers is to define a notion of the se-\nmantic information contained in a sentence. We\nformalize a listener ℓ(w | z) as the inverse of a\nspeaker: Given a text z ∈ X∗, a listener produces\na distribution over possible world states. Then, in a\ngiven world w we can define the information that a\ntext conveys to the listener as the reduction in the\nnumber of bits needed to transmit w to ℓ after they\nhave read z compared to before they have read z.\nDefinition 4 The information content of a text z ∈\nX∗ ∪ X∗$ to a listener ℓ(w | z) is7\nIℓ(z; w) = log ℓ(w | z) − log ℓ(w).\nIn other words, the information content of a text\nis the reduction in ℓ’s code length for the world\nafter having read the text compared to beforehand.\nWe can naturally extend Definition 4 to measure\nthe conditional information conveyed by sentence\ny given that x has already been produced:\nDefinition 5 The information content of y ∈ X∗ ∪\nX∗$ given x ∈ X∗ to a listener ℓ(w | z) is\nIℓ(y | x; w) = Iℓ(xy; w) − Iℓ(x; w)\n= log ℓ(w | xy) − log ℓ(w | x).\nInformative Speaker We now define a Gricean\nspeaker in terms of Iℓ. Our definition general-\nizes the rational speech acts model (Goodman\nand Frank, 2016), but makes weaker assumptions\nabout the listener and allows a dynamic semantics\nwhere later sentences can condition on previous\nones (Lewis, 1979; Kamp, 1981; Heim, 1982). We\ndefine an utterance’s utility as a convex combi-\nnation of its information content and its cost to\nproduce, operationalizing the Gricean idea that\nspeakers pick utterances by weighing their in-\nformativeness against their cost. The cost func-\ntion c : X∗ ∪ X∗$ → R can be any measure\nof sentence complexity (e.g., length) satisfying\nc(xy) = c(x) + c(y) for x, y∈ X∗ ∪ X∗$.8\nDefinition 6 A speaker p is Gricean if there exists\na listener ℓ(w | z), some α >0, and a cost function\nc such that, for all z ∈ X∗ ∪ X∗$:9\np(z | w) ∝ exp (αIℓ(z; w) − c(z)) .\n7For convenience, we let log 0 =−∞ and ∞ − ∞= 0.\n8This is satisfied when c(x) is the length of x, but also for\nother options like the corpus frequency of x (Goodman and\nFrank, 2016) or the depth of the syntactic tree of x.\n9To clarify, we assume p(z | w) is locally normalized at\neach sentence rather than globally. However, our results also\ngo through with global normalization as well.\nFurther, ℓ must satisfy the following for allx ∈ X∗,\ny ∈ X ∪ {$}, and w ∈ W,\nIℓ(y | x; w) = 0 ⇐⇒ JxK(w) → JyK(w).\nIn other words, the speaker must be trying to\nconvey information about the state of the world to\nsome listener who fully absorbs the semantic in-\nformation in all sentences they have already heard:\nclarifying already established information will not\nbenefit the listener. We can formalize this by deriv-\ning p(y | x, w) for x ∈ X∗ and y ∈ X ∪ {$}:\np(y | x, w) = p(xy | w)\np(x | w)\n∝ exp (αIℓ(y | x; w) − c(y)) .\nNotably, the probability ofy given x depends on the\nconditional information of y given x, which means\nonly information conveyed by y that is nonredun-\ndant with x will make y more likely.10\n4.2 Results\nProofs are in §C. Under a Gricean speaker, the cost\nof an utterance can be expressed:\nLemma 2 For any Gricean speaker p and x ∈ X,\np(x$)\np(xx) = exp(c(x))\nexp(c($)) .\nCorollary 2.1 Under a Gricean speaker, for all\nx ∈ X, c(x) = log p(x$) − log p(xx) + c($).\nCorollary 2.1 says that a sentence is costly to\nthe extent that it is unlikely to be repeated twice,\ngiving an intuitive characterization of this quantity\nin terms of text probabilities. Now, we will use this\ncharacterization of cost to derive a distributional\nrelation that is isomorphic to entailment.\nTheorem 2 Under any Gricean speaker p, en-\ntailment is isomorphic to a distributional relation.\nSpecifically, for all sentences x, y∈ X,\nJxK ⊆ JyK ⇐⇒ p(xy)\np(x$) = p(yy)\np(y$).\nIf we allow our decision rule to depend on the\ncost function c in addition to probabilities, we can\nsimplify Theorem 2 as follows:\n10From a technical perspective, the exp in Definition 6\nis justified by the fact that probabilities decompose multi-\nplicatively, i.e., p(xy | w) = p(x | w)p(y | x, w), but the\ninformation content and cost of text should decompose ad-\nditively across different sentences. Applying basic exponent\nrules shows that Definition 6 satisfies this desideratum.\nCorollary 2.1 Under any Gricean speaker p, for\nall sentences x, y∈ X, JxK ⊆ JyK if and only if\nlog p(x$) − log p(xy) = c(y) − c($).\nIf we imagine c(y) − c($) = 0 for a uniformly\ntruthful speaker, we see the equation in Theorem 2\nis a generalization of the equation in Theorem 1.\n4.3 Discussion\nGricean speakers are a general enough model of\nhumans speakers to capture the basic pragmatic\nprinciples influencing speech production. Thus, it\nis notable that Theorem 2 establishes a closed-form\ndistributional relation isomorphic to entailment.\nOne conceptual limitation of Gricean speakers\nis that their simulated listener must fully consume\ninformation, such that redundantly conveying the\nsame information twice will not lead to any infor-\nmation gain the second time. This contrasts with\nreal speech, where potential interpretation errors by\nthe listener incentivize the speaker to be somewhat\nredundant (Degen et al., 2019). Mathematically,\nthis would violate the axiom of Definition 6 that\nIℓ(y | x; w) = 0 ⇐⇒ JxK(w) → JyK(w).\nExtending Theorem 2 to speakers who use redun-\ndancy to account for noise and interpretation errors\nis an interesting direction for future work.\nAnother interesting extension would be formal-\nizing speakers who aim to be informative regarding\nsome question under discussion, rather than be-\ning generally informative about w (cf. Goodman\nand Lassiter, 2015). This could encompass both\n“what” questions that aim to clarify some aspect of\nthe world, and “why” questions that aim to convey\nexplanations for established facts.\n5 Decoding Entailment from Empirical\nText Frequencies\nWe have so far shown that entailment judgments\ncan be extracted from the sentence probabilities\nin the ideal distribution p(z). What happens if,\nmore practically, we estimate the probability of a\nsentence by its frequency in a large corpus sampled\nfrom p(z)? We prove this method enables feasible\nextraction of entailment judgments between very\nshort sentences, but the corpus size may become\nintractably large for longer sentences.\nImagine we have a finite corpus of iid sentences\n{Zi}n\ni=1, each sampled from p(z). Let ˆp(z) be the\nempirical frequency of a text z in the corpus, i.e., if\nπ(z, z′) returns whether text z is a prefix of text z′,\nˆp(z) = 1\nn\nnX\ni=1\nπ(z, Zi).\nSince p(z) encodes entailment via our extrac-\ntion rules, ˆp(z) will encode entailment between\nsentences if ˆp(z) is close to p(z). A naive notion\nof closeness is to guarantee, for all ϵ, there exists\nsome number of texts n such that, with high prob-\nability, |p(z) − ˆp(z)| < ϵ. But this notion is not\nstrict enough: if p(z) is small, this difference will\nalso be small, even if ˆp(z) is not a good approxima-\ntion of p(z) on a relative scale. Instead, we want to\nguarantee that ˆp(z)/p(z) converges to 1, or, equiv-\nalently, that their difference as log probabilities\nconverges to 0. This ensures that convergence will\nstill be meaningful for low-probability sentences,\nwhich most sentences are in natural language.\nUnder this standard, rarer sentences take more\nsamples to approximate. Define the sentence com-\nplexity Kp(z) = 1\np(z). We bound the approximation\nerror in terms of Kp(z).11\nLemma 3 For z ∈ X∗ ∪ X∗$ and δ >0, it holds\nwith probability at least 1 − δ − (1 − p(z))n that\n|log p(z) − log ˆp(z)| ≤\nr\nKp(z)\nδn .\nTo make this bound non-vacuous, n must be\nlarge enough to counteract Kp(z) and bring (1 −\np(z))n close to 0. Thus, good approximation re-\nquires fewer samples for more common sentences.\nTo get a more concrete view of the number of sam-\nples required to extract entailment judgments from\nan LM, we analyze Kp(z) for Gricean speakers.12\nRecall that we write c(z) for the cost that a\nGricean speaker assigns to producing a sentence z.\nFor Gricean speakers, Kp(z) is related to c(z) as\nwell as the probability z is true.\nTheorem 3 Assume that p(z | w) is a Gricean\nspeaker with respect to listener ℓ and JzK(w) =\n1 ⇐⇒ Iℓ(z; w) ≥ 0. Let gp(x, y) = log p(xy)\np(x$) −\nlog p(yy)\np(y$) . Let q = 1 − min{p(xy), p(yy)}. Then,\nfor all x, y∈ Xsuch that JxyK(p) > 0, for all\nδ >0, it holds with probability at least1−δ −4qn\nthat |gp(x, y) − gˆp(x, y)| is at most\n8\ns\nexp(max{c(xy), c(yy)})\np(JxyK) · 1\nδn .\n11Omitted proofs from §5 are in §D.\n12§D also analyzes uniformly truthful speakers.\n1 2 3 4 5 6 7 8 9 10\nSentence length\n107\n109\n1011\n1013\n1015\n1017\nTraining sentences required\nEntailment sample complexity (error 1.0, prob. 0.9)\nTheorem 3 bound\nGPT-3 training data\nFigure 2: Estimated number of training sentences for\nguaranteeing gˆp closely approximates gp, where ˆp is\nestimated using empirical text frequencies.\nTheorem 3 says we can use text frequencies to\ndecode entailment between sentences x, yfrom a\nGricean corpus, but the number of training sen-\ntences to guarantee this grows exponentially with\nthe cost of x and y. Thus, we probably cannot\nexpect to extract entailment judgments from text\nfrequencies except between very short sentences.\nWe make this more quantitative in Figure 2,\nwhere we estimate the number of training sentences\nneeded to ensuregp and gˆp are close on sentences of\nlength ≤ k as a function ofk. The main assumption\nbehind this calculation is that a sentence’s proba-\nbility vanishes exponentially in its length, where\nthe exponential base is the perplexity of the lan-\nguage. §E documents the underlying assumptions\nin more detail. Figure 2 predicts gp and gˆp can be\nmade close for length- 4 sentences using ∼ 1010\ntraining sentences: about as much data as GPT-3\nwas trained on. In contrast, handling (still short)\nsentences of length 10 can be done with ∼ 1017\ntraining sentences, or ∼ 107 GPT-3 corpora. Thus,\nrelying solely on corpus frequencies is likely not\na feasible way to extract entailment relations from\ntext generated by Gricean speakers.\n6 Decoding Entailment from LMs\nWe have just analyzed how many samples are nec-\nessary to decode entailment relations from the text\nfrequencies in a finite corpus. As shown by Theo-\nrem 3, this approach will require intractably many\nsamples for sentences of nontrivial length because\nlonger strings will appear infrequently (if at all)\nin the corpus. In order to estimate the probability\nof rare, longer, strings what if we use an LM to\nestimate ˆp(z) instead of text frequencies? Perhaps\na smoothed LM should allow us to extrapolate ˆp(z)\nwell enough for long sentences to extract entail-\nment judgments between them. In this section, we\nbriefly discuss some limitations of this approach.\nIt is tempting to take low LM perplexity as evi-\ndence that an LM estimates sentence probabilities\nwell enough to approximately satisfy the isomor-\nphism in Theorem 2. After all, low test perplexity\nimplies that ˆp(z) is, on average, a good approxima-\ntion of p(z): if the perplexity is bounded below ϵ,\nthen the KL divergenceKL(p, ˆp) is bounded below\nlog ϵ. ϵ decreases with the amount of training data\nn at a rate between Ω(1/√n) and Ω(1/n) (Wang\net al., 2013; Li and Liu, 2021). Thus, with enough\ndata, ˆp(z) will closely approximate p(z) for an\naverage sentence z in the training distribution.\nBut low error on an average z does not establish\nentailment can be decoded from ˆp because dˆp, as\nderived in Theorem 2, depends on the text z = yy,\nwhich is very unlikely in natural language.13 Poorly\nestimating p(yy) has little impact on KL(p, ˆp), so\nLMs trained to minimize KL(p, ˆp) have no reason\nto estimate p(yy) well unless they are imbued with\nstrong inductive biases. Thus, we expect that LMs\ntrained with a standard cross-entropy loss may not\nproduce reliable entailment judgments because they\npoorly estimate the probability of key valid (but un-\nlikely) texts.14 However, we find in the next section\nthat they do succeed in the easier setting of small\nartificial languages and fully Gricean speakers.\n7 Experiments: Extracting Semantics\nfrom Simulated Gricean Corpora\nWe test empirically whether we can extract entail-\nment judgments from LMs trained on unlabelled\ntext.15 Natural language corpora are unlikely to ad-\nhere exactly to our idealized assumptions about the\nspeakers generating texts, so we generate the train-\ning corpora from a simulated Gricean speaker (see\n§4). To make learning semantics more tractable\nwith limited computation, we set |W| = 3 and\nrestrict the vocabulary X to 7 utterances, each de-\nnoting one of the 7 non-empty subsets of W. Each\nsentence in the training corpus is generated by sam-\npling utterances from a Gricean speaker, condi-\ntioned on a uniformly sampled world state and the\n13yy is unlikely to be produced by a Gricean speaker be-\ncause the second y conveys no information.\n14Future work should more carefully analyze how much\ndata is required to extract complex entailment relations from\nLM predictions (rather than corpus frequencies). This is be-\nyond the scope of the current project.\n15https://github.com/viking-sudo-rm/\nformal-language-understanding\npreviously generated utterance, until the tautologi-\ncal utterance is generated. The semantic value of a\nsentence is taken to be the conjunction over all of\nits utterances. We set the rationality parameter α\nand the cost function heuristically (details in §G).\nWe generate training sets varying in size from\n2 texts to 10M texts, and train two types of mod-\nels on each: a simple empirical text frequency as\ndescribed in Section 5, and a trigram model im-\nplemented using NLTK (Bird, 2006). Then for\nall sentence pairs (x, y), where x and y have 6 ut-\nterances or fewer and each denotes a non-empty\nproposition, we compute gˆp(x, y) from §5. The-\norem 2 shows that, under the true distribution p,\ngˆp(x, y) = 0 if and only if x entails y.\nThe results are plotted in Figure 3. We arrive at\nthe following conclusions:\nEntailment relations can be extracted with\ngreater-than-chance performance from LM pre-\ndictions. The value of gˆp(x, y) is much closer to\n0 on average for entailed pairs than for non-entailed\npairs. This is predicted by Theorem 2.\nThe size of the corpus needed to extract entail-\nment grows predictably with sentence length.\nFor entailed pairs, the average value of gˆp(x, y) for\nshorter sentences approaches 0 more quickly with\na large training corpus. This is in line with the\npredictions of Theorem 4.\nModel inductive bias impacts the ease of extract-\ning entailment. Entailed and non-entailed pairs\nare better distinguished by the trigram model than\nthe text frequency model. Specifically, gˆp(x, y) is\ncloser to 0 for the trigram model for a given amount\nof data, and the trigram model’s predictions are less\nsensitive to sentence length.\n8 Generality of Extracting Semantics\nOur main result that entailment judgments can be\nextracted from an ideal LM assumes the corpus\nwas produced by Gricean speakers. While prag-\nmatic theory supports this assumption, real human\nspeakers are undoubtedly more complex. What if\nwe relax the assumption that speakers are Gricean?\nIn Theorem 6 in §F, we show that any semantic\nrelation is isomorphic to some distributional rela-\ntion as long as, for any pair of possible semantics,\nthere is some text whose probability distinguishes\nbetween the two candidate semantics.\nWe take it to be uncontroversial that semantics\ninfluences speech production, so we interpret Theo-\n103 105\nTraining sentences\n10 2\n10 1\n100\n101\n102\ngp(xy)\nx y\n103 105\nTraining sentences\nx y\nSentence\nlength\n4\n6\n7\n9\n10\n12\n(a) ˆp(x) given by text frequency model.\n103 105\nTraining sentences\n10 2\n10 1\n100\n101\n102\ngp(xy)\nx y\n103 105\nTraining sentences\nx y\nSentence\nlength\n4\n6\n7\n9\n10\n12\n(b) ˆp(x) given by trigram model.\nFigure 3: Plot of gˆp(x, y) = log ˆp(xy)\nˆp(x$) − log ˆp(yy)\nˆp(y$) as\na function of the number of sentences in the training\ncorpus and the length |xy|. Given the true distribution\np, gp(x, y) = 0 iff x entails y. We exclude pairs x, y\nwhere both xy and yy are absent from the training data.\nrem 6 to say all semantic relations are fully encoded\nin ideal LMs. In contrast to Theorem 2, however,\nthis result is nonconstructive, so we do not know\nwhich algorithm to use to decide entailment be-\ntween two sentences, even though one exists. Fur-\nther, without further assumptions about the speaker,\nwe cannot guarantee the extraction relation is effi-\nciently computable or even computable at all.\n9 Conclusion\nGiven a general, linguistically motivated model of\nhuman text production, we proved that entailment\njudgments can be decoded from the likelihood func-\ntion for texts because of semantic artifacts created\nby human authors. We also showed empirically that\nentailment could be extracted n-gram LMs trained\non simple formal languages. Thus, we have given\none explanation for why distributional information\nencodes semantic information (Firth, 1957) and\nhow semantic relations are, in principle, extractable\nfrom LMs. It is an open question whether entail-\nment judgments might be extractable from current\nlarge LMs, but we hypothesize that the complexity\nof natural language makes this substantially more\nchallenging than with our synthetic experiments,\nand that the loss function and inductive biases of\ncurrent neural LMs are not well suited for doing so\nwithout an infeasible amount of data.\nA natural next step for future work is to test\nthis hypothesis empirically by measuring whether\nentailment judgments can be extracted from large\nLMs using our theory. Similarly, it would be inter-\nesting to think about how LMs could be modified\nso that they can better pick up on the semantic\ninformation encoded in their training distribution.\n10 Acknowledgments\nThis project benefited from informal discussions\nwith Chris Barker, Sam Bowman, Lucius Bynum,\nKyunghun Cho, Tiwa Eisape, Yanai Elazar, Na-\njoung Kim, Alexander Koller, Vishakh Padmaku-\nmar, Chris Potts, Naomi Saphra, Sebastian Schus-\nter, and Noah A. Smith. We also thank the mem-\nbers of the CAP Lab and Semantics Group at NYU\nfor their feedback. This work was supported in\npart through the NYU IT High Performance Com-\nputing resources, services, and staff expertise. It\nwas funded by NSF award 1922658, and WM was\nsupported by an NSF graduate research fellowship.\nReferences\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nSteven Bird. 2006. NLTK: The Natural Language\nToolkit. In Proceedings of the COLING/ACL 2006\nInteractive Presentation Sessions, pages 69–72, Syd-\nney, Australia. Association for Computational Lin-\nguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie\nChen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Kr-\nishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-\nhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle\nLevent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,\nAli Malik, Christopher D. Manning, Suvir Mirchan-\ndani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,\nAvanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJulian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-\npadimitriou, Joon Sung Park, Chris Piech, Eva Porte-\nlance, Christopher Potts, Aditi Raghunathan, Rob\nReich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher Ré, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy\nShih, Krishnan Srinivasan, Alex Tamkin, Rohan\nTaori, Armin W. Thomas, Florian Tramèr, Rose E.\nWang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai\nWu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan\nYou, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn\nZhou, and Percy Liang. 2021. On the opportunities\nand risks of foundation models.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJudith Degen, Robert XD Hawkins, Caroline Graf, Elisa\nKreiss, and Noah D Goodman. 2019. When redun-\ndancy is rational: A Bayesian approach to “over-\ninformative” referring expressions. arXiv preprint\narXiv:1903.08237.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJohn R Firth. 1957. A synopsis of linguistic theory,\n1930-1955. Studies in linguistic analysis.\nNoah D Goodman and Michael C Frank. 2016. Prag-\nmatic language interpretation as probabilistic infer-\nence. Trends in cognitive sciences, 20(11):818–829.\nNoah D Goodman and Daniel Lassiter. 2015. Prob-\nabilistic semantics and pragmatics: Uncertainty in\nlanguage and thought. The handbook of contempo-\nrary semantic theory, 2nd edition. Wiley-Blackwell.\nHerbert P Grice. 1975. Logic and conversation. In\nSpeech acts, pages 41–58. Brill.\nIrene Heim and Angelika Kratzer. 1998. Semantics in\nGenerative Grammar. Blackwell.\nIrene Roswitha Heim. 1982. The semantics of defi-\nnite and indefinite noun phrases. University of Mas-\nsachusetts Amherst.\nHans A Kamp. 1981. A theory of truth and semantic\nrepresentation formal methods in the study of lan-\nguage, part 1, ed. by jeroen groenendijk, theo janssen\nand martin and stokhof. Amsterdam: Mathematisch\nCentrum.\nAngelika Kratzer. 2021. Situations in Natural Language\nSemantics. In Edward N. Zalta, editor, The Stanford\nEncyclopedia of Philosophy , Winter 2021 edition.\nMetaphysics Research Lab, Stanford University.\nStephen C Levinson, Stephen C Levinson, and S Levin-\nson. 1983. Pragmatics. Cambridge university press.\nDavid Lewis. 1979. Scorekeeping in a language game.\nIn Semantics from different points of view, pages 172–\n187. Springer.\nShaojie Li and Yong Liu. 2021. Towards sharper gener-\nalization bounds for structured prediction. Advances\nin Neural Information Processing Systems, 34.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A. Smith. 2021. Provable Limitations of Ac-\nquiring Meaning from Ungrounded Form: What Will\nFuture Language Models Understand? Transactions\nof the Association for Computational Linguistics ,\n9:1047–1060.\nJulian Michael. 2020. To dissect an octopus: Making\nsense of the form/meaning debate.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nChristopher Potts. 2020. Is it possible for language\nmodels to achieve understanding?\nShaojun Wang, Russell Greiner, and Shaomin Wang.\n2013. Consistency and generalization bounds for\nmaximum entropy density estimation. Entropy,\n15(12):5439–5463.\nA Limitations\nWe derived a recipe for computing entailment in terms of text probabilities, hinting that entailment\njudgments may be decodable from LM predictions. Yet two key concerns qualify this conclusion.\nLearnability We reduce entailment classification to computing probabilities in the target distribution of\nan LM, not probabilities predicted by an LM. In §6, we argue that the loss function of current LMs is not\nwell suited to producing models from which entailment can be extracted.\nSpeaker Assumptions Gricean speakers capture important factors influencing speech production in\npragmatic theory, but human speakers are undoubtedly more complex. Based on §8, we expect a similar\nisomorphism to hold under any reasonable speaker model, but the mathematical form may change and it\nmay become harder to compute.\nB Uncountable World Spaces\nIn this section, we assume W is an uncountably infinite set with a a probability density function p(w).\nWe then define “almost sure” entailment as follows:\nDefinition 7 For x, y∈ X, we say x almost surely entails y (i.e., JxK ⊑ JyK) if and only if\np(JxK \\ JyK) = 0.\nNote that if W is countable, then A ⊑ Breduces to A ⊆ B. We can generalize Lemma 1 as follows,\nwhich shows that all our results go through for almost sure entailment when W is uncountable.\nLemma 4 Let 1 S be the indicator function for set S. Let f : W →R be some function such that\ninfw∈W f(w) > 0. For any sets A, B such that A ⊆ B ⊆ W, then p(B \\ A) = 0 if and only if\nE\nw∼p(w)\n[1 A(w)f(w)] = E\nw∼p(w)\n[1 B(w)f(w)] .\nProof. If p(B \\ A) = 0, then the condition follows by construction. We thus only need to show that the\ncondition follows from p(B \\ A) = 0. Let q = p(B \\ A). By linearity of expectation, we rewrite the\npremise condition as\n0 = E\nw∼p(w)\n[(1 A(w) − 1 B(w)) f(w)]\n= E\nw∼p(w)\n[(1 A(w) − 1 B(w)) f(w) | w ∈ B \\ A] q\n+ E\nw∼p(w)\n[(1 A(w) − 1 B(w)) f(w) | w ̸∈ B \\ A] (1− q)\n≥ E\nw∼p(w)\n[f(w) | w ∈ B \\ A] q.\nLetting f∗ = infw∈W f(w) > 0, we get 0 ≥ f∗q. Since f∗ > 0 and q ≥ 0, q = p(B \\ A) = 0.\nC Gricean Speaker Proofs\nLemma 2 For any Gricean speaker p and x ∈ X,\np(x$)\np(xx) = exp(c(x))\nexp(c($)) .\nProof. Starting with the definition of an Gricean speaker, for any x ∈ X∗ and y ∈ X ∪ {$},\np(xy) = E\nw\n[p(x | w)p(y | x, w)] .\nNow, letting g(x, w) ≜ p(x | w)/\n\u0010P\ny′ exp (αIℓ(y′ | x; w) − c(y′))\n\u0011\n,\np(xy) = exp(−c(y)) E\nw\n[exp(αIℓ(y | x; w))g(x, w)] .\nWe apply this identity to both sides of the fraction in the lemma statement:\np(x$)\np(xx) = exp(−c($)) Ew [exp(αIℓ($ | x; w))g(x, w)]\nexp(−c(x)) Ew [exp(αIℓ(x | x; w))g(x, w)]\n= exp(c(x))\nexp(c($)) · Ew [exp(αIℓ($ | x; w))g(x, w)]\nEw [exp(αIℓ(x | x; w))g(x, w)].\nSince JxK ⊆ J$K and JxK ⊆ JxK, we know that the conditional information of both $ and x given x is 0,\nand, thus,\np(x$)\np(xx) = exp(c(x))\nexp(c($)) · Ew [exp(0)g(x, w)]\nEw [exp(0)g(x, w)] = exp(c(x))\nexp(c($)) .\nTheorem 2 Under any Gricean speaker p, entailment is isomorphic to a distributional relation. Specifi-\ncally, for all sentences x, y∈ X,\nJxK ⊆ JyK ⇐⇒ p(xy)\np(x$) = p(yy)\np(y$).\nProof. Recall from the proof of Lemma 2 that there exists a function g(x, w) such that, for all x ∈ X∗\nand y ∈ X ∪ {$},\np(xy) ∝ exp(−c(y)) E\nw\n[exp(αIℓ(y | x; w))g(x, w)] .\nThus, by Lemma 2, the proposed distributional relation can be expanded as\ndp(x, y) ⇐⇒ p(xy)\np(x$) = p(yy)\np(y$)\n⇐⇒ p(xy) · p(y$)\np(yy) = p(x$) · p(xx)\np(xx)\n⇐⇒ p(xy)exp(c(y))\nexp(c($)) = p(xx)exp(c(x))\nexp(c($))\n⇐⇒ p(xy) exp(c(y)) = p(xx) exp(c(x))\n⇐⇒ E\nw\n[exp(αIℓ(y | x; w))g(x, w)] = E\nw\n[exp(αIℓ(x | x; w))g(x, w)] .\nBy Lemma 1 (Here is the error: Lemma 1 does not apply! See §H), this holds if and only if, for all w,\nexp(αIℓ(y | x; w)) = exp(αIℓ(x | x; w))\nIℓ(y | x; w) = Iℓ(x | x; w)\nIℓ(y | x; w) = 0\nJxK(w) → JyK(w) = 1.\nWe conclude the distributional relation holds if and only if JxK ⊆ JyK.\nD Proofs for Learning Bounds\nLemma 3 For z ∈ X∗ ∪ X∗$ and δ >0, it holds with probability at least 1 − δ − (1 − p(z))n that\n|log p(z) − log ˆp(z)| ≤\nr\nKp(z)\nδn .\nProof. Without loss of generality, assume p(z) > 0. With probabiliy 1 − (1 − p(z))n over the draw of\nour sample, the random variable log ˆp(z) has finite variance defined by\nVar [log ˆp] = 1\nn · 1 − p(z)\np(z) ≤ Kp(z)\nn .\nWith finite variance, we can apply Chebyshev’s inequality to conclude that\nPr [|log p(z) − log ˆp(z)| ≥ϵ] ≤ Var [log ˆp]\nϵ2 ≤ Kp(z)\nnϵ2 .\nSolving for δ ≤ Pr [|log p(z) − log ˆp(z)|], we get\nδ ≤ Kp(z)\nnϵ2\n∴ ϵ ≤\nr\nKp(z)\nδn .\nWe conclude that that with probability 1 − δ − (1 − p(z))n,\n|log p(z) − log ˆp(z)| ≤\nr\nKp(z)\nδn .\nWe now characterize the complexity factorKp(z) for uniformly truthful speakers.\nLemma 5 For all z ∈ X∗ ∪ X∗$ such that JzK(p) > 0, it holds that\nKp(z) ≤ |X|\np(JzK).\nProof. We start by deriving a lower bound onp(z).\np(z) =\nX\nw\nJzK(w)P\nz′Jz′K(w)p(w)\n≥\nX\nw\nJzK(w)\n|X| p(w)\n= JzK(p)\n|X| .\nApplying this inequality to the definition of Kp(z), we conclude that\nKp(z) ≤ |X|\nJzK(p).\nLemma 5 lets us to derive the following guarantee for estimating entailment scores using a corpus\nproduced by uniformly truthful speakers:\nTheorem 4 For a uniformly truthful speaker p, let up(x, y) = log p(x$) − log p(xy). For x, y∈ Xsuch\nthat JxyK(p) > 0 and δ >0, it holds with probability at least 1 − δ − 2(1 − p(xy))n that\n|up(x, y) − uˆp(x, y)| ≤2\ns\n|X|\np(JxyK) · 2\nδn .\nProof. We expand the difference in scores as follows:\n|up(x, y) − uˆp(x, y)| ≤ |log p(x) − log ˆp(x$)| + |log p(xy) − log ˆp(xy)|.\nWe then apply Lemma 3 with δ\n2. Since p(x$) ≥ p(xy), this implies that with probability 1 − δ − 2(1 −\np(xy))n,\n|up(x, y) − uˆp(x, y)| ≤\nr\n2Kp(x$)\nδn +\nr\n2Kp(xy)\nδn\n≤ 2\nr\n2 max{Kp(x$), Kp(xy)}\nδn .\nFinally, we apply Lemma 5 to conclude that\n|up(x, y) − uˆp(x, y)| ≤2\ns\n|X|\nmin{Jx$K(p), JxyK(p)} · 2\nδn\n= 2\ns\n|X|\nJxyK(p) · 2\nδn .\nWe now characterize the complexity factor for Gricean speakers.\nLemma 6 Assume that p(z | w) is a Gricean speaker with respect to listener ℓ and JzK(w) = 1 ⇐⇒\nIℓ(z; w) ≥ 0. Then, for all z ∈ X∗ ∪ X∗$,\nKp(z) ≤ exp(c(z))\np(JzK) .\nProof. We start by writing out the form of p(z):\np(z) =\nP\nw exp(αIℓ(z; w))p(w)\nexp(c(z)) .\nBecause z ∈ X∗ ∪ X∗$, all terms where JzK(w) = 1 contribute at least 0 information; other terms\ncontribute negative information. Thus, we bound the information content of the “true” terms above 0, and\nignore the other terms to get the lower bound\np(z) ≥\nP\nwJzK(w) exp(0)p(w)\nexp(c(z))\n=\nP\nwJzK(w)p(w)\nexp(c(z))\n= JzK(p)\nexp(c(z)).\nPlugging this into Kp(z), we conclude that\nKp(z) ≤ exp(c(z))\nJzK(p) .\nTheorem 3 Assume that p(z | w) is a Gricean speaker with respect to listener ℓ and JzK(w) = 1 ⇐⇒\nIℓ(z; w) ≥ 0. Let gp(x, y) = log p(xy)\np(x$) − log p(yy)\np(y$) . Let q = 1 − min{p(xy), p(yy)}. Then, for all\nx, y∈ Xsuch that JxyK(p) > 0, for all δ > 0, it holds with probability at least 1 − δ − 4qn that\n|gp(x, y) − gˆp(x, y)| is at most\n8\ns\nexp(max{c(xy), c(yy)})\np(JxyK) · 1\nδn .\nProof. We start by expanding gp(x, y):\ngp(x, y) = log p(xy)\np(x$) − log p(yy)\np(y$)\n= log p(xy) − log p(x$) − log p(yy) + logp(y$).\nThus, following Theorem 4, we can bound\n|gp(x, y) − gˆp(x, y)| ≤ |log p(xy) − log ˆp(xy)| + |log p(x$) − log ˆp(x$)|\n+ |log p(yy) − log ˆp(yy)| + |log p(y$) − log ˆp(y$)|.\nWe apply Lemma 3 to each term with δ\n4. Since p(yy) ≤ p(y$) and p(xy) ≤ p(x$), we get that with\nprobability at least 1 − δ − 4qn,\n|gp(x, y) − gˆp(x, y)| ≤4\nr\n4 max{Kp(xy), Kp(x$), Kp(yy), Kp(y$)}\nδn\n= 8\nr\nmax{Kp(xy), Kp(x$), Kp(yy), Kp(y$)}\nδn .\nFinally, we apply Lemma 6 to conclude that, with probability at least 1 − δ − 4qn,\n|gp(x, y) − gˆp(x, y)| ≤8\ns\nmax\n\u001aexp(c(xy))\nJxyK(p) , exp(c(x$))\nJx$K(p) , exp(c(yy))\nJyyK(p) , exp(c(y$))\nJy$K(p)\n\u001b\n· 1\nδn\n≤ 8\ns\nexp(max{c(xy), c(yy)})\nJxyK(p) · 1\nδn .\nWe can use Corollary 2.1 to derive a tighter version of Theorem 3 by removing the dependence on the\nuncommon string yy:\nTheorem 5 Let sp(x, y) = log p(x$)\np(xy) − c(y) + c($) . Then, for all x, y∈ Xsuch that JxyK(p) > 0, for\nall δ >0, the following holds with probability 1 − δ − 2(1 − p(xy))n,\n|sp(x, y) − sˆp(x, y)| ≤2\ns\nexp(c(xy))\np(JxyK) · 2\nδn .\nThe proof follows analogously to Theorem 3. The main improvement of Theorem 5 compared to\nTheorem 3 is that the probability the bound holds no longer depends on the unlikely probability p(yy).\nWe also get the benefit that the cost complexity factor has been reduced to only depend on c(xy) and\nobtain better constants (2\n√\n2 instead of 8), although these changes are likely less important than removing\nthe dependence on p(yy). Of course, the drawback is that we are assuming access to the cost function\nc(y). If we have such access, though, the improvements in the bound suggest we may be able to extract\nentailment from a finite corpus of Gricean text with better sample complexity than if we did not.\nE Sample Complexity Estimation Details\nAssuming the approximation error in Theorem 3 is ≤ ϵ, we aim to solve the following inequality for n:\nϵ ≤ 8\ns\nexp(max{c(xy), c(yy)})\np(JxyK) · 1\nδn .\nSentence Length We make the simplifying assumption that max{c(xy), c(yy)} = 2w(ℓ + 1), where ℓ\nis a variable representing sentence length.16 Let Σ be the word-level vocabulary of English. We estimate\nthe value w by assuming q(z) = exp(−w(|z| + 1)) is a valid prior over Σ∗ and solving for the unique\nvalue of w to satisfy this condition:\nX\nz∈Σ∗\nexp(−w(|z| + 1)) = 1\n∞X\nℓ=0\n|Σ|ℓ\nexp(w(ℓ + 1)) = 1\nexp(−w)\n∞X\nℓ=0\n\u0012 |Σ|\nexp(w)\n\u0013ℓ\n= 1\n∴ w = log(|Σ| + 1).\nThis reveals that w should be set ≥ 1, but the question remains how to set |Σ|. In practice, we assume the\nspeaker prior is defined over the support of all syntactically valid or likely strings in English, not over all\npossible strings as derived above. Letting S be the word-level perplexity of English, we setw according to\nw ≈ log(S + 1).\nWe setS to the value estimated by GPT-3:∼ 20 nats/word (Brown et al., 2020). Simplifying the numerator\nin the bound yields\nexp(log(21)(ℓ + 1)) = 21ℓ+1.\nMaking the prior less strong, i.e., increasing |Σ| to be greater than this perplexity estimate, would only\nincrease the number of samples needed to extract entailment judgments.\nTruth Probability We conservatively assumep(JxyK) = 1\n2, although in practice it may be smaller for\nmore informative sentences. Reducing it would lead to higher sample complexity estimates.\nFinal Form Putting together our estimates for sentence length and truth probability yields\nϵ ≤ 8\nr\n2 · 21ℓ+1\nδn\n∴ n ≤ 128 · 21ℓ+1\nδϵ2 .\nThe final form captures the intuition that the likelihood of a string vanishes exponentially with its length,\nand that the base of this decay is roughly inversely proportional to the perplexity of the language. In\npractice, we set δ = 0.1 and ϵ = 1.0. Changing the value of ϵ (the desired approximation accuracy) would\nshift the curve.\nF General Relations and Speakers\nSo far, we have characterized concrete distributional relations that are isomorphic to entailment for\ndifferent classes of speaker models. In this section, we analyze the conditions under which a distribution\nrelation isomorphic to a semantic relation exists, given no assumptions about the speaker. Informally, we\nprove in Theorem 6 that a distributional isomorphism exists if and only if the speaker model depends\non semantics “at all”. This is a very weak condition, and should be satisfied by any reasonable model\nof natural speakers. Thus, we take this as evidence that any speaker model—not just the ones we have\nconsidered, admits a distributional relation isomorphic to entailment.\n16We write ℓ + 1instead of ℓ here for technical reasons: we want to guarantee that q(z) can be a valid probability distribution.\nWe now turn to the formal presentation of this result. Let M be the function that takes a set of worlds\nW and returns all semantic evaluation functions µ : X 7→2W over W. For a semantic evaluation function\nµ = λx.JxK, let pµ be a speaker model parameterized by semantics µ.\nSay two semantic evaluation functions µ, µ′ are isomorphic with respect to s if and only if, for all x, y,\nS(µ(x), µ(y)) ⇐⇒ S(µ′(x), µ′(y)).\nTheorem 6 The following are equivalent for any speaker p and semantic relation s:\n1. There exists a distribution relation d such that, for all W, for all µ ∈ M(W), s is isomorphic to dpµ.\n2. For all W, W′, for all µ ∈ M(W) and µ′ ∈ M(W′) such that µ, µ′ are not isomorphic w.r.t. s,\nthere exists z ∈ X∗ such that pµ(z) ̸= pµ′(z).\nProof. We will show that equivalence holds in both directions.\nForward Direction: We assume the second statement does not hold by way of modus tollens. Thus,\nthere exists W, W′ with µ ∈ M(W) and µ′ ∈ M(W′) with µ, µ′ not isomorphic such that, for allz ∈ X∗,\npµ(z) = pµ′(z). Thus, for all d and sentences x, y,\ndpµ(x, y) ⇐⇒ dpµ′ (x, y).\nBut µ and µ′ are not isomorphic, so there exist x, ysuch that S(µ(x), µ(y)) ̸ ⇐⇒S(µ′(x), µ′(y)). Thus,\nwe can conclude that one of the following must hold:\ndpµ(x, y) ̸ ⇐⇒S(µ(x), µ(y))\ndpµ′ (x, y) ̸ ⇐⇒S(µ′(x), µ′(y)).\nWe conclude by modus tollens that the first statement implies the second.\nBackward Direction: Assume the second statement holds. The function f(µ) = pµ is invertible up to\nisomorphism to s. In other words, there exists g(pµ) = µ∗ such that, for all x, y,\nS(µ∗(x), µ∗(y)) ⇐⇒ S(µ(x), µ(y)).\nThen we define d according to\ndpµ(x, y) ⇐⇒ S(g(pµ)(x), g(pµ)(y))\n⇐⇒ S(µ∗(x), µ∗(y))\n⇐⇒ S(µ(x), µ(y)).\nThus, the second statement implies the first.\nG Experimental Details\nG.1 Language Description\nWe set the vocabulary X = {100, 010, 001, 110, 011, 111} and define W = {1, 2, 3}. We refer to\neach three-digit binary string as an utterance, and define the evaluation function for an utterance x as\nJxK(w) = 1 ⇐⇒ xw = 1. Thus, 100 is true only in world 1, while 111 is true in all worlds (i.e., is\ntautological). We identify 111 with the end of sequence $.\nIn line with our formal definitions, we define a text z as a concatenation of utterances z1 ··· zn ending\nwith $. Recall that we define the evaluation function over a text as the intersection of the evaluation\nfunctions of the utterances it contains. For our language, this reduces to JzK(w) = 1 ⇐⇒ ∀i (zi = 1).\nThus, 011 101 111 is true only in w3, and 011 101 110 111 is true in no worlds (i.e., contradictory).\n111 110 101 011 100 010 001\nUtterance\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Relative frequency\n2 4 6 8 10\nT ext length\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Relative frequency\nFigure 4: Properties of the data generated by the speaker in our experiments, with α = 5 and c(x) = 0.1 · |x|.\nG.2 Speaker Model Parameters\nWe model the listener of the informative speaker as a literal listener (Goodman and Frank, 2016), which\nmeans our informative speaker is a rational speaker of depth 1 in the language of rational speech acts.\nWe set c(x) = 0 .1 · |x|, where |x| is the length of the string x. We set the rationality parameter\nα = 5. These choices were made heuristically, by inspecting the the properties of the speaker’s output,\nas summarized in Figure 4. These parameters led to a relatively uniform distribution over utterances\n(except for the stop token 111 which is present in all texts), and a variety of text lengths without excessive\nredundancy. We found that larger values of α or of the coefficient for the cost function produced short\ntexts, biasing maximally informative utterances (i.e., 100, 010, or 001); while smaller values produced\nlong, repetitive utterances or sometimes empty utterances.\nG.3 Training and Evaluation\nWe sample a dataset from a speaker by independently sampling n texts from the speaker model. We\ngenerate datasets of varying size from each speaker, with the number of texts n decreasing by factors of 2\nfrom 107 texts down to just 2 texts.\nWe train models of two kinds: a text frequency model, and a trigram model. The text frequency model\nsimply assigns a probability to a text proportional to its frequency in the training data, assigning a small\nϵ = 10−20 probability to an unknown sequence. The trigram model is trained using NLTK’s (Bird, 2006)\nMLE implementation, i.e., the probabilities are unsmoothed. We do not need to use smoothing due to the\nsmall number of possible trigrams in the language.\nFor evaluation data, we generate pairs of texts labeled for entailments. We include all pairs where each\ntext is 6 utterances or shorter, except for utterances that are contradictory or consist only of the end of\nsequence token. The total number of test pairs is about 1.1M.\nH Erratum Derivation\nFormally, JxK can be partitioned into two sets of worlds:\n1. Y = JxK ∩ JyK where x, yare both true\n2. ˜Y = JxK \\ JyK where x is true but y is false\nFollowing the initial reasoning in the original proof of Theorem 2, the entailment test is 0 if and only if\nE\nw\n[exp(Iℓ(y | x; w))] = E\nw\n[exp(Iℓ(x | x; w)| {z }\n=0\n)]\nE\nw\n[exp(Iℓ(y | x; w))] = 1\nE\nw\n[exp(Iℓ(y | x; w)) | w ∈ JxK] = 1\np(Y ) E\nw\n[exp(Iℓ(y | x; w)) | w ∈ Y ] +\nhhhhhhhhhhhhhhhhh\np( ˜Y ) E\nw∈˜Y\n[exp(Iℓ(y | x; w)| {z }\n=−∞\n) | w ∈ ˜Y ] = 1\n∴ p(Y )IY = 1,\nwhere IY ≜ Ew[exp(Iℓ(y | x; w)) | w ∈ Y ].",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8041499853134155
    },
    {
      "name": "Natural language processing",
      "score": 0.7294977903366089
    },
    {
      "name": "Logical consequence",
      "score": 0.7236946821212769
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.67551589012146
    },
    {
      "name": "Pragmatics",
      "score": 0.6215945482254028
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6157135367393494
    },
    {
      "name": "Textual entailment",
      "score": 0.6113072037696838
    },
    {
      "name": "Ideal (ethics)",
      "score": 0.5963284373283386
    },
    {
      "name": "Natural language",
      "score": 0.49531546235084534
    },
    {
      "name": "Language model",
      "score": 0.4164074659347534
    },
    {
      "name": "Linguistics",
      "score": 0.3696701228618622
    },
    {
      "name": "Programming language",
      "score": 0.2572888135910034
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}