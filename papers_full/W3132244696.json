{
    "title": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer",
    "url": "https://openalex.org/W3132244696",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5046898079",
            "name": "Rafał Powalski",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2551315506",
            "name": "Łukasz Borchmann",
            "affiliations": [
                "Poznań University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2983142581",
            "name": "Dawid Jurkiewicz",
            "affiliations": [
                "Adam Mickiewicz University in Poznań"
            ]
        },
        {
            "id": "https://openalex.org/A2395554080",
            "name": "Tomasz Dwojak",
            "affiliations": [
                "Adam Mickiewicz University in Poznań"
            ]
        },
        {
            "id": "https://openalex.org/A4297363604",
            "name": "Michał Pietruszka",
            "affiliations": [
                "Jagiellonian University"
            ]
        },
        {
            "id": "https://openalex.org/A5050602934",
            "name": "Gabriela Pałka",
            "affiliations": [
                "Adam Mickiewicz University in Poznań"
            ]
        },
        {
            "id": "https://openalex.org/A5046898079",
            "name": "Rafał Powalski",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2551315506",
            "name": "Łukasz Borchmann",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2983142581",
            "name": "Dawid Jurkiewicz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2395554080",
            "name": "Tomasz Dwojak",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4297363604",
            "name": "Michał Pietruszka",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5050602934",
            "name": "Gabriela Pałka",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2888302696",
        "https://openalex.org/W3096109555",
        "https://openalex.org/W3045462440",
        "https://openalex.org/W3105607850",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W3204562006",
        "https://openalex.org/W2962772269",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W2510759893",
        "https://openalex.org/W3003484198",
        "https://openalex.org/W3173306993",
        "https://openalex.org/W2986619406",
        "https://openalex.org/W2963420691",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W2996848635",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2131494463",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W2952728748",
        "https://openalex.org/W2962964995",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2922714365",
        "https://openalex.org/W3011221694",
        "https://openalex.org/W3120043490",
        "https://openalex.org/W2963687456",
        "https://openalex.org/W3000758063",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2964223283",
        "https://openalex.org/W3034363136",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3106859150",
        "https://openalex.org/W2979382951",
        "https://openalex.org/W3201693581",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3176851559",
        "https://openalex.org/W2997154779",
        "https://openalex.org/W3035231859",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W3094522776",
        "https://openalex.org/W2940024477",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W2809324505",
        "https://openalex.org/W2919420119",
        "https://openalex.org/W3010341014",
        "https://openalex.org/W2964091467",
        "https://openalex.org/W2972985407",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3104953317",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W2407521645",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W2963202404",
        "https://openalex.org/W3091226465",
        "https://openalex.org/W3099388488",
        "https://openalex.org/W3026408381",
        "https://openalex.org/W2896585994",
        "https://openalex.org/W3006883036",
        "https://openalex.org/W3035812575"
    ],
    "abstract": null,
    "full_text": "Going Full-TILT Boogie on Document\nUnderstanding with Text-Image-Layout\nTransformer\nRafa l Powalski1,∗\n,  Lukasz Borchmann1,2,∗\n(\u0000 ), Dawid Jurkiewicz1,3,†\n,\nTomasz Dwojak1,3,†\n, Micha l Pietruszka1,4,†\n, and Gabriela Pa lka1,3\n1 Applica.ai, Warsaw, Poland\nname.surname@applica.ai\n2 Poznan University of Technology, Pozna´ n, Poland\n3 Adam Mickiewicz University in Pozna´ n, Poland\n4 Jagiellonian University, Cracow, Poland\nAbstract. We address the challenging problem of Natural Language\nComprehension beyond plain-text documents by introducing the TILT\nneural network architecture which simultaneously learns layout informa-\ntion, visual features, and textual semantics. Contrary to previous ap-\nproaches, we rely on a decoder capable of unifying a variety of problems\ninvolving natural language. The layout is represented as an attention\nbias and complemented with contextualized visual information, while\nthe core of our model is a pretrained encoder-decoder Transformer. Our\nnovel approach achieves state-of-the-art results in extracting information\nfrom documents and answering questions which demand layout under-\nstanding (DocVQA, CORD, SROIE). At the same time, we simplify the\nprocess by employing an end-to-end model.\nKeywords: Natural Language Processing · Transfer learning · Docu-\nment understanding · Layout analysis · Deep learning · Transformer.\n1 Introduction\nMost tasks in Natural Language Processing (NLP) can be uniﬁed under one\nframework by casting them as triplets of the question, context, and answer [29,\n39, 26]. We consider such uniﬁcation of Document Classiﬁcation, Key Informa-\ntion Extraction, and Question Answering in a demanding scenario where context\nextends beyond the text layer. This challenge is prevalent in business cases since\ncontracts, forms, applications, and invoices cover a wide selection of document\ntypes and complex spatial layouts.\n∗RP,  LB contributed equally.\n†DJ, TD, MP contributed equally.\narXiv:2102.09550v3  [cs.CL]  12 Jul 2021\n2 Powalski et al.\nImportance of Spatio-Visual Relations. The most remarkable successes\nachieved in NLP involved models that map raw textual input into raw textual\noutput, which usually were provided in a digital form. An important aspect of\nreal-world oriented problems is the presence of scanned paper records and other\nanalog materials that became digital.\nConsequently, there is no easily accessible information regarding the docu-\nment layout or reading order, and these are to be determined as part of the\nprocess. Furthermore, interpretation of shapes and charts beyond the layout\nmay help answer the stated questions. A system cannot rely solely on text but\nrequires incorporating information from the structure and image.\n95 90 PERCENT\nMORTALITY70\n90-DOSE TEST\n70 50 30 10 5\nFig. 1.The same document perceived diﬀerently depending on modalities. Respec-\ntively: its visual aspect, spatial relationships between the bounding boxes of detected\nwords, and unstructured text returned by OCR under the detected reading order.\nThus, it takes three to solve this fundamental challenge — the extraction of\nkey information from richly formatted documents lies precisely at the intersection\nof NLP, Computer Vision, and Layout Analysis (Figure 1). These challenges\nimpose extra conditions beyond NLP that we sidestep by formulating layout-\naware models within an encoder-decoder framework.\nLimitations of Sequence Labeling. Sequence labeling models can be trained\nin all cases where the token-level annotation is available or can be easily obtained.\nLimitations of this approach are strikingly visible on tasks framed in either key\ninformation extraction or property extraction paradigms [19, 9]. Here, no an-\nnotated spans are available, and only property-value pairs are assigned to the\ndocument. Occasionally, it is expected from the model to mark some particular\nsubsequence of the document. However, problems where the expected value is\nnot a substring of the considered text are unsolvable assuming sequence labeling\nmethods (Table 1). As a result, authors applying state-of-the-art entity recogni-\ntion models were forced to rely on human-made heuristics and time-consuming\nrule engineering.\nParticular problems one has to solve when employing a sequence-labeling\nmethod can be divided into three groups. We investigate them below to precisely\npoint out the limitations of this approach.\nText-Image-Layout Transformer 3\nTask Annotation Exact match Layout\nCoNLL 2003 word-level 100% −\nSROIE\n\n\n document-level\n93% +\nWikiReading 20% −\nKleister 27% +\nTable 1. Comparison of extraction tasks. Expected values are always present in a\nsubstring of a document in NER, but not elsewhere. Our estimation.\nTake, for example, the total amount assigned to a receipt in the SROIE\ndataset [19]. Suppose there is no exact match for the expected value in the\ndocument, e.g., due to an OCR error, incorrect reading order or the use of a\ndiﬀerent decimal separator. Unfortunately, a sequence labeling model cannot be\napplied oﬀ-the-shelf. Authors dealing with property extraction rely on either\nmanual annotation or the heuristic-based tagging procedure that impacts the\noverall end-to-end results [56, 52, 11, 18, 55, 36]. Moreover, when receipts with one\nitem listed are considered, the total amount is equal to a single item price, which\nis the source of yet another problem. Precisely, if there are multiple matches for\nthe value in the document, it is ambiguous whether to tag all of them, part or\nnone.\nAnother problem one has to solve is which and how many of the detected\nentities to return, and whether to normalize the output somehow. Consequently,\nthe authors of Kleister proposed a set of handcrafted rules for the ﬁnal selection\nof the entity values [52]. These and similar rules are either labor-intensive or\nprone to errors [40].\nFinally, the property extraction paradigm does not assume the requested\nvalue appeared in the article in any form since it is suﬃcient for it to be infer-\nable from the content, as in document classiﬁcation or non-extractive question\nanswering [9].\nResorting to Encoder-Decoder Models. Since sequence labeling-based ex-\ntraction is disconnected from the ﬁnal purpose the detected information is used\nfor, a typical real-world scenario demands the setting of Key Information Ex-\ntraction.\nTo address this issue, we focus on the applicability of the encoder-decoder\narchitecture since it can generate values not included in the input text explic-\nitly [16] and performs reasonably well on all text-based problems involving natu-\nral language [44]. Additionally, it eliminates the limitation prevalent in sequence\nlabeling, where the model output is restricted by the detected word order, pre-\nviously addressed by complex architectural changes (Section 2).\nFurthermore, this approach potentially solves all identiﬁed problems of se-\nquence labeling architectures and ties various tasks, such as Question Answering\nor Text Classiﬁcation, into the same framework. For example, the model may\n4 Powalski et al.\nEncoder \n-decoder Spatial \nMulti-modal \nLayoutLM \nOur work\nLAMBER T \nBER Tgrid \nT5 \nBAR T \nV isualBER T VL-BER T \nFig. 2.Our work in relation to encoder-decoder models, multi-modal transformers, and\nmodels for text that are able to comprehend spatial relationships between words.\ndeduce to answer yes or no depending on the question form only. Its end-to-end\nelegance and ease of use allows one to not rely on human-made heuristics and\nto get rid of time-consuming rule engineering required in the sequence labeling\nparadigm.\nObviously, employing a decoder instead of a classiﬁcation head comes with\nsome known drawbacks related to the autoregressive nature of answer generation.\nThis is currently investigated, e.g., in the Neural Machine Translation context,\nand can be alleviated by methods such as lowering the depth of the decoder [47,\n24]. However, the datasets we consider have target sequences of low length; thus,\nthe mentioned decoding overhead is mitigated.\nThe speciﬁc contribution of this work can be better understood in the context\nof related works.\n2 Related Works\nWe aim to bridge several ﬁelds, with each of them having long-lasting research\nprograms; thus, there is a large and varied body of related works. We restrict\nourselves to approaches rooted in the architecture of Transformer [54] and focus\non the inclusion of spatial information or diﬀerent modalities in text-processing\nsystems, as well as on the applicability of encoder-decoder models to Information\nExtraction and Question Answering.\nSpatial-aware Transformers. Several authors have shown that, when tasks\ninvolving 2D documents are considered, sequential models can be outperformed\nby considering layout information either directly as positional embeddings [17,\n11, 56] or indirectly by allowing them to be contextualized on their spatial neigh-\nborhood [6, 57, 15]. Further improvements focused on the training and inference\nText-Image-Layout Transformer 5\naspects by the inclusion of the area masking loss function or achieving inde-\npendence from sequential order in decoding respectively [18, 20]. In contrast to\nthe mentioned methods, we rely on a bias added to self-attention instead of\npositional embeddings and propose its generalization to distances on the 2D\nplane. Additionally, we introduce a novel word-centric masking method concern-\ning both images and text. Moreover, by resorting to an encoder-decoder, the\nindependence from sequential order in decoding is granted without dedicated\narchitectural changes.\nEncoder-decoder for IE and QA. Most NLP tasks can be uniﬁed under\none framework by casting them as Language Modeling, Sequence Labeling or\nQuestion Answering [43, 25]. The QA program of unifying NLP frames all the\nproblems as triplets of question, context and answer [29, 39, 26] or item, prop-\nerty name and answer [16]. Although this does not necessarily lead to the use of\nencoder-decoder models, several successful solutions relied on variants of Trans-\nformer architecture [54, 34, 9, 44]. The T5 is a prominent example of large-scale\nTransformers achieving state-of-the-art results on varied NLP benchmarks [44].\nWe extend this approach beyond the text-to-text scenario by making it possible\nto consume a multimodal input.\nMultimodal Transformers. The relationships between text and other me-\ndia have been previously studied in Visual Commonsense Reasoning, Video-\nGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]. In the\ncontext of images, this niche was previously approached with an image-to-text\ncross-attention mechanism, alternatively, by adding visual features to word em-\nbeddings or concatenating them [37, 33, 35, 53, 56]. We diﬀer from the mentioned\napproaches, as in our model, visual features added to word embeddings are al-\nready contextualized on an image’s multiple resolution levels (see Section 3).\n3 Model Architecture\nOur starting point is the architecture of the Transformer, initially proposed for\nNeural Machine Translation, which has proven to be a solid baseline for all\ngenerative tasks involving natural language [54].\nLet us begin from the general view on attention in the ﬁrst layer of the\nTransformer. If n denotes the number of input tokens, resulting in a matrix of\nembeddings X, then self-attention can be seen as:\nsoftmax\n(QXK⊤\nX√n + B\n)\nVX (1)\nwhere QX, KX and VX are projections of X onto query, keys, and value spaces,\nwhereas B stands for an optional attention bias. There is no B term in the\n6 Powalski et al.\n(A) Vanilla Transformer (B) T5 Architecture\nKQ V\n(C) Our model\nPairwise\n1+2D\ndistances\nSemantics Contextualized\nVision\n×\n×+\n+\nSequential\nword index\nKQ V\nSemantics\n×\n×\n+\nKQ V\nPairwise\nsequential\ndistances\nSemantics\n×\n×+\nFig. 3.(A) In the original Transformer, information about the order of tokens is pro-\nvided explicitly to the model by positional embeddings added to semantic embeddings.\n(B) T5 introduces sequential bias, thus separating semantics from sequential distances.\n(C) We maintain this clear distinction, extending biases with spatial relationships and\nproviding additional image semantics at the input.\nText-Image-Layout Transformer 7\noriginal Transformer, and information about the order of tokens is provided\nexplicitly to the model, that is:\nX = S + P B = 0n×d\nwhere S and P are respectively the semantic embeddings of tokens and positional\nembedding resulting from their positions [54]. 0 n×d denote a zero matrix.\nIn contrast to the original formulation, we rely on relative attention biases\ninstead of positional embeddings. These are further extended to take into account\nspatial relationships between tokens (Figure 3).\nSpatial Bias. Authors of the T5 architecture disregarded positional embed-\ndings [44], by settingX = S. They used relative bias by extending self-attention’s\nequation with the sequential bias term B = B1D, a simpliﬁed form of positional\nsignal inclusion. Here, each logit used for computing the attention head weights\nhas some learned scalar added, resulting from corresponding token-to-token oﬀ-\nsets.\nWe extended this approach to spatial dimensions. In our approach, biases\nfor relative horizontal and vertical distances between each pair of tokens are\ncalculated and added to the original sequential bias, i.e.:\nB = B1D + BH + BV\nSuch bias falls into one of 32 buckets, which group similarly-distanced token-\npairs. The size of the buckets grows logarithmically so that greater token pair\ndistances are grouped into larger buckets.\nAmount: 100.00\n2020 \n \nrelative \ndistance \nFig. 4.Document excerpt with distinguished vertical buckets for the Amount token.\n8 Powalski et al.\nContextualized Image Embeddings. Contextualized Word Embeddings are\nexpected to capture context-dependent semantics and return a sequence of vec-\ntors associated with an entire input sequence [10]. We designed Contextualized\nImage Embeddings with the same objective, i.e., they cover the image region\nsemantics in the context of its entire visual neighborhood.\nTo produce image embeddings, we use a convolutional network that con-\nsumes the whole page image of size 512 ×384 and produces a feature map of\n64×48×128. We rely on U-Net as a backbone visual encoder network [48] since\nthis architecture provides access to not only the information in the near neigh-\nborhood of the token, such as font and style but also to more distant regions of\nthe page, which is useful in cases where the text is related to other structures, i.e.,\nis the description of a picture. This multi-scale property emerges from the skip\nconnections within chosen architecture (Figure 5). Then, each token’s bounding\nbox is used to extract features from U-Net’s feature map with ROI pooling [5].\nThe obtained vector is then fed into a linear layer which projects it to the model\nembedding dimension.\nFig. 5.Truncated U-Net network. ■ conv ■ max-pool ■ up-conv ■ residual\nIn order to inject visual information to the Transformer, a matrix of contex-\ntualized image-region embeddings U is added to semantic embeddings, i.e. we\ndeﬁne\nX = S + U\nin line with the convention from Section 3 (see Figure 3).\n4 Regularization Techniques\nIn the sequence labeling scenario, each document leads to multiple training in-\nstances (token classiﬁcation), whereas in Transformer sequence-to-sequence mod-\nels, the same document results in one training instance with feature space of\nhigher dimension (decoding from multiple tokens).\nText-Image-Layout Transformer 9\nSince most of the tokens are irrelevant in the case of Key Information Ex-\ntraction and contextualized word embeddings are correlated by design, one can\nsuspect our approach to overﬁt easier than its sequence labeling counterparts.\nTo improve the model’s robustness, we introduced a regularization technique for\neach modality.\nCase Augmentation. Subword tokenization [49, 28] was proposed to solve the\nword sparsity problem and keep the vocabulary at a reasonable size. Although\nthe algorithm proved its eﬃciency in many NLP ﬁelds, the recent work showed\nthat it performs poorly in the case of an unusual casing of text [42], for instance,\nwhen all words are uppercased. The problem occurs more frequently in formated\ndocuments (FUNSD, CORD, DocVQA), where the casing is an important vi-\nsual aspect. We overcome both problems with a straightforward regularization\nstrategy, i.e., produce augmented copies of data instances by lower-casing or\nupper-casing both the document and target text simultaneously.\nSpatial Bias Augmentation. Analogously to Computer Vision practices of\nrandomly transforming training images, we augment spatial biases by multiply-\ning the horizontal and vertical distances between tokens by a random factor.\nSuch transformation resembles stretching or squeezing document pages in hor-\nizontal and vertical dimensions. Factors used for scaling each dimension were\nsampled uniformly from range [0 .8, 1.25].\nAﬃne Vision Augmentation. To account for visual deformations of real-\nworld documents, we augment images with aﬃne transformation, preserving\nparallel lines within an image but modifying its position, angle, size, and shear.\nWhen we perform such modiﬁcation to the image, the bounding box of every\ntoken is updated accordingly. The exact hyperparameters were subject to an\noptimization. We use 0.9 probability of augmenting and report the following\nboundaries for uniform sampling work best: [ −5, 5] degrees for rotation angle,\n[−5%, 5%] for translation amplitude, [0 .9, 1.1] for scaling multiplier, [ −5, 5] de-\ngrees for the shearing angle.\n5 Experiments\nOur model was validated on series of experiments involving Key Information\nExtraction, Visual Question Answering, classiﬁcation of rich documents, and\nQuestion Answering from layout-rich texts. The following datasets represented\nthe broad spectrum of tasks and were selected for the evaluation process (see\nTable 2 for additional statistics).\nThe CORD dataset [41] includes images of Indonesian receipts collected from\nshops and restaurants. The dataset is prepared for the information extraction\ntask and consists of four categories, which fall into thirty subclasses. The main\ngoal of the SROIE dataset [19] is to extract values for four categories (company,\n10 Powalski et al.\ndate, address, total) from scanned receipts. The DocVQA dataset [38] is focused\non the visual question answering task. The RVL-CDIP dataset [14] contains\ngray-scale images and assumes classiﬁcation into 16 categories such as letter,\nform, invoice, news article, and scientiﬁc publication. For DocVQA, we relied\non Amazon Textract OCR; for RVL-CDIP, we used Microsoft Azure OCR, for\nSROIE and CORD, we depended on the original OCR.\nTable 2.Comparison of datasets considered for supervised pretraining and evaluation\nprocess. Statistics given in thousands of documents or questions.\nDataset Data type Image Docs (k) Questions (k)\nCORD [41] receipts + 1.0 —\nSROIE [19] receipts + 0.9 —\nDocVQA [38] industry documents + 12.7 50.0\nRVL-CDIP [14] industry documents + 400.0 —\nDROP [8] \n\n\nWikipedia pages\n− 6.7 96.5\nQuAC [2] − 13.6 98.4\nSQuAD 1.1 [45] − 23.2 107.8\nTyDi QA [4] − 204.3 204.3\nNatural Questions [30] − 91.2 111.2\nWikiOps [1] Wikipedia tables − 24.2 80.7\nCoQA [46] various sources − 8.4 127.0\nRACE [31] English exams − 27.9 97.7\nQASC [27] school-level science − — 10.0\nFUNSD [21] RVL-CDIP forms + 0.1 —\nInfographics VQA infographics + 4.4 23.9\nTextCaps [50] Open Images + 28.4 —\nDVQA [22] synthetic bar charts + 300.0 3487.2\nFigureQA [23] synthetic, scientiﬁc + 140.0 1800.0\nTextVQA [51] Open Images + 28.4 45.3\n5.1 Training Procedure\nThe training procedure consists of three steps. First, the model is initialized\nwith vanilla T5 model weights and is pretrained on numerous documents in an\nunsupervised manner. It is followed by training on a set of selected supervised\ntasks. Finally, the model is ﬁnetuned solely on the dataset of interest. We trained\ntwo size variants of TILT models, starting from T5-Base and T5-Large models.\nOur models grew to 230M and 780M parameters due to the addition of Visual\nEncoder weights.\nUnsupervised Pretraining. We constructed a corpus of documents with rich\nstructure, based on RVL-CDIP (275k docs), UCSF Industry Documents Library\nText-Image-Layout Transformer 11\n(480k),∗ and PDF ﬁles from Common Crawl (350k). The latter were ﬁltered\naccording to the score obtained from a simple SVM business document classiﬁer.\nThen, a T5-like masked language model pretraining objective is used, but\nin a salient span masking scheme, i.e., named entities are preferred rather than\nrandom tokens [44, 12]. Additionally, regions in the image corresponding to the\nrandomly selected text tokens are masked with the probability of 80%. Models\nare trained for 100, 000 steps with batch size of 64, AdamW optimizer and linear\nscheduler with an initial learning rate of 2 e −4.\ndocVQA\nWikiOps \nSROIE \nRVL CDIP \nCORD \nWikiTable Questions \nWikiReading \nTyDi QA \nTextVQA \nTextCaps \nSQuAD 1.1 \nRACE Middle \nRACE High \nQuAC \nQASC \nNatural Questions \ninfographics VQA \nFUNSD \nDVQA \nDROP \nCoQA \n−4\n−2\n0\n2\n4        \n!\"#$\n !$\n #\"\n# &'\n( &)\n*+,-.\n!%\nd\nFig. 6.Scores on CORD, DocVQA, SROIE and RVL-CDIP compared to the baseline\nwithout supervised pretraining. The numbers represent the diﬀerences in the metrics,\norange text denote datasets chosen for the ﬁnal supervised pretraining run.\nSupervised Training. To obtain a general-purpose model which can reason\nabout documents with rich layout features, we constructed a dataset relying on\na large group of tasks, representing diverse types of information conveyed by a\ndocument (see Table 2 for datasets comparison). Datasets, which initially had\nbeen plain-text, had their layout produced, assuming some arbitrary font size\nand document dimensions. Some datasets, such as WikiTable Questions, come\nwith original HTML code – for the others, we render text alike. Finally, an image\nand computed bounding boxes of all words are used.\nAt this stage, the model is trained on each dataset for 10,000 steps or 5\nepochs, depending on the dataset size: the goal of the latter condition was to\navoid a quick overﬁtting.\nWe estimated each dataset’s value concerning a downstream task, assum-\ning a ﬁxed number of pretraining steps followed by ﬁnetuning. The results of\nthis investigation are demonstrated in Figure 6, where the group of WikiTable,\nWikiOps, SQuAD, and infographicsVQA performed robustly, convincing us to\nrely on them as a solid foundation for further experiments.\n∗http://www.industrydocuments.ucsf.edu/\n12 Powalski et al.\nModel pretrained in unsupervised, and then supervised manner, is at the\nend ﬁnetuned for two epochs on a downstream task with AdamW optimizer and\nhyperparameters presented in Table 3.\nTable 3. Parameters used during the ﬁnetuning on a downstream task. Batch size,\nlearning rate and scheduler were subject of hyperparameter search with consid-\nered values of respectively {8, 16, ...,2048}, {5e −5, 2e −5, 1e −5, 5e −4, ...,1e −3},\n{constant, linear}. We have noticed that the classiﬁcation task of RVL-CDIP requires\na signiﬁcantly larger bath size. The model with the highest validation score within the\nspeciﬁed steps number limit was used.\nDataset Batch size Steps Learning rate Scheduler\nSROIE 8 6,200 1e-4 constant\nDocVQA 64 100,000 2e-4 linear\nCORD 8 36,000 2e-4 linear\nRVL-CDIP 1,024 12,000 1e-3 linear\n5.2 Results\nThe TILT model achieved state-of-the-art results on three out of four considered\ntasks (Table 4). We have conﬁrmed that unsupervised layout- and vision-aware\npretraining leads to good performance on downstream tasks that require com-\nprehension of tables and other structures within the documents. Additionally,\nwe successfully leveraged supervised training from both plain-text datasets and\nthese involving layout information.\nDocVQA. We improved SOTA results on this dataset by 0.33 points. Moreover,\ndetailed results show that model gained the most in table-like categories, i.e.,\nforms (89.5 → 94.6) and tables (87.7 → 89.8), which proved its ability to under-\nstand the spatial structure of the document. Besides, we see a vast improvement\nin the yes/no category (55.2 → 69.0).† In such a case, our architecture generates\nsimply yes or no answer, while sequence labeling based models require addi-\ntional components such as an extra classiﬁcation head. We noticed that model\nachieved lower results in the image/photo category, which can be explained by\nthe low presence of image-rich documents in our datasets.\nRVL-CDIP. Part of the documents to classify does not contain any readable\ntext. Because of this shortcoming, we decided to guarantee there are at least\n16 image tokens that would carry general image information. Precisely, we act\n†Per-category test set scores are available after submission on the competition web\npage: https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1.\nText-Image-Layout Transformer 13\nTable 4.Results of selected methods in relation to our base and large models. Bold\nindicates the best score in each category. All results on the test set, using the met-\nrics proposed by dataset’s authors. The number of parameters given for completeness\nthought encoder-decoder and LMs cannot be directly compared under this criterion.\nModel CORD SROIE DocVQA RVL-CDIP Size variant\nF1 F1 ANLS Accuracy (Parameters)\nLayoutLM [56] 94.72 94 .38 69 .79 94 .42 Base (113-160M)\n94.93 95 .24 72 .59 94 .43 Large (343M)\nLayoutLMv2 [55] 94.95 96 .25 78 .08 95 .25 Base (200M)\n96.01 97 .81 86 .72 95.64 Large (426M)\nLAMBERT [11] 96 .06 98.17 — — Base (125M)\nTILT (our) 95.11 97 .65 83 .92 95 .25 Base (230M)\n96.33 98 .10 87 .05 95.52 Large (780M)\nas there were tokens with bounding boxes covering 16 adjacent parts of the\ndocument. These have representations from U-Net, exactly as they were regular\ntext tokens. Our model places second, 0 .12 below the best model, achieving the\nsimilar accuracy of 95 .52.\nCORD. Since the complete inventory of entities is not present in all examples,\nwe force the model to generate a None output for missing entities. Our model\nachieved SOTA results on this challenge and improved the previous best score\nby 0.3 points. Moreover, after the manual review of the model errors, we noticed\nthat model’s score could be higher since the model output and the reference\ndiﬀer insigniﬁcantly e.g. ”2.00 ITEMS” and ”2.00”.\nSROIE. We excluded OCR mismatches and ﬁxed total entity annotations dis-\ncrepancies following the same evaluation procedure as Garncarek et al. [11].‡ We\nachieved results indistinguishable from the SOTA (98.10 vs. 98.17). Signiﬁcantly\nbetter results are impossible due to OCR mismatches in the test-set.\nThough we report the number of parameters near the name of the model size\nvariant, note it is impossible to compare the TILT encoder-decoder model to lan-\nguage models such as LayoutLMs and LAMBERT under this criterion. In par-\nticular, it does not reﬂect computational cost, which may be similar for encoder-\ndecoders twice as big as some language model [44, Section 3.2.2]. Nevertheless,\nit is worth noting that our Base model outperformed models with comparable\nparameter count.\n‡Corrections can be obtained by comparing their two public submissions.\n14 Powalski et al.\nTable 5.Results of ablation study. The minus sign indicates removal of the mentioned\npart from the base model.\nModel Score Relative change\nTILT-Base 82 .9 ±0.3 —\n– Spatial Bias 81 .1 ±0.2 −1.8\n– Visual Embeddings 81 .2 ±0.3 −1.7\n– Case Augmentation 82 .2 ±0.3 −0.7\n– Spatial Augmentation 82 .6 ±0.4 −0.3\n– Vision Augmentation 82 .8 ±0.2 −0.1\n– Supervised Pretraining 81 .2 ±0.1 −1.7\n6 Ablation study\nIn the following section, we analyze the design choices in our architecture, con-\nsidering the base model pretrained in an unsupervised manner and the same\nhyperparameters for each run. The DocVQA was used as the most representa-\ntive and challenging for Document Intelligence since its leaderboard reveals a\nlarge gap to human performance. We report average results over two runs of\neach model varying only in the initial random seed to account for the impact of\ndiﬀerent initialization and data order [7].\nSigniﬁcance of Modalities. We start with the removal of the 2D layout po-\nsitional bias. Table 5 demonstrates that information that allows models to rec-\nognize spatial relations between tokens is a crucial part of our architecture. It is\nconsistent with the previous works on layout understanding [55, 11]. Removal of\nthe UNet-based convolutional feature extractor results in a less signiﬁcant ANLS\ndecrease than the 2D bias. This permits the conclusion that contextualized image\nembeddings are beneﬁcial to the encoder-decoder.\nJustifying Regularization. Aside from removing modalities from the network,\nwe can also exclude regularization techniques. To our surprise, the results suggest\nthat the removal of case augmentation decreases performance most severely. Our\nbaseline is almost one point better than the equivalent non-augmented model.\nSimultaneously, model performance tends to be reasonably insensitive to the\nbounding boxes’ and image alterations. It was conﬁrmed that other modalities\nare essential for the model’s success on real-world data, whereas regularization\ntechniques we propose slightly improve the results, as they prevent overﬁtting.\nImpact of Pretraining. As we exploited supervised pretraining similarly to\nprevious authors, it is worth considering its impact on the overall score. In our\nablation study, the model pretreated in an unsupervised manner achieved signif-\nicantly lower scores. The impact of this change is comparable to the removal of\nText-Image-Layout Transformer 15\nspatial bias or visual embeddings. Since authors of the T5 argued that pretrain-\ning on a mixture of unsupervised and supervised tasks perform equally good\nwith higher parameter count, this gap may vanish with larger variants of TILT\nwe did not consider in the present paper [44].\n7 Summary\nIn the present paper, we introduced a novel encoder-decoder framework for\nlayout-aware models. Compared to the sequence labeling approach, the proposed\nmethod achieves better results while operating in an end-to-end manner. It can\nhandle various tasks such as Key Information Extraction, Question Answering\nor Document Classiﬁcation, while the need for complicated preprocessing and\npostprocessing steps is eliminated.\nAlthough encoder-decoder models are commonly applied to generative tasks,\nboth DocVQA, SROIE, and CORD we considered are extractive. We argue that\nbetter results were achieved partially due to the independence from the de-\ntected word order and resistance to OCR errors that the proposed architecture\npossesses. Consequently, we were able to achieve state-of-the-art results on two\ndatasets (DocVQA, CORD) and performed on par with the previous best scores\non SROIE and RVL-CDIP, albeit having a much simpler workﬂow.\nSpatial and image enrichment of the Transformer model allowed the TILT to\ncombine information from text, layout, and image modalities. We showed that\nthe proposed regularization methods signiﬁcantly improve the results.\nAcknowledgments. The authors would like to thank Filip Grali´ nski, Tomasz\nStanis lawek, and  Lukasz Garncarek for fruitful discussions regarding the paper\nand our managing directors at Applica.ai. Moreover, Dawid Jurkiewicz pays due\nthanks to his son for minding the deadline and generously coming into the world\na day after.\nThe Smart Growth Operational Programme supported this research under project\nno. POIR.01.01.01-00-0877/19-00 ( A universal platform for robotic automation\nof processes requiring text comprehension, with a unique level of implementation\nand service automation).\nReferences\n1. Cho, M., Amplayo, R., Hwang, S.w., Park, J.: Adversarial TableQA: Attention\nsupervision for question answering on tables. In: PMLR (2018)\n2. Choi, E., He, H., Iyyer, M., Yatskar, M., tau Yih, W., Choi, Y., Liang, P., Zettle-\nmoyer, L.: QuAC: Question answering in context. In: EMNLP (2018)\n3. Chuang, Y., Liu, C., Lee, H., Lee, L.: SpeechBERT: An audio-and-text jointly\nlearned language model for end-to-end spoken question answering. In: ISCA (2020)\n4. Clark, J.H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V.,\nPalomaki, J.: TyDi QA: A benchmark for information-seeking question answering\nin typologically diverse languages. TACL (2020)\n16 Powalski et al.\n5. Dai, J., Li, Y., He, K., Sun, J.: R-FCN: Object detection via region-based fully\nconvolutional networks. In: NeurIPS (2016)\n6. Denk, T.I., Reisswig, C.: BERTgrid: Contextualized embedding for 2d document\nrepresentation and understanding (2019), arXiv preprint\n7. Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H., Smith, N.A.: Fine-\ntuning pretrained language models: Weight initializations, data orders, and early\nstopping (2020), arXiv preprint\n8. Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., Gardner, M.: DROP: A\nreading comprehension benchmark requiring discrete reasoning over paragraphs.\nIn: NAACL-HLT (2019)\n9. Dwojak, T., Pietruszka, M., Borchmann,  L., Ch ledowski, J., Grali´ nski, F.: From\ndataset recycling to multi-property extraction and beyond. In: CoNLL (2020)\n10. Ethayarajh, K.: How contextual are contextualized word representations? compar-\ning the geometry of BERT, ELMo, and GPT-2 embeddings. In: EMNLP-IJCNLP\n(2019)\n11.  Lukasz Garncarek, Powalski, R., Stanis lawek, T., Topolski, B., Halama, P., Turski,\nM., Grali´ nski, F.: LAMBERT: Layout-aware (language) modeling using bert for\ninformation extraction (2021), accepted to ICDAR 2021\n12. Guu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M.: Retrieval augmented language\nmodel pre-training. In: ICML (2020)\n13. Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu,\nC., Xu, Y., Yang, Z., Zhang, Y., Tao, D.: A survey on visual transformer (2021),\narXiv preprint\n14. Harley, A.W., Ufkes, A., Derpanis, K.G.: Evaluation of deep convolutional nets for\ndocument image classiﬁcation and retrieval. In: ICDAR (2015)\n15. Herzig, J., Nowak, P.K., M¨ uller, T., Piccinno, F., Eisenschlos, J.: TaPas: Weakly\nsupervised table parsing via pre-training. In: ACL (2020)\n16. Hewlett, D., Lacoste, A., Jones, L., Polosukhin, I., Fandrianto, A., Han, J., Kelcey,\nM., Berthelot, D.: WikiReading: A novel large-scale language understanding task\nover Wikipedia. In: ACL (2016)\n17. Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multi-\ndimensional transformers (2019), arXiv preprint\n18. Hong, T., Kim, D., Ji, M., Hwang, W., Nam, D., Park, S.: BROS: A pre-trained lan-\nguage model for understanding texts in document (2021), openreview.net preprint\n19. Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., Jawahar, C.: IC-\nDAR2019 competition on scanned receipt OCR and information extraction. In:\nICDAR (2019)\n20. Hwang, W., Yim, J., Park, S., Yang, S., Seo, M.: Spatial dependency parsing for\nsemi-structured document information extraction (2020), arXiv preprint\n21. Jaume, G., Ekenel, H.K., Thiran, J.P.: FUNSD: A dataset for form understanding\nin noisy scanned documents. In: ICDAR-OST (2019)\n22. Kaﬂe, K., Price, B.L., Cohen, S., Kanan, C.: DVQA: understanding data visual-\nizations via question answering. In: CVPR (2018)\n23. Kahou, S.E., Michalski, V., Atkinson, A., K´ ad´ ar, ´A., Trischler, A., Bengio, Y.:\nFigureQA: An annotated ﬁgure dataset for visual reasoning. In: ICLR (2018)\n24. Kasai, J., Pappas, N., Peng, H., Cross, J., Smith, N.A.: Deep encoder, shallow\ndecoder: Reevaluating the speed-quality tradeoﬀ in machine translation (2020),\narXiv preprint\n25. Keskar, N., McCann, B., Xiong, C., Socher, R.: Unifying question answering and\ntext classiﬁcation via span extraction (2019), arXiv preprint\nText-Image-Layout Transformer 17\n26. Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., Hajishirzi,\nH.: UniﬁedQA: Crossing format boundaries with a single QA system. In: EMNLP-\nFindings (2020)\n27. Khot, T., Clark, P., Guerquin, M., Jansen, P., Sabharwal, A.: QASC: A dataset\nfor question answering via sentence composition. In: AAAI (2020)\n28. Kudo, T.: Subword regularization: Improving neural network translation models\nwith multiple subword candidates. In: ACL (2018)\n29. Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury, J., Gulrajani, I., Zhong,\nV., Paulus, R., Socher, R.: Ask me anything: Dynamic memory networks for natural\nlanguage processing. In: ICML (2016)\n30. Kwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M., Parikh, A., Alberti, C.,\nEpstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey,\nM., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural questions:\nA benchmark for question answering research. TACL (2019)\n31. Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.: RACE: Large-scale ReAding com-\nprehension dataset from examinations. In: EMNLP (2017)\n32. Le, H., Sahoo, D., Chen, N., Hoi, S.: Multimodal transformer networks for end-to-\nend video-grounded dialogue systems. In: ACL (2019)\n33. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked cross attention for image-text\nmatching. In: ECCV (2018)\n34. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy-\nanov, V., Zettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. In: ACL (2020)\n35. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: VisualBERT: A simple\nand performant baseline for vision and language (2019), arXiv preprint\n36. Liu, X., Gao, F., Zhang, Q., Zhao, H.: Graph convolution for multimodal informa-\ntion extraction from visually rich documents. In: NAACL-HLT (2019)\n37. Ma, J., Qin, S., Su, L., Li, X., Xiao, L.: Fusion of image-text attention for\ntransformer-based multimodal machine translation. In: IALP (2019)\n38. Mathew, M., Karatzas, D., Jawahar, C.: DocVQA: A dataset for VQA on document\nimages. In: WACV (2021)\n39. McCann, B., Keskar, N.S., Xiong, C., Socher, R.: The natural language decathlon:\nMultitask learning as question answering (2018), arXiv preprint\n40. Palm, R.B., Winther, O., Laws, F.: CloudScan - a conﬁguration-free invoice anal-\nysis system using recurrent neural networks. In: ICDAR (2017)\n41. Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., Lee, H.: CORD: A consoli-\ndated receipt dataset for post-ocr parsing. In: Document Intelligence Workshop at\nNeurIPS (2019)\n42. Powalski, R., Stanislawek, T.: UniCase – rethinking casing in language models\n(2020), arXiv prepint\n43. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners (2019), technical report\n44. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. JMRL (2020)\n45. Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: SQuAD: 100,000+ questions for\nmachine comprehension of text. In: EMNLP (2016)\n46. Reddy, S., Chen, D., Manning, C.D.: CoQA: A conversational question answering\nchallenge. TACL (2019)\n47. Ren, Y., Liu, J., Tan, X., Zhao, Z., Zhao, S., Liu, T.Y.: A study of non-\nautoregressive model for sequence generation. In: ACL (2020)\n18 Powalski et al.\n48. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Networks for Biomed-\nical Image Segmentation. In: MICCAI (2015)\n49. Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words with\nsubword units. In: ACL (2016)\n50. Sidorov, O., Hu, R., Rohrbach, M., Singh, A.: TextCaps: A dataset for image\ncaptioning with reading comprehension. In: ECCV (2020)\n51. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D.,\nRohrbach, M.: Towards VQA models that can read. In: CVPR (2019)\n52. Stanis lawek, T., Grali´ nski, F., Wr´ oblewska, A., Lipi´ nski, D., Kaliska, A., Rosalska,\nP., Topolski, B., Biecek, P.: Kleister: Key information extraction datasets involving\nlong documents with complex layouts (2021), accepted to ICDAR 2021\n53. Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: VL-BERT: pre-training\nof generic visual-linguistic representations. In: ICLR (2020)\n54. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)\n55. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C.,\nChe, W., Zhang, M., Zhou, L.: LayoutLMv2: Multi-modal pre-training for visually-\nrich document understanding (2020), arXiv preprint\n56. Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: LayoutLM: Pre-training of\ntext and layout for document image understanding. In: KDD (2020)\n57. Yin, P., Neubig, G., Yih, W.t., Riedel, S.: TaBERT: Pretraining for joint under-\nstanding of textual and tabular data. In: ACL (2020)"
}