{
  "title": "Evaluating Pre-trained BERT-based Language Models for Detecting Misinformation",
  "url": "https://openalex.org/W4229026567",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A181599030",
      "name": "Rini Anggrainingsih",
      "affiliations": [
        "Sebelas Maret University"
      ]
    },
    {
      "id": "https://openalex.org/A2135036438",
      "name": "Ghulam Mubashar Hassan",
      "affiliations": [
        "University of Western Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2215513321",
      "name": "Amitava Datta",
      "affiliations": [
        "University of Western Australia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2084591134",
    "https://openalex.org/W2281420995",
    "https://openalex.org/W4210408932",
    "https://openalex.org/W6603713569",
    "https://openalex.org/W2911840101",
    "https://openalex.org/W2741930413",
    "https://openalex.org/W3080378555",
    "https://openalex.org/W3125351797",
    "https://openalex.org/W4237544127",
    "https://openalex.org/W6600286444",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2420085356",
    "https://openalex.org/W4205387711",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W6600686112",
    "https://openalex.org/W6600002382",
    "https://openalex.org/W2945928904",
    "https://openalex.org/W2963704837",
    "https://openalex.org/W6603850445",
    "https://openalex.org/W6600655081",
    "https://openalex.org/W2765736339",
    "https://openalex.org/W3173180842",
    "https://openalex.org/W3175875420",
    "https://openalex.org/W3131858610",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3207040866",
    "https://openalex.org/W3013511804",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3004179129",
    "https://openalex.org/W2763572884",
    "https://openalex.org/W3139850432",
    "https://openalex.org/W3152219206",
    "https://openalex.org/W4286984556",
    "https://openalex.org/W4297730508",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035658351",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3017402509",
    "https://openalex.org/W2778443828",
    "https://openalex.org/W2888519984",
    "https://openalex.org/W2919877138",
    "https://openalex.org/W4210380386",
    "https://openalex.org/W2977886427",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W3005668675",
    "https://openalex.org/W3177190859",
    "https://openalex.org/W2952632192",
    "https://openalex.org/W3033625896",
    "https://openalex.org/W3046747294",
    "https://openalex.org/W3035317912",
    "https://openalex.org/W2944575651",
    "https://openalex.org/W4300952844",
    "https://openalex.org/W2977683229",
    "https://openalex.org/W3030030185",
    "https://openalex.org/W1055024227",
    "https://openalex.org/W4312877162",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3032990727",
    "https://openalex.org/W2815470153",
    "https://openalex.org/W2945804568",
    "https://openalex.org/W3017929466",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2971292190",
    "https://openalex.org/W3119467012",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3003214995",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2966765144",
    "https://openalex.org/W2912078280",
    "https://openalex.org/W2911704589",
    "https://openalex.org/W2568349553",
    "https://openalex.org/W3156800454",
    "https://openalex.org/W2577888896",
    "https://openalex.org/W3103872969",
    "https://openalex.org/W3012732565",
    "https://openalex.org/W3004200020"
  ],
  "abstract": "Abstract It is challenging to control the quality of online information due to the lack of supervision. Manual checking is almost impossible given the vast number of posts made on online media and how quickly they spread. Therefore, there is a need for automated rumour detection techniques to limit the adverse effects of spreading misinformation. Previous studies mainly focused on finding and extracting the significant features of text data. However, extracting features is time-consuming and not a highly effective process. This study proposes the BERT-based pre-trained language models to encode text data into vectors and utilise neural network models to classify these vectors to detect misinformation. Furthermore, different language models (LM) ’ performance with different trainable parameters was compared. The proposed technique is tested on different short and long text datasets. The result of the proposed technique has been compared with the state-of-the-art methods on the same datasets. The results show that the proposed technique performs better than the state-of-the-art techniques. We also tested the proposed technique by combining the datasets. The results demonstrated that the large training and testing size of data considerably improves the technique’s performance. Therefore, it is suggested that the dataset, splitting data, and classification techniques must be considered carefully to analyse performance of the solutions.",
  "full_text": "Evaluating Pre-trained BERT -based Language\nModels for Detecting Misinformation\nRini Anggrainingsih  (  rini.anggrainingsih@staff.uns.ac.id )\nSebelas Maret University\nGhulam Mubashar Hassan \nThe University of Western Australia\nAmitava Datta \nThe University of Western Australia\nResearch Article\nKeywords: BERT, Fake News Detection, Finetuned Language Model, Rumor Detection\nPosted Date: May 5th, 2022\nDOI: https://doi.org/10.21203/rs.3.rs-1608574/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nSpringer Nature 2021 L ATEX template\nEvaluating Pre-trained BERT-based\nLanguage Models for Detecting\nMisinformation\nRini Anggrainingsih1,2*, Ghulam Mubashar Hassan2†\nand Amitava Datta2†\n1*Informatics Department of Mathematics and Natural Sciences\nFaculty, Sebelas Maret University, Ir Sutami 36A, Surakarta,\n57126, Central Java, Indonesia.\n2Computer Science and Software Engineering, The University of\nWestern Australia, 35 Stirling Highway, Perth, 6009, WA,\nAustralia.\n*Corresponding author(s). E-mail(s):\nrini.anggrainingsih@staﬀ.uns.ac.id,\nrini.anggrainingsih@research.uwa.edu.au;\nContributing authors: ghulam.hassan@uwa.edu.au;\namitava.datta@uwa.edu.au;\n†These authors contributed equally to this work.\nAbstract\nIt is challenging to control the quality of online information d ue to\nthe lack of supervision. Manual checking is almost impossibl e given\nthe vast number of posts made on online media and how quickly t hey\nspread. Therefore, there is a need for automated rumour detection\ntechniques to limit the adverse eﬀects of spreading misinformat ion. Pre-\nvious studies mainly focused on ﬁnding and extracting the sign iﬁcant\nfeatures of text data. However, extracting features is time-consu ming\nand not a highly eﬀective process. This study proposes the BERT -\nbased pre-trained language models to encode text data into vec tors and\nutilise neural network models to classify these vectors to detec t misin-\nformation. Furthermore, diﬀerent language models (LM) ’ performance\nwith diﬀerent trainable parameters was compared. The proposed tec h-\nnique is tested on diﬀerent short and long text datasets. The res ult\n1\nSpringer Nature 2021 L ATEX template\n2 Article Title\nof the proposed technique has been compared with the state-of-t he-\nart methods on the same datasets. The results show that the propo sed\ntechnique performs better than the state- of-the-art technique s. We\nalso tested the proposed technique by combining the datasets . The\nresults demonstrated that the large training and testing size of data\nconsiderably improves the technique’s performance. Therefore, it i s sug-\ngested that the dataset, splitting data, and classiﬁcation techniques\nmust be considered carefully to analyse performance of the solutio ns.\nKeywords: BERT, Fake News Detection, Finetuned Language Model,\nRumor Detection\n1 Introduction\nThe rapid growth of internet technology and online social media has revol u-\ntionised communication. People can freely and easily access the latest news,\nshare, and express their opinions on social media. However, these conv eniences\nmay promote the creation and the spread of false information, such as rumour s\nand fake news that cause social problems, including ﬁnancial loss, pub lic\npanic, personal or community reputation defacement, and human life thr eats.\nRecently, spreading rumours and fake news on online media platforms h as\nbecome a serious issue. It is nearly impossible for ordinary people to distinguish\nrumour and non-rumour information from most online platforms. Therefore,\nthere is a need for automated detection technology to identify rumours or fake\nnews on online social media.\nCastillo et al. [\n1], who were known as pioneers in detecting credible informa-\ntion research area, used feature engineering and machine learning tec hniques\nto detect rumours on Twitter and provided a baseline result for othe r studies.\nOther studies then attempted to improve the accuracy of detectin g misinforma-\ntion by discovering the essential features from the context and cont ent of text\ndata. Furthermore, some other studies compared the performance of variou s\nmachine-learning algorithms for detecting rumours, such as Random Fore st,\nSupport Vector Machine, K-Nearest Neighbour, Na¨ ıve Bayes Classiﬁer, and\nLogistic Regression [ 2–5]. Similarly, feature engineering and machine learning\ntechniques are also widely used to detect fake news on online media [6–9].\nFollowing the success of the neural network based approaches in various\nclassiﬁcation problems, most recent studies leveraged diﬀerent ne ural net-\nwork based techniques for detecting rumours and fake news on online me dia,\nincluding Recurrent Neural Network (RNN) [ 10–12], Convolutional Neural\nNetwork (CNN) [ 13–17], and Long Short Term Memory (LSTM) network [ 18–\n20], statistical features fusion network [ 21], and propagation path aggregation\nnetwork [ 22]. Some studies used diﬀerent methodologies to detect rumours on\nTwitter, such as reinforcement learning [ 23], entity recognition and sentence\nconﬁguration [ 24], and graph-based neural networks [ 25–29].\nSpringer Nature 2021 L ATEX template\nArticle Title 3\nThe emergence of Bidirectional Encoder Representation from Transfor mer\n(BERT) as a pre-trained language model (LM) in 2018 by Devlin et al. [ 30] has\nsigniﬁcantly improved the area of Natural Language Processing (NLP). BERT\ncan understand a word’s contextual meaning by considering words from t he\nleft and the right sides. Furthermore, it can be ﬁnetuned to deal wi th other\nNLP-related problems. Pre-trained LMs have been applied on various typ es of\nNLP tasks, such as text summarisation [\n31–33], text generation [ 34–36], and\ntext classiﬁcation [ 37–41]. BERT has become the current research trend and\nmarks a new era of NLP because of its high-performance results. Despite the\nbeneﬁts and ability of BERT, some issues are attributed to its comple xity.\nThereofore, some studies attempted to improve the LM’s performance based\non BERT’s architecture and introduced other LM, such as RoBERTa [ 42] and\nDistilBERT [ 43] which were computationally less complex.\nMotivated by the ability of pre-trained LM to classify text, this stu dy aims\nto examine and compare the output performance of three diﬀerent BERT-\nbased language models when combined with neural network classiﬁers to detect\nmisinformation. Diﬀerent LMs used are RoBERTa, BERT and DistilBERT.\nEach LM is ﬁnetuned on labelled datasets and encoded into vectors. Afte r-\nward, these vectors were used to train the neural network-based tec hniques\nto detect misinformation. The model was validated on short and long text\ndatasets (Tweets and fake news) which are: Pheme1 and Pheme2 datasets\nfrom [\n3, 19], Twitter15 and Twitter16 datasets [ 10] and COVID-19 Fake News\ndataset from Mendeley [ 44]. The contributions of this study are:\n• Proposing a new model to improve rumour detection performance based on\ncombining ﬁnetuned LM and neural network.\n• Thoroughly evaluating the performance of three diﬀerent variants of the\nproposed model in detecting rumours and fake news on four diﬀerent\ndatasets.\n• Introducing a new benchmark testing procedure where all dataset s are\ndistributed in the same manner for training, validation and testing.\n• Demonstrating that the proposed model outperforms the recent state- of-\nthe-art techniques in rumour and fake news detection.\nThe rest of the paper is structured as follows: Sections 2 and 3 revie w\nthe related works on rumour detection and the details of the proposed mod el\non utilising pre-trained LM and neural networks to detect rumour and f ake\nnews, respectively. The experiments are discussed to invest igate the model’s\nperformance in Section 4. Finally, Section 5 summarises the study.\n2 Related Work\nThis section explains spreading rumours and fake news on online social\nmedia. Later, recent studies related to both rumour and fake news dete c-\ntion techniques are categorised and discussed systematically in diﬀ erent\nsubsections.\nSpringer Nature 2021 L ATEX template\n4 Article Title\n2.1 Rumour and Fake News Detection\nThough studies give diﬀerent deﬁnitions, rumor and fake news are often used\ninterchangeably. A rumour is a story that seems credible but complic ated\nto verify since some part of the information could remain unveriﬁed [\n45]. In\ncontrast, fake news is false information that spreads and appears credibl e to\ngain biased public opinion on particular issues [ 46]. The lack of supervision\nover social media posts makes rumours and fake news propagate quickly in the\nsociety.\nSpreading rumours and fake news has become a serious concern due to the\nfast growth of internet technology and online media platforms. These is sues\noften lead to anxiety or scepticism and attract readers to share them wi thout\nveriﬁcation [ 3, 47]. Most rumour and fake news on diﬀerent topics spread\nthrough online media daily and inﬂuence people’s opinions. Therefore , there\nis an urgent need to detect rumours or fake news automatically since manu al\nfact-checking is a challenging and time-consuming process.\n2.2 Traditional Machine Learning-based Approaches\nMost of the earlier studies regarding information credibility focus ed on ﬁnding\nand extracting the signiﬁcant features from the sources, such as the t ext con-\ntents and user details [\n1, 2, 4–9, 48, 49]. Standard machine learning techniques,\nsuch as Random Forest, Conditional Random Field, Support Vector Machine,\nNa¨ ıve Bayes, and K-Nearest Neighbour were used to detect the authent icity\nof the tweets.\nSimilarly, researchers handled fake news detection problems. Rec ently,\nAl-Ahmad et al. conducted a comprehensive study. The study applied an d\ncompared selected evolutionary classiﬁcation models, including Par ticle Swarm\nOptimisation (PSO), Genetic Algorithm (GA), and Salp Swarm Algorithm\n(SSA), to detect fake news related to COVID-19 issues [\n9]. This study reported\nthat the best results were obtained using K-Nearest Neighbor and Genet ic\nAlgorithm techniques.\nOverall, feature extraction is a fundamental step of a machine learnin g\napproach. However, manually extracting features is a time-consuming process\nthat becomes more challenging as some features are missing due to a user ’s\nsecurity settings. This condition aﬀects the eﬀectiveness of the feature-based\napproach.\n2.3 Deep Learning-based Approaches\nDeep learning with neural network approaches has shown promising res ults\nin various text classiﬁcation problems. In the context of false informat ion\ndetection, the widely implemented neural network framework approac hes on\ndetecting rumour are Recurrent Neural Network (RNN) [\n10–12], Long-Short\nTerm Memory (LSTM) [ 18–20] and Convolutional Neural Network (CNN) [ 13–\n17]. Recent studies that used neural network-based hybrid approaches have\nSpringer Nature 2021 L ATEX template\nArticle Title 5\nachieved the new state-of-the-art results, which are considered i n this study as\nbenchmark results and compared to the performance of the proposed meth ods.\nWu and Rao worked on rumour detection by utilising interaction betwee n\nthe features of rumours. The study proposed models called “Adaptive In ter-\naction Fusion Networks (AIFN)” and “Gated Adaptive Interaction Networks\n(GAIN)” to identify true and false rumours on Twitter [\n50]. These tech-\nniques were used to identify interaction features among the tweet s and capture\nsemantic conﬂict in posts and comments. The study evaluated their mo d-\nels on Pheme2 dataset [ 19]. Later a “Decision Tree-based CoAttention model\n(DTCA)” was proposed to extend this study by utilising the interac tion of\ncredible comments as evidence to detect the truth of the tweet [ 51]. The study\nreported 82.46% accuracy and 82.5% F1-score.\nWang et al. [ 28] aimed to detect rumour by considering a background hid-\nden knowledge in the post’s text content and proposed a “Knowledge-dr iven\nMultimodal Graph Convolutional Network”. This approach combined textual,\nconceptual and visual data to represent the semantic information into a uniﬁed\nframework for fake news identiﬁcation. It used Pheme1 dataset [\n3] to vali-\ndate the model. The study reported above 87% for all evaluation parameters\nincluding, accuracy, precision, recall and F1-score.\nLu and Li [\n27] aimed to predict whether the source of tweets was fake\nby using user proﬁles and social interactions. A Graph-Aware CoAttent ion\nNetwork (GCAN) was proposed based on neural network models to depict the\ninteraction among users and capture the relationship between the sour ce tweet,\nits propagation and user interaction to generate a prediction. The model was\nevaluated using Twitter15 and Twitter16 datasets [\n10]. Accuracy of 87% and\n90% on Twitter15 and Twitter16 datasets were reported, respectively.\nWang and Guo encoded tweets’ sentiment information and word represe n-\ntation with a two-layer Cascaded Gated Recurrent Unit (CGRU) to detec t\nrumours [ 52]. The model was validated using Twitter16 datasets [ 10]. The\nmodel achieved 88.5% accuracy that outperformed earlier studies. Anothe r\nstudy [ 15] classiﬁed tweets as rumour or non-rumour using a Dual Convolu-\ntional Neural Network (DCNN) technique using the inherent features of t he\ninformation set. Their study used Twitter15 and Twitter16 datasets [ 10] to\nevaluate the model and reported F1 scores of 86% and 87%, respectively.\nZhanh et al. proposed a lightweight propagation path aggregating (PPA)\nneural network for rumor embedding and classiﬁcation [ 22]. They ﬁrst modeled\nthe propagation structure of each rumor as an independent set of propagation\npaths in which each path represents the source post in a diﬀerent con versation\ncontext. Then aggregated all paths to obtain the representation of the whole\npropagation structure. Twitter 15, Twitter 16 and Pheme1 datasets were u sed\nto evaluate their model and achieved accuracies of 87.3%, 88.7% and 80.3%\nfor Twitter15, Twitter16 and Pheme1, respectively.\nAnother model proposed by Ma et al.[\n24] by integrating entity recognition,\nsentence reconﬁguration, and ordinary diﬀerential equation network un der a\nuniﬁed framework called ESODE. They used an entity recognition meth od to\nSpringer Nature 2021 L ATEX template\n6 Article Title\nenhance the semantic understanding of rumor texts, and designed a se ntence\nreconﬁguration to improve the frequency of important words, then est ablished\na complete feature map by collecting statistical features from three aspects,\nincluding linguistic features on the content of rumors, characteris tics of users\ninvolved in rumor propagating, and propagation network structures. They\nreported results on Twitter15-16 dataset by achieving 92.4% precision, 92. 6%\nof recall, and 92.5% for F1-score.\n2.4 BERT-based Language Model\nThe emergence of Language Models (LM) and transformers signiﬁcantly\nimproved the solutions related to Natural Language Processing (NLP). BERT\nis a pre-trained language model trained on vast unlabelled data from the\nBooksCorpus that includes 800 million words and English Wikipedia wit h\n2,500 million words without any genuine training objective. Therefore, it can\nbe ﬁnetuned for many NLP tasks. BERT can understand a word’s contextual\nmeaning by considering words from both the left and the right sides. F urther-\nmore, it can represent words and sentences that are converted into nu meric\nvectors [\n30]. There are two standard architectures of BERT: BERT BASE and\nBERTLARGE. In general, BERT BASE has 12 encoder layers with 110 million\nparameters and 768 hidden layers, and BERT LARGE has 16 encoder layers with\n340 million parameters and 1,024 hidden layers.\nDespite the beneﬁt and ability of the pre-trained language model, the re are\nissues regarding training, memory consumption, and computational power due\nto a large amount of training data. Therefore, some researchers introduc ed dif-\nferent LMs to address these issues, such as RoBERTa [ 42] to address a training\nmodel issue and DistilBERT [ 43] to deal with the memory and computational\nineﬃciency problems.\nLiu et al. examined the eﬀect of hyperparameter tuning and training si ze\non BERT architecture [ 42]. The results showed that BERT was signiﬁcantly\nunder-trained. Therefore, an improved BERT training model calle d Robustly\nOptimised BERT Approach (RoBERTa) was proposed. The training proce-\ndures were modiﬁed, including; 1) training the model with a larger dataset; 2)\nremoving the next sentence prediction objective; 3) training in a more extended\nsequence; and 4) changing the masking pattern dynamically to the train ing\ndata. RoBERTa was trained by following BERT LARGE architecture. However,\nunlike BERT, which initially trained using 16GB of the dataset, a more exten-\nsive training dataset containing 160GB of text was used to train RoBERTa.\nIn addition, RoBERTa was trained for more iterations of 300,000, which was\nlater extended to 500,000. RoBERTa consistently outperformed BERT in all\nindividual tasks on the standard GLUE benchmark [\n53].\nAlthough BERT and RoBERTa, are leading in the current NLP research,\nthe large models raise some issues regarding the computational cost and\nrequirements. Sanh et al. proposed a lighter and faster LM based on BERT\narchitecture using knowledge distillation to compress the model to deal with\ncomputational issues [\n43]. Furthermore, they trained the compressed model\nSpringer Nature 2021 L ATEX template\nArticle Title 7\nto reproduce the behaviour of the large model. The compressed mode l is\ncalled DistilBERT. It has 66 million parameters, 40% fewer than BERT and\ntrains 60% faster than BERT. The results showed that DistilBERT mod els\ncould achieve good results on several NLP tasks and are computationally light\nenough to run on mobile devices.\nSome researchers in the information credibility area who applied BE RT-\nbased LMs in classifying misinformation have reported that they obtain ed an\nexcellent performance by using LMs instead of using standard machine learning\nmethods [\n54–56]. Furthermore, other researchers compared LMs’ performance\nto evaluate the cross-source failure problem in the current misinf ormation\ndetection methods. They focused on ﬁnding generalisable represe ntation so\nthat the classiﬁcation model would be more applicable in real-world data [57].\nThey examined the cross-source generalisability by choosing one datas et as a\ntraining set and treating other datasets as testing sets.\nPelrine et al. evaluated the performance of a few pre-trained Language\nModels on detecting rumours [ 58]. However, they took a diﬀerent direction to\nfacilitate a solid standard benchmark by using diﬀerent distribut ion of data\non each dataset just same with the previous studies to make a fair bench mark\ncomparison. The LMs were ﬁnetuned, and a single layer perceptron was u sed\nas a classiﬁer in their study. The results showed that the perform ance of the\nproposed method was better than the state-of-the-art models. There fore, Pel-\nrine’s et al. study is included as a current state-of-the-art mod el and compared\nwith the proposed methods.\n3 Material and Method\nThis section describes the proposed models and datasets used to vali date the\nproposed models and explains the experimental steps used in this s tudy. This\nstudy focuses on comparing and evaluating the performance of three diﬀ er-\nent size BERT-based LMs, including RoBERTa, BERT BASE, and DistilBERT\non distinguishing rumour and non-rumour tweets, classifying true an d false-\nrumours, and detecting true and fake-news by using neural network- based\nclassiﬁer. Various datasets were then used to validate the performanc e of the\nproposed methods. The proposed method and datasets are explained belo w.\n3.1 The Proposed Model\nThis study utilises the ﬁnetuned model of LMs as an encoder to repr esent\ntext into vectors and applying neural network models as a classiﬁer. The\narchitecture of the proposed method is presented in Figure\n1.\nThe ﬁnetuned LM was leveraged to encode the sentences into vectors . After\nthe data was encoded into vectors by ﬁnetuned LMs, two diﬀerent ne ural net-\nwork techniques were used for classiﬁcation, including Multilay er Perceptron\n(MLP) and Residual Network on Convolutional Neural Network (Resnet-\nCNN). MLP is a neural network that comprises more than one perceptron.\nSpringer Nature 2021 L ATEX template\n8 Article Title\nFig. 1 The proposed architecture for detecting credibility of inf ormation\nGenerally, it consists of an input layer to receive the signal, an outp ut for pre-\ndicting the input, and an arbitrary number of hidden layers betwee n the input\nand output [ 59]. This study uses four layers of MLP (4L-MLP) and performed\nregularisation and dropout on the 4L-MLP model (4L-MLP+Reg+Drop).\nWhile ResNet-CNN is an improved model for addressing the vanishing gradient\nproblem that appears on a Convolutional Neural Network (CNN) with deeper\nlayers. A CNN is similar to MLP, though it comprises several layers in its hid-\nden layer, including convolutional, pooling, fully connected, and r egularisation\nlayers. These additional layers play an essential role in the succe ss of most\ndeep neural networks. However, a study [\n60] empirically showed a maximum\nthreshold for the depth of the additional layers of the CNN model. A ne w neu-\nral layer called the residual network was introduced to deal with th e problem\nof deeper networks. ResNet comprises residual blocks, which skip connections\nto some layers and join the outputs from prior layers to the output of st acked\nlayers. This skipped connections could solve the vanishing gradien t problem in\nCNN and improve neural networks’ performance with more layers [ 60]. This\nstudy uses 10-layers and 18-layers ResNet-CNN for classiﬁcation purpose s.\n3.2 Datasets\nBoth long-text (fake news) and short-text (tweets) datasets were use d to\nvalidate the proposed method’s performance. Regarding the splittin g data\ntechnique, this study suggests to take the same distribution of data for all\ndatasets to make a standard testing procedure. It is diﬀerent from m any exist-\ning studies, that took a diﬀerent distribution based on the state-of -the-art\nresults of each dataset. The datasets are explained in detail in the foll owing\nsubsections.\nSpringer Nature 2021 L ATEX template\nArticle Title 9\n3.2.1 COVID-19 Fake News Dataset\nCOVID-19 Fake News Dataset from Mendeley [\n44] was selected to represent\na long-text dataset. This dataset contains news about COVID-19 issues from\nDecember 2019 to July 2020. Each item is labelled as true-news (2,061) and\nas false-news (1,058). It is called as Covid T/F dataset.\n3.2.2 Pheme Datasets\nThere are two types of Pheme datasets: Pheme1 and Pheme2. Pheme1 cont ains\n5,791 tweets about ﬁve breaking news topics where 1,969 tweets are labelle d as\nrumours and 3,822 are labelled as non-rumours by journalists [\n3]. Pheme2 is the\nextended version of Pheme1, with four additional news topics, meaning that\nPheme2 contained 6,425 tweets about nine news events in total. The journ alists\nveriﬁed whether these tweets are rumour (2,402 tweets) or non-rumour (4,023\ntweets). Then, rumour tweets are further classiﬁed and labelled as true-rumour\n(1,067 tweets), false-rumour (638 tweets) or unveriﬁed-rumour (697 twe ets)\n[19]. However, this study does not consider the data with unveriﬁed- rumour\nlabel as it is beyond the scope of this study.\nPheme datasets are used in diﬀerent manner in diﬀerent state-of-th e-art\ntechniques. Therefore we evaluated the proposed models in the same manner\nto make a fair comparison with existing studies. These detailed sub -datasets\nare explained below:\n• Pheme1 R/NR: It contains 5,791 tweets where 1,969 are labelled as rumour\ntweets and 3,822 are labelled as non-rumour tweets from Pheme1 dataset.\n• Pheme2 T/F: It contains only 1,705 tweets that are marked as true or false\nrumours and taken from Pheme2 dataset. It consists of 1,067 and 638 tweets\nlabelled as true-rumours and false-rumours, respectively.\n• Pheme2 R/NR: It consists of 6,425 tweets from Pheme2 dataset where 2,402\ntweets are categorised as rumour tweets while 4,023 are categorised as non-\nrumour tweets.\n3.2.3 Twitter15 and Twitter16\nTwitter 15 and 16 are publicly available datasets containing 1,490 and\n818 tweets. The journalists labelled each tweet as rumour or non-rumour.\nAfterwards, each rumour tweet is classiﬁed and labelled as a true-rum our,\nfalse-rumour or unveriﬁed rumour [\n10]. Similar to the Pheme2 dataset, this\nstudy did not consider the data with unveriﬁed-rumour labels. Thi s study uses\nfour versions of this datasets, as follows:\n• Twitter15 R/NR: It contains 1,490 tweets from the Twitter15 dataset where\n1,118 are labelled as a rumour while 372 are labelled as non-rumour.\n• Twitter16 R/NR: It consists of 818 tweets from the Twitter16 dataset\nmarked as a rumour (613 tweets) or non-rumour (205 tweets).\n• Twitter15 T/F: It contains tweets labelled as true-rumour (375 tweets ) or\nfalse-rumour (370 tweets) from the Twitter15 dataset.\nSpringer Nature 2021 L ATEX template\n10 Article Title\n• Twitter16 T/F: It comprises 410 tweets from Twitter16 dataset, which ar e\nmarked as true-rumour ( 205 tweets) or false-rumour (205 tweets).\n3.2.4 Combined Dataset\nIn order to generalise the performance of the proposed method, we combi ned\nthe dataset as suggested by [\n58]. In this study, we approached the problem\ndiﬀerently than [ 57] where diﬀerent datasets are used for training and testing.\nThis study generalised combined short-text datasets with the same l abel from\nPheme2, Twitter15, and Twitter16 datasets to obtain a generalised perfor -\nmance result in classifying rumour and non-rumour tweets and, disti nguishing\ntrue and false rumour tweets.\nA single combined dataset with rumour and non-rumour label named\nCombined R/NR was created from Pheme2 R/NR, Twitter15 R/NR, and\nTwitter16 R/NR to validate the model’s performance in classifying ru mour\nand non-rumour tweets. Hence, Combined R/NR dataset comprises 4,600 non-\nrumour tweets and 4,133 rumour tweets. Similarly, Pheme2 T/F, Twitte r15\nT/F, and Twitter16 T/F datasets were incorporated into a single dataset\nnamed Combined T/F dataset. The dataset consists of 1,213 false-rumour\ntweets and 1,646 true-rumour tweets. Table\n1 describes the more detailed\ndistribution of the datasets used in this study.\nTable 1 Distribution of labels in the datasets used in this study\nDatasets\nand\nlabels\nCOVID-\n19 Fake\nNews\nPheme1 Pheme2 Twitter\n15\nTwitter\n16\nCombined\nR/NR\nCombined\nT/F\n[44] [3] [19] [10] [10]\nFalse-\nnews\n1,058 - - - - - -\nTrue-\nnews\n2,061 - - - - - -\nNon-\nrumours\n- 3,822 4,023 372 205 4,600 -\nRumour - 1,969 2,402 1,118 613 4,133 -\nFalse-rumours 638 370 205 - 1,213\nTrue-rumours 1,067 374 205 - 1,646\nUnveriﬁed (not used ) 697 374 203 - -\n3.3 Experimental Procedure\nThe experiments used a RTX 2080 GPU to train each model. The experim ental\nsetup for identifying rumours/non-rumours tweets, and true/fake-n ews using\nLMs and neural network models is shown in Figure\n2.\nAs a ﬁrst step, dataset was split into a training, validation, and testin g\nsets. From each dataset, 10% of the data is reserved ﬁrst for testing. The\nremaining data was split into 75% and 25% for training and validation sets,\nrespectively. For the combined datasets, 10% of the data was taken from eac h\nSpringer Nature 2021 L ATEX template\nArticle Title 11\nFig. 2 The experiment setup to identify rumours/non-rumour tweet s and true/fake news\nusing the proposed model.\ndataset for the testing-set before blending them into a single Combi ned T/F\ndataset or Combined R/NR dataset for training and validation. The rest of\ncombined dataset was split into 3:1 proportions for the training and valid ation\nsets, respectively.\nThe next step was the ﬁnetuning of the LM using Huggingface library [\n61]\nand the labelled data as an input. Adam optimiser was used while the learn ing\nrate was set at 5e-5 with a batch size of eight and experiments were run f or\nten epochs. The mean pooling layer was set as a pooler to encode the d atasets\ninto vectors. Primarily, these vectors were used to train the clas siﬁer model.\nIn the classiﬁcation step, the learning rate was set at 2e-4, batch size at\n512, and the number of maximum epochs selected was 1,000. The Models were\ntrained using Adam optimiser, and cross-entropy as the loss functions. At the\nend, the models were evaluated by calculating their accuracy, prec ision, recall,\nand F1-score using equations(1) to (4):\nSpringer Nature 2021 L ATEX template\n12 Article Title\nAccuracy(A) = T P+ T N\nT P+ T N+ F P+ F N (1)\nP recision(P ) = T P\nT P+ F P (2)\nRecall(R) = T P\nT P+ F N (3)\nF 1 = 2(P )(R)\nP + R (4)\nwhere TP represents True-positive cases, FN represents False- negative\ncases, FP represents False-positive cases, and TN represents Tru e-negative\ncases.\n4 Results and discussion\nThis section presents the experimental results of the proposed mo dels and\ncompares them with the results of state-of-the-art models for each dat aset. The\nworks which are considered as the state-of-the-art models on each datase t and\ntheir performance are presented in Table\n2. The comparative performance of\nall compared models are presented in Table 3 to Table 10. Each table’s ﬁrs t and\nsecond rows provide details of the state-of-the-art models and thei r splitting\ndata strategy that achieved the state-of-the-art performance. The bes t result\nfrom each column is in bold, and the second-best and the third-best re sults\nare represented in underlined and italic font, respectively.\n4.1 Comparison of Results with State-of-the-art Methods\nTable\n3, Table 4, and Table 5 present the experimental results of the pro-\nposed methods on classifying rumour or non-rumour on diﬀerent datasets .\nTable 3 presents the experimental results and their comparison in distin guish-\ning rumour and non-rumour tweets on the Pheme1 R/NR dataset. It can be\nobserved that all the proposed models using ﬁnetuned RoBERTa LM out-\nperformed the state-of-the-art technique of [\n28]. The best scores achieved are\n88.9% accuracy, 87.9% precision, 87.7% recall and 87.8% F1-score. Although\nthe proposed models attained slightly lower F1-score than [\n58] while other\nmetrics were not available for [ 58].\nTable 4 illustrates the experimental results of the proposed models on\nPheme2 R/NR dataset that distinguishes rumour and non-rumour tweets. T he\nSpringer Nature 2021 L ATEX template\nArticle Title 13\nTable 2 State-of-the-art results for rumour and fake news detectio n for each dataset.\nDatasets Sources Splitting data\nstrategy\nBest performance\nA (%) P (%) R (%) F1 (%)\nPheme1 R/NR\n[28] 70:10:20 for\ntrain:val:test\n87.56 87.62 87.65 87.64\n[58] 70:10:20 for\ntrain:val:test\n- - - 89.4+\n0.3\nPheme2 R/NR [7] Not provided 83.36 83.59 99.64 90.91\nPheme2 T/F\n[51] 70:10:20 for\ntrain:val:test\n82.46 79.08 86.24 82.5\n[58] 70:10:20 for\ntrain:val:test\n- - - 93.2+0.9\nTwitter15 T/F [27] 70:30 for\ntrain:test\n87.67 82.57 82.95 82.50\n[58] 70:10:20 for\ntrain:val:test\n- - - 94.4+0.8\nTwitter16 T/F [27] 70:30 for\ntrain:test\n90.84 75.94 76.32 75.93\n[58] 70:10:20 for\ntrain:val:test\n- - - 95.7+2.8\nTwitter15\nR/NR\n[62] Not provided 86.3 - - -\n[15] 60:40 for\ntrain:test\n86 - - -\nTwitter16\nR/NR\n[52] 75:25 for\ntrain:test\n88.5 - - -\n[15] 60:40 for\ntrain:test\n87 - - -\nCovid T/F [9] Not provided 75.43 66.22 56.33 60.9\nAbbreviations: A:Accuracy, P: Precision, R: Recall, F1: F1 sco re\nobservations are similar to Pheme1 R/NR results, where RoBERTa perfor med\ngenerally better than other LMs. This can be contributted to the fact t hat\nRoBERTa has higher number of parameters than other LMs. It can also be\nobserved that 4L-MLP with regularisation and dropout, outperformed both\nResNet-CNN classiﬁers. However, the diﬀerence in classiﬁer’s pe rformance\nbetween RoBERTa, BERT, and DistillBERT on the Pheme2 R/NR dataset is\nsmall, and it only varies from 1% to 3%. In summary, all the proposed mod-\nels outperform the state-of-the-art model in accuracy and precision, while are\nsimilar on F1 score.\nTable\n5 compares the proposed models’ performance on Twitter15 R/NR\nand Twitter16 R/NR datasets. The results show that all the proposed mode ls\nperform exceptionally well on Twitter15 R/NR and Twitter16 R/NR datasets.\nThe proposed models using ﬁnetuned RoBERTa and BERT LMs outperform\nthe state-of-the-art models ([\n15, 28, 62]) on Twitter15 R/NR dataset. Inter-\nestingly on the Twitter16 R/NR dataset, RoBERTa LM which has highest\nSpringer Nature 2021 L ATEX template\n14 Article Title\nTable 3 Comparison of the proposed method with state-of-the-art te chniques for Pheme1\nR/NR dataset\nModels Best results on Pheme1 R/NR\n(5,791 tweets; 1,969 R, 3,822 NR)\nSOTA models and their split of dataset A (%) P (%) R (%) F1 (%)\n[28] 70:10:20 for\ntrain:val:test\n87.56 87.62 87.65 87.64\n[58] 70:10:20 for\ntrain:val:test\n- - - 89.4+0.3\nProposed Models [10%:testing, 75% and 25%\nof rest of the data for training and validation respectively]\nClassiﬁers Language\nmodels\nA (%) P (%) R (%) F1 (%)\n4MLP\nBERT 87.304 86.416 85.448 85.929\nRoBERTa 88.215 86.862 87.643 87.251\nDistilBERT 87.130 86.258 85.201 85.726\n4L-MLP +\nReg+Drop\nBERT 87.040 86.017 84.684 85.345\nRoBERTa 88.908 87.900 87.728 87.814\nDistilBERT 87.130 86.184 85.314 85.747\n10L- ResNet-\nCNN\nBERT 87.840 86.612 86.093 86.352\nRoBERTa 88.000 87.215 85.640 86.420\nDistilBERT 87.130 86.337 85.087 85.707\n18-L ResNet-\nCNN\nBERT 87.040 86.017 84.684 85.345\nRoBERTa 87.868 86.755 86.588 86.671\nDistilBERT 87.652 86.808 85.830 86.316\nAbbreviations:\nA:Accuracy, P: Precision, R: Recall, F1: F1 score\nSOTA: State-of-the-art\nnumber of trainable parameters obtained the worst accuracy, while Dist il-\nBERT LM which has smallest number of trainable parameters achieved the\nbest accuracy. It is expected that size and imbalance of the dataset may have\naﬀected the performance of LMs.\nFor classifying true and false rumours, Tables\n6 and 7 presents the perfor-\nmance of the proposed methods on classifying true and false rumours usi ng\nPheme2 T/F, Twitter15 T/F, and Twitter16 T/F datasets. It can be observe d\nfrom the tables that these datasets are relatively small in size but are well bal-\nanced. Table 6 presents the experimental results of the proposed models on\nPheme2 T/F dataset for identifying true and false rumours in tweets. All the\nproposed models outperform the state-of-the-art model [ 51] in all metrics while\nsimilar to [ 58] in terms of F1 score. This shows that all the proposed models\nconsistently achieve a high performance on Pheme2 T/F dataset. Intere stingly,\nby using 4-layers MLP and ten-layers ResNet-CNN classiﬁer models, BERT\nand DistilBERT LMs obtained better result as compared to RoBERTa LM\nwhich has more trainable parameters. Table\n7 compares the proposed mod-\nels’ performance on classifying true and false-rumour on Twitter15 T/F and\nTwitter16 T/F datasets. It can clearly observed that all the proposed mod els\nSpringer Nature 2021 L ATEX template\nArticle Title 15\nTable 4 Comparison of the proposed method with state-of-the-art te chniques for Pheme2\nR/NR dataset\nClassiﬁer models Best results on Pheme2 R/NR\n(6425 tweets; 2402 R, 4023 NR)\nSOTA models and their split of dataset A (%) P (%) R (%) F1 (%)\n[63] Not\nprovided\n83.36 83.59 99.64 90.91\nProposed Models [10%:testing, 75% and 25%\nof rest of the data for training and validation respectively]\n4MLP\nBERT 89.404 88.812 88.532 88.672\nRoBERTa 90.066 89.280 89.676 89.478\nDistilBERT 86.342 85.553 85.983 85.767\n4L-MLP +\nReg+Drop\nBERT 88.907 88.156 88.222 88.189\nRoBERTa 90.232 89.518 89.721 89.61\nDistilBERT 86.424 85.819 85.006 85.411\n10L-ResNet-\nCNN\nBERT 89.073 88.403 88.267 88.335\nRoBERTa 89.735 88.895 89.411 89.152\nDistilBERT 87.252 86.478 86.282 86.380\n18-L\nResNet-CNN\nBERT 89.735 89.227 88.797 89.011\nRoBERTa 90.232 89.565 89.634 89.599\nDistilBERT 87.086 86.279 86.150 86.214\nAbbreviations:\nA:Accuracy, P: Precision, R: Recall, F1: F1 score\nSOTA: State-of-the-art\nachieve high performance and outperform the state-of-the-art models ( [\n27, 58])\non both datasets. For Twitter15 T/F dataset, BERT LM with 4-layers MLP\nclassiﬁer performs the best while for Twitter16 T/F dataset, RoBERTa LM\nwith the same classiﬁer obtained the best scores. Therefore, it can b e concluded\nthat MLP as a classiﬁer performs better than all other considered classi ﬁers.\nThe experimental results of the proposed models for classifying tr ue and\nfake news on Covid T/F as a long-text dataset are presented in Table\n8.\nThe results indicate that all the proposed models perform better on C ovid\nT/F dataset as compared to the State-of-the-art methods [ 9]. The models’\nperformance considerably improves the accuracy by around 5% - 7% and F1-\nScore by 16%-19% as compared to the state-of-the-art technique. RoBERTa as\nthe largest LM combined with a simple 4-layers MLP classiﬁer performs bet-\nter than other LMs and classiﬁers. However, when incorporated with a mor e\ncomplex classiﬁer, such as 18L-Resnet-CNN, RoBERTa’s performance slight ly\ndecreased. In general, there is no signiﬁcant diﬀerence between t he performance\nof three LMs.\n4.2 Classiﬁcation on Combined Datasets\nTable\n9 and Table 10 present the result on distinguishing rumour/non-rumour\nand true/false-rumour on combined datasets. As mentioned earlier, this study\ncombined all the same labelled datasets from diﬀerent sources into a n ew single\nSpringer Nature 2021 L ATEX template\n16 Article Title\nTable 5 Comparison of the proposed method with state-of-the-art te chniques for\nTwitter15 R/NR and Twitter16 R/NR datasets\nClassiﬁer models Best results on Twit15 R/NR Best results on Twitter16 R/NR\n(1490 tweets; 1118 R, 372 NR) (818 tweets; 613 R, 205 NR)\nSOTA and split of dataset A (%) P (%) R (%) F1 (%) A (%) P (%) R (%) F1 (%)\n[62] Not\npro-\nvided\n86.3 - - - 88.5 - - -\n[15] 60:40\nfor\ntrain:test\n86 - - - 87 - - -\nProposed Models [10%:testing, 75% and 25%\nof rest of the data for training and validation respectively]\n4MLP\nBERT 87.586 82.711 82.711 82.711 91.358 90.211 85.861 87.982\nRoBERTa 92.466 89.769 88.944 89.355 85.294 83.103 73.077 77.768\nDistilBERT 85.616 81.155 76.287 78.646 91.358 92.121 84.180 87.972\n4MLP +Reg\n+Dropout\nBERT 88.966 87.266 80.551 83.774 90.123 91.205 81.680 86.18\nRoBERTa 92.466 90.505 87.920 89.194 86.420 82.767 79.221 80.955\nDistilBERT 85.616 80.266 78.335 79.289 91.358 92.121 84.180 87.972\n10L- ResNet\n-CNN\nBERT 87.586 85.882 77.610 81.537 90.123 91.205 81.680 86.18\nRoBERTa 93.151 91.118 89.391 90.246 86.420 83.939 77.541 80.613\nDistilBERT 86.986 82.803 79.228 80.976 91.358 92.121 84.180 87.972\n18L -ResNet\n-CNN\nBERT 86.538 82.181 81.624 81.902 90.123 91.205 81.680 86.18\nRoBERTa 92.466 90.505 87.920 89.194 87.654 85.240 80.041 82.559\nDistilBERT 88.356 83.539 84.217 83.877 91.358 92.121 84.180 87.972\nAbbreviations:\nA:Accuracy, P: Precision, R: Recall, F1: F1 score\nSOTA: State-of-the-art\ndataset to attain a larger and more balanced dataset. It is expected that large\ndata will help to generalise and improve the training of the proposed m odels.\nThe testing was done with testing set of each individual dataset whic h was\nseparated before combining the data for training and validation, as well as with\ncombined testing sets taken from all datasets. Therefore, Table\n9 compares the\nmodels’ performance on four diﬀerent datasets, including Twitter 16 R/NR,\nTwitter15 R/NR, Pheme1 R/NR, Pheme2 R/NR and their combination, and\nTable\n10 compares the models’ performance on four diﬀerent datasets includin g\nTwitter16 T/F, Twitter15 T/F, Pheme2 T/F, and their combination.\nIt can be observed from the results presented in Table 9 and Table 10 that\nthe proposed models perform well and obtain high performance as compared\nto the results obtained on individual datasets presented in Tables\n3-8. This is\ndue to the fact that more training and validation data is made available for\nLMs and classiﬁers. There was no signiﬁcant diﬀerence in performance for light\nor large LMs with a simple or more complex classiﬁer. The diﬀerence bet ween\nall the models on combined dataset was around 1% to 2%. This shows that\ncombining data can increase generalisation during the training proce ss and\nhelp stabilise the proposed model’s performance.\nSpringer Nature 2021 L ATEX template\nArticle Title 17\nTable 6 Comparison of the proposed method with state-of-the-art te chniques for Pheme2\nT/F dataset\nClassiﬁer models Best results on Pheme2 T/F\n(1705 tweets; 1067 T, 638 F)\nSOTA techniques and their split of dataset A (%) P (%) R (%) F1 (%)\n[51] 70:10:20 for\ntrain:val:test\n82.46 79.08 86.24 82.5\n[58] 70:10:20 for\ntrain:val:test\nsets\n- - - 93.2\n+\n0.9\nProposed Models [10%:testing, 75% and 25%\nof rest of the data for training and validation respectively ]\n4MLP\nBERT 90.419 90.286 89.179 89.729\nRoBERTa 89.873 89.522 89.015 89.268\nDistilBERT 90.323 92.113 86.866 89.413\n4L-MLP +\nReg+Drop\nBERT 89.820 89.782 88.385 89.078\nRoBERTa 89.241 89.251 87.891 88.566\nDistilBERT 89.820 90.544 87.759 89.13\n10L-ResNet-\nCNN\nBERT 91.398 91.840 89.116 90.457\nRoBERTa 88.623 90.162 85.859 87.958\nDistilBERT 89.241 88.741 88.499 88.62\n18-L\nResNet-CNN\nBERT 90.419 90.614 88.866 89.731\nRoBERTa 89.241 88.741 88.499 88.62\nDistilBERT 87.425 87.140 85.836 86.483\nAbbreviations:\nA:Accuracy, P: Precision, R: Recall, F1: F1 score\nSOTA: State-of-the-art\nTable 7 Comparison of the proposed method with state-of-the-art te chniques for\nTwitter15 T/F and Twitter16 T/F datasets\nClassiﬁer models Best results on Twitter15 T/F Best results on Twitter16 T/F\n(754 tweets; 374 T, 370 F) (410 tweets; 205 T, 205 F)\nState-of-the-art techniques A (%) P (%) R (%) F1 (%) A (%) P (%) R (%) F1 (%)\nand their split of dataset\n[27] 70:30 for\ntrain:test\n87.67 82.57 82.95 82.50 90.84 75.94 76.32 75.93\n[58] 70:10:20 for\ntrain:val:test\n- - - 94.4+0.8 - - - 95.7+2.8\nProposed Models [10%:testing, 75% and 25%\nof rest of the data for training and validation respectively]\n4MLP\nBERT 97.015 97.059 97.143 97.101 92.308 93.182 92.500 92.840\nRoBERTa 97.015 97.009 97.009 97.009 97.436 97.500 97.500 97.500\nDistilBERT 95.522 95.499 95.580 95.539 88.889 88.889 88.259 88.573\n4L-MLP +\nReg+Drop\nBERT 95.000 95.455 95.000 95.227 92.308 93.182 92.500 92.840\nRoBERTa 97.015 97.009 97.009 97.009 97.436 97.500 97.500 97.500\nDistilBERT 94.030 94.018 94.018 94.018 88.889 88.889 88.259 88.573\n10L-ResNet-\nCNN\nBERT 95.522 95.714 95.714 95.714 94.872 95.455 94.737 95.095\nRoBERTa 97.015 97.009 97.009 97.009 97.436 97.500 97.500 97.500\nDistilBERT 94.030 94.018 94.018 94.018 97.436 97.619 97.368 97.493\n18-L\nResNet-CNN\nBERT 92.537 93.243 92.857 93.05 94.872 94.868 94.868 94.868\nRoBERTa 95.522 95.499 95.580 95.539 97.436 97.500 97.500 97.500\nDistilBERT 97.015 97.059 97.143 97.101 88.889 88.889 88.259 88.573\nAbbreviations: A:Accuracy, P: Precision, R: Recall, F1: F1 sco re\nSpringer Nature 2021 L ATEX template\n18 Article Title\nTable 8 Comparison of the proposed method with state-of-the-art te chniques for Covid\nT/F dataset\nClassiﬁer models Best results on Covid T/F\n(3119 News; 2061 T, 1058 F)\nState-of-the-art techniques ([ 9]) A (%) P (%) R (%) F1 (%)\nSplitting data strategies are not provided\nk-NN-BSSA 72.610 59.620 59.740 59.68\nk-NN-BPSO 73.120 61.980 53.780 57.59\nk-NN-BGA 75.430 66.220 56.330 60.88\nk-NN 70.650 56.420 59.360 57.85\nJ48 72.290 59.600 56.900 0.58.22\nRF 70.410 63.420 30.150 40.87\nSVM 70.750 56.650 58.790 57.7\nProposed Models [10%:testing, 75% and 25%\nof rest of the data for training and validation respectively]\n4MLP\nBERT 80.192 78.640 77.024 77.824\nRoBERTa 82.026 79.961 79.518 79.739\nDistilBERT 80.511 79.612 76.438 77.993\n4L-MLP +\nReg+Drop\nBERT 81.046 79.223 77.345 78.273\nRoBERTa 82.353 80.549 79.286 79.913\nDistilBERT 80.511 79.788 76.229 77.968\n10L-ResNet-\nCNN\nBERT 81.046 79.223 77.345 78.273\nRoBERTa 81.046 78.730 79.258 78.993\nDistilBERT 80.511 79.982 76.021 77.951\n18-L\nResNet-CNN\nBERT 81.699 79.474 79.750 79.612\nRoBERTa 81.046 78.722 79.497 79.108\nDistilBERT 80.192 79.679 75.567 77.569\nAbbreviations: A:Accuracy, P: Precision, R: Recall, F1: F1 sco re\nTable 9 Comparison of the proposed method with State-of-the-art te chniques for\nTwitter16 R/NR, Twitter15 R/NR, Pheme1 R/NR, Pheme2 R/NR an d Combined R/NR\ndatasets\nClassiﬁer models Twitter16 Twitter15 Pheme1 Pheme2 Combined\nR/NR R/NR R/NR R/NR R/NR\n(613/205) (1,118/372) (1,969/3,822) (2,402/4,023) (4,133/4,600)\nApproaches LM A% F1 (%) A (%) (F1%) A (%) F1 (%) A (%) F1 (%) A (%) F1 (%)\n4MLP\nBERT 91.358 87.982 87.586 82.711 88.215 87.251 90.066 89.478 88.449 88.479\nRoBERTa 85.294 77.768 92.466 89.355 87.304 85.929 89.404 88.672 90.099 90.092\nDistilBERT 91.358 87.972 85.616 78.646 87.130 85.726 86.342 85.726 88.999 88.991\n4L-MLP+\nReg+Drop\nBERT 90.123 86.18 88.966 83.774 88.908 87.814 90.232 89.613 88.779 88.831\nRoBERTa 86.420 80.955 92.466 89.194 87.040 85.345 88.907 88.189 90.099 90.091\nDistilBERT 91.358 87.972 85.616 79.289 87.130 85.747 86.424 85.411 89.109 89.101\n10L-ResNet-\nCNN\nBERT 90.123 86.180 87.586 81.537 88.000 86.420 89.735 89.152 88.889 88.945\nRoBERTa 86.420 80.613 93.151 90.246 87.840 86.352 89.073 88.335 90.099 90.107\nDistilBERT 91.358 87.972 86.986 80.976 87.130 85.707 87.252 86.380 89.219 89.210\n18-L\nResNet-CNN\nBERT 90.123 86.180 86.538 81.902 87.868 86.671 90.232 89.599 88.559 88.593\nRoBERTa 87.654 82.559 92.466 89.194 87.040 85.345 89.735 89.011 90.096 90.090\nDistilBERT 91.358 87.972 88.356 83.877 87.652 86.316 87.086 86.214 88.991 88.991\nAbbreviations: A:Accuracy, F1: F1 score\n5 Conclusion\nThis study demonstrates that incorporating a BERT-based ﬁnetuned l anguage\nmodel as an encoder and a neural network as a classiﬁer can improve the ac cu-\nracy of misinformation detection. The proposed models consistently achieved\nSpringer Nature 2021 L ATEX template\nArticle Title 19\nTable 10 Comparison of the proposed method with state-of-the-art te chniques for\nTwitter16 T/F, Twitter15 T/F, Pheme T/F and Combined T/F dat asets\nClassiﬁer models Twitter16 Twitter15 Pheme2 Combined\nT/F 15 T/F T/F T/F\n(410 tweets) (754 tweets) (1705 tweets) (2869 tweets)\nApproaches LMs A% F1 (%) A (%) F1 (%) A (%) F1 (%) A (%) F1 (%)\n4MLP\nBERT 92.308 92.840 97.015 97.101 90.419 89.729 93.624 93.567\nRoBERTa 97.436 97.500 97.015 97.009 89.873 89.268 92.617 92.459\nDistilBERT 88.889 88.573 95.522 95.539 90.323 89.413 92.883 92.814\n4L-MLP+\nReg+Drop\nBERT 92.308 92.840 95.000 95.227 89.820 89.078 93.624 93.567\nRoBERTa 97.436 97.500 97.015 97.009 89.241 88.566 92.282 92.124\nDistilBERT 88.889 88.573 94.030 94.018 89.820 89.130 92.953 92.778\n10L-ResNet-\nCNN\nBERT 94.872 95.095 95.522 95.714 91.398 90.457 92.617 92.421\nRoBERTa 97.436 97.500 97.015 97.009 88.623 87.958 92.282 92.100\nDistilBERT 97.436 97.493 94.030 94.018 89.241 88.620 92.953 92.778\n18-L\nResNet-CNN\nBERT 94.872 94.868 92.537 93.050 90.419 89.731 92.953 92.930\nRoBERTa 97.436 97.500 95.522 95.539 89.241 88.620 92.617 92.459\nDistilBERT 88.889 88.573 97.015 97.101 87.425 86.483 92.282 92.100\nAbbreviations: A:Accuracy, F1: F1 score\nhigh-performance on both short and long-text datasets and outperformed the\ncurrent state-of-the-art models.\nThe experimental results show that the performance of proposed mode ls\nimproved signiﬁcantly when trained and tested on larger amount of data wh ich\nwas achieved by combining all the considered datasets. It was also obser ved\nthat there was no signiﬁcant diﬀerence between diﬀerent LMs, whet her large\nLM or a light LM. The performance diﬀer by 1-2% only between diﬀerent\nLMs. Hence, it can be concluded that combining datasets helps to evaluat e\nthe generalised performance of the proposed model.\nThe results also indicate that a sophisticated model does not guarante e a\nbetter outcome. Many studies implied that using models with large n umber of\ntrainable parameters improve performance. However, a simple 4-laye rs MLP\nclassiﬁer with simple LM can perform better than combination of complex\nclassiﬁers (ResNet-CNN) and LMs (BERT or DistillBERT). The diﬀeren ce of\nresults obtained from RoBERTa, BERT and DistilBERT with diﬀerent c las-\nsiﬁers were insigniﬁcant: only 1% to 3% diﬀerence. Therefore, it is suggested\nthat in real-world scenarios training time, cost and computational comple xity\nof the models should be considered carefully rather than small impro vements\nin resuls. Future studies should examine the interaction betwee n datasets,\nalgorithm models, and splitting of data to understand the solution in m ore\ncomprehensive manner.\nStatements and Declarations\n• Ethics approval and Consent to participate\nNot applicable\n• Human and Animal Ethics\nNot applicable\n• Consent for publication\nNot applicable\n• Availability of data and materials\nSpringer Nature 2021 L ATEX template\n20 Article Title\nNot applicable\n• Competing interests We conﬁdent to conﬁrm that we do not have any\nconﬂicts of interest associated with this publication.\n• Funding\nThis study is funded by Sebelas Maret University Indonesia.\n• Authors’ contributions\nAll of the authors contributed equally to this work. The ﬁrst draft of th e\narticle was written by Rini Anggrainingsih, All authors read, commented al l\nthe versions of the manuscript, and approved the ﬁnal manuscript.\n• Acknowledgements\nThe authors express gratitude to the Sebelas Maret University for exc ep-\ntional support as a sponsor of this research.\n• Authors information\nRini Anggrainingsih , received her bachelor degree and masters from\nDiponegoro University and Gadjahmada University, respectively. She is cur-\nrently working as academic staﬀ at Sebelas Maret University while purs uing\na PhD at The University of Western Australia. Her current research int erest\nare false information detection on the online platforms, and other natural\nlanguage processing tasks to improve info-surveillance.\nDr. Ghulam Mubashar Hassan, received his B.Sc. Electrical and Elec-\ntronics Engineering degree (with honors) from University of Engineer ing and\nTechnology (UET) Peshawar, Pakistan. He completed his MS in Electrical\nand Computer Engineering from Oklahoma State University USA and his\nPhD from The University of Western Australia (UWA) in Joint Schools of\nComputer Science & Software Engineering and Civil & Resource Engine er-\ning. He was valedictorian and received many awards for his PhD. Currentl y,\nhe is working in UWA and previously he worked in UET Peshawar and King\nSaud University. His research interests are multidisciplinary pr oblems which\ninclude using artiﬁcial intelligence, machine learning, pattern r ecognition,\noptimization in diﬀerent ﬁelds of engineering and education.\nAmitava Datta (A’04–M’10), received the M.Tech. and Ph.D. degrees\nfrom IIT Madras in 1988 and 1992, respectively. He did the post-doctoral\nresearch at the Max Planck Institut f¨ ur Informatik, Germany, and the Uni-\nversity of Freiburg, Germany. He joined the University of New England\nin 1995 and The University of Western Australia in 1998, where he is\ncurrently a Professor with the School of Computer Science and Softwar e\nEngineering. His current research interests are in optical computi ng, data\nmining, bioinformatics, and social network analysis. He has authored over\n150 papers in various international journals and conference proceedings,\nincluding the IEEE Transactions on Computers, the IEEE Transactions on\nParallel and Distributed Systems, the IEEE Transactions on Visualizat ion\nand Computer Graphics, the IEEE Transactions on Mobile Computing, the\nIEEE/ACM Transactions on Networking, the IEEE Transactions on Com-\nputational Social Systems and the IEEE Transactions on Systems, Man, and\nCybernetics.\nSpringer Nature 2021 L ATEX template\nArticle Title 21\nReferences\n1. Castillo, C., Mendoza, M., Poblete, B.: Information credibility on T wit-\nter. Proceedings of the 20th International Conference Companion on\nWorld Wide Web, WWW 2011, 675–684 (2011).\nhttps://doi.org/10.1145/\n1963405.1963500\n2. Ito, J., Song, J., Toda, H., Koike, Y., Oyama, S.: Assessment of tweet\ncredibility with lda features. In: Proceedings of the 24th Intern ational\nConference on World Wide Web, pp. 953–958 (2015)\n3. Zubiaga, A., Liakata, M., Procter, R., Wong Sak Hoi, G., Tolmie,\nP.: Analysing how people orient to and spread rumours in social\nmedia by looking at conversational threads. PLoS ONE 11(3) (2016)\narXiv:1511.07487. https://doi.org/10.1371/journal.pone.0150989\n4. Hassan, N.Y., Haggag, M.H.: Supervised Learning Approach for Twitter\nCredibility Detection. 2018 13th International Conference on Computer\nEngineering and Systems (ICCES), 196–201 (2018)\n5. Herzallah, W., Faris, H., Adwan, O.: Feature engineering for detectin g\nspammers on twitter: Modelling and analysis. Journal of Information\nScience 44(2), 230–247 (2018)\n6. Ahmed, H., Traore, I., Saad, S.: Detecting opinion spams and fake news\nusing text classiﬁcation. Security and Privacy 1(1), 9 (2018)\n7. Dong, X., Victor, U., Chowdhury, S., Qian, L.: Deep two-path\nsemi-supervised learning for fake news detection. arXiv preprint\narXiv:1906.05659 (2019)\n8. Reis, J.C., Correia, A., Murai, F., Veloso, A., Benevenuto, F.: Supe rvised\nlearning for fake news detection. IEEE Intelligent Systems 34(2), 76–81\n(2019)\n9. Al-Ahmad, B., Al-Zoubi, A., Abu Khurma, R., Aljarah, I.: An evolutionary\nfake news detection method for covid-19 pandemic information. Symme try\n13(6), 1091 (2021)\n10. Ma, J., Gao, W., Mitra, P., Kwon, S., Jansen, B.J., Wong, K.-f., Cha,\nM.: Detecting Rumors from Microblogs with Recurrent Neural Net-\nworks. Proceedings of the 25th International Joint Conference on Artiﬁci al\nIntelligence (IJCAI 2016), 3818–3824 (2015)\n11. Ruchansky, N.: CSI : A Hybrid Deep Model for Fake News Detection. Pro-\nceedings of the 2017 ACM on Conference on Information and Knowledge\nManagement, 797–806 (2017)\nSpringer Nature 2021 L ATEX template\n22 Article Title\n12. Alkhodair, S.A., Ding, S.H.H., Fung, B.C.M.: Detecting breaking ne ws\nrumors of emerging topics in social media. Information Processing and\nManagement 57(2), 102018 (2020).\nhttps://doi.org/10.1016/j.ipm.2019.\n02.016\n13. Yu, F., Liu, Q., Wu, S., Wang, L., Tan, T.: A Convolutional Approach\nfor Misinformation Identiﬁcation. IJCAI International Joint Conferenc e\non Artiﬁcial Intelligence 0, 3901–3907 (2017). https://doi.org/10.24963/\nijcai.2017/545\n14. Xu, F., Sheng, V.S., Wang, M.: Knowledge-Based Systems Near real-time\ntopic-driven rumor detection in source microblogs. Knowledge-Based Sys-\ntems 207, 106391 (2020). https://doi.org/10.1016/j.knosys.2020.106391\n15. Santhoshkumar, S., Babu, L.D.: Earlier detection of rumors in online\nsocial networks using certainty-factor-based convolutional neural ne t-\nworks. Social Network Analysis and Mining 10(1), 1–17 (2020)\n16. Bharti, M., Jindal, H.: Automatic Rumour Detection Model on Social\nMedia. 2020 Sixth International Conference on Parallel, Distributed\nand Grid Computing (PDGC), 367–371 (2021).\nhttps://doi.org/10.1109/\npdgc50313.2020.9315738\n17. Wang, W.Y.: ” liar, liar pants on ﬁre”: A new benchmark dataset for fake\nnews detection. arXiv preprint arXiv:1705.00648 (2017)\n18. Ajao, O.: Fake News Identiﬁcation on Twitter with Hybrid CNN and RNN\nModels. Proceedings of the 9th international conference on social med ia\nand society, 226–230 (2018)\n19. Kochkina, E., Liakata, M., Zubiaga, A.: All-in-one: Multi-task learning for\nrumour veriﬁcation. arXiv preprint arXiv:1806.03713 (2018)\n20. Karimi, H., Roy, P., Saba-Sadiya, S., Tang, J.: Multi-source multi-c lass\nfake news detection. In: Proceedings of the 27th International Confere nce\non Computational Linguistics, pp. 1546–1557 (2018)\n21. Das, S.D., Basak, A., Dutta, S.: A heuristic-driven uncertainty b ased\nensemble framework for fake news detection in tweets and news articl es.\nNeurocomputing (2021)\n22. Zhang, P., Ran, H., Jia, C., Li, X., Han, X.: A lightweight propagation\npath aggregating network with neural topic model for rumor detection.\nNeurocomputing 458, 468–477 (2021)\n23. Zhou, K., Li, B.: Early Rumour Detection. Proceedings of the 2019 Confer -\nence of the North American Chapter of the Association for Computational\nSpringer Nature 2021 L ATEX template\nArticle Title 23\nLinguistics: Human Language Technologies, Volume 1, 1614–1623 (2019)\n24. Ma, T., Zhou, H., Tian, Y., Al-Nabhan, N.: A novel rumor detection algo-\nrithm based on entity recognition, sentence reconﬁguration, and ordinar y\ndiﬀerential equation network. Neurocomputing 447, 224–234 (2021)\n25. Yuan, C., Ma, Q., Zhou, W., Han, J., Hu, S.: Jointly embedding the loc al\nand global relations of heterogeneous graph for rumor detection. In: 2019\nIEEE International Conference on Data Mining (ICDM), pp. 796–805\n(2019). IEEE\n26. Wu, Z., Pi, D., Chen, J., Xie, M., Cao, J.: Rumor detection based on pr op-\nagation graph neural network with attention mechanism. Expert systems\nwith applications 158, 113595 (2020)\n27. Lu, Y.-J., Li, C.-T.: Gcan: Graph-aware co-attention networks for expl ain-\nable fake news detection on social media. arXiv preprint arXiv:2004.11648\n(2020)\n28. Wang, Y., Qian, S., Hu, J., Fang, Q., Xu, C.: Fake news detection via\nknowledge-driven multimodal graph convolutional networks. In: Proc eed-\nings of the 2020 International Conference on Multimedia Retrieval, pp.\n540–547 (2020)\n29. Koloski, B., Perdih, T.S., Robnik- ˇSikonja, M., Pollak, S., ˇSkrlj, B.:\nKnowledge graph informed fake news classiﬁcation via heterogeneous\nrepresentation ensembles. Neurocomputing (2022)\n30. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-traini ng of\ndeep bidirectional transformers for language understanding. arXiv prepr int\narXiv:1810.04805 (2018)\n31. Moradi, M., Dorﬀner, G., Samwald, M.: Deep contextualized embeddi ngs\nfor quantifying the informative content in biomedical text summariz ation.\nComputer methods and programs in biomedicine 184, 105117 (2020)\n32. Ma, T., Pan, Q., Rong, H., Qian, Y., Tian, Y., Al-Nabhan, N.: T-bertsum:\nTopic-aware text summarization based on bert. IEEE Transactions on\nComputational Social Systems (2021)\n33. Liu, Y., Lapata, M.: Text summarization with pretrained encoders. arXiv\npreprint arXiv:1908.08345 (2019)\n34. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: Bertsc ore:\nEvaluating text generation with bert. arXiv preprint arXiv:1904.09675\n(2019)\nSpringer Nature 2021 L ATEX template\n24 Article Title\n35. Chen, Y.-C., Gan, Z., Cheng, Y., Liu, J., Liu, J.: Distilling knowl edge\nlearned in bert for text generation. arXiv preprint arXiv:1911.03829 (2019)\n36. Qu, Y., Liu, P., Song, W., Liu, L., Cheng, M.: A text generation and\nprediction system: Pre-training on new corpora using bert and gpt-2. In:\n2020 IEEE 10th International Conference on Electronics Information and\nEmergency Communication (ICEIEC), pp. 323–326 (2020). IEEE\n37. Li, W., Gao, S., Zhou, H., Huang, Z., Zhang, K., Li, W.: The auto-\nmatic text classiﬁcation method based on bert and feature union. In: 2019\nIEEE 25th International Conference on Parallel and Distributed System s\n(ICPADS), pp. 774–777 (2019). IEEE\n38. Sun, C., Qiu, X., Xu, Y., Huang, X.: How to ﬁne-tune bert for text\nclassiﬁcation? In: China National Conference on Chinese Computational\nLinguistics, pp. 194–206 (2019). Springer\n39. Gonz´ alez-Carvajal, S., Garrido-Merch´ an, E.C.: Comparing bert\nagainst traditional machine learning text classiﬁcation. arXiv preprint\narXiv:2005.13012 (2020)\n40. Rietzler, A., Stabinger, S., Opitz, P., Engl, S.: Adapt or get left be hind:\nDomain adaptation through bert language model ﬁnetuning for aspect-\ntarget sentiment classiﬁcation. arXiv preprint arXiv:1908.11860 (2019)\n41. Yu, J., Jiang, J.: Adapting bert for target-oriented multimodal sent iment\nclassiﬁcation. In: IJCAI,2019, pp. 323–326 (2019). IJCAI\n42. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lew is,\nM., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized be rt\npretraining approach. arXiv preprint arXiv:1907.11692 (2019)\n43. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distill ed\nversion of bert: smaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108 (2019)\n44. Koirala, A.: Covid-19 fake news dataset. In: Mendeley Data, p. 1 (2021).\nMendeley. 10.17632/zwfdmp5syg.1\n45. Bondielli, A., Marcelloni, F.: A survey on fake news and rumour detec tion\ntechniques. Information Sciences 497, 38–55 (2019).\nhttps://doi.org/10.\n1016/j.ins.2019.05.035\n46. Meel, P., Vishwakarma, D.K.: Fake news, rumor, information pollution i n\nsocial media and web: A contemporary survey of state-of-the-arts, chal -\nlenges and opportunities. Expert Systems with Applications 153, 112986\n(2020)\nSpringer Nature 2021 L ATEX template\nArticle Title 25\n47. Pamungkas, E.W., Basile, V., Patti, V.: Stance classiﬁcation for rumour\nanalysis in twitter: Exploiting aﬀective information and conversation\nstructure. arXiv preprint arXiv:1901.01911 (2019)\n48. Ghenai, A., Mejova, Y.: Catching Zika Fever: Application of Crowdsour c-\ning and Machine Learning for Tracking Health Misinformation on Twitter.\nProceedings - 2017 IEEE International Conference on Healthcare Infor-\nmatics, ICHI 2017, 518 (2017)\narXiv:1707.03778. https://doi.org/10.1109/\nICHI.2017.58\n49. Chatterjee, S., Deng, S., Liu, J., Shan, R., Jiao, W.: Classifying f acts and\nopinions in twitter messages: a deep learning-based approach. Journal of\nBusiness Analytics 1(1), 29–39 (2018)\n50. Wu, L., Rao, Y.: Adaptive interaction fusion networks for fake news\ndetection. arXiv preprint arXiv:2004.10009 (2020)\n51. Wu, L., Rao, Y., Zhao, Y., Liang, H., Nazir, A.: Dtca: Decision tree-based\nco-attention networks for explainable claim veriﬁcation. arXiv preprin t\narXiv:2004.13455 (2020)\n52. Wang, Z., Guo, Y.: Rumor events detection enhanced by encoding sen-\ntimental information into time series division and word representat ions.\nNeurocomputing 397, 224–243 (2020)\n53. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.:\nGlue: A multi-task benchmark and analysis platform for natural language\nunderstanding. arXiv preprint arXiv:1804.07461 (2018)\n54. Anggrainingsih, R., Hassan, G.M., Datta, A.: Bert based classiﬁcation\nsystem for detecting rumours on twitter. arXiv preprint arXiv:2109.02975\n(2021)\n55. Kaliyar, R.K., Goswami, A., Narang, P.: Fakebert: Fake news detection in\nsocial media with a bert-based deep learning approach. Multimedia tools\nand applications 80(8), 11765–11788 (2021)\n56. Gupta, P., Gandhi, S., Chakravarthi, B.R.: Leveraging transfer lear ning\ntechniques-bert, roberta, albert and distilbert for fake review d etection.\nIn: Forum for Information Retrieval Evaluation, pp. 75–82 (2021)\n57. Huang, Y.-H., Liu, T.-W., Lee, S.-R., Calderon Alvarado, F.H., Chen,\nY.-S.: Conquering cross-source failure for news credibility: Learn ing gen-\neralizable representations beyond content embedding. In: Proceed ings of\nThe Web Conference 2020, pp. 774–784 (2020)\n58. Pelrine, K., Danovitch, J., Rabbany, R.: The surprising perfor mance of\nSpringer Nature 2021 L ATEX template\n26 Article Title\nsimple baselines for misinformation detection. In: Proceedings of t he Web\nConference 2021, pp. 3432–3441 (2021)\n59. Taud, H., Mas, J.F.: Multilayer Perceptron ( MLP ) Neural networks.\nGeomatic Approaches for Modeling Land Change Scenarios, 451–455\n(2018)\n60. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image\nrecognition. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 770–778 (2016)\n61. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A.,\nCistac, P., Rault, T., Louf, R., Funtowicz, M., et al.: Huggingface’s\ntransformers: State-of-the-art natural language processing. arXiv prepr int\narXiv:1910.03771 (2019)\n62. Ma, J., Gao, W., Wong, K.-F.: Detect rumors on twitter by promoting\ninformation campaigns with generative adversarial learning. In: The World\nWide Web Conference, pp. 3049–3055 (2019)\n63. Dong, X., Victor, U., Chowdhury, S., Qian, L., View, P.: Deep two-\npath semi-supervised learning for fake news detection. arXiv prepr int\narXiv:1906.05659 (2019)\narXiv:arXiv:1906.05659v1",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8450652956962585
    },
    {
      "name": "Misinformation",
      "score": 0.7973981499671936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5513743758201599
    },
    {
      "name": "Process (computing)",
      "score": 0.5320307612419128
    },
    {
      "name": "Machine learning",
      "score": 0.5098799467086792
    },
    {
      "name": "ENCODE",
      "score": 0.5079753994941711
    },
    {
      "name": "Language model",
      "score": 0.5018002986907959
    },
    {
      "name": "Artificial neural network",
      "score": 0.4877408444881439
    },
    {
      "name": "Data mining",
      "score": 0.377910315990448
    },
    {
      "name": "Natural language processing",
      "score": 0.35152363777160645
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I119896790",
      "name": "Sebelas Maret University",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I177877127",
      "name": "The University of Western Australia",
      "country": "AU"
    }
  ],
  "cited_by": 7
}