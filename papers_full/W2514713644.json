{
  "title": "Using the Output Embedding to Improve Language Models",
  "url": "https://openalex.org/W2514713644",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A4227826633",
      "name": "Press, Ofir",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2501682014",
      "name": "Wolf, Lior",
      "affiliations": [
        "Tel Aviv University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2112184938",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W1943583106",
    "https://openalex.org/W2183112036",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2176085882",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W2118776487",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2417736714",
    "https://openalex.org/W2131571251",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2260194779",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2142625445",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2533523411",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W1889624880",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2418388682",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2292877769",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.",
  "full_text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 157–163,\nValencia, Spain, April 3-7, 2017.c⃝2017 Association for Computational Linguistics\nUsing the Output Embedding to Improve Language Models\nOﬁr Press and Lior Wolf\nSchool of Computer Science\nTel-Aviv University, Israel\n{ofir.press,wolf}@cs.tau.ac.il\nAbstract\nWe study the topmost weight matrix of\nneural network language models. We\nshow that this matrix constitutes a valid\nword embedding. When training language\nmodels, we recommend tying the input\nembedding and this output embedding.\nWe analyze the resulting update rules and\nshow that the tied embedding evolves in\na more similar way to the output embed-\nding than to the input embedding in the\nuntied model. We also offer a new method\nof regularizing the output embedding. Our\nmethods lead to a signiﬁcant reduction in\nperplexity, as we are able to show on a va-\nriety of neural network language models.\nFinally, we show that weight tying can re-\nduce the size of neural translation models\nto less than half of their original size with-\nout harming their performance.\n1 Introduction\nIn a common family of neural network language\nmodels, the current input word is represented as\nthe vector c ∈ I RC and is projected to a dense\nrepresentation using a word embedding matrix U.\nSome computation is then performed on the word\nembedding U⊤c, which results in a vector of ac-\ntivations h2. A second matrix V then projects h2\nto a vector h3 containing one score per vocabulary\nword: h3 = Vh2. The vector of scores is then con-\nverted to a vector of probability values p, which\nrepresents the models’ prediction of the next word,\nusing the softmax function.\nFor example, in the LSTM-based language\nmodels of (Sundermeyer et al., 2012; Zaremba\net al., 2014), for vocabulary of size C, the one-\nhot encoding is used to represent the input c and\nU ∈I RC×H. An LSTM is then employed, which\nresults in an activation vector h2 that similarly to\nU⊤c, is also in I RH. In this case, U and V are of\nexactly the same size.\nWe call U the input embedding, and V the out-\nput embedding. In both matrices, we expect rows\nthat correspond to similar words to be similar: for\nthe input embedding, we would like the network\nto react similarly to synonyms, while in the out-\nput embedding, we would like the scores of words\nthat are interchangeable to be similar (Mnih and\nTeh, 2012).\nWhile U and V can both serve as word embed-\ndings, in the literature, only the former serves this\nrole. In this paper, we compare the quality of the\ninput embedding to that of the output embedding,\nand we show that the latter can be used to improve\nneural network language models. Our main results\nare as follows: (i) We show that in the word2vec\nskip-gram model, the output embedding is only\nslightly inferior to the input embedding. This is\nshown using metrics that are commonly used in or-\nder to measure embedding quality. (ii) In recurrent\nneural network based language models, the output\nembedding outperforms the input embedding. (iii)\nBy tying the two embeddings together, i.e., enforc-\ning U = V, the joint embedding evolves in a more\nsimilar way to the output embedding than to the in-\nput embedding of the untied model. (iv) Tying the\ninput and output embeddings leads to an improve-\nment in the perplexity of various language mod-\nels. This is true both when using dropout or when\nnot using it. (v) When not using dropout, we pro-\npose adding an additional projection P before V,\nand apply regularization to P. (vi) Weight tying\nin neural translation models can reduce their size\n(number of parameters) to less than half of their\noriginal size without harming their performance.\n157\n2 Related Work\nNeural network language models (NNLMs) assign\nprobabilities to word sequences. Their resurgence\nwas initiated by (Bengio et al., 2003). Recur-\nrent neural networks were ﬁrst used for language\nmodeling in (Mikolov et al., 2010) and (Pascanu\net al., 2013). The ﬁrst model that implemented\nlanguage modeling with LSTMs (Hochreiter and\nSchmidhuber, 1997) was (Sundermeyer et al.,\n2012). Following that, (Zaremba et al., 2014) in-\ntroduced a dropout (Srivastava, 2013) augmented\nNNLM. (Gal, 2015; Gal and Ghahramani, 2016)\nproposed a new dropout method, which is referred\nto as Bayesian Dropout below, that improves on\nthe results of (Zaremba et al., 2014).\nThe skip-gram word2vec model introduced\nin (Mikolov et al., 2013a; Mikolov et al., 2013b)\nlearns representations of words. This model learns\na representation for each word in its vocabulary,\nboth in an input embedding matrix and in an out-\nput embedding matrix. When training is com-\nplete, the vectors that are returned are the input\nembeddings. The output embedding is typically\nignored, although (Mitra et al., 2016; Mnih and\nKavukcuoglu, 2013) use both the output and input\nembeddings of words in order to compute word\nsimilarity. Recently, (Goldberg and Levy, 2014)\nargued that the output embedding of the word2vec\nskip-gram model needs to be different than the in-\nput embedding.\nAs we show, tying the input and the output em-\nbeddings is indeed detrimental in word2vec. How-\never, it improves performance in NNLMs.\nIn neural machine translation (NMT) mod-\nels (Kalchbrenner and Blunsom, 2013; Cho et\nal., 2014; Sutskever et al., 2014; Bahdanau et\nal., 2014), the decoder, which generates the trans-\nlation of the input sentence in the target lan-\nguage, is a language model that is conditioned on\nboth the previous words of the output sentence\nand on the source sentence. State of the art re-\nsults in NMT have recently been achieved by sys-\ntems that segment the source and target words\ninto subword units (Sennrich et al., 2016a). One\nsuch method (Sennrich et al., 2016b) is based on\nthe byte pair encoding (BPE) compression algo-\nrithm (Gage, 1994). BPE segments rare words into\ntheir more commonly appearing subwords.\nWeight tying was previously used in the log-\nbilinear model of (Mnih and Hinton, 2009), but the\ndecision to use it was not explained, and its effect\non the model’s performance was not tested. In-\ndependently and concurrently with our work (Inan\net al., 2016) presented an explanation for weight\ntying in NNLMs based on (Hinton et al., 2015).\n3 Weight Tying\nIn this work, we employ three different model cat-\negories: NNLMs, the word2vec skip-gram model,\nand NMT models. Weight tying is applied sim-\nilarly in all models. For translation models, we\nalso present a three-way weight tying method.\nNNLM models contain an input embedding ma-\ntrix, two LSTM layers (h1 and h2), a third hidden\nscores/logits layer h3, and a softmax layer. The\nloss used during training is the cross entropy loss\nwithout any regularization terms.\nFollowing (Zaremba et al., 2014), we employ\ntwo models: large and small. The large model em-\nploys dropout for regularization. The small model\nis not regularized. Therefore, we propose the fol-\nlowing regularization scheme. A projection matrix\nP ∈I RH×H is inserted before the output embed-\nding, i.e., h3 = VPh 2. The regularizing term\nλ∥P∥2 is then added to the small model’s loss\nfunction. In all of our experiments, λ= 0.15.\nProjection regularization allows us to use the\nsame embedding (as both the input/output embed-\nding) with some adaptation that is under regular-\nization. It is, therefore, especially suited for WT.\nWhile training a vanilla untied NNLM , at\ntimestep t, with current input word sequence\ni1:t = [i1,...,i t] and current target output word\not, the negative log likelihood loss is given by:\nLt = −log pt(ot|i1:t), where pt(ot|i1:t) =\nexp (V ⊤\not h(t)\n2 )\n∑C\nx=1 exp(V ⊤x h(t)\n2 )\n, Uk (Vk) is the kth row of U (V),\nwhich corresponds to wordk, and h(t)\n2 is the vector\nof activations of the topmost LSTM layer’s output\nat time t. For simplicity, we assume that at each\ntimestep t, it ̸= ot. Optimization of the model is\nperformed using stochastic gradient descent.\nThe update for row kof the input embedding is:\n∂Lt\n∂Uk\n=\n{\n(∑C\nx=1 pt(x|i1:t) ·V⊤\nx −V⊤\not )\n∂h(t)\n2\n∂Uit\nk= it\n0 k̸= it\nFor the output embedding, row k’s update is:\n∂Lt\n∂Vk\n=\n{\n(pt(ot|i1:t) −1)h(t)\n2 k= ot\npt(k|i1:t) ·h(t)\n2 k̸= ot\nTherefore, in the untied model, at every timestep,\nthe only row that is updated in the input embed-\nding is the row Uit representing the current input\n158\nword. This means that vectors representing rare\nwords are updated only a small number of times.\nThe output embedding updates every row at each\ntimestep.\nIn tied NNLMs , we set U = V = S. The\nupdate for each row in Sis the sum of the updates\nobtained for the two roles of S as both an input and\noutput embedding.\nThe update for row k ̸= it is similar to the up-\ndate of rowkin the untied NNLM’s output embed-\nding (the only difference being that U and V are\nboth replaced by a single matrix S). In this case,\nthere is no update from the input embedding role\nof S.\nThe update for row k= it, is made up of a term\nfrom the input embedding (casek= it) and a term\nfrom the output embedding (case k ̸= ot). The\nsecond term grows linearly with pt(it|i1:t), which\nis expected to be close to zero, since words sel-\ndom appear twice in a row (the low probability\nin the network was also veriﬁed experimentally).\nThe update that occurs in this case is, therefore,\nmostly impacted by the update from the input em-\nbedding role of S.\nTo conclude, in the tied NNLM, every row ofS\nis updated during each iteration, and for all rows\nexcept one, this update is similar to the update of\nthe output embedding of the untied model. This\nimplies a greater degree of similarity of the tied\nembedding to the untied model’s output embed-\nding than to its input embedding.\nThe analysis above focuses on NNLMs for\nbrevity. In word2vec, the update rules are simi-\nlar, just that h(t)\n2 is replaced by the identity func-\ntion. As argued by (Goldberg and Levy, 2014), in\nthis case weight tying is not appropriate, because\nif pt(it|i1:t) is close to zero then so is the norm\nof the embedding of it. This argument does not\nhold for NNLMs, since the LSTM layers cause a\ndecoupling of the input and output embedddings.\nFinally, we evaluate the effect of weight ty-\ning in neural translation models. In this model:\npt(ot|i1:t,r) =\nexp(V ⊤\not G(t))\n∑Ct\nx=1 exp(V ⊤x G(t)) where r =\n(r1,...,r N ) is the set of words in the source sen-\ntence, U and V are the input and output embed-\ndings of the decoder and W is the input embed-\nding of the encoder (in translation models U,V ∈\nI RCt×H and W ∈I RCs×H, where Cs / Ct is the\nsize of the vocabulary of the source / target). G(t)\nis the decoder, which receives the context vector,\nthe embedding of the input word (it) in U, and its\nLanguage Subwords Subwords Subwords\npairs only in source only in target in both\nEN→FR 2K 7K 85K\nEN→DE 3K 11K 80K\nTable 1: Shared BPE subwords between pairs of languages.\nprevious state at each timestep. ct is the context\nvector at timestep t, ct = ∑\nj∈r atjhj, where atj\nis the weight given to the jth annotation at time t:\natj = exp(etj)∑\nk∈r exp(eik) , and etj = at(hj), where ais\nthe alignment model. F is the encoder which pro-\nduces the sequence of annotations (h1,...,h N ).\nThe output of the decoder is then projected to\na vector of scores using the output embedding:\nlt = VG(t). The scores are then converted to prob-\nability values using the softmax function.\nIn our weight tied translation model, we tie the\ninput and output embeddings of the decoder.\nWe observed that when preprocessing the ACL\nWMT 2014 EN→FR1 and WMT 2015 EN→DE2\ndatasets using BPE, many of the subwords ap-\npeared in the vocabulary of both the source and\nthe target languages. Tab. 1 shows that up to\n90% (85%) of BPE subwords between English and\nFrench (German) are shared.\nBased on this observation, we propose three-\nway weight tying (TWWT), where the input em-\nbedding of the decoder, the output embedding of\nthe decoder and the input embedding of the en-\ncoder are all tied. The single source/target vocab-\nulary of this model is the union of both the source\nand target vocabularies. In this model, both in the\nencoder and decoder, all subwords are embedded\nin the same duo-lingual space.\n4 Results\nOur experiments study the quality of various em-\nbeddings, the similarity between them, and the\nimpact of tying them on the word2vec skip-gram\nmodel, NNLMs, and NMT models.\n4.1 Quality of Obtained Embeddings\nIn order to compare the various embeddings, we\npooled ﬁve embedding evaluation methods from\nthe literature. These evaluation methods involve\ncalculating pairwise (cosine) distances between\nembeddings and correlating these distances with\nhuman judgments of the strength of relationships\nbetween concepts. We use: Simlex999 (Hill et al.,\n1http://statmt.org/wmt14/translation-task.html\n2http://statmt.org/wmt15/translation-task.html\n159\nInput Output Tied\nSimlex999 0.30 0.29 0.17\nVerb-143 0.41 0.34 0.12\nMEN 0.66 0.61 0.50\nRare-Word 0.34 0.34 0.23\nMTurk-771 0.59 0.54 0.37\nTable 2: Comparison of input and output embeddings\nlearned by a word2vec skip-gram model. Results are also\nshown for the tied word2vec model. Spearman’s correlationρ\nis reported for ﬁve word embedding evaluation benchmarks.\nPTB text8\nEmbedding In Out Tied In Out Tied\nSimlex999 0.02 0.13 0.14 0.17 0.27 0.28\nVerb143 0.12 0.37 0.32 0.20 0.35 0.42\nMEN 0.11 0.21 0.26 0.26 0.50 0.50\nRare-Word 0.28 0.38 0.36 0.14 0.15 0.17\nMTurk771 0.17 0.28 0.30 0.26 0.48 0.45\nTable 3: Comparison of the input/output embeddings of the\nsmall model from (Zaremba et al., 2014) and the embeddings\nfrom our weight tied variant. Spearman’s correlationρis pre-\nsented.\n2016), Verb-143 (Baker et al., 2014), MEN (Bruni\net al., 2014), Rare-Word (Luong et al., 2013) and\nMTurk-771 (Halawi et al., 2012).\nWe begin by training both the tied and untied\nword2vec models on the text8 3 dataset, using a\nvocabulary consisting only of words that appear\nat least ﬁve times. As can be seen in Tab. 2,\nthe output embedding is almost as good as the\ninput embedding. As expected, the embedding\nof the tied model is not competitive. The situa-\ntion is different when training the small NNLM\nmodel on either the Penn Treebank (Marcus et\nal., 1993) or text8 datasets (for PTB, we used the\nsame train/validation/test set split and vocabulary\nas (Mikolov et al., 2011), while on text8 we used\nthe split/vocabulary from (Mikolov et al., 2014)).\nThese results are presented in Tab. 3. In this case,\nthe input embedding is far inferior to the output\nembedding. The tied embedding is comparable to\nthe output embedding.\nA natural question given these results and the\nanalysis in Sec. 3 is whether the word embedding\nin the weight tied NNLM model is more similar to\nthe input embedding or to the output embedding\nof the original model. We, therefore, run the fol-\nlowing experiment: First, for each embedding, we\ncompute the cosine distances between each pair of\nwords. We then compute Spearman’s rank corre-\nlation between these vectors of distances. As can\nbe seen in Tab. 4, the results are consistent with\n3http://mattmahoney.net/dc/textdata\nA B ρ(A,B) ρ(A,B) ρ(A,B)\nword2vec NNLM(S) NNLM(L)\nIn Out 0.77 0.13 0.16\nIn Tied 0.19 0.31 0.45\nOut Tied 0.39 0.65 0.77\nTable 4: Spearman’s rank correlationρof similarity values\nbetween all pairs of words evaluated for the different embed-\ndings: input/output embeddings (of the untied model) and the\nembeddings of our tied model. We show the results for both\nthe word2vec models and the small and large NNLM models\nfrom (Zaremba et al., 2014).\nModel Size Train Val. Test\nLarge (Zaremba et al., 2014) 66M 37.8 82.2 78.4\nLarge + Weight Tying 51M 48.5 77.7 74.3\nLarge + BD (Gal, 2015) + WD 66M 24.3 78.1 75.2\nLarge + BD + WT 51M 28.2 75.8 73.2\nRHN (Zilly et al., 2016) + BD 32M 67.4 71.2 68.5\nRHN + BD + WT 24M 74.1 68.1 66.0\nTable 5: Word level perplexity (lower is better) on PTB\nand size (number of parameters) of models that use either\ndropout (baseline model) or Bayesian dropout (BD). WD –\nweight decay.\nour analysis and the results of Tab. 2 and Tab. 3:\nfor word2vec the input and output embeddings are\nsimilar to each other and differ from the tied em-\nbedding; for the NNLM models, the output em-\nbedding and the tied embeddings are similar, the\ninput embedding is somewhat similar to the tied\nembedding, and differs considerably from the out-\nput embedding.\n4.2 Neural Network Language Models\nWe next study the effect of tying the embeddings\non the perplexity obtained by the NNLM models.\nFollowing (Zaremba et al., 2014), we study two\nNNLMs. The two models differ mostly in the size\nof the LSTM layers. In the small model, both\nLSTM layers contain 200 units and in the large\nmodel, both contain 1500 units. In addition, the\nlarge model uses three dropout layers, one placed\nright before the ﬁrst LSTM layer, one between h1\nand h2 and one right after h2. The dropout proba-\nbility is 0.65. For both the small and large models,\nwe use the same hyperparameters (i.e. weight ini-\ntialization, learning rate schedule, batch size) as\nin (Zaremba et al., 2014).\nIn addition to training our models on PTB and\ntext8, following (Miyamoto and Cho, 2016), we\nalso compare the performance of the NNLMs on\nthe BBC (Greene and Cunningham, 2006) and\nIMDB (Maas et al., 2011) datasets, each of which\nwe process and split into a train/validation/test\n160\nModel Size Train Val. Test\nKN 5-gram 141\nRNN 123\nLSTM 117\nStack RNN 8.48M 110\nFOFE-FNN 108\nNoisy LSTM 4.65M 111.7 108.0\nDeep RNN 6.16M 107.5\nSmall model 4.65M 38.0 120.7 114.5\nSmall + WT 2.65M 36.4 117.5 112.4\nSmall + PR 4.69M 50.8 116.0 111.7\nSmall + WT + PR 2.69M 53.5 104.9 100.9\nTable 6: Word level perplexity on PTB and size for mod-\nels that do not use dropout. The compared models are:\nKN 5-gram (Mikolov et al., 2011), RNN (Mikolov et al.,\n2011), LSTM (Graves, 2013), Stack / Deep RNN (Pas-\ncanu et al., 2013), FOFE-FNN (Zhang et al., 2015), Noisy\nLSTM (G ¨ulc ¸ehre et al., 2016), and the small model from\n(Zaremba et al., 2014). The last three models are our models,\nwhich extend the small model. PR – projection regulariza-\ntion.\nModel Small S + WT S + PR S + WT + PR\ntext8\nTrain 90.4 95.6 92.6 95.3\nVal. - - - -\nTest 195.3 187.1 199.0 183.2\nIMDB\nTrain 71.3 75.4 72.0 72.9\nVal. 94.1 94.6 94.0 91.2\nTest 94.3 94.8 94.4 91.5\nBBC\nTrain 28.6 30.1 42.5 45.7\nVal. 103.6 99.4 104.9 96.4\nTest 110.8 106.8 108.7 98.9\nTable 7: Word level perplexity on the text8, IMDB and\nBBC datasets. The last three models are our models, which\nextend the small model (S) of (Zaremba et al., 2014).\nsplit (we use the same vocabularies as (Miyamoto\nand Cho, 2016)).\nIn the ﬁrst experiment, which was conducted\non the PTB dataset, we compare the perplexity\nobtained by the large NNLM model and our ver-\nsion in which the input and output embeddings are\ntied. As can be seen in Tab. 5, weight tying sig-\nniﬁcantly reduces perplexity on both the valida-\ntion set and the test set, but not on the training set.\nThis indicates less overﬁtting, as expected due to\nthe reduction in the number of parameters. Re-\ncently, (Gal and Ghahramani, 2016), proposed a\nmodiﬁed model that uses Bayesian dropout and\nweight decay. They obtained improved perfor-\nmance. When the embeddings of this model are\ntied, a similar amount of improvement is gained.\nWe tried this with and without weight decay and\ngot similar results in both cases, with slight im-\nprovement in the latter model. Finally, by re-\nplacing the LSTM with a recurrent highway net-\nwork (Zilly et al., 2016), state of the art results are\nachieved when applying weight tying. The contri-\nSize Validation Test\nEN→FR Baseline 168M 29.49 33.13\nDecoder WT 122M 29.47 33.26\nTWWT 80M 29.43 33.46\nEN→DE Baseline 165M 20.96 16.79\nDecoder WT 119M 21.09 16.54\nTWWT 79M 21.02 17.15\nTable 8: Size (number of parameters) and BLEU score of\nvarious translation models. TWWT – three-way weight tying.\nbution of WT is also signiﬁcant in this model.\nPerplexity results are often reported separately\nfor models with and without dropout. In Tab. 6, we\nreport the results of the small NNLM model, that\ndoes not utilize dropout, on PTB. As can be seen,\nboth WT and projection regularization (PR) im-\nprove the results. When combining both methods\ntogether, state of the art results are obtained. An\nanalog table for text8, IMDB and BBC is Tab. 7,\nwhich shows a signiﬁcant reduction in perplexity\nacross these datasets when both PR and WT are\nused. PR does not help the large models, which\nemploy dropout for regularization.\n4.3 Neural Machine Translation\nFinally, we study the impact of weight tying in at-\ntention based NMT models, using the DL4MT 4\nimplementation. We train our EN →FR models\non the parallel corpora provided by ACL WMT\n2014. We use the data as processed by (Cho et al.,\n2014) using the data selection method of (Axelrod\net al., 2011). For EN →DE we train on data from\nthe translation task of WMT 2015, validate on\nnewstest2013 and test on newstest2014 and new-\nstest2015. Following (Sennrich et al., 2016b) we\nlearn the BPE segmentation on the union of the\nvocabularies that we are translating from and to\n(we use BPE with 89500 merge operations). All\nmodels were trained using Adadelta (Zeiler, 2012)\nfor 300K updates, have a hidden layer size of 1000\nand all embedding layers are of size 500.\nTab. 8 shows that even though the weight tied\nmodels have about 28% fewer parameters than the\nbaseline models, their performance is similar. This\nis also the case for the three-way weight tied mod-\nels, even though they have about 52% fewer pa-\nrameters than their untied counterparts.\n4https://github.com/nyu-dl/dl4mt-tutorial\n161\nReferences\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao.\n2011. Domain adaptation via pseudo in-domain data\nselection. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 355–362, Edinburgh, Scotland, UK., July.\nAssociation for Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nSimon Baker, Roi Reichart, and Anna Korhonen. 2014.\nAn unsupervised model for instance level subcate-\ngorization acquisition. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 278–289. Asso-\nciation for Computational Linguistics.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. J. Mach. Learn. Res., 3:1137–1155,\nMarch.\nElia Bruni, Nam-Khanh Tran, and Marco Baroni.\n2014. Multimodal distributional semantics. J. Ar-\ntif. Intell. Res.(JAIR), 49(1-47).\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1724–\n1734, Doha, Qatar, October. Association for Com-\nputational Linguistics.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. The C Users Journal, 12(2):23–38.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout\nas a Bayesian approximation: Representing model\nuncertainty in deep learning. In Proceedings of the\n33rd International Conference on Machine Learning\n(ICML-16).\nYarin Gal. 2015. A Theoretically Grounded Appli-\ncation of Dropout in Recurrent Neural Networks.\narXiv preprint arXiv:1512.05287.\nYoav Goldberg and Omer Levy. 2014. word2vec\nexplained: deriving mikolov et al.’s negative-\nsampling word-embedding method. arXiv preprint\narXiv:1402.3722.\nAlex Graves. 2013. Generating sequences\nwith recurrent neural networks. arXiv preprint\narXiv:1308.0850.\nDerek Greene and P ´adraig Cunningham. 2006. Prac-\ntical solutions to the problem of diagonal dom-\ninance in kernel document clustering. In Proc.\n23rd International Conference on Machine learning\n(ICML’06), pages 377–384. ACM Press.\nC ¸ aglar G¨ulc ¸ehre, Marcin Moczulski, Misha Denil, and\nYoshua Bengio. 2016. Noisy activation functions.\narXiv preprint arXiv:1603.00391.\nGuy Halawi, Gideon Dror, Evgeniy Gabrilovich, and\nYehuda Koren. 2012. Large-scale learning of\nword relatedness with constraints. In Proceedings of\nthe 18th ACM SIGKDD international conference on\nKnowledge discovery and data mining, pages 1406–\n1414.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2016.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput., 9(8):1735–\n1780, November.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In Proceedings of\nthe 2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1700–1709, Seattle,\nWashington, USA, October. Association for Compu-\ntational Linguistics.\nThang Luong, Richard Socher, and Christopher Man-\nning, 2013. Proceedings of the Seventeenth Confer-\nence on Computational Natural Language Learning,\nchapter Better Word Representations with Recursive\nNeural Networks for Morphology, pages 104–113.\nAssociation for Computational Linguistics.\nL. Andrew Maas, E. Raymond Daly, T. Peter Pham,\nDan Huang, Y . Andrew Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150. Asso-\nciation for Computational Linguistics.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large anno-\ntated corpus of english: The penn treebank. Com-\nput. Linguist., 19(2):313–330, June.\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, 11th Annual Conference of the\nInternational Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010,\npages 1045–1048.\n162\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn 2011 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n5528–5531. IEEE.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111–3119.\nTomas Mikolov, Armand Joulin, Sumit Chopra,\nMicha¨el Mathieu, and Marc’Aurelio Ranzato. 2014.\nLearning longer memory in recurrent neural net-\nworks. arXiv preprint arXiv:1412.7753.\nBhaskar Mitra, Eric Nalisnick, Nick Craswell, and\nRich Caruana. 2016. A dual embedding space\nmodel for document ranking. arXiv preprint\narXiv:1602.01137.\nYasumasa Miyamoto and Kyunghyun Cho. 2016.\nGated word-character recurrent language model. In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, pages\n1992–1997. Association for Computational Linguis-\ntics.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model. In\nAdvances in neural information processing systems,\npages 1081–1088.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings efﬁciently with noise-contrastive\nestimation. In Advances in Neural Information Pro-\ncessing Systems, pages 2265–2273.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and\nsimple algorithm for training neural probabilistic\nlanguage models. arXiv preprint arXiv:1206.6426.\nRazvan Pascanu, C ¸ aglar G¨ulc ¸ehre, Kyunghyun Cho,\nand Yoshua Bengio. 2013. How to construct\ndeep recurrent neural networks. arXiv preprint\narXiv:1312.6026.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Edinburgh neural machine translation sys-\ntems for wmt 16. arXiv preprint arXiv:1606.02891.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of ACL.\nNitish Srivastava. 2013. Improving Neural Net-\nworks with Dropout. Master’s thesis, University of\nToronto, Toronto, Canada, January.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. Lstm neural networks for language modeling.\nIn Interspeech, pages 194–197, Portland, OR, USA,\nSeptember.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nMatthew D Zeiler. 2012. Adadelta: an adaptive learn-\ning rate method. arXiv preprint arXiv:1212.5701.\nShiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou,\nand Li-Rong Dai. 2015. A ﬁxed-size encoding\nmethod for variable-length sequences with its ap-\nplication to neural network language models. arXiv\npreprint arXiv:1505.01504.\nJulian G. Zilly, Rupesh Kumar Srivastava, Jan Koutn´ık,\nand J¨urgen Schmidhuber. 2016. Recurrent highway\nnetworks. arXiv preprint arXiv:1607.03474.\n163",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.9137629270553589
    },
    {
      "name": "Perplexity",
      "score": 0.7754117250442505
    },
    {
      "name": "Tying",
      "score": 0.700255811214447
    },
    {
      "name": "Language model",
      "score": 0.637617826461792
    },
    {
      "name": "Computer science",
      "score": 0.6240537166595459
    },
    {
      "name": "Word embedding",
      "score": 0.5184123516082764
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5165354609489441
    },
    {
      "name": "Artificial neural network",
      "score": 0.48118478059768677
    },
    {
      "name": "Matrix (chemical analysis)",
      "score": 0.4463568329811096
    },
    {
      "name": "Translation (biology)",
      "score": 0.4251590669155121
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3449309468269348
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    }
  ]
}