{
  "title": "Visual Parser: Representing Part-whole Hierarchies with Transformers",
  "url": "https://openalex.org/W3178340970",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2347466819",
      "name": "Sun Shu-yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2315808385",
      "name": "Yue, Xiaoyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111813221",
      "name": "Bai Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202207711",
      "name": "Torr, Philip",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W2007361173",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3102696055",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2414711238",
    "https://openalex.org/W3006014043",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2785994986",
    "https://openalex.org/W3132401450",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W3122484828",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2056860348",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2964151039",
    "https://openalex.org/W2971044234",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2963319519",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2127859399",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2998108143"
  ],
  "abstract": "Human vision is able to capture the part-whole hierarchical information from the entire scene. This paper presents the Visual Parser (ViP) that explicitly constructs such a hierarchy with transformers. ViP divides visual representations into two levels, the part level and the whole level. Information of each part represents a combination of several independent vectors within the whole. To model the representations of the two levels, we first encode the information from the whole into part vectors through an attention mechanism, then decode the global information within the part vectors back into the whole representation. By iteratively parsing the two levels with the proposed encoder-decoder interaction, the model can gradually refine the features on both levels. Experimental results demonstrate that ViP can achieve very competitive performance on three major tasks e.g. classification, detection and instance segmentation. In particular, it can surpass the previous state-of-the-art CNN backbones by a large margin on object detection. The tiny model of the ViP family with $7.2\\times$ fewer parameters and $10.9\\times$ fewer FLOPS can perform comparably with the largest model ResNeXt-101-64$\\times$4d of ResNe(X)t family. Visualization results also demonstrate that the learnt parts are highly informative of the predicting class, making ViP more explainable than previous fundamental architectures. Code is available at https://github.com/kevin-ssy/ViP.",
  "full_text": "Visual Parser: Representing Part-whole Hierarchies\nwith Transformers\nShuyang Sun‚àó‚Ä†, Xiaoyu Yue‚àó, Song Bai‚Ä°, Philip Torr‚Ä†\n‚Ä†University of Oxford, ‚Ä°ByteDance AI Lab\n{kevinsun, phst}@robots.ox.ac.uk, {yuexiaoyu002, songbai.site}@gmail.com\nAbstract\nHuman vision is able to capture the part-whole hierarchical information from the\nentire scene. This paper presents the Visual Parser (ViP) that explicitly constructs\nsuch a hierarchy with transformers. ViP divides visual representations into two\nlevels, the part level and the whole level. Information of each part represents a\ncombination of several independent vectors within the whole. To model the repre-\nsentations of the two levels, we Ô¨Årst encode the information from the whole into\npart vectors through an attention mechanism, then decode the global information\nwithin the part vectors back into the whole representation. By iteratively parsing the\ntwo levels with the proposed encoder-decoder interaction, the model can gradually\nreÔ¨Åne the features on both levels. Experimental results demonstrate that ViP can\nachieve very competitive performance on three major tasks e.g. classiÔ¨Åcation,\ndetection and instance segmentation. In particular, it can surpass the previous state-\nof-the-art CNN backbones by a large margin on object detection. The tiny model of\nthe ViP family with 7.2√ófewer parameters and 10.9√ófewer FLOPS can perform\ncomparably with the largest model ResNeXt-101-64 √ó4d of ResNe(X)t family.\nVisualization results also demonstrate that the learnt parts are highly informative\nof the predicting class, making ViP more explainable than previous fundamental\narchitectures. Code is available at https://github.com/kevin-ssy/ViP.\n1 Introduction\nStrong evidence has been found in psychology that human vision is able to parse a complex scene\ninto part-whole hierarchies with many different levels from the low-level pixels to the high-level\nproperties (e.g. parts, objects, scenes) [28, 38]. Constructing such a part-whole hierarchy enables\nneural networks to capture compositional representations directly from images, which can promisingly\nhelp to detect properties of many different levels with only one network.\nTo the best of our knowledge, most current visual feature extractors do not model such hierarchy\nexplicitly. Due to the lack of such part-whole hierarchy in representation modeling, existing feature\nextractors cannot Ô¨Ånd the compositional features directly from the network. Ideal modeling of the\nvisual representation should be able to model the part-whole hierarchy as humans do so that we can\nleverage representations of all levels directly from one backbone model.\nBuilding up a framework that includes different levels of representations in the part-whole hierarchy\nis difÔ¨Åcult for conventional neural networks as it requires neurons to dynamically respond to the\ninput, while neural networks with Ô¨Åxed weights cannot dynamically allocate a group of neurons to\nrepresent a node in a parse tree [29]. With the rise of the Transformer [62], such a problem can be\npossibly resolved due to its dynamic nature.\nIn this paper, we show how to construct a simplest part-whole hierarchy for visual representation.\nThe hierarchy of the proposed network has two levels. One represents the part, which only contains\nthe most essential information describing the visual input, and the other is for the whole, which\nPreprint. Under review. * Equal Contribution.\narXiv:2107.05790v2  [cs.CV]  8 Jan 2022\n38404244464850\n0 100200300400\nCOCOBBoxmAP\nBackboneFLOPS(G)\nViPResNe(X)tRes2NetResNeStR18R50R101X101-32 X101-64\nViP-Mo\nViP-Ti\nViP-SViP-M\nR2-101S-101S-50\n38404244464850\n0 20 40 60 80 100\nCOCOBBoxmAP\nBackboneParams(M)\nViPResNe(X)tRes2NetResNeStR18R50R101X101-32 X101-64\nViP-Mo\nViP-Ti\nViP-SViP-M\nR2-101S-101S-50 10.9√ófewerFLOPS7.2√ófewerparams\nFigure 1: ViP can outperform all state-of-the-art CNN backbones by a large margin on object\ndetection. All listed results are trained under the regular 1√óregime in Cascade Mask-RCNN [6].\ndescribes the visual input in a regular spatial coordinate frame system. Normally a part vector can be\ndynamically associated with several vectors of the whole, forming a one-to-many mapping between\nthe two levels. To obtain information for the part, we Ô¨Årst apply an encoder between the two levels to\nÔ¨Åll each part with the features of the whole. Then the encoded part feature will be mapped back to the\nwhole by a transformer-based decoder. Such cross-level interaction is iteratively applied throughout\nthe network, constituting a bi-directional pathway between the two levels.\nOur main contributions are as follows: (1) We propose Visual Parser (ViP) that can explicitly\nconstruct a part-whole hierarchy within the network for basic visual representations. This is the Ô¨Årst\nstep towards learning multi-level part-whole representations. (2) With the help of such a part-whole\nhierarchy, the network can be more explainable compared with previous networks. (3) ViP can be\ndirectly applied as a backbone for versatile uses. Experimental results also demonstrate that ViP can\nachieve very competitive results compared to the current state-of-the-art backbones. As shown in\nFigure 1, it outperforms all state-of-the-art CNN counterparts by a large margin on object detection.\n2 Method\n2.1 Overview\nThe overall pipeline is shown in Figure 2. There are two inputs of ViP, including an input image and\na set of learnable parameters. These parameters represent the prototype of the parts, and will be used\nas initial clues indicating the region that each part should associate with. The entire network consists\nof several basic blocks (iterations). For block i‚àà{2,3,...,B }, there are two kinds of representations\ndescribing the two hierarchical levels. One is the part representation pi ‚ààRN√óC and the other is the\nwhole feature maps xi ‚ààRL√óC. Here N is a pre-deÔ¨Åned constant number indicating the number of\nparts within the input image, and Lis the number of pixels of the feature map, which is identical\nto width√óheight. B,C are the numbers of blocks and channels respectively. The representation of\nparts for each block is dynamically encoded from the corresponding whole feature maps through an\nattention-based approach. Given the representation of the part pi‚àí1 and the whole xi‚àí1 from the\nprevious block i‚àí1, an attention-based encoder is applied to Ô¨Åll the information of the whole into\nthe part pi of the current block. Since the attention mechanism assigns each pixel on the feature map\nwith a weight indicating the afÔ¨Ånity between the pixel and the corresponding part, only the spatial\ninformation in xi‚àí1 that is semantically close to the input part pi‚àí1 can be updated into pi.\nInformation within the encoded parts will be then transferred back into the feature maps, so each\npixel on the feature map can interact with the information in a wider range. Since the information\nwithin the parts is highly condensed, the computational cost between the pixels and the parts is much\nlower than the original pixel-wise global attention [65, 54]. This encoder-decoder process constitutes\nthe basic building block of ViP. By stacking the building block iteratively, the network can learn to\nconstruct a two-level part-whole hierarchy explicitly.\n2\n‚Ä¶\nPartLevel\nWholelevel\nPrototypeùê©ùüè\nInputImageùê±\" ùê±# ‚Ä¶\nEncodeDecode\nùê±$ ùê±%\nùê©$\n ùê©%\n ùê©ùüí\nFigure 2: The overall pipeline of the Visual Parser (ViP). Given an input image and the prototype\nparameters p1 as input, we iteratively reÔ¨Åne the features of the two levels with the proposed encoder-\ndecoder process.\n2.2 Part Encoder\nThe part encoder is responsible for extracting the part information based on the previous part-whole\ninput. The encoder is implemented with an attention mechanism. Given the part representation\npi‚àí1 ‚ààRN√óC of the last block i‚àí1, we Ô¨Årst normalize it with Layer Normalization [1], then use it\nas the query of the attention block. The whole feature map from the last block xi‚àí1 ‚ààRL√óC serves\nas the key and value after the normalization. The information of the whole will be condensed into the\npart representations via attention, which can be formulated as:\nÀÜ pi‚àí1 = pi‚àí1 + Attention(pi‚àí1 + de,xi‚àí1 + dw,xi‚àí1), (1)\nwhere ÀÜ pi‚àí1 ‚ààRN√óC is the output of the attention block, and Attention(query,key,value) denotes\nthe attention mechanism. dw ‚ààRL√óC,de ‚ààRN√óC are positional encodings for the whole and the\npart respectively. The positional encoding dw follows the sinusoidal design proposed in [7], and de\nis a set of learnable weights. We follow the common practice of the classic attention calculation,\nwhich Ô¨Årst outputs an afÔ¨Ånity matrix M ‚ààRN√óL between the query and key and then use it to select\ninformation lying in the value.\nM = 1‚àö\nC\nq(LN(pi‚àí1 + de)) ¬∑k(LN(xi‚àí1 + dw))T , (2)\nwhere the functions q(¬∑),k(¬∑) denote the linear mappings for the inputs of query and key, LN is the\nLayer Normalization. Here we omit the learnable weights within LN for simplicity. The product of\nthe query and the key is normalized by a temperature factor 1‚àö\nC to avoid it being one-hot after the\nlateral softmax calculation. The softmax operation guarantees the sum of the afÔ¨Ånity matrix on the\nwhole dimension to be one, which can be formulated as:\nÀÜMa,b = eMa,b\n‚àë\nl eMa,l\n. (3)\nThe normalized afÔ¨Ånity matrix can be easy to explain, as it assigns an independent weight to each\nspatial location indicating where each part is lying on the spatial feature maps. To aggregate these\nweighted spatial locations into part vectors, we follow the classic attention that weighted averaging\nthe values together with the afÔ¨Ånity matrix:\nAttention(pi‚àí1 + de,xi‚àí1 + dw,xi‚àí1) = ÀÜM ¬∑v(LN(xi‚àí1)), (4)\nwhere v(¬∑) is the linear mapping for values, and ÀÜM is the afÔ¨Ånity matrix after softmax.\nReasoning across different parts. After each part is Ô¨Ålled with the information from the feature\nmaps, we apply a part-wise reasoning module to enable information communication between parts.\nIn order to save computational cost, we just apply a single linear projection with learnable weights\nWp ‚ààRN√óN . An identity mapping and the normalization are also applied here as a residual block.\nThe process of the part-wise reasoning can be formulated as:\nÀÜ pi‚àí1\nr = ÀÜ pi‚àí1 + Wp ¬∑LN(ÀÜ pi‚àí1), (5)\nwhere ÀÜ pi‚àí1\nr represents the output for the part-wise reasoning.\n3\n‚Ä¶\nùëû ùëò ùë£Norm &Multi-headAttention\nNorm&MLP\nNorm&MLP\nNorm&Multi-headAttentionùëò ùë£ùëû\nùê©!\"#:N√óC ùê±!\"#:L√óC\nNorm&Part-wiseLinear\nNorm&Multi-headSelf-Attention\nùê©!:N√óC\nNorm&MLP\nùê±!:L√óC\nEncoder Decoder\nùêù$:L√óC\nùêù%\nùêù&:N√óC\n,ùê©'!\"# ùê±(!\n-ùê±!\n-ùê±)!\n,ùê©!\"#\nFigure 3: The basic building block of ViP. The symbol‚®Ådenotes element-wise summation. Here we omit the\nrelative positional embedding ri for clear demonstration.\nStage L Block Settings\n7√ó7,64, Conv, stride 2\n3√ó3Max Pool, stride 2\nStage1 HW\n42\nPatch Embedding,C1\n[C1,N1,G1]√óB1\nStage2 HW\n82\nPatch Embedding,C2\n[C2,N2,G2]√óB2\nStage3 HW\n162\nPatch Embedding,C3\n[C3,N3,G3]√óB3\nStage4 HW\n322\nPatch Embedding,C4\n[C4,N4,G4]√óB4\nLast Encoder: [C4,N4,G4]\n1000, Linear\nTable 1: General speciÔ¨Åcation for ViP\nfamily. Building blocks of ViP are\nshown in brackets. The last encoder is\nonly set for ViP-Mo/Ti/S.\nActivating the part representations. The part representation learnt above may not be all meaningful\nsince different objects may have different numbers of parts describing themselves. We thereby further\napply a Multi-Layer Perceptron (MLP) that has two linear mappings with weight Wf1,Wf2 ‚àà\nRC√óC and an activation function (GELU [26]) œÉ(¬∑) in its module. The activation function will only\nkeep the useful parts to be active, while those identiÔ¨Åed to be less helpful will be squashed. In this\nway, we obtain the part representation pi for block iby:\npi = ÀÜ pi‚àí1\nr + MLP(ÀÜ pi‚àí1\nr ),\nMLP(ÀÜ pi‚àí1\nr ) =œÉ(LN(ÀÜ pi‚àí1\nr ) ¬∑Wf1) ¬∑Wf2.\n(6)\nThe above process demonstrates that the part representation generated by the previous block will be\nused to initialize the parts of the next iteration. Thus the randomly initialized part representations\nwill be gradually reÔ¨Åned with the information from the whole within each block.\n2.3 Whole Decoder\nAs shown in Figure 3, there are two inputs for the decoder, the part representation pi and the\nwhole representation xi‚àí1. Interactions within the decoder can be divided into a part-whole global\ninteraction between the parts and the feature maps, and a patch-based local attention between pixels\nin a local window of the whole feature maps.\nPart-whole global interaction. We Ô¨Årst apply the part-whole global attention to Ô¨Åll each pixel on the\nwhole representations xi‚àí1 with the global information encapsulated in the parts pi. The part-whole\nglobal attention completely follows the classic attention paradigm [62], which takes xi‚àí1 as the query\ninput, and pi as inputs of the key and value. Therefore each pixel of the whole representation can\nhave a long-range interaction with the encoded parts. Before feeding into the attention, both part and\nwhole representations will be normalized by Layer Normalization. An identity mapping and a MLP\nare also applied as what does in common practice. The process of the part-whole interaction in the\ndecoder can be written as:\nxi\ng = xi‚àí1 + Attention(xi‚àí1 + dw,pi + dd,pi),\nÀÜ xi = xi\ng + MLP(xi\ng),\n(7)\nwhere dd ‚ààRN√óC is the positional encoding for parts in decoders, the deÔ¨Ånitions of the attention\nmechanism and MLP are identical to those deÔ¨Åned in Eq. (1) (6). Note that dd is shared across\nall blocks of each stage. The axis that the softmax function normalizes on is the last dimension\n(speciÔ¨Åcally, the part dimension with N inputs).\nPatch-based local attention. The above process, in both the encoder and the decoder, has completed\nthe cross-level interactions for the ith iteration. In addition to the long-range modeling that the\ncross-level interaction provided, we also apply a local attention for Ô¨Åne-grained feature modeling. We\n4\ndivide the spatial feature maps into non-overlapping patches with size k√ók, then apply a multi-head\nself-attention module for all pixels within each patch. We denote the pixels of patchtas xi\nt ‚ààRk2√óC,\nthen the process of the local attention can be written as:\nÀÜ xi\nt = xi\nt + Attention(xi\nt,xi\nt + ri,xi\nt),\nÀÜ xi\nl = {ÀÜ xi\n1,..., ÀÜ xi\nt,..., ÀÜ xi\nNp},\nxi = ÀÜ xi\nl + MLP(ÀÜ xi\nl),\n(8)\nwhere Np = L\nk2 denotes the total number of patches,ri ‚ààRk2√óC is the relative positional embedding.\nThe implementation of the relative positional embedding follows the design in [4, 54]. To save the\ncomputational cost, ri is factorized into two embeddings ri\nh ‚ààR(2k‚àí1)√óC\n2 ,ri\nw ‚ààR(2k‚àí1)√óC\n2 for the\ndimension of height and width respectively.\n2.4 Architecture SpeciÔ¨Åcation.\nIn this paper, we design Ô¨Åve kinds of different variants called ViP-Mobile (Mo), ViP-Tiny (Ti),\nViP-Small (S), ViP-Medium (M), ViP-Base (B) respectively. These variants have some common\nfeatures in design. (1) Architectures of all these models are divided into four stages according to the\nspatial resolution Lof the feature map. Given an input with spatial size H√óW, the output spatial\nsizes of the feature maps for the four stages are H\n4 √óW\n4 , H\n8 √óW\n8 , H\n16 √óW\n16 and H\n32 √óW\n32 . (2) The\nexpansion rates of the MLP within the encoder and the decoder, which indicates the ratio between\nthe number of channels of the hidden output and the input, are set to be 1 and 3 separately. (3) The\npatch size for the self-attention module of the decoder is set to {8,7,7,7}for four different stages.\n(4) At the beginning of each stage, there is a patch embedding responsible for down-sampling and\nchannel-wise expansion. We employ a separable convolution with normalization here with kernel\nsize 3 √ó3 to perform the down-sampling operation for the whole representation. Since the number of\nchannels of the part representation for each stage may vary, another linear operation is applied to\nalign the number of channels between parts in different stages.\nApart from these common hyper-parameters, for a speciÔ¨Åc stage s, these variants mainly differ in the\nfollowing aspects: (1) The number of channels Cs, (2) The number of parts Ns, (3) The number of\nblocks Bs, (4) The number of groups (heads) Gs for the attention mechanism. (5) For small models\nViP-Mo, ViP-Ti and ViP-S, we employ a part encoder on top of the whole representation before the\nÔ¨Ånal global pooling and fully connected layer, while for ViP-M and ViP-B, we replace such encoder\nwith a linear layer. The overall architecture of the ViP family is shown in Table 1. The detailed\nspeciÔ¨Åcation of the four variants can be found in the appendix.\n3 Related Work\nConvolutional Neural Networks (CNNs). Conventional CNNs [53, 57, 24, 35, 58, 49] are prevalent\nin nearly all Ô¨Åelds of computer vision since AlexNet [41] demonstrates its power for image recognition\non ImageNet [16]. Now CNNs are still dominating nearly all major tasks in computer vision e.g.\nimage classiÔ¨Åcation [5, 3, 58, 24, 49, 56], object detection [21, 20, 51, 45, 25, 6, 44] and semantic\nsegmentation [48, 9, 70, 2, 73].\nPart-whole hierarchies in visual representations. Tu et al. [61] Ô¨Årst devise a Bayesian framework\nto parse the image into a part-whole hierarchy for unifying all the major vision tasks. Capsule\nNetworks (CapsNets) [52] were Ô¨Årst proposed to use a dynamic routing algorithm to dynamically\nallocate neurons to represent a small portion of the visual input. There are some other extensive works\nbased on CapsNets [30, 40, 60] showing remarkable performance on some small datasets, however,\nthese works cannot be well scaled onto large datasets. The recent proposal of GLOM [29] gives an\nidea to build up a hierarchical representation with attention, but it gives no practical experiments.\nThis paper borrows some ideas from these works to build a rather simple hierarchy with two levels for\nmodeling basic visual representation. For example, the iterative attention mechanism in our model is\nsimilar to the dynamic routing designed in CapsNet [52] or iterative attention mechanism [60, 47, 7].\nTransformers and self-attention mechanism. With the success of Memory Networks [ 55] and\nTransformers [62] for natural language modeling [ 67, 17, 14, 69], lots of works in the Ô¨Åeld of\ncomputer vision attempted to migrate similar self-attention mechanism as an independent block\n5\nModel Input\nSize\nParams\n(M)\nFLOPS\n(G)\nTop-1\nAcc (%)\nCNN Architectures\nBoTNet-T3 [54]2242 33.5 7.3 81.7\nBoTNet-T4 [54]2242 54.7 10.9 82.8\nBoTNet-T5 [54]2562 75.1 19.3 83.5\nRegNetY-4G [49]2242 20.6 4.0 80.0\nRegNetY-8G [49]2242 39.2 8.0 81.7\nRegNetY-16G [49]2242 83.6 15.9 82.9\nTransformer Architectures\nViT-B [18] 3842 86.4 55.4 77.9\nViT-L [59] 3842 307 190.7 76.5\nDeiT-Ti [59] 2242 5.7 1.6 72.2\nDeiT-S [59] 2242 22.1 4.6 79.8\nDeiT-B [59] 2242 86.6 17.6 81.8\nDeiT-B‚Üë384 [59] 3842 86.6 55.4 83.1\nPVT-Tiny [64] 2242 13.2 1.9 75.1\nPVT-Small [64]2242 24.5 3.8 79.8\nPVT-Medium [64]2242 44.2 6.7 81.2\nPVT-Large [64]2242 61.4 9.8 81.7\nT2T-ViT-14 [71]2242 21.5 5.2 81.5\nT2T-ViT-19 [71]2242 39.2 8.9 81.9\nT2T-ViT-24 [71]2242 64.1 14.1 82.3\nTNT-S [23] 2242 23.8 5.2 81.5\nTNT-B [23] 2242 65.6 14.1 82.9\nSwin-T [46] 2242 29 4.5 81.3\nSwin-S [46] 2242 50 8.7 83.0\nSwin-B [46] 2242 88 15.4 83.3\nViP-Mo 2242 5.3 0.8 75.1\nViP-Ti 2242 12.8 1.7 79.0\nViP-S 2242 32.1 4.5 81.9\nViP-M 2242 49.6 8.0 83.3\nViP-B 2242 87.8 15.0 83.8\nViP-B‚Üë384 3842 87.8 39.1 84.2\nTable 2: Results on ImageNet-1K.\nFigure 4: Speed-Accuracy comparison with HaloNet.\nN FLOPS\n(G)\nTop-1\n(%)\nViP-Ti\n8 1.6 77.6\n16 1.6 78.1\n32 1.7 79.0\n64 1.8 79.1\nTable 3: Effect of number of parts for ViP-Ti.\nPredict\non parts\nPredict\non wholes\nTop-1\n(%)\nViP-Ti ‚úì 79.0\n‚úì 78.3\nViP-S ‚úì 81.9\n‚úì 81.5\nViP-M ‚úì 82.7\n‚úì 83.3\nTable 4: Effect of predicting on part/whole level.\ninto CNNs for image classiÔ¨Åcation [ 33, 11, 4, 66], object detection [ 32, 54, 7] and video action\nrecognition [37, 65, 19].\nRecent works tried to replace all convolutional layers in neural networks with local attention layers to\nbuild up self-attention-based networks [32, 50, 74, 13, 31, 63]. To resolve the inefÔ¨Åciency problem,\nVision Transformer (ViT) [18, 59] chose to largely reduce the image resolution and only retain the\nglobal visual tokens while processing. To aid the global token-based transformer with local inductive\nbiases, there are several papers that incorporate convolution-like design into ViT [71, 12, 15, 27, 68].\nApart from the token-based approach, concurrent works [46, 64] that retain the spatial pyramids has\nalso been proven to be effective. Different from the above existing works that extract either tokens or\nspatial feature maps for Ô¨Ånal prediction, ViP extracts both the token-based representations (the part)\nand spatial feature maps (the whole).\nToken-based global attention mechanism.The interaction between the part and the whole is related\nto the token-based global attention mechanism. Recent works including [11, 3, 10, 66] propose to\ntokenize the input feature map generated by the convolution block. Our work is different from theirs\nin three aspects: (1) The part representations are explicitly and iteratively reÔ¨Åned in ViP, while in\nthese works, the tokens are latent bi-product of each block. (2) We intend to design a hierarchy\nthat can be used for Ô¨Ånal prediction while these works focus on designing a module then plug it\ninto limited blocks of the network. (3) In detail, the token extraction and the bi-directional pathway\ndesigned in ViP are quite different from their pipelines.\n4 Experiments\n4.1 Image ClassiÔ¨Åcation on ImageNet-1K\nExperimental Settings. For image classiÔ¨Åcation, we evaluate our models on ImageNet-1K [ 16],\nwhich consists of 1.28M training images and 50K validation images categorized into 1,000 classes.\nThe network is trained for 300 epochs using AdamW [39] and a half-cosine annealing learning rate\n6\nRetinaNet1√ó RetinaNet3√ó\nBackbone APb APb50APb75 APbS APbM APbL APb APb50APb75 APbS APbM APbL\nParams\n(M)\nFLOPS\n(G)\nViP-Mo 36.5 56.7 38.6 23.439.7 48.439.2 59.7 41.4 25.542.3 51.7 5.3 (15.2)15 (166)\nR18 [44] 31.8 49.6 33.6 16.3 34.3 43.235.4 53.9 37.6 19.5 38.2 46.811.0 (21.3) 37 (189)\nPVT-T [64] 36.7(+4.9) 56.9 38.922.6 38.8 50.039.4(+4.0) 59.8 42.025.5 42.0 52.112.3 (23.0) 70 (221)\nViP-Ti 39.7(+7.9) 60.6 42.2 23.942.9 53.041.6(+6.2) 62.6 44.0 27.245.1 54.211.2(21.4)29 (181)\nR50 [44] 36.5 55.4 39.1 20.4 40.3 48.139.0 58.4 41.8 22.4 42.8 51.623.3 (37.7) 84 (239)\nPVT-S [64] 40.4(+3.9) 61.3 43.025.0 42.9 55.742.2(+3.2) 62.7 45.026.2 45.2 57.223.6 (34.2) 134 (286)\nViP-S 43.0(+6.5) 64.0 45.9 28.946.7 56.344.0(+5.0) 65.1 47.2 28.847.3 57.229.0(39.9)75 (227)\nR101 [44] 38.5 57.8 41.2 21.4 42.6 51.140.9 60.1 44.0 23.7 45.0 53.842.3 (56.7) 160 (315)\nX101-32 [44]39.9(+1.4) 59.6 42.722.3 44.2 52.541.4(+0.5) 61.0 44.323.9 45.5 53.741.9 (56.4) 164 (319)\nPVT-M [64]41.9(+3.4) 63.1 44.325.0 44.9 57.643.2(+2.3) 63.8 46.127.3 46.3 58.943.7 (54.3) 222 (374)\nViP-M 44.3(+5.8) 65.9 47.4 30.748.0 57.945.3(+4.4) 66.4 48.5 29.748.6 59.348.8(59.8)135(287)\nX101-64 [44]41.0 60.9 44.0 23.9 45.2 54.041.8 61.5 44.4 25.2 45.4 54.681.0 (95.5) 317 (473)\nPVT-L [64] 42.6 63.7 45.4 25.8 46.0 58.443.4 63.6 46.1 26.1 46.0 59.560.9 (71.5) 324 (476)\nTable 5: Various backbones with RetinaNet. Here R and X are abbreviations for ResNet and ResNeXt.\nParameters and FLOPS in black are for backbones, while those in (gray) are for the whole frameworks.\nscheduler. The learning rate is warmed up for 20 epochs to reach the initial 1 √ó10‚àí3. Weight decays\nfor ViP-Mo, ViP-Ti are set to be 0.03, while those for ViP-S, ViP-M are 0.05. The drop ratios of\nStochastic Depth (a.k.a DropPath) [34] are linearly scaled from 0 to 0.1, 0.1, 0.2, 0.3 along the layer\ndepth for each layer of ViP-Ti, ViP-S, ViP-M and ViP-B respectively. We do not apply Stochastic\nDepth to ViP-Mo during training. The total training batch size is set to be 1024 for all model variants.\nWe primarily follow the settings of the data augmentation adopted in [ 59], except for the repeat\naugmentation as we found it unhelpful towards the Ô¨Ånal prediction. Note that for all results for image\nclassiÔ¨Åcation on ImageNet reported in this paper, we did not use any external dataset for pre-training.\nViP vs. CNNs. Table 2 compares ViP family with some of the state-of-the-art CNN models on\nImageNet. The RegNet is also better tuned using training tricks in [ 59]. ViP is both cost-efÔ¨Åcient\nand parameter-efÔ¨Åcient compared to these state-of-the-art models. For example, ViP-M can achieve\na competitive 83.3% with only 49.6M parameters and 8.0G FLOPS. The counterpart BOTNet-T5\nneeds 25.5M more parameters and 11.3G FLOPS to achieve similar performance. When scaling the\ninput to resolution 3842, ViP-B is able to further improve its top-1 accuracy to 84.2%.\nViP vs. ViT/DeiT.We Ô¨Årst compare ViP to the token-based Vision Transformer (ViT), which radically\nreduces the image resolution at the beginning of the network. When both networks are trained from\nscratch on the training set of ImageNet-1K, ViP-Ti can outperform ViT-B by 1.1% with only about 1\n7\nof its number of parameters and a fraction of its FLOPS.\nAnother token-based vision transformer, DeiT, is also listed in Table 2 for comparison. The basic\nstructure of DeiT is identical to what was proposed in ViT but is trained with more data augmentations\nand regularization techniques. When compare ViP with DeiT, we observe that ViP-Ti can surpass\nDeiT-Tiny by a signiÔ¨Åcant 6.8%. As for small models like ViP-S, it can outperform DeiT-S by\n2.1%, which is even better than a way larger variant DeiT-B of the DeiT family. The remarkable\nimprovement on ImageNet demonstrates the importance of retaining the local features within the\nnetwork for image recognition.\nViP vs. HaloNet. To the best of our knowledge, HaloNet [63] is the current state-of-the-art network\non ImageNet-1K. Figure 4 shows the speed-accuracy Pareto curve of the ViP family compared to the\nHaloNet family. Note that the HaloNet is re-implemented by us on PyTorch. As shown in Figure 4,\nViP achieves better speed-accuracy trade-off than HaloNet. Concretely, ViP-S is 6.3√ófaster than\nHaloNet-H4 with similar top-1 accuracy on ImageNet-1K.\nViP vs. other state-of-the-art Transformers. As shown in Table 2, ViP consistently outperforms\nprevious state-of-the-art Transformer-based models in terms of accuracy and model size. Especially,\nViP-B achieves 83.8% ImageNet top-1 accuracy, which is 0.5% higher than Swin-B [46] with fewer\nparameters and FLOPS. A similar trend can also be observed when scaled onto larger models, e.g.\nViP-M achieves 83.3% top-1 accuracy, outperforming TNT-B [23], T2T-ViT-24 [71], PVT-Large\n[64] by 0.4%, 1.0%, 1.6% respectively.\n7\nBackboneAPb APb50APb75 APb\nS APb\nM APb\nL APm APm50APm75 APm\nS APm\nM APm\nL\nParams\n(M)\nFLOPS\n(G)\nViP-Mo 42.6 61.8 46.0 27.1 44.9 56.437.7 59.2 40.1 22.5 39.8 51.0 5.9 (63.9) 16 (665)\nR18 38.7 56.2 41.3 21.3 40.8 52.934.0 53.5 36.4 17.4 36.0 48.111.0 (69.0) 37 (686)\nViP-Ti 45.4(+6.7) 64.6 48.9 29.1 48.8 60.139.9(+5.9) 61.7 42.7 24.1 43.0 54.311.2 (69.2) 29 (678)\nR50 41.2 59.4 45.0 23.9 44.2 54.435.9 56.6 38.4 19.4 38.5 49.323.3 (82.0) 84 (739)\nS50 45.4 64.1 49.2 28.3 49.1 58.839.5 61.4 42.5 23.1 43.0 52.825.1 (82.9) 110 (763)\nViP-S 48.5(+7.3) 67.5 52.5 31.9 51.8 63.242.2(+6.3) 64.8 45.7 25.9 45.5 56.729.0 (87.1) 75 (725)\nR101 42.9 61.0 46.6 24.4 46.5 57.037.3 58.2 40.1 19.7 40.6 51.542.3 (101.0) 160 (815)\nX101-32 44.3 62.7 48.4 25.4 48.4 58.138.3 59.7 41.2 20.6 42.0 52.341.9 (100.6) 164 (819)\nS101 47.7 66.4 51.9 30.1 51.8 61.441.4 63.7 45.1 24.7 45.2 54.945.7 (104) 209 (862)\nViP-M 49.9(+7.0) 69.5 54.2 33.1 53.4 65.143.5(+6.2) 66.4 47.2 26.8 46.9 59.148.8(107.0)135(785)\nX101-64 45.3 63.9 49.6 26.7 49.4 59.939.2 61.1 42.2 21.6 42.8 53.781.0 (139.7) 317 (972)\nTable 6: Various backbones with Cascade Mask R-CNN. All results are trained under1√óschedule.\nHere S denotes ResNeSt [72].\n4.2 Object Detection and Instance Segmentation\nExperimental settings. For object detection and instance segmentation, we evaluate ViP on the\nchallenging MS COCO dataset [42], which contains 115k images for training (train-2017) and 5k\nimages (val-2017) for validation. We train models on train-2017 and report the results on val-2017.\nWe measure our results following the ofÔ¨Åcial deÔ¨Ånition of Average Precision (AP) metrics given by\nMS COCO, which includes AP50 and AP75 (averaged over IoU thresholds50 and 75) and APS, APM ,\nAPL (AP at scale Small, Medium and Large). Annotations of MS COCO include both bounding\nboxes and polygon masks for object detection and instance segmentation respectively. Experiments\nare implemented based on the open source mmdetection [8] platform. All models are trained under\ntwo different training schedules 1√ó(12 epochs) and 3√ó(36 epochs) using the AdamW [39] optimizer\nwith the same weight decay set for image classiÔ¨Åcation. After a 500 iteration‚Äôs warming-up, the\nlearning rate is initialized at 1 √ó10‚àí4 then decayed by 0.1 after [8, 11] and [27, 33] epochs for 1√ó\nand 3√órespectively. For data augmentation, we only apply random Ô¨Çipping with a probability of 0.5\nand scale jittering from 640 to 800. The batch size for each GPU is 2 and we use 8 GPUs to train the\nnetwork for all experiments. Stochastic Depth is also applied here as what proposed on ImageNet. We\nembed ViP into two popular frameworks for object detection and instance segmentation, RetinaNet\n[44] and Cascade Mask-RCNN [6]. When incorporating ViP into these frameworks, ViP serves as\nthe backbone followed by a Feature Pyramid Network (FPN) [ 43] reÔ¨Åning the multi-scale whole\nrepresentations. All weights within the backbone are Ô¨Årst pre-trained on ImageNet-1K, while those\noutside the backbone are initialized with Xavier [22].\nViP can outperform ResNe(X)t with 4√óless computational cost in RetinaNet. Table 5 exhibits\nthe experimental results when embedding different backbones into RetinaNet. When trained under\nthe 1√óschedule, our ViP-Ti can outperform its counterpart ResNet-18 by 7.9, which is a large\nmargin since it is even higher than the performance obtained by ResNet-101 (4√ólarger than ViP-Ti\nin terms of FLOPS and parameters). The ViP-S, which is just about the size of ResNet-50, can even\noutperforms the largest model ResNeXt-101-64√ó4d listed in Table 5 by a clear2.0. For larger variant\nViP-M, it can further boost the performance to a higher level44.3. The performance of the ViP family\ncan be steadily boosted by the longer 3√ótraining schedule. As shown in Table 5, all variants of the\nViP family can retain their superiority compared with their ResNe(X)t and PVT counterparts.\nViP-Tiny can be comparable with the largest variant of ResNe(X)t family in Cascade Mask\nRCNN. Table 6 shows the results when incorporating different backbones into Cascade Mask RCNN\n[6]. As shown in Table 6, when trained under 1√óschedule, all variants of the ViP family can achieve\nsigniÔ¨Åcantly better performance compared to their counterparts. Notably, as a tiny model with only\n11.2M parameters and 29G FLOPS, ViP-Ti can achieve comparable performance obtained by the\nlargest variant in ResNe(X)t family ResNeXt-101-64√ó4d which contains 81M parameters and 317G\nFLOPS. ViP also scales well with larger models. ViP-M further lifts the performance to 49.9 for\nobject detection and 43.5 for instance segmentation. When compared with state-of-the-art variants\nof ResNet family like ResNeSt [72], ViP can also outperform them by a clear margin. SpeciÔ¨Åcally,\nViP-S and ViP-M outperforms ResNeSt-50 and ResNeSt-101 by3.1 and 2.2 respectively on object\ndetection and 2.7 and 2.1 on instance segmentation.\n8\nBlock 3 Block 4 Block 5\nFigure 5: Visualization results about where the part representations attend on. Pixels rendered in\ndifferent colors are associated to different parts. Best viewed in color.\n4.3 Ablation studies\nNumber of parts: As shown in Table 3, the number of partsN is crucial when the model is small.\nConcretely, for ViP-Ti,N=32 can lead to a remarkable 0.8% improvement on ImageNet compared to\nN=16. However, the improvement comes to be saturated when adding more parts into the network.\nEffects of the part-wise linear. Different from the original design in Transformer [62] that uses a\nself-attention module for part-wise communication, we replace it with a simple linear operation to\nsave the computational cost. Introducing such a simple linear operation into ViP-S can lead to a 0.4%\ngain on ImageNet with only a fractional increase in parameters (0.03M) and FLOPS (0.02G).\nPredicting on parts vs. wholes. As ViP has two levels, we can choose either of them for the Ô¨Ånal\nprediction. When using the parts for the Ô¨Ånal prediction, we employ an additional encoder on top of\nthe whole before the Ô¨Ånal global pooling and fully connected layer to gather all parts obtained by the\nencoder. Otherwise, we replace the encoder with a linear projection. Table 4 compares the results\nof predicting on the part representation. For small models like ViP-Ti and ViP-small, to predict on\nparts can lead to remarkable improvements (+0.7% and +0.4%). However, predicting on part level\nencountered an overÔ¨Åtting problem when incorporated into ViP-M with a 0.6% drop.\n4.4 Visualization\nThe visualization results of a pre-trained ViP-S are shown as an example in Figure 5. We average\nall heads of the afÔ¨Ånity matrix M in Eq. (2) and then normalize it to [0, 255]. For each image, we\nvisualize two parts in total for clearness. It can be observed that the attention maps tend to cover more\narea in the shallow blocks, and then gradually focus on salient objects through multiple iterations. The\nobservation suggests that the part encoder can effectively aggregate features from a part of the image,\nand the part representations can be reÔ¨Åned with the information from the whole. The visualization\nresults show that a meaningful part-whole hierarchy is constructed by the proposed ViP.\n9\n5 Conclusion\nIn this work, we construct a framework that includes different levels of representations called Visual\nParser (ViP). ViP divides visual representations into the part level and the whole level with a novel\nencoder-decoder interaction. Extensive experiments demonstrate that the proposed ViP can achieve\nvery competitive results on three major vision tasks. Particularly, ViP outperforms the previous\nstate-of-the-art CNN backbones by a large margin on object detection and instance segmentation.\nVisualization results also indicate that the learned part representations are highly informative to the\npredicting classes. As the Ô¨Årst step towards learning multi-level part-whole representations, our ViP\nis more explainable compared to previous architectures and shows great potential in visual modeling.\nAcknowledgement. We would like to thank Pau de Jorge, Francesco Pinto, Hengshuang Zhao and\nXiaojuan Qi for proof-reading and helpful comments. This work is supported by the ERC grant\nERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant\nEP/N019474/1. We would also like to thank the Royal Academy of Engineering and FiveAI.\n6 Appendix\n6.1 Network SpeciÔ¨Åcation of ViP\nStage L ViP-Mobile ViP-Tiny ViP-Small ViP-Medium ViP-Base\n7√ó7,64, Conv, stride 2;3√ó3Max Pool, stride 2\nStage1 HW\n42\nPatch Embedding,48 Patch Embedding,64 Patch Embedding,96 Patch Embedding,96 Patch Embedding,128[C1 = 48\nN1 = 16\nG1 = 1\n]\n√ó1\n[C1 = 64\nN1 = 32\nG1 = 1\n]\n√ó1\n[C1 = 96\nN1 = 64\nG1 = 1\n]\n√ó1\n[C1 = 96\nN1 = 64\nG1 = 1\n]\n√ó1\n[C1 = 128\nN1 = 64\nG1 = 1\n]\n√ó1\nStage2 HW\n82\nPatch Embedding,96 Patch Embedding,128Patch Embedding,192Patch Embedding,192Patch Embedding,256[C2 = 96\nN2 = 16\nG2 = 2\n]\n√ó1\n[C2 = 128\nN2 = 16\nG2 = 2\n]\n√ó1\n[C2 = 192\nN2 = 16\nG2 = 2\n]\n√ó1\n[C2 = 192\nN2 = 16\nG2 = 2\n]\n√ó1\n[C2 = 256\nN2 = 16\nG2 = 2\n]\n√ó1\nStage3 HW\n162\nPatch Embedding,192Patch Embedding,256Patch Embedding,384Patch Embedding,384Patch Embedding,512[C3 = 192\nN3 = 16\nG3 = 4\n]\n√ó1\n[C3 = 256\nN3 = 32\nG3 = 4\n]\n√ó2\n[C3 = 384\nN3 = 64\nG3 = 12\n]\n√ó3\n[C3 = 384\nN3 = 64\nG3 = 12\n]\n√ó8\n[C3 = 512\nN3 = 128\nG3 = 16\n]\n√ó8\nStage4 HW\n322\nPatch Embedding,384Patch Embedding,512Patch Embedding,768Patch Embedding,768Patch Embedding,1024[C4 = 384\nN4 = 32\nG4 = 8\n]\n√ó1\n[C4 = 512\nN4 = 32\nG4 = 8\n]\n√ó1\n[C4 = 768\nN4 = 64\nG4 = 24\n]\n√ó1\n[C4 = 768\nN4 = 64\nG4 = 24\n]\n√ó1\n[C4 = 1024\nN4 = 128\nG4 = 32\n]\n√ó1\n(C4 = 384\nN4 = 32\nG4 = 8\n)\n√ó1\n(C4 = 512\nN4 = 32\nG4 = 8\n)\n√ó1\n(C4 = 768\nN4 = 64\nG4 = 24\n)\n√ó1 768, Linear\n768, BatchNorm\n1024, Linear\n1024, BatchNorm\nGlobal Average Pool;1000, Linear\nTable 7: Detailed speciÔ¨Åcation for ViP family. Contents in [¬∑] represents the basic building block\nof ViP, and those in(¬∑) are just part encoders. Note that we only apply a part decoder at the end of\nnetwork to make the Ô¨Ånal prediction on part level in small models like ViP-Mo/Ti/S.\nThe general speciÔ¨Åcation of ViP is illustrated in Section 2.4. Here in Table 7 we show the detailed\nstructure of different variants of the ViP family. Note that for small models including ViP-Mobile,\nViP-Tiny, and ViP-Small, we apply another encoder at the end of the network but replacing MLP with\nthe activation function GELU to predicting on the part level. While for larger models ViP-Medium\nand ViP-Base, we replace the encoder with a linear projection (with Batch Normalization [36]) so\nthat it can predict on the whole level.\n6.2 More Visualization Results\nAdditional visualization results are shown in Figure 6. We follow the visualization method mentioned\nin Section 4.4, and it is obvious that the proposed part encoder is robust and works well even for the\ncomplex scene.\n10\nBlock 3 Block 4 Block 5\nFigure 6: More visualized results. All results are visualized using the same method proposed in\nSection 4.4.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016. 3\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder\narchitecture for image segmentation. TPAMI, 2017. 5\n[3] Irwan Bello. Lambda networks: Modeling long-range interactions without attention. ICLR, 2021. 5, 6\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented\nconvolutional networks. In ICCV, 2019. 5, 6\n[5] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\nrecognition without normalization. arXiv preprint arXiv:2102.06171, 2021. 5\n11\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In CVPR,\npages 6154‚Äì6162, 2018. 2, 5, 8\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 3, 5, 6\n[8] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019. 8\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Semantic\nimage segmentation with deep convolutional nets and fully connected CRFs. ICLR, 2015. 5\n[10] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng.a2-nets: Double attention\nnetworks. arXiv preprint arXiv:1810.11579, 2018. 6\n[11] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Shuicheng Yan, Jiashi Feng, and Yannis Kalantidis.\nGraph-based global reasoning networks. In CVPR, 2019. 6\n[12] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit position\nencodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021. 6\n[13] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention\nand convolutional layers. In ICLR, 2020. 6\n[14] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a Ô¨Åxed-length context. In ACL, 2019. 5\n[15] St√©phane d‚ÄôAscoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021. 6\n[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR, 2009. 5, 6\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT, 2019. 5\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 6\n[19] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph\nFeichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227, 2021. 6\n[20] Ross Girshick. Fast R-CNN. In ICCV, 2015. 5\n[21] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate\nobject detection and semantic segmentation. In CVPR, 2014. 5\n[22] Xavier Glorot and Yoshua Bengio. Understanding the difÔ¨Åculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on artiÔ¨Åcial intelligence and statistics,\npages 249‚Äì256. JMLR Workshop and Conference Proceedings, 2010. 8\n[23] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021. 6, 7\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016. 5\n[25] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 5\n[26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016. 4\n[27] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\nRethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021. 6\n[28] Geoffrey Hinton. Some demonstrations of the effects of structural descriptions in mental imagery.Cognitive\nScience, 3(3):231‚Äì250, 1979. 1\n12\n[29] Geoffrey Hinton. How to represent part-whole hierarchies in a neural network. arXiv preprint\narXiv:2102.12627, 2021. 1, 5\n[30] Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In International\nconference on learning representations, 2018. 5\n[31] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional\ntransformers. arXiv:1912.12180, 2019. 6\n[32] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection.\nIn CVPR, 2018. 6\n[33] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018. 6\n[34] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In ECCV, pages 646‚Äì661. Springer, 2016. 7\n[35] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolu-\ntional networks. In CVPR, 2017. 5\n[36] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In ICML, 2015. 10\n[37] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic Ô¨Ålter networks. In NIPS, 2016. 6\n[38] Daniel Kahneman, Anne Treisman, and Brian J Gibbs. The reviewing of object Ô¨Åles: Object-speciÔ¨Åc\nintegration of information. Cognitive psychology, 24(2):175‚Äì219, 1992. 1\n[39] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\n6, 8\n[40] Adam R Kosiorek, Sara Sabour, Yee Whye Teh, and Geoffrey E Hinton. Stacked capsule autoencoders.\narXiv preprint arXiv:1906.06818, 2019. 5\n[41] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep convolutional\nneural networks. In NeurIPS, 2012. 5\n[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740‚Äì755. Springer,\n2014. 8\n[43] Tsung-Yi Lin, Piotr Doll√°r, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie.\nFeature pyramid networks for object detection. In CVPR, 2017. 8\n[44] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. Focal loss for dense object\ndetection. In ICCV, 2017. 5, 7, 8\n[45] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and\nAlexander C. Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 5\n[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021. 6, 7\n[47] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob\nUszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. arXiv\npreprint arXiv:2006.15055, 2020. 5\n[48] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In CVPR, 2015. 5\n[49] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll√°r. Designing network\ndesign spaces. In CVPR, 2020. 5, 6\n[50] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019. 6\n[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In NeurIPS, 2015. 5\n13\n[52] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. arXiv preprint\narXiv:1710.09829, 2017. 5\n[53] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. In ICLR, 2015. 5\n[54] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. arXiv:2101.11605, 2021. 2, 5, 6\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In\nNeurIPS, 2015. 5\n[56] Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, and Wanli Ouyang. Fishnet: A versatile backbone\nfor image, region, and pixel level prediction. arXiv preprint arXiv:1901.03495, 2019. 5\n[57] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. In CVPR, 2015. 5\n[58] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model scaling for convolutional neural networks. In\nICML, 2019. 5\n[59] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√©\nJ√©gou. Training data-efÔ¨Åcient image transformers & distillation through attention. arXiv:2012.12877, 2020.\n6, 7\n[60] Yao-Hung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov. Capsules with inverted\ndot-product attention routing. arXiv preprint arXiv:2002.04764, 2020. 5\n[61] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-Chun Zhu. Image parsing: Unifying segmentation,\ndetection, and recognition. International Journal of computer vision, 63(2):113‚Äì140, 2005. 5\n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 4, 5, 9\n[63] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efÔ¨Åcient visual backbones. arXiv preprint\narXiv:2103.12731, 2021. 6, 7\n[64] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021. 6, 7\n[65] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,\n2018. 2, 6\n[66] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer,\nand Peter Vajda. Visual transformers: Token-based image representation and processing for computer\nvision. arXiv:2006.03677, 2020. 6\n[67] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. In ICLR, 2019. 5\n[68] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021. 6\n[69] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. XLNet:\nGeneralized autoregressive pretraining for language understanding. In NeurIPS, 2019. 5\n[70] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. ICLR, 2016. 5\n[71] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021. 6, 7\n[72] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas\nMueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020. 8\n[73] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\nnetwork. In CVPR, 2017. 5\n[74] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR,\n2020. 6\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8004218935966492
    },
    {
      "name": "Parsing",
      "score": 0.7798716425895691
    },
    {
      "name": "Encoder",
      "score": 0.6195312738418579
    },
    {
      "name": "Transformer",
      "score": 0.5858491659164429
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5718812346458435
    },
    {
      "name": "Segmentation",
      "score": 0.5093053579330444
    },
    {
      "name": "ENCODE",
      "score": 0.500687837600708
    },
    {
      "name": "Natural language processing",
      "score": 0.46244317293167114
    },
    {
      "name": "Visualization",
      "score": 0.45590585470199585
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4123039245605469
    },
    {
      "name": "Machine learning",
      "score": 0.2685399651527405
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}