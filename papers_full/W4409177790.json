{
    "title": "A Bilingual On-premise AI agent for Clinical Drafting: Seamless EHR integration in the Y-KNOT Project",
    "url": "https://openalex.org/W4409177790",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5102972169",
            "name": "Hanjae Kim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100714393",
            "name": "So‐Yeon Lee",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5063502620",
            "name": "Seng Chan You",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5104359826",
            "name": "Sookyung Huh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5103036394",
            "name": "Jai-Eun Kim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5090755281",
            "name": "Sung-Tae Kim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5114130483",
            "name": "Dong-Ryul Ko",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100410791",
            "name": "Ji Hoon Kim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100387451",
            "name": "Jae Hoon Lee",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5054897594",
            "name": "Joon Seok Lim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5062914526",
            "name": "Moo Suk Park",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5088184916",
            "name": "Kang Young Lee",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4403420208",
        "https://openalex.org/W4405594071",
        "https://openalex.org/W4401567177",
        "https://openalex.org/W3023680935",
        "https://openalex.org/W4220962859",
        "https://openalex.org/W4393119065",
        "https://openalex.org/W4392193048",
        "https://openalex.org/W4386120650",
        "https://openalex.org/W4407150885",
        "https://openalex.org/W4401306886",
        "https://openalex.org/W4405736820",
        "https://openalex.org/W2972522091",
        "https://openalex.org/W4392537688",
        "https://openalex.org/W4399737726",
        "https://openalex.org/W4391494845",
        "https://openalex.org/W2027197625",
        "https://openalex.org/W2767996342",
        "https://openalex.org/W4376134609",
        "https://openalex.org/W4393392853",
        "https://openalex.org/W4404961577",
        "https://openalex.org/W4317790370",
        "https://openalex.org/W4404637131"
    ],
    "abstract": "Abstract Large Language Models (LLMs) have shown promise in reducing clinical documentation burden, yet their real-world implementation faces significant challenges, particularly in non-English speaking countries with strict data sovereignty requirements. Here we present Your-Knowledgeable Navigator of Treatment (Y-KNOT), the first successful implementation of an on-premise bilingual LLM-based artificial intelligence system integrated with electronic health records (EHR) for automated clinical documentation. In collaboration with multiple stakeholders, we developed and deployed Y-KNOT at a tertiary hospital in South Korea. The system processes emergency department discharge summaries and pre-anesthetic assessments with high evaluation scores across multiple clinical metrics while maintaining FHIR compliance for scalability. Our study demonstrates a practical framework for implementing LLM-based clinical documentation systems in resource-constrained healthcare settings while addressing key challenges of data security, bilingual requirements, and workflow integration.",
    "full_text": "A Bilingual On-premise AI agent for Clinical Drafting: Seamless EHR integration in the \nY-KNOT Project \n \nAuthors: Hanjae Kim, B.S.,1 So-Yeon Lee, M.D., Ph.D., 2,3 Seng Chan You, M.D., Ph.D.,1,2,3 \nSookyung Huh, M.S.,4 Jai-Eun Kim, Ph.D.,5 Sung-Tae Kim, M.S., 5 Dong-Ryul Ko, M.S.,5 Ji \nHoon Kim, M.D., Ph.D.,2,6 Jae Hoon Lee, M.D., Ph.D.,7 Joon Seok Lim, M.D., Ph.D.,8 Moo \nSuk Park, M.D., Ph.D.,9 Kang Young Lee, M.D., Ph.D.10 \n \n1 Department of Biomedical Systems Informatics, Yonsei University College of Medicine, \nSeoul, Republic of Korea \n2 PHI Digital Healthcare, Seoul, Republic of Korea \n3 Institute for Innovation in Digital Healthcare, Yonsei University, Seoul, Republic of Korea \n4 Department of Medical Records, Severance Hospital, Yonsei University Health System, \nSeoul, Republic of Korea \n5 Saltlux Inc., Seoul, Republic of Korea \n6 Department of Emergency Medicine, Yonsei University College of Medicine, Seoul, \nRepublic of Korea \n7 Department of Anesthesiology and Pain Medicine, Anesthesia and Pain Research Institute, \nYonsei University College of Medicine, Seoul, Republic of Korea \n8 Department of Radiology, Yonsei University College of Medicine, Seoul, Korea \n9 Division of Pulmonary and Critical Care Medicine, Department of Internal Medicine, \nYonsei University College of Medicine, Seoul, Republic of Korea \n10 Department of Surgery, Yonsei University College of Medicine, Seoul, Republic of Korea \n \nAddress for Correspondence: \nSeng Chan You, M.D., Ph.D. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nDepartment of Biomedical Systems Informatics, Yonsei University College of Medicine, 50-\n1, Yonsei-Ro, Seodaemun-gu, Seoul, Republic of Korea \nTel: +82-2228-2500 \nE-mail: chandryou@yuhs.ac \n \n \nAbstract \nLarge Language Models (LLMs) have shown promise in reducing clinical documentation \nburden, yet their real-world implementation faces significant challenges, particularly in non-\nEnglish speaking countries with strict data sovereignty requirements. Here we present Your-\nKnowledgeable Navigator of Treatment (Y-KNOT), the first successful implementation of an \non-premise bilingual LLM-based artificial intelligence system integrated with electronic \nhealth records (EHR) for automated clinical documentation. In collaboration with multiple \nstakeholders, we developed and deployed Y-KNOT at a tertiary hospital in South Korea. The \nsystem processes emergency department discharge summaries and pre-anesthetic assessments \nwith high evaluation scores across multiple clinical metrics while maintaining FHIR \ncompliance for scalability. Our study demonstrates a practical framework for implementing \nLLM-based clinical documentation systems in resource-constrained healthcare settings while \naddressing key challenges of data security, bilingual requirements, and workflow integration. \n \nKeywords: AI Agent, Large Language Models, Documentation, Electronic Health Records, \nClinical Note, Clinical Workflow, On-premise, Data Sovereignty, Insights, Methodology \n \n \nINTRODUCTION \nLarge Language Models (LLMs) have recently garnered significant attention, raising \nexpectations for their applications in healthcare systems.1,2 However, most research has \nfocused on implementations in the United States, and has mainly address tasks related to \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nmedical knowledge.3 \nSouth Korea's efficient healthcare system balances low costs with high accessibility and \nquality. However, this efficiency comes with inherent challenges in resource allocation. \nHealthcare providers manage substantial workloads, seeing many patients in limited time \nframes, which has been particularly exacerbated by recent mass resignation of residents.\n4,5 \nClinical documentation represents a significant burden for healthcare providers,6,7 and there is \ngrowing optimism about LLMs' potential to alleviate this burden.8 Clinical documentation \ninvolves condensing previous records, a task LLMs excel at.9,10 However, implementing \nLLMs in South Korea faces several unique challenges. Korean medical regulations mandate \nthat all medical records be stored on domestic servers,11 making it impossible to utilize \nforeign commercial services like ChatGPT.12 Additionally, medical documents in Korea often \nexhibits bilingual usage of Korean and English, adding further complexity.  \nAlthough some projects have attempted incorporating LLMs within electronic health records \n(EHRs), full-scale integration in real clinical settings remains rare. Due to their separate \ninterface, manually retrieving information from EHRs and typing it into LLMs may ironically \ntime-consuming. In the study by Goh et al.,\n13 interaction with LLM led to increased time in \npatient management reasoning. For LLMs to be effectively utilized by healthcare providers, \nconnecting LLMs directly to EHRs is necessary.   \nTo address these challenges, we initiated the Your-Knowledgeable Navigator of Treatment \n(Y-KNOT) project, aimed at developing an artificial intelligence (AI) agent that seamlessly \nintegrates a bilingual small LLM with existing systems for automatic clinical drafting. This \nproject demonstrates a practical approach to leveraging LLMs within the constraints of \nhealthcare system, providing valuable insights to other institutions on integrating AI-assisted \nclinical drafting tools while ensuring regulatory compliance and addressing specific linguistic \nrequirements. \n \n \nMETHODS \nThe Y-KNOT project was conducted at Severance Hospital, a tertiary hospital in Seoul, South \nKorea. Initiated in June 2024, it launched its first service in routine clinical practice in \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nNovember 2024. The project encompassed three major phases: medical foundation LLM \ndevelopment, clinical co-development, and EHR integration, which were carried out \nsimultaneously. Fig. 1 displays the overall project landscape. \n \nDevelopment of Medical Foundation LLM \nWe first developed ‘Y-KNOT-med-base’, a bilingual, small LLM for general medical \npurpose. We used Luxia 2 developed by ‘Saltlux Inc.’ (Seoul, South Korea) as a base model, \nwhich was built upon Llama 3 (8 billion parameters)14 and specialized for Korean through \npre-training on 1.5 terabytes of general corpus datasets. To adept the model for medical \napplication, we further trained it with 90 gigabytes (GB) of medical and 9 GB of general \ncorpus datasets in Korean and English. All datasets were sourced from outside of the hospital \nto prevent sensitive data leakage from adversarial attacks15 in case the project is adopted by \nother institutions in the future.  \nTo assess its capability to understand medical knowledge, we evaluated ‘Y-KNOT-med-base’ \non PubMedQA16 for English and KorMedMCQA17 for Korean. We used 5-shot learning for \nboth benchmarks and compared the results with other baseline models. \n \nClinical Co-development Phase \nThe Y-KNOT project involved collaboration between related departments, including \nphysicians, data scientists, software engineers, and medical record specialists. Working \nclosely together, we established 6 core values: innovation, collaboration, integration, \nsovereignty, scrutiny, and efficiency (Fig. 2). \nWith the core values internalized, we conducted iterative cycles of defining clinical \ndocumentation needs, identifying available EHR data, assessing the technical feasibility of \nLLM implementation, and refining results through data adjustments and retraining of the \nmodel. Through these cycles, we refined our understanding of automatable document types, \naligning them with data availability and technical capabilities.  \nTo adapt the LLM for drafting specific document types, we instruction-tuned the Y-KNOT-\nmed-base. Emergency department (ED) discharge summaries and pre-anesthetic assessments \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nwere confirmed as target documents We call the resulting model as ‘Y-KNOT-MD’, which is \nan abbreviation for ‘Y-KNOT medical document’. Patients’ data for the model prompts were \nselected and anonymized from the hospital’s EHR to prevent the LLM from reproducing \npersonal information.18 The completions were prepared by physicians, fulfilling their needs \nwhile adhering to the principles provided from data scientists. Multiple iterations of \ninstruction-tuning, testing and refinement helped us confirm the document templates and \ndetermine the optimal approach for automation – whether through rule-based systems or \nLLM inference.  \nThis process was crucial in establishing a system that not only met immediate clinical needs \nbut also ensured standardization across departments while maintaining compliance with \nclinical requirements.  \n \nEHR Integration Phase \nIn parallel with other phases, we focused on seamlessly integrating the AI agent into existing \nEHR workflows by proceeding through three key components: medical document \nstandardization, service trigger point definition, and user interaction optimization. \nFirst, out of 2,201 different document forms from EHR, we standardized 989 forms based on \nFast Healthcare Interoperability Resource (FHIR)19 standards. The rest were excluded due to \ninconsistent usage, absence of textual content, or because they were related to surveys, \nreferrals, palliative care or physical therapies. This decision was reached after numerous \nmeetings with the medical records team and physicians. The standardization not only \nenhanced interoperability for existing documentation but also established a robust framework \nfor future development. \nSecond, we mapped precise trigger points for AI agent activation to ensure assistance without \ndisrupting existing clinical routines. The system supports both real-time triggers and batch \nprocessing. We carefully selected the optimal time for batch processing, as it could place \nextra load on the system, and tested system latency to ensure that the integration would not \nimpact the EHR's overall performance. \nThird, we established a user interaction framework that maximized efficiency while \npreserving physician control over final documentation. The interface enabled quick review \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nand editing of AI-generated content through intuitive controls, emphasizing minimal click \npaths to streamline the documentation process. \n \nPre-defined Clinical Evaluation Criteria \nBefore the deployment, we evaluated the qualities of automatically generated ED discharge \nsummaries and pre-anesthetic assessments. For each type of document, 2 physicians were \nprovided with 100 datasets consisting of medical records used as input and consequent output \ntexts generated by the agent. Physicians graded the outputs in terms of consistency, \ncoherence, fluency, relevance, safety, subjective satisfactory rate (SSR), and usability (Table \nS1). In addition, the impact on decision-making (IDM) was graded for pre-anesthetic \nassessments. All metrics were graded using 5-point Likert Scales, except for usability, which \nhad a maximum score of 4, and IDM, which had a maximum score of 3. Higher scores \nindicated better output quality for all metrics. Mean scores were calculated for all metrics, \nexcept for IDM, where the proportion for each score was calculated. \n \n \nRESULTS \nPerformance Evaluation of Medical Knowledge and Language Capabilities \nThe ‘Y-KNOT-med-base’ achieved an accuracy score of 75.2 on the PubMedQA. Despite its \nrelatively small size and absence of fine-tuning, the performance was comparable to state-of-\nthe-art baselines which were fine-tuned on larger parameter scales. The average accuracy \nscore was 55.8 on the KorMedMCQA, outperforming other multilingual pre-trained models \non all three exam categories (Table S2). \n \nAutomatic Drafting of Clinical Documents \nFor ED discharge summaries, the AI agent drafts past medical histories, reason for the visit, \nand the details of specialty consultations or treatments in one paragraph. In response to the \nurgent nature of the ED, the outputs are designed to be as concise as possible.  \nFor pre-anesthetic assessments, the agent drafts a patient’s background information required \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nfor preparing anesthesia, including basic information, past medical histories, medications, \nexamination results, and other specialty consultation histories. Contents requiring medical \njudgement, such as anesthesiologist’s opinion or pre-medication guides, were excluded as an \nLLM that makes medical judgements could be risky.  \nDetailed examples of input data and subsequent output contents are provided in Fig. 3. \n \nIntegration and Implementation in Clinical Practice   \nThe Y-KNOT service is currently deployed for real-world use. Since the model is fully \nintegrated into the EHR system, the drafting process is automatically triggered through two \nfamiliar physicians’ workflows. For acute care settings like ED, physicians can initiate \ndrafting by placing a “draft creation” order, similar to medication orders. For scheduled \nprocedures like surgeries, the system generates drafts in batch according to the predetermined \nschedule. Corresponding patients’ data are automatically fed into the LLM and physicians \ncan transfer auto-generated drafts to the target document with a simple click of a button (Fig. \nS1). By eliminating the manual interaction with LLM, significant time and effort can be \nsaved. This also could prevent potential risks of adversarial attacks by keeping users away \nfrom instructing the LLM.  \nWhen the drafting is initiated, relevant data in FHIR format are transmitted from the EHR \nserver to the Y-KNOT system, which processes them using the LLM and rule-based \napproaches. The system creates multiple prompts, each designed to extract specific aspects of \nthe document, and the LLM generates outputs which are eventually synthesized into a \ncomprehensive draft. This final draft is returned to the EHR for physician review, \nmodification, and approval. This automatic process (Fig. 4) operates through predefined \napplication programming interfaces (APIs) that specify the data exchange formats between \nsystem components. To ensure data sovereignty, all system components including servers and \ndatabases are located within the hospital's secure on-premise environment. \n \nClinical Performance and Impact Assessment \nThe mean scores graded on drafted ED discharge summaries were 4.78 for consistency, 4.60 \nfor coherence, 4.55 for fluency, 4.72 for relevance, 4.73 for safety, 3.95 for SSR, and 3.32 for \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nusability. The mean scores on drafted pre-anesthetic assessments were 3.29 for consistency, \n3.86 for coherence, 4.23 for fluency, 3.37 for relevance, 3.88 for safety, 3.14 for SSR, and \n2.58 for usability. Additionally, physicians graded 34.5% of pre-anesthetic assessment drafts \nto have positive impacts on decision-making and 49.0% to have no impacts, while 16.5% \nwere graded to have negative impacts (Fig. 5).  \n \n \nDISCUSSION \nThe Y-KNOT project demonstrates a successful implementation of a bilingual on-premise \nLLM-based clinical drafting system that seamlessly integrates with existing EHR workflows \nin a high-throughput healthcare setting. Through close collaboration with stakeholders, we \naddressed several critical challenges in implementing AI-assisted clinical drafting in \nhealthcare environments. \nWe decided to utilize an 8B parameter model for minimal latency in clinical settings, rapid \nproject completion, and environmental and economic sustainability. The balance between \nmodel size and performance is crucial, as larger models require substantial computational \nresources and costs. Although smaller models may have limitations in processing lengthy \ncontexts and complex medical information, proper instruction-tuning enable them to perform \nspecific tasks on par with larger models.\n20  \nOur implementation addresses the challenges of resource-limited healthcare settings. South \nKorea's healthcare system operates at significantly low costs, with the average cost per \noutpatient visit at tertiary hospitals being less than $15, whereas in the United States, it \nexceeds $100.21 This makes it unfeasible to deploy large-scale LLMs as the operational costs \nwould significantly exceed the revenue per visit. South Korea's healthcare system is also \nhighly efficient, with outpatient consultation times averaging merely 4.2 minutes22 compared \nto 20 minutes in the United States.23 This presented both an opportunity and a challenge: \nwhile it highlighted an urgent need for documentation assistance, it also demanded \nexceptional efficiency in implementation. We addressed this challenge through strategic EHR \nintegration, enabling documentation drafting to occur concurrently with other clinical tasks \nand maintaining the rapid pace of clinical practice. This approach demonstrates how AI can \nbe successfully integrated even in highly time-constrained, cost-sensitive clinical \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nenvironments. \nTo ensure scalable deployment across different institutions, we standardized all documents to \nFHIR format and implemented API-based data exchange. Through this approach, we created \na system that can be readily deployed to any EHR that adheres to FHIR standards, thereby \nproviding a blueprint for widespread implementation. This architectural decision not only \nensures interoperability but also reduces the technical barriers for other institutions wanting \nto implement similar AI-assisted systems. \nOur study has several limitations. First, we have not validated its performance across multiple \ninstitutions, thus not proving the generalizability of our approach and identify potential \ninstitution-specific requirements. Second, we have not yet conducted prospective studies to \nmeasure the system's impact on physician workload. Previous studies have raised concerns \nthat validating AI-generated outputs might paradoxically increase physician workload,\n24 \nmaking it crucial to evaluate the actual time savings through rigorous clinical studies.25,26 \nImpacts on clinical decision-making or patient outcomes should also be assessed through \nlong-term studies. Third, the financial implications remain to be fully understood. While there \nare expectations of cost benefits from AI implementation in healthcare,\n27 similar technologies \nlike ambient-listening AI have shown no significant financial advantages.28 Future research \nshould address these limitations through multi-center implementation, prospective evaluation \nof efficiency gains, clinical impacts, and cost-effectiveness. \n \n \nData availability \nSource data, including medical records from electronic health records, is not publicly \navailable due to the policy of the healthcare institution and privacy protection regulations. \nDatasets used as benchmarks are publicly accessible via the provided references. Raw scores \ngraded for clinical performance can be provided upon request to the authors.  \n \nCode availability \nThe code used in this study is not publicly available due to company policies and data \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nconfidentiality restrictions. \n \nAcknowledgements \nThe authors thank the following contributors at Yonsei University Health System for their \ntechnical assistance and advice to the Y-KNOT project. They did not receive any separate \ncompensation beyond their regular institutional responsibilities for these contributions: \nJihyun Yang and Jeeeun Jung at the Department of Medical Records, Division of Digital \nHealth; Eunhye Kang, Hyekyung Jung, Younghee Lim, and JaeHyeon Park at the Department \nof Information Services, Division of Digital Health; Young ah Kim, Heui seok Kang, and \nHyunsook Seong at the Department of Data Services, Division of Digital Health; Eun Jung \nKang, Kyung Han Kim, and Jong Myoung Kim at the Digital Health Strategy Team, Division \nof Digital Health, Yonsei University Health System, Seoul, Republic of Korea. \nFurthermore, the authors thank the members of the Data Science Department, PHI Digital \nHealthcare, Seoul, Republic of Korea, for their contributions to this project.  \n \n \nDisclosures \nSCY reports grants from Daiichi Sankyo. He is a coinventor of granted Korea Patent DP-\n2023-1223 and DP-2023-0920, and pending Patent Applications DP-2024-0909, DP-2024-\n0908, DP-2022-1658, DP-2022-1478, and DP-2022-1365 unrelated to current work. SCY is a \nchief executive officer of PHI Digital Healthcare. HK was an employee of PHI Digital \nHealthcare during this study. SYL is an employee of PHI Digital Healthcare. JEK, STK, and \nDRK are employees of Saltlux Inc. KYL serves as a general director of Severance Hospital, \nYonsei University Health System. Other authors have no potential conflicts of interest to \ndisclose. \nThis research was supported by PHI Digital Healthcare. \nT\nhis study was reviewed and approved by the Institutional Review Board (IRB No. 4-2023-\n003) and the Data Review Board (DRB No. 24-01-005) of Severance Hospital. \n \n \nAuthor contributions \nHK: Validation, Formal analysis, Investigation, Data Curation, Writing – Original Draft, \nWriting – Review & Editing, Visualization. SYL: Conceptualization, Methodology, \nValidation, Investigation, Writing – Original Draft, Writing – Review & Editing, Project \nadministration. SH: Resources, Writing – Review & Editing, Project administration. JHK: \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nMethodology, Validation, Writing – Review & Editing. JHL: Methodology, Validation, \nWriting – Review & Editing. JSL: Validation, Resources, Writing – Review & Editing, \nFunding acquisition. MSP: Validation, Writing – Review & Editing. KYL: Validation, \nResources, Writing – Review & Editing, Funding acquisition. JEK: Software, Formal \nanalysis, Data Curation, Writing – Review & Editing. STK: Software, Formal analysis, Data \nCuration, Writing – Review & Editing. DRK: Software, Formal analysis, Data Curation, \nWriting – Review & Editing. SCY: Conceptualization, Methodology, Investigation, \nResources, Writing – Original Draft, Writing – Review & Editing, Supervision, Project \nadministration, Funding acquisition. \n \n \nReferences \n1. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large \nlanguage models in medicine. Nat Med 2023;29(8):1930-1940. DOI: 10.1038/s41591-\n023-02448-8. \n2. Clusmann J, Kolbinger FR, Muti HS, et al. The future landscape of large language \nmodels in medicine. Communications Medicine 2023;3(1):141. DOI: \n10.1038/s43856-023-00370-1. \n3. Bedi S, Liu Y , Orr-Ewing L, et al. Testing and Evaluation of Health Care Applications \nof Large Language Models: A Systematic Review. JAMA 2024. DOI: \n10.1001/jama.2024.21700. \n4. Yoon J, Lee J-Y . Challenges Arising from Disruptions in Psychiatry Training: \nImplications of Residents’ Mass Resignation in South Korea. Academic Psychiatry \n2024. DOI: 10.1007/s40596-024-02108-0. \n5. Park J, Shin CH, Lee J- Y . Why Did All the Residents Resign? Key Takeaways From \nthe Junior Physicians’ Mass Walkout in South Korea. Journal of Graduate Medical \nEducation 2024;16(4):402-406. DOI: 10.4300/jgme-d-24-00227.1. \n6. Tajirian T, Stergiopoulos V , Strudwick G, et al. The Influence of Electronic Health \nRecord Use on Physician Burnout: Cross-Sectional Survey. J Med Internet Res \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \n2020;22(7):e19274. DOI: 10.2196/19274. \n7. Gaffney A, Woolhandler S, Cai C, et al. Medical Documentation Burden Among US \nOffice-Based Physicians in 2019: A National Study. JAMA Intern Med \n2022;182(5):564-566. DOI: 10.1001/jamainternmed.2022.0372. \n8. Haltaufderheide J, Ranisch R. The ethics of ChatGPT in medicine and healthcare: a \nsystematic review on Large Language Models (LLMs). NPJ Digit Med \n2024;7(1):183. DOI: 10.1038/s41746-024-01157-x. \n9. Van Veen D, Van Uden C, Blankemeier L, et al. Adapted large language models can \noutperform medical experts in clinical text summarization. Nature Medicine \n2024;30(4):1134-1142. DOI: 10.1038/s41591-024-02855-5. \n10. Tang L, Sun Z, Idnay B, et al. Evaluating large language models on medical evidence \nsummarization. npj Digital Medicine 2023;6(1):158. DOI: 10.1038/s41746-023-\n00896-7. \n11. Guidelines for the Standards on Facilities and Equipment Required for the \nManagement and Preservation of Electronic Medical Records. Korea Health \nInformation Service, 2022. (https://www.k-\nhis.or.kr/board.es?mid=a10306020000&bid=0016&act=view&list_no=614&tag=&nP\nage=1.) \n12. Introducing ChatGPT. OpenAI. 2022 ( https://openai.com/blog/chatgpt). \n13. Goh E, Gallo RJ, Strong E, et al. GPT- 4 assistance for improvement of physician \nperformance on patient care tasks: a randomized controlled trial. Nature Medicine \n2025. DOI: 10.1038/s41591-024-03456-y. \n14. Dubey A, Jauhri A, Pandey A, et al. The llama 3 herd of models. arXiv preprint \narXiv:240721783 2024. DOI: 10.48550/arXiv.2407.21783. \n15. Kim M, Kim Y , Kang HJ, et al. Fine- Tuning LLMs with Medical Data: Can Safety Be \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nEnsured? NEJM AI 2025;2(1):AIcs2400390. DOI: 10.1056/AIcs2400390. \n16. Jin Q, Dhingra B, Liu Z, Cohen WW, Lu X. Pubmedqa: A dataset for biomedical \nresearch question answering. arXiv preprint arXiv:190906146 2019. DOI: \n10.48550/arXiv.1909.06146. \n17. Kweon S, Choi B, Kim M, Park RW, Choi E. KorMedMCQA: Multi- Choice Question \nAnswering Benchmark for Korean Healthcare Professional Licensing Examinations. \narXiv preprint arXiv:240301469v2 2024. DOI: 10.48550/arXiv.2403.01469. \n18. Ong J, Chang SY- H, Wasswa W, et al. Medical Ethics of Large Language Models in \nMedicine. NEJM AI 2024;1. DOI: 10.1056/AIra2400038. \n19. HL7 FHIR. HL7 International. ( https://hl7.org/fhir/). \n20. Zhang T, Ladhak F, Durmus E, Liang P, McKeown K, Hashimoto TB. Benchmarking \nLarge Language Models for News Summarization. Transactions of the Association for \nComputational Linguistics 2024;12:39-57. DOI: 10.1162/tacl_a_00632. \n21. Lee J, Son K, Kang T. A Review of Outpatient Visit Trends in Korea and Other \nCountries. Research Institute for Healthcare Policy, Korean Medical Association \n2019. \n22. Lee CH, Lim H, Kim Y , Park AH, Park E-C, Kang J-G. Analysis of appropriate \noutpatient consultation time for clinical departments. Health Policy and Management \n2014;24(3):254-260. \n23. Irving G, Neves AL, Dambha- Miller H, et al. International variations in primary care \nphysician consultation time: a systematic review of 67 countries. BMJ Open \n2017;7(10):e017902. DOI: 10.1136/bmjopen-2017-017902. \n24. Preiksaitis C, Sinsky CA, Rose C. ChatGPT is not the solution to physicians' \ndocumentation burden. Nat Med 2023;29(6):1296-1297. DOI: 10.1038/s41591-023-\n02341-4. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \n25. Roberts K. Large language models for reducing clinicians' documentation burden. Nat \nMed 2024;30(4):942-943. DOI: 10.1038/s41591-024-02888-w. \n26. Landman AB, Tilak SS, Walker GA. Artificial Intelligence–Generated Emergency \nDepartment Summaries and Hospital Handoffs. JAMA Network Open \n2024;7(12):e2448729-e2448729. DOI: 10.1001/jamanetworkopen.2024.48729. \n27. Sahni N, Stein G, Zemmel R, Cutler DM. The Potential Impact of Artificial \nIntelligence on Healthcare Spending. National Bureau of Economic Research \nWorking Paper Series 2023;No. 30857 (presented at \"Economics of Artificial \nIntelligence Conference\", September 22-23, 2022). DOI: 10.3386/w30857. \n28. Liu T-L, Hetherington TC, Dharod A, et al. Does AI-Powered Clinical Documentation \nEnhance Clinician Efficiency? A Longitudinal Study. NEJM AI \n2024;1(12):AIoa2400659. DOI: doi:10.1056/AIoa2400659. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nFig. 1. Overall landscape of the Y-KNOT project \n \nEHR indicates electronic health record; GB, gigabytes; B, billions; LLM, large language model; ED, \nemergency department \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nFig. 2. Core values of the Y-KNOT project \n \n \nAI indicates artificial intelligence; LLM, large language model; EHR, electronic health record; IT, \ninformation technology \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nFig. 3. Examples of input data types and subsequent output contents of auto-generated drafts \n \nAll medical records used as input data are converted into FHIR standards. Criteria for selecting input \ndata are stated in parentheses. \nNote that the examples provided in the figure are simplified versions of the actual data. \nOCS indicates order communication system; LLM, large language model; ED, emergency department \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nFig. 4. Overview of the automated drafting process with the AI agent in the EHR system   \n \nEHR indicates electronic health record; FHIR, Fast Healthcare Interoperability Resource; LLM, large \nlanguage model; AI, artificial intelligence \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nFig. 5. Clinical evaluation results on drafts generated by the Y-KNOT AI agent   \n \nED indicates emergency department; SSR, subjective satisfactory rate \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nFig. S1. User interaction with the EHR system for automatic clinical drafting \n \na: An automatic clinical drafting is triggered by ordering a draft creation from the order \ncommunication system. In case of the batch processes triggered by scheduled procedures like \nsurgeries, this step could be skipped. \nb: As a physician opens a form for documentation, auto-generated drafts show up in the pop-up \nwindow. Selected draft is directly transferred to the form if the physician clicks the ‘text transfer’ \nbutton. Drafts then can be edited and saved on the document form. \nED indicates emergency department. \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nTable S1. Criteria for evaluating auto-generated drafts \nMetrics Range Criteria \nConsistency 1-5 The consistency of the information provided on the output. \nCoherence 1-5 The logical structure of the output in context. \nFluency 1-5 The appropriateness in grammatical, lexical, or structural aspects of the output. \nRelevance 1-5 The alignment of the output with the topic. \nSafety 1-5 The correctness of medical information in the output. \nSubjective satisfactory rate 1-5 Subjective measurement of overall satisfaction with the output. \nUsability 1-4 Whether the output can be provided to the user without modifications. \nImpact on decision-making 1-3 The extent to which the response influences medical judgment, categorized into three levels: \npositive, no impact, and negative \nA higher score indicates better quality of output in all metrics. \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint \nTable S2. Evaluation results of Y-KNOT-med-base along with other baseline models \nModel Accuracy \n PubMedQA \nMeditron-70B 81.6 \nPalmyra-Med-40B 81.1 \nAntGLM-Med-10B 80.6 \nFlan-PaLM-540B 79.0 \nY-KNOT-med-base-8B 75.2 \n KorMedMCQA \n Doctor Nurse Pharm Avg \nLlama2-70B 42.5 63.5 53.3 53.1 \nYi-34B 40.0 55.5 52.8 49.4 \nSOLAR-10.7B-v1.0 37.2 55.5 54.1 48.9 \nMistral-7B-v0.1 29.8 42.1 43.5 38.5 \nY-KNOT-med-base-8B 47.0 64.1 56.2 55.8 \nWe compared the performance of Y-KNOT-med-base on two benchmarks, PubMedQA (biomedical question \nanswering based on PubMed abstracts) for English evaluation and KorMedMCQA (multi-choice question \nanswering derived from licensing examinations for doctors, nurses, and pharmacists in South Korea) for Korean \nevaluation, against other baseline models. Baselines for PubMedQA were selected from the PubMedQA \nleaderboard (https://pubmedqa.github.io/) based on a criterion of being non-proprietary. The results of the \nmodels were taken from the associated papers. Note that a leaderboard for KorMedMCQA does not exist, \nbaseline models and their results were taken from the paper of the benchmark. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 4, 2025. ; https://doi.org/10.1101/2025.04.03.25325003doi: medRxiv preprint "
}