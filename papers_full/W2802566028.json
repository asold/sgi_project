{
  "title": "Enhancing recurrent neural network-based language models by word tokenization",
  "url": "https://openalex.org/W2802566028",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A352362178",
      "name": "Hatem M. Noaman",
      "affiliations": [
        "Mansoura University",
        "Beni-Suef University"
      ]
    },
    {
      "id": null,
      "name": "Shahenda S. Sarhan",
      "affiliations": [
        "Mansoura University"
      ]
    },
    {
      "id": "https://openalex.org/A2691783201",
      "name": "Mohsen A. A. Rashwan",
      "affiliations": [
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A352362178",
      "name": "Hatem M. Noaman",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Shahenda S. Sarhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2691783201",
      "name": "Mohsen A. A. Rashwan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2077566853",
    "https://openalex.org/W2620507731",
    "https://openalex.org/W2401969231",
    "https://openalex.org/W2040711288",
    "https://openalex.org/W2399981156",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2154137718",
    "https://openalex.org/W2408752871",
    "https://openalex.org/W2557752243",
    "https://openalex.org/W2802422770",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W1972099155",
    "https://openalex.org/W1622676895",
    "https://openalex.org/W2125336414",
    "https://openalex.org/W179707045",
    "https://openalex.org/W2143910127",
    "https://openalex.org/W179850243",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W179875071",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2727082984",
    "https://openalex.org/W4214717370",
    "https://openalex.org/W2251944986",
    "https://openalex.org/W2293185259",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W1996160600"
  ],
  "abstract": "Abstract Different approaches have been used to estimate language models from a given corpus. Recently, researchers have used different neural network architectures to estimate the language models from a given corpus using unsupervised learning neural networks capabilities. Generally, neural networks have demonstrated success compared to conventional n-gram language models. With languages that have a rich morphological system and a huge number of vocabulary words, the major trade-off with neural network language models is the size of the network. This paper presents a recurrent neural network language model based on the tokenization of words into three parts: the prefix, the stem, and the suffix. The proposed model is tested with the English AMI speech recognition dataset and outperforms the baseline n-gram model, the basic recurrent neural network language models (RNNLM) and the GPU-based recurrent neural network language models (CUED-RNNLM) in perplexity and word error rate. The automatic spelling correction accuracy was enhanced by approximately 3.5% for Arabic language misspelling mistakes dataset.",
  "full_text": "Enhancing recurrent neural \nnetwork‑based language models by word \ntokenization\nHatem M. Noaman1,2*† , Shahenda S. Sarhan1† and Mohsen. A. A. Rashwan3†\nIntroduction\nStatistical language models estimate the probability for a given sequence of words. \nGiven a sentence s with n words such as s= (w 1 ,w 2 ... w n), the language model assigns \nP(s). Statistical language models assess good word sequence estimations based on the \nsequence probability estimation. Building robust, fast and accurate language models is \none of the main factors in the success of building systems such as machine translation \nsystems and automatic speech recognition systems. Statistical language models can be \nestimated based on various approaches. Classical language models are estimated based \non n-gram word sequences such as P (s) = P (w 1 ,w 2 ... w n ) = ∏n\ni= 1 P (w i|w i− 1 ) and \ncan be approximated based on the Markov concept for shorter contexts (for example, \nbigram if n  =  2 or trigram if n  =  3 and so on). Recent researchers have applied neu -\nral networks different architectures to build and estimate language models. The classical \nfeed-forward neural network-based language models have been continuously report -\ning good results among the traditional n-gram language modeling techniques [ 1]. It \nAbstract \nDifferent approaches have been used to estimate language models from a given cor-\npus. Recently, researchers have used different neural network architectures to estimate \nthe language models from a given corpus using unsupervised learning neural net-\nworks capabilities. Generally, neural networks have demonstrated success compared to \nconventional n-gram language models. With languages that have a rich morphologi-\ncal system and a huge number of vocabulary words, the major trade-off with neural \nnetwork language models is the size of the network. This paper presents a recurrent \nneural network language model based on the tokenization of words into three parts: \nthe prefix, the stem, and the suffix. The proposed model is tested with the English AMI \nspeech recognition dataset and outperforms the baseline n-gram model, the basic \nrecurrent neural network language models (RNNLM) and the GPU-based recurrent \nneural network language models (CUED-RNNLM) in perplexity and word error rate. \nThe automatic spelling correction accuracy was enhanced by approximately 3.5% for \nArabic language misspelling mistakes dataset.\nKeywords: Recurrent neural networks, Statistical language modeling, Automatic \nspeech recognition\nOpen Access\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\nRESEARCH\nNoaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nhttps://doi.org/10.1186/s13673‑018‑0133‑x\n*Correspondence:   \nhnoaman@bsu.edu.eg \n†Hatem M. Noaman, \nShahenda S. Sarhan and \nMohsen. A. A. Rashwan \ncontributed equally to this \nwork\n2 Computer Science \nDepartment, Beni-Suef \nUniversity, Beni-Suef, Egypt\nFull list of author information \nis available at the end of the \narticle\nPage 2 of 13Noaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \ncomputes the probability of a word given its context. Inputs into the network are the \nprevious n-words according to the language models order. A word feature vector is \nprojected using the word index in the projection layer. The hidden layer output is com -\nputed using Tanh function [2]. The final network output is computed using the Softmax \nactivation function [3] to ensure that network output is a valid probability between 0 \nand 1. Network weights are updated using the back-propagation training algorithm [4]. \nNeural network-based language models offer several advantages. Smoothing is applied \nimplicitly, while in n-gram language models, smoothing must be handled explicitly for \nan unseen n-gram. Semantically, similar words are clustered due to the projection of \nthe entire vocabulary into a small hidden layer [5]. Recurrent neural network-based lan\n-\nguage models [6] are the other proposed models. In them, the feedback between hidden \nand input layer allows the hidden neurons to remember the history of the previously \nprocessed words, as shown in Fig.  1. The results that are reported using recurrent neural \nnetwork-based language models are always better than those of basic neural network \nmodels and other traditional model results. The results reported by this work show that \nrecurrent neural network-based language models results outperform traditional models \nresults for two different tasks: English automatic speech recognition and Arabic auto\n-\nmatic spelling error correction. The neural network input vector is presented using a \nbinary representation where the current word is set to one in the vector according to its \nindex in the vocabulary, and the other vector values are set to zero. The main problem \nwith this model is the size of both the input and output layer where it needs to be at least \nas large as the number of words in the language vocabulary. The vocabulary size varies \nfrom languages such as English that have simple morphological systems to languages \nthat have richer morphological systems such as the Arabic language [7] (where every \nFig. 1 Basic recurrent neural network language model. Basic recurrent neural network language model con-\nsists of three layers: input layer, hidden layer and output layer; input word is presented to the network input \nlayer using a 1-of-n encoding. The feedback between hidden and input layer allows the hidden neurons to \nremember the history of the previously processed word. The hidden layer output is computed using Tanh \nfunction [2]. The final network output is computed using the Softmax activation function [3]\nPage 3 of 13\nNoaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nsingle root can be converted to several other valid forms using morphological deriva -\ntion rules). Rich morphological system makes this architecture inefficient if it is used \nin rich morphological language applications since it needs high computational costs to \ntrain and use these models within real-time applications, such as speech recognition and \nautomatic machine translation.\nIn this work, we try to build recurrent neural network-based language models that can \nhandle the network training speed problem with languages that have rich morphologi -\ncal systems based on word tokenization. The proposed model input layer size is shown \nin Fig.   2 for the different training sets extracted from an open source Arabic language \ncorpus [8]. As shown in the figure, the network size remarkably increases as the number \nof words in the training corpus increases. In real applications, to build a robust language \nmodel, a large corpus with several millions of words is needed. Networks with a very \nlarge number of neurons have two main problems. The first one is that training time is \nvery long to achieve network learning convergence. It is reported that the network learn\n-\ning process takes several weeks with a corpus size of approximately 6.4 M words [6].\nThe second problem is the memory size. Recurrent neural networks-based language \nmodels with large numbers of neurons are expected to need larger memory sizes than \nthat of other traditional language models. Researchers tried to find solutions to these \nproblems through merging all words that occur less than a given threshold into a special \nrare token or by adding classes of neurons in the output layer and factorizing the output \nlayer into classes [9]. The main contribution of this work is building a recurrent neural \nnetwork language modeling model that outperforms the basic RNNLM [6]. It is faster \nand consumes less memory than the RNNLM and its enhanced versions (the factored \nrecurrent neural network language model (fRNNLM) [10] and the CUED-RNNLM \n[11]). It also adds word features implicitly with no need to add a different vector for each \nword, as proposed by the related work in the fRNNLM [10] or the FNLM [12]. These \nfeatures make the proposed model suitable for highly inflected languages and in build\n-\ning models with dynamic vocabulary expansion. It also decreases the number of vocabu -\nlary words since unseen words can be inferred from other seen words if the words have \nthe same stem. This paper is organized as follows. “Related work ” section  includes an \nFig. 2 Effect of corpus size on the network input layer neurons number. Recurrent neural network-based \nlanguage models input layer size depends on the vocabulary words number, training time and memory cost \nincreased remarkably as the number of neurons in input and output layers becomes big number. This figure \nillustrates the relationship between corpus size and the recurrent neural network input layer size, as shown as \nwe add more words to the corpus it will cause vocabulary expansion and as a result recurrent neural network \ninput layer size will increase which will downgrade the network performance.\nPage 4 of 13Noaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \noverview of the related works. In “Proposed model ” section , the researchers discuss the \nword tokenization process for English and Arabic then the proposed model is presented. \nThe experimental results are discussed in  “Experiments and results” section . Finally, the \nconclusion is presented in “Conclusion” section .\nRelated work\nNeural networks different architectures have been investigated and applied to language \nmodel estimations by many researchers. Feed forward neural networks [1] have been \nadapted in language modeling estimation [1]; feed forward neural network language \nmodels simultaneously learn the probability functions for word sequences and build the \ndistributed representation for individual words, but this model has a drawback in that a \nfixed number of words can be considered as a context window for the current or target \nword. To enhance the conventional feed forward neural network language models train\n-\ning time, researchers proposed continuous space language modeling (CSLM), which is a \nmodular open-source toolkit of feed forward neural network language models [13]; this \nmodel introduces support for GPU cards that enable neural networks to build models \nwith corpora that contain more than five billion words in less than 24 hours with about a \n20% perplexity reduction [13]. Recurrent neural networks have been applied to estimate \nlanguage models. However, with this model, there is no need to specify the context win\n-\ndow size by using feedback from the hidden to the input layer as a kind of network mem-\nory for the word context. Experiment results have proved that recurrent neural networks \nin language models outperform n-gram language models [5, 6, 9, 14, 15]. An RNNLM \ntoolkit was designed to estimate the class-based language model using recurrent neu\n-\nral networks [5, 6]. It can also provide functions such as an internist model evaluation \nusing perplexity, N-best rescoring and model-based text generation. The training speed \nis the main RNNLM drawback, especially with large vocabulary sizes and large hidden \nlayers. The RWTHLM [16] is another recurrent neural network-based toolkit with long \nshort-term memory (LSTM) implementation, and the RWTHLM toolkits BLAS library \nwas used to support reduced training time and efficient network training. The CUED-\nRNNLM [ 11] provides an implementation for the recurrent neural network-based \nmodel, and it has GPU support to achieve a more efficient training speed. Both the basic \nfeed forward network and the recurrent neural network-based language models do not \ninclude any type of word level morphological features, but some researchers tried to add \nthis type of word feature explicitly by input layer factorization. Factored neural language \nmodels (FNLM) [12] add word features explicitly in the neural network input layer in the \nfeed-forward based neural network language model and the factored recurrent neural \nnetwork language model (fRNNLM) [10]. They also add word features to the recurrent \nneural network input layer to model the results better than the basic model. Their com\n-\nplexity is higher than that of the original models since they add word features explicitly \nto the input layer. While adding these features improves network performance, it adds \nmore complexity to the models estimation and the application performance, especially \nwhen applying it to large size vocabulary applications or language with rich morphologi\n-\ncal features. Researches tries to build RNNLM personalization models [17] using data -\nset collected from social media networks, model-based RNNLM personalization aims to \ncaptures patterns posted by used and his/her related friends while another approach is \nPage 5 of 13\nNoaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nfeature-based where RNNLM parameters are static throw users. Recently neural-based \nlanguage modeling models added as an extension to Kaldi automatic speech recogni -\ntion (Kaldi-RNNLM) [18] software, this architecture combines the use of subword fea -\ntures and one-hot encoding of words with high frequency to handle large vocabularies \ncontaining infrequent words. Also Kaldi-RNNLM architecture improves cross-entropy \nobjective function to train unnormalized probabilities. In addition to feed forward net\n-\nwork and the recurrent neural network-based language models architectures convolu -\ntion neural network (CNN) [19] was applied to estimate language models with inputs to \nthe network in the form of character and output predictions is at the word-level.\nProposed model\nThe proposed model is a modified version of the basic recurrent neural network language \nmodel [6]. Instead of presenting the full word to the network input layer, we split the word \ninto three parts: the prefix, the stem and the suffix. Both the prefix and the suffix may or \nmay not exist. “Word tokenization” section presents a full description about word tokeni\n-\nzation and how we implement it with English and Arabic text using modified versions of \ntwo free open source stemmers in “English word tokenization” and  “Arabic word tokeni\n-\nzation” sections. Next, the proposed models architecture is discussed in “ The proposed \nmodel” section with full details about the model’s components, inputs and outputs.\nWord tokenization\nThe first step to build the word vector is using a stemmer. Generally, in this step, the \ninput to the stemmer is a complete surface word, and the output is the stemmed word \nvector consisting of a prefix ID, a stem ID and a suffix ID. The general framework of this \nprocess is shown in Fig.   3. As illustrated in the figure, the stemmer has 3 lookup tables. \nThe first one is for the prefixes to get the prefix ID using this table. The second one is \nfor the stems. The last one has the suffix IDs. After splitting the word into its compos\n-\ning parts, the stemmer assigns each part a unique ID. If a word does not have a prefix or \na suffix, it has the value − 1 to indicate that this part is not present for the given word. \n“English word tokenization ” section gives a detailed description for the English stem\n-\nming process. “Arabic word tokenization” section shows that the Arabic language has a \nmore different and richer morphological system than English to prove that this proposed \nmodel can effectively handle languages with highly inflected systems.\nEnglish word tokenization\nTable  1 presents different examples of the stemmer input and its corresponding output \nprocessed by a modified version of PorterStemmer . The original version of this stem -\nmer generates only the word stem. The proposed work needs to tokenize any word into \nits three components. Therefore, the researchers constructed a list of common English \nprefixes (ante, anti, co, de, dis, em, en, epi, ex and un) and check if the word starts with \nany of them. Then, the researchers separate it from the original word. If the remaining \npart of the word is still a valid English word, the stemmer tries to get the stem from it. \nOtherwise, it returns back to the original word and tries to stem it directly. After get\n-\nting the word stem, if it is different from the original word, the stemmer assumes that \nthe word has a suffix and splits the word into the stem and suffix. The final modified \nPage 6 of 13Noaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nstemmer output always provides the word in the form of (prefix + stem + suffix). At the \nend, every part has a unique ID so that it can be used in the neural networks internal \nrepresentation.\nArabic word tokenization\nThe Arabic morphological system applies different generation rules on a given root to \ngenerate a list of words from the same root. Many approaches have been applied to \nsolve tokenization problem in Arabic. This paper investigates applying the Khoja stem -\nmer [20] to handle the Arabic word tokenization problem. Khoja stemmer is a java open \nsource tool that finds the roots for Arabic words. To find the root for any given Arabic \nword, first, the Khoja stemmer attempts to remove definite articles, prefixes, and suf\n-\nfixes. It then tries to find the word root using a set of Arabic morphological patterns. If \nthe found root is not applicable, it returns the word as is. We have tried to modify the \nFig. 3 Word tokenization process flowchart. The proposed approach uses stemmer to split the word into 3 \nparts word prefix, word stem and word suffix, the input to the stemmer is a complete surface word, and the \noutput is the stemmed word vector consisting of a prefix ID, a stem ID and a suffix ID. After splitting the word \ninto its composing parts, the stemmer assigns each part a unique ID. If a word does not have a prefix or a \nsuffix, it has the value − 1 to indicate that this part is not present for the given word.\nTable 1 English stemmer original input and output after converting it into prefix, stem \nand suffix form\nInput word Prefix Stem Suffix\nYears – Year s\nGovernment – Govern ment\nUnless Un Less –\nHope – Hope –\nPage 7 of 13\nNoaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nstemmers functionality to fit our needs. The original stemmer produces only one out -\nput for any word, the word root. To find the word root, Khoja’s algorithm removes any \nprefix or suffix that is not considered as a part of a word root and compares the rest of \nthe word to the Arabic morphological patterns. If it fits with any given pattern, the word \nroot is then extracted using this pattern. The work in this paper modifies the original \nKhoja stemmer to generate 3 parts [the words prefix, suffix (if any), and the word stem] \nby keeping any removed characters from the word if the word roots extracted word is \ndivided into the prefix, stem and suffix. The original Khoja stemmer includes a list of \n168 stop words that are removed while preprocessing Arabic words since these words \ncannot be rooted. In the modified version, the stop words are not removed. Additionally, \nthe word may be normalized by the Khoja stemmer to find its root. In the modified ver\n-\nsion, the word stem is not normalized since we try to tokenize the word into parts and \nnot find the valid root for this word. Figure   3 depicts the Arabic word tokenization pro\n-\ncess using the modified Khoga stemmer tool. The input is an Arabic word, and the final \noutput is a stemmed word vector that consists of three parts (a prefix ID, a stem ID and \na suffix ID). To construct such a vector, the stemmer tries to get an input word root by \nremoving any possible prefixes or suffixes attached to the original root. Finally, the stem\n-\nmer tries to extract the root based on different morphological patterns. If the input word \ncannot be rooted (or there is no prefix or suffix attached to the stem), the prefix ID and \nsuffix ID values are set to − 1, which indicates that there is no prefix or suffix attached to \nthe stem. The stemmer output may be the same as the input word in another case when \nthe input word cannot be stemmed for some words such as stop words and non-Arabic \nwords (i.e., words from other languages that are written in the Arabic alphabet without \nhaving an Arabic root). The proposed stemming algorithm can be used to tokenize a \nsingle word or to tokenize a complete paragraph. Table   2 shows the modified stemmer \ninput, its corresponding Buckwalter transliteration [21], the English translation and its \nfinal output for a selected paragraph from a policy section in the open corpus [8]. As \nshown, the prefixes and suffixes don’t appear in all words and the stemmer. The stemmer \noutput is the word divided into the prefix that is followed by the (+) Markup, then the \nstem followed by the (+) Markup, and finally the word suffix.\nThe proposed model\nThe proposed network architecture (as shown in Fig.   4) splits the input layer into three \nparts: the word prefix part, the word stem part and the word suffix part. Each part cor -\nresponds to the word input part where each part of the word is presented to the network \nusing a 1-of-n encoding. The input layer size equals the sum of the hidden layer size, \nTable 2 Arabic stemmer original input and output after scanning words and convert it \ninto prefix + stem + suffix form\nInput text\nBuckwalter transliteration AqtSAd w?‘EmAl-AlhA$my yqATE mnAqSp Eqwd AlTAqp AlErAqyp\nEnglish translation Economics and business-Hashemi boycott Iraqi energy contracts tender\nStemmer output\nPage 8 of 13Noaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nprefix, stem and suffix counts. The training input vector x(t) is formed by concatenat -\ning vector w that is representing the current word and the output from the neurons in \ncontext layer s at time t  − 1. The current word is split into three parts. The word vector \nw is concatenated as expressed in Eq.  1. The prefix, the stem and the suffix vectors are \nencoded with a 1-of-n coding by setting the ith element to 1 and any other values to 0.\nAfter representing the concatenated word vector to the input neurons, the previous time \nsteps hidden layer is copied to its corresponding history neurons in the input layer, Eq. 2. \nThe hidden and output layers are then computed as shown in Eqs.  3 and 4:\n(1)w (t) = pr(t) + stem(t) + suf(t)\n(2)x(t) = w (t) + s(t − 1)\n(3)sj(t) = f\n(∑\ni\nx i(t)u ij\n)\n(4)yk (t) = g\n\n�\nj\nsj(t)vkj\n\n\nFig. 4 The proposed recurrent neural network-based language model architecture with input layer seg-\nmented into three components: the prefix, the stem and the suffix. Work in this paper suggests splitting the \ninput layer of the recurrent neural network-based language models to contains three parts of the word; prefix \npart, the word stem part and the word suffix part, where each part of the word is presented to the network \nusing a 1-of-n encoding.\nPage 9 of 13\nNoaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nwhere f is the sigmoid function [22] and g are Softmax functions [3]. It is computed as \nfollows:\nwhere u is the weight matrix between the input and hidden layer, and v is the weight \nmatrix between the hidden and output layers.\nExperiments and results\nThe proposed model is compared with other language modeling approaches based on \nthree different evaluation approaches. The first one is the model computation com\n-\nplexity presented in “Model complexity ” section. The second one in “Models perplex -\nity results” section is the proposed model perplexity enhancement and word error rate. \nThe results are shown for English automatic speech recognition using the AMI meeting \ncorpus and the model perplexity enhancement plus the entropy reduction result for the \nOnline Open Source Arabic language corpuss experimental results. Finally, in “ Arabic \nautomatic spelling correction application ” section, the Arabic automatic spelling error \ncorrection results are presented using the Qatar Arabic Language Bank (QALP) corpus.\nModel complexity\nThe proposed system shows a very efficient memory and processing performance. To \nprove this, we have compared the neural network input layer size (i.e.: the number of \nneurons in the input layer) in the proposed architecture with the basic RNN architec\n-\nture. The results are shown in Fig.   5. It is clear that the number of neurons in the input \nlayer generated by the proposed architecture is always smaller than that in the basic \nRNN, even with a very limited number of corpus words. This outcome confirms that the \ntokenization of Arabic words before presenting it to the neural network decreases the \n(5)f (z ) = 1\n1 + e−z\n(6)g(zm ) = ezm\n∑\nk ezk\nFig. 5 Neural network input layer size in the proposed architecture against the basic RNN architecture with \nincreasing size of training corpus. In this figure we compare the number of neurons in the input layer in the \nproposed architecture with the basic RNN architecture. The figure shows that the input layer size is decreased \nfrom 360K to only 98K neurons with nearly 9.6 M words corpus size which is a very efficient architecture for \nmemory usage and processing performance.\nPage 10 of 13Noaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nmemory use and computation time. The values presented in Fig.   5 show that the input \nlayer size is decreased from 360K to only 98K neurons.\nComplexity of the RNNLM and factored RNNLM models are \n(|V|+H ) ∗ H + H ∗ (C +| V|) and (|f1|+... +| fK|+ H ) ∗ H + H ∗ (C +| V |) , \nrespectively. While proposed model complexity is \n(|pr|+| stem|+|suﬀ|+ H ) ∗ H + H ∗ (C +| V |), where V, H, C fi are the vocabulary \ncount, the hidden layer size, the classes count and the ith feature vector respectively. The \nresearchers observe that the sum of the prefix number and the stems count and the suf -\nfixes count (|pr|+| stem|+| suﬀ |) will be much less than vocabulary words count (|V|) \nespecially for highly inflected language and language with rich morphological system. \nAlso proposed model does not need extra GPU processing capabilities as needed with \nCUED-RNNLM system.\nModels perplexity results\nEnglish AMI meeting corpus experiments\nOur first experiment evaluates the proposed model against the n-gram, he RNNLM and \nthe CUED-RNNLM using the English AMI meeting corpus [23]; the corpus consists of \n100 annotated hours of meeting records. This dataset has various numbers of features \nthat make it good to be used as a benchmarking dataset for language model evaluation. \nIt is a free of charge dataset that can be used for training and testing. Other good fea\n-\ntures of this dataset are its annotations that present time and how the annotations are \nrelated to the transcription and to each other. This paper divided the English AMI meet\n-\ning corpus [23] into a training set that consists of 78 h, and the remaining part is used \nas the development and test sets. Table   3 presents the performances of different lan\n-\nguage modeling techniques using the AMI meeting corpus. To build an n-gram model, \nwe have used the popular SRILM toolkit [24]; the CRNN and FRNN with the AMI data\n-\nset are reported to outperform the basic 3-gram model models that were built using the \nRNNLM, and the toolkit was used to build the RNNLM [6] and CUED-RNNLM [11]. \nThe proposed language model results are measured by their perplexity and word error \nrate (WER), as shown in Table 4.\nThe results show that the proposed token-based recurrent neural network language \nmodel has outperformed the n-gram LM by approximately 3% and enhances the basic \nRNNLM and its GPU version CUED-RNNLM by approximately 1.5% when using the \nTable 3 English AMI meeting corpus perplexity and WER results using RNNLM and CUED-\nRNNLM against our proposed model\nPPL WER\nDev Eval Dev Eval\n3g [25] 93.6 82.8 25.2 25.4\n+CRNN (CE) [25] 83.3 75.2 23.9 24.1\n+FRNN(CE) [25] 81.0 71.7 23.9 24.0\n+FRNN(VR) [25] 80.4 71.6 23.9 23.9\n+FRNN(NCE) [25] 81.1 72.8 24.0 24.1\n+Proposed model 75.3 69.7 22.3 22.5\nPage 11 of 13\nNoaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nEnglish AMI meeting corpus dataset. While the proposed approach is relatively close \nto the CUED-RNNLMs reported results with the same dataset, the proposed systems \ntraining and decoding times are much improved compared to those of the RNNLM and \nCUED-RNNLM. Moreover, memory consumption is much lower with our proposed \nmodel, which make it much more applicable to be used for rescoring tasks rather than \nfor the models generated by the RNNLM (that have no GPU support and have high \nmemory needs) and the CUED-RNNLM (that relies on the GPU architecture and needs \nmore computational resources).\nOnline Open Source Arabic language corpus experiments\nTo evaluate the proposed model, we have chosen the Online Open Source Arabic Lan -\nguage corpus, which is available online. It has an ample amount of words to test and \nevaluate our proposed model. In this work, the Arabic Open corpus is used as the source \nof the training, validation and test data. Approximately 1.9 M words are used as training \ndata, and 70K words are used as the test set. To train the models using this dataset, the \nvocabulary of the 10K most common Arabic words is constructed in the online open \nsource Arabic language corpus. All words outside this vocabulary are counted as out-\nof-vocabulary (OOV) words and are treated as unknown words (UNK-token). Table   4 \nshows the results of various language models. To build the N-gram models, the SRILM \ntoolkit [24] and the RNNLM tool [6] are used to build the basic RNN language models. \nThe results have shown that the modified Kneser-Ney smoothing with order 5 (KN5) \nperforms the best among traditional n-gram models. Thus, it was used as the benchmark \nfor our test set. As shown in Table   4, our proposed models perplexity outperforms the \nbaseline n-gram model by up to 30% with about a 2% enhancement compared to basic \nRNN models.\nArabic automatic spelling correction application\nThe automatic spelling correction problem involves two main parts. The first part \ndetects the spelling mistake in the written text. The second corrects the spelling errors \n[26, 27]. Spelling errors can be identified by using a lexicon of Arabic words. If any given \nword in the text is outside the lexicon, it is considered as a spelling error. We have built \nan Arabic automatic spelling correction hybrid system based on the confusion matrix \nand the noisy channel spelling correction model to detect and automatically correct \nArabic spelling errors. It searches for the correct word that can generate this misspelled \nword (typo) using Eq. (7) [20].\nTable 4 Perplexity on 70K word as test from Arabic Open Corpus using different smooth-\ning techniques against proposed algorithm\nItalic values represent proposed model perplexity and entropy reduction against different smoothing techniques\nModel Perplexity Entropy reduction (%)\nGT5 113.473 –\nKN3 99.1785 2.85\nKN5 98.9021 2.9\nBasic RNN 70.58 10.04\nProposed model 68.42 10.69\nPage 12 of 13Noaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nwhere P(typoword) is computed based on a confusion matrix and P(word) using the \nlanguage model probability of a given word. In this paper, the proposed algorithm was \ncompared with traditional n-gram language models. The proposed algorithm was used \nto estimate the probability of a given correction candidate for typos. Table   5 shows \nthe overall correction accuracy using the 3-gram language model with Good–Turing \nsmoothing and the proposed algorithm. The results show that the correction accuracy \nwas enhanced by nearly 3.5% using the 4-gram token-based language model where \nthe test set consists of 1500 test cases extracted from the Qatar Arabic Language Bank \n(QALP) corpus [28]. It was divided into 3 test sets where each one consists of 500 test \ncases.\nConclusion\nIn this paper, we have introduced a modified recurrent neural network-based language \nmodel for language modeling. The modification was to segment the network input into \nthree parts. It is observed that the computational complexity is much lower than that \nin the basic recurrent neural network model. This outcome makes it possible to build \nlanguage models for highly inflective languages (such as Arabic) from large corpora with \nsmaller training times and memory costs. Using the intrinsic evaluation (perplexity), it is \nobserved that our proposed model outperforms the baseline n-gram model by up to 30% \nbased on the Arabic Open Corpus experimental results shown in Table  4. The results \nobtained from applying the proposed model to the Arabic automatic spelling correc\n-\ntion problem show about a 3.5% total accuracy enhancement. This finding indicates that \nmore complex and advanced Arabic language applications (such as speech recognition \nand automatic machine translation) can make use of the model described in this paper.\nAuthors’ contributions\nHN is the corresponding author, and SS and MR are the co-authors. HN has made substantial contributions in the design \nand implementation of proposed algorithm. SH was involved in drafting the manuscript or critically revising it. All \nauthors read and approved the final manuscript.\nAuthor details\n1 Computer Science Department, Mansoura University, Mansoura, Egypt. 2 Computer Science Department, Beni-Suef \nUniversity, Beni-Suef, Egypt. 3 Electronics and Communications Department, Cairo University, Cairo, Egypt. \nCompeting interests\nThe authors declare that they have no competing interests.\nEthics approval and consent to participate\nWe confirm that this manuscript has not been published elsewhere and is not under consideration by another journal. \nAll authors have approved the manuscript and agree with its submission.\nFunding\nNot applicable.\n(7)˙w = argmaxword = P(word|typo)= argmaxword P(typo|word) ∗ P(word)\nTable 5 Automatic Arabic spelling correction application\nModel Correction accuracy Average (%)\nTest set 1 (%) Test set 2 (%) Test set 3 (%)\n3-gram+ GT3 72.65 70.85 69.03 70.84\nRNNLM 74. 5 73.85 71.07 73.14\nProposed model 76.03 75.36 71.50 74.30\nPage 13 of 13\nNoaman et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:12 \nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nReceived: 26 January 2018   Accepted: 9 April 2018\nReferences\n 1. Bengio Y, Ducharme R, Vincent P , Jauvin C (2003) A neural probabilistic language model. J Mach Learn Res \n3:1137–1155\n 2. Abramowitz M, Stegun IA (1964) Handbook of mathematical functions: with formulas. Graphs Math Tables 55:83\n 3. Sutton RS, Barto AG (1998) Reinforcement learning: an introduction. MIT Press Cambridge, Cambridge, p 30\n 4. Rumelhart DE, Hinton GE, Williams RJ (1986) Learning representations by back-propagating errors. Nature \n323(6088):533\n 5. Kombrink S, Mikolov T, Karafiát M, Burget L (2011) Recurrent neural network based language modeling in meeting \nrecognition. In: Twelfth annual conference of the international speech communication association\n 6. Mikolov T, Karafiát M, Burget L, Černockỳ J, Khudanpur S (2010) Recurrent neural network based language model. \nIn: Eleventh annual conference of the international speech communication association\n 7. Bousmaha KZ, Rahmouni MK, Kouninef B, Hadrich LB (2016) A hybrid approach for the morpho-lexical disambigua-\ntion of arabic. J Inf Proces Syst 12(3):358–380\n 8. Saad MK, Ashour W (2010) Osac: open source arabic corpora. In: 6th ArchEng international symposiums, EEECS, vol. \n10\n 9. Mikolov T, Kopecky J, Burget L, Glembek O et al (2009) Neural network based language models for highly inflective \nlanguages. In: IEEE international conference on acoustics, speech and signal processing. ICASSP 2009, pp 4725–4728\n 10. Wu Y, Yamamoto H, Lu X, Matsuda S, Hori, C, Kashioka H (2012) Factored recurrent neural network language model \nin ted lecture transcription. In: International workshop on spoken language translation (IWSLT)\n 11. Chen X, Liu X, Qian Y, Gales M, Woodland PC (2016) Cued-rnnlman open-source toolkit for efficient training and \nevaluation of recurrent neural network language models. In: 2016 IEEE international conference on acoustics, \nspeech and signal processing (ICASSP), pp 6000–6004\n 12. Alexandrescu A, Kirchhoff K (2006) Factored neural language models. In: Proceedings of the human language tech-\nnology conference of the NAACL, companion volume: short papers. Association for computational linguistics, pp 1–4\n 13. Schwenk H (2013) Cslm-a modular open-source continuous space language modeling toolkit. In: INTERSPEECH, pp \n1198–1202\n 14. Devlin J, Zbib R, Huang Z, Lamar T, Schwartz R, Makhoul J (2014) Fast and robust neural network joint models for \nstatistical machine translation. In: Proceedings of the 52nd annual meeting of the association for computational \nlinguistics (Vol. 1: long papers), vol. 1, pp 1370–1380\n 15. De Mulder W, Bethard S, Moens M-F (2015) A survey on the application of recurrent neural networks to statistical \nlanguage modeling. Comput Speech Lang 30(1):61–98\n 16. Sundermeyer M, Schlüter R, Ney H (2014) rwthlmthe RWTH Aachen University neural network language modeling \ntoolkit. In: Fifteenth annual conference of the international speech communication association\n 17. Tseng B-H, Wen T-H (2017) Personalizing recurrent-neural-network-based language model by social network. IEEE/\nACM Trans Audio Speech Lang Proces (TASLP) 25(3):519–530\n 18. Xu H, Li K, Wang Y, Wang J, Kang S, Chen X, Povey D, Khudanpur S (2018) Neural network language modeling with \nletter-based features and importance sampling. In: 2018 IEEE international conference on acoustics, speech and \nsignal processing (ICASSP)\n 19. Kim Y, Jernite Y, Sontag D, Rush AM (2016) Character-aware neural language models. In: AAAI, pp 2741–2749\n 20. Kernighan MD, Church KW, Gale WA (1990) A spelling correction program based on a noisy channel model. In: \nProceedings of the 13th conference on computational linguistics. Association for computational linguistics, vol. 2, \npp 205–210\n 21. Buckwalter T (2004) Buckwalter arabic morphological analyzer version 2.0. linguistic data consortium, University of \nPennsylvania, 2002. ldc cat alog no.: Ldc2004l02. Technical report, ISBN 1-58563-324-0\n 22. Han J, Moraga C (1995) The influence of the sigmoid function parameters on the speed of backpropagation learn-\ning. In: International workshop on artificial neural networks. Springer, Berlin, pp 195–201\n 23. Carletta J, Ashby S, Bourban S, Flynn M, Guillemot M, Hain T, Kadlec J, Karaiskos V, Kraaij W, Kronenthal M et al (2005) \nThe ami meeting corpus: a pre-announcement. In: International workshop on machine learning for multimodal \ninteraction. Springer, Berlin, pp 28–39\n 24. Alumäe T, Kurimo M (2010) Efficient estimation of maximum entropy language models with n-gram features: An \nsrilm extension. In: Eleventh annual conference of the international speech communication association\n 25. Chen X (2015) Cued rnnlm toolkit\n 26. Noaman HM, Sarhan SS, Rashwan M (2016) Automatic arabic spelling errors detection and correction based on \nconfusion matrix-noisy channel hybrid system. Egypt Comput Sci J 40(2):2016\n 27. Attia M, Al-Badrashiny M, Diab M (2014) Gwu-hasp: hybrid arabic spelling and punctuation corrector. In: Proceed-\nings of the EMNLP 2014 workshop on Arabic natural language processing (ANLP), pp 148–154\n 28. Zaghouani W, Mohit B, Habash N, Obeid O, Tomeh N, Rozovskaya A, Farra N, Alkuhlani S, Oflazer K (2014) Large scale \nArabic error annotation: guidelines and framework. In: LREC, pp 2362–2369",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8642670512199402
    },
    {
      "name": "Perplexity",
      "score": 0.8268594741821289
    },
    {
      "name": "Language model",
      "score": 0.8120185136795044
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6838008761405945
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6637649536132812
    },
    {
      "name": "Lexical analysis",
      "score": 0.6473744511604309
    },
    {
      "name": "Natural language processing",
      "score": 0.616137683391571
    },
    {
      "name": "Artificial neural network",
      "score": 0.581073522567749
    },
    {
      "name": "Cache language model",
      "score": 0.49348199367523193
    },
    {
      "name": "Suffix",
      "score": 0.4847535789012909
    },
    {
      "name": "Time delay neural network",
      "score": 0.4620484709739685
    },
    {
      "name": "Speech recognition",
      "score": 0.4109065532684326
    },
    {
      "name": "Natural language",
      "score": 0.3117998242378235
    },
    {
      "name": "Universal Networking Language",
      "score": 0.17418628931045532
    },
    {
      "name": "Linguistics",
      "score": 0.13279280066490173
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Comprehension approach",
      "score": 0.0
    }
  ]
}