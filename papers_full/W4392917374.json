{
  "title": "Assessing the Proficiency of Large Language Models in Automatic Feedback Generation: An Evaluation Study",
  "url": "https://openalex.org/W4392917374",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4365973856",
      "name": "Flora Jin",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1969670978",
      "name": "Dragan Gašević",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2231463661",
      "name": "Yi-Shan Tsai",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": null,
      "name": "angusglchen",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A5107157987",
      "name": "Ahmad Aldino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1994056853",
      "name": "Wei Dai",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2297517972",
      "name": "Tongguang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2980838338",
      "name": "Jionghao Lin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6990734492",
    "https://openalex.org/W2940725132",
    "https://openalex.org/W2969278757",
    "https://openalex.org/W2801630847",
    "https://openalex.org/W6684059656",
    "https://openalex.org/W2802110796",
    "https://openalex.org/W2138228303",
    "https://openalex.org/W6730284005",
    "https://openalex.org/W6760804589",
    "https://openalex.org/W4285782968",
    "https://openalex.org/W7000828130",
    "https://openalex.org/W3107855336",
    "https://openalex.org/W2761487768",
    "https://openalex.org/W3121259022",
    "https://openalex.org/W3038648614",
    "https://openalex.org/W4384659608",
    "https://openalex.org/W3036685446",
    "https://openalex.org/W2164668300",
    "https://openalex.org/W2142995122",
    "https://openalex.org/W4220717796",
    "https://openalex.org/W3156128204",
    "https://openalex.org/W6671816753",
    "https://openalex.org/W2126091572",
    "https://openalex.org/W4360891295",
    "https://openalex.org/W2111497108",
    "https://openalex.org/W4291476001",
    "https://openalex.org/W4385632485",
    "https://openalex.org/W4321442039",
    "https://openalex.org/W4313262066",
    "https://openalex.org/W4365515834",
    "https://openalex.org/W2611176525",
    "https://openalex.org/W2162700863",
    "https://openalex.org/W4288059321",
    "https://openalex.org/W4389325641",
    "https://openalex.org/W2905050135",
    "https://openalex.org/W4317790171",
    "https://openalex.org/W4383175697",
    "https://openalex.org/W4383472720",
    "https://openalex.org/W2135995390",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4382491911",
    "https://openalex.org/W2933483998"
  ],
  "abstract": "Assessment feedback is important to student learning. Learning analytics (LA) powered by artificial intelligence exhibits profound potential in helping instructors with the laborious provision of feedback. Inspired by the recent advancements made by Generative Pre-trained Transformer (GPT) models, we conducted a study to examine the extent to which GPT models hold the potential to advance the existing knowledge of LA-supported feedback systems towards improving the efficiency of feedback provision. Therefore, our study explored the ability of two versions of GPT models – i.e., GPT-3.5 (ChatGPT) and GPT-4 to generate assessment feedback on students’ writing assessment tasks, common in higher education, with open-ended topics for a data science-related course. We compared the feedback generated by GPT models (namely GPT-3.5 and GPT-4) with the feedback provided by human instructors in terms of readability, effectiveness (content containing effective feedback components), and reliability (correct assessment on student performance). Results showed that (1) both GPT-3.5 and GPT-4 were able to generate more readable feedback than human instructors, (2) GPT-4 outperformed GPT-3.5 and human instructors in providing feedback containing information about effective feedback dimensions, including feeding-up, feeding-forward, process level, and self-regulation level, and (3) GPT-4 demonstrated higher reliability of feedback compared to GPT-3.5.",
  "full_text": "Assessing the Proficiency of Large Language Models in Automatic Feedback Generation:\nAn Evaluation Study\nWei Daia, Yi-Shan Tsaia, Jionghao Linb, Ahmad Aldinoa, Hua Jina, Tongguang Lia, Dragan Gaˇsevi´ca, Guanliang Chena\naMonash University, Melbourne, Australia\nbCarnegie Mellon University, USA\nAbstract\nAssessment feedback is important to student learning. Learning analytics (LA) powered by artificial intelligence exhibits profound\npotential in helping instructors with the laborious provision of feedback. Inspired by the recent advancements made by Generative\nPre-trained Transformer (GPT) models, we conducted a study to examine the extent to which GPT models hold the potential\nto advance the existing knowledge of LA-supported feedback systems towards improving the e fficiency of feedback provision.\nTherefore, our study explored the ability of two versions of GPT models – i.e., GPT-3.5 (ChatGPT) and GPT-4 – to generate\nassessment feedback on students’ writing assessment tasks, common in higher education, with open-ended topics for a data science-\nrelated course. We compared the feedback generated by GPT models (namely GPT-3.5 and GPT-4) with the feedback provided by\nhuman instructors in terms of readability, effectiveness (content containing effective feedback components), and reliability (correct\nassessment on student performance). Results showed that (1) both GPT-3.5 and GPT-4 were able to generate more readable feedback\nthan human instructors, (2) GPT-4 outperformed GPT-3.5 and human instructors in providing feedback containing information\nabout effective feedback dimensions, including feeding-up, feeding-forward, process level, and self-regulation level, and (3) GPT-4\ndemonstrated higher reliability of feedback compared to GPT-3.5.\nKeywords: Assessment Feedback, Learning Analytics, Feedback Generation, Generative Pre-Trained Transformer, Feedback\nEffectiveness, Automated Feedback System\nEmail addresses:wei.dai1@monash.edu (Wei Dai),\nyi-shan.tsai@monash.edu (Yi-Shan Tsai), jiongh.lin@gmail.com\n(Jionghao Lin), dragan.gasevic@monash.edu (Dragan Gaˇsevi´c),\nguanliang.chen@monash.edu (Guanliang Chen)\nPreprint submitted to Elsevier December 25, 2023\nTitle Page\nAssessing the Proficiency of Large Language Models in Automatic Feedback Generation:\nAn Evaluation Study\nAbstract\nAssessment feedback is important to student learning. Learning analytics (LA) powered by artificial intelligence exhibits profound\npotential in helping instructors with the laborious provision of feedback. Inspired by the recent advancements made by Generative\nPre-trained Transformer (GPT) models, we conducted a study to examine the extent to which GPT models hold the potential\nto advance the existing knowledge of LA-supported feedback systems towards improving the e fficiency of feedback provision.\nTherefore, our study explored the ability of two versions of GPT models – i.e., GPT-3.5 (ChatGPT) and GPT-4 – to generate\nassessment feedback on students’ writing assessment tasks, common in higher education, with open-ended topics for a data science-\nrelated course. We compared the feedback generated by GPT models (namely GPT-3.5 and GPT-4) with the feedback provided by\nhuman instructors in terms of readability, effectiveness (content containing effective feedback components), and reliability (correct\nassessment on student performance). Results showed that (1) both GPT-3.5 and GPT-4 were able to generate more readable feedback\nthan human instructors, (2) GPT-4 outperformed GPT-3.5 and human instructors in providing feedback containing information\nabout effective feedback dimensions, including feeding-up, feeding-forward, process level, and self-regulation level, and (3) GPT-4\ndemonstrated higher reliability of feedback compared to GPT-3.5.\nKeywords: Assessment Feedback, Learning Analytics, Feedback Generation, Generative Pre-Trained Transformer, Feedback\nEffectiveness, Automated Feedback System\n1. Introduction\nAssessment feedback is important to student learning. Qual-\nity feedback justifies grades, clarifies students’ strengths and\nareas needing improvement, and develops students’ learning\nskills. The limited quality of feedback may demotivate stu-\ndents’ understanding and uptake of feedback, harm student-\ninstructor relationships, and thus impact student success (Mid-\ndleton et al., 2023). However, due to the expansion of higher\neducation, instructors often express that they generally do not\nhave the capacity to o ffer constructive comments for a large\ncohort of students (Henderson et al., 2019). Many students\nin higher education institutions have complained about the\nfeedback they received concerning their performance, and ex-\npressed a desire for constructive assessment feedback (Mulliner\nand Tucker, 2017; Henderson et al., 2019).\nLearning analytics (LA) is an interdisciplinary field where di-\nverse disciplines such as education, psychology, computer sci-\nence and statistics intertwine with each other to generate knowl-\nedge for the improvement of learning and teaching (Chatti et al.,\n2012). The emergence of the field of LA exhibits profound po-\ntential in helping instructors with the provision of personalised\nand real-time feedback at scale by setting up rules according\nto students’ behaviour data on the learning management system\n(Arthars et al., 2019; Pardo et al., 2018). For instance, On-\nTask (Pardo et al., 2018) is one of the LA-supported feedback\nsystems that enables instructors to deliver feedback at scale by\nsetting up conditional rules about students’ learning activities\nsuch as lesson attendance, risk of failing a course, and academic\nperformance. Although these LA-supported feedback systems\nalleviate instructors’ workload in feedback provision to some\nextent, they still rely on instructors to assess students’ perfor-\nmance and formulate personalised feedback, which poses a risk\nthat students may receive feedback of sub-optimal quality due\nto instructors’ limited capacity. This substantial reliance on in-\nstructors to oversee the feedback process is incongruent with\nthe contemporary conceptualisation of feedback which posi-\ntions students as having a pivotal role in managing their learn-\ning, and focuses on developing student feedback literacy, i.e.,\n“the understandings, capacities and dispositions needed [by the\nlearner] to make sense of information and use it to enhance\nwork or learning strategies” (Carless and Boud, 2018, p.1316).\nTherefore, LA-supported systems for supporting the provision\nof feedback necessitate further exploration and investigation,\nespecially in the growing power of generative artificial intelli-\ngence (Yan et al., 2024).\nWe posit that Generative Pre-trained Transformer (GPT)\nmodels hold the potential to advance the existing knowledge\nof LA-supported feedback systems towards improving the effi-\nciency of feedback provision Yan et al. (2023). The adoption of\nself-attention mechanisms enables GPT models to handle long-\nrange dependencies, and the fine-tuning on a large amount of\nconversational data allows it to generate coherent and contex-\ntually relevant answers. GPT models have shown their poten-\ntial for feedback generation in the literature, such as generating\ncorrective feedback on students’ coding assignments (MacNeil\net al., 2022; Pankiewicz and Baker, 2023), responding to stu-\ndents’ posts on MOOC discussion forums (Li and Xing, 2021),\nand evaluating the use of e ffective praise, e.g., praising stu-\nPreprint submitted to Elsevier December 25, 2023\nManuscript\n Click here to view linked References\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \ndents’ effort, acknowledging students’ learning process instead\nof learning outcomes, in the feedback provided by instructors\n(Hirunyasiri et al., 2023). For instance, GPT-2 was adopted\nto give feedback on students’ posts published on MOOC dis-\ncussion forums (Li and Xing, 2021) and achieved comparable\nperformance on evaluation metrics of word perplexity, readabil-\nity, and coherence to that of human experts. However, lim-\nited studies have applied GPT models to generate assessment\nfeedback on writing tasks with open-ended topics such as stu-\ndents’ project proposals. This kind of writing assignment on\nopen-ended tasks is common in higher education yet instruc-\ntors often struggle to deliver comprehensive feedback for each\nstudent in large enrolment classes (Beckman et al., 2021). To\nthe best of our knowledge, only one study conducted by Dai\net al. (2023) adopted GPT-3.5 to generate assessment feedback\non writing tasks with open-ended topics. They compared the\nquality of feedback generated by GPT-3.5 and feedback written\nby instructors based on the theoretical framework of e ffective\nfeedback developed by Hattie and Timperley (2007). Results\nshowed that effectiveness of GPT-3.5 feedback is inferior to that\nof feedback written by instructors (Hirunyasiri et al., 2023).\nInspired by the recent advancements made by GPT-4 in var-\nious tasks such as solving medical challenge problems (Nori\net al., 2023) and detecting praise in tutoring dialogues (Hirun-\nyasiri et al., 2023), the current study extended the investigation\ninto the capabilities of GPT models by comparing the quality\nof feedback generated by GPT-3.5 with its advanced model,\ni.e., GPT-4, on the same task as examined by Dai et al. (2023).\nWe measured the quality of feedback generated by GPT mod-\nels in terms of their readability, e ffectiveness, and reliability.\nProviding readable feedback is pivotal to facilitating students’\nunderstanding of the information conveyed, thereby promot-\ning their actions for improvement (Evans, 2013). We eval-\nuated the readability of feedback by focusing on examining\nthe grammar, word choice and coherence. E ffective feedback\nis paramount as it guarantees that the information conveyed\nhas the greatest impact on student learning (Hattie and Tim-\nperley, 2007). We assessed the e ffectiveness of feedback by\nusing a well-known theoretical framework for feedback pro-\nposed by Hattie and Timperley (2007) and analysing the pres-\nence of e ffective feedback components in feedback generated\nby GPT-3.5, GPT-4, and instructors. We define the reliabil-\nity of machine-generated feedback as the agreement between\nmachine-generated feedback and human-generated feedback in\nterms of feedback polarity (i.e., Positive if the feedback is pro-\nvided to acknowledge students’ performance or Negative if it\nsuggests areas for improvement; Dai et al., 2023) by view-\ning human-generated feedback as the ground truth. Reliable\nfeedback, which accurately reflects the quality of students’ per-\nformance, is crucial in fostering a sense of trust between stu-\ndents and feedback providers, consequently enhancing student\nengagement in learning (Leighton and Bustos G ´omez, 2018).\nSpecifically, this study was guided by the following three re-\nsearch questions:\nRQ1 To what extent is the feedback generated by GPT models\nreadable in comparison to human-produced feedback?\nRQ2 To what extent does the GPT-generated feedback contain\neffective feedback components to guide student learning in\ncomparison to human-produced feedback?\nRQ3 To what extent is the feedback generated by GPT models\nreliable when assessing students’ performance in compar-\nison to human-produced feedback?\n2. Literature Review\n2.1. E ffective feedback design\nSince feedback plays a crucial role in the learning process\n(Lin et al., 2023a,b; Dai et al., 2023), feedback researchers have\nbeen working on developing theoretical models that explain\nhow feedback affects learning and what principles contribute to\neffective feedback design. For example, Nicol and Macfarlane-\nDick (2006) identified seven principles of good feedback prac-\ntice from the research literature on formative assessment, which\nsupport self-regulation from the cognitive, behavioural and mo-\ntivational aspects. Hattie and Timperley (2007) proposed a\nfeedback model that categorized e ffective feedback into three\ndimensions (i.e., feeding up, feeding back and feeding forward)\nand four-level focuses (i.e., task, process, self-regulatory, and\nself ). Specifically, feeding up information tells the student\nwhere they are going, which is related to the judgment about\nthe student’s attainment of the learning goal (e.g., “ In this as-\nsignment, your goal is to clearly describe the business bene-\nfits of the project. You achieved the goal. ”). Feeding back in-\nformation answers how the student is going and often consists\nof assessment comments on student performance on a specific\npart of the task (e.g., “ Your report has a very clear structure.”.\nFeeding forward information addresses where the student is go-\ning next, which usually includes future improvements on the\ntask. Feedback on task normally contains corrective informa-\ntion that indicates how well a task is performed (e.g., “ The\ninterpretation of this machine learning model is incorrect. ”).\nFeedback on process is primarily aimed at suggesting strategies\nfor completing the task (e.g., “This page may make more sense\nif you use the strategies we talked about earlier. ”). Feedback\non self-regulation addresses how students monitor their learn-\ning (e.g., “ You already know the key features of the opening\nof an argument. Check to see whether you have incorporated\nthem in your first paragraph. ”), and feedback on self is about\npersonal evaluations (e.g., “ You are a great student. ”, “ Well\ndone!”) (Hattie and Timperley, 2007). As principles proposed\nby Nicol and Macfarlane-Dick (2006) are considered too gen-\neral and unsuitable for analysing textual feedback (Cavalcanti\net al., 2020), and the feedback model proposed by Hattie and\nTimperley (2007) has been adopted widely in previous research\nto analyse textual feedback (Cavalcanti et al., 2020; Lin et al.,\n2023a; Osakwe et al., 2022), our study used the latter one to\nassess the effectiveness of feedback (RQ2).\n2.2. Learning analytics-supported feedback\nFeedback, as an essential component in guiding student\nlearning, has gained much attention from LA researchers who\n2\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \naim to enhance the quality and e fficiency of feedback by using\nlearning analytics technologies. OnTask (Pardo et al., 2018)\nis a feedback provision system where instructors can interpret\nthe results about students’ learning progress generated by au-\ntomated data analytic methods and set appropriate actions to\npersonalise feedback for students at scale. For example, if\nstudents’ activity completion is over 70%, instructors can cus-\ntomise feedback for this group of students as the following, “It\nlooks like you have been actively engaging in the activities in\nthe course. Well done!” (Lim et al., 2021). Azcona et al. (2019)\ndeveloped a predictive model to predict students’ performance\non a programming course based on their behavioural logs on\ncourseware and demographics. For groups of students with dif-\nferent levels of predicted performance, personalised feedback\nemails regarding their engagement with courseware were de-\nsigned by human experts and sent to students. Quantitative\nanalysis showed that students gained learning improvement af-\nter receiving the feedback emails and survey results indicated\nthat students’ motivation and engagement increased because\nof the LA-supported feedback emails (Azcona et al., 2019).\nThe Student Relationship Engagement System (SRES) (Arthars\net al., 2019) allows instructors to import electronic data about\nstudent learning engagement and performance data, and con-\nduct analysis to obtain students’ learning patterns, based on\nwhich instructors can provide personalised support to students\nsuch as sending emails about student learning progress to stu-\ndents. These LA-supported feedback systems typically pro-\nvide feedback based on pre-defined rules about student learn-\ning progress, which encounter limitations when applied to the\nautomation of assessment feedback for writing tasks with open-\nended topics such as essay assignments and project proposals.\nThese tasks are gaining prominence in higher educational set-\ntings due to their role in developing students’ self-regulated\nlearning skills (Beckman et al., 2021). However, pre-defining\nsample answers or common errors for these types of tasks is\nchallenging as student answers can be very diverse (Beckman\net al., 2021). Moreover, the existing LA-supported feedback\nsystems substantially rely on instructors to create the person-\nalised feedback, which does not guarantee the quality of the\nfeedback disseminated to students. Hence, it is imperative to\npursue innovative solutions for advancing LA-supported feed-\nback systems towards the goal of delivering quality feedback,\nespecially for open-ended tasks.\n2.3. Large language models for educational feedback\nWe posit that GPT-based large language models (LLMs) have\nthe potential to augment the e fficiency of feedback provision,\ndue to the encouraging outcomes demonstrated by GPT-based\nLLMs in tasks associated with feedback generation. MacNeil\net al. (2022) explored the ability of GPT-3 model to give feed-\nback on the correctness of beginner programmers’ code snip-\npets. Results revealed that GPT-3 was able to provide feedback\non the time complexity of the code, identify common mistakes,\nand summarise the goal of the code (MacNeil et al., 2022).\nA similar application of GPT-based LLMs is proposed by\nPankiewicz and Baker (2023) who utilised the GPT-3.5 model\nto generate feedback on students’ programming assignments.\nComparative experiments were conducted to assess the impact\nof feedback generated by the GPT-3.5 model on students’ code\nsubmissions. The results indicated that students who received\nfeedback generated by the GPT-3.5 model achieved a higher\npercentage of successful submissions than those who did not\nreceive GPT-3.5 generated feedback (Pankiewicz and Baker,\n2023). Instead of providing feedback to students, Hirunyasiri\net al. (2023) aimed at o ffering timely feedback to human tu-\ntors specifically on their performance on giving effective praise\nto students. They adopted the latest version of GPT models,\ni.e., GPT-4, and experimental results on synthetic tutoring di-\nalogues showed that GPT-4 performed better than human an-\nnotators in detecting specific and immediate praise. Despite\nthat these studies have presented the feasibility of GPT-based\nmodels on feedback generation, none of them has attempted to\ngenerate assessment feedback on writing tasks with open-ended\ntopics such as students’ project proposals. This kind of writ-\ning assignment on open-ended tasks is common in higher ed-\nucation yet instructors often struggle to deliver comprehensive\nfeedback for each student in large enrolment classes (Beckman\net al., 2021).\nA recent study (Dai et al., 2023) explored the capability\nof GPT-3.5 model (ChatGPT) to generate assessment feed-\nback for students’ project writing proposals in higher education\nand found that GPT-3.5 could provide more readable feedback\nthan human assessors. Nonetheless, when evaluated against\nthe presence of effective components as suggested in the feed-\nback model by Hattie and Timperley (2007), the e ffectiveness\nof GPT-3.5 generated feedback was inferior to that of feed-\nback composed by instructors, despite a considerable num-\nber of e ffective feedback components (such as task-level and\nprocess-level feedback) observed in GPT-3.5 generated feed-\nback. Moreover, GPT-3.5 did not achieve high agreement with\nhuman instructors in terms of feedback polarity (i.e., Positive\nif the feedback is provided to acknowledge students’ perfor-\nmance or Negative if it suggests areas for improvement). Unre-\nliable assessment feedback provided by the machine, i.e., pos-\nitive feedback on poor performance or negative feedback on\ngood performance, may inadvertently convey misleading infor-\nmation to students about their learning and thus discourage stu-\ndent achievement (Dawson et al., 2019; Dai et al., 2023).\nGPT-4 exhibited superior performance compared to GPT-3.5\nin various educational tasks including solving medical chal-\nlenge problems (Nori et al., 2023) and detecting praise in tu-\ntoring dialogues (Hirunyasiri et al., 2023). This enhanced per-\nformance is attributed to a significant increase in the number of\nmodel parameters and the volume of training data (Nori et al.,\n2023). Therefore, the current study aimed to incorporate this\nadvanced model to further explore the capabilities of the GPT\nmodels on the task of feedback generation.\n3. Methods\n3.1. Data Collection\n3.1.1. Instructor-generated feedback collection\nOur study obtained ethics approval from ANONYMOUS\nUniversity under project number [BLINDED]. We retrieved\n3\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nTable 1: Statistics for report length counted by the number of words.\nMin Median Max Mean Std\n203 800 1811 786.87 241.94\nthe dataset from a postgraduate-level course teaching introduc-\ntory data science skills that was delivered during the second\nsemester of 2022. In this course, students were required to pro-\npose a data science project relevant to a specific domain includ-\ning medical, financial, and educational systems, and submit a\nproject writing proposal for marking. The proposals should\ninclude two sections (project description and business model)\nabout the application of data science techniques in a specific\ndomain. Instructors evaluated the submitted proposal and pro-\nvided textual feedback for each student according to the follow-\ning five aspects specified in the marking rubric: i) clear descrip-\ntion of the goals of the project (Goal), ii) appropriateness of the\ntopic to data science ( Topic), iii) clear description of the busi-\nness benefits (Benefit), iv) novelty/creativity (Novelty), and v)\noverall clarity of the report (Clarity).\nAfter removing the student records without feedback, we fi-\nnally obtained 103 students’ proposal reports and the associ-\nated instructor-generated feedback. Note that we removed the\nidentifiable personal information of students both in reports and\nfeedback for the protection of privacy. The length of students’\nproposal reports varied from 203 to 1,811 words, which falls\nwithin GPT’s maximum input capacity. Descriptive statistics\nof the length of proposal reports, and the length of instructor-\ngenerated feedback are presented in Table 1 and Table 2, re-\nspectively.\n3.1.2. Feedback Generation by GPT-3.5 and GPT-4\nWe accessed GPT-3.5 and GPT-4 via the interface of Chat-\nGPT developed by OpenAI Aydın and Karaarslan (2022). GPT\nmodels are able to generate responses from seeing a prompt\ndescribing the task (i.e., an instruction or a query written in\nnatural language by a user for the model to execute). In the\ncurrent study, the task for GPT models was to generate textual\nfeedback on students’ proposal reports in terms of five assess-\nment aspects. Thus, we designed the prompt for GPT-3.5 and\nGPT-4 as follows, “Please give feedback on the following text\nin terms of a clear description of the goals of the project, ap-\npropriateness of the topic to data science, a clear description\nof the business benefits, novelty/creativity and overall clarity of\nthe report. <INSERT THE TEXT OF A REPORT >”. For each\nstudent’s proposal, we inserted the text of their proposal report\ninto the prompt and submitted it to GPT-3.5 and GPT-4 to ob-\ntain generated feedback. To prevent student assignments from\nbeing collected by models, we unchecked the option of “Chat\nhistory & training” in the settings of the model interface. The\nstatistics of the length of feedback generated by GPT-3.5 and\nGPT-4 are shown in Table 2.\nTable 2: Statistics for feedback length counted by the number of words.\nFeedback Min Median Max Mean Std\nInstructor 6 51 143 57.34 30.60\nGPT-3.5 102 160 270 166.44 34.81\nGPT-4 275 361 522 369.88 51.81\n3.2. Measuring readability\nTo answer RQ1, we adopted a widely-used measure – i.e.,\nreadability (van der Lee et al., 2021; Jia et al., 2022) – to exam-\nine the quality of the machine-generated text in grammar, word\nchoice, and coherence. In line with the process of evaluating\nreadability in written products (Jia et al., 2022), we invited three\nhuman raters, and each of them was asked to score all pieces of\nfeedback generated by instructors, GPT-3.5 and GPT-4 using a\nfive-point scale where: (i) 0 denoted Incomprehensible, (ii) 1\nNot fluent and incoherent, (iii) 2 Somewhat fluent but incoher-\nent, (iv) 3Fluent but somewhat incoherent, and (v) 4Fluent and\ncoherent. As the evaluation for text readability varied from one\nindividual human rater to another, we calculated the average\nscore of the three human raters as the final metric of readability\nfor each piece of feedback.\n3.3. Measuring feedback e ffectiveness\nTo answer RQ2, we used a well-known theoretical frame-\nwork for feedback proposed by Hattie and Timperley (2007)\nand analysed the presence of e ffective feedback components\nin the feedback generated by GPT-3.5, GPT-4, and instructors.\nPlease refer to Section 2.1 for the details of this framework. We\nrecruited two experts to annotate feedback by using the three\ndimensions and four levels proposed by Hattie and Timperley\n(2007). After a pre-training session about annotation rules, each\nexpert annotated 309 pieces of feedback in our dataset (103\npieces of instructor-generated feedback, 103 feedback gener-\nated by GPT-3.5, and 103 feedback by GPT-4). To avoid in-\ntroducing bias towards human instructors or GPT models, we\ndid not inform the experts which feedback was written by in-\nstructors and which was generated by GPT-based models. Two\nexperts achieved high agreement scores between their annota-\ntions, as shown in Table 3. For feedback received inconsistent\nlabels from two experts, a third expert was invited to make final\ndecision. We used the labels with inconsistencies resolved by\nthe third expert for analysis.\n3.4. Measuring the reliability of feedback generated by GPT-\nbased models\nProviding students with reliable feedback regarding their as-\nsignment performance is of great value for on-going learning\nexperience (LeClercq, 1999). When students accomplish the\nset goal, the instructor may acknowledge the students’ effort in\nthe feedback. When students perform undesirably, the instruc-\ntor may indicate the areas that they should further improve. In\nthis paper, we use Polarity to denote whether the feedback is\ngiven to acknowledge students’ effort (Positive) or indicate the\n4\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nareas that they should further improve (Negative). If a feedback\ngenerator was unable to give reliable feedback that accurately\nindicated how well students’ performance was – i.e., positive\nfeedback on poor performance or negative feedback on good\nperformance – the generated feedback may have inadvertently\nmisled the student and negatively impacted learning. Hence,\nwe investigated the reliability of GPT-generated feedback by\nmeasuring the agreement between GPT models and human in-\nstructors in terms of feedback polarity.\nTwo human annotators were recruited to identify the feed-\nback polarity. Each piece of feedback from either instructor or\nGPT-based models was generated by assessing the students’ re-\nports based on five aspects (i.e., Goal, Topic, Benefit, Novelty\nand Clarity). Thus, we identified the polarity of feedback for\neach of these aspects. Specifically, if feedback was given to\nacknowledge a student’s effort on a specific aspect, then the an-\nnotator marked it with Positive, whereas if feedback was given\nto indicate that the student needs to improve on a specific as-\npect, the annotator marked it with Negative. If the feedback did\nnot contain any comments on a specific aspect, then the annota-\ntor marked it with None. For each piece of instructor-generated\nor GPT-generated feedback, we obtained five labels, and each\nof them indicates the feedback polarity (i.e., “Positive”, “Neg-\native” or “None”) on each of the five assessment aspects.\nWe then evaluated the ability of GPT models on generating\nfeedback for each assessment aspect with accurate polarity. By\ndoing so, we calculated precision, recall and F1 score (shown\nin Equations 1, 2 and 3), three commonly-used metrics for re-\nliability of multi-class classification tasks, by regarding three\nfeedback polarities as three classes, the labels of instructor feed-\nback as the ground truth, and labels of feedback generated by\nGPT-based models as predicted classes, as the purpose of this\nstudy was to evaluate the feasibility of using GPT-based models\nto support human educators in feedback provision. For exam-\nple, supposing that GPT-3.5 gave positive feedback on the as-\npect of Topic to four reports, while only two of them obtained\npositive feedback from the instructor on the same aspect, then\nthe precision was 2 /4 (i.e., Precision = 0.50). Supposing that\nthe instructor gave positive feedback on the aspect of Topic to\n6 reports in total, the recall was calculated as 2 /6 (0.33). F1\nscore was calculated as 0.4, i.e., the harmonic mean of the pre-\ncision and recall scores, as shown below.\nPrecision = TruePositive\nTruePositive + FalsePositive (1)\nRecall = TruePositive\nTruePositive + FalseNegative (2)\nF1 = 2 ∗Precision ∗Recall\nPrecision + Recall (3)\nWhen we annotated feedback polarity, we found that hu-\nman instructors did not comment on some of the assessment\n/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000052/uni00000055/uni0000002a/uni00000033/uni00000037 /uni00000010/uni00000016/uni00000011/uni00000018/uni0000002a/uni00000033/uni00000037 /uni00000010/uni00000017\n/uni00000015/uni00000011/uni00000013\n/uni00000015/uni00000011/uni00000018\n/uni00000016/uni00000011/uni00000013\n/uni00000016/uni00000011/uni00000018\n/uni00000017/uni00000011/uni00000013\nFigure 1: The distribution of feedback readability.\naspects defined in the marking rubric (i.e., Goal, Topic, Ben-\nefit, Novelty and Clarity), while GPT models o ffered compre-\nhensive feedback (i.e., feedback on more assessment aspects in\nthe marking rubric). For example, for the assignment submit-\nted by a specific student, the human instructor provided com-\nments only on the assessment aspects ofGoal and Topic. How-\never, GPT-generated feedback on the same assignment con-\ntained comments on three assessment aspects including Goal,\nTopic and Benefit. In this case, for the assessment aspect of\nBenefit, we did not have the ground truth, i.e., the human in-\nstructor’s feedback on this aspect, to compare the identified\nfeedback polarity on Benefit between a GPT model and the\nhuman instructor. Therefore, we need to further evaluate the\nreliability of this group of GPT-generated feedback comments.\nWe first extracted all the GPT-generated feedback comments on\nthe assessment aspects that human instructors did not comment\non from our dataset. Then, we asked a human assessor to as-\nsess the assignment submitted by the student who received this\nfeedback and then answer the question:“Do you agree with this\nfeedback regarding the student’s assignment?”. The human as-\nsessor selected the answer from the following three options: i)\nAgree; ii) Partially agree, and iii) Disagree. As a piece of feed-\nback could contain multiple sentences of comments on different\nassessment aspects and the human assessor agrees with some of\nthese sentences, we provided the option of “Partially agree” for\nthe human assessor as an option to choose.\n4. Results\n4.1. RQ1\nTable 4 presents the readability scores between GPT-3.5,\nGPT-4, and instructor feedback. Statistical di fference of\nreadability between human, GPT-3.5, and GPT-4 are shown\nin Table 5. We can see that feedback generated by both\nGPT-based models was significantly more readable than\ninstructor-generated feedback. Figure 1 shows the distribution\nof readability scores for the feedback provided by instructors\nand two GPT models. The distribution indicates that the\nmajority of readability scores on GPT-generated feedback was\nrated between 3.75 to 4.0, whereas most instructor feedback\n(over 75%) was below the readability score of 3.75 with a\nhigher standard deviation than those of the GPT-based models.\n5\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nTable 3: Agreement scores and Cohen’s κbetween two annotators.\nCategories Agreement score Cohen’sκ\nFive assessment aspects Instructor GPT-3.5 GPT-4 Instructor GPT-3.5 GPT-4\nGoal 1.00 0.98 0.99 1.00 0.87 0.98\nTopic 0.98 0.98 1.00 0.93 0.85 1.00\nBenefit 0.99 0.95 0.98 0.98 0.83 0.96\nNovelty 0.98 0.88 0.98 0.98 0.81 0.96\nClarity 0.97 0.92 0.96 0.94 0.82 0.89\nSeven effective feedback components\nFeeding up 1.00 1.00 0.98 1.00 None 2 0.79\nFeeding back 0.97 1.00 1.00 0.99 1.00 All 1\nFeeding forward 0.99 0.98 0.99 0.95 0.96 0.80\nTask 1.00 1.00 1.00 All1 All1 All1\nProcess 0.95 0.92 0.98 0.85 0.84 0.74\nSelf-regulation 0.96 1.00 0.94 0.81 None 2 0.77\nSelf 0.98 1.00 1.00 0.95 None 2 None2\n1All feedback contained comments in this category.\n2No feedback contained comments in this category.\nTable 4: Statistics for feedback readability.\nFeedback Min Median Max Mean Std\nInstructor 2.33 3.33 4 3.21 0.52\nGPT-3.5 2.67 3.67 4 3.76 0.28\nGPT-4 3.33 4 4 3.91 0.18\nWithin GPT-based models, GPT-4 performed significantly\nbetter than GPT-3.5 did in generating readable feedback. By\nscrutinizing the content of feedback generated by instructors,\nGPT-3.5 and GPT-4 (Feedback examples on the same student\nassignment extracted from our dataset can be seen in Table 6),\nwe found some di fferences between these three types of feed-\nback. Feedback from the instructor consisted mostly of simple\nexpressions about the evaluation of student performance, while\nGPT-generated feedback usually contained a brief summary\nof students’ works and the evaluation feedback followed by\nan explanation of how this feedback was given. Comparing\nGPT-3.5 and GPT-4, we can see most of the feedback by GPT-4\nwas formatted using bullet points to clarify the comments for\neach aspect in the marking rubric.\n4.2. RQ2\nTable 7 reveals that human assessors provided feeding-up\ninformation (i.e., information related to the judgment about the\nstudent’s attainment of the learning goal) only in six pieces of\nfeedback in the used dataset. GPT-3.5 did not o ffer any of this\ninformation in its generated feedback. In contrast, GPT-4 gen-\nerated feeding-up information in almost every piece of feed-\nback (94.17%). This di fference was not observed in the pro-\nvision of feeding-back information (i.e., information related\nto the assessment on student performance on a specific part of\nthe task), as nearly 100% feedback generated by human asses-\nsors, GPT-3.5, and GPT-4 contained this information.Feeding-\nforward information (i.e., information related to suggestions\nfor future improvements on the task) was detected in 93 pieces\nof feedback written by human assessors. The same informa-\ntion was detected less frequently in the feedback generated by\nGPT-3.5. It is worth noting that GPT-4 performed the best in\ngenerating feeding-forward information (existing in 100 out\nof 103 pieces of the generated feedback).\nWe observed task level information in every piece of feed-\nback generated by human assessors, GPT-3.5 and GPT-4. As\nfor process level information, we detected it in 80% of feed-\nback from human assessors, compared to 57% of feedback gen-\nerated by GPT-3.5. Notably, this information was detected\n100% in the feedback generated by GPT-4. Information on\nself-regulation was detected in only 11 (out of 103) pieces of\nfeedback written by human assessors. This information was\nnot observed in GPT-3.5-generated feedback, but in 18 pieces\nof feedback generated by GPT-4, slightly higher than human-\ngenerated feedback. Finally, the self level information was de-\ntected in 25 pieces of human feedback, but not in GPT-3.5 or\nGPT-4 generated feedback at all.\nOverall, GPT-4 appeared to have outperformed both GPT-3.5\nand human instructors in terms of producing balanced feedback\nthat consistently covers most of the seven e ffective feedback\ncomponents except for the self-regulation and self levels. It is\nworth noting that despite the relatively lower presence of infor-\nmation on self-regulation, GPT-4 still outperformed GPT-3.5\nand human instructors in this aspect. However, it is clear that\nboth GPT models placed less emphasis on self-level feedback,\nwhich can be an important feedback element to motivate stu-\ndents, especially through a ffirming their performance (Hattie\nand Timperley, 2007).\n6\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nTable 5: Statistical difference between the three groups using t-test and Cohen’s d. Significant values are denoted with “*”\nGroups t-test1 Cohen’s d\nt-score p-value adjusted p-value 2 Mean of X Mean of Y e ffect size\nInstructor vs GPT-3.5 -9.50 1.07e-15 3.20e-15 ∗∗∗ 3.21 3.76 -1.31\nInstructor vs GPT-4 -12.56 2.00e-22 5.99e-22 ∗∗∗ 3.21 3.91 -1.79\nGPT-3.5 vs GPT-4 -4.79 5.75e-06 1.73e-05 ∗∗∗ 3.76 3.91 -0.64\n1t-tests were conducted as post hoc tests after ANOV A test (f-value= 109.76 and p-value = 1.17e-36).\n2p-value was adjusted using Bonferroni correction.\nTable 6: Feedback examples on the same assignment generated by human instructor, GPT-3.5 and GPT-4. Sentences that summarise the assignment are omitted.\nFeedback generator Feedback example\nInstructor “The description is clear. Good choice of topic. Good description of the business model. A novel creative idea. Clear\nand well structured report.”\nGPT-3.5 “The text provided describes a project that aims to ... The text also mentions ... Overall, the goals of the project are\nclear and the use of data science in analyzing job listings is appropriate. The potential business benefits of the project,\nsuch as reducing costs and identifying the requirements for different data science roles, are also clearly described. The\nnovelty and creativity of the project may depend on the specific methods and analyses used in the project. The overall\nclarity of the report could be improved by providing more detail on the methods and analyses that will be used, as well\nas the expected results and implications of the project.”\nGPT-4 “Overall, the report has a promising project idea but needs substantial editing for clarity, precision, and professional-\nism. Here’s a breakdown of the feedback based on your requested criteria:\n• Clear Description of the Goals of the Project : The goal of the project is generally clear. It aims to ... However,\nthese goals could be presented more concisely and clearly.\n• Appropriateness of the Topic to Data Science: The topic is very relevant to data science as it involves the analysis\nof job listings, a type of structured data. The proposal mentions various analytical tasks, such as...\n• Clear Description of the Business Benefits : The report mentions the benefits for students, companies, and the\ngovernment, but the explanations are somewhat vague and confusing. The report suggests that ... It’s not clear\nwhat costs and problems it refers to, and how exactly they would be reduced. The report also suggests... , but this\nconcept is not well defined or explained.\n• Novelty/Creativity: The project is reasonably novel in focusing specifically on ... However, the creativity of the\nproposed methods is hard to assess based on the report. The report does not elaborate on the specific techniques\nor algorithms to be used in the analysis.\n• Overall Clarity of the Report : The overall clarity of the report could be improved. The report contains several\nlong, run-on sentences, which make it hard to follow. The writing style could be more academic and professional.\nThere are also some spelling and grammar errors. The structure of the report could be clearer, with distinct\nsections for the project goals, the data to be used, the analysis methods, the expected benefits, and the potential\nchallenges. The references section could also be formatted more professionally. ”\n7\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nTable 7: The comparison of the distribution of seven effective feedback components between the feedback provided by the human instructor, generated by GPT-3.5\nand GPT-4. For each component, ANOV A tests were performed to measure the statistical difference between the three feedback generators. Subsequent post hoc\ncomparisons were conducted using t-tests. Paris of feedback generators that demonstrated significant di fferences (p-value <= 0.05) are listed in the final column.\nFor example, “GPT-4 vs {Instrutcor,GPT-3.5}” signifies that significant di fferences were identified between GPT-4 and Instructor, as well as between GPT-4 and\nGPT-3.5. The term “All” indicates that each pair of feedback generators exhibited a significant di fference, whereas “None” denotes the absence of significant\ndifference among all three feedback generators.\nComponents Instructor GPT-3.5 GPT-4 Significant results\nQuantity Frequency Quantity Frequency Quantity Frequency\nFeeding up 6 5.83% 0 0% 97 94.17% GPT-4 vs {Instructor, GPT-3.5}\nFeeding back 101 98.06% 102 99.03% 103 100% None\nFeeding forward 93 90.29% 63 61.17% 100 97.09% GPT-3.5 vs {Instructor, GPT-4}\nTask 103 100% 103 100% 103 100% None\nProcess 82 80% 57 55% 100 97.09% All\nSelf-regulation 11 11% 0 0% 18 17.48% GPT-3.5 vs {Instructor, GPT-4}\nSelf 25 24% 0 0% 0 0% Instructor vs {GPT-3.5, GPT-4}\nTable 8: Polarity of feedback generated by human instructors, GPT-3.5, and GPT-4 according to the five assessment aspects in the marking rubric. The precision,\nrecall and F1 for measuring the agreement between GPT models and human assessors in terms of feedback polarity are included.\nAspects Polarity Quantity Precision Recall F1\nInstructor GPT-3.5 GPT-4 GPT-3.5 GPT-4 GPT-3.5 GPT-4 GPT-3.5 GPT-4\nPositive 5 95 74 0.04 0.05 0.80 0.80 0.08 0.10\nGoal Negative 40 3 29 0.33 0.62 0.03 0.45 0.06 0.52\nNone 58 5 0 0.40 0.00 0.03 0.00 0.06 0.00\nPositive 86 95 99 0.84 0.85 0.93 0.98 0.88 0.91\nTopic Negative 5 0 3 0.00 0.67 0.00 0.40 0.00 0.50\nNone 12 8 1 0.13 0.00 0.08 0.00 0.10 0.00\nPositive 19 85 64 0.20 0.22 0.90 0.74 0.33 0.34\nBenefit Negative 59 5 37 0.60 0.73 0.05 0.46 0.09 0.56\nNone 25 13 2 0.31 0.00 0.16 0.00 0.21 0.00\nPositive 31 52 64 0.35 0.44 0.58 0.90 0.44 0.59\nNovelty Negative 19 22 39 0.27 0.33 0.32 0.68 0.29 0.44\nNone 53 29 0 0.59 0.00 0.32 0.00 0.41 0.00\nPositive 24 77 76 0.21 0.28 0.67 0.88 0.32 0.42\nClarity Negative 11 17 27 0.29 0.19 0.46 0.46 0.36 0.27\nNone 68 9 0 0.44 0.00 0.06 0.00 0.11 0.00\nTable 9: “No. of feedback” denotes the number of feedback generated by GPT-3.5 or GPT-4 on the assessment aspect that was not commented on by the original\nhuman assessors. “Quantity” denotes the number of feedback that the human instructor agreed (partially agreed /disagreed) with. “Percent” was calculated as\n“Quantity” divided by “No. of feedback”.\nModel No. of feedback Agree PartAgree Disagree\nQuantity Percent Quantity Percent Quantity Percent\nGPT-3.5 60 44 73.33% 9 15% 7 11.67%\nGPT-4 101 91 90.10% 7 6.93% 3 2.97%\n8\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \n4.3. RQ3\nThe results for answering RQ3 are presented in Table 8. In\nterms of generating feedback according to the marking rubric,\nGPT-4 provided comments on almost all the five assessment as-\npects in every single feedback except for a relatively low num-\nber of feedback that failed to cover the Topic (n = 1) and Ben-\nefit (n = 2) aspects. Compared to GPT-4, GPT-3.5 performed\nslightly worse in covering these five aspects. In particular, 29\npieces of GPT-3.5 feedback missed comments on the Novelty\naspect. Compared to GPT-4 and GPT-3.5, human instructors\nappeared to have missed comments on these five aspects more\nfrequently. In particular, over half of the students did not re-\nceive comments on Goal, Novelty, and Clarity.\nBy observing the quantity of feedback of di fferent polarity,\nwe found that both the instructor and GPT models gave a ma-\njority of students positive feedback on the aspect of Topic, i.e.,\n86 out of 103 positive feedback by the instructor, 95 out of 103\nby GPT-3.5, and 74 out of 103 by GPT-4. On the other four\naspects, students mostly received either negative feedback or\nempty feedback (i.e., feedback that did not contain any com-\nments on a specific aspect, which was denoted as None in Table\n8) from the instructor, in comparison with GPT models which\ngenerated more positive feedback than the instructor.\nBy observing values of precision, recall and F1 (introduced\nin Sec. 3.4) in Table 8, we can see that among the five assess-\nment aspects, the GPT models achieved the highest agreement\nscore with human instructors (precision: 0.85, recall: 0.98, F1:\n0.91) in positive polarity for the aspect of Topic (note that we\nconsidered human-generated feedback as the ground truth in\nterms of the detected feedback polarity). This suggests that both\nthe instructor and GPT models gave a majority of students pos-\nitive feedback on the aspect of Topic, i.e., 86, 95, and 99 out of\nthe 103 feedback by human instructors, GPT-3.5, and GPT-4,\nrespectively, contained positive comments on this aspect. Al-\nthough GPT models did not achieve high agreement with in-\nstructors on the other four assessment aspects, we can observe\nan increase of F1 for GPT-4 compared to GPT-3.5 except for\nnegative polarity for the aspect of Clarity.\nAccording to Table 9, we can see that 60 out of 103 pieces\nof feedback generated by GPT-3.5 had comments related to the\nassessment aspects that were not commented on by the origi-\nnal human instructors, and 73.33% of these pieces of feedback\nobtained agreement from the human assessor. In comparison to\nGPT-3.5, more feedback pieces generated by GPT-4 (101 out\nof 103 pieces of feedback) contained comments that were not\ncommented on by the original human instructors, and 90.10%\nof these pieces of feedback obtained agreement from the hu-\nman assessor. The proportion of the feedback that the human\nassessor disagreed with for both generators – i.e., GPT-3.5 and\nGPT-4 – was lower than those the human assessor agreed and\npartially agreed with. We extracted examples of feedback com-\nments generated by GPT-3.5 and GPT-4 on the assessment as-\npects that were not commented on by the human instructor in\nTable 10\n5. Discussion\n5.1. Interpretation of the Results\nThe results of RQ1 showed that the GPT models were able to\ngenerate more readable feedback with greater consistency than\nhuman instructors. This is not surprising because the ability\nof LLMs to provide readable responses in various contexts has\nbeen demonstrated in the literature Li and Xing (2021); Patel\net al. (2023). For example, Li and Xing (2021) indicated that\nGPT-2 could provide readable and coherent replies to human\nlearners’ posts on MOOC discussion forums to a similar ex-\ntent compared to human instructors. Human instructors often\nstruggle to consistently deliver quality feedback that meets stu-\ndents’ expectations Weaver (2006). As investigated by Dai et al.\n(2022), there can be inconsistent delivery of polite feedback\nacross students with different levels of performance and demo-\ngraphics. Existing LA-supported feedback systems mostly rely\non human instructors to create feedback Pardo et al. (2018);\nArthars et al. (2019), which risks delivering feedback of incon-\nsistent quality. This could be mitigated by incorporating large\nlanguage models such as GPT models in LA-supported feed-\nback systems to help human instructors provide feedback of\nconsistent readability to make students understand the feedback\nbetter and thus act upon the given feedback Evans (2013).\nThe results of RQ2 indicated that GPT-4 was able to gen-\nerate feedback containing e ffective components more consis-\ntently than human instructors, based on the prominent theoreti-\ncal framework proposed by Hattie and Timperley (2007). This\nis surprising as the evidence has previously been reported (Dai\net al., 2023) that large language models such as the GPT-3.5\nmodel performed less well than human instructors in providing\nfeedback that covers e ffective components by using the same\ntheoretical framework (Hattie and Timperley, 2007). Specifi-\ncally, GPT-4 was superior to GPT-3.5 and even human instruc-\ntors in providing feedback with more e ffective components,\nespecially the feeding-up information, process-level and self-\nregulation level information. The high presence of feeding-up\ninformation in GPT-4 generated feedback boils down to the\nfact that GPT-4 explicitly included the assessment aspects in\nthe marking rubric, i.e., i) clear description of the goals of the\nproject, ii) appropriateness of the topic to data science, iii) clear\ndescription of the business benefits, iv) novelty /creativity, and\nv) overall clarity of the report, and connected them with student\nperformance in the generated feedback to inform students of the\nattainment of learning goals, as marked in bold in the feedback\nexamples provided in Table 6. By contrast, GPT-3.5 and human\ninstructors very rarely referred to the assessment aspects in the\nprovided feedback. Our study also found that human instructors\nprovided less feedback at the self-regulation level compared to\nthe other three levels ( task, process, and self). This finding\naligns with the findings previous reported in the literature (Os-\nakwe et al., 2022; Cavalcanti et al., 2020).\nThe results of RQ3 in Table 8 indicated low precision, recall\nand F1 for the GPT models in all assessment aspects except\nfor the aspect of Topic in terms of the agreement on feedback\npolarity by considering the polarity of feedback generated by\nhuman instructors as the ground truth. This means that GPT\n9\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nTable 10: Examples of feedback comments generated by GPT-3.5 and GPT-4 on the assessment aspects the human instructor did not comment on.\nGPT-3.5\nExample 1: “However, the text could be made more concise and clearer in some parts, such as the description of the project roles\nand the classification of content composers. ”\nExample 2: “In terms of novelty and creativity, the text could benefit from discussing any unique or innovative approaches to data\nanalysis or prediction being used on the platform. ”\nExample 3: “However, it may be helpful to include more information on the specific data sources and methods that will be used in\nthe analysis, as well as the expected outcomes and potential impact of the project. It would also be helpful to provide more detail\non the real-world problem the project aims to solve and how the proposed solution will address it. ”\nGPT-4\nExample 1: “However, a few improvements could be made for better clarity: The Abstract could be more precise about the specific\ngoals and objectives of the study. The ’Business Model’ section could be more directly related to Liverpool FC, and how data\nscience contributes to each revenue stream. The report could be more explicit about the project’s specific objectives, methods, and\nexpected outcomes, which is typical in a project proposal. ”\nExample 2 : “The topic is appropriate for data science as it involves data collection, cleaning, and analysis, as well as the\napplication of algorithms to make recommendations based on user needs and market demands. However, it would be beneficial to\nprovide more detail about the types of data science techniques and methodologies to be used, for example, which machine learning\nalgorithms might be relevant for this project. ”\nExample 3: “One suggestion is to elaborate more on the data science methods to be employed, particularly on the machine\nlearning aspect. Which specific models or algorithms do you plan to use? Also, consider discussing how you would address\npotential privacy concerns related to collecting personal data like medical history. ”\nmodels could generate relatively reliable results in assessing\nwhether the students performed well or poorly in generating\na report that aligned with the topics defined for the assignment.\nHowever, GPT models could not offer a reliable assessment of\nstudent performance on the other aspects of the rubric. Table 8\nshows that human instructors did not offer comprehensive feed-\nback (i.e., feedback on each of the assessment aspects in the\nmarking rubric) to students. This may be attributed to limited\ntime human instructors have to provide feedback on students’\nassignments. Therefore, they need to prioritise particular infor-\nmation they want to emphasise in their feedback. However, this\nfeedback can be constrained, as human instructors might only\ncomment on the assessment aspects that they noticed in the as-\nsignment within the limited time available. This is not appli-\ncable to GPT models, as they were asked to generate feedback\non all the assessment aspects. This di fference might explain\nthe low precision, recall and F1 in terms of feedback polarity.\nThe results in Table 9 revealed that the human assessor agreed\nwith the majority of feedback that GPT models generated on\nthe assessment aspects that human instructors did not comment\non. This observation suggests that GPT models possess the po-\ntential to assist human instructors in delivering feedback that\ncovers the comprehensive aspects of student performance.\n5.2. Implications\nTo promote the effective use of feedback, instructors need to\npay more attention to the presence of e ffective components in\nthe crafted feedback (Hattie and Timperley, 2007). According\nto the results of our study, GPT-4 yielded better performance in\ngenerating feedback that contains e ffective feedback elements.\nTherefore, instructors can leverage GPT models to improve the\nquality of their feedback by promoting GPT models to include\nmore feeding up, process level and self-regulation level infor-\nmation. The instructors can also use LLMs to improve their\nfeedback skills (Markel et al., 2023). However, manually pol-\nishing feedback via ChatGPT interface for a large number of\nstudents can be time-consuming. Prompt design might be a\nchallenging task for instructors without a background in engi-\nneering large language models. An alternative way is to encour-\nage students to seek feedback from GPT models. As shown\nin our study, GPT models can generate more comprehensive\nfeedback than human instructors and can detect issues in their\nwork, which could be omitted by human instructors. More-\nover, by positioning the student as the central actor in the feed-\nback process, the skills required to evaluate their own work,\nseek feedback from multiple sources, and use feedback to im-\nprove, commonly known asfeedback literacy, can be developed\n(Ryan et al., 2023; Carless and Boud, 2018). Given that LA\ncan offer opportunities to trace students’ engagement with feed-\nback (Lang et al., 2017), the integration of GPT models into\nLA-supported feedback systems can close a full feedback loop\n(which is always at the core of LA Clow (2012)). This loop\naims not only to develop and enhance student feedback liter-\nacy by understanding students’ sense-making and action-taking\nprocesses, but also encourage students to actively engage with\ntheir feedback Carless and Boud (2018).\nDespite the benefits brought by generative artificial intelli-\ngence such as GPT models in education, issues associated with\n10\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nthe incorporation of GPT models in feedback provision need\nto be carefully addressed. While GPT models are advanced,\nthey are not infallible and may generate inaccurate or unreli-\nable feedback information which could inadvertently mislead\nstudent learning and lead to negative emotions towards learn-\ning (Middleton et al., 2023). As revealed in our study, GPT\nmodels could not offer a reliable assessment of student perfor-\nmance on the aspects of the rubric except for Topic. Therefore,\nstudents can not solely rely on GPT-based tools to obtain feed-\nback. Feedback from instructors, peers and other educational\nresources is necessary to ensure an accurate and reliable learn-\ning environment (Shute, 2008).\nIn the meantime, system developers and LA researchers who\nare seeking ways to incorporate LLMs into LA-supported feed-\nback systems need to devise a way to validate and update GPT\nmodels regularly to improve the reliability of the generated\nfeedback. A potential approach could be creating an iterative\nfeedback loop where instructors and students can provide im-\nmediate responses to the feedback given by GPT models re-\ngarding its relevance to student assignment, and its reliability\nas a source of assessment information. This mechanism could\nhelp with immediate refinements and adaptations on GPT mod-\nels, thereby contributing to the progressive enhancement of the\nmodels for the provision of reliable assessment feedback.\n5.3. Limitations\nFirstly, GPT models generated feedback in an unsupervised\nway, which could potentially influence the reliability of the gen-\nerated assessment feedback in our study. Future e fforts need\nto consider designing feedback samples that clearly and accu-\nrately describe student performance, and submit them to train\nGPT models to generate feedback with higher reliability. Sec-\nondly, we measured the e ffectiveness of feedback based on\none single theoretical feedback framework proposed by Hat-\ntie and Timperley (2007), but the e ffectiveness of feedback\ngoes beyond the content coverage of feedback (Yang and Car-\nless, 2013; Carless and Boud, 2018; Middleton et al., 2023).\nTherefore, feedback models to facilitate relationships between\ninstructors and students (Carless and Boud, 2018; Middleton\net al., 2023), or to develop student feedback literacy (Yang and\nCarless, 2013) could be adopted to evaluate the quality of GPT-\ngenerated feedback. Future studies are also needed to com-\npare the learning outcomes between students who receive GPT-\ngenerated feedback and those who receive instructor-written\nfeedback to better evaluate the effectiveness of feedback based\non LLMs. Lastly, the assignments and corresponding feedback\nwere collected from the same course and for a single type of as-\nsessment task. It is important to examine the ability of GPT\nmodels for feedback generation for other courses in various\nsubjects and other types of assessment tasks to determine the\ngeneralisation of the findings presented in this paper.\nReferences\nArthars, N., Dollinger, M., Vigentini, L., Liu, D.Y .T., Kondo, E., King, D.M.,\n2019. Empowering teachers to personalize learning support: Case studies of\nteachers’ experiences adopting a student-and teacher-centered learning an-\nalytics platform at three australian universities. Utilizing learning analytics\nto support study success , 223–248.\nAydın, ¨O., Karaarslan, E., 2022. OpenAI ChatGPT generated literature review:\nDigital twin in healthcare. Available at SSRN 4308687 .\nAzcona, D., Hsiao, I.H., Smeaton, A.F., 2019. Detecting students-at-risk in\ncomputer programming classes with learning analytics from students’ digital\nfootprints. User Modeling and User-Adapted Interaction 29, 759–788.\nBeckman, K., Apps, T., Bennett, S., Dalgarno, B., Kennedy, G., Lockyer, L.,\n2021. Self-regulation in open-ended online assignment tasks: the impor-\ntance of initial task interpretation and goal setting. Studies in Higher Edu-\ncation 46, 821–835.\nCarless, D., Boud, D., 2018. The development of student feedback literacy:\nenabling uptake of feedback. Assessment & Evaluation in Higher Education\n43, 1315–1325.\nCavalcanti, A.P., Diego, A., Mello, R.F., Mangaroska, K., Nascimento, A., Fre-\nitas, F., Gaˇsevi´c, D., 2020. How good is my feedback? a content analysis of\nwritten feedback, in: Proceedings of the 10th International Conference on\nLearning Analytics & Knowledge, pp. 428–437.\nChatti, M.A., Dyckho ff, A.L., Schroeder, U., Th ¨us, H., 2012. A reference\nmodel for learning analytics. International Journal of Technology Enhanced\nLearning 4, 318–331.\nClow, D., 2012. The learning analytics cycle: closing the loop e ffectively, in:\nProceedings of the 2nd international conference on learning analytics and\nknowledge, pp. 134–138.\nDai, W., Lin, J., Jin, F., Li, T., Tsai, Y .S., Gasevic, D., Chen, G., 2023. Can large\nlanguage models provide feedback to students? a case study on ChatGPT,\nin: Proceedings of the 23rd IEEE International Conference on Advanced\nLearning Technologies, pp. 323–325.\nDai, W., Tsai, Y .S., Fan, Y ., Gaˇsevi´c, D., Chen, G., 2022. Measuring incon-\nsistency in written feedback: A case study in politeness, in: Proceedings of\nthe 23rd International Conference of Artificial Intelligence in Education, pp.\n560–566.\nDawson, P., Henderson, M., Mahoney, P., Phillips, M., Ryan, T., Boud, D.,\nMolloy, E., 2019. What makes for e ffective feedback: sta ff and student\nperspectives. Assessment & Evaluation in Higher Education 44, 25–36.\nEvans, C., 2013. Making sense of assessment feedback in higher education.\nReview of educational research 83, 70–120.\nHattie, J., Timperley, H., 2007. The power of feedback. Review of Educational\nResearch 77, 81–112.\nHenderson, M., Ryan, T., Phillips, M., 2019. The challenges of feedback in\nhigher education. Assessment & Evaluation in Higher Education .\nHirunyasiri, D., Thomas, D.R., Lin, J., Koedinger, K.R., Aleven, V ., 2023.\nComparative analysis of gpt-4 and human graders in evaluating praise given\nto students in synthetic dialogues. arXiv preprint arXiv:2307.02018 .\nJia, Q., Young, M., Xiao, Y ., Cui, J., Liu, C., Rashid, P., Gehringer, E., 2022.\nInsta-reviewer: A data-driven approach for generating instant feedback on\nstudents’ project reports. Proceedings of the 15th International Conference\non Educational Data Mining , 5–16.\nLang, C., Siemens, G., Wise, A., Gasevic, D., 2017. Handbook of learning\nanalytics. SOLAR, Society for Learning Analytics and Research New York.\nLeClercq, T., 1999. Principle 4: Good practice gives prompt feedback. J. Legal\nEduc. 49, 418.\nvan der Lee, C., Gatt, A., van Miltenburg, E., Krahmer, E., 2021. Human\nevaluation of automatically generated text: Current trends and best practice\nguidelines. Comput Speech Lang 67, 101151.\nLeighton, J.P., Bustos G ´omez, M.C., 2018. A pedagogical alliance for trust,\nwellbeing and the identification of errors for learning and formative assess-\nment. Educational Psychology 38, 381–406.\nLi, C., Xing, W., 2021. Natural language generation using deep learning to\nsupport mooc learners. International Journal of Artificial Intelligence in\nEducation 31, 186–214.\nLim, L.A., Dawson, S., Ga ˇsevi´c, D., Joksimovi ´c, S., Pardo, A., Fudge, A.,\nGentili, S., 2021. Students’ perceptions of, and emotional responses to,\npersonalised learning analytics-based feedback: an exploratory study of four\ncourses. Assessment & Evaluation in Higher Education 46, 339–359.\nLin, J., Dai, W., Lim, L.A., Tsai, Y .S., Mello, R.F., Khosravi, H., Gasevic, D.,\nChen, G., 2023a. Learner-centred analytics of feedback content in higher ed-\nucation, in: LAK23: 13th International Learning Analytics and Knowledge\nConference, pp. 100–110.\nLin, J., Thomas, D.R., Han, F., Gupta, S., Tan, W., Nguyen, N.D., Koedinger,\n11\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 \nK.R., 2023b. Using large language models to provide explanatory feedback\nto human tutors. arXiv preprint arXiv:2306.15498 .\nMacNeil, S., Tran, A., Mogil, D., Bernstein, S., Ross, E., Huang, Z., 2022.\nGenerating diverse code explanations using the gpt-3 large language model,\nin: Proceedings of the 2022 ACM Conference on International Computing\nEducation Research-V olume 2, pp. 37–39.\nMarkel, J.M., Opferman, S.G., Landay, J.A., Piech, C., 2023. GPTeach: In-\nteractive TA training with GPT-based students, in: Proceedings of the 10th\nACM Conference on Learning @ Scale, Association for Computing Ma-\nchinery, New York, NY , USA. p. 226–236. doi:10.1145/3573051.3593393.\nMiddleton, T., ahmed Shafi, A., Millican, R., Templeton, S., 2023. Develop-\ning effective assessment feedback: Academic buoyancy and the relational\ndimensions of feedback. Teaching in Higher Education 28, 118–135.\nMulliner, E., Tucker, M., 2017. Feedback on feedback practice: perceptions of\nstudents and academics. Assessment & Evaluation in Higher Education 42,\n266–288.\nNicol, D.J., Macfarlane-Dick, D., 2006. Formative assessment and self-\nregulated learning: A model and seven principles of good feedback practice.\nStud. High. Educ. 31, 199–218.\nNori, H., King, N., McKinney, S.M., Carignan, D., Horvitz, E., 2023.\nCapabilities of gpt-4 on medical challenge problems. arXiv preprint\narXiv:2303.13375 .\nOsakwe, I., Chen, G., Whitelock-Wainwright, A., Ga ˇsevi´c, D., Pin-\nheiro Cavalcanti, A., Ferreira Mello, R., 2022. Towards auto-\nmated content analysis of educational feedback: A multi-language\nstudy. Computers and Education: Artificial Intelligence 3, 100059.\ndoi:https://doi.org/10.1016/j.caeai.2022.100059.\nPankiewicz, M., Baker, R.S., 2023. Large language models (gpt) for automating\nfeedback on programming assignments. arXiv preprint arXiv:2307.00150 .\nPardo, A., Bartimote, K., Shum, S.B., Dawson, S., Gao, J., Ga ˇsevi´c, D., Le-\nichtweis, S., Liu, D., Mart ´ınez-Maldonado, R., Mirriahi, N., et al., 2018.\nOntask: Delivering data-informed, personalized learning support actions.\nJournal of Learning Analytics 5, 235–249.\nPatel, N., Nagpal, P., Shah, T., Sharma, A., Malvi, S., Lomas, D., 2023. Improv-\ning mathematics assessment readability: Do large language models help?\nJournal of Computer Assisted Learning 39, 804–822.\nRyan, T., Henderson, M., Ryan, K., Kennedy, G., 2023. Identifying the compo-\nnents of effective learner-centred feedback information. Teaching in Higher\nEducation 28, 1565–1582.\nShute, V .J., 2008. Focus on formative feedback. Review of educational research\n78, 153–189.\nWeaver, M.R., 2006. Do students value feedback? student perceptions of tutors’\nwritten responses. Assessment & Evaluation in Higher Education 31, 379–\n394.\nYan, L., Martinez-Maldonado, R., Gaˇsevi´c, D., 2024. Generative artificial intel-\nligence in learning analytics: Contextualising opportunities and challenges\nthrough the learning analytics cycle, in: Proceedings of the 14th Interna-\ntional Conference on Learning Analytics and Knowledge, Association for\nComputing Machinery, New York, NY , USA. p. in press.\nYan, L., Sha, L., Zhao, L., Li, Y ., Martinez-Maldonado, R., Chen, G., Li, X.,\nJin, Y ., Gaˇsevi´c, D., 2023. Practical and ethical challenges of large lan-\nguage models in education: A systematic scoping review. British Journal of\nEducational Technology .\nYang, M., Carless, D., 2013. The feedback triangle and the enhancement of\ndialogic feedback processes. Teaching in Higher Education 18, 285–297.\n12\n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \n40 \n41 \n42 \n43 \n44 \n45 \n46 \n47 \n48 \n49 \n50 \n51 \n52 \n53 \n54 \n55 \n56 \n57 \n58 \n59 \n60 \n61 \n62 \n63 \n64 \n65 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6163909435272217
    },
    {
      "name": "Natural language processing",
      "score": 0.4668099880218506
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3579922020435333
    }
  ]
}