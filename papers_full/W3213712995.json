{
  "title": "A Survey of Visual Transformers",
  "url": "https://openalex.org/W3213712995",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102234800",
      "name": "Liu Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133649703",
      "name": "Zhang, Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2069322513",
      "name": "Wang Yixin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227815635",
      "name": "Hou, Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095702329",
      "name": "Yuan Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106160532",
      "name": "Tian Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999911198",
      "name": "Zhang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2303340915",
      "name": "Shi, Zhongchao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223144001",
      "name": "Fan Jianping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A410212432",
      "name": "He, Zhiqiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3098085362",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W3164823914",
    "https://openalex.org/W3120031785",
    "https://openalex.org/W2342792901",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3019166713",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W2307425316",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3202088435",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W3031696893",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W3202715235",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3100039191",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W3202863625",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3180659539",
    "https://openalex.org/W3202163745",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3204076343",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W2907252220",
    "https://openalex.org/W3095121901",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W3167695527",
    "https://openalex.org/W3177183540",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W3170943566",
    "https://openalex.org/W3195108980",
    "https://openalex.org/W2167460663",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2963870605",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3168783492",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3159833358",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2884367402",
    "https://openalex.org/W3203701986",
    "https://openalex.org/W3031955466",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3192125374",
    "https://openalex.org/W3146455718",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W3034681942",
    "https://openalex.org/W3164208409",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2989676862",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3201717975",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3170188883",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W3171649327",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W3129012257",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3173631098",
    "https://openalex.org/W3157917012",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2999219213",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3176153963",
    "https://openalex.org/W3212756788"
  ],
  "abstract": "Transformer, an attention-based encoder-decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern Convolution Neural Networks (CNNs). In this survey, we have reviewed over one hundred of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where a taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, three promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.",
  "full_text": "THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\nA Survey of Visual Transformers\nYang Liu, Yao Zhang, Yixin Wang, Feng Hou, Jin Yuan,\nJiang Tian, Yang Zhang, Zhongchao Shi, Jianping Fan, Zhiqiang He\nAbstract—Transformer, an attention-based encoder-decoder\nmodel, has already revolutionized the ﬁeld of natural language\nprocessing (NLP). Inspired by such signiﬁcant achievements,\nsome pioneering works have recently been done on employing\nTransformer-liked architectures in the computer vision (CV)\nﬁeld, which have demonstrated their effectiveness on three fun-\ndamental CV tasks (classiﬁcation, detection, and segmentation)\nas well as multiple sensory data stream (images, point clouds,\nand vision-language data). Because of their competitive modeling\ncapabilities, the visual Transformers have achieved impressive\nperformance improvements over multiple benchmarks as com-\npared with modern Convolution Neural Networks (CNNs). In this\nsurvey, we have reviewed over one hundred of different visual\nTransformers comprehensively according to three fundamental\nCV tasks and different data stream types, where a taxonomy\nis proposed to organize the representative methods according to\ntheir motivations, structures, and application scenarios. Because\nof their differences on training settings and dedicated vision\ntasks, we have also evaluated and compared all these existing\nvisual Transformers under different conﬁgurations. Furthermore,\nwe have revealed a series of essential but unexploited aspects\nthat may empower such visual Transformers to stand out\nfrom numerous architectures, e.g., slack high-level semantic\nembeddings to bridge the gap between the visual Transform-\ners and the sequential ones. Finally, two promising research\ndirections are suggested for future investment. We will continue\nto update the latest articles and their released source codes at\nhttps://github.com/liuyang-ict/awesome-visual-transformers .\nIndex Terms—Visual Transformer, attention, high-level vision,\n3D point clouds, multi-sensory data stream, visual-linguistic pre-\ntraining, self-supervision, neural networks, computer vision.\nI. I NTRODUCTION\nT\nRANSFORMER [1], which adopts an attention-based\nstructure, has ﬁrst demonstrated its tremendous effects\non the tasks of sequence modeling and machine transla-\ntion. As illustrated in Fig. 1, Transformers have gradually\nemerged as the predominant deep learning models for many\nNLP tasks. The most recent dominant models are the self-\nsupervised Transformers, which are pre-trained over sufﬁcient\ndatasets and then ﬁne-tuned over a small sample set for a\ngiven downstream task [2]–[9]. The Generative Pre-trained\nTransformer (GPT) families [2]–[4] leverage the Transformer\ndecoders to enable auto-regressive language modeling, while\nthe Bidirectional Encoder Representations from Transformers\nThis work was done at AI Lab, Lenovo Research. ( Corresponding authors:\nZ. He; Z. Shi; Y. Zhang. )\nY . Liu, Y . Zhang, Y . Wang, F. Hou, and Z. He are with Institute of\nComputing Technology, Chinese Academy of Sciences, Beijing, 100000,\nChina and also with University of Chinese Academy of Sciences, Beijing,\n100000, China (e-mail: liuyang20c@mails.ucas.ac.cn).\nJ. Yuan is with Southeast University, Nanjing, 214135, China.\nJ. Tian, Y . Zhang, Z. Shi, and J. Fan are with AI Lab, Lenovo Research,\nBeijing, 100000, China. Y . Zhang and Z. He are also with Lenovo Ltd.,\nBeijing, 100000, China (e-mail: {hezq; shizc2; zhangyang20 }@lenovo.com).\n(BERT) [5] and its variants [6], [7] serve as auto-encoder\nlanguage models built on the Transformer encoders.\nIn the CV ﬁeld, prior to the visual Transformers, Con-\nvolution Neural Networks (CNNs) have emerged as a dom-\ninant paradigm [10]–[12]. Inspired by the great success of\nsuch self-attention mechanisms for the NLP tasks [1], [13],\nsome CNN-based models attempted to capture the long-range\ndependencies through adding a self-attention layer at either\nspatial level [14]–[16] or channel level [17]–[19], while others\ntry to replace the traditional convolutions entirely with the\nglobal [20] or local self-attention blocks [21]–[27]. Although\nRamachandr et al. have demonstrated the efﬁciency of self-\nattention block [24] without the help from CNNs, such pure\nattention model is still inferior to the State-of-The-Art (SoTA)\nCNN models on the prevailing benchmarks.\nWith the grateful achievements of linguistic Transformers\nand the rapid development of visual attention-based models,\nnumerous recent works have migrated the Transformers to the\nCV tasks, and some comparable results have been achieved.\nCordonnier et al. [28] theoretically demonstrated the equiva-\nlence between multi-head self-attention and CNNs, and they\ndesigned a pure Transformer by using patch downsampling\nand quadratic position encoding to verify their theoretical con-\nclusion. Dosovitskiy et al. [29] further extended such a pure\nTransformer for large-scale pre-training, which has achieved\nSoTA performance over many benchmarks. Additionally, the\nvisual Transformers have also obtained great performances for\nother CV tasks, such as detection [30], segmentation [31], [32],\ntracking [33], generation [34], and enhancement [35].\nAs illustrated in Fig. 1, following the pioneer works [29],\n[30], hundreds of Transformer-based models have been pro-\nposed for various vision applications within the last year. Thus,\na systematic literature survey is strongly desired to identify,\ncategorize, and evaluate these existing visual Transformers.\nConsidering that the readers may come from different areas,\nwe review all these visual Transformers according to three\nfundamental CV tasks (i.e., classiﬁcation, detection, and seg-\nmentation) and different types of data streams (i.e., images,\npoint clouds, multi-stream data). As illustrated in Fig. 3, this\nsurvey categorizes all these existing methods into multiple\ngroups according to their dedicated vision tasks, data stream\ntypes, motivations, and structural characteristics.\nBefore us, several reviews on the Transformers have been\npublished, where Tay et al. [46] reviewed the efﬁciency of the\nlinguistic Transformers, Khan et al. [47] and Han et al. [48]\nsummarized the early visual Transformers and attention-based\nmodels. The most recent review of the Transformers is intro-\nduced by Lin et al., which provides a systematic review of\nvarious Transformers, but they only mention vision applica-\ntions sketchily [49]. Distinctively, this paper aims to provide\narXiv:2111.06091v4  [cs.CV]  6 Dec 2022\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\nViT\n2020 Oct. 22\nSwin\n2021 Mar. 25\nCvT\n2021 Mar. 29\nDeiT\n2021 Jan. 15\nRefiner\n2021 Jun. 7\nViT-G\n2021 Jun. 8\nCoAtNet\n2021 Jun. 9\nVOLO\n2021 Jun. 28\nCaiT\n2021 Apr. 7\nLV-ViT\n2021 Apr. 23\nPVT\n2021 Feb. 24\nTransformer\n2017 Oct.\nXL-Net\n2019 Jun.\nGPT\n2018 Jun.\nBERT\n2018 Oct.\nRoberta\n2019 Jul.\nALBERT\n2020 Feb.\nGPT-3\n2020 Jun.\nGPT-2\n2019 Oct.\n20\n40\n60\n80\n100Growth of ViT Citations Vs. MonthGrowth of Transformer Citations Vs. Half Year\nAuto-Encoder Language Model\nAuto-Regressive Language Model\nBackbone SOTA on ImageNet-1k \nwith external data\nBackbone SOTA on ImageNet-1k \nwith no external data\n40\n80\n120\n160\n200\nNumber of Citations\nNumber of Citations\nFig. 1. Odyssey of Transformer application & Growth of both Transformer [1] and ViT [29] citations according to Google Scholar. (Upper Left) Growth of\nTransformer citations in multiple conference publication including: NIPS, ACL, ICML, IJCAI, ICLR, and ICASSP. (Upper Right) Growth of ViT citations\nin Arxiv publications. (Bottom Left) Odyssey of language model [1]–[8]. (Bottom Right) Odyssey of visual Transformer backbone where the black [29],\n[36]–[40] is the SoTA with external data and the blue [41]–[45] refers to the SoTA without external data (best viewed in color).\nmore comprehensive review of the most-recently visual Trans-\nformers and categorize them systematically:\n(1) Comprehensiveness & Readability. This paper compre-\nhensively reviews over a hundred visual Transformers\naccording to their applications on three fundamental CV\ntasks (i.e., classiﬁcation, detection, and segmentation)\nand different types of data streams (i.e., image, point\nclouds, multi-stream data). We select more representa-\ntive methods with detailed descriptions and analyses,\nbut introduce other related works brieﬂy. In addition\nto analysing each model independently, we also build\ntheir internal connections from certain senses such as\nprogressive, contrastive, and multi-view analysis.\n(2) Intuitive Comparison. As these existing visual Transform-\ners follow different training schemes and hyper-parameter\nsettings for various vision tasks, this survey presents\nmultiple lateral comparisons over different datasets and\nrestrictions. More importantly, we summarize a series of\npromising components designed for each task, including:\n(a) shallow local convolution with hierarchical structure\nfor backbone; (b) spatial prior acceleration with sparse\nattention for neck detector; and (c) general-purpose mask\nprediction scheme for segmentation .\n(3) In-depth Analysis. We further provide well-thought in-\nsights from the following aspects: (a) How visual Trans-\nformers bridge the traditional sequential tasks to the\nvisual ones (Why does Transformer work effectively in\nCV); (b) the correspondence between the visual Trans-\nformers and other neural networks; (c) the double edges\nof the visual Transformers; and (d) the correlation of\nthe learnable embeddings (i.e., class token, object query,\nmask embedding) adopted in different tasks and data\nstream types. Finally, we outline some future research\ndirections. For example, the encoder-decoder Transformer\nbackbone can unify multiple visual tasks and data stream\ntypes through query embeddings.\nThe rest of this paper is organized as follows. An overview\nof the architectures and the critical components for the vanilla\nsequential Transformers are introduced in Sec. II. A compre-\nhensive taxonomy for the Transformer backbones is summa-\nrized in Sec. III with a brief discussion of their applications\nfor image classiﬁcation. We then review contemporary Trans-\nX\nLinear\nY\nLinear\nY\nLinear\nScaled Dot-Product \nAttention\nConcatenate\nLinear\nQi\nQh\nQ1\nKi\nKh\nK1\nVi\nVh\nV1\nZi\nZh\nZ1\nMatMul\nQi\nMatMul\nScale\n(opt.) Mask\nSoftMax\nKi Vi\nZi\nFig. 2. The structure of the attention layer. Left: Scaled Dot-Product Attention.\nRight: Multi-Head Attention Mechanism.\nformer detectors, including Transformer necks and backbones\nin Sec. IV. Sec. V clariﬁes the mainstream and its variants for\nthe visual Transformers in the segmentation ﬁeld according\nto their embedding forms (i.e., patch embedding and query\nembedding). Sec. III-V also brieﬂy analyzes a speciﬁc aspect\nof their corresponding ﬁelds with performance evaluation. In\naddition to 2D visual recognition, Sec. VI brieﬂy introduces\nthe recently-developed 3D visual recognition from the per-\nspective of point clouds. Sec. VII further overviews the fusion\napproaches within the visual Transformers for multiple data\nstream types (e.g., multi-view, multi-modality, visual-linguistic\npre-training, and visual grounding). Finally, Sec. VIII provides\nthree aspects for further discussion and points out some\npromising research directions for future investment.\nII. O RIGINAL TRANSFORMERS\nThe original Transformer [1] is ﬁrst applied to the task\nfor sequence-to-sequence auto-regression. Compared with pre-\nvious sequence transduction models [50], [51], such orig-\ninal Transformer inherits the encoder-decoder structure but\ndiscards the recurrence and convolutions entirely by using\nmulti-head attention mechanisms and point-wise feed-forward\nnetworks. In the following sub-sections, we will provide\nan architectural overview of the original Transformers and\ndescribe their four key components.\nA. (Multi-Head) Attention Mechanism\nThe mechanism with one single head attention can be\ngrouped into two parts: 1) A transformation layer maps the\ninput sequences X ∈ Rnx×dx,Y ∈ Rny×dy into three\ndifferent vectors (query Q, key K, and value V), where nand\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\nVisual Transformers\nClassiﬁcation\nOriginal Visual Transformer SA-Net [24], FAN [28], ViT [29].\nTransformer Enhanced CNN VTs [52], BoTNet [53].\nCNN Enhanced Transformer\nSoft Inductive Bias: DeiT [41], ConViT [54].\nStraightforward: CeiT [55], LocalViT [56], CPVT [57], ResT [58].\nCombination: Early Conv. [59], CoAtNet [40].\nTransformer with Local Attn. Local Only: HaloNet [27], Swin [36], VOLO [45].\nLocal-Global: TNT [60], Twins [61], ViL [62], Focal [63].\nHierarchical Transformer T2T [64], PVT [42], PiT [65], PVT v2 [66], CvT [37].\nDeep Transformer Structure Improvement: CaiT [43], DeepViT [67], Reﬁner [38].\nLoss Regulation: Diverse Patch [68].\nSelf-Supervised Transformer Generative: iGPT [69], MST [70], BEIT [71], MAE [72].\nDiscriminative: MoCo v3 [73], DINO [74], MoBY [75].\nDetection\nTransformer Neck\nOriginal Transformer DETR [30], Pix2seq [76].\nSparse Attention Deformable DETR [77], ACT [78],\nPnP-DETR [79], Sparse-DETR [80].\nSpatial Prior\nOne-Stage: SMCA [81],\nConditional DETR [82],\nAnchor DETR [83],\nDAB-DETR [84],\nSAP-DETR [85].\nTwo-Stage: Deformable DETR [77],\nEfﬁcient DETR [86],\nDynamic DETR [87].\nStructural Redesign TSP [88], YOLOS [89].\nPre-trained Model UP-DETR [90], FP-DETR [91].\nMatching Optimiz. DN-DETR [92], DINO [93].\nTransformer Backbone General: Focal [63], PVT [42], ViL [62], Swin [36].\nSpecialized: FPT [94], HRFormer [95], HRViT [96].\nSegmentation\nPatch-Based Transformer SETR [97], TransUNet [98], SegFormer [99].\nQuery-Based Transformer\nObject Query\nSerial: Panoptic DETR [30].\nParalleled: Cell-DETR [100],\nVisTR [101].\nCascaded: QueryInst [102].\nMask Embedding\nBox-auxiliary: ISTR [103],\nSOLQ [104].\nBox-Free: Max-DeepLab [31],\nSegmenter [105],\nMaskformer [32].\n3D Visual Recognition\nRepresentation Learning\nBasic: Point Transformer [106], PCT [107],\n3DCTN [108], Fast Point Transformer [109].\nFine-Grained: Pointformer [110], SST [111],\nV oTr [112], V oxSeT [113].\nSelf-Supervised: Point-BERT [114], Point-MAE [115],\nMaskPoint [116].\nCognition Mapping\nPoint-Based: 3DETR [117], Group-Free [118], CT3D [119].\nCamera-Based: MonoDTR [120], MonoDETR [121],\nDETR3D [122], TransFusion [123].\nSpeciﬁc Processing PoinTr [124], SnowﬂakeNet [125], PointRecon [126].\nMulti-Sensory Data Stream\nHomologous Stream\nInteractive Fusion: MVT [127], MVDeTr [128],\nTransFuser [129], COTR [130]\nWang et al. [131], FUTR3D [132]\nTransformerFusion [133], mmFormer [134]\nTransfer Fusion: Tulder er al. [135], Long et al. [136],\nDRT [137].\nHeterologous Stream\nVis.-Lin. Pre-train.: VideoBETR [138], ViLBERT [139],\nLXMERT [140] VisualBERT [141],\nVL-BERT [142], UNITER [143],\nOscar [144], Uniﬁed [145],\nViLT [146], VinVl [147],\nCLIP [148], DALL-E [149],\nALIGN [150], UniT [151],\nSimVLM [152], Data2Vec [153].\nVisual Grounding: MDETR [154], TransVG [155],\nVGTR [156], Referring Transformer [157],\nPseudo-Q [158], LanguageRefer [159],\nTransRefer3D [160], MVT(2022) [161],\nTubeDETR [162].\nFig. 3. Taxonomy of Visual Transformers\nPadding\nMasking\n〈BOS〉\n〈EOS〉\nEncoder \nEmbedding\nDecoder \nEmbedding\n+ +\nMulti-Head \nAttention\nMasked \nMulti-Head \nAttention\nAdd & Norm\nPoint-wise\nFFN\nAdd & Norm\nAdd & Norm\nMulti-Head \nAttention\nPoint-wise\nFFN\nAdd & Norm\nAdd & Norm\nLinear & \nSoftMax\nInputs Outputs\n(shifted right)\nOutputs\nProbabilities\nPositional \nEncoding\nPositional \nEncoding\nFig. 4. The overall architecture of Transformer [1].The 2D lattice represents\nthe each states of queries during training (best viewed in color).\ndare the length and the dimension of the inputs, respectively.\n2) An attention layer, as shown in Fig. 2, explicitly aggregates\nthe query with the corresponding key, assigns them to the value\nand updates the output vector.\nThe formula for the transformation layer is deﬁned as\nQ= XWQ, K = YW K, V = YW V, (1)\nwhere WQ ∈Rdx×dk\n, WK ∈Rdy×dk\n, and WV ∈Rdy×dv\nare linear matrices. dk and dv are the dimension of the query-\nkey pair and the value which are projected from Y and X,\nrespectively. Such two sequence inputs are referred as the\ncross-attention mechanism. It can also be regarded as a self-\nattention when Y = X. In form, self-attention is applied\nto both Transformer encoder and decoder, while the cross-\nattention severs as a junction within the decoder.\nThen, the scale-dot attention mechanism is formulated as\nAttention(Q,K,V ) = Softmax\n(QKT\n√dk\n)\nV, (2)\nwhere the attention weights are generated by a dot-product\noperation between Q and the K, a scaling factor √dk and\na softmax operation are supplied to translate the attention\nweights into a normalized distribution. The resulting weights\nare assigned to the corresponding value elements, thereby\nyielding the ﬁnal output vector.\nDue to the restricted feature subspace, the modeling capabil-\nity of the single-head attention block is quite coarse. To tackle\nthis issue, as shown in Fig. 2, a Multi-Head Self-Attention\nmechanism (MHSA) is proposed to linearly project the input\ninto multiple feature sub-spaces and process them by using\nseveral independent attention heads (layers) parallelly. The\nresulting vectors are concatenated and mapped to the ﬁnal\noutputs. The process of MHSA can be formulated as\nQi = XWQi, Ki = XWKi, Vi = XWVi,\nZi = Attention(Qi,Ki,Vi), i= 1 ...h,\nMultiHead(Q,K,V ) = Concat(Z1,Z2,...,Z h)WO,\n(3)\nwhere h is the head number, WO ∈Rhdv×dmodel denotes the\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\noutput projected matrix, Zi denotes the output vector of each\nhead, WQi ∈Rdmodel×dk , WKi ∈Rdmodel×dk , and WVi ∈\nRdmodel×dv are three different groups of matrices. Multi-head\nattention separates the inputs into h independent attention\nheads with dmodel/h-dimensional vectors, and integrates each\nhead features dependently. Without extra costs, multi-head\nattention enriches the diversity of the feature subspaces.\nB. Position-wise Feed-Forward Networks\nThe output of MHSA is then fed into two successive feed-\nforward networks (FFN) with a ReLU activation as\nFFN(x) = RELU(W1x+ b1)W2 + b2. (4)\nThis position-wise feed-forward layer can be viewed as a\npoint-wise convolution, which treats each position equally but\nuses different parameters between each layer.\nC. Positional Encoding\nSince the Transformer/Attention operates on the input em-\nbedding simultaneously and identically, the order of the se-\nquence is neglected. To make use of the sequential informa-\ntion, a common solution is to append an extra positional vector\nto the inputs, hence term the “positional encoding”. There are\nmany choices for positional encoding. For example, a typical\nchoice is cosine functions with different frequencies as\nPE(pos,i) =\n{\nsin(pos·ωk) if i= 2k\ncos(pos·ωk) if i= 2k+ 1,\nωk = 1\n100002k/d, k = 1,··· ,d/2,\n(5)\nwhere posand dare the position and the length of the vector,\nrespectively, and i is the index of each element within vector.\nD. Transformer Model\nFig. 4 shows the overall Transformer models with the\nencoder-decoder architecture. Speciﬁcally, it consists of N\nsuccessive encoder blocks, each of which is composed of two\nsub-layers. 1) An MHSA layer aggregates the relationship\nwithin the encoder embeddings. 2) A position-wise FFN\nlayer extracts feature representations. For the decoder, it also\ninvolves N consecutive blocks that follow a stack of the\nencoders. Compared with the encoder, each decoder block\nappends to a multi-head cross-attention layer to aggregate\nboth decoder embeddings and encoder outputs, where Y\ncorresponds to the former, and X is the latter as shown\nin Eq. (1). Moreover, all of the sub-layers in both encoder\nand decoder employ a residual connection [11] and a Layer\nNormalization [163] to enhance the scalability of the Trans-\nformer. In order to record the sequential information, each\ninput embedding is attached with a positional encoding at the\nbeginning of the encoder stack and the decoder stack. Finally,\na softmax operation are used for predicting the next word.\nIn an auto-regressive language model, the Transformer is\noriginated from the machine translation tasks. Given a se-\nquence of words, the Transformer vectorizes the input se-\nquence into the word embeddings, adds the positional encod-\nings, and feeds the resulting sequence of the vectors into an\n7\nTransformers in High-level Vision: A Survey\nTransformer Enhanced CNN\nCNN\nLocal\nGlobal Deep\nMemory\nHierarchical\nDeepViT\nZhou et al.\nMar 2021.\nCaiT\nTouvron et al.\nMar 2021.\nCross ViT\nChen et al.\nMar 2021.\nPiT\nHeo et al. \nMar 2021.\nPVT\nWang et al.\nFeb 2021.\nCvT\nWu et al.\nMar 2021\nHaloNets\nVaswani et al.\nMar 2021.\nVT\nWu et al.\nJun 2020.\nBoTNet\nSrinivas et al.\nJan 2021.\nTNT\nHan et al.\nFeb 2021.\nSwin \nTransformer\nLiu et al.\nMar 2021.\nViT\nDosovitskiy et al.\nSep 2020.\nFAN\nCordonnier et al.\nJan 2020.\nCeiT\nYuan et al.\nMar 2021.\nConViT\nd'Ascoli et al.\nMar 2021.\nLocal CNN Enhanced Transformer\nOriginal Visual Transformer\nDeep Transformer\nHierarchical Transformer\nLocal Attention Enhanced Transformer\nT2T ViT\nYuan et al.\nJan 2021.\nDeiT\nTouvron et al.\nDec 2020.\nLocalViT\nLi et al.\nApr 2021.\nTaxonomy of Transformer in Vision\nSANet\nRamachandran et al.\nJun 2019.\nFig. 5. Taxonomy of Visual Transformer Backbone (best viewed in color).\nencoder. During training, as illustrated in Fig. 4, Vaswani et al.\ndesign a masking operation according to the rule for the auto-\nregressive task [1], where the current position only depends on\nthe outputs of the previous positions. Based on this masking,\nthe Transformer decoder is able to process the sequence of the\ninput labels parallelly. During the inference time, the sequence\nof the previously-predicted words is processed by the same\noperation to predict the next word.\nIII. T RANSFORMER FOR CLASSIFICATION\nFollowing the prominent developments of the Transformers\nin NLP [2]–[5], recent works attempt to introduce visual\nTransformers for image classiﬁcation. This section compre-\nhensively reviews over 40 visual Transformers and groups\nthem into six categories, as shown in Fig. 5. We start with in-\ntroducing the Fully-Attentional Network [24], [28] and the Vi-\nsion Transformer (ViT) [29], suchOriginal Visual Transformer\nﬁrst demonstrates its efﬁcacy on multiple classiﬁcation bench-\nmarks. Then we discuss Transformer Enhanced CNN methods\nthat utilize Transformer to enhance the representation learning\nin CNNs. Due to the negligence of local information in the\noriginal ViT, the CNN Enhanced Transformer employs an\nappropriate convolutional inductive bias to augment the visual\nTransformer, while the Local Attention Enhanced Transformer\nredesigns patch partition and attention blocks to improve their\nlocality. Following the hierarchical and deep structures in\nCNNs [164], the Hierarchical Transformer replaces the ﬁxed-\nresolution columnar structure with a pyramid stem, while\nthe Deep Transformer prevents the attention map from over-\nsmooth and increases its diversity in the deep layer. Moreover,\nwe also review the existing visual Transformers with Self-\nSupervised Learning . Finally, we make a brief discussion\nbased on intuitive comparisons for further investigation. More\nvisual Transformers’ milestones are introduced in App. A.\nA. Original Visual Transformer\nInspired by the tremendous achievements of the Trans-\nformers in the NLP ﬁeld [2]–[5], the previous technology\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\ntrends for the vision tasks [14]–[17], [165] incorporate the\nattention mechanisms with the convolution models to augment\nthe models’ receptive ﬁeld and global dependency.\nBeyond such hybrid models, Ramachandran et al. contem-\nplate whether the attention can completely replace the convo-\nlution, and then present a Stand-Alone self-attention network\n(SANet) [24], which has achieved superior performance on the\nvision tasks as compared with the original baseline. Given a\nResNet [11] architecture, the authors straightforwardly replace\nthe spatial convolution layer ( 3 ×3 kernel) in each bottleneck\nblock with a locally spatial self-attention layer and keep other\nstructures the same as the original setting in ResNet. Moreover,\nlots of ablations have shown that the positional encodings and\nconvolutional stem can further improve the network efﬁcacy.\nFollowing [24], Cordonnier et al. pioneer a prototype design\n(called Fully-Attentional Network in their original paper) [28],\nincluding a fully vanilla Transformer and a quadratic positional\nencoding. The authors also theoretically prove that a convo-\nlutional layer can be approximated by a single MHSA layer\nwith relative positional encoding and sufﬁcient heads. With the\nablations on CIFAR-10 [166], they further verify that such a\nprototype design does learn to attend a grid-like pattern around\neach query pixel, as their theoretical conclusion.\nDifferent from [28] that only focuses on lite scale model, the\nVision Transformer (ViT) [29] further explores the effective-\nness of the vanilla Transformer with large-scale pre-trained\nlearning, and such a pioneer work impacts the community\nsigniﬁcantly. Because the vanilla Transformers only accept\nthe sequential inputs, the input image in ViT is ﬁrstly split\ninto a series of non-overlapped patches and they are then\nprojected into patch embeddings. Then a 1D learnable po-\nsitional encoding is added on the patch embeddings to retain\nthe spatial information, and the joint embeddings are then fed\ninto the encoder, as shown in Fig. 6. Similar to BERT [5], a\nlearned [class] token is attached with the patch embeddings to\naggregate the global representation and it serves as the ﬁnal\noutput for classiﬁcation. Moreover, a 2D interpolation com-\nplements the pre-trained positional encoding to maintain the\nconsistent order of the patches when the feeding images are in\narbitrary resolution. By pre-training with a large-scale private\ndataset (JFT-300M [167]), ViT has achieved similar or even\nsuperior results on multiple image recognition benchmarks\n(ImageNet [168] and CIFAR-100 [166]) as compared with the\nmost prevailing CNNs methods. However, its generalization\ncapability tends to be eroded with limited training data.\nB. Transformer-enhanced CNNs\nAs described in Section II, the Transformer has two keys:\nMHSA and FFN. There exists an approximation between the\nconvolutional layer and the MHSA [28], and Dong et al. sug-\ngest that the Transformer can further mitigate the strong bias\nof MHSA with the help of skip connections and FFN [169].\nRecently, some methods attempt to integrate the Transformer\ninto CNNs to enhance representation learning. VTs [52] de-\ncouples semantic concepts for the input image into different\nchannels and relates them densely through the encoder block,\nnamely VT-block. Such VT-block substitutes the last convo-\nlution stage to enhance the CNN model’s ability on semantic\nPreprint. Under review.\nTransformer \nEncoder\nMLP \nHead\n Vision Transformer (ViT)\n*\nLinear \nProjection \nof \nFlattened \nPatches\n*\n \nExtra \nlearnable\n     \n[class]\n \nembedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch \n+ \nPosition \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL\n \nx\n+\n Transformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3.1 V ISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W×C into a\nsequence of ﬂattened 2D patches xp ∈RN×(P2·C), where (H,W ) is the resolution of the original\nimage, Cis the number of channels,(P,P ) is the resolution of each image patch, andN = HW/P2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to Ddimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT’s[class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder ( z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\ntached to z0\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at ﬁne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.3). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\nThe MLP contains two layers with a GELU non-linearity.\nz0 = [xclass; x1\npE; x2\npE; ··· ; xN\np E] +Epos, E ∈R(P2·C)×D, Epos ∈R(N+1)×D (1)\nz′\nℓ = MSA(LN(zℓ−1)) +zℓ−1, ℓ = 1...L (2)\nzℓ = MLP(LN(z′\nℓ)) +z′\nℓ, ℓ = 1...L (3)\ny = LN(z0\nL) (4)\nHybrid Architecture. As an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\n3\nFig. 6. Illustration of ViT. The ﬂatten image patches with an additional class\ntoken are fed into the vanilla Transformer encoder after positional encoding.\nOnly the class token can be predicted for classiﬁcation. (from [29].)\nmodelling. Unlike previous approaches that directly replace\nconvolution with attention structure, Vaswani et al. propose a\nconceptual redeﬁnition that the successive bottleneck blocks\nwith MHSA can be formulated as the Bottleneck Transformer\n(BoTNet) [53] blocks. The relative position encoding [170]\nis adopted to further mimic the original Transformer. Based\non ResNet [11], BoTNet outperforms the most CNN models\nwith similar parameter settings on the ImageNet benchmark\nand further demonstrates the efﬁcacy of hybrid models.\nC. CNN-enhanced Transformer\nInductive bias is deﬁned as a set of assumptions on data dis-\ntribution and solution space, whose manifestations within con-\nvolution are the locality and the translation invariance [171].\nAs the covariance within local neighborhoods is large and\ntends to be gradually stationary across an image, CNNs can\nprocess an image effectively with the help of the biases. Nev-\nertheless, strong biases also limit the upper bound of CNNs\nwhen sufﬁcient data are available. Recent efforts attempt to\nleverage an appropriate CNN bias to enhance Transformer.\nTouvron et al. propose a Data-efﬁcient image Transformer\n(DeiT) [41] to moderate the ViT’s dependence on large\ndatasets. In addition to the existing strategies for data augmen-\ntation and regularization, a teacher-student distillation strategy\nis applied for auxiliary representation learning. The student\nmodel is the ViT, where a distilled token is attached to\nthe patch embeddings and it is supervised by the pseudo\nlabels from the teacher model. Extensive experiments have\ndemonstrated that CNN is a better teacher model than the\nTransformer. Surprisingly, the distilled student Transformer\neven outperforms its teacher CNN model. These observations\nare explained in [172]: the teacher CNN transfers its induc-\ntive bias in a soft way to the student Transformer through\nknowledge distillation. Based on ViT’s architecture, DeiT-B\nattains the top-1 accuracy of 85.2% without external data.\nConViT [54] appends a parallel convolution branch with\nvanilla Transformer to impose inductive biases softly. The\nmain idea of the convolution branch is a learnable embedding\nthat is ﬁrst initialized to approximate the locality as similar\nas the convolution and then explicitly gives each attention\nhead freedom to escape the locality by adjusting a learned\ngating parameter. CeiT [55] and LocalViT [56] extract the\nlocality by directly adding a depth-wise convolution in FFN.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\nAs point-wise convolution is equal to position-wise FFN, they\nextend FFN to an inverted residual block [173] to build a\ndepth-wise convolutional framework. Based on the assumption\nof positional encoding [58] and the observation in [174],\nResT [58] and CPVT [57] try to adapt the inherent positional\ninformation of the convolution to the arbitrary size of inputs\ninstead of interpolating the positional encoding. Including\nCvT [37], these methods replace the linear patch projection\nand positional encoding with the convolution stacks. Both\nmethods beneﬁt from such convolutional position embedding,\nespecially for small model.\nBesides the “internal” fusion, many approaches focus on an\n“apparent” combination according to different visual Trans-\nformer’s structures. For standard columnar structure, Xiao et\nal. substitute the original patchify stem (single non-overlapped\nlarge kernel) with several stacked stride-2 3 ×3 kernels [59].\nSuch a Convolutional Stem signiﬁcantly improves ViT by 1-\n2% on accuracy for ImageNet-1k and facilitates its stability\nand generalization for the downstream tasks. For hierarchical\nstructures, Dai et al. [40] investigate an optimal combination\nof hybrid models to beneﬁt the performance trade-off. By com-\nparing a series of hybrid models, they propose a Convolution\nand Attention Network (CoAtNet) to leverage the strength of\nboth CNNs and Transformer. The authors observe that depth-\nwise convolution can be naturally integrated into the attention\nblock, and stacking convolution vertically in the shallow layer\nis more effective than the original hierarchical methods. It has\nachieved the SoTA performance across multiple datasets.\nD. Local Attention Enhanced Transformer\nThe coarse patchify process in ViT [29] neglects the local\nimage information. In addition to adding CNNs, various local\nattention mechanisms are proposed to dynamically attend the\nneighbour elements and augment the local extraction ability.\nOne of the representative methods is the Shifted windows\n(Swin) Transformer [36]. Similar to TSM [175] (Fig. 7(a)),\nSwin utilizes a shifted window along the spatial dimension to\nmodel the global and boundary features. In detail, two suc-\ncessive window-wise attention layers can facilitate the cross-\nwindow interactions (Fig. 7(b)-(c)), similar to the receptive\nﬁeld expansion in CNNs. Such operation also reduces the\ncomputational complexity from O(2n2C) to O(4M2nC) in\none attention layer, where n and M denote the patch length\nand the window size, respectively. Swin Transformer achieves\n84.2% accuracy on ImageNet and the latest SoTA on multiple\ndense prediction benchmarks (see Sec. IV-B).\nInspired by [176], Han et al. leverage a Transformer-iN-\nTransformer (TNT) [60] model to aggregate both patch- and\npixel-level representations. Each layer of TNT consists of two\nsuccessive blocks, an inner block models the pixel-wise inter-\naction within each patch, and an outer block extracts the global\ninformation. Twins [61] employs a spatially separable self-\nattention mechanism, similar to depth-wise convolution [173]\nor window-wise TNT [60], to model the local-global represen-\ntation. Another separate form is ViL [62], which replaces the\nsingle class token with a series of local embeddings (termed\nas global memory). These local embeddings only perform an\nChannel C\nTemporal T\npad zero\ntemporal shift\ntruncate\nT\nC\nH,W\nt=0\nt=3\n…\nt=1\nt=2\nChannel C\nwindow shift\nLayer l: Regular Window Layer l+1: Shifted Window\nH\nW\n(b)\n(a)\nMLP\nLN\nLN\nW-MSA\nMLP\nLN\nLN\nSW-MSA\n(c)\nFig. 7. An overview of Swin Transformer and TSM. (a) TSM with bi-\ndirection and uni-direction operation. (b) The shifted window method. (c) Two\nsuccessive Transformer blocks of Swin Transformer. The regular and shifted\nwindow correspond to W-MSA and SW-MSA, respectively. (from [36], [175]).\ninner attention and an interaction with their corresponding 2D\nspatial neighbors. VOLO [45] proposes an outlook attention,\nwhich is similar to a patch-wise dynamic convolution, to focus\non the ﬁner-level features, including three operations: unfold,\nlinear-wights attention, and refold. Based on [44], it achieves\nSoTA results on ImageNet without external data.\nE. Hierarchical Transformer\nDue to the columnar structure of ViT [29] with a ﬁxed\nresolution across the entire Transformer layers, it loses the\nﬁne-grained features and suffers from heavy computational\ncosts. Followed by the hierarchical models, Tokens-to-Token\nViT (T2T-ViT) [64] ﬁrst introduces a paradigm of hierarchical\nTransformer and employs an overlapping unfold operation\nfor down-sampling. However, such operation brings heavy\nmemory and computation costs. Therefore, Pyramid Vision\nTransformer (PVT) [42] leverages a non-overlapping patch\npartition to reduce feature size. Furthermore a spatial-reduction\nattention (SRA) layer is applied in PVT to further reduce\nthe computational cost by learning low-resolution key-value\npairs. Empirically, PVT adapts the Transformer to the dense\nprediction tasks on many benchmarks which demand large\ninputs and ﬁne-grained features with computational efﬁciency.\nMoreover, both PiT [65] and CvT [37] utilize pooling and\nconvolution to perform token downsampling, respectively. In\ndetail, CvT [37] improves the SRA of PVT [42] by replacing\nthe linear layer with a convolutional projection. Based on the\nconvolutional bias, CvT [37] can adapt to arbitrary size inputs\nwithout positional encodings.\nF . Deep Transformer\nEmpirically, increasing model’s depth always strengthens its\nlearning capacity [11]. Recent works apply a deep structure\nto Transformer and massive experiments are conducted to\ninvestigate its scalability by analyzing cross-patch [68] and\ncross-layer [38], [67] similarities, and the contribution of\nresidual blocks [43]. In the deep Transformer, the features\nfrom the deeper layers tend to be less representative (attention\ncollapse [67]), and the patches are mapped into the indistin-\nguishable latent representations (patch over-smoothing [68]).\nTo address such limitations mentioned above, current methods\npresent the corresponding solutions from two aspects.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\nFrom the aspect of model’s structure, Touvron et al. present\nefﬁcient Class-attention in image Transformers (CaiT [43]),\nincluding two stages: 1) Multiple self-attention stages without\nclass token. In each layer, a learned diagonal matrix initialized\nby small values is exploited to update the channel weights\ndynamically, thereby offering a certain degree of freedom for\nchannel adjustment. 2) Last few class-attention stages with\nfrozen patch embeddings. A later class token is inserted to\nmodel global representations, similar to DETR [30] with an\nencoder-decoder structure. This explicit separation is based on\nthe assumption that the class token is invalid for the gradient\nof patch embeddings in the forward pass. With distillation\ntraining strategy [41], CaiT achieves a new SoTA on imagenet-\n1k (86.5% top-1 accuracy) without external data. Although\ndeep Transformer suffers from attention collapse and over-\nsmoothing problems,it still largely preserves the diversity of\nthe attention map between different heads. Based on this\nobservation, Zhou et al. propose Deep Vision Transformer\n(DeepViT) [67] that aggregates different head attention maps\nand re-generates a new one by using a linear layer to increase\ncross-layer feature diversity. Furthermore, Reﬁner [38] applies\na linear layer to expand the dimension of the attention maps\n(indirectly increasing the head number) for diversity promo-\ntion. Then, a Distributed Local Attention (DLA) is employed\nto achieve better modeling of both the local features and the\nglobal ones, which is implemented by a head-wise convolution\neffecting on the attention map.\nFrom the aspect of training strategy, Gong et al. present\nthree Patch Diversity losses for deep Transformer that can\nsigniﬁcantly encourage patches’ diversity and offset over-\nsmoothing problem [68]. Similar to [177], a patch-wise cosine\nloss minimizes pairwise cosine similarity among patches. A\npatch-wise contrastive loss regularizes the deeper patches\nby their corresponding one in the early layer. Inspired by\nCutmix [178], a patch-wise mixing loss mixes two different\nimages and forces each patch to only attend to the patches from\nthe same image and ignore unrelated ones. Compared with LV-\nViT [44], their similar loss function is based on a distinctive\nmotivation. The former focuses on the patch diversity, while\nthe latter focuses on data augmentation about token labeling.\nG. Transformers with Self-Supervised Learning\nFollowing the grateful success of self-supervised in the NLP\nﬁeld [5], recent works also attempt to design various self-\nsupervised learning schemes for the visual Transformers in\nboth generative and discriminative ways.\nFor the generative models, Chen et al. propose an image\nGenerative Pre-training Transformer (iGPT) [69] for self-\nsupervised visual learning. Different from the patch embed-\nding of ViT [29], iGPT directly resizes and ﬂattens the image\nto a lower resolution sequences. The resized sequences are\nthen input into a GPT-2 [4] for auto-regressive pixel prediction.\niGPT demonstrates the effectiveness of the Transformer in the\nvisual tasks without any help from image-speciﬁc knowledge,\nbut its considerable computation cost is hard to be accepted\n(roughly 2500 V100-days for pre-training). Instead of gener-\nating such raw pixels directly, Bao et al. propose a BERT-\nstyle [5] visual Transformer (BEiT) [71] by reconstructing the\nmasked image in the latent space. Similar to the dictionary\nin BERT [5], dV AE [149] vectorizes the image into discrete\nvisual tokens. The resulting visual token serves as pseudo label\nto pre-train ViT for masked patch construction.\nFor the discriminative models, Chen et al. [73] go back\nto basics and investigate the effects of several fundamental\ncomponents for stabilized self-supervised ViT training. They\nobserve that the unstable training process mildly affects the\neventual performance, and extend MoCo series to MoCo v3,\ncontaining a series of training strategies such as freezing\nprojection layer. Following DeiT [41], Caron et al. further\nextend the teacher-student recipe to self-supervised learning\nand propose DINO (2021) [74]. The core concepts of DINO\ncan be summarized into three points. A momentum encoder\ninherited SwA V [179], serves as a teacher model that outputs\nthe centered pseudo labels over a batch. An online encoder\nwithout the prediction head serves as a student model to ﬁt the\nteacher’s output. A standard cross-entropy loss connects self-\ntraining with knowledge distillation. More interestingly, self-\nsupervised ViT can learn ﬂourishing features for segmentation,\nwhich are normally unattainable by the supervised models.\nH. Discussion\n1) Algorithm Evaluation and Comparative Analysis: In our\ntaxonomy, all the existing supervised models are grouped into\nsix categories. Tab. I summarizes the performances of these\nexisting visual Transformers on ImageNet-1k benchmarks. To\nevaluate them objectively and intuitively, we use the following\nthree ﬁgures to illustrate their performances on ImageNet-\n1k under different conﬁgurations. Fig. 8(a) summarizes the\naccuracy of each model under 2242 inputs size. Fig. 8(b)\ntakes the FLOPs as the horizontal axis, which focuses on\ntheir performances under higher-resolution. Fig. 8(c) focuses\non the pre-trained models with external datasets. From these\ncomparison results, we brieﬂy summarize several performance\nimprovements on efﬁciency and scalability as follows.\n• Compared with the most structure-improved methods, the\nbasic training strategies like DeiT [41] and LV-ViT [44],\nare more universal for various models, tasks, and inputs.\n• The locality is indispensable for the Transformer, which\nis reﬂected by the dominant of VOLO [45] and Swin [36]\non classiﬁcation and dense prediction tasks, respectively.\n• The convolutional patchify stem (ViT c [59]) and early\nconvolutional stage (CoAtNet [40]) can signiﬁcantly\nboost the accuracy of the Transformers, especially for\nlarge models. We speculate the reason is because these\ndesigns introduce a more stringent high-level features\nthan the non-overlapping patch projection in ViT [29].\n• The deep Transformer, such as Reﬁned-ViT [38] and\nCaiT [43], has great potential. As the model size grows\nquadratically with the channel dimension, the trade-off in\ndeep Transformer is considered for further investigation.\n• CeiT [55] and CvT [37] show signiﬁcant advantages\nin training a small or medium model (0 −40M), which\nsuggests that such kinds of hybrid attention blocks for\nlightweight models are worth further exploring.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\nAccuracy(%)\nParameters (M)\n72\n76\n80\n81\n82\n83\n84\n85\n86\n0 20 40 60 80 100 200 300\n87\n(a)\nBoTNet-S1-59-T2   BoTNet-S1-110-T4\nVT-ResNet18  VT-ResNet34  VT-ResNet 50  VT-ResNet101\nResT-Lite  ResT-Small  ResT-Base  ResT-Large\nViTC-1GF  ViTC-4GF ViTC-18GF  ViTC-36GF\nCoAtNet-0  CoAtNet-1  CoAtNet-2  CoAtNet-3\nVOLO-D1  VOLO-D2  VOLO-D3  VOLO-D4  VOLO-D5\nPVT-Tiny  PVT-Small  PVT-Medium  PVT-Large\nCaiT-XS-24  CaiT-S-24  CaiT-S-36  CaiT-M-24  CaiT-M-36\nRefined-ViT-S  Refined-ViT-M  Refined-ViT-L\nCrossViT-9  CrossViT-15  CrossViT-18  CrossViT-18*\nCeiT-T  CeiT-S\nTNT-S  TNT-B\nCvT-13  CvT-21\nDeiT-Ti DeiT-S  DeiT-B\nLocalViT-T  LocalViT-S\nSwin-T  Swin-S  Swin-B\nT2T-ViT-14  T2T-ViT-19\nDeepViT-S  DeepViT-L\nLV-ViT-S  LV-ViT-M\nPiT-Ti PiT-XS  PiT-S  PiT-B\nConViT-Ti ConViT-S  ConViT-B\nAccuracy(%)\n10\nFLOPs (G)\n0 20 30 40 50 60 160 260\n74\n88\n87\n86\n85\n84\n83\n82\n81\n80\n(c)\nViT-B/16↑ ViT-L/16↑\nViTc-4GF  ViTc-18GF  ViTc-36GF\nCoAtNet-2  CoAtNet-3  CoAtNet-4-E150↑\nSwin-B  Swin-B↑  Swin-L↑ \nDeiT-Ti DeiT-S  DeiT-B  DeiT-B↑\nPiT-Ti PiT-S  PiT-M  PiT-B\nCaiT-S-24  CaiT-S-36  CaiT-M-24  \nCaiT-M-36  CaiT-M-48↑\nCvT-13↑ CvT-21↑  CvT-24↑ \nAccuracy(%)\n(b)\n82\n83\n84\n85\n86\nFLOPs (G)\n0 50 100 150 200 250\n76\n79\nViT-B/16↑ ViT-L/16↑\nLV-ViT-S↑  LV-ViT-M↑  LV-ViT-L↑\nCoAtNet-0↑ CoAtNet-1↑ CoAtNet-2↑ CoAtNet-3↑\nVOLO-D2↑ VOLO-D3↑↑ \nVOLO-D4↑↑ VOLO-D5↑↑ \nDeiT-B↑\nRefined-S↑ Refined-M↑ Refined-L↑ Refined-L↑↑ \nCaiT-S-24↑ CaiT-S-36↑ CaiT-S-48↑\nCaiT-M-24↑ CaiT-M-36↑ CaiT-M-48↑↑\nCvT-13↑ CvT-21↑\nSwin-B↑\nCeiT-T↑ CeiT-S↑\nVOLO-D1↑\nFig. 8. Comparisons of recent visual Transformers on ImageNet-1k benchmark, including ViT [29], DeiT [41], BoTNet [53], VTs [52], ConViT [54],\nCeiT [55], LocalViT [56], TNT [60], Swin [36], PiT [65], T2T-ViT [64], PVT [42], CvT [37], DeepViT [67], CaiT [43], Cross ViT [180] (best viewed in\ncolor). (a) The bubble plot of the mentioned models with 2242 resolution input, the size of cycle denotes GFLOPs. (b) Comparison of visual Transformers\nwith high-resolution inputs, the square indicates 4482 input resolution. (c) The accuracy plot of some pre-trained models on ImageNet-21k.\nTABLE I\nTOP-1 ACCURACY COMPARISON OF VISUAL TRANSFORMERS ON IMAGE NET-1K. “1K ONLY” DENOTES TRAINING ON IMAGE NET-1K ONLY; “21 K PRE.”\nDENOTES PRE -TRAINING ON IMAGE NET-21K AND FINE -TUNING ON IMAGE NET-1K; “D ISTILL .” DENOTES APPLYING DISTILLATION TRAINING SCHEME\nOF DEIT [41]; T HE COLOR OF “LEGEND ” CORRESPONDING TO EACH MODEL ALSO DENOTES SAME MODEL IN FIG. 8.\nMethod #Params.(M) FLOPs(G)\nImageNet-1k Top-1 Acc.Legend\n1K 21K/Distill.(†/Υ)\nViT-B/16↑384 [29] 86.8 49.4 77.9 83.97 †\nViT-L/16↑384 [29] 304.7 174.8 76.5 85.15 †\nVT-Rest18 [52] 11.7 1.57 76.8 -\nVT-Rest34 [52] 19.2 3.24 79.9 -VT-Rest50 [52] 21.4 3.41 80.6 -VT-Rest101 [52] 41.5 7.13 82.3 -\nBoTNet-T2 [53] 33.5 7.3 81.7 -\nBoTNet-T4 [53] 54.7 10.9 82.8 -BoTNet-T5↑256 [53] 75.1 19.3 83.5 -\nDeiT-Ti [41] 5.7 1.1 72.2 74.5 Υ\nDeiT-S [41] 22.1 4.3 79.8 81.2 ΥDeiT-B [41] 86.6 16.9 81.8 83.4 ΥDeiT-B↑384 [41] 86.8 49.4 83.1 84.5 Υ\nConViT-Ti [54] 6 1 73.1 -\nConViT-S [54] 27 5.4 81.3 -ConViT-B [54] 86 17 82.4 -\nLocalViT-T [56] 5.9 1.3 74.8 -\nLocalViT-S [56] 22.4 4.6 80.8 -\nCeiT-T [55] 6.4 1.2 76.4 -\nCeiT-S [55] 24.2 4.5 82.0 -CeiT-T↑384 [55] 6.4 3.6 78.8 -CeiT-S↑384 [55] 24.2 12.9 83.3 -\nResT-Small [58] 13.7 1.9 79.6 -\nResT-Base [58] 30.3 4.3 81.6 -ResT-Large [58] 51.6 7.9 83.6 -\nViTC-1GF [62] 4.6 1.1 75.3 -\nViTC-4GF [62] 17.8 4.0 81.4 81.2 †ViTC-18GF [62] 81.6 17.7 83.0 84.9 †ViTC-36GF [62] 167.8 35 84.2 85.8 †\nCoAtNet-0 [40] 25 4.2 81.6 -\nCoAtNet-1 [40] 42 8.4 83.3 -CoAtNet-2 [40] 75 15.7 84.1 87.1 †CoAtNet-3 [40] 168 34.7 84.5 87.6 †CoAtNet-4↑384 [40] 275 189.5 - 88.4 †\nTNT-S [60] 23.8 5.2 81.3 -\nTNT-B [60] 65.6 14.1 82.8 -TNT-S↑384 [60] 23.8 - 83.1 -TNT-B↑384 [60] 65.6 - 83.9 -\nSwin-T [36] 29 4.5 81.3 -\nSwin-S [36] 50 8.7 83.0 -Swin-B [36] 88 15.4 83.3 85.2 †Swin-L↑384 [36] 197 104 - 86.4 †\nLV-ViT-S [44] 26 6.6 83.3 -\nLV-ViT-M [44] 56 16.0 84.0 -LV-ViT-L↑288 [44] 150 59.0 85.3 -LV-ViT-M↑384 [44] 56 42.2 85.4 -LV-ViT-L↑448 [44] 150 157.2 85.9 -\nMethod #Params.(M) FLOPs(G)\nImageNet-1k Top-1 Acc.Legend\n1K 21K/Distill.( Υ)\nVOLO-D1 [45] 27 6.8 84.2 -\nVOLO-D2 [45] 59 14.1 85.2 -VOLO-D3 [45] 86 20.6 85.4 -VOLO-D4 [45] 193 43.8 85.7 -VOLO-D5 [45] 296 69.0 86.1 -VOLO-D3↑448 [45] 86 67.9 86.3 -VOLO-D4↑448 [45] 193 197 86.8 -VOLO-D5↑448 [45] 296 304 87.0 -\nT2T-ViT-14 [64] 21.5 5.2 81.5 -\nT2T-ViT-19 [64] 39.2 8.9 81.9 -\nPVT-Ti [42] 13.2 1.9 75.1 -\nPVT-S [42] 24.5 3.8 79.8 -PVT-M [42] 44.1 6.7 81.2 -PVT-L [42] 61.4 9.8 81.7 -\nPVTv2-B2 [66] 25.4 4.0 82.0 -PVTv2-B4 [66] 62.6 10.1 83.6 -\nPiT-Ti [65] 4.9 0.7 73.0 74.6 Υ\nPiT-XS [65] 10.6 1.4 78.1 79.1 ΥPiT-S [65] 23.5 2.9 80.9 81.9 ΥPiT-B [65] 73.8 12.5 82.0 84.0 Υ\nCvT-13 [37] 20 4.5 81.6 -\nCvT-21 [37] 32 7.1 82.5 -CvT-13↑384 [37] 20 16.3 83.0 83.3 †CvT-21↑384 [37] 32 24.9 83.3 84.9 †CvT-W24↑384 [37] 277 193.2 - 87.7 †\nDeepViT-S [67] 27 6.2 82.3 -\nDeepViT-L [67] 55 12.5 83.1 -\nCaiT-XS-24 [43] 26.6 5.4 81.8 82.0 Υ\nCaiT-S-24 [43] 46.9 9.4 82.7 83.5 ΥCaiT-S-36 [43] 68.2 13.9 83.3 84.0 ΥCaiT-M-24 [43] 185.9 36.0 83.4 84.7 ΥCaiT-M-36 [43] 270.9 53.7 83.8 85.1 Υ\nDiversePatch-S12 [68] 22 - 81.2 -DiversePatch-S24 [68] 44 - 82.2 -DiversePatch-B12 [68] 86 - 82.9 -DiversePatch-B24 [68] 172 - 83.3 -DiversePath-B12↑384 [68] 86 - 84.2 -\nReﬁned-ViT-S [38] 25 7.2 83.6 -\nReﬁned-ViT-M [38] 55 13.5 84.6 -Reﬁned-ViT-L [38] 81 19.1 84.9Reﬁned-ViT-M↑384 [38] 55 49.2 85.6 -Reﬁned-ViT-L↑384 [38] 81 69.1 85.7 -\nCrossViT-9 [180] 8.6 1.8 73.9 -\nCrossViT-15 [180] 27.4 5.8 81.5 -CrossViT-18 [180] 43.3 9.0 82.5 -CrossViT-15*↑384 [180] 28.5 21.4 83.5 -CrossViT-18*↑384 [180] 44.6 32.4 83.9 -\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\n2) Brief Discussion on Alternatives: During the develop-\nment of the visual Transformers, the most common question\nis whether the visual Transformers can replace the tradi-\ntional convolution completely. By reviewing the history of\nthe performance improvements in the last year, there is no\nsign of relative inferiority here. The visual Transformers have\nreturned from a pure structure to a hybrid form, and the\nglobal information has gradually returned to a mixed stage\nwith the locality bias. Although the visual Transformers can\nbe equivalent to CNN or even has a better modeling capability,\nsuch a simple and effective convolution operation is enough to\nprocess the locality and the semantic features in the shallow\nlayer. In the future, the spirit of combining both of them shall\ndrive more breakthroughs for image classiﬁcation.\nIV. T RANSFORMER FOR DETECTION\nIn this section, we review visual Transformers for object\ndetection, which can be grouped into two folds: Transformer\nas the neck and Transformer as the backbone . For the neck\ndetectors, we mainly focus on a new representation speci-\nﬁed to the Transformer structure, called object query, that\na set of learned parameters aggregate instance features from\ninput images. The recent variants try to solve an optimal\nfusion paradigm in terms of either convergence acceleration\nor performance improvement. Besides these neck design, a\nproportion of backbone detectors also take speciﬁc strategies\ninto consideration. Finally, we evaluate them and analyze some\npotential methods for these detectors.\nA. Transformer Neck\nWe ﬁrst review DETR [30] and Pix2seq [76], the orig-\ninal Transformer detectors that reformulate two different\nparadigms for object detection. Subsequently, we mainly focus\non the DETR-based variants, improving such Transformer\ndetector in accuracy and convergence from ﬁve aspects: sparse\nattention, spatial prior , structural redesign, assignment opti-\nmization, and pre-training model.\n1) The Original Detectors: DEtection with TRansformer\n(DETR) [30] is the ﬁrst end-to-end Transformer detector\nthat eliminates hand-designed representations [181]–[184] and\nnon-maximum suppression (NMS) post-processing, which re-\ndeﬁnes the object detection as a set prediction problem. In\ndetail, a small set of learnable positional encodings, called\nobject queries, that are parallelly fed into the Transformer\ndecoder to extract the instance information from the image\nfeatures. Then, these object queries are independently pre-\ndicted to be a detection result. Instead of the vanilla k-class\nclassiﬁcation, a special class, no object label ( ∅) is added for\nk+1 class classiﬁcation. During the training process, a bipartite\nmatching strategy is applied between the predicted objects\nand the ground-truth to identify one-to-one label assignment,\nhence removing the redundant predictions at the inference time\nwithout NMS. In back propagation, a Hungarian loss includes\na log-likelihood loss for all classiﬁcation results and a box loss\nfor all the matched pairs. More details about the Hungarian\nmatching strategy are available in the App. B.\nbackbone\nset of image features\nCNN\npositional encoding\nencoder\ncontext embedding\n+\n+\ntransformer\ndecoder\ndecoder\ncontent embedding\ntransformer\nencoder\nobject query\nprediction heads\nFFN\nFFN\nFFN\nFFN\nclass,\nbox\nclass,\nbox\nclass,\nbox\nno\nobject\nprediction ground truth\nno object (∅) no object (∅)\nFig. 9. An overview of DETR. (Modiﬁed from [30].)\nOverall, DETR provides a new paradigm for end-to-end\nobject detection. The object query gradually learns an instance\nrepresentation during the interaction with image features. The\nbipartite matching allows a direct set prediction and easily\njoints to the one-to-one label assignment, hence eliminating\ntraditional post-processing. DETR achieves competitive per-\nformance on the COCO benchmark but suffers from slow\nconvergence as well as poor performance on small objects.\nAnother pioneered work is Pix2seq [76], treating generic\nobject detection as a language modeling task. Given an image\ninput, a vanilla sequential Transformer is executed to extract\nfeatures and generate a series of object descriptions (i.e.\nclass labels and bounding boxes) auto-regressively. Such a\nsimpliﬁed but more elaborate image caption method is derived\nunder the assumption that if a model learns about both location\nand label of an object, it can be taught to produce a description\nwith speciﬁed sequence [76]. Compared with DETR, Pix2seq\nattains a better result on small objects. How to combine both\nkinds of concepts is worthy of further consideration.\n2) Transformer with Sparse Attention: In DETR, the dense\ninteraction across both object queries and image features costs\nunbearable resources and slows down the convergence of\nDETR. Therefore, the most recent efforts aim to design a data-\ndependent sparse attention to address these issues.\nFollowing [185], Zhu et al. develop Deformable DETR to\nameliorate both training convergence and detection perfor-\nmance signiﬁcantly via multi-scale deformable attention [77].\nCompared with original DETR, the deformable attention mod-\nule only samples a small set of key (reference) points for\nfull features aggregation. Such sparse attention can be easily\nstretched to multi-scale feature fusion without the help of\nFPN [186]. Moreover, an iterative bounding box reﬁnement\nand a two-stage prediction strategy (Sec. IV-A3) are devel-\noped to further enhance the detection accuracy. Empirically,\nDeformable DETR achieves a higher accuracy (especially for\nsmall objects) with 10×less training epochs and reduces the\ncomputing complexity to O(2NqC2+min(HWC2,NqKC2))\nwith 1.6×faster inference speed. Please see the App. C for\nmore details of deformable attention mechanism.\nBy visualizing the attention map of DETR [30], Zheng et al.\nobserve that the semantically similar and spatially close ele-\nments always have a similar attention map in the encoder [78].\nThen they present an Adaptive Clustering Transformer (ACT),\nleveraging a multi-round sensitivity hashing to dynamically\ncluster the queries into different prototypes. The attention\nmap of the prototype is then broadcast to their corresponding\nqueries. Unlike the redesign of on sparse attention, Wang et al.\nintroduce a Poll and Pool (PnP) sampling model [79] to extract\nthe ﬁne foreground features and condense the contextual back-\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\nground features into a smaller one. Such ﬁne-coarse tokens\nare then fed into DETR to generate the detection results.\nInstead of the input sparsiﬁcation, Sparse DETR [80] applies a\nhysteretic scoring network (corresponding to the Poll operation\nin [79]) to update the expected tokens selectively within\nthe transformer encoder, where the top-k selected tokens are\nsupervised by pseudo labels from the binarized decoder cross-\nattention map with BCE loss.\n3) Transformer with Spatial Prior: Unlike anchor or other\nrepresentations directly generated by content and geometry\nfeatures [181], [187], object queries implicitly model the\nspatial information with random initialization, which is weakly\nrelated with the bounding box. The mainstream for spatial\nprior applications are the one-stage detector with empirical\nspatial information and the two-stage detector with geometric\ncoordinates initialization or Region-of-Interest (RoI) features.\nIn one-stage methods, Gao et al. suggest Spatially Mod-\nulated Cross-Attention (SMCA) [81] to estimate the object\nqueries’ spatial prior explicitly. Speciﬁcally, a Gaussian-like\nweight map generated by object queries is multiplied with\nthe corresponding cross-attention map to augment the RoI for\nconvergence acceleration. Furthermore, both intra-scale and\nmulti-scale self-attention layers are utilized in the Transformer\nencoder for multi-scale feature aggregation, and the scale-\nselection weights generated from object queries are applied\nfor scale-query adaptation. Meng et al. [82] extract the spatial\nattention map from the cross-attention formulation and observe\nthat the extreme region of such attention map has larger\ndeviations at the early training. Consequently, they propose\nConditional DETR where a new spatial prior mapped from\nreference points is adopted in the cross-attention layer, thereby\nattending to extreme regions of each object explicitly. The\nreference point is predicted by the object query or serves\nas a learned parameter replacing the object query. Follow-\ning [82], Anchor DETR [83] suggests to explicitly learn\nthe 2D anchor points ([ cx,cy]) and different anchor patterns\ninstead of the high-dimensional spatial embedding. Similar\nto [181], the pattern embeddings are assigned to the meshed\nanchor points so that they can detect different scale objects\nanywhere. DAB-DETR [84] then extends the 2D concept to a\n4D anchor box ([ cx,cy,w,h ]) to explicitly provide proposal\nbounding box information during the cross-attention. With\nthe auxiliary decoder loss of the coordinates offset [77],\nsuch a 4D box can be dynamically reﬁned layer-by-layer in\nthe decoder. However, the same reference boxes/points may\nseverely deteriorate queries’ saliency and confuses detector\ndue to the indiscriminative spatial prior. By assigning query-\nspeciﬁc reference points to object queries, SAP-DETR [85]\nonly predicts the distance from each side of the bounding\nbox to these points. Such a query-speciﬁc prior discrepancies\nqueries’ saliency and pave the way for fast model convergency.\nIn two-stage methods, Zhe et al. empower the Top-K region\nproposals from the outputs of the encoder to initialize the\ndecoder embedding instead of the learned content query [77].\nEfﬁcient DETR [86] also adopts a similar initialization oper-\nation for dense proposals and reﬁnes them in the decoder to\nget sparse prediction by using a shared detection head with\nthe dense parts. More interestingly, it is observed that small\nTABLE II\nCOMPARISON BETWEEN TRANSFORMER NECKS AND REPRESENTATIVE\nCNN S WITH RESNET-50 BACKBONE ON COCO 2017 VAL SET .\nMethod Epochs FLOPs(G) #Para.(M)FPS MS AP/AP50/AP75ApS/ApM/ApL\nCNN Backbone with Other RepresentationsFCOS [88], [187] 36 177 - 17 ✓41.0 /59.8/44.1 26.2/44.6/52.2Faster R-CNN [181] 37 180 42 26 ✓ 40.2/61.0/43.8 24.2/43.5/52.0Faster R-CNN+ [181] 109 180 42 26 ✓42.0 /62.1/45.5 26.6/45.4/53.4Mask R-CNN [188] 36 260 44 - ✓ 41.0/61.7/44.9 - / - / -Cas. Mask R-CNN [189] 36 739 82 18 ✓ 46.3/64.3/50.5 - / - / -Transformer Model as NeckDETR-R50 [30] 500 86 41 28 \u0017 42.0/62.4/44.2 20.5/45.8/61.1DETR-DC5 [30] 500 187 41 12 \u0017 43.3/63.1/45.9 22.5/47.3/61.1Pix2seq [76] 300 - 37 - \u0017 43.0/61.0/45.6 25.1/46.9/59.4Pix2seq-DC5 [76] 300 - 38 - \u0017 43.2/61.0/46.1 26.6/47.0/58.6Defor. DETR [77] 50 78 34 23 \u0017 39.7/60.1/42.4 21.2/44.3/56.0Defor. DETR-DC5 [77] 50 128 34 22 \u0017 41.5/61.8/44.9 24.1/45.3/56.0Defor. DETR-Iter [77] 50 173 40 19 ✓ 43.8/62.6/47.7 26.4/47.1/58.0Defor. DETR-Two [77] 50 173 40 19 ✓ 46.2/65.2/50.0 28.8/49.2/61.7ACT-DC5 (L=16) [78] MTKD [78] 156 - 14\u0017 40.6/ - / - 18.5/44.3/59.7ACT-DC5 (L=32) [78] MTKD [78] 169 - 16\u0017 43.1/ - / - 22.2/47.1/61.4PnP-DETR-0.33 [79] 500 77 - - \u0017 41.1/61.5/43.7 20.8/44.6/60.0PnP-DETR-0.5 [79] 500 79 - - \u0017 41.8/62.1/44.4 21.2/45.3/60.8PnP-DETR-DC5-0.5 [79] 500 136 - - \u0017 43.1/63.4/45.3 22.7/46.5/61.1Sparse-DETR-0.1 [80] 50 105 41 25 ✓ 45.3/65.8/49.3 28.4/48.3/60.1Sparse-DETR-0.5 [80] 50 136 41 21 ✓ 46.3/66.0/50.1 29.0/49.5/60.8SMCA [81] 50 86 40 22 \u0017 41.0/ - / - 21.9/44.3/59.1SMCA+ [81] 108 86 40 22 \u0017 42.7/ - / - 22.8/46.1/60.0SMCA [81] 50 152 40 10 ✓ 43.7/63.6/47.2 24.2/47.0 /60.4SMCA+ [81] 108 152 40 10 ✓ 45.6/65.5/49.1 25.9/49.3/62.6Condit. DETR [82] 108 90 44 17 \u0017 43.0/64.0/45.7 22.7/46.7/61.5Condit. DETR-DC5 [82] 108 195 44 11\u0017 45.1/65.4/48.5 25.3/49.0/62.2Anchor-DETR [83] 50 85 39 20 \u0017 42.1/63.1/44.9 22.3/46.2/60.0Anchor-DETR-DC5 [83] 50 151 39 14 \u0017 44.2/64.7/47.5 24.7/48.2/60.6DAB-DETR [84] 50 90 44 17 \u0017 42.2/63.1/44.7 21.5/45.7/60.3DAB-DETR-DC5 [84] 50 194 44 11 \u0017 44.5/65.1/47.7 25.3/48.2/62.3SAP-DETR [85] 50 92 47 16 \u0017 43.1/63.8/45.4 22.9/47.1/62.1SAP-DETR-DC5 [85] 50 197 47 9 \u0017 46.0/65.5/48.9 26.4/50.2/62.6Efﬁcient DETR [86] 36 159 32 - ✓ 44.2/62.2/48.0 28.4/47.5/56.6Efﬁcient DETR* [86] 36 210 35 - ✓ 45.1/63.1/49.1 28.3/48.4/59.0Dynamic DETR [87] 50 - 58 - ✓ 47.2/65.9/51.1 28.6/49.3/59.1TSP-FCOS [88] 36 189 52 15 ✓ 43.1/62.3/47.0 26.6/46.8/55.9TSP-RCNN [88] 36 188 64 11 ✓ 43.8/63.3/48.3 28.6/46.9/55.7TSP-RCNN+ [88] 96 188 64 11 ✓ 45.0/64.5/49.6 29.7/47.7/58.0YOLOS-S(800×) [89] 150 194 31 6 \u0017 36.1/56.4/37.1 15.3/38.5/56.1YOLOS-S(784×) [89] 150 172 28 6 ✓ 37.6/57.6/39.2 15.9/40.2/57.3YOLOS-B [89] 150 538 127 3 \u0017 42.0/62.2/44.5 19.5/45.3/62.1UP-DETR [90] 150 86 41 28 \u0017 40.5/60.8/42.6 19.0/44.4/60.0UP-DETR+ [90] 300 86 41 28 \u0017 42.8/63.0/45.3 20.8/47.1/61.7FP-DETR-Base [91] 50 - 36 - \u0017 43.3/63.9/47.7 27.5/46.1/57.0DN-DETR [92] 50 94 44 17 \u0017 44.1/64.4/46.7 22.9/48.0/63.4DN-DETR-DC5 [92] 50 202 44 8 \u0017 46.3/66.4/49.7 26.7/50.0/64.3DN-Defor.-DETR [92] 50 196 48 23 ✓ 46.3/66.4/49.7 26.7/50.0/64.3DINO-4scale [93] 36 279 47 24 ✓ 50.5/68.3/55.1 32.7/53.9/64.9DINO-5scale [93] 36 860 47 10 ✓ 51.0/69.0/55.6 34.1/53.6/65.6\n“MS” denotes to multi-scale features.Both GFLOPs and Params are measured by Detectron2. FPS is measured on a single A100 GPU.\nstacking decoder layers bring slight performance improve-\nment, but even more stacks yield even worse results. Dynamic\nDETR [87] regards the object prediction in a coarse-to-ﬁne\nprocess. Different from the previous RoI-based initialization\ndetectors, a cross-attention between the pool region features\nand the object embeddings is used to perform decoder em-\nbedding updates. The RoI features associated with the box\nembedding are then reﬁned accordingly in the next layer.\n4) Transformer with Redesigned Structure: Besides the\nmodiﬁcations focusing on the cross-attention, some works\nredesign an encoder-only structure to avoid the problem of\nthe decoder directly. TSP [88] inherits the idea of set predic-\ntion [30] and dismisses the decoder and the object query to\naccelerate convergence. Such encoder-only DETR reuses pre-\nvious representations [181], [187], and generates a set of ﬁxed-\nsize Features of Interests (FoI) [187] or proposals [181] that\nare subsequently fed into the Transformer encoder. In addition,\na matching distillation is applied to resolve the instability of\nthe bipartite matching, especially in the early training stage.\nFang et al. [89] combine the encoder-decoder neck of DETR\nand the encoder-only backbone of ViT into an encoder-only\ndetector and develop YOLOS, a pure sequence-to-sequence\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\nTransformer to unify the classiﬁcation and detection tasks. It\ninherits ViT’s structure and replaces the single class token\nwith ﬁxed size learned detection tokens. These object tokens\nare ﬁrst pre-trained on the transfer ability for the classiﬁcation\ntasks and then ﬁne-tuned on the detection benchmark.\n5) Transformer with Bipartite Matched Optimization: In\nDETR [30], the bipartite matching strategy forces the pre-\ndiction results to fulﬁl one-to-one label assignment during the\ntraining scheme. Such a training strategy simpliﬁes detection\npipeline and directly builds up an end-to-end system without\nthe help of NMS. To deeply understand the efﬁcacy of the\nend-to-end detector, Sun et al. devote to exploring a theoret-\nical view of one-to-one prediction [190]. Based on multiple\nablation and theoretical analyses, they conclude that the clas-\nsiﬁcation cost for one-to-one matching strategy serves as the\nkey component for signiﬁcantly avoiding duplicate predictions.\nEven so, DETR is suffering from multiple problems caused\nby bipartite matching. Li et al. [92] exploit a denoising DETR\n(DN-DETR) to mitigate the instability of bipartite matching.\nConcretely, a series of objects with slight perturbation is sup-\nposed to reconstruct their actual coordinates and classes. The\nmain ingredients of the denoising (or reconstruction) part are\nan attention mask that prevents information leakage between\nthe matching and noised parts, and a speciﬁed label embedding\nto indicate the perturbation. Recently, Zhang et al. [93] present\nan improved denoising training model called DINO (2022) by\nincorporating a contrastive loss for the perturbation groups.\nBased on DN-DETR [92], DINO attaches a “no object” class\nfor the negative example if the distance is far enough from\nthe perturbation, which avoids redundant prediction due to the\nconfusion of multiple reference points near an object. As a\nresult, DINO attains the current SoTA on the COCO dataset.\n6) Transformer Detector with Pre-Training: Inspired by the\npre-trained linguistic Transformer [3], [5], Dai et al. devise an\nUnsupervised Pre-training DETR (UP-DETR) [90] to assist\nthe convergence for supervised training. The objective of pre-\ntraining is to localize the random cropped patches from a\ngiven image. Speciﬁcally, each patch is assigned to a set of\nqueries and predicted independently via the attention mask.\nAn auxiliary reconstruction loss forces the detector to preserve\nthe feature discrimination so as to avoid over-bias towards the\nlocalization in pre-training. FP-DETR [91] devotes to narrow-\ning the gap between upstream and downstream tasks. During\nthe pre-training, a fully encoder-only DETR like YOLOS [89]\nviews query positional embeddings as a visual prompt to\nenhance target area attention and object discrimination. A\ntask adapter implemented by self-attention is used to enhance\nobject interaction during ﬁne-tuning.\nB. Transformer Backbone\nWe have reviewed numerous Transformer-based backbones\nfor image classiﬁcation [29], [41] in Sec. III. These back-\nbones can be easily incorporated into various frameworks\n(e.g., Mask R-CNN [188], RetinaNet [183], DETR [30],\netc.) to perform dense prediction tasks. For example, the\nhierarchical structure like PVT [42], [66], constructs the\nvisual Transformer as a high-to-low resolution process to\nlearn multi-scale features. The locally enhanced structure con-\nstructs the backbone as a local-to-global combination, which\ncan efﬁciently extract both short-range and long-range visual\ndependencies and avoid quadratic computational overhead,\nsuch as Swin-Transformer [36], ViL [62], and Focal Trans-\nformer [63]. App. D includes more detailed comparisons of\nthese models for the dense prediction tasks. In addition to the\ngeneric Transformer backbone, the Feature Pyramid Trans-\nformer (FPT) [94] combines the characteristics across both\nthe spaces and the scales, by using self-attention, top-down\ncross-attention, and bottom-up cross channel attention. Follow-\ning [191], HRFormer [95] introduces the advantages of multi-\nresolution to the Transformer along with non-overlapping local\nself-attention. HRViT [96] redesigns a heterogeneous branch\nand a cross-shaped attention block to further optimize the\ntrade-off between efﬁciency and accuracy.\nC. Discussion\nWe summarize ﬁve folds of the Transformer neck detectors\nin Tab. II, and more details of Transformer backbone for dense\nprediction tasks are referred to in Tab. IV. The majority of\nTransformer neck promotions concentrate on the following\nﬁve aspects: 1) The sparse attention model and the scoring\nnetwork are proposed to address the problem of redundant\nfeature interaction. These methods can signiﬁcantly alleviate\ncomputational costs and accelerate model convergence. 2) The\nexplicit spatial prior, which is decomposed into the selected\nfeature initialization and the positional information extracted\nby learned parameters, would enable the detector to predict\nthe results precisely. 3) Multi-scale features and layer-by-layer\nupdating are extended in the Transformer decoder for small\nobject reﬁnement. 4) The improved bipartite matching strategy\nis beneﬁcial to avoid redundant prediction as well as perform\nend-to-end object detection. 5) The encoder-only structure\nreduces the overall Transformer stack layers but increases the\nFLOPs excessively, while the encoder-decoder structure is a\ngood trade-off between FLOPs and Parameters, but the deeper\ndecoder layers may cause the problems of long training time\nand over-smooth.\nMoreover, there are many Transformer backbones for im-\nproving classiﬁcation performance, but few works are devel-\noped for the dense prediction tasks. In the future, we anticipate\nthat the Transformer backbone would cooperate with the deep\nhigh-resolution network to solve dense prediction tasks.\nV. T RANSFORMER FOR SEGMENTATION\nPatch-Based and Query-Based Transformer are the two ma-\njor ways for Segmentation. The latter can be further grouped\ninto Object Query and Mask Embedding methods.\nA. Patch-Based Transformer\nBecause of the receptive ﬁeld expansion strategy [192],\nCNNs require multiple decoder stacks to map the high-level\nfeatures into the original spatial resolution. Instead, patch-\nbased Transformer can easily incorporate with a simple de-\ncoder for segmented mask prediction because of its global\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\nBbox & \nClass\nMask & \nClass\nBbox & \nClass\nMask\n(a)\n(d)\nObject Queries\nMask Embeddings\nPre-training\nTraining\nPost-processing\nDetector Head\nMask Head\nTransformer with object queries\nTransformer with mask embeddings\nBbox & \nClass\nMask\nBbox & \nClass\nMask\n(b) (c)\nMask\n(e)\nFig. 10. Query-based frameworks for segmentation tasks. (a) Transfer learning\nfor ﬁne-tuning mask head. (b) Multi-task learning for two independent task.\n(c) Cascade learning for ﬁne-grained mask generation on the coarse region\nprediction. (d) The query embeddings are independently supervised by mask\nembeddings and boxes. (e) The box-free model directly predicts masks without\nbox branch and views segmentation task as a mask prediction problem.\nmodelling capability and resolution invariance. Zheng et al.\nextend ViT [29] for semantic segmentation tasks, and present\nSEgmentation TRansformer (SETR) [97] by employing three\nfashions of the decoder to perform per-pixel classiﬁcation:\nnaive up-sampling, progressive up-sampling, and multi-level\nfeature aggregation (MLA). SETR demonstrates the feasibility\nof the visual Transformer for the segmentation tasks, but it\nalso brings unacceptably extra GPU costs. TransUNet [98] is\nthe ﬁrst for medical image segmentation. Formally, it can be\nviewed as either a variant of SETR with MLA decoder [97],\nor a hybrid model of U-Net [193] and Transformer. Thanks to\nthe strong global modeling capability of Transformer encoder,\nSegformer [99] designs a lightweight decoder with only four\nMLP layers. Segformer shows superior performance as well\nas stronger robustness than CNNs when tested with multiple\ncorrupted types of images.\nB. Query-Based Transformer\nQuery embeddings are a set of scratch semantic/instance\nrepresentations gradually learning from the image inputs.\nUnlike patch embeddings, queries can more “fairly” integrate\nthe information from features and naturally join with the set\nprediction loss [30] for post-processing elimination. Existed\nquery-based models can be grouped into two categories. One\nis driven by both the tasks of detection and segmentation,\nsimultaneously (called object queries ). The other is only\nsupervised by the segmentation task (called mask embeddings).\n1) Object Queries: There are three training manners for\nobject query based methods (Fig. 10(a)-(c)). With the success\nof DETR [30] for the object detection tasks, the authors\nextend it to panoptic segmentation (hence termed Panoptic\nDETR [30]) by training a mask head based on the pre-trained\nobject queries (Fig. 10(a)). In detail, a cross-attention block\nbetween the object queries and the encoded features is applied\nto generate an attention map for each object. After an up-\nsampling FPN-style CNN, a spatial argmax operation fuses\nthe resulting binary masks to a non-overlapping prediction.\nInstead of using a multi-stage serial training process, Cell-\ntransformerdecoder\nbackbonepixeldecoder\n𝑁queries\nimage features ℱ per-pixel embeddings\nMLP𝑁class predictions\n𝑁mask embeddings\n𝑁mask predictions\nsemantic segmentationinference onlysemanticsegmentation\nclassification loss\nbinary mask loss\n𝐶ℰ×𝐻×𝑊𝐶ℰ×𝑁\n𝑁×𝐻×𝑊\n𝑁×(𝐾+1)\n𝐾×𝐻×𝑊pixel-level module\nsegmentation moduletransformer module\ndrop ∅\n𝒬\nℰ\"#$%&ℰ'()*\nFigure 2: MaskFormer overview. We use a backbone to extract image features F. A pixel decoder\ngradually upsamples image features to extract per-pixel embeddings Epixel. A transformer decoder\nattends to image features and producesNper-segment embeddingsQ. The embeddings independently\ngenerate Nclass predictions with Ncorresponding mask embeddings Emask. Then, the model predicts\nN possibly overlapping binary mask predictions via a dot product between pixel embeddings Epixel\nand mask embeddings Emask followed by a sigmoid activation. For semantic segmentation task we\ncan get the ﬁnal prediction by combining N binary masks with their class predictions using a simple\nmatrix multiplication (see Section 3.4). Note, the dimensions for multiplication ⨂are shown in gray.\nNote, that most existing mask classiﬁcation models use auxiliary losses ( e.g., a bounding box\nloss [21, 4] or an instance discrimination loss [42]) in addition to Lmask-cls. In the next section we\npresent a simple mask classiﬁcation model that allows end-to-end training with Lmask-cls alone.\n3.3 MaskFormer\nWe now introduce MaskFormer, the new mask classiﬁcation model, which computesN probability-\nmask pairs z = {(pi,mi)}N\ni=1. The model contains three modules (see Fig. 2): 1) a pixel-level\nmodule that extracts per-pixel embeddings used to generate binary mask predictions; 2) a transformer\nmodule, where a stack of Transformer decoder layers [41] computes N per-segment embeddings;\nand 3) a segmentation module, which generates predictions {(pi,mi)}N\ni=1 from these embeddings.\nDuring inference, discussed in Sec. 3.4, pi and mi are assembled into the ﬁnal prediction.\nPixel-level module takes an image of size H ×W as input. A backbone generates a (typically)\nlow-resolution image feature map F∈ RCF×H\nS ×W\nS , where CF is the number of channels and S\nis the stride of the feature map ( CF depends on the speciﬁc backbone and we use S = 32 in this\nwork). Then, a pixel decoder gradually upsamples the features to generate per-pixel embeddings\nEpixel ∈RCE×H×W, where CEis the embedding dimension. Note, that any per-pixel classiﬁcation-\nbased segmentation model ﬁts the pixel-level module design including recent Transformer-based\nmodels [37, 53, 29]. MaskFormer seamlessly converts such a model to mask classiﬁcation.\nTransformer module uses the standard Transformer decoder [41] to compute from image features\nFand N learnable positional embeddings (i.e., queries) its output, i.e., N per-segment embeddings\nQ∈ RCQ×N of dimension CQthat encode global information about each segment MaskFormer\npredicts. Similarly to [4], the decoder yields all predictions in parallel.\nSegmentation module applies a linear classiﬁer, followed by a softmax activation, on top of the\nper-segment embeddings Qto yield class probability predictions {pi ∈∆K+1}N\ni=1 for each segment.\nNote, that the classiﬁer predicts an additional “no object” category (∅) in case the embedding does\nnot correspond to any region. For mask prediction, a Multi-Layer Perceptron (MLP) with 2 hidden\nlayers converts the per-segment embeddingsQto N mask embeddings Emask ∈RCE×N of dimension\nCE. Finally, we obtain each binary mask prediction mi ∈[0,1]H×W via a dot product between the\nith mask embedding and per-pixel embeddings Epixel computed by the pixel-level module. The dot\nproduct is followed by a sigmoid activation, i.e., mi[h,w] = sigmoid(Emask[:,i]T ·Epixel[:,h,w ]).\nNote, we empirically ﬁnd it is beneﬁcial to not enforce mask predictions to be mutually exclusive to\neach other by using a softmax activation. During training, the Lmask-cls loss combines a cross entropy\nclassiﬁcation loss and a binary mask loss Lmask for each predicted segment. For simplicity we use the\nsame Lmask as DETR [4], i.e., a linear combination of a focal loss [27] and a dice loss [33] multiplied\nby hyper-parameters λfocal and λdice respectively.\n4\nFig. 11. Illustration of Maskformer. (from [32].)\nDETR and VisTR develop a parallel model for end-to-end\ninstance segmentation (Fig. 10(b)). Based on DETR [30], Cell-\nDETR leverages a cross-attention block to extract instance-\nwise features from the box branch and fuses the previous\nbackbone features to augment the CNN decoder for accu-\nrate instance mask segmentation of biological cells. Another\nextension is VisTR [101] that directly formulates the video\ninstance segmentation (VIS) task as parallel sequence predic-\ntion. Apart from the similar structure as Cell-DETR [100],\nthe key of VisTR is a bipartite matching loss at the instance\nsequence level to maintain the order of outputs, so as to adapt\nDETR [30] to VIS for direct one-to-one predictions. Unlike\nprior works that treat detection and mask generation branches\nseparately, QueryInst [102] builds a hybrid cascaded network\n(Fig. 10(c)), where the previous box outputs together with the\nshared queries serve as the inputs of the mask head for accurate\nmask segmentation. Notably, QueryInst leverages the shared\nqueries to keep the instance correspondences across multi-\nstage, so that mitigating the problem of inconsistent objects\nin previous non-query based methods [189], [194]. QueryInst\nobtains the latest SoTA results on the COCO datasets.\n2) Mask Embeddings: The other framework makes efforts\nto use queries to predict mask directly, and we refer to this\nlearned mask-based query as mask embeddings. Unlike the\nobject queries, mask embeddings are only supervised by the\nsegmentation tasks. As shown in Fig. 10(d), two disjoint sets of\nqueries are employed parallelly for different tasks, and the box\nlearning is viewed as an auxiliary loss for further enhancement.\nFor semantic and box-free instance segmentation, a series of\nquery-based Transformers predict the mask directly without\nthe help of the box branch (Fig. 10(e)).\nFrom the auxiliary training perspective, the core is how to\nenable 1D sequence outputs to be supervised by 2D mask\nlabels directly. To this end, ISTR [103] empowers a mask pre-\ncoding method to encode the ground-truth spatial mask into\nlow-dimensional mask embedding for instance segmentation.\nSimilarly, Dong et al. propose a more straightforward pipeline\nSOLQ [104] and explore three reversible compression encod-\ning methods for mask embeddings. In detail, a set of uniﬁed\nqueries is applied to perform multiple representation learning\nparallelly: classiﬁcation, box regression, and mask encoding.\nBased on the original DETR [192], SOLQ adds a mask branch\nto produce mask embedding loss. Both ISTR and SOLQ obtain\ncomparable results and outperform previous methods even\nwith approximation-based suboptimal embeddings. However,\nthere exists a huge gap between APbox and APseg (Tab. III).\nFrom the box-free perspective, Wang et al. pioneer a new\nparadigm Max-DeepLab [31] that directly predicts panoptic\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\nmasks from the query without the help of the box branch.\nSpeciﬁcally, it forces the query to predict the corresponding\nmask via a PQ-style bipartite matching loss and a dual-\npath Transformer structure. Given a set of mask embeddings\nand an image input, Max-DeepLab processes them separately\nin both Transformer and CNN path, and then generates a\nbinary mask and a class for each query, respectively. Max-\nDeepLab achieves new SoTA with 51.3% PQ on COCO test-\ndev set, but leads to heavy computational costs due to its\ndual-path high-resolution processing. Segmenter [105] views\nthe semantic segmentation task as a sequence-to-sequence\nproblem. In detail, a set of mask embeddings that represent\ndifferent semantic classes are fed into the Transformer encoder\ntogether with image patches, and then a set of labeled masks\nare predicted for each patch via an argmax operation.\nUnlike the conventional semantic segmentation methods that\npredict mask at the pixel level, Cheng et al. reformulate the\nsemantic segmentation task as a mask prediction problem\nand enable this output format to the query-based Trans-\nformer, which is called Maskformer [32]. Different from Max-\nDeepLab [31], Maskformer leverages a simple Transformer\ndecoder without redundant connection as well as a sigmoid\nactivation for overlapping binary masks selection. It not only\noutperforms the current per-pixel classiﬁcation SoTA on large-\nclass semantic segmentation datasets, but also generalizes the\npanoptic segmentation task with a new SoTA result (Tab. III).\nC. Discussion\nWe summarize the aforementioned Transformers according\nto three different tasks. Table III(a) focuses on ADE20K (170\nclasses). It can be shown that when trained on the datasets with\nlarge numbers of classes, the segmentation performance of\nvisual Transformers is improved signiﬁcantly. Table III(b) fo-\ncuses on COCO test dataset for instance segmentation. Clearly,\nthe visual Transformers with mask embeddings surpass most\nprevailing models for both segmentation and detection tasks.\nHowever, there is a huge performance gap between APbox\nand APseg. With the cascaded framework, QueryInst [102]\nattains the SoTA among various Transformer models. It is\nworthy of further study for combining the visual Transformers\nwith the hybrid task cascade structures. Table III(c) focuses on\npanoptic segmentation. Max-DeepLab [31] is general to solve\nboth foreground and background in the panoptic segmentation\ntask via a mask prediction format, while Maskformer [32]\nsuccessfully employs this format for semantic segmentation\nand uniﬁes both semantic and instance segmentation tasks\ninto a single model. Based on their performances in the\npanoptic segmentation ﬁeld, we can conclude that the visual\nTransformers could unify multiple segmentation tasks into one\nbox-free framework with mask prediction.\nVI. T RANSFORMER FOR 3D V ISUAL RECOGNITION\nWith the rapid development of 3D acquisition technology,\nstereo/monocular images and LiDAR (Light Detection And\nRanging) point clouds become the popular sensory data for 3D\nrecognition. Discriminated from the RGB(D) data, point cloud\nrepresentation pays more attention to distance, geometry, and\nTABLE III\nCOMPARISON BETWEEN CNN- BASED AND TRANSFORMER -BASED MODEL\nON ADE20K AND COCO FOR DIFFERENT SEGMENTATION TASKS . “+MS”\nDENOTES THE PERFORMANCE TRAINED WITH MULTI -SCALE INPUTS .\n(a) ADE20K Val. Set for Semantic Segmentation\nMethod Backbone imagesize #Params.(M) FLOPs(G) FPS mIoU +MS\nUperNet[32][195][196]\nR-50 [11] 512 67 238 23.4 42.1 42.8R-101 [11] 512 86 257 20.3 43.8 44.9Swin-T [36] 512 60 236 18.5 44.5 46.1Swin-S [36] 512 81 259 15.2 47.6 49.3Swin-B†[36] 640 121 471 8.7 50.0 51.6Swin-L†[36] 640 234 647 6.2 52.0 53.5\nSegformer [99] MiT-B3 512 47 79 - 49.4 50.0MiT-B4 512 64 96 15.4 50.3 51.1MiT-B5 640 85 183 9.8 51.0 51.8\nSegmenter [105]ViT-S/16†[29] 512 27 - 34.8 45.3 46.9ViT-B/16†[29] 512 106 - 24.1 48.5 50.0ViT-L/16†[29] 640 334 - - 51.8 53.6\nMaskFormer [32]\nR-50 [11] 512 41 53 24.5 44.5 46.7R-101 [11] 512 60 73 19.5 45.5 47.2Swin-T [36] 512 42 55 22.1 46.7 48.8Swin-S [36] 512 63 79 19.6 49.8 51.0Swin-B†[36] 640 102 195 12.6 52.7 53.9Swin-L†[36] 640 212 375 7.9 54.1 55.6\n(b): COCO Test-Dev for Instance Segmentation\nMethod Backbone Epochs AP box/APseg APsegS ApsegM ApsegL FPS\nMask R-CNN[188]R-50-FPN [11] 36 41.3/37.5 21.1 39.6 48.3 15.3R-101-FPN [11] 36 43.1/38.8 21.8 41.4 50.5 11.8\nBlend Mask[197] R-50-FPN [11] 36 43.0/37.8 18.8 40.9 53.6 15.0R-101-FPN 36 44.7/39.6 22.4 42.2 51.4 11.5\nSOLO v2[198] R-50-FPN [11] 36 40.7/38.2 16.0 41.2 55.4 10.5R-101-FPN [11] 36 42.6/39.7 17.3 42.9 57.4 9.0\nISTR [103] R-50-FPN [11] 36 46.8/38.6 22.1 40.4 50.6 13.8R-101-FPN [11] 36 48.1/39.9 22.8 41.9 52.3 11.0\nSOLQ [104] R-50 [11] 50 47.8/39.7 21.5 42.5 53.1 -R-101 [11] 50 48.7/40.9 22.5 43.8 54.6 -Swin-L†[36] 50 55.4/45.9 27.8 49.3 60.5 -\nQueryInst[102]\nR-50-FPN [11]36 44.8/40.1 23.3 42.1 52.0 10.5R-50-FPN [11] 36 45.6/40.6 23.4 42.5 52.8 7.0R-101-FPN [11] 36 47.0/41.7 24.2 43.9 53.9 6.1Swin-L†[36] 50 56.1/49.1 31.5 51.8 63.2 3.3\n(c): COCO Panopticon Minival. for Panoptic Segmentation\nMethod Backbone Epochs #Params.(M) FLOPs(G) PQ PQThPQSt\nDETR [30] R-50 [11]500+25 43 137 43.4 48.2 36.3R-101 [11] 62 157 45.1 50.5 37.0\nMaxDeepLab [31]Max-S 54 62 162 48.4 53.0 41.5Max-L 451 1846 57.0 42.2 51.1\nMaskFormer [199]\nR-50 [11]\n300\n45 181 46.5 51.0 39.8R-101 [11] 64 248 47.6 52.5 40.3Swin-T [36] 42 179 47.7 51.7 41.7Swin-S [36] 63 259 49.7 54.4 42.6Swin-B [36] 102 411 51.1 56.3 43.2Swin-L†[36] 212 792 52.7 58.5 44.0\n†denotes the model pre-trained on ImageNet-21k\nshape information. Notably, such a geometric feature is signif-\nicantly suitable for Transformer on account of its characteristic\non sparseness, disorder, and irregularity. Following the success\nof 2D visual Transformers, substantial approaches are devel-\noped for 3D visual analysis. This section exhibits a compact\nreview for 3D visual Transformers following Representation\nlearning, Cognition mapping, and Speciﬁc processing.\nA. Representation Learning\nCompared with conventional hand-designed networks, vi-\nsual Transformer is more appropriate for learning semantic\nrepresentations from point clouds, in which such irregular\nand permutation invariant nature can be transformed into a\nseries of parallel embeddings with positional information.\nIn view of this, Point Transformer [106] and PCT [107]\nﬁrstly demonstrate the efﬁcacy of the visual Transformer for\n3D representation learning. The former merges a hierarchical\nTransformer [106] with the down-sampling strategy [200]\nand extends their previous vector attention block [25] to\n3D point clouds. The latter ﬁrst aggregates neighbour points\nand then processes such neighbour embeddings on a global\noff-set Transformer where a knowledge transfer from Graph\nConvolution Network (GCN) is applied for noise mitigation.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\nNotably, the positional encoding, a signiﬁcant operation of the\nvisual Transformer, is diminished in both the approaches be-\ncause of points’ inherent coordinate information. PCT directly\nprocesses on the coordinates without positional encodings,\nwhile Point Transformer adds a learnable relative positional\nencoding for further enhancement. Lu et al. leverage a local-\nglobal aggregation module 3DCTN [108] to achieve local\nenhancement and cost-efﬁciency. Given the multi-stride down-\nsampling groups, an explicit graph convolution with max-\npooling operation are used to aggregate the local informa-\ntion within each group. The resulting group embeddings are\nconcatenated and fed into the improved transformer [106],\n[107] for global aggregation. Park et al. present Fast Point\nTransformer [109] to optimize the model efﬁciency by using\nvoxel-hashing neighbor search, voxel-bridged relative posi-\ntional encoding, and cosine similarity based local attention.\nFor dense prediction, Pan et al. propose a customized point-\nbased backbone Pointformer [110] for attending the local and\nglobal interactions separately within each layer. Different from\nprevious local-global forms, a coordinate reﬁnement operation\nafter the local attention is adopted to update the centroid point\ninstead of the surface one. And a local-global cross attention\nmodel fuses the high-resolution features, followed by global\nattention. Fan et al. return to a Single-stride Sparse Trans-\nformer (SST) [111] rather than the down-sampling operation\nto address the problem for small scale detection. Similar to\nSwin [36], a shifted group in continuous Transformer block\nis adopted to attend to each group of tokens separately, which\nfurther mitigates the computation problem. In voxel-based\nmethods, V oxel Transformer (V oTr) [112] separately operate\non the empty and non-empty voxel positions effectively via\nlocal attention and dilated attention blocks. V oxSeT [113]\nfurther decomposes the self-attention layer into two cross-\nattention layers, and a group of latent codes link them to\npreserve global features in a hidden space.\nFollowing the mentioned methods in Sec. III-G, a series\nof self-supervised Transformers are also extended to 3D\nspaces [114]–[116]. Speciﬁcally, Point-BERT [114] and Point-\nMAE [115] directly transfer the previous works [71], [72] to\npoint clouds, while MaskPoint [116] changes the generative\ntraining scheme by using a contrastive decoder as similar\nas DINO (2022) [93] for binary noise/part classiﬁcation.\nBased on large experiments, we can conclude that such\ngenerative/contrastive self-training methods empower visual\nTransformers to be valid in either images or points.\nB. Cognition Mapping\nGiven rich representation features, how to directly map the\ninstance/semantic cognition to the target outputs also arouse\nconsiderable interests. Different from 2D images, the objects in\n3D scenes are independent and can be intuitively represented\nby a series of discrete surface points. To bridge the gap, some\nexisted methods transfer domain knowledge into 2D prevailing\nmodels. Following [30], 3DETR [117] extends an end-to-end\nmodule for 3D object detection via farthest point sampling and\nFourier positional embeddings for object queries initialization.\nGroup-Free 3D DETR [118] applies a more speciﬁed and\nstronger structure than [117]. In detail, it directly selects a\nset of candidate sample points from the extracted point clouds\nas the object queries and updates them in the decoder layer-\nby-layer iteratively. Moreover, the K-closed inside points are\nassigned positive and supervised by a binary objectiveness loss\nin both sampler and decoder heads. Sheng et al. proposes\na typical two-stage method that leverages a Channel-wise\nTransformer 3D Detector (CT3D) [119] to simultaneously ag-\ngregate proposal-aware embedding and channel-wise context\ninformation for the point features within each proposal.\nFor monocular sensors, both MonoDTR [120] and Mon-\noDETR [121] utilize an auxiliary depth supervision to estimate\npseudo Depth Positional Encodings (DPEs) during the training\nprocess. In MonoDETR [120], DPEs are ﬁrst attached with the\nimage features for Transformer encoder and then serve as the\ninputs of the DETR-like [30] decoder to initialize the object\nqueries. In MonoDETR [121], both visual features and DPEs\nare ﬁrst extracted by two different encoders parallelly and then\ninteract with object queries via two successive cross-attention\nlayers. Based on foreground depth supervision and narrow\ncategorisation interval, MonoDETR obtains the SoTA result\non the KITTI benchmark. DETR3D [122] introduces a multi-\ncamera 3D object detection paradigm where both 2D images\nand 3D positions are associated by the camera transformation\nmatrices and a set of 3D object queries. TransFusion [123]\nfurther takes the advantages of both LiDAR points and RGB\nimages by interacting with object queries through two Trans-\nformer decoder layers successively. More multi-sensory data\nfusion are introduced in Sec. VII-A.\nC. Speciﬁc Processing\nLimited by sensor resolution and view angle, point clouds\nare afﬂicted with incompletion, noise, and sparsity problems\nin real-world scenes. To this end, PoinTr [124] represents\nthe original point cloud as a set of local point proxies and\nleverages a geometry-aware encoder-decoder Transformer to\nmigrate the centre point proxies towards incomplete points\ndirection. SnowﬂakeNet [125] formulates the process of com-\npleting point clouds as a snowﬂake-like growth, which pro-\ngressively generates child points from their parent points im-\nplemented by a point-wise splitting deconvolution strategy. A\nskip-Transformer for adjacent layers further reﬁnes the spatial-\ncontext features between parents and children to enhance\ntheir connection regions. Choe et al. unify various genera-\ntion tasks (e.g. denosing, completing and super-resolution)\ninto a Point cloud Reconstruction problem, hence termed\nPointRecon [126]. Based on voxel hashing, it covers the\nabsolute-scale local geometry and utilizes a PointTransformer-\nlike [106] structure to aggregate each voxel (the query) with\nits neighbours (the value-key pair) for ﬁne-grained conversion\nfrom the discrete voxel to a group of point sets. Moreover,\nan ampliﬁed positional encoding is adapted to the voxel local\nattention scheme, implemented by using a negative exponential\nfunction with L1-loss as weights for vanilla positional encod-\nings. Notably, compared with masked generative self-training,\nthe completion task directly generates a set of complete points\nwithout the explicit spatial prior of incomplete points.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\nVII. T RANSFORMER FOR MULTI -SENSORY DATA STREAM\nIn the real world, multiple sensors are always used com-\nplementarily rather than a single one. To this end, recent\nworks start to explore different fusing methods to coop-\nerate multi-sensory data stream effectively. Compared with\nthe typical CNNs, Transformer is naturally appropriate for\nmulti-stream data fusion because of its nonspeciﬁc embedding\nand dynamically interactive attention mechanism. This section\ndetails these methods according to their data stream sources:\nHomologous Stream and Heterologous Stream.\nA. Homologous Stream\nHomologous stream is a set of multi-sensory data with\nsimilar inherent characteristics, such as multi-view, multi-\ndimension, and multi-modality visual stream data. They can be\ncategorized into two groups: Interactive Fusion and Transfer\nFusion, according to their fusion mechanism.\n1) Interactive Fusion: The classical fusion pattern of CNN\nadopts a channel concatenation operation. However, the same\npositions from different modalities might be anisotropic, which\nis unsuitable for the translation-invariant bias of CNN. Instead,\nthe spatial concatenation operation of Transformer enables\ndifferent modalities to interact beyond the local restriction.\nFor the local interaction, MVT [127] spatially concatenates\nthe patch embeddings from different views and strengthens\ntheir interaction via a modal-agnostic Transformer encoder.\nConsidering the redundant features from different modalities,\nMVDeTr [128] projects each view of features onto the ground\nplane and extends the multi-scale deformable attention [77]\nto a multi-view design. TransFuser [129], COTR [130], and\nmmFormer [134] deploy a hybrid model. TransFuser models\nimage and LiDAR inputs separately by using two different\nconvolution backbones and links the intermediate feature maps\nvia a Transformer encoder together with a residual connection.\nCOTR shares the CNN backbone for each of view images\nand inputs the resulted features into a Transformer encoder\nblock with a spatially expanded mesh-grid positional encoding.\nmmFormer exploits a modality-speciﬁc Transformer encoder\nfor each sequence of MRI image and a modality-correlated\nTransformer encoder for multi-modal modeling.\nFor the global interaction, Wang et al. [131] leverage a\nshared backbone to extract the features for different views.\nInstead of pixel/patch wise concatenation in COTR [130], the\nextracted view-wise global features are spatially concatenated\nto perform view fusion within a Transformer. Considering\nthe angular and position discrepancy across different camera\nviews, TransformerFusion [133] ﬁrst converts each view fea-\nture into an embedding vector with the intrinsics and extrinsics\nof their camera views. These embeddings are then fed into\na global Transformer whose attention weights are used for a\nframe selection so as to compute efﬁciently. To unify the multi-\nsensory data in 3D detection, FUTR3D [132] projects the\nobject queries in DETR-like decoder into a set of 3D reference\npoints. These points together with their related features are\nsubsequently sampled from different modalities and spatially\nconcatenated to update the object queries.\n2) Transfer Fusion: Unlike the interactive fusion imple-\nmented by the Transformer encoder with self-attention, the\nother fusing form is more like a transfer learning from the\nsource data to the target one via a cross-attention mecha-\nnism. For instance, Tulder et al. [135] insert two cooperative\ncross-attention Transformers into the intermediate backbone\nfeatures for bridging the unregistered multi-view medical\nimages. Instead of the pixel-wise attention form, a token-\npixel cross-attention is further developed to alleviate arduous\ncomputation. Long et al. [136] propose a epipolar spatio-\ntemporal Transformer for multi-view image depth estimation.\nGiven a single video containing a series of static multi-view\nframes, the neighbour frames are ﬁrst concatenated and the\nepipolar is then warped into the centre camera space. The\nresulted frame volume ﬁnally serves as the source data to\nperform fusion with the centre frame through a cross-attention\nblock. With the spatially-aligned data streams, DRT [137]\nﬁrst explicitly models the relation map between different data\nstreams by using a convolution layer. The resulting maps are\nsubsequently fed into a dual-path cross-attention to build both\nlocal and global relationships parallelly, thereby it can collect\nmore regional information for glaucoma diagnosis.\nB. Heterologous Stream\nVisual Transformers also perform excellently on heterolo-\ngous data fusion, especially in visual-linguistic representation\nlearning. Although different tasks may adopt different train-\ning schemes, such as supervised/self-supervised learning or\ncompact/large-scale datasets, we here categorize them into two\nrepresentative groups only according to their cognitive forms:\n1) Visual-Linguistic Pre-Training including Vision-Language\nPre-training (VLP) and Contrastive Language-Image Pre-\ntraining (CLIP), 2) and Visual Grounding such as Phrase\nGrounding (PG), Referring Expression Comprehension (REC).\nMore comparisons please see Tab. V.\n1) Visual-Linguistic Pre-Training: Due to limited anno-\ntated data, early VLP methods commonly rely on off-the-\nshelf object detector [201] and text encoder [5] to extract\ndata-speciﬁc features for joint distribution learning. Given\nan image-text pair, an object detector pre-trained on Visual\nGenome (VG) [202] ﬁrst extracts a set of object-centric RoI\nfeatures from the image. The RoI features serving as visual\ntokens are then merged with text embeddings for pre-deﬁned\ntasks pre-training. Basically, these methods are grouped into\ndual-stream and single-stream fusion.\nThe dual-stream methods, including ViLBERT [139] and\nLXMERT [140], apply a vision-language cross-attention layer\nbetween two data-speciﬁc frameworks for multi-modal trans-\nferring fusion. Concretely, ViLBERT [139] is pre-trained\nthrough Masked Language Modeling (MLM), Masked Region\nClassiﬁcation (MRC), and Image Text Alignment (ITA) on\nConceptual Captions (CC) [203] with 3M image-text pairs.\nLXMERT [140] extends the pre-training datasets to a large-\nscale combination and further indicates that the pre-trained\ntask-speciﬁc (BERT [5]) weights initialization is harmful to\nthe pre-training of multi-sensory data fusion.\nVideoBERT [138] is the ﬁrst single-stream VLP method,\nwhich clusters latent space features of each video frame as vi-\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\nsual tokens and organizes their corresponding text embeddings\nby using a captioning API. Subsequently, these features are\ntogether fed into a cross-modality self-attention layer for joint\nrepresentation learning. Following [138], VisualBERT [141]\nextends such a single-stream framework for various image-\ntext tasks and adds a segment embedding to distinguish\nbetween textual and visual tokens. VL-BERT [142] suggests\nthat unmatched image-caption pairs over the ITA pre-training\nmay decrease the accuracy of downstream tasks. And the\nauthors further introduce both text-only corpus and unfrozen\ndetector strategies for pre-training enhancement. Instead, such\na “harmful” pre-training strategy is refuted by UNITER [143],\nand the authors deploy an optimal transport loss to explicitly\nbuild Word-Region Alignment (WRA) at the instance level.\nTo the same end, Oscar [144] uses shared linguistic semantic\nembeddings of a salient object class (called tag) as an anchor\npoint to link both region and its paired words. Zhou et al.\npropose Uniﬁed VLP [145] to handle both generation and\nunderstanding tasks via a shared Transformer encoder-decoder\nwith two customized attention masks. Without extra auxiliary\ntraining, Uniﬁed VLP only adopts MLM during pre-training\nand attains superior results on Visual Question Answering\n(VQA) [204] and Visual Captioning (VC) [205] tasks.\nHowever, these methods rely heavily on the visual extrac-\ntor or predeﬁned visual vocabulary, leading to a bottleneck\nof the VLP expressive upper bound. To address this issue,\nVinVL [147] develops an improved object detector for VLP\npre-training on multiple large-scale dataset combination. In-\nstead of the object-centric RoI features, ViLT [146] initializes\nthe interaction Transformer weights from a pre-trained ViT,\nand adopts whole word masking and image augmentation strat-\negy for VLP pre-training. UniT [151] follows the architecture\nof DETR and applies a wide range of task for uniﬁed Trans-\nformer pre-training via different task-speciﬁc output heads\nsimultaneously. SimVLM [152] adopts [40] to obtain image\nfeatures and designs a Preﬁx Language Modeling as pre-\ntraining objective to generalize zero-shot image captioning.\nBesides the conventional pre-training scheme with multi-\ntask supervision, another recent line has been developed\nfor contrastive learning. The most representative work is\nCLIP [148]. Based on the 400M Internet image-text pairs\ndatasets, both image and text encoder are jointly trained by\na contrastive loss for ITA. Different from previous methods,\nCLIP enables the pre-trained model with a linear classiﬁer\nto zero-shot transfer to the most visual downstream tasks\nefﬁciently by embedding the whole semantics of the objective\ndataset’s classes. Based on extensive experiments on over\n30 existing CV tasks (e.g., classiﬁcation and action recog-\nnition), CLIP attains superior results to classical supervised\nmethods, demonstrating that such task-agnostic pre-training is\nalso generalized well in the CV ﬁeld. ALIGN [150] further\nexpands a noisy dataset of over one billion image alt-text\npairs rather than the elaborate ﬁltering or post-processing steps\nin CLIP [148]. Combining masked modeling and contrastive\nlearning pre-training strategy, Data2Vec [153] proposes a self-\ndistilled network treating the masked features as a type of\ndata augmentation, whose structure is analogous to DINO\n(2021) [74]. By testing on different sensory benchmarks\nI 1 ·T 2 I 1 ·T 3 …\nI 2 ·T 1 I 2 ·T 3 …\nI 3 ·T 1 I 3 ·T 2 …\n⋮ ⋮ ⋮ \nI 1 ·T 1 \nI 2 ·T 2 \nI 3 ·T 3 \n(1) Contrastive pre-training\nImage \nEncoder \nT ext \nEncoder Pepper\tthe\naussie\tpup\nPepper\tthe\naussie\tpup\nPepper\tthe\naussie\tpup\nPepper\tthe\naussie\tpup\nT 1 T 2 T 3 …\nI 1 \nI 2 \nI 3 \n⋮ \n(2) Create dataset classiﬁer from label text\nplane\ncar\ndog\n⋮ \nbird\nA\tphoto\tof\na\t{object}.\n⋮ \nT ext \nEncoder \nT 1 T 2 T 3 T N \n…\n(3) Use for zero-shot prediction\nImage \nEncoder I 1 I 1 ·T 2 I 1 ·T N I 1 ·T 1 \n…\n…\nA\tphoto\tof\n\ta\tdog.\nT N \nI N ·T 1 I N ·T 2 I N ·T 3 \nI 1 ·T N \nI 2 ·T N \nI 3 ·T N \n⋮ \n…I N \n…\n⋮ ⋱ \nI N ·T N \nI 1 ·T 3 \nFig. 12. The overview of CILP (from [148]).\n(voice, image, and language), it achieves competitive or better\nresults compared with the existing self-supervised methods.\n2) Visual Grounding: Compared with VLP, visual ground-\ning has more concrete target signal supervision whose ob-\njective is to locate the target objects according to their\ncorresponding descriptions. In the image space, Modulated\nDETR (MDETR) [154] extends its previous work [30] to\nphrase grounding pre-training that locates and assigns the\nbounding box to each instance phrase in one description.\nBased on the proposed combined dataset from many existing\nones, MDETR is ﬁrst pre-trained on the 1.3M aligned text-\nimage pairs for PG and then ﬁne-tuned on other downstream\ntasks. During pre-training, the image-text pair features are\nseparately processed by two speciﬁc extractors, and fed into a\nDETR-like Transformer for salient object localization. Besides\nthe box loss, two auxiliary losses are adopted to enforce\nnetwork to model an alignment between image feature and\ntheir corresponding phrase tokens. With the large-scale image-\ntext pairs pre-training, MDETR can be easily generalized\nin few-shot learning, even on long-tail data. Different from\nMDETR [154] adding two auxiliary losses for box-phrase\nalignments, Referring Transformer [157] directly initializes\nobject queries with phrase-speciﬁc embeddings for PG, which\nexplicitly reserves an one-to-one phrase assignment for ﬁnal\nbounding box prediction. VGTR [156] reformulates the REC\nas a task for single salient object localization from the lan-\nguage features. In detail, a text-guided attention mechanism\nencapsulates both self-attention block and text-image cross-\nattention one to update the image features simultaneously. The\nresulted image features, which serve as the key-value pairs,\ninteract with language queries when regressing bounding box\ncoordinates in the decoder. Following ViT [29], TransVG [155]\nkeeps the class token to aggregate the image and language\nfeatures simultaneously for the mentioned object localization\nin REC. Pseudo-Q [158] focuses on REC for the unsupervised\nlearning, where a pseudo-query generation module based on\na pre-trained detector and a series of attributes&relationship\ngeneration algorithm is applied to generate a set of pseudo\nphrase descriptions, and a query prompt is introduced to match\nfeature proposals and phrase queries for REC adaptation.\nIn the 3D spaces, LanguageRefer [159] redeﬁnes the multi-\nstream data reasoning as a language modeling problem, whose\ncore idea is to omit point cloud features and infuse the\npredicted class embeddings together with a caption into a\nlanguage model to get a binary prediction for object selection.\nFollowing the conventional two-stream methods, TransRe-\nfer3D [160] further enhances the relationship of the object\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\nfeatures by using a cross-attention between asymmetric object\nrelation maps and linguistic features. Considering the speciﬁc\nview for varied descriptions, Huang et al. present a Multi-\nView Transformer (MVT 2022) [161] for 3D visual grounding.\nGiven a shared point cloud feature for each object, MVT ﬁrst\nappends the converted bounding box coordinates to the shared\nobjects in order to get speciﬁc view features. These multi-\nview features are then fed into a stack of the Transformer\ndecoders for text data fusion. Finally, the multi-view features\nare merged by an order-independent aggregation function and\nconverted to the grounding score. MVT achieves the SoTA\nperformance on Nr3D and Sr3D datasets [206]. In the video\nspace, a speciﬁc 3D data (with temporal dimension), Yang et\nal. propose TubeDETR [162] to address the problem of Spatio-\nTemporal Video Grounding (STVG). Concretely, a slow-fast\nencoder sparsely samples the frames and performs cross-\nmodal self-attention between the sampled frames and the text\nfeatures in the slow branch, and aggregates the updated sample\nfeatures into the full-frame features from fast branch via a\nbroadcast operation. A learnable query attached with different\ntime encodings, called time-speciﬁc queries in the decoder is\nthen predicted as either a time-aligned bounding box or “no\nobject”. It attains SoTA results on STVG leaderboards.\nVIII. D ISCUSSION AND CONCLUSION\nThis section brieﬂy conducts a summary of the performance\nimprovements provided in Sec. VIII-A, some critical issues\ndiscussed in Sec. VIII-B, future research directions suggested\nin Sec. VIII-C, and ﬁnal conclusion given in Sec. VIII-D.\nA. Summary of Recent Improvements\nWe brieﬂy summarize the major performance improvements\nfor three fundamental CV tasks as follows.\n(1) For classiﬁcation, a deep hierarchical Transformer back-\nbone is valid for decreasing the computational complexity [42]\nand avoiding the feature over-smooth [38], [43], [67], [68] in\nthe deep layer. Meanwhile, the early-stage convolution [40]\nis enough to capture the low-level features, which can sig-\nniﬁcantly enhance the robustness and reduce the computa-\ntional complexity in the shallow layer. Moreover, both the\nconvolutional projection [55], [56] and the local attention\nmechanism [36], [45] can improve the locality of the visual\nTransformers. The former [57], [58] may also be a new\napproach to replace the positional encoding.\n(2) For detection, the Transformer necks beneﬁt from the\nencoder-decoder structure with less computation than the\nencoder-only Transformer detector [89]. Thus, the decoder\nis necessary but it requires more spatial prior [77], [81]–\n[84], [86], [87] owing to its slow convergence [88]. Further-\nmore, sparse attention [77] and scoring network [79], [80]\nfor fore-grounding sampling are conducive to reducing the\ncomputational costs and accelerating the convergence of visual\nTransformers.\n(3) For segmentation, the encoder-decoder Transformer\nmodels may unify three segmentation sub-tasks into a mask\nprediction problem via a set of learnable mask embed-\ndings [31], [105], [199]. This box-free approach has achieved\nthe latest SoTA performance on multiple benchmarks [199].\nMoreover, the speciﬁc hybrid task is cascaded with the\nmodel [102] of the box-based visual Transformers, which have\ndemonstrated a higher performance for instance segmentation.\n(4) For 3D visual recognition, the local hierarchical Trans-\nformer with a scoring network could efﬁciently extract features\nfrom the point clouds. Instead of the elaborate local design, the\nglobal modeling capability enables the Transformer to easily\naggregate surface points. In addition, the visual Transformers\ncan handle multi-sensory data in 3D visual recognition, such\nas multi-view and multi-dimension data.\n(5) The mainstream approaches of visual-linguistic pre-\ntraining has gradually abandoned the pre-trained detector [146]\nand focused on the alignments [148] or similarities [153]\namong different data streams in the latent space based on\nthe large-scale noised datasets [150]. Another concern is to\nadapt the downstream visual tasks to the pre-training scheme\nto perform zero-short transferring [148].\n(6) The recent prevailing architecture for multi-sensory data\nstream fusion is the single-stream method, which spatially\nconcatenates different data streams and performs interaction\nsimultaneously. Based on the single-stream model, numerous\nrecent works devote to ﬁnding a latent space to make different\ndata streams semantically consistent.\nB. Discussion on Visual Transformers\nDespite that the visual Transformer models are evolved sig-\nniﬁcantly, the “essential” understanding remains insufﬁcient.\nTherefore, we will focus on reviewing some key issues for a\ndeep and comprehensive understanding.\n1) How Transformers Bridge the Gap Between Language\nand Vision: Transformers are initially designed for machine\ntranslation tasks [1], where each word of a sentence is taken as\na basic unit representing the high-level semantics. These words\ncan be embedded into a series of vector representations in\nthe N-dimensional feature space. For visual tasks, each single\npixel of an image is unable to carry semantic information,\nwhich is not full compliance with the feature embedding as\ndone for the traditional NLP tasks. Therefore, the key for trans-\nferring such feature embeddings (i.e., word embedding) to im-\nage features and applying Transformer to various vision tasks\nis to build an image-to-vector transformation and maintain\nthe image’s characteristics effectively. For example, ViT [29]\ntransforms an image into patch embeddings with multiple low-\nlevel information under strong slackness conditions. And its\nvotarist [40], [59] leverages convolution to extract the low-\nlevel features and reduce the redundancy from patches.\n2) The Relationship between Transformers, Self-Attention\nand CNNs: From the perspective of CNNs, its inductive bias\nis mainly shown as locality, translation invariance, weight\nsharing, and sparse connection. Such a simple convolutional\nkernel can perform template matching efﬁciently in lower-level\nsemantic processing but its upper-bound tend to be lower than\nTransformers due to the excessive bias.\nFrom the perspective of Transformers, as detailed in\nSec. III-B and Sec. III-D, attention layer can theoretically\nexpress any convolution when a sufﬁcient number of heads\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\nEncoder-Only\nEncoder-\nDecoder\nSingle\nToken\nMultiple\nTokens\nInitial\nToken(s)\nLater\nToken(s)\nPatch Embedding\nLearnable Positional \nEncodings\nClass Token(s)\nLearnable Positional \nEncodingsSeg-\nmenter\nCaiT [43]\n[99]\nYOLOS [89]\nViT [29]\nImplicit \nSpatial Prior\nExplicit \nSpatial Prior\nLearned\nPositional\nEncoding\nLearned\nDecoder\nEmbedding\nConstant Decoder \nInputs\nLearnable Positional \nEncodings\nLearned Decoder \nInputs\nLearnable Positional \nEncodings\nConditional\nDETR [82]\nDeformable\nDETR [77]\nDETR \n[30]\nMask-\nFormer\n[32]\nFig. 13. Taxonomy of the learnable embedding.\nare adopted [28]. Such fully-attentional operation can com-\nbine both local-level and global-level attentions, and generate\nattention weights dynamically according to the feature rela-\ntionships. Dong et al. demonstrate that the self-attention layer\nmanifests strong inductive bias towards “token uniformity”\nwhen it is trained on deep layers without short connection\nor FFNs [169]. Yu et al. also argue that such an elaborate\nattention mechanism can be replaced by a pooling operation\nreadily [207]. Therefore, it is concluded that Transformer must\nconsist of two key components: a global token mixer (e.g., self-\nattention layer) aggregates the relationship of tokens, and a\nposition-wise FFN extracts the features from the inputs .\nBy comparison, the visual Transformer has a powerful\nglobal modelling capability, making it efﬁciently attend to\nhigh-level semantic features. CNNs can effectively process the\nlow-level features [40], [59], enhance the locality of the visual\nTransformers [54], [82], and append the positional features via\npadding operations [57], [58], [174].\n3) Double Edges of Visual Transformers: We conclude\nthree double-edged properties of visual Transformers as fol-\nlows. Global property enables Transformer to acquire ca-\npacious receptive ﬁelds and interact easily between various\nhigh-level semantic features, while it becomes inefﬁciency\nand debility during low-level processing because of quadratic\ncomputing and noised low-level features. Slack bias offers\nvisual Transformer a higher upper bound than CNNs based on\nsufﬁcient training data without sophistic assumptions but per-\nforms inferiority and slow convergency in small datasets [208].\nLow-pass is also a signiﬁcant property of visual Transformer\nshowing excellent robustness, whereas it is insensitive to low-\nlevel features (e.g., complicated textures and edges) compared\nwith CNN. Accordingly, it is concluded that Transformers at\na high-level stage play a vital role in various vision tasks.\n4) Learnable Embeddings for Different Visual Tasks: Vari-\nous learnable embeddings are designed to perform different\nvisual tasks, such as class token, object query, and mask\nembedding. These learnable tokens are mainly adopted into\ntwo different Transformer patterns, i.e., encoder-only and\nencoder-decoder ones, as illustrated in Fig. 13. On the quantity\nlevel, the number of learned tokens depends on the target\nprediction. For example, the visual Transformers [29], [41]\nin the classiﬁcation task adopt only one class token, and\nthe DETR’s votarist in detection [30], [82] and segmenta-\ntion [199] tasks employ multiple learned queries. On the\nposition level, encoder-only Transformers capitalize on the\ninitial token(s) [29], [89] and later token(s) [43], [105], while\nthe learned positional encoding [30], [82], [199] and the\nlearned decoder input embedding [77] are applied to the\nencoder-decoder structure. Different from the vanilla ViT with\ninitial class token, CaiT [43] observes that the later class token\ncan reduce FLOPs of Transformer and improve model per-\nformance slightly. Segmenter [105] also shows such strategy\nefﬁciency for the segmentation tasks. From the viewpoint of\nthe encoder-decoder Transformer, the decoder input token is\nconsidered as a special case of the encoder-only Transformer\nwith later token. It standardizes visual Transformers in the\nﬁelds of detection [30] and segmentation [199] by using a\nsmall set of object queries (mask embeddings). By combing\nboth later tokens and object queries (mask embeddings), the\nstructure like Deformable DETR [77], which takes object\nqueries and the learnable decoder embeddings (equivalent to\nthe later tokens) as the inputs, may unify the learnable embed-\ndings for different tasks into the encoder-decoder Transformer.\nC. Future Research Directions\nVisual Transformers have achieved signiﬁcant progresses\nand obtained promising results. However, some key technolo-\ngies are still insufﬁcient to cope with complicated challenges\nin the CV ﬁelds. Based on the above analysis, we point out\nsome promising research directions for future investigation.\n1) Set Prediction: Touvron et al. found that multiple class\ntokens would converge consistently due to the same gradient\nfrom the loss function [41], whereas it does not emerge in\ndense prediction tasks [30], [199]. We conclude that their\nmarked difference lies in the label assignment and the number\nof targets. Thus, it is natural to consider a set prediction\ndesign for the classiﬁcation tasks, e.g., multiple class tokens\nare aligned to mix-patches via set prediction, like the data\naugmentation training strategy in LV-ViT [44]. Furthermore,\nthe label assignment in the set prediction strategy leads to\ntraining instability during the early process, which degrades\nthe accuracy of the ﬁnal results. Redesigning the label as-\nsignments and set prediction losses may be helpful for the\ndetection frameworks.\n2) Self-Supervised Learning: Self-supervised pre-training\nof Transformers has standardized the NLP ﬁeld and obtained\ntremendous successes in various applications [2], [5]. Because\nof the popularity of self-supervision paradigms in the CV\nﬁeld, the convolutional Siamese networks employ contrastive\nlearning to perform self-supervised pre-training, which differs\nfrom the masked auto-encoders used in the NLP ﬁeld. Re-\ncently, some studies have tried to design self-supervised visual\nTransformers to bridge the discrepancy of pre-training method-\nology between vision and language. Most of them inherit the\nmasked auto-encoders in the NLP ﬁeld or contrastive learning\nschemes in the CV ﬁeld. There is no speciﬁc supervised\nmethod for the visual Transformers, but it has revolutionized\nthe NLP tasks such as GPT-3. As described in Sec. VIII-B4,\nthe encoder-decoder structure may unify the visual tasks by\nlearning the decoder embedding and the positional encoding\njointly. Thus it is worth of further investigating the encoder-\ndecoder Transformers for self-supervised learning.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\nD. Conclusion\nSince ViT demonstrated its effectiveness for the CV tasks,\nthe visual Transformers have received considerable attentions\nand undermined the dominant of CNNs in the CV ﬁeld.\nIn this paper, we have comprehensively reviewed more than\none hundred of visual Transformer models which have been\nsuccessively applied to various vision tasks (i.e., classiﬁca-\ntion, detection, and segmentation) and data streams (e.g.,\nimages, point clouds, image-text pairs, and other multiple data\nstreams). For each vision task and data stream, a speciﬁc\ntaxonomy is proposed to organize the recently-developed vi-\nsual Transformers and their performances are further evaluated\nover various prevailing benchmarks. From our integrative\nanalysis and systematic comparison of all these existing meth-\nods, a summary of remarkable performance improvements is\nprovided in this paper, four essential issues for the visual\nTransformers are also discussed, and several potential research\ndirections are further suggested for future investment. We do\nexpect that this review paper can help readers have better\nunderstandings of various visual Transformers before they\ndecide to perform deep explorations.\nAPPENDIX A\nOVERVIEW OF DEVELOPMENT TREND ON VISUAL\nTRANSFORMERS\nTransformer backbones sprang up within the last year. When\nour systematics matches the timeline of these models, we\ncan clearly trace the development tendency of Transformer\nfor image classiﬁcation (the Fig. 1 in main text). As a type\nof self-attention mechanism, visual Transformers are mainly\nredesigned according to either the vanilla structure in NLP\n(ViT [29] and iGPT [69]) or attention-based model in CV\n(VTs [52] and BoTNet [53]).\nThen, many approaches start to extend the hierarchical or\ndeep structure of CNN to visual Transformer. T2T-ViT [64],\nPVT [42], CvT [37] and PiT [65] share a motivation that\ntransferring the hierarchical structure into Transformer but\nthey perform downsampling differently. CaiT [43], Diverse\nPatch [68], DeepViT [67], and Reﬁner [38] focus on the prob-\nlem in deep Transformer. Moreover, some approaches move\non to the internal components to further enhance the image\nprocessing capability in previous Transformers, i.e., positional\nencoding [57], [209], [210], MHSA [28], and MLP [169].\nThe next wave of Transformers is locality paradigm. Most of\nthem introduce locality into Transformer via introducing local\nattention mechanism [36], [45], [60], [61] or convolution [54]–\n[56]. Nowadays, the most recent supervised Transformers\nare exploring both the structural combination [40], [59] and\nscaling laws [39], [211]. In addition to supervised Transform-\ners, self-supervised learning accounts for a substantial part\nof vision Transformers [69]–[71], [73]–[75]. However, it is\nunclear what tasks and structures are more beneﬁcial to self-\nsupervised Transformer in CV .\nAPPENDIX B\nMORE DETAIL FORMULA OF DETR\nThe bipartite matching loss Lmatch is applied between pre-\ndiction ˆyσ(i) and ground-truth objects yi to identify one-to-one\nDecoder\nObject Queries\nBounding Box Predictions\n× 4\nMulti-scale Feature Maps\nEncoder\nMulti-scale Deformable\nSelf-Attention in Encoder\nMulti-scale Deformable \nCross-Attention in Decoder\nTransformer \nSelf-Attention in Decoder\nImage\nImage Feature Maps\n× 4\nFigure 1: Illustration of the proposed Deformable DETR object detector.\nIn this paper, we propose Deformable DETR, which mitigates the slow convergence and high com-\nplexity issues of DETR. It combines the best of the sparse spatial sampling of deformable convo-\nlution, and the relation modeling capability of Transformers. We propose the deformable attention\nmodule, which attends to a small set of sampling locations as a pre-ﬁlter for prominent key elements\nout of all the feature map pixels. The module can be naturally extended to aggregating multi-scale\nfeatures, without the help of FPN (Lin et al., 2017a). In Deformable DETR , we utilize (multi-scale)\ndeformable attention modules to replace the Transformer attention modules processing feature maps,\nas shown in Fig. 1.\nDeformable DETR opens up possibilities for us to exploit variants of end-to-end object detectors,\nthanks to its fast convergence, and computational and memory efﬁciency. We explore a simple and\neffective iterative bounding box reﬁnement mechanism to improve the detection performance. We\nalso try a two-stage Deformable DETR, where the region proposals are also generated by a vaiant of\nDeformable DETR, which are further fed into the decoder for iterative bounding box reﬁnement.\nExtensive experiments on the COCO (Lin et al., 2014) benchmark demonstrate the effectiveness\nof our approach. Compared with DETR, Deformable DETR can achieve better performance (es-\npecially on small objects) with 10 ×less training epochs. The proposed variant of two-stage De-\nformable DETR can further improve the performance. Code is released at https://github.\ncom/fundamentalvision/Deformable-DETR.\n2 R ELATED WORK\nEfﬁcient Attention Mechanism. Transformers (Vaswani et al., 2017) involve both self-attention\nand cross-attention mechanisms. One of the most well-known concern of Transformers is the high\ntime and memory complexity at vast key element numbers, which hinders model scalability in many\ncases. Recently, many efforts have been made to address this problem (Tay et al., 2020b), which can\nbe roughly divided into three categories in practice.\nThe ﬁrst category is to use pre-deﬁned sparse attention patterns on keys. The most straightforward\nparadigm is restricting the attention pattern to be ﬁxed local windows. Most works (Liu et al., 2018a;\nParmar et al., 2018; Child et al., 2019; Huang et al., 2019; Ho et al., 2019; Hu et al., 2019; Parmar\net al., 2019; Qiu et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020) follow\nthis paradigm. Although restricting the attention pattern to a local neighborhood can decrease the\ncomplexity, it loses global information. To compensate, Child et al. (2019); Huang et al. (2019);\nHo et al. (2019) attend key elements at ﬁxed intervals to signiﬁcantly increase the receptive ﬁeld\n2\nFig. 14. Illustration of Deformable DETR. A ﬁxed number of key samples\nin each scale feature interacting with all queries.(from [77].)\nlabel assignment ˆσ as\nˆσ= argmin\nσ∈SN\nN∑\ni\nLmatch(yi,ˆyσ(i)),\nLmatch(yi,ˆyσ(i)) = −1{ci̸=∅}ˆpσ(i)(ci)\n+1{ci̸=∅}Lbox(bi,ˆbσ(i)).\n(6)\nIn back propagation, the Hungarian loss LH includes a neg-\native log-likelihood loss for all label predictions ( i= 1 ···N)\nand a box loss for all matched pairs ( ci ̸= ∅) as\nLH(yi,ˆyσ(i))=\nN∑\ni=1\n[−logˆpˆσ(i)(ci)+1{ci̸=∅}Lbox(bi,ˆbσ(i))]. (7)\nAPPENDIX C\nMORE DETAILED FORMULA OF DEFORMABLE DETR\nGiven L feature maps Xl ∈ RHl×Wl×C and a query\nsequence z ∈RNq×C, MSDA samples offsets ∆p ∈R2 of\neach query for Nk sample keys at each layer’s head via two\nlinear layers. While it is sampling the features of key points\nVi ∈RL×Nk×Cv , a linear projection layer is applied to the\nquery to generate an attention map Ai ∈RNq×L×Nk for key\nsamples, where Nq and C are query’s length and dimension,\nrespectively (see Fig. 14). The process is formulated as\nAl\nqik= zqWA\nilk, Vl\nik=Xl(φl(ˆpq) + ∆pilqk)WV\ni ,\nMSDAttn(Al\nqik,V l\nik) =\nh∑\ni=1\n(\nL∑\nl=1\nNk∑\nk=1\nAl\nqikVl\nik)WO\ni ,\n(8)\nwhere mdenotes the attention head, WA\nil∈RC×Nk, WV\ni ∈RC×Cv\nand WO\ni ∈RCv×Care linear matrices. ˆpq∈[0,1]2is normalized\ncoordinates of each query.\nAPPENDIX D\nMORE COMPARISON OF VISUAL TRANSFORMERS FOR\nDENSE PREDICTION AND VISUAL -LINGUISTIC\nPRE-TRAINING\nBased on RetinaNet [183] and Mask R-CNN [188], Tab. IV\ncompares several visual Transformer backbones on the COCO\ndatasets for the dense prediction tasks. And Tab. V collects\nthe visual Transformers as mentioned in visual-linguistic pre-\ntraining (Sec. VII-B1). Concretely, we summarize the main\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\nTABLE IV\nDENSE PREDICTION RESULTS OF COCO 2017 VAL. SET BASED ON RETINA NET [183] AND MASK R-CNN [188], WHEN TRAINED WITH 3× SCHEDULE\nAND MULTI -SCALE INPUTS (MS). T HE NUMBERS BEFORE AND AFTER “/” CORRESPOND TO THE PARAMETER OF RETINA NET AND MASK R-CNN,\nRESPECTIVELY . (M OST OF DATA FROM [63].)\nBackbone #Params(M) FLOPs(G)\nRetinaNet 3×schedule + MS Mask R-CNN 3 ×schedule + MS\nAPbox APbox50 APbox75 APboxS APboxM APboxL APbox APbox50 APbox75 APseg APseg\n50 APseg\n75\nResNet50 [11] 38 / 44 239 / 260 39.0 58.4 41.8 22.4 42.8 51.6 41.0 61.7 44.9 37.1 58.4 40.1PVTv1-Small [42] 34 / 44 226 / 245 42.2 62.7 45.0 26.2 45.2 57.2 43.0 65.3 46.9 39.9 62.5 42.8ViL-Small [62] 36 / 45 252 / 174 42.9 63.8 45.6 27.8 46.4 56.3 43.4 64.9 47.0 39.6 62.1 42.4Swin-Tiny [36] 39 / 48 245 / 264 45.0 65.9 48.4 29.7 48.9 58.1 46.0 68.1 50.3 41.6 65.1 44.9PVTv2-B2-Li [66] 32 / 42 - / - - - - - - - 46.8 68.7 51.4 42.3 65.7 45.4Focal-Tiny [63] 39 / 49 265 / 291 45.5 66.3 48.8 31.2 49.2 58.7 47.2 69.4 51.9 42.7 66.5 45.9PVTv2-B2 [66] 35 / 45 - / - - - - - - - 47.8 69.7 52.6 43.1 66.8 46.7\nResNet101 [11] 57 / 63 315 / 336 40.9 60.1 44.0 23.7 45.0 53.8 42.8 63.2 47.1 38.5 60.1 41.3ResNeXt101-32x4d [212] 56 / 63 319 / 340 41.4 61.0 44.3 23.9 45.5 53.7 44.0 64.4 48.0 39.2 61.4 41.9PVTv1-Medium [42] 54 / 64 283 / 302 43.2 63.8 46.1 27.3 46.3 58.9 44.2 66.0 48.2 40.5 63.1 43.5ViL-Medium [62] 51 / 60 339 / 261 43.7 64.6 46.4 27.9 47.1 56.9 44.6 66.3 48.5 40.7 63.8 43.7Swin-Small [36] 60 / 69 335 / 354 46.4 67.0 50.1 31.0 50.1 60.3 48.5 70.2 53.5 43.3 67.3 46.6Focal-Small [63] 62 / 71 367 / 401 47.3 67.8 51.0 31.6 50.9 61.1 48.8 70.5 53.6 43.8 67.7 47.2\nResNeXt101-64x4d [212] 96 / 102 473 / 493 41.8 61.5 44.4 25.2 45.4 54.6 44.4 64.9 48.8 39.7 61.9 42.6PVTv1-Large [42] 71 / 81 345 / 364 43.4 63.6 46.1 26.1 46.0 59.5 44.5 66.0 48.3 40.7 63.4 43.7ViL-Base [62] 67 / 76 443 / 365 44.7 65.5 47.6 29.9 48.0 58.1 45.7 67.2 49.9 41.3 64.4 44.5Swin-Base [36] 98 / 107 477 / 496 45.8 66.4 49.1 29.9 49.4 60.3 48.5 69.8 53.2 43.4 66.8 46.9Focal-Base [63] 101 / 110 514 / 533 46.9 67.8 50.3 31.9 50.3 61.5 49.0 70.1 53.6 43.7 67.6 47.0\nTABLE V\nDETAILS OF VISUAL -LINGUISTIC PRE -TRAINING METHODS , WHERE •AND ••DENOTE SINGLE - AND DUAL -STREAM ARCHITECTURE , RESPECTIVELY ,\nAND THE ZERO -SHOT DENOTES THE METHOD CAN BE ZERO -SHOT TRANSFERED INTO DOWN STREAM TASKS . IN THE PRE -TRAINING TASKS , MRM IS\nMASKED REGION MODELING , OD IS OBJECT DETECTION , SMLM AND BMLM DENOTE BOTH SEQUENTIALLY AND BIDIRECTIONALLY MASKED\nLANGUAGE MODELING , AND MVM IS MASED VISUAL -TOKEN MODELING .\nMethods Arch. Visual Token Pre-training ZeroShot PublictionMain Dataset(s) Data Size Tasks\nRegion-Besed Methods\nVideoBERT [138] • S3D [213]/w k-means YouTube Cooking [138] 312K ITA,MLM, MVM ✓ ICCV 2019\nViLBERT [139] •• RoI [201] CC3M [203] 3.1M ITA,MLM, MRC-KL - NeurIPS 2019\nLXMERT [140] •• RoI [201] VG-QAVQAv2 [214], VG [202],COCO [215], GQA [216] 9.2M ITA, MLM,MRM, MRC - IJCNLP 2019\nVisualBERT [141] • Faster RCNN [181] COCO [215] 0.9M ITA, MLM - Arxiv 2019\nVL-BERT [142] • RoI [201] CC3M [203], BooksCorpus&English Wikipedia 11M MLM, MRC - ICLR 2020\nUNITER [143] • RoI [201] CC3M [203], SBU [217],COCO [215], VG [202] 9.5M ITA, WRA,MLM/MRM - ECCV 2020\nOscar [144] • RoI [201]+Tags\nCOCO [215], GQA [216],CC3M [203], SBU [217],VG [202], Fliker30K [218] 11.4M ITA, MLM - ECCV 2020\nUniﬁed-VLP [145] • RoI [201] CC3M [203] 3.1M SMLM, BMLM - AAAI 2020\nVinVL(Oscar+) [147] • RoI [201]/w NMS+Tags\nSBU [217], VG-Qas [202],COCO [215], CC3M [203],GQA [216], Fliker30K [218],VQA [204], OpenImages [219]\n8.9M MLM, ITA - CVPR 2021\nFeature-Based Methods\nViLT [146] • Patches from ViT [29] SBU [217], CC3M [203],COCO [215], VG [202] 10M ITM, MLM ✓ ICML 2021\nUniT [151] • DETR-ResNet50 [30] COCO [215], VG [202],VQAv2 [214], SNLI-VEFour LM Datasets - OD, 4LM,2ILM ✓ ICCV 2021\nCLIP [148] •• ViT [29] Internet Pairs [148] 400M Contrasive ✓ ICML 2021\nDALL-E [149] • dV AE Extension [149]from COCO 250M Contrasive ✓ ICML 2021\nALIGN [150] • EfﬁcientNet [12] Noise Englishal-text data [150] 1.8B Contrasive ✓ ICML 2021\nSimVLM [152] • CoAtNet [40] Noise Englishal-text data [150] 1.8B PLM ✓ ICLR 2022\nData2Vec [153] • ViT [29]\nImageNet-1kLS-960Books Corpus &English Wikipedia data\n1k960h1M Self-Distillation ✓ Arxiv 2022\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\ndatasets, data size, and objective task for each visual Trans-\nformer during the pre-training. The architecture (single-/dual-\nstream) and the type of visual token inputs are also organized\nfor each approach.\nREFERENCES\n[1] A. Vaswani et al. Attention is all you need. In NeurIPS, pp. 5998–6008,\n2017.\n[2] A. Radford et al. Improving language understanding by generative\npre-training. 2018.\n[3] A. Radford et al. Language models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[4] T. B. Brown et al. Language models are few-shot learners. In NeurIPS,\npp. 1877–1901, 2020.\n[5] J. Devlin et al. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. In NAACL, pp. 4171–4186, 2018.\n[6] Y . Liu et al. Roberta: A robustly optimized bert pretraining approach.\narXiv:1907.11692, 2019.\n[7] Z. Lan et al. Albert: A lite bert for self-supervised learning of language\nrepresentations. In ICLR, 2020.\n[8] Z. Yang et al. Xlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS, pp. 5753–5763, 2019.\n[9] D. W. Otter et al. A survey of the usages of deep learning for\nnatural language processing. IEEE Trans. Neural Netw. Learn. Syst. ,\n32(2):604–624, 2020.\n[10] A. Krizhevsky et al. Imagenet classiﬁcation with deep convolutional\nneural networks. In NeurIPS, pp. 1097–1105, 2012.\n[11] K. He et al. Deep residual learning for image recognition. In CVPR,\npp. 770–778, 2016.\n[12] M. Tan and Q. Le. Efﬁcientnet: Rethinking model scaling for\nconvolutional neural networks. In ICML, pp. 6105–6114, 2019.\n[13] A. Galassi et al. Attention in natural language processing. IEEE Trans.\nNeural Netw. Learn. Syst. , 32(10):4291–4308, 2020.\n[14] X. Wang et al. Non-local neural networks. In CVPR, pp. 7794–7803,\n2018.\n[15] Z. Huang et al. Ccnet: Criss-cross attention for semantic segmentation.\nIn ICCV, pp. 603–612, 2019.\n[16] Y . Cao et al. Gcnet: Non-local networks meet squeeze-excitation\nnetworks and beyond. In ICCV Workshop, pp. 1971–1980, 2019.\n[17] J. Hu et al. Squeeze-and-excitation networks. In CVPR, pp. 7132–7141,\n2018.\n[18] S. Woo et al. Cbam: Convolutional block attention module. In ECCV,\npp. 3–19, 2018.\n[19] Q. Wang et al. Eca-net: Efﬁcient channel attention for deep convolu-\ntional neural networks. In CVPR, pp. 11534–11542, 2020.\n[20] N. Parmar et al. Image transformer. In ICML, pp. 4055–4064, 2018.\n[21] H. Hu et al. Relation networks for object detection. In CVPR, pp.\n3588–3597, 2018.\n[22] H. Hu et al. Local relation networks for image recognition. In ICCV,\npp. 3464–3473, 2019.\n[23] I. Bello et al. Attention augmented convolutional networks. In ICCV,\npp. 3286–3295, 2019.\n[24] P. Ramachandran et al. Stand-alone self-attention in vision models. In\nNeurIPS, pp. 68–80, 2019.\n[25] H. Zhao et al. Exploring self-attention for image recognition. In CVPR,\npp. 10076–10085, 2020.\n[26] Z. Zheng et al. Global and local knowledge-aware attention network for\naction recognition. IEEE Trans. Neural Netw. Learn. Syst. , 32(1):334–\n347, 2020.\n[27] A. Vaswani et al. Scaling local self-attention for parameter efﬁcient\nvisual backbones. In CVPR, pp. 12894–12904, 2021.\n[28] J.-B. Cordonnier et al. On the relationship between self-attention and\nconvolutional layers. In ICLR, 2020.\n[29] A. Dosovitskiy et al. An image is worth 16x16 words: Transformers\nfor image recognition at scale. In ICLR, 2021.\n[30] N. Carion et al. End-to-end object detection with transformers. In\nECCV, pp. 213–229, 2020.\n[31] H. Wang et al. Max-deeplab: End-to-end panoptic segmentation with\nmask transformers. In CVPR, pp. 5463–5474, 2021.\n[32] B. Cheng et al. Per-pixel classiﬁcation is not all you need for semantic\nsegmentation. In NeurIPS, pp. 17864–17875, 2021.\n[33] X. Chen et al. Transformer tracking. In CVPR, pp. 8126–8135, 2021.\n[34] Y . Jiang et al. Transgan: Two pure transformers can make one strong\ngan, and that can scale up. arXiv:2102.07074, 2021.\n[35] H. Chen et al. Pre-trained image processing transformer. In CVPR, pp.\n12299–12310, 2021.\n[36] Z. Liu et al. Swin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV, pp. 10012–10022, 2021.\n[37] H. Wu et al. Cvt: Introducing convolutions to vision transformers. In\nICCV, pp. 22–31, 2021.\n[38] D. Zhou et al. Reﬁner: Reﬁning self-attention for vision transformers.\narXiv:2106.03714, 2021.\n[39] X. Zhai et al. Scaling vision transformers. arXiv:2106.04560, 2021.\n[40] Z. Dai et al. Coatnet: Marrying convolution and attention for all data\nsizes. In NeurIPS, pp. 3965–3977, 2021.\n[41] H. Touvron et al. Training data-efﬁcient image transformers &\ndistillation through attention. In ICLR, pp. 10347–10357, 2021.\n[42] W. Wang et al. Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. In ICCV, pp. 568–578, 2021.\n[43] H. Touvron et al. Going deeper with image transformers. In ICCV,\npp. 32–42, 2021.\n[44] Z.-H. Jiang et al. All tokens matter: Token labeling for training better\nvision transformers. In NeurIPS, pp. 18590–18602, 2021.\n[45] L. Yuan et al. V olo: Vision outlooker for visual recognition.\narXiv:2106.13112, 2021.\n[46] Y . Tay et al. Efﬁcient transformers: A survey. ACM Comput. Surv.\n(CSUR), 2020.\n[47] S. Khan et al. Transformers in vision: A survey. ACM Comput. Surv.\n(CSUR), 2021.\n[48] K. Han et al. A survey on vision transformer. IEEE Trans. Pattern\nAnal. Mach. Intell. , 2022.\n[49] T. Lin et al. A survey of transformers. AI Open, 2022.\n[50] I. Sutskever et al. Sequence to sequence learning with neural networks.\nIn NeurIPS, pp. 3104–3112, 2014.\n[51] K. Greff et al. Lstm: A search space odyssey. IEEE Trans. Neural\nNetw. Learn. Syst. , 28(10):2222–2232, 2016.\n[52] B. Wu et al. Visual transformers: Token-based image representation\nand processing for computer vision. arXiv:2006.03677, 2020.\n[53] A. Srinivas et al. Bottleneck transformers for visual recognition. In\nCVPR, pp. 16519–16529, 2021.\n[54] S. d’Ascoli et al. Convit: Improving vision transformers with soft\nconvolutional inductive biases. In ICLR, pp. 2286–2296, 2021.\n[55] K. Yuan et al. Incorporating convolution designs into visual transform-\ners. In ICCV, pp. 579–588, 2021.\n[56] Y . Li et al. Localvit: Bringing locality to vision transformers.\narXiv:2104.05707, 2021.\n[57] X. Chu et al. Conditional positional encodings for vision transformers.\narXiv:2102.10882, 2021.\n[58] Q. Zhang and Y .-B. Yang. Rest: An efﬁcient transformer for visual\nrecognition. In NeurIPS, pp. 15475–15485, 2021.\n[59] T. Xiao et al. Early convolutions help transformers see better. In\nNeruIPS, pp. 30392–30400, 2021.\n[60] K. Han et al. Transformer in transformer. In NeurIPS, pp. 15908–\n15919, 2021.\n[61] X. Chu et al. Twins: Revisiting the design of spatial attention in vision\ntransformers. In NeurIPS, pp. 9355–9366, 2021.\n[62] P. Zhang et al. Multi-scale vision longformer: A new vision transformer\nfor high-resolution image encoding. In ICCV, pp. 2998–3008, 2021.\n[63] J. Yang et al. Focal self-attention for local-global interactions in vision\ntransformers. arXiv:2107.00641, 2021.\n[64] L. Yuan et al. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. In ICCV, pp. 558–567, 2021.\n[65] B. Heo et al. Rethinking spatial dimensions of vision transformers. In\nICCV, pp. 11936–11945, 2021.\n[66] W. Wang et al. Pvtv2: Improved baselines with pyramid vision\ntransformer. arXiv:2106.13797, 2021.\n[67] D. Zhou et al. Deepvit: Towards deeper vision transformer.\narXiv:2103.11886, 2021.\n[68] C. Gong et al. Vision transformers with patch diversiﬁcation.\narXiv:2104.12753, 2021.\n[69] M. Chen et al. Generative pretraining from pixels. In ICML, pp.\n1691–1703, 2020.\n[70] Z. Li et al. Mst: Masked self-supervised transformer for visual\nrepresentation. In NeurIPS, pp. 13165–13176, 2021.\n[71] H. Bao et al. Beit: Bert pre-training of image transformers. In ICLR,\n2021.\n[72] K. He et al. Masked autoencoders are scalable vision learners. In\nCVPR, pp. 16000–16009, 2022.\n[73] X. Chen et al. An empirical study of training self-supervised vision\ntransformers. In ICCV, pp. 9640–9649, 2021.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 22\n[74] M. Caron et al. Emerging properties in self-supervised vision trans-\nformers. In ICCV, pp. 9650–9660, 2021.\n[75] Z. Xie et al. Self-supervised learning with swin transformers.\narXiv:2105.04553, 2021.\n[76] T. Chen et al. Pix2seq: A language modeling framework for object\ndetection. In ICLR, 2021.\n[77] X. Zhu et al. Deformable detr: Deformable transformers for end-to-end\nobject detection. In ICLR, 2021.\n[78] M. Zheng et al. End-to-end object detection with adaptive clustering\ntransformer. arXiv:2011.09315, 2020.\n[79] T. Wang et al. Pnp-detr: towards efﬁcient visual analysis with\ntransformers. In ICCV, pp. 4661–4670, 2021.\n[80] B. Roh et al. Sparse detr: Efﬁcient end-to-end object detection with\nlearnable sparsity. In ICLR, 2021.\n[81] P. Gao et al. Fast convergence of detr with spatially modulated co-\nattention. In ICCV, pp. 3621–3630, 2021.\n[82] D. Meng et al. Conditional detr for fast training convergence. In ICCV,\npp. 3651–3660, 2021.\n[83] Y . Wang et al. Anchor detr: Query design for transformer-based\ndetector. In AAAI, pp. 2567–2575, 2022.\n[84] S. Liu et al. Dab-detr: Dynamic anchor boxes are better queries for\ndetr. In ICLR, 2021.\n[85] Y . Liu et al. Sap-detr: Bridging the gap between salient points\nand queries-based transformer detector for fast model convergency.\narXiv:2211.02006, 2022.\n[86] Z. Yao et al. Efﬁcient detr: Improving end-to-end object detector with\ndense prior. arXiv:2104.01318, 2021.\n[87] X. Dai et al. Dynamic detr: End-to-end object detection with dynamic\nattention. In ICCV, pp. 2988–2997, 2021.\n[88] Z. Sun et al. Rethinking transformer-based set prediction for object\ndetection. In ICCV, pp. 3611–3620, 2021.\n[89] Y . Fang et al. You only look at one sequence: Rethinking transformer in\nvision through object detection. In NeurIPS, pp. 26183–26197, 2021.\n[90] Z. Dai et al. Up-detr: Unsupervised pre-training for object detection\nwith transformers. In CVPR, pp. 1601–1610, 2021.\n[91] W. Wang et al. Fp-detr: Detection transformer advanced by fully pre-\ntraining. In ICLR, 2021.\n[92] F. Li et al. Dn-detr: Accelerate detr training by introducing query\ndenoising. In CVPR, pp. 13619–13627, 2022.\n[93] H. Zhang et al. Dino: Detr with improved denoising anchor boxes for\nend-to-end object detection. arXiv:2203.03605, 2022.\n[94] D. Zhang et al. Feature pyramid transformer. In ECCV, pp. 323–339,\n2020.\n[95] Y . Yuan et al. Hrformer: High-resolution vision transformer for dense\npredict. NeurIPS, pp. 7281–7293, 2021.\n[96] J. Gu et al. Hrvit: Multi-scale high-resolution vision transformer.\narXiv:2111.01236, 2021.\n[97] S. Zheng et al. Rethinking semantic segmentation from a sequence-\nto-sequence perspective with transformers. In CVPR, pp. 6881–6890,\n2021.\n[98] J. Chen et al. Transunet: Transformers make strong encoders for\nmedical image segmentation. arXiv:2102.04306, 2021.\n[99] E. Xie et al. Segformer: Simple and efﬁcient design for semantic\nsegmentation with transformers. In NeurIPS, pp. 12077–12090, 2021.\n[100] T. Prangemeier et al. Attention-based transformers for instance seg-\nmentation of cells in microstructures. In BIBM, pp. 700–707, 2020.\n[101] Y . Wang et al. End-to-end video instance segmentation with transform-\ners. In CVPR, pp. 8741–8750, 2021.\n[102] Y . Fang et al. Instances as queries. In ICCV, pp. 6910–6919, 2021.\n[103] J. Hu et al. Istr: End-to-end instance segmentation with transformers.\narXiv:2105.00637, 2021.\n[104] B. Dong et al. Solq: Segmenting objects by learning queries. In\nNeurIPS, pp. 21898–21909, 2021.\n[105] R. Strudel et al. Segmenter: Transformer for semantic segmentation.\nIn ICCV, pp. 7262–7272, 2021.\n[106] H. Zhao et al. Point transformer. In ICCV, pp. 16259–16268, 2021.\n[107] M.-H. Guo et al. Pct: Point cloud transformer. Comput. Vis. Med. ,\n7(2):187–199, 2021.\n[108] D. Lu et al. 3dctn: 3d convolution-transformer network for point cloud\nclassiﬁcation. IEEE Trans. Intell. Transp. Syst. , 2022.\n[109] C. Park et al. Fast point transformer. In CVPR, pp. 16949–16958,\n2022.\n[110] X. Pan et al. 3d object detection with pointformer. In CVPR, pp.\n7463–7472, 2021.\n[111] L. Fan et al. Embracing single stride 3d object detector with sparse\ntransformer. In CVPR, pp. 8458–8468, 2022.\n[112] J. Mao et al. V oxel transformer for 3d object detection. In ICCV, pp.\n3144–3153, 2021.\n[113] C. He et al. V oxel set transformer: A set-to-set approach to 3d object\ndetection from point clouds. In CVPR, pp. 8417–8427, 2022.\n[114] X. Yu et al. Point-bert: Pre-training 3d point cloud transformers with\nmasked point modeling. In CVPR, pp. 19313–19322, 2022.\n[115] Y . Pang et al. Masked autoencoders for point cloud self-supervised\nlearning. arXiv:2203.06604, 2022.\n[116] H. Liu et al. Masked discrimination for self-supervised learning on\npoint clouds. arXiv:2203.11183, 2022.\n[117] I. Misra et al. An end-to-end transformer model for 3d object detection.\nIn ICCV, pp. 2906–2917, 2021.\n[118] Z. Liu et al. Group-free 3d object detection via transformers. In ICCV,\npp. 2949–2958, 2021.\n[119] H. Sheng et al. Improving 3d object detection with channel-wise\ntransformer. In ICCV, pp. 2743–2752, 2021.\n[120] K.-C. Huang et al. Monodtr: Monocular 3d object detection with depth-\naware transformer. In CVPR, pp. 4012–4021, 2022.\n[121] R. Zhang et al. Monodetr: Depth-aware transformer for monocular 3d\nobject detection. arXiv:2203.13310, 2022.\n[122] Y . Wang et al. Detr3d: 3d object detection from multi-view images via\n3d-to-2d queries. In CoRL, pp. 180–191, 2022.\n[123] X. Bai et al. Transfusion: Robust lidar-camera fusion for 3d object\ndetection with transformers. In CVPR, pp. 1090–1099, 2022.\n[124] X. Yu et al. Pointr: Diverse point cloud completion with geometry-\naware transformers. In ICCV, pp. 12478–12487, 2021.\n[125] P. Xiang et al. Snowﬂakenet: Point cloud completion by snowﬂake\npoint deconvolution with skip-transformer. In ICCV, pp. 5479–5489,\n2021.\n[126] J. H. Choe et al. Deep point cloud reconstruction. arXiv:2111.11704,\n2021.\n[127] S. Chen et al. Mvt: Multi-view vision transformer for 3d object\nrecognition. arXiv:2110.13083, 2021.\n[128] Y . Hou and L. Zheng. Multiview detection with shadow transformer\n(and view-coherent data augmentation). In ACMM, pp. 1673–1682,\n2021.\n[129] A. Prakash et al. Multi-modal fusion transformer for end-to-end\nautonomous driving. In CVPR, pp. 7077–7087, 2021.\n[130] W. Jiang et al. Cotr: Correspondence transformer for matching across\nimages. In ICCV, pp. 6207–6217, 2021.\n[131] D. Wang et al. Multi-view 3d reconstruction with transformers. In\nICCV, pp. 5722–5731, 2021.\n[132] X. Chen et al. Futr3d: A uniﬁed sensor fusion framework for 3d\ndetection. arXiv:2203.10642, 2022.\n[133] A. Bozic et al. Transformerfusion: Monocular rgb scene reconstruction\nusing transformers. In NeurIPS, pp. 1403–1414, 2021.\n[134] Y . Zhang et al. mmformer: Multimodal medical transformer for\nincomplete multimodal learning of brain tumor segmentation. In\nMICCAI, pp. 107–117, 2022.\n[135] G. v. Tulder et al. Multi-view analysis of unregistered medical images\nusing cross-view transformers. In MICCAI, pp. 104–113, 2021.\n[136] X. Long et al. Multi-view depth estimation using epipolar spatio-\ntemporal networks. In CVPR, pp. 8258–8267, 2021.\n[137] D. Song et al. Deep relation transformer for diagnosing glaucoma with\noptical coherence tomography and visual ﬁeld function. IEEE Trans.\nMed. Imaging, 40(9):2392–2402, 2021.\n[138] C. Sun et al. Videobert: A joint model for video and language\nrepresentation learning. In ICCV, pp. 7464–7473, 2019.\n[139] J. Lu et al. Vilbert: Pretraining task-agnostic visiolinguistic represen-\ntations for vision-and-language tasks. In NeurIPS, pp. 13–23, 2019.\n[140] H. Tan and M. Bansal. Lxmert: Learning cross-modality encoder\nrepresentations from transformers. In EMNLP-IJCNLP, pp. 5100–5111,\n2019.\n[141] L. H. Li et al. Visualbert: A simple and performant baseline for vision\nand language. arXiv:1908.03557, 2019.\n[142] W. Su et al. Vl-bert: Pre-training of generic visual-linguistic represen-\ntations. arXiv:1908.08530, 2019.\n[143] Y .-C. Chen et al. Uniter: Universal image-text representation learning.\nIn ECCV, pp. 104–120, 2020.\n[144] X. Li et al. Oscar: Object-semantics aligned pre-training for vision-\nlanguage tasks. In ECCV, pp. 121–137, 2020.\n[145] L. Zhou et al. Uniﬁed vision-language pre-training for image caption-\ning and vqa. In AAAI, pp. 13041–13049, 2020.\n[146] W. Kim et al. Vilt: Vision-and-language transformer without convolu-\ntion or region supervision. In ICML, pp. 5583–5594, 2021.\n[147] P. Zhang et al. Vinvl: Revisiting visual representations in vision-\nlanguage models. In CVPR, pp. 5579–5588, 2021.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 23\n[148] A. Radford et al. Learning transferable visual models from natural\nlanguage supervision. In ICML, pp. 8748–8763, 2021.\n[149] A. Ramesh et al. Zero-shot text-to-image generation. In ICML, pp.\n8821–8831, 2021.\n[150] C. Jia et al. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, pp. 4904–4916, 2021.\n[151] R. Hu and A. Singh. Unit: Multimodal multitask learning with a uniﬁed\ntransformer. In ICCV, pp. 1439–1449, 2021.\n[152] Z. Wang et al. Simvlm: Simple visual language model pretraining with\nweak supervision. In ICLR, 2021.\n[153] A. Baevski et al. Data2vec: A general framework for self-supervised\nlearning in speech, vision and language. arXiv:2202.03555, 2022.\n[154] A. Kamath et al. Mdetr-modulated detection for end-to-end multi-\nmodal understanding. In ICCV, pp. 1780–1790, 2021.\n[155] J. Deng et al. Transvg: End-to-end visual grounding with transformers.\nIn ICCV, pp. 1769–1779, 2021.\n[156] Y . Du et al. Visual grounding with transformers. In ICME, pp. 1–6,\n2022.\n[157] M. Li and L. Sigal. Referring transformer: A one-step approach to\nmulti-task visual grounding. In NeurIPS, pp. 19652–19664, 2021.\n[158] H. Jiang et al. Pseudo-q: Generating pseudo language queries for visual\ngrounding. In CVPR, pp. 15513–15523, 2022.\n[159] J. Roh et al. Languagerefer: Spatial-language model for 3d visual\ngrounding. In CoRL, pp. 1046–1056, 2022.\n[160] D. He et al. Transrefer3d: Entity-and-relation aware transformer for\nﬁne-grained 3d visual grounding. In ACMM, pp. 2344–2352, 2021.\n[161] S. Huang et al. Multi-view transformer for 3d visual grounding. In\nCVPR, pp. 15524–15533, 2022.\n[162] A. Yang et al. Tubedetr: Spatio-temporal video grounding with\ntransformers. In CVPR, pp. 16442–16453, 2022.\n[163] J. L. Ba et al. Layer normalization. arXiv:1607.06450, 2016.\n[164] P. P. Brahma et al. Why deep learning works: A manifold disentangle-\nment perspective. IEEE Trans. Neural Netw. Learn. Syst., 27(10):1997–\n2008, 2015.\n[165] Y . Chen et al. a2-nets: Double attention networks. In NeurIPS, pp.\n352–361, 2018.\n[166] A. Krizhevsky et al. Learning multiple layers of features from tiny\nimages. 2009.\n[167] C. Sun et al. Revisiting unreasonable effectiveness of data in deep\nlearning era. In ICCV, pp. 843–852, 2017.\n[168] J. Deng et al. Imagenet: A large-scale hierarchical image database. In\nCVPR, pp. 248–255, 2009.\n[169] Y . Dong et al. Attention is not all you need: pure attention loses rank\ndoubly exponentially with depth. In ICLR, pp. 2793–2803, 2021.\n[170] P. Shaw et al. Self-attention with relative position representations. In\nNAACL, pp. 464–468, 2018.\n[171] P. W. Battaglia et al. Relational inductive biases, deep learning, and\ngraph networks. arXiv:1806.01261, 2018.\n[172] S. Abnar et al. Transferring inductive biases through knowledge\ndistillation. arXiv:2006.00555, 2020.\n[173] M. Sandler et al. Mobilenetv2: Inverted residuals and linear bottle-\nnecks. In CVPR, pp. 4510–4520, 2018.\n[174] A. Islam et al. How much position information do convolutional neural\nnetworks encode. In ICLR, 2020.\n[175] J. Lin et al. Tsm: Temporal shift module for efﬁcient video under-\nstanding. In ICCV, pp. 7083–7093, 2019.\n[176] Y . Pang et al. Convolution in convolution for network in network.\nIEEE Trans. Neural Netw. Learn. Syst. , 29(5):1587–1597, 2017.\n[177] J. Gao et al. Representation degeneration problem in training natural\nlanguage generation models. In ICLR, 2019.\n[178] S. Yun et al. Cutmix: Regularization strategy to train strong classiﬁers\nwith localizable features. In ICCV, pp. 6023–6032, 2019.\n[179] M. Caron et al. Unsupervised learning of visual features by contrasting\ncluster assignments. In NeurIPS, pp. 9912–9924, 2020.\n[180] C.-F. Chen et al. Crossvit: Cross-attention multi-scale vision trans-\nformer for image classiﬁcation. In ICCV, pp. 357–366, 2021.\n[181] S. Ren et al. Faster r-cnn: Towards real-time object detection with\nregion proposal networks. IEEE Trans. Pattern Anal. Mach. Intell. ,\n39(6):1137–1149, 2017.\n[182] J. Redmon et al. You only look once: Uniﬁed, real-time object\ndetection. In CVPR, pp. 779–788, 2016.\n[183] T.-Y . Lin et al. Focal loss for dense object detection. In ICCV, pp.\n2980–2988, 2017.\n[184] Z.-Q. Zhao et al. Object detection with deep learning: A review. IEEE\nTrans. Neural Netw. Learn. Syst. , 30(11):3212–3232, 2019.\n[185] J. Dai et al. Deformable convolutional networks. In ICCV, pp. 764–\n773, 2017.\n[186] T.-Y . Lin et al. Feature pyramid networks for object detection. In\nCVPR, pp. 2117–2125, 2017.\n[187] Z. Tian et al. Fcos: Fully convolutional one-stage object detection. In\nICCV, pp. 9627–9636, 2019.\n[188] K. He et al. Mask r-cnn. In ICCV, pp. 2961–2969, 2017.\n[189] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality\nobject detection. In CVPR, pp. 6154–6162, 2018.\n[190] P. Sun et al. What makes for end-to-end object detection? In ICML,\npp. 9934–9944, 2021.\n[191] K. Sun et al. Deep high-resolution representation learning for human\npose estimation. In CVPR, pp. 5693–5703, 2019.\n[192] L.-C. Chen et al. Deeplab: Semantic image segmentation with deep\nconvolutional nets, atrous convolution, and fully connected crfs. IEEE\nTrans. Pattern Anal. Mach. Intell. , 40(4):834–848, 2017.\n[193] O. Ronneberger et al. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI, pp. 234–241, 2015.\n[194] P. Sun et al. Sparse r-cnn: End-to-end object detection with learnable\nproposals. In CVPR, pp. 14454–14463, 2021.\n[195] T. Xiao et al. Uniﬁed perceptual parsing for scene understanding. In\nECCV, pp. 418–434, 2018.\n[196] M. Chen et al. Searching the search space of vision transformer. In\nNeurIPS, 2021.\n[197] H. Chen et al. Blendmask: Top-down meets bottom-up for instance\nsegmentation. In CVPR, pp. 8573–8581, 2020.\n[198] X. Wang et al. Solov2: Dynamic and fast instance segmentation. In\nNeurIPS, pp. 17721–17732, 2020.\n[199] Y . Wang et al. Not all images are worth 16x16 words: Dynamic\ntransformers for efﬁcient image recognition. In NeurIPS, pp. 11960–\n11973, 2021.\n[200] C. R. Qi et al. Pointnet++: Deep hierarchical feature learning on point\nsets in a metric space. In NeurIPS, 2017.\n[201] P. Anderson et al. Bottom-up and top-down attention for image\ncaptioning and visual question answering. In CVPR, pp. 6077–6086,\n2018.\n[202] R. Krishna et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. Int. J. Comput. Vis. ,\n123(1):32–73, 2017.\n[203] P. Sharma et al. Conceptual captions: A cleaned, hypernymed, image\nalt-text dataset for automatic image captioning. In ACL, pp. 2556–2565,\n2018.\n[204] S. Antol et al. Vqa: Visual question answering. In ICCV, pp. 2425–\n2433, 2015.\n[205] O. Vinyals et al. Show and tell: A neural image caption generator. In\nCVPR, pp. 3156–3164, 2015.\n[206] P. Achlioptas et al. Referit3d: Neural listeners for ﬁne-grained 3d object\nidentiﬁcation in real-world scenes. In ECCV, pp. 422–440, 2020.\n[207] W. Yu et al. Metaformer is actually what you need for vision. In\nCVPR, pp. 10819–10829, 2022.\n[208] N. Park and S. Kim. How do vision transformers work? In ICLR,\n2021.\n[209] K. Wu et al. Rethinking and improving relative position encoding for\nvision transformer. In ICCV, pp. 10033–10041, 2021.\n[210] M. A. Islam et al. Position, padding and predictions: A deeper look at\nposition information in cnns. arXiv:2101.12322, 2021.\n[211] C. Riquelme et al. Scaling vision with sparse mixture of experts.\narXiv:2106.05974, 2021.\n[212] S. Xie et al. Aggregated residual transformations for deep neural\nnetworks. In CVPR, pp. 1492–1500, 2017.\n[213] S. Xie et al. Rethinking spatiotemporal feature learning: Speed-\naccuracy trade-offs in video classiﬁcation. In ECCV, pp. 305–321,\n2018.\n[214] Y . Goyal et al. Making the v in vqa matter: Elevating the role of image\nunderstanding in visual question answering. In CVPR, pp. 6904–6913,\n2017.\n[215] T.-Y . Lin et al. Microsoft coco: Common objects in context. In ECCV,\npp. 740–755, 2014.\n[216] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world\nvisual reasoning and compositional question answering. In CVPR, pp.\n6700–6709, 2019.\n[217] V . Ordonez et al. Im2text: Describing images using 1 million captioned\nphotographs. In NeurIPS, 2011.\n[218] P. Young et al. From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descriptions. ACL,\npp. 67–78, 2014.\n[219] A. Kuznetsova et al. The open images dataset v4. Int. J. Comput. Vis.,\n128(7):1956–1981, 2020.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7317178845405579
    },
    {
      "name": "Computer science",
      "score": 0.6950005292892456
    },
    {
      "name": "Encoder",
      "score": 0.4767211973667145
    },
    {
      "name": "Segmentation",
      "score": 0.46944260597229004
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4343387484550476
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43332377076148987
    },
    {
      "name": "Engineering",
      "score": 0.2077050507068634
    },
    {
      "name": "Electrical engineering",
      "score": 0.136595219373703
    },
    {
      "name": "Voltage",
      "score": 0.08651852607727051
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}