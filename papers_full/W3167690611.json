{
  "title": "Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model",
  "url": "https://openalex.org/W3167690611",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2989173685",
      "name": "Amane Sugiyama",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2106128893",
      "name": "Naoki Yoshinaga",
      "affiliations": [
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3042199843",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2971347700",
    "https://openalex.org/W3093843097",
    "https://openalex.org/W2970845336",
    "https://openalex.org/W3167690611",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2984500026",
    "https://openalex.org/W2767019613",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964289193",
    "https://openalex.org/W3099770676",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W3101683892",
    "https://openalex.org/W3103878009",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2962943802",
    "https://openalex.org/W2888159079",
    "https://openalex.org/W2608029998",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964120396",
    "https://openalex.org/W2952446148",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2806412155",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2964291396",
    "https://openalex.org/W2964093087",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5781–5791\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5781\nContext-aware Decoder for Neural Machine Translation\nusing a Target-side Document-Level Language Model\nAmane Sugiyama\nThe University of Tokyo∗\nsugi@tkl.iis.u-tokyo.ac.jp\nNaoki Yoshinaga\nInstitute of Industrial Science,\nThe University of Tokyo\nynaga@iis.u-tokyo.ac.jp\nAbstract\nAlthough many end-to-end context-aware neu-\nral machine translation models have been pro-\nposed to incorporate inter-sentential contexts\nin translation, these models can be trained only\nin domains where parallel documents with sen-\ntential alignments exist. We therefore present\na simple method to perform context-aware\ndecoding with any pre-trained sentence-level\ntranslation model by using a document-level\nlanguage model. Our context-aware decoder\nis built upon sentence-level parallel data and\ntarget-side document-level monolingual data.\nFrom a theoretical viewpoint, our core contri-\nbution is the novel representation of contex-\ntual information using point-wise mutual in-\nformation between context and the current sen-\ntence. We demonstrate the effectiveness of our\nmethod on English to Russian translation, by\nevaluating with BLEU and contrastive tests for\ncontext-aware translation.\n1 Introduction\nNeural machine translation ( NMT ) has typically\nbeen explored in sentence-level translation settings.\nSuch sentence-level NMT models inevitably suf-\nfer from ambiguities when a source sentence has\nmultiple plausible interpretations. Examples of\nsuch ambiguities include anaphora, ellipsis, and\nlexical coherence (V oita et al., 2019b); although\nresolving these ambiguities has only a minor im-\npact on the translation performance measured by\nBLEU scores (Papineni et al., 2002), they are vital\nin smoothly reading the translated documents.\nTo address this issue, context-awareNMT models\nwhich incorporate document-level information in\ntranslation have recently been explored (Jean et al.,\n2017; Wang et al., 2017; Tiedemann and Scherrer,\n2017; Maruf and Haffari, 2018; V oita et al., 2018;\nBawden et al., 2018; Miculicich et al., 2018; Maruf\net al., 2019; V oita et al., 2019b; Yu et al., 2020;\n∗Currently at Mitsubishi UFJ Morgan Stanley Securities\nKang et al., 2020; Zhang et al., 2020). Most of\nthese models are end-to-end models that require\ndocument-level parallel data with sentential align-\nments for training. However, this data is available\nin only a few domains (Sugiyama and Yoshinaga,\n2019). Researchers have therefore started to utilize\ntarget-side monolingual data to construct auxiliary\nmodels which help a sentence-levelNMT model per-\nform context-aware translation (V oita et al., 2019a;\nStahlberg et al., 2019; Yu et al., 2020).\nIn this study, we propose a simple yet effective\napproach to context-aware NMT using two primi-\ntive components, a sentence-level NMT model and\na document-level language model ( LM). We can\nindependently train the two components on com-\nmon sentence-level parallel data and document-\nlevel monolingual data, respectively, without us-\ning document-level parallel data. Our approach\nthereby makes it possible to perform context-aware\ntranslation with any pre-trained sentence-levelNMT\nmodel, using a pre-trained document-level LM.\nTo give a probabilistic foundation to this combi-\nnation of two independent models, we exploit the\nprobabilistic nature of NMT decoding. When gener-\nating a sequence, a left-to-right decoder outputs a\ncategorical probability distribution over the vocabu-\nlary at every time step. The decoder assigns higher\nprobabilities to the tokens that would be more suit-\nable at that step. Therefore, when multiple valid\ntranslations are possible for the source sentence,\nthe decoder just gives a higher probability to the\ntranslation that is plausible without considering\ncontexts. We thus adjust the probability distribu-\ntions in a context-aware manner using a target-side\ndocument-level LM which models inter-sentential\ndependencies in the target-side document.\nWe evaluate our methods on English to Rus-\nsian translations with the OpenSubtitles2018 cor-\npus (Lison et al., 2018) in terms of the BLEU\nscores and contrastive discourse test sets (V oita\net al., 2019b). Experimental results conﬁrm that\n5782\nour method achieved comparable performance with\nexisting context-aware NMT models that require\neither document-level parallel data (Zhang et al.,\n2018; Sugiyama and Yoshinaga, 2019) or more\nthan one additional model (V oita et al., 2019a; Yu\net al., 2020) for capturing contexts in translation.\nThe contributions of this paper are as follows:\n• We theoretically derived C-SCORE , a score to\nqualify context-aware translation without\nthe need for document-level parallel data.\n• Two formulations with C-SCORE turn any\npre-trained sentence-level NMT model into\na context-aware model, if it generatesn-best\noutputs or performs left-to-right decoding.\n• A comparison between our approach and shal-\nlow fusion (Gulcehre et al., 2015) reveals that\nour approach reformulates shallow fusion\nwhile adding a probabilistic foundation.\n2 Context-aware Decoding using\nDocument-level Language Model\nIn this section, assuming a sentence-level encoder-\ndecoder model (Bahdanau et al., 2015; Vaswani\net al., 2017), we ﬁrst derivecontext-aware score(C-\nSCORE for short), a context-aware objective func-\ntion of outputs to be maximized in decoding. We\nthen describe how to compute the C-SCORE using\nthe decoder with a document-level language model\n(D-LM) (§ 2.1). We ﬁnally detail how to perform\ncontext-aware decoding based on C-SCORE (§ 2.2).\n2.1 C-SCORE : objective function for\ncontext-aware NMT decoding\nLet us consider the problem of ﬁnding a transla-\ntion y of a source sentence x in a document. The\ntarget-side context sentence(s) preceding y, c(y),\nare to be given by the past translations. We formu-\nlate context-aware translation conditioned on c(y)\nas the maximization of the conditional probability\np(y|x,c(y)),\nˆy = arg max\ny\nlog p(y|x,c(y))\n= arg max\ny\nlog p(c(y)|x,y)p(y|x)\np(c(y)|x)\n= arg max\ny\nlog p(c(y)|x,y)p(y|x). (1)\nAssuming that x and y are semantically similar,\nwe make the following approximation,\np(c(y)|y,x) ≈p(c(y)|y). (2)\nFrom Eq. 1 and Eq. 2, we obtain\nˆy ≈arg max\ny\nlog p(c(y)|y)p(y|x)\n= arg max\ny\nlog p(c(y),y)\np(c(y))p(y)p(y|x)\n= arg max\ny\nC-SCORE (y; x,c(y))\nwhere\nC-SCORE (y; x,c(y)) = log p(y|x) + PMI (c(y),y)\n(3)\nPMI (c(y),y) = log p(c(y),y)\np(c(y))p(y) = log p(y|c(y))\np(y)\n(4)\nPMI (c(y),y) is the point-wise mutual information\nof c(y) and y which represents the degree of co-\noccurrence of y and c(y). Given x, y and c(y), we\ncan evaluate the C-SCORE by computing the two\nterms in Eq. 3 using a sentence-level NMT (S-NMT )\nand a document-level LM (D-LM), respectively.\nNotations We ﬁrst introduce some notation to\nexplain the computation in Eq. 3 and Eq. 4 using\n(auto-regressive) neural sequence generation mod-\nels in NMT and LM. For a sequence s (|s|≥ 0)\nand token w, a neural sequence generation model\nparameterized by θcan compute the log probability\nthat wfollows s, which we denote bylog pθ(w|s)):\nlog pθ(wfolows s) = log pθ(s ·w)\npθ(s) = log pθ(w|s)\nwhere “·” denotes sequence concatenation. Apply-\ning this auto-regressively, for any sequence s(1)\n(|s(1)|≥ 0) and s(2) (|s(2)|≥ 1), the probability\nthat s(2) follows s(1) is thereby computed as:\nlog pθ(s(2) follows s(1))\n= log pθ(s(2)|s(1)) =\n|s(2)|∑\nt=1\nlog pθ(s(2)\nt |s(1) ·s(2)\n<t),\nwhere s(2)\n<t = [s1,...,s t−1]. (5)\np(y|x) computed by sentence-level NMT Com-\nputing log p(y|x) using an S-NMT is straightfor-\nward. Suppose y to be a sequence of raw tokens,\ny = [y1,...,y T]. Then log p(y|x) is computed by\nlog p(y|x) = log pS-NMT (˜y; x) (6)\nwhere ˜y = [y1,...,y T,</s>] and </s> is a spe-\ncial token to indicate the end of sentence.\n5783\nPMI computed by document-level LM To com-\npute the components of PMI (c(y),y), p(y) and\np(y|c(y)), we use a document-level language\nmodel (D-LM) which can handle long text spans\ncontaining multiple sentences.\nWe generate training examples for D-LM from a\ndocument as follows. We assume D-LM explicitly\nmodels sentence boundaries. We ﬁrst insert the\nspecial token </s> into every sentence boundary\nincluding the start and end of the document. With\nthis preprocessing, all the sentences start imme-\ndiately after an </s> token and end immediately\nbefore an </s> token. We then sample text spans\nfrom the document using a sliding window, where\nthe start and end of the span do not have to match\nsentence boundaries. The sliding window’s size is\nlarger than the stride size, so adjacent spans may\noverlap. The resulting sequence is fed to the D-LM\nfor training. Note that </s> for D-LM indicates\nsentence boundaries, in other words, both the start\nand end of the sequence.\nUsing D-LM, p(y) is computed by\np(y) = pD-LM (˜y|</s>). (7)\nwhere ˜y = [y1,...,y T,</s>].\nTo computep(y|c(y)), we ﬁrst obtain the context\nsequence ˜c(y) by concatenating all the sentences in\nc(y) with </s>. We then compute the conditional\nprobability p(y|c(y)) by\np(y|c(y)) = pD-LM (˜y|˜c(y)) (8)\nwhere ˜y = [y1,...,y T,</s>].\nLet us explain why we use the boundary-aware\nD-LM rather than boundary-agnostic D-LM.1\nFirstly, boundary-agnostic LMs cannot compute\nthe probability that a sentence is closed with a cer-\ntain length, namely, Eq. 7 cannot be computed.\nSecondly, they also cannot compute p(y|c(y)) cor-\nrectly. For example, suppose the context c(y) is\n“he’s my friend” (with the punctuation “.” omitted),\nand the current target sentence y is “he’s nice.” In\nthis case, Eq. 8 is computed by\np(y|c(y)) = pD-LM ([he,’s,nice]|[he,’s,my,friend]).\nHowever, this estimation of p(y|c(y)) can underes-\ntimate the actualp(y|c(y)) because Eq. 8 inevitably\ngives signiﬁcant probabilities to other y such as “’s\nfather” as well, since “He’s my friend’s father” is\n1We cannot rely on punctuations to know sentence bound-\naries, since they can be omitted in some domains.\nﬂuent as a sequence. This behavior is unsuitable\nfor y,2 since “’s father” is not a complete sentence.\n2.2 Searching for the optimal solution\nSearching for the optimal output y that maximizes\nthe C-SCORE is not trivial since there are O(VT)\ncandidate sequences where V is the vocabulary\nsize and T is the maximum length of sequences\nto be searched. We investigate two approaches to\nobtain approximate solutions: reranking (§ 2.2.1)\nand context-aware beam search (§ 2.2.2).\n2.2.1 Reranking with C-SCORE\nWe ﬁrst generate B hypotheses of the translation\nHB = {y1,..., yB}with beam search of beam\nsize B using the sentence-level NMT model. We\nthen choose the one that maximizes the C-SCORE .\nˆy= arg max\ny∈HB\nC-SCORE (y; x,c(y)) (9)\nAn issue with reranking is that we need to set B\nto a large value when the diversity of models’ out-\nputs is limited (Yu et al., 2020), which increases the\ncost of decoding. We therefore attempt to integrate\nC-SCORE into the decoding with beam search.\n2.2.2 Context-aware beam search\nContext-aware beam search ( C-AWARE beam) is\nbeam search that is extended to work with C-\nSCORE . C-SCORE (Eq. 3) can be decomposed into\ntoken-wise C-SCORE s (Eq. 5 through Eq. 8).\nC-SCORE (y; x,c(y)) = log p(y|x) + PMI (c(y),y)\n=\nT+1∑\nt=1\nC-SCORE w(˜yt|˜y<t)\n(10)\nwhere\nC-SCORE w(˜yt|˜y<t) = logpS-NMT (˜yt|˜y<t; x)\n+ log pD-LM (˜yt|˜c(y) ·˜y<t)\npD-LM (˜yt|</s> ·˜y<t)\n(11)\nBy this decomposition, C-SCORE w is conditioned\non the partial sequence generated by time step t.\nWe can therefore apply beam search to generate\nsequences in an auto-regressive manner.\nThe ﬁrst term of Eq. 11 represents the translation\nprobability for the t-th token. The second term can\n2Strictly speaking, we assume y to be a realization of a\nrandom variable Y which is a sentence sampled from the\nspace of an inﬁnitely large document.\n5784\nbe interpreted as PMI between the t-th token and\nthe context, that is, how consistent the t-th token\nis with the context. Compared to the reranking\napproach, C-AWARE beam can be considered to\nmaximize the C-SCORE more directly in the sense\nthat disambiguation and token selection based on\nthe context are performed at every step in beam\nsearch. Thus C-AWARE beam will more space-\nefﬁciently consider diverse hypotheses with the\nsame beam size Bthan C-AWARE rerank.\n2.2.3 Smoothing probabilities for PMI\nIn our preliminary experiments, we observe that\nthe original C-AWARE beam signiﬁcantly improves\ncontrastive tests but deteriorates BLEU at the same\ntime. By analyzing contextual PMI correlation be-\ntween source and target texts, we ﬁnd the PMI term\nin the C-SCORE sometimes takes an excessively\nlarge value against the translation probability term,\nwhich destroys the C-SCORE . This is understood\nintuitively by the fact that the calculation of PMI in-\ncludes subtraction of log probability, and log prob-\nability may take a very small negative value to\nrepresent a probability close to zero.\nTo alleviate this problem, we adopt a smoothing\nmethod for probabilities. For simplicity, in this\npaper, we only present the temperature scaling (T-\nscaling, for short) (Guo et al., 2017). T-scaling\nreplaces py=w by\n¯py=w = p1/T\ny=w\n∑\nw′p1/T\ny=w′\n(12)\nwhere T is a hyper-parameter. T = 1 is equivalent\nto no smoothing. We choose T from [1,∞) to\nﬂatten the probability distribution. T-scaling is\napplied to both the numerator and denominator\nusing the same T.\n2.2.4 On the relation to shallow fusion\nShallow fusion (Gulcehre et al., 2015) is a method\nto integrate probability distribution outputs ob-\ntained by NMT and LM at sentence level to form\na new translation objective that is expected to pro-\nmote ﬂuency of translations. The original shallow\nfusion score is computed using a sentence-level\nNMT (S-NMT ) and language model ( S-LM). The\ntoken-wise formula of the computation is\nlog p(yt) = log pS-NMT (yt; x) + βlog pS-LM (yt),\n(13)\nwhere βis a hyper-parameter. In our notation with\nthe document-level LM, this is written as\nlog p(yt) = log pS-NMT (˜yt|˜y<t; x)\n+ βlog pS-LM (˜yt|</s> ·˜y<t). (14)\nA natural extension of this objective to the context-\naware scenario should be\np(yt|c(y)) = log pNMT (˜yt|˜y<t; x)\n+ βlog pD-LM (˜yt|˜c(y) ·˜y<t), (15)\nwhere context ˜c(y) is integrated into the condition.\nWe call this conditional (document-level) shallow\nfusion. Obviously, this is what we obtain from\nEq. 11 by ignoring the discount of the uncondi-\ntional LM probability pD-LM (˜yt|</s> ·˜y<t).\nDue to the absence of discounting with the un-\nconditional LM, conditional shallow fusion would\nprefer tokens which frequently occur regardless of\nthe context. It is also worth noting that, when the\ncontext is empty, conditional shallow fusion falls\nback to the original shallow fusion, whereas our C-\nSCORE falls back to sentence-levelNMT . Therefore,\nwe view C-SCORE as a reformulation of shallow\nfusion for context-aware translation.\n3 Experimental Setup\nWe evaluate our methods on English to Russian\ntranslation, in terms of BLEU scores (Papineni et al.,\n2002) and contrastive tests (V oita et al., 2019b).\n3.1 Datasets and preprocessing\nWe use the OpenSubtitles2018 corpus (Lison et al.,\n2018) for parallel and monolingual data. Following\nthe criteria for document segmentation and ﬁlter-\ning on sentence pairs presented by (V oita et al.,\n2019b), we build monolingual and parallel data\nas follows. To build monolingual data, we add\ndocument boundary information into each docu-\nment such that they consist of contiguous subtitle\nsentences from the same movie and the timestamp\ndifference of any two adjacent sentences is no more\nthan seven seconds. To build parallel data, we pick\nsubtitle pairs where the time overlap between the\nsource and target language subtitles is at least 0.9\n(to reduce alignment errors). For the training of\nmulti-encoder NMT models, document boundary\ninformation is added to the parallel data based on\nthe source-side timestamps as with the monolin-\ngual data. Prior to building the Russian data, we\n5785\nTrain Dev. Test\nsrc trg (mono) src trg (mono) src trg\n# sentences 5.8M 30M 6.0k 23k 15.5k\navg. # tokens 9.9 9.4 8.5 10.1 9.6 8.9 9.8 9.1\nTable 1: Statistics of the parallel and monolingual data.\nremove the movies from which the contrastive test\nsets (§ 3.4) were made.\nWe perform punctuation normalization, tokeniza-\ntion, and truecasing on the source and target texts\nusing Moses toolkit v4.0. 3 We then encode the\ntexts into subwords using SentencePiece (v0.1.81)4\nwith unigram LM. The subword vocabularies are of\n16,000 tokens and trained for each language. The\nstatistics of the datasets are listed in Table 1.\n3.2 Models\nWe compare our methods to one sentence-level\ntranslation model ( SentTransformer) (Vaswani\net al., 2017) and three context-aware translation\nmodels: Document transformer (Zhang et al.,\n2018), DocRepair (V oita et al., 2019a), and Bayes\nDocument Reranker (Yu et al., 2020). All the\ncontext-aware models use the previous three sen-\ntences as context.\nDocument Transformer (DocTransformer, for\nshort) is a multi-encoder document-level NMT\nmodel which takes source-side context as an auxil-\niary input and can be thus trained from document-\nlevel parallel data. We follow (Zhang et al., 2018)’s\nconﬁguration for DocTransformer.\nDocRepair is a sequence-to-sequence post-editing\nmodel. It repairs document-level inconsistencies in\na text, each sentence of which has been translated\nseparately by a sentence-level NMT model. DocRe-\npair is trained on a pseudo parallel data made\nby pairing a monolingual corpus and its round-\ntrip translations obtained using a back-translation\nmodel and a forward-translation model.\nBayes Document Reranker (hereafter, Bayes\nDocReranker) performs document-level transla-\ntion on a document containing Dsentences in the\nfollowing steps. First, it produces B-best transla-\ntions for each sentence in the document and then\nproduces a lattice of width Band depth D, where\neach node corresponds to a candidate sentence. It\n3http://www.statmt.org/moses/\n4https://github.com/google/\nsentencepiece\nthen performs document-level beam search of beam\nsize B′on the lattice using the following score:\nScore(yi; y<i,xi) =\npD-LM (yi|y<i) + Score(yi−1; y<i−1,xi−1)\n+ λ1pNMT (yi|xi) + λ2pBACK -NMT (xi|yi) + λ3|yi|\n(16)\nNote that this document-level beam search is equiv-\nalent to the reranking procedure (§ 2.2.1) when\nB′ = 1 . Therefore, the essential difference\nbetween Bayes DocReranker and our C-SCORE\nreranking is the score function.\nSentTransformer, the post-editing model of\nDocRepair, and the back-translation models are\nbased on the same conﬁguration of Transformer\nbase (see (Vaswani et al., 2017) for hyperparame-\nter settings). The SentTransformer is trained using\nthe 5.8M sentence pairs and is also used as the\nsentence-level NMT model in DocRepair, Bayes\nDocReranker, and our methods. For the training of\nDocTransformer, we use the 5.8M sentence pairs\nwith document-level source context, which share\nthe target-side sentences with the training data of\nSentTransformer. Consequently, scores obtained\nfrom the model are for reference. 5 We also eval-\nuate DocTransformer and SentTransformer using\nback-translation (BT) (Sennrich et al., 2016) with\nthe same monolingual data as the other models.\nWe use no pre-existing document-level paral-\nlel data to train the neural networks of DocRepair,\nBayes DocReranker, and our methods, although\nwe use a small amount of document-level parallel\ndata as the development set to tune hyperparame-\nters in the methods that combine multiple models.\nInstead, document-level information is fed to the\nmodels via the round-trip augmented data (DocRe-\npair) or language models (Bayes DocReranker and\nour methods).\nHyper-parameters We tune the models’ hyper-\nparameters based on BLEU score on the develop-\nment set in the evaluation with BLEU , while we\ntune these hyper-parameters in the evaluation of\ncontrastive tests by maximizing the coefﬁcient of\nD-LM under the constraint that it does not deterio-\nrate BLEU compared to the SentTransformer.\nFor beam search to produce B-best outputs in\nBayes DocReranker and our C-AWARE Rerank, we\n5Although we can train DocTransformer only on pseudo\ndocument-level parallel data generated by back-translation,\nwe conﬁrmed in preliminary experiments that the resulting\nmodel exhibited poor performance.\n5786\nModels para monolingual data\nonly 6M 15M 30M\nSentTransformer (w/ BT) 32.36 32.32 32.40 32.40\nShallow Fusion n/a 32.39 32.56 32.52\nbaselines\nDocTransformer (w/ BT) 32.50 32.36 31.88 31.59\nDocRepair n/a 32.13 32.36 32.35\nBayes DocReranker n/a 32.80∗ 33.58∗∗33.75∗∗\nw/o context n/a 32.53 33.44 ∗∗33.67∗∗\nproposed\nC-AWARE Rerank n/a 32.74 ∗ 33.01∗∗32.93∗\nC-AWARE Beam n/a 32.26 32.28 32.27\nCond. Shallow Fusion n/a 32.38 32.55 32.55\nTable 2: Test set BLEU scores. ‘*’ and ‘**’ indicate\nthat gains from SentTransformer in the same column\nare statistically signiﬁcant (p< 0.05 and p< 0.01) by\nbootstrap resampling with 1000 samples, respectively.\nuse a beam size of B = 20. For document-level\nbeam search of Bayes DocReranker, we use a beam\nsize B′= 5. For beam search of SentTransformer,\nDocTransformer, C-AWARE beam, and shallow fu-\nsion, we use a beam size of B = 4.\n3.3 Document-level Language models\nThe architecture of the document-level LM is the\ndecoder part of a Transformer. The number of\ndecoder blocks is 12. The model size is 768 with\n12 attention heads, and the inner layer of the feed-\nforward networks has 3072 units. We use position\nembeddings to represent position information.\nAs described in § 2.1, when training the lan-\nguage models, a special control symbol </s> is\ninserted at every sentence boundary. Each training\nmini-batch contains text spans each of which is a\nrandomly sampled fragment of a document with\na maximum span length of W = 384. Text spans\nare batched such that about 32,000 tokens are in a\ntraining batch.\n3.4 Evaluation methods\nThe existing automatic metrics are not adequate\nto evaluate gains from additional contexts (Baw-\nden et al., 2018; Läubli et al., 2018; Müller et al.,\n2018; V oita et al., 2019b; Sugiyama and Yoshinaga,\n2019). We thus adopt a contrastive test set (V oita\net al., 2019b) to evaluate the model’s ability to\ncapture contextual information in translation, in ad-\ndition to the evaluation by BLEU scores (Papineni\net al., 2002) to conﬁrm that the methods do not\nsacriﬁce general translation performance. BLEU\nis computed using multi-bleu.perl from the\nMoses Toolkit after decoding the subword repre-\nModels deixis lex.c ell.inﬂ ell.vp\nSentTransformer 50.0 45.9 53.2 27.0\nw/ BT 50.0 45.9 51.6 26.8\nbaselines\nDoc-Transformer 50.0 45.9 56.0 57.2\nw/ BT 50.0 45.9 64.4 68.2\nDocRepair 89.1 75.8 82.2 67.2\nBayes DocReranker 65.2 72.2 59.6 44.6\nproposed\nC-SCORE 86.9 94.9 78.2 77.0\nCond. Shallow Fusion 54.7 55.3 53.4 32.4\nD-LM PMI (c(y), y) 96.8 97.8 75.8 90.6\np(y|c(y)) 89.7 95.7 77.4 81.6\nTable 3: Results on contrastive test sets.\nsentation of the models’ outputs into words using\nSentencePiece.\nThe contrastive test set consists of contrastive\nquestions for context-aware NMT models to answer.\nEach question has a source sentence x, a source\ncontext c(x), a target context c(y), and translation\ncandidates Y= {y1,...,y M}. Models must an-\nswer with a candidate ˆy ∈Y which would be the\nmost appropriate translation of x, i.e.\nˆy= arg max\ny∈Y\np(y|x,c(x),c(y))\nThe test sets consist of 6000 examples in total.\n4 Results and Analysis\n4.1 General translation performance\nmeasured by BLEU scores\nTable 2 lists the performance of the models in\nterms of BLEU scores. Bayes DocReranker and\nour C-AWARE Rerank consistently outperformed\nthe baseline SentTransformer, even when it used\ndata augmentation by back-translation, while the\nother methods are just comparable to the baseline.\nAlthogh Bayes DocReranker performed the best\namong all the models, the comparison to Bayes\nDocReranker without context information (using\npS-LM (yi) instead of pD-LM (yi|y<i)) reveals that\nmost of the improvement is not obtained by the\nuse of contexts. Back-translation did not contribute\nto BLEU possibly because the original parallel data\nis already large and there was little room for im-\nprovement with additional pseudo data.\n4.2 Results on contrastive test sets\nTables 3 lists evaluation results (accuracy) of the\ncontrastive tests with models using 30M mono-\nlingual data. The highest scores on each column\n5787\n(a) PMI\n (b) PMI with rand. context\n (c) Cond. prob.\n (d) Cond. prob. (rand. context)\nFigure 1: Source-target correlation of contextual PMI (a, b) and conditional probability (c, d), calculated based on\nthe correct context (a, c) and wrong context that is randomly chosen from the dataset (b, d). The dataset is a subset\nof the training data from the English-Russian parallel corpus. Plots are for 4166 sentence pairs in the dataset.\nare in bold, and additionally, the higher one of\nthe two D-LM-based scores is shown in bold. The\ncontrastive test include four test sets: deixis is for\nperson deixis, lex.c is for lexical cohesion, ell.inﬂ\nis for inﬂection of Russian nouns caused by ellipsis\nin the source sentence, and ell.vp is for verb ellipsis\nin English text which is not allowed in Russian.\nAlthough the contrastive test is targeted at context-\naware NMT models, it is possible to answer the\ncontrastive questions by arg maxy PMI (c(y),y) or\narg maxyp(y|c(y)). Scores obtained by these two\nobjectives are also reported in the table in addition\nto the scores obtained by SentTransformer.\nOur C-SCORE outperforms all the context-aware\nmodels other than DocRepair. The performance\nof C-SCORE is slightly worse than DocRepair\nfor deixis (2.2 points) and ell.inﬂ (4.0 points),\nwhile achieving large improvements for lex.c (19.1\npoints) and ell.vp (9.8 points) over DocRepair.\nD-LM only objectives achieve higher scores than\nC-SCORE , except for ell.inﬂ. This is not surprising\nbecause the choices in the tests are guaranteed to\nbe valid as translation for the source sentences if\ngiven some appropriate context, so the questions\ncan be solved without translation. This result still\nindicates that the D-LM scores give good hints for\ntackling contextual ambiguities. The advantage\nof C-SCORE over the SentTransformer is demon-\nstrated by the excellent performance of D-LM in\ncapturing contexts in translation.\n4.3 On translation efﬁciency\nThe inference speed depends mainly on the model\nsize and beam size. In our experiments on a sin-\ngle TITAN Xp GPU, SentTransformer decoded the\nfastest at 66 sents/sec, followed by DocTransformer\nthat ran in 40 sents/sec. DocRepair ran in about\n28 sents/sec, slightly slower because it decodes in\ntwo passes. C-AWARE Rerank and Bayes DocRe-\nranker were about 4.3 sents/sec and 7.7 sents/sec\nrespectively. We expect that these models would be\naccelerated by using a language model with a better\ncache mechanism (e.g. TransformerXL (Dai et al.,\n2019)). C-AWARE Beam ran in about 13 sents/sec.6\nWe leave thorough analysis on speed/performance\ntrade-offs to future work.\n4.4 PMI correlation analysis\nIn § 4.2 we have conﬁrmed the effectiveness ofPMI\nas a measure of a valid translation given context\nusing contrastive tests. To gain a deeper insight\ninto how well PMI conveys semantic connections\nbetween the current sentence and its context, we\nanalyze the correlation of PMI between source and\ntarget sentences.\nPMI correlation between source and target\nThe main result we show in this section is that the\nPMI of the source and target correlate well. This is\nimportant because this supports the idea that PMI is\na language-independent measure of the connection\nbetween the current sentence and its context.\nAlthough we have discussed only target-side\nPMI (c(y),y) deﬁned by Eq. 4, we can compute the\nsource-side PMI (c(x),x) in the same way. Given\na document-level parallel corpus, we measure a\ncorrelation between PMI (c(x),x) and PMI (c(y),y)\nfor each sentence pair (x,y) in the corpus.\nFigure 1a shows the PMI correlation for about\n6Note that the running time of NMT decoding also depends\non the degree of parallelism, and for C-AWARE Beam, de-\ncoding multiple sentences in parallel is less trivial since it\ndemands that all the previous sentences in the document are\ntranslated by the time it starts to translate the current one. In\nour experiments, assuming a practical scenario where a large\nnumber of users input their documents for translation, we\ntranslate multiple documents in parallel so that multiple sen-\ntences from different documents can be translated in parallel.\n5788\nFigure 2: Correlation of contextual PMI between the source sentences (from the training data) and the outputs of\nsome models (SentTransformer,C-AWARE beam without T-scaling, and C-AWARE beam with T-scaling of T = 4).\n4000 sentence pairs taken from the dev data. The\npairs of PMI values are computed using English\nand Russian language models trained on the train-\ning data. We observe a clear correlation between\nsource and target, which agrees with the intuition\nthat if the target sentence matches well in the con-\ntext, so does the source sentence. What is also\nobvious in Figure 1a is that most of the points lay\nin the ﬁrst quadrant where both the source and\ntarget contextual PMI is greater than 0, which is\nexplained by the simple intuition that most sen-\ntences should have positive co-occurrence relation\nwith their contexts. This behavior is lost when\ncomputing the contextual PMI using an incorrect\ncontext ˜crandomly chosen in the dataset as shown\nin Figure 1b.\nThe effectiveness of PMI as a measure of the\nvalid translation of the current sentence given con-\ntext is further emphasized when compared to the\nconditional probability p(y|c(y)), which could be\nan alternative measure of how suitable y is in the\ncontext as described in § 2.2.4. Figure 1c and 1d\nare the conditional probability version of Figure 1a\nand 1b: (p(x|c(x)),p(y|c(y))) for each sentence\npair (x,y) in the same dataset are plotted in Fig-\nure 1c and the same tuples but with random con-\ntexts are plotted in Figure 1d. Unlike the contextual\nPMI correlation, conditional probability correlation\nremains high even when we give wrong contexts.\nThis is because the conditional probability of a\nsentence is highly affected by how frequently the\nsentence is observed regardless of context; if the\nsource sentence is written with common expres-\nsions, then so is the target sentence and they are\nlikely to be observed regardless of the context.\nAnalysis of the model outputs\nPMI correlation gives us a good explanation of how\nC-AWARE beam without T-scaling fails. We plot\nthe PMI correlation between the source sentences\nand their translations obtained with NMT models\n(Figure 2). We can ﬁnd some outliers in the bottom\nright area of the plot for C-AWARE beam without\nT-scaling, which is the cause of the low correla-\ntion coefﬁcient R = 0.610 < Rsrc−ref = 0.695.\nThis result suggests that C-AWARE beam without\nT-scaling chooses some tokens based on exces-\nsively high token-wise PMI , which breaks some\ntranslations resulting in the low BLEU . Translation\nof the SentTransformer shows a higher correlation\nwith the source texts than the reference translation\n(Figure 1a). One possible explanation for this is\nalignment errors in the corpus: although worse than\nthe reference translations in quality, outputs of Sent-\nTransformer are considered to be perfectly aligned\nto the source sentences. C-AWARE beam with T-\nscaling ( T = 4 ) seems to solve this issue and\nachieves the highest PMI correlation R= 0.740.\n5 Related Work\nThe effectiveness of incorporating context into\ntranslation was shown in earlier literature on\ndocument-level NMT (Tiedemann and Scherrer,\n2017; Bawden et al., 2018) using the single en-\ncoder architecture. Multi-encoder architectures\nwere explored to better capture contextual infor-\nmation (Wang et al., 2017; Tu et al., 2018; Jean\net al., 2017; Miculicich et al., 2018; V oita et al.,\n2018; Bawden et al., 2018; Maruf and Haffari,\n2018; Maruf et al., 2019; Kang et al., 2020; Zhang\net al., 2020). However, since parallel data is often\nconstructed by picking up reliable sentential align-\nments from comparable documents, document-\nlevel sentence-aligned parallel data for training\nthese document-level NMT models are expensive\nto obtain and available in only a few domains and\nlanguage pairs (Sugiyama and Yoshinaga, 2019).\nRecent studies have therefore started to focus\non modeling contexts using document-level mono-\nlingual data. The current approaches are grouped\ninto three categories: data augmentation via back-\n5789\ntranslation (Sugiyama and Yoshinaga, 2019), a\npost-editing model (V oita et al., 2019a), and mod-\neling document-level ﬂuency via document-level\nLMs (Stahlberg et al., 2019; Yu et al., 2020; Jean\nand Cho, 2020). In what follows, we review these\napproaches in detail.\nSugiyama and Yoshinaga (2019) reported that\nthe data augmentation by back-translation (Sen-\nnrich et al., 2016) enhances a document-level NMT\nmodel with a single encoder architecture in low-\nresource settings. However, we have obtained lim-\nited improvements in our settings (Table 2 and Ta-\nble 3). Moreover, this approach is expensive since\nit learns a document-level NMT model from a mas-\nsive amount of pseudo parallel data.\nV oita et al. (2019a) proposed DocRepair, a\ncontext-aware post-editing model that corrects out-\nputs of a sentence-level NMT model. Because\nDocRepair ignores the conﬁdence of the ﬁrst-\nstage sentence-level translation and possible alter-\nnative translations, it can miscorrect outputs of the\nsentence-level NMT model when they are irregular\nbut correct. Moreover, when we change the tar-\nget sentence-level NMT model, the accompanying\npost-editing model must be trained from its outputs.\nOur approaches, on the other hand, attempt a more\n“soft” revision, taking into account the output prob-\nabilities, i.e., conﬁdence of the sentence-level NMT ,\nand can perform context-aware decoding with any\nsentence-level NMT model, reusing a pre-trained\ndocument-level LM.\nStahlberg et al. (2019) and Yu et al. (2020) uti-\nlize a document-level LM to model document-level\nﬂuency of outputs; these approaches are similar\nto shallow fusion (Gulcehre et al., 2015) 7 with\ndocument-level LM (§ 2.2.4), although they per-\nform a document-level reranking of translation\nhypotheses generated for individual source sen-\ntences by using sentence-level NMT . In particular,\nYu’s formulation has a probabilistic foundation like\nour approaches, and additionally utilizes a back-\nward translation model. Although their formulation\nbrings a signiﬁcant improvement inBLEU (Table 2),\nthe score is not obtained by better document-level\n7Our work is also related to shallow fusion (Gulcehre et al.,\n2015), in which token-wise probabilities output by an NMT\nmodel and a sentence-level LM are combined to be used as\ntranslation scores in decoding. The theoretical background\nof shallow fusion and our C-SCORE are different: in shallow\nfusion, the LM is intended to promote ﬂuency of translations,\nwhereas in our C-SCORE , we use the probability ratio of two\nLM probabilities which only provides contextual difference\nand ﬂuency is still left to the translation model.\ntranslation; the comparable BLEU score of the no-\ncontext version of the method (Table 2) and the re-\nsults of the contrastive tests (Table 3) reveal that the\nimprovement is mostly due to the context-agnostic\nlanguage model prior and the backward translation\nmodel. As we have discussed in § 2.2.4, document-\nlevel LM scores prefer tokens which frequently ap-\npear regardless of context and are unlikely to lead\nto better document-level translation. Moreover,\ntheir method requires training a back-translation\nmodel corresponding to the target sentence-level\nNMT model.\nFinally, we noticed that Jean and Cho (2020)\n(which appeared after the preprint version of this\npaper (Sugiyama and Yoshinaga, 2020)8 had been\nsubmitted) have reached a formulation that is very\nsimilar to the one presented in this paper by refor-\nmulating a noisy channel model of Bayes DocRe-\nranker (Yu et al., 2020). Concrete differences be-\ntween our work and theirs include the fact that we\nconducted thorough analysis on the performance of\ndifferent decoding strategies (not only beam search\nbut also reranking). We also interpreted the sub-\ntraction of LM scores as point-wise mutual informa-\ntion and analyzed it by observing PMI correlation\nbetween source and target PMI to deepen the under-\nstanding of the formulation.\n6 Conclusions\nWe present an approach to context-aware NMT\nbased on PMI between the context and the cur-\nrent sentence. We ﬁrst provide the formulation of\nthe objective, C-SCORE , and the computation pro-\ncess of the C-SCORE using a sentence-level transla-\ntion model and a document-level language model.\nWe investigate two search methods, reranking and\nbeam search, and evaluate the methods for English-\nRussian translation. We also provide some analysis\nand visualization to better understand the nature of\nPMI between the context and the current sentence.\nWe plan to design context-aware BLEU using\nPMI for evaluating context-aware NMT models. We\nwill evaluate our method on non-autoregressive\nNMT (Gu et al., 2017). We will release all code and\ndata to promote the reproducibility of results.9\n8This preprint is submitted to and rejected from EMNLP\n2020; the interested reader may refer to this paper for experi-\nments on other language pairs such as English to French and\nEnglish to Japanese translation.\n9http://www.tkl.iis.u-tokyo.ac.jp/\n~sugi/NAACL2021/\n5790\nAcknowledgements\nWe thank anonymous reviewers for their valuable\ncomments. We also thank Joshua Tanner for proof-\nreading this paper. We also thank Masato Neishi\nfor technical advice on implementations of neural\nmachine translation. The research was supported\nby NII CRIS collaborative research program oper-\nated by NII CRIS and LINE Corporation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe third International Conference on Learning Rep-\nresentations (ICLR).\nRachel Bawden, Rico Sennrich, Alexandra Birch, and\nBarry Haddow. 2018. Evaluating discourse phenom-\nena in neural machine translation. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1304–1313, New Orleans, Louisiana.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2017. Non-\nautoregressive neural machine translation. In Pro-\nceedings of the the ﬁfth International Conference for\nLearning Representations (ICLR).\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On\nusing monolingual corpora in neural machine\ntranslation. Computing Research Repository ,\narXiv:1503.03535.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International\nConference on Machine Learning , volume 70 of\nProceedings of Machine Learning Research , pages\n1321–1330. PMLR.\nSébastien Jean and Kyunghyun Cho. 2020. Log-\nlinear reformulation of the noisy channel model for\ndocument-level neural machine translation. In Pro-\nceedings of the Fourth Workshop on Structured Pre-\ndiction for NLP, pages 95–101, Online.\nSebastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machine trans-\nlation beneﬁt from larger context? arXiv preprint\narXiv:1704.05135.\nXiaomian Kang, Yang Zhao, Jiajun Zhang, and\nChengqing Zong. 2020. Dynamic context selection\nfor document-level neural machine translation via re-\ninforcement learning. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2242–2254, On-\nline.\nSamuel Läubli, Rico Sennrich, and Martin V olk. 2018.\nHas machine translation achieved human parity? a\ncase for document-level evaluation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4791–4796,\nBrussels, Belgium.\nPierre Lison, Jörg Tiedemann, and Milen Kouylekov.\n2018. OpenSubtitles2018: Statistical rescoring of\nsentence alignments in large, noisy parallel corpora.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\nSameen Maruf and Gholamreza Haffari. 2018. Docu-\nment context neural machine translation with mem-\nory networks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1275–1284,\nMelbourne, Australia.\nSameen Maruf, André F. T. Martins, and Gholamreza\nHaffari. 2019. Selective attention for context-aware\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3092–3102, Minneapolis, Min-\nnesota.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2947–2954, Brussels, Belgium.\nMathias Müller, Annette Rios, Elena V oita, and Rico\nSennrich. 2018. A large-scale test set for the eval-\nuation of context-aware pronoun translation in neu-\nral machine translation. In Proceedings of the Third\nConference on Machine Translation: Research Pa-\npers, pages 61–72, Brussels, Belgium.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany.\n5791\nFelix Stahlberg, Danielle Saunders, Adrià\nde Gispert, and Bill Byrne. 2019.\nCUED@WMT19:EWC&LMs. In Proceedings\nof the Fourth Conference on Machine Translation\n(Volume 2: Shared Task Papers, Day 1) , pages\n364–373, Florence, Italy.\nAmane Sugiyama and Naoki Yoshinaga. 2019. Data\naugmentation using back-translation for context-\naware neural machine translation. In Proceedings\nof the Fourth Workshop on Discourse in Machine\nTranslation (DiscoMT 2019) , pages 35–44, Hong\nKong, China.\nAmane Sugiyama and Naoki Yoshinaga. 2020.\nContext-aware decoder for neural machine transla-\ntion using a target-side document-level lan-\nguage model. Computing Research Repository ,\narXiv:2010.12827.\nJörg Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, pages 82–92, Copenhagen, Denmark.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang.\n2018. Learning to remember translation history with\na continuous cache. Transactions of the Association\nfor Computational Linguistics, 6:407–420.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, page 6000–6010. Cur-\nran Associates, Inc.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 877–886, Hong Kong, China.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019b.\nWhen a good translation is wrong in context:\nContext-aware machine translation improves on\ndeixis, ellipsis, and lexical cohesion. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 1198–1212, Flo-\nrence, Italy.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274, Melbourne, Australia.\nLongyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu.\n2017. Exploiting cross-sentence context for neural\nmachine translation. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2826–2831, Copenhagen,\nDenmark.\nLei Yu, Laurent Sartran, Wojciech Stokowiec, Wang\nLing, Lingpeng Kong, Phil Blunsom, and Chris\nDyer. 2020. Better document-level machine trans-\nlation with Bayes’ rule. Transactions of the Associ-\nation for Computational Linguistics, 8:346–360.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 533–542, Brussels, Bel-\ngium.\nPei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020.\nLong-short term masking transformer: A simple\nbut effective baseline for document-level neural ma-\nchine translation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1081–1087, Online.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8933092355728149
    },
    {
      "name": "Machine translation",
      "score": 0.8851154446601868
    },
    {
      "name": "Sentence",
      "score": 0.7539136409759521
    },
    {
      "name": "Natural language processing",
      "score": 0.7301485538482666
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6962756514549255
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6708812117576599
    },
    {
      "name": "Translation (biology)",
      "score": 0.5513078570365906
    },
    {
      "name": "Decoding methods",
      "score": 0.5315931439399719
    },
    {
      "name": "Language model",
      "score": 0.5276545286178589
    },
    {
      "name": "Language translation",
      "score": 0.5240017175674438
    },
    {
      "name": "Point (geometry)",
      "score": 0.4612314701080322
    },
    {
      "name": "Context model",
      "score": 0.41894739866256714
    },
    {
      "name": "Speech recognition",
      "score": 0.3671521842479706
    },
    {
      "name": "Algorithm",
      "score": 0.0787234902381897
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}