{
    "title": "Modeling actions of PubMed users with n-gram language models",
    "url": "https://openalex.org/W1964820892",
    "year": 2008,
    "authors": [
        {
            "id": "https://openalex.org/A2163619555",
            "name": "Jimmy Lin",
            "affiliations": [
                "National Center for Biotechnology Information",
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A2303463049",
            "name": "W. John Wilbur",
            "affiliations": [
                "National Center for Biotechnology Information"
            ]
        },
        {
            "id": "https://openalex.org/A2163619555",
            "name": "Jimmy Lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2303463049",
            "name": "W. John Wilbur",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2125771191",
        "https://openalex.org/W256610757",
        "https://openalex.org/W2064522604",
        "https://openalex.org/W2156037541",
        "https://openalex.org/W2323257365",
        "https://openalex.org/W1982896842",
        "https://openalex.org/W1969053216",
        "https://openalex.org/W2097927681",
        "https://openalex.org/W1593045043",
        "https://openalex.org/W2137591261",
        "https://openalex.org/W2099548400",
        "https://openalex.org/W2116780029",
        "https://openalex.org/W2012451152",
        "https://openalex.org/W1530232666",
        "https://openalex.org/W6606440582",
        "https://openalex.org/W2098146716",
        "https://openalex.org/W2039950197",
        "https://openalex.org/W2145734075",
        "https://openalex.org/W2154724067",
        "https://openalex.org/W1974360117",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2001779021",
        "https://openalex.org/W2111016837",
        "https://openalex.org/W4242151277",
        "https://openalex.org/W2061172803",
        "https://openalex.org/W1574901103",
        "https://openalex.org/W191409295",
        "https://openalex.org/W1984417756",
        "https://openalex.org/W2077221142",
        "https://openalex.org/W1525289242",
        "https://openalex.org/W2104217798",
        "https://openalex.org/W2108168165",
        "https://openalex.org/W1982889956",
        "https://openalex.org/W2165159668",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W1984581259",
        "https://openalex.org/W1858526431",
        "https://openalex.org/W2950186769",
        "https://openalex.org/W2917915395",
        "https://openalex.org/W2152707065",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2917056928",
        "https://openalex.org/W4290649644",
        "https://openalex.org/W1511735137",
        "https://openalex.org/W2123273303",
        "https://openalex.org/W1564163720",
        "https://openalex.org/W2007709031",
        "https://openalex.org/W1985844568",
        "https://openalex.org/W2032958446",
        "https://openalex.org/W2508925437"
    ],
    "abstract": "Transaction logs from online search engines are valuable for two reasons: First, they provide insight into human information-seeking behavior. Second, log data can be used to train user models, which can then be applied to improve retrieval systems. This article presents a study of logs from PubMed((R)), the public gateway to the MEDLINE((R)) database of bibliographic records from the medical and biomedical primary literature. Unlike most previous studies on general Web search, our work examines user activities with a highly-specialized search engine. We encode user actions as string sequences and model these sequences using n-gram language models. The models are evaluated in terms of perplexity and in a sequence prediction task. They help us better understand how PubMed users search for information and provide an enabler for improving users' search experience.",
    "full_text": "Modeling actions of PubMed users with n-gram language\nmodels\nJimmy Lin Æ W. John Wilbur\nReceived: 28 February 2008 / Accepted: 20 August 2008 / Published online: 12 September 2008\n/C211 The Author(s) 2008. This article is published with open access at Springerlink.com\nAbstract Transaction logs from online search engines are valuable for two reasons: First,\nthey provide insight into human information-seeking behavior. Second, log data can be\nused to train user models, which can then be applied to improve retrieval systems. This\narticle presents a study of logs from PubMed\n/C210 , the public gateway to the MEDLINE /C210\ndatabase of bibliographic records from the medical and biomedical primary literature.\nUnlike most previous studies on general Web search, our work examines user activities\nwith a highly-specialized search engine. We encode user actions as string sequences and\nmodel these sequences using n-gram language models. The models are evaluated in terms\nof perplexity and in a sequence prediction task. They help us better understand how\nPubMed users search for information and provide an enabler for improving users’ search\nexperience.\nKeywords Search behavior /C1 Query log analysis\n1 Introduction\nInformation seeking is fundamentally an iterative activity that involves multiple interac-\ntions between a user and a search system. With the advent of online search engines, it has\nbecome standard practice to store records of users’ activities—commonly known as search\ntransaction logs (or query logs, as a special case). These resources are useful for two main\nreasons: First, they shed light on human information-seeking behavior—what users want\nand how they go about accomplishing it (e.g., Silverstein et al. 1999; Beitzel et al. 2004;\nRose and Levinson 2004; Jansen and Spink 2006). Second, log data can be exploited to\nJ. Lin ( &)\nThe iSchool, College of Information Studies, University of Maryland, College Park, MD, USA\ne-mail: jimmylin@umd.edu\nJ. Lin /C1 W. J. Wilbur\nNational Center for Biotechnology Information, National Library of Medicine, Bethesda, MD, USA\ne-mail: wilbur@ncbi.nlm.nih.gov\n123\nInf Retrieval (2009) 12:487–503\nDOI 10.1007/s10791-008-9067-7\nimprove the user’s search experience; examples include query suggestion (Anick2003; Cui\net al. 2003) and improved ranking (Agichtein et al. 2006; Joachims et al. 2007).\nThe nature of users’ interactions with search engines depends on many factors,\nincluding characteristics of the user, type of task, problem domain, etc. While there has\nbeen a signiﬁcant amount of work on analysis of search logs, previous studies have almost\nexclusively focused on general Web search; see Jansen and Spink ( 2006) for an overview.\nIn this article, we present an analysis of logs from the PubMed search engine (more details\nin Sect. 2). Note that due to the subject domain of PubMed (the life sciences) and the\ndemographic proﬁle of its users (biologists, physicians, clinical researchers, etc.), our\ndataset differs signiﬁcantly from typical Web search logs, such as those collected by\nGoogle or Yahoo, which represent a broader demographic and a greater variety of user\nneeds. However, these differences are not the subject of this article.\nOur work has two main goals: First, we wish to better understand how users interact\nwith PubMed (Sects. 3, 5). Second, we wish to build computational models of user–system\ninteractions as a ﬁrst step to improving the search experience. This article explores the idea\nthat users’ actions can be encoded as a string sequence and modeled usingn-gram language\nmodels (Sect. 4). We evaluate these models both in terms of perplexity and in a sequence\nprediction task.\n2 PubMed transaction logs\nPubMed is the public gateway to MEDLINE, the authoritative repository of bibliographic\nrecords from the medical and biomedical primary literature, and more broadly, topics in the\nlife sciences ranging from biophysics to public health.\n1 Both resources are maintained by\nthe U.S. National Library of Medicine (NLM). As of March 2008, MEDLINE contains\nover 17.8 m records (called citations) dating back to 1949, all with basic bibliographic\ninformation. In addition, most records (especially those added in more recent decades)\ncontain abstract text, and increasingly, links to full text. Citations are added to MEDLINE\nat a rate of approximately 65k records a month. PubMed is frequently used by physicians,\nscientists (e.g., biologists, biochemists), and lay-people to solve complex tasks (particu-\nlarly the ﬁrst two groups). Although PubMed provides access to a wealth of resources (e.g.,\ngene databases, sequence information, etc.), this study focuses speciﬁcally on users’\ninteraction with MEDLINE.\nPubMed is a sophisticated Boolean search engine that allows users to query not only on\ntitle and abstract text, but also on metadata ﬁelds (e.g., journal or author) and using\ncontrolled vocabulary MeSH\n/C210 terms. PubMed also provides a number of pre-deﬁned\n‘‘search templates’’, which allow users to narrow the scope of the articles that are retrieved\n(Haynes et al. 1994). These ﬁlters are implemented as ﬁxed Boolean query fragments that\nthe system automatically appends to each query.\nThis work examines transaction logs gathered over an 8-day span, June 20–27, 2007.\nThe basic unit of analysis is the session, which is tracked through a browser cookie.\nSessions are comprised of transactions, each of which corresponds to a CGI invocation.\nDue to the nature of this tracking mechanism, a user who engages PubMed with multiple\nbrowser windows (or tabs) will show up as a single session, since there is no effective way\nto separate the source of the CGI requests. Note that our deﬁnition of a session is very\ncoarse-grained; we explore different segmentation techniques in Sect. 3.\n1 http://www.nlm.nih.gov/pubs/factsheets/medline.html.\n488 Inf Retrieval (2009) 12:487–503\n123\nThe logs contain a wealth of information, including timestamp and details of the CGI\ninvocation (encoded parameters), which allows us to reconstruct with reasonable accuracy\nthe actions of a particular user. Certain client-side actions, such as use of the browser\n‘‘back’’ button, are not captured, although it is possible to infer some of these behaviors.\nThis data provides a unique glimpse into the actions of PubMed users—although PubMed\nqueries have been previously studied (Herskovic et al. 2007), this work represents the ﬁrst\nsystematic analysis of session data. Although there have been previous studies of trans-\naction logs from search systems for the medical literature, e.g., studies on PaperChase\n(Horowitz et al. 1983; King 1991) and Grateful Med (Cahan 1989), our work involves\nsigniﬁcantly more data and users. It is our intention to make an appropriately-processed\nversion of this dataset available to the research community.\nIn addition to the domain, we note another unique characteristic of our logs. Whereas\nmost datasets studied by academic researchers contain only search queries (Jansen and\nSpink 2006), our logs capture all interactions between the user and PubMed. In fact, search\nqueries account for only 33% of user actions. A more complete record allows us to\nconstruct richer models of users’ activities and understand their browsing behaviors (see\nSect. 5).\nOver the 8-day span, we observed a total of 8.68 m sessions, totaling 41.8 m transac-\ntions (for clarity, we refer to this as the raw dataset). A number of ﬁlters were ﬁrst applied\nto discard sessions not of interest. We found that 63.2%, or 5.49 m sessions, consisted of\nonly one transaction—most of these represent direct access into MEDLINE, e.g., through\nan embedded link or another search engine; this number is consistent with previous studies\non Web search (Jansen and Spink 2006). Although single-transaction sessions account for a\nlarge portion of all trafﬁc, we discarded them since they do not represent interesting user\nbehavior. At the other end of the spectrum, we discarded all sessions with more than 500\ntransactions (an arbitrary threshold), since they were likely to be robots spidering the\ncollection. This removes only 162 sessions, but 271k transactions. Finally, we removed\nsessions in which the user was not primarily engaged with MEDLINE. In addition to\nbibliographic records, PubMed provides access to other databases (e.g., gene records),\nwhich are not of interest for this study. As a heuristic, we discarded sessions in which more\nthan half the transactions did not involve MEDLINE; this removed an additional 2.72 m\nsessions. Ultimately, we ended up with a dataset containing 476k sessions, 7.65 m\ntransactions; for clarity, we refer to this as the ﬁltered dataset. Although the size of the\ndataset after applying these ﬁlters is a fraction of the original raw dataset, we argue that the\nprocessing steps allow us to focus on ‘‘interesting’’ user behavior, at least for the purposes\nof this study. Note that similar ﬁltering is often applied to general Web query logs so that a\nresearcher can focus on phenomena of interest. For example, previous analyses (Rose and\nLevinson 2004; Broder 2002) have found that, depending on the actual dataset, up to a\nquarter of all queries are navigational in nature, and up to a third of all queries are intended\nto locate a particular resource (e.g., ﬁnding a currency converter or downloading a video\nclip) rather than to gather information.\nBuilding on Murray et al. ( 2006), the main idea explored in this work is to analyze user\nactivities with n-gram language models; cf. (Chen and Cooper 2002). This is accomplished\nby ﬁrst mapping classes of user actions into symbols drawn from a ﬁnite alphabet. Our\nencoding is shown in Table 1. Thus, sessions can be represented as sequences of symbols,\nand building computational models of users’ activities can be recast into a sequence\nmodeling problem. For reference, Table 1 also shows the distribution of the 7.65 m user\nactions in the ﬁltered dataset. The following are the beginnings of three sample sessions\nencoded according to our scheme:\nInf Retrieval (2009) 12:487–503 489\n123\nQNRRRRLRQNRQQQQQQ...\nQNQQQQQQQNQNQQQQN...\nQNNNNNQNRQVNRRQNR...\nEven disregarding details such as the actual query and the timestamp, it is possible to\nconstruct an understanding of user behavior in each of these cases. In the ﬁrst example, the\nsequence of R’s indicate that the user examined four MEDLINE citations on the same\nresult page. The long sequences of Q’s in the second session suggest that the user had\ndifﬁculty formulating a good PubMed query. The sequence of N’s in the third example\nindicates that the user looked at several pages of results without selecting a MEDLINE\ncitation to examine in detail, before ﬁnally giving up and issuing a different query.\n3 Session segmentation\nWe ﬁrst tackled the session segmentation problem. Note that there is no consistent deﬁ-\nnition of a ‘‘session’’ in the literature. For some researchers, sessions are semantic in\nnature. As an example, He and Go¨ker (2000) proposed that ‘‘the start and end of a session\nare the points where the role behind a query changes’’, which intimately ties the notion of a\nsession to information-seeking tasks. For others, sessions are non-semantic constructs,\ndeﬁned purely in terms of temporal inactivity (Silverstein et al. 1999) or in terms of the\ngranularity at which the log data is gathered (Jansen et al. 2000). We adopted the second\napproach, deﬁning sessions in terms of browser cookies. Despite differences in\nTable 1 Mapping of PubMed transactions into symbols representing different action types\nQuery: the user issued a PubMed query 2518955 32.9%\nRetrieve: the user clicked on a search result to display a MEDLINE record, which\ncontains bibliographic information and abstract text (in most cases). This view\nprovides a link to the full-text article if available, but these actions are not recorded in\nour logs. The view of the MEDLINE record also contains links to ﬁve related articles\n(see below)\n3036158 39.7%\nNext: the user requested the next page of search results. PubMed displays 20 hits per page 658214 8.6%\nRelated Link: the user clicked on a related article link. Whenever the user examines a\nMEDLINE record, the right panel of the browser window is automatically populated\nwith titles of the 5 related articles most similar in content to the one currently being\nexamined (Lin and Wilbur 2007). This feature allows the user to browse citations\nwithout explicitly issuing new queries (Lin et al. 2008)\n284974 3.7%\nMore links: the user requested more related articles. In the MEDLINE record view, only\nthe top 5 related articles are shown; however, more related articles can be requested via\na separate link\n52546 0.7%\nModify View: the user manipulated search results using advanced features in the\n‘ ‘Limits’ ’ tab of the PubMed interface. Options in that tab allow the user to restrict the\nretrieved set in terms of journal, author, availability, date, etc.\n515720 6.7%\nP (other MEDLINE): catch-all category for other MEDLINE-related actions. These\ninclude actions in the ‘‘Preview/Index’ ’, ‘‘History’ ’, ‘ ‘Clipboard’ ’, and ‘ ‘Details’ ’ tabs of\nthe PubMed interface, as well as clicks on the ‘ ‘LinkOut’ ’ resource supplied by external\nproviders\n287604 3.8%\nX (other PubMed): catch-all category for other actions not involving MEDLINE (e.g.,\naccess to gene records)\n291104 3.8%\nTotal 7645275\n490 Inf Retrieval (2009) 12:487–503\n123\nterminology, there is nevertheless consensus that user activities can be segmented into\n‘‘meaningful’’ units.\nTo reduce confusion, we use the term ‘‘episode’’ to refer to meaningful units of\nactivities within sessions. An episode might, for example, correspond to a speciﬁc infor-\nmation-seeking task. Although the identiﬁcation of these units is difﬁcult without\nunderstanding the semantics of users’ queries, temporal information alone provides sig-\nniﬁcant information (Catledge and Pitkow 1995; He and Goker 2000). That is, a system\ncould segment a sequence of user actions into episodes based on periods of inactivity. This\nmethod is extremely fast (a pre-requisite for online processing) and sidesteps thorny issues\nof query semantics. In addition, methods based purely on temporal information do not raise\nprivacy concerns, compared to methods that depend on the actual semantics of the user\naction (e.g., queries and clickthroughs).\nWhat should this inactivity threshold be? Naturally, we face a tradeoff: the longer the\nduration, the more likely we are to create episodes that span multiple, possibly unrelated,\ninformation-seeking tasks. On the other hand, shorter durations may yield incomplete\nepisode fragments. For example, if the user spends a long time reading a result page, and\nthen returns to continue browsing additional search results, the system might infer multiple\nepisodes if the threshold is too short.\nHowever, this tradeoff can be used to our advantage: in absence of ‘‘ground truth’’,\nsegmenting sessions into episodes based on different thresholds can serve as a probe into\nusers’ behavior, by analyzing the characteristics of the resulting episodes. We did exactly\nthis, segmenting sessions based on different inactivity thresholds, ranging from 5 to 60 min\nin 5 min increments. The line marked by diamonds in Fig. 1 denotes the total number of\nepisodes that is generated at each threshold. Naturally, smaller thresholds generate more\nepisodes. The line marked by ﬁlled squares represents the number of singleton episodes\n(i.e., episodes with only one transaction); the line marked by empty squares represents the\nnumber of singleton episodes consisting of a citation retrieval. The ratio between these two\nvalues is expressed as a percentage above the ﬁlled squares, i.e., the percentage of sin-\ngleton episodes where the user retrieved a MEDLINE record. Note that from the citation\nview, users can access available full text (these actions are not recorded in our logs). We\nsee that nearly 70% of all singleton episodes are retrievals.\nCombined with the observation that about 60% of episodes end with a retrieval\n(regardless of threshold), we infer that singleton retrieval episodes are likely to be an\nextension of the previous episode—since the user is directly accessing MEDLINE content.\nBy comparing these results with ﬁgures cited in Jansen and Spink (2004), we conclude that\nPubMed users spend a longer period of time examining content. This makes sense given\nthe more complex tasks that PubMed users typically engage in—for example, physicians\nsearching for clinical evidence in the context of patient care (De Groote and Dorsch 2003;\nHerskovic et al. 2007), biologists combing the literature for studies that link a particular\ngene to a disease (Hersh et al. 2005, 2007).\nFigure 2 shows the distribution of episodes in terms of two different measures of length:\nnumber of transactions (top) and duration (bottom). For both graphs, we show the results of\nsegmentation based on three representative thresholds: 5, 15, and 30 min. The plots for\neven longer thresholds are not substantially different from the plot for the 30-min\nthreshold, and unfortunately showing any more lines would result in too much clutter.\nDuration is measured as the time difference between the ﬁrst transaction of the episode and\nthe last transaction (thus singleton episodes have zero duration). The bottom graph shows\nduration binned in increments of 5 min, e.g., increment 6 corresponds to all durations\nbetween 25 and 30 min long. In terms of number of interactions, the median episode length\nInf Retrieval (2009) 12:487–503 491\n123\nis 3 with an inactivity threshold of 5 and 10 min; 4 with a threshold of 15, 20, 25, 30 min; 5\nwith any longer thresholds. In terms of duration, the median episode length is less than\n5 min for an inactivity threshold of 5, 10, 15, 20, 25, 30 min, rising to between 5 and\n10 min for any longer thresholds. In both cases, the means are much larger than the\nmedians since the distributions exhibit long tail characteristics.\nNote that a meaningful comparison between PubMed data and data from general-\npurpose Web search engines (e.g., Google or Yahoo) is difﬁcult, since most existing logs\navailable to academic researchers contain only queries; see overview in Jansen and Spink\n(2006). In the case of PubMed, we have shown that episode boundaries cannot be accu-\nrately delineated without records of citation retrievals, since PubMed users may spend\nsigniﬁcant time examining MEDLINE citations.\nThese results seem to suggest that the choice of threshold is perhaps not as signiﬁcant as\none might think. With the exception of the 5-min threshold (which seems too short given\nthe discussion above), the plots for episode length distribution don’t actually differ by\nmuch. In terms of transactions, longer thresholds are primarily reducing the number of\nsingleton episodes, which are mostly citation retrievals. This has the effect of appending an\nadditional ‘R’ symbol at the end of other sequences, and has a relatively minor effect for\nthe experiments we describe in subsequent sections. Similarly, different thresholds have\nrelatively minor impact on episode duration (disregarding the 5-min threshold).\nTo determine if there is any value in our simple notion of episodes and to better\nfacilitate subsequent computational modeling, we prepared another dataset, which we refer\nto as the episode dataset. From the original set of ﬁltered sessions we obtained a set of\nepisodes by ﬁrst segmenting the logs using a 30-min threshold. We then discarded the\nfollowing:\n• all singleton episodes (since they cannot contribute to the user action prediction task we\ndescribe in Sect. 4);\n 0\n 200\n 400\n 600\n 800\n 1000\n 1200\n 1400\n60 min55 min50 min45 min40 min35 min30 min25 min20 min15 min10 min5 min\nEpisode Count (thousands)\nInactivity Threshold\nAnalysis of Singleton Episodes\n65.5%\n67.2%\n68.1% 68.5% 68.9% 69.2% 69.4% 69.6% 69.9% 70.1% 70.2% 70.3%\nall episodes\nsingleton episodes (all actions)\nsingleton episodes (abstract retrieval)\nFig. 1 Characteristics of episodes generated by applying different thresholds to segment sessions. Values\nabove each ﬁlled square indicate the percentage of singleton episodes that consists of a single retrieve action\n492 Inf Retrieval (2009) 12:487–503\n123\n• all episodes that do not involve MEDLINE, i.e., consisting exclusively of the symbol\n‘X’ (since our study focuses on searching MEDLINE);\n• all episodes that do not begin with a query (since they often represent direct access into\nMEDLINE, i.e., through another search engine, and we are primarily interested in\nstudying search with PubMed).\nFrom the 476k sessions in the ﬁltered dataset, we ended up with 373k episodes containing\n4.34 m transactions. For convenience, a summary of all datasets mentioned in this article is\nshown in Table 2.\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 1  2  3  4  5  6  7  8  9  10\nFraction\nEpisode Length (number of transactions)\nDistribution of Episode Length from PubMed: Number of Transactions\nsegmentation with 5 min. threshold\nsegmentation with 15 min. threshold\nsegmentation with 30 min. threshold\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0  2  4  6  8  10  12\nFraction\nEpisode Length (duration in intervals of 5 minutes)\nDistribution of Episode Length from PubMed: Duration\nsegmentation with 5 min. threshold\nsegmentation with 15 min. threshold\nsegmentation with 30 min. threshold\nFig. 2 Distribution of episode length, in terms of number of transactions (top) and duration (bottom).\nDuration is binned in 5 min intervals (e.g., ‘5’ represents intervals between 20 and 25 min)\nInf Retrieval (2009) 12:487–503 493\n123\n4 Modeling user activities\nOne advantage of encoding user actions as sequences of symbols is the ability to use\nstandard natural language processing techniques to build computational models of user\nactivity. In this work, we experimented with n-gram language models. Language models\ndeﬁne a probability distribution over string sequences:\nPðw\n1w2w3... wn/C0 1wnÞ/C17 Pðwn\n1Þð 1Þ\nIn language processing, this typically means sequences of words; see Manning and\nSchu¨tze (1999) for an overview. In our application, we are modeling sequences of symbols,\neach of which represent a user action. By the chain rule of probability theory:\nPðwn\n1Þ¼ Pðw1ÞPðw2jw1ÞPðw3jw1w2Þ... Pðwnjw1w2... wn/C0 1Þð 2Þ\n¼ Pðw1ÞPðw2jw1ÞPðw3jw2\n1Þ... Pðwnjwn/C0 1\n1 Þð 3Þ\n¼\nYn\nk¼1\nPðwkjwk/C0 1\n1 Þð 4Þ\nDue to the extremely large number of parameters involved in estimating such a model,\nit is customary to make the Markov assumption, that the sequence histories only depend on\nprior local context. That is, an n-gram language model is equivalent to a ( n - 1)-order\nMarkov model. Thus, we can approximate P(wk|w1\nk-1) as follows:\nbigrams: Pðwkjwk/C0 1\n1 Þ/C25 Pðwkjwk/C0 1Þð 5Þ\ntrigrams: Pðwkjwk/C0 1\n1 Þ/C25 Pðwkjwk/C0 1wk/C0 2Þð 6Þ\nn-grams: Pðwkjwk/C0 1\n1 Þ/C25 Pðwkjwk/C0 1\nk/C0 nþ1Þð 7Þ\nIn this work, we used the SRI Language Modeling Toolkit (Stolcke 2002), a standard\npackage commonly used by the computational linguistics and speech processing com-\nmunities. Standard settings were used: Good-Turing discounting (Chen and Goodman\n1996) and Katz backoff for smoothing (Katz 1987). For building the language models, we\nused both the ﬁltered session data and the episode data, as summarized in Table 2. The\ndatasets were divided in the following manner: for the ﬁltered session dataset, a 400/76k\ntraining/test split; for the episode dataset, a 300/73k split. We varied the order of n-grams\nfrom 2-gram (bigrams) up to 8-gram.\nIn our ﬁrst experiment, we built language models on training data and then evaluated\nthese models on held-out test data. Cross entropy is frequently used as a metric:\n1\nNT\nXn\ni¼1\n/C0 log2 PmðtiÞð 8Þ\nTable 2 Summary of the three\ndatasets referenced in this article Dataset Size Brief description\nRaw 8.68 m sessions, 41.8 m trans. Unprocessed logs\nFiltered 476k sessions, 7.65 m trans. Filtered sessions\nEpisode 373k episodes, 4.34 m trans. Filtered episodes\n494 Inf Retrieval (2009) 12:487–503\n123\nwhere Pm(ti) denotes the probability assigned by language model m and where the test data\nT is composed of sequences ( t1, t2,…,tn) containing a total of NT symbols. The cross\nentropy is inversely related to the average probability a model assigns to sentences in the\ntest data, and it is accepted that lower entropy models are preferable. Following common\npractices of the speech community, we report perplexity instead of cross entropy ( H),\nwhere perplexity is 2\nH. One advantage is that the measure has an intuitive reading: a\nperplexity of k means that one is as surprised on average as one would have been guessing\nfrom k equiprobable choices at each symbol in the sequence.\nResults for the perplexity experiments are shown in Fig. 3 for both ﬁltered session data\nand episode data. We see that perplexity does decrease with higher-order models, although\nit appears that nothing is gained beyond 6-grams, and perplexity actually increases slightly\ndue to the increasing sparsity of higher-order n-grams. Nevertheless, these results do\nsuggest that there is regularity in sequences of user actions, and that this regularity can be\nmodeled by taking into account history of previous actions.\nIn our second experiment, we applied the language models to a sequence prediction\ntask. That is, after observing the user’s current history of actions, can the model ‘‘guess’’\nwhat the user is going to do next? The prediction is performed as follows: given a sequence\nof actions, the system generates eight sequences, one corresponding to each possible next\nuser action. These sequences are then scored by the language model; the user action that\ngenerates the most probable sequence is then predicted.\nThe experimental runs were structured in the following manner: a sequence of n user\nactions (either from a single session or episode, depending on the dataset) is divided into\nn - 1 trials. At each trial, the system’s task is to predict the next symbol, provided the\nhistory. Note that this experimental setup places higher-order n-gram models at a disad-\nvantage, since for the ﬁrst few predictions of any session or episode, information learned\nfrom higher-order n-grams cannot be exploited. However, this procedure does have the\nadvantage that the number of trials is constant across all models.\n 3\n 3.5\n 4\n 4.5\n 5\n8-gram7-gram6-gram5-gram4-gram3-gram2-gram\nPerplexity\nLanguage Model\nPerplexity of N-Gram Language Models\nSession data\nEpisode data\nFig. 3 Perplexity of session and episode test data on different n-gram language models\nInf Retrieval (2009) 12:487–503 495\n123\nExperiments were conducted using the language models built from the training data.\nFrom the session test data, we selected 10k sessions for testing prediction accuracy, which\nyielded 154,333 trials. From the episode data, we selected 20k episodes, which yielded\n213,107 trials. We measured the accuracy of the predictions by comparing system output\nwith the actual user actions—these results are shown in Fig. 4. The error bars denote the\n99% conﬁdence intervals, as computed by the Clopper-Pearson method for calculating\nexact binomial conﬁdence intervals (Clopper and Pearson 1934). Due to the large number\nof trials, the conﬁdence intervals are quite small, thus allowing us to discriminate small\n 0.35\n 0.4\n 0.45\n 0.5\n 0.55\n8-gram7-gram6-gram5-gram4-gram3-gram2-gram\nPrediction Accuracy\nLanguage Model\nUser Action Prediction Accuracy Using Different N-Gram Language Models (Sessions)\n 0.35\n 0.4\n 0.45\n 0.5\n 0.55\n8-gram7-gram6-gram5-gram4-gram3-gram2-gram\nPrediction Accuracy\nLanguage Model\nUser Action Prediction Accuracy Using Different N-Gram Language Models (Episodes)\nFig. 4 Accuracy of predicting next user action using differentn-gram language models: session data on top,\nepisode data on bottom. Solid line in each graph indicates baseline (most frequent class)\n496 Inf Retrieval (2009) 12:487–503\n123\ndifferences. In both graphs, the solid line indicates the baseline (frequency of most com-\nmon class: 0.388 for episode, 0.397 for sessions).\nResults from the prediction accuracy experiment are consistent with the perplexity\nevaluation. For both session and episode data, 5-gram models signiﬁcantly outperform\n4-gram and lower order models. The differences in prediction accuracy between 5-, 6-, 7-,\nand 8-gram models are not statistically signiﬁcant for both session and episode data. To\nfacilitate comparison, the scales of the bar graphs in Fig. 4 are the same. We can see that\nrelative to the baseline, prediction accuracy is slightly higher on episode data than it is on\nsession data: for example, 6-gram prediction accuracy for episodes is 0.503 or 30% above\nthe baseline, while 6-gram prediction accuracy for sessions is 0.508 or 28% above the\nbaseline. Recall that, by construction, the episode data contained more ‘‘interesting’’\nsequences of user activities—these results suggest that there is some value in the episodes\ngenerated by our simple session segmentation technique.\n5 Detecting activity collocates\nBy encoding users’ actions as string sequences, we can leverage well-known natural\nlanguage processing techniques to identify interesting patterns. Computational linguists\nhave devised a number of techniques for extracting collocations, or commonly-occurring\npatterns of words; see Pearce ( 2002) for a survey. Can such techniques be applied to help\nus better understand user behavior?\nFirst, what would collocations in activity sequences derived from our logs mean?\nCollocations in natural language are co-occurring words that appear more frequently than\none would expect by chance; often, their semantics are non-compositional (e.g., compound\nnouns, verb-particle constructions, etc.). In our context, collocations represent sequences\nof actions that are ‘‘surprising’’ and ‘‘meaningful’’. These ‘‘activity collocates’’ might\nencapsulate fragments of information-seeking strategies or ‘‘idioms’’ that PubMed\nsearchers have adopted, and analyzing such sequences could reveal interesting insights\nabout user behavior. Depending on the absolute frequency of occurrence, these informa-\ntion-seeking strategies might be relatively common, or exclusively used by a small\npopulation.\nTo ﬁnd activity collocates, we used Pointwise Mutual Information (Church and Hanks\n1990) to score n-grams from our language models:\nPMIða\n1; a2; ... ; anÞ¼ log pða1; a2; ... ; anÞ\npða1Þpða2Þ... pðanÞ\nThat is, PMI quantiﬁes the probability of seeing a particular sequence relative to the\nprobability that the individual symbols making up the sequence co-occurred randomly. We\nopted for PMI instead of more complex formulations such as the log likelihood ratio\n(Dunning 1993) since our alphabet is small and thus our modeling task is not plagued by\nproblems associated with rare events; cf. (Moore 2004).\nTable 3 shows the top ﬁve activity collocates in terms of PMI for 2-, 3-, 4-action\nsequences. Analysis was performed on the 400k training samples in the ﬁltered session\ndata, but the episode data gives rise to similar results. The table also shows the log\nprobability of each n-gram to quantify the prevalence of that particular pattern. For ref-\nerence, the most frequently-occurring patterns of activity are shown in Table 4. Note that\nInf Retrieval (2009) 12:487–503 497\n123\nin both cases, we discarded patterns that involved non-MEDLINE actions (the symbol\n‘X’), since the probabilities of those actions are distorted by our data preparation process.\nIn the PubMed interface, the detailed view of a MEDLINE citation (which contains\nbibliographic information, and in most cases, abstract text) is accompanied by ﬁve links to\nrelated articles, as suggested by a probabilistic content-similarity algorithm (Lin and\nWilbur 2007). Below the ﬁve titles is an option to see a longer list of related articles.\nAction ‘L’ represents a click on one of these suggested article titles; action ‘M’ represents a\nclick on the option to see a longer list of related articles. Overall, ‘L’ and ‘M’ represent\nTable 3 Top ﬁve 2-, 3-, and\n4-sequence activity collocates,\nranked by PMI value\nL = related link, M = more\nlinks, N = next, P = other\nMEDLINE\nSequence Count log p PMI\nL L 100447 -1.77 1.08\nL M 10778 -2.74 0.84\nP P 53218 -2.05 0.80\nN N 224020 -1.42 0.71\nM M 1258 -3.67 0.64\nL L L 55087 -2.00 2.27\nL L M 5337 -3.02 1.99\nMMM 1 6 2 -4.53 1.94\nP P P 23421 -2.37 1.90\nM R M 4225 -3.12 1.60\nMMMM 4 4 -5.07 3.56\nLLLL 33315 -2.19 3.51\nLLLM 3002 -3.24 3.20\nP P P P 14128 -2.57 3.14\nLMMM 3 3 -5.20 2.71\nTable 4 Top ﬁve most fre-\nquently-observed 2-, 3-, and\n4-sequences of actions, ranked by\nabsolute frequency\nQ = query, R = retrieve,\nN = next\nSequence Count log p PMI\nR R 1108739 -0.73 0.07\nQ Q 905413 -0.82 0.15\nQ R 729704 -0.91 -0.03\nR Q 670231 -0.95 -0.06\nN N 224020 -1.42 0.71\nR R R 605996 -0.96 0.24\nQ Q Q 497841 -1.05 0.40\nR Q R 281937 -1.29 -0.01\nQ R R 271682 -1.31 -0.03\nQ Q R 255293 -1.34 0.03\nR R R R 379589 -1.14 0.46\nQ Q Q Q 305904 -1.23 0.70\nQ R R R 124797 -1.62 0.06\nQ Q Q R 119846 -1.64 0.21\nR Q Q Q 109770 -1.68 0.17\n498 Inf Retrieval (2009) 12:487–503\n123\n3.7% and 0.7% of all transactions, respectively, yet most of the activity collocates involve\nthese two actions. That is, these sequences occur far more frequently than one would\nexpect by chance. Consecutive L’s indicate that the user was clicking on related article\nsuggestions repeatedly (navigating from citation to citation); consecutive M’s indicate that\nthe user was navigating from lists of related articles to other lists of related articles\n(essentially, using MEDLINE citations repeatedly as ‘‘queries’’ to retrieve lists of similar\ncitations).\nOur analysis suggests that browsing related articles (in its two forms) represents a\ndeliberate information-seeking strategy, or more generally, a distinct way of using Pub-\nMed. Furthermore, once users begin browsing the collection in this manner, they are likely\nto continue—as shown by the long sequences of ‘L’ and ‘M’ actions in Table 3. Com-\nplementary evidence presented in Lin et al. ( 2008) supports an even stronger claim, that\nthe feature is indeed useful and effective for information seeking. Through analysis of\ndocument networks connected via content-similarity links, Lin et al. showed that for\ntypical information needs, relevant documents tend to cluster together. Therefore, a user\ncan navigate from relevant document to relevant document via these links. Finally, these\nresults are consistent with previous studies that demonstrate the effectiveness of content-\nsimilarity browsing in simulated environments (Wilbur and Coffee 1994; Smucker and\nAllan 2006; Lin and Smucker 2008). Although similar features are available in many Web\nsearch engines, we are not aware of any published evidence regarding their effectiveness.\nThe other activity collocates are comprised of N’s (next page) and P’s (other MEDLINE\nactions). The ‘‘N N’’ sequence suggests that users are often persistent in examining the\nretrieved set—that is, they browse through at least two pages of PubMed results (each of\nwhich contains 20 citations). A natural explanation is the recall-oriented nature of typical\ntasks that PubMed users engage in, e.g., physicians searching for clinical evidence or\nbiologists searching for relevant literature. This behavior stands in contrast with general\nWeb search, where users are much more cursory in their consumption of search results. For\nexample, Jansen et al. ( 2000) observed that 58% of users don’t look past the ﬁrst page of\nresults (10 hits), and only around 10% of users view results past the fourth page.\nAs described in Table 1, the symbol ‘P’ serves as a catch-all category for otherwise\nuncategorized MEDLINE-related actions. These include actions in the ‘‘Preview/Index’’,\n‘‘History’’, ‘‘Clipboard’’, and ‘‘Details’’ tabs of the PubMed interface, as well as clicks on the\n‘‘LinkOut’’ resource supplied by external providers. Examination of the logs reveals that\nsequences of P’s represent activities of advanced PubMed users. For example, the ‘‘Preview/\nIndex’’ tab allows the user to see the number of hits that would be retrieved by a particular\nquery—an important feature for Boolean retrieval since the result set size is often difﬁcult to\ncontrol. The ‘‘History’’ tab allows the user to revisit previously-issued queries. Overall, such\nactions are rare (only 3.8% of all page views in the ﬁltered session dataset), but we observe\nconsecutive P’s having high PMI. As with sequences of consecutive L’s and M’s, we believe\nthat this represents a distinct mode of information seeking—for example, an advanced user\nencountering difﬁculty in choosing good search terms might switch to the ‘‘Preview/Index’’\ntab as a tool for assisting in the query formulation process.\nFocusing on Table 4, it is not surprising that the most frequent patterns of activity\ncomprise mostly of ‘Q’ (query) and ‘R’ (retrieve) actions. Together, they account for\nnearly three quarters of total transactions. Sequences of these two actions represent perhaps\nthe ‘‘core’’ of information-seeking behavior: issuing queries, examining results, and\nreformulating previous queries. We also note that activity collocates (i.e., those in Table 3)\naren’t necessarily rare in terms of absolute frequency. For example, ‘‘N N’’ is the 5th most\nfrequent 2-gram and ‘‘L L’’ is the 13th most frequent 2-gram.\nInf Retrieval (2009) 12:487–503 499\n123\nSome frequent n-grams actually have surprisingly high PMI values. For example, we\nobserve the prevalence of long query sequences. One possible interpretation is that at least\nsome users experience difﬁculty formulating Boolean queries. Since almost all commercial\nWeb search engines implement some sort of best-match algorithm, users have grown\naccustomed to using ranked retrieval systems. In contrast, the query formulation process in\nPubMed may feel quite foreign. In fact, based on analysis of a separate set of logs, we\nfound that approximately a ﬁfth of all PubMed queries return zero results. Related to these\nchallenges is the difﬁculty associated with controlling the result set size, which is another\ncharacteristic of Boolean retrieval. For example, adding an additional term to a query that\nretrieves 1000 hits might yield 0 hits (this helps to explain the usefulness of the ‘‘Preview/\nIndex’’ tab in the PubMed interface, for searchers who are aware of the functionality).\nThese issues point to query formulation aids as features that could potentially beneﬁt many\nPubMed users.\nThe sequences of consecutive repeated actions in both Tables 3 and 4 suggest an\ninteresting behavior pattern: once a user commits to an action, he or she is likely to\nrepeat the same action again. We can quantify this by computing P(L|L)/P(L); that is,\nhow much more likely is action ‘L’ to follow another action ‘L’ (compared to chance)?\nThe answer in this case is around 12 times more likely: ‘L’ follows another ‘L’ about\n45% of the time. We computed similar values for longer sequences and for other actions,\nthe results of which are shown in Fig. 5. Given the above discussion, the plots for action\n‘L’ and action ‘N’ (next result page) are not unexpected: once users begin browsing\nrelated articles or pages in the result set, they are likely to continue doing so. However,\nwe also observe the same effect for Query and Retrieve (albeit the effects are more\nminor): given a consecutive sequence of four ‘Q’ or ‘R’ actions, we are 2.07 and 1.86\nmore likely to see another ‘Q’ or ‘R’, respectively. This appears to support our char-\nacterization of PubMed users as ‘‘persistent’’, which makes sense given the complex\nnature of their information needs.\n 0\n 5\n 10\n 15\n 20\n 1  2  3  4\nRelative Likelihood of Same Action\nNumber of Consecutive Actions\nRelative Likelihood of Sequences of Consecutive Actions\nrelated Link\nQuery\nRetrieve\nNext\nFig. 5 Relative likelihood of observing a particular action after a consecutive sequence of the same action.\nFor example, the probability of ‘L’ followed by another ‘L’ is 12 times higher than expected by chance\n500 Inf Retrieval (2009) 12:487–503\n123\n6 Conclusion\nIn this work, we demonstrate that simple n-gram models can capture regularities in users’\nactivities, based only on a rough characterization of their actions. With these techniques,\nwe are able to identify activity collocates and characterize interesting patterns of behavior.\nOverall, the models are able to correctly predict the user’s next action about half the time.\nWe ﬁnd this to be promising, considering the impoverished input—no temporal infor-\nmation (beyond pre-segmented sequences), no information about the semantics of the\nuser’s actions (e.g., queries they issued, the contents of the pages they clicked on, etc.). Of\ncourse, models can be enriched by exactly such data to provide a more accurate picture of\nusers’ behavior. However, the biggest barrier to this is not technological, but rather one\nrelated to policy: concerns over invasion of privacy and release of personally-sensitive\ninformation. For a discussion, see Murray and Teevan ( 2007). In this respect, log-mining\ntechniques that are able to extract information from impoverished datasets are particularly\nvaluable, since they alleviate these concerns. For academic researchers, it is more likely\nthat such datasets can be shared.\nWhile user modeling may be interesting in itself, our ultimate goal is to exploit such\nmodels to improve the search experience. We believe that user models can be used in three\nmajor ways:\n• Predictively. In anticipation of what the user is likely to do next, the system can provide\ncustomized assistance. One might consider this as a generalization of query suggestion\ntechniques that have been proposed (Anick 2003; Cui et al. 2003). That is, in addition\nto suggesting related search terms, the system might offer helpful actions. For example,\nactivity collocates might be generalized into a library of ‘‘search strategies’’ and\npresented as search aids. For example, the system may suggest something like, ‘‘similar\nusers have found related article links to be helpful—perhaps you might want to\nconsider…’’\n• Retrospectively. Computational models can help systems automatically cluster and\nclassify user behavior. Results in Sect. 4 offer some possibilities for how this might be\naccomplished. The biggest application is user proﬁling and demographic modeling,\nwhich is highly relevant to many tasks ranging from personalized search (Eirinaki and\nVazirgiannis 2003; Shen et al. 2005) to targeted advertisement.\n• Prescriptively. That is, results of log-based studies could be used as a basis for\neducating users on effective search strategies. This is not an unrealistic scenario in the\ncontext of PubMed: due to the nature of its users and their work, PubMed searchers are\noften willing to learn effective search techniques and advanced features.\n2\nWe believe that this work represents an enabler for such advances. Transaction logs help\nus better understand how PubMed users search for information and provide a valuable\nresource for building computational models of user activities. We demonstrated how user\nactions can be encoded as string sequences and captured with n-gram language models.\nThe application of natural language processing techniques to tackle this information\nretrieval problem provides an example of how the two ﬁelds might productively\ncollaborate.\n2 Empirical evidence for this claim is demonstrated by the numerous tutorials and mini-courses offered on\nPubMed, as any casual Web search will reveal.\nInf Retrieval (2009) 12:487–503 501\n123\nAcknowledgments We’d like to thank three anonymous reviewers for their helpful comments. This work\nwas supported by the Intramural Research Program of the NIH, National Library of Medicine. The ﬁrst\nauthor would also like to thank Esther and Kiri for their kind support.\nOpen Access This article is distributed under the terms of the Creative Commons Attribution Noncom-\nmercial License which permits any noncommercial use, distribution, and reproduction in any medium,\nprovided the original author(s) and source are credited.\nReferences\nAgichtein, E., Brill, E., & Dumais, S. (2006). Improving Web search ranking by incorporating user behavior\ninformation. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2006) (pp. 19–26). Seattle, WA.\nAnick, P. (2003). Using terminological feedback for Web search reﬁnement—A log-based study. In Pro-\nceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2003) (pp. 88–95). Toronto, Canada.\nBeitzel, S. M., Jensen, E. C., Chowdhury, A., Grossman, D., & Frieder, O. (2004). Hourly analysis of a very\nlarge topically categorized Web query log. In Proceedings of the 27th Annual International ACM\nSIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004) (pp. 321–\n328). Shefﬁeld, UK.\nBroder, A. (2002). A taxonomy of Web search. SIGIR Forum, 36 (2), 3–10.\nCahan, M. A. (1989). GRATEFUL MED: A tool for studying searching behavior. Medical Reference\nServices Quarterly, 8 (4), 61–79.\nCatledge, L. D., & Pitkow, J. E. (1995). Characterizing browsing strategies in the World-Wide Web.\nComputer Networks and ISDN Systems, 27 (6),1065–1073.\nChen, H.-M., & Cooper, M. D. (2002). Stochastic modeling of usage patterns in a Web-based information\nsystem. Journal of the American Society for Information Science and Technology, 53 (7), 536–548.\nChen, S. F., & Goodman, J. (1996). An empirical study of smoothing techniques for language modeling. In\nProceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL 1996)\n(pp. 310–318). Santa Cruz, CA.\nChurch, K. W., & Hanks, P. (1990). Word association norms, mutual information, and lexicography.\nComputational Linguistics, 16 (1), 22–29.\nClopper, C. J., & Pearson, E. S. (1934). The use of conﬁdence or ﬁducial limits illustrated in the case of the\nbinomial. Biometrika, 26.404–413.\nCui, H., Wen, J.-R., Nie, J.-Y., & Ma, W.-Y. (2003). Query expansion by mining user logs. IEEE Trans-\nactions on Knowledge and Data Engineering, 15 (4), 829–839.\nDe Groote, S. L., & Dorsch, J. L. (2003). Measuring use patterns of online journals and databases.Journal of\nthe Medical Library Association, 91 (2), 231–240.\nDunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Lin-\nguistics, 19(1), 61–74.\nEirinaki, M., & Vazirgiannis, M. (2003). Web mining for Web personalization. ACM Transactions on\nInternet Technology, 3 (1), 1–27.\nHaynes, R. B., Wilczynski, N., McKibbon, K. A., Walker, C. J., & Sinclair, J. C. (1994). Developing optimal\nsearch strategies for detecting clinically sound studies in MEDLINE. Journal of the American Medical\nInformatics Association, 1 (6), 447–458.\nHe, D., & Go¨ker, A. (2000). Detecting session boundaries from Web user logs. In Proceedings of the BCS-\nIRSG 22nd Annual Colloquium on Information Retrieval Research (pp. 57–66). Cambridge, UK.\nHersh, W. R., Cohen, A., Ruslen, L., & Roberts, P. (2007). TREC 2007 Genomics Track overview. In\nProceedings of the Sixteenth Text REtrieval Conference (TREC 2007) . Gaithersburg, MD.\nHersh, W. R., Cohen, A., Yang, J., Bhupatiraju, R., Roberts, P., & Hearst, M. (2005). TREC 2005 Genomics\nTrack overview. In Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005) , Gai-\nthersburg, MD.\nHerskovic, J. R., Tanaka, L. Y., Hersh, W. R., & Bernstam, E. V. (2007). A day in the life of PubMed:\nAnalysis of a typical day’s query log.Journal of the American Medical Informatics Association, 14(2),\n212–220.\nHorowitz, G. L., Jackson, J. D., & Bleich, H. L. (1983). PaperChase. Self-service bibliographic retrieval.\nJAMA, 250(18), 2494–2499.\n502 Inf Retrieval (2009) 12:487–503\n123\nJansen, B. J., & Spink, A. (2004). An analysis of documents viewing patterns of Web search engine users. In\nA. Scime (Ed.), Web mining: Applications and techniques (pp. 339–354). Hershey, PA: IGI\nPublishing.\nJansen, B. J., & Spink, A. (2006). How are we searching the World Wide Web? A comparison of nine search\nengine transaction logs. Information Processing and Management, 42 (1), 248–263.\nJansen, B. J., Spink, A., & Saracevic, T. (2000). Real life, real users, and real needs: A study and analysis of\nuser queries on the Web. Information Processing and Management, 36 (2), 207–227.\nJoachims, T., Granka, L., Pan, B., Hembrooke, H., Radlinski, F., & Gay, G. (2007). Evaluating the accuracy\nof implicit feedback from clicks and query reformulations in Web search. ACM Transactions on\nInformation Systems, 25 (2), 1–27.\nKatz, S. M. (1987). Estimation of probabilities from sparse data for the language model component of a\nspeech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35 (3), 400–401.\nKing, N. S. (1991). Search characteristics and the effects of experience on end users of PaperChase. College\nand Research Libraries, 52 (4), 360–374.\nLin, J., DiCuccio, M., Grigoryan, V., & Wilbur, W. J. (2008). Navigating information spaces: A case study\nof related article search in PubMed. Information Processing & Management, 44 (5), 1771–1783.\nLin, J., & Smucker, M. D. (2008). How do users ﬁnd things with PubMed? Towards automatic utility\nevaluation with user simulations. In Proceedings of the 31st Annual International ACM SIGIR Con-\nference on Research and Development in Information Retrieval (SIGIR 2008) (pp. 19–26). Singapore.\nLin, J., & Wilbur, W. J. (2007). PubMed related articles: A probabilistic topic-based model for content\nsimilarity. BMC Bioinformatics, 8 , 423.\nManning, C. D., & Schu¨tze, H. (1999). Foundations of statistical natural language processing. Cambridge,\nMA: MIT Press.\nMoore, R. C. (2004). On log-likelihood-ratios and the signiﬁcance of rare events. InProceedings of the 2002\nConference on Empirical Methods in Natural Language Processing (EMNLP 2002) (pp. 333–340).\nBarcelona, Spain.\nMurray, G. C., Lin, J., & Chowdhury, A. (2006). Action modeling: Using language models to predict query\nbehavior. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2006) (pp. 681–682). Seattle, WA.\nMurray, G. C., & Teevan, J. (2007). Query log analysis: Social and technological challenges. SIGIR Forum,\n41(2), 112–120.\nPearce, D. (2002). A comparative evaluation of collocation extraction techniques. In Proceedings of the\nThird International Conference on Language Resources and Evaluation (LREC 2002) (pp. 1530–\n1536). Canary Islands, Spain.\nRose, D. E., & Levinson, D. (2004). Understanding user goals in Web search. In Proceedings of the\nThirteenth International World Wide Web Conference (WWW 2004) (pp. 13–19). New York, NY.\nShen, X., Tan, B., & ChengXiang, Z. (2005). Implicit user modeling for personalized search. InProceedings\nof the 2005 ACM CIKM International Conference on Information and Knowledge Management\n(pp. 824–831). Bremen, Germany.\nSilverstein, C., Marais, H., Henzinger, M., & Moricz, M. (1999). Analysis of a very large Web search engine\nquery log. SIGIR Forum, 33 (1), 6–12.\nSmucker, M. D., & Allan, J. (2006). Find-similar: Similarity browsing as a search tool. InProceedings of the\n29th Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 2006) (pp. 461–468). Seattle, WA.\nStolcke, A. (2002). SRILM—An extensible language modeling toolkit. In Proceedings of the 7th Inter-\nnational Conference on Spoken Language Processing (ICSLP 2002) (pp. 901–904). Denver, CO.\nWilbur, W. J., & Coffee, L. (1994). The effectiveness of document neighboring in search enhancement.\nInformation Processing and Management, 30 (2), 253–266.\nInf Retrieval (2009) 12:487–503 503\n123"
}