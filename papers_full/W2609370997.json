{
  "title": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling",
  "url": "https://openalex.org/W2609370997",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281674761",
      "name": "Kawakami, Kazuya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221974744",
      "name": "Dyer, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2900299007",
      "name": "Blunsom, Phil",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1517590677",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2469894155",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2409027918",
    "https://openalex.org/W2300605907",
    "https://openalex.org/W2952276042",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2154099718",
    "https://openalex.org/W2963304263",
    "https://openalex.org/W2964121744"
  ],
  "abstract": "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the \"bursty\" distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus, MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",
  "full_text": "arXiv:1704.06986v1  [cs.CL]  23 Apr 2017\nLearning to Create and Reuse W ords in\nOpen-V ocabulary Neural Language Modeling\nKazuya Kawakami ♠ Chris Dyer ♣ Phil Blunsom ♠♣\n♠Department of Computer Science, University of Oxford, Oxfo rd, UK\n♣DeepMind, London, UK\n{kazuya.kawakami,phil.blunsom}@cs.ox.ac.uk,cdyer@google.com\nAbstract\nFixed-vocabulary language models fail to\naccount for one of the most character-\nistic statistical facts of natural language:\nthe frequent creation and reuse of new\nword types. Although character-level lan-\nguage models offer a partial solution in\nthat they can create word types not at-\ntested in the training corpus, they do not\ncapture the “bursty” distribution of such\nwords. In this paper, we augment a hierar-\nchical LSTM language model that gener-\nates sequences of word tokens character by\ncharacter with a caching mechanism that\nlearns to reuse previously generated words.\nT o validate our model we construct a new\nopen-vocabulary language modeling cor-\npus (the Multilingual Wikipedia Corpus;\nMWC) from comparable Wikipedia arti-\ncles in 7 typologically diverse languages\nand demonstrate the effectiveness of our\nmodel across this range of languages.\n1 Introduction\nLanguage modeling is an important problem in\nnatural language processing with many practi-\ncal applications (translation, speech recognition,\nspelling autocorrection, etc.). Recent advances\nin neural networks provide strong representational\npower to language models with distributed repre-\nsentations and unbounded dependencies based on\nrecurrent networks (RNNs). However, most lan-\nguage models operate by generating words by sam-\npling from a closed vocabulary which is composed\nof the most frequent words in a corpus. Rare\ntokens are typically replaced by a special token,\ncalled the unknown word token, ⟨U N K⟩. Although\nﬁxed-vocabulary language models have some im-\nportant practical applications and are appealing\nmodels for study , they fail to capture two empir-\nical facts about the distribution of words in nat-\nural languages. First, vocabularies keep growing\nas the number of documents in a corpus grows:\nnew words are constantly being created (\nHeaps,\n1978). Second, rare and newly created words of-\nten occur in “bursts”, i.e., once a new or rare word\nhas been used once in a document, it is often re-\npeated (\nChurch and Gale , 1995; Church, 2000).\nThe open-vocabulary problem can be solved\nby dispensing with word-level models in favor of\nmodels that predict sentences as sequences of char-\nacters (\nSutskever et al. , 2011; Chung et al. , 2017).\nCharacter-based models are quite successful at\nlearning what (new) word forms look like (e.g.,\nthey learn a language’s orthographic conventions\nthat tell us that sustinated is a plausible English\nword and bzoxqir is not) and, when based on mod-\nels that learn long-range dependencies such as\nRNNs, they can also be good models of how words\nﬁt together to form sentences.\nHowever, existing character-sequence models\nhave no explicit mechanism for modeling the fact\nthat once a rare word is used, it is likely to be used\nagain. In this paper, we propose an extension to\ncharacter-level language models that enables them\nto reuse previously generated tokens (§\n2). Our\nstarting point is a hierarchical LSTM that has been\npreviously used for modeling sentences (word by\nword) in a conversation ( Sordoni et al. , 2015), ex-\ncept here we model words (character by character)\nin a sentence. T o this model, we add a caching\nmechanism similar to recent proposals for caching\nthat have been advocated for closed-vocabulary\nmodels (\nMerity et al. , 2017; Grave et al. , 2017).\nAs word tokens are generated, they are placed in\nan LRU cache, and, at each time step the model\ndecides whether to copy a previously generated\nword from the cache or to generate it from scratch,\ncharacter by character. The decision of whether\nto use the cache or not is a latent variable that\nis marginalised during learning and inference. In\nsummary , our model has three properties: it cre-\nates new words, it accounts for their burstiness us-\ning a cache, and, being based on LSTM s over\nword representations, it can model long range de-\npendencies.\nT o evaluate our model, we perform ablation ex-\nperiments with variants of our model without the\ncache or hierarchical structure. In addition to stan-\ndard English data sets (PTB and WikiT ext-2), we\nintroduce a new multilingual data set: the Multi-\nlingual Wikipedia Corpus (MWC), which is con-\nstructed from comparable articles from Wikipedia\nin 7 typologically diverse languages (§\n3) and show\nthe effectiveness of our model in all languages\n(§4). By looking at the posterior probabilities\nof the generation mechanism (language model vs.\ncache) on held-out data, we ﬁnd that the cache\nis used to generate “bursty” word types such as\nproper names, while numbers and generic content\nwords are generated preferentially from the lan-\nguage model (§\n5).\n2 Model\nIn this section, we describe our hierarchical char-\nacter language model with a word cache. As is typ-\nical for RNN language models, our model uses the\nchain rule to decompose the problem into incre-\nmental predictions of the next word conditioned\non the history:\np(w) =\n|w|∏\nt=1\np(wt | w<t).\nW e make two modiﬁcations to the traditional\nRNN language model, which we describe in turn.\nFirst, we begin with a cache-less model we call the\nhierarchical character language model ( HCLM;\n§\n2.1) which generates words as a sequence of\ncharacters and constructs a “word embedding” by\nencoding a character sequence with an LSTM\n(\nLing et al. , 2015). However, like conventional\nclosed-vocabulary , word-based models, it is based\non an LSTM that conditions on words represented\nby ﬁxed-length vectors.\n1\n1 The HCLM is an adaptation of the hierarchical recurrent\nencoder-decoder of Sordoni et al. (2015) which was used to\nmodel dialog as a sequence of actions sentences which are\nthemselves sequences of words. The original model was pro-\nposed to compose words into query sequences but we use it\nto compose characters into word sequences.\nThe HCLM has no mechanism to reuse words\nthat it has previously generated, so new forms will\nonly be repeated with very low probability . How-\never, since the HCLM is not merely generating\nsentences as a sequence of characters, but also\nsegmenting them into words, we may add a word-\nbased cache to which we add words keyed by the\nhidden state being used to generate them (§\n2.2).\nThis cache mechanism is similar to the model pro-\nposed by Merity et al. (2017).\nNotation. Our model assigns probabilities to se-\nquences of words w = w1, . . . , w |w|, where |w| is\nthe length, and where each word wi is represented\nby a sequence of characters ci = ci, 1, . . . , c i, |ci| of\nlength |ci|.\n2.1 Hierarchical Character-level Language\nModel ( HCLM)\nThis hierarchical model satisﬁes our linguistic in-\ntuition that written language has (at least) two dif-\nferent units, characters and words.\nThe HCLM consists of four components, three\nLSTMs (\nHochreiter and Schmidhuber , 1997): a\ncharacter encoder, a word-level context encoder,\nand a character decoder (denoted LSTMenc,\nLSTMctx , and LSTMdec , respectively), and a soft-\nmax output layer over the character vocabulary .\nFig.\n1 illustrates an unrolled HCLM.\nSuppose the model reads word wt−1 and pre-\ndicts the next word wt. First, the model reads the\ncharacter sequence representing the word wt−1 =\nct−1, 1, . . . , c t−1, |ct− 1| where |ct−1| is the length\nof the word generated at time t − 1 in charac-\nters. Each character is represented as a vector\nvct− 1,1 , . . . , vct− 1,|ct− 1| and fed into the encoder\nLSTMenc . The ﬁnal hidden state of the encoder\nLSTMenc is used as the vector representation of\nthe previously generated word wt−1,\nhenc\nt = LSTMenc(vct− 1,1 , . . . , vct− 1,|ct|).\nThen all the vector representations of words\n(vw1 , . . . , vw|w|) are processed with a context\nLSTMctx . Each of the hidden states of the context\nLSTMctx are considered representations of the his-\ntory of the word sequence.\nhctx\nt = LSTMctx (henc\n1 , . . . , henc\nt )\nFinally , the initial state of the decoder LSTM\nis set to be hctx\nt and the decoder LSTM reads a\nvector representation of the start symbol v⟨S⟩ and\nP  o  k  é  m  o  n  </s> \nThe Pokémon Company International (formerly Pokémon  USA Inc.), a subsidiary of Japan's Pokémon Co., oversees all Pokémon licensing … \nC  o  m  p  a  n  y  </s> …. (      f     o     r    m    e    r    l    y    </s> \nCache rt\n. . . . . . . .\n<s> P o k é m o n\nP o k é m o n </s> \nh enc \nt\nh ctx \nt\nw t− 1\nw t\np(Pok´ emon) = λtplm (Pok´ emon) + (1 − λt)pptr (Pok´ emon) \nu t\nλtpptr (Pok´ emon) plm (Pok´ emon) \nFigure 1: Description of Hierarchical Character Language M odel with Cache.\ngenerates the next word wt+1 character by charac-\nter. T o predict the j-th character in wt, the decoder\nLSTM reads vector representations of the previ-\nous characters in the word, conditioned on the con-\ntext vector hctx\nt and a start symbol.\nhdec\nt,j = LSTMdec (vct,1 , . . . , vct,j− 1 , hctx\nt , v⟨S⟩).\nThe character generation probability is deﬁned\nby a softmax layer for the corresponding hidden\nrepresentation of the decoder LSTM .\np(ct,j | w<t, ct,<j ) = softmax(Wdec hdec\nt,j + bdec)\nThus, a word generation probability from\nHCLM is deﬁned as follows.\nplm(wt | w<t) =\n|ct|∏\nj=1\np(ct,j | w<t, ct,<j )\n2.2 Continuous cache component\nThe cache component is an external memory\nstructure which store K elements of recent his-\ntory . Similarly to the memory structure used in\nGrave et al. (2017), a word is added to a key-value\nmemory after each generation of wt. The key at\nposition i ∈ [1, K ] is ki and its value mi. The\nmemory slot is chosen as follows: if the wt ex-\nists already in the memory , its key is updated (dis-\ncussed below). Otherwise, if the memory is not\nfull, an empty slot is chosen or the least recently\nused slot is overwritten. When writing a new word\nto memory , the key is the RNN representation that\nwas used to generate the word ( ht) and the value is\nthe word itself ( wt). In the case when the word al-\nready exists in the cache at some position i, the ki\nis updated to be the arithmetic average of ht and\nthe existing ki.\nT o deﬁne the copy probability from the cache\nat time t, a distribution over copy sites is deﬁned\nusing the attention mechanism of\nBahdanau et al.\n(2015). T o do so, we construct a query vector ( rt)\nfrom the RNN’s current hidden state ht,\nrt = tanh(Wqht + bq),\nthen, for each element i of the cache, a ‘copy\nscore, ’ ui,t is computed,\nui,t = vT tanh(Wuki + rt).\nFinally , the probability of generating a word via\nthe copying mechanism is:\npmem(i | ht) = softmaxi(ut)\npptr(wt | ht) =pmem(i | ht)[mi = wt],\nwhere [mi = wt] is 1 if the ith value in memory\nis wt and 0 otherwise. Since pmem deﬁnes a distri-\nbution of slots in the cache, pptr translates it into\nword space.\n2.3 Character-level Neural Cache Language\nModel\nThe word probability p(wt | w<t) is deﬁned as\na mixture of the following two probabilities. The\nﬁrst one is a language model probability , plm (wt |\nw<t) and the other is pointer probability , pptr(wt |\nw<t). The ﬁnal probability p(wt | w<t) is\nλtplm(wt | w<t) + (1− λt)pptr (wt | w<t),\nwhere λt is computed by a multi-layer perceptron\nwith two non-linear transformations using ht as its\ninput, followed by a transformation by the logistic\nsigmoid function:\nγt = MLP(ht), λ t = 1\n1 − e−γt\n.\nW e remark that Grave et al. (2017) use a clever\ntrick to estimate the probability , λt of drawing\nfrom the LM by augmenting their (closed) vocab-\nulary with a special symbol indicating that a copy\nshould be used. This enables word types that are\nhighly predictive in context to compete with the\nprobability of a copy event. However, since we\nare working with an open vocabulary , this strategy\nis unavailable in our model, so we use the MLP\nformulation.\n2.4 T raining objective\nThe model parameters as well as the character pro-\njection parameters are jointly trained by maximiz-\ning the following log likelihood of the observed\ncharacters in the training corpus,\nL = −\n∑\nlog p(wt | w<t).\n3 Datasets\nW e evaluate our model on a range of datasets, em-\nploying preexisting benchmarks for comparison to\nprevious published results, and a new multilingual\ncorpus which speciﬁcally tests our model’s perfor-\nmance across a range of typological settings.\n3.1 Penn T ree Bank (PTB)\nW e evaluate our model on the Penn Tree Bank.\nFor fair comparison with previous works, we fol-\nlowed the standard preprocessing method used\nby\nMikolov et al. (2010). In the standard prepro-\ncessing, tokenization is applied, words are lower-\ncased, and punctuation is removed. Also, less fre-\nquent words are replaced by unknown an token\n(UNK),\n2 constraining the word vocabulary size to\nbe 10k. Because of this preprocessing, we do not\nexpect this dataset to beneﬁt from the modeling in-\nnovations we have introduced in the paper. Fig.\n1\nsummarizes the corpus statistics.\nTrain Dev T est\nCharacter types 50 50 48\nW ord types 10000 6022 6049\nOOV rate - 0.00% 0.00%\nW ord tokens 0.9M 0.1M 0.1M\nCharacters 5.1M 0.4M 0.4M\nT able 1: PTB Corpus Statistics.\n3.2 WikiT ext-2\nMerity et al. (2017) proposed the WikiT ext-2 Cor-\npus as a new benchmark dataset. 3 They pointed\nout that the preprocessed PTB is unrealistic for\nreal language use in terms of word distribution.\nSince the vocabulary size is ﬁxed to 10k, the\nword frequency does not exhibit a long tail. The\nwikiT ext-2 corpus is constructed from 720 articles.\nThey provided two versions. The version for word\nlevel language modeling was preprocessed by dis-\ncarding infrequent words. But, for character-level\nmodels, they provided raw documents without any\nremoval of word or character types or lowercas-\ning, but with tokenization. W e make one change\nto this corpus: since Wikipedia articles make ex-\ntensive use of characters from other languages; we\nreplaced character types that occur fewer than 25\ntimes were replaced with a dummy character (this\nplays the role of the ⟨U N K⟩ token in the character\nvocabulary). T ab.\n2 summarizes the corpus statis-\ntics.\n3.3 Multilingual Wikipedia Corpus (MWC)\nLanguages differ in what word formation pro-\ncesses they have. For character-level modeling\nit is therefore interesting to compare a model’s\nperformance across languages. Since there is at\npresent no standard multilingual language model-\ning dataset, we created a new dataset, the Mul-\n2 When the unknown token is used in character-level\nmodel, it is treated as if it were a normal word (i.e. UNK is\nthe sequence U, N, and K). This is somewhat surprising mod-\neling choice, but it has become conventional (\nChung et al. ,\n2017).\n3 http://metamind.io/research/the-wikitext-long-term-\nTrain Dev T est\nCharacter types 255 128 138\nW ord types 76137 19813 21109\nOOV rate - 4.79% 5.87%\nW ord tokens 2.1M 0.2M 0.2M\nCharacters 10.9M 1.1M 1.3M\nT able 2: WikiT ext-2 Corpus Statistics.\ntilingual Wikipedia Corpus (MWC), a corpus of\nthe same Wikipedia articles in 7 languages which\nmanifest a range of morphological typologies. The\nMWC contains English (EN), French (FR), Span-\nish (ES), German (DE), Russian (RU), Czech (CS),\nand Finnish (FI).\nT o attempt to control for topic divergences\nacross languages, every language’s data consists\nof the same articles. Although these are only com-\nparable (rather than true translations), this ensures\nthat the corpus has a stable topic proﬁle across lan-\nguages.\n4\nConstruction & Preprocessing W e constructed\nthe MWC similarly to the WikiT ext-2 corpus. Ar-\nticles were selected from Wikipedia in the 7 target\nlanguages. T o keep the topic distribution to be ap-\nproximately the same across the corpora, we ex-\ntracted articles about entities which explained in\nall the languages. W e extracted articles which ex-\nist in all languages and each consist of more than\n1,000 words, for a total of 797 articles. These\ncross-lingual articles are, of course, not usually\ntranslations, but they tend to be comparable. This\nﬁltering ensures that the topic proﬁle in each lan-\nguage is similar. Each language corpus is approxi-\nmately the same size as the WikiT ext-2 corpus.\nWikipedia markup was removed with WikiEx-\ntractor,\n5 to obtain plain text. W e used the\nsame thresholds to remove rare characters in the\nWikiT ext-2 corpus. No tokenization or other nor-\nmalization (e.g., lowercasing) was done.\nStatistics After the preprocessing described\nabove, we randomly sampled 360 articles. The ar-\nticles are split into 300, 30, 30 sets and the ﬁrst 300\narticles are used for training and the rest are used\n4 The Multilingual Wikipedia Corpus\n(MWC) is available for download from\nhttp://k-kawakami.com/research/mwc\n5 https://github.com/attardi/wikiextractor\nfor dev and test respectively . T able 3 summarizes\nthe corpus statistics.\nAdditionally , we show in Fig. 2 the distribution\nof frequencies of OOV word types (relative to the\ntraining set) in the dev +test portions of the corpus,\nwhich shows a power-law distribution, which is ex-\npected for the burstiness of rare words found in\nprior work. Curves look similar for all languages\n(see Appendix\nA).\nFigure 2: Histogram of OOV word frequencies in\nthe dev +test part of the MWC Corpus (EN).\n4 Experiments\nW e now turn to a series of experiments to show\nthe value of our hierarchical character-level cache\nlanguage model. For each dataset we trained the\nmodel with LSTM units. T o compare our results\nwith a strong baseline, we also train a model with-\nout the cache.\nModel Conﬁguration For HCLM and HCLM\nwith cache models, W e used 600 dimensions for\nthe character embeddings and the LSTMs have\n600 hidden units for all the experiments. This\nkeeps the model complexity to be approximately\nthe same as previous works which used an LSTM\nwith 1000 dimension. Our baseline LSTM have\n1000 dimensions for embeddings and reccurence\nweights.\nFor the cache model, we used cache size 100\nin every experiment. All the parameters includ-\ning character projection parameters are randomly\nsampled from uniform distribution from −0. 08 to\n0. 08. The initial hidden and memory state of\nLSTMenc and LSTMctx are initialized with zero.\nMini-batches of size 25 are used for PTB experi-\nments and 10 for WikiT ext-2, due to memory lim-\nitations. The sequences were truncated with 35\nChar. T ypes W ord T ypes OOV rate T okens Characters\nTrain V alid T est Train V alid T est V alid T est Train V alid T est Train V alid T est\nEN 307 160 157 193808 38826 35093 6.60% 5.46% 2.5M 0.2M 0.2M 15 .6M 1.5M 1.3M\nFR 272 141 155 166354 34991 38323 6.70% 6.96% 2.0M 0.2M 0.2M 12 .4M 1.3M 1.6M\nDE 298 162 183 238703 40848 41962 7.07% 7.01% 1.9M 0.2M 0.2M 13 .6M 1.2M 1.3M\nES 307 164 176 160574 31358 34999 6.61% 7.35% 1.8M 0.2M 0.2M 11 .0M 1.0M 1.3M\nCS 238 128 144 167886 23959 29638 5.06% 6.44% 0.9M 0.1M 0.1M 6. 1M 0.4M 0.5M\nFI 246 123 135 190595 32899 31109 8.33% 7.39% 0.7M 0.1M 0.1M 6. 4M 0.7M 0.6M\nRU 273 184 196 236834 46663 44772 7.76% 7.20% 1.3M 0.1M 0.1M 9. 3M 1.0M 0.9M\nT able 3: Summary of MWC Corpus.\nwords. Then the words are decomposed to char-\nacters and fed into the model. A Dropout rate of\n0. 5 was used for all but the recurrent connections.\nLearning The models were trained with the\nAdam update rule (\nKingma and Ba , 2015) with a\nlearning rate of 0.002. The maximum norm of the\ngradients was clipped at 10.\nEvaluation W e evaluated our models with bits-\nper-character (bpc) a standard evaluation metric\nfor character-level language models. Following\nthe deﬁnition in\nGraves (2013), bits-per-character\nis the average value of − log2 p(wt | w<t) over\nthe whole test set,\nbpc = − 1\n|c| log2 p(w),\nwhere |c| is the length of the corpus in characters.\n4.1 Results\nPTB T ab.\n4 summarizes results on the\nPTB dataset. 6 Our baseline HCLM model\nachieved 1.276 bpc which is better performance\nthan the LSTM with Zoneout regulariza-\ntion (\nKrueger et al. , 2017). And HCLM with\ncache outperformed the baseline model with\n1.247 bpc and achieved competitive results with\nstate-of-the-art models with regularization on\nrecurrence weights, which was not used in our\nexperiments.\nExpressed in terms of per-word perplexity (i.e.,\nrather than normalizing by the length of the corpus\nin characters, we normalize by words and expo-\nnentiate), the test perplexity on HCLM with cache\nis 94.79. The performance of the unregularized\n2-layer LSTM with 1000 hidden units on word-\nlevel PTB dataset is 114.5 and the same model\nwith dropout achieved 87.0. Considering the fact\n6 Models designated with a * have more layers and more\nparameters.\nthat our character-level models are dealing with an\nopen vocabulary without unknown tokens, the re-\nsults are promising.\nMethod Dev T est\nCW -RNN ( Koutnik et al. , 2014) - 1.46\nHF-MRNN ( Mikolov et al. , 2012) - 1.41\nMI-RNN ( Wu et al. , 2016) - 1.39\nME n-gram ( Mikolov et al. , 2012) - 1.37\nRBN ( Cooijmans et al. , 2017) 1.281 1.32\nRecurrent Dropout ( Semeniuta et al. , 2016) 1.338 1.301\nZoneout ( Krueger et al. , 2017) 1.362 1.297\nHM-LSTM ( Chung et al. , 2017) - 1.27\nHyperNetwork ( Ha et al. , 2017) 1.296 1.265\nLayerNorm HyperNetwork ( Ha et al. , 2017) 1.281 1.250\n2-LayerNorm HyperLSTM ( Ha et al. , 2017)* - 1.219\n2-Layer with New Cell ( Zoph and Le , 2016)* - 1.214\nLSTM (Our Implementation) 1.369 1.331\nHCLM 1.308 1.276\nHCLM with Cache 1.266 1.247\nT able 4: Results on PTB Corpus (bits-per-\ncharacter). HCLM augmented with a cache ob-\ntains the best results among models which have\napproximately the same numbers of parameter as\nsingle layer LSTM with 1,000 hidden units.\nWikiT ext-2 T ab.\n5 summarizes results on the\nWikiT ext-2 dataset. Our baseline, LSTM achieved\n1.803 bpc and HCLM model achieved 1.670 bpc.\nThe HCLM with cache outperformed the base-\nline models and achieved 1.500 bpc. The word\nlevel perplexity is 227.30, which is quite high\ncompared to the reported word level baseline re-\nsult 100.9 with LSTM with ZoneOut and V aria-\ntional Dropout regularization (\nMerity et al. , 2017).\nHowever, the character-level model is dealing with\n76,136 types in training set and 5.87% OOV rate\nwhere the word level models only use 33,278 types\nwithout OOV in test set. The improvement rate\nover the HCLM baseline is 10.2% which is much\nhigher than the improvement rate obtained in the\nPTB experiment.\nMethod Dev T est\nLSTM 1.758 1.803\nHCLM 1.625 1.670\nHCLM with Cache 1.480 1.500\nT able 5: Results on WikiT ext-2 Corpus .\nMultilingual Wikipedia Corpus (MWC)\nT ab. 6 summarizes results on the MWC dataset.\nSimilarly to WikiT ext-2 experiments, LSTM is\nstrong baseline. W e observe that the cache mecha-\nnism improve performance in every languages. In\nEnglish, HCLM with cache achieved 1.538 bpc\nwhere the baseline is 1.622 bpc. It is 5.2% im-\nprovement. For other languages, the improvement\nrates were 2.7%, 3.2%, 3.7%, 2.5%, 4.7%, 2.7%\nin FR, DE, ES, CS, FI, RU respectively . The best\nimprovement rate was obtained in Finnish.\n5 Analysis\nIn this section, we analyse the behavior of pro-\nposed model qualitatively . T o analyse the model,\nwe compute the following posterior probability\nwhich tell whether the model used the cache given\na word and its preceding context. Let zt be a ran-\ndom variable that says whether to use the cache or\nthe LM to generate the word at time t. W e would\nlike to know , given the text w, whether the cache\nwas used at time t. This can be computed as fol-\nlows:\np(zt | w) = p(zt, w t | ht, cachet)\np(wt | ht, cachet)\n= (1 − λt)pptr(wt | ht, cachet)\np(wt | ht, cachet) ,\nwhere cache t is the state of the cache at time\nt. W e report the average posterior probability of\ncache generation excluding the ﬁrst occurrence of\nw,\np(z | w).\nT ab. 7 shows the words in the WikiT ext-2 test\nset that occur more than 1 time that are most/least\nlikely to be generated from cache and character\nlanguage model (words that occur only one time\ncannot be cache-generated). W e see that the model\nuses the cache for proper nouns: Lesnar, Gore,\netc., as well as very frequent words which always\nstored somewhere in the cache such as single-\ntoken punctuation, the, and of. In contrast, the\nmodel uses the language model to generate num-\nbers (which tend not to be repeated): 300, 770\nand basic content words: sounds, however, unable,\netc. This pattern is similar to the pattern found in\nempirical distribution of frequencies of rare words\nobserved in prior wors (\nChurch and Gale , 1995;\nChurch, 2000), which suggests our model is learn-\ning to use the cache to account for bursts of rare\nwords.\nT o look more closely at rare words, we also in-\nvestigate how the model handles words that oc-\ncurred between 2 and 100 times in the test set, but\nfewer than 5 times in the training set. Fig.\n3 is a\nscatter plot of p(z | w) vs the empirical frequency\nin the test set. As expected, more frequently re-\npeated words types are increasingly likely to be\ndrawn from the cache, but less frequent words\nshow a range of cache generation probabilities.\nFigure 3: A verage p(z | w) of OOV words in test\nset vs. term frequency in the test set for words not\nobsered in the training set. The model prefers to\ncopy frequently reused words from cache compo-\nnent, which tend to names (upper right) while char-\nacter level generation is used for infrequent open\nclass words (bottom left).\nT ab.\n8 shows word types with the highest and\nlowest average p(z | w) that occur fewer than\n5 times in the training corpus. The pattern here\nis similar to the unﬁltered list: proper nouns are\nextremely likely to have been cache-generated,\nwhereas numbers and generic (albeit infrequent)\ncontent words are less likely to have been.\n6 Discussion\nOur results show that the HCLM outperforms a ba-\nsic LSTM. With the addition of the caching mech-\nanism, the HCLM becomes consistently more\npowerful than both the baseline HCLM and the\nLSTM. This is true even on the PTB, which\nhas no rare or OOV words in its test set (because\nof preprocessing), by caching repetitive common\nEN FR DE ES CS FI RU\ndev test dev test dev test dev test dev test dev test dev test\nLSTM 1.793 1.736 1.669 1.621 1.780 1.754 1.733 1.667 2.191 2.155 1 .943 1.913 1.942 1.932\nHCLM 1.683 1.622 1.553 1.508 1.666 1.641 1.617 1.555 2.070 2.035 1 .832 1.796 1.832 1.810\nHCLM with Cache 1.591 1.538 1.499 1.467 1.605 1.588 1.548 1.4 98 2.010 1.984 1.754 1.711 1.777 1.761\nT able 6: Results on MWC Corpus (bits-per-character).\nW ord p(z |w) ↓ W ord p(z |w) ↑\n. 0.997 300 0.000\nLesnar 0.991 act 0.001\nthe 0.988 however 0.002\nNY 0.985 770 0.003\nGore 0.977 put 0.003\nBintulu 0.976 sounds 0.004\nNerva 0.976 instead 0.005\n, 0.974 440 0.005\nUB 0.972 similar 0.006\nNero 0.967 27 0.009\nOsbert 0.967 help 0.009\nKershaw 0.962 few 0.010\nManila 0.962 110 0.010\nBoulter 0.958 Jersey 0.011\nStevens 0.956 even 0.011\nRifenburg 0.952 y 0.012\nArjona 0.952 though 0.012\nof 0.945 becoming 0.013\n31B 0.941 An 0.013\nOlympics 0.941 unable 0.014\nT able 7: W ord types with the highest/lowest av-\nerage posterior probability of having been copied\nfrom the cache while generating the test set. The\nprobability tells whether the model used the cache\ngiven a word and its context. Left: Cache is\nused for frequent words ( the, of ) and proper nouns\n(Lesnar, Gore). Right: Character level generation\nis used for basic words and numbers.\nwords such as the. In true open-vocabulary set-\ntings (i.e., WikiT ext-2 and MWC), the improve-\nments are much more pronounced, as expected.\nComputational complexity . In comparison\nwith word-level models, our model has to read\nand generate each word character by character,\nand it also requires a softmax over the entire\nmemory at every time step. However, the com-\nputation is still linear in terms of the length of\nthe sequence, and the softmax over the memory\ncells and character vocabulary are much smaller\nthan word-level vocabulary . On the other hand,\nsince the recurrent states are updated once per\ncharacter (rather than per word) in our model,\nthe distribution of operations is quite different.\nDepending on the hardware support for these\noperations (repeated updates of recurrent states\nW ord p(z |w) ↓ W ord p(z |w) ↑\nGore 0.977 770 0.003\nNero 0.967 246 0.037\nOsbert 0.967 Lo 0.074\nKershaw 0.962 Pitcher 0.142\n31B 0.941 Poets 0.143\nKirby 0.935 popes 0.143\nCR 0.926 Y ap 0.143\nSM 0.924 Piso 0.143\nimpedance 0.923 consul 0.143\nBlockbuster 0.900 heavyweight 0.143\nSuperfamily 0.900 cheeks 0.154\nAmos 0.900 loser 0.164\nSteiner 0.897 amphibian 0.167\nBacon 0.893 squads 0.167\nﬁlters 0.889 los 0.167\nLim 0.889 Keenan 0.167\nSelfridge 0.875 sculptors 0.167\nﬁlter 0.875 Gen. 0.167\nLockport 0.867 Kipling 0.167\nGermaniawerft 0.857 T abasco 0.167\nT able 8: Same as T able 7, except ﬁltering for word\ntypes that occur fewer than 5 times in the training\nset. The cache component is used as expected even\non rare words: proper nouns are extremely likely\nto have been cache-generated, whereas numbers\nand generic content words are less likely to have\nbeen; this indicates both the effectiveness of the\nprior at determining whether to use the cache and\nthe burstiness of proper nouns.\nvs. softmaxes), our model may be faster or\nslower. However, our model will have fewer\nparameters than a word-based model since most\nof the parameters in such models live in the word\nprojection layers, and we use LSTMs in place of\nthese.\nNon-English languages. For non-English lan-\nguages, the pattern is largely similar for non-\nEnglish languages. This is not surprising since\nmorphological processes may generate forms that\nare related to existing forms, but these still have\nslight variations. Thus, they must be generated by\nthe language model component (rather than from\nthe cache). Still, the cache demonstrates consis-\ntent value in these languages.\nFinally , our analysis of the cache on English\ndoes show that it is being used to model word\nreuse, particularly of proper names, but also of\nfrequent words. While empirical analysis of rare\nword distributions predicts that names would be\nreused, the fact that cache is used to model fre-\nquent words suggests that effective models of lan-\nguage should have a means to generate common\nwords as units. Finally , our model disfavors copy-\ning numbers from the cache, even when they are\navailable. This suggests that it has learnt that\nnumbers are not generally repeated (in contrast to\nnames).\n7 Related W ork\nCaching language models were proposed to\naccount for burstiness by\nKuhn and De Mori\n(1990), and recently , this idea has been in-\ncorporated to augment neural language models\nwith a caching mechanism ( Merity et al. , 2017;\nGrave et al. , 2017).\nOpen vocabulary neural language models have\nbeen widely explored ( Sutskever et al. , 2011;\nMikolov et al. , 2012; Graves, 2013, inter alia ).\nAttempts to make them more aware of word-\nlevel dynamics, using models similar to our\nhierarchical formulation, have also been pro-\nposed (\nChung et al. , 2017).\nThe only models that are open vocabulary lan-\nguage modeling together with a caching mech-\nanism are the nonparametric Bayesian language\nmodels based on hierarchical Pitman–Y or pro-\ncesses which generate a lexicon of word types us-\ning a character model, and then generate a text\nusing these (\nT eh, 2006; Goldwater et al. , 2009;\nChahuneau et al. , 2013). These, however, do not\nuse distributed representations on RNNs to capture\nlong-range dependencies.\n8 Conclusion\nIn this paper, we proposed a character-level lan-\nguage model with an adaptive cache which selec-\ntively assign word probability from past history\nor character-level decoding. And we empirically\nshow that our model efﬁciently model the word\nsequences and achieved better perplexity in every\nstandard dataset. T o further validate the perfor-\nmance of our model on different languages, we\ncollected multilingual wikipedia corpus for 7 typo-\nlogically diverse languages. W e also show that our\nmodel performs better than character-level models\nby modeling burstiness of words in local context.\nThe model proposed in this paper assumes the\nobservation of word segmentation. Thus, the\nmodel is not directly applicable to languages, such\nas Chinese and Japanese, where word segments\nare not explicitly observable. W e will investigate\na model which can marginalise word segmentation\nas latent variables in the future work.\nAcknowledgements\nW e thank the three anonymous reviewers for their\nvaluable feedback. The third author acknowledges\nthe support of the EPSRC and nvidia Corporation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Y oshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. ICLR .\nV ictor Chahuneau, Noah A. Smith, and Chris Dyer.\n2013. Knowledge-rich morphological priors for\nbayesian language models. In Proc. NAACL .\nJunyoung Chung, Sungjin Ahn, and Y oshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In Proc. ICLR .\nKenneth W Church. 2000. Empirical estimates of adap-\ntation: the chance of two Noriegas is closer to p/ 2\nthan p2. In Proc. COLING .\nKenneth W Church and William A Gale. 1995. Poisson\nmixtures. Natural Language Engineering 1(2):163–\n190.\nTim Cooijmans, Nicolas Ballas, César Laurent, Ça ˘glar\nGülçehre, and Aaron Courville. 2017. Recurrent\nbatch normalization. In Proc. ICLR .\nSharon Goldwater, Thomas L Grifﬁths, and Mark John-\nson. 2009. A Bayesian framework for word segmen-\ntation: Exploring the effects of context. Cognition\n112(1):21–54.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a con-\ntinuous cache. In Proc. ICLR .\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nDavid Ha, Andrew Dai, and Quoc V Le. 2017. Hyper-\nnetworks. In Proc. ICLR .\nHarold Stanley Heaps. 1978. Information retrieval:\nComputational and theoretical aspects . Academic\nPress, Inc.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation 9(8):1735–\n1780.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proc. ICLR .\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juer-\ngen Schmidhuber. 2014. A clockwork RNN. In\nProc. ICML .\nDavid Krueger, T egan Maharaj, János Kramár, Moham-\nmad Pezeshki, Nicolas Ballas, Nan Rosemary Ke,\nAnirudh Goyal, Y oshua Bengio, Hugo Larochelle,\nAaron Courville, et al. 2017. Zoneout: Regulariz-\ning rnns by randomly preserving hidden activations.\nIn Proc. ICLR .\nRoland Kuhn and Renato De Mori. 1990. A cache-\nbased natural language model for speech recogni-\ntion. IEEE transactions on pattern analysis and ma-\nchine intelligence 12(6):570–583.\nW ang Ling, Tiago Luís, Luís Marujo, Ramón Fernan-\ndez Astudillo, Silvio Amir, Chris Dyer, Alan W\nBlack, and Isabel Trancoso. 2015. Finding function\nin form: Compositional character models for open\nvocabulary word representation. In Proc. EMNLP .\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proc. ICLR .\nT omas Mikolov, Martin Karaﬁát, Lukas Burget, Jan\nCernock `y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proc. In-\nterspeech.\nT omáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSon Le, Stefan Kombrink, and Jan Cernocky.\n2012. Subword language modeling with neu-\nral networks. preprint (http://www . ﬁt. vutbr .\ncz/imikolov/rnnlm/char . pdf) .\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt\nBarth. 2016. Recurrent dropout without memory\nloss. In Proc. COLING .\nAlessandro Sordoni, Y oshua Bengio, Hossein V ahabi,\nChristina Lioma, Jakob Grue Simonsen, and Jian-\nY un Nie. 2015. A hierarchical recurrent encoder-\ndecoder for generative context-aware query sugges-\ntion. In Proc. CIKM .\nIlya Sutskever, James Martens, and Geoffrey E Hin-\nton. 2011. Generating text with recurrent neural net-\nworks. In Proc. ICML .\nY ee Whye T eh. 2006. A hierarchical Bayesian lan-\nguage model based on Pitman-Y or processes. In\nProc. ACL .\nY uhuai Wu, Saizheng Zhang, Y ing Zhang, Y oshua Ben-\ngio, and Ruslan R Salakhutdinov. 2016. On multi-\nplicative integration with recurrent neural networks.\nIn Proc. NIPS .\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578 .\nA Corpus Statistics\nFig.\n4 show distribution of frequencies of OOV\nword types in 6 languages.\nFR DE \nES CS \nFI RU \nFigure 4: Histogram of OOV word frequencies in MWC Corpus in d ifferent languages.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.866133451461792
    },
    {
      "name": "Vocabulary",
      "score": 0.7243703603744507
    },
    {
      "name": "Natural language processing",
      "score": 0.7077937722206116
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6641420125961304
    },
    {
      "name": "Reuse",
      "score": 0.6547530889511108
    },
    {
      "name": "Character (mathematics)",
      "score": 0.6537443399429321
    },
    {
      "name": "Word (group theory)",
      "score": 0.6504548788070679
    },
    {
      "name": "Language model",
      "score": 0.6113225817680359
    },
    {
      "name": "Natural language",
      "score": 0.4405999481678009
    },
    {
      "name": "Artificial neural network",
      "score": 0.436326265335083
    },
    {
      "name": "Linguistics",
      "score": 0.2920083999633789
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ]
}