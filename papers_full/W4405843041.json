{
  "title": "Evaluation of the integration of retrieval-augmented generation in large language model for breast cancer nursing care responses",
  "url": "https://openalex.org/W4405843041",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2231488110",
      "name": "Ruiyu Xu",
      "affiliations": [
        "Binzhou Medical University",
        "Ningbo No. 2 Hospital",
        "Binzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2129525717",
      "name": "Ying Hong",
      "affiliations": [
        "Ningbo No. 2 Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2098245490",
      "name": "Feifei Zhang",
      "affiliations": [
        "Ningbo No. 2 Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2110665591",
      "name": "Hongmei Xu",
      "affiliations": [
        "Binzhou University",
        "Binzhou Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2231488110",
      "name": "Ruiyu Xu",
      "affiliations": [
        "Binzhou Medical University",
        "Ningbo No. 2 Hospital",
        "Ningbo No.6 Hospital",
        "Binzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2129525717",
      "name": "Ying Hong",
      "affiliations": [
        "Ningbo No.6 Hospital",
        "Ningbo No. 2 Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2098245490",
      "name": "Feifei Zhang",
      "affiliations": [
        "Ningbo No.6 Hospital",
        "Ningbo No. 2 Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2110665591",
      "name": "Hongmei Xu",
      "affiliations": [
        "Binzhou Medical University",
        "Binzhou University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205724345",
    "https://openalex.org/W4309564159",
    "https://openalex.org/W4211032970",
    "https://openalex.org/W4395453679",
    "https://openalex.org/W4399803256",
    "https://openalex.org/W4323347432",
    "https://openalex.org/W4384699745",
    "https://openalex.org/W4378782046",
    "https://openalex.org/W4362458614",
    "https://openalex.org/W4385564356",
    "https://openalex.org/W4367174347",
    "https://openalex.org/W4376116452",
    "https://openalex.org/W4376872023",
    "https://openalex.org/W4399305008",
    "https://openalex.org/W4399211335",
    "https://openalex.org/W4398182105",
    "https://openalex.org/W4211069901",
    "https://openalex.org/W4395050972",
    "https://openalex.org/W4393388987",
    "https://openalex.org/W4394661002",
    "https://openalex.org/W4399054577",
    "https://openalex.org/W4386830080",
    "https://openalex.org/W4387583347",
    "https://openalex.org/W4390347789",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4385933294"
  ],
  "abstract": null,
  "full_text": "Evaluation of the integration of \nretrieval-augmented generation \nin large language model for breast \ncancer nursing care responses\nRuiyu Xu1,2, Ying Hong2, Feifei Zhang2 & Hongmei Xu1\nBreast cancer is one of the most common malignant tumors in women worldwide. Although large \nlanguage models (LLMs) can provide breast cancer nursing care consultation, inherent hallucinations \ncan lead to inaccurate responses. Retrieval-augmented generation (RAG) technology can improve \nLLM performance, offering a new approach for clinical applications. In the present study, we evaluated \nthe performance of a LLM in breast cancer nursing care using RAG technology. In the control group \n(GPT-4), questions were answered directly using the GPT-4 model, whereas the experimental group \n(RAG-GPT) used the GPT-4 model combined with RAG. A knowledge base for breast cancer nursing \nwas created for the RAG-GPT group, and 15 of 200 real-world clinical care questions were answered \nrandomly. The primary endpoint was overall satisfaction, and the secondary endpoints were accuracy \nand empathy. RAG-GPT included a curated knowledge base related to breast cancer nursing care, \nincluding textbooks, guidelines, and traditional Chinese therapy. The RAG-GPT group showed \nsignificantly higher overall satisfaction than that of the GPT-4 group (8.4 ± 0.84 vs. 5.4 ± 1.27, p < 0.01) \nas well as an improved accuracy of responses (8.6 ± 0.69 vs. 5.6 ± 0.96, p < 0.01). However, there was \nno inter-group difference in empathy (8.4 ± 0.85 vs. 7.8 ± 1.22, p > 0.05). Overall, this study revealed \nthat RAG technology could improve LLM performance significantly, likely because of the increased \naccuracy of the answers without diminishing empathy. These findings provide a theoretical basis for \napplying RAG technology to LLMs in clinical nursing practice and education.\nKeywords Breast cancer nursing care, Large language models, Retrieval-augmented generation, GPT-4, \nChatGPT, Nurse\nBreast cancer is one of the most common malignant tumors in women worldwide 1,2. Nursing care plays an \nimportant role in patient recovery 1,3. Traditionally, consultations with specialized nursing teams in hospitals \nhave provided the most precise information; however, this approach is not always convenient to implement. With \nadvances in artificial intelligence, generative large language models (LLMs) have shown significant promise in \nnursing4,5. Technologies such as ChatGPT are versatile, comprehensive, and intelligent, and can operate around \nthe clock without fatigue. As such, they have significant potential for application in nursing practice, research, \nand education6–8. However, recent research has identified many limitations of LLMs, and their application in the \nmedical field remains highly debated9,10. These issues include the generation of irrelevant answers, inaccuracies, \nand lack of explanatory ability9,11. Because medical questions have implications for human health and life, LLM \nresponses require extremely high accuracy. Therefore, “unstable and/or inaccurate” answers are major obstacles \nto the use of LLMs in clinical settings12,13.\nRetrieval-augmented generation (RAG) technology is a technique 14 that can enhance the accuracy of the \nresponse capabilities of LLMs by integrating local knowledge bases 15,16. This technology has demonstrated \nsignificant potential in clinical applications 16. Therefore, the present study developed a local knowledge base \nfocused on breast cancer to evaluate the application of RAG technology to the LLM for breast cancer nursing \ncare question and answer (Q&A) systems. The overall performance of RAG-enhanced LLMs was evaluated, \nproviding a theoretical foundation for their future application in clinical nursing practice and education.\n1College of Nursing, Binzhou Medical University, Yantai, Shandong Province, China. 2Department of Breast Surgery, \nNingbo No. 2 Hospital, Ningbo, Zhejiang Province, China. email: hmx58@163.com\nOPEN\nScientific Reports |        (2024) 14:30794 1| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports\n\nMethods\nThis study used GPT-4 (OpenAI) as the foundational model, as our research team has significant experience \nin using the ChatGPT-3.5 API. Two groups were established in this study: a control group, utilizing GPT-4 \nfor direct question answering, and an experimental group (RAG-GPT), employing a BGE-embedding RAG \nmodel17 for embedding, which performed semantic matching retrieval on the input questions. The retrieved \nlocal knowledge was integrated into the input question before being sent to the GPT-4 system (Fig. 1).\nHardware platform\nThe platform was based on Windows 11, with a computer equipped with an Intel Core i9 CPU, 64 GB of flash \nmemory, an NVIDIA 3090 GPU, a 4 TB mechanical hard drive, and a 512 GB solid-state drive. The software was \nprogrammed in Python 3.11, and the base model was assessed using the GPT-4 API provided by OpenAI. The \nRAG model was run using CUDA 11.8, in conjunction with a GPU.\nQ&A dataset\nA Q&A evaluation dataset for breast cancer nursing care was collected from publicly available datasets and \nnurses at a tertiary hospital. A total of 200 Q&A pairs were included in this study following manual selection \nby two nurses with more than 10 years of work experience, through the application of stringent inclusion and \nexclusion criteria. During the final evaluation, 15 questions were selected using a computer randomization \nmethod (LZZ, Python software). Each question was sent to the GPT-4 and RAG-GPT groups, and the final \nresponses were recorded.\nThe inclusion criteria were as follows: (1) questions related to breast cancer medical care, nursing, or \nrehabilitation and (2) questions with accurate answers. The exclusion criteria were as follows: (1) questions that \nviolate ethical or legal standards; (2) questions with obvious biases or conflicts of interest; and (3) questions \ninvolving complex conditions and multiple severe organ diseases.\nDirect GPT-4 question-answering\nIn the direct GPT-4 group, users input questions directly into the GPT-4. The system automatically recorded \nboth the questions and final answers. The System Prompt (“Please act as a nurse specialist in breast cancer to \nassist me.“) for GPT-4 was consistent with that for the RAG-GPT group. Default settings were used for all other \nparameters, without any modifications.\nLocal breast cancer knowledge base\nWe retrieved and collected data for Chinese breast cancer cases from various online sources ( www.bing.com, \nwww.wanfang.com.cn, www.cnki.net, and www.baidu.com), including texts related to breast cancer medical \ncare, nursing, and rehabilitation. These data sources included breast cancer textbooks, research papers, clinical \nguidelines, and traditional Chinese medicine knowledge.\nFig. 1. Flowchart showing the RAG-GPT and GPT question-answering processes.\n \nScientific Reports |        (2024) 14:30794 2| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports/\nTwo clinical nurses manually reviewed the materials, compared and removed duplicates, and extracted the \nrelevant breast cancer knowledge. In cases of conflicting or inconsistent data, a third nurse provided the final \nadjudication. The search terms were as follows: (Breast Cancer AND (Nursing OR Diagnosis AND Treatment \nOR Rehabilitation OR Traditional Chinese Medicine OR Guidelines OR Textbooks)).\nRAG process\nWe utilized the BGE model17, a free and open-source pretrained model that supports both Chinese and English \nvector embeddings, for the RAG-GPT group. Initially, breast cancer knowledge-based texts obtained in the \nprevious step were sliced into paragraphs. Each paragraph was vectorized using the BGE model to create a vector \nretrieval database.\nThe RAG model was configured using a cosine similarity threshold of 0.8. If the similarity score exceeded this \nthreshold, the matched content from the knowledge base was considered valid. Otherwise, a warning message \nstating “No valid information retrieved for this question” was output.\nRAG-GPT question-answering\nDuring the question-answering procedure, when a user input a question, the BGE model retrieved semantically \nrelevant information from a local knowledge base. The top five retrieved pieces of information were then sent to \nGPT-4 and combined with the users’ questions to generate a response. The program automatically recorded both \nthe questions and final answers. The system prompt was “Please act as a nurse specialist in breast cancer to assist \nme. ” All other parameters were maintained at default settings without modification.\nEvaluation\nThree senior nurses with > 10 years of clinical experience (XRY , HY , and XHM) independently evaluated the 15 \nQ&A results from the GPT-4 and RAG-GPT groups. The primary evaluation endpoint was overall satisfaction, \nand the secondary endpoints were accuracy and empathy. The assessment used a Likert scale in which each \nendpoint was rated with a continuous numerical value from 1 to 10, with 1 indicating “very dissatisfied, ” 5 \nrepresenting “neutral, ” and 10 denoting “very satisfied. ” The evaluators assigned numerical scores between 1 and \n10 for each endpoint.\nStatistical analysis\nAs this study did not involve any human or animal research, and the ChatGPT API is freely accessible online, no \nethical committee approval was required. Word-frequency statistics for the knowledge base were described using \nPython and visualized using the Matplotlib library. For endpoint measures, normally distributed measurement \ndata are presented as the means ± standard deviations. Comparisons between groups were conducted using two \nindependent sample t-tests. Data with skewed distributions are presented as medians and interquartile ranges. \nStatistical significance was set at p < 0.05. All statistical analyses were performed using GraphPad software \n(version 7).\nResults\nLocal knowledge base\nAfter filtering and removing duplicates, approximately 60,000 relevant words were retained. The database \nprimarily incorporated key documents such as the “Breast Cancer Diagnosis and Treatment Guidelines (China \n2022), ” “Expert Consensus on the Diagnosis and Treatment of Y oung Breast Cancer in China (2022), ” and \n“Guidelines for Follow-up and Health Management of Breast Cancer in China (2022 Edition). ” The keyword \ndistribution in the database showed that the most frequent terms included breast cancer, treatment, neoplasm, \nexamination, and organization (Fig.  2). The embedding time for the local database was approximately 2 min, \nwhereas query retrieval required only 0.01 s, on average.\nOverall satisfaction evaluation\nFifteen questions were tested, for which the GPT model provided answers (Table  1, Supplementary Table 1). \nThree nurses with over 10 years of clinical experience evaluated all question-answer pairs. The evaluation of \noverall satisfaction indicated that RAG-GPT outperformed the direct GPT-4 group (8.4 ± 0.84 vs. 5.4 ± 1.27, \np < 0.01) (Table 2; Fig. 3).\nAccuracy and empathy\nThe RAG-GPT model demonstrated a significantly higher accuracy after integrating the local knowledge base \ndata than the GPT-4 group (8.6 ± 0.69 vs. 5.6 ± 0.96, p < 0.01). However, there was no statistically significant \ndifference between the two groups for the secondary outcome measure of empathy (8.4 ± 0.85 vs. 7.8 ± 1.22, \np > 0.05) (Table 2; Fig. 3).\nDiscussion\nBreast cancer is one of the most common malignant tumors in women worldwide, and its treatment and care in \nclinical settings are challenging. In this study, we evaluated the application of RAG with LLM technology in breast \ncancer care for Q&A, compared with direct GPT-4. The results indicated that RAG technology can enhance the \noverall satisfaction with LLM responses to breast cancer care questions. Further evaluation indicated that LLMs \nutilizing RAG technology significantly improved response accuracy without compromising empathy.\nDue to advances in medicine, patients require increased personalized nursing care and rehabilitation during \ntreatment1,18. However, healthcare professionals are often too busy to spend time contacting patients to address \nScientific Reports |        (2024) 14:30794 3| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports/\nIndex Query GPT-4 RAG-GPT\n1 Does all breast cancer chemotherapy \ncause hair loss? Not. Not all breast cancer chemotherapy causes hair loss.\n2 What to eat after breast cancer \nchemotherapy to recover quickly?\nPay attention to a balanced diet during the \nrecovery period, and eat more foods rich \nin protein, vitamins and minerals, such as \nfish, lean meat, beans, vegetables and fruits. \nAt the same time, avoid eating spicy and \nstimulating foods and greasy foods.\nAfter breast cancer chemotherapy, you can eat foods high in protein, vitamins, \nand trace elements to accelerate wound healing. At the same time, avoid foods \ncontaining carcinogens, foods containing estrogen and growth hormones, and \nhigh-fat diets. In addition, do not drink alcohol and drink less coffee. According \nto the theory of traditional Chinese medicine dialectics, the foods that breast \ncancer patients are encouraged to take can be divided into two categories: \nsupporting the body and eliminating evil.\n3 What happens if breast cancer is not \ntreated? Breast cancer will be fatal if left untreated. Untreated breast cancer can lead to disease progression and worsening.\n4 Can the breast cancer chemotherapy \ncycle be changed? Cannot be changed. The cycle of breast cancer chemotherapy can be adjusted according to the \nspecific condition of the patient.\n5 What is TNM staging of breast \ncancer?\nBreast cancer TNM staging is a system used \nto assess the severity of breast cancer.\nBreast cancer TNM staging is used to assess the stage of breast cancer based on \nthe size and extent of the primary tumor (pT), the presence or absence of lymph \nnode involvement (pN), and the presence or absence of distant metastases (pM).\n……\n15 Can Chinese medicine be taken \nduring breast cancer chemotherapy? Can drink. Can drink, but must go to the hospital to check the condition and prescribe \nmedication before drinking.\nTable 1. The questions and corresponding answers.\n \nFig. 2. Frequency distribution of the top 15 keywords in the local breast cancer knowledge base, with “breast \ncancer treatment” identified as the most frequently mentioned term.\n \nScientific Reports |        (2024) 14:30794 4| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports/\ntheir needs. LLMs are currently the most advanced artificial intelligence (AI) technologies available, and have \nshown significant potential for use in medical consultations. However, they have many limitations, including \nhallucinations and outdated training data, which can lead to “unprofessional and unstable” responses to medical \nquestions9,10. Retraining these models is costly; therefore, improving the accuracy of LLM responses to breast \ncancer care questions at a minimal cost remains a significant research challenge19,20.\nTo the best of our knowledge, this is the first study to use RAG technology to improve the response of LLMs to \nbreast cancer care. Previous studies have explored RAG technology in the medical field21,22, showing significant \nimprovements in LLM performance. As technology advances, tools such as LangChain and GraphRAG and the \nuse of RAG technology in medicine could become easier in the future23.\nSatisfaction is defined as a deeply personal sense of ‘contentment’ or ‘agreement. ’ Achieving satisfaction \namong clinical workers is crucial before implementing LLMs in clinical practice. The present study focused \non the satisfaction of senior clinical nurses with LLM responses and showed that the integration of RAG \ntechnology led to higher satisfaction among nurses compared with that for direct-GPT4. Guo et al. found that \nLLMs provided significantly higher satisfaction in answering thymic issues than Junior and Senior specialists24. \nGomez-Cabello et al. further suggested that LLMs could achieve higher patient satisfaction by offering timely \nand accurate services 25. However, the present study demonstrated greater satisfaction using LLMs combined \nwith RAG than using LLMs alone.\nAccurate question answering is crucial in the medical field. For example, patients with breast cancer \nrequire precise and timely information to achieve effective recovery. The present study demonstrated that the \nintegration of RAG technology using a local knowledge base significantly improved the accuracy of the GPT \nresponses. Similarly, Kresevic et al. showed that RAG technology could significantly enhance LLM performance \nin liver disease, increasing the accuracy from 43–99% 21. Similarly, another study has shown that agent-based \nRAG technology with a brain rehabilitation knowledge base significantly improves LLM responses 22. In \notolaryngology, RAG technology has been shown to reduce the error rate of LLM responses by 58.4% 26. These \nresults are consistent with those of the present study.\nEmpathy reflects the professional competency of nurses. Empathetic verbal comfort can be considered a \nform of clinical treatment. In this study, RAG technology not only enhanced responses based on specialized \nknowledge but also maintained the empathetic quality of answers. Many studies have highlighted empathy as a \ncrucial focus in the use of LLMs in healthcare 27,28. Ayers et al. found that LLMs might perform better in terms \nof empathy than busy healthcare professionals 29. Better empathy may be a driving factor for LLMs in clinical \npractice.\nThis study had several limitations that should be addressed. First, owing to time and ethical constraints, \nthe constructed models were not evaluated by patients with breast tumors; therefore, patient evaluations were \nlacking. Second, current LLM technology faces challenges. This study did not provide a fundamental theoretical \nexplanation for how RAG technology improves LLM responses to breast cancer nursing care questions. The \nuse of such LLMs in clinical settings should be conducted under the supervision of the medical staff. Finally, \nbreast cancer knowledge is continually developing and different medical centers have varying care philosophies. \nFurther refinement of the breast cancer knowledge base and clinical randomized controlled trials are required \nto validate the results of this study.\nAs RAG technology continues to advance, its applications in clinical care are expected to increase. In \nhospitals, RAG technology can integrate nursing experiences from various hospitals with LLMs and provide \npersonalized care recommendations. From a technological development perspective, RAG technology should \nevolve towards multimodal fusion, incorporating nursing videos, images, and textual information to further \nenhance the support for nursing-related tasks.\nIn summary, the present study evaluated the application of RAG technology to LLMs in breast cancer care, \nproviding a theoretical foundation for the clinical use of RAG technology and offering technical insights into the \ndevelopment of specialized LLMs for nursing.\nConclusion\nThis study revealed that RAG technology could significantly improve LLM performance for breast cancer \nnursing care. This improvement is likely due to the increased accuracy of answers without diminishing empathy. \nThe study results provide a theoretical basis for the application of RAG technology to LLMs in clinical nursing \npractice and education.\nGPT-4(Mean ± SD) RAG-GPT(Mean ± SD) p-value\nOverall satisfaction 5.4 ± 1.27 8.4 ± 0.84 < 0.01\nAccuracy 5.6 ± 0.96 8.6 ± 0.69 < 0.01\nEmpathy 7.8 ± 1.22 8.4 ± 0.85 > 0.05\nTable 2. Evaluation results.\n \nScientific Reports |        (2024) 14:30794 5| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports/\nData availability\nThe original data presented in the study are included in the article/supplementary material.\nReceived: 29 July 2024; Accepted: 25 November 2024\nFig. 3. Evaluation results (A) The RAG-GPT group showed significantly higher satisfaction than that in the \nGPT-4 group. (B) Satisfaction scores for each question. (C) The RAG-GPT group demonstrated significantly \nhigher accuracy compared with that of the GPT-4 group. (D) Accuracy scores for each question. (E) Empathy \ndid not differ significantly between the two groups. (F) Empathy scores for the 15 questions.\n \nScientific Reports |        (2024) 14:30794 6| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports/\nReferences\n 1. Loibl, S. et al. Breast cancer. Lancet 397(10286): 1750–1769. (2021).\n 2. Martin, R. F . Breast cancer. Surg. Clin. North. Am. 103 (1), xiii–xiv (2023).\n 3. Harbeck, N. & Gnant, M. Breast cancer. Lancet 389 (10074), 1134–1150 (2017).\n 4. Pais, C. et al. Large language models for preventing medication direction errors in online pharmacies. Nat. Med. 30 (6), 1574–1582 \n(2024).\n 5. Farquhar, S. et al. Detecting hallucinations in large language models using semantic entropy. Nature 630 (8017), 625–630 (2024).\n 6. Scerri, A. & Morin, K. H. Using chatbots like ChatGPT to support nursing practice. J. Clin. Nurs. 32 (15–16), 4211–4213 (2023).\n 7. Tam, W . et al. Nursing education in the age of artificial intelligence powered Chatbots (AI-Chatbots): Are we ready yet? Nurse \nEduc. Today 129, 105917 (2023).\n 8. Ray, P . P . & Majumder, P . Dr. ChatGPT in Cardiovascular nursing: A deeper dive into trustworthiness, value, and potential risks. \nEur. J. Cardiovasc. Nurs. 23, 11–12 (2024).\n 9. Vitorino, L. M. & Junior, G. ChatGPT and the teaching of contemporary nursing: And now professor? J. Clin. Nurs. 32 (21–22), \n7921–7922 (2023).\n 10. Woodnutt, S. et al. Could artificial intelligence write mental health nursing care plans? J. Psychiatr. Ment. Health Nurs. 31 (1), 79–86 \n(2024).\n 11. Abdulai, A. F . & Hung, L. Will ChatGPT undermine ethical values in nursing education, research, and practice? Nurs. Inq. 30 (3), \ne12556 (2023).\n 12. Allen, C. & Woodnutt, S. Can ChatGPT pass a nursing exam? Int. J. Nurs. Stud. 145, 104522 (2023).\n 13. Kleebayoon, A. & Wiwanitkit, V . ChatGPT and the teaching of contemporary nursing: Comment. J. Clin. Nurs. 32 (19–20), 7642 \n(2023).\n 14. Matsumoto, N. et al. KRAGEN: A knowledge graph-enhanced RAG framework for biomedical problem solving using large \nlanguage models. Bioinformatics 40 (6), 353 (2024).\n 15. Giuffrè, M. et al. Optimizing Large Language Models in Digestive Disease: Strategies and Challenges to Improve Clinical Outcomes \n(Liver International, 2024).\n 16. Miao, J. et al. How to improve ChatGPT performance for nephrologists: A technique guide. J. Nephrol., epub (2024).\n 17. Xiao, S. et al. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv, 2309.07597. (2023).\n 18. Veronesi, U. Boyle, P ., Goldhirsch, A. et al. Breast cancer. Lancet 365 (9472), 1727–1741 (2005).\n 19. Jieyi, L. Large Language Model Guided Tree-of-Thought (arxiv, 2023).\n 20. Maciej, B. et al. Graph of Thoughts: Solving Elaborate Problems with Large Language Models (arxiv, 2023).\n 21. Kresevic, S. et al. Optimization of hepatological clinical guidelines interpretation by large language models: A retrieval augmented \ngeneration-based framework. NPJ Digit. Med. 7 (1), 102 (2024).\n 22. Zhenzhu, L. et al. GPT-agents based on medical guidelines can improve the responsiveness and explainability of outcomes for \ntraumatic brain injury rehabilitation. Sci. Rep. 14 (1), 7626 (2024).\n 23. Bagiya, L. S. et al. Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms \n(arxiv, 2023).\n 24. Guo, S. et al. Comparing ChatGPT’s and surgeon’s responses to thyroid-related questions from patients. J. Clin. Endocrinol. Metab. \n10, dgae235 (2024).\n 25. Gomez-Cabello, C. A. et al. Artificial intelligence in postoperative care: Assessing large language models for patient \nrecommendations in plastic surgery. Healthcare 12 (11), 1083 (2024).\n 26. Long, C. Subburam, D., Lowe, K. et al. ChatENT: Augmented Large Language Model for Expert Knowledge Retrieval in \nOtolaryngology–Head and Neck Surgery (Otolaryngology–Head and Neck Surgery, 2024). ohn.864.\n 27. Mesko, B. The impact of multimodal large language models on health care’s future. J. Med. Internet Res. 25, e52865 (2023).\n 28. Koranteng, E. et al. Empathy and equity: Key considerations for large language model adoption in health care. JMIR Med. Educ. 9, \ne51199 (2023).\n 29. Ayers, J. W . et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social \nmedia forum. JAMA Intern. Med. 183 (6), 589–596 (2023).\nAcknowledgements\nWe extend our sincere gratitude to Li Zhenzhu (coding with Python, husband of XRY), the medical team at \nNingbo No. 2 Hospital, and the volunteers who participated in this study.\nAuthor contributions\nXRY: Writing of the original draft, writing the review, and editing. HY: Database, review, and editing. ZFF: Eval-\nuation, review, and editing. XHM: Design, funding, writing, review, and editing.\nFunding\nThe study was funded by the 2021 Hwa Mei Medical Education Research Project (2021HMJYZD05), HwaMei \nResearch Foundation of Ningbo No. 2 Hospital (2024HMZD16), HwaMei Research Foundation of Ningbo No. \n2 Hospital (2024HMKYA58), HwaMei Research Foundation of Ningbo No. 2 Hospital (2023HMZD03), Ningbo \nNatural Science Foundation (2023J325), and Project of Ningbo Leading Medical & Health Discipline (2022-\nB13).\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nApproval for human experiments\nAs this study did not involve human or animal research and the ChatGPT API is freely accessible online, no \nethical committee approval was required.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 4 - 8 1 0 5 2 - 3     .  \nScientific Reports |        (2024) 14:30794 7| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports/\nCorrespondence and requests for materials should be addressed to H.X.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2024 \nScientific Reports |        (2024) 14:30794 8| https://doi.org/10.1038/s41598-024-81052-3\nwww.nature.com/scientificreports/",
  "topic": "Empathy",
  "concepts": [
    {
      "name": "Empathy",
      "score": 0.8081269264221191
    },
    {
      "name": "Breast cancer",
      "score": 0.6859019994735718
    },
    {
      "name": "Medicine",
      "score": 0.6100921034812927
    },
    {
      "name": "Cancer",
      "score": 0.48253414034843445
    },
    {
      "name": "Nursing care",
      "score": 0.4568936824798584
    },
    {
      "name": "Nursing",
      "score": 0.40687623620033264
    },
    {
      "name": "Oncology",
      "score": 0.40080341696739197
    },
    {
      "name": "Internal medicine",
      "score": 0.3678293824195862
    },
    {
      "name": "Family medicine",
      "score": 0.33740776777267456
    },
    {
      "name": "Psychiatry",
      "score": 0.1355111300945282
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I134841294",
      "name": "Binzhou Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I151013683",
      "name": "Binzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210101660",
      "name": "Ningbo No. 2 Hospital",
      "country": "CN"
    }
  ],
  "cited_by": 11
}