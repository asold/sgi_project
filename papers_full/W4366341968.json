{
  "title": "OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization",
  "url": "https://openalex.org/W4366341968",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1972119850",
      "name": "Guo Cong",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2374153871",
      "name": "Tang Jia-ming",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2484033935",
      "name": "Hu, Weiming",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4222108197",
      "name": "Leng, Jingwen",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2101296206",
      "name": "Zhang Chen",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2095316391",
      "name": "Yang Fan",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2350497426",
      "name": "Liu Yun-xin",
      "affiliations": [
        "Tsinghua University",
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2181632981",
      "name": "Guo, Minyi",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A3177344041",
      "name": "Zhu Yuhao",
      "affiliations": [
        "University of Rochester"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2516141709",
    "https://openalex.org/W1979527452",
    "https://openalex.org/W3034940165",
    "https://openalex.org/W3034887213",
    "https://openalex.org/W2604514113",
    "https://openalex.org/W2323909431",
    "https://openalex.org/W4253012315",
    "https://openalex.org/W2804032941",
    "https://openalex.org/W6662587704",
    "https://openalex.org/W2442974303",
    "https://openalex.org/W3158444059",
    "https://openalex.org/W3005664618",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W2067523571",
    "https://openalex.org/W2044535169",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W2981758446",
    "https://openalex.org/W3190062760",
    "https://openalex.org/W2606722458",
    "https://openalex.org/W2963480671",
    "https://openalex.org/W3102510044",
    "https://openalex.org/W4234833047",
    "https://openalex.org/W2000967104",
    "https://openalex.org/W4214512541",
    "https://openalex.org/W2156077332",
    "https://openalex.org/W2150139096",
    "https://openalex.org/W2883920103",
    "https://openalex.org/W2117696986",
    "https://openalex.org/W3016542674",
    "https://openalex.org/W2963989532",
    "https://openalex.org/W4242577057",
    "https://openalex.org/W2963367920",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3043504674",
    "https://openalex.org/W2982479999",
    "https://openalex.org/W2023214828",
    "https://openalex.org/W3100985894",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2094756095",
    "https://openalex.org/W2884150179",
    "https://openalex.org/W4240168186",
    "https://openalex.org/W3012249773",
    "https://openalex.org/W2904902077",
    "https://openalex.org/W4312060054",
    "https://openalex.org/W2980186997",
    "https://openalex.org/W2464177207",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4298392854",
    "https://openalex.org/W3092209569",
    "https://openalex.org/W3187908937",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W1841592590",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3018278400",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2786771851",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W3115555382",
    "https://openalex.org/W3024621361",
    "https://openalex.org/W4312513797",
    "https://openalex.org/W3165976180",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W4308083739",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3092319711",
    "https://openalex.org/W4213153339",
    "https://openalex.org/W4287118909",
    "https://openalex.org/W3132616766",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W2982041622",
    "https://openalex.org/W4280546523",
    "https://openalex.org/W4288026258",
    "https://openalex.org/W4226109952",
    "https://openalex.org/W2970601456",
    "https://openalex.org/W4293327315",
    "https://openalex.org/W3133253223",
    "https://openalex.org/W4251575795",
    "https://openalex.org/W4297948009",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W2048266589",
    "https://openalex.org/W4234212765",
    "https://openalex.org/W4221162983",
    "https://openalex.org/W1606129576",
    "https://openalex.org/W4318541535",
    "https://openalex.org/W2963148663"
  ],
  "abstract": "Transformer-based large language models (LLMs) have achieved great success\\nwith the growing model size. LLMs' size grows by $240\\\\times$ every two years,\\nwhich outpaces the hardware progress and makes model inference increasingly\\ncostly. Model quantization is a promising approach to mitigate the widening gap\\nbetween LLM size and hardware capacity. However, the existence of outliers,\\nvalues with significant magnitudes, in LLMs makes existing quantization methods\\nless effective. Prior outlier-aware quantization schemes adopt sparsity\\nencoding techniques to separate outliers from normal values where the process\\nrequires global coordination (e.g., a global sparsity coordination list). This\\nincurs complex encoding/decoding hardware logics and an extra orchestration\\ncontroller for the computation between outlier and normal values. As such, it\\nis not hardware-efficient and hence only achieves sub-optimal quantization\\nbenefits.\\n We propose OliVe, an algorithm/architecture co-designed solution that adopts\\nan outlier-victim pair (OVP) quantization and handles outlier values locally\\nwith low hardware overheads and high performance gains. The key insight of\\nOliVe is that outliers are important while the normal values next to them are\\nnot. Thus those normal values (called victims) can be sacrificed to accommodate\\noutliers. This enables a memory-aligned OVP encoding scheme, which can be\\nefficiently integrated to the existing hardware accelerators like systolic\\narray and tensor core. As a result, OliVe-based accelerator surpasses the\\nexisting outlier-aware accelerator, GOBO, by 4.5$\\\\times$ speedup and\\n4.0$\\\\times$ energy reduction, respectively, with a superior model accuracy.\\n",
  "full_text": "OliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization\nCong Guoâˆ—\nguocong@sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai Qi Zhi Institute\nShanghai, China\nJiaming Tangâˆ—\nsakits_tjm@sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai Qi Zhi Institute\nShanghai, China\nWeiming Huâ€ \nhuweim1120@gmail.com\nShanghai Jiao Tong University\nShanghai Qi Zhi Institute\nShanghai, China\nJingwen Lengâ€¡\nleng-jw@cs.sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai Qi Zhi Institute\nShanghai, China\nChen Zhang\nchzhang1990@gmail.com\nMicrosoft Research\nBeijing, China\nFan Yang\nfanyang@microsoft.com\nMicrosoft Research\nBeijing, China\nYunxin Liu\nliuyunxin@air.tsinghua.edu.cn\nInstitute for AI Industry Research\n(AIR), Tsinghua University\nBeijing, China\nShanghai Artificial Intelligence\nLaboratory\nShanghai, China\nMinyi Guoâ€¡\nguo-my@cs.sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai Qi Zhi Institute\nShanghai, China\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nRochester, New York, USA\nABSTRACT\nTransformer-based large language models (LLMs) have achieved\ngreat success with the growing model size. LLMsâ€™ size grows by\n240Ã—every two years, which outpaces the hardware progress and\nmakes model inference increasingly costly. Model quantization is a\npromising approach to mitigate the widening gap between LLM size\nand hardware capacity. However, the existence of outliers, values\nwith significant magnitudes, in LLMs makes existing quantization\nmethods less effective. Prior outlier-aware quantization schemes\nadopt sparsity encoding techniques to separate outliers from nor-\nmal values where the process requires global coordination (e.g.,\na global sparsity coordination list). This incurs complex encod-\ning/decoding hardware logics and an extra orchestration controller\nfor the computation between outlier and normal values. As such,\nit is not hardware-efficient and hence only achieves sub-optimal\nquantization benefits.\nâˆ—Contribute equally to this paper.\nâ€ Work done while affiliated with ShanghaiTech University.\nâ€¡Jingwen Leng and Minyi Guo are corresponding authors of this paper.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0095-8/23/06. . . $15.00\nhttps://doi.org/10.1145/3579371.3589038\nWe propose OliVe, an algorithm/architecture co-designed so-\nlution that adopts an outlier-victim pair (OVP) quantization and\nhandles outlier values locally with low hardware overheads and\nhigh performance gains. The key insight ofOliVe is that outliers are\nimportant while the normal valuesnext to them are not. Thus those\nnormal values (called victims) can be sacrificed to accommodate\noutliers. This enables a memory-aligned OVP encoding scheme,\nwhich can be efficiently integrated to the existing hardware accel-\nerators like systolic array and tensor core. As a result, OliVe-based\naccelerator surpasses the existing outlier-aware accelerator, GOBO,\nby 4.5Ã—speedup and 4.0Ã—energy reduction, respectively, with a\nsuperior model accuracy.\nCCS CONCEPTS\nâ€¢ Computer systems organization â†’Neural networks; Data\nflow architectures; Single instruction, multiple data; Systolic\narrays.\nKEYWORDS\nLarge Language Model, Outlier-Victim Pair, Quantization\nACM Reference Format:\nCong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan\nYang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2023. OliVe: Accelerating\nLarge Language Models via Hardware-friendly Outlier-Victim Pair Quantiza-\ntion. In Proceedings of the 50th Annual International Symposium on Computer\nArchitecture (ISCA â€™23), June 17â€“21, 2023, Orlando, FL, USA. ACM, New York,\nNY, USA, 15 pages. https://doi.org/10.1145/3579371.3589038\narXiv:2304.07493v1  [cs.AR]  15 Apr 2023\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA Guo and Tang, et al.\n1 Outlier Value\n1 Normal Value\n2 3 -5 0\n0 4 7 -7\n1 6 0 6\n(a) Sparsity-Based Encoding (GOBO)\n-98\n18\n31\n(0,3)\n(1,0)\n(2,2)\n8-bit 4-bit\n1.5 2.6 0 -98\n17.6 0 7.1 -6.8\n1.2 6.3 30.7 0\n2 3 1000 1111 \n(0, -96)\n0010 1000\n(16, 0) 7 -7\n1 6 0100 1000\n(32, 0)\n(b) Outlier-Victim Pair Encoding (Ours)0 Victim Value\nUnaligned\nAligned\nOV Pair\n+\nSparse Outlier: \nCoordinate List1.5 2.6 -5.2 -98\n17.6 4.2 7.1 -6.8\n1.2 6.3 30.7 5.9\n Matrix\nDense Normal\nPrune Victims Quantization &\nCompress Outliers\nQuantize &\nEmbed Outliers\n4-bit 8-bit\n4-bit OVP \nEncoding\nFigure 1: Outlier-aware encoding comparison. (a) Prior\nquantization works adopt sparsity-based encoding that\nstore normal and outlier values separately. (b) Our proposed\noutlier-victim pair encoding stores normal and outlier val-\nues locally.\n1 INTRODUCTION\nTransformer-based large language models (LLMs) [77] have demon-\nstrated great success in the past years. Such success is often achieved\nwith the increasingly larger model size: the model size grows\nby 240Ã—every two years, significantly outpacing the hardware\nprogress (3.1Ã—per two years) [ 24]. As a result, the inference of\nLLMs becomes challenging and costly. For instance, OPT-175B [90],\na recent Transformer-based LLM, has 175 billion parameters, which\ncannot fit in the latest high-end H100 GPU with 80GB memory.\nQuantization [ 6, 7, 21, 22, 72, 74, 79, 93] is one of the most\nhardware-efficient ways to reduce inference costs for large models.\nIt uses low-precision data types to compress models and accelerate\nthe computation with practical hardware implementations, e.g.,\nTPU [42] and GPU tensor core [60].\nHowever, existing quantization schemes [18, 74, 86] are less ef-\nfective in Transformer-based LLMs. Recent studies show when the\nmodel size exceeds a threshold (e.g., 6 billion), the model perfor-\nmance is vulnerable to only a tiny fraction ( < 0.1%) of outliers,\nwhose values are much more significant than normal values [18].\nIndiscriminately clipping both outlier and normal values will lead\nto significant drops in model accuracy [ 18, 82]. As a result, the\ncommon practice is to adopt a larger bit-width, e.g., 8-bit or 16-bit,\nto quantize Transform-based models, compared to convolutional\nnetworks (CNNs).\nResearchers have proposed various quantization/architecture\nco-design works [39, 61, 75, 82, 85] to deal with the outliers in Trans-\nformer models. For example, outlier suppression [82] proposes to\nsuppress the outliers. But it still has significant accuracy loss in the\nlower bit-width (4-bit), suggesting the difficulty in accommodating\nthe effects of outliers. In addition, architecture researchers have de-\nsigned sophisticated outlier-aware hardware architectures to store\noutliers with high precision to maintain model accuracy. These\noutlier-aware quantization frameworks divide the tensor into nor-\nmal and outlier values, and encode them separately using different\nways. For normal values, a dense matrix with low precision (e.g., 4-\nbit) quantization is adopted. And the sparse and high-precision (e.g.,\n8-bit and 16-bit) outlier values can be compressed with sparsity-\nbased encoding. Such encoding unfortunately leads to unaligned\nmemory access. For example, GOBOs [85] and OLAccels [61] use\nthe coordinate list to indicate the location of each outlier value in\nthe matrix, as shown in Fig. 1a. BiScaled-DNNs [39] exploits block\nsparse indices format to store the outlier indices, and DRQ [ 75]\nuses the direct bitmap for outliers. These outlier-aware solutions\nrequire complex architectural designs with significant hardware\noverheads to accommodate outliers. Moreover, due to the random\nand unaligned memory access, the sparsity-based encoding is in-\ncompatible with the memory sub-systems of existing accelerators,\nsuch as GPU and TPU. Specifically, GOBO [85] can only de/com-\npress weight tensors on the off-chip DRAM, it still relies on the\noriginal on-chip memory and computation architecture of GPU\nwith high precision FP16/32.\nThe aforementioned outlier-aware architectures separate nor-\nmal values from outliers in a global way. For instance, GOBO [85]\ninvolves a global sparse coordinate list in the quantization and\ncomputation, leading to a large hardware overhead and low perfor-\nmance benefits. In this work, we aim to design an architecture to\nhandle outliers in a localized way with high hardware efficiency.\nTo achieve that, we group two consecutive fixed-size values in a\ntensor and analyze their impact to model accuracy. There can be\nthree kinds of pairs: i) a normal pair with two normal values, ii)\none-outlier pair with one normal value and one outlier value, iii)\ntwo-outlier pair with two outlier values. We observe that the third\ntwo-outlier pair almost never shows up in well-trained LLMs. For\nthe second one-outlier pair, we find that only keeping its outlier\nvalue while pruning its normal value (i.e., treating it as zero) is\nsufficient to maintain the model accuracy.\nBased on the above observations, we propose a novel outlier-\naware quantization architecture, called OliVe, based on the outlier-\nvictim pair (OVP) encoding. The salient feature ofOliVe is memory-\naligned and therefore hardware-friendly. As illustrated in Fig. 1b,\nOliVe first prunes normal values that are adjacent to the outliers\nas zero. These pruned normal values are called victims, which\nsacrifice themselves and make space for outliers. Then, we exploit\nthe extra space provided by victims and embed the outliers into the\nlow-precision matrix.\nOliVe is able to maintain a high accuracy for large Transformer\nmodels with a low hardware overhead due to the following reasons.\nFirst, OliVe incorporates victims to tackle outliers in LLMs. The\neffects of victims resemble model pruning [36]. Although clipping\na few (0.1%) outliers will lead to a disastrous accuracy drop [18, 82],\npruning the same amount of â€œnormalâ€ values will only impact\nmodel accuracy slightly (< 0.1% drop). Therefore, OliVe sacrifices\n(â€œprunesâ€) those insignificant values as victims for the outliers,\nallowing a more aggressive encoding scheme to accommodate ex-\ntremely significant values. Second, the OVP encoding follows a spe-\ncific outlier-victim (or victim-outlier) pattern to achieve memory\nalignment with little hardware overheads. Each victim is adjacent\nto an outlier, and the outlier-victim pair must align the memory\naccess pattern. For example, in Fig. 1b, right outlier âˆ’98 in the OV\npair needs a left victim, and left outliers 17.6 and 30.7 require the\nright victims. That can align 8-bit (1-byte) memory accesses with\nhigh efficiency. This design enables a completely localized outlier\ndecoding/encoding process.\nOliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization ISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\n28Ïƒ\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0\n10\n20\n30\n40\n1 11 21 31 41\nPercentage (%)\nNormlized Maximum Value (Ïƒ)\nTe n so r ID by  M ax Ïƒ Order\n>3Ïƒ % >6Ïƒ %\nMax Ïƒ\n(a) ResNet-18 on ImageNet.\n325Ïƒ\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0\n50\n100\n150\n200\n250\n300\n350\n1 21 41 61 81 101 121 141\nPercentage (%)\nNormlized Maximum Value (Ïƒ)\nTe n so r ID by  M ax Ïƒ Order\n>3Ïƒ % >6Ïƒ %\nMax Ïƒ (b) BERTğ‘ğ‘ğ‘ ğ‘’ on MNLI.\nFigure 2: Outlier Comparison of CNN model and Trans-\nformer model. The ğœ is the standard deviation of the tensor.\nWe normalize the maximum number by ğœ to plot the Max ğœ\ncurve (left y-axis). The > 3ğœ% and > 6ğœ% (right y-axis) are\nthe percentage of the values of > 3ğœ and > 6ğœ, respectively.\nTo implement OliVe, different data types are employed for out-\nliers and normal values, which have different dynamic ranges and\nrepresentation formats, includingint4 and FP4. As shown in Fig. 1b,\nwe propose a novel encoding method (Sec. 3) for the 4-bit OV pair,\nwhich composes a 4-bit outlier and a 4-bit victim into a special\n8-bit format and differs from the original int8 or FP8. Due to its\nhardware-friendly and compatible design, OliVe can be easily in-\ntegrated into existing quantization frameworks and accelerator\narchitectures such as systolic array in Google TPUs [41] and tensor\ncore in NVIDIA GPUs [58, 60]. OliVe can also inherently support\nthe mixed-precision and mixed-type architecture, showing its flexi-\nbility and practicality for larger-scale Transformer models.\nTo the best of our knowledge, OliVe is the first work push-\ning the limit of Transformer post-training quantization (PTQ) [4],\nwhich requires no retraining after quantization, to the 4-bit level\nfor both the weight and activation tensors with the accuracy loss\nof < 1%. Surprisingly, OliVeâ€™s 4-bit PTQ accuracies for BERT [19]\nand BART [49] models outperform the 6-bit PTQ results of out-\nlier suppression [82], a state-of-the-art Transformer quantization\nmethod. OliVe-based accelerator surpasses the existing outlier-\naware accelerators OLAccel [61] and GOBO [85] by 3.8Ã—and 4.5Ã—\nperformance improvement, and 2.1Ã—and 4.0Ã—energy reduction,\nrespectively. More importantly, the OliVe-based accelerator has\nmore comprehensive and practical applicability than other outlier-\nspecific architectures.\nWe make the following contributions in this paper.\nâ€¢We conduct the pair-wise importance analysis and show that\noutliers are important while their adjacent normal values are\nnot, revealing the algorithmic opportunity of outlier-victim\npair (OVP) that sacrifices the colocated normal values (called\nvictims) to accommodate the outliers.\nâ€¢We propose the OVP-based quantization framework, called\nOliVe, which includes an efficient hardware encoding and\nnovel outlier representation data type.\nâ€¢We propose the efficient architectural implementation and\nintegration of OliVe quantization, and show that its effi-\nciency and benefits outperform the existing outlier-aware\nquantization algorithms and hardware accelerators.\n2 MOTIVATION: ALIGNED OUTLIER\nIn this section, we first show that the outlier of the Transformer\nmodel is much more significant and important compared to convolu-\ntion neural networks (CNN). Previous works [74, 75, 85, 86] propose\nthe outlier-aware quantization microarchitecture with adaptive bit\nlength to accomplish the low-bit quantization but necessitate sub-\nstantial hardware resources to deal with the variable-length data,\nwhich cause unaligned memory accesses and are incompatible with\nthe memory sub-system of existing accelerators, e.g., GPU [ 60].\nIn contrast, we propose a memory-aligned and hardware-friendly\nmethod, called outlier-victim pair mechanism, which is inspired\nby DNN pruning and our outlier group location analysis for Trans-\nformers. We can prune some â€œvictimsâ€ to make space to embed\nhigh-precision outliers into the memory-aligned low-bit tensor\nwith ignorable accuracy loss.\n2.1 Outlier Matters\nWe visually demonstrate how significant the Transformerâ€™s outlier\nis in Fig. 2. We adopt the empirical 3ğœ rules [83] of the normal\ndistribution to divide the values into outlier and normal values. We\nemploy the ResNet-18 [37] as the representative for the CNN model\nand the BERTğ‘ğ‘ğ‘ ğ‘’ [19] for the Transformer model. We fit the DNN\ntensors with normal distribution, i.e., Equation 1, where ğ‘¥ is the\nvalue, ğœ‡is the mean, and ğœ is the standard deviation. We convert\nthe tensor into a standard normal distribution.\nf (ğ‘¥)= 1\nğœ\nâˆš\n2ğœ‹\nğ‘’âˆ’1\n2 (ğ‘¥âˆ’ğœ‡\nğœ )\n2\n(1)\nWe collect all tensorsâ€™ maximum values and normalize them by the\nğœ (Max ğœ). We sort and plot the tensors by their Max ğœ in Fig. 2.\nMost tensors can fit the normal distribution 3ğœ rules, i.e., about\n99.7% of the values lie within three standard deviations of the mean.\nThe outlier (> 3ğœ) ratio of most tensors is lower than 0.5%, and\nthe values of > 6ğœ are extremely few in tensors. Therefore, normal\nvalues are relatively concentrated, indicating that we can quantize\nthe normal values with a narrow range to enhance the resolution\nof quantization.\nThe more obvious observation is that the Max ğœ of the Trans-\nformer is larger than that of CNN by one order of magnitude. Some\nresearch [14, 43] shows that although the outliers are clipped for\nCNN models, the accuracy can still be restored to the original value\nwith the retraining algorithm under ultra-low-bit precision, e.g., 4-\nbit. However, it is challenging for Transformer models, which have\nmuch more significant outliers. The state-of-the-art quantization\nworks [18, 82] also demonstrate a similar observation and only can\nachieve the original accuracy with higher-precision quantization\nfor large-scale Transformer models due to the outliers. Therefore,\nkeeping the outlier without clipping will significantly benefit quan-\ntizing Transformer models.\n2.2 Outlier Is Unaligned\nThe importance of outliers has attracted many research interests,\nwhich sparked several outlier-aware architectures, as depicted in\nTbl. 1. OLAccel [ 61] and GOBO [ 85] are similar and exploit the\ncoordinate list to indicate the location of outliers, which use high-\nprecision (8-bit or 16-bit) quantization. BiScaled-DNN [ 39] and\nDRQ [ 75] employ block sparse index and bitmap, respectively.\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA Guo and Tang, et al.\nAccelerator Encoding Aligned\nMemory?\nGPU\nCompatible?\nOLAccel [61] Coordinate list No No\nBiScaled-\nDNN [39] Block sparse index Alined data\nUnaligned index No\nDRQ [75] Binary mask map Unalined data\nAligned index No\nGOBO [85] Coordinate list No DRAM-only\nOliVe (Ours) Outlier-victim pair Yes Yes\nTable 1: Comparison between existing outlier-aware acceler-\nators and our proposed method OliVe.\nBiScaled-DNN quantizes all values with the same bit-width but\ndifferent scale factors for normal values and outliers, which are\naligned. However, the extra index compressed in the block sparsity\nmethod is unaligned. On the contrary, DRQâ€™s bitmap is aligned, but\ndata is stored by mixed and thus unaligned 4- & 8-bit values.\nIn summary, prior works design the outlier-aware architecture\nbased on the sparsity of outliers, which leads to unaligned memory\nstorage and accesses. More seriously, the indices of sparsity-based\nencoding and the outliers are separate. As such, they need the extra\noutlier controller to parse indices for the outliers and orchestrate\nthe computation between normal values and outlier values. For\nexample, the extra outlier controllers of GOBO and OLAccel count\nup to 55% and 71% overhead to the total area of the processing\nelement (PE) array [61, 85]. The sparsity-based encoding for outliers\nis also incompatible with the memory sub-system of existing\naccelerators. For the GOBO design [85], it can only compress and\ndecompress the memory at the DRAM level for GPU. This greatly\nlimits the applicability of its proposed outlier-aware architecture.\nTherefore, a more hardware-friendly and applicable outlier de-\ncoding/encoding method should be proposed to fit the outlier-aware\nquantization. Our proposedOliVe architecture is able to align mem-\nory accesses and is also compatible with existing accelerators based\non the OVP mechanism.\n2.3 Outlier and Victim Analysis\nGenerally, the sparsity-based encoding borrowed from DNN prun-\ning is a straightforward and effective solution for sparse outliers.\nHowever, these works ignored that quantization is different from\npruning. For pruning, the pruned zero values do not participate\nin the computation. As such, the pruning method has to compress\nPair Type Normal-NormalOutlier-NormalOutlier-Outlier\nBERTğ‘ğ‘ğ‘ ğ‘’[19] 99.12% 0.84% 0.04%\nBERTğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’[19] 99.24% 0.71% 0.05%\nGPT2-XL[66] 98.80% 1.14% 0.06%\nOPT-6.7B[90] 99.33% 0.64% 0.03%\nTable 2: The percentage of three types of pair.\n0\n-15\n0\n15\n30\n45\n60\n75\n90\nCoLA\n(Matt.)\nSST-2\n(Acc.)\nMNLI\n(Acc.)\nQQP\n(Acc.)\nQNLI\n(Acc.)\nRTE\n(Acc.)\nSTSB\n(Pear.)\nMRPC\n(Acc.)\nAccuracy (%)\nClipping Outlier Pruning Victim Pruning Normal Value Source Accuracy\nFigure 3: Accuracy comparison of multiple pruning meth-\nods.\nthe sparse values with sparsity-based encoding. For quantization,\nthe quantized normal values are the majority and need computa-\ntion. Naturally, the outlier values can exploit the normal values to\nachieve memory alignment instead of sparsity-based encoding.\nAs depicted in Fig. 1b in Sec. 1, we employ the insight of prun-\ning but with a different perspective from prior works. The new\nmethod employs the outlier-victim pair (OVP) mechanism. We\nfirst prune some quantized low-precision normal values, which we\ncall victims. These victims are adjacent to the outliers and make\nextra space for the high-precision outliers. Therefore, we can em-\nbed the outliers in their original location without explicit sparse\nindexing. That can avoid the complex indexing hardware and make\nit compatible with GPU. To align the memory, we distinguish the\nâ€œright outlierâ€ and â€œleft outlierâ€ according to their position in the\npair. We assign a right victim for the left outlier (e.g.,17.6 in Fig. 1b)\nand a left victim for the right outlier (e.g., âˆ’98 in Fig. 1b).\nThe OVP mechanism is based on our observation of large Trans-\nformer models, including BERT-base [19], BERT-large [19], GPT2-\nXL [66], and OPT-6.7B [90]. We collect all tensors, calculate their\nstandard varianceğœ, and divide the values into normal values (< 3ğœ)\nand outlier values (> 3ğœ) by the 3ğœ rule. We then pair every two\nadjacent values (no overlapping), which leads to three types: normal-\nnormal pair, outlier-normal pair, and outlier-outlier pair, as shown\nin Tbl. 2. These three types have two normal values, one normal\nvalue and one outlier value, and two outlier values, respectively.\nTbl. 2 demonstrates that most (about 99%) pairs are normal-\nnormal pairs, with only around 1% of outlier-normal pairs. Outlier-\noutlier pairs need to prune the smaller outlier in the pair. Fortu-\nnately, the outlier-outlier pairs only have an extremely low proba-\nbility of less than 0.06% in all studied models. Therefore, the outlier\ndistribution is extremely dispersed, and we can retain most outliers.\nWe also conducted the accuracy experiments with the BERTğ‘ğ‘ğ‘ ğ‘’\nmodel [82] on the GLUE dataset [78], as depicted in Fig. 3. First, we\nclip the outliers to the 3ğœ, where clipping is the common method\nadopted by quantization. Then, we prune the victims and normal\nvalues to zero. The victims are adjacent to the outliers, and normal\nvalues are randomly pruned with the same amount as the outliers.\nWe keep the rest values with full precision (FP32). Although such\nfew outliers (about 1%) are clipped, as shown in Fig. 3 clipping\noutlier, the accuracy loss is unacceptable for the BERT model. The\nresults emphasize the importance of outliers in Transformer-based\nmodel. For comparison, pruning random normal values has almost\nno accuracy loss than the source accuracy. The pruning of victim\nvalues only shows a negligible accuracy decrease than the pruning\nof normal values because the victims include some outliers due to\nOliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization ISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\n 7\n 6\nâ€¦\n-6\n-7\nINT4\n 7\n 6\nâ€¦\n-6\n-7\n-8\nINT4\nVictim Outlier\nOutlier Victim\n1000 xxxx\nxxxx 1000\nQuant& \nEncoding\n2.6 4.2 0011 0100\n2. Outlier-Victim Pair Encoding1. Eliminate the identifier IdentifierFP32\n4-bit\nOV Pair\n4-bit 4-bit\nINT4\nLeft Right\nFigure 4: The 4-bit outlier-victim pair encoding.\nthe outlier-outlier pair and have specific locations corresponding\nto the adjacent outlier.\nIn summary, our analysis indicates that outliers are important\nwhile the victims are not, so that we can sacrifice victims to ac-\ncommodate the outliers. This motivates us to design the hardware-\nfriendly OVP mechanism that provides aligned outlier-aware quan-\ntization to accelerate the large Transformer models. In the next\nsection, we will introduce the outlier-victim pair encoding design.\n3 OUTLIER-VICTIM PAIR ENCODING\nIn this section, we present the details of outlier-victim pair (OVP)\nencoding that is globally identical but locally distinguishable\nfor outlier and normal values. The OVP encoding can maintain\nglobally aligned memory access and distinguish the outliers locally\nwith ignorable overhead. For normal values, we can support multi-\nple data types to fit the adaptive data type. For encoding outliers, we\ndesign an outlier-specific data type, adaptive bias float, abfloat,\nwhich can avoid range overlapping between normal values and\noutliers, thus improving the utilization ratio of the numerical rep-\nresentation space of outlier encoding. Finally, based on the OVP\nencoding, we propose a framework that can automatically select\nthe outlier threshold for OVP encoding to determine a suitable ratio\nof the outlier-victim pair.\n3.1 OVP Encoding Algorithm\nBased on the previous pair-wise tenor value analysis, there are\nthree pair types: normal-normal, outlier-normal, and outlier-outlier.\nFor outlier-normal, the normal value in the pair will be pruned and\nturned into a victim. For outlier-outlier, we remain the large one\nAlgorithm 1: The 4-bit OVP encoding algorithm.\nInput: Values, ğ‘£ğ‘ğ‘™1,ğ‘£ğ‘ğ‘™2; Outlier threshold, ğ‘‡.\nOutput: OVP encoding, ğ‘œğ‘¢ğ‘¡1,ğ‘œğ‘¢ğ‘¡2.\n1 def OVPairEncoding(ğ‘£ğ‘ğ‘™1, ğ‘£ğ‘ğ‘™2, ğ‘‡):\n2 if ğ‘£ğ‘ğ‘™1 > ğ‘‡ and ğ‘£ğ‘ğ‘™1 > ğ‘£ğ‘ğ‘™2 then\n3 ğ‘œğ‘¢ğ‘¡1 = OutlierQuantization(ğ‘£ğ‘ğ‘™1);\n4 ğ‘œğ‘¢ğ‘¡2 = 10002; // Outlier identifier.\n5 else if ğ‘£ğ‘ğ‘™2 > ğ‘‡ then\n6 ğ‘œğ‘¢ğ‘¡1 = 10002\n7 ğ‘œğ‘¢ğ‘¡2 = OutlierQuantization(ğ‘£ğ‘ğ‘™2);\n8 else\n9 ğ‘œğ‘¢ğ‘¡1 = NormalQuantization(ğ‘£ğ‘ğ‘™1);\n10 ğ‘œğ‘¢ğ‘¡2 = NormalQuantization(ğ‘£ğ‘ğ‘™2);\n11 return ğ‘œğ‘¢ğ‘¡1,ğ‘œğ‘¢ğ‘¡2\nand prune the other. Then, we get the normal-normal pairs and\noutlier-victim pairs in the DNN tensors.\nOutlier Identifier. To distinguish from the normal-normal pair,\nwe need a special identifier for the outlier-victim pair. And this\ndistinct identifier cannot appear in the normal-normal pair, which\nmeans we need to eliminate one number in the representation of\nnormal values. For example, as shown in Fig. 4, we employ the\nsigned int4 (4-bit integer) for the normal value quantization. The\noriginal int4 can represent the integers in the range of [âˆ’8,7],\nwhere 10002 represents the value of âˆ’8. First, we make 10002\nthe outlier identifier and remove the value of 10002 from int4,\nwhose encoding range becomes [âˆ’7,7]. Second, we quantize the\noutlier-victim pairs with 4-bit OVP encoding. We set the victims\nwith the outlier identifier 10002 and quantize the outlier with the\noutlier-specific data type (Sec. 3.3). Naturally, there are two types\nof OV pair, i.e., left outlier (O-V) and right outlier (V-O) pair. Due to\nthe distinct outlier identifier design, we can implicitly distinguish\nthem without using an extra index bit (Sec. 4.2).\nAlgo. 1 shows the 4-bit OVP encoding algorithm, which needs to\nread two values simultaneously, where the requirement is very easy\nto meet. For the hardware implementation, we can add a buffer\nfor the encoder. Also, the OVP encoder can be implemented by\nembedding in the quantization unit with ignorable overheads. For\nthe software implementation, we can make a thread handle two\nvalues simultaneously. As a result, the encoding algorithm can be\nimplemented efficiently in both hardware and software, which we\ndescribe more details later.\n3.2 Data Type for Normal Values\nFor normal values, we build upon prior work [32], which can sup-\nport multiple data types, including int4, flint4 (4-bit flint),\nand int8, as shown in Tbl. 3. The int4 type is one of the most\nwidely used data types for 4-bit quantization with integers in the\nvalue range of [âˆ’7,7]. The flint4 type is proposed by prior work\nANT [32], which has shown that selecting the data type according\nto a tensorâ€™s distribution achieves the state-of-the-art performance\nand accuracy.\nBased on the above insights, we also adopt the mixed data types\nto quantize normal values in our OVP pair encoding. For flint4,\nwe use the same binary value of 10002 as the outlier identifier.\nSpecifically, 10002 of flint4 corresponds to âˆ’0, which is not used\nin the original design. In other words, our OVP encoding seamlessly\nworks for flint4 without wasting any number representations.\nWe use the original flint4 encoding algorithm [32] to quantize\nnormal values.\nMoreover, the OVP encoding can be generally extended to higher-\nprecision quantization, such as the 8-bit. Similarly, the 8-bit normal\nData Type Values Outlier Identifier\nint4 0,Â±1,Â±2,Â±3,Â±4,Â±5,Â±6,Â±7 10002 (-8)\nflint4[32] 0,Â±1,Â±2,Â±3,Â±4,Â±6,Â±8,Â±16 10002 (-0)\nint8 0,Â±1,Â±2,Â·Â·Â· ,Â±126,Â±127 100000002 (-128)\nTable 3: Data types for normal values of OVP encoding.\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA Guo and Tang, et al.\n102030405060\nE0M3E1M2E2M1E3M0\nNormalizedMeanError BERT-baseBERT-largeBART-baseGPT2-XL\nFigure 5: The rounding error of the largest outliers quan-\ntized with different data types. Experiments were conducted\non BERT-base, BERT-large, BART-base, and GPT2-XL.\nvalue also needs to eliminate one number. For instance, int8 can\nrepresent [âˆ’128,127]integers, and we can make 100000002 the\noutlier identifier for int8 and narrow its range to [âˆ’127,127].\nSimilarly, the encoding algorithm can easily extend to read two\n8-bit elements simultaneously.\n3.3 Data Type for Outliers: Abfloat\nNext, we quantize outliers using the outlier-specific data type. The\nlarge outliers usually have a wide range, for which we use float-\nbased data to quantize. We propose a data type called adaptive\nbiased float , abfloat in short. The key idea is that by adding a\nproper bias to the exponent, all encoded values can skip the interval\nwhere normal values lie and provide more range for outliers.\nFloat-to-Fixed Conversion. To accommodate the normal values\nand avoid fractions, we first convert the floating-point encoding to\nthe fixed point with an exponent. Also, the fixed point is friendly to\nthe hardware implementation and has a lower overhead than the\nfloating point. We transform the the floating point to fixed point\nwith the following equation,\nsign Ã—(1 â‰ªmb +mantissa)â‰ª( exponent +bias), (2)\nwhere mb is the mantissa bit-width. Therefore, this fixed-point\nencoding scheme is more friendly and efficient for hardware im-\nplementation, as it only involves shift operations. Tbl. 4 shows the\nexample of fixed-point E2M1 data type.\nAdaptive Bias. Obviously, Tbl. 3 and Tbl. 4 show that the range of\nfixed-point abfloat overlaps with the normal values. For example,\nint4 and E2M1 contain the same numbers, 3, 4, and 6. Another\nexample is that flint4 and E2M1 have almost the same number\nrange except for 24. Therefore, we need the adaptive bias to ad-\njust the range of abfloat. For example, we set bias = 2 for E2M1,\nwhose real values will be extended to {12,Â·Â·Â· ,96}, which is com-\nplementary with the int4 normal value. Similarly, we setbias = 3\nBinary Exponent Integer Real Value\n000 0 0 0\n001 0 3 3 Ã—20 = 3\n01x 1 2, 3 2 Ã—21 = 4,3 Ã—21 = 6\n10x 2 2, 3 2 Ã—22 = 8,3 Ã—22 = 12\n11x 3 2, 3 2 Ã—23 = 16,3 Ã—23 = 24\nTable 4: The 3-bit unsigned E2M1, which means two bits for\nexponent and one bit for mantissa, with bias = 0.\nAlgorithm 2: The abfloat encoding algorithm.\nInput: Element ğ‘’; Bias, ğ‘;\nOutput: Quantized Element ğ‘;\n1 def AbfloatQuant(ğ‘’, ğ‘):\n// Get exponent and base integer.\n2 ğ‘’ğ‘¥ğ‘ = âŒŠğ‘™ğ‘œğ‘”2 (ğ‘ğ‘ğ‘ (ğ‘’))âŒ‹âˆ’ 1;\n3 ğ‘ğ‘ğ‘ ğ‘’_ğ‘–ğ‘›ğ‘¡ = ğ‘…ğ‘œğ‘¢ğ‘›ğ‘‘[ğ‘’/2ğ‘’ğ‘¥ğ‘ ];\n4 if ğ‘ğ‘ğ‘ ğ‘’_ğ‘–ğ‘›ğ‘¡ == 4 then\n5 ğ‘’ğ‘¥ğ‘ = ğ‘’ğ‘¥ğ‘ +1;\n6 ğ‘ğ‘ğ‘ ğ‘’_ğ‘–ğ‘›ğ‘¡ = ğ‘ğ‘ğ‘ ğ‘’_ğ‘–ğ‘›ğ‘¡ âˆ’2;\n// Encoded as abfloat data type.\n7 ğ‘’ğ‘¥ğ‘ = ğ‘’ğ‘¥ğ‘ âˆ’ğ‘;\n8 ğ‘ğ‘ğ‘ ğ‘’_ğ‘–ğ‘›ğ‘¡ = ğ‘ğ‘ğ‘ ğ‘’_ğ‘–ğ‘›ğ‘¡ & 1;\n9 ğ‘¢ğ‘›ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘= ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘’ğ‘¥ğ‘,ğ‘ğ‘ğ‘ ğ‘’ _ğ‘–ğ‘›ğ‘¡);\n10 ğ‘= ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘’ < 0,ğ‘¢ğ‘›ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘)\n11 return ğ‘\nand extend range to {24,Â·Â·Â· ,192}for flint4 data type. We de-\nsign a new decoder and instruction to implement adaptive bias in\naccelerators for the abfloat (Sec. 4.2).\nE2M1 Abfloat. The 4-bit signed float has four possible configu-\nrations of exponent and mantissa: E0M3, E1M2, E2M1, and E3M0.\nThey have different ranges and precisions. We conduct the follow-\ning experiments to choose the most appropriate configuration as\nthe final outlier-specific data type. To accommodate the broad range\nof outlier values, we quantize the largest outlier values (i.e., Max\nğœ in Fig. 2) in Transformer models using all abfloat types. Then,\nwe collect the average absolute error, as shown in Fig. 5. We found\nthat E2M1 gives the least error in all tests, which provides both a\nlarge enough range and a certain degree of precision, and it also\npresents the best results in our subsequent evaluations. Similarly,\nwe adopt signed E4M3 for 8-bit abfloat.\nAlgo. 2 shows in detail how an element is encoded as abfloat.\nThe outlier encoding is an element-wise function, which can be im-\nplemented on software and hardware efficiently. Outlier encoding\nshould also eliminate the outlier identifier. Otherwise, the decoder\ncannot distinguish the outlier-victim pair. Abfloat has two zero\nnumbers: 1000 (-0) and0000 (0). Therefore, we disable the1000 and\n0000 for outlier values to avoid conflict with the outlier identifier.\n3.4 Quantization Framework\nWe now apply OVP (outlier-victim pair) encoding for quantizing\nTransformer models. To decide the scale factor (i.e., outlier-victim\nthreshold), we embed the OVP encoding with the existing mean\nsquared error (MSE) minimization algorithm, which is commonly\nused by many quantization works [4, 6, 88]. The OVP-based quan-\ntization algorithm determines the threshold for distinguishing out-\nliers and normal values. On one hand, a small threshold would lead\nto more outlier-victim pairs, which could potentially minimize the\nquantization error (i.e., MSE). On the other hand, it also increases\nthe ratio of outlier-outlier pairs, where both values are outliers in\nthe pair. If there are too many such outlier-outlier pairs, the MSE\nwould increase owing to the pruning of outliers. Thus, we need to\ncontrol the ratio of outlier-outlier pairs for better accuracy.\nOliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization ISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\nInput\nValue 1\nValue 2\n0:3\n4:7\nNormal\nDecoder\nNormal\nDecoder\n0\n1\nOutlier\nDecoder\n0\n1\n1\n0\n00000000\n00000000\n== 1000\n== 1000\n0\n1\n1\n0\n0:7\n8:15\nExp-Int\nPairsTensor Core\nINT32FP64/32\nScheduler\nLD/ST\nDispatch\nInst. \nCache\nReg File\nGPU SM\nRegister File\nOctet Matrix A\nBuffer\nMatrix A\nBuffer\nMatrix B\nBuffer\nThread Group\nInt4 Exp4\nInt4\nâœ–\nExp4\n+ â€¦â€¦\n.\n16Ã—\n<<\nInt4 Exp4\nInt4\nâœ–\nExp4\n+<<\nâ•\nAdder Tree\nThread Group\nâ€¦â€¦\n.\n16-bit FEDP\n8-bit 8EDP\n4-bit 16EDP\nWrite Back\nOVP Decoder OVP Decoder\nOperation Unit Buffers Register New OliVe Component\n(a) GPU Tensor Core (b) Outlier-Victim Pair Decoder\nFigure 6: OliVe integration on GPU tensor cores (a), which only requires a set of lightweight OVP decoder (b).\nIn our work, we target the post-training quantization (PTQ) [57],\nwhich does not require retraining and hence is best suitable for\nlarge models as their trainings are expensive. However, we still\nneed to use one batch of data from the training set for the scale\nfactor selection. Intuitively, inspired by the 3ğœ rule, we take 3ğœ\nas the initial scale factor. Then the algorithm will search for the\nbest scale factor with the smallest MSE within a specific range of\nthis baseline, which shows good results in our evaluations. For\nquantization-aware training (QAT) [57], we can get a suitable scale\nfactor by retraining it with the straight-through estimator (STE) [5].\n4 OLIVE ARCHITECTURE\nThis section presents how to integrate OliVe in GPU and output-\nstationary systolic array architecture. We then present the hardware\ndecoder for the aforementioned outlier-victim pair encoding and\noutlier data type. On these architectures, our proposedOliVe archi-\ntecture can directly support the mixed precision [60, 72] and mixed\ndata type [60, 72], which are efficient for quantizing DNN tensors\nthat have different importance and distribution.\n4.1 GPU Tensor Core\nWe first describe how to integrate the OliVe design into the ten-\nsor core architecture of GPU in the Fig. 6a. We employ Turing\narchitecture [59] as our baseline GPU, which has 68 streaming mul-\ntiprocessors (SMs), and each SM has eight tensor cores (544 in total),\nas shown in Tbl. 5. According to the modeling of prior work [67],\neach tensor core has two octets, which have eight FEDPs (four-\nelement dot product). As such, there are68 Ã—8 Ã—2 Ã—8 Ã—4 = 34,816\n16-bit float multipliers. The Turing architecture can originally sup-\nport mixed-precision computation. For example, the RTX 2080Ti\nGPU with Turing architecture [59] provides 107.6, 215.2, and 430.3\nTOPS (tera operations per second) for 16-bit float, 8-bit int, and\n4-bit int, respectively. Therefore, we assume that the tensor core\ncan simultaneously support 8-bit 8EDP (eight-element dot product)\nand 4-bit 16EDP (16-element dot product), as shown in Fig. 6a.\nArchitecture SM TC 16-bit Unit 8-bit Unit 4-bit Unit\nTuring[59] 68 544 34,816 69,632 139,264\nTable 5: The Turing GPU architecture.\nWe can easily embed our proposed OliVe architecture in GPU,\nwhich adopts the SIMD architecture. We first put the 4-bit outlier-\nvictim pair decoders (Fig. 6b) for each 16EDP. To support the new\nOliVe data types, we add an adder and a shifter for each 16EDP.\nSimilarly, we also design the 8-bit decoder for the 8EDP units.\n4.2 Decoders\nOutlier-Victim Pair Decoder. To support outlier-victim pair de-\ncoding, we design a new decoder that can be easily embedded in\nexisting accelerators. As shown in Fig. 6b, the decoder reads 1 byte,\nwhich is the smallest addressable memory unit in many architec-\ntures, and exactly one value pair. Then, the decoder transforms the\noutlier identifier 10002 to 0 and decodes the outlier value with the\noutlier decoder. To accommodate the computation of the outlier\nabfloat values, the decoder will generate an exponent-integer pair.\nTherefore, the decoder needs to append a 00002 as the exponent\nnumber for the normal int4 data type. For flint4, we exploit its\noriginal decoder [32] to get the exponent-integer pair.\nOutlier Decoder. The above OVP decoder contains an outlier\ndecoder for outlier values with the E2M1 abfloat data type. Fig. 7\nshows the details of the 4-bit abfloat decoder design. For a 4-bit\nE2M1 abfloat number ğ‘¥ = (ğ‘2ğ‘1ğ‘0)2, following equations decode\nexponent and integer:\nexponent = bias +(b2b1)2\ninteger =\n(\n0 ğ‘–ğ‘“ ğ‘¥= 0002\n(1ğ‘0)2 ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’\nInput\nBias\n0:2\n3\n001\n+\n0000\n0\n1\n+1 1\n0\nExponent\nInteger\n1:2\n0 0\n1:3\n0:3\n4:7\nFigure 7: The 4-bit abfloat decoder for outlier values.\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA Guo and Tang, et al.\nDRAM\nWeight Buffer\nInput Buffer\nOutput Buffer\nâ€¦â€¦â€¦â€¦\nâ€¦ â€¦ â€¦ â€¦\nExp4 Int4\nExp4Int4\n0:34:7\nâœ–\nâ•\nâ•\n â•\nâ•\nâ•\n32-bit \nAccumulator\n<<\nâ•\nOliVe MAC Unit\n OVP Decoder\nFigure 8: OliVe integration on systolic array.\nFor example, when the bias is 2, a number 01012 is 4810, since\nits exponent is 210 +102 = 410 and base integer is 112 = 310.\nTherefore, its real value is 3 â‰ª4 = 48.\nSimilarly, we also design and implement the 8-bit outlier-victim\npair decoder and the E4M3 abfloat outlier decoder, which are\nstraightforward extensions of 4-bit instances. As such, we do not\npresent their details due to the limited space.\n4.3 Systolic Array\nThe systolic array (SA) integration is shown in Fig. 8. SA uses the\nsame outlier-victim pair decoder design (Fig. 6b) as GPU, which\nshows the wide applicability of our design. But, unlike GPU, we\nonly place the decoders along the borderlines, which can save most\ndecoders. For example, if the array size isğ‘›Ã—ğ‘š, we only needğ‘›+ğ‘š\ninstead of ğ‘›Ã—ğ‘šdecoders. That is one advantage of SA over the\nGPUâ€™s SIMD architecture. Our proposed OliVe-based data type can\nalso support the systolic array processing element (PE) with an\nextra adder and shifter. We add an extra adder for every four PEs\nto support high-precision quantization, e.g., int8.\n4.4 OliVe MAC unit\nAfter decoding for outlier and normal values, they are all trans-\nformed into unified exponent-integer pairs. To support the decoded\nexponent-integer pair computation, we need to add a shifter and\nan adder for the fixed-point MAC (multiply and accumulation) unit,\nas shown in Fig. 8 and the unit of Fig. 6 4-bit 16EDP. For example,\nwe have two exponent-integer pairs < ğ‘,ğ‘ > and < ğ‘,ğ‘‘ >, where ğ‘\nand ğ‘ are exponents, ğ‘and ğ‘‘ are integers, and < ğ‘,ğ‘ > represents:\n< ğ‘,ğ‘ >= ğ‘ â‰ªğ‘\nThen, we can get the result:\n< ğ‘,ğ‘ > Ã—< ğ‘,ğ‘‘ >\n= (ğ‘Ã—ğ‘‘)â‰ª( ğ‘+ğ‘)\n= < ğ‘+ğ‘,ğ‘ Ã—ğ‘‘ >\nNote that the final result can store with a 32-bit int.\n4.5 Mixed Precision\nAs mentioned in Sec. 3, OliVe quantization can support the int8\nfor normal values and E4M3 abfloat for outlier values. Therefore,\nwe propose the mixed-precision processing element (PE) for the\nhigher precision data types.\n8-bit Int. For the GPU tensor core architecture, it is originally\ndesigned with mixed-precision computation. For the systolic array,\nour architecture naturally supports 8-bit computation with four\n4-bit PEs [ 72]. For an int8 number ğ‘¥, the higher 4 bits and the\nlower 4 bits can be split into two 4-bit numbers â„and ğ‘™, and the ğ‘¥\ncan be represented by:\nğ‘¥ = (â„ğ‘¥ â‰ª4)+ğ‘™ğ‘¥ =< 4,â„ğ‘¥ > +< 0,ğ‘™ğ‘¥ > .\nWe then can multiply two int8 numbers of ğ‘¥ and ğ‘¦:\nğ‘¥Ã—ğ‘¦ = < 4,â„ğ‘¥ > Ã—< 4,â„ğ‘¦ >\n|                      {z                      }\nğ‘ƒğ¸0\n+< 4,â„ğ‘¥ > Ã—< 0,ğ‘™ğ‘¦ >\n|                      {z                      }\nğ‘ƒğ¸1\n+< 0,ğ‘™ğ‘¥ > Ã—< 4,â„ğ‘¦ >\n|                      {z                      }\nğ‘ƒğ¸2\n+< 0,ğ‘™ğ‘¥ > Ã—< 0,ğ‘™ğ‘¦ >\n|                     {z                     }\nğ‘ƒğ¸3\nTherefore, we can use four 4-bit PEs to calculate the above four\nmultiplications and accumulate the products to get the final product\nvalue of ğ‘¥Ã—ğ‘¦.\n8-bit Abfloat Similarly, multiplication of 8-bit abfloat can be\nsupported using the same approach. For an 8-bit abfloat number\nğ‘§, it is first decoded into an exponent ğ‘’ğ‘§ and an integer ğ‘–ğ‘§. For ğ‘–ğ‘§,\nwe similarly split it into ğ‘–ğ‘§ = (â„ğ‘§ << 4)+ğ‘™ğ‘§, then ğ‘§ =< 4 +ğ‘’ğ‘§,â„ğ‘§ >\n+< ğ‘’ğ‘§,ğ‘™ğ‘§ >. Hence the same method can be used to perform 8-bit\nabfloat multiplication with four 4-bit PEs, where theabfloat has\nan extra ğ‘’ğ‘§ than int8.\nIn the most extreme case, two outliers with abfloat may be\nmultiplied together. Because we adopt the 32-bit int as the accu-\nmulator, the maximum multiplicand should not be over\nâˆš\n231 âˆ’1.\nTherefore, for the outlier value with the abfloat type, we will clip\nthe absolute value of the outlier within 215 <\nâˆš\n231 âˆ’1 to avoid\nthe overflow for the int32 accumulators. Our experiments show\nthat the outlier values of the Transformer models are much smaller\nthan 215. Specifically, 215 is about 768ğœ after normalization and\nquantization. As shown in Fig. 2, the maximum value of outliers\ndoes not exceed 325ğœ. Thus, we observe that no outlier is truncated\nin practice.\n4.6 Instruction Set\nFor 4-bit tensor cores, the Turing GPU architecture adopts the\ninstruction mma.s32.s4.s4.s32. These four operands are matrices\nğ· (int32), ğ´(int4), ğµ(int4), and ğ¶ (int32), and ğ· = ğ´Ã—ğµ+ğ¶.\nTo support the OVP-based computation on GPU, we design a new\ninstruction called mmaovp:\nğ‘šğ‘šğ‘ğ‘œğ‘£ğ‘\n|    {z    }\nOVPâˆ’MMA\n.s32.ğ‘œğ‘£ğ‘ğ‘– 4\n|{z}\nint4\n.ğ‘œğ‘£ğ‘ğ‘“ 4\n|{z}\nflint4\n.s32. ğ‘ 4|{z}\nbias\n.\nMoreover, because of the memory-aligned design of the data type,\nOliVe maintains the original programming interface for GPUs. We\ncan replace the original int-based instruction with OVP-based\ninstruction (e.g., mmaovp) to easily construct the OVP-supported\nDNN quantization framework. Therefore, our OliVe framework\nhas comprehensive and practical applicability, which is the most\nsignificant advantage of OliVe.\nOliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization ISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\n5 EVALUATION\nIn this section, we evaluate the LLMâ€™s accuracy with OliVe quanti-\nzation. We also demonstrate OliVeâ€™s area overhead, speedup, and\nenergy efficiency on GPU and systolic array, respectively.\n5.1 Methodology\nFramework and Evaluation Models. To evaluate our OliVe\nquantization framework, we implement it in Pytorch [62]. We eval-\nuate BERT-base [19], BERT-large [19], and BART-base [ 49], the\nthree most commonly used language models, on eight datasets of\nthe GLUE benchmark [78]. In addition, we evaluate BERT-base [19]\nand BART-base [49] on the summarization tasks SQuAD v1.1 and\nSQuAD v2.0 [68]. To valid our quantization framework on large\nlanguage models, we also evaluate GPT2-XL [66], BLOOM-7B1 [70],\nand OPT-6.7B [90] on Wikitext103 [83] and C4 [20] datasets. For\nall models mentioned above, we use state-of-the-art checkpoints\nfrom the huggingface repositories [55].\nQuantization Baselines. We compare OliVe with existing quan-\ntization works, including GOBO [ 85], Outlier Suppression [ 82],\nQ8BERT [86], and ANT [32]. Outlier suppression [82] is the state-\nof-the-art Transformer quantization work. GOBO [85] is also an\noutlier-aware quantization work. Q8BERT [86] is a method for quan-\ntizing GEMM operations to 8-bit. ANT [32] is a hardware-friendly\nquantization framework that achieves state-of-the-art results in\nboth performance and accuracy.\nAccelerator Baselines. We compare the performance and energy\nof OliVe against five DNN quantization accelerators, including\nOLAccel [61], AdaptivFloat [76] (shorted as AdaFloat), GOBO [61],\nANT [ 32], and original int8 tensor cores in GPU [ 59]. OLAc-\ncel [61] first proposed the outlier-aware quantization architecture\nfor CNNs. We extend OLAccel to the Transformer-based models\nwith element-wise mixed-precision weight and activation quanti-\nzation. AdaFloat [76] extends the float type with a tensor-wise\nexponent bias. GOBO [85] is similar to OLAccel, but only supports\nthe weight quantization for Transformer-based networks.\nOliVe Implementation. We implement our decoder in Verilog\nRTL and synthesize it with Synopsys design compiler [ 47] with\na 22 nm TSMC technology library to estimate its area, latency,\nand power. We use CACTI [56] to estimate the area and power of\non-chip memories. We integrate OliVe into GPU and hardware\naccelerator for the end-to-end performance and energy evaluation.\nFor the GPU integration and evaluation, we modify and ex-\ntend GPGPU-Sim 4.0 [ 3] and AccelSim [ 45] with the configura-\ntion of NVIDIA 2080 Ti architecture. We use AccelWattch [ 46],\nGPUWattch [48], and CACTI [56] for the energy estimation. The\nmajority of Transformer layers are matrix multiplication opera-\ntions. For GEMM implementation on the tensor core, we use CUT-\nLASS [44], which is NVIDIAâ€™s open-source implementation.\nFor the accelerator evaluation, we compare AdaFloat, OLAccel\nand ANT with OliVe. We develop a cycle-level simulator to esti-\nmate the overall performance of OliVe based on DnnWeaver [71].\nAlthough DnnWeaver [71] is a FPGA tool set, prior DNN quantiza-\ntion accelerators, which include the BitFusion [72], and ANT [32],\nhave extended its frontend to add the ASIC performance and energy\nsimulation. As OliVe does not redesign the baseline accelerator\nMethod Algorithm CoLA SST-2 MNLI QQP MRPC\nBERTğ‘ğ‘ğ‘ ğ‘’ 32-bit 59.60 93.35 84.94 90.91 87.75\nOurs 4-bit PTQ 59.30 92.43 84.10 90.36 87.99\nANT 4-bit QAT 53.91 92.43 83.45 - -\nANT 4-bit PTQ 42.90 90.48 73.36 78.04 68.87\nOS 4-bit QAT 50.56 91.86 83.05 90.33 84.31\nOS 6-bit PTQ 54.40 91.86 82.02 88.94 83.33\nQ8 8-bit QAT 58.48 92.24 - - -\nBERTğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ 32-bit 63.35 93.46 86.65 91.07 87.99\nOurs 4-bit PTQ 63.99 92.89 84.89 90.14 86.52\nBARTğ‘ğ‘ğ‘ ğ‘’ 32-bit 56.32 93.35 86.45 91.34 87.50\nOurs 4-bit PTQ 54.30 92.89 85.33 91.23 86.76\nOS 4-bit QAT 50.83 92.43 84.57 90.93 87.01\nOS 6-bit PTQ 44.51 90.94 82.98 88.45 80.88\nTable 6: Results on GLUE datasets. Q8 and OS are\nQ8BERT [86] and outlier suppression [82] for short, respec-\ntively. Prior works do not report results in BERT ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ so we\nonly compare against the original full-precision model.\narchitecture, we can directly embed newOliVe-related instructions\nand data format in the simulator without breaking the original sim-\nulation flow. In other words, we have used and modified the open-\nsourced implementaions of BitFusion [72, 73], and ANT [32, 33].\n5.2 Accuracy Results\nWe first evaluate the accuracy ofOliVe quantization framework on\ndifferent tasks and datasets, which is the prerequisite for applying\nit to reduce the inference cost of large language models (LLMs).\nGLUE Dataset. We evaluate BERT-base [19], BERT-large [19] and\nBART-base [49] on eight datasets of GLUE benchmark, but due to\nspace limitation, we only show the results on CoLA, SST-2, MNLI,\nQQP and MRPC datasets in Fig. 6. For the BERT-base model, our\n4-bit PTQ method accuracy drop less than 1% compared to the\noriginal full precision model on all eight datasets and outperforms\nall studied methods including 4-bit, 6-bit, and 8-bit PTQ and QAT\nmethods. Since GOBO [ 85] only quantizes weights, we use the\nsame method to compare with it and the result is shown in Tbl. 7.\nOur method also outperforms the GOBO under the weight-only\nquantization setting. In addition, we evaluate the BERT-large model,\nwhich is evaluated by few prior quantization works due to the larger\nnumber of parameters and hence much more challenging compared\nto BERT-base. The results in Tbl. 6 show the accuracy loss for BERT-\nlarge is around 1% on the five presented datasets and similar results\nare found on other datasets. For the BART-base model, our 4-bit\nPTQ results in Tbl. 6 show around 2% accuracy loss compared to\nthe accuracy of original full-precision in all datasets. In the above\nevaluation, our 4-bit PTQ results are better than all the PTQ and\nmost of the QAT results reported by prior works.\nSQuAD Dataset. We also evaluate the accuracy of OliVe quan-\ntization on summarization task SQuAD [68], which is more chal-\nlenging than the previous GLUE dataset. Tbl. 8 shows the results\non SQuAD v1.1 and SQuAD v2.0 datasets. On both datasets, our\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA Guo and Tang, et al.\n4-bit PTQ method obtains a less than 2% accuracy loss on the BERT-\nbase model and around 3% accuracy loss on the BART-base model,\nwhich is better than the 6-bit PTQ method of the state-of-the-art\nquantization work outlier suppression.\nLarge Language Models. We evaluate the accuracy of OliVe\nfor LLMs under the PTQ setting. LLMsâ€™ inference is challenging\nas it requires significant memory, which makes their retraining\neven more resource-consuming. Thus, the PTQ method without\nretraining is more desirable than the QAT method for LLMs.\nThe recent work [ 18] has shown that the int8 quantization\nhas a significant accuracy drop when the number of parameters\nof the OPT model grows to 6.7B. As shown in Tbl. 9, our 8-bit\nPTQ method has only a negligible perplexity increase on OPT-6.7B\n(lower is better), while the accuracy of theint8-based quantization\nmethod has a significant degradation and is worse than our 4-bit\nPTQ method on the C4 dataset. On GPT2-XL and BLOOM-7B1\nmodels, our 8-bit PTQ method essentially achieves the original\nperplexity, and the 4-bit PTQ method achieves the performance\nclose to int8. For comparison, the accuracy results of int4 and\n4-bit ANT are unacceptable (10-1000Ã—worse than FP32 model).\nTo summarize, our OliVe quantization framework pushes the\nlimit of 4-bit quantization to a new state-of-the-art, as it is able to\nachieve nearly original accuracy for the commonly used language\nmodels including BERT-base, BERT-large, and BART-base on most\ndatasets. Moreover, OliVe also gives the state-of-the-art results of\n4-bit and 8-bit quantization on large language models like GPT2-XL,\nBLOOM-7B1, and OPT-6.7B.\n5.3 GPU Performance and Energy\nWe evaluate LLMs on the GPU simulator, where the batch size is set\nto 2 for GPT-like models and 16 for BERT-like models. ForOliVe,\n4-bit quantization can limit the loss to a relatively small error range.\nGOBO [85] can achieve the original accuracy of all models but has\na significant overhead on compressing weight in DRAM. Note that\nGOBO only quantizes the weight tensors and computes with FP16.\nWe implemented GOBOâ€™s memory organization in the GPU. For\nANT [32], we make all models close to the original accuracy or\nperplexity by mixed precision (BERT-like models [19, 49] with < 1%\nloss and GPT-like models [66, 70, 90] with < 3 perplexity) with the\nPTQ setting. In addition, we also compare the originalint8 of GPU,\nwhich has unacceptable accuracy loss, just for performance and\nMethod Bits MNLI STSB (Pear.)\nBERTğ‘ğ‘ğ‘ ğ‘’ 32 84.94 89.70\nOurs (weights only) 4 84.75 89.62\nGOBOâˆ—(weights only) 4 84.45 88.33\nTable 7: Comparison with GOBO on the MNLI and STSB\ndataset. âˆ—The accuracy of our GOBO implementation\nslightly differs from the number reported in the original pa-\nper [85].\nMethod Bits SQuAD v1.1 SQuAD v2.0\nBERTğ‘ğ‘ğ‘ ğ‘’ 32 88.28/80.82 77.34/73.60\nOurs 4 86.38/78.16 75.90/72.08\nOutlier Suppression 6 84.48/75.53 74.69/70.55\nBARTğ‘ğ‘ğ‘ ğ‘’ 32 91.63/84.79 80.82/77.41\nOurs 4 88.15/79.87 77.37/73.69\nOutlier Suppression 6 83.68/75.34 74.44/70.36\nTable 8: PTQ results on SQuAD datasets.\nenergy comparison to GPU baseline. We compare the GPU archi-\ntecture integrated with our OliVe design against various baselines.\nThe performance and energy results are shown in Fig. 9.\nPerformance. Fig. 9a compares the speedup values of different\nquantization methods on GPUs. OliVe achieves the best perfor-\nmance and has higher speedups on the larger language models than\nGOBO. Due to the FP16 computation and weight-only quantization,\nGOBO [85] achieves the lowest performance among all studied\ndesigns. In contrast, OliVe quantizes both activation and weight to\nlow bits and does not increase the memory access overhead. This\navoids performance degradation when the number of parameters\nincreases. The PTQ seriously degrades the accuracy of ANT [32] as\nit cannot handle outliers. In ANT, 80% of layers ends up usingint8\nquantization so the performance results between ANT and int8\nare close. On average, OliVe achieves 4.5Ã—, 2.7Ã—, and 2.4Ã—speedup\nvalues over GOBO, int8, and ANT, respectively.\nEnergy. Fig. 9b shows the normalized energy comparison of differ-\nent designs, including constant, static, and dynamic power. And the\ndynamic power includes DRAM, L2 cache, L1 data cache, shared\nmemory, register file, and processing elements (CUDA core and\ntensor core). The L1 contains the sum of the L1 cache and shared\nmemory energy. OliVe has the lowest energy due to the aligned\n4-bit design and GPU compatibility. Due to the worse accuracy\nresult of the mixed precision, ANT is also close to int8 on the\nenergy. Overall, 4-bitOliVe is very hardware-friendly so that it can\ntake full advantage of the energy savings with lower bits. OliVe\nachieves average 4.0Ã—, 2.3Ã—, and 2.0Ã—energy reduction over GOBO,\nint8, and ANT, respectively.\nArea. To measure the overhead of OliVe decoder on the GPU, we\nscale the OliVe decoder to 12 ğ‘›ğ‘š, which is the same manufacturing\nprocess as RTX 2080 Ti [59] and calculate the tile area. According\nto Tbl. 5, there are 139,264 4-bit decoders and 69,632 8-bit decoders\nMethod GPT2-XL BLOOM-7B1 OPT-6.7B\nWiki C4 Wiki C4 Wiki C4\nFP32 17.48 16.30 13.05 14.94 22.14 10.63\nint8 18.29 17.35 14.04 16.18 37.45 74.30\n8-bit OliVe 17.49 16.37 13.13 15.04 22.34 10.73\nint4 1E+4 9E+3 3E+6 9E+6 5E+2 1E+2\n4-bit ANT 27.79 27.35 23.22 27.36 4E+4 4E+4\n4-bit OliVe 19.11 18.08 15.16 17.18 55.44 32.41\nTable 9: PTQ results on large language models. The accuracy\nmetric is perplexity, and lower is better.\nOliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization ISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\n4.5\n2.7\n2.4\n0\n2\n4\n6\n8\nBERT-base BERT-large BART-base GPT2-XL BLOOM-7B1 Geomean\nSpeedup\nOlive ANT Int8 GOBO\n(a) Speedup on GPU.\n0.25\n0.43\n0.49\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nOliVe\nANT\nINT8\nGOBO\nOliVe\nANT\nINT8\nGOBO\nOliVe\nANT\nINT8\nGOBO\nOliVe\nANT\nINT8\nGOBO\nOliVe\nANT\nINT8\nGOBO\nOliVe\nANT\nINT8\nGOBO\nBERT-base BERT-large BART-base GPT2-XL BLOOM-7B1 Geomean\nNorm. Energy\nConst Static Dram+L2 L1+Reg Core\n(b) Normalized energy on GPU.\nFigure 9: Comparison of four different designs on GPU.\non the GPU die and their area is shown in Tbl. 10. Since the GPU\ndie size of RTX 2080 Ti is 754 ğ‘šğ‘š2, the 4-bit decoder and 8-bit\ndecoder only account for 0.250% and 0.166% of the entire GPU area\nrespectively, which we believe is a tiny and worthy overhead.\n5.4 Accelerator Performance and Energy\nAs explained in Sec. 5.1, we also integrate OliVe to the systolic-\narray-based hardware accelerator and compare its performance and\nenergy against existing designs of ANT [ 32], OLAccel [61], and\nAdaFloat [76]. Similar to its GPU implementation, ANT is a mixed-\nprecision design. Since AdaFloat does not support mixed precision,\nwe only provide the 8-bit quantization results. All accelerators can\nachieve close to original accuracy for all Transformer models.\nPerformance. As shown in Fig. 10a, OliVe has the most signifi-\ncant advantage in latency speedup. Owing to its inability to deal\nwith outliers, the performance of ANT is similar to OLAccel on\nmost models. The speedup values of OliVe are very similar on all\nmodels, and they do not change with the increasing number of\nmodel parameters. On average, OliVe achieves 4.8Ã—, 3.8Ã—, and 3.7Ã—\nspeedup value over AdaFloat, OLAccel, and ANT, respectively.\nEnergy. Fig. 10b shows the normalized energy consumption of\ndifferent designs composed of static and dynamic energy (DRAM,\non-chip buffer, and core).OliVe has the lowest energy consumption.\nCompared to OLAccel, OliVe has a significant advantage in terms\nComponent Number Area (ğ‘šğ‘š2) Area Ratio\n4-bit Decoder (13.53ğœ‡ğ‘š2) 139,264 1.88 0.250%\n8-bit Decoder (18.00ğœ‡ğ‘š2) 69,632 1.25 0.166%\nTable 10: The area of OliVe decoder on RTX 2080 Ti.\n4.8\n1.29\n1.28\n0\n1\n2\n3\n4\n5\n6\nBERT-base BERT-large BART-base GPT2-XL BLOOM-7B1 Geomean\nSpeedup\nOlive ANT OLAccel AdaFloat\n(a) Speedup on hardware accelerator.\n0.27\n0.88\n0.56\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nOliVe\nANT\nOLAccel\nAdaFloat\nOliVe\nANT\nOLAccel\nAdaFloat\nOliVe\nANT\nOLAccel\nAdaFloat\nOliVe\nANT\nOLAccel\nAdaFloat\nOliVe\nANT\nOLAccel\nAdaFloat\nOliVe\nANT\nOLAccel\nAdaFloat\nBERT-base BERT-large BART-base GPT2-XL BLOOM-7B1 Geomean\nNorm. Energy\nStatic Dram Buffer Core\n(b) Normalized energy on hardware accelerator.\nFigure 10: Comparison of different designs on accelerators.\nof static and DRAM. Worse mixed-precision results increase ANT\nenergy consumption, which is even close to AdaFloat in BLOOM-\n7B1 model. On average, OliVe achieves 3.7Ã—, 2.1Ã—, and 3.3Ã—energy\nreduction over AdaFloat, OLAccel, and ANT, respectively.\nArea. Tbl. 11 shows the area breakdown of OliVe-based systolic\narray architecture under 22 ğ‘›ğ‘šprocess. In this scenario, the 4-bit\nand 8-bit decoders introduce about 2.2% and 1.5% overhead of the\ncore area, respectively, which is inconsiderable compared to the\narea of PEs in the array. Considering on-chip memory structures,\nthe overall area overhead would be even smaller. In addition, we\nalso scale other accelerators to 22ğ‘›ğ‘šusing DeepScaleTool [69] and\nget similar results to those numbers. Note that we implement all\naccelerators with a similar area size. The small area overhead of our\nOliVe directly benefits from the carefully-designed outlier-victim\npair (OVP) encoding.\n6 RELATED WORK AND DISCUSSION\nThis section presents and discusses research on DNN acceleration\nand compression. With the growing computation requirements of\nDNN models, it is crucial to design the algorithms and architecture\nto accelerate DNN models. Various compression methods, such\nComponent Number Area (ğ‘šğ‘š2) Area Ratio\n4-bit Decoder (37.22ğœ‡ğ‘š2) 128 0.00476 2.2%\n8-bit Decoder (49.50ğœ‡ğ‘š2) 64 0.00317 1.5%\n4-bit PE (50.01ğœ‡ğ‘š2) 4096 0.205 96.3%\nTable 11: Area breakdown of OliVe under 22 ğ‘›ğ‘šprocess.\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA Guo and Tang, et al.\nas pruning and quantization, have been proposed to exploit the\nredundancy property of DNNs.\nDNN Acceleration. In the past few years, various architectures [10,\n12, 13, 23, 25, 34, 35, 51, 63, 64, 87, 96] have been proposed to match\nthe computation characteristics of DNN models. To accelerate the\nDNN system, most optimizations focus on compilation [11, 40, 91,\n92, 95, 97] and scheduling [8, 9, 15â€“17, 31, 52â€“54, 84].\nThe DNN acceleration highly relies on the performance of ma-\ntrix multiplication. Therefore, several works focus on improving\ndata reuse and simplifying control logic through a tailored dataflow\narchitecture for matrix multiplication[10, 12, 25, 34, 35, 41, 63, 64,\n87, 96]. TPU [41] introduces a highly optimized dataflow architec-\nture that efficiently reuses data across multiple computation stages.\nModern GPUs [60] now incorporate matrix multiplication accelera-\ntors, such as tensor core, optimized for SIMD operations to enhance\nDNN workload acceleration further.\nPruning. Pruning means removing a portion of weight, input, or\noutput of DNN layers, resulting in a sparse model with reduced\nmodel size. However, a significant reduction leads to irregular mem-\nory accesses, which are negative for the acceleration of inference\nand training. To address this issue, researchers propose several\nsparse optimizations in algorithms and hardware architectures to\nreduce inefficient computation [2, 26â€“29, 36, 64, 65, 80, 89, 94, 98].\nIn addition, a sparse tensor core is introduced in NVIDIA Ampere\nGPU architecture [1] to support the 2:4 structured sparsity.\nQuantization. Quantization is another effective and efficient way\nto reduce the DNN model size and computation burden. There are\ntwo popular quantization methods, i.e., quantization-aware training\n(QAT) [38, 50, 81, 99] and post-training quantization (PTQ) [30, 35,\n38, 81]. QAT allows the model to adapt to quantization noise by\nretraining. PTQ is very effective to implement since it converts the\noriginal FP32 model directly into a lower-bit model without the\ntraining data and pipeline. Thus, PTQ is more feasible for language\nmodels at billion scales.\nBy quantizing data to low bit-width, quantization accelerators\ncan significantly reduce memory bandwidth requirements and in-\ncrease the computation speed. BitFusion [72] combines the low-bit\nPEs to support different bit-width quantization. OLAccel [61] uti-\nlizes 16-bit MAC to the first layer and 4-bit MAC to other layers.\nDRQ [75] quantizes data in sensitive and insensitive areas with\ndifferent precision, which is similar to outlier-aware quantization.\nGOBO [85] is an accelerator that takes advantage of outlier-aware\nquantization, which quantizes the outliers of weights with higher\nprecision. However, the outlier-aware quantization accelerators\nmentioned above have unaligned memory accesses, resulting in\nadditional overhead and a limited computing speed. ANT [32] pro-\nposes a fixed-length adaptive quantization framework but only\ntakes the distribution of tensors into account and ignores the im-\nportance of outliers. In contrast, our proposed novel OliVe quanti-\nzation framework can handle outlier values in a memory-aligned\nand hardware-friendly way.\nAdaptivFloat [76] is similar to abfloat in adding a bias to the\nexponent, but the motivations and how the bias is determined are\ndifferent. AdaptivFloat is to adapt to the dynamic ranges of different\nlayers and calculates the optimal bias at a layer granularity using its\nalgorithm. Our abfloat is to make full use of the encoding range,\nso it simply adds a uniform bias to all encoding values to skip the\nrange of normal values, which is simpler to implement.\nGPU Architecture. NVIDIA has been updating its new genera-\ntions of GPUs, e.g., Ampere architecture [1], which adds the sparse\ntensor core for structured sparsity in DNNs and compute data com-\npression to increase the memory access bandwidth. The structured\nsparsity for tensor cores is orthogonal to our proposed quantization\nas our element-wise quantization does not affect (sparse) tensor\ncore dataflow. Ampere GPUâ€™s compute data compression can com-\npress zero values and similar bytes in DRAM and L2 cache. As such,\nit is lossless and therefore general-purpose. It is also transparent and\northogonal to OliVe, which does not modify the memory system.\nIn contrast, prior quantization work [85] perform compression at\nthe DRAM-level, which could be impacted by the data compression\nin Ampere GPUs.\nOn the other hand, DNN quantization is a lossy compression.\nWe believe the strictly lossless compression would have limited\nbenefits for DNN quantization. Thus, our work could complement\nAmpereâ€™s current compute data compression as a special-purpose\nsolution. Since existing GPU simulators [3, 45] cannot support data\ncompression, we will continue to follow up and study this problem\nin the future work.\n7 CONCLUSION\nIn this work, we propose a novel outlier-victim pair (OVP) quantiza-\ntion, which can handle outlier values with low hardware overhead\nand achieve high performance gains. The key insight is to sacrifice\nthe normal values next to those essential outliers (called victims)\nto accommodate them. The OVP encoding designed based on this\nidea is able to make outliers and normal values globally identical\nbut locally distinguishable. To the best of our knowledge, OliVe\npushes the limit of 4-bit quantization to a new state-of-the-art, as it\nis able to achieve nearly original accuracy for commonly used lan-\nguage models. Moreover, our architecture design can be efficiently\nintegrated into existing hardware accelerators such as tensor core\nand systolic array. Finally,OliVe-based accelerator surpasses the\nexisting outlier-aware accelerator, GOBO, by 4.5Ã—speedup and 4.0Ã—\nenergy reduction, respectively.\nACKNOWLEDGMENTS\nThis work was supported by the National Key R&D Program of\nChina under Grant 2022YFB4501401, the National Natural Science\nFoundation of China (NSFC) grant (62222210, and 62072297, and\n61832006). The authors would like to thank the anonymous review-\ners for their constructive feedback for improving the work. We\nalso thank Tailong Wangliu, Shuangjie Ruan for their continuous\nsupport.\nREFERENCES\n[1] 2020. Nvidia ampere architecture whitepaper. https://images.nvidia.com/aem-\ndam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf.\n[2] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright\nJerger, and Andreas Moshovos. 2016. Cnvlutin: Ineffectual-neuron-free deep\nneural network computing. ACM SIGARCH Computer Architecture News 44, 3\n(2016), 1â€“13.\n[3] Ali Bakhoda, George Yuan, Wilson Fung, Henry Wong, and Tor Aamodt. 2009.\nAnalyzing CUDA workloads using a detailed GPU simulator. ISPASS 2009 -\nOliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization ISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\nInternational Symposium on Performance Analysis of Systems and Software , 163 â€“\n174. https://doi.org/10.1109/ISPASS.2009.4919648\n[4] Ron Banner, Yury Nahshan, and Daniel Soudry. 2019. Post training 4-bit quan-\ntization of convolutional networks for rapid-deployment. Advances in Neural\nInformation Processing Systems 32 (2019).\n[5] Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. 2013. Estimating or\npropagating gradients through stochastic neurons for conditional computation.\narXiv preprint arXiv:1308.3432 (2013).\n[6] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and\nKurt Keutzer. 2020. Zeroq: A novel zero shot quantization framework. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n13169â€“13178.\n[7] Zhaowei Cai and Nuno Vasconcelos. 2020. Rethinking differentiable search for\nmixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 2349â€“2358.\n[8] Quan Chen, Hailong Yang, Minyi Guo, Ram Srivatsa Kannan, Jason Mars, and\nLingjia Tang. 2017. Prophet: Precise QoS Prediction on Non-Preemptive Acceler-\nators to Improve Utilization in Warehouse-Scale Computers. InProceedings of the\nTwenty-Second International Conference on Architectural Support for Programming\nLanguages and Operating Systems, ASPLOS 2017, Xiâ€™an, China, April 8-12, 2017 .\nACM, 17â€“32. https://doi.org/10.1145/3037697.3037700\n[9] Quan Chen, Hailong Yang, Jason Mars, and Lingjia Tang. 2016. Baymax:\nQoS Awareness and Increased Utilization for Non-Preemptive Accelerators in\nWarehouse Scale Computers. In Proceedings of the Twenty-First International\nConference on Architectural Support for Programming Languages and Operat-\ning Systems, ASPLOS 2016, Atlanta, GA, USA, April 2-6, 2016 . ACM, 681â€“696.\nhttps://doi.org/10.1145/2872362.2872368\n[10] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,\nand Olivier Temam. 2014. Diannao: A small-footprint high-throughput accel-\nerator for ubiquitous machine-learning. ACM SIGARCH Computer Architecture\nNews 42, 1 (2014), 269â€“284.\n[11] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q. Yan,\nHaichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos\nGuestrin, and Arvind Krishnamurthy. 2018. TVM: An Automated End-to-End\nOptimizing Compiler for Deep Learning. In13th USENIX Symposium on Operating\nSystems Design and Implementation, OSDI 2018, Carlsbad, CA, USA, October 8-10,\n2018. USENIX Association, 578â€“594. https://doi.org/10.5555/3291168.3291211\n[12] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li,\nTianshi Chen, Zhiwei Xu, Ninghui Sun, et al . 2014. Dadiannao: A machine-\nlearning supercomputer. In 2014 47th Annual IEEE/ACM International Symposium\non Microarchitecture. IEEE, 609â€“622.\n[13] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. 2016. Eyeriss:\nAn energy-efficient reconfigurable accelerator for deep convolutional neural\nnetworks. IEEE journal of solid-state circuits 52, 1 (2016), 127â€“138.\n[14] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,\nVijayalakshmi Srinivasan, and Kailash Gopalakrishnan. 2018. Pact: Parameterized\nclipping activation for quantized neural networks.arXiv preprint arXiv:1805.06085\n(2018).\n[15] Yujeong Choi, Yunseong Kim, and Minsoo Rhu. 2021. Lazy Batching: An\nSLA-aware Batching System for Cloud Machine Learning Inference. In IEEE\nInternational Symposium on High-Performance Computer Architecture, HPCA\n2021, Seoul, South Korea, February 27 - March 3, 2021 . IEEE, 493â€“506. https:\n//doi.org/10.1109/HPCA51647.2021.00049\n[16] Weihao Cui, Mengze Wei, Quan Chen, Xiaoxin Tang, Jingwen Leng, Li Li, and\nMingyi Guo. 2019. Ebird: Elastic Batch for Improving Responsiveness and\nThroughput of Deep Learning Services. In 37th IEEE International Conference on\nComputer Design, ICCD 2019, Abu Dhabi, United Arab Emirates, November 17-20,\n2019. IEEE, 497â€“505. https://doi.org/10.1109/ICCD46524.2019.00075\n[17] Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui Li, Deze Zeng, Chao Li, and\nMinyi Guo. 2022. DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for\nEfficient Processing of DNN Services on GPUs. In 2022 USENIX Annual Technical\nConference (USENIX ATC 22) . 183â€“198.\n[18] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm.\nint8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint\narXiv:2208.07339 (2022).\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[20] Jesse Dodge, Maarten Sap, Ana MarasoviÄ‡, William Agnew, Gabriel Ilharco,\nDirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting\nLarge Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.\narXiv:arXiv:2104.08758\n[21] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,\nand Kurt Keutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of\nneural networks. Advances in neural information processing systems 33 (2020),\n18518â€“18529.\n[22] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer.\n2019. Hawq: Hessian aware quantization of neural networks with mixed-\nprecision. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 293â€“302.\n[23] Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo,\nXiaobing Feng, Yunji Chen, and Olivier Temam. 2015. ShiDianNao: Shifting vision\nprocessing closer to the sensor. In Proceedings of the 42nd Annual International\nSymposium on Computer Architecture . 92â€“104.\n[24] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer.\n2021. AI and Memory Wall. RiseLab Medium Post (2021).\n[25] Vinayak Gokhale, Jonghoon Jin, Aysegul Dundar, Berin Martini, and Eugenio\nCulurciello. 2014. A 240 g-ops/s mobile coprocessor for deep neural networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition\nworkshops. 682â€“687.\n[26] Yue Guan, Jingwen Leng, Chao Li, Quan Chen, and Minyi Guo. 2020. How\nFar Does BERT Look At: Distance-based Clustering and Analysis of BERT â€²s\nAttention. arXiv preprint arXiv:2011.00943 (2020).\n[27] Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, and Minyi Guo. 2022. Tran-\nskimmer: Transformer Learns to Layer-wise Skim.arXiv preprint arXiv:2205.07324\n(2022).\n[28] Yue Guan, Zhengyi Li, Zhouhan Lin, Yuhao Zhu, Jingwen Leng, and Minyi Guo.\n2022. Block-skim: Efficient question answering for transformer. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , Vol. 36. 10710â€“10719.\n[29] Cong Guo, Bo Yang Hsueh, Jingwen Leng, Yuxian Qiu, Yue Guan, Zehuan Wang,\nXiaoying Jia, Xipeng Li, Minyi Guo, and Yuhao Zhu. 2020. Accelerating sparse dnn\nmodels without hardware-support via tile-wise sparsity. In SC20: International\nConference for High Performance Computing, Networking, Storage and Analysis .\nIEEE, 1â€“15.\n[30] Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu,\nFan Yang, Yuhao Zhu, and Minyi Guo. 2022. SQuant: On-the-Fly Data-Free\nQuantization via Diagonal Hessian Approximation. In International Conference\non Learning Representations . https://openreview.net/forum?id=JXhROKNZzOc\n[31] Cong Guo, Yuxian Qiu, Jingwen Leng, Chen Zhang, Ying Cao, Quanlu Zhang,\nYunxin Liu, Fan Yang, and Minyi Guo. 2022. Nesting Forward Automatic Differ-\nentiation for Memory-Efficient Deep Neural Network Training. In 2022 IEEE 40th\nInternational Conference on Computer Design (ICCD) . IEEE, 738â€“745.\n[32] Cong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi\nGuo, and Yuhao Zhu. 2022. ANT: Exploiting Adaptive Numerical Data Type for\nLow-bit Deep Neural Network Quantization. In2022 55th IEEE/ACM International\nSymposium on Microarchitecture (MICRO) . IEEE, 1414â€“1433.\n[33] Cong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi\nGuo, and Yuhao Zhu. 2022. ANT github repository. https://github.com/clevercool/\nANT_Micro22.\n[34] Cong Guo, Yangjie Zhou, Jingwen Leng, Yuhao Zhu, Zidong Du, Quan Chen,\nChao Li, Bin Yao, and Minyi Guo. 2020. Balancing Efficiency and Flexibility for\nDNN Acceleration via Temporal GPU-Systolic Array Integration. In 2020 57th\nACM/IEEE Design Automation Conference (DAC) . 1â€“6.\n[35] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.\n2015. Deep learning with limited numerical precision. In International conference\non machine learning . PMLR, 1737â€“1746.\n[36] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing\ndeep neural networks with pruning, trained quantization and huffman coding.\narXiv preprint arXiv:1510.00149 (2015).\n[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[38] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew\nHoward, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and\ntraining of neural networks for efficient integer-arithmetic-only inference. In\nProceedings of the IEEE conference on computer vision and pattern recognition .\n2704â€“2713.\n[39] Shubham Jain, Swagath Venkataramani, Vijayalakshmi Srinivasan, Jungwook\nChoi, Kailash Gopalakrishnan, and Leland Chang. 2019. BiScaled-DNN: Quantiz-\ning long-tailed datastructures with two scale factors for deep neural networks.\nIn 2019 56th ACM/IEEE Design Automation Conference (DAC) . IEEE, 1â€“6.\n[40] Zhihao Jia, Oded Padon, James J. Thomas, Todd Warszawski, Matei Zaharia, and\nAlex Aiken. 2019. TASO: optimizing deep learning computation with automatic\ngeneration of graph substitutions. In Proceedings of the 27th ACM Symposium\non Operating Systems Principles (SOSP) . ACM, 47â€“62. https://doi.org/10.1145/\n3341301.3359630\n[41] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B.\nJablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas\nNorrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou, and David\nPatterson. 2021. Ten Lessons From Three Generations Shaped Googleâ€™s TPUv4i:\nIndustrial Product. In 2021 ACM/IEEE 48th Annual International Symposium on\nComputer Architecture (ISCA) .\n[42] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,\nRaminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017.\nISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA Guo and Tang, et al.\nIn-datacenter performance analysis of a tensor processing unit. In Proceedings of\nthe 44th annual international symposium on computer architecture . 1â€“12.\n[43] Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun\nKwak, Sung Ju Hwang, and Changkyu Choi. 2019. Learning to quantize deep\nnetworks by optimizing quantization intervals with task loss. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 4350â€“4359.\n[44] Andrew Kerr, Haicheng Wu, Manish Gupta, Dustyn Blasig, Pradeep Ramini,\nDuane Merrill, Aniket Shivam, Piotr Majcher, Paul Springer, Markus Hohnerbach,\nJin Wang, and Matt Nicely. 2022. CUTLASS. https://github.com/NVIDIA/cutlass\n[45] Mahmoud Khairy, Zhesheng Shen, Tor Aamodt, and Timothy Rogers. 2020. Accel-\nSim: An Extensible Simulation Framework for Validated GPU Modeling. 473â€“486.\nhttps://doi.org/10.1109/ISCA45697.2020.00047\n[46] Mahmoud Khairy, Zhesheng Shen, Tor M Aamodt, and Timothy G Rogers. 2020.\nAccel-Sim: An extensible simulation framework for validated GPU modeling. In\n2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture\n(ISCA). IEEE, 473â€“486.\n[47] Pran Kurup and Taher Abbasi. 2012. Logic synthesis using Synopsys Â®. Springer\nScience & Business Media.\n[48] Jingwen Leng, Tayler Hetherington, Ahmed ElTantawy, Syed Gilani, Nam Kim,\nTor Aamodt, and Vijay Janapa Reddi. 2013. GPUWattch: enabling energy opti-\nmizations in GPGPUs. ACM SIGARCH Computer Architecture News 41 (07 2013).\nhttps://doi.org/10.1145/2508148.2485964\n[49] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising\nsequence-to-sequence pre-training for natural language generation, translation,\nand comprehension. arXiv preprint arXiv:1910.13461 (2019).\n[50] Zhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian Gao, Jing-\nwen Leng, and Minyi Guo. 2022. Efficient Activation Quantization via Adaptive\nRounding Border for Post-Training Quantization. arXiv preprint arXiv:2208.11945\n(2022).\n[51] Daofu Liu, Tianshi Chen, Shaoli Liu, Jinhong Zhou, Shengyuan Zhou, Olivier\nTeman, Xiaobing Feng, Xuehai Zhou, and Yunji Chen. 2015. PuDianNao: A Poly-\nvalent Machine Learning Accelerator. InProceedings of the Twentieth International\nConference on Architectural Support for Programming Languages and Operating\nSystems. 369â€“381.\n[52] Zihan Liu, Jingwen Leng, Zhihui Zhang, Quan Chen, Chao Li, and Minyi Guo.\n2022. VELTAIR: towards high-performance multi-tenant deep learning services\nvia adaptive compilation and scheduling. In ASPLOS â€™22: 27th ACM International\nConference on Architectural Support for Programming Languages and Operating\nSystems, Lausanne, Switzerland, 28 February 2022 - 4 March 2022 , Babak Falsafi,\nMichael Ferdman, Shan Lu, and Thomas F. Wenisch (Eds.). ACM, 388â€“401. https:\n//doi.org/10.1145/3503222.3507752\n[53] David Lo, Liqun Cheng, Rama Govindaraju, Parthasarathy Ranganathan, and\nChristos Kozyrakis. 2015. Heracles: improving resource efficiency at scale. In\nProceedings of the 42nd Annual International Symposium on Computer Architecture\n(ISCA). https://doi.org/10.1145/2749469.2749475\n[54] Jason Mars, Lingjia Tang, Robert Hundt, Kevin Skadron, and Mary Lou Soffa.\n2011. Bubble-Up: increasing utilization in modern warehouse scale computers via\nsensible co-locations. In IEEE/ACM International Symposium on Microarchitecture\n(MICRO). https://doi.org/10.1145/2155620.2155650\n[55] ModelTC. 2022. repositories. https://huggingface.co/ModelTC.\n[56] Naveen Muralimanohar, Rajeev Balasubramonian, and Norman P Jouppi. 2009.\nCACTI 6.0: A tool to model large caches. HP laboratories 27 (2009), 28.\n[57] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart\nvan Baalen, and Tijmen Blankevoort. 2021. A white paper on neural network\nquantization. arXiv preprint arXiv:2106.08295 (2021).\n[58] Nvidia. 2017. NVIDIA Tesla V100 GPU Architecture. InTechnical report. NVIDIA.\n[59] Nvidia. 2018. NVIDIA Turing GPU Architecture. In Technical report. NVIDIA.\n[60] Nvidia. 2020. NVIDIA A100 tensor core architecture. InTechnical report. NVIDIA.\n[61] Eunhyeok Park, Dongyoung Kim, and Sungjoo Yoo. 2018. Energy-efficient neural\nnetwork â€˜accelâ€™erator based on outlier-aware low-precision computation. In 2018\nACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA) .\nIEEE, 688â€“698.\n[62] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPytorch: An imperative style, high-performance deep learning library. Advances\nin neural information processing systems 32 (2019), 8026â€“8037.\n[63] Maurice Peemen, Arnaud AA Setio, Bart Mesman, and Henk Corporaal. 2013.\nMemory-centric accelerator design for convolutional neural networks. In 2013\nIEEE 31st International Conference on Computer Design (ICCD) . IEEE, 13â€“19.\n[64] Eric Qin, Ananda Samajdar, Hyoukjun Kwon, Vineet Nadella, Sudarshan Srini-\nvasan, Dipankar Das, Bharat Kaul, and Tushar Krishna. 2020. Sigma: A sparse\nand irregular gemm accelerator with flexible interconnects for dnn training. In\n2020 IEEE International Symposium on High Performance Computer Architecture\n(HPCA). IEEE, 58â€“70.\n[65] Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, and\nYuhao Zhu. 2019. Adversarial Defense Through Network Profiling Based Path\nExtraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) .\n[66] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).\n[67] Md Aamir Raihan, Negar Goli, and Tor M Aamodt. 2019. Modeling deep learning\naccelerator enabled gpus. In 2019 IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS) . IEEE, 79â€“92.\n[68] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics.\n[69] Satyabrata Sarangi and Bevan Baas. 2021. DeepScaleTool: A Tool for the Accurate\nEstimation of Technology Scaling in the Deep-Submicron Era. In 2021 IEEE\nInternational Symposium on Circuits and Systems (ISCAS) . IEEE, 1â€“5.\n[70] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel\nHesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias\nGallÃ©, et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language\nModel. arXiv preprint arXiv:2211.05100 (2022).\n[71] Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung Kim,\nChenkai Shao, Asit Mishra, and Hadi Esmaeilzadeh. 2016. From high-level deep\nneural models to FPGAs. In 2016 49th Annual IEEE/ACM International Symposium\non Microarchitecture (MICRO) . IEEE, 1â€“12.\n[72] Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas\nChandra, and Hadi Esmaeilzadeh. 2018. Bit fusion: Bit-level dynamically compos-\nable architecture for accelerating deep neural network. In 2018 ACM/IEEE 45th\nAnnual International Symposium on Computer Architecture (ISCA) . IEEE, 764â€“775.\n[73] Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas\nChandra, and Hadi Esmaeilzadeh. 2018. Bitfusion github repository. https:\n//github.com/hsharma35/bitfusion.\n[74] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 8815â€“8821.\n[75] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing,\nand Xiaoyao Liang. 2020. Drq: dynamic region-based quantization for deep neural\nnetwork acceleration. In 2020 ACM/IEEE 47th Annual International Symposium\non Computer Architecture (ISCA) . IEEE, 1010â€“1021.\n[76] Thierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi,\nAlexander Rush, David Brooks, and Gu-Yeon Wei. 2020. Algorithm-hardware co-\ndesign of adaptive floating-point encodings for resilient deep learning inference.\nIn 2020 57th ACM/IEEE Design Automation Conference (DAC) . IEEE, 1â€“6.\n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[78] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. arXiv preprint arXiv:1804.07461 (2018).\n[79] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. 2019. Haq: Hardware-\naware automated quantization with mixed precision. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 8612â€“8620.\n[80] Yang Wang, Chen Zhang, Zhiqiang Xie, Cong Guo, Yunxin Liu, and Jingwen Leng.\n2021. Dual-side sparse tensor core. In 2021 ACM/IEEE 48th Annual International\nSymposium on Computer Architecture (ISCA) . IEEE, 1083â€“1095.\n[81] Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. 2019. Learning\nchannel-wise interactions for binary convolutional neural networks. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n568â€“577.\n[82] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,\nQi Zhang, Fengwei Yu, and Xianglong Liu. 2022. Outlier Suppression: Pushing\nthe Limit of Low-bit Transformer Language Models. In Advances in Neural Infor-\nmation Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (Eds.). https://openreview.net/forum?id=yW5zeRSFdZ\n[83] Wikipedia contributors. 2022. 68â€“95â€“99.7 rule â€” Wikipedia, The Free Encyclope-\ndia. [Online].\n[84] Hailong Yang, Alex D. Breslow, Jason Mars, and Lingjia Tang. 2013. Bubble-flux:\nprecise online QoS management for increased utilization in warehouse scale\ncomputers. In The 40th Annual International Symposium on Computer Architecture\n(ISCA). https://doi.org/10.1145/2485922.2485974\n[85] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020.\nGobo: Quantizing attention-based nlp models for low latency and energy ef-\nficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO). IEEE, 811â€“824.\n[86] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\nQuantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning\nand Cognitive Computing-NeurIPS Edition (EMC2-NIPS) . IEEE, 36â€“39.\n[87] Chen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Cong.\n2015. Optimizing fpga-based accelerator design for deep convolutional neural\nnetworks. In Proceedings of the 2015 ACM/SIGDA international symposium on\nfield-programmable gate arrays . 161â€“170.\nOliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization ISCA â€™23, June 17â€“21, 2023, Orlando, FL, USA\n[88] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. 2018. Lq-nets:\nLearned quantization for highly accurate and compact deep neural networks. In\nProceedings of the European conference on computer vision (ECCV) . 365â€“382.\n[89] Shijin Zhang, Zidong Du, Lei Zhang, Huiying Lan, Shaoli Liu, Ling Li, Qi Guo,\nTianshi Chen, and Yunji Chen. 2016. Cambricon-X: An accelerator for sparse\nneural networks. In 2016 49th Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO). IEEE, 1â€“12.\n[90] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt:\nOpen pre-trained transformer language models. arXiv preprint arXiv:2205.01068\n(2022).\n[91] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer\nHaj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez,\nand Ion Stoica. 2020. Ansor: Generating High-Performance Tensor Programs\nfor Deep Learning. In 14th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI) . https://doi.org/10.5555/3488766.3488815\n[92] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. 2020. Flex-\nTensor: An Automatic Schedule Exploration and Optimization Framework for\nTensor Computation on Heterogeneous System. In Architectural Support for\nProgramming Languages and Operating Systems, Lausanne (ASPLOS) . https:\n//doi.org/10.1145/3373376.3378508\n[93] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.\n2016. Dorefa-net: Training low bitwidth convolutional neural networks with low\nbitwidth gradients. arXiv preprint arXiv:1606.06160 (2016).\n[94] Xuda Zhou, Zidong Du, Qi Guo, Shaoli Liu, Chengsi Liu, Chao Wang, Xuehai\nZhou, Ling Li, Tianshi Chen, and Yunji Chen. 2018. Cambricon-S: Addressing\nirregularity in sparse neural networks through a cooperative software/hardware\napproach. In 2018 51st Annual IEEE/ACM International Symposium on Microarchi-\ntecture (MICRO). IEEE, 15â€“28.\n[95] Yangjie Zhou, Jingwen Leng, Yaoxu Song, Shuwen Lu, Mian Wang, Chao Li, Minyi\nGuo, Wenting Shen, Yong Li, Wei Lin, et al. 2023. uGrapher: High-Performance\nGraph Operator Computation via Unified Abstraction for Graph Neural Networks.\nIn Proceedings of the 28th ACM International Conference on Architectural Support\nfor Programming Languages and Operating Systems, Volume 2 . 878â€“891.\n[96] Yangjie Zhou, Mengtian Yang, Cong Guo, Jingwen Leng, Yun Liang, Quan Chen,\nMinyi Guo, and Yuhao Zhu. 2021. Characterizing and demystifying the implicit\nconvolution algorithm on commercial matrix-multiplication accelerators. In 2021\nIEEE International Symposium on Workload Characterization (IISWC) . IEEE, 214â€“\n225.\n[97] Hongyu Zhu, Ruofan Wu, Yijia Diao, Shanbin Ke, Haoyu Li, Chen Zhang, Jilong\nXue, Lingxiao Ma, Yuqing Xia, Wei Cui, Fan Yang, Mao Yang, Lidong Zhou, Asaf\nCidon, and Gennady Pekhimenko. 2022. ROLLER: Fast and Efficient Tensor\nCompilation for Deep Learning. In16th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 22) . 233â€“248.\n[98] Maohua Zhu, Tao Zhang, Zhenyu Gu, and Yuan Xie. 2019. Sparse tensor core:\nAlgorithm and hardware co-design for vector-wise sparse neural networks on\nmodern gpus. In Proceedings of the 52nd Annual IEEE/ACM International Sympo-\nsium on Microarchitecture . 359â€“371.\n[99] Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, and Chunhua Shen.\n2021. Effective training of convolutional neural networks with low-bitwidth\nweights and activations. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (2021).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6657193899154663
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.597240686416626
    },
    {
      "name": "Outlier",
      "score": 0.5170422196388245
    },
    {
      "name": "Computer hardware",
      "score": 0.3320358097553253
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2704593241214752
    },
    {
      "name": "Algorithm",
      "score": 0.2514804005622864
    }
  ]
}