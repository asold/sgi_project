{
  "title": "ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference",
  "url": "https://openalex.org/W4285253555",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2130499761",
      "name": "Kai-hui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128637305",
      "name": "Honglei Zhuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084622737",
      "name": "Tao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096947334",
      "name": "Zhen Qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1985523177",
      "name": "Jing Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2890704379",
      "name": "Dara Bahri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096852187",
      "name": "Ji Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2505043072",
      "name": "Jai Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158019794",
      "name": "Cicero Nogueira dos Santos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2738935859",
      "name": "Yi Tay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151486164",
      "name": "Donald Metzler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2937036051",
    "https://openalex.org/W3189117283",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2952250961",
    "https://openalex.org/W3013936901",
    "https://openalex.org/W4221166196",
    "https://openalex.org/W3148323213",
    "https://openalex.org/W3206455169",
    "https://openalex.org/W3104738015",
    "https://openalex.org/W3034212969",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W3104657626",
    "https://openalex.org/W3176438635",
    "https://openalex.org/W3173211693",
    "https://openalex.org/W2970072000",
    "https://openalex.org/W3129868498",
    "https://openalex.org/W3214554611",
    "https://openalex.org/W3034912391",
    "https://openalex.org/W3030698044",
    "https://openalex.org/W4287645694",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3117185530",
    "https://openalex.org/W3204250176",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3105136066",
    "https://openalex.org/W3146365155",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3089973398",
    "https://openalex.org/W3168875417",
    "https://openalex.org/W3172119680",
    "https://openalex.org/W3155114168",
    "https://openalex.org/W3208564698",
    "https://openalex.org/W3198073108",
    "https://openalex.org/W3153728054",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3154498239",
    "https://openalex.org/W3175111331",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W3132466668",
    "https://openalex.org/W3101007570",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W3017018726",
    "https://openalex.org/W3034439313",
    "https://openalex.org/W3103420681",
    "https://openalex.org/W3105817677"
  ],
  "abstract": "Kai Hui, Honglei Zhuang, Tao Chen, Zhen Qin, Jing Lu, Dara Bahri, Ji Ma, Jai Gupta, Cicero Nogueira dos Santos, Yi Tay, Donald Metzler. Findings of the Association for Computational Linguistics: ACL 2022. 2022.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3747 - 3758\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nED2LM: Encoder-Decoder to Language Model\nfor Faster Document Re-ranking Inference\nKai Hui∗ Honglei Zhuang Tao Chen Zhen Qin Jing Lu Dara Bahri\nJi Ma Jai Gupta Cicero Nogueira dos Santos Yi Tay Donald Metzler\nkaihuibj@google.com\nGoogle Research\nAbstract\nState-of-the-art neural models typically\nencode document-query pairs using cross-\nattention for re-ranking. To this end, models\ngenerally utilize an encoder-only (like BERT)\nparadigm or an encoder-decoder (like T5)\napproach. These paradigms, however, are\nnot without ﬂaws, i.e., running the model on\nall query-document pairs at inference-time\nincurs a signiﬁcant computational cost. This\npaper proposes a new training and inference\nparadigm for re-ranking. We propose to\nﬁnetune a pretrained encoder-decoder model\nusing in the form of document to query\ngeneration. Subsequently, we show that\nthis encoder-decoder architecture can be\ndecomposed into a decoder-only language\nmodel during inference. This results in\nsigniﬁcant inference time speedups since the\ndecoder-only architecture only needs to learn\nto interpret static encoder embeddings during\ninference. Our experiments show that this new\nparadigm achieves results that are comparable\nto the more expensive cross-attention ranking\napproaches while being up to 6.8X faster. We\nbelieve this work paves the way for more\nefﬁcient neural rankers that leverage large\npretrained models.\n1 Introduction\nLeveraging transformer architecture to model the\nconcatenation of a query-document pair is a\nwell-established approach for document ranking\n(Nogueira et al., 2020). Today, modern neural\nmethods for re-ranking are based on the encoder-\nonly (e.g., BERT (Devlin et al., 2019)) or encoder-\ndecoder (e.g., T5 (Raffel et al., 2020)) paradigm\nwhere query-document interactions are modeled\nby the encoder’s attention mechanism. Unfortu-\nnately, these paradigms are computationally pro-\nhibitive given that the model has to be run on all\ndocument-query pairs during inference. To this\n∗Corresponding Author\nend, it is commonplace to use less powerful but\ncomputationally lightweight dual encoder models\n(Nogueira et al., 2019a; Karpukhin et al., 2020;\nXiong et al., 2020; Qu et al., 2021; Gao et al., 2021)\nfor ﬁrst-pass retrieval and to only run the more ex-\npensive re-ranker on a small subset of retrieved can-\ndidates. Even with this setup, cross-attention-based\nre-ranking can still be expensive, especially when\nlarger pretrained Transformer models are used. As\nsuch, this paper is primarily concerned with im-\nproving inference-time re-ranking efﬁciency while\nmaintaining comparable effectiveness to existing\ncross-attention models.\nThe novelty of this paper lies in a new paradigm\nfor re-ranking that provides up to 6.8X speedup\nwithout any degradation in shallow-pool effective-\nness. Concretely, we propose a new method for\ninference-time decomposition of encoder-decoder\narchitectures into decoder-only language models.\nGiven a pretrained sequence-to-sequence model,\nwe ﬁnetune the encoder-decoder model using a\ndocument-to-query multi-task loss. At inference,\nwe decompose the encoder-decoder architecture\ninto a decoder-only language model (LM) that\nlearns to interpret from a memory store of encoded\ndocument tokens representations using attention.\nThe document-query pair score can be interpreted\nas the likelihood of generating the query given the\nencoded document term representations.\nThere are multiple efﬁciency beneﬁts to our\nproposed design. First, signiﬁcant inference-time\ncost savings are unlocked since the document term\nmemory store can be pre-computed in advance\nand acts as a read-only memory. Second, our re-\ndesign also exploits the fact that queries are gen-\nerally much shorter than documents. During in-\nference time, only query tokens have to be passed\nthrough the decoder stack when attending to the\npre-computed document representations which al-\nlows us to also obtain an additional speed advan-\ntage over encoder-only BERT-like models. Third,\n3747\ncomputing the query likelihood is computationally\nsimple and does not require the typical costs asso-\nciated with autoregressive generation models.\nThe overall contributions of this work can be\nsummarized as follows:\n•We propose a new re-ranking paradigm,\nED2LM (Encoder-Decoder to Language\nModel) for fast and efﬁcient inference-time\nre-ranking. Our method is based on inference-\ntime decomposition of an encoder-decoder\nmodel into a decoder-only language model.\n•The proposed method utilizes a new ﬁne-\ntuning paradigm by incorporating a new ob-\njective function that combines the generative\nquery likelihood and the discriminative cross-\nentropy loss.\n•Via extensive experiments, we show that the\nproposed method performs competitively with\nT5-based cross-attention re-rankers (Nogueira\net al., 2020) while being up to more than 6.8X\nfaster during inference.\n2 Related Work\nNeural text ranking. Traditional ranking sys-\ntems focus on numeric input features (Qin et al.,\n2021; Yan et al., 2021). Recently, text ranking is\npopular given the prevalence of large pretrained\nlanguage models. A number of so-called cross-\nattention models concatenate a query and a can-\ndidate document into a string and feed it into the\nmodel (Han et al., 2020; Nogueira et al., 2020;\nChen et al., 2022), which allows the attention mech-\nanism of the model to capture interactions across\nquery and document terms. However, deploying\nsuch models to millions or billions of documents\nis usually intractable due to the exorbitant compu-\ntational cost. To combat this cost, other studies\nhave explored more efﬁcient models, e.g., dual-\nencoder models (Karpukhin et al., 2020; Qu et al.,\n2021; Ren et al., 2021), BERT with late interac-\ntion (Khattab and Zaharia, 2020), or using contex-\ntual language models to improve term weighting in\ntraditional inverted indexes (Nogueira et al., 2019a;\nDai and Callan, 2020; Gao et al., 2021).\nA few studies that are most closely related to\nthis work focus on leveraging the generative nature\nof pretrained encoder-decoder language models. A\nnatural practice is to directly use the likelihood\nof generating the query given a document to rank\nthe documents (Zhuang and Zuccon, 2021; Zhuang\net al., 2021b; Lesota et al., 2021). However, these\nmethods mostly perform substantially worse than\ncross-attention ranking models. Another work (dos\nSantos et al., 2020) transforms the likelihood of\ngenerating the query into a discriminative loss,\nwhere an “unlikelihood” loss is introduced for neg-\native query-document pairs. Despite relatively bet-\nter performance than using vanilla maximum likeli-\nhood estimation (MLE), we found that their method\nstill underperforms cross-attention ranking models.\nOur proposed method uses a combination of query\ngeneration loss and a cross-entropy loss on a spe-\nciﬁc token, which is capable of achieving compara-\nble performance to cross-attention models.\n(Ju et al., 2021) proposes query generation as an\nauxiliary task during training and shows improved\nperformance. However, the proposed model still\ntakes both a query and a document as input in the\nmain ranking task and hence would be as costly\nas cross-attention ranking models during inference.\nFinally, the recent differentiable search index (Tay\net al., 2022) proposes end-to-end ranking via text\ngeneration using an encoder-decoder T5 model.\nEfﬁcient neural IR. Due to the excessive com-\nputational cost of inference in pretrained language\nmodels, there is a series of studies aiming to im-\nprove the efﬁciency.\nA major trend is to distill expensive models into\ncheaper ones (Hinton et al., 2015; Sanh et al., 2019).\nSome distillation approaches have speciﬁcally fo-\ncused on text ranking applications (Zhang et al.,\n2020; Zhuang et al., 2021a; Chen et al., 2021a;\nHofstätter et al., 2020a).\nAnother trend is to improve model efﬁciency by\nmodifying the model architecture. A typical ap-\nproach used by ColBERT (Khattab and Zaharia,\n2020) and PreTTR (MacAvaney et al., 2020) de-\nfer query-document interactions to upper layers so\nthat part of the model can be pre-computed. Our\nmodel can be categorized into this class of models,\nexcept that the late interaction is naturally aligned\nwith the decomposition of encoder-decoder models.\nThis alignment allows us to better leverage knowl-\nedge learned by the model during pretraining, and\ncan be the reason behind our stronger performance\ncompared to ColBERT and PreTTR.\nThere are a couple of other efﬁcient model struc-\ntures, such as early exiting (Soldaini and Moschitti,\n2020; Xin et al., 2020), Transformer-Kernel (TK)\nmodel (Hofstätter et al., 2020b), and contextualized\n3748\nEncoder\nd0\nTransformer Layer\nTransformer Layer\nTransformer Layer\nd1 d2 dndn-1\nMemory: Doc Representation\nDecoder\nq0\nTransformer Layer\nTransformer Layer\nTransformer Layer\nq1 [True|False]\nOffline Online\nFigure 1: Overview of the proposed ED2LM.\nofﬂine relevance weighting (Chen et al., 2021b).\nIn terms of storage cost, Cohen et al. (2021) pro-\nposed the succinct document representation which\nreduces the dimension of token representation to\ncompress document representations. These tech-\nniques are orthogonal to our study and can be com-\nbined with our work to further improve the time\nand storage efﬁciency.\n3 The Proposed Method\nThis section describes the ED2LM model. See\nFig. 1 for an overview of the approach.\n3.1 Overview\nThe proposed ED2LM model is based on the T5\nencoder-decoder architecture. It encodes the docu-\nments without looking at the queries and produces\nranking scores by decoding the queries and attend-\ning to the document representations.\nIn particular, for a query-document pair, the doc-\nument tokens are encoded with a stack of Trans-\nformer layers as in BERT (Devlin et al., 2019),\nwhere the tokens attend to one another before go-\ning through the position-wise feed-forward layer.\nThe output of the encoder is in the form of dense\nrepresentations for the document tokens. During\ndecoding, the query tokens are decoded with a stack\nof decoder layers, where the query tokens ﬁrst at-\ntend to other query tokens before going through a\nmulti-head attention block to attend to the docu-\nment tokens from the encoder.\nInspired by T5 (Nogueira et al., 2020) for rank-\ning and the use of BART for discrimination (dos\nSantos et al., 2020; Lewis et al., 2020), a special\ntrue/false token is appended to the end of the query\nbefore the end of the query sequence (EOS). During\ntraining, inspired by (Ju et al., 2021), the model is\ntrained to generate the query tokens and determine\nthe relevance of the query-document pair. During\ninference, only the score for the true/false token is\nused for ranking.\n3.2 ED2LM for Re-ranking\nIn this section, we describe the details of training\nand inference for ED2LM.\n3.2.1 Fine-tuning\nDuring ﬁne-tuning, ED2LM involves an encoder-\ndecoder architecture which maps RLD discrete\nsymbols to RLQ discrete symbols. Here, LD refers\nto the length of the document and LQ refers to the\nquery length.\nTask formulation. The input to the model is a\nsequence of document tokens and the output of the\nmodel is a sequence of query tokens. In order to im-\nbue our model with discriminative capabilities, we\nappend the class token (true/false) that represents\nthe query-document pair at the end of the query.\nThe ranking score of a query-document pair is the\nnormalised probability of the true token at the end\nof the query. Given a query q and a document d,\nthe ground-truth correctness of d relative to q is\ndenoted as a binary label y.\nLoss function. The loss function optimized for\nﬁne-tuning has two components. The ﬁrst compo-\nnent is the maximum likelihood estimation (MLE)\nloss of the individual question tokens, which is\ndeﬁned as:\nLossQL = −\n∑\ni∈0···LQ−1\nlog(P(qi|q:i; d)) (1)\nSince we want the model to learn the correctness\nof the question using the trailing true/false tokens,\nwe also compute the likelihood of those tokens as\nfollows.\np+ = P(true,eos|q; d)\np−= P(false,eos|q; d)\nThe cross-entropy loss LossCE can then be written\nas:\nLossCE = −ylogp+ −(1 −y)logp− (2)\nThe ﬁnal training loss can the be written as:\nLoss = LossCE + yLossQL (3)\n3749\nThe cross-entropy loss is applied to all examples\nwhereas the query likelihood loss only applies to\nthe positive examples. Our ﬁne-tuning loss is\ntrained with teacher forcing.\nScoring. The normalised scores from the true\nand false tokens are combined as in (Nogueira et al.,\n2020).\n3.3 Efﬁcient Re-ranker\nThis section discusses using ED2LM for more efﬁ-\ncient inference, by decoupling the encoder-decoder\ninto a decoder-only language model.\n3.3.1 Decomposing Encoder-Decoder to\nDecoder-only LM\nThe key idea for fast inference is to only extract\nthe decoder from the trained Encoder-Decoder\nmodel. Recall a decoder-stack is comprised of\ndecoder-side causal self-attention and encoder-\ndecoder cross-attention.\nX′\nℓ = CausalSelfAttention(Xℓ, Xℓ) (4)\nYℓ = MultiheadAttention(Mℓ, X′\nℓ) (5)\nwhere X ∈ RLQ×dmodel is the input to the de-\ncoder stack at layer ℓ. M refers to a sequence\nof memory tokens. In this case, we note that M\nhere refers to computed encoder representations\nthat pass through the encoder-stack. During ﬁne-\ntuning, this encoder-stack is trained end-to-end.\nHowever, this paradigm generalizes these embed-\ndings as “memory”, which can be extended to other\nuse cases or applications. We can also interpret this\nmemory as a form of soft prompt.\n3.3.2 Reading from Memory\nThe decoder reads from M. In the standard setup,\nM are static representations that originate from the\nﬁnal output of the encoder in the Seq2Seq architec-\nture and the MultiheadAttention is the encoder-\ndecoder cross attention. Here, M can be com-\npressed along the presentation dimension (dmodel)\nas in (MacAvaney et al., 2020; Gao et al., 2021; Co-\nhen et al., 2021), which is orthogonal to our studies,\nor along the sequence dimension (LD), which is in-\ntroduced below. We ﬁnd that this generalization is\na practically useful way to interpret the ED2LM ar-\nchitecture. We propose to explore not only standard\nM from encoder outputs but also compressed mem-\nory stores from Funnel Transformers (Dai et al.,\n2020). Herein, we employ the Funnel Transformer\nwith b blocks in the encoder, leading to 2b storage\ncompression, by reducing the RLD for 2b. Between\neach block, a mean-pooling layer is used to down-\nsample the input sequence by two in the sequence\nlength dimension.\n4 Experiment Setup\nThis section describes our experimental setup.\nDataset and metrics. We employ the MS\nMARCO (Nguyen et al., 2016) passage re-ranking\ntask, for which we report the ofﬁcial evaluation\nmetric MRR@10 on the 6980 development queries\nusing the binary labels from the dev dataset. We\nalso use the 43 test queries from the TREC Deep\nLearning (DL) Track 2019 (Craswell et al., 2020)\nand the 54 test queries from 2020 (Craswell et al.,\n2021). The TREC data sets include graded rele-\nvance judgments. We report the ofﬁcial evaluation\nmetrics NDCG@10 as well as mean average preci-\nsion (MAP). When computing MAP, following the\nofﬁcial TREC setup, we map passage judgments\n2 and 3 to relevant and 0 and 1 to non-relevant.\nStatistical signiﬁcance is reported using a paired\ntwo-tailed t-test. We use a maximum sequence\nlength of 256 tokens for paragraphs and 32 tokens\nfor queries in our experiments, similar to (Hofstät-\nter et al., 2020b,a).\nWe employ the training data from Rock-\netQA (Qu et al., 2021), which is derived from\nthe MS MARCO training dataset as dual-encoder\nmodels trained on it demonstrate strong perfor-\nmance. Speciﬁcally, we use the hard-question\nsplit (“RQA-Hard”), which only includes the hard-\nnegative samples and positive samples from MS\nMARCO, and the merge split (“RQA-Merge”),\nwhich includes extra unlabeled questions from Ya-\nhoo! Answers1, ORCAS (Fisch et al., 2019), and\nNatural Questions (Kwiatkowski et al., 2019) on\ntop of “RQA-Hard”. For validation purposes, we\nuse the 1500 dev2 validation queries with at least\none relevance judgment from the TREC DL Track\n20212. Given our focus on shallow-pool effective-\nness, the model with highest MRR@10 on the\nvalidation dataset is selected. We employ Mesh\nTensorﬂow (Shazeer et al., 2018) for training and\nevaluation. The T5 models have been trained and\ninferred as in (Nogueira et al., 2020), and ED2LM\nhas been primarily trained using the loss deﬁned\n1http://answers.yahhoo.com\n2https://msmarco.blob.core.windows.\nnet/msmarcoranking/passv2_dev2_queries.\ntsv\n3750\nin Eq. 3. We train models for ablation study by\nusing Eq. 1 and Eq. 2 separately. During training,\na constant learning rate of 1e-3 is used.\nBaselines. ED2LM is compared to ranking mod-\nels using four variants of T5 (T5-small, T5-base,\nT5-large, and T5-xl), BERT-base, BERT-large,\nand PreTTR (MacAvaney et al., 2020). The\nPreTTR (MacAvaney et al., 2020) model decou-\nples the encoding of the query and the document\non top of the BERT architecture and is directly\ncomparable to the T5-based ED2LM. We ﬁne-tune\nBERT-base models using TF-ranking (Pasumarthi\net al., 2019) and achieve similar results with the\nresults reported in (Nogueira et al., 2020). We also\nre-implement the PreTTR model using TF-ranking.\nTherein, following the conﬁgurations in (MacA-\nvaney et al., 2020), a query and a document are\nencoded independently in the ﬁrst l-layers using\nthe BERT-base conﬁguration before interacting via\ncross-attention. The BERT-base pre-trained check-\npoint is used for initialisation. We report the results\nby setting l = 6, which leads to similar FLOPs and\nlatency as ED2LM-base (26.1T vs 20.6T).\nVariants of ED2LM. We investigate the effec-\ntiveness and inference efﬁciency of ED2LM based\non T5-small, T5-base, T5-large, and T5-xl archi-\ntectures, leading to ED2LM-small, ED2LM-base,\nED2LM-large, and ED2LM-xl, respectively. We\nexperiment with two Funnel-Transformer variants,\nwhere two six-layers funnel blocks ( b = 2) and\nthree eight-layers funnel blocks (b = 3) are used in\nthe encoder, respectively. They are named ED2LM-\nF-6L×2 and ED2LM-F- 8L×3, correspondingly.\nThese conﬁgurations lead to a 4X (when b = 2)\nand a 8X (when b = 3) reduction in the sequence\nlength. The Funnel-Transformer variants are pre-\ntrained using the same task as in T5 on top of the\nC4 corpus (Raffel et al., 2020).\nInitial rankings. Since we primarily focus on\nthe re-ranking setting, we consider several retrieval\nmodels to generate initial ranking candidates. For\nthe MS MARCO passage re-ranking task, we\nuse BM25 (an implementation from Terrier (Mac-\ndonald et al., 2012)) to generate the top-1K pas-\nsages per query. In addition, we implemented\nthe docT5query model (Nogueira et al., 2019b,a)\nby training a T5 seq2seq model to generate 40\nquestions (i.e., expansions) per paragraph and use\nBM25 to retrieve top-1K passages. This serves as a\nhigh-recall initial ranking, wherein the recall@1K\nincreases from 86.7 (MRR@10=19.3) in the base\nBM25 ranking to 93.76 (MRR@10=25.3) with\ndocument expansion. For the TREC DL Track,\nwe use the ofﬁcial top-1k initial rankings from\nBM25 (Craswell et al., 2020, 2021).\nEfﬁciency metrics. To compare inference efﬁ-\nciency, we report FLOPs and latency as encouraged\nby Dehghani et al. (2022). To compute FLOPs we\nmake use of a public repository 3. To compute la-\ntency, we do as follows: each model is exported in\nthe Tensorﬂow Saved Model format before serving\nvia the Tensorﬂow Model Server 4 on a Intel Xeon\nCPU desktop with 8 CPU cores, 16 CPU threads,\nand 132 GB RAM. We randomly select 500 queries\nand passages from the MS MARCO dataset. As\nfor PreTTR (MacAvaney et al., 2020), to enable\nfair comparisons, we add an additional 500 queries,\nleading to a total of 1000 query-passages pairs, to\nfully utilise the shared computation of the query\nencoder. For each query-passage pair, we time the\ninference call to the model server 10 times and\nrecord the minimum. For each model, we report\nthe 50 and 95-percentile of the 500 timing (1000\nfor PreTTR) as a two-number summary of latency.\nThe time for tokenization is included for all models.\nFor PreTTR and ED2LM, we assume the token rep-\nresentations of passages have already been loaded\nin the memory akin to (MacAvaney et al., 2020;\nGao et al., 2021).\n5 Results\nIn this section, we examine the effectiveness-\nefﬁciency trade-off of ED2LM on the passage re-\nranking task. The results of T5, ED2LM, BERT,\nand PreTTR have been displayed in Table 1. In\nTable 2, we further summarise the comparisons\n(ED2LM vs. baseline models) from Table 1 and\nhighlight the results that ED2LM provides a bet-\nter trade-off. We also visualise the results from\ndifferent models on the MS MARCO benchmark\nin Fig. 2 when using docT5query (Nogueira et al.,\n2019a) as the initial ranking.\n5.1 Trade-off in Re-ranking\nResults for the baseline models. We achieve\ncomparable results as previous studies on all\nthree benchmarks. In particular, (Nogueira et al.,\n3https://github.com/google-research/\nelectra/blob/master/flops_computation.py\n4https://www.tensorflow.org/tfx/\ntutorials/serving/rest_simple\n3751\nModels MS MARCO (MRR@10) Trec DL Track 2019 Trec DL Track 2020FLOPs Latency (ms)\nBM25+ docT5query+ nDCG@10 MAP nDCG@10 MAP (T) P50 P95\nBaseline Models\nPreTTR (p) 36.7 37.4 70.0 39.8 71.5 45.5 26 159 189\nBERT-base (b) 36.5 37.2 68.5 41.9 71.9 45.7 52 309 443\nT5-small (t5s) 35.9 36.6 68.8 42.3 68.1 42.1 22 123 127\nT5-base (t5b) 38.3 39.2 71.1 43.1 73.7 48.6 67 405 425\nT5-large (t5l) 39.4 40.3 72.0 42.9 73.0 48.0 202 1111 1140\nT5-xl (t5x) 39.6 40.6 71.8 42.2 74.6 49.2 752 2490 2515\nVariants of ED2LM\nED2LM-small37.2 (↑t5s↓t5blx↑b) 37.9 (↑t5s↓t5blx↑b) 69.5 (↓t5l) 40.8 69.6 (↓t5blx) 43.3 (↓t5blx↓b) 5 60 65\nED2LM-base38.7 (↑t5s↓t5lx↑b↑p) 39.6 (↑t5s↓t5lx↑b↑p) 70.2 42.5 ( ↑p) 71.5 (↑t5s↓t5x) 47.2 (↑t5s↓t5x) 21 157 185\nED2LM-large38.0 (↑t5s↓t5lx↑b↑p) 39.0 (↑t5s↓t5lx↑b↑p) 70.3 42.3 ( ↑p) 72.8 (↑t5s) 47.6 ( ↑t5s) 73 317 336\nED2LM-xl 39.4 (↑t5sb↑b↑p) 40.4 ( ↑t5sb↑b↑p) 71.4 44.8 (↑t5sbx↑b↑p) 71.6 (↑t5s↓t5x) 48.2 (↑t5s↑b↑p) 287 811 834\nED2LM with Funnel Blocks\nED2LM-F-6L×2 36.5 (↓t5blx) 37.4 ( ↑t5s↓t5blx) 68.0 (↓t5blx) 40.5 ( ↓t5b) 70.4 (↓t5bx) 44.1 ( ↓t5blx) 9 130 151\nED2LM-F-8L×3 35.4 (↓t5blx↓b↓p) 36.2 ( ↓t5blx↓b↓p) 69.2 (↓t5l) 40.2 ( ↓t5bl) 70.5 (↓t5bx) 44.7 ( ↓t5blx) 7 108 126\nTable 1: The re-ranking performance when re-ranking top-1K paragraphs. We note down the signiﬁcant difference\nat 0.05 level with ↑and ↓for the variants of ED2LM. The comparisons are relative to T5-small, T5-base, T5-large,\nand, T5-xl (with subscriptions t5s, t5b, t5l, t5x), BERT-base (with subscriptions b), PreTTR with six layers of\ndecoupled encoding (with subscriptions p).\n102 103\nLatency P95 (ms, log-scale)\n36\n37\n38\n39\n40\n41MRR@10 on MS MARCO dev small\nED2LM-F-6x2\nMRR@10=37.4\nlatency=151ms\nED2LM-F-8x3\nMRR@10=36.2\nlatency=126ms\nT5-S\nMRR@10=36.6\nlatency=127ms\nT5-B\nMRR@10=39.2\nlatency=425ms\nT5-L\nMRR@10=40.3\nlatency=1140ms\nT5-XL\nMRR@10=40.6\nlatency=2515ms\nPreTTR\nMRR@10=37.4\nlatency=189ms\nBERT-B\nMRR@10=37.2\nlatency=443ms\nED2LM-S\nMRR@10=37.9\nlatency=65ms\nED2LM-B\nMRR@10=39.6\nlatency=185ms ED2LM-L\nMRR@10=39.0\nlatency=336ms\nED2LM-XL\nMRR@10=40.4\nlatency=834ms\nFigure 2: MRR@10 on MS MARCO dev small (6980 test queries) after re-ranking top-1K documents from\ndocT5query (Nogueira et al., 2019a) vs. latency. The x-axis is the latency (95 percentile out of 500 calls); y-\naxis is the MRR@10 score. The point (ED2LM models) and the cross (baseline models) are the mean MRR@10\nand the bar indicates the 95% conﬁdence interval.\n2020) reports MRR@10 = 37 .2, 38.1, 39.3,\nand 39.8 when using BERT-large, T5-base, T5-\nlarge, and T5-xl to re-rank top-1K paragraphs\nfrom BM25 on MS MARCO passage re-ranking\nbenchmark. Besides, we list the re-ranking results\non MS MARCO from COIL (Gao et al., 2021)\n(MRR@10=34.8) and ColBERT (Khattab and Za-\nharia, 2020) (MRR@10=34.9) here for references.\nFor the TREC DL Track, we select the submit-\nted runs that are most comparable to ours, namely,\nthe top re-ranking run (Yan et al., 2019) in 2019\n(nDCG@10 = 72.5 and MAP= 45.3) and the 4th\nbest re-ranking run (Cao et al., 2020) 5 for 2020\n(nDCG@10 = 73.7 and MAP= 48.8).\n5The 1st-3rd best runs (Qiao et al., 2021) in 2020 used\nTREC DL 2019 data for ﬁne-tuning.\nEffectiveness-efﬁciency trade-off. ED2LM de-\ncouples the encoding of the document and query,\nthereby allowing for caching the document repre-\nsentation ofﬂine. After pre-computing the docu-\nment presentation as in PreTTR (MacAvaney et al.,\n2020), ED2LM achieves a highly favorable trade-\noff. From Table 1 and 2, we make the following\nobservations. (1) ED2LM-small and ED2LM-base\nperform at least as good as T5-small and T5-base,\nrespectively, while providing more than a 2X speed\nup. For ED2LM-base, its effectiveness is not signif-\nicantly different from T5-large on both TREC DL\nTracks and under-performs by 0.7 (38.7 vs 39.4)\non MS MARCO, while providing a 6.2X speed up.\nWhen comparing with BERT-base and PreTTR,\nboth ED2LM-small and ED2LM-base perform at\nleast as good (for MRR@10 and nDCG@10) and\n3752\nED2LM→ Small Base Large xl F-6L×2 F-8L×2\nT5-small F:4.4x/L:2.0x - - - F:2.4x/L:0.8xF:3.1x/L:1.0x\nr:↑/n:~/m:~ r:~/n:~/m:~ r:~/n:~/m:~\nPreTTR F:5.2x/L:2.9x F:1.2x/L:1.0x - - F:2.9x/L:1.3x F:3.7x/L:1.5x\nr:~/n:~/m:~ r:↑/n:~/m:↑ r:~/n:~/m:~ r:↓/n:~/m:~\nBERT-baseF:10.4x/L:6.8xF:2.5x/L:2.4x - - F:5.8x/L:2.9xF:7.4x/L:3.5x\nr:↑/n:~/m:↓ r:↑/n:~/m:~ r:~/n:~/m:~ r:↓/n:~/m:~\nT5-base - F:3.2x/L:2.3x - - - -r:~/n:~/m:~\nT5-large - F:9.6x/L:6.2x F:2.8x/L:3.4x - - -r:↓/n:~/m:~ r:↓/n:~/m:~\nT5-xl - - F:10.3x/L:7.5x F:2.6x/L:3.0x - -r:↓/n:~/m:~ r:~/n:↓/m:~\nTable 2: The comparison of the effectiveness-efﬁciency trade-off for ED2LM derived from Table 1. Each row\nincludes one baseline model, and individual columns are one of the ED2LM variants. In each comparison (cell),\nthe upper part is the efﬁciency comparison, where F indicates FLOPs and L is the latency (P95). In the lower\npart, the comparisons for the effectiveness are summarised. ↑, ↓, and, ~ denote the signiﬁcant better, worse, and,\nno signiﬁcant difference (at level 0.05) when comparing ED2LM models with the baseline. Herein, r indicates\nMRR@10 on MS Marco dev small dataset (re-ranking top-1k from BM25); n and m denote nDCG@10 and\nMAP, respectively, on TREC DL Track. We highlight comparisons that ED2LM could provide better effectiveness\n(MRR@10 or nDCG@10) or smaller latency.\nare up to 6.8X faster. (2) ED2LM-large performs\non par with T5-large on the TREC DL Tracks, but\nunder performs on MS MARCO by 1.4; whereas\nED2LM-xl achieves similar MRR@10 on MS\nMARCO (39.4 vs 39.6), but performs worse in\nterms of nDCG@10 on TREC DL Track 2020. Fur-\nthermore, in Fig. 2 (MRR@10 on MS MARCO vs\nthe latency (P95) by re-ranking the top-1K from\ndocT5query) the leftmost ED2LM-small achieves\nbetter effectiveness than T5-small, PreTTR, and\nBERT-base. Likewise, ED2LM-base achieves sim-\nilar latency as PreTTR and is 2.3X more efﬁcient\nthan BERT-base but achieves higher MRR@10. In\nthe meantime, though more efﬁcient, ED2LM-xl\nand ED2LM-large perform close to their counter-\nparts, once again conﬁrming the observations. We\nargue that, on the one hand, co-training of query\nlikelihood and the discriminative cross-entropy\nleads to better ranking quality, which is especially\ntrue for the smaller variants (small and base); On\nthe other hand, not attending to the query dur-\ning document encoding leads to performance de-\ncreases, which dominates the outcomes in larger\nmodel variants (like large and xl).\nED2LM-F: Storage compression with Funnel\nTransformer. The results for the two variants\nof ED2LM with Funnel blocks are summarised\nin the bottom block of Table 1 and the rightmost\ncolumns in Table 2. In terms of storage, ED2LM-\nF-6L×2 provides 4X compression and ED2LM-F-\n8L×3 provides 8X compression by reducing the\nsequence length in the encoder. It can be seen that,\nED2LM-F-6L×2 outperforms T5-small and per-\nforms as well as BERT-base and PreTTR. Further-\nmore, while ED2LM-F- 8L×3 provides 8X com-\npression, the effectiveness drops below that of T5-\nsmall and BERT-base on the MS MARCO bench-\nmark. However, it achieves on-par results relative\nto T5-small and BERT-base on the TREC DL Track\nin terms of both nDCG@10 and MAP. As for efﬁ-\nciency, ED2LM-F-8L×3 is similar to T5-small and\nPreTTR, but is 3.5X faster than BERT-base.\n5.2 Ablation Analysis\nThe use of RocketQA-Merge dataset for train-\ning. In our experiments, we ﬁnd that the rank-\ning quality of the proposed ED2LM, as well\nas PreTTR model, beneﬁt considerably from\nRocketQA-Merge. We demonstrate the training\nperformance (upper part) in Table 3 on Rock-\netQA and the MS MARCO training dataset. It\ncan be seen that T5 achieves similar performance\non both training data sets. In the meantime,\nED2LM achieves MRR@10=37.5 when trained\non the MS MARCO training dataset, and can\nachieve 38.7 when trained on the “RQA-Merge”\ndataset. This is also true for PreTTR, which sees\n3753\nModels MS Marco\nTraining Data Loss MRR@10\nPreTTR MS Marco - 35.2\nT5-base MS Marco - 38.4\nT5-base RQA-Hard - 38.0\nED2LM-base MS Marco - 37.5\nED2LM-base RQA-Hard - 37.3\nED2LM-base MS Marco LUL (dos Santos et al., 2020)31.2\nED2LM-baseRQA-Merge LUL (dos Santos et al., 2020)33.6\nED2LM-baseRQA-Merge MLE (Eq. 1) 30.2\nED2LM-baseRQA-Merge CE (Eq. 2) 38.2\nTable 3: Ablation study. In the upper half, the uses of alternative training data are explored. In the lower half,\ndifferent loss functions are used to train ED2LM, including the LUL loss from (dos Santos et al., 2020), negative\nlog-likelihood loss on questions as in (Nogueira et al., 2019a), and the cross-entropy loss on true/false token as\nin (Nogueira et al., 2020).\nan MRR@10 increase from 35.2 to 36.7. We con-\njecture that the decoupled encoding of query and\ndocuments, as in ED2LM and PreTTR, requires\nmore queries for training whereas models that use\nfull cross-attention beneﬁt less from the extra train-\ning data. The training performance of ED2LM-\nbase on RocketQA-Hard in Table 3 provides evi-\ndences for this, where ED2LM-base achieves an\neven lower MRR@10. RocketQA-Hard is a subset\nof RocketQA-Merge and includes hard negative\nsamples but without the extra queries. Therefore,\nwe conclude that more unique questions for train-\ning is one of the ED2LM’s key ingredients.\nAlternative loss functions for training. In (dos\nSantos et al., 2020), the unlikelihood loss (re-\nferred as LUL) was used to train a BART (Lewis\net al., 2020) model for question answering. In\nthis section, we train ED2LM using the LUL loss\nfrom (dos Santos et al., 2020) on both the MS\nMARCO and RQA-Merge training sets. We also\nuse the negative log-likelihood loss in Eq. 1 (as in\ndocT5query (Nogueira et al., 2019a)) and the cross-\nentropy loss in Eq. 2 (as in (Nogueira et al., 2020))\nto train ED2LM separately. From Table 3 (lower\npart), LUL leads to signiﬁcantly worse MRR@10\nthan using the loss in Eq. 3 (33.6 vs 38.7), but out-\nperforms the use of negative log-likelihood loss\nfrom Eq. 1 as in (Zhuang et al., 2021b). When only\nusing the cross-entropy loss of the true/false token\n(Eq. 2), effectiveness is slightly worse than when\nusing the loss in combination with query likelihood\n(38.2 vs 38.7), mirroring the ﬁndings from (Ju et al.,\n2021). Therefore, we conclude that the use of both\ntrue/false tokens and query likelihood for training\n(as in Eq. 3) is another key ingredient for ED2LM.\n6 ED2LM for Question Generation\nQuestion generation has played an important role\nfor different downstream tasks (Shakeri et al., 2020;\nPuri et al., 2020; Del Tredici et al., 2021). We\nconjecture that the combination of generation and\nranking loss used in ED2LM has the potential to im-\nprove question generation when compared to mod-\nels trained with generation loss only. We evaluate\nthis conjecture by comparing questions generated\nby vanilla generator trained with question likeli-\nhood only (Nogueira et al., 2019b) and ED2LM in\ndifferent scenarios: manual inspection, assessment\nwith automatic metrics and synthetic training data\ngeneration. For question generation task, an extra\n“eos” token (namely, the end of sequence token)\nis inserted between the question and the true/false\ntoken. Our pilot experiments show that this change\ndoes not inﬂuence the ranking performance but\nboosts the generation quality of ED2LM. We adopt\nthe top-k sampling decoding (Fan et al., 2018) (set\nk = 10) in question generation for all models.\n6.1 Question Generation with Less\nHallucination\nManual inspection of the generated questions.\nWe investigate the reasons why ED2LM can\nsigniﬁcantly outperform deep query likelihood\n(MRR@10=38.7 vs 30.2 from Table 3) by a big\nmargin. We compare the questions generated by\nED2LM and T5 trained with query likelihood as\nin Eq. 1. We sample 66 documents from the\nMS MARCO passage corpus with at least one\ncorrect query in the MS MARCO development\ndataset, and collect 10 unique generated queries\nfrom both ED2LM and T5, ending up with 660\nquery-documents pairs for annotation. These pairs\n3754\nParagraph\nAn experience modiﬁer is an adjustment factor assigned to an Employer’s FEIN by the rating bureau\n(NCCI or State Bureau). The factor compares your loss data to other employers with the same class\ncodes, and is expressed as a credit or debit on your policy.\nModel Question Answerable ?\nT5 is a modiﬁer factor English No\nT5 what is experience modiﬁer rating No\nED2LM what is an experience modiﬁer in an insurance policy Yes\nED2LM experience modiﬁer deﬁnition Yes\nTable 4: Example generations from ED2LM-base and T5-base.\nare labeled by eight annotators with a single bi-\nnary question: “Is the generated query (question)\nanswered by the given document (passage)?”. We\navoid potential bias during annotation by not in-\nforming the annotators which system generated\nwhich questions. According to the annotated data,\n70.6% of the queries generated by ED2LM are an-\nswerable by the source document, while 52.1% of\nthe queries generated by T5 are answerable. We\nconjecture that the use of Eq. 3 for training makes\nthe query generator stick to the document better,\nleading to fewer hallucinations, thus producing bet-\nter ranking when the decoder is used as a ranker.\nQuestion vs. paragraph overlap. We measured\nthe overlap between generated questions and their\nrespective source passages using a set of 3k gener-\nated questions from each system. Intuitively, ques-\ntion generators that hallucinate less are more likely\nto stick to the text from the source paragraph. The\noverlap is computed as the macro-average of the\nquestion-paragraph word-level overlap, and is nor-\nmalised using the length of the question. While\nT5 has an overlap rate of 55.62% (i.e., 55.62%\nof question tokens also appear in the source para-\ngraph), ED2LM has an overlap rate of 62.14%,\nwhich is more than 6% higher than T5 model. In\nTable 4, we present example questions generated\nby T5 and ED2LM and their source paragraphs.\nAlthough T5 questions are somewhat related to the\nparagraph, the paragraph is not a good answer for\nthem. For example, in Table 4, the ﬁrst question T5\nhallucinates the word English, which compromises\nthe question quality.\n6.2 Synthetic Training Data for Retrieval\nFinally, we demonstrate the advantages of the gen-\nerated questions from ED2LM by using them to\ntrain a dual-encoder based passage retrieval model,\nfollowing the conﬁgurations in (Lu et al., 2021).\nSpeciﬁcally, we train a BERT large dual encoder\nmodel using the synthetic question-passage pairs\ngenerated by ED2LM and T5 respectively and re-\nport the results on MS MARCO dev set. For each\npassage, we generate three synthetic questions. We\nalso extract hard negatives by randomly sampling\npassages from the same document. During train-\ning, we use both in-batch negatives and hard nega-\ntives. During inference, we retrieve top 1000 pas-\nsages for each question from the passage collection\ncontaining about 8.8 million passages and report\nMRR@10. The model using ED2LM generated\ndata achieves MRR@10=30.4, whereas the model\nusing T5 generated data gets MRR@10=26.5. We\nargue that the boost is due to that the synthetic\ntraining data from ED2LM is with less generation\nhallucination (18% according to the manual anno-\ntation), thus including few training noise.\n7 Conclusion\nIn this work, we propose a novel model named\nED2LM. ED2LM encodes documents and decodes\nthe query using a trailing binary class token ap-\npended to the query for ranking. By training on\na dataset with more unique questions (namely,\n“RocketQA-Merge” (Qu et al., 2021)) and opti-\nmizing both query likelihood and a discriminative\nloss over the true/false token, ED2LM achieves\ncompetitive results compared to corresponding T5\nmodels. When used as a decoder-only language\nmodel during inference, ED2LM provides up to\n6.8X speedup without sacriﬁcing effectiveness. We\nfurther demonstrate that ED2LM could generate\nquestions with less hallucination. For future works,\nwe plan to investigate the uses of ED2LM for dif-\nferent (generation) tasks such as multi-sentence\ncompression (MRC) (Zhao et al., 2019), headline\ngeneration (Shen et al., 2019), and list question\nanswering (Katti et al., 2021).\n3755\nReferences\nLiyu Cao, Yixuan Qiao, Hao Chen, Peng Gao, Yuan Ni,\nand Guotong Xie. 2020. A Multiple Models Ensem-\nbling Method in Trec Deep Learning. In TREC.\nXiaoyang Chen, Kai Hui, Ben He, Xianpei Han,\nLe Sun, and Zheng Ye. 2022. A Context-Aware\nBERT Retrieval Model Incorporating Local and\nQuery-speciﬁc Context. In European Conference on\nInformation Retrieval, ECIR ’22. Springer.\nXuanang Chen, Ben He, Kai Hui, Le Sun, and Yingfei\nSun. 2021a. Simpliﬁed TinyBERT: Knowledge Dis-\ntillation for Document Retrieval. In Advances in\nInformation Retrieval - Proceedings of the 43rd Eu-\nropean Conference on IR Research, Part II, volume\n12657 of Lecture Notes in Computer Science, pages\n241–248. Springer.\nXuanang Chen, Ben He, Kai Hui, Yiran Wang, Le Sun,\nand Yingfei Sun. 2021b. Contextualized Ofﬂine Rel-\nevance Weighting for Efﬁcient and Effective Neural\nRetrieval. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 1617–1621.\nNachshon Cohen, Amit Portnoy, Besnik Fetahu, and\nAmir Ingber. 2021. SDR: Efﬁcient Neural Re-\nranking using Succinct Document Representation.\narXiv preprint arXiv:2110.02065.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, and\nDaniel Campos. 2021. Overview of the TREC 2020\nDeep Learning Track. TREC.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\nCampos, and Ellen M V oorhees. 2020. Overview of\nthe TREC 2019 Deep Learning Track. TREC.\nZhuyun Dai and Jamie Callan. 2020. Context-aware\nTerm Weighting for First Stage Passage Retrieval. In\nProceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 1533–1536.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\n2020. Funnel-transformer: Filtering out Sequen-\ntial Redundancy for Efﬁcient Language Processing.\nAdvances in neural information processing systems ,\n33:4271–4282.\nMostafa Dehghani, Yi Tay, Anurag Arnab, Lucas\nBeyer, and Ashish Vaswani. 2022. The Efﬁciency\nMisnomer. In International Conference on Learn-\ning Representations.\nMarco Del Tredici, Gianni Barlacchi, Xiaoyu Shen,\nWeiwei Cheng, and Adrià de Gispert. 2021. Ques-\ntion Rewriting for Open-Domain Conversational\nQA: Best Practices and Limitations. In Proceedings\nof the 30th ACM International Conference on In-\nformation & Knowledge Management , pages 2974–\n2978.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL-HLT (1).\nCicero dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhi-\nheng Huang, and Bing Xiang. 2020. Beyond [CLS]\nthrough Ranking by Generation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1722–\n1727.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical Neural Story Generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nShared Task: Evaluating Generalization in Reading\nComprehension. In Proceedings of the 2nd Work-\nshop on Machine Reading for Question Answering ,\npages 1–13.\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021.\nCOIL: Revisit Exact Lexical Match in Information\nRetrieval with Contextualized Inverted List. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3030–3042.\nShuguang Han, Xuanhui Wang, Mike Bendersky, and\nMarc Najork. 2020. Learning-to-Rank with BERT\nin TF-Ranking. CoRR, abs/2004.08476.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the Knowledge in a Neural Net-\nwork. In NIPS Deep Learning and Representation\nLearning Workshop.\nSebastian Hofstätter, Sophia Althammer, Michael\nSchröder, Mete Sertkan, and Allan Hanbury. 2020a.\nImproving Efﬁcient Neural Ranking Models with\nCross-architecture Knowledge Distillation. arXiv.\nSebastian Hofstätter, Markus Zlabinger, and Allan\nHanbury. 2020b. Interpretable & Time-Budget-\nConstrained Contextualization for Re-Ranking. In\nECAI 2020, pages 513–520. IOS Press.\nJia-Huei Ju, Jheng-Hong Yang, and Chuan-Ju Wang.\n2021. Text-to-text Multi-view Learning for Pas-\nsage Re-ranking. In Proceedings of the 44th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 1803–\n1807.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6769–6781.\n3756\nAnoop R Katti, Kai Hui, Adria de Gispert, and Hagen\nFuerstenau. 2021. Question Answering using Web\nLists. In Proceedings of the 30th ACM International\nConference on Information & Knowledge Manage-\nment, pages 3132–3136.\nOmar Khattab and Matei Zaharia. 2020. Col-BERT:\nEfﬁcient and Effective Passage Search via Contextu-\nalized late Interaction over BERT. In Proceedings\nof the 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval ,\npages 39–48.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, et al. 2019. Natural Questions:\nA Benchmark for Question Answering Research.\nTransactions of the Association for Computational\nLinguistics, 7:453–466.\nOleg Lesota, Navid Rekabsaz, Daniel Cohen,\nKlaus Antonius Grasserbauer, Carsten Eickhoff, and\nMarkus Schedl. 2021. A Modern Perspective on\nQuery Likelihood with Deep Generative Retrieval\nModels. In Proceedings of the 2021 ACM SIGIR\nInternational Conference on Theory of Information\nRetrieval, pages 185–195.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7871–7880.\nJing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni,\nand Yinfei Yang. 2021. Multi-stage Training with\nImproved Negative Contrast for Neural Passage Re-\ntrieval. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing:\nFindings.\nSean MacAvaney, Franco Maria Nardini, Raffaele\nPerego, Nicola Tonellotto, Nazli Goharian, and\nOphir Frieder. 2020. Efﬁcient Document Re-\nRanking for Transformers by Precomputing Term\nRepresentations. In Proceedings of the 43rd Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 49–58.\nCraig Macdonald, Richard McCreadie, Rodrygo L. T.\nSantos, and Iadh Ounis. 2012. From Puppy to Matu-\nrity: Experiences in Developing Terrier. In Proceed-\nings of the SIGIR 2012 Workshop on Open Source\nInformation Retrieval , pages 60–63. University of\nOtago, Dunedin, New Zealand.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A Human Generated Machine\nReading Comprehension Dataset. In CoCo@ NIPS.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and\nJimmy Lin. 2020. Document Ranking with a Pre-\ntrained Sequence-to-Sequence Model. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings , pages\n708–718.\nRodrigo Nogueira, Jimmy Lin, and AI Epistemic.\n2019a. From doc2query to docTTTTTquery. On-\nline preprint.\nRodrigo Nogueira, Wei Yang, Jimmy Lin, and\nKyunghyun Cho. 2019b. Document Expan-\nsion by Query Prediction. arXiv preprint\narXiv:1904.08375.\nRama Kumar Pasumarthi, Sebastian Bruch, Xuanhui\nWang, Cheng Li, Michael Bendersky, Marc Na-\njork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, and\nStephan Wolf. 2019. Tf-Ranking: Scalable Tensor-\nﬂow Library for Learning-to-rank. In Proceedings\nof the 25th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining.\nRaul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa\nPatwary, and Bryan Catanzaro. 2020. Training\nQuestion Answering Models From Synthetic Data.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5811–5826.\nYixuan Qiao, Hao Chen, Liyu Cao, Liping Chen,\nPengyong Li, Jun Wang, Peng Gao, Yuan Ni, and\nGuotong Xie. 2021. PASH at TREC 2020 Deep\nLearning Track: Dense Matching for Nested Rank-\ning.\nZhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Ku-\nmar Pasumarthi, Xuanhui Wang, Michael Bender-\nsky, and Marc Najork. 2021. Are Neural Rankers\nstill Outperformed by Gradient Boosted Decision\nTrees? In International Conference on Learning\nRepresentations.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\nHaifeng Wang. 2021. RocketQA: An Optimized\nTraining Approach to Dense Passage Retrieval for\nOpen-domain Question Answering. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5835–5847.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. 2020. Exploring the\nLimits of Transfer Learning with a Uniﬁed Text-to-\nText Transformer. Journal of Machine Learning Re-\nsearch, 21:1–67.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. RocketQAv2: A Joint Training Method\nfor Dense Passage Retrieval and Passage Re-ranking.\n3757\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2825–2835.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a Distilled Ver-\nsion of BERT: Smaller, Faster, Cheaper and Lighter.\narXiv preprint arXiv:1910.01108.\nSiamak Shakeri, Cicero dos Santos, Henghui Zhu,\nPatrick Ng, Feng Nan, Zhiguo Wang, Ramesh Nal-\nlapati, and Bing Xiang. 2020. End-to-End Synthetic\nData Generation for Domain Adaptation of Question\nAnswering Systems. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5445–5460.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, Ryan Sepassi, and Blake Hechtman. 2018.\nMesh-TensorFlow: Deep Learning for Supercomput-\ners. In Neural Information Processing Systems.\nXiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Di-\netrich Klakow, and Satoshi Sekine. 2019. Select\nand Attend: Towards Controllable Content Selec-\ntion in Text Generation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 579–590.\nLuca Soldaini and Alessandro Moschitti. 2020. The\nCascade Transformer: an Application for Efﬁcient\nAnswer Sentence Selection. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5697–5708.\nYi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Gupta, et al. 2022. Transformer mem-\nory as a differentiable search index. arXiv preprint\narXiv:2202.06991.\nJi Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin.\n2020. Early Exiting BERT for Efﬁcient Document\nRanking. In Proceedings of SustaiNLP: Workshop\non Simple and Efﬁcient Natural Language Process-\ning, pages 83–88.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N Bennett, Junaid Ahmed, and\nArnold Overwijk. 2020. Approximate Nearest\nNeighbor Negative Contrastive Learning for Dense\nText Retrieval. In International Conference on\nLearning Representations.\nLe Yan, Zhen Qin, Rama Kumar Pasumarthi, Xuanhui\nWang, and Mike Bendersky. 2021. Diversiﬁcation-\nAware Learning to Rank using Distributed Represen-\ntation. In The Web Conference.\nMing Yan, Chenliang Li, Chen Wu, Bin Bi, Wei Wang,\nJiangnan Xia, and Luo Si. 2019. IDST at TREC\n2019 Deep Learning Track: Deep Cascade Ranking\nwith Generation-based Document Expansion and\nPre-trained Language Modeling. In TREC.\nWangshu Zhang, Junhong Liu, Zujie Wen, Yafang\nWang, and Gerard de Melo. 2020. Query Distilla-\ntion: BERT-based Distillation for Ensemble Rank-\ning. In Proceedings of the 28th International Confer-\nence on Computational Linguistics: Industry Track ,\npages 33–43.\nYang Zhao, Xiaoyu Shen, Wei Bi, and Akiko Aizawa.\n2019. Unsupervised Rewriter for Multi-sentence\nCompression. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2235–2240.\nHonglei Zhuang, Zhen Qin, Shuguang Han, Xuanhui\nWang, Mike Bendersky, and Marc Najork. 2021a.\nEnsemble Distillation for BERT-Based Ranking\nModels. In Proceedings of the 2021 ACM SIGIR\nInternational Conference on the Theory of Informa-\ntion Retrieval (ICTIR ’21).\nShengyao Zhuang, Hang Li, and Guido Zuccon. 2021b.\nDeep Query Likelihood Model for Information Re-\ntrieval. In The 43rd European Conference On Infor-\nmation Retrieval (ECIR).\nShengyao Zhuang and Guido Zuccon. 2021. TILDE:\nTerm Independent Likelihood moDEl for Passage\nRe-ranking. In Proceedings of the 44th Interna-\ntional ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval , pages 1483–\n1492.\n3758",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.5714364051818848
    },
    {
      "name": "Inference",
      "score": 0.569078266620636
    },
    {
      "name": "Computer science",
      "score": 0.5465788245201111
    },
    {
      "name": "Natural language processing",
      "score": 0.502321720123291
    },
    {
      "name": "Cicero",
      "score": 0.4906822443008423
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.4632388949394226
    },
    {
      "name": "Linguistics",
      "score": 0.42197397351264954
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4157872200012207
    },
    {
      "name": "History",
      "score": 0.3304216265678406
    },
    {
      "name": "Philosophy",
      "score": 0.262203574180603
    },
    {
      "name": "Classics",
      "score": 0.19633379578590393
    },
    {
      "name": "Geology",
      "score": 0.0662529468536377
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}