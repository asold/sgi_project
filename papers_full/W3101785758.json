{
    "title": "TNT: Text Normalization based Pre-training of Transformers for Content Moderation",
    "url": "https://openalex.org/W3101785758",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2006335743",
            "name": "Fei Tan",
            "affiliations": [
                "Yahoo (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2099832399",
            "name": "Yifan Hu",
            "affiliations": [
                "Yahoo (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2155257647",
            "name": "Changwei Hu",
            "affiliations": [
                "Yahoo (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2102298416",
            "name": "Keqian Li",
            "affiliations": [
                "Yahoo (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2611888445",
            "name": "Kevin Yen",
            "affiliations": [
                "Yahoo (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2973034686",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2962699416",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2340954483",
        "https://openalex.org/W1976526581",
        "https://openalex.org/W2948433173",
        "https://openalex.org/W2148362501",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W1993301141",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2082983083",
        "https://openalex.org/W3103061166",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2966989210",
        "https://openalex.org/W1506426069",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2995923603",
        "https://openalex.org/W2151504427",
        "https://openalex.org/W2962797668",
        "https://openalex.org/W2967043539"
    ],
    "abstract": "In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion. Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task. Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4735–4741,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n4735\nTNT: Text Normalization based Pre-training of Transformers for Content\nModeration\nFei Tan, Yifan Hu, Changwei Hu, Keqian Li, Kevin Yen\nYahoo Research, New York, NY , USA\n{fei.tan, yifanhu, changweih, keqian.li, kevinyen}@verizonmedia.com\nAbstract\nIn this work, we present a new language\npre-training model TNT (Text Normalization\nbased pre-training of Transformers) for con-\ntent moderation. Inspired by the masking strat-\negy and text normalization, TNT is developed\nto learn language representation by training\ntransformers to reconstruct text from four op-\neration types typically seen in text manipula-\ntion: substitution, transposition, deletion, and\ninsertion. Furthermore, the normalization in-\nvolves the prediction of both operation types\nand token labels, enabling TNT to learn from\nmore challenging tasks than the standard task\nof masked word recovery. As a result, the ex-\nperiments demonstrate that TNT outperforms\nstrong baselines on the hate speech classiﬁca-\ntion task. Additional text normalization experi-\nments and case studies show that TNT is a new\npotential approach to misspelling correction.\n1 Introduction\nLanguage model pre-training (self-supervised or\nunsupervised learning) has been a popular thread\nin Natural Language Understanding (NLP) stud-\nies recently due to its universal representation ca-\npacity (Radford et al., 2018; Devlin et al., 2019;\nRadford et al., 2019; Brown et al., 2020). It has\nbeen thus widely used in a multitude of language\nprocessing tasks such as named entity recognition,\nsentiment analysis, question answering and con-\ntent moderation (Bodapati et al., 2019). In addi-\ntion, the masking pre-training paradigm introduced\nby BERT (Bidirectional Encoder Representations\nfrom Transformers) (Devlin et al., 2019) has been\nemployed for other tasks such as image processing\n(Trinh et al., 2019), optical ﬂow (Liu et al., 2019a),\nand audio-visual co-segmentation (Rouditchenko\net al., 2019).\nRecently, many variants have been proposed to\nfurther improve the pre-training procedure (Liu\net al., 2019b; Wang et al., 2019; Sun et al., 2019).\nThey have also advanced the state-of-the-art perfor-\nmance on multiple downstream natural language\nunderstanding tasks (Leaderboard) consistently. Al-\nmost all these studies train language models by\npredicting the masked words in different manners.\nThe underlying mechanism is Cloze task (Taylor,\n1953). The pre-training model itself, however, has\nnot been fully exploited to address complicated yet\nfeasible tasks. It is reasonable to expect that models\ncan learn a better universal language representation\nif the pre-training procedure can be aligned with\nmore challenging tasks.\nIn this article, we attempt to improve the lan-\nguage representation by proposing TNT: Text\nNormalization based pre-training of Transformers.\nTNT enhances the language learning by utilizing\ntext normalization pre-training objective, inspired\nby misspelling correction. Speciﬁcally, TNT ran-\ndomly manipulates tokens from the input text. The\nobjective is then to reconstruct the original tokens\nof the manipulated words based on the context by\npredicting both recovery operation type and orig-\ninal token labels as illustrated in Fig. 1. Unlike\nthe masked language model, TNT has to offer two\npredictions to reconstruct the original text. In addi-\ntion, TNT does not have to be given the prediction\npositions in advance. This aligns with the fact\nthat misspelling correction needs to perform the\nposition-agnostic prediction for both aspects.\nPerpetrators often intentionally obfuscate cer-\ntain words about groups, or abusive words, by\nmisspelling, or leetspeak (e.g.,“f@ggot”,“ph*ck”,\n“w.e.t.b.a.c.k.”) (Perea et al., 2008). This could\nsidestep the content moderation algorithms easily\nas exempliﬁed in Table 5. To assess the learning\ncapacity of TNT for obfuscated text and reduce the\ntraining cost, it is pre-trained only on one dataset\nand then applied to three datasets related to a hate\nspeech detection task. TNT achieves better results\n4736\ncompared to strong baselines on these datasets. Fur-\nthermore, we conduct an experiment on misspelling\ncorrection, and demonstrates that TNT has appeal-\ning language understanding capacity.\nOur contributions are summarized as follows:\n(1) we introduce text normalization into the lan-\nguage training paradigm, which involves challeng-\ning tasks of predicting both operation types and to-\nken labels; (2) we show that TNT advances the chal-\nlenging downstream text classiﬁcation task, which\nbeneﬁts the content moderation; (3) TNT offers a\nnew perspective on misspelling correction.\n2 Related Works\nBERT (Devlin et al., 2019) is developed by intro-\nducing the bidirectional encoder of well-known\ntransformers to learn the contextual representation\nof text, which is underpinned by the attention mech-\nanism (Vaswani et al., 2017). It randomly masks a\ncertain portion of tokens from the input and then\nlearns to predict these masked words. This cloze\ntask based pre-training strategy enables BERT to\nadvance the state-of-the-art performance on vari-\nous key NLP tasks. It also inspires the commu-\nnity with a plethora of subsequent works (Yang\net al., 2019; Sun et al., 2019; Wang et al., 2019;\nLiu et al., 2019b). Among them several are closely\nrelated to our approach: XLNet (Yang et al., 2019)\nand StructBERT (Wang et al., 2019) improve the\nmasking by imposing the permutation and shufﬂing\namong words and sentences. StructBERT is one\nof the current state-of-the-art algorithms topping\nthe GLUE leaderboard1 (Leaderboard), and is most\nsimilar to this work. TNT differs from StructBERT\nin that it is inspired by the need for misspelling cor-\nrection, and therefore not only allows permutation\nof words, but also deletion and insertion.\nFor online abusive language moderation, BERT\nhas also been shown effective and advances the\noverall performance largely (Bodapati et al., 2019).\nIn addition, early works formulate the hate speech\ndetection as the generic text classiﬁcation, alterna-\ntively focus on certain ethnic groups or building\nup blacklists of swear words (Nobata et al., 2016a;\nBadjatiya et al., 2017). Misspelling correction is\nalso a long-standing problem in NLP (Hirst and\nBudanitsky, 2005; Bassil, 2012; Islam and Inkpen,\n1The General Language Understanding Evaluation\n(GLUE) benchmark is a collection of resources for training,\nevaluating, and analyzing natural language understanding sys-\ntems. It consists of 9 sentence- or sentence-pair language\nunderstanding tasks.\n2009) and has been widely used in real-word sce-\nnarios like word processing system and email spell\nchecking.\nTable 1: Real user manipulated text examples. 0, 1,\n2 and 3 in the bracket correspond to recovery opera-\ntion types: substitution, transposition, deletion and in-\nsertion. Identity (4) is also exempliﬁed here for com-\npleteness\nManipulation Text Normalization\nsubstitution I damn sure didn’t vote\nfor the Marxist 8astard!\n8 → b(0)\ntransposition mario= dumb cutn t ↔ n(1)\ndeletion Don Lemon is a shthead. i (3)\ninsertion Pi–ss on Putin\n!!!!!!!!!! !!!!!!!!!! \u0001– (2)\nidentity She is a nice lady. c(4)\n3 Pre-training\n3.1 Architecture\nWe develop TNT based on a multi-layer bidirec-\ntional Transformer network as encoder used in pop-\nular language models like BERT (Devlin et al.,\n2019). The well-known masking strategy em-\nployed in BERT and subsequent works is inspired\nby cloze task for human-level language understand-\ning. People are required to ﬁll out the omitted\nwords from a passage based on context.\nUnlike the cloze procedure, text normalization\ninvolves more diversiﬁed and challenging tasks.\nThe goal is to rewrite a sentence that was not prop-\nerly formed, either due to misspelling, or due to\nintentional manipulation. A perpetrator would mis-\nspell on purpose, with the intention of evade detec-\ntion, through substitution, transposition, deletion\nand insertion as exempliﬁed in Table 1. The task\nof text normalization is to understand the manipu-\nlated sentence, and normalize it to the correct form.\nTherefore in TNT, motivated by the task of text\nnormalization, we propose the pre-training tasks of\nsubstitution, transposition, deletion and insertion.\nThe masking procedure could be viewed as a spe-\ncial case of the text normalization objective, under\nsubstitution.\nFor substitution (e.g., masking) and insertion,\nthe corresponding token label predictions along-\nside operation type is required to reconstruct text.\nSpeciﬁcally, we have normalization type o ∈O =\n{0, 1, 2, 3, 4}, where o is the operation type with\nvalues 0 (substitution), 1 (transposition), 2 (dele-\ntion), 3 (deletion) or 4 (identity), as exempliﬁed\nin Table 1. The normalized token labels l ∈ V\n4737\nt 1 \nT ransformers \nt S U B t 3 t 5 t 4 t 6 t I N S E t 7 t 8 \nt 1 t 2 t 3 t 4 t 5 t 6 t 7 t 8 \nt ' 1 t ' 2 t ' 3 t ' 4 t ' 5 t ' 6 t ' 7 t ' 8 \nManipulation \nIdentity\n(o1 =4) Substi\ntution (o2 =0)\nIdentity\n (o3 =4)\nTranspos\nition(o4 =1) None Insertion\n(o6 =3)\nDeletion\n(o7 =2)\nIdentity\n(o8 =4)Operation Type\nl1 =[NLB] l2 =t2 l3 =t3 l4 =[NLB] None l6 =t6 l7 =[NLB]l8 =[NLB]Token Label\nNormalization \nt 1 t 2 t 3 t 4 t 5 t 6 t 7 t 8 \nT om and Jerry love play ##ing foot ##ball \nT om is Jerry play love foot she ##ball \nT om and Jerry love play ##ing foot ##ball \nFigure 1: Wordpiece illustration of two pre-training subtasks in TNT. “None” means no prediction as the preceding\ntransposition covers it already. [CLS], [SEP], positional and segment embedding are omitted for brevity.\nare ground-truth tokens for corresponding types,\nwhere Vis the vocabulary. It is noted that for trans-\nposition, deletion and identity, no token labels are\nrequired to reconstruct the original text. Thus, we\nintroduce a special token symbol [NLB] as the\nplaceholder. Fig. 1 illustrates the generation proce-\ndure of pre-training instances and the joint training\nof two subtasks.\n3.2 Objective\nGiven an input sequence with manipulated to-\nkens, the operation type and token label objec-\ntives can be denoted simply as Ooperation =\narg max\nθo\n∑M\ni=1 wo\ni logP(oi|t′\ni; θo) and Olabel =\narg max\nθl\n∑M\ni=1 wl\nilogP(li|t′\ni; θl), respectively. t′\ni is\nthe observed token from the input sentence. oi and\nli are ground-truth operation type and token label\nas illustrated in Fig. 1. M is the maximum length\nof input sequence. θo and θl are sets of trainable\nparameters. wo\ni and wl\ni are weights for operation\ntype and token label, respectively. The overall pre-\ntraining objective is O= Ooperation + Olabel.\n4 Experiments\nWe perform the hate speech classiﬁcation and mis-\nspelling correction tasks based on the pre-trained\nmodel.\n4.1 Datasets\nOur primary dataset is extracted from user com-\nments on Yahoo News and Finance, and consisted\nof 1.43M labeled comments. Among them, 7% of\nthe comments are labelled as abusive (including\nhatespeech and profanity). The labeled data were\ncollected as follows: comments that are reported as\n“abusive” for any reason by users of Yahoo proper-\nties are sent to in-house trained raters for review,\nand the decisions of the raters form the labels. Fur-\nther details can be found at (Nobata et al., 2016b).\nIn addition, we experimented on two publicly avail-\nable hatespeech datasets: Twitter, and Wikipedia\n(Wiki) (Agrawal and Awekar, 2018). Wiki set here\nis a collection of discussions among editors on talk\npages for improving Wiki articles, which includes\ninﬂammatory posts. The statistics of three datasets\nare shown in Table 2.\nWe split the dataset into train/development/test\nsets with a ratio 70%/10%/20%. We generate vo-\ncabulary, pretrain the language modeling tasks, and\ntrain the hate speech prediction task using only the\ntraining set. We tune hyper-parameters on the de-\nvelopment set, and report ﬁnal results on the test\nset.\n4738\nTable 2: Basic statistics of datasets\nSource # Abusive # Clean Total % Abusive\nYahoo 100,652 1,328,486 1,429,138 7.04%\nTwitter 5,054 11,036 16,090 31.4%\nWiki 13,590 102,274 115,864 11.7%\n4.2 Experiment Setups\nFor TNT pre-training, the substitution follows the\nmasking setting in BERT. We also set up 5% ma-\nnipulation rate for transposition, deletion and in-\nsertion, respectively. The rest of the tokens remain\nunchanged.\nThe vocabulary generation, wordpiece tokeniza-\ntion, learning rate, weight decay, warm-up and\nother training settings follow BERT. The model\nsize is reduced to quarter of the original BERT. The\nmaximum length of input sequence M is set to 256.\nParameter scale is O = (V + M + S) ×H + L ×\n12H2 + H2 where V , S are vocabulary size |V|,\nsegment type size. H and L are the hidden layer\ndimension and the number of transformer block\nlayers, respectively.\nWe mainly report two models with wordpiece\nand character inputs as detailed in Table 3. The\nmain difference is that manipulation and normaliza-\ntion are performed on different levels. Wordpiece-\nand character-level operations are exempliﬁed in\nFig. 1 and Table 1, respectively.\nTable 3: Parameter scale and weight settings, character\nfor misspelling correction. Tuples of wo\ni and wl\ni are\nweights associated with operation type {0,1,2,3,4}\nModel V F S L H O woi wli\nWordpiece 40K 256 1 3 192 9.09M (1,1,1,1,0) (1,0,0,1,0)\nCharacter 6.8K 512 1 3 192 2.77M (1,1,1,1,1) (1,0,0,1,0)\nFor both TNT models, we run the pre-training\nprocedure for 64 epochs. We pre-train quarter size\nof BERT (wordpiece) and StructBERT (wordpiece)\nwith the same dataset, epochs and wordpiece vocab-\nulary. In the ﬁne-tuning phase, batch size, the num-\nber of batches and learning rate are set to 64, 10 and\n2e-5. For all models, we discard the sentence-pair\ntraining objective.\n4.3 Results\nFor the downstream hate speech classiﬁcation,\nwe do ﬁne-tuning on top of aggregate embed-\nding [CLS] of wordpiece TNT as BERT does.\nThreshold-free AUC@ROC and AUC@PR2 (Davis\n2Precision-Recall\nTable 4: Performance comparisons on wordpiece TNT\nfor hate speech classiﬁcation\nSource Method AUC@ROC AUC@PR MCC F1 Score\nYahoo\nBERT 93.92 69.56 60.34 61.10\nStructBERT 94.04 70.22 60.66 61.15\nTNT 94.30 (↑0.26) 70.91 (↑0.69) 62.04 (↑1.38) 63.18 (↑2.03)\nTwitter\nBERT 89.52 81.10 62.38 73.99\nStructBERT 89.96 82.77 62.58 74.09\nTNT 90.42 (↑0.46) 82.81 (↑0.04) 63.99 (↑1.41) 75.29 (↑1.20)\nWiki\nBERT 97.10 87.07 76.04 78.67\nStructBERT 97.10 87.07 75.59 78.37\nTNT 97.30 (↑0.20) 87.86 (↑0.79) 76.32 (↑0.73) 79.10 (↑0.73)\nTable 5: Case studies for different models\nCase BERT StructBERT TNT Text1 0.0434 0.0653 0.7401Ifnjjjjgersare killing more people daily..whyin the world would anyone want gun control.2 0.421 0.9275 0.9651Buyout Announced - $45/sharemassfaggot3 0.2445 0.4729 0.8083Good, we don’t want afoggatPresident anyway4 0.0925 0.0902 0.622DeyBigFatAceNeggirBeach be’sLOUD!!! lol!!!\nand Goadrich, 2006) and threshold (0.5 used here)\nbased F1 score and Matthews correlation coefﬁ-\ncient (MCC) are adopted. MCC and AUC@PR are\ngenerally regarded as balanced metrics.\nWe ﬁrst report the comparison results on Yahoo\nset. As shown in Table 4, our TNT outperforms\nBERT and StructBERT on test set. Particularly, it\nachieves more than 1% and 2% improvement in\nterms of MCC and F1 score, respectively. The per-\nformance gain could be reasonably attributed to the\nnew training objective. We further do ﬁne-tuning\nbased on pre-trained models directly for Twitter\nand Wikipedia sets, respectively. The advantage of\nTNT over baselines still holds. The superiority of\nTNT on classiﬁcation task over multiple datasets\nsignalizes that text normalization based training\nstrategy is a promising direction for better univer-\nsal language representation learning. Although all\nthree sets are user generated content, Wiki users\nare somewhat different. As Wiki itself is a collab-\norative knowledge repository, editors are likely to\nattack others due to disputes on speciﬁc domain\nknowledge. However, the users are the general\npublic who post comments and tweets more casu-\nally for Yahoo and Twitter. In this context, text\nmisspellings in Wiki are likely to be less severe\nand intentional than others. The way we develop\nthe model enables it to learn better representations\nespecially for garbled text compared to standard\nmasking schemes. Thus, the performance gain is\nmore salient in Yahoo and Twitter.\nTo better understand their performance differ-\nence intuitively, we illustrate in Table 5 some spe-\nciﬁc error case analysis, where toxic comments are\ncreated by users to attack a certain group of people.\nThe key parts are all intentionally manipulated to\n4739\nTable 6: Misspelling correction comparison examples. For Bing Spell Check, we render the best results from Proof\nand Spell modes. The corrected results of Google Docs and Grammarly are based on their top suggestions\nCase Text Autocorrect Bing Spell Check Google Docs Grammarly TNT\n1 M ke Y*h0! Graet Aga 1n M he Y*h0! Great Age 1n M ke Y*h0! Graet Aga 1n M ke Y*h0! Great Aga 1n M ke Y*h0! Great Aga 1n Make Yahoo Great Again\n2 T R⃝ump anf B*!den T R⃝ump and B*!den Trump and Biden T R⃝ump and B*!den T R⃝ump and B*!den Trump and Biden\n3 UAS is a great county Was is a great county USA is a great county UAS is a great county UAS is a great county USA is a great country\n4 UAS stands for Unmanned\nAircraft Systems\nWas stands for Unmanned\nAircraft Systems\nUAS stands for Unmanned\nAircraft Systems\nUAS stands for Unmanned\nAircraft Systems\nUAS stands for Unmanned\nAircraft Systems\nUAS stands for Unmanned\nAircraft Systems\n5 she recieved her prize she received her prize she received her prize she received her prize she received her prize she received her prize\n6 we had heard from\nyou more deﬁnately\nwe had heard from\nyou more deﬁnitely\nwe had heard from\nyou more deﬁnitely\nwe had heard from\nyou more deﬁnitely\nwe had heard from\nyou more deﬁnitely\nwe had heard from\nyou more deﬁnately\nobfuscate moderation algorithms. For case 1 with\nsubstitution, TNT functions well but others work\npoorly. Both StructBERT and TNT work better\nthan BERT for case 2 involving white space dele-\ntion. Case 3 comes out with a subtle transposition,\nTNT performs robust than the other two. Regarding\nthe most challenging case 4 with combination of\nwhite space deletion, transposition and substitution,\nonly TNT still works well overall.\n5 Misspelling Correction\nMisspelling correction is a long-standing research\ntopic (Islam and Inkpen, 2009; Whitelaw et al.,\n2009; Bassil, 2012) and has been widely commer-\ncialized as a service such as Bing Spell Check\n(Bing) and Grammarly (Grammarly). TNT can\nbe readily employed in misspelling correction. We\nhere evaluate TNT using its character-level3 variant\nwithout additional ﬁne-tuning.\nWe aggressively misspell the test set of Yahoo in\nTable 2 by 15% for each sample. Then we employ\nthe pre-trained TNT model to recover the text. As a\ncomparison, we also examine an open-source4 tool\nautocorrector (Autocorrect) for reference. Edit dis-\ntance (Distance), and BLEU (BLEU) are adopted\nto measure the distance and similarity between cor-\nrected samples and original ones as detailed in Ta-\nble 7. TNT performs signiﬁcantly better than the\ndictionary look-up algorithm.\nTable 7: Misspelling correction comparison\nMetric Misspelling Autocorrect TNT\nEdit Distance (↓) 16.9391 16.0261 4.0115\nNormalized Edit Distance (↓) 0.1295 0.1259 0.0307\nBLEU (↑) 0.3818 0.4609 0.8309\nIn addition, we cross-check the results between\nTNT and popular commercial spell check products\nthrough case studies as reported in Table 6. Among\nall tools, Bing leads the performance, followed\n3As misspelling usually involves with many subtle changes,\nwe resort to character sets as the vocabulary with much ﬂexi-\nbility.\n4only free API with large-scale calls\nby Google Docs and Grammarly, and Autocorrect\nperforms the worst. Overall, TNT functions very\nwell particularly for case 1 as a combination of\nmultiple challenging misspellings. It is noted that\nreceived and deﬁnitely are two of most commonly\nmisspelled words (Words), but TNT fails on the\ncorrection of “deﬁnately”. Overall, “deﬁnitely”\nis not a strongly contextual word derived from the\nwhole sentence here. The limited training set might\nrestrict the correction capacity as well.\n6 Discussions and future work\nWe conducted experiments on three classiﬁcation\ntasks. The data size for Yahoo Finance and News,\nwhile being one of the largest in the context of hate-\nspeech classiﬁcation, is nevertheless small in the\ncontext of language modeling. We plan to perform\nlarge-scale pre-training and evaluation on GLUE\ndatasets for the comprehensive analysis.\nThis work targets sentence level language un-\nderstanding. As far as we know, no data available\nfor misspelled words in the context of sentences,\nwe thus have to generate the evaluation set by our-\nselves. The main goal here is not to develop a\nmore powerful misspelling corrector, but rather to\npropose a new and stronger language modeling ap-\nproach. We thus don’t set up the strict and compre-\nhensive evaluation for apples-to-apples comparison\non spelling correction. We will continue to explore\nthis line in the future.\n7 Conclusion\nIn this work, we propose a new language repre-\nsentation training strategy TNT. TNT improves\nlanguage modeling by training a transformer to\nreconstruct text from four operation types typically\nseen in text manipulation. We show that when ﬁne-\ntuned for the content moderation task of detecting\nhatespeech, the new model performed better than\nthe state of the art baselines. We also demonstrate\nits effectiveness in misspelling correction.\n4740\nReferences\nSweta Agrawal and Amit Awekar. 2018. Deep learn-\ning for detecting cyberbullying across multiple so-\ncial media platforms. In European Conference on\nInformation Retrieval, pages 141–153. Springer.\nAutocorrect. https://github.com/fsondej/\nautocorrect.\nPinkesh Badjatiya, Shashank Gupta, Manish Gupta,\nand Vasudeva Varma. 2017. Deep learning for hate\nspeech detection in tweets. In Proceedings of the\n26th International Conference on World Wide Web\nCompanion, pages 759–760.\nYoussef Bassil. 2012. Parallel spell-checking algo-\nrithm based on yahoo! n-grams dataset. arXiv\npreprint arXiv:1204.0184.\nBing. https://azure.microsoft.com/\nen-in/services/cognitive-services/\nspell-check/.\nBLEU. https://en.wikipedia.org/wiki/BLEU.\nSravan Babu Bodapati, Spandana Gella, Kasturi Bhat-\ntacharjee, and Yaser Al-Onaizan. 2019. Neural word\ndecomposition models for abusive language detec-\ntion. arXiv preprint arXiv:1910.01043.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nJesse Davis and Mark Goadrich. 2006. The relation-\nship between precision-recall and roc curves. InPro-\nceedings of the 23rd international conference on Ma-\nchine learning, pages 233–240.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nEdit Distance. https://en.wikipedia.org/\nwiki/Edit_distance.\nGrammarly. https://app.grammarly.com/.\nGraeme Hirst and Alexander Budanitsky. 2005. Cor-\nrecting real-word spelling errors by restoring lex-\nical cohesion. Natural Language Engineering ,\n11(1):87–111.\nAminul Islam and Diana Inkpen. 2009. Real-word\nspelling correction using google web 1tn-gram data\nset. In Proceedings of the 18th ACM conference\non Information and knowledge management , pages\n1689–1692.\nGLUE Leaderboard. https://gluebenchmark.\ncom/leaderboard/.\nPengpeng Liu, Michael Lyu, Irwin King, and Jia Xu.\n2019a. Selﬂow: Self-supervised learning of opti-\ncal ﬂow. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n4571–4580.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nChikashi Nobata, Joel Tetreault, Achint Thomas,\nYashar Mehdad, and Yi Chang. 2016a. Abusive lan-\nguage detection in online user content. In Proceed-\nings of the 25th international conference on world\nwide web, pages 145–153.\nChikashi Nobata, Joel Tetreault, Achint Thomas,\nYashar Mehdad, and Yi Chang. 2016b. Abusive lan-\nguage detection in online user content. In WWW.\nManuel Perea, Jon Andoni Du ˜nabeitia, and Manuel\nCarreiras. 2008. R34d1ng w0rd5 w1th numb3r5.\nJournal of experimental psychology. Human percep-\ntion and performance, 34:237–41.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAndrew Rouditchenko, Hang Zhao, Chuang Gan,\nJosh McDermott, and Antonio Torralba. 2019.\nSelf-supervised audio-visual co-segmentation. In\nICASSP 2019-2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 2357–2361. IEEE.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism quarterly,\n30(4):415–433.\nTrieu H Trinh, Minh-Thang Luong, and Quoc V Le.\n2019. Selﬁe: Self-supervised pretraining for image\nembedding. arXiv preprint arXiv:1906.02940.\n4741\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao,\nLiwei Peng, and Luo Si. 2019. Structbert: In-\ncorporating language structures into pre-training\nfor deep language understanding. arXiv preprint\narXiv:1908.04577.\nCasey Whitelaw, Ben Hutchinson, Grace Y Chung, and\nGerard Ellis. 2009. Using the web for language\nindependent spellchecking and autocorrection. In\nProceedings of the 2009 Conference on Empirical\nMethods in Natural Language Processing: Volume\n2-Volume 2, pages 890–899. Association for Com-\nputational Linguistics.\nMisspelled Words. https://www.msn.com/en-\nus/lifestyle/lifestyle-buzz/the-10-most-commonly-\nmisspelled-words-in-the-english-language/ar-\nBBOS1ZB.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764."
}