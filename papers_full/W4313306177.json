{
  "title": "Local Information Interaction Transformer for Hyperspectral and LiDAR Data Classification",
  "url": "https://openalex.org/W4313306177",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2103569835",
      "name": "Yuwen Zhang",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135806602",
      "name": "Yishu Peng",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105618510",
      "name": "Bing Tu",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2150906733",
      "name": "Yaru Liu",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2888119354",
    "https://openalex.org/W3023351371",
    "https://openalex.org/W2962770389",
    "https://openalex.org/W3024007459",
    "https://openalex.org/W2465503420",
    "https://openalex.org/W4226264593",
    "https://openalex.org/W2907147407",
    "https://openalex.org/W2791006446",
    "https://openalex.org/W2898590000",
    "https://openalex.org/W2116248378",
    "https://openalex.org/W2900660594",
    "https://openalex.org/W2059217921",
    "https://openalex.org/W2008233110",
    "https://openalex.org/W2140940625",
    "https://openalex.org/W2565258258",
    "https://openalex.org/W2900827684",
    "https://openalex.org/W2002392274",
    "https://openalex.org/W1976416886",
    "https://openalex.org/W2606929568",
    "https://openalex.org/W3033482004",
    "https://openalex.org/W2122431438",
    "https://openalex.org/W2029316659",
    "https://openalex.org/W2603422184",
    "https://openalex.org/W2811355488",
    "https://openalex.org/W3174236562",
    "https://openalex.org/W2765739551",
    "https://openalex.org/W3004968762",
    "https://openalex.org/W3081753142",
    "https://openalex.org/W2890133123",
    "https://openalex.org/W4206702230",
    "https://openalex.org/W3136140595",
    "https://openalex.org/W3016244469",
    "https://openalex.org/W3094484482",
    "https://openalex.org/W2977002487",
    "https://openalex.org/W3031696400",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W4285124347",
    "https://openalex.org/W4223616928",
    "https://openalex.org/W3128776197",
    "https://openalex.org/W3171853541",
    "https://openalex.org/W4210794570",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W6790830454",
    "https://openalex.org/W4226379819",
    "https://openalex.org/W2136251662",
    "https://openalex.org/W2243870093",
    "https://openalex.org/W3132524115",
    "https://openalex.org/W3214821343",
    "https://openalex.org/W4225931144",
    "https://openalex.org/W3098388691",
    "https://openalex.org/W3105997607",
    "https://openalex.org/W3169064633"
  ],
  "abstract": "The multisource remote sensing classification task has two main challenges. 1) How to capture hyperspectral image (HSI) and light detection and ranging (LiDAR) features cooperatively to fully mine the complementary information between data. 2) How to adaptively fuse multisource features, which should not only overcome the imbalance between HSI and LiDAR data but also avoid the generation of redundant information. The local information interaction transformer (LIIT) model proposed herein can effectively address these above issues. Specifically, multibranch feature embedding is first performed to help in the fine-grained serialization of multisource features; subsequently, a local-based multisource feature interactor (L-MSFI) is designed to explore HSI and LiDAR features together. This structure provides an information transmission environment for multibranch features and further alleviates the homogenization processing mode of the self-attention process. More importantly, a multisource feature selection module (MSTSM) is developed to dynamically fuse HSI and LiDAR features to solve the problem of insufficient fusion. Experiments were carried out on three multisource remote-sensing classification datasets, the results of which show that LIIT has more performance advantages than the state-of-the-art CNN and transformer methods.",
  "full_text": "1130 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nLocal Information Interaction Transformer for\nHyperspectral and LiDAR Data Classiﬁcation\nYuwen Zhang, Student Member, IEEE, Yishu Peng , Member, IEEE,B i n gT u, Member, IEEE,\nand Yaru Liu, Student Member, IEEE\nAbstract—The multisource remote sensing classiﬁcation task has\ntwo main challenges. 1) How to capture hyperspectral image (HSI)\nand light detection and ranging (LiDAR) features cooperatively\nto fully mine the complementary information between data. 2)\nHow to adaptively fuse multisource features, which should not\nonly overcome the imbalance between HSI and LiDAR data but\nalso avoid the generation of redundant information. The local\ninformation interaction transformer (LIIT) model proposed herein\ncan effectively address these above issues. Speciﬁcally, multibranch\nfeature embedding is ﬁrst performed to help in the ﬁne-grained\nserialization of multisource features; subsequently, a local-based\nmultisource feature interactor (L-MSFI) is designed to explore\nHSI and LiDAR features together. This structure provides an\ninformation transmission environment for multibranch features\nand further alleviates the homogenization processing mode of the\nself-attention process. More importantly, a multisource feature\nselection module (MSTSM) is developed to dynamically fuse HSI\nand LiDAR features to solve the problem of insufﬁcient fusion.\nExperiments were carried out on three multisource remote-sensing\nclassiﬁcation datasets, the results of which show that LIIT has\nmore performance advantages than the state-of-the-art CNN and\ntransformer methods.\nIndex Terms—Feature fusion, local information interaction\ntransformer (LIIT), multisource data classiﬁcation, transformer.\nI. INTRODUCTION\nT\nHE classiﬁcation of remote-sensing images, a pixel-level\nclassiﬁcation task, is the process of recognizing unmarked\nareas by learning to obtain prior knowledge[1], [2], [3], [4],\nand applied in various practices[5], [6]. Hyperspectral (HSI)\nManuscript received 17 October 2022; revised 2 December 2022; accepted 21\nDecember 2022. Date of publication 30 December 2022; date of current version\n10 January 2023. This work was supported in part by the National Natural\nScience Foundation of China under Grant 61977022 and Grant 62271200, in part\nby the Foundation of Department of Water Resources of Hunan Province under\nGrant XSKJ2021000-12, Grant XSKJ2021000-13, and Grant XSKJ2022068-\n48, in part by the Natural Science Foundation of Hunan Province under Grant\n2021JJ40226, in part by the Foundation of Education Bureau of Hunan Province\nunder Grant 21B0590, Grant 21B0595, and Grant 20B062, and in part by the\nScientiﬁc Research Fund of Education Department of Hunan Province under\nGrant 19A200.(Corresponding author: Yishu Peng.)\nYuwen Zhang, Yishu Peng, and Yaru Liu are with the School of In-\nformation Science and Engineering, Hunan Institute of Science and Tech-\nnology, Yueyang 414000, China (e-mail: yuwen_zhang@vip.hnist.edu.cn;\nlovepys@hnist.edu.cn; liuyarua@foxmail.com).\nBing Tu is with the School of Information Science and Engineering, Hu-\nnan Institute of Science and Technology, Yueyang 414000, China, and also\nwith the Guangxi Key Laboratory of Cryptography and Information Security,\nGuilin University of Electronic Technology, Guilin 541000, China (e-mail:\ntubing@hnist.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3232995\ndata is obtained by imaging spectrometer, which can provide\na large amount of narrowband spectral information from the\nvisible spectrum to the infrared spectrum for each pixel[7].\nDue to its rich spectral information content, HSI data have more\ndetailed surface feature description ability than other remote-\nsensing data, and is one of the most suitable remote sensing\nclassiﬁcation task data sources[8]. However, it is worth noting\nthat there are often similar spectral curves with different types\nof ground objects in real ground objects; furthermore, the same\ntype of ground objects can show different spectral curves due to\ndifferences in their regional distribution. Therefore, single HSI\ndata has limitations when dealing with the task of classifying\ncomplex terrain scenes. With the development of remote-sensing\nimaging technology, each sensor can obtain various remote-\nsensing data with different physical characteristics from the\nsame geographical space [9], [10], [11]. Light detection and\nranging (LiDAR) point cloud data carries distance information\nbetween sensors and ground objects, and can be converted\ninto its image version DSM through preprocessing. Images can\ndescribe the elevation information of the ﬁgure by the size of\nthe gray value[12], [13], [14]. Since LiDAR data are not easily\naffected by the external atmospheric medium and weather, its\nelevation information also provides strong support for accurate\nsurface-feature classiﬁcation tasks.\nTo overcome the shortcomings of single HSI data derived\nfrom classiﬁcation tasks, many researchers try to use integration\nstrategies to combine the unique features of HSI and LiDAR data\nto develop complementary advantages and carry out collabo-\nrative feature recognition of multimodal data[15], [16], [17].\nMattia et al.[18] initially obtained the extended attribute proﬁle\n(EAP) of HSI and LiDAR data from the morphology perspective.\nAfter feature concatenation and fusion, they were sent to the clas-\nsiﬁer to complete the classiﬁcation. However, a simple fusion\nmethod will generate feature redundancy and a the existence of\nlarge number of dimensions after stacking will easily cause the\nHughes phenomenon. In this regard, Behnood et al.[19] intro-\nduced the kernel principal component analysis (KPCA) method\nto reduce the dimensions of features after obtaining the extended\nproﬁle of HSI and LiDAR data; and realized feature fusion with\nthe help of orthogonal TV component analysis (OTVCA). Jia\net al. [20] used superpixel-guided KPCA to preprocess HSI\ndata. Then, the 2-D and 3-D Gabor ﬁlter is used to extract\nthe features of LiDAR and processed HSI data, respectively,\nso as to obtain the identiﬁable multisource Gabor features with\nthe magnitude and phase information. Zhang et al.[21] tried\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nZHANG et al.: LOCAL INFORMATION INTERACTION TRANSFORMER FOR HYPERSPECTRAL AND LIDAR DATA CLASSIFICATION 1131\nusing various classiﬁers, i.e., SVM, RF, and KNN, to obtain\npreliminary HSI and LiDAR classiﬁcation results, and to achieve\nmore robust classiﬁcation of multisource remote sensing data\nfrom the decision level through majority weighted voting. Al-\nthough the above classical methods fuse HSI and LiDAR data\nfrom multiple perspectives, the processing of multisource data\nis incomplete and the information is not fully utilized.\nThe emergence of deep neural network (DNN) helps to extract\ndeeper semantic information from remote-sensing data [22],\n[23], [24], [25]. By placing each modal data in different struc-\ntural branches, data information can be more fully mined. Xu\net al.[26] designed different network structures for the feature\nextraction of HSI and LiDAR data, and proposed a two-branch\nCNN model. One branch uses a 2-D and 1-D hybrid CNN\nstructure to capture the spectral− spatial features of HSI and\nthe other branch designs a cascade-based CNN to explore the\nelevation information of LiDAR data. Zhao et al.[27] applied\na weight contribution mechanism to the dual-branch structure\nand proposed the coupled CNN model, that model not only alle-\nviates the calculation pressure of the model, but also guides the\nlearning process of the dual branches, strengthening the feature\nconsistency by sharing the last two convolutions of the HSI and\nLiDAR branches. Hong et al.[28] applied the full connection\nlayer model to multisource remote sensing classiﬁcation tasks,\nand designed additional feature reconstruction structures after\nthe encoding process of HSI and LiDAR data, with the aim of\npromoting feature fusion more compactly. In[29], multiscale\nPToP CNN was designed to obtain HSI and LiDAR features at\ndifferent scales to make full use of multisource remote sensing\ninformation. However, the conventional deep learning methods\nhave defects in dealing with the interference of spatial edge\npixels. The CNN-based methods are even more due to the\nlimitation of the ﬁxed convolution kernel size, leading to the\nintroduction of extra classes of pixels, which affects the training\neffect of the model[30], [31], [32].\nTherefore, researchers start to designing a network module\nwith attention capability[33], [34], [35]. By adaptively identi-\nfying the importance of features and giving them correspond-\ning weight values, it can highlight important information and\nweaken the function of secondary information to enhance the\nfeature recognition. The transformer is designed with an atten-\ntion module as the basic framework and has made remarkable\nachievements in natural-language processing tasks[36], [37],\n[38], [39]. Alexey et al.[40] introduced a transformer into the\nimage ﬁeld for the ﬁrst time by serializing images, and proposed\nvision transformer (ViT), that can model features at the global\nlevel both simply and effectively, and established dependencies\nbetween the sequence data. Through the introduction and im-\nprovement of the ViT model, the joint classiﬁcation task of\nHSI and LiDAR data has been further broken through. Dong\net al. [41] proposed an effective multibranch feature fusion\nnetwork with self- and cross-guided attention. This method\nstarted by obtaining the weight graph of LiDAR and is used to\nguide the self-attention of LiDAR data and the cross-attention\nof HSI data, respectively. Xue et al.[42] used deep hierarchical\nvision transformer (DHViT) to extract sequence features of HSI\nand LiDAR data, and fused various modal sequences after the\ncross-attention module. Although the above methods effectively\ncapture the heterogeneous features of HSI and LiDAR, they do\nnot take the information imbalance between HSI and LiDAR\ndata into account during the fusion process.\nFrom the perspective of multimodal information interaction\nand feature screening, we propose a local information interac-\ntion transformer (LIIT) model to capture and fuse multimodal\nremote sensing data dynamically. Speciﬁcally, the dual-branch\ntransformer was ﬁrst designed to fully extract the sequence\nfeatures of HSI and LiDAR. In this process, local based mul-\ntisource feature interactor (L-MSFI) is developed to endow\nthe global-based transformer model with a local spatial feature\ninformation interaction ability. In addition, a multisource feature\nselection module (MSFSM) is introduced to give weight to each\nmodal data to realize the dynamic multimodal data ﬁltering\nfunction and solve the imbalance problem between features.\nSubsequently, the fused feature is put into the convolutional\ntransformer module to help with further training and classiﬁ-\ncation. Compared to state-of-the-art methods in several open\nmultisource remote sensing datasets, LIIT can achieve better\nclassiﬁcation performance. The main contributions of this article\nare as follows.\n1) A local-based multisource feature interactor (L-MSFI) is\ndesigned to provide an information interaction environ-\nment for HSI and LiDAR features, avoid independent\nfeature extraction process, and guide features to learn from\neach other.\n2) The convolution module is added to the self-attention,\nwhich overcomes the process of the gradual homogeniza-\ntion of different features due to them having the same\noperation in the self-attention, and makes its description\nof features more detailed.\n3) An MSFSM has been developed to solve the balance\nproblem of HSI and LiDAR features in the fusion and\nreduce the generation of redundant information by dy-\nnamically ﬁltering source components in the sequence\nfeatures.\nThe remainder of this article is organized as follows. SectionII\ndescribes the proposed LIIT method, SectionIII introduces the\ndesign of parameters in the LIIT method and the comparison\nwith multisource remote sensing classiﬁcation methods on the\nHouston, MUUFL, and Trento datasets, and SectionIV con-\ncludes this article.\nII. DE S C R I P T I O NO FT H EPROPOSED APPROACH\nThis section ﬁrst gives an overall introduction to the proposed\nLIIT method. On this basis, the functions and importance of\nL-MSFI and MSFSM are analyzed and explained in detail.\nA. Overview of the Proposed Method\nThe joint classiﬁcation task of HSI and LiDAR data aims\nto make full use of their respective outstanding features, com-\nplement each other’s advantages, and break through the perfor-\nmance bottleneck when using a single data source. The main\nchallenges are as follows. 1) How to effectively capture the se-\nmantic features of multisource data and maximize the retention\n1132 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 1. Framework of the proposed LIIT method, where the upper branch is used to extract HSI feature, and the lower is the respective LiDAR branch to extract\nelevation feature.\nof data information without Hughes phenomenon. 2) How to\navoid information redundancy and overcome the problem of data\nimbalance combined with the heterogeneous features between\nmultimodes, due to the imbalance of the importance between\nHSI and LiDAR data for classiﬁcation.\nThe LIIT model is proposed for the above analysis, and Fig.1\nshows its structural framework. Speciﬁcally, the dual-branch\ntransformer is ﬁrst adopted to obtain the semantic features of HSI\nand LiDAR data from the level of sequence global dependency.\nIn this process, L-MSFI is designed to mine the information of\nmultisource features from the local level, while also forming\ninformation interactions between multimodal features to avoid\nthe closed state of the feature extraction process of each branch.\nMSFSM is used to dynamically ﬁlter HSI and LiDAR features;\nits adaptive feature fusion method effectively avoids feature\nimbalance and fusion redundancy. Finally, the fusion features\nare further trained by the transformer and the ﬁnal classiﬁcation\nis completed.\nB. Embedding for HSI and LiDAR Data\nFeature embedding, which aims to perform sequence mapping\nof image data and establish the interdependence of sequence\nfeatures from the global level is the initial step of the trans-\nformer [43], [44], [45]. For the vanilla ViT, the convolution\nlayer with the same step size and kernel size is usually used\nto perform nonoverlapping blocking operations on the image,\nand then ﬂatten each block to form sequence data[46], [47].\nHowever, this process is obviously coarse-grained and there is\na lack of information transfer between blocks.\nFor the feature embedding in the LIIT method, using a princi-\npal component analysis (PCA) algorithm reduces the dimensions\nof the HSI dataXH∈ RH×W×C and preextracts its spectral fea-\ntures. Then, the convolution module helps conduct tokenization\nfor HSI and LiDAR data. Speciﬁcally, the convolution process\nis Conv-BN-ReLU-DWConv-BN-ReLU\nX1\nH = ReLU(BN(Conv(XH)))\nX1\nL = ReLU(BN(Conv(XL)))\nX2\nH = ReLU\n(\nBN\n(\nDWConv\n(\nX1\nH\n)))\nX2\nL = ReLU\n(\nBN\n(\nDWConv\n(\nX1\nL\n)))\n(1)\nwhere multisource data are put into different convolution\nbranches. This process is ﬁne-grained and ensures dimensional\nalignment between multimodal features. After features have\nbeen serialized, add class token (EH\ncls, EL\ncls) for classiﬁcation\nand position embeddings (EH\npos, EL\npos) for encoding sequence\nsequence to multisource data. The process is as follows:\nSH =\n[\nX2\nH; EH\ncls\n]\n+ EH\npos\nSL =\n[\nX2\nL; EL\ncls\n]\n+ EL\npos (2)\nwhere ; is the concatenation process,SH, SL∈ R(N+1)×D is the\noutput sequences of HSI and LiDAR data,N is the number of\nsequences, andD is the number of channels.\nZHANG et al.: LOCAL INFORMATION INTERACTION TRANSFORMER FOR HYPERSPECTRAL AND LIDAR DATA CLASSIFICATION 1133\nAfter the embedding process, the sequences will put into the\ntransformer for feature extraction.\nC. Local-Based Multisource Feature Interactor (L-MSFI)\nIt is worth noting that the self-attention process can help\neach token in the sequence to transmit information, but it will\nalso make different tokens homogeneous because of the sim-\nilar operation. Therefore, the use of a convolutional module\nhelps feature focus on local information, thus avoiding the\nover-smoothing of self-attention. Speciﬁcally, the class token\n(EH\ncls, EL\ncls) of each modal feature is stripped ﬁrst, and the rest of\nthe features are reshaped to a spatial feature block whose size is\nconsistent with the original input. Then, the convolution module\nis designed to establish local correlation for the spatial neigh-\nborhood intra of each source data, and the process is shown as\nfollows:\nQH,KH,VH\n= ConvQ1\n(\nX2\nH\n)\n,ConvK1\n(\nX2\nH\n)\n,ConvV 1\n(\nX2\nH\n)\nQL,KL,VL\n= ConvQ2\n(\nX2\nL\n)\n,ConvK2\n(\nX2\nL\n)\n,ConvV 2\n(\nX2\nL\n)\n(3)\nwhere Conv is represent the convolution module, which is\nconsistent with the embedding process, and includes com-\nmon convolutional layer and deep-wise convolutional layer.\nAmong them, the common convolutional layer is used for in-\nformation transfer between channel dimension features, while\ndeep-wise convolution can relieve the pressure of model\nparameters.\nThe information interaction between multisource data and\nfeature extraction is the key to the joint classiﬁcation task, which\nmakes the feature extraction process of each branch not isolated,\nand conducive to the expression of each source feature. Con-\nsidering that the class token is the classiﬁcation representation\nin each branch, global dependency can be established with the\nbranch feature in self-attention. To this end, we ﬂatten the HSI\nand LiDAR features after convolution, and concatenate each\nbranch’s class token onto another branch, which is shown in\nFig. 1, and the expression is shown as follows:\nQH,KH,VH =\n[\nQH; EL\ncls\n]\n,\n[\nKH; EL\ncls\n]\n,\n[\nVH; EL\ncls\n]\nQL,KL,VL =\n[\nQL; EH\ncls\n]\n,\n[\nKL; EH\ncls\n]\n,\n[\nVL; EH\ncls\n]\n. (4)\nAs the attention process proceeds, the class token continuously\nlearns the semantic information of another branch\nZH = Attention(QH,KH,VH)\n= Softmax\n( QHKH\nT\n√dk\n)\nVH\nZL = Attention(QL,KL,VL)\n= Softmax\n( QLKL\nT\n√dk\n)\nVL (5)\nwhere ZH and ZL are the output of the attention for HSI and\nLiDAR branches. After completion, class tokens are returned\nFig. 2. Structure of MSFSM, which fuses multisource features by adaptively\nﬁltering HSI and LiDAR heterogeneous features.\nto the original branch and sent to the MLP module along with\nthe original branch features to further capture the self-source\nfeatures\nS′\nH = MLP H\n[\nX′\nH; EH\ncls\n]\nS′\nL = MLP L\n[\nX′\nL; EL\ncls\n]\n(6)\nwhere X′\nH and X′\nL are the features after interactive attention\nfor HSI and LiDAR branches. Consistent with the vanilla self-\nattention, the residual structure is also retained to achieve the in-\nformation aggregation of the original features and the processed\nfeatures.\nIn L-MSFI, each data source feature can conduct local-based\nfeature learning on another source feature by exchanging class\ntokens, and further aggregate global features with self-attention.\nAfter the class token returns to the original branch and the sub-\nsequent feature extraction has completed, each branch feature\nrealizes the information interaction based on the class token.\nD. Multisource Feature Selection Module (MSFSM)\nConventional linear fusion methods fuse HSI and LiDAR\nfeatures indiscriminately, and lack the modulation of features,\nwhich introduces unnecessary redundant features. Beside, the\nrelative importance of HSI and LiDAR data for collaborative\nclassiﬁcation is unequal. Generally speaking, HSI data have a\nlarger information load because it occupies the main feature in\nthe fusion feature, so adopting a simple linear fusion method\nlimits the expression ability of the HSI feature. In this regard,\nwe learned about the SCN[48] module, introduced it into the\nfusion of HSI and LiDAR sequence features, and proposed\nMSFSM, as is shown in Fig.2. Speciﬁcally, for each branch\nfeature FH∈ RN×C and FL∈ RN×C (where N represents the\nnumber of sequences andC represents the number of channels)\nafter the joint feature extraction by L-MSFM, an fully connected\n(FC) layer is ﬁrst used to combine the features and map them to\nF∈ RN×2; the results are as follows:\nF = FC(FH + FL). (7)\nAfter F has been obtained, the probability map is obtained\nusing the softmax function, whose probability indicates which\nsource component the feature most likely resembles. To com-\nplete feature ﬁltering, one-hot data features are acquired through\n1134 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 3. Comparison of three fusion modes of multisource features. (a) Linear\naddition fusion. (b) Linear concatenation fusion. (c) MSFSM.\nthe hard sampling process of token-wise. Then, the one-hot\nfeatures are mapped into concatenated multisource feature to\nﬁnish the fusion work\nωi = argmax(f1,f2)\nFf =\n2∑\ni=1\nωi[Concat(FH,FL)]i (8)\nwhere f1, f2 are each channel ofF, ωi is the one-hot map, and\nFf is the fused feature.\nThrough MSFSM, multisource features can be effectively\ncombined into a mixed feature, and any token of sequence\nis described as one of the source data that is more important\nfor classiﬁcation performance. Fig.3 shows the comparison of\ndifferent fusion methods. It can be observed that MSFSM is not\nonly a process of fusing multisource features but also a feature\nscreening process; it is worth noting that MSFSM is a dynamic\nfeature screening process. When training the model, the FC layer\nis continuously optimized by adopting the reparameterization\nmethod gumbel softmax, which allows the discrete sampling\nprocess to propagate gradients.\nAfter the fusion feature is obtained, a convolution transformer\nis adopted to further optimize the fusion feature. Finally, the class\ntoken of the feature is separated and is sent to the classiﬁcation\nmodule to obtain the classiﬁcation result of the fused feature.\nIII. EXPERIMENTAL ANDANALYSIS\nIn this section, three well-known datasets (i.e., Houston,\nMUUFL, and Trento) and three evaluation metrics [i.e., average\naccuracy (AA), overall accuracy (OA), and kappa coefﬁcient\n(Kappa)] are applied to analyze the parameters of LIIT and\ncompare the performance of various state-of-the-art CNNs and\nAlgorithm 1: LIIT.\nInput: The raw HSI dataXH∈ RH×W×C, LiDAR data\nXL∈ RH×W , and ground truthY∈ RH×W .\nOutput: Classiﬁcation result and relative visualization\nmap for three datasets.\n1: Reduce the dimension of HSIXH and along with\nLiDAR dataXL to expand each pixel into spatial\npatch.\n2: Obtain the training set, validation set, and testing set,\nthen build their respective dataloaders\n3: Set train batchsize b = 64, optimizer Adam with the\nlearning ratelr = 0.001, and train epochese = 150.\n4: for e = 1t o1 5 0do\n5: Embedding multisource data by convolutional\nfeature embedding module.\n6: Perform the L-MSFI.\n7: Perform the MSFSM module.\n8: Extract the class token in the fused feature.\n9: Classify the fused class token using the MLP Head\nand SoftMax.\n10: end for\n11: Save the trained model to classify the testing set, and\nplot the visualization map.\nFig. 4. Houston dataset. (a) Pseudocolor image for HSI. (b) LiDAR DSM\nmap.\ntransformers. The LIIT method is implemented based on Python\n3.7. and its deep learning framework is built by PyTorch, which\nis proposed by Facebook. It not only provides a convenient deep\nlearning system, but also supports the code running process for\nGPU acceleration. All our experiments are performed on a PC\nwith Windows 10 OS, Intel Core i7-7800X CPU, 32-GB RAM,\nand an NVIDIA GeForce RTX 1080 Ti GPU. The implementa-\ntion process of CASST is presented in Algorithm1.\nA. Datasets\n1) Houston Dataset: The Houston 2013 dataset was taken\nover the University of Houston and its neighboring cities, which\nwas initially used in the 2013 IEEE GRSS data fusion contest.\nThis dataset contains HSI and LiDAR DSM data with a spatial\nsize of 349× 1905 and a spatial resolution of 2.5 m. The HSI\nbands range from 0.38–1.05μm and contain 144 available bands.\nIt has 15 categories and 15 029 sample pixels. Fig.4 shows the\nHSI pseudocolor map and LiDAR DSM map of the dataset, and\nZHANG et al.: LOCAL INFORMATION INTERACTION TRANSFORMER FOR HYPERSPECTRAL AND LIDAR DATA CLASSIFICATION 1135\nTABLE I\nNUMBER OF TRAINING,V ALIDATION, AND TESTING SAMPLES FOR THE\nHOUSTON DATASET\nFig. 5. MUUFL dataset. (a) Pseudocolor image for HSI. (b) LiDAR DSM\nmap.\nTable I shows the sample allocation when the dataset is used\nfor the experiment, in which all samples are selected randomly\namong the categories.\n2) MUUFL Dataset: The MUUFL dataset was collected at\nthe International University of Southern Mississippi campus and\ncontains LiDAR DSM data acquired by the Gemini LiDAR and\nHSI data captured by the CASI-1500. The spatial size of the HSI\nand LiDAR data is 325×220, the spatial resolution of HSI data\nis 0.54×1.0 m, and it contains a total of 64 available bands from\n375−1050 nm. There are 11 classes and 53 687 sample pixels.\nFig. 5 shows the HSI pseudocolor map and LiDAR DSM map\nof the dataset, and TableII shows the sample allocation when\nthe dataset is used for experimental comparison.\n3) Trento Dataset:The Trento dataset was acquired in a rural\narea in southern Trento, Italy. The spatial size of the HSI and\nLiDAR DSM data is 166× 600 and the spatial resolution is\n1 m. The number of available spectral bands for HSI data is 63.\nIt contains six object categories with a total of 30 214 sample\npixels. Fig.6 shows the HSI pseudocolor map and LiDAR DSM\nTABLE II\nNUMBER OF TRAINING,V ALIDATION, AND TESTING SAMPLES FOR THE\nMUUFL DATASET\nFig. 6. Trento dataset. (a) Pseudocolor image for HSI. (b) LiDAR DSM map.\nTABLE III\nNUMBER OF TRAINING,V ALIDATION, AND TESTING SAMPLES FOR THETRENTO\nDATASET\nmap of the dataset, and TableIII shows the number of samples\nin the training set, validation set, and testing set when the dataset\nis used for experimental comparison. The sampling method is a\nrandom sampling of each category.\nB. Experimental Setup\nIn this part, we conduct a series of parameter experiments\nto analyze and conﬁrm which parameters are most beneﬁcial\nto the performance expression of LIIT, including the number\nof heads of the self-attention and the patch size of the network\ninput. In addition, some parameters are set by default based\non experience. 1) The training epochs are 150, and the training\nbatch size and test batch size are set to 64 and 1000, respectively.\n1136 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 7. Inﬂuence of spatial patch size as network input on classiﬁcation performance. (a) Houston dataset. (b) MUUFL dataset. (c) Trento dataset.\nTABLE IV\nOVERALL ACCURACY (%) WITH DIFFERENT NUMBER OF HEADS FOR\nPROPOSED LIIT ON THREE DATASETS\n2) The gradient optimization algorithm uses adaptive moment\nestimation (Adam), and the learning rate is 0.001. 3) The number\nof feature channels during the transformer is 64.\nThe number of heads determines the effectiveness of the\nself-attention process to a certain extent. By assigning features\nto different subspaces for attention calculation and then aggre-\ngating them, this process makes the data processing more reﬁned\nand the processed features more discriminative. TableIV shows\nthe comparison of the classiﬁcation performance of the LIIT\nmethod on three common datasets with different numbers of\nheads. LIIT is most suitable when the number of heads is 2 on\nthe Houston and MUUFL datasets, while the number of heads is\n8 on the Trento dataset. In addition, it can still be observed that\nwith the increasing of the number of heads, the classiﬁcation\nperformance decreases instead of increasing. This is mainly due\nto the continuous growth of the number of heads, resulting in\nmutual redundancy of subspace information, which inhibits the\nperformance expression.\nIn the fusion classiﬁcation task of HSI and LiDAR data,\nthe sufﬁciency of spatial information interaction is the key to\nimproving the classiﬁcation accuracy of the model. Selecting\na more appropriate input patch size can further stimulate the\npotential performance of the model. Fig.7 shows the comparison\nresults of the classiﬁcation performance of LIIT when the spatial\ninput patch size is 5 to 15. The results show that the optimal\nsize is 11 × 11 under the Houston and Trento datasets, and\n9 × 9 under the MUUFL dataset. As the patch size grows, the\nintroduction of different categories of pixels will further interfere\nwith the expression of the original features of the patch and\nexacerbate this phenomenon through the self-attention process.\nIt is worth noting that the spatial patch used in the MUUFL\nTABLE V\nOVERALL ACCURACY (%) WITH DIFFERENT NUMBER OF PCS AFTER PCA FOR\nPROPOSED LIIT ON THREE DATASETS\ndataset is smaller than that in the Houston and Trento datasets\ndue to its complex land cover distribution.\nPCA process initially affects the quality of the HSI feature.\nAppropriate number of PCs can retain important feature infor-\nmation as well as remove redundant information. TableV shows\nhow the number of PCs affects the model accuracy of LIIT\nmethods. The results show that when PCs are ten, the accuracy is\nbest on MUUFL and Trento datasets and 30 on Houston datasets.\nC. Experimental Comparison With Competitive Methods\nTo verify the effectiveness and excellent classiﬁcation perfor-\nmance of the proposed LIIT method, we compare the algorithm\nperformance with some traditional methods and the state-of-\nthe-art method on the Houston, MUUFL, and Trento datasets.\nThe traditional methods include SVM and EMAP based on\nthe morphological algorithm. The state-of-the-art methods in-\nclude representative CNN-based methods, such as 3-DCNN,\nTBCNN, and CPCNN, as well as transformer-based ViT and\nSpectralFormer. The selection of all samples in this process\nis shown in Section III-A. Each method has been tested ten\ntimes, and its mean value is represented as its ﬁnal classiﬁcation\nresult.\n1) SVM [49] algorithm obtains the surface feature classiﬁ-\ncation labels by analyzing the HSI spectral features. The\nimplementation of the algorithm is based on LIBSVM\ntoolbox of MATLAB. With Gaussian RBF kernel func-\ntion, the model is trained by ﬁvefold cross-validation.\n2) The EMAP [50] method fully captures the morphologi-\ncal features of multimodal features by obtaining the ex-\npanded multiattribute proﬁles of HSI and LiDAR. In the\nimplementation process, HSI data is reserved to three\nZHANG et al.: LOCAL INFORMATION INTERACTION TRANSFORMER FOR HYPERSPECTRAL AND LIDAR DATA CLASSIFICATION 1137\nTABLE VI\nCLASSIFICATION PERFORMANCE OF THESVM, EMAP, 3-DCNN, CPCNN, TBCNN, VIT, SPECTRALFORMER, AND LIIT CLASSIFICATION METHOD ON THE\nHOUSTON DATASET INTERMS OF OA, AA,AND KAPPA\nchannels through PCA method, and morphological algo-\nrithm is used to expand HSI and LiDAR data to 60-band\nproﬁles and 15-band proﬁles, respectively.\n3) Three groups of 3×3 ×3 3-D convolutional layers, batch\nnormalization layers, ReLUs, and max pooling layers are\nused in the 3-DCNN method[51]. In addition, the spatial\npatch size of the input feature is set to 11×11 on the three\ndatasets.\n4) Coupled CNN optimizes the drawbacks of the traditional\ntwo-branch CNN model used for multimodal data clas-\nsiﬁcation tasks. By sharing the network weight of dual\nbranches, it can guide the mutual communication between\nfeatures, help feature fusion, and reduce training time. In\nthe experiment, the input feature spatial patch of HSI and\nLiDAR is set to 11× 11.\n5) TBCNN adopts the model design of tow-branch CNN\nto extract the spatial-spectral features of HSI and the\nelevation features of LiDAR data, respectively. During\nthe implementation, the parameters shall be consistent\nwith the code provided in the original paper. The training\nepochs are set to 100, and PCs are 30.\n6) ViT method is the ﬁrst time to introduce transformer\nmodel into the ﬁeld of computer vision. In the multimodal\nclassiﬁcation task, the spatial input is set to 9×9, and the\npatch embedding process adopts a nonoverlapping 3× 3\nconvolutional process.\n7) SpectralFormer [52] improves the input mode of ViT. By\nusing band grouping input, the model can extract local-\nbased HSI spectral features. The experimental parameters\nand epochs required for training are consistent with the\ncode provided in the article.\n8) The parameter design of the LIIT method proposed in\nthis article can be seen in part B of SectionIII, and its\nparameters are set to the values with the best classiﬁcation\nperformance on various datasets. 150 epochs are required\nfor model training.\nNote that all the traditional methods are implemented on\nMATLAB, and all the methods based on deep learning are coded\nusing Python 3.7.\nTables VI–VIII summarize the classiﬁcation performance\n[i.e., OA (%), AA (%), and Kappa] for various classiﬁcation\nmethods on the Houston, MUUFL, Trento datasets. As shown in\nthe Tables, LIIT outperforms other method on the three datasets.\n1) Houston Dataset: Table VI shows the comparison of the\nclassiﬁcation accuracy of each experimental method on the\nHouston dataset. As we can see, the SVM algorithm that fails\nto describe the spatial surface structure can only obtain 94.94%\nOA. The TBCNN model based on spatial blocks has excellent\nlocal space coding ability. On the premise of HSI spectral\ndimension feature extraction, it effectively captures and fuses\nheterogeneous features between HSI and LiDAR data, with\n95.44% OA. CPCNN promotes the information consistency\namong multimodal features through the introduction of weight\ncontribution mechanism, and further improves the classiﬁcation\nperformance. The implantation of the attention module makes\nthe communication between features closer and more recog-\nnizable. As a typical attention-based model, ViT obtains the\ncontextual semantic information of spatial blocks, establishes\nglobal dependencies, and adaptively ﬁlters important features. It\nhas excellent classiﬁcation performance on the Houston dataset.\nHowever, due to the lack of exploring the local spatial features,\nthe ViT model obviously has a performance bottleneck. The LIIT\nmethod proposed in this article solves the above problems. With\nthe use of MSFSM, the amount of data information is increased\nand the feature recognition is enhanced. Compared with ViT,\nthe classiﬁcation accuracy of LIIT in this dataset is improved by\n1.1%. As shown in Fig.8, the full pixel classiﬁcation result\nmap of each method on the Houston dataset, and the map\npresented by LIIT, is more accurate for the description of ground\nobjects.\n2) MUUFL Dataset:TableVII and Fig.9, respectively, show\nthe classiﬁcation accuracy comparison and classiﬁcation results\n1138 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE VII\nCLASSIFICATION PERFORMANCE OF THESVM, EMAP, 3-DCNN, CPCNN, TBCNN, VIT, SPECTRALFORMER, AND LIIT CLASSIFICATION METHOD ON THE\nMUUFL DATASET INTERMS OF OA, AA,AND KAPPA\nTABLE VIII\nCLASSIFICATION PERFORMANCE OF THESVM, EMAP, 3-DCNN, CPCNN, TBCNN, VIT, SPECTRALFORMER, AND LIIT CLASSIFICATION METHOD ON THE\nTRENTO DATASET INTERMS OF OA, AA,AND KAPPA\nFig. 8. Classiﬁcation result maps for different comparison methods on the Houston dataset. (a) Ground truth, (b) SVM (OA= 94.94%), (c) EMAP (OA=\n98.88%), (d) 3-DCNN (OA= 98.75%), (e) CPCNN (OA= 95.63%), (f) TBCNN (OA= 95.44%), (g) ViT (OA= 98.75%), (h) SpectralFormer (OA= 97.59%),\n(i) LIIT (OA= 99.84%), and (j) color map.\nZHANG et al.: LOCAL INFORMATION INTERACTION TRANSFORMER FOR HYPERSPECTRAL AND LIDAR DATA CLASSIFICATION 1139\nFig. 9. Classiﬁcation result maps for different comparison methods on the MUUFL dataset. (a) Ground truth, (b) SVM (OA= 82.25%), (c) EMAP (OA=\n88.31%), (d) 3-DCNN (OA= 79.28%), (e) CPCNN (OA= 84.85%), (f) TBCNN (OA= 84.32%), (g) ViT (OA= 80.50%), (h) SpectralFormer (OA= 86.27%),\n(i) LIIT (OA= 88.46%), and (j) color map.\nFig. 10. Classiﬁcation result maps for different comparison methods on the Trento dataset. (a) Ground truth, (b) SVM (OA= 91.71%), (c) EMAP (OA=\n98.68%), (d) 3DCNN (OA= 97.40%), (e) CPCNN (OA= 99.30%), (f) TBCNN (OA= 96.46%), (g) ViT (OA= 98.84%), (h) SpectralFormer (OA= 96.91%),\n(i) LIIT (OA= 99.59%), and (j) color map.\nof each classiﬁcation method on the MUUFL dataset. It can be\nobserved that because the CNN-based method focuses on the\ndescription of local space, its description of ground objects is\nsmoother and mostly blocky. Others are rougher, showing more\nmap noise. The LIIT method has better classiﬁcation perfor-\nmance, and the generated map has fewer noise pixels and fewer\nmisclassiﬁcation phenomena. Compared with the ViT model, the\nintroduction of convolution layer enables LIIT to have stronger\nspatial feature-encoding capability and more reasonable image\ndescription; Compared with TBCNN and CPCNN, LIIT is more\neffective in fusing HSI and LiDAR data, and more detailed in\nexpressing pixel boundaries.\n3) Trento Dataset:TableVIII and Fig.10, respectively, show\nthe classiﬁcation comparison of each classiﬁcation algorithm on\nthe Trento dataset. The Trento dataset is relatively easy to be clas-\nsiﬁed due to its orderly distribution of ground objects and blocky\nspace, but it still has certain challenges in some categories. For\nexample, in the buildings and roads categories, the similarity\nbetween the two on the spectral curve has caused difﬁculties\nin the classiﬁcation of single HSI data. Therefore, how to effec-\ntively combine HSI and LiDAR data is the key to overcoming this\nproblem. CPCNN introduced weight contribution into feature\nextraction to promote the consistency between multimodal data,\nand the classiﬁcation accuracy on both categories exceeded 97%\n1140 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 11. Classiﬁcation performance of various methods with different number of training samples on three datasets. (a) Houston dataset. (b) MUUFL dataset.\n(c) Trento dataset.\nTABLE IX\nCLASSIFICATION ACCURACY OF THEPROPOSED LIIT METHOD USING DIFFERENT ABLATION STRATEGIES ON THETHREE MULTISOURCE DATASETS\nOA. The LIIT proposed in this article promotes the commu-\nnication between multimodal data through the design of an\ninteractive structure. The use of MSFSM adaptively estimates\nthe importance of features and further overcomes the redundancy\nproblem of fusion features. The classiﬁcation accuracy of this\nmethod in buildings and roads categories exceeds 98.5% OA,\nwhich is superior to each state-of-the-art method, which proves\nthe rationality and effectiveness of LIIT in feature extraction and\ndata fusion.\nIn order to analyze the performance robustness of LIIT when\nthe number of samples changes, we further carried out the\nclassiﬁcation results comparison experiment between LIIT and\nvarious methods when the number of samples changes from less\nto more. Among them, the selection of training samples on the\nHouston and MUUFL datasets is 1%−20% of the total samples\nand there are 10−50 samples on the Trento dataset. The experi-\nmental results are shown in Fig.11. Not only in large sample size,\nbut also in small sample size, the classiﬁcation performance of\nthe LIIT method is still better than those of the advanced CNNs\nand transformers. This experiment shows that the LIIT method\nhas strong robustness and excellent classiﬁcation performance.\nIn addition, the most outstanding classiﬁcation results are ob-\ntained on all datasets, which also proves that the method is strong\nin universality and can be applied to various multimodal remote\nsensing data classiﬁcation scenarios.\nD. Ablation Experiments for the Proposed Method\nIn order to observe the impact of each module on the model\nperformance more intuitively, we reselected and allocated the\nsamples on the three datasets in block allocation mode, making\nthem more challenging for classiﬁcation. Figs.12–14 show the\nFig. 12. Housotn dataset. (a) Training labels for ablation experiment.\n(b) Testing labels for ablation experiment.\nFig. 13. MUUFL dataset. (a) Training labels for ablation experiment.\n(b) Testing labels for ablation experiment.\ndistribution of training samples and test samples for Houston,\nMUUFL, and Trento datasets, respectively. For TableIX,t h e\nresults of ablation experiments show that in the joint classiﬁ-\ncation task of HSI and LiDAR, the use of a single HSI data\nZHANG et al.: LOCAL INFORMATION INTERACTION TRANSFORMER FOR HYPERSPECTRAL AND LIDAR DATA CLASSIFICATION 1141\nTABLE X\nCOMPARISON OFRUNNING TIME OF EACH METHOD ON DIFFERENT DATASETS\nFig. 14. Trento dataset. (a) Training labels for ablation experiment. (b) Testing\nlabels for ablation experiment.\nhas superior classiﬁcation performance, indicating that HSI data\nplays a leading role in the classiﬁcation process, and the LiDAR\ndata is used as a supplement to the information level. With\nthe introduction of LiDAR data, the classiﬁcation accuracy of\neach dataset has signiﬁcantly increased, which shows that the\nreasonable use of LiDAR data can help it to achieve a more\ndetailed description of ground objects. L-MSFI is designed to\nextract the local-global semantic features of multisource data,\nand its interactive information transmission structure helps to\nachieve communication between features. The results show that\nboth the Houston and MUUFL datasets help improve OA by\nabout 1.1%. The introduction of MSFSM replaces the tradi-\ntional linear addition and concatenation fusion actions. With\nthe dynamic model training process, the high-weight features\nin the multimode are adaptively selected. This process avoids\nthe generation of fusion redundancy while ensuring sufﬁcient\ninformation. As shown in the table, the impact of MSFSM on\nperformance is also critical. The results of ablation experiments\nshow that all modules and modal data in the LIIT play an\nindispensable role in breaking through the model performance\nbottleneck. With the introduction of each component, the model\nclassiﬁcation accuracy continues to rise, which is sufﬁcient to\nprove the rationality of each component and the effectiveness of\nperformance improvement.\nE. Analysis of Running Time\nTime complexity is another important index to describe the\nmodel. Table X shows the comparison of training and testing\ntime required for each method to complete a classiﬁcation\nprocess. It can be seen that CNN-based methods generally have\nhigh time complexity. The transformer-based models also has\nmany network parameters due to its embedded self-attention\nmodule. The LIIT method is not superior to other methods in\nterms of time due to the use of transformer and convolutional\nlayer. It is worth noting that the number of samples has a\nsigniﬁcant impact on the running time. With the increase of\nthe number of samples, the running time of the model increases\nnonlinearly, especially for the complex model LIIT, which is\nreﬂected in TableX that the training time of the LIIT method\non the Houston dataset is signiﬁcantly longer than the other\ntwo datasets. In addition, the dual branch model has more\nstructural parameters than the single branch model, but it has\na more adequate feature extraction process, and the beneﬁt of\nits performance improvement is considerable.\nIV . CONCLUSION\nIn this article, an LIIT model is proposed to solve the prob-\nlems of incomplete HSI and LiDAR data collaborative feature\ncapture and insufﬁcient multisource feature fusion. Speciﬁcally,\na local-based multisource feature interactor (L-MSFI) is de-\nsigned. Its local-based feature modeling process alleviates the\nfeature homogeneity of self-attention. Meanwhile, a HSI and\nLiDAR data interactive feature coding environment has been\ncreated, which promotes mutual learning between multisource\nfeatures. In addition, an MSFSM is developed to dynamically\nﬁlter multimodal features, overcoming the balance problem of\nHSI and LiDAR features in fusion. The comparative analysis\nexperiment was carried out on three multisource remote sensing\nclassiﬁcation datasets (Houston, MUUFL, and Trento). Com-\npared with the state-of-the-art CNNs and transformers, LIIT\nhas more performance advantages. In the future, lightweight\nmultisource remote sensing classiﬁcation model is our goal to\nbetter balance performance and network complexity.\nREFERENCES\n[1] M. E. Paoletti, J. M. Haut, R. Fernandez-Beltran, J. Plaza, A. J. Plaza, and\nF. Pla, “Deep pyramidal residual networks for spectral-spatial hyperspec-\ntral image classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 57, no. 2,\npp. 740–754, Feb. 2019.\n[2] C. Yu, R. Han, M. Song, C. Liu, and C.-I. Chang, “A simpliﬁed 2D-3D\nCNN architecture for hyperspectral image classiﬁcation based on spatial-\nspectral fusion,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 13, pp. 2485–2501, Apr. 2020.\n[3] N. Audebert, B. Le Saux, and S. Lefevre, “Deep learning for classiﬁcation\nof hyperspectral data: A comparative review,”IEEE Geosci. Remote Sens.\nMag., vol. 7, no. 2, pp. 159–173, Jun. 2019.\n[4] L. Mou, X. Lu, X. Li, and X. X. Zhu, “Nonlocal graph convolutional\nnetworks for hyperspectral image classiﬁcation,” IEEE Trans. Geosci.\nRemote Sens., vol. 58, no. 12, pp. 8246–8257, Dec. 2020.\n1142 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\n[5] X. Zhang, Y . Sun, K. Shang, L. Zhang, and S. Wang, “Crop classiﬁcation\nbased on feature band set construction and object-oriented approach using\nhyperspectral images,”IEEE J. Sel. Topics Appl. Earth Observ. Remote\nSens., vol. 9, no. 9, pp. 4117–4128, Sep. 2016.\n[6] T. Chen, X. Zheng, R. Niu, and A. Plaza, “Open-pit mine area mapping\nwith Gaofen-2 satellite images using U-Net+,”IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens., vol. 15, pp. 3589–3599, Apr. 2022.\n[7] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “Cospace: Common\nsubspace learning from hyperspectral-multispectral correspondences,”\nIEEE Trans. Geosci. Remote Sens. , vol. 57, no. 7, pp. 4349–4359,\nJul. 2019.\n[8] Z. Lin, Y . Chen, P. Ghamisi, and J. A. Benediktsson, “Generative adversar-\nial networks for hyperspectral image classiﬁcation,”IEEE Trans. Geosci.\nRemote Sens., vol. 56, no. 9, pp. 5046–5063, Sep. 2018.\n[9] H. Aytaylan and S. E. Yuksel, “Fully-connected semantic segmentation\nof hyperspectral and LiDAR data,” IET Comput. Vis., vol. 13, no. 3,\npp. 285–293, 2019.\n[10] Z. Wen, B. Hu, L. Jing, M. E. Woods, and P. Courville, “Automatic forest\nspecies classiﬁcation using combined LiDAR data and optical imagery,”\nin Proc. IEEE Int. Geosci. Remote Sens. Symp., 2008, pp. III-134–III-137.\n[11] S. Morsy, A. Shaker, and A. El-Rabbany, “Multivariate Gaussian decom-\nposition for multispectral airborne LiDAR data classiﬁcation,” inProc.\nIEEE Int. Geosci. Remote Sens. Symp., 2018, pp. 8741–8744.\n[12] M. Dalponte, H. O. Orka, T. Gobakken, D. Gianelle, and E. Naesset,\n“Tree species classiﬁcation in boreal forests with hyperspectral data,”IEEE\nTrans. Image Process., vol. 51, no. 5, pp. 2632–2645, May 2012.\n[13] T. Matsuki, N. Yokoya, and A. Iwasaki, “Hyperspectral tree species\nclassiﬁcation of japanese complex mixed forest with the aid of LiDAR\ndata,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 8, no. 5,\npp. 2177–2187, May 2015.\n[14] J. Vauhkonen, T. Hakala, J. Suomalainen, S. Kaasalainen, O. Nevalainen,\nand M. Vastaranta, “Classiﬁcation of spruce and pine trees using active\nhyperspectral LiDAR,”IEEE Geosci. Remote Sens. Lett., vol. 10, no. 5,\npp. 1138–1141, Sep. 2013.\n[15] P. Ghamisi, B. Höﬂe, and X. X. Zhu, “Hyperspectral and LiDAR data\nfusion using extinction proﬁles and deep convolutional neural network,”\nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 10, no. 6,\npp. 3011–3024, Jun. 2017.\n[16] Q. Cao, Y . Zhong, A. Ma, and L. Zhang, “Urban land use/land cover\nclassiﬁcation based on feature fusion fusing hyperspectral image and\nLiDAR data,” in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2018,\npp. 8869–8872.\n[17] Y . Zhang, H. L. Yang, S. Prasad, E. Pasolli, J. Jung, and M. Crawford,\n“Ensemble multiple kernel active learning for classiﬁcation of multisource\nremote sensing data,”IEEE J. Sel. Topics Appl. Earth Observ. Remote\nSens., vol. 8, no. 2, pp. 845–858, Feb. 2015.\n[18] M. Pedergnana, P. R. Marpu, M. Dalla Mura, J. A. Benediktsson, and\nL. Bruzzone, “Classiﬁcation of remote sensing optical and LiDAR data\nusing extended attribute proﬁles,”IEEE J. Sel. Topics Signal Process.,\nvol. 6, no. 7, pp. 856–865, Nov. 2012.\n[19] B. Rasti, P. Ghamisi, and R. Gloaguen, “Hyperspectral and LiDAR\nfusion using extinction proﬁles and total variation component analy-\nsis,” IEEE Trans. Geosci. Remote Sens., vol. 55, no. 7, pp. 3997–4007,\nJul. 2017.\n[20] S. Jia et al., “Multiple feature-based superpixel-level decision fusion for\nhyperspectral and LiDAR data classiﬁcation,”IEEE Trans. Geosci. Remote\nSens., vol. 59, no. 2, pp. 1437–1452, Feb. 2021.\n[21] C. Zhang, M. Smith, and C. Fang, “Evaluation of Goddard’s LiDAR,\nhyperspectral, and thermal data products for mapping urban land-cover\ntypes,” GIScience Remote Sens., vol. 55, no. 1, pp. 1–20, 2017.\n[22] Y . Chen, Z. Lin, Z. Xing, W. Gang, and Y . Gu, “Deep learning-based\nclassiﬁcation of hyperspectral data,” IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 7, no. 6, pp. 2094–2107, Jun. 2017.\n[23] P. Zhong, Z. Gong, S. Li, and C. B. Schonlieb, “Learning to diversify\ndeep belief networks for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 55, no. 6, pp. 3516–3530, Jun. 2017.\n[24] X. Zhang, Y . Sun, J. Kai, L. Chen, L. Jiao, and H. Zhou, “Spatial se-\nquential recurrent neural network for hyperspectral image classiﬁcation,”\nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 11, no. 11,\npp. 4141–4155, Nov. 2018.\n[25] S. Jia et al., “3-D gabor convolutional neural network for hyperspectral\nimage classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–16,\nJun. 2022, Art. no. 5509216.\n[26] X. Xu, W. Li, Q. Ran, Q. Du, L. Gao, and B. Zhang, “Multisource remote\nsensing data classiﬁcation based on convolutional neural network,”IEEE\nTrans. Geosci. Remote Sens., vol. 56, no. 2, pp. 937–949, Feb. 2018.\n[27] R. Hang, Z. Li, P. Ghamisi, D. Hong, G. Xia, and Q. Liu, “Classiﬁcation of\nhyperspectral and LiDAR data using coupled CNNs,”IEEE Trans. Geosci.\nRemote Sens., vol. 58, no. 7, pp. 4939–4950, Jul. 2020.\n[28] D. Hong, L. Gao, R. Hang, B. Zhang, and J. Chanussot, “Deep\nencoder-decoder networks for classiﬁcation of hyperspectral and LiDAR\ndata,” IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1–5, Aug. 2020,\nArt. no. 5500205.\n[29] M. Zhang, W. Li, Q. Du, L. Gao, and B. Zhang, “Feature extraction\nfor classiﬁcation of hyperspectral and LiDAR data using patch-to-patch\nCNN,” IEEE Trans. Cybern., vol. 50, no. 1, pp. 100–111, Jan. 2020.\n[30] B. Tu, W. He, W. He, X. Ou, and A. Plaza, “Hyperspectral classiﬁcation via\nglobal-local hierarchical weighting fusion network,”IEEE J. Sel. Topics\nAppl. Earth Observ. Remote Sens., vol. 15, pp. 184–200, Dec. 2021.\n[31] X. Zhao, R. Tao, W. Li, W. Philips, and W. Liao, “Fractional Gabor\nconvolutional network for multisource remote sensing data classiﬁca-\ntion,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–18, Mar. 2022,\nArt. no. 5503818.\n[32] X. Zhao et al., “Joint classiﬁcation of hyperspectral and LiDAR data\nusing hierarchical random walk and deep CNN architecture,”IEEE Trans.\nGeosci. Remote Sens., vol. 58, no. 10, pp. 7355–7370, Oct. 2020.\n[33] H.-C. Li, W.-S. Hu, W. Li, J. Li, Q. Du, and A. Plaza, “A3 CLNN:\nSpatial, spectral and multiscale attention ConvLSTM neural network for\nmultisource remote sensing data classiﬁcation,”IEEE Trans. Neural Netw.\nLearn. Syst., vol. 33, no. 2, pp. 747–761, Feb. 2022.\n[34] L. Mou and X. X. Zhu, “Learning to pay attention on spectral domain: A\nspectral attention module-based convolutional network for hyperspectral\nimage classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 58, no. 1,\npp. 110–122, Jan. 2020.\n[35] M. Zhu, L. Jiao, F. Liu, S. Yang, and J. Wang, “Residual spectral-spatial\nattention network for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 59, no. 1, pp. 449–462, Jan. 2021.\n[36] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5998–6008.\n[37] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” 2021,arXiv:2103.14030.\n[38] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Transformer\nfor semantic segmentation,” 2021,arXiv:2105.05633.\n[39] X. Chen, B. Yan, J. Zhu, D. Wang, and H. Lu, “Transformer tracking,”\n2021, arXiv:2103.15436.\n[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, and N. Houlsby,\n“An image is worth16 ×16 words: Transformers for image recognition\nat scale,” 2020,arXiv:2010.11929.\n[41] W. Dong, T. Zhang, J. Qu, S. Xiao, T. Zhang, and Y . Li, “Multibranch\nfeature fusion network with self- and cross-guided attention for hyper-\nspectral and LiDAR classiﬁcation,”IEEE Trans. Geosci. Remote Sens.,\nvol. 60, pp. 1–12, Jun. 2022, Art. no. 5530612.\n[42] Z. Xue, X. Tan, X. Yu, B. Liu, A. Yu, and P. Zhang, “Deep hierarchical\nvision transformer for hyperspectral and LiDAR data classiﬁcation,”IEEE\nTrans. Image Process., vol. 31, pp. 3095–3110, Apr. 2022.\n[43] X. He, Y . Chen, and Z. Lin, “Spatial-spectral transformer for hyperspectral\nimage classiﬁcation,”Remote Sens., vol. 13, 2021, Art. no. 498.\n[44] Y . Qing, W. Liu, L. Feng, and W. Gao, “Improved transformer net for\nhyperspectral image classiﬁcation,”Remote Sens., vol. 13, no. 11, 2021,\nArt. no. 2216.\n[45] L. Sun, G. Zhao, Y . Zheng, and Z. Wu, “Spectral-spatial feature Tok-\nenization transformer for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 60, pp. 1–14, Jan. 2022, Art. no. 5522214.\n[46] C. F. Chen, Q. Fan, and R. Panda, “Crossvit: Cross-attention multi-scale\nvision transformer for image classiﬁcation,” 2021,arXiv:2103.14899.\n[47] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, and J. Carreira, “Perceiver:\nGeneral perception with iterative attention,” 2021,arXiv:2103.03206.\n[48] H. Liu, X. Jiang, X. Li, Z. Bao, D. Jiang, and B. Ren, “Nommer: Nominate\nsynergistic context in vision transformer for visual recognition,” 2021,\narXiv:2111.12994.\n[49] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote sens-\ning images with support vector machines,”IEEE Trans. Geosci. Remote\nSens., vol. 42, no. 8, pp. 1778–1790, Aug. 2004.\n[50] K. Wang, H. Rui, and S. Qian, “Spectral-spatial hyperspectral image\nclassiﬁcation using extended multi attribute proﬁles and guided bilateral\nﬁlter,” inProc. Int. Conf. Comput. Sci. Mech. Autom., 2015, pp. 235–239.\n[51] M. Kanthi, T. H. Sarma, and C. S. Bindu, “A 3D-deep CNN based feature\nextraction and hyperspectral image classiﬁcation,” inProc. IEEE India\nGeosci. Remote Sens. Symp., 2020, pp. 229–232.\n[52] D. Hong et al., “SpectralFormer: Rethinking hyperspectral image classi-\nﬁcation with transformers,”IEEE Trans. Geosci. Remote Sens., vol. 60,\npp. 1–15, Nov. 2022, Art. no. 5518615.\nZHANG et al.: LOCAL INFORMATION INTERACTION TRANSFORMER FOR HYPERSPECTRAL AND LIDAR DATA CLASSIFICATION 1143\nYuwen Zhang (Student Member, IEEE) received the\nB.S. degree in electrical engineering and automation\nfrom the Hunan Institute of Science and Technol-\nogy, Yueyang, China, in 2020, where he is currently\nworking toward the M.S. degree in information and\ncommunication engineering.\nHis research interests include image processing,\nclassiﬁcation of multisource remote sensing data, and\nobject detection.\nYishu Peng (Member, IEEE) received the B.S., M.S.,\nand Ph.D. degrees from Northeastern University,\nShenyang, China, in 2009, 2011, and 2017, respec-\ntively, all in mechanical design and theory.\nFrom 2017 to 2019, he was with the School of Me-\nchanical and Engineering, Hunan Institute of Science\nand Technology, Yueyang, China, and since 2019, he\nhas been with the School of Information Science and\nTechnology. His research interests include the image\nprocessing, object detection, and target tracing.\nBing Tu (Member, IEEE) received the M.S. degree\nin control science and engineering from the Guilin\nUniversity of Technology, Guilin, China, in 2009, and\nthe Ph.D. degree in mechatronic engineering from the\nBeijing University of Technology, Beijing, China, in\n2013.\nFrom 2015 to 2016, he was a Visiting Researcher\nwith the Department of Computer Science and En-\ngineering, University of Nevada, Reno, NV , USA,\nwhich is supported by the China Scholarship Council.\nSince 2018, he had been an Associate Professor with\nthe School of Information Science and Engineering, Hunan Institute of Science\nand Technology, Yueyang, China. He is currently a Full Professor with the School\nof Information Science and Engineering, Hunan Institute of Science and Tech-\nnology, Yueyang, China. His research interests include sparse representation,\npattern recognition, and analysis in remote sensing.\nDr. Tu is an Associate Editor of theIEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing.\nYaru Liu (Student Member, IEEE) received the\nB.S. degree in communication engineering from the\nLanzhou University of Technology, Lanzhou, China,\nin 2016. She is currently working toward the M.S. de-\ngree in information and communication engineering\nwith the Hunan Institute of Science and Technology,\nYueyang, China.\nHer research interests include hyperspectral image\nprocessing, computer vision, and deep learning.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.791147768497467
    },
    {
      "name": "Lidar",
      "score": 0.7575863599777222
    },
    {
      "name": "Hyperspectral imaging",
      "score": 0.6064426898956299
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5709367394447327
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.550116240978241
    },
    {
      "name": "Ranging",
      "score": 0.4780895411968231
    },
    {
      "name": "Feature extraction",
      "score": 0.43549680709838867
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40407174825668335
    },
    {
      "name": "Computer vision",
      "score": 0.33256036043167114
    },
    {
      "name": "Remote sensing",
      "score": 0.3259895443916321
    },
    {
      "name": "Engineering",
      "score": 0.09238353371620178
    },
    {
      "name": "Geography",
      "score": 0.08774250745773315
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100286613",
      "name": "Hunan Institute of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I5343935",
      "name": "Guilin University of Electronic Technology",
      "country": "CN"
    }
  ]
}