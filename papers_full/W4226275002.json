{
    "title": "xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography",
    "url": "https://openalex.org/W4226275002",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2343676251",
            "name": "Arnab Kumar Mondal",
            "affiliations": [
                "Indian Institute of Technology Delhi"
            ]
        },
        {
            "id": "https://openalex.org/A2126431989",
            "name": "Arnab Bhattacharjee",
            "affiliations": [
                "Indian Institute of Technology Delhi"
            ]
        },
        {
            "id": "https://openalex.org/A2046403946",
            "name": "Parag Singla",
            "affiliations": [
                "Indian Institute of Technology Delhi"
            ]
        },
        {
            "id": "https://openalex.org/A2654639127",
            "name": "A. P. Prathosh",
            "affiliations": [
                "Indian Institute of Science Bangalore"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3103674685",
        "https://openalex.org/W3137980281",
        "https://openalex.org/W3157660985",
        "https://openalex.org/W6746693533",
        "https://openalex.org/W3006110666",
        "https://openalex.org/W3006882119",
        "https://openalex.org/W3008985036",
        "https://openalex.org/W3144024447",
        "https://openalex.org/W3105081694",
        "https://openalex.org/W3033616466",
        "https://openalex.org/W3017644243",
        "https://openalex.org/W3114166611",
        "https://openalex.org/W3080568059",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W3007170347",
        "https://openalex.org/W3011149445",
        "https://openalex.org/W3017451406",
        "https://openalex.org/W3085306326",
        "https://openalex.org/W3139971937",
        "https://openalex.org/W3116116041",
        "https://openalex.org/W6726873649",
        "https://openalex.org/W3157140100",
        "https://openalex.org/W3104951425",
        "https://openalex.org/W6776475153",
        "https://openalex.org/W3126395150",
        "https://openalex.org/W3156717587",
        "https://openalex.org/W3080237299",
        "https://openalex.org/W3038003980",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W6761628794",
        "https://openalex.org/W6763239785",
        "https://openalex.org/W6769955919",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W6779248606",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W6769040946",
        "https://openalex.org/W3134772857",
        "https://openalex.org/W2887280559",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2963466845",
        "https://openalex.org/W6745535286",
        "https://openalex.org/W6777596526",
        "https://openalex.org/W6798444936",
        "https://openalex.org/W4232356144",
        "https://openalex.org/W6940613197",
        "https://openalex.org/W3139833881",
        "https://openalex.org/W3104739447",
        "https://openalex.org/W6966869944",
        "https://openalex.org/W6777714645",
        "https://openalex.org/W2798251715",
        "https://openalex.org/W3139487216",
        "https://openalex.org/W3013277995",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W3032155710",
        "https://openalex.org/W3176196997"
    ],
    "abstract": "<i>Objective:</i> Since its outbreak, the rapid spread of COrona VIrus Disease 2019 (COVID-19) across the globe has pushed the health care system in many countries to the verge of collapse. Therefore, it is imperative to correctly identify COVID-19 positive patients and isolate them as soon as possible to contain the spread of the disease and reduce the ongoing burden on the healthcare system. The primary COVID-19 screening test, RT-PCR although accurate and reliable, has a long turn-around time. In the recent past, several researchers have demonstrated the use of Deep Learning (DL) methods on chest radiography (such as X-ray and CT) for COVID-19 detection. However, existing CNN based DL methods fail to capture the global context due to their inherent image-specific inductive bias. <i>Methods:</i> Motivated by this, in this work, we propose the use of vision transformers (instead of convolutional networks) for COVID-19 screening using the X-ray and CT images. We employ a multi-stage transfer learning technique to address the issue of data scarcity. Furthermore, we show that the features learned by our transformer networks are explainable. <i>Results:</i> We demonstrate that our method not only quantitatively outperforms the recent benchmarks but also focuses on meaningful regions in the images for detection (as confirmed by Radiologists), aiding not only in accurate diagnosis of COVID-19 but also in localization of the infected area. The code for our implementation can be found here - https://github.com/arnabkmondal/xViTCOS. <i>Conclusion:</i> The proposed method will help in timely identification of COVID-19 and efficient utilization of limited resources.",
    "full_text": "MEDICAL IMAGING AND DIAGNOSTIC RADIOLOGY\nReceived 26 September 2021; revised 18 November 2021; accepted 6 December 2021.\nDate of publication 8 December 2021; date of current version 17 December 2021.\nDigital Object Identifier 10.1 109/JTEHM.2021.3134096\nxViTCOS: Explainable Vision Transformer Based\nCOVID-19 Screening Using Radiography\nARNAB KUMAR MONDAL\n 1, ARNAB BHATTACHARJEE2, PARAG SINGLA3,\nAND A. P. PRATHOSH\n4\n1Amar Nath and Shashi Khosla School of Information Technology, Indian Institute of Technology Delhi, New Delhi 110016, India\n2UQ-IITD Academy of Research, Indian Institute of Technology Delhi, New Delhi 110016, India\n3Department of Computer Science and Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India\n4Department of Electrical Communication Engineering, Indian Institute of Science (IISc), Bangalore 560 012, India\n(Arnab Kumar Mondal and Arnab Bhattacharjee contributed equally to this work.) CORRESPONDING AUTHOR: A. K. MONDAL\n(anz188380@iitd.ac.in)\nThis article has supplementary downloadable material available at https://doi.org/10.1109/JTEHM.2021.3134096,\nprovided by the authors.\nABSTRACT Objective: Since its outbreak, the rapid spread of COrona VIrus Disease 2019 (COVID-19)\nacross the globe has pushed the health care system in many countries to the verge of collapse. Therefore, it is\nimperative to correctly identify COVID-19 positive patients and isolate them as soon as possible to contain\nthe spread of the disease and reduce the ongoing burden on the healthcare system. The primary COVID-19\nscreening test, RT-PCR although accurate and reliable, has a long turn-around time. In the recent past, several\nresearchers have demonstrated the use of Deep Learning (DL) methods on chest radiography (such as X-ray\nand CT) for COVID-19 detection. However, existing CNN based DL methods fail to capture the global context\ndue to their inherent image-speciﬁc inductive bias. Methods: Motivated by this, in this work, we propose the\nuse of vision transformers (instead of convolutional networks) for COVID-19 screening using the X-ray\nand CT images. We employ a multi-stage transfer learning technique to address the issue of data scarcity.\nFurthermore, we show that the features learned by our transformer networks are explainable. Results: We\ndemonstrate that our method not only quantitatively outperforms the recent benchmarks but also focuses on\nmeaningful regions in the images for detection (as conﬁrmed by Radiologists), aiding not only in accurate\ndiagnosis of COVID-19 but also in localization of the infected area. The code for our implementation can\nbe found here - https://github.com/arnabkmondal/xViTCOS. Conclusion: The proposed method will help in\ntimely identiﬁcation of COVID-19 and efﬁcient utilization of limited resources.\nINDEX TERMS AI for COVID-19 detection, CT scan and CXR, deep learning, vision transformer.\nClinical and Translational Impact Statement:The proposed method can be used to complement RTPCR\ntest for accurate and rapid prognosis of COVID-19 from chest radiographs.\nI. INTRODUCTION\nA. BACKGROUND\nThe novel COronaVIrus Disease 2019 (COVID-19) is a\nviral respiratory disease caused by Severe Acute Respira-\ntory Syndrome COronaVirus 2 (SARS-CoV2). The World\nHealth Organization (WHO) has declared COVID-19 a\npandemic on 11 March 2020 [1]. This has pushed the\nhealth systems of several nations to the verge of collapse.\nIt is, therefore, of utmost importance to screen the positive\nCOVID-19 patients accurately for efﬁcient utilization of lim-\nited resources. Two types of viral tests are currently popularly\nused to detect COVID-19 infection: Nucleic Acid Ampli-\nﬁcation Tests (NAATs) [2] and Antigen Tests [3]. NAATs\ncan reliably detect SARS-CoV-2 and are unlikely to return a\nfalse-negative result of SARS-CoV-2. NAATs can use many\ndifferent methods, among which Reverse Transcription Poly-\nmerase Chain Reaction (RT-PCR) is the most preferred test\nfor COVID-19 due to its high speciﬁcity and sensitivity [4].\nHowever, this test is expensive as it has an elaborate kit and\ntime-consuming. An RT-PCR test uses nose or throat swabs\nto detect SARS-CoV-2 and requires trained professionals\ninstructed for the RT-PCR kit to carry out the RT-PCR test.\nRT-PCR requires a complete set-up that includes the trained\npractitioners, laboratory, and RT-PCR machine for detection\nand analysis.\nB. SCOPE AND CONTRIBUTIONS\nMotivated by the success of the Deep Learning in diagnosing\nrespiratory disorders [5], several recent works have proposed\nthe use of chest radiography images (X-ray and Computed\n1100110\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME 10, 2022\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nTomography, CT) as alternate modality to detect COVID-19\npositive cases [6]–[12] (Elaborated in Sec. II). Unlike in the\nchest CT/X-ray of a healthy person, the lungs of COVID-19\naffected patients show some visual marks like ground-glass\nopacity and/or mixed ground-glass opacity, and mixed\nconsolidation [6].\nWhile there has been a large body of literature on use\nof Deep Learning for Covid detection, most of them are\nbased on Convolutional Neural Networks (CNNs) [12]–[15].\nCNN, albeit powerful, lacks a global understanding of images\nbecause of its image-speciﬁc inductive biases. To cap-\nture long-range dependencies, CNNs require a large recep-\ntive ﬁeld, which necessitates designing large kernels or\nimmensely deep networks, leading to a complex model chal-\nlenging to train. Recently, Vision transformers [16] have\nprovided an alternative framework for learning tasks and\novercome the issues associated with convolutional inductive\nbias as they can learn the most suitable inductive bias depend-\ning on the task at hand. Motivated by this, in this work,\nwe propose to employ a vision transformer (ViT) based trans-\nfer learning method to detect COVID-19 infection from the\nchest radiography (X-ray and CT scan imaging). Speciﬁcally,\nthe below are our contributions:\n1) We propose a vision transformer based deep neural\nclassiﬁer, xViTCOS for screening of COVID-19 from\nchest radiography.\n2) We provide explanability-driven, clinically inter-\npretable visualizations where the patches responsible\nfor the model’s prediction are highlighted on the input\nimage.\n3) We employ a multi-stage transfer learning approach to\naddress the problem of need for large-scale data.\n4) We demonstrate the efﬁcacy of the proposed frame-\nwork in distinguishing COVID-19 positive cases from\nnon-COVID-19 Pneumonia and Normal control using\nboth chest CT scan and X-ray modality, through several\nexperiments on benchmark datasets.\nII. RELATED WORK\nA. COVID-19 DETECTION USING CHEST CT\nChest Computed Tomography (CT) imaging has been pro-\nposed as an alternative screening tool for COVID-19 infec-\ntion [6], [7]. In [17] multiple features, such as V olume,\nRadiomics features, Infected lesion number, Histogram dis-\ntribution and Surface area are extracted ﬁrst from the CT\nimages following which a deep forest algorithm, consisting\nof cascaded layers of multiple random forests, is used for\ndiscriminative feature selection and classiﬁcation.\nThe work in [13] performs a comparative study by\nexploiting transfer-learning to optimize 10 pre-trained CNN\nmodels viz AlexNet [18], VGG-16 [19], VGG-19 [19],\nSqueezeNet [20], GoogleNet [21], MobileNet-V2 [22],\nResNet-18 [23], ResNet-50 [23], ResNet-101 [23], and\nXception [24] on CT-scan images to differentiate between\nCOVID-19 and non-COVID-19 cases. As per the results\nreported in [13], ResNet-101 and Xception achieve best\nperformance. [25] segment out candidate infection regions\nfrom the pulmonary CT image set using a 3D CNN seg-\nmentation model and categorize these segments into the\nCOVID-19, IA VP, and irrelevant to infection (ITI) groups,\ntogether with the corresponding conﬁdence scores, using a\nlocation-attention classiﬁcation model. COVNet [26] is a\nResNet50 based CNN architecture that takes as input a series\nof CT slices and compute features from each slice of the CT\nseries, which are combined by a max-pooling operation, and\nthe resulting feature map is fed to a fully connected layer to\ngenerate a probability score for each class. Ref. [27] uses a\npre-trained EfﬁcientNet as the backbone and extracts features\nfrom each slice of CT data, and makes a binary prediction.\nNext, the slice level predictions are combined using a multi-\nlayer perceptron (MLP) to make a ﬁnal prediction at the\npatient level. COVIDNet-CT [15] on the other hand offers\narchitectural diversity, selective long-range connectivity, and\nlightweight design patterns. Ref. [28] proposes Contrastive\nCOVIDNet which is built upon the COVIDNet [11] archi-\ntecture by introducing domain speciﬁc batch normalization\nlayers along with a cross entropy classiﬁcation and a con-\ntrastive loss. In [29] a custom CNN model is built with two\nseparate lines of forward pass and deep feature aggregation\nto classify COVID and non-COVID. The network is trained\nto work both on CT and X-ray data. It employs a deep feature\naggregation strategy by aggregating layer outputs from vary-\ning depths following a classiﬁer network. ResGNet-C [30]\nexploits Graph Convolution Network (GCN) [31] to perform\nbinary classiﬁcation task using the Resnet-101 [23] extracted\nfeatures. Ref. [32] proposes an hybrid model based on deep\nfeatures and Parameter Free BAT (PF-BAT) optimized Fuzzy\nK-nearest neighbor (PF-FKNN) classiﬁer for COVID-19\nprognosis.\nB. COVID-19 DETECTION USING CHEST X-RAY\nAlthough chest-CT has more sensitivity as compared to\nRT-PCR [8], [9], associated cost and resource constraints\nmakes routine CT screening for COVID-19 detection a less\naccessible solution to the third world’s teeming millions.\nTherefore, digital X-ray based Covid detection is considered\nan easily accessible alternative.\nIn [34] the authors propose a two-stage pipeline for binary\nclassiﬁcation. In the ﬁrst stage, the signiﬁcant lung region\nis cropped from the chest X-ray images using a bounding\nbox segmentation. In the second stage, a GAN inspired\nclass – inherent transformation network is used to gener-\nate two class inherent transformations which are then used\nto solve a four-class classiﬁcation problem using a CNN.\nHowever, as the number of classes increase, the number\nof generators to be trained in the second stage of this\nmethod will increase accordingly, making it difﬁcult to\nscale for multi class classiﬁcation. COVID-Net [11] lever-\naged a human-machine collaborative design strategy to pro-\nduce a network architecture tailored for COVID-19 detection\nfrom chest X-ray images. CoroNet [12] uses Xception [24]\nVOLUME 10, 2022 1100110\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nFIGURE 1. xViTCOS: Illustration of our proposed network for COVID-19 detection using chest radiography (CT scan/CXR\nimage). The input image is split into equal-sized patches and embedded using linear projection. Position embedding are\nadded and the resulting sequence is fed to a Transformer encoder [33].\nbackbone for extracting CXR image features which are classi-\nﬁed using a multi-layer perceptron (MLP) classiﬁcation head.\nCovidAID [35] ﬁnetunes a pretrained CheXNet [5]. Ref. [36]\nproposes a novel architecture with multiscale attention-based\ngeneration augmentation and guidance for training a CNN\nmodel for COVID-19 diagnosis. The multi-scale attention\nfeatures are computed from the intermediate feature maps\nof a Resnet-50 [23] based feature extractor and are com-\nbined with the ﬁnal feature map to obtain the predictions.\nRef. [37] proposes another attention based CNN model incor-\nporating a teacher-student transfer learning framework for\nCOVID-19 diagnosis from Chest X-ray and CT images.\nCHP-Net [38] consists of three networks: a bounding box\nregression network to extract bi-pulmonary region coor-\ndinates, a discriminator deep learning model to predict\na differentiating probability distribution, and a localiza-\ntion deep network that represents all potential pulmonary\nlocations. In [10] the authors propose using shape depen-\ndent Fibonacci p patterns to extract features from chest\nX-ray images and then apply conventional machine learn-\ning algorithms. Ref. [39] ﬁrst extracts orthogonal moment\nfeatures using Fractional Multichannel Exponent Moments\n(FrMEMs). Next, the most signiﬁcant features are selected\nusing a differential evolution based modiﬁed Manta-Ray\nForaging Optimization (MRFO). Finally a KNN classiﬁer is\ntrained to distinguish COVID-19 positive cases from negative\ncases.\nC. TRANSFORMERS AND SELF ATTENTION IN VISION\nImages can be naively represented using a sequence of pix-\nels for analysis using transformers but that would lead to\nhuge computational expenses with a quadratic increase in\ncosts. This has led to a number of approximations. For\nexample, [40] used self attention in local neighbourhoods\nof query pixels instead of performing calculation globally\nwith the entire rest of the image. Such local multi head\nattentions can be shown to replace convolutions ( [41], [42],\n[43]). Ref. [44] proposed Sparse Transformers where scalable\napproximations to global self attention are employed for\nimages. Ref. [45] used an alternative way of scaling attention\nby applying them in blocks of varying sizes. Ref. [46] applies\nfull attention after extracting patches of size 2 ×2 from the\ninput image. The use of small patch size, however, enables the\nmodel to be used only for small resolution images. Other than\ntransformers, a number of researchers have combined convo-\nlutional neural networks with different forms of self attention.\nRef. [47] uses attention to augment feature maps for image\nclassiﬁcation. A lot of work has come up where the authors\nhave used self attention for further processing the output of a\nCNN for a number of tasks including, object detection ( [48])\nimage classiﬁcation ( [49]), video analysis ( [50], [51]), etc.\nA recent approach by [52] applies Transformers to pixel level\npatches after reducing image resolution and color space. The\nmodel named image GPT is trained like a generative model\nwhose representations are then ﬁne tuned or linearly probed\nfor performing classiﬁcation tasks.\nIII. PROPOSED METHOD\nUnlike the existing methods that incorporate CNNs, we pro-\npose a vision transformer (ViT) [16] based model for auto-\nmated COVID-19 screening and call it xViTCOS, illustrated\nin Figure 1. Since we use xViTCOS on two chest radiography\nmodalities CT scan images and chest X-ray images, we refer\nto them as xViTCOS-CT and xViTCOS-CXR respectively.\nA. VISION TRANSFORMERS\nA Vision Transformer [16] is a deep neural model that adapts\nthe attention-based transformer architecture [33] prevalent in\nthe domain of natural language processing (NLP) to make it\nsuitable for pattern recognition in visual image data. While\n1100110 VOLUME 10, 2022\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nthe original transformer architecture comprises of an encoder\nand a decoder, vision transformer is an encoder-only archi-\ntecture. For non-sequential image analysis tasks, like image\nclassiﬁcation, the input image, x ∈ RH×W ×C is broken\ndown into N image patches, x(i)\np ∈ RP×P×C , where i ∈\n{1, ···N}, and each patch is of shape P ×P in 2-D, C\ndenotes the number of channels (e.g. C =3 for RGB images)\nand N = H×W\nP×P . These patches derived from the image is\nthen effectively used as a sequence of input images for the\nTransformer. The input patches are ﬁrst ﬂattened and then\nmapped to a D dimensional latent vector through a trainable\nlinear projection layer, leading to the generation of patch\nembeddings. Throughout its layers, the transformer maintains\na constant latent vector size of D. Similar to the [class] token\nin BERT [53], a learnable embedding is embedded to the\nsequence of the patch embeddings (Z 0\n0 =xclass). The ﬁnal\ntransformer layer state corresponding to this class token, z0\nL ,\nrepresents in a compact form the classiﬁcation information\nthat the model is able to extract from the image(y). The clas-\nsiﬁcation head is attached to z0\nL during both pre-training and\nﬁne-tuning. In order to retain crucial positional information,\nstandard learnable 1D position embeddings are added to the\npatch embeddings. The ﬁnal resulting sequence is provided\nas input to the encoder. During pre-training, an MLP is used\nto represent the classiﬁcation head and it is replaced by a\nsingle linear layer during the ﬁne-tuning stage. As illus-\ntrated in the Figure 1, the transformer encoder of a vision\ntransformer consists of alternating layers of multiheaded self-\nattention (MSA) and MLP blocks. Layernorm (LN) is applied\nbefore every block, and residual or skip connections after\nevery block. The workings of the vision transformer can be\nmathematically described in Equations below:\nz0 =\n[\nxclass;x1\npE;x2\npE;··· xN\np E\n]\n+Epos (1)\nz′\nl =MSA (LN (zl−1)) +zl−1, ∀l =1 ···L (2)\nzl =MLP\n(\nLN\n(\nz′\nl\n))\n+z′\nl, ∀l =1 ···L (3)\ny =LN\n(\nz0\nL\n)\n(4)\nwhere E ∈R(P2C)×D and Epos ∈R(N+1)×D\nB. INDUCTIVE BIAS IN ViT\nUnlike CNN based models that impose inherent bias such\nas translation invariance and a local receptive ﬁeld, vision\ntransformer (ViT) [16] has much less image speciﬁc inductive\nbias. This is because ViT treats an image as a sequence, hence\nloses any structural and neighborhood information a CNN can\neasily recognize. Although MLP layers are local and trans-\nlationally equivariant, the self-attention layers are global.\nThe only mechanism that adds inductive bias and provides\nstructural information about the image to the encoder are the\nposition embeddings, that are concatenated with the patch\nembeddings. Without those, the Vision Encoder might ﬁnd\nit difﬁcult to make sense of the image patch sequence. Con-\nsequently, ViT does not generalize well when trained using\ninsufﬁcient amount of data. This might be a bit discouraging\nbut the entire status quo changes as the size of the dataset\nincreases. The large size of the training dataset overshadows\nthe dependence of the model on inductive bias for generaliza-\ntion. As can be expected, using a ViT model pretrained on a\nlarge training dataset under a transfer learning framework on\na smaller target dataset leads to improved performance. Next,\nwe propose a multi-stage transfer learning strategy.\nC. MULTI-STAGE TRANSFER LEARNING\nA domain and a task are the two main components of a\ntypical learning problem. For the speciﬁc case of a supervised\nclassiﬁcation problem, the domain, D might be deﬁned as\nthe tuple of the feature space, X, and the marginal feature\ndistribution, P(X), i.e. D =⟨X , P(X)⟩. The task, T is a tuple\nof label space, Y, and the posterior of the labels conditioned\non features, P(Y |X), i.e. T =⟨Y , P(Y |X)⟩. Any change in\neither of the two components of a machine learning problem\nwould cause severe degradation in the performance of the\ntrained model and necessitates rebuilding the model from\nscratch. Transfer Learning is a way to combat this issue.\nGiven a source domain, Ds and a corresponding task, Ts,\nand a target domain, Dt and a corresponding task, Tt , the\nobjective of transfer learning is to improve the performance of\na machine learning model in Dt using the knowledge acquired\nin Ds and Ts [54]. Transfer learning has played a signiﬁcant\nrole in the facilitating the use of deep learning in numerous\napplications [55]–[57]. In this work, we empirically demon-\nstrate how knowledge transfer is equally effective for vision\ntransformer based framework in medical image classiﬁcation.\nIn the current problem, the target domain consists of chest\nradiography image data i.e., for xViTCOS-CXR, the target\ndata is the COVID-19 CXR dataset and for the xViTCOS-CT\nmodel, the target data consists of the COVIDx-CT-2A\ndataset [58] with three classes – COVID-19 Pneumonia, non-\nCOVID-19 Pneumonia, and normal.\nThe ﬁrst source domain DS1 that our proposed ViT\nmodel is trained on consists of a large-scale general-purpose\nimage dataset, ImageNet [59]. Since effective ViT training\ndemands access to a sufﬁciently large number of data points,\nwe choose a model which is pretrained on ImageNet-21k [59](\nTS1\n)\nin a self-supervised manner and later ﬁnetuned on\nImageNet-2012 [60]\n(\nTS2\n)\n. This pre-training aims to ensure\nthat the model learns to extract crucial but generic image\nrepresentations to classify natural images.\nThe underlying distribution of clinical radiographic images\nis vastly different from an unconnected set of natural images\nlike those in ImageNet, and distributional divergence is very\nhigh between the two domains. Hence in cases where the\ntarget dataset is of insufﬁcient capacity, the pre-trained ViT\nmodel might ﬁnd it highly difﬁcult to bridge the domain\nshift between the learned source domain and the unseen\ntarget domain. However, with a sufﬁcient number of training\nexamples available from the target domain, the ViT model\ncan overcome the gap between these two domains. Keeping\nthis in mind, an intermediate stage of knowledge transfer is\nused in this paper to train our proposed model depending on\nVOLUME 10, 2022 1100110\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nTABLE 1. Summary of COVIDx CT-2A dataset [58].\nthe size of the target domain training data. The primary goal\nof this stage of transfer learning is to help the ViT model,\npre-trained on a generic image domains DS1 , DS2 , to learn\nchest radiography speciﬁc representations to overcome the\nexisting domain shift. In order to achieve this, we further\nﬁnetune the pre-trained ViT model on a large collection of\nchest radiographic data\n(\nDS3\n)\n[61] after replacing its existing\nclassiﬁcation head with one suitable for the corresponding\nclassiﬁcation task\n(\nTS3\n)\n.\nWith the COVIDx-CT-2A dataset [58] a moderate-sized\ndataset (refer to Table 1), xViTCOS-CT model was able\nto overcome the domain shift and achieved state-of-the-art\nperformance without the need for the intermediate ﬁnetuning\nstage. However, due to a limited number of COVID-19 CXR\nimages (refer to Table 2), an intermediate stage of knowl-\nedge transfer was employed to improve the performance of\nxViTCOS-CXR model. A publicly available large-scale CXR\ndataset, CheXpert [61] was used, and xViTCOS-CXR was\nﬁnetuned to classify ﬁve medical conditions (Atelectasis,\nCardiomegaly, Consolidation, Edema, and Pleural Effusion)\nand the case of no ﬁnding on that dataset. Following this, the\nexisting classiﬁcation head of the ViT network was replaced\nby a new head suited for the particular target task, i.e.,\nCOVID-19 detection, and the model was further ﬁnetuned\non the target domain. Refer to supplementary material for\nan ablation study to understand the impact of multi-stage\ntransfer.\nD. IMPLEMENTATION DETAILS\nA number of Vision Transformers architectures have been\nproposed in literature. In this paper we have tested our algo-\nrithm on architectures proposed in [53] and [16] over the task\nof classiﬁcation on the Chest X-Ray dataset. A detailed study\non all the architectures tested, namely ViT-B/16, ViT-B/32,\nViT-L/16 and ViT-L/32, and the results obtained has been\nadded in the supplementary. On the basis of classiﬁcation\nperformance and computational expense, we choose the\nViT-B/16 network as the most suitable amongst those tested\nfor further experimentation. For further details, please refer to\nthe Supplementary. ViT-B/16 architecture has the following\nconﬁguration- Patch size: 16 ×16, Fraction of the units\nto drop for dense layers (Dropout rate): 0.1, Dimensions\nof the MLP output in the transformers: 3072, Number of\ntransformer heads: 12, Number of transformer layers: 12,\nHidden size: 768. The model parameters are initialized with\nthe parameters of a model pretrained on ImageNet-21k [59]\nand ﬁne-tuned on ImageNet-2012 [60].\nWhile training xViTCOS-CXR, for the intermediate\nﬁnetuning step using CheXpert [61], we use standard\nbinary cross-entropy loss. This is because the classiﬁcation\nTABLE 2. Summarized description of CXR dataset.\ntask using CheXpert is a multi-label classiﬁcation prob-\nlem. Finally, while ﬁnetuning in the target COVID-19\nCXR images, categorical cross-entropy loss is used to\nsolve a multi-class classiﬁcation problem. While training\nxViTCOS-CT, we utilize categorical cross-entropy. We use\nKeras [62] with Tensorﬂow [63] backend and vit-Keras. 1\nIV. EXPERIMENTS AND RESULTS\nA. DATASETS\nSome of the existing works validate their methods using\nprivate datasets [30], and several other works [12], [14],\n[15], [35] combine data from different publicly available\nsources. While combining data from different public repos-\nitory, researchers should be careful to avoid duplication as\na contributor might upload the same image to many of the\nrepositories. Another interesting way to mitigate the issue of\ndata scarcity is through generative data augmentation where a\nneural generative framework [64]–[67] is trained to generate\nnovel data samples. However in this work, we use the datasets\ndescribed in the next section. We have rerun the codes of the\nbaseline models using same dataset and same split to ensure\na fair comparison.\n1) CT SCAN DATASET\nTo demonstrate the efﬁcacy of xViTCOS-CT, we use\nCOVIDx CT-2A dataset [58], derived from several public\nrepositories [68]–[75]. This dataset contains 194, 922 CT\nscans from 3,745 patients across the globe with clinically\nveriﬁed ﬁndings. Table 1 summarizes the important statistics\nof COVIDx CT-2A dataset.\n2) CHEST X-RAY DATASET\nTo benchmark xViTCOS-CXR against other deep learning\nbased methods for COVID-19 detection using CXR images,\nwe construct a custom dataset consisting of three cases: Nor-\nmal, Pneumonia, and COVID-19. Like in [12], [35], Normal\nand Pneumonia CXR images were obtained from the Kaggle\nrepository ‘Chest X-Ray Images (Pneumonia)’ [76], which is\nderived from [77]. COVID-19 images were collected from\nthe Kaggle repository ‘COVIDx CXR-2’ [78], which is a\ncompilation of several public repositories [79]–[84].\nCOVIDx-CXR-2 [78] provides only Train-Test split of\nthe data. To automatically select the best model based\non validation-set performance, we split Training set in\n80 :20 ratio as train and validation set. This would have\ncaused huge class imbalance in the validation set as ‘Chest\nX-Ray Images (Pneumonia)’ [77] contains only 8 images per\nclass in the validation set. Therefore, we combine the training\nand validation split and reconstruct the training and validation\n1https://github.com/faustomorales/vit-keras\n1100110 VOLUME 10, 2022\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nTABLE 3. Comparison of performance of xViTCOS-CT on CT scan dataset against state-of-the-art methods.\nsplit in 80 :20 ratio. Table 2 summarizes split-wise image\ndistribution. Note that, we have kept the test split intact in\nboth the datasets to prevent patient-wise information leakage\nas multiple images for the same patient could be present in\nthe dataset.\nB. DATA PREPROCESSING AND AUGMENTATION\n1) CT IMAGES\nCOVIDx CT-2A dataset [58] provides bounding box anno-\ntations for the body regions within the CT images. To stan-\ndardize the ﬁeld-of-view in the CT images, we crop the\nimages to the body region using this additional informa-\ntion. Next each cropped image is resized to a ﬁxed size\nof 224 ×224 pixels. To improve generalizability of the\nmodel, we augment the training data on the ﬂy by apply-\ning random afﬁne transformations such as rotation, scal-\ning and translation, random horizontal ﬂip and random\nshear.\n2) CXR IMAGES\nIn the compiled dataset, the chest X-ray images are of var-\nious sizes. To ﬁx this issue, all the images were resized to\na ﬁxed size of 224 ×224 pixels. Again as in the case of\nCT images, to improve the generalizability of the model,\nwe apply the same sets of augmentation techniques (refer to\nSection IV-B.1). In addition, we apply random zoom in and\nzoom out, and random channel shift.\nC. QUANTITATIVE RESULTS\nTo quantify and benchmark the performance of xViTCOS,\nwe compute and report Accuracy, Precision (Positive Predic-\ntion Value), Recall (Sensitivity), F1 score, Speciﬁcity, and\nNegative Prediction Value (NPV) as deﬁned and compared\nin the standard literature such as [14], [32].\nFIGURE 2. Confusion Matrix: The horizontal and vertical axis consists of\nthe ground true and predicted labels, respectively.\n1) xViTCOS-CT\nTable 3 presents the overall accuracy of xViTCOS-CT on the\ntest split of COVID-CT-2A dataset [58]. As can be observed,\nthe proposed method achieves the best accuracy score of\n98.1%, surpassing the current state of art methods. Next,\nwe discuss the precision, recall, speciﬁcity, PPV , NPV , and\nF1-scores attained by the model on test COVID CT images\nand interpret their signiﬁcance in determining the classiﬁca-\ntion caliber of the model. From table 3, it can be observed that\nxViTCOS-CT achieves a high value of recall or sensitivity at\n96%, implying that a small proportion of pneumonia cases\ncaused due to COVID-19 are incorrectly classiﬁed as hav-\ning non-COVID-19 origin. This implies a signiﬁcantly low\nnumber of false-negative cases, which is a highly sought-after\ncharacteristic in a medical data classiﬁer as in such cases,\na false negative situation may lead to denial or delay of\ntreatment to a person genuinely infected by the disease. The\nproposed method also attains a high precision or positive\npredictive value of 96% for COVID-19 cases, implying a little\nchance of the model classifying a non-COVID case as having\nVOLUME 10, 2022 1100110\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nTABLE 4. Comparison of performance of xViTCOS-CXR on chest X-ray dataset against state-of-the-art methods.\na COVID-19 origin. However, the usefulness of our proposed\nmethod lies in the fact that it achieves the highest F1 scores\nfor all the classes, implying that in terms of both precision\nand recall, the proposed method is the most balanced amongst\nall the baseline models. Also, it is well able to differentiate\nbetween the normal and Pneumonia cases of patients as well.\nSimilarly, we can see that the proposed model attains high\nspeciﬁcity and NPV values of 98.8% for the COVID-19 case,\nimplying that false positives are also very low. This is a useful\ncharacteristic in clinical scenarios since the model correctly\nrejects all the negative cases, facilitating efﬁcient utilization\nof limited resources.\nThe prowess of the proposed model can be further under-\nstood from examining the confusion matrix (Figure 2). The\nproposed model can distinguish the healthy patients from\nboth covid and non-covid pneumonia cases very efﬁciently,\nwith an accuracy of almost 99%. Particularly, out of a total\nof 12245 normal cases, 12120 have been classiﬁed correctly,\nwhile 11 (0.09%) and 114 (0.93%) cases have been wrongly\nclassiﬁed as non-COVID pneumonia and COVID pneumonia\nclasses, respectively. Another interesting point to note here\nis that while 114 normal cases have been misclassiﬁed as\nCOVID-19 and 204 COVID-19 cases have been assigned\nthe non-COVID pneumonia label; the classiﬁer has assigned\nonly 31 COVID-19 originated pneumonia cases a normal\nclass. This implies that the proposed method can distinguish\nthe normal cases from the diseased cases.\n2) xViTCOS-CXR\nThe observations regarding the performance of\nxViTCOS-CXR compared to its contemporaries are on the\nFIGURE 3. t-SNE plots of penultimate layers of xViTCOS.\nsame lines as that of xViTCOS-CT, if not better. In terms of\nclassiﬁcation accuracy, xViTCOS-CXR achieves an accuracy\nof 96%, outperforming the baseline methods by a consider-\nable margin as can be seen from Table 4. Further, it can be\nobserved that xViTCOS-CXR achieves high recall (100%)\nand precision values (99%) on the COVID-19 cases, implying\nthat the number of occasions on which the proposed model\nclassiﬁed a COVID-19 model as a non-COVID-19 model\nor vice-versa is extremely low. Examining the entries of\nTable 4, one can observe that the proposed method is the\nmost balanced in terms of precision-recall when compared\nwith the state-of-the-art baselines. Similarly, we can see that\nthe proposed model attains high speciﬁcity and NPV values\nof almost 100% for the COVID-19 case implying that the\nnumber of false positives is almost negligible. This is a\nvaluable characteristic in clinical scenarios since it allows for\nrapid identiﬁcation of patients who do not have COVID-19.\n1100110 VOLUME 10, 2022\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nFIGURE 4. Visualization of different cases (normal, Pneumonia, COVID-19) considered in this study and their associated critical factors in decision making\nby xViTCOS as identified using the explanability method laid out in [87] for transformers [16]. In each subfigure, the left figure presents the input to\nxViTCOS and its ground truth label; the right figure presents the predicted probabilities for each class and highlight the factors critical corresponding to\nthe top predicted class. Figure 4a, 4b and 4c corresponds to CT scan and Figure 4d, 4e and 4f corresponds to CXR images.\nAnalysing ﬁgure 2b, it can be seen that the class-wise\naccuracy of COVID-19 is 100%, i.e., all the ground truth\nCOVID-19 cases have been classiﬁed as COVID-19, imply-\ning that the number of false negatives is zero. This conﬁrms\nthe efﬁcacy of the proposed model in distinguishing between\nCOVID and non-COVID cases.\nD. QUALITATIVE RESULTS\n1) VISUALIZATION OF FEATURE SPACE\nTo visually analyze how clustered the feature space is, we per-\nform a t-SNE visualization of the penultimate layer’s features\nfor both the models using the test splits. As can be seen\nfrom Figure 3, the features in the penultimate layer clusters\ndistinctively for the three different classes.\n2) EXPLAINABILITY\nFor qualitative evaluation of xViTCOS we present sam-\nples of CXR images and CT scans along with their ground\ntruth labels and corresponding saliency maps along with\nthe prediction in Figure 4. In order to analyse the explain-\nability properties of our proposed method, we use the\nGradient Attention Rollout algorithm as outlined in [87].\nFurther details can be found in Section I of the sup-\nplementary document. Figure 4a, 4b and 4c presents CT\nscans of normal, Pneumonia and COVID-19 cases respec-\ntively; Figure 4d, 4e and 4f presents CXR images of normal,\nPneumonia and COVID-19 cases respectively.\nReport corresponding to Figure 4b as interpreted by a\npracticing radiologist: ground glass opacities, consolidation\nand secondary interlobar septal thickening, in bilateral lung,\nFIGURE 5. A case of failure. xViTCOS-CT fails to predict the ground truth\nnon-COVID-19 Pneumonia with confidence as it predicts non-COVID-19\nPneumonia with≈ 50% probability and COVID-19 with≈ 50% probability.\nThis might happen as the findings on chest imaging in COVID-19 are not\nexclusive and overlap with many other type of infections [88]. In such\ncases, human expert intervention is necessary. For a detailed discussion\nrefer to Section V.\nmore extensive in right. xViTCOS-CT correctly highlighted\nthese suspected regions. In Figure 4c xViTCOS-CT localized\nsuspicious lesion regions exhibiting ground glass opacities,\nconsolidation, reticulations in bilateral postero basal lung\nwith subpleural predominance. In Figure 4e Patchy air space\nopacities noted in right upper and midzone matches the\nregions highlighted by xViTCOS-CXR. In Figure 4f, radiol-\nogist’s interpretation is: thick walled cavity in right middle\nzone with surrounding consolidation. xViTCOS-CXR is able\nto correctly identify it. For the cases, where no abnormality is\ndetected (Figure 4a and 4d), xViTCOS focuses on the entire\nlungs and chest respectively to make a ﬁnal decision.\nVOLUME 10, 2022 1100110\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\nV. CONCLUSION\nIn this study, we introduce a novel vision transformer based\nmethod, xViTCOS for COVID-19 screening using chest\nradiography. We have empirically demonstrated the efﬁcacy\nof the proposed method over CNN based SOTA methods as\nmeasured by various metrics such as precision, recall, F1\nscore. Additionally, we examine the predictive performance\nof xViTCOS utilizing explanability-driven heatmap plot to\nhighlight the important factors for the predictive decision it\nmakes. These interpretable visual cues are not only a step\ntowards explainable AI, also might aid practicing radiolo-\ngists in diagnosis. We also analyzed the failure cases of\nour method. Thus, to enhance the effectiveness of diagnosis\nwe suggest that xViTCOS be used to complement RT-PCR\ntesting. In the next phase of this project, we aim to extend\nthis work to automate the analysis of the severity of infection\nusing vision transformers.\nREFERENCES\n[1] WHO Director-General’s Opening Remarks at the Media Brieﬁng on\nCOVID-19. Accessed: Mar. 11, 2020. [Online]. Available: https://www.\nwho.int/director-general/speeches/detail\n[2] M. M. Hellou et al., ‘‘Nucleic acid ampliﬁcation tests on respiratory\nsamples for the diagnosis of coronavirus infections: A systematic review\nand meta-analysis,’’Clin. Microbiol. Infection, vol. 27, no. 3, pp. 341–351,\nMar. 2021.\n[3] F. Colavita et al., ‘‘COVID-19 rapid antigen test as screening strategy at\npoints of entry: Experience in Lazio region, central Italy, August–October\n2020,’’Biomolecules, vol. 11, no. 3, p. 425, 2021.\n[4] K. Munne, V . Bhanothu, V . Bhor, V . Patel, S. D. Mahale, and S. Pande,\n‘‘Detection of SARS-CoV-2 infection by RT-PCR test: Factors inﬂuenc-\ning interpretation of results,’’ VirusDisease, vol. 32, no. 2, pp. 187–189,\nJun. 2021.\n[5] P. Rajpurkar et al., ‘‘ChexNet: Radiologist-level pneumonia detection on\nchest X-rays with deep learning,’’ CoRR, vol. abs/1711.05225, pp. 1–7,\nNov. 2017.\n[6] X. Xie, Z. Zhong, W. Zhao, C. Zheng, F. Wang, and J. Liu, ‘‘Chest CT for\ntypical coronavirus disease 2019 (COVID-19) pneumonia: Relationship\nto negative RT-PCR testing,’’ Radiology, vol. 296, no. 2, pp. E41–E45,\nAug. 2020.\n[7] A. Bernheim et al., ‘‘Chest CT ﬁndings in coronavirus disease-19\n(COVID-19): Relationship to duration of infection,’’ Radiology, vol. 295,\nno. 3, Jun. 2020, Art. no. 200463.\n[8] Y . Fang et al., ‘‘Sensitivity of chest CT for COVID-19: Comparison to RT-\nPCR,’’Radiology, vol. 296, no. 2, pp. E115–E117, Aug. 2020.\n[9] T. Ai et al., ‘‘Correlation of chest CT and RT-PCR testing for coronavirus\ndisease 2019 (COVID-19) in China: A report of 1014 cases,’’ Radiology,\nvol. 296, no. 2, pp. E32–E40, Aug. 2020.\n[10] K. Panetta, F. Sanghavi, S. Agaian, and N. Madan, ‘‘Automated detection\nof COVID-19 cases on radiographs using shape-dependent ﬁbonacci-P\npatterns,’’IEEE J. Biomed. Health Informat., vol. 25, no. 6, pp. 1852–1863,\nJun. 2021.\n[11] L. Wang, Z. Q. Lin, and A. Wong, ‘‘COVID-Net: A tailored deep convolu-\ntional neural network design for detection of COVID-19 cases from chest\nX-ray images,’’ Sci. Rep., vol. 10, no. 1, Dec. 2020, Art. no. 19549.\n[12] A. I. Khan, J. L. Shah, and M. M. Bhat, ‘‘CoroNet: A deep neural network\nfor detection and diagnosis of COVID-19 from chest X-ray images,’’ Com-\nput. Methods Programs Biomed., vol. 196, Nov. 2020, Art. no. 105581.\n[13] A. A. Ardakani, A. R. Kanaﬁ, U. R. Acharya, N. Khadem, and A. Moham-\nmadi, ‘‘Application of deep learning technique to manage COVID-19 in\nroutine clinical practice using CT images: Results of 10 convolutional neu-\nral networks,’’ Comput. Biol. Med., vol. 121, Jun. 2020, Art. no. 103795.\n[14] L. Wang, Z. Q. Lin, and A. Wong, ‘‘COVID-Net: A tailored deep convolu-\ntional neural network design for detection of COVID-19 cases from chest\nX-ray images,’’ Sci. Rep., vol. 10, no. 1, 2020, Art. no. 19549.\n[15] H. Gunraj, L. Wang, and A. Wong, ‘‘COVIDNet-CT: A tailored deep\nconvolutional neural network design for detection of COVID-19 cases from\nchest CT images,’’ Frontiers Med., vol. 7, Dec. 2020, Art. no. 608525.\n[16] A. Dosovitskiy et al., ‘‘An image is worth 16×16 words: Transformers for\nimage recognition at scale,’’ in Proc. ICLR, 2021, pp. 1–22.\n[17] L. Sun et al., ‘‘Adaptive feature selection guided deep forest for COVID-19\nclassiﬁcation with chest CT,’’ IEEE J. Biomed. Health Informat., vol. 24,\nno. 10, pp. 2798–2805, Oct. 2020.\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘Imagenet classiﬁca-\ntion with deep convolutional neural networks,’’ in NeurIPS, F. Pereira,\nC. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Red Hook, NY ,\nUSA: Curran Associates, 2012.\n[19] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ in Proc. ICLR, 2015, pp. 1–14.\n[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and\nK. Keutzer, ‘‘SqueezeNet: AlexNet-level accuracy with 50x fewer param-\neters and < 0.5 model size,’’ 2016, arXiv:1602.07360.\n[21] C. Szegedy et al., ‘‘Going deeper with convolutions,’’ in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 1–9.\n[22] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n‘‘MobileNetV2: Inverted residuals and linear bottlenecks,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\npp. 4510–4520.\n[23] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[24] F. Chollet, ‘‘Xception: Deep learning with depthwise separable convo-\nlutions,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJul. 2017, pp. 1251–1258.\n[25] X. Xu et al., ‘‘A deep learning system to screen novel coronavirus disease\n2019 pneumonia,’’ Engineering, vol. 6, no. 10, pp. 1122–1129, 2020.\n[26] L. Li et al., ‘‘Using artiﬁcial intelligence to detect COVID-19 and\ncommunity-acquired pneumonia based on pulmonary CT: Evaluation\nof the diagnostic accuracy,’’ Radiology, vol. 296, no. 2, pp. E65–E71,\nAug. 2020.\n[27] H. X. Bai et al., ‘‘Artiﬁcial intelligence augmentation of radiologist per-\nformance in distinguishing COVID-19 from pneumonia of other origin at\nchest ct,’’ Radiology, vol. 296, no. 3, pp. E156–E165, 2020.\n[28] Z. Wang, Q. Liu, and Q. Dou, ‘‘Contrastive cross-site learning with\nredesigned net for COVID-19 CT classiﬁcation,’’ IEEE J. Biomed. Health\nInformat., vol. 24, no. 10, pp. 2806–2813, Oct. 2020.\n[29] M. Owais, Y . W. Lee, T. Mahmood, A. Haider, H. Sultan, and K. R. Park,\n‘‘Multilevel deep-aggregated boosted network to recognize COVID-\n19 infection from large-scale heterogeneous radiographic data,’’ IEEE\nJ. Biomed. Health Informat., vol. 25, no. 6, pp. 1881–1891, Jun. 2021.\n[30] X. Yu, S. Lu, L. Guo, S.-H. Wang, and Y .-D. Zhang, ‘‘ResGNet-C: A graph\nconvolutional neural network for detection of COVID-19,’’ Neurocomput-\ning, vol. 452, pp. 592–605, Sep. 2021.\n[31] T. N. Kipf and M. Welling, ‘‘Semi-supervised classiﬁcation with graph\nconvolutional networks,’’ in Proc. ICLR, 2017, pp. 1–5.\n[32] T. Kaur, T. K. Gandhi, and B. K. Panigrahi, ‘‘Automated diagnosis of\nCOVID-19 using deep features and parameter free BAT optimization,’’\nIEEE J. Transl. Eng. Health Med., vol. 9, 2021, Art. no. 1800209.\n[33] A. Waswani et al., ‘‘Attention CS all you need,’’ in Proc. NeurIPS, 2017,\npp. 5998–6008.\n[34] S. Tabik et al., ‘‘COVIDGR dataset and COVID-SDNet methodology for\npredicting COVID-19 based on Chest X-Ray images,’’ IEEE J. Biomed.\nHealth Informat., vol. 24, no. 12, pp. 3595–3605, Dec. 2020.\n[35] A. Mangal et al., ‘‘CovidAID: COVID-19 detection using chest X-ray,’’\n2020, arXiv 2004.09803.\n[36] J. Li et al., ‘‘Multiscale attention guided network for COVID-19 diagnosis\nusing chest X-ray images,’’ IEEE J. Biomed. Health Informat., vol. 25,\nno. 5, pp. 1336–1346, May 2021.\n[37] W. Shi, L. Tong, Y . Zhu, and M. D. Wang, ‘‘COVID-19 automatic diagnosis\nwith radiographic imaging: Explainable attention transfer deep neural net-\nworks,’’IEEE J. Biomed. Health Informat., vol. 25, no. 7, pp. 2376–2387,\nJul. 2021.\n[38] Z. Wang et al., ‘‘Automatically discriminating and localizing COVID-19\nfrom community-acquired pneumonia on chest X-rays,’’ Pattern Recognit.,\nvol. 110, Feb. 2021, Art. no. 107613.\n[39] M. A. Elaziz, K. M. Hosny, A. Salah, M. M. Darwish, S. Lu, and\nA. T. Sahlol, ‘‘New machine learning method for image-based diagnosis\nof COVID-19,’’ PLoS ONE, vol. 15, no. 6, Jun. 2020, Art. no. e0235187.\n[40] N. Parmar et al., ‘‘Image transformer,’’ in Proc. Int. Conf. Mach. Learn.,\nPMLR, Jul. 2018, pp. 4055–4064.\n[41] H. Hu, Z. Zhang, Z. Xie, and S. Lin, ‘‘Local relation networks for\nimage recognition,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2019, pp. 3464–3473.\n1100110 VOLUME 10, 2022\nA. K. Mondalet al.: xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography\n[42] H. Zhao, J. Jia, and V . Koltun, ‘‘Exploring self-attention for image recog-\nnition,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2020, pp. 10076–10085.\n[43] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\nJ. Shlens, ‘‘Stand-alone self-attention in vision models,’’ in Advances\nin Neural Information Processing Systems, vol. 32, H. Wallach,\nH. Larochelle, A. Beygelzimer, F. Alché-Buc, E. Fox, and R. Garnett, Eds.\nRed Hook, NY , USA: Curran Associates, 2019.\n[44] R. Child, S. Gray, A. Radford, and I. Sutskever, ‘‘Generating long\nsequences with sparse transformers,’’ 2019, arXiv:1904.10509.\n[45] D. Weissenborn, O. Täckström, and J. Uszkoreit, ‘‘Scaling autoregressive\nvideo models,’’ in Proc. Int. Conf. Learn. Represent., 2020, pp. 1–24.\n[46] J.-B. Cordonnier, A. Loukas, and M. Jaggi, ‘‘On the relationship between\nself-attention and convolutional layers,’’ in Int. Conf. Learn. Represent.,\n2020, pp. 1–18.\n[47] I. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens, ‘‘Attention augmented\nconvolutional networks,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis.\n(ICCV), Oct. 2019, pp. 3286–3295.\n[48] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y . Wei, ‘‘Relation networks for object\ndetection,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\nJun. 2018, pp. 3588–3597.\n[49] B. Wu et al., ‘‘Visual transformers: Token-based image representation and\nprocessing for computer vision,’’ 2020, arXiv:2006.03677.\n[50] X. Wang, R. Girshick, A. Gupta, and K. He, ‘‘Non-local neural net-\nworks,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2018,\npp. 7794–7803.\n[51] C. Sun, A. Myers, C. V ondrick, K. Murphy, and C. Schmid, ‘‘VideoBERT:\nA joint model for video and language representation learning,’’\nin Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , Oct. 2019,\npp. 7463–7472.\n[52] M. Chen et al., ‘‘Generative pretraining from pixels,’’ in Proc. 37th Int.\nConf. Mach. Learn. (ICML), vol. 119, Jul. 2020, pp. 1691–1703.\n[53] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Tech-\nnol., vol. 1, Jun. 2019, pp. 4171–4186.\n[54] S. Pan and Q. Yang, ‘‘A survey on transfer learning,’’ IEEE Trans. Knowl.\nData Eng., vol. 22, no. 4, pp. 1345–1359, Nov. 2010.\n[55] M. V olpp et al., ‘‘Meta-learning acquisition functions for transfer learning\nin Bayesian optimization,’’ in Int. Conf. Learn. Represent., 2020, pp. 1–22.\n[56] A. Bhattacharjee, A. Verma, S. Mishra, and T. K. Saha, ‘‘Estimating state of\ncharge for xEV batteries using 1D convolutional neural networks and trans-\nfer learning,’’ IEEE Trans. Veh. Technol., vol. 70, no. 4, pp. 3123–3135,\nApr. 2021.\n[57] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, ‘‘A survey on\ndeep transfer learning,’’ in Proc. Int. Conf. Artif. Neural Netw. Cham,\nSwitzerland: Springer, Oct. 2018, pp. 270–279.\n[58] H. Gunraj. (2021). COVIDx CT-2A: A Large-Scale Chest CT\nDataset for COVID-19 Detection. [Online]. Available: https://www.\nkaggle.com/hgunraj/covidxct\n[59] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘Ima-\ngeNet: A large-scale hierarchical image database,’’ in Proc. CVPR, 2009,\npp. 248–255.\n[60] O. Russakovsky et al., ‘‘ImageNet large scale visual recognition chal-\nlenge,’’Int. J. Comput. Vis., vol. 115, no. 3, pp. 211–252, Dec. 2015.\n[61] J. Irvin et al., ‘‘Chexpert: A large chest radiograph dataset with uncertainty\nlabels and expert comparison,’’ in Proc. AAAI, 2019, pp. 590–597.\n[62] F. Chollet. (2015). Keras. [Online]. Available: https://keras.io\n[63] M. Abadi. (2015). TensorFlow: Large-Scale Machine Learning on Hetero-\ngeneous Systems. https://Software.available.from.tensorﬂow.org\n[64] I. Goodfellow et al., ‘‘Generative adversarial nets,’’ in Proc. NeuRIPS,\n2014, pp. 1–9.\n[65] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Scholkopf, ‘‘Wasserstein auto-\nencoders,’’ in Proc. ICLR, 2018, pp. 1–20.\n[66] A. K. Mondal, S. P. Chowdhury, A. Jayendran, P. Singla, H. Asnani, and\nA. Prathosh, ‘‘MaskAAE: Latent space optimization for adversarial auto-\nencoders,’’ in Proc. UAI, 2020, pp. 1–18.\n[67] A. K. Mondal, H. Asnani, P. Singla, and A. Prathosh, ‘‘FlexAE: Flexibly\nlearning latent priors for wasserstein auto-encoders,’’ in Proc. UAI, 2021,\npp. 525–535.\n[68] K. Zhang et al., ‘‘Clinically applicable ai system for accurate diagnosis,\nquantitative measurements, and prognosis of COVID-19 pneumonia using\ncomputed tomography,’’ Cell, vol. 181, no. 6, pp. 1423–1433, 2020.\n[69] P. An et al., ‘‘CT images in COVID-19,’’ Cancer Imag. Arch., Jun. 2020.\n[70] M. Rahimzadeh, A. Attar, and S. M. Sakhaei, ‘‘A fully automated deep\nlearning-based network for detecting COVID-19 from a new and large\nlung ct scan dataset,’’ Biomed. Signal Process. Control, vol. 68, Dec. 2021,\nArt. no. 102588.\n[71] W. Ning et al., ‘‘Open resource of clinical data from patients with pneumo-\nnia for the prediction of COVID-19 outcomes via deep learning,’’ Nature\nBiomed. Eng., vol. 4, no. 12, pp. 1197–1207, Dec. 2020.\n[72] J. Ma et al., ‘‘Towards data-efﬁcient learning: A benchmark for COVID-19\nCT lung and infection segmentation,’’ 2020, arXiv:2004.12537.\n[73] S. G. Armato III et al., ‘‘Data from LIDC-IDRI,’’ Cancer Imag. Arch.,\nDec. 2015.\n[74] Radiopaedia. COVID-19. Accessed: Feb. 4, 2021. [Online]. Available:\nhttps://radiopaedia.org/articles/covid-19-4\n[75] S. P. Morozov et al., ‘‘MosMedData: Chest CT scans with COVID-19\nrelated ﬁndings dataset,’’ 2020, arXiv:2005.06465.\n[76] P. Mooney. (2018). Chest X-ray Images (Pneumonia). [Online]. Available:\nhttps://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n[77] D. Kermany, K. Zhang, and M. Goldbaum, ‘‘Labeled optical coherence\ntomography (oct) and chest X-ray images for classiﬁcation,’’ Mendeley\nData, vol. 2, no. 2, Jun. 2018.\n[78] A. Zhao. (2021). COVIDx CXR-2: Chest X-ray Images for the\nDetection of COVID-19. [Online]. Available: https://www.kaggle.\ncom/andyczhao/covidx-cxr2\n[79] J. Paul Cohen, P. Morrison, L. Dao, K. Roth, T. Q Duong, and M. Ghassemi,\n‘‘COVID-19 image data collection: Prospective predictions are the future,’’\n2020, arXiv:2006.11988.\n[80] L. Wang. (2020). Figure 1 COVID-19 Chest X-ray Dataset Initia-\ntive. [Online]. Available: https://github.com/agchung/Figure1-COVID-\nchestxray-dataset\n[81] L. Wang. (2020). Actualmed COVID-19 Chest X-ray Dataset Initia-\ntive. [Online]. Available: https://github.com/agchung/Actualmed-COVID-\nchestxray-dataset\n[82] M. E. Chowdhury et al., ‘‘Can ai help in screening viral and COVID-19\npneumonia?’’ IEEE Access, vol. 8, pp. 132665–132676, 2020.\n[83] RS North America (2018). RSNA Pneumonia Detection Challenge: Can\nYou Build an Algorithm That Automatically Detects Potential Pneumonia\nCases. [Online]. Available: https://www.kaggle.com/c/rsna-pneumonia-\ndetection-challenge\n[84] E. B. Tsai et al., ‘‘Data from medical imaging data resource cen-\nter (MIDRC)-RSNA international covid radiology database (RICORD)\nrelease 1C—Chest X-ray, covid+ (MIDRC-RICORD-1C),’’ Tech. Rep.,\n2021.\n[85] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‘‘Rethinking\nthe inception architecture for computer vision,’’ in Proc. CVPR, 2016,\npp. 2818–2826.\n[86] N. Tsiknakis and E. Trivizakis, ‘‘Interpretable artiﬁcial intelligence frame-\nwork for COVID-19 screening on chest X-rays,’’ Exp. Ther. Med., vol. 20,\nno. 2, pp. 727–735, May 2020.\n[87] H. Chefer, S. Gur, and L. Wolf, ‘‘Transformer interpretability beyond\nattention visualization,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2021, pp. 782–791.\n[88] ACR Recommendations for the Use of Chest Radiography and Computed\nTomography (CT) for Suspected COVID-19 Infection, American College\nof Radiology, Richmond, V A, USA, 2020.\nVOLUME 10, 2022 1100110"
}