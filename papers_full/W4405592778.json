{
    "title": "T-LLaMA: a Tibetan large language model based on LLaMA2",
    "url": "https://openalex.org/W4405592778",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2111081875",
            "name": "Hui Lv",
            "affiliations": [
                "Lanzhou University",
                "Minzu University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2547913060",
            "name": "Chi Pu",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2109981544",
            "name": "La Duo",
            "affiliations": [
                "Qinghai Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2097672999",
            "name": "Yan Li",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2164233456",
            "name": "Qingguo Zhou",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A1988806642",
            "name": "Jun Shen",
            "affiliations": [
                "University of Wollongong"
            ]
        },
        {
            "id": "https://openalex.org/A2111081875",
            "name": "Hui Lv",
            "affiliations": [
                "Lanzhou University",
                "Minzu University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2547913060",
            "name": "Chi Pu",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2109981544",
            "name": "La Duo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097672999",
            "name": "Yan Li",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2164233456",
            "name": "Qingguo Zhou",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A1988806642",
            "name": "Jun Shen",
            "affiliations": [
                "University of Wollongong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6602752192",
        "https://openalex.org/W6607919353",
        "https://openalex.org/W6826116265",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2763150156",
        "https://openalex.org/W4389520455",
        "https://openalex.org/W4385569780",
        "https://openalex.org/W6600445788",
        "https://openalex.org/W4392255640",
        "https://openalex.org/W4285807172",
        "https://openalex.org/W6600103761",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3168656614",
        "https://openalex.org/W6816558821",
        "https://openalex.org/W6636568255",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W4200380247",
        "https://openalex.org/W4385570293",
        "https://openalex.org/W4309374440",
        "https://openalex.org/W6601141708",
        "https://openalex.org/W4378830714",
        "https://openalex.org/W4402604703",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W6606545560",
        "https://openalex.org/W4301184378",
        "https://openalex.org/W2150824314"
    ],
    "abstract": "Abstract The advent of ChatGPT and GPT-4 has generated substantial interest in large language model (LLM) research, showcasing remarkable performance in various applications such as conversation systems, machine translation, and research paper summarization. However, their efficacy diminishes when applied to low-resource languages, particularly in academic research contexts like Tibetan. In this study, we trained Tibetan LLaMA (T-LLaMA), a model based on efficient pre-training technology for three downstream tasks: text classification, news text generation and automatic text summarization. To address the lack of corpus, we constructed a Tibetan dataset comprising 2.2 billion characters. Furthermore, we augmented the vocabulary of LLaMA2 from META AI by expanding the Tibetan vocabulary using SentencePiece. Notably, the text classification task attains a state-of-the-art (SOTA) accuracy of 79.8% on a publicly available dataset Tibetan News Classification Corpus. In addition, manual review of 500 generated samples indicates satisfactory results in both news text generation and text summarization tasks. To our knowledge, T-LLaMA stands as the first large-scale language model in Tibetan natural language processing (NLP) with parameters in the billion range. We openly provide our trained models, anticipating that this contribution not only fills gaps in the Tibetan large-scale language model domain but also serves as foundational models for researchers with limited computational resources in the Tibetan NLP community. The T-LLaMA model is available at https://huggingface.co/Pagewood/T-LLaMA .",
    "full_text": "Vol.:(0123456789)\nComplex & Intelligent Systems (2025) 11:72 \nhttps://doi.org/10.1007/s40747-024-01641-7\nORIGINAL ARTICLE\nT‑LLaMA: a Tibetan large language model based on LLaMA2\nHui Lv1,2 · Chi Pu1 · La Duo3 · Yan Li1 · Qingguo Zhou1 · Jun Shen4\nReceived: 30 December 2023 / Accepted: 19 August 2024 / Published online: 19 December 2024 \n© The Author(s) 2024\nAbstract\nThe advent of ChatGPT and GPT-4 has generated substantial interest in large language model (LLM) research, showcasing \nremarkable performance in various applications such as conversation systems, machine translation, and research paper sum-\nmarization. However, their efficacy diminishes when applied to low-resource languages, particularly in academic research \ncontexts like Tibetan. In this study, we trained Tibetan LLaMA (T-LLaMA), a model based on efficient pre-training tech-\nnology for three downstream tasks: text classification, news text generation and automatic text summarization. To address \nthe lack of corpus, we constructed a Tibetan dataset comprising 2.2 billion characters. Furthermore, we augmented the \nvocabulary of LLaMA2 from META AI by expanding the Tibetan vocabulary using SentencePiece. Notably, the text clas-\nsification task attains a state-of-the-art (SOTA) accuracy of 79.8% on a publicly available dataset Tibetan News Classification \nCorpus. In addition, manual review of 500 generated samples indicates satisfactory results in both news text generation and \ntext summarization tasks. To our knowledge, T-LLaMA stands as the first large-scale language model in Tibetan natural \nlanguage processing (NLP) with parameters in the billion range. We openly provide our trained models, anticipating that \nthis contribution not only fills gaps in the Tibetan large-scale language model domain but also serves as foundational models \nfor researchers with limited computational resources in the Tibetan NLP community. The T-LLaMA model is available at \nhttps:// huggi ngface. co/ Pagew ood/T- LLaMA.\nKeywords Large language model · Tibetan · Low-resource languages · Text classification\nIntroduction\nLarge language model (LLM) has achieved remarkable suc-\ncess in the field of Natural Language Processing (NLP), par-\nticularly since the introduction of ChatGPT [1 ], which has \ngenerated extensive research interest. These models have \ndemonstrated exceptional performance across various tasks, \nincluding text generation, machine translation, sentiment \nanalysis, and question answering.\nHowever, a notable observation is that the majority of large \nlanguage models have predominantly concentrated on lan-\nguages with abundant corpora, such as English and Chinese [2, \n3]. Tibetan, spoken by over 4.5 million people, lags behind in \nresearch on large language models. This challenge stems from \nthe scarcity of Tibetan language corpora and limited engage-\nment from researchers in the Tibetan NLP domain. In previous \nresearch, to leverage large language models in scenarios with \nlimited corpora, researchers have demonstrated the effective-\nness of fine-tuning pre-trained multilingual models [4]. For-\ntunately, the open-sourcing of the LLaMA2 by META AI [5] \nhas obviated the need to initiate training from scratch. Addi-\ntionally, the introduction of Low-Rank Adaptation (LoRA) \nfor LLM, as proposed by Hu et al. [6], allows us to fine-tune \nlarge models with 7 billion parameters using significantly less \ncomputational power. These technological advancements open \navenues for training LLMs for low-resource languages, par-\nticularly Tibetan, laying the groundwork for the informatiza-\ntion of the Tibetan language.\n * Yan Li \n liyan_2007@lzu.edu.cn\n * Qingguo Zhou \n zhouqg@lzu.edu.cn\n1 School of Information Science and Engineering, Lanzhou \nUniversity, Lanzhou 730000, China\n2 School of Electrical Engineering, Northwest Minzu \nUniversity, Lanzhou 730000, China\n3 The State Key Laboratory of Tibetan Intelligent Information \nProcessing and Application, Qinghai Normal University, \nXining 810000, China\n4 School of Computing and Information Technology, \nUniversity of Wollongong, Wollongong, NSW, Australia\n Complex & Intelligent Systems (2025) 11:72\n72 Page 2 of 11\nIn this paper, we aim to bridge the gap in Tibetan NLP \nby leveraging large language models. To achieve this, we \nemployed LoRA technology to fine-tune a model named \nTibetan LLaMA (T-LLaMA), utilizing the pre-existing 7 \nbillion parameters of LLaMA2 [5 ]. The training data com-\nprised a Tibetan corpus containing 2.2 billion characters. \nIn order to assess the impact of vocabulary size on model \nperformance, we employed SentencePiece to construct \nvocabularies of varying sizes [7 ]. These vocabularies were \nsubsequently integrated into the LLaMA2 lexicon, and their \neffects on performance were assessed through a text classi-\nfication task. Moreover, we broadened our investigation by \nexamining the influence of different rank configurations on \nmodel performance. Finally, our model evaluation encom-\npassed three fundamental NLP tasks: text classification, \nnews text generation, and automatic text summarization.\nFor text classification, we utilized the Tibetan News Clas-\nsification Corpus (TNCC) [8 ]. T-LLaMA achieved a state-\nof-the-art accuracy of 79.8%, showcasing exceptional per -\nformance. We explored the impact of varying max sequence \nlengths on text classification accuracy, noting an improve-\nment as sequence length increased. Moving on to news text \ngeneration, we transformed proprietary news data into a \ndataset and manually reviewed 500 generated articles for \ngrammatical correctness and content relevance to headlines. \nResults showed that the majority of articles were grammati-\ncally correct, with 81% exhibiting content relevance. Finally, \nwe fine-tuned the T-LLaMA model for text summarization \nusing LoRA. The evaluation, conducted on 500 randomly \nchosen test samples, demonstrated the model’s effectiveness \nin generating accurate and concise summaries.\nThese results collectively underscore the effectiveness of \nT-LLaMA as a powerful LLM for the Tibetan language. It \nmarks the introduction of the first Tibetan large-scale model \nwith parameters in the billion range. However, challenges \npersist, including the lack of a substantial prompt engineer-\ning corpus for downstream tasks such as dialogue, reason-\ning, and translation. We acknowledge the need for a more \nextensive collaborative effort within the research community \nto address these challenges and propel the development of \nTibetan large language models. We encourage and aspire \nto see more researchers join this community in the future, \ncontributing their expertise and efforts to overcome the hur-\ndles and advance the capabilities of Tibetan large language \nmodels.\nRelated work\n Large language models\nThe recent evolution of NLP has been marked by a trans-\nformative paradigm shift, propelled by the advent of LLM. \nNotable models such as GPT-3 [9], PaLM [10] and LLaMA \n[5] exemplify the capabilities of pushing the boundaries of \nmachine understanding and text generation. Large language \nmodels have showcased exceptional performance across a \nspectrum of complex NLP tasks, including text classification \n[11], question answering (QA) [12, 13], text generation [14, \n15] and text summarization [16].\nHowever, the demanding computational resources \nrequired for training large models pose a challenge, hin-\ndering full-scale model pre-training and fine-tuning. Con-\nsequently, various efficient parameter fine-tuning methods \nhave been proposed, including adapter tuning [17], prefix \ntuning [18], prompt tuning [19], and LoRA [6].\nDespite the strides made by LLM in NLP, progress in \nlow-resource languages, such as Tibetan and Southeast Asia \nlanguages, has been notably sluggish [20, 21]. Furthermore, \nthe performance of multilingual large models on Tibetan, to \nthe best of our knowledge, remains unsatisfactory. Accord-\ning to our analysis and practical usage of ChatGPT [1 ], \nLLaMA [5 ], and Baichuan2 [22], it is evident that these \nmodels exhibit notable performance disparities in Tibetan \nwhen compared to high-resource languages like English and \nChinese. This highlights an existing gap in the application of \nthese advanced models to languages with limited resources, \nunderlining the importance of our efforts in addressing these \nchallenges within the Tibetan NLP domain.\n Tibetan pre‑trained language models\nPre-trained language models such as BERT [23], ALBERT \n[24] and GPT [25, 26] are widely recognized for their effec-\ntiveness in enhancing a model’s language understanding \ncapabilities. Language-specific pre-trained models have \nproven to be particularly effective in this regard [ 27, 28]. \nWithin the Tibetan NLP community, Li undertook the train-\ning of a pre-trained model based on ALBERT, finetuning \nit on self-constructed text classification and summariza-\ntion datasets, yielding satisfactory results [29]. Sun et al., \nby constructing a vocabulary based on SentencePiece [7 ], \ntrained a pre-trained model named TiBERT [30] to excel in \nTibetan NLP downstream tasks. Additionally, Yang et al. \nintroduced a Chinese minority pre-trained language model \n(CINO) based on XLM-R [31], achieving satisfactory results \nin downstream tasks for minority languages by enriching the \nvocabulary of the original XLM-R tokenizer.\nMore recently, a method for Tibetan text classification, \nutilizing pre-trained models and based on prompt, has \nbeen developed [32]. Similarly, Zhou et al. explored three \nefficient fine-tuning methods based on the TNCC dataset, \nachieving preliminary success [33]. While these works have \nmade strides in pre-trained models and prompt learning, \nthe exploration of billion-parameter LLM in the Tibetan \nComplex & Intelligent Systems (2025) 11:72 \n Page 3 of 11 72\ncontext remains largely uncharted. This gap has motivated \nour efforts in this field.\nOur model builds upon LLaMA2, a large language model \noffering various models ranging from 7 to 65 billion param-\neters. Through fine-tuning the LLaMA2 model with 7 billion \nparameters based on our own corpus and leveraging LoRA \ntechnology [6 ], our model has demonstrated satisfactory \nresults.\n T‑LLaMA\nIn this study, our approach involves the initial optimization \nof the Tibetan vocabulary, which is subsequently integrated \ninto the LLaMA2 tokenizer. Following this, we proceed to \ntrain a fine-tuned model utilizing LoRA technology. Finally, \nwe assess the performance of the model through evalua-\ntions on downstream Tibetan classification tasks, news text \ngeneration tasks and text summarization tasks. The code \nemployed in this paper is based on the implementation pro-\nvided in Ref. [4].\n Introduction of LLaMA2\nLLaMA, introduced by META AI in 2023 [2], stands out as \na large language model featuring four models with varying \nscales, spanning from 7 to 65 billion parameters. The ini-\ntial version of LLaMA, constructed using the Transformer \ndeep learning framework [34], adheres to scaling laws [35] \nby training on a more extensive dataset while employing \nsmaller models. Notably, LLaMA-13B, one of these models, \noutperformed GPT-3 (175B) [9 ] in the majority of bench-\nmark tests.\nLLaMA exhibits exceptional proficiency in various tasks, \nincluding text generation, machine translation, sentiment \nanalysis, and automated question answering, providing \nrobust support for a wide spectrum of NLP applications. In \na bid to enhance both the quality and quantity of the training \ndata, the researchers introduced LLaMA2 [5 ], which effec-\ntively doubled the context length of the model. LLaMA2 has \ndemonstrated state-of-the-art (SOTA) performance across \nmultiple tasks, solidifying its position among numerous \nopen-source models.\n Low‑Rank adaptation\nThe training of large language models is widely recognized \nas a resource-intensive process. The endeavor necessitates an \nextensive corpus, exemplified by the 45 TB data [36] volume \nutilized in the training of models like ChatGPT. Further -\nmore, it demands a substantial number of GPUs to provide \nthe necessary computational power, coupled with a consider-\nable amount of time to complete the training process. Given \nthese challenges, the prospect of training a large language \nmodel for Tibetan seems nearly impossible.\nIn response to this predicament, Edward Hu et al. intro-\nduced the Low-Rank Adaptation (LoRA) approach [6 ]. \nLoRA posits that over-parametrized models learned during \ntraining inherently exist within a lower intrinsic dimension. \nThis innovative approach empowers researchers to train \nmodels by optimizing much smaller low-rank matrices, sig-\nnificantly alleviating computational demands and dataset \nrequirements.\nThe adoption of LoRA technology has made it feasi-\nble for us to train a large language model for Tibetan, a \nlow-resource language where such training was previously \nchallenging due to resource constraints. This breakthrough \nnot only broadens the scope of language modeling but also \nfacilitates applications in Tibetan-specific NLP tasks.\n Data preparation\nWe constructed a dataset consisting of approximately 2.2 bil-\nlion characters, amalgamating content from diverse sources \nsuch as websites, various books, journal papers, textbooks, \nand historical documents. This dataset spans a broad spec-\ntrum of content, covering fields such as encyclopedic knowl-\nedge, legal documents, history, language, economics, and \nmore. The distribution of content across these categories is \noutlined in Table 1.\n Vocabulary optimization\nTokenization is a critical step in the domain of pre-trained \nmodels and large language models. Notably, among various \ntokenization models, WordPiece [19] and SentencePiece [7] \nstand out as prominent choices. WordPiece excels in break-\ning down words into smaller components, facilitating the \nhandling of intricate words and languages. In contrast, Sen-\ntencePiece offers notable flexibility, accommodating various \nlanguages and tokenization granularities, including whole \nwords, subwords, or characters, all of which can be referred \nto as tokens.\nThe Tibetan language, following a phonetic writing \nsystem, structures its syllables through a combination of \nTable 1  Data distribution of the \ncorpus Document source Size (GB)\nAncient literature 1.3\nWebsite news 2.8\nBlog and Weibo 0.26\nBooks 1.77\nJournal Article 0.12\nTeaching material 0.002\nTotal 6.27\n Complex & Intelligent Systems (2025) 11:72\n72 Page 4 of 11\nvertical and horizontal, separated by the separator . (Tsheg). \nThe combination of upper and lower characters is also called \nVertical Combination Character (VCC). Each syllable is \ncomposed of one or more characters [37], as illustrated in \nFig. 1 for the syllable  which consists of components \nlike Prefix , Superfix , Root , Subfix , Vowel \n, Suffix  and Postfix Suffix  named according to \ntheir positions. These components collectively form Tibetan \ncharacters. In contrast to the subject-verb-object structure \nfound in Chinese and English sentences, Tibetan sentence \nstructure follows a pattern where the subject precedes the \nobject, with the predicate coming after. Figure  2 illustrates \nthe fundamental structure of a Tibetan sentence, exempli-\nfied by  (He likes to draw). It mentions the \ncomponents of subject, object, and predicate. Each sen-\ntence concludes with  (shed) as a standard grammatical \nrequirement.\nIn the context of Tibetan script intricacies, Sentence-\nPiece emerges as a superior choice due to its adaptability \nin handling variations in multicomponent characters and \nadjustments in word segmentation granularity. Its ability to \nflexibly adapt to the unique requirements of Tibetan script \npositions SentencePiece as a crucial tool in achieving effec-\ntive tokenization.\nIn LLaMA2, a significant part of the training data is in \nEnglish, with a vocabulary consisting of only 315 Tibetan \ntokens. This has resulted in significant disparities in LLa-\nMA2's comprehension and generation capabilities in Tibetan \ncompared to English and Chinese. To address this issue, we \nutilized SentencePiece to generate vocabularies of varying \nsizes on our corpus. Subsequently, we fine-tuned the model \nseparately on each of the expanded vocabularies derived \nfrom the original vocabulary of LLaMA2 to choose a better \nmodel.\nHowever, during the direct generation of the vocabulary \nwith SentencePiece, two issues surfaced. Firstly, some words \nended with , which serves as a punctuation symbol in \nTibetan, making it inappropriate. Secondly, certain charac-\nter components were segmented as individual tokens in the \nvocabulary, increasing coverage but posing challenges for \nword segmentation. To address these issues, we removed \nclearly inappropriate cases during the vocabulary genera-\ntion process.\n Experiments and Results\n Fine‑tune of LLaMA2\nLoRA has proven to be a highly efficient tool for fine-tuning \npre-trained models. Different ranks can influence the mod-\nel’s performance post fine-tuning. To identify the optimal \nmodel, we conducted separate fine-tuning experiments with \nranks r = 1, r = 8, and r = 16. These fine-tuning tasks were \nexecuted on 8 NVIDIA A100 GPUs, with each task complet-\ning within one day. Figure 3 illustrates the loss curves of the \nthree models with different ranks. The experimental results \nindicate that as the rank increases, the rate of loss reduction \nexhibits a slight increment. To ascertain the influence of \ndifferent ranks on model performance, we conducted tests \non the text classification task with various rank configura-\ntions. Our results suggest that increasing the rank does not \nsignificantly enhance the model’s performance, as shown in \nTable 2. From this series of experiments, we obtained the \nhighest accuracy of 77.17% at a rank of r = 8, which is high-\nlighted in Table  2 (in bold). This model was then selected \nfor further validation.\nFurthermore, in large language models, the size of the \nvocabulary is another crucial factor that significantly impacts \nthe model's performance. To assess the influence of vocabu-\nlary size on model performance, we utilized SentencePiece \nto generate separate vocabularies containing 2000, 4000, \n6000, 8000, 10,000, and 12,000 tokens, respectively. These \nvocabularies were then merged with the original vocabulary \nof the LLaMA2 model and tested in the text classification \ntask. The results showcase the effectiveness of the additional \nTibetan vocabulary, and we achieved the best performance \nwith an additional vocabulary size of 8000, as highlighted \nin Table 3 (in bold).\nFig. 1  Tibetan syllable structure\nFig. 2  Tibetan Sentence Structure\nComplex & Intelligent Systems (2025) 11:72 \n Page 5 of 11 72\nOur experiments unveiled a lack of linear improve-\nment in performance with the increasing vocabulary size, \nranging from 2000 to 12,000. Instead, we observed fluc-\ntuations in the model's performance. Notably, even after \nfine-tuning the text classification task on the original \nLLaMA2, the accuracy failed to meet satisfactory lev -\nels. However, following the enlargement of the vocabu-\nlary, significant performance enhancement was evident, \neven with a vocabulary size of 2000. And the vocabulary \nexpanded by 8000 Tibetan tokens attained the highest per -\nformance in the fine-tuned text classification task. Despite \nthe limitations inherent in using performance from a single \ntask as a basis, we opted for the vocabulary size of 8000 \nfor subsequent experiments, given the results of the text \nclassification task.\n Text classification\nText classification, a fundamental task in NLP, finds appli-\ncations in sentiment analysis, spam detection, topic cate -\ngorization, and more. To evaluate the performance of our \ntrained T-LLaMA model, we selected text classification as \nthe natural language understanding task. The chosen data -\nset is TNCC, a publicly available dataset as referenced in \n[38], comprising 12 classes, with data distribution depicted \nin Fig. 4.\nThe fine-tuning for this task was accomplished using \nLoRA within 2 h on our 8 NVIDIA A100 GPUs. The clas-\nsification model, based on LLaMA2, prepared for fine-tun-\ning in downstream tasks by adding a sequence classification \nhead on top of LLaMA2. The code implementation lever -\naged the AutoModelForSequenceClassification module pro-\nvided by Hugging Face.\nOur training parameter settings are outlined in Table  4. \nWe conducted three sets of experiments on different ranks of \nT-LLaMA, with each set containing three experiments with \nvarying max sequence lengths.\nOur evaluation of text classification results on the TNCC \ndataset, when compared to publicly available benchmarks, \nhighlights the superior performance of our model in this \ntask (Table  5). The experimental results indicate that an \nincrease in the max sequence length generally leads to \nimproved performance, while an increase in rank does not \nconsistently result in performance enhancement. TLLaMA \nexhibited optimal performance with a configuration of r = 8 \nand msl = 1536, achieving an impressive accuracy rate of \n79.8%, as highlighted in Table  5 (in bold). Furthermore, a \ncomprehensive analysis of the confusion matrix is visualized \nFig. 3  The training loss during the fine-tune task of different rank\nTable 2  TNCC text classification results with different rank configu-\nration\nrank Accuracy (%) Macro-F1 (%)\n1 76.52 74.94\n8 77.17 75.9\n16 75.98 74.05\nTable 3  TNCC text classification results with varied extra Tibetan \ntoken counts\nExtra Tibetan token \ncounts\nAccuracy (%) Macro-F1 (%)\n0 68.3 68.3\n2000 78.61 78.64\n4000 77.96 77.91\n6000 78.94 79.11\n8000 79.8 79.77\n10,000 76.98 77.05\n12,000 78.72 78.73\nFig. 4  The data distribution of the Tibetan classification dataset\n Complex & Intelligent Systems (2025) 11:72\n72 Page 6 of 11\nin Fig. 5. The results underscore the efficacy of T-LLaMA \nin outperforming existing models in text classification tasks, \nparticularly in low-resource language scenarios like Tibetan.\nFrom the confusion matrix, we observe notable preci-\nsion and recall values for specific classes. For instance, \nClass Politics (0) demonstrates a precision of 82.27% and \na recall of 80.29%. Despite having the largest sample size \nin the dataset, Politics (0) does not exhibit the lowest per -\nformance among all categories. Conversely, classes Eco-\nnomics (1) and Tourism show precision of 67.96% and \n66.67%, respectively, making them influential categories \nin evaluating model performance. This is attributed to \nsample imbalance, as evident from the data distribution in \nFig. 4. Misclassifications for class Economics (1) are pri-\nmarily directed towards Politics (0), potentially stemming \nfrom the close ties between politics and the economy, as \npolitical news often includes elements of economic data.\nAccording to these experiments, T-LLaMA exhibits \nstrong performance in low-resource Tibetan text clas-\nsification tasks, particularly on the TNCC dataset. The \nmodel’s effectiveness, especially with optimized set-\ntings, highlights its potential for addressing challenges in \nresource-constrained language scenarios. The successful \nuse of LoRA technology underscores its practical utility. \nOpportunities for improvement include addressing data \nimbalances and further exploration of fine-tuning strat-\negies. Looking forward, T-LLaMA shows promise for \napplications in low-resource language contexts, with ongo-\ning opportunities for model refinement and enhancement.\n News text generation\nThe ability of LLM to generate text on specific topics is \na crucial aspect for various applications such as content \ncreation and automated writing. In this study, we sought \nto further validate the effectiveness of T-LLaMA by con-\nducting experiments on news text generation based on news \ntitles. We established a news text generation corpus using \nour extensive proprietary news dataset, consisting of a bil-\nlion Tibetan characters. Filtering out shorter news articles \n(less than 1024 characters), we retained 80% of the dataset, \nutilizing the titles as inputs and the corresponding contents \nas outputs.\nIn our experiment, we explored two types of prompts \ninspired by the structure of Stanford Alpaca for prompt tun-\ning using LoRA. The first prompt featured an instruction \nsection describing the task, with the title and content as input \nand output, respectively (Fig.  6a). The second prompt took \na different approach, utilizing the title as a direct instruction \nand the content as the output, with null input (Fig. 6b). And \nboth prompts underwent prompt tuning with LoRA, proving \neffective in this task.\nTable 4  Train parameters of text classification tasks\nParameters Value\nLoRA rank of T-LLaMA 8\nMax sequence length 512, 1024, 1536\nRank 2\nlora_alpha 32\nNum of epochs 20\nLearning rate 1e-4\nBatch size per device 1\nTable 5  Text classification results of TNCC, r means the rank of \nT-LLaMA and msl means the max sequence length of input text\nModels Accuracy (%) Macro-F1 (%)\nTransformers 28.63 28.79\nCNN(syllable) 61.51 57.34\nTextCNN 61.71 61.53\nDPCNN 62.91 61.17\nTextRCNN 63.67 62.81\nBERT-base-Tibetan – 51\nCINO-base 73.1 70\nCINO-large 76.3 73.7\nTiKEM 74.46 72.61\nT-LLaMA r = 8, msl = 512 77.17 75.90\nT-LLaMA r = 8, msl = 1024 79.02 77.35\nT-LLaMA r = 8, msl = 1536 79.8 79.77\nFig. 5  The confusion matrix of the text classification task of the \nmodel with r = 8 and msl = 1536. The numerical labels ranging from \n0 to 11 correspond to specific categories: Politics, Economics, Edu-\ncation, Tourism, Environment, Language, Literature, Religion, Arts, \nMedicine, Customs, and Instruments\nComplex & Intelligent Systems (2025) 11:72 \n Page 7 of 11 72\nConsidering that the primary beneficiaries of our devel-\noped Tibetan language model are users with constrained \ncomputational resources, we recognized the necessity of \nmodel quantization during the validation phase. Quanti-\nzation refers to the process of reducing the bit precision \nof model parameters to minimize storage requirements \nand computational overhead. By quantizing the model to \nlower bit precision, we can enhance inference efficiency on \nresource-constrained systems. This becomes particularly \ncrucial in scenarios where resources are limited, such as \nwhen conducting inference on low-performance GPUs or \nCPUs. Thus the model's performance was evaluated after \nquantization using llama.cpp [39]. We randomly selected \n500 news headlines from the test dataset and generated cor-\nresponding news content for manual evaluation using the \nsecond prompt.\nThe manual evaluation revealed that the model exhib-\nited minimal grammatical errors and demonstrated strong \nalignment between the generated content and the headlines. \nTable 6 showcases two sets of samples with closely matched \ngenerated content and headlines. Conversely, Table  7 pre-\nsents two sets with poorer alignment. Out of the 500 sam-\nples, 51 were generated as empty lines. Almost all remaining \ndata exhibited correct grammar, with only one exception. \nDespite the labor-intensive nature of manual evaluation, it \nremains the most accurate assessment method. In terms of \ncontent relevance, 81% of the samples closely matched the \nFig. 6  The two different \nprompts we used in our experi-\nments\nTable 6  The generated content exhibits a strong alignment with the headline\n Complex & Intelligent Systems (2025) 11:72\n72 Page 8 of 11\nkeywords in the headlines. Among the remaining 19%, 10% \ncontained empty lines, while the other 9% did not closely \nalign with the headline keywords. These results underscore \nthe proficiency of T-LLaMA in generating news text, show-\ncasing its capabilities beyond classification tasks.\n Text summarization\nText summarization plays a pivotal role in NLP by address-\ning the need for efficient information extraction and compre-\nhension. In the vast sea of textual data available, summari-\nzation techniques enable the distillation of key information, \nfacilitating quicker and more effective understanding. This \nis particularly crucial in scenarios where the volume of text \nis overwhelming, such as news articles, research papers, and \nlengthy documents.\nThe dataset employed in this experiment comprises \n35,381 samples, each consisting of a lengthy text and 1–3 \ncorresponding summaries. To adapt this dataset for prompt \nlearning, samples containing multiple summaries were split \ninto individual instances. Utilizing the prompt structure \ndepicted in Fig. 7, a total of 92,906 samples were generated. \nHalf of these were selected as the training set. The hyper-\nparameters of this task is set as shown in Table  8 and the \ntraining duration for this task was approximately 40 min on \n8 A100 GPUs.\nTable 7  The generated content does not have a strong match with the headline\nFig. 7  The prompt we used in \nour text summarization experi-\nments\nTable 8  Train parameters of \ntext summarization tasks Parameters Value\nMax sequence length 1024\nRank 16\nlora_alpha 128\nNum of epochs 1\nLearning rate 1e-5\nBatch size per device 8\nTable 9  ROUGE Scores\nMetric F1 Recall Precision\nROUGE-1 53.26 42.25 72.0\nROUGE-2 15.89 12.60 21.49\nROUGE-L 24.54 19.47 33.18\nComplex & Intelligent Systems (2025) 11:72 \n Page 9 of 11 72\nTo validate the effectiveness of our model, we employed \nthe Recall-Oriented Understudy for Gisting Evaluation \n(ROUGE) metrics for the evaluation of text summarization \ntask [40]. ROUGE constitutes a set of automatic evaluation \nmetrics designed to gauge the quality of text summaries, \nwith a primary emphasis on word overlap between the gen-\nerated summary and the reference summary. The various \nsub-metrics within ROUGE include ROUGE-1 (single-\nword overlap), ROUGE-2 (bigram overlap), and ROUGE-L \n(length of the longest common subsequence). The results \nof the evaluation on a subset of 500 samples from the test \nset are presented in Table  9. Table 10 illustrates a case of \nour model performing text summarization. The prelimi-\nnary results of our experiments indicate that our model has \nachieved satisfactory performance in this task.\n Conclusion\nIn this study, we presented T-LLaMA, a large language \nmodel fine-tuned for Tibetan language processing, with a \nfocus on text classification, news text generation and text \nsummarization. The experiments conducted on the TNCC \nshowcased TLLaMA’s robust performance in text classifica-\ntion, surpassing existing models with an accuracy of up to \n79.8%. Moreover, T-LLaMA demonstrated its proficiency \nin news text generation, leveraging a proprietary dataset and \ntwo distinct prompts for prompt tuning with LoRA. Manual \nevaluations of the generated content revealed minimal gram-\nmatical errors and a strong alignment with news headlines. \nAnd we have also achieved satisfactory results in the text \nsummarization task.\nThe incorporation of LoRA technology proved instru-\nmental in fine-tuning T-LLaMA, enabling effective training \nin resource-constrained environments. The model’s excep-\ntional performance highlights the significant potential of \nlarge language models in advancing natural language pro-\ncessing tasks for minority languages. Although we have \nachieved good results in several downstream tasks using \nlarge language models, there is currently a lack of specific \nevaluation methods for Tibetan natural language processing \nin comparison to the established evaluation approaches for \nmainstream large language models.\nLooking ahead, our focus is on constructing datasets \nthat cover a wider array of downstream tasks, allowing \nthe model to demonstrate its effectiveness across various \nTibetan natural language processing challenges. Addition-\nally, there are promising directions for future research, \nincluding delving into hyperparameter tuning, enhancing \nmodel interpretability, and exploring applications in addi-\ntional low-resource language tasks. T-LLaMA’s notable \nachievements in text classification, news text generation, \nand text summarization position it as a valuable asset for \nthe advancement of Tibetan NLP. Furthermore, its con-\ntributions extend to the broader landscape of other low-\nresource languages, signifying its importance in driving \nprogress in these linguistic domains. We anticipate that the \nrelease of the T-LLaMA model will significantly contrib-\nute to the advancement of the Tibetan NLP community.\nAcknowledgements This work was partially supported by National \nNatural Science Foundation of China under Grant No. U22A20261 \nand 61402210, National Key R & D Program of China under Grant \nNo. 2020YFC0832500, National Natural Science Foundation of China \nunder Grant No. 62266037, the Fundamental Research Funds for the \nCentral Universities under Grant No. lzujbky2022-kb12Science and \nTechnology Plan of Qinghai Province under Grant No.2020-GX-164, \nand Natural Science Foundation of Gansu Province under Grant No. \n22JR5RA186.\nTable 10  A case of our model performing text summarization\n Complex & Intelligent Systems (2025) 11:72\n72 Page 10 of 11\nData availability The classification data used in this study are pub-\nlicly available from the Tibetan News Classification Corpus (TNCC) \nat https:// github. com/ Fudan NLP/ Tibet an- Class ifica tion.\nDeclarations \nConflict of interest Corresponding authors declare on behalf of all au-\nthors that there is no conflict of interest. We declare that we do not \nhave any commercial or associative interest that represents a conflict \nof interest in connection with the work submitted.\nOpen Access  This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit \nto the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material \nderived from this article or parts of it. The images or other third party \nmaterial in this article are included in the article’s Creative Commons \nlicence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons licence and \nyour intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http:// creat iveco \nmmons. org/ licen ses/ by- nc- nd/4. 0/.\nReferences\n 1. OpenAI. (2022) ChatGPT [GPT-3.5]. https:// openai. com/ chatg pt\n 2. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lac-\nroix T, Lample G (2023) Llama: Open and efficient foundation \nlanguage models. arXiv preprint. arXiv: 2302.13971\n 3. Bai J, Bai S, Chu Y, Cui Z, Dang K, Deng X, Zhu T (2023) Qwen \ntechnical report. arXiv preprint. arXiv: 2309.16609\n 4. Cui Y, Yang Z, Yao X (2023) Efficient and effective text encoding \nfor chinese llama and alpaca. arXiv preprint. arXiv: 2304.08177\n 5. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, \nScialom T (2023) Llama 2: Open foundation and fine-tuned chat \nmodels. arXiv preprint. arXiv: 2307.09288\n 6. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Chen W \n(2021) Lora: Low-rank adaptation of large language models. \narXiv preprint. arXiv: 2106.09685\n 7. Kudo T, Richardson J (2018) Sentencepiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neural \ntext processing. arXiv preprint. arXiv: 1808.06226\n 8. Qun N, Li X, Qiu X, Huang X (2017) End-to-end neural text \nclassification for tibetan. In: Chinese computational linguistics \nand natural language processing based on naturally annotated big \ndata, pp 472–480\n 9. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhari-\nwal P, Amodei D (2020) Language models are few-shot learn-\ners. In: Advances in neural information processing systems, pp \n1877–1901\n 10. Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts \nA, Fiedel N (2023) Palm: scaling language modeling with path-\nways. J Mach Learn Res 24(240):1–113\n 11. Sun X, Li X, Li J, Wu F, Guo S, Zhang T, Wang G (2023) Text \nclassification via large language models. arXiv preprint. arXiv: \n2305.08377\n 12. Kasai J, Sakaguchi K, Le Bras R, Asai A, Yu X, Radev D, Inui K \n(2024) RealTime QA: what's the answer right now?. In: Advances \nin neural information processing systems, vol 36\n 13. Kamalloo E, Dziri N, Clarke CL, Rafiei D (2023) Evaluating \nopen-domain question answering in the era of large language \nmodels. arXiv preprint. arXiv: 2305.06984\n 14. Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Wen JR \n(2023) A survey of large language models. arXiv preprint. arXiv: \n2303.18223\n 15. Xiao L, Shan X, Chen X (2023) PatternGPT: a pattern-driven \nframework for large language model text generation. In: Proceed-\nings of the 2023 12th International Conference on Computing and \nPattern Recognition, pp 72–78\n 16. Xie Q, Bishop JA, Tiwari P, Ananiadou S (2022) Pre-trained lan-\nguage models with domain knowledge for biomedical extractive \nsummarization. Knowl-Based Syst 252:109460\n 17. Houlsby N, Giurgiu A, Jastrzebski S, Morrone B, De Laroussilhe \nQ, Gesmundo A, Gelly S (2019) Parameter-efficient transfer learn-\ning for NLP. In: International Conference on machine learning, pp \n2790–2799\n 18. Li XL, Liang P (2021) Prefix-tuning: Optimizing continuous \nprompts for generation. arXiv preprint. arXiv: 2101.00190\n 19. Lester B, Al-Rfou R, Constant N (2021) The power of scale \nfor parameter-efficient prompt tuning. arXiv preprint. arXiv: \n2104.08691\n 20. Hedderich MA, Lange L, Adel H, Strötgen J, Klakow D (2020) \nA survey on recent approaches for natural language processing in \nlow-resource scenarios. arXiv preprint. arXiv: 2010.12309\n 21. Nguyen XP, Zhang W, Li X, Aljunied M, Tan Q, Cheng L, \nBing L (2023) SeaLLMs—large language models for Southeast \nAsia. arXiv preprint. arXiv: 2312.00738\n 22. Yang A, Xiao B, Wang B, Zhang B, Bian C, Yin C, Wu Z (2023) \nBaichuan 2: open large-scale language models. arXiv preprint. \narXiv: 2309.10305\n 23. Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-train-\ning of deep bidirectional transformers for language understanding. \narXiv preprint. arXiv: 1810.04805\n 24. Lan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R \n(2019) Albert: a lite bert for self-supervised learning of language \nrepresentations. arXiv preprint arXiv: 1909.11942\n 25. Radford A, Narasimhan K, Salimans T, Sutskever I (2018) \nImproving language understanding by generative pre-training\n 26. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) \nLanguage models are unsupervised multitask learners. OpenAI \nblog 1(8):9\n 27. Jiang S, Fu S, Lin N, Fu Y (2021) Pretrained models and eval-\nuation data for the Khmer language. Tsinghua Sci Technol \n27(4):709–718\n 28. Alghamdi A, Duan X, Jiang W, Wang Z, Wu Y, Xia Q, Ghaddar \nA (2023) Aramus: pushing the limits of data and model scale \nfor Arabic natural language processing. arXiv preprint. arXiv: \n2306.06800.\n 29. Li L (2020) Tibetan pre-trained model based on ALBERT and its \napplication. Lanzhou University\n 30. Liu S, Deng J, Sun Y, Zhao X (2022) Tibert: Tibetan pre-trained \nlanguage model. In: 2022 IEEE International Conference on Sys-\ntems, Man, and Cybernetics (SMC), pp 2956–2961\n 31. Yang Z, Xu Z, Cui Y, Wang B, Lin M, Wu D, Chen Z (2022) \nCINO: A Chinese minority pre-trained language model. arXiv \npreprint. arXiv: 2202.13558.\n 32. An B (2023) Prompt-based for low-resource Tibetan text clas-\nsification. ACM Trans Asian Low-Resour Lang Inform Process \n22(8):1–13\n 33. Zhou M, Daiqing Z, Qun N, Nyima T (2023) PEFTT: Parameter-\nEfficient Fine-Tuning for low-resource Tibetan pre-trained lan-\nguage models. arXiv preprint. arXiv: 2309.12109.\nComplex & Intelligent Systems (2025) 11:72 \n Page 11 of 11 72\n 34. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez \nAN, Polosukhin I (2017) Attention is all you need. Advances in \nneural information processing systems\n 35. Hoffmann J, Borgeaud S, Mensch A, Buchatskaya E, Cai T, \nRutherford E, Sifre L (2022) Training compute-optimal large \nlanguage models. arXiv preprint. arXiv: 2203.15556.\n 36. Cooper K (2023) OpenAI GPT-3: Everything you need to know. \n[Online]. Available: https:// www. sprin gboard. com/ blog/ data- scien \nce/ machi ne- learn ing- gpt-3- open- ai/. Accessed 17 May 2023\n 37. Yi Z, Wu Q, Yu J, Tang Y, Liu X, Peng L, Ma J (2022) Tibetan \nSyllable Prediction with Pre-trained Cross-lingual Language \nModel. In: 2022 IEEE 5th International Conference on computer \nand communication engineering technology (CCET), pp 162–166\n 38. Qun N, Li X, Qiu X, Huang X (2017) End-to-end neural text \nclassification for Tibetan. In: Chinese Computational Linguistics \nand Natural Language Processing Based on Naturally Annotated \nBig Data: 16th China National Conference, CCL 2017, and 5th \nInternational Symposium, pp 472–480.\n 39. Gerganov, Georgi (2022) llama.cpp. GitHub. https:// github. com/ \nggerg anov/ llama. cpp.\n 40. Lin CY, Hovy E (2003) Automatic evaluation of summaries using \nn-gram co-occurrence statistics. In: Proceedings of the 2003 \nHuman Language Technology Conference of the North Ameri-\ncan chapter of the association for computational linguistics, pp \n150–157.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}