{
  "title": "A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser Ney Smoothing",
  "url": "https://openalex.org/W2106433837",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A1893650370",
      "name": "Rene Pickhardt",
      "affiliations": [
        "University of Koblenz and Landau",
        "Universität Koblenz"
      ]
    },
    {
      "id": "https://openalex.org/A223387194",
      "name": "Thomas Gottron",
      "affiliations": [
        "Universität Koblenz",
        "University of Koblenz and Landau"
      ]
    },
    {
      "id": "https://openalex.org/A2021267997",
      "name": "Martin Korner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2599205895",
      "name": "Paul Georg Wagner",
      "affiliations": [
        "University of Koblenz and Landau",
        "Universität Koblenz"
      ]
    },
    {
      "id": "https://openalex.org/A2495284823",
      "name": "Till Speicher",
      "affiliations": [
        "University of Koblenz and Landau",
        "Universität Koblenz"
      ]
    },
    {
      "id": "https://openalex.org/A1941402522",
      "name": "Steffen Staab",
      "affiliations": [
        "University of Koblenz and Landau",
        "Universität Koblenz"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1605371805",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W190040066",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W2013196554",
    "https://openalex.org/W10704533",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2759336060",
    "https://openalex.org/W4233559841",
    "https://openalex.org/W1560013842",
    "https://openalex.org/W2119659342",
    "https://openalex.org/W2141440284",
    "https://openalex.org/W2951166594",
    "https://openalex.org/W2016871293",
    "https://openalex.org/W2123842387",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W1625582487"
  ],
  "abstract": "We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity.",
  "full_text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1145–1154,\nBaltimore, Maryland, USA, June 23-25 2014.c⃝2014 Association for Computational Linguistics\nA Generalized Language Model as the Combination of Skipped n-grams\nand Modiﬁed Kneser-Ney Smoothing\nRene Pickhardt, Thomas Gottron, Martin K ¨orner, Steffen Staab\nInstitute for Web Science and Technologies,\nUniversity of Koblenz-Landau, Germany\n{rpickhardt,gottron,mkoerner,staab}@uni-koblenz.de\nPaul Georg Wagner and Till Speicher\nTypology GbR\nmail@typology.de\nAbstract\nWe introduce a novel approach for build-\ning language models based on a system-\natic, recursive exploration of skip n-gram\nmodels which are interpolated using modi-\nﬁed Kneser-Ney smoothing. Our approach\ngeneralizes language models as it contains\nthe classical interpolation with lower or-\nder models as a special case. In this pa-\nper we motivate, formalize and present\nour approach. In an extensive empirical\nexperiment over English text corpora we\ndemonstrate that our generalized language\nmodels lead to a substantial reduction of\nperplexity between 3. 1% and 12. 7% in\ncomparison to traditional language mod-\nels using modiﬁed Kneser-Ney smoothing.\nFurthermore, we investigate the behaviour\nover three other languages and a domain\nspeciﬁc corpus where we observed consis-\ntent improvements. Finally, we also show\nthat the strength of our approach lies in\nits ability to cope in particular with sparse\ntraining data. Using a very small train-\ning data set of only 736 KB text we yield\nimprovements of even 25. 7% reduction of\nperplexity.\n1 Introduction motivation\nLanguage Models are a probabilistic approach for\npredicting the occurrence of a sequence of words.\nThey are used in many applications, e.g. word\nprediction (Bickel et al., 2005), speech recogni-\ntion (Rabiner and Juang, 1993), machine trans-\nlation (Brown et al., 1990), or spelling correc-\ntion (Mays et al., 1991). The task language models\nattempt to solve is the estimation of a probability\nof a given sequence of words wl\n1 = w1, . . . , w l.\nThe probability P(wl\n1) of this sequence can be\nbroken down into a product of conditional prob-\nabilities:\nP(wl\n1) =P(w1) ·P(w2|w1) ·. . . ·P(wl|w1 · · ·wl□1)\n=\nlY\ni=1\nP(wi|w1 · · ·wi□1) (1)\nBecause of combinatorial explosion and data\nsparsity, it is very difﬁcult to reliably estimate the\nprobabilities that are conditioned on a longer sub-\nsequence. Therefore, by making a Markov as-\nsumption the true probability of a word sequence\nis only approximated by restricting conditional\nprobabilities to depend only on a local context\nwi□1\ni□n+1 of n □ 1 preceding words rather than the\nfull sequence wi□1\n1 . The challenge in the construc-\ntion of language models is to provide reliable esti-\nmators for the conditional probabilities. While the\nestimators can be learnt—using, e.g., a maximum\nlikelihood estimator over n-grams obtained from\ntraining data—the obtained values are not very re-\nliable for events which may have been observed\nonly a few times or not at all in the training data.\nSmoothing is a standard technique to over-\ncome this data sparsity problem. Various smooth-\ning approaches have been developed and ap-\nplied in the context of language models. Chen\nand Goodman (Chen and Goodman, 1999) in-\ntroduced modiﬁed Kneser-Ney Smoothing, which\nup to now has been considered the state-of-the-\nart method for language modelling over the last\n15 years. Modiﬁed Kneser-Ney Smoothing is\nan interpolating method which combines the es-\ntimated conditional probabilities P(wi|wi□1\ni□n+1)\nrecursively with lower order models involving a\nshorter local context wi□1\ni□n+2 and their estimate for\nP(wi|wi□1\ni□n+2). The motivation for using lower\norder models is that shorter contexts may be ob-\nserved more often and, thus, suffer less from data\nsparsity. However, a single rare word towards the\nend of the local context will always cause the con-\ntext to be observed rarely in the training data and\nhence will lead to an unreliable estimation.\n1145\nBecause of Zipﬁan word distributions, most\nwords occur very rarely and hence their true prob-\nability of occurrence may be estimated only very\npoorly. One word that appears at the end of a local\ncontext wi□1\ni□n+1 and for which only a poor approx-\nimation exists may adversely affect the conditional\nprobabilities in language models of all lengths —\nleading to severe errors even for smoothed lan-\nguage models. Thus, the idea motivating our ap-\nproach is to involve several lower order models\nwhich systematically leave out one position in the\ncontext (one may think of replacing the affected\nword in the context with a wildcard) instead of\nshortening the sequence only by one word at the\nbeginning.\nThis concept of introducing gaps in n-grams\nis referred to as skip n-grams (Ney et al., 1994;\nHuang et al., 1993). Among other techniques, skip\nn-grams have also been considered as an approach\nto overcome problems of data sparsity (Goodman,\n2001). However, to best of our knowledge, lan-\nguage models making use of skip n-grams mod-\nels have never been investigated to their full ex-\ntent and over different levels of lower order mod-\nels. Our approach differs as we consider all pos-\nsible combinations of gaps in a local context and\ninterpolate the higher order model with all possi-\nble lower order models derived from adding gaps\nin all different ways.\nIn this paper we make the following contribu-\ntions:\n1. We provide a framework for using modiﬁed\nKneser-Ney smoothing in combination with a\nsystematic exploration of lower order models\nbased on skip n-grams.\n2. We show how our novel approach can indeed\neasily be interpreted as a generalized version\nof the current state-of-the-art language mod-\nels.\n3. We present a large scale empirical analysis\nof our generalized language models on eight\ndata sets spanning four different languages,\nnamely, a wikipedia-based text corpus and\nthe JRC-Acquis corpus of legislative texts.\n4. We empirically observe that introducing skip\nn-gram models may reduce perplexity by\n12. 7% compared to the current state-of-the-\nart using modiﬁed Kneser-Ney models on\nlarge data sets. Using small training data sets\nwe observe even higher reductions of per-\nplexity of up to 25. 6%.\nThe rest of the paper is organized as follows.\nWe start with reviewing related work in Section 2.\nWe will then introduce our generalized language\nmodels in Section 3. After explaining the evalua-\ntion methodology and introducing the data sets in\nSection 4 we will present the results of our evalu-\nation in Section 5. In Section 6 we discuss why a\ngeneralized language model performs better than\na standard language model. Finally, in Section 7\nwe summarize our ﬁndings and conclude with an\noverview of further interesting research challenges\nin the ﬁeld of generalized language models.\n2 Related Work\nWork related to our generalized language model\napproach can be divided in two categories: var-\nious smoothing techniques for language models\nand approaches making use of skip n-grams.\nSmoothing techniques for language models\nhave a long history. Their aim is to overcome data\nsparsity and provide more reliable estimators—in\nparticular for rare events. The Good Turing es-\ntimator (Good, 1953), deleted interpolation (Je-\nlinek and Mercer, 1980), Katz backoff (Katz,\n1987) and Kneser-Ney smoothing (Kneser and\nNey, 1995) are just some of the approaches to\nbe mentioned. Common strategies of these ap-\nproaches are to either backoff to lower order mod-\nels when a higher order model lacks sufﬁcient\ntraining data for good estimation, to interpolate\nbetween higher and lower order models or to inter-\npolate with a prior distribution. Furthermore, the\nestimation of the amount of unseen events from\nrare events aims to ﬁnd the right weights for in-\nterpolation as well as for discounting probability\nmass from unreliable estimators and to retain it for\nunseen events.\nThe state of the art is a modiﬁed version of\nKneser-Ney smoothing introduced in (Chen and\nGoodman, 1999). The modiﬁed version imple-\nments a recursive interpolation with lower order\nmodels, making use of different discount values\nfor more or less frequently observed events. This\nvariation has been compared to other smooth-\ning techniques on various corpora and has shown\nto outperform competing approaches. We will\nreview modiﬁed Kneser-Ney smoothing in Sec-\ntion 2.1 in more detail as we reuse some ideas to\ndeﬁne our generalized language model.\n1146\nSmoothing techniques which do not rely on us-\ning lower order models involve clustering (Brown\net al., 1992; Ney et al., 1994), i.e. grouping to-\ngether similar words to form classes of words, as\nwell as skip n-grams (Ney et al., 1994; Huang et\nal., 1993). Yet other approaches make use of per-\nmutations of the word order in n-grams (Schukat-\nTalamazzini et al., 1995; Goodman, 2001).\nSkip n-grams are typically used to incorporate\nlong distance relations between words. Introduc-\ning the possibility of gaps between the words in\nan n-gram allows for capturing word relations be-\nyond the level of n consecutive words without an\nexponential increase in the parameter space. How-\never, with their restriction on a subsequence of\nwords, skip n-grams are also used as a technique\nto overcome data sparsity (Goodman, 2001). In re-\nlated work different terminology and different def-\ninitions have been used to describe skip n-grams.\nVariations modify the number of words which can\nbe skipped between elements in an n-gram as well\nas the manner in which the skipped words are de-\ntermined (e.g. ﬁxed patterns (Goodman, 2001) or\nfunctional words (Gao and Suzuki, 2005)).\nThe impact of various extensions and smooth-\ning techniques for language models is investigated\nin (Goodman, 2001; Goodman, 2000). In partic-\nular, the authors compared Kneser-Ney smooth-\ning, Katz backoff smoothing, caching, clustering,\ninclusion of higher order n-grams, sentence mix-\nture and skip n-grams. They also evaluated com-\nbinations of techniques, for instance, using skip\nn-gram models in combination with Kneser-Ney\nsmoothing. The experiments in this case followed\ntwo paths: (1) interpolating a 5-gram model with\nlower order distribution introducing a single gap\nand (2) interpolating higher order models with\nskip n-grams which retained only combinations of\ntwo words. Goodman reported on small data sets\nand in the best case a moderate improvement of\ncross entropy in the range of 0. 02 to 0. 04.\nIn (Guthrie et al., 2006), the authors investi-\ngated the increase of observed word combinations\nwhen including skips in n-grams. The conclusion\nwas that using skip n-grams is often more effective\nfor increasing the number of observations than in-\ncreasing the corpus size. This observation aligns\nwell with our experiments.\n2.1 Review of Modiﬁed Kneser-Ney\nSmoothing\nWe brieﬂy recall modiﬁed Kneser-Ney Smoothing\nas presented in (Chen and Goodman, 1999). Mod-\niﬁed Kneser-Ney implements smoothing by inter-\npolating between higher and lower order n-gram\nlanguage models. The highest order distribution\nis interpolated with lower order distribution as fol-\nlows:\nPMKN(wi|wi□1\ni□n+1) =\nmax{c(wi\ni□n+1) − D(c(wi\ni□n+1)), 0}\nc(wi□1\ni□n+1)\n+ γhigh(wi□1\ni□n+1) ˆPMKN(wi|wi□1\ni□n+2) (2)\nwhere c(wi\ni□n+1) provides the frequency count\nthat sequence wi\ni□n+1 occurs in training data, D is\na discount value (which depends on the frequency\nof the sequence) and γhigh depends on D and is the\ninterpolation factor to mix in the lower order dis-\ntribution1. Essentially, interpolation with a lower\norder model corresponds to leaving out the ﬁrst\nword in the considered sequence. The lower order\nmodels are computed differently using the notion\nof continuation counts rather than absolute counts:\nˆPMKN(wi|(wi□1\ni□n+1)) =\nmax{N1+(•wi\ni□n+1) − D(c(wi\ni□n+1)), 0}\nN1+(•wi□1\ni□n+1•)\n+ γmid(wi□1\ni□n+1) ˆPMKN(wi|wi□1\ni□n+2)) (3)\nwhere the continuation counts are deﬁned as\nN1+(•wi\ni□n+1) = |{wi□n : c(wi\ni□n) > 0}|, i.e.\nthe number of different words which precede the\nsequence wi\ni□n+1. The term γmid is again an inter-\npolation factor which depends on the discounted\nprobability mass D in the ﬁrst term of the for-\nmula.\n3 Generalized Language Models\n3.1 Notation for Skip n-gram with k Skips\nWe express skip n-grams using an operator no-\ntation. The operator ∂i applied to an n-gram\nremoves the word at the i-th position. For in-\nstance: ∂3w1w2w3w4 = w1w2 w4, where is\nused as wildcard placeholder to indicate a re-\nmoved word. The wildcard operator allows for\n1The factors γ and D are quite technical and lengthy. As\nthey do not play a signiﬁcant role for understanding our novel\napproach we refer to Appendix A for details.\n1147\nlarger number of matches. For instance, when\nc(w1w2w3aw4) =x and c(w1w2w3bw4) =y then\nc(w1w2 w4) ≥ x + y since at least the two se-\nquences w1w2w3aw4 and w1w2w3bw4 match the\nsequence w1w2 w4. In order to align with stan-\ndard language models the skip operator applied to\nthe ﬁrst word of a sequence will remove the word\ninstead of introducing a wildcard. In particular the\nequation ∂1wi\ni□n+1 = wi\ni□n+2 holds where the\nright hand side is the subsequence of wi\ni□n+1 omit-\nting the ﬁrst word. We can thus formulate the in-\nterpolation step of modiﬁed Kneser-Ney smooth-\ning using our notation as ˆPMKN(wi|wi□1\ni□n+2) =\nˆPMKN(wi|∂1wi□1\ni□n+1).\nThus, our skip n-grams correspond to n-grams\nof which we only use k words, after having applied\nthe skip operators ∂i1 . . . ∂ in□k\n3.2 Generalized Language Model\nInterpolation with lower order models is motivated\nby the problem of data sparsity in higher order\nmodels. However, lower order models omit only\nthe ﬁrst word in the local context, which might not\nnecessarily be the cause for the overall n-gram to\nbe rare. This is the motivation for our general-\nized language models to not only interpolate with\none lower order model, where the ﬁrst word in a\nsequence is omitted, but also with all other skip n-\ngram models, where one word is left out. Combin-\ning this idea with modiﬁed Kneser-Ney smoothing\nleads to a formula similar to (2).\nPGLM(wi|wi□1\ni□n+1) =\nmax{c(wi\ni□n+1) − D(c(wi\ni□n+1)), 0}\nc(wi□1\ni□n+1)\n+ γhigh(wi□1\ni□n+1)\nn□1X\nj=1\n1\nn− 1\nˆPGLM(wi|∂jwi□1\ni□n+1)\n(4)\nThe difference between formula (2) and formula\n(4) is the way in which lower order models are\ninterpolated.\nNote, the sum over all possible positions in\nthe context wi□1\ni□n+1 for which we can skip a\nword and the according lower order models\nPGLM(wi|∂j(wi□1\ni□n+1)). We give all lower order\nmodels the same weight 1\nn□1 .\nThe same principle is recursively applied in the\nlower order models in which some words of the\nfull n-gram are already skipped. As in modi-\nﬁed Kneser-Ney smoothing we use continuation\ncounts for the lower order models, incorporating\nthe skip operator also for these counts. Incor-\nporating this directly into modiﬁed Kneser-Ney\nsmoothing leads in the second highest model to:\nˆPGLM(wi|∂j(wi□1\ni□n+ 1)) = (5)\nmax{N1+(∂j(wi\ni□n)) − D(c(∂j(wi\ni□n+ 1))), 0}\nN1+(∂j(wi□1\ni□n+ 1)•)\n+γmid(∂j(wi□1\ni□n+ 1))\nn□1X\nk=1\nk̸ =j\n1\nn− 2\nˆPGLM(wi|∂j∂k(wi□1\ni□n+ 1))\nGiven that we skip words at different positions,\nwe have to extend the notion of the count function\nand the continuation counts. The count function\napplied to a skip n-gram is given by c(∂j(wi\ni□n))=∑\nwj c(wi\ni□n), i.e. we aggregate the count informa-\ntion over all words which ﬁll the gap in the n-\ngram. Regarding the continuation counts we de-\nﬁne:\nN1+(∂j(wi\ni□n)) = |{wi□n+j□1 :c(wi\ni□n)>0}| (6)\nN1+(∂j(wi□1\ni□n)•) = |{(wi□n+j□1, w i): c(wi\ni□n)>0}| (7)\nAs lowest order model we use—just as done for\ntraditional modiﬁed Kneser-Ney (Chen and Good-\nman, 1999)—a unigram model interpolated with a\nuniform distribution for unseen words.\nThe overall process is depicted in Figure 1, il-\nlustrating how the higher level models are recur-\nsively smoothed with several lower order ones.\n4 Experimental Setup and Data Sets\nTo evaluate the quality of our generalized lan-\nguage models we empirically compare their abil-\nity to explain sequences of words. To this end we\nuse text corpora, split them into test and training\ndata, build language models as well as generalized\nlanguage models over the training data and apply\nthem on the test data. We employ established met-\nrics, such as cross entropy and perplexity. In the\nfollowing we explain the details of our experimen-\ntal setup.\n4.1 Data Sets\nFor evaluation purposes we employed eight differ-\nent data sets. The data sets cover different domains\nand languages. As languages we considered En-\nglish ( en), German ( de), French ( fr), and Italian\n(it). As general domain data set we used the full\ncollection of articles from Wikipedia ( wiki) in the\ncorresponding languages. The download dates of\nthe dumps are displayed in Table 1.\n1148\nFigure 1: Interpolation of models of different or-\nder and using skip patterns. The value of n in-\ndicates the length of the raw n-grams necessary\nfor computing the model, the value of k indicates\nthe number of words actually used in the model.\nThe wild card symbol marks skipped words in\nan n-gram. The arrows indicate how a higher or-\nder model is interpolated with lower order mod-\nels which skips one word. The bold arrows cor-\nrespond to interpolation of models in traditional\nmodiﬁed Kneser-Ney smoothing. The lighter ar-\nrows illustrate the additional interpolations intro-\nduced by our generalized language models.\nde en fr it\nNov 22 nd Nov 04 th Nov 20 th Nov 25 th\nTable 1: Download dates of Wikipedia snapshots\nin November 2013.\nSpecial purpose domain data are provided by\nthe multi-lingual JRC-Acquis corpus of legislative\ntexts ( JRC) (Steinberger et al., 2006). Table 2\ngives an overview of the data sets and provides\nsome simple statistics of the covered languages\nand the size of the collections.\nStatistics\nCorpus total words unique words\nin Mio. in Mio.\nwiki-de 579 9.82\nJRC-de 30.9 0.66\nwiki-en 1689 11.7\nJRC-en 39.2 0.46\nwiki-fr 339 4.06\nJRC-fr 35.8 0.46\nwiki-it 193 3.09\nJRC-it 34.4 0.47\nTable 2: Word statistics and size of of evaluation\ncorpora\nThe data sets come in the form of structured text\ncorpora which we cleaned from markup and tok-\nenized to generate word sequences. We ﬁltered the\nword tokens by removing all character sequences\nwhich did not contain any letter, digit or common\npunctuation marks. Eventually, the word token se-\nquences were split into word sequences of length\nn which provided the basis for the training and\ntest sets for all algorithms. Note that we did not\nperform case-folding nor did we apply stemming\nalgorithms to normalize the word forms. Also,\nwe did our evaluation using case sensitive training\nand test data. Additionally, we kept all tokens for\nnamed entities such as names of persons or places.\n4.2 Evaluation Methodology\nAll data sets have been randomly split into a train-\ning and a test set on a sentence level. The train-\ning sets consist of 80% of the sentences, which\nhave been used to derive n-grams, skip n-grams\nand corresponding continuation counts for values\nof n between 1 and 5. Note that we have trained\na prediction model for each data set individually.\nFrom the remaining 20% of the sequences we have\nrandomly sampled a separate set of 100, 000 se-\nquences of 5 words each. These test sequences\nhave also been shortened to sequences of length 3,\nand 4 and provide a basis to conduct our ﬁnal ex-\nperiments to evaluate the performance of the dif-\nferent algorithms.\nWe learnt the generalized language models on\nthe same split of the training corpus as the stan-\ndard language model using modiﬁed Kneser-Ney\nsmoothing and we also used the same set of test se-\nquences for a direct comparison. To ensure rigour\nand openness of research the data set for training\nas well as the test sequences and the entire source\ncode is open source. 2 3 4 We compared the\nprobabilities of our language model implementa-\ntion (which is a subset of the generalized language\nmodel) using KN as well as MKN smoothing with\nthe Kyoto Language Model Toolkit 5. Since we\ngot the same results for small n and small data sets\nwe believe that our implementation is correct.\nIn a second experiment we have investigated\nthe impact of the size of the training data set.\nThe wikipedia corpus consists of 1. 7 bn. words.\n2http://west.uni-koblenz.de/Research\n3https://github.com/renepickhardt/generalized-language-\nmodeling-toolkit\n4http://glm.rene-pickhardt.de\n5http://www.phontron.com/kylm/\n1149\nThus, the 80% split for training consists of 1. 3 bn.\nwords. We have iteratively created smaller train-\ning sets by decreasing the split factor by an order\nof magnitude. So we created 8% / 92% and 0. 8%\n/ 99. 2% split, and so on. We have stopped at the\n0. 008%/ 99. 992% split as the training data set in\nthis case consisted of less words than our 100k\ntest sequences which we still randomly sampled\nfrom the test data of each split. Then we trained\na generalized language model as well as a stan-\ndard language model with modiﬁed Kneser-Ney\nsmoothing on each of these samples of the train-\ning data. Again we have evaluated these language\nmodels on the same random sample of 100, 000\nsequences as mentioned above.\n4.3 Evaluation Metrics\nAs evaluation metric we use perplexity: a standard\nmeasure in the ﬁeld of language models (Manning\nand Sch ¨utze, 1999). First we calculate the cross\nentropy of a trained language model given a test\nset using\nH(Palg) =□\n∑\ns∈T\nPMLE(s) ·log2 Palg(s) (8)\nWhere Palg will be replaced by the probability\nestimates provided by our generalized language\nmodels and the estimates of a language model us-\ning modiﬁed Kneser-Ney smoothing. PMLE, in-\nstead, is a maximum likelihood estimator of the\ntest sequence to occur in the test corpus. Finally,\nT is the set of test sequences. The perplexity is\ndeﬁned as:\nPerplexity(Palg) = 2H(Palg) (9)\nLower perplexity values indicate better results.\n5 Results\n5.1 Baseline\nAs a baseline for our generalized language model\n(GLM) we have trained standard language models\nusing modiﬁed Kneser-Ney Smoothing (MKN).\nThese models have been trained for model lengths\n3 to 5. For unigram and bigram models MKN and\nGLM are identical.\n5.2 Evaluation Experiments\nThe perplexity values for all data sets and various\nmodel orders can be seen in Table 3. In this table\nwe also present the relative reduction of perplexity\nin comparison to the baseline.\nmodel length\nExperiments n = 3 n = 4 n = 5\nwiki-de MKN 1074.1 778.5 597.1\nwiki-de GLM 1031.1 709.4 521.5\nrel. change 4.0% 8.9% 12.7%\nJRC-de MKN 235.4 138.4 94.7\nJRC-de GLM 229.4 131.8 86.0\nrel. change 2.5% 4.8% 9.2%\nwiki-en MKN 586.9 404 307.3\nwiki-en GLM 571.6 378.1 275\nrel. change 2.6% 6.1% 10.5%\nJRC-en MKN 147.2 82.9 54.6\nJRC-en GLM 145.3 80.6 52.5\nrel. change 1.3% 2.8% 3.9%\nwiki-fr MKN 538.6 385.9 298.9\nwiki-fr GLM 526.7 363.8 272.9\nrel. change 2.2% 5.7% 8.7%\nJRC-fr MKN 155.2 92.5 63.9\nJRC-fr GLM 153.5 90.1 61.7\nrel. change 1.1% 2.5% 3.5%\nwiki-it MKN 738.4 532.9 416.7\nwiki-it GLM 718.2 500.7 382.2\nrel. change 2.7% 6.0% 8.3%\nJRC-it MKN 177.5 104.4 71.8\nJRC-it GLM 175.1 101.8 69.6\nrel. change 1.3% 2.6% 3.1%\nTable 3: Absolute perplexity values and relative\nreduction of perplexity from MKN to GLM on all\ndata sets for models of order 3 to 5\nAs we can see, the GLM clearly outperforms\nthe baseline for all model lengths and data sets.\nIn general we see a larger improvement in perfor-\nmance for models of higher orders ( n = 5). The\ngain for 3-gram models, instead, is negligible. For\nGerman texts the increase in performance is the\nhighest ( 12. 7%) for a model of order 5. We also\nnote that GLMs seem to work better on broad do-\nmain text rather than special purpose text as the\nreduction on the wiki corpora is constantly higher\nthan the reduction of perplexity on the JRC cor-\npora.\nWe made consistent observations in our second\nexperiment where we iteratively shrank the size\nof the training data set. We calculated the rela-\ntive reduction in perplexity from MKN to GLM\n1150\nfor various model lengths and the different sizes\nof the training data. The results for the English\nWikipedia data set are illustrated in Figure 2.\nWe see that the GLM performs particularly well\non small training data. As the size of the training\ndata set becomes smaller (even smaller than the\nevaluation data), the GLM achieves a reduction of\nperplexity of up to 25. 7% compared to language\nmodels with modiﬁed Kneser-Ney smoothing on\nthe same data set. The absolute perplexity values\nfor this experiment are presented in Table 4.\nmodel length\nExperiments n = 3 n = 4 n = 5\n80% MKN 586.9 404 307.3\n80% GLM 571.6 378.1 275\nrel. change 2.6% 6.5% 10.5%\n8% MKN 712.6 539.8 436.5\n8% GLM 683.7 492.8 382.5\nrel. change 4.1% 8.7% 12.4%\n0. 8% MKN 894.0 730.0 614.1\n0. 8% GLM 838.7 650.1 528.7\nrel. change 6.2% 10.9% 13.9%\n0. 08% MKN 1099.5 963.8 845.2\n0. 08% GLM 996.6 820.7 693.4\nrel. change 9.4% 14.9% 18.0%\n0. 008% MKN 1212.1 1120.5 1009.6\n0. 008% GLM 1025.6 875.5 750.3\nrel. change 15.4% 21.9% 25.7%\nTable 4: Absolute perplexity values and relative\nreduction of perplexity from MKN to GLM on\nshrunk training data sets for the English Wikipedia\nfor models of order 3 to 5\nOur theory as well as the results so far suggest\nthat the GLM performs particularly well on sparse\ntraining data. This conjecture has been investi-\ngated in a last experiment. For each model length\nwe have split the test data of the largest English\nWikipedia corpus into two disjoint evaluation data\nsets. The data set unseen consists of all test se-\nquences which have never been observed in the\ntraining data. The set observed consists only of\ntest sequences which have been observed at least\nonce in the training data. Again we have calcu-\nlated the perplexity of each set. For reference, also\nthe values of the complete test data set are shown\nin Table 5.\nmodel length\nExperiments n = 3 n = 4 n = 5\nMKNcomplete 586.9 404 307.3\nGLMcomplete 571.6 378.1 275\nrel. change 2.6% 6.5% 10.5%\nMKNunseen 14696.8 2199.8 846.1\nGLMunseen 13058.7 1902.4 714.4\nrel. change 11.2% 13.5% 15.6%\nMKNobserved 220.2 88.0 43.4\nGLMobserved 220.6 88.3 43.5\nrel. change □0. 16% □0. 28% □0. 15%\nTable 5: Absolute perplexity values and relative\nreduction of perplexity from MKN to GLM for the\ncomplete and split test ﬁle into observed and un-\nseen sequences for models of order 3 to 5. The\ndata set is the largest English Wikipedia corpus.\nAs expected we see the overall perplexity values\nrise for the unseen test case and decline for the ob-\nserved test case. More interestingly we see that the\nrelative reduction of perplexity of the GLM over\nMKN increases from 10. 5% to 15. 6% on the un-\nseen test case. This indicates that the superior per-\nformance of the GLM on small training corpora\nand for higher order models indeed comes from its\ngood performance properties with regard to sparse\ntraining data. It also conﬁrms that our motivation\nto produce lower order n-grams by omitting not\nonly the ﬁrst word of the local context but system-\natically all words has been fruitful. However, we\nalso see that for the observed sequences the GLM\nperforms slightly worse than MKN. For the ob-\nserved cases we ﬁnd the relative change to be neg-\nligible.\n6 Discussion\nIn our experiments we have observed an im-\nprovement of our generalized language models\nover classical language models using Kneser-Ney\nsmoothing. The improvements have been ob-\nserved for different languages, different domains\nas well as different sizes of the training data. In\nthe experiments we have also seen that the GLM\nperforms well in particular for small training data\nsets and sparse data, encouraging our initial mo-\ntivation. This feature of the GLM is of partic-\nular value, as data sparsity becomes a more and\nmore immanent problem for higher values of n.\nThis known fact is underlined also by the statis-\n1151\n0%\n5%\n10%\n15%\n20%\n25%\n30%\n0.1 1 10 100 1000\nrelativechangeinperplexity\ndatasetsize[miowords]\nRelativechangeofperplexityforGLMoverMKN\nMKN(baseline)forn=3,4,and5\nn=5\nn=4\nn=3\nFigure 2: Variation of the size of the training data on 100k test sequences on the English Wikipedia data\nset with different model lengths for GLM.\ntics shown in Table 6. The fraction of total n-\ngrams which appear only once in our Wikipedia\ncorpus increases for higher values of n. However,\nfor the same value of n the skip n-grams are less\nrare. Our generalized language models leverage\nthis additional information to obtain more reliable\nestimates for the probability of word sequences.\nwn\n1 total unique\nw1 0. 5% 64. 0%\nw1w2 5. 1% 68. 2%\nw1 w3 8. 0% 79. 9%\nw1 w4 9. 6% 72. 1%\nw1 w5 10. 1% 72. 7%\nw1w2w3 21. 1% 77. 5%\nw1 w3w4 28. 2% 80. 4%\nw1w2 w4 28. 2% 80. 7%\nw1 w4w5 31. 7% 81. 9%\nw1 w3 w5 35. 3% 83. 0%\nw1w2 w5 31. 5% 82. 2%\nw1w2w3w4 44. 7% 85. 4%\nw1 w3w4w5 52. 7% 87. 6%\nw1w2 w4w5 52. 6% 88. 0%\nw1w2w3 w5 52. 3% 87. 7%\nw1w2w3w4w5 64. 4% 90. 7%\nTable 6: Percentage of generalized n-grams which\noccur only once in the English Wikipedia cor-\npus. Total means a percentage relative to the total\namount of sequences. Unique means a percentage\nrelative to the amount of unique sequences of this\npattern in the data set.\nBeyond the general improvements there is an\nadditional path for beneﬁtting from generalized\nlanguage models. As it is possible to better lever-\nage the information in smaller and sparse data sets,\nwe can build smaller models of competitive per-\nformance. For instance, when looking at Table 4\nwe observe the 3-gram MKN approach on the full\ntraining data set to achieve a perplexity of 586. 9.\nThis model has been trained on 7 GB of text and\nthe resulting model has a size of 15 GB and 742\nMio. entries for the count and continuation count\nvalues. Looking for a GLM with comparable but\nbetter performance we see that the 5-gram model\ntrained on 1% of the training data has a perplexity\nof 528. 7. This GLM model has a size of 9. 5 GB\nand contains only 427 Mio. entries. So, using a far\nsmaller set of training data we can build a smaller\nmodel which still demonstrates a competitive per-\nformance.\n7 Conclusion and Future Work\n7.1 Conclusion\nWe have introduced a novel generalized language\nmodel as the systematic combination of skip n-\ngrams and modiﬁed Kneser-Ney smoothing. The\nmain strength of our approach is the combination\nof a simple and elegant idea with an an empiri-\ncally convincing result. Mathematically one can\nsee that the GLM includes the standard language\nmodel with modiﬁed Kneser-Ney smoothing as a\nsub model and is consequently a real generaliza-\ntion.\nIn an empirical evaluation, we have demon-\nstrated that for higher orders the GLM outper-\nforms MKN for all test cases. The relative im-\nprovement in perplexity is up to 12. 7% for large\ndata sets. GLMs also performs particularly well\non small and sparse sets of training data. On a very\n1152\nsmall training data set we observed a reduction of\nperplexity by 25. 7%. Our experiments underline\nthat the generalized language models overcome in\nparticular the weaknesses of modiﬁed Kneser-Ney\nsmoothing on sparse training data.\n7.2 Future work\nA desirable extension of our current deﬁnition of\nGLMs will be the combination of different lower\nlower order models in our generalized language\nmodel using different weights for each model.\nSuch weights can be used to model the statistical\nreliability of the different lower order models. The\nvalue of the weights would have to be chosen ac-\ncording to the probability or counts of the respec-\ntive skip n-grams.\nAnother important step that has not been con-\nsidered yet is compressing and indexing of gen-\neralized language models to improve the perfor-\nmance of the computation and be able to store\nthem in main memory. Regarding the scalability\nof the approach to very large data sets we intend to\napply the Map Reduce techniques from (Heaﬁeld\net al., 2013) to our generalized language models in\norder to have a more scalable calculation.\nThis will open the path also to another interest-\ning experiment. Goodman (Goodman, 2001) ob-\nserved that increasing the length of n-grams in\ncombination with modiﬁed Kneser-Ney smooth-\ning did not lead to improvements for values of\nn beyond 7. We believe that our generalized\nlanguage models could still beneﬁt from such an\nincrease. They suffer less from the sparsity of\nlong n-grams and can overcome this sparsity when\ninterpolating with the lower order skip n-grams\nwhile beneﬁting from the larger context.\nFinally, it would be interesting to see how ap-\nplications of language models—like next word\nprediction, machine translation, speech recogni-\ntion, text classiﬁcation, spelling correction, e.g.—\nbeneﬁt from the better performance of generalized\nlanguage models.\nAcknowledgements\nWe would like to thank Heinrich Hartmann for\na fruitful discussion regarding notation of the\nskip operator for n-grams. The research lead-\ning to these results has received funding from the\nEuropean Community’s Seventh Framework Pro-\ngramme (FP7/2007-2013), REVEAL (Grant agree\nnumber 610928).\nReferences\nSteffen Bickel, Peter Haider, and Tobias Scheffer.\n2005. Predicting sentences using n-gram language\nmodels. In Proceedings of the conference on Hu-\nman Language Technology and Empirical Methods\nin Natural Language Processing, HLT ’05, pages\n193–200, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\nPeter F Brown, John Cocke, Stephen A Della Pietra,\nVincent J Della Pietra, Fredrick Jelinek, John D Laf-\nferty, Robert L Mercer, and Paul S Roossin. 1990.\nA statistical approach to machine translation. Com-\nputational linguistics, 16(2):79–85.\nPeter F. Brown, Peter V . deSouza, Robert L. Mer-\ncer, Vincent J. Della Pietra, and Jenifer C. Lai.\n1992. Class-based n-gram models of natural lan-\nguage. Comput. Linguist., 18(4):467–479, Decem-\nber.\nStanley Chen and Joshua Goodman. 1998. An em-\npirical study of smoothing techniques for language\nmodeling. Technical report, TR-10-98, Harvard\nUniversity, August.\nStanley Chen and Joshua Goodman. 1999. An\nempirical study of smoothing techniques for lan-\nguage modeling. Computer Speech & Language,\n13(4):359–393.\nJianfeng Gao and Hisami Suzuki. 2005. Long dis-\ntance dependency in language modeling: An em-\npirical study. In Keh-Yih Su, Junichi Tsujii, Jong-\nHyeok Lee, and OiYee Kwong, editors, Natural\nLanguage Processing IJCNLP 2004, volume 3248\nof Lecture Notes in Computer Science, pages 396–\n405. Springer Berlin Heidelberg.\nIrwin J. Good. 1953. The population frequencies of\nspecies and the estimation of population parameters.\nBiometrika, 40(3-4):237–264.\nJoshua T. Goodman. 2000. Putting it all together:\nlanguage model combination. In Acoustics, Speech,\nand Signal Processing, 2000. ICASSP ’00. Proceed-\nings. 2000 IEEE International Conference on, vol-\nume 3, pages 1647–1650 vol.3.\nJoshua T. Goodman. 2001. A bit of progress in lan-\nguage modeling – extended version. Technical Re-\nport MSR-TR-2001-72, Microsoft Research.\nDavid Guthrie, Ben Allison, Wei Liu, Louise Guthrie,\nand York Wilks. 2006. A closer look at skip-\ngram modelling. In Proceedings LREC’2006, pages\n1222–1225.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modiﬁed\nkneser-ney language model estimation. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics.\n1153\nXuedong Huang, Fileno Alleva, Hsiao-Wuen Hon,\nMei-Yuh Hwang, Kai-Fu Lee, and Ronald Rosen-\nfeld. 1993. The sphinx-ii speech recognition sys-\ntem: an overview. Computer Speech & Language,\n7(2):137 – 148.\nF. Jelinek and R.L. Mercer. 1980. Interpolated estima-\ntion of markov source parameters from sparse data.\nIn Proceedings of the Workshop on Pattern Recogni-\ntion in Practice, pages 381–397.\nS. Katz. 1987. Estimation of probabilities from sparse\ndata for the language model component of a speech\nrecognizer. Acoustics, Speech and Signal Process-\ning, IEEE Transactions on, 35(3):400–401.\nReinhard Kneser and Hermann Ney. 1995. Im-\nproved backing-off for m-gram language modeling.\nIn Acoustics, Speech, and Signal Processing, 1995.\nICASSP-95., 1995 International Conference on, vol-\nume 1, pages 181–184. IEEE.\nChristopher D. Manning and Hinrich Sch ¨utze. 1999.\nFoundations of statistical natural language process-\ning. MIT Press, Cambridge, MA, USA.\nEric Mays, Fred J Damerau, and Robert L Mercer.\n1991. Context based spelling correction. Informa-\ntion Processing & Management, 27(5):517–522.\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994.\nOn structuring probabilistic dependences in stochas-\ntic language modelling. Computer Speech & Lan-\nguage, 8(1):1 – 38.\nLawrence Rabiner and Biing-Hwang Juang. 1993.\nFundamentals of Speech Recognition. Prentice Hall.\nErnst-G¨unter Schukat-Talamazzini, R Hendrych, Ralf\nKompe, and Heinrich Niemann. 1995. Permugram\nlanguage models. In Fourth European Conference\non Speech Communication and Technology.\nRalf Steinberger, Bruno Pouliquen, Anna Widiger,\nCamelia Ignat, Tomaz Erjavec, Dan Tuﬁs, and\nDaniel Varga. 2006. The jrc-acquis: A multi-\nlingual aligned parallel corpus with 20+ languages.\nIn LREC’06: Proceedings of the 5th International\nConference on Language Resources and Evaluation.\nA Discount Values and Weights in\nModiﬁed Kneser Ney\nThe discount value D(c) used in formula (2) is de-\nﬁned as (Chen and Goodman, 1999):\nD(c) =\n8\n>>><\n>>>:\n0 if c = 0\nD1 if c = 1\nD2 if c = 2\nD3+ if c > 2\n(10)\nThe discounting values D1, D2, and D3+ are de-\nﬁned as (Chen and Goodman, 1998)\nD1 = 1− 2Y n2\nn1\n(11a)\nD2 = 2− 3Y n3\nn2\n(11b)\nD3+ = 3− 4Y n4\nn3\n(11c)\nwith Y = n1\nn1+n2\nand ni is the total number of n-\ngrams which appear exactly i times in the training\ndata. The weight γhigh(wi□1\ni□n+1) is deﬁned as:\nγhigh(wi□1\ni□n+ 1) = (12)\nD1N1(wi□1\ni□n+ 1•)+D2N2(wi□1\ni□n+ 1•)+D3+N3+(wi□1\ni□n+ 1•)\nc(wi□1\ni□n+ 1)\nAnd the weight γmid(wi□1\ni□n+1) is deﬁned as:\nγmid(wi□1\ni□n+ 1) = (13)\nD1N1(wi□1\ni□n+ 1•)+D2N2(wi□1\ni□n+ 1•)+D3+N3+(wi□1\ni□n+ 1•)\nN1+(•wi□1\ni□n+ 1•)\nwhere N1(wi□1\ni□n+1•), N2(wi□1\ni□n+1•), and\nN3+(wi□1\ni□n+1•) are analogously deﬁned to\nN1+(wi□1\ni□n+1•).\n1154",
  "topic": "Smoothing",
  "concepts": [
    {
      "name": "Smoothing",
      "score": 0.7359121441841125
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.47867685556411743
    },
    {
      "name": "Computer science",
      "score": 0.4519616365432739
    },
    {
      "name": "Linguistics",
      "score": 0.4039277732372284
    },
    {
      "name": "Philosophy",
      "score": 0.34936994314193726
    },
    {
      "name": "Mathematical economics",
      "score": 0.34868520498275757
    },
    {
      "name": "Art",
      "score": 0.33508944511413574
    },
    {
      "name": "Art history",
      "score": 0.32630792260169983
    },
    {
      "name": "Mathematics",
      "score": 0.2959630489349365
    },
    {
      "name": "Physics",
      "score": 0.1438763439655304
    },
    {
      "name": "Computer vision",
      "score": 0.06854337453842163
    },
    {
      "name": "Thermodynamics",
      "score": 0.06447726488113403
    }
  ]
}