{
    "title": "Towards Generalist Foundation Model for Radiology",
    "url": "https://openalex.org/W4386829267",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2413558386",
            "name": "Weidi Xie",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2290108068",
            "name": "Chaoyi Wu",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2107527362",
            "name": "Xiaoman Zhang",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2100362878",
            "name": "Ya Zhang",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2098837876",
            "name": "Yanfeng Wang",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225323055",
        "https://openalex.org/W2152772232",
        "https://openalex.org/W2990660335",
        "https://openalex.org/W1904878066",
        "https://openalex.org/W3134144764",
        "https://openalex.org/W2995225687",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2901466771",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W3132611748",
        "https://openalex.org/W4365143687",
        "https://openalex.org/W4385436420",
        "https://openalex.org/W3114128166",
        "https://openalex.org/W4220972931",
        "https://openalex.org/W3174657284",
        "https://openalex.org/W4306820534",
        "https://openalex.org/W2914203365",
        "https://openalex.org/W4296027312",
        "https://openalex.org/W2611650229",
        "https://openalex.org/W4321650237",
        "https://openalex.org/W4377121462",
        "https://openalex.org/W3169064633",
        "https://openalex.org/W4366197770",
        "https://openalex.org/W3203774985",
        "https://openalex.org/W4387211014",
        "https://openalex.org/W4379259189",
        "https://openalex.org/W4221106669",
        "https://openalex.org/W4385546024",
        "https://openalex.org/W4386768656",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W4377130955",
        "https://openalex.org/W4367623495",
        "https://openalex.org/W4392044798",
        "https://openalex.org/W4289489408",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W3165058054",
        "https://openalex.org/W3160840375",
        "https://openalex.org/W3101156210",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W2963466845",
        "https://openalex.org/W2338908902",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3195577433"
    ],
    "abstract": "<title>Abstract</title> In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of dataset construction, model design, and thorough evaluation. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases. To the best of our knowledge, this is the first large-scale, high-quality, medical visual-language dataset, with both 2D and 3D scans; (ii ), we propose an architecture that enables visually conditioned generative pre-training, i.e., allowing for integration of text input with 2D or 3D medical scans, and generate responses for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently fine-tuned on the domain-specific dataset, which is a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs, termed as RadMD; (iii), we propose a new evaluation benchmark, RadBench, that comprises five tasks, including modality recognition, disease diagnosis, visual question answering, report generation and rationale diagnosis, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems. We conduct both automatic and human evaluation on RadBench, in both cases, RadFM significantly outperforms existing multi-modal foundation models. The codes, data, and model checkpoint will all be made publicly available to promote further research and development in the field.",
    "full_text": "Towards Generalist Foundation Model for Radiology\nWeidi Xie¬† ( ÔÉ† weidi@sjtu.edu.cn )\nShanghai Jiao Tong University https://orcid.org/0009-0002-8609-6826\nChaoyi Wu¬†\nShanghai Jiao Tong University\nXiaoman Zhang¬†\nShanghai Jiao Tong University https://orcid.org/0000-0002-7696-9366\nYa Zhang¬†\nShanghai Jiaotong University\nYanfeng Wang¬†\nShanghai Jiao Tong University\nArticle\nKeywords:\nPosted Date: September 18th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3324530/v1\nLicense: Ôâû Ôìß This work is licensed under a Creative Commons Attribution 4.0 International License. ¬†\nRead Full License\nAdditional Declarations: There is NO Competing Interest.\nT owards Generalist F oundation Model for Radiology\nChaoyi W u‚àó,1,2, Xiaoman Zhang ‚àó,1,2,\nY a Zhang 1,2, Y anfeng W ang 1,2 and W eidi Xie 1,2\n1Shanghai Jiao T ong University 2Shanghai AI Laboratory\nhttps://chaoyi-wu.github.io/RadFM\nIn this study , we aim to initiate the development of Radiology Foundation Model, termed as RadFM.\nW e consider the construction of foundational models from the perspectives of dataset construction, model\ndesign, and thorough evaluation. Our contribution can be concluded as follows: ( i ), we construct a\nlarge-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans\nwith high-quality text descriptions or reports across various data formats, modalities, and tasks, covering\nover 5000 distinct diseases. T o the best of our knowledge, this is the Ô¨Årst large-scale, high-quality , medical\nvisual-language dataset, with both 2D and 3D scans; ( ii ), we propose an architecture that enables visually\nconditioned generative pre-training, i.e., allowing for integration of text input with 2D or 3D medical scans,\nand generate responses for diverse radiologic tasks. The model was initially pre-trained on MedMD and\nsubsequently Ô¨Åne-tuned on the domain-speciÔ¨Åc dataset, which is a radiologic cleaned version of MedMD,\ncontaining 3M radiologic visual-language pairs, termed as RadMD; ( iii ), we propose a new evaluation\nbenchmark, RadBench, that comprises Ô¨Åve tasks, including modality recognition, disease diagnosis,\nvisual question answering, report generation and rationale diagnosis, aiming to comprehensively assess the\ncapability of foundation models in handling practical clinical problems. W e conduct both automatic and\nhuman evaluation on RadBench, in both cases, RadFM signiÔ¨Åcantly outperforms existing multi-modal\nfoundation models. The codes, data, and model checkpoint will all be made publicly available to promote\nfurther research and development in the Ô¨Åeld.\n1 Introduction\nGeneralist foundation models [ 5], the latest generation of AI models pre-trained on large-scale dataset, have\ndemonstrated remarkable success in various domains, for example, natural language processing, computer\nvision [ 38, 21]. Their ability to address diverse and challenging tasks has also attracted tremendous attention\namong researchers in the Ô¨Åeld of ArtiÔ¨Åcial Intelligence for Medicine (AI4Medicine) [ 20, 25, 27, 39, 45]. Despite\nthe promising clinical use cases, the progress in developing medical foundation models has been fundamentally\nhindered by the challenges from three aspects:\n‚Ä¢ Lack of multimodal datasets for training: medicine by its nature, requires understanding multimodal\ndata, spanning text (electronic health record, medical reports), 1D signals (ECG), 2D images (ultrasound,\nX-ray), 3D images (CT or MRI scans), genomics, and more. T o support the training of the medical\ngeneralist foundation model, a large-scale, diverse, multimodal dataset is desperately required;\n‚Ä¢ Lack of general architecture formulation: in the literature of AI4Medicine, various clinical tasks\nhave largely been tackled by following a divide-and-conquer paradigm, i.e., diÔ¨Äerent architectures\nare designed for the problem of interest, like diagnosis [ 37, 44] or segmentation [ 6, 14]. In contrast,\ndeveloping a medical foundation model requires one general architecture that is capable of tackling a\nwide spectrum of clinical tasks, by fusing information from a mixture of diÔ¨Äerent modalities;\n‚Ä¢ Lack of eÔ¨Äective benchmark to monitor progress: benchmarking the models‚Äô clinical knowledge\npredominantly relies on task-speciÔ¨Åc datasets with a limited number of testing cases. An high-quality\nbenchmark is yet to be established, to comprehensively measure the progress of the development on\nmedical foundation model across a wide range of clinical tasks.\nConsidering the above-mentioned challenges, in this paper, we take a preliminary , yet realistic step toward\n‚àó Equal contributions. Email addresses: {wtzxxxwcy02, xm99sjtu, weidi}@sjtu.edu.cn\nOpenFlamingo MedVInT Med-Flamingo RadFM\n1\n10\n100\nModality\nRecognition\nDisease\nDiagnosis\nMedical\nVQA\nReport\nGeneration\nRationale\nDiagnosis\n0 10 20 30 40\nUMLS_Recall\nUMLS_Precision\nROUGUE\nBLEU\nMedical VQA\n0 10 20 30\nUMLS_Recall\nUMLS_Precision\nROUGUE\nBLEU\nReport Generation\n0 20 40 60\nUMLS_Recall\nUMLS_Precision\nROUGUE\nBLEU\nRationale Diagnosis\n0 50 100\nACC\nModality Recognition\n0 50 100\nF1\nACC\nDisease Diagnosis\n0\n1\n2\n3\nMedical VQA Report Generation Rationale Diagnosis\nScore\nHuman Rating\nFigure 1 | The general comparison between RadFM and diÔ¨Äerent SOT A methods, i.e., OpenFlamingo [ 1], MedVInT [ 45] and\nMed-Flamingo [ 27]. On the left we plot the radar Ô¨Ågure of the four methods, the average of diÔ¨Äerent automatic metrics are\nplotted and the coordinate axes are logarithmized. On the right upper, we draw the comparison of the Ô¨Åve diÔ¨Äerent tasks with\ndiÔ¨Äerent automatic metrics in detail. At the right bottom, we show the human rating comparison for the four methods under\nthree open-ended task types (medical VQA, report generation, and rationale diagnosis). All of them can indicate the superiority\nof RadFM, surpassing former methods signiÔ¨Åcantly .\ndeveloping a generalist medical foundation model for radiology , which has shown to play a vital role in\nclinical scenarios, for example, disease diagnosis, treatment planning, and monitoring patient progression.\nSpeciÔ¨Åcally , we present our progress towards such a Radiology Foundation Model ( RadFM), that aims to\ntackle a wide spectrum of clinical radiology tasks, by learning from medical scans and corresponding text\ndescription/reports, for example, X-ray , CT, MRI, with the same set of model weights.\nIn order to train our proposed RadFM, we start by constructing a novel, large-scale, Medical Multi-modal\nDataset, named MedMD, with around 16M medical scans in total, consisting of 15.5M 2D images and\n180k 3D radiology scans (equivalent to 7M 2D slices) accompanied with high-quality textual descriptions,\nfor example, radiology reports, visual-language instruction, or crucial disease diagnosis labels. MedMD\nencompasses a wide range of radiological modalities and anatomical regions of the human body , featuring over\n5000 diseases, thus can potentially serve as the cornerstone for developing foundation models in radiology .\nArchitecturally , RadFM refers to a visually conditioned autoregressive text generation model, that enables to\nseamlessly integrate natural language with 2D or 3D medical scans, and addresses a wide range of medical\ntasks with natural language as output. The proposed model is initially pre-trained on the MedMD dataset\nand subsequently Ô¨Åne-tuned through visual instruction on a Ô¨Åltered subset, comprising 3M meticulously\ncurated multi-modal samples with only radiologic cases, termed as RadMD, ensuring a high-quality and\nreliable dataset for the domain-speciÔ¨Åc Ô¨Åne-tuning process.\nT o monitor the developmental progress on the foundation model for radiology , we establish a novel, com-\nprehensive evaluation benchmark, RadBench, covering a variety of clinical tasks, for example, disease\ndiagnosis, report generation, and visual question-answering on radiologic modalities and anatomical regions.\nAll samples in RadBench have undergone meticulous manual veriÔ¨Åcation to ensure data quality . W e conduct\nboth automatic and human evaluation on RadBench with existing strong models that are open-source, for\nexample, Open-Ô¨Çamingo [ 3], MedVInT [ 45] and MedFlamingo [ 27], and observe signiÔ¨Åcant beneÔ¨Åts across all\nconsidered tasks.\nOverall, in this work, we demonstrate the potential of developing a generalist foundation model for radiology ,\nby making the following contributions:\n‚Ä¢ A large-scale multimodal biomedical dataset, that encompasses diverse modalities of medical\n|2\nimaging and high-quality text description, such as clinical reports, featuring over 5000 diseases, thus\ncan serve as a tremendous resource to advance generalist foundation models for medicine;\n‚Ä¢ A demonstration of radiology foundation model (RadFM) , that surpasses the constraints of\nexisting models by overcoming limitations on data size, modalities, and applications. RadFM sets a\nnew paradigm as the Ô¨Årst multi-modal foundation model for seamlessly integrating natural languages\nwith both 2D and 3D radiologic images, across various imaging modalities, that enables to tackle a wide\nspectrum of downstream clinical tasks with one single model;\n‚Ä¢ A comprehensive benchmark for radiology (RadBench) , that enables to monitor the progress of\ndeveloping foundation models. SpeciÔ¨Åcally , we conduct thorough comparison with a set of open-source,\nstrong models, RadFM is shown to be signiÔ¨Åcantly superior in all of the tasks as shown in Fig 1.\n2 Related W ork\nWith the success of generative language foundation models such as GPT-4 [ 32] and PaLM-2 [ 2], there has\nbeen a surge of interest in multi-modal foundation models. While signiÔ¨Åcant strides have been made in the\nrealm of natural scenery , as evidenced by BLIP-2 [ 21] and Flamingo [ 1], the development of generalist medical\nartiÔ¨Åcial intelligence is still in its nascent stages [ 26]. The relevant research can be bifurcated into two primary\nareas, namely , dataset construction and model training.\nDataset Construction. Contrary to the natural scenery domain, which boasts numerous large-scale multi-\nmodal datasets such as MMC4 [ 47], Visual Genome [ 18], and LION-5B [ 35], the medical domain is somewhat\nlacking. The most widely utilized medical multi-modal dataset is MIMIC-CXR [ 17], which only contains\nchest X-ray images with caption reports and its quantity (224K) is relatively small. In PMC-OA [ 23], the\nauthors have compiled a dataset containing 1.6M image-caption pairs. Although it encompasses various image\nmodalities, many 3D medical scans are presented as 2D slices since the images are extracted from papers.\nThere are also some medical VQA datasets, such as VQA-RAD [ 19], SLAKE [ 24], and PMC-VQA [ 45], but\nthey are also limited to 2D images. In Med-Flamingo [ 27], they have collected a dataset, MTB, consisting of\napproximately 0.8M images interleaved with texts while it is not open-source.\nModel T raining. T o date, several works have focused on building medical foundation models. LLaV A-\nMed [ 20] and MedVInT [ 45] utilize image captions in PubMed Central to generate medical visual instruction\ntuning data, but they are limited to 2D images and single image inputs. In Med-PaLM M [ 39], the authors\namalgamate existing medical image or multi-modal datasets, but the majority of images are X-rays, which\nare not suÔ¨Éciently accurate for clinical practice. Similar to previous works, it also limits itself to 2D images.\nMedBLIP [ 7] focuses on a speciÔ¨Åc 3D imaging modality , MRI. A very recent work Med-Flamingo [ 27] has\nenabled multi-interleaved 2D image input.\nIn conclusion, there is currently a dearth of a large-scale medical multi-modal dataset that satisÔ¨Åes all the\nfollowing characteristics: containing both 3D and 2D image inputs, multi-image input per case, and images\ninterleaved with text. On the other hand, there is also no model that can support this series of heterogeneous\ninput forms simultaneously . This paper aims to address these gaps, aligning more closely with clinical practice.\n3 Dataset\nHere, we start by introducing the procedure for constructing Medical Multimodal Dataset ( MedMD) in\nSec. 3.1, to the best of our knowledge, this is the Ô¨Årst large-scale, high-quality medical vision-language dataset,\ncovering a wide range of anatomies with over 5000 diseases. In Sec. 3.2, we further contribute a Ô¨Åltered\nradiology subset, named Radiology Multimodal Dataset ( RadMD), dedicated to domain-speciÔ¨Åc Ô¨Åne-tuning\ntowards building foundation model for radiology . In Sec. 3.3, we introduce a new Radiology Benchmark for\nevaluation, termed as RadBench, with Ô¨Åve distinct tasks, e.g., modality recognition, disease diagnosis, visual\nquestion answering, report generation and rationale diagnosis, aiming to monitor the progress of developing\nfoundation models.\n|3\n‚Ñê:              What type of \nimaging modality is used to \nacquire the given image? \n‚Ñõ: MRI\n‚Ñê: Please caption this scan \nwith findings and impression. \n‚Ñõ: Findings: Left tongue mass with \na depth of invasion of 1 cm (T2) ‚Ä¶\nImpression: Squamous cell ‚Ä¶\nHead and Neck\nBrain\nùíØ:  As shown in Fig.1,       \nan oblique X-ray of a 17 \nyears old patient‚Ä¶        \nshows avulsion fracture. \n‚Ñê: What abnormality can \nbe observed in the areas \nof subpleural? \n‚Ñõ: Small cystic changes\nThorax\nUpper Limb\n‚Ñê: Patient: Age: 20 years Gender: Male.\nRadiculopathy and left upper limb \nweakness. Identify the disease and \ndescribe  the classic radiological pre‚Ä¶.\n‚Ñõ: Osteochondroma. Radiological features..\n‚Ñê: Patient: Age: 30 years Gender: Male. \nIdentify the disease and describe the ‚Ä¶\n‚Ñõ: Tuberculous lymphadenitis. The \nradiological features are follows. CT: ‚Ä¶\nAbdomen\nSpine\n‚Ñê:             Please caption \nthis scan with findings and \nimpression. \n‚Ñõ: Findings: Below knee \namputation margins of the \ntibia and fibular are ‚Ä¶\nLower Limb\n Pelvis\n‚Ñê: What disease \nis shown on the given \nimages? \n‚Ñõ: The images show a \ntypical pedunculated \nosteochondroma\nFigure 2 | Overview of Medical Multimodal Dataset (MedMD). Our collected data covers the majority of radiologic modalities\nand anatomical regions of the human body , such as brain, head and neck, thorax, spine, abdomen, upper limb, lower limb, and\npelvis, etc. The dataset mixes two types of datasets, i.e., interleaved datasets and visual instruction datasets. T refers to the\ntext of interleaved data, I refers to the instruction input text, and R refers to the response text.\n3.1 Medical Multimodal Dataset (MedMD)\nT o start, we construct a candidate data pool by pulling a variety of existing visual-language medical datasets\ntogether, for example, MIMIC-CXR [ 17] and PMC-OA [ 23]. Despite the scale of these high-quality datasets,\nthey are fundamentally limited in several aspects: (i) Data format. These datasets are only composed of\n2D medical images, which do not fully capture the complexities in clinical use cases, for example, 3D medical\nimaging modalities, like CT, MRI; (ii) Modality diversity . A noteworthy limitation arises from the fact\nonly chest X-ray images are provided with medical reports, which has clearly restricted the generalizability of\nthe models to a broader range of imaging modalities and anatomical regions; (iii) Report quality . Another\ncritical limitation lies in the use of data extracted from Ô¨Ågures and captions from research papers. The gap\nbetween research-oriented data and real-world clinical scenarios may hinder the model‚Äôs ability to provide\naccurate and reliable diagnoses, and limit its applicability toward clinical decision-making. Therefore, to\nsupport the training of our proposed Radiology F oundation Model (RadFM), we augment the dataset with\nfour new ones, including PMC-Inline, PMC-CaseReport, Rad3D, and MPx, totalling 15.5M 2D scans, 180k\n3D scans (equivalent to 7M 2D slices), with corresponding captions or diagnosis labels, as shown in Fig. 2.\n|4\nGenerally speaking, we split the candidate data pool into two parts, one is of interleaved image-language data\nthat is collected from academic papers, while the other one contains data constructed for visual-language\ninstruction tuning, as detailed below.\nT able 1| Description of the collected dataset Medical Multimodal Dataset (MedMD) for model pre-training.\nInterleaved Dataset\nDataset Name Description Data size\nPMC-Inline A medical dataset containing PMC-papers which links with images\nthrough inline reference, e.g., ‚Äúas shown in Ô¨Åg.X‚Äù.\n11M\nVisual Instruction T uning Dataset\nDataset Name Description Data size\nImage data Image number\nVinDr-Mammo [ 29] A mammography dataset consists of four-view exams with level\nassessment and Ô¨Ånding annotations.\n20K\nVinDr-SpineXR [ 30] A dataset consists of spine X-ray images with annotations of 13 types\nof abnormalities.\n10K\nVinDr-PCXR [ 31] A dataset consists of pediatric chest X-ray images with annotations of\n36 critical Ô¨Åndings and 15 diseases.\n9K\nCXR-Mix [ 41] A collection of diÔ¨Äerent chest X-ray diagnosis datasets. 449K.\nRadChest-CT [ 10] A dataset consists of chest CT scans labeled with 84 abnormality\nlabels and 52 location labels.\n4K\n2D Image-text data Pair number\nPMC-OA [ 23] A medical dataset contains paired Ô¨Ågures and captions collected from\nPubMed Central.\n1.65M\nPMC-VQA [ 45] A medical visual question-answering dataset generated from\nPMC-OA.\n413k\nPMC-CaseReport A sub-dataset Ô¨Åltered from PMC-Inline containing 103K cases report\npapers. W e generate VQA pairs by querying ChatGPT. we keep some\nbackground information of the patient to form context input.\n1.1M\nMPx-Single A medical vision-language dataset contains the modality , plane and\ncaptions for each image.\n114K\nMPx-Multi A medical vision-language dataset contains Ô¨Åndings, discussion, and\ndiagnoses for each case which may contain a series of radiology\nimages.\n39K\nVQA-RAD [\n19] A medical visual question-answering dataset consists of\nquestion‚Äìanswer pairs on 315 radiology images.\n3.5K\nSLAKE [ 24] A bilingual visual question-answering dataset consisting of 642 images. 6K\nMIMIC-CXR [ 17] A chest image-report dataset contains 377k images corresponding to\n227k studies.\n227K\n2D & 3D Image-text data Pair number\nRad3D-Caption A medical dataset consists of images and corresponding captions. 73k\nRad3D-VQA A medical visual question-answering dataset generated from the\ncaptions in Rad3D-Caption.\n205k\nRad3D-Modality\nA medical vision-language dataset contains the modality question for\neach image.\n264K\nRad3D-Rationale A medical vision-language dataset contains disease rationale diagnosis\nfor each case which may contain a series of radiology images.\n73K\n|5\nT able 2| Description of the Ô¨Åltered dataset Radiology Multimodal Dataset (RadMD) for domain-speciÔ¨Åc Ô¨Åne-tuning.\nDataset Name Filter Strategy Filtered Data Size\nImage data Image number\nDiagnosis datasets Balancing classes and prompt some judegment diagnosis question\nanswer pairs.\n491K\nImage-text data Pair number\nPMC-OA [ 23] All cases will be dismissed. 0\nPMC-Inline All cases will be dismissed. 0\nPMC-VQA [ 45] Keep all data without any exclusions. 413K\nPMC-CaseReport Dismiss the non-radiologic cases; avoid some cheating cases in context;\nremove the question about size and age.\n438K\nMPx-Single Keep all data without any exclusions. 114K\nMPx-Multi Keep all data without any exclusions. 39K\nVQA-RAD [\n19] Keep all data without any exclusions. 3.5K\nSLAKE [ 24] Keep all data without any exclusions. 14K\nMIMIC-CXR [ 17] Keep all data without any exclusions. 227K\n2D & 3D Image-text data Pair number\nRad3D-Caption Dismiss the non-radiologic cases; remove phrases that contain size. 48K\nRad3D-VQA\nDismiss the non-radiologic cases; remove questions about size and age. 205K\nRad3D-Modality Dismiss the non-radiologic cases. 176k\nRad3D-Rationale Dismiss the non-radiologic cases. 48K\n3.1.1 Interleaved Dataset\nPMC-Inline. PMC-Inline contains 11M 2D radiology images that are collected from PubMed Central\npapers. In contrast to existing work, for example, PMC-OA [ 23], that only contains Ô¨Ågures and corresponding\ncaptions, here, we focus on the inline reference from the main body of papers. F or example, one paper may\ncontain many sentences like ‚ÄúAs shown in Fig.2, we can see . . . ‚Äù, we localise the keyword ‚ÄúFig.2‚Äù and link\nits corresponding Ô¨Ågure back into sentences, ending up with interleaved images and texts, with rich context.\nThis dataset shares the same format as MMC4 [ 47], which has shown to be eÔ¨Äective in training foundation\nmodels in computer vision community , for example, Flamingo [ 1].\n3.1.2 Visual-language Instruction T uning Dataset\nPMC-CaseReport. PMC-CaseReport is a Ô¨Åltered subset of PMC-Inline with around 103K cases report\npapers, where the doctors typically document the valuable clinical cases, based on their contact with the\npatients, such as family medical history , preliminary diagnosis, radiographic exam results, surgical records,\netc., together with critical radiologic scans, that generally follows the real timeline.\nSimilar to PMC-VQA [ 45] that generates VQA pairs by querying ChatGPT with image captions, we also\ngenerate 1.1M question-answer pairs by querying ChatGPT with the sentences containing inline references\nin case reports. However, in contrast to PMC-VQA, we keep background information of the patients to\nsimulate the clinical diagnosis scenario, thus can be seen as a medical contextual VQA dataset. F or example,\na question-answer pair may like ‚ÄúQuestion: A 58-year-old woman presented to the emergency department\n. . . Postoperatively , her pain signiÔ¨Åcantly relieved. What did the MRI indicate? Answer: The MRI indicated\ntumor recurrence at L2 and S1-S2.‚Äù\nRad3D. Rad3D is a novel dataset with 3D radiology scans, sourced from Radiopaedia website ‚àó , that has\nalready resolved the privacy issue. SpeciÔ¨Åcally , each patient case comprises one or more modality images,\naccompanied with high-quality captions that have been meticulously veriÔ¨Åed by multiple experts. In addition,\n‚àó https://radiopaedia.org/\n|6\nfor each disease, we can get corresponding radiological features across diÔ¨Äerent modalities.\nIn this paper, we convert the image-caption pairs into a variety of formats, namely , Rad3D-Caption, Rad3D-\nModality , and Rad3D-Rationale, Rad3D-VQA, depending on their corresponding text content. SpeciÔ¨Åcally ,\nRad3D-Caption represents the images paired with their corresponding captions; Rad3D-Modality refers to\nimages with modality labels; Rad3D-Rationale incorporates radiological features with disease labels for each\ncase; Rad3D-VQA involves visual question-answering pairs generated from captions by querying ChatGPT, as\nillustrated in Fig. 3.\nPrompting ChatGPT to generate medical visual question answering data for Rad3D\nfewshot_samples = [\n{\n'context': \"Radiology report: A vertical fracture line extending through the radial head's articular surface, with less than a 2 mm \ngap. No other fracture was detected. The alignment of the elbow is intact. Impression: Non -displaced intra-articular fracture of the \nradial head, consistent with a Mason type I fracture. \",\n'response': \"User: What's the disease shown in the CT scan? \\\nAssistant: Non-displaced intra-articular fracture of the radial head, consistent with a Mason type I fracture. \\\nUser: What is the location of the fracture within the radial head? \\\nAssistant: The radial head's articular surface. \\\nUse: According to the CT image, is there any displacement or separation of the fracture fragments? \\\nAssistant: No. \\\nUse: What is the size of the gap observed in the fracture? \\\nAssistant: Less than 2 mm. \\\nUser: What's the reason of diagnosing this case as a non-displaced fracture of the radial head - Mason type 1? \\\nAssistant: Radial head fractures are a common type of elbow injury in adults 1 -4. They usually occur during a fall on an outstretched \narm with the forearm pronated and discrete flexion of the elbow joint 1 -4. CT scan helps identify the location and size, number,\nmorphology, articulate margin, fracture gap, and associated bone fractures and distinguishes the type of fracture in Mason \nclassification 1-4.\" \\\n} \n] # Ôªøone of the few-shot examples used\nmessages = [ \n{\"role\": \"system\", \"content\": \"You are an AI assistant specialized in biomedical topics. \nYou are provided with a clinical case report including the radiology report. Unfortunately, you don‚Äôt have access to the actu al image. \nBelow are the requirements for generating the questions and answers in the conversation: \n- Avoid quoting or referring to specific facts, terms, abbreviations, dates, numbers, or names, as these may reveal the convers ation \nis based on the text information, rather than the image itself. Focus on the visual aspects of the image that can be inferred without \nthe text information. \n- Do not use phrases like 'mentioned', 'report', 'discussion', or 'context' in the conversation. Instead, refer to the informat ion as \nbeing 'in the image.' \n- Ensure that questions are diverse and cover a range of visual aspects of the image. \n- The conversation should include at least 2-3 turns of questions and answers about the visual aspects of the image. \n- Answer responsibly, avoiding overconfidence, and do not provide medical advice or diagnostic information. Encourage the user to \nconsult a healthcare professional for advice.\"\n}\n]\nfor \nsample in fewshot_samples:\nmessages.append({\"role\":\"user\", \"content\":sample['context']})\nmessages.append({\"role\":\"assistant\", \"content\":sample['response‚Äô]})\nmessages.append({\"role\":\"user\", \"content\":query})\nFigure 3 | messages refers to the text description that we use to prompt ChatGPT to generate medical visual question-answering\ndata for Rad3D. Manually curated few-shot examples are included in the prompt, where each example has input sample[‚Äòcontext‚Äô]\nand output sample[‚Äòresponse‚Äô].\nMPx. MPx is collected from MedPix website ‚Ä† and organized by cases. Each case contains multiple radiologic\nscans, along with general clinical Ô¨Åndings, discussions, and diagnostic results. In addition, MPx also provides\nannotations on the scan-level, including information such as image modality , shooting plane, and captions for\neach scan. Thus we separate it into MPx-Single and MPx-Multi, containing annotations on case-level and\nscan-level respectively .\n3.2 Radiology Multimodal Dataset (RadMD)\nF or domain-speciÔ¨Åc Ô¨Ånetuning, we Ô¨Ålter out the non-radiology images from MedMD, and construct a clean\nsubset, named Radiology Multimodal Dataset ( RadMD), dedicating to supervised visual instruction-tuning.\nIt contains a total of 3M images, spanning various data formats, modalities, and tasks, featuring over 5000\n‚Ä† https://medpix.nlm.nih.gov/\n|7\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nsupport devices\natelectasis\npneumonia\nfracture\nforeign object\nbirads negative\npleural thickening\nbreast scattered areas of fibroglandular\npleural effusion\nobstructive hydrocephalus\ndural venous sinus thrombosis\ngallstones\nlymphoma\nosteochondroma\ncirrhosis\ninfection\nspondylodiscitis\nneurofibromatosis type 1\nmature cystic ovarian teratoma\ndevelopmental venous anomaly\nendometriosis\nidiopathic intracranial hypertension\nendometrioma\nbirads suspicious malignant\ncerebral ring enhancing lesions\nchondrosarcoma\nhepatic hydatid infection\npathological fracture\ncolorectal cancer\nsarcoidosis\nascites\ncyst\nchiari i malformation\nmultiple myeloma\nscattered calcifications\ncolloid cyst of the third ventricle\nmass nodule\nspinal meningioma\nseptic arthritis\nosteoid osteoma\nposterior reversible encephalopathy syndrome\ncerebral venous thrombosis\nfalse aneurysm\ntuberculosis (pulmonary manifestations)\nhaemangioblastoma (central nervous system)\nvon hippel-lindau disease\nmiddle cerebral artery (mca) infarct\nintracranial haemorrhage\nportal vein thrombosis\nlarge bowel obstruction\nother lesions\noptic pathway glioma\nrib fractures\nsubependymal grey matter heterotopia\nextradural haemorrhage\ntenosynovitis\nintramedullary spinal tumours\nreticulation\npolymicrogyria\nsubcutaneous emphysema\nrenal trauma\nherpes simplex encephalitis\nepiploic appendagitis\nhepatic steatosis\nvasogenic cerebral oedema\nclip\ngout\nparanasal sinuses\nhaemoperitoneum\nhorseshoe kidney\ntuberculous meningitis\nspondylolysthesis\ndiffuse hepatic steatosis\nspondylolysis\nsialolithiasis\nsynovial sarcoma\nliver trauma\ncortical tubers\nrenal agenesis\nvesicoureteric reflux\npaget disease (bone)\nsternotomy\ntransitional cell carcinoma (urinary bladder)\nintraductal papillary mucinous neoplasm\nintraventricular haemorrhage\nseptal thickening\nosteochondritis dissecans\nsubependymal giant cell astrocytoma\ntree-in-bud sign (lung)\ngallbladder carcinoma\ncongenital diaphragmatic hernia\nventricular septal defect\nglobe rupture\nhydrocele\ncarotid body tumour\nendometrial polyp\nprimary synovial chondromatosis\ncraniofacial fibrous dysplasia\nencephalomalacia\nspinal nerve sheath tumours\ntorus fracture\nen plaque meningioma\ntesticular torsion\natrial septal defect\nspinal ependymoma\nairway foreign bodies in children\nprimary sclerosing cholangitis\ncryptorchidism\ntumour thrombus\nbrown tumour\nossification of the posterior longitudinal ligament\nepithelial ovarian tumours\nsusceptibility weighted imaging\ncortical vein thrombosis\ntumefactive demyelinating lesion\ncovid-19 (summary)\npapillary thyroid cancer\nbirads highly suggestive of malignant\nwilms tumour\ntraumatic brain injury\nnormal pressure hydrocephalus\ntentorium cerebelli\nintestinal malrotation\nastrocytoma, idh-mutant\nchoroid plexus xanthogranuloma\nunicornuate uterus\nextrapulmonary tuberculosis\ncreutzfeldt-jakob disease\nright-sided aortic arch\nanterior talofibular ligament injury\npartial anomalous pulmonary venous return\nalzheimer disease\nosgood-schlatter disease\nslipped upper femoral epiphysis\nmega cisterna magna\ndiffuse idiopathic skeletal hyperostosis\nvertebral collapse\nsuperficial siderosis\nintraventricular meningioma\ncarcinoid tumour\nperforated appendix\ndilatation or ectatic\nhaemarthrosis\npulmonary edema\ngliomatosis cerebri\nprimary uveal malignant melanoma\npapilloedema\ntubal ectopic pregnancy\nbowel and mesenteric trauma\nextra-axial\ngastric band\ndyke-davidoff-masson syndrome\nfreiberg disease\ntesticular seminoma\npulmonary nodule\nascariasis\nleft-sided superior vena cava\nparathyroid adenoma\nadenomyomatosis of the gallbladder\nrenal infarction\ngastric volvulus\nsegond fracture\nmyxopapillary ependymoma\nbone marrow oedema\nright heart strain\novarian serous tumours\novarian mucinous cystadenoma\nhypertensive microangiopathy\nlower gastrointestinal bleeding\nsinus pericranii\ndorsal intercalated segment instability\npyloric stenosis\nperiventricular leukomalacia\npleural plaque\nrasmussen encephalitis\nidiopathic orbital inflammation\nlissencephaly-pachygyria spectrum\ngynaecomastia\nlobar pneumonia\njuvenile nasopharyngeal angiofibroma\nvitreous haemorrhage\ntriquetral fracture\npulmonary hydatid infection\ntumour pseudoresponse\nlacunar infarct\nocular foreign body\nparapharyngeal space\ncytotoxic lesions of the corpus callosum (cloccs)\nsmall bowel lymphoma\nproximal phalanx fracture\nfibrosis pleural thickening\nposterior cruciate ligament tear\nretropharyngeal abscess\nstaghorn calculus (kidney)\npseudosubarachnoid haemorrhage\nhypoglycaemic encephalopathy\nsleeve gastrectomy\nbreast hamartoma\nspigelian hernia\ngartner duct cyst\nhypersensitivity pneumonitis\njefferson fracture\nsecondary cns lymphoma\nsubchondral insufficiency fracture\nintramural uterine leiomyoma\nhypertrophic olivary degeneration\nprogressive massive fibrosis\nenchondromatosis\necchordosis physaliphora\nemphysema pleural thickening\ndepressed skull fracture\nintracranial teratoma\ncolles fracture\nspinal astrocytoma\ndolichoectasia\nhepatic and splenic tuberculosis\nperthes lesion\ntethered cord syndrome\nbreast mri\nImage number\n2D\nCXR\n-Mix \n(36.12%)\nPMC-Series \n(37.33%)\nRad3D\n-Series (2.90%)\nMpx-Series \n(9.17%)\n3D\nRad3D\n-Series \n(11.26%)\n2D/3D\nX-ray\nCXR\n-Mix \n(32.09%)\nRad3D (2.23%)\nMpx\n-Series (2.24%)\nPMC\n-Series (4.54%)\nVinDr\n-‚Ä¶\nCT\nRad3D (5.17%)\nMpx\n-Series (5.76%)\nPMC-Series \n(9.09%)\nMRIRad3D (7.31%)\nMpx\n-Series \n(10.40%)\nPMC\n-Series \n(10.72%)\nUltrasound\nRad3D (1.28%)PMC\n-Ser ies (1.99%)\nPET\n-CT\nPMC\n-Ser ies (3.17%)\nModality\nModality \nRecognitionRad3D-\nModality \n(11.47%)\nMpx\n-Single (4.95%)\nDisease \nDiagnosis\nVinDr\n-‚Ä¶\nCXR\n-Mix \n(19.51%)\nMedical \nVisual \nQuestion \nAnswering PMC-VQA (17.94%)\nPMC\n-\nCaseReport (19.03%)\nRad3D\n-VQA \n(8.91%)Mpx\n-Multi (1.69%)\nRadiology \nReport Generation\nMIMIC\n-CXR \n(9.86%)\nRad3D\n-‚Ä¶\nRad3D\n-‚Ä¶\nTask\nFigure 4 | The data statistics of RadMD. The left image in the top row shows the distribution of 2D and 3D images. The\ncenter image shows the distribution of diÔ¨Äerent modalities. The collected dataset comprises more than 16M data samples from a\ndiverse range of radiology modalities. And the right image shows the distribution of Ô¨Åve downstream tasks, including modality\nrecognition, disease diagnosis, medical visual question answering, radiology report generation, and rationale diagnosis. The bar\nchart below illustrates diseases with more than 100 images. Note that when the image count exceeds 1000, it is represented as\n1000 in the chart. There are a total of 876 diseases with over 100 images per disease, 1582 diseases with more than 50 images,\nand 3351 diseases with more than 10 images.\ndiseases, as shown in Fig.\n4.\nIn general, we have conducted the following Ô¨Åltering process: (i) remove the PMC-OA and PMC-Inline\ndatasets due to the inconsistency in writing styles between academic papers and real clinical reports; (ii)\nremove non-radiologic images; (iii) Ô¨Ålter out the information about patient age or structure size, as the\nimage spacing and patient background information are not provided; (iv) balance the number of normal and\nabnormal patients in the diagnosis datasets, as generative models are sensitive to data imbalances. More\ncomprehensive details regarding the Ô¨Åltering process and the resulting dataset sizes can be found in T ab. 2.\n3.3 Radiology Evaluation Benchmark (RadBench)\nBuilding upon RadMD, we introduce RadBench, a comprehensive evaluation benchmark for radiology .\nRadBench encompasses Ô¨Åve distinct tasks, including modality recognition, disease diagnosis, visual question\nanswering, report generation and rationale diagnosis, aiming to monitor the progress of developing foundation\nmodels. RadBench comprises a collection of 13 diverse datasets, encompassing a wide range of distributions.\nA detailed breakdown of each dataset, including task descriptions and modalities, is provided in T able 3.\nIt is important to note that RadMD is generated using a scalable pipeline. Consequently , it is extensive and\ndiverse, and it may include some noisy data. However, for the purposes of evaluation, all samples in RadBench\nhave undergone meticulous manual veriÔ¨Åcation to ensure data quality . As a result, we have obtained 14,419\nsamples for modality recognition, 112,256 for disease diagnosis, 7,329 for visual question answering, 2,098 for\n|8\nT able 3| Description of the RadBench. Note that, considering many datasets are collected or generated from PMC papers, we\nsplit them on the paper-level, i.e., if a paper is randomly split into any test set, the arbitrary cases related to it will be dismissed\nin the whole training set, including for the two pre-training datasets, PMC-Inline and PMC-OA. F or Rad3D series, we split them\naccording to the published date of the cases, with those before 2023 as training data and the others as test data.\nDataset Name T ask Description Modalities Split\nRad3D-Modality Modality Recognition Image / T ext Random split\nVinDr-Mammo [ 29] Disease diagnosis Image OÔ¨Écial split\nVinDr-SpineXR [ 30] Disease diagnosis Image OÔ¨Écial split\nVinDr-PCXR [ 31] Disease diagnosis Image OÔ¨Écial split\nCXR-Mix [ 41] Disease diagnosis Image OÔ¨Écial split\nRadChest-CT [ 10] Disease diagnosis Image OÔ¨Écial split\nPMC-VQA [ 45] Medical visual question answering Image / T ext OÔ¨Écial split\nPMC-CaseReport Medical visual question answering Image / T ext Random split\nVQA-RAD [\n19] Medical visual question answering Image / T ext OÔ¨Écial split\nSLAKE [ 24] Medical visual question answering Image / T ext OÔ¨Écial split\nRad3D-VQA Medical visual question answering Image / T ext Random split\nMIMIC-CXR [ 17] Report generation Image / T ext Random split\nRad3D-Caption Report generation Image / T ext Random split\nMPx-Single Report generation Image / T ext Random split\nMPx-Multi Report generation Image / T ext Random split\nRad3D-Rationale Rationale diagnosis. Image / T ext Random split\nreport generation, and 1,000 for rationale diagnosis. The details of the Ô¨Åve evaluation tasks are introduced in\nthe following.\nModality recognition. This task involves analyzing the radiology images to determine the modality of the\ninput radiology images. Here we modify this task to a prompt-based visual question-answering task, i.e.,\ngiven a medical image, we randomly select a prompt sentence like ‚ÄúWhat is the modality of the given image?‚Äù,\nand match the output with a ground-truth list {‚ÄòCT‚Äô, ‚ÄòMRI‚Äô, ‚ÄòUltrasound‚Äô, ‚ÄòPET‚Äô, ‚ÄòX-ray‚Äô, ‚Äòangiography‚Äô}\nusing difflib.SequenceMatcher, and choose the most similar one as the prediction of the model to calculate\nthe ACC and F1 score.\nDisease diagnosis. This task involves analyzing the radiology images to determine the likelihood of speciÔ¨Åc\ndiseases. Here we modify this task to a prompt-based visual question-answering task, given a medical image,\nwe randomly select a disease and randomly select a prompt sentence like ‚ÄúIs {disease} shown in this image‚Äù\nas input to ask the network to answer whether the case has this disease. Due to this being formulated as\na generation task, ‚ÄúAUC‚Äù cannot be calculated, so we match the output with ground-truth to calculate\nthe ACC and F1 score. Similarly , we match the output with a closed ground-truth list {‚Äòyes‚Äô, ‚Äòno‚Äô} using\ndifflib.SequenceMatcher, and choosing the most similar one as the prediction of the model. Considering\nACC scores may suÔ¨Äer from data unbalancing, we keep the same ratio to sample positive and negative cases.\nIn our dataset, we do not put prior on the disease, and over 5000 diseases are considered, with a balanced\nratio of ‚Äòyes‚Äô or ‚Äòno‚Äô responses.\nMedical visual question answering. This task is a combination of popular visual question-answering\nchallenges. Given a medical image and a clinically relevant question in natural language as a prompt, the\nmedical VQA system is expected to predict a plausible and convincing answer. W e adopt the widely-used\nevaluation metrics BLEU [\n33]. Additionally , we use UMLS_Precision and UMLS_Recall to evaluate the\naccuracy of medical-related word prediction. Note that , considering some VQA datasets are generated by\nChatGPT [ 32], like PMC-VQA, PMC-CaseReport and Rad3D-VQA, their test split may still contain a lot of\nnoisy cases. Thus, we manually Ô¨Ålter their test split again, resulting in 7.5K reliable evaluation cases, ensuring\nour evaluation to be convincing.\n|9\nRadiology report generation. This task focuses on the automatic generation of diagnostic reports, i.e.,\nsummarizing the radiologic Ô¨Åndings based on radiology images, such as X-rays, CT scans, and MRI scans.\nGiven a medical image, we randomly select a prompt sentence like ‚ÄúPlease caption this scan with Ô¨Åndings.‚Äù\nas input. W e adopt the widely-used evaluation metrics BLEU [ 33], and our proposed UMLS_Precision and\nUMLS_Recall to evaluate the accuracy of medical-related word prediction. Similarly to medical VQA, due to\nthat Rad3D-Caption, MPx-Single and MPx-multi are downloaded from online, they may also contain some\nnoisy cases in test split, and MPx-Multi thus we manually Ô¨Ålter them and keep 2K cases left to do Ô¨Ånal\nevaluation.\nRationale diagnosis. This task involves analyzing radiology images to predict both the underlying disease\nand the typical radiologic features of diÔ¨Äerent modalities such as X-rays, CT scans, and MRI scans associated\nwith that disease. Given a medical image, we randomly select a prompt sentence like ‚ÄúDetermine the disease\nthat corresponds to the given radiographic images, starting with the established radiological features and\nconcluding with the ultimate diagnosis.‚Äù Since W e have evaluated disease diagnosis accuracy in the common\n‚ÄúDisease Diagnosis‚Äù setting, for rationale diagnosis, we mainly focus on how well the foundation model can\ngive reasons. Thus, we adopt BLEU, ROUGE, UMLS_Precision and UMLS_Recall here, to measure the\nplausibility of the rationale prediction. Similarly , we manually check and Ô¨Ålter the test set and pick 1K cases\nout for evaluation.\n4 T owards Building Generalist F oundation Model for Radiology\nIn this section, we start by describing the learning paradigm for unifying diÔ¨Äerent medical tasks into a\ngenerative framework, followed by detailing the proposed RadFM model, and its training details. Our training\nadopts two types of datasets, namely , interleaved datasets and visual instruction datasets. It is worth noting\nthat their training objectives diÔ¨Äer slightly , which will be detailed in the following.\n4.1 UniÔ¨Åed Learning Paradigm\nIn both of our proposed multimodal datasets, i.e., MedMD and RadMD, each training sample is essentially\nconsisting of two elements, i.e., X = {T ,V}, where T refers to the language part in the case, with special\nplaceholder tokens for images, e.g., ‚ÄúThe patient is 47-year-old. <image-1> <image-2> W e can see opacity on\nthe X-ray‚Äù. V refer to the visual parts containing a set of 2D or 3D image scans, i.e., V = {v1, v2, . . . , vN },\nvi ‚àà RH√ó W √ó C or vi ‚àà RH√ó W √ó D√ó C , H, W, D, Care height, width, depth, channel respectively , corresponding\nto the ‚Äú<image- i>‚Äù token in T . In general, T and V can be considered as prompts input to model with\ninterleaved language and image.\nThe goal is therefore to model the likelihood of generated text tokens in T , conditioned on interleaved medical\nscans as follows:\np(T |V) =\n‚àè\np(Tl|V<l,T<l), (1)\nwhere Tl represents the l-th token in T and V<l, T<l represent the image and language text appearing before\nthe l-th token. W e use a generative model ( Œ¶ RadFM) to parameterize the probability p, and our Ô¨Ånal training\nobjective can be expressed as the negative log-likelihood of the correct next token in the text sequence:\nLreg = ‚àí\n‚àë\nwl log Œ¶ RadFM(Tl|V<l,T<l), (2)\nwhere wl refers to a per-token weighting, aiming to either emphasize key tokens or skip special tokens. Its\nvalue diÔ¨Äers for diÔ¨Äerent datasets and we detail this in the following.\nInterleaved Datasets. F or samples in visual-language interleaved dataset, i.e., PMC-Inline, there are no\nstrong question-and-answer relationships between contexts, we extract medical-related words in each sentence\nby using UniÔ¨Åed Medical Language System (UMLS) [ 4], and give them a high loss weights. Additionally , we\n|10\navoid calculate loss on the image placeholder token. Overall, wl can be formulated as,\nwl =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\n3, Tl ‚àà USML\n1, Tl /‚àà USML\n0, Tl = <image-i>\n. (3)\nNote that, PMC-Inline is the only dataset Ô¨Åt in this case.\nVisual Instruction Datasets. F or samples from visual instruction datasets like PMC-VQA [ 45] or PMC-\nCaseReport, they are often in the format of dialogue, for example, ‚ÄúWhat can you see from the image?\n<image-1> I can see lesions.‚Äù or ‚ÄúPlease dscribe the scans <image-1>. The scan is . . . ‚Äù, we further separate\nthe language part T into instruction and response, denoted as I and R respectively . F or example, as in the\nformer two cases, I refers to ‚ÄúWhat can you see from the image? <image-1>‚Äù and ‚ÄúPlease dscribe the scans\n<image-1>‚Äù. In a practical scenario, I is expected to be given by users, and the model is only required to\noutput correct responses. Overall, wl can be formulated as,\nwl =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\n3, Tl ‚àà R & Tl ‚àà USML\n1, Tl ‚àà R & Tl /‚àà USML\n0, Tl ‚àà I\n. (4)\nMost samples from MedMD Ô¨Åt weighting formulation, and we describe the detailed prompting for diÔ¨Äerent\nproblem settings:\n‚Ä¢ Modality recognition. Here, we adopt two types of prompts, (i) we use judgment-type templates\nand the 2D or 3D medical scan as input, for example, ‚Äú<image-1> Is this image shot by {modality}?‚Äù,\nand the modality category is randomly sampled from the modality set, forming the text input I and if\nthe modality matches the ground truth labels we set the R as ‚Äúyes‚Äù otherwise ‚Äúno‚Äù. (ii) we use open\nprompts, like ‚ÄúWhat‚Äôs the modality of the input scan <image-1>?‚Äù to form the I, and translate the\ncorresponding modality label into R. Samples for training such functionality are from Rad3D-Modality\nand MPx-Single, with modality annotations available.\n‚Ä¢ Disease diagnosis. All the datasets listed as ‚Äúimage data‚Äù in T able 1 are built for diagnosis, they only\nhave binary labels for diseases. Similarly to modality recognition, we use two prompts to transform them\ninto our desired format, (i) we use judgment prompts, like ‚Äú<image-1> Does the patient have {disease}?‚Äù\nand the disease category is randomly sampled from a disease set, forming the text input I and if the\ndisease matches the ground truth labels we set the R as ‚Äúyes‚Äù otherwise ‚Äúno‚Äù, note that, during sampling,\nwe balance the positive and negative ratio, (ii) we use open diagnosis prompts, like ‚ÄúPlease make\ndiagnosis based on the images <image-1> <image-2>.‚Äù to construct the instruction ( I), and translate\nthe positive disease labels into response ( R), by simply using their category names. A simple example\nis, I=‚ÄúPlease make diagnosis based on the image <image-1>.‚Äù with R=‚ÄúEdema, pneumothorax.‚Äù.\n‚Ä¢ Visual question answering. Beyond the abovementioned task formulation, there are many more\ncomplex questions that can be asked, such as questions about the spatial relationships among objects\n(What is the location of the lesion?) and common sense reasoning questions (Given the image context\nand patient history , what is likely to be cause of the observed symptoms?). A robust medical VQA\nsystem must be capable of solving a wide range of classic medical diagnosis tasks as well as the ability to\nreason about images. Existing medical VQA datasets like VQA-RAD [\n19], SLAKE [ 24], PMC-VQA [ 45]\nand Rad3D-VQA naturally Ô¨Åt into this paradigm. They contain a mixture of question types, thus the\nlanguage questions can naturally be treated as text instruction ( I) and the corresponding answer as\nresponse ( R). It is worth noting that, our constructed PMC-CaseReport dataset also falls into this\ncategory , with more contextual information available for instruction, for example, history diagnosis, is\nalso available, thus providing critical information for answering the question.\n‚Ä¢ Report generation. MIMIC-CXR [ 17], Rad3D-Caption, PMC-OA [ 23], MPx-Multi and MPx-Single\nare all captioning datasets, the task is to write a long caption or report given one or a set of images. The\nlanguage instruction for this task are like ‚ÄúWhat can you Ô¨Ånd from the scans <image-1> <image-2>?‚Äù.\n|11\nInput:  Visual scans (3D or 2D) interleaving with texts\nVisual\nEncoder\nPerceiver\nPMC - VQA\nMIMIC- CXR\nMedMD & RadMD\nPMC - InlinePMC - CaseReport\nRad3D\nMPx\nOther  Existing Datasets\nPMC -OA\nNIH ChestXray14 \na. Dataset b. Training Procedure\nc. Architecture Detail\nText Output\nLarge Language Model\n1- st Transformer Decoder Block\nn- th Transformer Decoder Block\n‚Ä¶\nMedMD\nPre - training\n(16M)\nRadMD\nDomain - specific \nFine - tuning\n(3M)\nFigure 5 | (a) shows the the component of our datasets and the colored datasets are new proposed in this paper. (b) shows\nour training procedure, for better radiologic performance, we Ô¨Årst pre-train our model on the whole medical domain with 16M\nscans (MedMD) than Ô¨Åne-tuned on a cleaned dataset with 3M radiologic scanss (RadMD). (c) shows the main architecture of our\nmethod. Our architecture enables multi-image input interleaving with texts regardless of whether they are 3D or 2D.\n‚Ä¢ Rationale diagnosis. W e construct Rad3D-Rationale based on the Rad3D dataset. This task\nencompasses disease prediction and the generation of typical radiological features associated with the\ndiagnosed disease. SpeciÔ¨Åcally , we design some prompts like ‚ÄúWhat disease can be diagnosed from\nthese radiological images and what speciÔ¨Åc features are typically observed on the images? <image-1>\n<image-2>‚Äù as instruction ( I), and response ( R) refers to the disease label along with radiological\nfeatures collected from the Radiopaedia website.\n4.2 Architecture Detail\nIn this section, we aim to describe the proposed model in detail. As shown in Fig. 5, our proposed RadFM\nmodel consists of a visual encoder Œ¶ vis, that can process both 2D and 3D medical scans; a perceiver [ 16]\nmodule Œ¶ per for aggregating a sequence of scans into a Ô¨Åxed number of tokens, for example, taken with\ndiÔ¨Äerent modalities (CT, MRI) or various time point; and a Large Language Model (LLM) Œ¶ llm that enables\nto generate free-form text responses, based on the input visual-language information.\nVisual encoding. Given one sample instance from our dataset, denoted as X = {T ,V}, where V =\n{v1, v2, . . . , vN }, we Ô¨Årst encode each input image separately with an image encoder Œ¶ vis. SpeciÔ¨Åcally , we\nadopt 3D ViT here to be compatible with both 2D and 3D image input. F or 2D images, we expand a new\ndimension for depth by replicating the slices. Therefore, each image scan can be denoted as vi ‚àà RH√ó W √ó Di√ó C ,\nwhere C denotes the image channels and H, W, Di are the height, width, and depth of the image respectively .\nNote that , comparing to the typical visual encoding scenario that assumes diÔ¨Äerent images have uniÔ¨Åed\nshape, we do not normalize the depth dimension into an exact size, only round into a factor of 4, depending\n|12\non their original resolution. W e convert the image into 3D patches, and embed them into a token sequence\nand feed into the encoder ( Œ¶ vis). T o retain the 3D position of these tokens, we adopt learnable 3D position\nembeddings, the detailed procedure can be formulated as:\nvi = Œ¶ vis(vi) ‚àà RPi√ó d, (5)\nwhere vi is the output embedding for image vi, encoded with 3D ViT, Pi is the total number of tokens, and d\nis the feature dimension. Due to the inconsistency in depth dimension, Pi varies across 2D and 3D images.\nAggregation with Perceiver. After visual encoding, we adopt a perceiver [ 16] module Œ¶ per to aggregate\nvisual representation. SpeciÔ¨Åcally , Œ¶ per follows the classical perceiver architecture with a Ô¨Åx number of\nlearnable queries as the latent array input, and the visual embedding vi is treated as the byte array input, so\nthat the Ô¨Ånal output embeddings will be normalized into the same length with the pre-deÔ¨Åned learnable query\nsequence. The aggregation procedure can be formulated as:\nui = Œ¶ per (vi) ‚àà RP √ó d, (6)\nwhere ui refers to the aggregated visual embedding, P is the pre-deÔ¨Åned sequence length number of the\nlearnable queries. Leveraging perceiver architecture, we can map an arbitrary number of patch tokens into\nthe same length, such that images of diÔ¨Äerent sizes can be treated equally in the following fusion Ô¨Çow.\nMulti-modal fusion. T o fuse the visual-language information, we interleave the visual embedding with\ntext embeddings from tokenization, where the special image placeholder token is simply replaced with the\ncorresponding visual embedding. The resulting interleaved sequence is then passed into a decoder-only\nlarge language model ( Œ¶ llm), the self-attention transformer layers in LLM can thus naturally be re-used as\nmulti-modal fusion modules:\np = Œ¶ llm(concat(t1,u1,t2,u2,t3, . . .)), (7)\nwhere ti, ui refer to the text and visual embeddings, p is the probability distribution for the next token.\n4.3 T raining Procedure\nOur training procedure includes two stages, namely , pre-training, and domain-speciÔ¨Åc Ô¨Åne-tuning, as shown in\nFig. 5. Note that, all training settings are the same in the two stages, with the only distinction lying in the\ntraining data, from generalist to radiologic-speciÔ¨Åc.\nPre-training. At this stage, we use all available data in MedMD as listed in T able 1, the main components\nof the data are PMC-Inline and PMC-OA [ 23], which are all collected from 2.4M PMC papers. These two\ndatasets contain diverse medical vocabularies and images with cutting-edge medical knowledge, however, they\nare relatively noisy , so we only use them during pre-training in the hope that the network can accumulate\nenough knowledge about medical-speciÔ¨Åc terminologies and images. Additionally , we also include other VQA,\ncaptioning, and diagnosis datasets, as they are much cleaner.\nDomain-speciÔ¨Åc Ô¨Åne-tuning. At this stage, we adopt RadMD for domain-speciÔ¨Åc Ô¨Ånetuning, which\ncontains over 3M radiologic images, with high-quality language instruction or response.\n4.3.1 T raining Details\nImage preprocessing. T o dismiss the diÔ¨Äerences of medical images in diÔ¨Äerent modalities, certain prepro-\ncessing steps are applied. SpeciÔ¨Åcally , (i) to align the intensity distributions, we employ min-max normalization\nof all images; (ii) given that medical images can exist in either 3D or 2D formats (such as MRI being 3D and\nX-ray being 2D), we convert all 2D images to 3D simply by expanding an extra dimension. Consequently ,\nall images, irrespective of their original format, can be processed uniformly as 3D images; (iii) to ensure\nconsistent sizes across all images, we resize them using the torchvision.transforms.Resize function. F or\nheight and weight dimensions, we resize them to 512 √ó 512 for 2D images and 256 √ó 256 for 3D images because\n3D data has more slices, thus taking more computational memorization. F or depth dimension, since our visual\nencoder, a 3D Vision T ransformer (ViT), requires the input image sizes to be divisible by the patch size of\n|13\n32 √ó 32 √ó 4, we resize the depth dimension to the nearest multiple of 4 and will not surpass 64.\nImplementation. F or the visual encoder, we adopt a 12-layer 3D ViT with 768 feature dimensions and the\nperceiver is chosen as 6-layer transformer decoder with the learnable latent array in 32 √ó 5120 dimension, so\nthat all images will be embeded as a 32 √ó 5120 feature embedding after passing visual encoding and perceiver\naggregation. When inserting them into the text embedding, we will add two extra special tokens <image>,\n</image> at the beginning and ending respectively to distinguish them from common text tokens. F or the\nlarge language model, we initialize it with the MedLLaMA-13B model introduced by PMC-LLaMA [ 42],\nwhich has further Ô¨Åne-tuned the LLaMA-13B [ 38] model on the medical corpus. Our Ô¨Ånal model has 14B\nparameters.\nIn training, we vary the batch size, i.e., 1 batch size per device for 3D images and 4 batch size per device\nfor 2D images with 4-step gradient accumulation, and the max token length is set to be 2048. W e totally\ntrain the model for 8 epochs, 4 epochs for pre-training and 4 epochs for instruction-tuning. In the Ô¨Årst 1\nepoch, we freeze the language model to align image embedding space with that of texts, in the following\nepochs, all parameters are updated. T o improve the training speed, we adopt FSDP acceleration strategy [ 46],\ntogether with Automatic Mixed Precision (AMP) and gradient checkpointing [ 8]. All models are implemented\nin PyT orch and trained on 32 NVIDIA A100 GPUs with 80 GB memory .\n5 Evaluation\nIn this section, we Ô¨Årst introduce the evaluation metrics, subsequently , we present Ô¨Åve evaluation tasks and\ncorresponding datasets.\n5.1 Metrics\n5.1.1 Machine Rating\nT o evaluate the model‚Äôs performance across a range of tasks, distinct evaluation metrics are employed based on\nthe task type. F or tasks with pre-deÔ¨Åned answer choices, such as modality recognition and disease diagnosis,\nwe adopted standard metrics developed in the community , for example, F1 stands for ‚ÄúF1 score‚Äù, and ACC\nstands for ‚ÄúAccuracy‚Äù. Conversely , for tasks involving open-ended responses, like report generation and visual\nquestion answering (VQA), alternative evaluation metrics, like BLEU and ROUGE, are employed. BLEU\nstands for ‚ÄúBiLingual Evaluation Understudy‚Äù [\n33], ROUGE stands for ‚ÄúRecall-Oriented Understudy for\nGisting Evaluation‚Äù [ 22]. F or BLEU and ROUGE we all use 1-gram by default.\nIn addition, inspired by the score RadCliQ [ 43] designed speciÔ¨Åcally for evaluating generated chest X-ray\nreports, we also propose two new metrics, UMLS_Precision and UMLS_Recall, which aim to measure the\noverlapping ratio of medical-related words between ground truth and predicted response. SpeciÔ¨Åcally , given a\npair of ground-truth and prediction, we extract the medical-related words from them by using UniÔ¨Åed Medical\nLanguage System (UMLS) [\n4], and count the overlap words as true-positive. UMLS_Precision is deÔ¨Åned with\nclassical precision concept, i.e., the number of true-positive divides the whole generated medical-related word\nnumber. On the other hand, UMLS_Recall also follows the recall concept, i.e., the number of true-positive\nwords divides the total number of medical-related words in ground truth.\n5.1.2 Human Rating\nF or the sake of clinical utility , we further involve manual checking in the evaluation stage and compute the\nhuman rating score . Three radiologists with at least one-year clinical experienced are asked to rate the\nquality of the generated answers with scores from 0 to 5. Here are the speciÔ¨Åcs of each rating:\n0. Garbled - The content is incomprehensible and lacks any readability .\n1. Inaccurate - While readable, the content is entirely incorrect and lacks meaningful information.\n2. Partially Informative - The content holds some reference value, yet its correctness is subpar.\n3. Moderately Accurate - The content provides reference points, with approximately half of the\ninformation being correct, but containing several errors.\n|14\nPrediction1 : Findings: Large, enhancing right frontal extra-axial tumor with invasion through the right frontal \nbone into the scalp soft tissues. There is mass effect on the right frontal lobe, with a small amount of adjacent \nedema, and partial effacement of the frontal horns.\n0 1 2 3 4 5\nPrediction2 : 100% accurate.\n0 1 2 3 4 5\nPrediction3 : The tumor is located in the right frontal lobe. The tumor is surrounded by a large area of edema. \nThe tumor is in the region of the motor cortex. The tumor is surrounded by a large area of edema. The tumor is \nsurrounded by a large area of edema. \n0 1 2 3 4 5\nHuman Evaluation Website\nQuestion: Please write a radiology report consists of findings that explains this medical scan.\nGround truth : Findings: There is a large hyperintense T2/FLAIR, lobulated, and frond-like mass expanding \nthe right atrium of the lateral ventricle. The mass demonstrates avid, homogeneous enhancement and has \nlarge central flow voids. There is vasogenic edema within the right frontal and parietal lobes. There is \nenlargement of the right temporal horn, right atrium and third ventricle, as well as compression of the right \nfrontal horn and foramen of Monroe. There is leftward midline shift. The midbrain is displaced to the left \nand there is mild right uncal herniation.\nFigure 6 | Illustration of our human evaluation interface that we created for clinical experts to rate generated answers.\n4. Mostly Accurate - The content is almost entirely correct, with only a few omissions or errors present.\n5. Completely Correct - The content is accurate in its entirety , without any mistakes.\nT o facilitate this assessment, we have developed a human evaluation interface, visually presenting the generative\ninstances with images, as depicted in Fig. 6. In the evaluation, raters are provided with the images, the\nquestion, the correct answer, and a set of generated responses from diÔ¨Äerent models, arranged in a randomized\norder. The evaluation score given by the professional radiologists diÔ¨Äers from the automatic evaluation metrics,\noÔ¨Äering greater accuracy and Ô¨Çexibility . In the context of the report generation example shown in the Ô¨Ågure,\nthey focus on the most crucial aspects, rather than solely on word matching recall or precision.\nNote that, human rating is only performed for the open-ended tasks, i.e., medical VQA, report generation\nand rationale diagnosis. As for modality recognition and disease diagnosis, their answers are Ô¨Åxed without\nconfusion, thus, the automatic metrics can already well reÔ¨Çect the performance. Considering the cost for\nhuman rating, for each open-ended task, we randomly sample 400 test cases from Rad3D-series test split, as\nthey are generally collected from clinical practice across the world, and can represent real scenarios, resulting\nin 1.2K cases for human rating in total.\n5.2 Baseline Models\nT o our knowledge, there are currently no existing foundation models that can eÔ¨Äectively handle both 2D and\n3D radiology images. F or comparison, we pick three strong baseline models, OpenFlamingo [ 3], MedVInT [ 45]\nand Med-Flamingo [ 27], which have demonstrated eÔ¨Écacy in processing slices and making predictions.\n|15\nOpenFlamingo [ 3]. This is an open-source implementation of the prior state-of-the-art generalist visual\nlanguage model Flamingo [ 1], that was trained on large-scale data from general visual-language domain. W e\nutilized the released checkpoint for zero-shot evaluation in our study .\nMedVInT [ 45]. This is a visual instruction-tuned visual language model based on LLaMA [ 38], which\nwas trained on PMC-VQA [ 45]. W e directly use the released checkpoint of the MedVInT-TD model with\nPMC-LLaMA and PMC-CLIP backbone for zero-shot evaluation.\nMed-Flamingo [ 27]. This is a multimodal model developed based on OpenFlamingo-9B [ 3], that can handles\nmulti-image input interleaving with texts. W e use the released checkpoint for zero-shot evaluation.\n6 Results\nIn this section, we present the evaluation results on RadBench, including Ô¨Åve diÔ¨Äerent tasks (Fig. 7), namely ,\nmodality recognition, disease diagnosis, report generation, and visual question-answering on radiologic\nmodalities and anatomical regions. Results are summarized in T ab. 4 and in Fig. 8 we show the comparison\nresults on the diÔ¨Äerent subsets of RadBench.\nT able 4 | Comparison of proposed RadFM with SOT A model on RadBench. The benchmark includes 5 tasks, modality\nrecognition, disease diagnosis, medical visual question answering, report generation, and rationale diagnosis. ACC, F1, BLEU,\nROUGE, UMLS_Precision, and UMLS_Recall are reported, and the metrics refer to the average score on all test samples.\nNumbers within parentheses indicate 95% CI.\nT asks Metric OpenFlamingo [ 3] MedVInT [ 45] Med-Flamingo [ 27] RadFM\nModality Recognition ACC 49.41%\n(48.07%,50.72%)\n84.25%\n(83.18%,85.39%)\n32.87%\n(31.09%,34.22%)\n92.95%\n(91.15%,94.60%)\nDisease Diagnosis\nACC 50.43%\n(49.90%,50.96%)\n49.36%\n(48.91%,49.86%)\n50.13%\n(49.66%,50.61%)\n80.62%\n(80.16%,81.10%)\nF1 24.37%\n(23.70%,25.03%)\n66.99%\n(66.54%,67.38%)\n66.13%\n(65.70%,66.57%)\n80.10%\n(79.62%,80.10%)\nMedical VQA\nBLEU 13.23%\n( 13.11%, 13.35%)\n14.07%\n(13.79%, 14.28%)\n12.24%\n(10.67%, 13.59%)\n30.64%\n(29.35%,31.90%)\nROUGE 21.16%\n(20.93%,21.38%)\n18.85%\n(18.57%, 19.08%)\n24.59%\n(22.68%, 26.34%)\n36.38%\n(35.01%,37.58%)\nUMLS_Precision 14.55%\n(14.34%,14.76%)\n13.09%\n(12.78%,13.39%)\n12.63%\n(10.89%,14.36%)\n31.77%\n(29.76%,33.25%)\nUMLS_Recall 14.56%\n(14.33%,14.79%)\n10.52%\n(10.26%,10.78%)\n18.72%\n(16,43%,21.01%)\n24.93%\n(23.66%,26.01%)\nReport Generation\nBLEU 5.84%\n(5.44%,6.24%)\n1.73%\n(1.20%,2.30%)\n8.39%\n(7.89%,8.91%)\n12.81%\n(11.78%,13.96%)\nROUGE 6.37%\n(5.96%,6.81%)\n4.72%\n(4.21%,5.27%)\n8.78%\n(8.24%,9.32%)\n18.22%\n(17.29%,19.29%)\nUMLS_Precision 11.16%\n(10.09%12.24%)\n9.93%\n(7.51%,11.95%)\n2.65%\n(2.03%,3.27%)\n22.49%\n(20.82%,23.82%)\nUMLS_Recall 2.84%\n(2.46%,3.21%)\n1.45%\n(0.95%,1.95%)\n1.04%\n(0.81%,1.33%)\n12.07%\n(11.08%,13.17%)\nRationale Diagnosis\nBLEU 8.40%\n(7.78%,9.15%)\n0.09%\n(0.01%,0.17%)\n7.64%\n(6.80%,8.27%)\n34.60%\n(31.69%,37.74%)\nROUGE 9.17%\n(8.58%,9.76%)\n0.67%\n(0.52%,0.83%)\n7.38%\n(6.69%,7.90%)\n41.89%\n(39.20%,44.77%)\nUMLS_Precision 7.72%\n(6.52%,8.92%)\n9.07%\n(1.91%,16.23%)\n6.04%\n(4.92%,7.17%)\n42.95%\n(39.59%,46.22%)\nUMLS_Recall 2.82%\n(2.39%,3.34%)\n0.08%\n(0.01%,0.15%)\n2.16%\n(1.78%,2.62%)\n33.07%\n(30.93%,36.17%)\n|16\nTask Input Prompt RadFM Output\nDisease \nDiagnosis\n<image-1>Based on this X-ray, \nwhat type of spine disease is \nsuspected?\nOsteophytes, Disc space \nnarrowing\n<image-1>\nModality\nClassification\nWhat type of imaging modality \nis used to acquire the above \nimage? <image-1>\nX-ray\n<image-1>\nRadiology \nReport \nGeneration\nPatient: Age 35 years \nGender: Male.\nHeadache for two weeks. \nNausea. Bradypsychia.\nPlease caption this scan with \nfindings. <image-1>\nFindings: Juxta ventricular intraracial lesion on the left above \nbasal ganglia with marked mass effect and perifocal oedema. \nAnother mass in the suprasellar region. Both lesions express \nmarked restriction of diffusion and relatively homogeneous \nenhancement.<image-1>\nRationale\nDiagnosis\nPatient Data Age: 55 Gender: \nFemale Presented with seizures \nin the context of 2 weeks of \nheadache and labile BP. \nAdmitted to ICU for sedation \nand aggressive BP management.\n<image-1> <image-2> <image-\n3> Based on your extensive \nknowledge of radiology, \ndiagnosticate the given image, \nstarting with the established \nradiological features and \nconcluding with the ultimate \ndiagnosis. \nPosterior reversible encephalopathy syndrome. The \nRadiographic features can be summarized as follows. \nSignal characteristics of affected areas usually reflect \nvasogenic edema. T1 is hypointense in affected regions. T1 \nC+ (Gd) may show patchy variable enhancement in around \n35% of patients, in either a leptomeningeal or cortical pattern. \nT2 is hyperintense in affected regions. DWI is usually normal \nbut may be hyperintense due to edema or true restricted \ndiffusion.\n<image-1>\n<image-2>\n<image-3>\nMedical \nVQA\nWhat is the location of the \ncontrast blush in the image? \n<image-1>\nMidpoint of the left colon\n<image-1>\nFigure 7 | Examples of inputs and outputs of Ô¨Åve diÔ¨Äerent evaluation tasks obtained from RadFM.\n6.1 Modality Recognition\nRecognizing image modality is one basic skill expected of any advanced foundation models, however, as\ndepicted in T ab.\n4, the foundation models like OpenFlamingo and Med-Flamingo do not perform well on\nthis task, with a disappointing accuracy score (ACC) of merely 49.47%, highlighting their models cannot\ndistinguish various medical imaging modalities. In contrast, our proposed RadFM model outperforms the\nother competitors by a substantial margin. Note that, such task has been formulated as an open-ended\nquestion, where the model‚Äôs output needs to explicitly state the recognized modality as ‚ÄòCT‚Äô, ‚ÄòMRI‚Äô, etc.\n6.2 Disease Diagnosis\nIn clinical practice, the primary and indispensable function of any advanced medical foundation model is\nto provide assistance in disease diagnosis. As depicted in T ab.\n4, the performance of existing models has\nbeen rather poor on such task, with an achieved accuracy score (ACC) of only 50.43%. Considering that we\nprompt the problem with a judgment format, i.e., ‚ÄúDoes the patient have {disease}?‚Äù, this score is nearly\nrandom. In contrast, our model, RadFM, proves its superiority over existing methods with an ACC score of\n80.62%, making it more suited for real-world clinical applications. Essentially , this resembles a multi-label\n|17\nOpenFlamingo MedVInT Med-Flamingo RadFM\n0\n20\n40\n60\n80\n100\nVQA-RAD\nAVG\n0\n20\n40\n60\n80\n100\nSLAKE\nAVG\n0\n10\n20\n30\n40\n50\nRad3D-VQA\nAVG\n0\n10\n20\n30\n40\n50\nMIMIC-CXR\nAVG\n0\n5\n10\n15\n20\n25\nRad3D-Caption\nAVG\n0\n5\n10\n15\n20\n25\nMPx-Single\nAVG\n0\n5\n10\n15\n20\n25\nMPx-Multi\nAVG\n0\n10\n20\n30\n40\n50\nRad3D-Ration\nAVG\n0\n20\n40\n60\n80\n100\nRadChest-CT\nACC\n0\n20\n40\n60\n80\n100\nCXR-Mix\nACC\n0\n10\n20\n30\n40\n50\nPMC-VQA\nAVG\n0\n10\n20\n30\n40\n50\nPMC-CaseReport\nAVG\n0\n20\n40\n60\n80\n100\nRad3D-Modality\nACC\n0\n20\n40\n60\n80\n100\nVinDr-Mammo\nACC\n0\n20\n40\n60\n80\n100\nVinDr-SpineXr\nACC\n0\n20\n40\n60\n80\n100\nVinDr-PCXR\nACC\nFigure 8 | Comparison of RadFM with various SOT A models on diÔ¨Äerent subsets. Except from task-level, we also report\ndetailed results on each combined dataset. F or the dataset involving modality recognition and diagnosis, like Rad3D-Modality ,\nVinDr-Mammo, VinDr-SpineX, VinDr-PCXR, CXR-Mix, RadChest-CT, ACC scores are plotted in the Ô¨Ågure. F or the left datasets,\nA VG scores, denoting the average of the four evaluation metrics, i.e., BLEU, ROUGE, UMLS_Precision and UMLS_Recall, are\nplotted and the error bars are based on 95% CI.\nclassiÔ¨Åcation task, with over 5000 distinct categories, further compounding the challenges of models.\n6.3 Medical VQA\nIn contrast to the aforementioned tasks, Medical Visual Question Answering (VQA) represents a more\ncomprehensive and versatile challenge in the Ô¨Åeld of medical image analysis. In a clinical setting, patients and\nradiologists may pose a wide variety of questions related to medical images, ranging from simple inquiries\nabout image modality to more complex reasoning queries. Here, we combine 5 diÔ¨Äerent medical VQA datasets\nfor evaluation, namely , PMC-VQA, PMC-CaseReport, VQA-RAD, SLAKE and Rad3D-VQA.\nAs shown in T ab. 4 and Fig. 8, comparing to the second best model, MedVInT, which was speciÔ¨Åcally\ntrained on visual question answering, despite achieving better results on its in-domain PMC-VQA test set, its\ngeneralisation is quite poor, even though the task is still medical visual question answering. F or example,\nMedVInT struggles on contextual VQA, like PMC-CaseReport, which requires a deeper understanding of the\ncontext related to the patient and image, and on real 3D medical scans, like Rad3D-VQA, which requires\na model capturing the information form an extra image dimension. In contrast, our RadFM model shows\na substantial improvement in UMLS_Precision from 14.55% to 31.77% and UMLS_Recall from 14.56% to\n|18\nQualitative examples of medical visual question answering task\nRadFM PredictionInput Prompt\nA 25-year-old Iranian man presented with symptoms and signs of \nCushing‚Äôs syndrome (weight gain, proximal muscle weakness, abdominal \npurplish striae, hypertension, diabetes mellitus, and fragility fractures in \nthoracic and lumbar spines due to osteoporosis; Z score-5. The whole-\nbody CT scan was unremarkable. Bilateral adrenalectomy was performed. \nFour year later, follow-up chest CT scan revealed a 17 √ó 11 mm mass in \nthe posterior basal segment of the right lung.\nWhat is the abnormality in the posterior aspect of the inferior third of the \nright lung highly suggestive of? <image-1>\nThe abnormality is highly suggestive of a \nprimary \nlung neoplasm.\nInput Prompt\nThe postoperative laboratory results were notable for hemoglobin of 7. 0 \nU.  CT pulmonary angiography demonstrated a nearly occlusive thrombus \nin the left pulmonary artery (LPA) . Radiology indicated that it most likely \nrepresented tumor embolism, with additional bilateral segmental and \ndistal subsegmental emboli. \nWhat was noted about the right ventricle? <image-1>\n<image-1> \n<image-1>\nGround Truth\nThe abnormality is highly suggestive of an ectopic \nACTH-producing tumor.\nRadFM Prediction\nIt was noted that the right ventricle was dilated.\nGround Truth\nRight ventricular dilation was noted.\nRadFM PredictionInput Prompt\nA 19-year-old female reported to radiology for evaluation of palpable  \nmass in the submandibular region on the left side. The lesion had few \ncystic foci within and had increased vascularity on color Doppler. It was \nseparate from, but compressing the adjacent submandibul ar gland. \nMagnetic Resonance Imaging (MRI) of the neck was performed for the \ncharacterisation of the lesion.\n<image-1>Where was the lesion found on the MRI?‚Äô\nThe lesion was found in the left submandibular \nregion.\n<image-1>\nGround Truth\nThe lesion was found in the submandibular region on \nthe left side.\nFigure 9 | Qualitative examples of medical visual question answering (VQA). W e present several examples with answers\ngenerated by RadFM along with the target ground truth. The green color highlights accurate keywords, while the red color\nindicates prediction errors.\n24.93% across the whole test set, demonstrating its proÔ¨Åciency in comprehensively understanding the given\ntextual information and Ô¨Çexible adaptation to various complex clinical scenarios.\nQualitatively , as shown in Fig.\n9, RadFM demonstrates its ability to comprehend the questions and provide\nanswers in a consistent format, accurately addressing the questions. However, in some challenging cases,\nsuch as the Ô¨Årst example where the question pertains to the type of abnormality , the model faces diÔ¨Éculty\npredicting ‚Äúectopic ACTH-producing tumor‚Äù and mistakenly identiÔ¨Åes it as ‚Äúprimary lung neoplasm‚Äù, that\nrequires Ô¨Åne-grained discrimination within tumor types.\n|19\nQualitative examples of report generation task\nRadFM PredictionInput Prompt\n<image-1> <image-2> Patient Data  Age: 40 years  \nGender: Male Perianal purulent discharge and \ntenderness. Please caption this scan with findings. \n<image-3> <image-4> \nFinding: \nInjection of contrast via the orifice of purulent discharge showed \nopacification of the perianal abscess. No evidence of obvious contrast \nextravasation into the anal canal or rectum is seen.\nInput Prompt\nPatient Data  Age: 4 years  Gender: Male Both forearms \nare Fixed in a pronated position since birth. Describe the \nfindings of the medical image you see.\n<image-1> <image-2> <image-3> <image-4> \nReference Report\nFinding: Injection of contrast via the orifice of purulent discharge showed \nopacification of the left perianal abscess. No evidence of obvious contrast \nextravasation into the anal canal or rectum.\nRadFM Prediction\nFinding: Bilateral bony fusion at the proximal third of the radius and ulna. \nHypoplastic with posterior dislocation of the radial head\nReference Report\nFinding: Bilateral proximal radioulnar bony fusion of  length __ . Hypoplastic with \nposterior dislocation of the radial heads.\nRadFM PredictionInput Prompt\nPlease generate a radiology report for this scan <image-\n1>\nFindings: The heart size is moderately enlarged. The mediastinal silhouette and \nhilar contours are unchanged. A moderate to large right-sided pleural effusion \nis slightly increased in volume compared to prior examination with collapse of \nmuch of the right lower lobe and right middle lobe. There is also some \nconsolidation at the base of the right upper lobe which could be due to \ncompressive atelectasis. There is no left effusion. The upper lung zones appear \nclear. There is no pneumothorax.\n<image-1>\nReference Report\nFindings: The heart is mildly enlarged. There is mild unfolding and calcification \nalong the aorta. The mediastinal and hilar contours appear unchanged. There is \nsimilar moderate relative elevation of the right hemidiaphragm compared to the \nleft. There is no pleural effusion or pneumothorax. The lungs appear clear. Mild \ndegenerative changes are similar along the thoracic spine.\n<image-1> <image-2> <image-3> <image-4>\n<image-1> <image-2> <image-3> <image-4>\nFigure 10 | Qualitative examples of report generation. W e present several examples with reports generated by RadFM and\nreference reports. The green color highlights accurate keywords, while the red color indicates prediction errors. Additionally , the\nblue color denotes instances where the model missed this information that has been mentioned in the reference reports.\n6.4 Report Generation\nReport generation is a crucial and prominent use case for generative medical foundational models. Unlike\nMedical VQA, this application generally requires the model to emphasize clinically signiÔ¨Åcant observations\nbased on the image. As shown in T ab. 4, RadFM shows signiÔ¨Åcant improvement over existing models, across\nvarious metrics, particularly in relation to medical-speciÔ¨Åc terminology . F or instance, RadFM improves\nUMLS_Precision from 11.16% to 22.49%, and UMLS_Recall from 2.84% to 12.07%.\nIn Fig. 10, we provide qualitative examples of radiology reports generated by RadFM. It can be observed that\nthe model is capable of identifying the underlying diseases and, in some cases, performs exceptionally well.\nHowever, the report generated by RadFM may lack speciÔ¨Åc location information, such as the ‚Äòleft‚Äô or ‚Äòright‚Äô\nof an anatomy region.\n|20\n6.5 Rationale Diagnosis\nIn addition to basic diagnosis, the ability to scrutinize diagnostic prediction outcomes is crucial, particularly\nin light of the stringent demands for precision and interpretability within medical contexts. Much like\nreport generation, this task also requires proÔ¨Åciency in generating extended paragraphs and comprehensive\nunderstanding on medical knowledge.\nAs indicated in T able 4, RadFM is the only model that can eÔ¨Äectively respond on this task. RadFM\noutperforms traditional BLEU and ROUGE scores by 8.56% and 15.46% respectively . Moreover, it exhibits\nsigniÔ¨Åcant improvements in UMLS_Precision and UMLS_Recall scores, showcasing advancements of 23.16%\nand 10.96% respectively .\nFig. 11 shows two rationale diagnosis cases. The Ô¨Årst case is a patient with pulmonary embolism and the\nlatter is with subarachnoid haemorrhage. On both cases, RadFM can make accurate diagnosis in free form\nand give further related radiologic reasoning. However, the limitation can also be observed that the reasoning\nresults are still general and more like background medical knowledge, yet not speciÔ¨Åc to the input case.\nQualitative examples of rationale diagnosis task\nRadFM PredictionInput Prompt\nThis case is a 30-year-old male patient. Believed to have \nrazor in mouth. Evaluate for razor in mouth and body. \nBased on your extensive knowledge of radiology, \ndiagnosticate the given image, starting with the \nestablished radiological features and concluding with the \nultimate diagnosis. <image-1> <image-2> <image-3> > \n<image-4>\nThe Radiographic features can be summarized as follows. \nMetal, glass, and stone \nforeign bodies can be visualized very well using conventional plain film \nradiography. The radiological appearance depends on the x-ray attenuation of \nthe foreign body, surrounding structures, and any overlying structures that may \nveil the object. The anatomical location affects the radiopacity and magnification \nrate of the foreign body.\nThe diagnosis is Foreign body (teeth).\nReference Ground Truth\nThe Radiographic features can be summarized as follows. Metal, glass, and stone \nforeign bodies can be visualized very well using conventional plain film \nradiography. The radiological appearance depends on the x-ray attenuation of \nthe foreign body, surrounding structures, and any overlying structures that may \nveil the object. The anatomical location affects the radiopacity and magnification \nrate of the foreign body. \nThe diagnosis is Swallowed foreign body - razor.\n<image-1> <image-2> <image-3> <image-4>\nRadFM PredictionInput Prompt\nReferred for incidental left hepatic lobe lesion seen on \nultrasound. Does the image exhibit any Primary hepatic \nlymphoma? Systematically diagnose the given image, \ncommencing with the typical radiologic features of \nPrimary hepatic lymphoma. <image-1><image-\n2><image-3><image-4>\nThe Radiographic features can be summarized as follows. \nT1 hypointense relative to \nthe remainder of the liver. T2 often shows variable intensity. T1 C+ (Gd) can show \npoor enhancement but may have faint peripheral rim enhancement. DWI-ADC \nrestricts diffusion, even compared to liver parenchyma, due to hypercellularity. \nThe answer is No.\nReference Ground Truth\nThe Radiographic features can be summarized as follows. T1 hypointense relative to \nthe remainder of the liver. T2 often shows variable intensity. T1 C+ (Gd) can show \npoor enhancement but may have faint peripheral rim enhancement. DWI-ADC \nrestricts diffusion, even compared to liver parenchyma, due to hypercellularity. \nThe answer is No.\n<image-1> <image-2> <image-3> <image-4>\nFigure 11 | Qualitative examples of rationale diagnosis. W e present several examples generated by RadFM and reference ground\ntruth. The green color highlights accurate keywords, while the red color indicates prediction errors. Additionally , the blue color\ndenotes instances where the model missed this information that has been mentioned in the reference ground truth.\n6.6 Human Rating\nIn Fig. 12, we show the human rating results of all the models. As shown on the left of the Ô¨Ågure, RadFM\ncan achieve higher scores on all the three generative-based tasks compared with former methods. On the\nright, we further show the relative comparison between RadFM and a certain model. In all cases, outputted\nresults from RadFM are preferred by human clinicians. It is worth highlighting that this conclusion still holds\ntrue even if comparing RadFM with the best results given by all the other models each time.\n|21\n214\n189\n257\n176\n187\n116\n10\n24\n27\n0 100 200 300 400\nRationale\nCaption\nVQA\nRadFM vs MedVInT\nWin Tie Lose\nMedical VQA\nReport\nGeneration\nRationale\nDiagnosis\nOpenFlamingo 2.21 0.89 1.09\nMedVInT 1.44 1.16 0.83\nMed-Flamingo 1.74 0.86 1.03\nRadFM 2.87 1.88 1.76\n0.00\n1.00\n2.00\n3.00\nRating Score\n214\n235\n236\n150\n140\n140\n36\n25\n24\n0 200 400\nRationale Diagnosis\nReport Generation\nMedical VQA\nRadFM vs Med - Flamingo  \n187\n224\n184\n191\n166\n145\n22\n10\n71\n0 200 400\nRationale Diagnosis\nReport Generation\nMedical VQA\nRadFM vs OpenFlamingo\n214\n189\n257\n176\n187\n116\n10\n24\n27\n0 200 400\nRadFM vs MedVInT\n171\n191\n160\n183\n179\n147\n46\n30\n93\n0 200 400\nRadFM vs Others\nFigure 12 | Comparison of RadFM with other methods on human rating scores. On the left, we show the absolute human\nrating scores of diÔ¨Äerent methods on the three generative tasks. i.e., VQA, Report Generation and Rationale Diagnosis. On the\nright, we show the relative comparison. Each sub-Ô¨Ågure in right shows the number of RadFM win/tie/lose cases when comparing\nagainst a certain model. It is worth emphasising that ‚ÄúRadFM vs Others‚Äù refers to compare RadFM with the best of all other\nmodels and RadFM will only win when its score surpasses all of others and lose when it is worse than any of others.\n7 Discussion\n7.1 RadFM is the Ô¨Årst foundation model that uniÔ¨Åed 2D and 3D radiologic images.\nIn the Ô¨Åeld of radiologic images, one signiÔ¨Åcant challenge on developing foundation model lies on the disparity\nof image dimensions, i.e., medical scans are either 2D or 3D, posing challenges on integrating real 3D MRI\nor CT images alongside with 2D images like X-rays or ultrasounds. As a consequence, the development\nof foundational models has been signiÔ¨Åcantly impeded, with most current models only accommodating 2D\nimages. T o overcome these limitations, we propose a new training structure that uniÔ¨Åes 2D and 3D images,\nallowing a wide variety of real clinical images to be fed into one network. By unifying the training process, our\nmodel beneÔ¨Åts from a more comprehensive understanding of the diverse clinical images, leading to improved\nperformance and versatility . Additionally , to facilitate research and foster collaboration in the Ô¨Åeld, we collect\na new medical multimodal dataset, containing 16M 2D or 3D medical scans with text descriptions or labels.\n7.2 RadFM uniÔ¨Åes the medical tasks with a generative model.\nWhile developing AI for medicine, traditional approaches consider a divide-and-conquer idea, that tackles\na myriad of speciÔ¨Åc tasks individually , such as diagnosis, report generation, and medical visual question\nanswering, resulting in separated approaches with limited generalization ability and eÔ¨Éciency . Here, we\nformulate diverse medical tasks as multi-modal question-answering and develop a generative visual-language\nmodel RadFM that can answer arbitrary questions or follow instructions. By unifying the tasks, RadFM\nachieves promising performance across a wide spectrum of clinical tasks. On the medical VQA task, RadFM\nsurpasses the performance of MedVINT, a pre-trained model trained solely on a single Medical VQA dataset.\n7.3 RadFM supports multiple images as input.\nTill now, most existing multi-modal foundation models in the medical Ô¨Åeld have been limited to supporting\nonly single image input per interaction. However, such design poses critical challenges in medical scenarios\nwhere diagnosis and treatment decisions often necessitate longitudinal clinical follow-ups, that comprise a series\nof radiologic images. T o overcome this limitation and pave the way for more comprehensive medical image\nanalysis, our proposed RadFM supports multi-image input. T o support the training, our constructed dataset\nis largely composed of multi-image input data, and our innovative training Ô¨Çow seamlessly accommodates this\nunique medical scenery , fostering advancements in medical image analysis.\n|22\n7.4 A general evaluation benchmark for radiology foundation models.\nEvaluating the performance of medical foundation models is a complex undertaking due to the specialized\nnature of medical tasks. In the pursuit of advancing radiology foundation models, we propose RadBench,\na novel benchmark that encompasses a diverse range of medical scenarios. The benchmark comprises 5\ntasks, namely modality recognition, disease diagnosis, visual question answering, and report generation. By\nincorporating both 2D and 3D images, RadBench oÔ¨Äers a more comprehensive and realistic evaluation platform\ncompared to existing benchmarks. Additionally , as existing evaluation metrics are primarily designed for\ngeneral natural language tasks, which may not adequately capture the intricacies and nuances speciÔ¨Åc to\nmedical image analysis, thus may not reÔ¨Çect the model‚Äôs true capabilities in real-world clinical scenarios. T o\naddress this limitation, we propose two new evaluation metrics, namely UMLS_Precision and UMLS_Recall.\nUnlike conventional metrics, UMLS Precision and Recall are tailored to measure the model‚Äôs performance\nin medical tasks. By leveraging the UniÔ¨Åed Medical Language System (UMLS), a comprehensive medical\nknowledge resource, these metrics provide a more tailored evaluation, ensuring that the model‚Äôs outputs align\nwith medical domain expertise.\n7.5 The superiority of RadFM.\nAs shown in T ab. 4 and Fig. 1, RadFM outperforms previous methods by a signiÔ¨Åcant margin across all Ô¨Åve\ntasks, showcasing its exceptional capabilities. Notably , RadFM excels in particularly challenging tasks such as\nmedical VQA, report generation, and rationale diagnosis, which demand profound understanding of both\ntextual information and images. In medical VQA, the questions can be drastically varying, from simple\nqueries like ‚ÄúWhat modality is the given image?‚Äù to more complex and context-rich questions, such as ‚ÄúBased\non the provided images, patient data (age, gender, medical history), can you identify the disease that is\ncommonly associated with such radiological manifestations?‚Äù The complexity of questions makes medical VQA\na comprehensive and versatile task. By integrating visual and textual information, RadFM can handle these\nvarying question types, delivering accurate and meaningful answers. Similarly , in report generation, RadFM\nshowcases signiÔ¨Åcant improvement. The model‚Äôs ability to discern relevant information from the provided\nimages and weave it cohesively with textual prompts leads to highly informative and contextually-rich reports,\nsetting it apart from traditional methods. Overall, the performance of RadFM across these diverse tasks\nconÔ¨Årms its versatility and transformative potential in medical image analysis.\n7.6 Limitations.\nDespite our eÔ¨Äorts in developing a foundation model for radiology , RadFM still exhibits several limitations:\nFirst, the capacity to generate meaningful and accurate long sentences remains underdeveloped, causing the\nfoundation models still far from clinically useful. As demonstrated in T ab. 4, for rationale diagnosis and\nreport generation, the quantitative results surpass previous works but are still far from practical satisfactory .\nIn human rating, similar results are also observed. As shown in Fig. 6, none of the model gets over score\n3 that represents moderately accurate, showing that there is still long way to go for developing generalist\nmedical foundation models.\nSecond, the proportion of actual 3D images in the data remains limited. As illustrated in Fig. 4, although we\nattempt to compensate for the lack of 3D images, yet 2D images remain to be dominating.\nThird, the automatic evaluation metrics fall short of expectations. Compared to general contexts where the\nemphasis is placed on the overall coherence and Ô¨Çuency of sentences, medical texts prioritize precision in key\nstatements and contain many synonyms, like ‚ÄòMRI‚Äô and ‚ÄòMagnetic Resonance Imaging‚Äô, overlooking minor\nsyntax errors. Although we employ UMLS_Precision and UMLS_Recall to mitigate this issue, they do not\nfully reÔ¨Çect true performance. On the other hand, though human evaluation is Ô¨Çexible and accurate, it is\ncostly and cannot be carried out on a large scale. A robust automatic evaluation metric is essential to guide\nthe construction of reliable and robust medical foundation models.\nLastly, as the 3D images in our dataset are downloaded from the internet, some metadata is missing, for\nexample, the imaging spacing. Such lack of precise distance measurement makes it impossible to make certain\nstatements, such as ‚ÄúThe tumor is 3cm large‚Äù. This speciÔ¨Åcation is crucial for report writing.\n|23\n7.7 Conclusion.\nIn conclusion, in this paper, we have constructed a complete set of medical foundation model-building processes,\nincluding data collection, problem formulation, model design, training, and evaluation. W e construct the\nlargest medical multi-modal database in this paper. In model capabilities, compared to existing work, our\nmodel is able to process multiple 3D or 2D image inputs interleaved with texts, which Ô¨Åts the practical usage\nmore. W e surpass the latest open-source multi-modal foundation model signiÔ¨Åcantly . W e will release all\ncorresponding data, codes, and models. W e believe this can greatly promote the development of medical\nfoundation models.\n8 Data availability\nThe datasets used for pre-training are listed in T able\n5.\nT able 5| Data availability . F or the Rad3D-series and MPx-series datasets, researchers may obtain by reasonable request from\nthe authors.\nDataset Name Link Access\nRad3D-series - Restricted Access\nMPx-series - Restricted Access\nPMC-Inline https://huggingface.co/datasets/chaoyi-wu/PMC-Inline Open Access\nPMC-CaseReport https://huggingface.co/datasets/chaoyi-wu/PMC-\nCaseReport_original\nhttps://huggingface.co/datasets/chaoyi-wu/PMC-CaseReport\nOpen Access\nVinDr-Mammo [\n29] https://www.physionet.org/content/vindr-mammo/1.0.0/ Credentialed Access\nVinDr-SpineXR [ 30] https://www.physionet.org/content/vindr-spinexr/1.0.0/ Credentialed Access\nVinDr-PCXR [ 31] https://physionet.org/content/vindr-pcxr/1.0.0/ Credentialed Access\nRadChest-CT [ 3] https://zenodo.org/record/6406114/ Credentialed Access\nPMC-OA [ 23] https://huggingface.co/datasets/axiong/pmc_oa_beta Open Access\nPMC-VQA [ 45] https://huggingface.co/datasets/xmcmic/PMC-VQA Open Access\nVQA-RAD [ 19] https://osf.io/89kps/ Open Access\nSLAKE [ 24] https://www.med-vqa.com/slake/ Open Access\nMIMIC-CXR [ 17] https://physionet.org/content/mimic-cxr/2.0.0 Credentialed Access\nVinDr-CXR [ 28] https://physionet.org/content/vindr-cxr/1.0.0/ Credentialed Access\nNIH ChestXray14 [ 40] https://nihcc.app.box.com/v/ChestXray-\nNIHCC/folder/36938765345\nOpen Access\nCheXpert [ 13] https://aimi.stanford.edu/chexpert-chest-x-rays Open Access\nCovid-CXR2 [ 34] https://www.kaggle.com/datasets/andyczhao/covidx-cxr2 Open Access\nNLM-TB [ 15] https://openi.nlm.nih.gov/imgs/collections/NLM-\nMontgomeryCXRSet.zip\nhttps://openi.nlm.nih.gov/imgs/collections/ChinaSet_AllFiles.zip\nOpen Access\nObject-CXR [\n12] https://web.archive.org/web/20201127235812/https:\n//jfhealthcare.github.io/object-CXR/\nOpen Access\nOpenI [ 9] https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-\nuniversity\nOpen Access\nRSNA [ 36] https://www.rsna.org/education/ai-resources-and-training/ai-image-\nchallenge/rsna-pneumonia-detection-challenge-2018\nOpen Access\nSIIM-ACR [ 11] https://www.kaggle.com/datasets/jesperdramsch/siim-acr-\npneumothorax-segmentation-data\nOpen Access\n9 CODE availability\nThe code is available on GitHub at https://github.com/chaoyi-wu/RadFM\n|24\nReferences\n[1] Jean-Baptiste Alayrac, JeÔ¨Ä Donahue, Pauline Luc, Antoine Miech, Iain Barr, Y ana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, T engda\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning.\nArXiv, abs/2204.14198, 2022.\n[2] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre T achard\nPassos, Siamak Shakeri, Emanuel T aropa, Paige Bailey , Z. Chen, Eric Chu, J. Clark, Laurent El Shafey ,\nY anping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi T ay , Kefan Xiao, Y uanzhong Xu, Y ujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury , Siddhartha Brahma,\nKevin Michael Brooks, Michele Catasta, Y ongzhou Cheng, Colin Cherry , Christopher A. Choquette-Choo,\nAakanksha Chowdhery , C Cr√©py , Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C.\nD‚Äôiaz, Nan Du, Ethan Dyer, Vladimir F einberg, F an F eng, Vlad Fienber, Markus F reitag, Xavier Garc√≠a,\nSebastian Gehrmann, Lucas Gonz√°lez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, An Ren Hu, JeÔ¨Ärey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski,\nW en Hao Jia, Kathleen Kenealy , Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin\nLee, Eric Li, Mu-Li Li, W ei Li, Y aguang Li, Jun Y u Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu,\nF rederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, V edant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex\nPolozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley , Alexandra Ros, Aurko Roy ,\nBrennan Saeta, Rajkumar Samuel, Renee Marie Shelby , Ambrose Slone, Daniel Smilkov, David R. So,\nDaniela Sohn, Simon T okumine, Dasha V alter, Vijay V asudevan, Kiran V odrahalli, Xuezhi W ang, Pidong\nW ang, Zirui W ang, T ao W ang, John Wieting, Y uhuai W u, Ke Xu, Y unhan Xu, Lin W u Xue, Pengcheng\nYin, Jiahui Y u, Qiaoling Zhang, Steven Zheng, Ce Zheng, W ei Zhou, Denny Zhou, Slav Petrov, and\nY onghui W u. Palm 2 technical report. ArXiv, abs/2305.10403, 2023.\n[3] Anas A wadalla, Irena Gao, Joshua Gardner, Jack Hessel, Y usuf Hanafy , W anrong Zhu, Kalyani Marathe,\nY onatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang W ei Koh, Gabriel Ilharco, Mitchell\nW ortsman, and Ludwig Schmidt. OpenÔ¨Çamingo, Mar. 2023.\n[4] Olivier Bodenreider. The uniÔ¨Åed medical language system (umls): integrating biomedical terminology .\nNucleic acids research , 32(suppl_1):D267‚ÄìD270, 2004.\n[5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\n[6] Jieneng Chen, Y ongyi Lu, Qihang Y u, Xiangde Luo, Ehsan Adeli, Y an W ang, Le Lu, Alan L Y uille, and\nY uyin Zhou. T ransunet: T ransformers make strong encoders for medical image segmentation. arXiv\npreprint arXiv:2102.04306 , 2021.\n[7] Qiuhui Chen, Xinyue Hu, Zirui W ang, and Yi Hong. Medblip: Bootstrapping language-image pre-training\nfrom 3d medical images and texts. arXiv preprint arXiv:2305.10799 , 2023.\n[8] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. T raining deep nets with sublinear memory\ncost. arXiv preprint arXiv:1604.06174 , 2016.\n[9] Dina Demner-F ushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer\nAntani, George R Thoma, and Clement J McDonald. Preparing a collection of radiology examinations\nfor distribution and retrieval. Journal of the American Medical Informatics Association , 23(2):304‚Äì310,\n2016.\n[10] Rachel Lea Draelos, David Dov, Maciej A Mazurowski, Joseph Y. Lo, Ricardo Henao, GeoÔ¨Ärey D. Rubin,\nand Lawrence Carin. Rad-chestct dataset, Oct. 2020.\n[11] Ross W Filice, Anouk Stein, Carol C W u, V eronica A Arteaga, Stephen Borstelmann, Ramya Gaddikeri,\net al. Crowdsourcing pneumothorax annotations using machine learning annotations on the nih chest\nx-ray dataset. Journal of digital imaging , 33:490‚Äì496, 2020.\n|25\n[12] JF Healthcare. Object-cxr - automatic detection of foreign objects on chest x-rays. 2020.\n[13]\nJeremy Irvin et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert\ncomparison. In Proceedings of the AAAI conference on artiÔ¨Åcial intelligence , volume 33, pages 590‚Äì597,\n2019.\n[14] F abian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F Jaeger, Simon Kohl, Jakob\nW asserthal, Gregor Koehler, T obias Norajitra, Sebastian Wirkert, et al. nnu-net: Self-adapting framework\nfor u-net-based medical image segmentation. arXiv preprint arXiv:1809.10486 , 2018.\n[15] Stefan Jaeger, Sema Candemir, Sameer Antani, Y√¨-Xi√°ng J W√°ng, Pu-Xuan Lu, and George Thoma. T wo\npublic chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in\nmedicine and surgery , 4(6):475, 2014.\n[16] Andrew Jaegle, F elix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.\nPerceiver: General perception with iterative attention. In International conference on machine learning ,\npages 4651‚Äì4664. PMLR, 2021.\n[17] Alistair EW Johnson, T om J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren,\nChih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identiÔ¨Åed publicly available database\nof chest radiographs with free-text reports. ScientiÔ¨Åc data , 6(1):317, 2019.\n[18] Ranjay Krishna, Y uke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nY annis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International journal of computer vision , 123:32‚Äì73, 2017.\n[19] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-F ushman. A dataset of clinically\ngenerated visual questions and answers about radiology images. ScientiÔ¨Åc data , 5(1):1‚Äì10, 2018.\n[20] Chunyuan Li, CliÔ¨Ä W ong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Y ang, T ristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. Llava-med: T raining a large language-and-vision assistant for\nbiomedicine in one day . arXiv preprint arXiv:2306.00890 , 2023.\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023.\n[22] Chin-Y ew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, pages 74‚Äì81, 2004.\n[23] W eixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi W u, Y a Zhang, Y anfeng W ang, and W eidi\nXie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. arXiv preprint\narXiv:2303.07240, 2023.\n[24] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Y an Y ang, and Xiao-Ming W u. Slake: A semantically-labeled\nknowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International\nSymposium on Biomedical Imaging (ISBI) , pages 1650‚Äì1654. IEEE, 2021.\n[25] Michael Moor, Oishi Banerjee, Zahra F H Abad, Harlan M. Krumholz, Jure Leskovec, Eric J. T opol, and\nPranav Rajpurkar. F oundation models for generalist medical artiÔ¨Åcial intelligence. Nature, 616:259‚Äì265,\n2023.\n[26] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J\nT opol, and Pranav Rajpurkar. F oundation models for generalist medical artiÔ¨Åcial intelligence. Nature,\n616(7956):259‚Äì265, 2023.\n[27] Michael Moor, Qian Huang, Shirley W u, Michihiro Y asunaga, Cyril Zakka, Y ash Dalmia, Eduardo Pontes\nReis, Pranav Rajpurkar, and Jure Leskovec. Med-Ô¨Çamingo: A multimodal medical few-shot learner. July\n2023. arXiv:2307.15189.\n[28] Ha Q Nguyen, Khanh Lam, Linh T Le, Hieu H Pham, Dat Q T ran, Dung B Nguyen, et al. Vindr-cxr:\nAn open dataset of chest x-rays with radiologist‚Äôs annotations. ScientiÔ¨Åc Data , 9(1):429, 2022.\n[29] Hieu T Nguyen, Ha Q Nguyen, Hieu H Pham, Khanh Lam, Linh T Le, Minh Dao, and V an V u. Vindr-\nmammo: A large-scale benchmark dataset for computer-aided diagnosis in full-Ô¨Åeld digital mammography .\nScientiÔ¨Åc Data , 10(1):277, 2023.\n|26\n[30] Hieu T Nguyen, Hieu H Pham, Nghia T Nguyen, Ha Q Nguyen, Thang Q Huynh, Minh Dao, and V an V u.\nVindr-spinexr: A deep learning framework for spinal lesions detection and classiÔ¨Åcation from radiographs.\nIn Medical Image Computing and Computer Assisted Intervention‚ÄìMICCAI 2021: 24th International\nConference, Strasbourg, France, September 27‚ÄìOctober 1, 2021, Proceedings, Part V 24 , pages 291‚Äì301.\nSpringer, 2021.\n[31] Ngoc H Nguyen, Hieu H Pham, Thanh T T ran, T uan NM Nguyen, and Ha Q Nguyen. Vindr-pcxr: An\nopen, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children.\nmedRxiv, pages 2022‚Äì03, 2022.\n[32] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[33] Kishore Papineni, Salim Roukos, T odd W ard, and W ei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pages 311‚Äì318, 2002.\n[34] Maya Pavlova et al. Covid-net cxr-2: An enhanced deep convolutional neural network design for detection\nof covid-19 cases from chest x-ray images. Frontiers in Medicine , 9, 2022.\n[35] Christoph Schuhmann, Romain Beaumont, Richard V encu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell W ortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. Advances in Neural Information Processing\nSystems, 35:25278‚Äì25294, 2022.\n[36] George Shih, Carol C W u, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, et al. Augmenting the\nnational institutes of health chest radiograph dataset with expert annotations of possible pneumonia.\nRadiology: ArtiÔ¨Åcial Intelligence , 1(1):e180041, 2019.\n[37] Ekin Tiu, Ellie T alius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, and Pranav Rajpurkar. Expert-\nlevel detection of pathologies from unannotated chest x-ray images via self-supervised learning. Nature\nBiomedical Engineering , 6(12):1399‚Äì1406, 2022.\n[38] Hugo T ouvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e\nLacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, F aisal Azhar, et al. Llama: Open and eÔ¨Écient\nfoundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[39] T ao T u, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew\nCarroll, Chuck Lau, Ryutaro T anno, Ira Ktena, et al. T owards generalist biomedical ai. arXiv preprint\narXiv:2307.14334, 2023.\n[40] Xiaosong W ang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.\nChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classiÔ¨Åcation and\nlocalization of common thorax diseases. In CVPR, pages 2097‚Äì2106, 2017.\n[41] Chaoyi W u, Xiaoman Zhang, Y anfeng W ang, Y a Zhang, and W eidi Xie. K-diag: Knowledge-enhanced\ndisease diagnosis in radiographic imaging. MICCAI Workshop , 2023.\n[42] Chaoyi W u, Xiaoman Zhang, Y a Zhang, Y anfeng W ang, and W eidi Xie. Pmc-llama: F urther Ô¨Ånetuning\nllama on medical papers. arXiv preprint arXiv:2304.14454 , 2023.\n[43] F eiyang Y u, Mark Endo, Rayan Krishnan, Ian Pan, Andy T sai, Eduardo Pontes Reis, Eduardo Kaiser\nUrurahy Nunes F onseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al.\nEvaluating progress in automatic chest x-ray radiology report generation. medRxiv, pages 2022‚Äì08, 2022.\n[44] Xiaoman Zhang, Chaoyi W u, Y a Zhang, Y anfeng W ang, and W eidi Xie. Knowledge-enhanced pre-training\nfor auto-diagnosis of chest radiology images. Nature Communication , 2023.\n[45] Xiaoman Zhang, Chaoyi W u, Ziheng Zhao, W eixiong Lin, Y a Zhang, Y anfeng W ang, and W eidi Xie.\nPmc-vqa: Visual instruction tuning for medical visual question answering. ArXiv, abs/2305.10415, 2023.\n[46] Y anli Zhao, Andrew Gu, Rohan V arma, Liang Luo, Chien-Chin Huang, Min Xu, Less W right, Hamid\nShojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data\nparallel. arXiv preprint arXiv:2304.11277 , 2023.\n[47] W anrong Zhu, Jack Hessel, Anas A wadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex F ang, Y oungjae Y u,\nLudwig Schmidt, William Y ang W ang, and Y ejin Choi. Multimodal C4: An open, billion-scale corpus of\nimages interleaved with text. arXiv preprint arXiv:2304.06939 , 2023.\n|27\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nmiclaimchecklistrepo.docx\n\u0000atWXieepc.pdf\n\u0000atWXiers.pdf"
}