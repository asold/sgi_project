{
    "title": "Identification of Congenital Valvular Murmurs in Young Patients Using Deep Learning-Based Attention Transformers and Phonocardiograms",
    "url": "https://openalex.org/W4391130279",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2761475541",
            "name": "Mohanad Alkhodari",
            "affiliations": [
                "Khalifa University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A246110353",
            "name": "Leontios J. Hadjileontiadis",
            "affiliations": [
                "Khalifa University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A737942982",
            "name": "Ahsan H. Khandoker",
            "affiliations": [
                "Khalifa University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3033776559",
        "https://openalex.org/W3002652237",
        "https://openalex.org/W2166474541",
        "https://openalex.org/W3004727408",
        "https://openalex.org/W2949916237",
        "https://openalex.org/W2122010694",
        "https://openalex.org/W2884472067",
        "https://openalex.org/W2900987296",
        "https://openalex.org/W3159550770",
        "https://openalex.org/W3178921142",
        "https://openalex.org/W3122189984",
        "https://openalex.org/W3209297158",
        "https://openalex.org/W3039053306",
        "https://openalex.org/W2198753695",
        "https://openalex.org/W3113332308",
        "https://openalex.org/W2811161392",
        "https://openalex.org/W2576404523",
        "https://openalex.org/W3211390666",
        "https://openalex.org/W3216962276",
        "https://openalex.org/W6734201357",
        "https://openalex.org/W2790928799",
        "https://openalex.org/W3001215021",
        "https://openalex.org/W4225302726",
        "https://openalex.org/W4312832682",
        "https://openalex.org/W4386615065",
        "https://openalex.org/W3191193899",
        "https://openalex.org/W2127228666",
        "https://openalex.org/W3169147854",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4213451870",
        "https://openalex.org/W3167986199",
        "https://openalex.org/W4307090970",
        "https://openalex.org/W2148143831",
        "https://openalex.org/W4323027672",
        "https://openalex.org/W4323027649",
        "https://openalex.org/W4323027594",
        "https://openalex.org/W4323027434",
        "https://openalex.org/W4323027476",
        "https://openalex.org/W4313362601",
        "https://openalex.org/W3135128169",
        "https://openalex.org/W2002378318",
        "https://openalex.org/W4293249042",
        "https://openalex.org/W6716515988",
        "https://openalex.org/W6638659990",
        "https://openalex.org/W2941342993",
        "https://openalex.org/W2416220028"
    ],
    "abstract": "One in every four newborns suffers from congenital heart disease (CHD) that causes defects in the heart structure. The current gold-standard assessment technique, echocardiography, causes delays in the diagnosis owing to the need for experts who vary markedly in their ability to detect and interpret pathological patterns. Moreover, echo is still causing cost difficulties for low- and middle-income countries. Here, we developed a deep learning-based attention transformer model to automate the detection of heart murmurs caused by CHD at an early stage of life using cost-effective and widely available phonocardiography (PCG). PCG recordings were obtained from 942 young patients at four major auscultation locations, including the aortic valve (AV), mitral valve (MV), pulmonary valve (PV), and tricuspid valve (TV), and they were annotated by experts as absent, present, or unknown murmurs. A transformation to wavelet features was performed to reduce the dimensionality before the deep learning stage for inferring the medical condition. The performance was validated through 10-fold cross-validation and yielded an average accuracy and sensitivity of 90.23 % and 72.41 %, respectively. The accuracy of discriminating between murmurs' absence and presence reached 76.10 % when evaluated on unseen data. The model had accuracies of 70 %, 88 %, and 86 % in predicting murmur presence in infants, children, and adolescents, respectively. The interpretation of the model revealed proper discrimination between the learned attributes, and AV channel was found important (score 0.75) for the murmur absence predictions while MV and TV were more important for murmur presence predictions. The findings potentiate deep learning as a powerful front-line tool for inferring CHD status in PCG recordings leveraging early detection of heart anomalies in young people. It is suggested as a tool that can be used independently from high-cost machinery or expert assessment.",
    "full_text": "IEEE JOURNAL OF BIOMEDICAL HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2023 1\nIdentification of congenital valvular murmurs in young patients using\ndeep learning-based attention transformers and phonocardiograms\nMohanad Alkhodari, Student Member, IEEE, Leontios J. Hadjileontiadis, Senior Member, IEEE, and Ahsan H.\nKhandoker, Senior Member, IEEE\nAbstract— One in every four newborns suffers from congenital\nheart disease (CHD) that causes defects in the heart structure. The\ncurrent gold-standard assessment technique, echocardiography,\ncauses delays in the diagnosis owing to the need for experts who\nvary markedly in their ability to detect and interpret pathological\npatterns. Moreover, echo is still causing cost difficulties for low-\nand middle-income countries. Here, we developed a deep learning-\nbased attention transformer model to automate the detection of\nheart murmurs caused by CHD at an early stage of life using\ncost-effective and widely available phonocardiography (PCG). PCG\nrecordings were obtained from 942 young patients at four major\nauscultation locations, including the aortic valve (AV), mitral valve\n(MV), pulmonary valve (PV), and tricuspid valve (TV), and they were\nannotated by experts as absent, present, or unknown murmurs. A\ntransformation to wavelet features was performed to reduce the\ndimensionality before the deep learning stage for inferring the\nmedical condition. The performance was validated through 10-fold\ncross-validation and yielded an average accuracy and sensitivity of\n90.23% and 72.41%, respectively. The accuracy of discriminating\nbetween murmurs’ absence and presence reached 76.10 % when\nevaluated on unseen data. The model had accuracies of 70%, 88%,\nand 86% in predicting murmur presence in infants, children, and\nadolescents, respectively. The interpretation of the model revealed\nproper discrimination between the learned attributes, and AV chan-\nnel was found important (score > 0.75) for the murmur absence\npredictions while MV and TV were more important for murmur\npresence predictions. The findings potentiate deep learning as a\npowerful front-line tool for inferring CHD status in PCG recordings\nleveraging early detection of heart anomalies in young people. It\nis suggested as a tool that can be used independently from high-\ncost machinery or expert assessment. With additional validation\non external datasets, more insights on the generalizability of deep\nlearning tools could be obtained before being implemented in real-\nworld clinical settings.\nIndex Terms— Deep learning, attention transformer, con-\ngenital heart disease, heart murmur, phonocardiography\nI. I NTRODUCTION\nBirth heart defects, better known as congenital heart disease\n(CHD), are nearly affecting 1.2% of newborns worldwide with one in\nevery four suffering from severe CHD [1]. It causes at least 220,000\ndeaths every year; majority during the first year after birth [2]. CHD\naffects blood flowing inside coronary arteries or outside the heart to\nthe rest of the body. In severe cases, CHD can affect the function\nof formed parts in the heart such as valves, which require surgery in\nthe baby’s first year of life [3]. Although CHD mortality rates have\nThis work was funded by a grant (award number: 8474000132)\nfrom the Healthcare Engineering Innovation Center (HEIC) at Khalifa\nUniversity, Abu Dhabi, UAE.\nMohanad Alkhodari, Leontios J. Hadjileontiadis, and Ahsan H. Khan-\ndoker are with the Healthcare Engineering Innovation Center (HEIC),\nDepartment of Biomedical Engineering, Khalifa University 127788, Abu\nDhabi, UAE. (e-mails: {mohanad.alkhodari, leontios.hadjileontiadis, ah-\nsan.khandoker}@ku.ac.ae).\nMohanad Alkhodari is also with the Cardiovascular Clinical Research\nFacility, Radcliffe Department of Medicine, University of Oxford OX12JD,\nOxford, UK. (e-mail: mohanad.alkhodari@rdm.ox.ac.uk).\nLeontios J. Hadjileontiadis is also with the Department of Electrical\nand Computer, Engineering, Aristotle University of Thessaloniki 54124,\nThessaloniki, Greece. (e-mail: leontios@auth.gr).\nheavily decreased in high-income countries owing to the advances\nin healthcare services, it is still a long-term increasing death factor\nin low- and middle-income countries [4]. These counties often suffer\nfrom the lack of capacity and advanced healthcare services to perform\ncongenital heart surgeries; which results in a higher amount of CHD-\nrelated costs and loss of lives [5]. Therefore, timely assessment at\nan early stage of life with developing diagnostic techniques could\npotentially decrease mortality rates by almost 58 % in these countries\n[6], which eventually can prevent many disabilities or death cases.\nCurrently, CHD is widely diagnosed using echocardiography,\nwhich is the current gold standard for comprehensive pre- and post-\nnatal screening [7]. Although reliable, echo still poses difficulties\nin low- and middle-income countries because it requires expensive\nequipment. In addition, it is highly dependent on trained and ex-\nperienced clinician for proper interpretation [8], which delays the\nidentification of heart defects at an early stage especially since its\ninterpretation still markedly vary between clinicians [9]. In many\nsituations. patients still have to travel long distances to reach medical\ncenters where equipment and expertise are available [10].\nA potential alternative could be the use of phonocardiography\n(PCG) which is a non-invasive cardiac auscultation device that carries\ninformation on the mechanical function of the heart [11], [12]. A\nPCG signal, recorded through an electronic stethoscope, describes\nthe function of heart valves during blood flow inside and outside the\nheart including the mitral, tricuspid, aortic, and pulmonic valves [13].\nThe flow of blood through valves orifices produces four important\nheart sounds; namely S1, S2, S3, and S4. In systole, an S1 heart\nsound is produced while in diastole the remaining heart sounds\nappear [14]. CHD can cause heart murmurs which are abnormal\nheart sounds during systole or diastole. A systolic heart murmur\nappears in the interval between S1 and S2, while a diastolic heart\nmurmur occurs after S2 and before S1 [15]. Although PCG provides\nreliable information on the mechanical malfunction of the heart\ndue to CHD, it is not commonly used by cardiologists who often\nprefer echocardiography. However, with the current advances of\ncomputerised algorithms, i.e., artificial intelligence, and with the\nincreasing demand for timely, continuous, and personalised healthcare\nservices, PCG could play a pivotal role in facilitating early diagnosis\nprotocols to reduce and prevent heart diseases [16].\nThe use of AI algorithms in healthcare has been rising in different\naspects [17]–[19], and several studies have investigated its efficiency\nin murmur detection using PCG recordings. Machine learning (ML)\nalgorithms such as support vector machine (SVM) and feature ex-\ntraction techniques were used to identify murmurs after signal trans-\nformation using Mel-frequency cepstral coefficient (MFCC) [20].\nMoreover, systole and diastole sound segmentation was utilised in\nseveral studies as input to feature extraction algorithms such as hidden\nMarkov model (HMM) and Gammatone frequency cepstral coeffi-\ncient (GFCC) [14], [21]. Most recently, deep learning algorithms\nincluding convolutional neural networks (CNN) and recurrent neural\nnetworks (RNN) were more frequently used in multiple studies [11],\n[16], [22] owing to their ability to extract features without any feature\nengineering techniques. However, there is still a research gap when\nit comes to the overall performance in murmur identification. The\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 IEEE JOURNAL OF BIOMEDICAL HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2023\nTABLE I\nTHE BASELINE CHARACTERISTICS OF PATIENTS INCLUDED IN THE STUDY WITH GROUPING BASED ON VALVULAR MURMURS CONDITION\nCategory Overall\nn = 942\nGroups p-value\nAbsent\nn = 695\n(73.78 %)\nPresent\nn = 179\n(19.00 %)\nUnknown\nn = 68\n(7.22 %)\nAge\nNeonate: 6 (0.65 %)\nInfant: 126 (13.38 %)\nChild: 664 (70.49 %)\nAdolescent: 72 (7.64 %)\nUnlabeled: 74 (7.86 %)\nNeonate: 4 (0.58 %)\nInfant: 76 (10.94 %)\nChild: 495 (71.22 %)\nAdolescent: 53 (7.63 %)\nUnlabeled: 67 (9.64 %)\nNeonate: 1 (0.56 %)\nInfant: 25 (13.97 %)\nChild: 132 (73.74 %)\nAdolescent: 16 (8.94 %)\nUnlabeled: 5 (2.79 %)\nNeonate: 1 (1.47 %)\nInfant: 25 (36.76 %)\nChild: 37 (54.41 %)\nAdolescent: 3 (4.41 %)\nUnlabeled: 2 (2.94 %)\n<0.001 •+\nSex Female: 486\n(51.59 %)\nFemale: 355\n(51.08 %)\nFemale: 92\n(51.40 %)\nFemale: 39\n(57.35 %)\n0.614\nHeight 108\n(74-130)\n117\n(93-134)\n111\n(87-130)\n82\n(66-123)\n<0.001 •+\nWeight 18.55\n(9.59-28.80)\n21.70\n(13.60-32.50)\n18.55\n(11.10-28.40)\n12.15\n(7.70-25.60)\n0.002 •\nBMI 16.32\n(14.32-18.45)\n16.81\n(15.20-19.10)\n16.10\n(14.47-17.81)\n17.87\n(15.88-20.00)\n0.009 +\nPregnant 70\n(7.43 %)\n65\n(9.35 %)\n3\n(1.68 %)\n2\n(2.94 %)\n<0.001 *•\nAll values are represented as median (inter-quartile range) or n ( %). Bold p-values show statistically significant differences (p <\n0.050) amongst the three groups using the one-way analysis of variance (ANOV A) test. * : Significant difference between absent and\npresent groups; • : Significant difference between absent and unknown groups; + : Significant difference between present and unknown\ngroups. BMI = Body mass index\nmajority of these studies still require pre-processing steps and human\ninterference such as filtration or heart sound segmentation; which\nin most cases are highly affected by the original quality of signals.\nIn addition, several studies utilized basic and curated datasets that\nmay not be representing the raw nature of PCG recordings that are\noften contaminated by noise sources. Moreover, deep learning models\nmay encounter uncertainty problems due to the lack of knowledge on\noptimal parameters, their randomness, and meaning [23]. Therefore,\nthere is still a need for more sophisticated approaches that could better\nhandle complicated recordings regardless of their original quality with\nless interference with the signals and with better interpretability.\nIn this study, we investigate the use of a deep learning approach\nbased on the latest self-attention transformer network to automatically\npredict the occurrence of congenital murmurs in PCG recordings.\nThe proposed approach does not require heavy memory demands\nfor computations due to the use of simple network architecture and\na reduced dimensionality on input signals through wavelet feature\ntransformation. Each PCG recording was transformed to wavelet-\nbased features sequence of a short length and thus, reducing the\noverall complexity and dimension of the input data. Moreover, the\nutilisation of deep learning for the prediction of the absence or\npresence of heart murmurs allows for interpreting the decisions\nthroughout network layers and thus, obtaining more insights into the\nimpact of auscultation channels on predictions. This study has several\nadvantages. First, our approach does not require any segmentation\nfor heart sounds or prior interference with the input signals, and\nthus, raw PCG signals are enough which reduces the need for\nexpert annotations or additional processing algorithms. Second, the\nuse of the attention mechanism allows the network to focus on\nrelevant parts within the inputs during training while ignoring the\nless relevant ones; which ensures enhanced learning especially for\nproblems such as the one we are trying to solve where the presence\nof murmurs occurs at different time points within the signals and\nnot continuously. Third, since we are including raw PCG signals, it\nis essential to use wavelet decomposition to capture fine details in\nthe signals and provide simultaneous time and frequency localization.\nMoreover, wavelet transform allows for better analyzing the dynamic\nnature of the signal’s frequency spectrum and thus, detecting with\nhigher resolutions the small changes happening due to heart murmurs.\nLast, a two-step approach using wavelet transform and attention\ntransformers, which was previously investigated in the 2D image\nand computer vision applications [24], leverages the basic design\nof attention networks by adding more focus on multi-scale feature\nmaps identified through the transformation; which eventually would\nresult in enhanced performance in solving complex problems.\nII. M ATERIALS AND METHODS\nA. Dataset and patient enrollment\nThe dataset used in this study was obtained from the new George\nB. Moody PhysioNet 2022 Challenge [25], the CirCor DigiScope\ndataset [26], which included pediatric patients enrolled in Northeast\nBrazil during two mass screening campaigns conducted in July-\nAugust 2014 and June-July 2015; the Caravana do Corac ¸˜ao (Caravan\nof the Heart). The data collection protocol was approved by the 5192-\nComplexo Hospitalar HUOC/PROCAPE institutional review board,\nunder the request of the Real Hospital Portugues de Beneficencia\nempernambuco. All young patients (aged 21 years or younger)\nprovided a signed consent form or a legal guardian consent form\nif they were below 18 years old.\nB. Data preparation and labelling\nThe study included a total of 1568 patients (only 942 have publicly\navailable data – nearly 60 %) who sequentially recorded one or more\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nALKHODARI et al.: IDENTIFICATION OF CONGENITAL VALVULAR MURMURS IN YOUNG PATIENTS USING DEEP LEARNING-BASED ATTENTION TRANSFORM-\nERS AND PHONOCARDIOGRAMS 3\nFig. 1. Transformation of the phonocardiogram (PCG) recording (showing mitral valve (MV) channel) into stacked power features using wavelet\ndecomposition. a, An example of a healthy patient PCG recording (showing only 10 seconds). The signal gets decomposed sequentially for every\nwindow (a total of 5000 windows of 32 samples each) using Symlets 8 wavelet, and the corresponding decomposition coefficients are generated.\nThe power of each coefficient segment is used to extract additional features. The final feature transformation (showing wavelet energy) is stacked\nfor every window segment to form a new wavelet feature-based signal, which then gets stacked on a channel-by-channel basis. b, An example of a\npatient with heart murmur.\nPCG signals from multiple auscultation locations, namely aortic valve\n(A V), mitral valve (MV), pulmonary valve (PV), and tricuspid valve\n(TV), with a sampling frequency of 4,000. Out of the 942 available\ndata, a total of 695 had no heart murmurs, 179 had heart murmurs,\nand 68 were labeled as unknown. An expert annotator labeled all\nsignals based on the presence or absence of murmurs. In addition, if\nthe annotator was not certain about the condition of the patient, a label\nof ”unknown” was chosen. All patients recorded at least 5 seconds\non each channel and at most 65 seconds. We designed our approach\nfirst by identifying missing and very short recordings. In the case of a\nmissing recording from a specific channel, the previous channel was\nduplicated to fill the gap. Moreover, we ensured that all signals are\n40 seconds in length to be enough to capture the majority of sound\ninformation about heart function in each recording. Furthermore,\nshort signals were padded, and longer signals were truncated. Signals\nwere z-score normalized before any further analysis.\nC. patients information and statistical analysis\nWe performed statistical analysis using one-way student t-test\nanalysis of variance (ANOV A) on all available patient information\n(Table I). The provided information included age (neonate [birth to\n27 days old], infant [28 days old to 1-year-old], child [1 to 11 years\nold], adolescent [12 to 18 years old], and unlabeled), sex (male or\nfemale), height (in cm), weight (in kg), body mass index (BMI, in\nkg/m2), and pregnancy (yes or no). A statistical significance was\nnoted if the p-value was below or equal to 0.05.\nD. Transformation to wavelet features\nTo capture internal heart function characteristics contaminated\nwithin signals, a wavelet decomposition approach (Figure 1) was\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 IEEE JOURNAL OF BIOMEDICAL HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2023\nfollowed based on the original discrete wavelet transform (DWT)\nalgorithm [27], [28]. Each PCG channel signal (40 seconds – 160,000\nsamples) was segmented into 32-sample windows that shift with a\nstep of 32 samples to reduce overlapping (total of 5,000 windows\nper channel). The selection of window length, step, and wavelet type\nwas based on an ad-hoc manner to capture enough signal information\nwithout over- or under-estimations. Symlets 8 (Sym8) wavelet family\nwas used for decomposition into 5 main levels ( J) determined based\non the sampling frequency ( f) as follows,\nJ = log(f)\n2 × log(2) (1)\nThis decomposition level allows for capturing time and frequency\ncharacteristics of the input signals. Then, the transformed signals\nwere converted into 105 concatenated approximation coefficient vec-\ntors. Each vector was then squared to generate the corresponding\npower (P) vector, and the corresponding wavelet features were then\nextracted from every coefficient including the energy (E), variance\n(σ2), standard deviation ( σ), waveform length (L), and Shannon\nentropy (SE). The energy, which reflects the time-scale density in\nthe signal, was calculated as,\nE =\nkX\ni=1\nPti (2)\nwhere ti corresponds to the selected coefficient and k represents\nthe total of 15 coefficients.\nThe variance and standard deviation, revealing the variability in\nthe transformed power signal, were calculated as follows,\nσ2 =\nPk\ni=1(xti − µk)2\nk (3)\nσ =\nsPk\ni=1(xti − µk)2\nk (4)\nwhere x corresponds to the selected value.\nMoreover, the waveform length and Shannon entropy, which re-\nflects the power signal complexity, were calculated as,\nL =\nkX\ni=1\n|(xti − xt+1)| (5)\nSE = −\nkX\ni=1\nP − ln(P) (6)\nwhere P is the probability mass function given as,\nP = Ptk\nE (7)\nA total of 30 features were obtained for every 32-sample segment.\nThen, all features extracted per channel were concatenated to form\na wavelet power transformation of the raw PCG channel. The\nalignment of all four channels results in having a transformed signal\nof 20,000 samples (4 channels x 5000 32-sample segments) that will\nbe used for further analysis. Upon this transformation, a stronger\ntransformation of the original PCG signals was achieved represented\nby the contaminated wavelet power features.\nE. Development of the self-attention transformer network\nTransformers have been commonly used as an effective deep\nlearning network in natural language processing (NLP) applications\n[29], [30]. A transformer network is more advanced than the con-\nventional CNN and RNN networks in its ability to capture global\ndependencies in the input through self-attention mechanisms [31],\nwhich forces the network to learn from regions of most significant\nimpact on the class prediction. Most recently, transformers were\nused in medical applications [31]–[33] and provided an enhanced\nperformance relative to conventional deep learning approaches. A\ntransformer network (Figure 2) consists of four major components;\nnamely feature encoder, positional encoder, transformer unit, and\ndecoder, which can be designed according to the problem in hand\nand input type.\n1) Feature encoder: A feature encoder, which is often called\nthe CNN backbone, is the deep feature transformation block that\ntransforms the input into an easily identifiable format. In this block,\nconsecutive convolution operations are applied to the input followed\nby a series of max-pooling layers to reduce the dimensionality of the\ninput to lower resolution, i.e., reducing the length on input signals,\nA convolutional operation is defined as follows,\nClj\ni = h(bj +\nMX\nm=1\nwj\nmxj\ni+m−1) (8)\nwhere xi = [x1, x2, ..., xn] is the input, n is the total number of\npoints, l is the layer index, h is the activation function, b is the bias\nof the jth feature map, M is the kernel size, wj\nm is the weight of\nthe jth feature map and mth filter index.\nIn this work, and since the input (30x20,000) is already a wavelet-\nbased transformation of the raw heart sound signal, we designed\nthe feature encoder to include only two CNN blocks. The first\nconvolution is built with a kernel of 64 points and 32 filters, whereas\nthe second has a kernel of 32 points and 64 filters. After each\nconvolutional step, batch normalisation and Gaussian error linear unit\n(GeLU) was applied. To reduce the dimensionality, i.e., make the\ninput shorter, a max-pooling layer followed each convolutional step\nwith a kernel size of 3 and stride of 2. Hence, the output from the\nfeature encoder is a set of DL feature vectors of 64x1250 size that\nrepresents the most essential features within the inputs.\n2) Positional encoder: Positional encoding is an essential step\nin time-sequence analysis especially before self-attention transformer\nlayers as it describes the location of the feature vectors in a sequence\n(Figure 2b) such that it is assigned with a unique representation1.\nIn this work, the unique representation was chosen to be a set of\nsinusoidal signals within the positional encoder block (PE) as follows,\nPE (i, 2j) =sin( i\n10, 0002j/d ) (9)\nPE (i, 2j + 1) =cos( i\n10, 0002j/d ) (10)\nwhere i and j define the location in the feature and time-sequence\ndimensions, respectively, and d is the model dimension, i.e., number\nof features. In this layer, the encoding value, ω, was selected to be\nas low as 10,000 to ensure having a unique encoding at different\npositions.\nThe sine and cosine functions used here allows for representing\ncomplex periodic information in the input at different phases. The\n10,000 allows for distinguishing elements based on their relative\npositions in the sequence at a wider range of positional information.\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nALKHODARI et al.: IDENTIFICATION OF CONGENITAL VALVULAR MURMURS IN YOUNG PATIENTS USING DEEP LEARNING-BASED ATTENTION TRANSFORM-\nERS AND PHONOCARDIOGRAMS 5\nFig. 2. Modelling of the self-attention transformer network with details on the structure and design. a, The complete framework of the proposed\nmodel includes the four main components, namely the feature encoder, positional encoder, transformer unit, and decoder. The feature encoder\nstarts by reducing the dimensions of the input from [30x20,000] to [64x1,250], focusing on the most important features in the input. b, The positional\nencoder with a zoomed-in view of the sinusoidal signals guides the positioning of input vectors in proper time sequences. c, The main block of\nthe transformer includes the calculations of the scaled dot-product attention and the feed-forward network. d, By using multiple heads within each\ntransformer unit, the joint attention gets captured by concatenating information from multiple subspaces. e, The last block in the network, the\ndecoder, was designed to be simple full-connected layers. To handle data imbalance, the safe-level synthetic minority over-sampling technique\n(SMOTE) is applied to the attention extracted from the last layer of the network.\n3) Transformer unit: A transformer unit consists of the major\ncomponents for the attention mechanism proposed in this study;\nnamely the scaled dot-production attention followed by the multi-\nhead attention.\nThe scaled dot-product attention (Figure 2c) is calculated as the\ndot product between three inputs which are the queries (Q), keys (K),\nand values (V) of the input with features dimension d. First, Q’s and\nK’s are multiplied and divided by a scaling factor equal to the square\nroot of the dimension as follows,\nQKT = Q × Kp\nd/N\n(11)\nwhere N is the number of heads used in the model, i.e., 2 heads.\nThen, a softmax function is applied to normalise the values before\nthe second multiplication step with the V’s to calculate the attention\n(A) as follows,\nA(Q, K, V) =softmax(QKT ) × V (12)\nThe resulting attention from this step goes as input to the multi-\nhead attention mechanism (Figure 2d) which applies a series of\nlinear projections to the input Q’s, K’s, and V’s. Therefore, instead\nof having a single attention mechanism, calculating multi-attention\nM(Q, K, V) in parallel across queries, keys, and values results in\nmore jointly focused information from multiple subspaces with dv\ndimension across the three inputs as follows,\nM(Q, K, V) =concat(h1, h2, ..., hN ) × W0 (13)\nwhere hi can be calculated as,\nhi = A(QWQ\ni , KWK\ni , V WV\ni ) (14)\nwhere Wi is the linear projection of each input for the selected\nhead i.\n4) Feature decoder:The decoder used in this study was simpler\nthan the usual implementation of decoders by only utilising linear\nprojections in fully connected layers (Figure 2e). This is to ensure\nthat the input attention is properly decoded by obtaining the features\nwithin the subspaces formed within the self-attention mechanism\nexplained earlier.\n5) Network enhancements end experimental design: The\ntransformer network was further enhanced by handling any data\nimbalance in the provided data during training. Thus, the learning\ncapabilities of the network during training get improved by imple-\nmenting the synthetic minority over-sampling technique (SMOTE)\n[34] technique. By applying the Safe-level SMOTE mechanism, the\nminority class gets over-sampled at the attention stage from activa-\ntions extracted from the global average pooling layer by generating\nsynthetic samples from the last layer of the network.\nThe training of the model was based on the adaptive moment\nestimation (ADAM) optimiser with an initial learning rate of 0.001\nthat drops by 10 % at the 40th epoch (total of 60 epochs). We\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 IEEE JOURNAL OF BIOMEDICAL HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2023\nevaluated the trained model following a 10-fold cross-validation\nscheme over the whole dataset. In addition, we applied two testing\nscenarios; one with three classes, i.e., absent, present, and unknown\nmurmurs, and the other with two classes, i.e., absent and present\nmurmurs. In both scenarios, the validation was patient-independent.\nAt each fold, 10 % of the data (94 patients) were hidden for testing.\nIII. R ESULTS\nA. Baseline characteristics of patients\nThe participating patients (Table 1) were mostly from the child\ncategory (n = 664) followed by infants (n = 126) out of the whole\ndataset (n = 942). The gender of the patients was almost balanced\nwith 51.59 % for the female category (n = 486). BMI inter-quartile\nrange was 14.32— 18.45 for the BMI with a median of 16.32, which\ncan be considered slightly underweight. Amongst the 942 patients,\n70 were pregnant (7.43 %).\nThe statistical significance between the three groups, i.e., absent,\npresent, and unknown, was below 0.05 in age, height, weight, BMI,\nand pregnancy, which demonstrates the whole characteristics except\nfor sex (p-value: 0.614). The age group was highly significant (p-\nvalue: <0.001) between each group versus the unknown group.\nMoreover, BMI was significant (p-value: 0.009) between the present\nand unknown groups where each had a median and inter-quartile\nrange of 16.10 (14.47–17.81) and 17.87 (15.88–20.00), respectively.\nConcerning pregnancy status, the majority of patients who were preg-\nnant had no murmurs, which demonstrated a statistical significance\nbetween the absence and the other two groups (p-value: <0.001). It\nis worth noting that both groups were significantly different in most\ncharacteristics versus the unknown group.\nB. Computation complexity\nThe complexity of the computational processes in the proposed\napproach, i.e., multiplications or additions, includes two main parts;\nnamely the wavelet decomposition and transformer unit. The wavelet\nstep had a complexity of O(Nw); which is dependent on the length\nof input (Nw). Moreover, the transformer unit had a O(N2\nt ×d) with\n64 input features dimension (d). However, the input length ( Nt) of\nthe transformer gets heavily reduced from 20,000 to 1,250 because\nof the input decoder; thus reducing the overall complexity of the\nmulti-head attention.\nThe training of the transformer model required only 5 minutes\nfor a complete training over 60 epochs. The model was trained\non an NVIDIA RTX3080 GPU with 4GB of RAM. The training\ntime is expected to increase slightly if simpler GPUs are used.\nThe complexity of the approach was not a big concern due to the\ndimensionality reduction using wavelet features transformation which\nreduced the length of each signal from 160,000 to 20,000. Further,\nthe input length gets reduced in the feature encoder stage before\nprocessing with the transformer.\nC. Detection of three-class heart murmurs\nThe confusion matrix of the accumulated predictions after the\ncross-validation scheme with three classes is shown in Figure 3a\nwhere the model had an average accuracy of 90.23 %, an average\nsensitivity of 72.41 %, and an average precision of 70.22 %. Exclud-\ning the unknown group, the values become 89.49 %, 87.18 %, and\n83.27% for the accuracy, sensitivity, and precision, respectively. The\noverall performance metrics breakdown is provided in Figure 3b;\nhighlighting that the model achieved normalized Matthews correla-\ntion coefficient (NMCC) of 82.02 %, 87.45 %, and 69.51 % for the\nabsent, present, and unknown groups, respectively. By excluding the\nunknown group, the average NMCC becomes 84.74 %.\nWe further analyzed the performance of the model through the\nreceiver operating characteristics (ROC) and precision-recall (PR)\ncurves (Figure 3c). The highest area under the ROC curve (AUROC)\nand the PR curve (AUPR) was achieved in predicting the absence of\nmurmurs (AUROC: 0.915, 95% Confidence Interval: 0.828-1.000). In\npredicting the presence of murmurs, the model reached an AUROC\nof 0.932±0.048 and AUPR of 0.561±0.075. Moreover, the unknown\nclass had the lowest values amongst the three groups with an overall\nAUROC of 0.899 (95 % Confidence Interval (CI): 0.835-0.963) and\nAUPR of 0.418 (95 % CI: 0.370-0.466).\nD. Analysis of predictions based on age group\nWe analysed the predictions made by the developed model after the\ncross-validation scheme based on age groups (Figure 7) provided in\nTable I. We evaluated the model for the overall dataset and when sep-\narating patients according to the murmur group, i.e., absent, present,\nand unknown murmurs, for five age groups including neonate, infant,\nchild, adolescent, and unlabeled. Overall, the model had accuracies\nof 50%, 70%, 88%, 86%, and 95% in predictions at each age group,\nrespectively. The absent group had roughly more than 75 % correct\npredictions in each age group. On the other hand, the present group\nhad a close pattern but with no correct neonate predictions (1 out of\n1) and with a slight decrease in the correct child group predictions\n(77%). The adolescent group predictions were almost divided equally\n(56%) between correct and wrong predictions which were close to the\nunlabeled percentage in terms of miss-classification accuracy ( 40 %).\nThe unknown group was similar to the present group in the neonate\ncategory with no correct predictions (1 out of 1). However, the wrong\npredictions in the infant and child groups were higher than correct\npredictions by 5 % and 50 %, respectively. Most interestingly, the\nadolescent and unlabeled groups had no wrong predictions.\nE. Interpretation of model decisions\nTo interpret the model’s ability to discriminate between the three\ngroups, i.e., absent, present, and unknown murmurs, we visualise the\nactivations at each layer of the network (Figure 5 – top row) while\nincluding the 95 % confidence interval across all patients in each\ngroup. Through visual inspection, the model discrimination ability\nwas enhanced by going deeper with the network. In both the training\nand testing sets (Figure 5a,b), the model’s best separation was in\nthe last layers, i.e., decoder fully connected layers, and final global\naverage pooling. Moreover, by using the t-distributed stochastic\nneighbor embedding (t-SNE) analysis (Figure 5 – middle row), the\nclustering of the averaged activations becomes more efficient starting\nfrom the transformer layer in both sets. The analysis of each layer\nusing the gradient-weighted class activation mapping (GradCAM)\nrevealed that wavelet features extracted from the A V channel were\nhighly important (score > 0.75) for the decisions of the absence of\nmurmurs (Figure 5 – bottom row). In addition, channels such as MV\nand TV had the highest importance for the prediction of the presence\nof murmurs. Overall, the PV and TV channels were important for the\ndecisions of all three groups.\nF . Performance with two classes\nWe tested the transformer network by training it on predicting two\nclasses only, i.e., absent and present (Figure 6) without the inclusion\nof the unknown group to further investigate the discrimination ability\nof the model. The accuracy has reached 91.76 % with sensitivity\nand NMCC values of 93.81 % and 86.98%, respectively. In addition,\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nALKHODARI et al.: IDENTIFICATION OF CONGENITAL VALVULAR MURMURS IN YOUNG PATIENTS USING DEEP LEARNING-BASED ATTENTION TRANSFORM-\nERS AND PHONOCARDIOGRAMS 7\nFig. 3. Performance of the trained self-attention transformer model validated using a 10-fold cross-validation scheme with three classes. a,\nConfusion matrix for the predictions of absent (A), present (P), and unknown (U) murmur classes with the corresponding one-versus-all per-class\nsensitivity and specificity. b, Performance metrics calculated from the confusion matrix for the accuracy, sensitivity, specificity, precision, F1-score,\nand normalized Matthews correlation coefficient (NMCC). c, Analysis of the receiver operating characteristic (ROC) and precision-recall (PR) curves\nwith the 95% confidence interval (CI) calculations for each class.\nthe AUROC between the two classes has reached 0.93 whereas the\nAUPR values was 0.97 for the absent class and 0.57 for the present\nclass. When interpreting the model decision, similar discrimination\nability was observed (Figure 6c) between the two groups with proper\nattention through GradCam analysis to the A V channel for the absence\ngroup and to the TV channel for the presence group.\nG. Additional experiments\nWe have performed an additional comparison between the proposed\ntransformer model with another two conventional models; namely\nthe CNN and Bi-directional long short-term memory (Bi-LSTM).\nThe CNN had a simple structure of three one-dimensional (1D)\nconvolutional blocks, each followed by batch normalization and a\nrectified linear unit (ReLU). At every convolutional layer, the kernel\nsizes were 64, 32, and 16, with 32 filters each. On the other hand, the\nBi-LSTM had a total of 50 hidden units operating in both directions\nof the input signals. The analysis of accuracy and AUC (Table II)\nrevealed that the transformer outperformed conventional models by\nnearly 10-20%.\nTo elaborate on the importance of PCG channels, we trained the\nmodel on each channel separately and observed the performance\nrelative to the original combined PCG-based model (Figure ??).\nInterestingly, we have found that the highest accuracy of 85.68 %\nwas observed for the absent group using the A V channel; which\nwas supported by a higher AUROC of 0.876 compared to other\nchannels. Moreover, the present group showed a variable performance\nacross channels, but most importantly, the MV , PV , and TV channels\nhad the highest performance. The highest accuracy of 89.85 % was\naccounted for the TV channel with an AUROC of 0.895. Lastly,\nthe unknown group had a balanced performance across all channels\nwith no differences in the metrics. Overall, the performance was\nmaximized when combining all channels which ensures enhanced\nlearning capabilities by the model.\nMoreover, we have provided a summary of the proposed approach\nrelative to other studies in the detection of heart murmurs using the\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 IEEE JOURNAL OF BIOMEDICAL HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2023\nFig. 4. Analysis based on age groups for the developed model. Showing the true and wrong (light gray) predictions for the overall dataset (green),\nabsent (blue), present (red), and unknown (black) groups. The age groups included are neonate, infant, child, adolescent, and unlabeled. More\ndetails on the number of patients in each category is provided in Table I.\nTABLE II\nTHREE -CLASS PERFORMANCE OF THE PROPOSED TRANSFORMER MODEL COMPARED WITH A BASIC CONVOLUTIONAL NEURAL NETWORK (CNN)\nAND BI-DIRECTIONAL LONG SHORT -TERM MEMORY (BI-LSTM) MODELS\nGroups Accuracy Sensitivity Specificity AUC\nTransformer CNN Bi-LSTM Transformer CNN Bi-LSTM Transformer CNN Bi-LSTM Transformer CNN Bi-LSTM\nAbsent 86.41% 80.74% 61.89% 89.65% 85.29% 75.53% 76.21% 62.80% 30.28% 0.92 0.83 0.57\nPresent 92.57% 85.34% 67.62% 84.71% 79.73% 45.39% 94.14% 88.32% 63.38% 0.93 0.86 0.62\nUnknown 91.72% 83.88% 90.66% 42.86% 36.59% 34.29% 95.64% 89.49% 73.01% 0.90 0.82 0.67\nAverage 89.57% 83.32% 73.39% 72.41% 67.20% 51.74% 88.66% 80.20% 55.56% 0.92 0.83 0.62\nsame dataset (Table III). Several studies have utilized different pre-\nprocessing steps including de-noising and heart sound segmentation.\nIn addition, studies used feature transformation using methods such\nas Mel-spectrogram, Mel-frequency cepstral coefficients (MFCC),\nHilbert transform, and power spectral density. However, the per-\nformance was less than our proposed approach in accuracy and\nAUC. Furthermore, compared with the majority of these studies, our\napproach did not require any pre-processing steps.\nH. Validation on unseen data\nThe proposed approach was validated on the PhysioNet 2022\nhidden validation and testing sets that were unseen during training\nthe model [35]. The model had an accuracy level of 76.10 % on the\nvalidation set that included 10% of the original dataset (156 patients).\nMoreover, the accuracy reached up to 75.70 % when evaluated on the\ntesting set that included 30 % patients of the original dataset (470\npatients). It is worth noting that the validation and testing sets were\ninaccessible and thus, no further analysis could be performed [25].\nIV. D ISCUSSION\nOur study suggests that deep learning with its latest transformer\nnetwork would be a powerful decision support tool to clinicians\nto facilitate early, timely, and continuous assessment of congenital\ndiseases affecting the heart, i.e., valvular anomalies. To put this in\nperspective, mortality rates can be decreased by more than 50 %\nin low- and middle-income countries if cost-effective and accurate\ntechnological tools have been utilised for diagnosis [6]. Because we\ninclude patients who are from a young cohort, a murmur prediction\naccuracy of 90 % at an early stage of life and by an automated\napproach could leave its impact on guiding medication, treatment,\nand intervention plans with less dependency on experts who have\nvariable interpretation for murmurs [9]; all of which would improve\nthe quality of life to these patients and reduce missed diagnosis.\nThe findings of this study have several implications. First, the\naccurate discrimination between the absence or presence of con-\ngenital murmurs in PCG recordings is of significance in promoting\nautomated cardiac auscultation through deep learning as a tool to\ncomplement echocardiography that requires expert interpretation and\nexpensive machinery. The use of physiological signals to assess\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nALKHODARI et al.: IDENTIFICATION OF CONGENITAL VALVULAR MURMURS IN YOUNG PATIENTS USING DEEP LEARNING-BASED ATTENTION TRANSFORM-\nERS AND PHONOCARDIOGRAMS 9\nFig. 5. Interpretation of the model’s discrimination ability between the absent, present, and unknown murmurs groups. a, Visualisation of the\nwhole training set showing the activations of each layer in the network (top row) with the 95% confidence interval across all patients in each group.\nIn addition, the t-distributed stochastic neighbor embedding (t-SNE) analysis (middle-row) was used to cluster the averaged activations of each\nlayer. Lastly, the gradient-weighted class activation mapping (GradCAM) was used to visualise each auscultation channel’s impact on the network’s\ndecisions at each layer. b, Visualisation of a single fold from the testing set.\ncardiovascular diseases has been widely used in literature [42],\nhowever, due to its lengthy nature, it could be hard to interpret\nvisually by clinicians. Therefore, signals and their extracted features\nserve as a solid material to build deep learning tools that are capable\nof handling them, learning from thousands of patient clinical details\nand hidden characteristics, and providing automated predictions of\nmedical conditions. We keep in mind that there is a necessity to\ndevelop such tools to serve as a first-stage screening before requesting\nadditional tests that are more expensive and not universally available.\nMoreover, we tried to utilize PCG signals while being as raw as\npossible without any human interference or prior sound segmentation.\nTherefore, the proposed approach ensures more robustness in clinical\napplication; reducing the need for expert annotations or automated\ntechniques that may lack the required accuracy.\nSecondly, although the expert annotator was uncertain of the\nmedical condition in some PCG recordings, i.e., labeled as un-\nknown, training a model to infer the condition after learning deep\ncharacteristics from big multi-class patient data is advantageous as\nit gives more insights on this ambiguous group. For example, out\nof the 68 unknown cases, only 30 were found to have common\npatterns that isolate them from the other two groups, i.e., absence\nand presence of murmurs, whereas 33 were found to carry patterns\nthat characterise them similarly to the absence of murmurs signals.\nThis could raise a question as to whether to consider the unknown\ngroup as a separate entity on its own; more specifically as a transition\nfrom the absence to the presence of murmurs, or just an uncertain\ndiagnosis. Several studies have shown that there could be some\ninnocent murmur types that are still ambiguous to experts [43],\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 IEEE JOURNAL OF BIOMEDICAL HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2023\nFig. 6. Interpretation of the model discrimination ability between the absent, present, and unknown murmurs groups. a, Visualisation of the whole\ntraining set showing the activations of each layer in the network (top row) with the 95 % confidence interval across all patients in each group.\nIn addition, the t-distributed stochastic neighbor embedding (t-SNE) analysis (middle-row) was used to cluster the averaged activations of each\nlayer. Lastly, the gradient-weighted class activation mapping (GradCAM) was used to visualise each auscultation channel’s impact on the network’s\ndecisions at each layer. b, Visualisation of a single fold from the testing set.\nFig. 7. Evaluation of the model’s performance when trained on each channel separately relative to the original combined PCG-based model. PCG\nchannels are the aortic valve (AV), mitral valve (MV), pulmonary valve (PV), and tricuspid valve (TV). The combined model performance is the last\nbar on each plot.\nTABLE III\nCOMPARISON OF PERFORMANCE WITH OTHER STUDIES IN VALVULAR MURMURS DETECTION USING THE PHYSIO NET 2022 DATASET\nStudy Year Dataset Pre-processing Features Model Performance\nMonteiro et al. [36]\n2022\nCirCor DigiScope [26]\nSound segmentation\nBand-pass filtering\nHomomorphic\nHilbert transform\nPower spectral density\nWavelet envelopes\nBi-LSTM Accuracy: 75.70%\nPatwa et al. [37] Sound segmentation\nOut-of-bound de-noising\nWavelet scattering CNN Accuracy: 83.81%\nAUC: up to 0.91\nSummerton et al. [38] S1 and S2 sound\nsegmentation\nMFCC Gradient boosting\nEnsemble CNN\nAccuracy: 75.30%\nMcDonald et al. [39] S1 and S2 sound\nsegmentation\nLog-spectrogram transform RNN\nHidden semi-Markov model\nAccuracy: 81.70%\nParvaneh et al. [40] N/A - Raw signals Mel-spectrogram CNN (Y AMNet) Accuracy: 83.10%\nFuadah et al. [41] 2023 Sound segmentation\nBand-pass filtering\nMFCC k-NN Accuracy: 76.31%\nAUC: up to 0.76\nThis study 2023 N/A - Raw signals Wavelet features transformationAttention transformer Accuracy: 89.57%\nAUC: up to 0.92\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nALKHODARI et al.: IDENTIFICATION OF CONGENITAL VALVULAR MURMURS IN YOUNG PATIENTS USING DEEP LEARNING-BASED ATTENTION TRANSFORM-\nERS AND PHONOCARDIOGRAMS 11\nespecially when unusual changes were found in some cases of mitral\nvalve prolapse [44]. Figure 7 shows that some of the unknown\ncases were separated clearly from the other two groups through deep\nlearning, yet they carry both importance patterns of absent and present\nwhen evaluating the importance of auscultation channels. This was\nsupported by the observations when models were trained using each\nchannel separately; which revealed specific channels to have a higher\nimpact on detecting the presence of murmurs or being unknown.\nWe note that a 100 % correct prediction was made for the neonate,\nadolescent, and unlabelled age group; which could be either driven\nby the low number of subjects in each group (max of 3) or by a\nspecific pattern for these participants that deviates from the patterns\nof absence or presence of murmurs. To the best of our knowledge,\nour study is the first to touch on this point through PCG recordings\nand with discoveries of a trained deep learning tool.\nThirdly and most importantly, inferring the condition of CHD\naccording to heart murmurs is of high importance, especially at\na young stage in life, i.e., infant and child. This is supported by\nthe findings in this study (Figure 3) where the majority of correct\npredictions for the absence and presence of murmurs were obtained in\nthese two age groups. Several studies [45], [46] have suggested timely\nclinical assessment for CHD during the first few weeks of life, i.e.,\nneonate to infant, as they can heavily reduce morbidity and mortality\nrates in newborns. However, it could be a hard task for clinicians to\ndetect murmurs at an early stage [47] and reduce false positive rates\nin the assessment. Therefore, a trained tool based on deep learning\ncould aid in decision-making and provide some guidance to clinicians\nwhen evaluating murmurs in a timely fashion. Interestingly and as\nmentioned in the previous paragraph, the unknown group had more\nwrong classifications for the infant and child group which could be\nbecause it is not a separate entity on its own when compared to the\nabsence and presence of murmurs groups. However, it had a 100 %\ncorrect prediction rate in the adolescent group which could provide\ninsight on the nature of this group in older patients.\nFourthly, the performance and efficacy of the proposed tool could\npave the way for applying new techniques in clinical settings to\naid clinicians in decision-making. While the current gold-standard\ntechnique, echocardiography, is considered the first line of defence\nin evaluating heart murmurs, the proposed tool could assist in\nperforming cost-effective and continuous screening for patients with\nreliable results. Heart sounds in nature, being continuous acoustic\nwaves, could benefit from frequent monitoring applications with a\nsimple, yet effective, tool that is driven by AI technology; especially\nat an early stage or before advanced assessment. However, AI tools\nin general, especially those relying on deep learning, require wider\nexploration of their performance when the source of training data is\nvarying. The diversity in such tools and the ability to generalize over\na wider patient cohort should be carefully taken into consideration\nbefore implementing them in clinical settings.\nDespite the presented study’s potential, some limitations could be\nconsidered before applying our study in the clinic. First, we included\npatient data which was publicly available by the challenge to train\nand evaluate our model. We also evaluated the performance of unseen\ndata taken from the same database. However, it will be interesting\nto explore the reliability of the model with additional data and extra\nvalidation through external datasets. Moreover, a multi-center eval-\nuation across different hospitals with varying patient characteristics\nis needed. Second, we provided insights on the separate group, i.e.,\nthe unknown murmur, with a high accuracy level, therefore, more\nstudies, yet, are needed to verify this group clinically to whether\nconsider it as a special entity between absence and presence of\ncongenital murmurs, as it was originally annotated by experts in the\nused dataset, or just an uncertainty in diagnosis. This would add more\ninsights into the reliability of the proposed approach and AI tools\nin general in being considered as an assistant to doctors in clinical\ndecision-making. Third, our approach transforms raw PCG recordings\nto their corresponding wavelet-domain features which reduces the\nnoise impact on the signals. However, the model may also benefit\nfrom additional features describing the segments within the PCG\nrecordings, i.e., S1 and S2. Last, the proposed structure of the deep\nlearning model with attention mechanism was basic yet efficient,\nhowever, with the recent advancement of transformers, an enhanced\nperformance could be achieved when utilizing advanced structures;\ntaking into consideration the trade-off between performance and\nmodel complexity which was relatively low in this study.\nCONCLUSION\nOverall, we have developed a deep-learning tool to provide an\nautomated prediction for CHD-induced murmurs. The findings sug-\ngest deep learning as a powerful front-line tool that could potentially\nguide clinical decision-making for early detection of heart anomalies\nin young people. As a complementary tool to echocardiography, PCG\nand deep learning can leverage accurate discrimination between the\nabsence and presence of congenital murmurs with less dependency\non expert assessment and high-cost machinery, especially in low-\nand middle-income countries. Therefore, future research should be\ndirected toward evaluating the clinical cost-effectiveness of the ap-\nproach while maintaining high performance.\nCONFLICT OF INTEREST\nAll authors have reported that they have no relationships relevant\nto the contents of this paper to disclose.\nACKNOWLEDGEMENT\nThe authors would like to acknowledge Syafiq Azman from the\nAIQ, ADNOC H.Q., Abu Dhabi, United Arab Emirates, for his advice\nand review while developing the transformer network. All authors had\nfinal responsibility for the decision to submit for publication. Data\nused in this study were fully accessed by the corresponding authors.\nDATA AVAILABILITY\nThe dataset used in this study was part of the George\nB. Moody PhysioNet 2022 challenge which is publicly\navailable at: https://moody-challenge.physionet.org/2022/.\nWavelet decomposition method can be obtained from\nhere: https://github.com/RamiKhushaba/getmswtfeat. The\ntransformer network was developed in MATLAB\nfor the first time and can be obtained from here:\nhttps://github.com/malkhodari/Transformer MATLAB.git.\nREFERENCES\n[1] W. Wu, J. He, and X. Shao, “Incidence and mortality trend of congenital\nheart disease at the global, regional, and national level, 1990–2017,”\nMedicine, vol. 99, no. 23, 2020.\n[2] M. S. Zimmerman, A. G. C. Smith, C. A. Sable, M. M. Echko, L. B.\nWilner, H. E. Olsen, H. T. Atalay, A. Awasthi, Z. A. Bhutta, J. L.\nBoucher, et al. , “Global, regional, and national burden of congenital\nheart disease, 1990–2017: a systematic analysis for the global burden\nof disease study 2017,” The Lancet Child & Adolescent Health , vol. 4,\nno. 3, pp. 185–200, 2020.\n[3] M. E. Oster, K. A. Lee, M. A. Honein, T. Riehle-Colarusso, M. Shin,\nand A. Correa, “Temporal trends in survival among infants with critical\ncongenital heart defects,” Pediatrics, vol. 131, no. 5, pp. e1502–e1508,\n2013.\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12 IEEE JOURNAL OF BIOMEDICAL HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2023\n[4] M. Zimmerman and C. Sable, “Congenital heart disease in low-and-\nmiddle-income countries: focus on sub-saharan africa,” in American\nJournal of Medical Genetics Part C: Seminars in Medical Genetics ,\nvol. 184, pp. 36–46, Wiley Online Library, 2020.\n[5] S. Rahman, B. Zheleva, K. Cherian, J. T. Christenson, K. E. Doherty,\nD. De Ferranti, K. Gauvreau, P. A. Hickey, R. K. Kumar, J. K. Kupiec,\net al. , “Linking world bank development indicators and outcomes of\ncongenital heart surgery in low-income and middle-income countries:\nretrospective analysis of quality improvement data,” BMJ open , vol. 9,\nno. 6, p. e028307, 2019.\n[6] H. Higashi, J. J. Barendregt, N. J. Kassebaum, T. G. Weiser, S. W.\nBickler, and T. V os, “The burden of selected congenital anomalies\namenable to surgery in low and middle-income regions: cleft lip and\npalate, congenital heart anomalies and neural tube defects,” Archives of\ndisease in childhood , vol. 100, no. 3, pp. 233–238, 2015.\n[7] R. Lytzen, N. Vejlstrup, J. Bjerre, O. B. Petersen, S. Leenskjold, J. K.\nDodd, F. S. Jørgensen, and L. Søndergaard, “Live-born major congenital\nheart disease in denmark: incidence, detection rate, and termination of\npregnancy rate from 1996 to 2013,” JAMA cardiology, vol. 3, no. 9,\npp. 829–837, 2018.\n[8] G. Mcleod, K. Shum, T. Gupta, S. Chakravorty, S. Kachur, L. Bienvenu,\nM. White, and S. B. Shah, “Echocardiography in congenital heart\ndisease,” Progress in cardiovascular diseases, vol. 61, no. 5-6, pp. 468–\n475, 2018.\n[9] J. S. Chorba, A. M. Shapiro, L. Le, J. Maidens, J. Prince, S. Pham,\nM. M. Kanzawa, D. N. Barbosa, C. Currie, C. Brooks, et al. , “Deep\nlearning algorithm for automated cardiac murmur detection via a digital\nstethoscope platform,” Journal of the American Heart Association ,\nvol. 10, no. 9, p. e019905, 2021.\n[10] Z. Hoodbhoy, U. Jiwani, S. Sattar, R. Salam, B. Hasan, and J. K. Das,\n“Diagnostic accuracy of machine learning models to identify congenital\nheart disease: a meta-analysis,” Frontiers in artificial intelligence, vol. 4,\np. 708365, 2021.\n[11] M. Alkhodari and L. Fraiwan, “Convolutional and recurrent neural\nnetworks for the detection of valvular heart diseases in phonocardiogram\nrecordings,” Computer Methods and Programs in Biomedicine, vol. 200,\np. 105940, 2021.\n[12] J. Burns, M. Ganigara, and A. Dhar, “Application of intelligent phono-\ncardiography in the detection of congenital heart disease in pediatric\npatients: a narrative review,” Progress in Pediatric Cardiology, vol. 64,\np. 101455, 2022.\n[13] S. Aziz, M. U. Khan, M. Alhaisoni, T. Akram, and M. Altaf, “Phonocar-\ndiogram signal processing for automatic diagnosis of congenital heart\ndisorders through fusion of temporal and cepstral features,” Sensors,\nvol. 20, no. 13, p. 3790, 2020.\n[14] A. A. Sepehri, A. Kocharian, A. Janani, and A. Gharehbaghi, “An\nintelligent phonocardiography for automated screening of pediatric heart\ndiseases,” Journal of medical systems , vol. 40, pp. 1–10, 2016.\n[15] J.-K. Wang, Y .-F. Chang, K.-H. Tsai, W.-C. Wang, C.-Y . Tsai, C.-H.\nCheng, and Y . Tsao, “Automatic recognition of murmurs of ventricular\nseptal defect using convolutional recurrent neural networks with tempo-\nral attentive pooling,” Scientific Reports, vol. 10, no. 1, pp. 1–10, 2020.\n[16] B. Bozkurt, I. Germanakis, and Y . Stylianou, “A study of time-frequency\nfeatures for cnn-based automatic heart sound classification for pathology\ndetection,” Computers in biology and medicine , vol. 100, pp. 132–143,\n2018.\n[17] P. Hamet and J. Tremblay, “Artificial intelligence in medicine,”\nMetabolism, vol. 69, pp. S36–S40, 2017.\n[18] O. Tutsoy, “Pharmacological, non-pharmacological policies and muta-\ntion: an artificial intelligence based multi-dimensional policy making\nalgorithm for controlling the casualties of the pandemic diseases,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 44,\nno. 12, pp. 9477–9488, 2021.\n[19] M. Alkhodari, H. F. Jelinek, A. Karlas, S. Soulaidopoulos, P. Arsenos,\nI. Doundoulakis, K. A. Gatzoulis, K. Tsioufis, L. J. Hadjileontiadis,\nand A. H. Khandoker, “Deep learning predicts heart failure with pre-\nserved, mid-range, and reduced left ventricular ejection fraction from\npatient clinical profiles,” Frontiers in Cardiovascular Medicine , vol. 8,\np. 755968, 2021.\n[20] J. J. G. Ortiz, C. P. Phoo, and J. Wiens, “Heart sound classification based\non temporal alignment techniques,” in 2016 computing in cardiology\nconference (CinC), pp. 589–592, IEEE, 2016.\n[21] M. Daibo, “Toroidal vector-potential transformer,” in 2017 Eleventh\nInternational Conference on Sensing Technology (ICST), pp. 1–4, IEEE,\n2017.\n[22] M. Gjoreski, A. Gradi ˇsek, B. Budna, M. Gams, and G. Poglajen,\n“Machine learning and end-to-end deep learning for the detection of\nchronic heart failure from heart sounds,” Ieee Access, vol. 8, pp. 20313–\n20324, 2020.\n[23] O. Tutsoy and M. Y . Tanrikulu, “Priority and age specific vaccination\nalgorithm for the pandemic diseases: a comprehensive parametric pre-\ndiction model,” BMC Medical Informatics and Decision Making, vol. 22,\nno. 1, p. 4, 2022.\n[24] T. Yao, Y . Pan, Y . Li, C.-W. Ngo, and T. Mei, “Wave-vit: Unifying\nwavelet and transformers for visual representation learning,” inEuropean\nConference on Computer Vision , pp. 328–345, Springer, 2022.\n[25] M. A. Reyna, Y . Kiarashi, A. Elola, J. Oliveira, F. Renna, A. Gu,\nE. A. Perez Alday, N. Sadr, A. Sharma, J. Kpodonu, et al. , “Heart\nmurmur detection from phonocardiogram recordings: The george b.\nmoody physionet challenge 2022,” PLOS Digital Health , vol. 2, no. 9,\np. e0000324, 2023.\n[26] J. Oliveira, F. Renna, P. D. Costa, M. Nogueira, C. Oliveira, C. Ferreira,\nA. Jorge, S. Mattos, T. Hatem, T. Tavares, et al., “The circor digiscope\ndataset: from murmur detection to murmur classification,” IEEE journal\nof biomedical and health informatics , vol. 26, no. 6, pp. 2524–2535,\n2021.\n[27] P. Abry and P. Flandrin, “On the initialization of the discrete wavelet\ntransform algorithm,” IEEE Signal Processing Letters , vol. 1, no. 2,\npp. 32–34, 1994.\n[28] A. Silik, M. Noori, W. A. Altabey, and R. Ghiasi, “Selecting optimum\nlevels of wavelet multi-resolution analysis for time-varying signals in\nstructural health monitoring,” Structural Control and Health Monitoring,\nvol. 28, no. 8, p. e2762, 2021.\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems , vol. 30, 2017.\n[30] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, T. Rault, R. Louf, M. Funtowicz, et al., “Transformers: State-\nof-the-art natural language processing,” in Proceedings of the 2020\nconference on empirical methods in natural language processing: system\ndemonstrations, pp. 38–45, 2020.\n[31] R. Hu, J. Chen, and L. Zhou, “A transformer-based deep neural network\nfor arrhythmia detection using continuous ecg signals,” Computers in\nBiology and Medicine , vol. 144, p. 105325, 2022.\n[32] C. Che, P. Zhang, M. Zhu, Y . Qu, and B. Jin, “Constrained transformer\nnetwork for ecg signal processing and arrhythmia classification,” BMC\nMedical Informatics and Decision Making , vol. 21, no. 1, pp. 1–13,\n2021.\n[33] P. Lu, C. Wang, J. Hagenah, S. Ghiasi, T. Zhu, L. Thwaites, D. A.\nClifton, et al., “Improving classification of tetanus severity for patients\nin low-middle income countries wearing ecg sensors by using a cnn-\ntransformer network,” IEEE Transactions on Biomedical Engineering ,\n2022.\n[34] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:\nsynthetic minority over-sampling technique,” Journal of artificial intel-\nligence research, vol. 16, pp. 321–357, 2002.\n[35] M. Alkhodari, S. K. Azman, L. J. Hadjileontiadis, and A. H. Khandoker,\n“Ensemble transformer-based neural networks detect heart murmur in\nphonocardiogram recordings,” in 2022 Computing in Cardiology (CinC),\nvol. 498, pp. 1–4, IEEE, 2022.\n[36] S. Monteiro, A. Fred, and H. P. da Silva, “Detection of heart sound mur-\nmurs and clinical outcome with bidirectional long short-term memory\nnetworks,” in 2022 Computing in Cardiology (CinC) , vol. 498, pp. 1–4,\nIEEE, 2022.\n[37] A. Patwa, M. M. U. Rahman, and T. Y . Al-Naffouri, “Heart murmur and\nabnormal pcg detection via wavelet scattering transform & a 1d-cnn,”\narXiv preprint arXiv:2303.11423 , 2023.\n[38] S. Summerton, D. Wood, D. Murphy, O. Redfern, M. Benatan, M. Kaisti,\nand D. C. Wong, “Two-stage classification for detecting murmurs from\nphonocardiograms using deep and expert features,” in 2022 Computing\nin Cardiology (CinC) , vol. 498, pp. 1–4, IEEE, 2022.\n[39] A. McDonald, M. J. Gales, and A. Agarwal, “Detection of heart murmurs\nin phonocardiograms with parallel hidden semi-markov models,” in 2022\nComputing in Cardiology (CinC) , vol. 498, pp. 1–4, IEEE, 2022.\n[40] S. Parvaneh, Z. Ardalan, J. Song, K. Vyas, and C. Potes, “Heart\nmurmur detection using ensemble of deep learning classifiers for phono-\ncardiograms recorded from multiple auscultation locations,” in 2022\nComputing in Cardiology (CinC) , vol. 498, pp. 1–4, IEEE, 2022.\n[41] Y . N. Fuadah, M. A. Pramudito, and K. M. Lim, “An optimal approach\nfor heart sound classification using grid search in hyperparameter\noptimization of machine learning,” Bioengineering, vol. 10, no. 1, p. 45,\n2022.\n[42] K. Bayoumy, M. Gaber, A. Elshafeey, O. Mhaimeed, E. H. Dineen,\nF. A. Marvel, S. S. Martin, E. D. Muse, M. P. Turakhia, K. G. Tarakji,\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nALKHODARI et al.: IDENTIFICATION OF CONGENITAL VALVULAR MURMURS IN YOUNG PATIENTS USING DEEP LEARNING-BASED ATTENTION TRANSFORM-\nERS AND PHONOCARDIOGRAMS 13\net al. , “Smart wearable devices in cardiovascular care: where we are\nand how to move forward,” Nature Reviews Cardiology, vol. 18, no. 8,\npp. 581–599, 2021.\n[43] D. A. Danford, A. Martin, S. Fletcher, and C. H. Gumbiner, “Echocar-\ndiographic yield in children when innocent murmur seems likely but\ndoubts linger,” Pediatric cardiology, vol. 23, pp. 410–414, 2002.\n[44] S. Honda, M. Yamano, and T. Kawasaki, “Unusual change in murmurs\nin a case of mitral valve prolapse,” Cureus, vol. 14, no. 8, 2022.\n[45] D. Laohaprasitiporn, T. Jiarakamolchuen, P. Chanthong, K. Durong-\npisitkul, J. Soongswang, and A. Nana, “Heart murmur in the first week of\nlife: Siriraj hospital,” J Med Assoc Thai , vol. 88, no. Suppl 8, pp. S163–\n8, 2005.\n[46] M. Mirzarahimi, H. Saadati, H. Doustkami, R. Alipoor, K. Isazadehfar,\nand A. Enteshari, “Heart murmur in neonates: how often is it caused by\ncongenital heart disease?,” Iranian Journal of Pediatrics, vol. 21, no. 1,\np. 103, 2011.\n[47] Q.-m. Zhao, C. Niu, F. Liu, L. Wu, X.-j. Ma, and G.-y. Huang, “Accuracy\nof cardiac auscultation in detection of neonatal congenital heart disease\nby general paediatricians,” Cardiology in the Young , vol. 29, no. 5,\npp. 679–683, 2019.\nThis article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JBHI.2024.3357506\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}