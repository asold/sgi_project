{
  "title": "Extending a Pretrained Language Model (BERT) using an Ontological Perspective to Classify Students’ Scientific Expertise Level from Written Responses",
  "url": "https://openalex.org/W4391250553",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2522851148",
      "name": "Heqiao Wang",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A253590937",
      "name": "Kevin C. Haudek",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A4377644365",
      "name": "Amanda D. Manzanares",
      "affiliations": [
        "University of Northern Colorado"
      ]
    },
    {
      "id": "https://openalex.org/A2767230730",
      "name": "Chelsie L. Romulo",
      "affiliations": [
        "University of Northern Colorado"
      ]
    },
    {
      "id": "https://openalex.org/A2935357046",
      "name": "Emily A. Royse",
      "affiliations": [
        "Aims Community College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1802346448",
    "https://openalex.org/W2912605186",
    "https://openalex.org/W3082892415",
    "https://openalex.org/W2092161955",
    "https://openalex.org/W6628677735",
    "https://openalex.org/W4288407534",
    "https://openalex.org/W4365140191",
    "https://openalex.org/W1576954243",
    "https://openalex.org/W3171413158",
    "https://openalex.org/W6813861048",
    "https://openalex.org/W4389925395",
    "https://openalex.org/W4288059447",
    "https://openalex.org/W4381956168",
    "https://openalex.org/W2145713659",
    "https://openalex.org/W2585501642",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W4318917372",
    "https://openalex.org/W2299484695",
    "https://openalex.org/W3013861244",
    "https://openalex.org/W4388092438",
    "https://openalex.org/W4281788336",
    "https://openalex.org/W3034733147",
    "https://openalex.org/W2346967877",
    "https://openalex.org/W3030030185",
    "https://openalex.org/W2133109597",
    "https://openalex.org/W1976030211",
    "https://openalex.org/W4287634294",
    "https://openalex.org/W4328051995",
    "https://openalex.org/W2179541540",
    "https://openalex.org/W2989874066",
    "https://openalex.org/W3088736593",
    "https://openalex.org/W2115389369",
    "https://openalex.org/W1516171683",
    "https://openalex.org/W4311357538",
    "https://openalex.org/W4309934831",
    "https://openalex.org/W2475426101",
    "https://openalex.org/W2145555861",
    "https://openalex.org/W3131876108",
    "https://openalex.org/W4317938049",
    "https://openalex.org/W4323783971",
    "https://openalex.org/W2921248496",
    "https://openalex.org/W2995998574",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6637051603",
    "https://openalex.org/W1558051978",
    "https://openalex.org/W4318751300",
    "https://openalex.org/W2229270207",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3143722809",
    "https://openalex.org/W4403506242",
    "https://openalex.org/W4318464630",
    "https://openalex.org/W2124714467",
    "https://openalex.org/W2611059584",
    "https://openalex.org/W2102866687",
    "https://openalex.org/W89901217",
    "https://openalex.org/W325314667",
    "https://openalex.org/W2886409168",
    "https://openalex.org/W1511930410",
    "https://openalex.org/W2120320346",
    "https://openalex.org/W242266841",
    "https://openalex.org/W2151605842",
    "https://openalex.org/W3092997597",
    "https://openalex.org/W4296041706",
    "https://openalex.org/W1991302655",
    "https://openalex.org/W3112686520",
    "https://openalex.org/W3201077663",
    "https://openalex.org/W2788036042",
    "https://openalex.org/W4386116667",
    "https://openalex.org/W3128465392",
    "https://openalex.org/W4238049043",
    "https://openalex.org/W8576404",
    "https://openalex.org/W4319068540",
    "https://openalex.org/W2966579296",
    "https://openalex.org/W2910233085",
    "https://openalex.org/W2068192203",
    "https://openalex.org/W2772336132",
    "https://openalex.org/W4385782898",
    "https://openalex.org/W4225279564",
    "https://openalex.org/W4385632485",
    "https://openalex.org/W2154273617",
    "https://openalex.org/W3107851623",
    "https://openalex.org/W2047697290",
    "https://openalex.org/W4320489039",
    "https://openalex.org/W3081416018",
    "https://openalex.org/W4283773002",
    "https://openalex.org/W4226315886",
    "https://openalex.org/W3101804645",
    "https://openalex.org/W3012071850",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W4285053945",
    "https://openalex.org/W1602873706",
    "https://openalex.org/W4244808033",
    "https://openalex.org/W2048670267",
    "https://openalex.org/W1659668238"
  ],
  "abstract": "<title>Abstract</title> The complex and interdisciplinary nature of scientific concepts presents formidable challenges for students in developing their knowledge-in-use skills. The utilization of computerized analysis for evaluating students’ contextualized constructed responses offers a potential avenue for educators to develop personalized and scalable interventions, thus supporting the teaching and learning of science consistent with contemporary calls. While prior research in artificial intelligence has demonstrated the effectiveness of algorithms, including Bidirectional Encoder Representations from Transformers (BERT), in tasks like automated classifications of constructed responses, these efforts have predominantly leaned towards text-level features, often overlooking the exploration of conceptual ideas embedded in students’ responses from a cognitive perspective. Despite BERT’s performance in downstream tasks, challenges may arise in domain-specific tasks, particularly in establishing knowledge connections between specialized and open domains. These challenges become pronounced in small-scale and imbalanced educational datasets, where the available information for fine-tuning is frequently inadequate to capture task-specific nuances and contextual details. The primary objective of the present study is to investigate the effectiveness of a pretrained language model (BERT), when integrated with an ontological framework aligned with a contextualized science assessment, in classifying students’ expertise levels in scientific explanation. Our findings indicate that while pretrained language models such as BERT contribute to enhanced performance in language-related tasks within educational contexts, the incorporation of identifying domain-specific terms and extracting and substituting with their associated sibling terms in sentences through ontology-based systems can significantly improve classification model performance. Further, we qualitatively examined student responses and found that, as expected, the ontology framework identified and substituted key domain specific terms in student responses that led to more accurate predictive scores. The study explores the practical implementation of ontology in assessment evaluation to facilitate formative assessment and formulate instructional strategies.",
  "full_text": "Page 1/31\nExtending a Pretrained Language Model (BERT) using anOntological Perspective to Classify Students’ Scienti\u0000cExpertise Level from Written Responses\nHeqiao Wang \nMichigan State University\nKevin C. Haudek \nMichigan State University\nAmanda D. Manzanares \nUniversity of Northern Colorado\nChelsie L. Romulo \nUniversity of Northern Colorado\nEmily A. Royse \nAims Community College\nResearch Article\nKeywords: Pretrained language model, ontology, science education, expertise level, text classi\u0000cation, constructed\nresponse assessment\nPosted Date: January 26th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3879583/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.   Read Full\nLicense\nAdditional Declarations: No competing interests reported.\nPage 2/31\nAbstract\nThe complex and interdisciplinary nature of scienti\u0000c concepts presents formidable challenges for students in\ndeveloping their knowledge-in-use skills. The utilization of computerized analysis for evaluating students’\ncontextualized constructed responses offers a potential avenue for educators to develop personalized and scalable\ninterventions, thus supporting the teaching and learning of science consistent with contemporary calls. While prior\nresearch in arti\u0000cial intelligence has demonstrated the effectiveness of algorithms, including Bidirectional Encoder\nRepresentations from Transformers (BERT), in tasks like automated classi\u0000cations of constructed responses, these\nefforts have predominantly leaned towards text-level features, often overlooking the exploration of conceptual ideas\nembedded in students’ responses from a cognitive perspective. Despite BERT’s performance in downstream tasks,\nchallenges may arise in domain-speci\u0000c tasks, particularly in establishing knowledge connections between\nspecialized and open domains. These challenges become pronounced in small-scale and imbalanced educational\ndatasets, where the available information for \u0000ne-tuning is frequently inadequate to capture task-speci\u0000c nuances\nand contextual details. The primary objective of the present study is to investigate the effectiveness of a pretrained\nlanguage model (BERT), when integrated with an ontological framework aligned with a contextualized science\nassessment, in classifying students’ expertise levels in scienti\u0000c explanation. Our \u0000ndings indicate that while\npretrained language models such as BERT contribute to enhanced performance in language-related tasks within\neducational contexts, the incorporation of identifying domain-speci\u0000c terms and extracting and substituting with\ntheir associated sibling terms in sentences through ontology-based systems can signi\u0000cantly improve classi\u0000cation\nmodel performance. Further, we qualitatively examined student responses and found that, as expected, the ontology\nframework identi\u0000ed and substituted key domain speci\u0000c terms in student responses that led to more accurate\npredictive scores. The study explores the practical implementation of ontology in assessment evaluation to\nfacilitate formative assessment and formulate instructional strategies.\n1. INTRODUCTION\nThe advent of the 21st century has intensi\u0000ed global apprehension surrounding the advancement of Science,\nTechnology, Engineering, and Mathematics (STEM) education. The imperative to enhance the quality of STEM\neducation and its associated skills resonates within contemporary economic and political discourse (Zeidler, 2016).\nSTEM education, as delineated by the National Research Council (NRC, 2011), imparts vital competencies to K-12\nstudents, including problem-solving, critical thinking, logical reasoning, decision-making, along with domain-speci\u0000c\nknowledge. These foundational skill sets bear signi\u0000cance not only for individual aspirations (Lee, Capraro, and\nViruru, 2018) but also for academic and vocational growth in today's workforce (Chiu and Krajcik, 2020; Jang, 2016).\nHowever, there is also widespread recognition of the inherent challenges in developing and assessing these STEM\nskills due to the intricate and interdisciplinary nature of the underlying concepts and theories (English, 2016; Kelley\nand Knowles, 2016). Evaluating STEM-related skills often hinges on practical applications, involving activities such\nas conceptualization, designing, prototyping, and evaluating outcomes pertaining to educational artifacts and\nsolutions (Falloon et al., 2020). An integral component of these evaluative products is scienti\u0000c literacy, typically\ndemonstrated through a written product that exhibit a student's ability to assess the quality of scienti\u0000c information,\narticulate positions based on evidence, and apply conclusions to the natural world using scienti\u0000c methodologies,\nsuch as inquiry (Lehrer and Schauble, 2006; Goldman et al., 2016). The manifestation of scienti\u0000c literacy varies\namong individuals, whether through the adept use of technical terms or the practical application of scienti\u0000c\nconcepts and processes (Bauer, 1992).\nPage 3/31\nEmpirically, the analysis of scienti\u0000c literacy yields insights into students’ knowledge and comprehension, which\nenables STEM teachers to re\u0000ne instructional methods and address academic and psychological challenges (e.g.,\nmisconception; Udompong & Wongwanich, 2014). Incorporating the teaching of scienti\u0000c literacy into general\neducation across different grade levels entails merging the understanding of content-area STEM concepts with\nfundamental reading comprehension and written expression activities (Norris and Phillips, 2003; Yore, 2003). This\nintegration is in accordance with national and state standards such as the Framework for K-12 Science Education\n(NRC, 2011), Common Core State Standard for English Language Arts (CCSS, 2010), and Next Generation Science\nStandards (NGSS Lead States, 2013). Additionally, the scienti\u0000c literacy acquired during primary and secondary\nschool can serve as the foundation for postsecondary academic and career achievement, as emphasized by the\nNational Science Education Standards (NRC, 1996). These standards collectively emphasize the development of\nknowledge-in-use skills, which require students to apply their learned knowledge in explaining natural phenomena\nacross diverse contexts while expressing ideas effectively (NRC, 2014).\nThe standards and frameworks within the domain of science education champion a transformative shift in the\neducational paradigm where students should prioritize developing a profound and systemic understanding of core\nideas (Krajcik, Sutherland, Drago, and Merritt, 2012). This departure from the traditional pedagogical approach that\ninvolves accumulating super\u0000cial knowledge across a broad spectrum of discrete information, aligns with the\nconcept of systems thinking (Meadows, 2008). Achieving competency in systems thinking is critical for science\nlearners as they develop literacy in scienti\u0000c practices (Libarkin & Kurdziel, 2006; Hmelo-Silver et al., 2007). The\naccompanying challenge lies in establishing high-quality and reliable educational assessments that can closely\ncorrespond to this overarching goal (Pellegrino, 2013). Constructed response (CR; i.e., open-ended questions)\nassessments have been well documented in its effectiveness in evaluating students' knowledge-in-use capabilities\n(Maestrales et al. 2021; Jung, Tyack, and Davier, 2022; Snow, 2012). CR assessments in STEM education often\nengage students with rich, contextualized information and create authentic scenarios for them to convey related\nideas (Jescovitch et al., 2021). Unlike multiple-choice items, CR items are designed to reveal examinees' original\nthoughts and allow students to demonstrate knowledge application by demanding responses in their own words\n(Krajcik, 2021). Thus, CR items are deemed a valuable method for effectively analyzing students’ scienti\u0000c literacy\nand higher-order thinking.\nThe automation of the scoring process in CR assessments using approaches from Arti\u0000cial Intelligence (AI)\npresents a promising avenue for improving e\u0000ciency of evaluation of these types of assessments (see Zhai, Shi,\nand Nehm, 2021, for a review). Our study proposes leveraging the considerable success of pretrained language\nrepresentation models and integrating them with an ontology-based approach to identify specialized science\ndisciplinary vocabulary and extracting sibling terms. This integration streamlines the automatic scoring process.\nImportantly, we posit that the proposed approach harmonizes with the prevailing assessment paradigm in science\neducation, which emphasizes the analysis of the presence and interconnections of core ideas within domain-\ncontextualized scenarios. A key challenge of automatic evaluation of CR assessments in science remains that as\nstudents gain knowledge in the discipline, their use of domain speci\u0000c terminology can increase (Jescovitch et al.,\n2021). Further, detecting student reasoning in CR when explaining phenomenon can be complicated by the domain\nspeci\u0000c terms based on the provided context of the item itself (Shiroda et al., 2023). This study anticipates\ncontributing to STEM disciplinary education more broadly and, speci\u0000cally, to environmental science education by\nintroducing a faster and more resource-e\u0000cient scoring algorithmic model from an ontological perspective. In\nsummary, our method is tailored for \u0000ne-tuning a generalized pretrained language model (BERT: Bidirectional\nEncoder Representations from Transformers) to score contextualized open-ended responses by considering\ndomain-speci\u0000c vocabulary not covered in BERT’s prede\u0000ned glossary utilizing an ontology-based approach.\nPage 4/31\n2. LITERATURE REVIEW\n2.1 Coding Frameworks of CR Assessment\nAccording to the Framework for K-12 Science Education (NRC, 2011), fostering deep science understanding and\nreasoning in K-12 students is achieved by providing support for three dimensions of science learning: disciplinary\ncore ideas, scienti\u0000c and engineering practices, and crosscutting concepts. This paradigm provides educators with\na holistic means of nurturing students' knowledge-in-use skills when constructing explanations for phenomena and\naddressing real-world problems within the educational process. Scienti\u0000c CR assessments tied to these three\ndimensions help gauge students' pro\u0000ciency in establishing coherence and making sense of these elements\n(Underwood et al., 2018).\nObtaining a precise and reliable CR assessment involves an iterative process that includes the development,\nutilization, and re\u0000nement of an expert-validated scoring rubric along with expert scores (Nehm et al., 2010). Prior\nresearch studies have employed two primary coding schemes to characterize the quality of student CRs. Analytic\nrubrics consist of dichotomous criteria designed to ascertain the presence or absence of construct-relevant ideas in\nstudent responses (Kaldaras and Haudek, 2022). Each analytic rubric bin represents a distinct concept, and each\nresponse must be scored for each analytic rubric bin. While a response may receive a score of “1” in multiple bins\ndue to the coexistence of multiple concepts, some rubric bins may be mutually exclusive. In these instances, not all\nbins receive a score of “1” based on the design of descriptors by researchers and educators to assess students’\nunderstanding of a speci\u0000c item with precision. Analytic rubrics are often favored in educational settings for their\nreliability compared to other coding schemes as they evaluate key content components of reasoning and can\nprovide speci\u0000c feedback to students (Jönsson and Svingby, 2007; Jescovitch et al., 2019). A second coding\napproach—using a holistic rubric—involves the use of multi-leveled coding schemes with the goal of providing a\nsingular, overall judgment of students’ CRs grounded in the accuracy or quality of their explanation or reasoning\n(Jescovitch et al., 2021). This holistic approach is typically most suitable when the overall excellence of a response\nsurpasses the combined merit of its individual components (Tomas, Whitt, Lavelle-Hill, and Severn, 2019). It aims to\ncapture general features of answer quality (e.g., organization, style, and persuasiveness) relying on raters’\nsensitivities to the construct (Klein et al., 1998), backgrounds and knowledge (Zhai, Haudek, Stuhlsatz, and Wilson,\n2020). It is widely documented that the reliability of holistic scores can be in\u0000uenced by various sources of\nmeasurement errors such as raters' effects, the writer's individual characteristics, and the writing prompt itself used\nto elicit a writing sample (Barkaoui, 2007; Wang and Troia, 2023).\nIn the \u0000eld of science education, there is inadequate evidence supporting the universal superiority of one coding\nscheme over the other for human coding (Tomas, Whitt, Lavelle-Hill, and Severn, 2019). The choice of coding\nmethod is highly dependent upon the speci\u0000c writing constructs being assessed and the intended purpose of the\nentire judgement. However, a prevalent notion exists that quantitative measures are less susceptible to threats to\ninternal validity when compared to qualitative scoring rubrics (Troia, Shen, and Brandon, 2019). To mitigate potential\nbias, researchers suggested both the deconstruction of holistic rubrics and reconstruction of analytic rubrics to\nsingular levels, respectively, to ensure the rubric descriptors measure the intended aspects in a quantitative,\nmeasurable manner (Jescovitch et al., 2019; Martin and Graulich, 2023). Previous studies have shown the\ndecomposition of holistic rubrics of multi-dimensional science CRs, which can be achieved by breaking down a\nholistic rubric into discrete conceptual components which form the basis of analytic rubrics, and be used with high\ninterrater reliability (Kaldaras, Yoshida and Haudek, 2022). Conversely, other studies acknowledge the sophisticated\nconstruct implied by the holistic nature of assessment. This perspective is evident in Jescovitch et al.’s study\nPage 5/31\n(2021), where analytic codes were amalgamated using validated Boolean logic to align to hypothesized learning\nprogression levels in science education.\n2.2 Automated Analysis of CR Assessment\nA valid and reliable coding framework in science education serves not only to assist educators in capturing\nstudents’ conceptual acquisition based on rubric descriptors but also holds promise for improving the reliability of\nfuture automated scoring tools by serving as labels for model training and validation based on human raters’ codes.\nSupervised machine learning (ML) models have demonstrated signi\u0000cant success in both holistic and analytic\nscoring using various algorithms to evaluate students’ contextualized science CRs (see Jescovitch et al., 2019; Zhai,\nHaudek, and Ma, 2023; Zhai, He, and Krajcik, 2022). In a systematic review conducted by Zhai and Yin et al. (2020),\nthe synthesis of 45 studies substantiated the effectiveness of supervised ML models in scoring scienti\u0000c responses\ncomposed by K-16 students, as indicated by a median Cohen’s Kappa of 0.72 across all investigated studies. Zhai\nand colleagues also concluded a thought-provoking observation that the predominant focus of existing ML studies\nin this synthesis was on replacing human efforts rather than deepening them, such as gaining a deeper\nunderstanding of what cognitive factors can contribute to students' performance in scienti\u0000c tasks.\nGiven the limitations in automated scoring models, it is vital to consider integrating more nuanced writing\nconstructs in CR assessments. It may include individual differences related to sociocultural and cognitive factors\n(e.g., Crossley, Allen, Snow, and McNamara, 2016), along with academic attributes (e.g., Murphy and Yancey, 2008;\nWang and Troia, 2023) such as keywords, words frequency, sentence structure, text length, given their substantial\nin\u0000uence on essay quality and characteristics. For instance, coherent and quali\u0000ed CRs tend to demonstrate a\ngreater and more appropriate use of academic, sophisticated vocabulary, coupled with a more advanced level of\nsyntactic complexity in explaining scienti\u0000c phenomena (Wang et al., 2023). An exemplary approach in this domain\nis the Constructed Response Classi\u0000er (CRC; see Noyes et al., 2020) which comprises an 8-classi\u0000cation ML-based\nalgorithm ensemble implemented in R program (Jurka et al., 2012). In the CRC, each student response is treated as\na document, with coding rubric bins considered as classes. Text features, extracted as n-grams, TF-IDF, stemmed at\nthe word level, serve as input variables for classi\u0000cation algorithms. The target variable for training these algorithms\nis expert-assigned codes. The eight ML algorithms independently provide categorizations, and the \u0000nal prediction is\ndetermined by a majority vote.\nUnsupervised pre-trained language representation (LR) models have displayed considerable potential in text\nclassi\u0000cation, particularly in assessing scoring levels within science education. An illustrative example of this\napplication is evident in the work of Cochran, Cohn, Hastings, et al. (2023), wherein LR models were employed to\nidentify the causal structure present in students’ scienti\u0000c explanations. Transformer-based Natural Language\nProcessing (NLP) models, as exempli\u0000ed by prominent instances such as BERT and GPT, have become the de facto\nindustry standard for a diverse range of NLP downstream tasks (Cochran, Cohn, Rouet, and Hastings, 2023; Wulff et\nal., 2023). Prior research (e.g., Cochran et al., 2022) has consistently highlighted the effectiveness of BERT-based\ntransformers in evaluating students’ responses to STEM-related questions. These models undergo an initial phase\nof pre-training on extensive corpora to acquire generalized language representations, followed by \u0000ne-tuning for\nspeci\u0000c tasks to incorporate domain-speci\u0000c knowledge. In contrast to conventional word vectorization methods\n(e.g., one-hot encoding, word2vec, GloVe), pretrained LR models, e.g., BERT, dynamically represent words by avoiding\nthe assignment of speci\u0000c and \u0000xed embedding vectors to words. These models have garnered recognition for their\noutstanding performance across 11 downstream tasks. However, the direct use of embeddings extracted from LR\nmodels to address domain-speci\u0000c NLP problems often results in suboptimal performance, primarily due to the\nknowledge gap between training corpora and domain-speci\u0000c contexts (Liu et al., 2020). To tackle this challenge,\nPage 6/31\nspecialized contextualized embedding LR models such as SciBERT (Beltagy, Lo, and Cohan, 2019) and BioBERT (Lee\net al., 2020) have been developed and utilized extensively, which are trained on large-scale scienti\u0000c and biomedical\ncorpora, respectively. Additionally, SciEdBERT (Liu et al., 2023), explicitly designed for science education contexts,\nemphasizes the importance of domain-speci\u0000c pretraining based on the data derived from prominent science\neducation journals. It introduces a generalized strategy for automating science education tasks, particularly those\nrelated to scoring and text classi\u0000cation.\n2.3 Contextualized Word Embedding\nWithin the NLP realm, word embeddings refer to representations in a continuous space for words that preserve both\nsemantic and syntactic similarities between them (Chen, Perozzi, Al-Rfou, and Skiena, 2013). The fundamental\ndistinction between a word’s non-contextualized core meaning and the senses expressed in speci\u0000c linguistic\ncontexts can be clari\u0000ed through the understanding and analysis of contextualized word embeddings (Hofmann,\nPierrehumbert, and Schütze, 2020). This involves aligning type-level representations with token-level representations\nbased on the linguistic context. The integration of contextualized word embeddings into pretrained learning models\nhas markedly improved performance across diverse tasks compared to the capabilities of using static word\nembeddings that exclusively capture type-level representations (see Selva Birunda and Kanniga Devi, 2021, for a\nreview). Recognizing that the linguistic properties and meanings of words can vary across extralinguistic contexts\n(e.g., time and social space; see Rudolph and Blei, 2018), comprehending the contextualized word embeddings of\nwords is particularly paramount. In educational assessment, understanding the contextualized word embeddings of\ndomain-speci\u0000c academic vocabulary holds signi\u0000cant value. For instance, Technical Language Processing\ntechniques can e\u0000ciently extract text-based scienti\u0000c information using word embeddings. These embeddings are\nsubsequently utilized in constructing language models to re\u0000ect students’ understanding of topical background,\nthereby practically enhancing overall modeling performance (Kumar, Starly, and Lynch, 2023).\nSimilar to static word embeddings, contextualized word embeddings are usually generated through training on\nextensive unlabeled corpora using some variant of Large Language Models, as exempli\u0000ed by the BERT architecture\n(Devlin et al., 2019). BERT employs transformer encoders featuring a self-attention mechanism and a masked\nlanguage modeling target to predict missing words in a sentence by considering both left and right contexts of a\ntarget word simultaneously. This unique contextualization sets BERT apart from other language models. In contrast\nto static word embeddings, this approach signi\u0000cantly enhances performance by incorporating self-attention and\nthe non-directionality inherent in the language modeling task. Notably, the BERT Tokenizer utilizes a predetermined\nvocabulary of 30,522 distinct tokens, each assigned a unique token ID. During BERT pretraining on extensive corpora\nsuch as Wikipedia and Book Corpus datasets, the model learns to convert each token ID into contextualized\nembeddings, which effectively captures nuanced information for each token within the provided sequence.\nWhile Google’s BERT stands out as a leading contextualized embedding model for most domain-speci\u0000c tasks,\nfollowed by ELMo, GPT, and XLNET (Yunianto, Permanasari, and Widyawan, 2020), it encounters challenges in\ncertain domain-speci\u0000c tasks due to limited knowledge connections between speci\u0000c and open domains. These\nchallenges are accentuated when dealing with small-scale and imbalanced datasets, where essential information\nfor \u0000ne-tuning is often lacking to capture task-speci\u0000c nuances and contextual details. One potential solution to this\nproblem is to pretrain a model with an emphasis on domain speci\u0000city by generating domain-speci\u0000c contextualized\nembeddings rather than relying on the publicly provided ones that offer generalized word embeddings for\npredetermined words and subwords. Nevertheless, pretraining such models from scratch can be time-consuming\nand computationally expensive, rendering it impractical for most users. Moreover, implementing this approach in\neducational contexts poses signi\u0000cant challenges, including potential issues with \u0000ne-tuning instabilities (Dodge et\nPage 7/31\nal., 2020; Lee, Cho, and Kang, 2019), and raises ethical concerns regarding diversity and representation (Baird and\nSchuller, 2020; Yan et al., 2023) given the inherent constraints of small and skewed datasets.\n2.4 Environmental Ontology\nThe term “ontology” \u0000nds its origins in philosophy, representing a collection of concepts utilized to depict tangible\nobjects in the world (Smith, 2012). In the current era marked by an information explosion and AI revolution, ontology\nemerges as a potent method for storing, organizing, and retrieving valuable information (Asim et al., 2019).\nFunctioning as an abstract description system for knowledge representation within speci\u0000c domains, ontology\ntakes on the fundamental task of capturing domain knowledge and concepts. Its applicability now widely extends to\nthe AI research community, facilitating the processing and reuse of existing data for communication among\nprograms, services, agents, and users (Rahman and Hussain, 2020). In an ontology, de\u0000nitions establish\nconnections between the names of entities (represented as nodes in a knowledge graph) in the universe of\ndiscourse (e.g., classes, relations, functions, or other objects) and “human-readable text describing what the names\nmean, and formal axioms that constrain the interpretation and well-formed use of these terms” (as cited in Gruber,\n1995, p.908).\nBy organizing ontological classes (terms) hierarchically and describing relationships between terms with a limited\nset of relational descriptors (e.g., part_of, is_a, located_in, instance_of), ontology establishes a standardized\nvocabulary for representing entities in a given domain (Arp, Smith, and Spear, 2015). However, applications\nleveraging domain ontologies necessitate the quanti\u0000cation of relationships between two terms. The semantic\nsimilarity between terms, given the underlying domain ontology, serves as suitable measure for such relationships.\nFor example, in a provided ontology like wildlife ontology, the computation of semantic distance using geometrical\nmetrics such as cosine similarity reveals the relatedness between embedding vectors of a seed term (e.g., animal)\nand those of its sibling terms (e.g., mammal, reptile, insect, amphibian, mollusk) within the hierarchical structure of\nthe domain. This nuanced representation proves particularly valuable when dealing with terms lacking exact\nsynonyms, as ontologies provide related siblings that share overlapping meanings and relationships. The precision\nintroduced by these relationships enhances the detailed and nuanced representation of information within the\nspeci\u0000ed domain, extending beyond mere synonymy to capture subtle variations and distinctions among related\nterms.\nA diverse array of ontologies is available on the DBpedia website[1]. For example, in environmental science, the\nEnvironment Ontology (EnvO) is noteworthy, encompassing three hierarchies, including subclasses such as biome,\nenvironmental feature, and environmental material (refer to Buttigieg et al., 2013 for details). An optimal approach to\nannotating entities with EnvO involves combining classes from each hierarchy to comprehensively describe an\nenvironmental system from these three different perspectives. Baker and colleagues (2009) have expounded on the\nsigni\u0000cant contributions of ontologies, such as EnvO, to the development of educational assessments and learning\ndesign. By enhancing domain transparency for students, ontologies can effectively reveal important elements or\nconcepts and their interrelationships. A notable feature of a domain ontology lies in its capability to convey the\nimportance of a class through its level of connectivity, potentially guiding educational decisions about which ideas\nare central to the teaching and learning of a science domain. This integration of ontologies into educational\nassessments not only fosters a deeper understanding of domain-speci\u0000c concepts but also provides a structured\nframework for developing and evaluating essential skills in alignment with contemporary educational goals (e.g.,\nLibarkin and Kurdziel, 2006).\n3. RESEARCH PURPOSES\nPage 8/31\nThe primary objective of the present study is to investigate the effectiveness of the established industrial standard\npretrained language model BERT, when integrated with an ontological framework aligned with our science\nassessment context, in classifying students’ expertise levels in written scienti\u0000c explanations. To this end, the study\nspeci\u0000cally addresses two research questions: RQ1. To what extent can pretrained language models (i.e., BERT) and\nmachine learning models (i.e., ensemble learning algorithms) accurately identify students’ scienti\u0000c expertise levels\nas revealed by CR assessments in environmental science? RQ2. To what extent does the inclusion of domain-\nspeci\u0000c vocabulary from environmental science ontology (EnvO) impact the performance of BERT models? The\nmain contributions of this study can be summarized as follows.\nFirst, our study proposes an extension to the BERT model that transcends general word representations by\nintegrating domain-speci\u0000c word embeddings. Unlike traditional specialized vocabularies built solely from\nsemantically similar domain words, we argue that contextualized words, speci\u0000cally ontological siblings of each\ndomain-speci\u0000c word, can provide valuable semantic and syntactic information to model Out-of-Vocabulary (OOV)\nword embeddings.\nSecond, in contrast to methods using synonyms extracted from lexical databases such as WordNet and Concept\nNet to replace OOV words, our approach of using ontological siblings proves to be more suitable for our dataset.\nThere are several reasons. Firstly, many domain-speci\u0000c academic words, particularly those describing an\nenvironmental entity in our study, lack direct synonyms and thus pose a challenge in \u0000nding appropriate\nreplacements through conventional synonym extraction methods. Secondly, we posit that this novel approach also\nis well-suited for handling small-scale, imbalanced educational datasets. It offers a faster and more resource-\ne\u0000cient auto-scoring model without the need for costly pre-training on large-scale corpora. This is particularly\nadvantageous in educational contexts with limited data resources. Lastly, our chosen approach demonstrates the\ncapability to perform OOV word embedding and downstream tasks, such as text classi\u0000cation in science education.\nThis underscores the versatility and effectiveness of our methodology in addressing the speci\u0000c challenges\npresented by our dataset and educational context.\nLast but not least, our study focuses on evaluating undergraduate students’ expertise levels (ranging from 0 to 4)\nserving as labels for model training and testing. This evaluation is grounded in the analysis of students’ written CRs\nthat re\u0000ect their use of foundational discipline-speci\u0000c concepts and systems thinking abilities. These elements are\nintricately connected to their usage and understanding of domain-speci\u0000c words.\nCollectively, these aspects underscore the potential advantages of integrating an ontological framework that\ncomprehensively captures the expansive domain of environmental science within the scoring process. This\nintegration shows promise in enhancing our comprehension of how students employ language and articulate their\nideas, thereby facilitating the classi\u0000cation of their expertise levels based on the conveyed concepts.\n4. METHODOLOGY\n4.1 Dataset\n4.1.1 CR Data Collection\nData for our research was originally collected for a collaborative research project in creating the Next Generation\nConcept Inventory (NGCI) a concept inventory that utilizes short answer questions in leu of the traditional concept\ninventory, multiple choice questions. Data collection centered on undergraduate students in introductory\nPage 9/31\ninterdisciplinary environmental courses from ten institutions. These higher education institutions were purposefully\nselected to represent the three primary categories of four-year colleges according to the Carnegie Classi\u0000cations of\nInstitutions of Higher Education (Carnegie Foundation for the Advancement of Teaching, 2011) and the three\napproaches to interdisciplinary environmental science (IES) curriculum design outlined by a representative survey of\nhigher education institutions (see Vincent 2013 for further description of the three curricular designs in IES\nprogram). The sample includes baccalaureate colleges (n = 4), master’s college and universities (n = 3), and\ndoctoral/research universities (n = 4), and degree programs/tracks representative of the three approaches to\ncurriculum design—emphasis on natural systems (n = 7), emphasis on societal systems (n = 6), and emphasis on\nsolutions development (n = 4).\nStudent responses were collected from introductory IES courses during Fall and Spring semesters from Spring 2022\nthrough Spring 2023 by having students complete the assessment questions pre- and post-course discussion of the\nFEW Nexus (Food-Energy-Water). We then added the items in a Qualtrics survey and administered the survey to over\n400 IES undergraduates from seven post-secondary institutions across the United States to collect student\nresponses (UNCO IRB#158867-1). Responses were then de-identi\u0000ed for coding to create training and testing data\nfor machine learning.\nFor the purposes of this investigation, we use student responses (n = 1293) to two questions from the full NGCI\nassessment which were selected for analysis with different assumed levels of di\u0000culty: (1) a question about\nidentifying energy sources related to water reservoirs (Reservoir), and (2) a question about assessing tradeoffs\n(Biomass). These items were chosen as a related pair, as the Reservoir item has students identify sources of and\nconnections between FEW and Biomass has students explain the possible tradeoffs around FEW given a certain\nscenario (see Table 1). Further, responses to both items were coded on a similar scale in their respective coding\nrubrics related to expertise of the responses. For analysis, we combined the two parts of Reservoir, since it was\nnecessary to consider both sources and connections to understand student answers. For Biomass, we analyzed\nresponses for part A and B using separate rubrics.\nTable 1\nDescriptions of Constructed Response Assessment Items\nReservoir\na. A reservoir is an arti\u0000cial lake that stores water. What types of energy does the water in this reservoirpossess?\nb. Explain how the kinds of energy listed in your previous response could be used for food production by nearbyfarmers.\nBiomass\nBiomass energy production involves growing certain crops and converting them to energy. Corn, in the form ofethanol, is a common source of biomass energy. To increase its energy independence from natural gas, yourcommunity decides to convert half of their existing agricultural bean \u0000elds to corn biomass crops that willeventually provide energy to the surrounding area. You have taken an environmental course and know thatburning natural gas has a greater energy return than burning biomass (e.g., one unit of natural gas requires lessenergy to produce than one unit of ethanol biomass).\na. You realize that corn requires more water to produce than beans; much of your irrigation water comes from anearby river and limited rainfall. How would you expect the area’s water use to change as a result of this shiftfrom agriculture to biomass production?\nb. Tradeoffs describe the compromise between positive and negative outcomes of a decision. A tradeoff resultsin something decreasing in return for gains in something else. Describe the tradeoffs to food, energy, and watersystems in switching from bean farming to biomass (corn) farming.\nPage 10/31\n4.1.2 Rubric Development\nRubric development began by reviewing examples of published rubrics that were used in similar assessments and\nintended for use with automatic scoring (Jescovitch et al., 2021; Sripathi et al., 2023). We agreed upon a scale that\nwould best represent the students’ varying levels of knowledge (Table 2). We created each rubric by \u0000rst analyzing\nthe range of student answers we had received from the different participating institutions. During this initial review,\nwe used an inductive approach and read student answers to identify common themes that revealed student\nknowledge regarding food, energy, and water systems and their relationships to each other, to other natural, and to\nhuman systems.\nFor the \u0000rst stage of coding, we developed dichotomous, analytic rubrics aligned to key disciplinary concepts (e.g.,\nchanges to water usage) for the content analysis of responses. We used an inductive-deductive approach to coding,\nstarting with an initial coding schema based on Vincent et al.’s (2013) dimensions of knowledge. Recurring\nelements that students included in their written responses, such as predictions about energy transfer, were\nconsolidated into a coding rubric (supplemental information in Royse et al., in review). These rubrics had parallel\nstructures for each node of the FEW Nexus. Each response was categorized based on the ideas it contained. Using\nthese rubrics, two researchers coded responses to each of the questions. After scoring independently, the scorers\ncompared assigned scores. If scores on a response agreed, then the scores were taken as consensus \u0000nal scores.\nIf scores disagreed, then scorers met to discuss and resolve the issue to a \u0000nal consensus score. The coding team\nthen engaged in \u0000ve iterative rounds of independently coding artifacts; meeting in between rounds to further modify\nthe coding schema until an agreement level of at least 0.85 was reached between all coders. After reaching a\nconsensus for the rubric, all student responses were divided between two members of the research team and were\nscored independently. A total of 346 responses were scored for the Reservoir question item and 947 responses for\nthe Biomass question item (see Table 3 for details). We provide an example rubric for parts of the Tradeoffs of FEW\nSystems: Biomass Energy Production question item (hereafter referred to as “Biomass question item”) in Table 2,\nand the other rubrics are available in the Appendix.\nTable 2\nAnalytic coding rubric for Tradeoffs Systems: Biomass Energy Production\nPart A Rubric Part B Rubric\nBin Brief Description Bin Brief Description\nA Increased Water Consumption A1 Less food produced\nB1 Water scarcity/not enough water total A2 More land converted to meet food needs\nB2 Generally less water/decrease in wateravailable B1 More water use\nB3 Less water for other things B2 Less water available\nB4 Change in human behavior in water useC1 Energy produced (corn creates biomass energy)\nC Water prices change C2 Energy return on investment (ethanol is lesse\u0000cient/lower EROI than other sources)\nD Changes to river C3 Renewable energy, more sustainable energy, lowerenvironmental impact source of energy\nE Impacts tobiodiversity/wildlife/ecosystem    \nPage 11/31\n4.1.3 Expertise Level\nFollowing the initial phase of constructing dichotomous, analytic rubrics, the subsequent coding stage involved the\nutilization of both student and instructor responses to our two selected assessment items. This process aimed to\nformulate four levels of expertise codes. Instructor responses were used to develop these levels based on expected\nstudent learning in IES courses. We then used a range of student responses to re\u0000ne the distinct levels of expertise.\nBased on the results from the \u0000rst stage of content coding, we then assigned each response for Reservoir, Biomass\npart A and Biomass part B to a single holistic expertise level, ranging from zero to four. These codes represent a\nmore holistic view of the response as a whole, instead of a more discrete code based on speci\u0000c content. The\nhigher levels represent more expertise in using disciplinary content and connections in response to the assessment\nprompt; these higher levels focus on making multiple connections in explanations. Conversely, lower levels in this\nscheme tend to focus on speci\u0000c vertices of the FEW Nexus, make singular connections or do not address the\nprompt. Speci\u0000cally, these expertise levels do not just identify speci\u0000c disciplinary terms or phrases, but instead\nfocus on the inclusion and utilization of ideas within the entire response. If students could explain the idea correctly,\neven without using the domain-speci\u0000c term, the response would still be coded at a high expertise level. We use\nthese expertise levels as our human-assigned codes in the data corpus.\nTable 3 illustrates the distribution of student data disaggregated by expertise level, ranging from 0 to 4. In terms of\nthe textual statistics concerning student responses, the dataset for Biomass Part A encompasses a total of 481\nstudent responses that were collected and coded (character length; mean = 146.76, SD = 93.98, range = 5-579). For\nthe dataset of Biomass Part B, a total of 466 responses were gathered (character length; mean = 231.16, SD = \n156.49, range = 3-1054). Regarding the dataset of Reservoir, it includes a total of 346 responses (character length;\nmean = 189.32, SD = 145.63, range = 11-1169). These statistics indicate that our NGCI assessment items require\nstudent to compose short, constructed responses to the designated open-ended questions. Table 4 displays\nexpertise level coding rubric for Biomass (part A and B) and Reservoir Assessment Items.\nTable 3\nDistribution of Student Data Disaggregated by Expertise Level\nDataset Expertise Level  \n0 1 2 3 4 Total\nReservoir 10 59 165 95 17 346\nBiomass part a 6 66 4 281 124 481\nBiomass part b 61 58 281 65 1 466\nTotal 77 183 450 441 142 1293\nPage 12/31\nTable 4\nExpertise Level Coding Rubric for Biomass and Reservoir Assessment Items\nExpertiselevel Biomass Reservoir\nPart A Part B\nDescriptionExample DescriptionExample Description Example\n4 Responseincludes acausalstatement,making oneor moreconnectionsbetweenincreasedwaterconsumption(given in theitem prompt)and a directeffect(s),such aschanges tothe river,waterscarcity oreconomicimpact.\nThe beansmay havebeen grownwith just thelimitedrainfall;switching tocorn wouldrequire waterto be pumpedin from theriver.Increasingthe amountof waterbrought tothe \u0000elds mayrequire newinfrastructure,and couldeven causewatershortages ifthe river issmall enoughor there is adrought.\nResponseincludesmultipleconnectionsor tradeoffsbetweenFood, Energyand Water inresponse tothephenomenonand includesspeci\u0000cdetails orexamples.\nAlthoughwe mayhave morebiomassfrom thecorn therewill be lessbeanproduction,steeringthe food tobe madewith lesshealthyingredients.Also wemay haveless waterfor beansandcitizens,but we willhave newbiomassenergy. theenergy maybe greatbut it takesmore waterto feed thecorn andmoreenergy thannatural gasto produce.\nResponseincludesmultipleconnectionsbetween all ofHydroelectricity(speci\u0000c),Energyproduction(general), Usesof Energy andUses of Water.\nI guesspotentialenergy. Myassumptionis that thewater will beturned intohydroelectricenergythough oncepassedthrough thedam.\nThe energyproduced bythis might beused byfarmers tomove otherwater forirrigation totheir crops. Itmay also beused topower somemachinery oreven justtheir homesand facilities.\n3 Response re-explainswaterconsumptionwill increaseor that lesswater will beavailable forother usesbut withoutexplicitconnection.\nAs a result ofthe cropchange thewater usagewouldincreasebecause cornrequires morewater thanthe beans.\nResponseincludesmultiple,generalconnectionsor tradeoffsbetweenFood, Energyand Water inresponse tothephenomenon,with little tonoexplanation.\nIn order toget moreenergyfrombiomass,we mustproduceless foodand usemorewater.\nResponseincludes atleast threeideas andconnectionsrelated toHydroelectricity(speci\u0000c),Energyproduction(general), Usesof Energy andUses of Water.\nHydroelectricpower and asa waterresource. Thewater helpsfarmers growtheir cropsbut alsohelps powertheir farmsandmachinery.\n2 Responsedescribes achange inwater priceswithout\nWater billswould go upfor thesurroundingcommunity.\nResponseincludes asingleconnection ortradeoff\nEnergywould bethe variablethat wouldincrease.\nResponseincludes atleast two ideasandconnections\nThe waterwouldpossesshydropower.The\nPage 13/31\nExpertiselevel Biomass Reservoir\nPart A Part B\nDescriptionExample DescriptionExample Description Example\nconnecting todemand orphenomenon.\nbetween twoof thevertices ofFood, Energyor Water inresponse tothephenomenon.\nSince thecorn wouldneed morewater to begrown thanbeans do,waterwoulddecrease.Cornproducesmoreenergy thanbeans, soenergywouldincrease.\nrelated toHydroelectricity(speci\u0000c),Energyproduction(general), Usesof Energy andUses of Water.\nelectricitygenerated bythehydropowercould beused inagriculturalequipmentthat requireselectricity,likesprinklers.\n1 Responseincludesinaccuracythat wateruse willdecrease.\nI expect thearea's waterto not beused up asfast from theshift ofagriculture.\nResponsefocuses on asingle vertex(Food, Energyor Water)relevant tophenomenonand does notinclude aconnectionbetweenvertices.\nIncreasedwater usefor corndecreasewater forbeans = less beansproduced.\nResponseincludes onlyone connectionrelated toHydroelectricity(speci\u0000c),Energyproduction(general), Usesof Energy andUses of Water.\nWaterturbines.They coulduse them topower theirmachines.\n0 Student doesnot answerthe questionor addressthe relevantphenomenon.\nI believe thatthere ispositive infood, energyand waterbecausethose areveryimportant.\nStudent doesnot answerthe questionor addressthe relevantphenomenon.\nGrow morebeans. Student doesnot answer thequestion orprovides noconnections inresponse.\nI am not surewhat type ofenergy thewater in thereservoirpossesses.\n4.1.4 Augmented Data\nText augmentation techniques have been employed to balance and expand the data set so that models can be\ndeveloped which exhibit relatively higher accuracy (Cochran, Cohn, Rouet, and Hastings, 2023). The primary\nobjective is to address the insu\u0000ciency and imbalance in data sets and allow smaller data sets, typical in many\neducational contexts (Ferrara & Qunbar, 2022), to emulate larger data volumes so that the augmented data set can\nenhance model training. This technique is frequently applied in educational assessments, particularly in tasks\nrelated to text classi\u0000cation (Cochran, Cohn, Rouet, and Hastings, 2023; Fang, Lee, and Zhai, 2023). In our study, we\nemployed an NLP sentence augmentation package operating at the sentence level to augment textual input based\non contextual word embeddings, resulting in the augmentation of each response \u0000ve times, yielding a total of 6465\naugmented responses. It is important to note that this augmentation technique utilizes BERT’s predetermined\nvocabulary and does not replace specialized domain words. Additionally, determining the optimal frequency for\naugmentation lacks a universally robust guideline, as thresholds may vary across different cases, and our study\nPage 14/31\ndoes not speci\u0000cally address this aspect. However, we posit that employing \u0000ve times augmentation introduces\nvaluable diversity and variability to the dataset. This practice exposes the models to a wider spectrum of linguistic\nvariations and contextual nuances, thereby mitigating over\u0000tting to speci\u0000c patterns present in the original data. In\nour preliminary experiment, we trained models using only the collected CR data. In our results table (see Table 5), we\ninclude this a priori baseline metric of Cohen’s Kappa without utilizing augmented data. This metric serves as a\nreference point for comparison, allowing an assessment of the e\u0000cacy achieved through using the augmented\ndataset. Recognizing the need for improvement, particularly in line with recommendations from the measurement\n\u0000eld (Ramesh and Sanampudi, 2022), we opted to integrate a data augmentation method in this study and report\nresults based on this approach. This strategy, commonly used in the educational \u0000eld (Bonthu, Sree, and Prasad,\n2023), aims to surmount the challenge of data scarcity and enhance machine-based modeling for practical\napplications in the future. Each response was augmented \u0000ve times, with the augmented data used exclusively\nduring the training phase, while the original student CR data remained reserved as a model validation set.\n4.2 Model Architecture\nThe proposed model architecture (see Fig. 1) comprises three main layers: the ontology-based system layer, the\nBERT encoder layer, and the text classi\u0000cation layer. The ontology-based system layer consists of three modules:\ndomain-speci\u0000c word identi\u0000cation, siblings extraction, and word substitution. Given an input sentence S denoted as\na sequence of tokens , the word identi\u0000cation module calculates semantic similarity between\neach token and nodes in the target ontology, identifying domain-speci\u0000c academic vocabularies with high similarity\nscores. In Fig. 1, “photosynthesis” is chosen as a domain-speci\u0000c word due to its high similarity scores with\nontological nodes and its absence in the prede\u0000ned glossary of BERT. Although the word “lake” also has a high\nsemantic similarity score as indicative of a domain-speci\u0000c word, we ignored this since the term was already\nincluded in the BERT glossary and thus did not encounter OOV issues. Then the siblings extraction module produces\nthree selected sibling terms for the identi\u0000ed vocabulary based on inclusion criteria, and the word substitution\nmodule replaces the domain-speci\u0000c term with these siblings, resulting in three modi\u0000ed sentences. If a sentence\ncontains multiple domain-speci\u0000c terms, then in each training epoch, we randomly select one of them to be\nsubstituted by three siblings. In the BERT encoding layer, these modi\u0000ed sentences undergo three iterations of BERT\ntraining, each corresponding to one of the chosen siblings for replacement. The outputs are three  feature\nvectors re\u0000ecting semantic nuances introduced by the selected siblings. These vectors are averaged to enhance\nrepresentation. Finally, the feature vectors are employed in text classi\u0000cation, where the label is students’ expertise\nlevels ranging from 0 to 4.\n4.2.1 Domain-Speci\u0000c Term Identi\u0000cation\nRationale and Prior Work\nThe adept use of domain-speci\u0000c terms ensures that students demonstrate a precise and sophisticated command\nof the subject matter. Beyond linguistic considerations, incorporating domain-speci\u0000c terms in assessments mirrors\na student’s engagement in system thinking of key concepts within the targeted academic domain (Coxhead, 2000;\nNagy and Townsend, 2012). This alignment of assessment tasks with the discipline’s requisite vocabulary ensures\nthat students showcase their ability to apply acquired principles in a contextually relevant manner.\nDrawing on insights gleaned from our prior investigations, which entailed content analysis of eight college-level\nintroductory environmental course syllabi (Horne et al., 2023) and interviews with undergraduate students probing\ntheir knowledge of natural environment and ecological topics (Manzanares et al., under review), we consolidated\n[w1,w2,…,wj]\n1×768\nPage 15/31\nconcepts associated with the FEW Nexus as focal themes in our CR assessments. These CR assessments\nprompted students to articulate their ideas and facilitate the evaluation of their systems thinking and application\nability in practical scenarios centered on the FEW Nexus. Students were tasked with identifying boundaries,\ncomponents, and connections within the FEW Nexus and predicting outcomes resulting from alterations in\ncomponents or connections (Royse et al., in review). This approach demands a comprehensive understanding of\ndiverse FEW concepts, encouraging students to establish connections to their environment. Meanwhile, the use of\nspecialized terms and ideas proves indispensable for effective expression. For example, in our recent observation\n(see Royse et al., under review), we conducted conventional content analysis to discern commonalities and\ndifferences among the machine-misscored responses in terms of words, phrases, and ideas qualitatively. This\nprompted us to recognize the importance of identifying and extracting domain-speci\u0000c vocabulary and ideas from\nstudents’ CR.\nExperimental Design\nOntologies are increasingly employed in academic research studies for their capacity to describe the semantic\nrelationships of domain knowledge in a formal, structured, and machine-processable manner (Patel and Debnath,\n2024). In the context of this study, we used Environment Ontology (EnvO)[2] as the designated ontology to map each\nword (after preprocessing through the removal of defaulted English stopwords) contained in students’ CR data onto\nnodes within the ontological knowledge graph. The conceptual frameworks enshrined within this ontology, align\nwith the CR assessments and rubrics in college-level environmental science, speci\u0000cally focusing on the FEW\nNexus.\nThe mapping procedure was executed using the text2term ontology mapper[3]. This tool plays a pivotal role in our\nstudy by serving two functions: (1) generating a semantic similarity metric (with options) to compare the provided\nstrings with all nodes within the target ontology. The results are then converted into a CSV \u0000le, facilitating the\nidenti\u0000cation of domain-speci\u0000c terms within the dataset. (2) generating knowledge graphs depicting the locations\nof sibling terms of each ontology-based term, which, in our case, pertains to domain-speci\u0000c vocabulary. This\nprocedure helps identifying sibling terms of each recognized domain-speci\u0000c word in the subsequent experimental\nstep of the siblings extraction module.\nIn the experiment, the matching in the \u0000rst function was conducted in Python using the “map_terms” function, where\nthe parameters were set follows: target ontology = “http://purl.obolibrary.org/obo/envo.owl”, mapper = \nMapper.TFIDF, min_score = 0.7, and save_term_graphs = TRUE. The mapping function can be presented as follows:\n,\n(1)\nHere,  represents the mapping of source term  (i.e., words in students’ CR) to best-matched terms ,\nwhere A represents the set of source terms and B represents the set of target terms (i.e., nodes in EnvO). The\nfunction considers only those cases where the similarity score  between the source term and the\ntarget term  exceeds or equals the speci\u0000ed minimum similarity score threshold (min_score). The similarity score\n is computed using algorithms such as Levenshtein distance. In this particular case, the defaulted TF-\nIDF metrics were utilized, as illustrated in Eq. (2). The mapping process aims to \u0000nd the best-matched terms in the\ntarget ontology based on the speci\u0000ed similarity score threshold. The cutoff value (i.e., min_score) of 0.70 was\nf(wi)=M(wi)={wi∈A,ni∈B|Sim(wij,n)≥min_score}\nf(wi) wi M(wi)\nSim(wij,n) wi\nni\nSim(wij,n)\nPage 16/31\ndetermined through experience, with a smaller setting enhancing tolerance but risking misjudgments. After multiple\niterations, we set the threshold at 0.70 for TFIDF cosine similarity calculations, considering values greater than 0.70\nas indicative of words being close to exact matches.\nThe similarity score between a source term  and a target term  is computed using cosine similarity, which is\nexpressed as follows:\n(2)\nwhere  represents the TF-IDF vector of the source term  and  represents the TF-IDF vector\nof the target term . The cosine similarity is calculated by taking the dot product of the TF-IDF vectors and\nnormalizing it by the product of their magnitudes. In simpler terms, the cosine similarity measures the cosine of the\nangle between the TF-IDF vectors of the source and target terms. A result closer to 1 implies greater semantic\nsimilarity between the terms, with 1 indicating that the terms are semantically identical in the given context.\n4.2.2 Siblings Extraction\nRecent synonym discovery methods operate on domain speci\u0000c synonym words as the input unit. They utilize raw\ntext as contextual information for downstream BERT applications such as text classi\u0000cation (e.g., Graichen, 2023;\nZeng, Yao, Zhang, and Xie, 2023). However, these methods are ill-suited in our case and lead to serious OOV\nproblems. Moreover, \u0000nding appropriate synonyms for certain domain terms in our environmental science\nassessment proves di\u0000cult to accomplish. For example, one student's response contains the word Eukaryote as\npart of an explanation about why a restricted number of living systems can adversely affect a lake environment.\nAccording to the Google BERT glossary, Eukaryote is considered OOV, less common, and challenging to predict in a\nmasked language model due to limited educational datasets. The quest for suitable synonyms related to Eukaryote\nfurther complicates matters.\nTo tackle this issue, we propose leveraging siblings extracted from the domain ontology. The Text2term Ontology\nMapper can save knowledge graphs for each mapped term in a machine-processable manner, serving as\nrepresentations of semantic relationships between entities within the domain for analysis. Ontologies enhance\nthese representations by adding semantic meaning to entities and relationships (e.g., is_a) in a knowledge graph,\nenabling more sophisticated reasoning and inference. Consider the term photosynthesis in Fig. 2, a frequent\noccurrence in our students' CR items and deemed important according to the rubric. Terms/phrases like cellular\nprocess, light reaction, and dark reaction could serve as viable substitutes for photosynthesis. It is important to note\nthat we avoid using terms/phrases like metabolic, photosystem, and photosynthetic membrane to represent and\nsubstitute photosynthesis, despite their proximity according to the knowledge graph. This decision is grounded in\ntheir specialized nature and their absence from the Google glossary. Incorporating such terms would introduce\nfurther complexity to our methodology.\nTo sum up, for each domain-speci\u0000c word, we applied a selection criterion to choose three sibling terms based on\n(1) Proximity to domain words: prioritizing sibling terms closely related in the ontology, and (2) Commonality:\nselecting words and/or variants present in the BERT glossary to facilitate subsequent embedding procedures.\n4.2.3 Word Substitutions\nwi ni\nSim(wi,ni)= \u0000tfidf(wi)∙tfidf(ni)\n∥tfidf(wi)∥∥tfidf(ni)∥\ntfidf(wi) wi tfidf(ni)\nni\nPage 17/31\nAfter extracting three suitable siblings for a given domain-speci\u0000c term, these selected sibling terms are employed\nas substitutes for the identi\u0000ed academic term, leading to the creation of three new sentences. Subsequently, these\nmodi\u0000ed sentences are used to train BERT in order to generate three distinct features and each captures the\nsemantic nuances introduced by the selected siblings. This process yields three  feature vectors, which are\nfurther enhanced by an averaging process to improve their representation.\n4.3 Experimental Setup and Hyperparameters\nBERT Pre-Training. We utilized the pre-trained BERT model from the Huggingface library[4] and con\u0000gured our\nsystem to align with the parameter settings speci\u0000ed in the basic version of Google BERT (see Devlin et al., 2018).\nOur model was con\u0000gured with 12 self-attention layers, 12 attention heads, and a hidden dimension of 768 for the\nembedding vectors.\nBERT Fine-Tuning. We concatenated the pre-trained models with the text classi\u0000cation model. During the training\nphase, each CR data was augmented \u0000ve times, and the augmented CR texts were input into BERT to generate a list\nof feature vectors. The mean of these feature vectors was forwarded into the text classi\u0000cation layer to generate\nprediction results based on 4 expertise levels. Finally, the cross entropy loss was computed based on the prediction\nand ground truth. In these experiments, we employed the Adam optimizer with consistent hyperparameters: a\nlearning rate of 1e-04, batch size of 10, input sequence length of 128, 20 training epochs, weight decay of 0.0005,\nand a dropout rate of 0.3. The training process was conducted on a 1080Ti NVIDIA GPU. All experiments involved\ntraining on augmented CR data, and testing and validating on parts of original student CRs with a test size of 0.3.\n4.4 Baselines\nThe CRC tool, which facilitates computerized analysis of students' constructed responses, is publicly accessible for\nevaluation purposes at a group website[5]. This is our \u0000rst baseline model. Our investigation also sought to assess\nthe effectiveness of incorporating an ontology-based system as an extension to the BERT models on overall model\nperformance. To this end, we employed the original BERT as a pretrained model for the text classi\u0000cation\ndownstream task, serving as our baseline model two. We also utilize the SciBERT model[6] (Beltagy, Lo, and Cohan,\n2019), pretrained on an extensive multi-domain corpus of scienti\u0000c publications, as our baseline model three.\n4.5 Evaluation Metrics\nIn this study, we employed Cohen’s Kappa, classi\u0000cation accuracy, and F1 score as key metrics for evaluating and\ncomparing the performance of the auto-scoring models. We obtained and reported the best results for the three\nevaluation metrics across 20 epochs utilizing a tenfold cross-validation method. It is also noted that computations\nof these evaluation metrics were conducted using a validation subset (n = 239), while the remaining dataset (n = \n1000) was employed for training and testing the models.\nCohen's Kappa is a widely recognized metric utilized to assess inter-rater agreement between machine-based\nscoring systems and human expert scorers (Zhai, Shi and Nehm, 2021). This metric quanti\u0000es the degree of\nconcordance between the two while accounting for chance agreements, providing valuable insights into the\nreliability of automated scoring mechanisms. A Cohen’s Kappa value greater than 0.6 indicates an acceptable level\nof agreement.\nClassi\u0000cation accuracy is a fundamental metric in classi\u0000er model evaluation to measure the correctness of\npredictions by dividing the number of accurate predictions by the total number of predictions made. This metric\n1×768\nPage 18/31\nprovides a straightforward assessment of overall accuracy, indicating the proportion of correct predictions relative\nto the total predictions made.\nGiven the presence of class imbalance, we also adopt a macro-averaged-F1 method for multi-class evaluation to\nassess the overall performance of our proposed approach in comparison to others. The macro-average F1 score is\ncomputed independently for each class, and subsequently, the average of all individual F1 scores is calculated. This\nmethodology provides a comprehensive evaluation metric, considering the model's performance across all classes\nand offering a balanced perspective, particularly in scenarios with class imbalances.\n5. RESULT\nThe performance results of all approaches are presented in Table 4. Each model achieved a Cohen’s Kappa value\nexceeding the threshold of 0.60, indicating a moderate to high level of agreement between human and machine\nscores. Incorporating an ontology-based term identi\u0000cation system into pre-trained language models exhibited\nincreased Cohen’s kappa metrics for the both the BERT and SciBERT models, with the highest performance of\nSciBERT + Ontology with a Cohen’s Kappa of 0.780. This result suggests a superior level of performance with\nreduced variations between human and machine scores, and a more stable and reliable performance compared to\nother approaches. Accuracy metrics between original and ontology-extended BERT models generally exhibited\nequality. However, in terms of F1 score, extended BERT models with ontology-based systems outperformed their\noriginal counterparts. These observations validated our proposition that leveraging features that semantically\ncapture contextual information from an ontological perspective could improve model performance.\nTable 5\nResults of Various Models on Text Classi\u0000cation Task\nModels a priori Cohen’s Kappa Accuracy F1 Score\nCRC 0.350 0.656 0.765 0.628\nBERT 0.493 0.735 0.877 0.636\nSciBERT 0.518 0.761 0.881 0.649\nBERT + Ontology 0.533 0.775 0.879 0.660\nSciBERT + Ontology 0.542 0.780 0.881 0.665\nNote. The “a priori” column presents Cohen’s Kappa metrics derived from the non-augmented data corpus, asdiscussed in Section 4.1.3. The reported results for BERT and its variant models within this table showcase theoptimal outcomes observed after 20 epochs. As for the CRC model, only the \u0000nal result following tenfold cross-validation is documented. The bolded entries signify the top performance level across all models.\nIn the absence of supervision from the training set, the unsupervised baseline methods, namely BERT and SciBERT,\ndemonstrated good performance, with SciBERT consistently outperforming BERT across all evaluation metrics in\nour investigation. Conversely, the supervised method utilizing the CRC classi\u0000er displayed suboptimal performance\nin predicting score levels within our dataset across all evaluation metrics. This outcome is not unexpected, given the\nconstrained predictive capabilities inherent in a supervised model for a multi-class classi\u0000cation algorithm lacking\nsu\u0000cient training data for all classes. Given the success of the SciBERT pretraining model on our study dataset\nacross all metrics, particularly for Cohen’s Kappa, which indicates a robust agreement between machine-based\nscores and human-assigned scores, we further assessed the performance difference between BERT, with and\nwithout incorporating ontological terms, and their corresponding SciBERT models.\nPage 19/31\nWe also conducted a series of hypothesis tests originated from our proposed research questions, i.e., a one-sample\ntwo tailed t-test for the differences between paired observations (n = 20) by comparing the Cohen’s Kappa\nperformances after 20 epochs among the four LR models. Six paired t-tests were performed to assess all possible\ncomparisons between the four LR models. Our analysis revealed signi\u0000cant performance differences in four of the\ncomparisons at a signi\u0000cance level of p < 0.05. To answer RQ1, the t-test results yielded t(19) = 1.73 with a p-value\nof 0.048 between BERT and SciBERT. This \u0000nding suggests a modest yet statistically signi\u0000cant difference between\nthe two models. To answer RQ2, even after incorporating domain-speci\u0000c vocabulary from EnvO ontology into BERT\nmodels, the difference between BERT and SciBERT remained signi\u0000cant. Speci\u0000cally, SciBERT + Ontology (mean = \n0.740, SD = 0.005) exhibited a signi\u0000cantly higher performance compared to BERT + Ontology (mean = 0.713, SD = \n0.001) with t(19) = 1.72, p < 0.01. Moreover, Cohen’s Kappa for SciBERT + Ontology was signi\u0000cantly higher than\nSciBERT (mean = 0.732, SD = 0.006) with t(19) = 1.73, p < 0.05, as well as BERT (mean = 0.713, SD = 0.012) with t(19) \n= 1.73, p < 0.01. The other two t-tests (i.e., BERT vs. BERT + Ontology, SciBERT vs. BERT + Ontology) resulted in non-\nsigni\u0000cant differences.\nThe confusion matrices for our validation set (n = 293), including CRC, BERT, SciBERT, BERT + Ontology, and Sci-\nBERT + Ontology, are provided in the Appendix.\nQualitative error analysis. Seven student CR examples were selected for a qualitative comparison of scoring from\ndifferent approaches, as presented in Table 6. To optimize the presentation in terms of space and readability, we\nfocused on two baseline models in this section: the supervised CRC classi\u0000er and an unsupervised model, i.e.,\nSciBERT, which demonstrated improved performance compared to the original BERT model. Correspondingly, the\nSciBERT + Ontology system was chosen for comparison with the aforementioned baselines.\nUpon qualitative examination, our proposed approach incorporating ontology-based information exhibited superior\nperformance, in analyzing student CRs characterized by a greater abundance of specialized terms, regardless of the\nresponse length – be it longer (e.g., CR#2) or shorter (e.g., CR#3, 4, 7). In the case of CR#5, where the student’s\nresponse adopted a more colloquial and communicative tone, both SciBERT and our approach exhibited pro\u0000ciency\nin discerning complex language nuances and linguistic patterns by effectively considering the contextual\ninformation associated with each word. This pro\u0000ciency was derived from pre-training of SciBERT on vast amount\nof text data. Conversely, the CRC inaccurately predicted this speci\u0000c case, possibly attributable to a de\u0000ciency in\ncontextual awareness and sensitivity to language usage. This limitation is inherent in ML models that rely on\nmanually crafted features and exhibit constrained contextual understanding. It is important to avoid oversimplifying\nthe broader landscape of supervised models, as some models may excel in detecting language nuances owing to\ntheir design and training data. This discussion is particularly relevant to our case in science educational assessment\nwhere the data exhibits distinct characteristics and writing styles.\nWhile the SciBERT and SciBERT + Ontology approach showcased notable pro\u0000ciency, it is essential to acknowledge\ntheir limitations. Instances of incorrect predictions were observed, as exempli\u0000ed in CR#1 and CR#6. We attributed\nthese errors to the brevity of the sentences and the presence of less meaningful words, as well as grammatical and\nspelling errors (e.g., “prodouced”), which might introduce noise during the prediction process.\nPage 20/31\nTable 6\nIllustration of Correctly and Incorrectly Scored Responses to Biomass Item Using Various Approaches\nNo. Example student CR LengthActualScore Predicted Scores\nCRC SciBERT SciBERT + Onto\n1 more food, less energy used, less water 7 1 1 3 3\n2 The shift to corn biomass production could decreasethe availability and affordability of beans, which are asource of nutrition and income for the community.Moreover, the shift may require the use of fertilizersand pesticides, which can harm soil quality andaffect the health of the ecosystem. The shift to cornbiomass production could increase the community'senergy independence from natural gas, but it comeswith the cost of lower energy returns. For example,one unit of natural gas requires less energy toproduce than one unit of ethanol biomass, and thiscould lead to higher energy costs for the community.The shift to corn biomass production could increasethe area's water use, as corn requires more waterthan beans. This increase in water use may affectthe availability of water for other purposes, such asdrinking water, irrigation, and recreation. Additionally,the shift could lead to water pollution, as the use offertilizers and pesticides could contaminate watersources and harm aquatic ecosystems.\n165 2 4 3 2\n3 food: beans for corn\nenergy: using energy to grow corn, being able tocreate ethanol\nwater: more water to grow corn than beans\n23 2 1 1 2\n4 Switching to biomass farming would reduce theamount of food being grown and require more watertoo, but ethanol would provide a cleaner energysource that would reduce carbon emissions.\n30 3 2 2 3\n5 If I make this decision, my food supply will be dividedin half. I will have less of a surplus of food, so I needto make sure the amount of food I farm matches themarket demand. Energy systems will bene\u0000tbecause I will amass biofuel from this process. I willbe participating in sustainable energy production.Water systems will suffer because more water willbe needed to execute this decision. I might have toconnect to bigger waterways to sustain this change.\n84 3 2 3 3\n6 Switching corn farming reduced the amount of foodproduced and increased the amount of water usedand energy prodouced.\n19 3 2 1 1\n7 Although we may have more biomass from the cornthere will be less bean production, steering the foodto be made with less healthy ingredients. Also wemay have less water for beans and citizens, but wewill have new biomass energy. the energy may be\n64 4 2 2 4\nNote. The bolded words represent example terms identi\u0000ed by the Text2Term ontology mapper, indicating thatthese are domain-speci\u0000c words found in students' CRs.\nPage 21/31\nNo. Example student CR LengthActualScore Predicted Scores\nCRC SciBERT SciBERT + Onto\ngreat but it takes more water to feed the corn andmore energy than natural gas to produce.\nNote. The bolded words represent example terms identi\u0000ed by the Text2Term ontology mapper, indicating thatthese are domain-speci\u0000c words found in students' CRs.\n6. DISCUSSION\nThis study addresses the challenging task of automated CR evaluation within introductory, college-level IES courses.\nThe primary objective encompasses exploring the integration of ontological perspectives into the automatic scoring\nsystem as part of an approach to alleviate the evaluation burden on instructors. To this end, we extended BERT\nmodel by incorporating an ontology (EnvO) aligned to the assessment domain and tasks to discern domain terms\nand extract their sibling terms. The experiments were conducted on a real-world educational dataset that\ndemonstrated the effectiveness of our proposed approach. In conclusion, our baseline models (CRC, BERT,\nSciBERT) without the inclusion of an ontology-based system, yielded reasonably good Cohen’s Kappa, accuracy, and\nF1 scores, while our combined approach integrating the ontology-based system onto BERT/SciBERT showcased\nfurther performance enhancement of the automated scoring models.\nTo answer RQ1, when comparing supervised machine learning models with unsupervised pre-trained LR models, the\nrelatively lower performance of the CRC classi\u0000er in multi-class classi\u0000cation was anticipated. This \u0000nding aligns\nwith the study by González-Carvajal and Garrido-Merchán (2020), where the BERT model outperformed classical\nNLP strategies relying on feature-based ML models. BERT’s effectiveness in capturing intricate linguistic patterns\nand contextual information (Zhang et al., 2020) aligns well with the nature of our text classi\u0000cation task, which\ninvolves assessing students’ expertise levels based on their pro\u0000ciency to employ correct conceptual ideas,\nestablish connections, and apply them in explanations.\nDespite BERT’s commendable performance in classi\u0000cation, the unique nature of science education, particularly in\nenvironmental science at the post-secondary education level, introduces distinctions in our dataset. Undergraduate\nstudents in many science courses are prone to using some specialized and scienti\u0000c domain-speci\u0000c vocabulary in\nexpressing ideas and constructing responses, as highlighted in earlier investigations (Horne et al., 2023; Jescovitch\net al., 2021; Manzanares et al., in review). Consequently, our students’ responses collected from environmental\nscience courses feature OOV words not encompassed in BERT’s prede\u0000ned glossary. To address this lexical gap,\nour proposed ontology-based system facilitated a theoretically grounded and reliable identi\u0000cation of domain terms.\nAdditionally, the system extracted sibling terms considered as substitutes for the original OOV specialized terms.\nThese substitutes share semantically overlapping information from an ontological perspective and, importantly, can\nbe integrated into subsequent BERT models to generate valid feature vectors. In response to RQ2, this methodology\nhas proven advantages, as evidenced by the enhanced performance of the models compared to their original\ncounterparts (e.g., SciBERT + Ontology to SciBert). The outcomes of the statistical t-test pair comparison further\nsubstantiate that our proposed methods of incorporating features derived from ontological terms signi\u0000cantly\nenhance the model’s e\u0000cacy in classifying student expertise levels. It is noteworthy that SciBERT, being a more\nscienti\u0000cally oriented pre-trained model, outperformed the generic BERT model in our speci\u0000c context of science\neducational assessment. Even so, the SciBERT model still exhibited performance improvements when coupled to\nthe ontology approach.\nPage 22/31\n7. IMPLICATIONS, CONCLUSIONS, FUTURE SCOPES\nIn the contemporary landscape of STEM education, educational scholars and practitioners have long sought\neffective assessment techniques to measure the depth and breadth of students’ acquired knowledge (Klassen,\n2006; Millar, 2010; NRC, 2001). These tools aim to provide immediate feedback and allow for adjustments in\ninstructional focus to enhance students’ ability to understand and apply knowledge. In the realm of science\neducation CR assessment, the proliferation of diverse auto-scoring models and formative feedback systems\nunderscores the signi\u0000cance of standardizing and popularizing AI-based grading systems. While state-of-the-art\nlarge language models continue to develop in the industrial market, we see utility in our proposed ontology\napproach. We present this novel perspective for future research in assessment evaluation in science education, as\nclassi\u0000cation of domain-speci\u0000c reasoning has been a well noted issue for automated scoring systems (Liu et al.,\n2016; Jescovitch et., 2021; Maestrales et al. 2021). Our approach has been tested on real-world datasets fraught\nwith challenges such as noise, imbalance, bias, and data scarcity. By achieving in addressing these challenges\nimplies that, within the ontology, core concepts in a given domain possess distinct rules for interpreting their\nexistence and connections, and these rules align with the concepts underlying students’ open-ended responses.\nAccording to Baker et al.’s (2009) insights, this unique property of ontology can guide pedagogic decisions regarding\nthe selection of assessment targets and understanding students’ grasp of ideas within the domain. Baker and\ncolleagues also accentuate the hierarchical structure in ontology for tailoring assessments. For example, in our\nstudy, sibling terms distant from a given term and those proximate to it demand different cognitive demands for\ncomprehension. Our method refrains from introducing complex vocabulary to subsequent BERT models for feature\ngeneration. However, we envision future assessments that showcase student knowledge by considering ontological\nnodes’ location, complexity, proximity, and other relevant properties.\nOntology philosophically is linked with scienti\u0000c literacy, epistemology, and education. However, to the best of our\nknowledge, there is a dearth of formal studies investigating the application of ontology-based methods in the\nautomated assessment of scienti\u0000c responses. Nevertheless, this gap should not be interpreted as suggesting that\nontology lacks relevance and signi\u0000cance in the education domain. We consider our research as a pathway for\nfuture studies. In the present educational science landscape, where students are tasked integrating extensive\ndomain knowledge with authentic science practices to solve problems and engage in phenomena (Gao, Li, Shen,\nand Sun, 2020; Krajcik and Shin, 2023), the utility of ontology in education can be evident. Our primary objective, as\narticulated in Zhai and colleagues’ synthesis (2020) on auto-scoring models in science education, is not to compete\nwith high-performance model development. Instead, our focus is on enriching human work by providing a nuanced\nunderstanding of students’ reasoning abilities in science and how this understanding can inform science instruction\nfrom an ontological perspective. For instance, teachers may utilize ontological information, such as the knowledge\ngraph (as depicted in Fig. 2) to help understand and evaluate relevant student connections as part of formative\nassessment practice.\nFurther, these \u0000ndings suggest a way to expand the generalizability of classi\u0000cation models in science assessment.\nCurrent applications of text classi\u0000cation models in science education tend to produce classi\u0000cation models that\nare unique to an assessment item or a small set of closely related items (Shiroda, Doherty and Haudek, in press) or\nrely on gathering of a very large training corpus that includes multiple items targeting the same construct (Moharerri\net. al., 2014). Thus, adding an ontological layer which associates domain speci\u0000c terms automatically may be a way\nto apply a single classi\u0000cation model to evaluate a number of assessment items within a given science domain and\nfocused on the same assessment construct, but varying only in item context or surface features. Using such an\napproach would speed model development and allow items that vary only in surface features to be used\nPage 23/31\ninterchangeably during formative assessment. This is necessary as undergraduate explanations of science\nphenomena can be in\u0000uenced by context or variable presented in the item despite depending on the same\nfoundational principles for the answer (Nehm and Ha, 2011; Doherty et al., 2023).\nDeclarations\nCon\u0000ict of Interest\nThe authors have no competing interests to declare that are relevant to the content of this article.\nFunding\nThis material is based upon work supported by the National Science Foundation Grant number 2013359 and Grant\nNo. 2013373.\nAuthor Contribution\nConceptualization: HW, KCH; Material preparation and data collection: ADM, CLR, EAR; Formal analysis,\ninterpretation, and writing – original draft preparation: HW; Writing – review and editing: KCH, ADM, CLR, EAR; Fund-\ning Acquisition: CLR, KCH. All authors read and approved the \u0000nal manuscript.\nAcknowledgements\nWe appreciate the contributions of the other members of our NGCI project team.\nData and Code Availability\nThe datasets used and/or analyzed during the current study are available from the corresponding author on\nreasonable request. Code available at https://github.com/IvyWang845/Ontology-Use-For-Textual-Analysis.\nReferences\n1. Arp, R., Smith, B., & Spear, A. D. (2015). Building ontologies with basic formal ontology. MIT Press.\n2. Asim, M. N., Wasim, M., Khan, M. U. G., Mahmood, N., & Mahmood, W. (2019). The use of ontology in retrieval: a\nstudy on textual, multilingual, and multimedia retrieval. IEEE Access, 7, 21662-21686. doi:\n10.1109/ACCESS.2019.2897849.\n3. Baker, E., Chung, G., & Herman, J. (2009). Ontology-based educational design: Seeing is believing. Los Angeles,\nCA: CRESST.\n4. Baird, A., & Schuller, B. (2020). Considerations for a more ethical approach to data in ai: on data representation\nand infrastructure. Frontiers in big Data, 3, 25. https://doi.org/10.3389/fdata.2020.00025\n5. Barkaoui, K. (2007). Rating scale impact on EFL essay marking: A mixed-method study. Assessing writing,\n12(2), 86-107. https://doi.org/10.1016/j.asw.2007.07.001\n\u0000. Bauer, H. H. (1992). Scienti\u0000c literacy and the myth of the scienti\u0000c method. University of Illinois Press.\nPage 24/31\n7. Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scienti\u0000c text. arXiv preprint\narXiv:1903.10676. https://doi.org/10.48550/arXiv.1903.10676\n\u0000. Bonthu, S., Sree, S. R., & Prasad, M. K. (2023). Improving the performance of automatic short answer grading\nusing transfer learning and augmentation. Engineering Applications of Arti\u0000cial Intelligence, 123, 106292.\nhttps://doi.org/10.1016/j.engappai.2023.106292\n9. Carnegie Foundation for the Advancement of Teaching. (2001). The Carnegie classi\u0000cation of institutions of\nhigher education, 2010 edition. The Carnegie Classi\u0000cation of Institutions of Higher Education. Retrieved 11th\nJanuary 2023 from http://carnegieclassi\u0000cations.iu.edu/2010/\n10. Chen, Y., Perozzi, B., Al-Rfou, R., & Skiena, S. (2013). The expressive power of word embeddings. arXiv preprint\narXiv:1301.3226. https://doi.org/10.48550/arXiv.1301.3226\n11. Chiu, M. H., & Krajcik, J. (2020). Re\u0000ections on Integrated Approaches to STEM Education: An International\nPerspective. Integrated Approaches to STEM Education: An International Perspective, 543-559.\nhttps://doi.org/10.1007/978-3-030-52229-2_29\n12. Clarke, V., Braun, V., & Hay\u0000eld, N. (2015). Thematic analysis. Qualitative psychology: A practical guide to\nresearch methods, 3, 222-248.\n13. Cochran, K., Cohn, C., Hastings, P., Tomuro, N., & Hughes, S. (2023). Using BERT to Identify Causal Structure in\nStudents’ Scienti\u0000c Explanations. International Journal of Arti\u0000cial Intelligence in Education, 1-39.\nhttps://doi.org/10.1007/s40593-023-00373-y\n14. Cochran, K., Cohn, C., Hutchins, N., Biswas, G., & Hastings, P. (2022, July). Improving automated evaluation of\nformative assessments with text data augmentation. In International Conference on Arti\u0000cial Intelligence in\nEducation (pp. 390-401). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-11644-\n5_32\n15. Cochran, K., Cohn, C., Rouet, J. F., & Hastings, P. (2023, June). Improving Automated Evaluation of Student Text\nResponses Using GPT-3.5 for Text Data Augmentation. In International Conference on Arti\u0000cial Intelligence in\nEducation (pp. 217-228). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-36272-9_18\n1\u0000. Common Core State Standards Initiative (June, 2010). Common Core State Standards for English language arts\n& literacy in history/social studies, science, and technical subjects. Retrieved December 29th from:\nhttps://corestandards.org/wp-content/uploads/2023/09/ELA_Standards1.pdf\n17. Coxhead, A. (2000). A new academic word list. TESOL quarterly, 34(2), 213-238.\nhttps://doi.org/10.2307/3587951\n1\u0000. Crossley, S. A., Allen, L. K., Snow, E. L., & McNamara, D. S. (2016). Incorporating learning characteristics into\nautomatic essay scoring models: What individual differences and linguistic features tell us about writing quality.\nJournal of Educational Data Mining, 8(2), 1-19.\n19. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805. https://doi.org/10.48550/arXiv.1810.04805\n20. Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H., & Smith, N. (2020). Fine-tuning pretrained language\nmodels: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305.\nhttps://doi.org/10.48550/arXiv.2002.06305\n21. Doherty, J. H., Cerchiara, J. A., Scott, E. E., Jescovitch, L. N., McFarland, J. L., Haudek, K. C., & Wenderoth, M. P.\n(2023). Oaks to arteries: the Physiology Core Concept of \u0000ow down gradients supports transfer of student\nreasoning. Advances in Physiology Education, 47(2), 282-295. https://doi.org/10.1152/advan.00155.2022\nPage 25/31\n22. English, L. D. (2016). STEM education K-12: Perspectives on integration. International Journal of STEM\neducation, 3, 1-8. https://doi.org/10.1186/s40594-016-0036-1\n23. Falloon, G., Hatzigianni, M., Bower, M., Forbes, A., & Stevenson, M. (2020). Understanding K-12 STEM education:\nA framework for developing STEM literacy. Journal of Science Education and Technology, 29, 369-385.\nhttps://doi.org/10.1007/s10956-020-09823-x\n24. Fang, L., Lee, G. G., & Zhai, X. (2023). Using gpt-4 to augment unbalanced data for automatic scoring. arXiv\npreprint arXiv:2310.18365. https://doi.org/10.48550/arXiv.2310.18365\n25. Ferrara, S., & Qunbar, S. (2022). Validity Arguments for AI-Based Automated Scores: Essay Scoring as an\nIllustration. Journal of Educational Measurement, 59(3), 288–313. https://doi.org/10.1111/jedm.12333\n2\u0000. Gao, X., Li, P., Shen, J., & Sun, H. (2020). Reviewing assessment of student learning in interdisciplinary STEM\neducation. International Journal of STEM Education, 7(1), 1-14. https://doi.org/10.1186/s40594-020-00225-4\n27. Goldman, S. R., Britt, M. A., Brown, W., Cribb, G., George, M., Greenleaf, C., Lee, C. D., Shanahan, C., & READI, P.\n(2016). Disciplinary literacies and learning to read for understanding: A conceptual framework for disciplinary\nliteracy. Educational Psychologist, 51(2), 219-246. https://doi.org/10.1080/00461520.2016.1168741\n2\u0000. González-Carvajal, S., & Garrido-Merchán, E. C. (2020). Comparing BERT against traditional machine learning\ntext classi\u0000cation. arXiv preprint arXiv:2005.13012. https://doi.org/10.48550/arXiv.2005.13012\n29. Graichen, E. (2023). Context-aware Swedish Lexical Simpli\u0000cation: Using pre-trained language models to\npropose contextually \u0000tting synonyms (Dissertation). Retrieved December 29th 2023 from\nhttps://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-194982\n30. Gruber, T. R. (1995). Toward principles for the design of ontologies used for knowledge sharing?. International\njournal of human-computer studies, 43(5-6), 907-928. https://doi.org/10.1006/ijhc.1995.108\n31. Hmelo-Silver, C. E., Marathe, S., & Liu, L. (2007). Fish swim, rocks sit, and lungs breathe: Expert-novice\nunderstanding of complex systems. The Journal of the Learning Sciences, 16(3), 307-331.\nhttps://doi.org/10.1080/10508400701413401\n32. Hofmann, V., Pierrehumbert, J. B., & Schütze, H. (2020). Dynamic contextualized word embeddings. arXiv\npreprint arXiv:2010.12684. https://doi.org/10.48550/arXiv.2010.12684\n33. Horne, L., Manzanares, A., Babin, N., Royse, E., Arawaka, L., Blavascunas, E., Doner, L., Druckenbrod, D., Fairchild,\nE., Jarchow, M., Muchnick, B., Panday, P., Perry, D., Thomas, B., Toomey, A., Tucker, B., Washington-Ottombre, C.,\nVincent, S., Anderson, S., & Romulo, C. (2023). Alignment among environmental programs in higher education:\nWhat Food-Energy-Water Nexus concepts are covered in introductory courses? Journal of Geoscience\nEducation, 1-18. DOI: 10.1080/10899995.2023.2187680\n34. Jang, H. (2016). Identifying 21st century STEM competencies using workplace data. Journal of science\neducation and technology, 25, 284-301. https://doi.org/10.1007/s10956-015-9593-1\n35. Jescovitch, L. N., Scott, E. E., Cerchiara, J. A., Doherty, J. H., Wenderoth, M. P., Merrill, J. E., ... & Haudek, K. C.\n(2019). Deconstruction of holistic rubrics into analytic rubrics for large-scale assessments of students’\nreasoning of complex science concepts. Practical Assessment, Research, and Evaluation, 24(1), 7.\nhttps://doi.org/10.7275/9h7f-mp76\n3\u0000. Jescovitch, L. N., Scott, E. E., Cerchiara, J. A., Merrill, J., Urban-Lurain, M., Doherty, J. H., & Haudek, K. C. (2021).\nComparison of machine learning performance using analytic and holistic coding approaches across\nconstructed response assessments aligned to a science learning progression. Journal of Science Education\nand Technology, 30(2), 150-167. https://doi.org/10.1007/s10956-020-09858-0\nPage 26/31\n37. Jönsson, A., & Svingby, G. (2007). The use of scoring rubrics: Reliability, validity and educational consequences.\nEducational research review, 2(2), 130-144. https://doi.org/10.1016/j.edurev.2007.05.002\n3\u0000. Jung, J. Y., Tyack, L., & von Davier, M. (2022). Automated scoring of constructed-response items using arti\u0000cial\nneural networks in international large-scale assessment. Psychological Test and Assessment Modeling, 64(4),\n471-494.\n39. Jurka, T. P., Collingwood, L., Boydstun, A. E., & Grossman, E. (2013). RTextTools: A Supervised Learning Package\nfor Text Classi ﬁ cation. The R journal, 5(1), 6-12. doi: 10.32614/rj-2013-001\n40. Kaldaras, L., & Haudek, K. C. (2022). Validation of automated scoring for learning progression-aligned Next\nGeneration Science Standards performance assessments. In Frontiers in Education (Vol. 7, p. 968289).\nFrontiers Media SA. https://doi.org/10.3389/feduc.2022.968289\n41. Kaldaras, L., Yoshida, N. R., & Haudek, K. C. (2022). Rubric development for AI-enabled scoring of three-\ndimensional constructed-response assessment aligned to NGSS learning progression. In Frontiers in Education\n(Vol. 7, p. 983055). Frontiers. https://doi.org/10.3389/feduc.2022.983055\n42. Kelley, T. R., & Knowles, J. G. (2016). A conceptual framework for integrated STEM education. International\nJournal of STEM education, 3, 1-11. https://doi.org/10.1186/s40594-016-0046-z\n43. Klassen, S. (2006). Contextual assessment in science education: Background, issues, and policy. Science\nEducation, 90(5), 820-851. https://doi.org/10.1002/sce.20150\n44. Krajcik, J. S. (2021). Commentary—applying machine learning in science assessment: opportunity and\nchallenges. Journal of Science Education and Technology, 30(2), 313-318. https://doi.org/10.1007/s10956-021-\n09902-7\n45. Krajcik, J., & Shin, N. (2023). Student Conceptions, Conceptual Change, and Learning Progressions. Handbook\nof Research on Science Education: Volume III.\n4\u0000. Kumar, A., Starly, B., & Lynch, C. ManuBERT: A Pretrained Manufacturing Science Language Representation\nModel. Available at SSRN 4375613: http://dx.doi.org/10.2139/ssrn.4375613\n47. Lee, Y., Capraro, M. M., & Viruru, R. (2018). The factors motivating students’ STEM career aspirations: Personal\nand societal contexts. International Journal of Innovation in Science and Mathematics Education, 26(5).\n4\u0000. Lee, C., Cho, K., & Kang, W. (2019). Mixout: Effective regularization to \u0000netune large-scale pretrained language\nmodels. arXiv preprint arXiv:1909.11299. https://doi.org/10.48550/arXiv.1909.11299\n49. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., & Kang, J. (2020). BioBERT: a pre-trained biomedical\nlanguage representation model for biomedical text mining. Bioinformatics, 36(4), 1234-1240.\nhttps://doi.org/10.1093/bioinformatics/btz682\n50. Lehrer, R., & Schauble, L. (2006). Scienti\u0000c thinking and science literacy. Handbook of child psychology, 4, 153-\n196.\n51. Libarkin, J. C., & Kurdziel, J. P. (2006). Ontology and the teaching of earth system science. Journal of\nGeoscience Education, 54(3), 408-413. https://doi.org/10.5408/1089-9995-54.3.408\n52. Liu, Z., He, X., Liu, L., Liu, T., & Zhai, X. (2023). Context matters: A strategy to pre-train language model for\nscience education. arXiv preprint arXiv:2301.12031. https://doi.org/10.48550/arXiv.2301.12031\n53. Liu, O. L., Rios, J. A., Heilman, M., Gerard, L., & Linn, M. C. (2016). Validation of automated scoring of science\nassessments. Journal of Research in Science Teaching, 53(2), 215-233. https://doi.org/10.1002/tea.21299\n54. Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., & Wang, P. (2020, April). K-bert: Enabling language\nrepresentation with knowledge graph. In Proceedings of the AAAI Conference on Arti\u0000cial Intelligence (Vol. 34,\nNo. 03, pp. 2901-2908). https://doi.org/10.1609/aaai.v34i03.5681\nPage 27/31\n55. Maestrales, S., Zhai, X., Touitou, I., Baker, Q., Schneider, B., & Krajcik, J. (2021). Using machine learning to score\nmulti-dimensional assessments of chemistry and physics. Journal of Science Education and Technology, 30,\n239-254. https://doi.org/10.1007/s10956-020-09895-9\n5\u0000. Manzanares, A. D., Horne, L., Royse, E. A., Azzarello, C. B., Jarchow, M., Druckenbrod, D., Babin, N., Atalan-\nHelicke, N., Vincent, S., Anderson, S. W., & Romulo, C. (in review). Undergraduate students’ knowledge about the\nrelationships between climate change and the Food-Energy-Water Nexus. Journal for STEM Education\nResearch.\n57. Martin, P. P., & Graulich, N. (2023). When a machine detects student reasoning: a review of machine learning-\nbased formative assessment of mechanistic reasoning. Chemistry Education Research and Practice. DOI:\n10.1039/D2RP00287F\n5\u0000. Millar, R. (2010). Analysing practical science activities to assess and improve their effectiveness. Hat\u0000eld:\nAssociation for Science Education.\n59. Moharreri, K., Ha, M., & Nehm, R. H. (2014). EvoGrader: an online formative assessment tool for automatically\nevaluating written evolutionary explanations. Evolution: Education and Outreach, 7, 1-14.\nhttps://doi.org/10.1186/s12052-014-0015-2\n\u00000. Murphy S., Yancey K. B. (2008). Construct and consequence: Validity in writing assessment. In Bazerman C.\n(Ed.), Handbook of research on writing: History, society, school, individual, text (pp. 365-385). Routledge.\n\u00001. Nagy, W., & Townsend, D. (2012). Words as tools: Learning academic vocabulary as language acquisition.\nReading research quarterly, 47(1), 91-108. https://doi.org/10.1002/RRQ.011\n\u00002. National Research Council. (1996). National science education standards. National Academies Press.\n\u00003. National Research Council. (2001). Knowing what students know: The science and design of educational\nassessment. Committee on the Foundations of Assessment. J. Pelligrino, N. Chudowsky, & R. Glaser (Eds.).\nBoard on Testing and Assessment, Center for Education, Division of Behavioral and Social Sciences and\nEducation. Washington, DC: The National Academies Press.\n\u00004. National Research Council (2011). A framework for K–12 science education: Practices, crosscutting concepts,\nand core ideas. Washington, DC: National Academies Press.\n\u00005. National Research Council. (2014). Developing Assessments for the Next Generation Science Standards. The\nNational Academies Press. https://doi.org/10.17226/18409\n\u0000\u0000. Nehm, R. H., & Ha, M. (2011). Item feature effects in evolution assessment. Journal of Research in Science\nTeaching, 48(3), 237-256. https://doi.org/10.1002/tea.20400\n\u00007. Nehm, R. H., Ha, M., Rector, M., Opfer, J. E., Perrin, L., Ridgway, J., & Mollohan, K. (2010). Scoring guide for the\nopen response instrument (ORI) and evolutionary gain and loss test (ACORNS). Technical Report of National\nScience Foundation REESE Project, 0909999.\n\u0000\u0000. NGSS Lead States. (2013). Next Generation Science Standards: For States, By States. Washington, DC: The\nNational Academies Press.\n\u00009. Norris, S. P., & Phillips, L. M. (2003). How literacy in its fundamental sense is central to scienti\u0000c literacy.\nScience Education, 87(2), 224-240. https://doi.org/10.1002/sce.10066\n70. Noyes, K., McKay, R. L., Neumann, M., Haudek, K. C., & Cooper, M. M. (2020). Developing computer resources to\nautomate analysis of students’ explanations of London dispersion forces. Journal of Chemical Education,\n97(11), 3923-3936. https://doi.org/10.1021/acs.jchemed.0c00445\n71. Patel, A., & Debnath, N. C. (2024). A Comprehensive Overview of Ontology: Fundamental and Research\nDirections. Current Materials Science: Formerly: Recent Patents on Materials Science, 17(1), 2-20.\nPage 28/31\nhttps://doi.org/10.2174/2666145415666220914114301\n72. Pellegrino, J. W. (2013). Pro\u0000ciency in science: Assessment challenges and opportunities. Science, 340(6130),\n320-323. DOI: 10.1126/science.1232065\n73. Rahman, H., & Hussain, M. I. (2021). A light-weight dynamic ontology for Internet of Things using machine\nlearning technique. ICT Express, 7(3), 355-360. https://doi.org/10.1016/j.icte.2020.12.002\n74. Ramesh, D., & Sanampudi, S. K. (2022). An automated essay scoring system: a systematic literature review.\nArti\u0000cial Intelligence Review, 55(3), 2495-2527. https://doi.org/10.1007/s10462-021-10068-2\n75. Royse, E., Manzanares, A., Wang H., Haudek, K., Azzarello, C., Horne, L., Druckenbrod, D., Shiroda, M., Adams, S.,\nFairchild, E., Vincent, S., Anderson, S., Romulo, C. (in review). FEW Questions, Many Answers: Using Machine\nLearning Analysis to Assess How Students Connect Food-Energy-Water Concepts. Humanities and Social\nSciences Communications.\n7\u0000. Rudolph, M., & Blei, D. (2018, April). Dynamic embeddings for language evolution. In Proceedings of the 2018\nworld wide web conference (pp. 1003-1011). https://doi.org/10.1145/3178876.3185999\n77. Shiroda, M., Doherty, J. H., Scott, E. E., & Haudek, K. C. (2023). Covariational reasoning and item context affect\nlanguage in undergraduate mass balance written explanations. Advances in Physiology Education, 47(4), 762-\n775. https://doi.org/10.1152/advan.00156.2022\n7\u0000. Shiroda, M., Doherty, J. H., & Haudek, K. C. (in press). Exploring Attributes of Successful Machine Learning\nAssessments for Scoring of Undergraduate Constructed Responses. In Uses of Arti\u0000cial Intelligence in STEM\nEducation (1st ed.). Oxford University Press.\n79. Selva Birunda, S., & Kanniga Devi, R. (2021). A review on word embedding techniques for text classi\u0000cation.\nInnovative Data Communication Technologies and Application: Proceedings of ICIDCA 2020, 267-281.\nhttps://doi.org/10.1007/978-981-15-9651-3_23\n\u00000. Smith, B. (2012). Ontology. In The furniture of the world (pp. 47-68). Brill.\n\u00001. Snow, R. E. (2012). Construct validity and constructed-response tests. In Construction versus choice in\ncognitive measurement (pp. 45-60). Routledge.\n\u00002. Sripathi, K. N., Moscarella, R. A., Steele, M., Yoho, R., You, H., Prevost, L. B., Urban-Lurain, M., Merrill, J., &\nHaudek, K. C. (2023). Machine Learning Mixed Methods Text Analysis: An Illustration From Automated Scoring\nModels of Student Writing in Biology Education. Journal of Mixed Methods Research, 15586898231153946.\nhttps://doi.org/10.1177/15586898231153946\n\u00003. Tomas, C., Whitt, E., Lavelle-Hill, R., & Severn, K. (2019, September). Modeling holistic marks with analytic\nrubrics. In Frontiers in Education (Vol. 4, p. 89). Frontiers Media SA. https://doi.org/10.3389/feduc.2019.00089\n\u00004. Troia, G. A., Shen, M., & Brandon, D. L. (2019). Multidimensional levels of language writing measures in grades\nfour to six. Written Communication, 36(2), 231-266. https://doi.org/10.1177/0741088318819473\n\u00005. Udompong, L., & Wongwanich, S. (2014). Diagnosis of the scienti\u0000c literacy characteristics of primary students.\nProcedia-Social and Behavioral Sciences, 116, 5091-5096. doi: 10.1016/j.sbspro.2014.01.1079\n\u0000\u0000. Underwood, S. M., Posey, L. A., Herrington, D. G., Carmel, J. H., & Cooper, M. M. (2018). Adapting assessment\ntasks to support three-dimensional learning. Journal of Chemical Education, 95(2), 207-217.\nhttps://doi.org/10.1021/acs.jchemed.7b00645\n\u00007. Vincent S, Bunn S, & Sloane S (2013). Interdisciplinary environmental and sustainability education on the\nnation’s campuses: curriculum design. National Council for Science and the Environment, Washington, DC.\nAvailable December 29th, 2023 from: https://gcseglobal.org/sites/default/\u0000les/inline-\n\u0000les/2013%20Curriculum%20Design%20Full%20Report.pdf\nPage 29/31\n\u0000\u0000. Wang, H., & Troia, G. A. (2023). Writing Quality Predictive Modeling: Integrating Register-Related Factors. Written\nCommunication, 40(4), 1070-1112. https://doi.org/10.1177/07410883231185287\n\u00009. Wulff, P., Mientus, L., Nowak, A., & Borowski, A. (2023). Utilizing a pretrained language model (BERT) to classify\npreservice physics teachers’ written re\u0000ections. International Journal of Arti\u0000cial Intelligence in Education,\n33(3), 439-466. https://doi.org/10.1007/s40593-022-00290-6\n90. Yan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., ... & Ga š evi ć , D. (2023). Practical and ethical\nchallenges of large language models in education: A systematic literature review. arXiv preprint\narXiv:2303.13379. https://doi.org/10.1111/bjet.13370\n91. Yore, L. D. (2003). Examining the literacy component of science literacy: 25 years of language arts and science\nresearch. International Journal of Science Education, 25(6), 689-725. https://doi.org/10.1080/09500690305018\n92. Yunianto, I., Permanasari, A. E., & Widyawan, W. (2020, October). Domain-speci\u0000c contextualized embedding: A\nsystematic literature review. In 2020 12th International Conference on Information Technology and Electrical\nEngineering (ICITEE) (pp. 162-167). IEEE. doi: 10.1109/ICITEE49829.2020.9271752.\n93. Zeidler, D. L. (2016). STEM education: A de\u0000cit framework for the twenty \u0000rst century? A sociocultural\nsocioscienti\u0000c response. Cultural Studies of Science Education, 11, 11-26. https://doi.org/10.1007/s11422-014-\n9578-z\n94. Zeng, L., Yao, C., Zhang, M., & Xie, Z. (2022, August). SynBERT: Chinese Synonym Discovery on Privacy-\nConstrain Medical Terms with Pre-trained BERT. In Asia-Paci\u0000c Web (APWeb) and Web-Age Information\nManagement (WAIM) Joint International Conference on Web and Big Data (pp. 331-344). Cham: Springer\nNature Switzerland. https://doi.org/10.1007/978-3-031-25158-0_25\n95. Zhai, X., Haudek, K. C., Stuhlsatz, M. A., & Wilson, C. (2020). Evaluation of construct-irrelevant variance yielded\nby machine and human scoring of a science teacher PCK constructed response assessment. Studies in\nEducational Evaluation, 67, 100916. https://doi.org/10.1016/j.stueduc.2020.100916\n9\u0000. Zhai, X., Haudek, K. C., & Ma, W. (2023). Assessing argumentation using machine learning and cognitive\ndiagnostic modeling. Research in Science Education, 53(2), 405-424. https://doi.org/10.1007/s11165-022-\n10062-w\n97. Zhai, X., He, P., & Krajcik, J. (2022). Applying machine learning to automatically assess scienti\u0000c models.\nJournal of Research in Science Teaching, 59(10), 1765-1794. https://doi.org/10.1002/tea.21773\n9\u0000. Zhai, X., Shi, L., & Nehm, R. H. (2021). A meta-analysis of machine learning-based science assessments: Factors\nimpacting machine-human score agreements. Journal of Science Education and Technology, 30, 361-379.\nhttps://doi.org/10.1007/s10956-020-09875-z\n99. Zhai, X., Yin, Y., Pellegrino, J. W., Haudek, K. C., & Shi, L. (2020). Applying machine learning in science\nassessment: a systematic review. Studies in Science Education, 56(1), 111-151.\nhttps://doi.org/10.1080/03057267.2020.1735757\n100. Zhang, Z., Wu, Y., Zhao, H., Li, Z., Zhang, S., Zhou, X., & Zhou, X. (2020, April). Semantics-aware BERT for\nlanguage understanding. In Proceedings of the AAAI Conference on Arti\u0000cial Intelligence (Vol. 34, No. 05, pp.\n9628-9635). https://doi.org/10.1609/aaai.v34i05.6510\nFootnotes\n1. https://www.dbpedia.org/resources/ontology/\n2. https://sites.google.com/site/environmentontology/\nPage 30/31\n3. https://github.com/ccb-hms/ontology-mapper\n4. https://huggingface.co/docs/transformers/model_doc/bert\n5. https://beyondmultiplechoice.org/\n\u0000. https://huggingface.co/allenai/scibert_scivocab_uncased\nFigures\nFigure 1\nArchitecture Overview of the Proposed Method\nPage 31/31\nFigure 2\nKnowledge Graph of Our Example Domain-Speci\u0000c Word\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nAPPENDIX.docx",
  "topic": "Perspective (graphical)",
  "concepts": [
    {
      "name": "Perspective (graphical)",
      "score": 0.8303446769714355
    },
    {
      "name": "Natural language processing",
      "score": 0.6053418517112732
    },
    {
      "name": "Computer science",
      "score": 0.5956708192825317
    },
    {
      "name": "Artificial intelligence",
      "score": 0.505833625793457
    },
    {
      "name": "Language model",
      "score": 0.49316591024398804
    },
    {
      "name": "Linguistics",
      "score": 0.4303806722164154
    },
    {
      "name": "Psychology",
      "score": 0.38400983810424805
    },
    {
      "name": "Cognitive science",
      "score": 0.358310729265213
    },
    {
      "name": "Philosophy",
      "score": 0.10298222303390503
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87216513",
      "name": "Michigan State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I71966907",
      "name": "University of Northern Colorado",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I346547768",
      "name": "Aims Community College",
      "country": "US"
    }
  ],
  "cited_by": 1
}