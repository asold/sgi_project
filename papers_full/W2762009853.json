{
  "title": "Low-Rank RNN Adaptation for Context-Aware Language Modeling",
  "url": "https://openalex.org/W2762009853",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A1228126434",
      "name": "Aaron Jaech",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A1953639869",
      "name": "Mari Ostendorf",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2020073413",
    "https://openalex.org/W2029867605",
    "https://openalex.org/W2962686221",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2963461183",
    "https://openalex.org/W17500809",
    "https://openalex.org/W2549476280",
    "https://openalex.org/W2275625487",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2399550240",
    "https://openalex.org/W2294684023",
    "https://openalex.org/W2165279024",
    "https://openalex.org/W2472014052",
    "https://openalex.org/W3018009202",
    "https://openalex.org/W2963385194",
    "https://openalex.org/W2251149908",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W2963251942",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2557508245",
    "https://openalex.org/W2335122196",
    "https://openalex.org/W2963940534",
    "https://openalex.org/W1948566616",
    "https://openalex.org/W2591807639",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2593887162",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2798523458",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W1922655562"
  ],
  "abstract": "A context-aware language model uses location, user and/or domain metadata (context) to adapt its predictions. In neural language models, context information is typically represented as an embedding and it is given to the RNN as an additional input, which has been shown to be useful in many applications. We introduce a more powerful mechanism for using context to adapt an RNN by letting the context vector control a low-rank transformation of the recurrent layer weight matrix. Experiments show that allowing a greater fraction of the model parameters to be adjusted has benefits in terms of perplexity and classification for several different types of context.",
  "full_text": "Low-Rank RNN Adaptation for Context-Aware Language Modeling\nAaron Jaech and Mari Ostendorf\nDepartment of Electrical Engineering, University of Washington\n185 Stevens Way, Paul Allen Center AE100R, Seattle, W A\n{ajaech,ostendor}@uw.edu\nAbstract\nA context-aware language model uses loca-\ntion, user and/or domain metadata (context)\nto adapt its predictions. In neural language\nmodels, context information is typically rep-\nresented as an embedding and it is given to the\nRNN as an additional input, which has been\nshown to be useful in many applications. We\nintroduce a more powerful mechanism for us-\ning context to adapt an RNN by letting the\ncontext vector control a low-rank transforma-\ntion of the recurrent layer weight matrix. Ex-\nperiments show that allowing a greater frac-\ntion of the model parameters to be adjusted\nhas beneﬁts in terms of perplexity and classi-\nﬁcation for several different types of context.\n1 Introduction\nIn many language modeling applications, the speech\nor text is associated with some metadata or contex-\ntual information. For example, in speech recogni-\ntion, if a user is speaking to a personal assistant then\nthe system might know the time of day or the iden-\ntity of the task that the user is trying to accomplish.\nIf the user takes a picture of a sign to translate it with\ntheir smart phone, the system would have contextual\ninformation related to the geographic location and\nthe user’s preferred language. The context-aware\nlanguage model targets these types of applications\nwith a model that can adapt its predictions based on\nthe provided contextual information.\nThere has been much work on using context infor-\nmation to adapt language models. Here, we are in-\nterested in contexts described by metadata (vs. word\nhistory or related documents) and in neural network\napproaches due to their ﬂexibility for representing\ndiverse types of contexts. Speciﬁcally, we focus\non recurrent neural networks (RNNs) due to their\nwidespread use.\nThe standard approach to adapt an RNN language\nmodel is to concatenate the context representation\nwith the word embedding at the input to the RNN\n(Mikolov and Zweig, 2012). Optionally, the con-\ntext embedding is also concatenated with the out-\nput from the recurrent layer to adapt the softmax\nlayer. This basic strategy has been adopted for var-\nious types of adaptation such as for LM personal-\nization (Wen et al., 2013; Li et al., 2016), adapting\nto television show genres (Chen et al., 2015), and\nadapting to long range dependencies in a document\n(Ji et al., 2016), etc.\nWe propose a more powerful mechanism for us-\ning a context vector, which we call the FactorCell.\nRather than simply using context as an additional\ninput, it is used to control a factored (low-rank)\ntransformation of the recurrent layer weight matrix.\nThe motivation is that allowing a greater fraction of\nthe model parameters to be adjusted in response to\nthe input context will produce a model that is more\nadaptable and responsive to that context.\nWe evaluate the resulting models in terms of\ncontext-dependent perplexity and context classiﬁca-\ntion accuracy on six tasks reﬂecting different types\nof context variables, comparing to baselines that rep-\nresent the most popular methods for using context\nin neural models. We choose tasks where context\nis speciﬁed by metadata, rather than text samples\nas used in many prior studies. The combination\n497\nTransactions of the Association for Computational Linguistics, vol. 6, pp. 497–510, 2018. Action Editor: Phil Blunsom.\nSubmission batch: 1/2018; Revision batch: 3/2018; Published 7/2018.\nc⃝2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nof experiments on a variety of data sources pro-\nvides strong evidence for the utility of the Factor-\nCell model, but the results show that it can be useful\nto consider more than just perplexity in training a\nlanguage model.\nThe remainder proceeds as follows. In Section 2,\nwe introduce the FactorCell model and show how it\ndiffers mathematically from alternative approaches.\nNext, Section 3 describes the six datasets used to\nprobe the performance of different models. Ex-\nperiments and analyses contrasting perplexity and\nclassiﬁcation results for a variety of context vari-\nables are provided in Section 4, demonstrating con-\nsistent improvements in both criteria for the Fac-\ntorCell model but also conﬁrming that perplexity is\nnot correlated with classiﬁcation performance for all\nmodels. Analyses explore the effectiveness of the\nmodel for characterizing high-dimensional context\nspaces. The model is compared to related work in\nSection 5. Finally, Section 6 summarizes contribu-\ntions and open questions.\n2 Context-Aware RNN\nOur model uses adaptation in both the recurrent\nlayer and in the bias vector of the output layer. In\nthis section we describe how we represent context\nas an embedding and methods for adapting the re-\ncurrent layer and the softmax layer, showing that our\nproposed model is a generalization of prior methods.\nThe novelty of our model is that instead of using\ncontext as an additional input to the model, it uses\nthe context information to transform the weights of\nthe recurrent layer. This is accomplished using a\nlow-rank decomposition in order to control the ex-\ntent of parameter sharing between contexts, which\nis important for handling high-dimensional, sparse\ncontexts.\n2.1 Context representation\nWe assume the availability of contextual in-\nformation (metadata or other side information)\nthat is represented as a set of context variables\nf1:n = f1,f2,...f n, from which we produce a k-\ndimensional representation in the form of an embed-\nding, c∈Rk. Each of the context variables, fi, rep-\nresents some type of information or metadata about\nthe sequence and can be either categorical or numer-\nical. The embeddings can either be learned off-line\nusing a topic model (Mikolov and Zweig, 2012) or\nend-to-end as part of the adapted LM (Tang et al.,\n2016). Here, we use end-to-end learning, where the\ncontext embedding is the output of a feed-forward\nnetwork with a ReLU activation function. The re-\nsulting embedding, c, is used for adapting both the\nrecurrent layer and the output layer of the RNN.\n2.2 Adapting the recurrent layer\nThe basic operation of the recurrent layer is to use a\nmatrix W to transform the concatenation of a word\nembedding, wt ∈Re, with the hidden state from the\nprevious time step, ht−1 ∈Rd, and produce a new\nhidden state, ht, as given by Equation 1:\nht = σ(W1wt + W2ht−1 + b)\n= σ(W[wt,ht−1] +b). (1)\nThe size of W is d×(e+ d). For simplicity, our\nequations assume a simple RNN. Appendix A shows\nhow the equations can be adjusted to work with an\nLSTM.\nThe standard approach to recurrent layer adapta-\ntion is to include (via concatenation) the context em-\nbedding as an additional input to the recurrent layer\n(Mikolov and Zweig, 2012). When the context em-\nbedding is constant across the whole sequence, it is\neasy to show that this concatenation is equivalent to\nusing a context-dependent bias at the recurrent layer:\nht = σ( ˆW[wt,ht−1,c] +b)\n= σ(W[wt,ht−1] +Vc+ b)\n= σ(W[wt,ht−1] +b′),\n(2)\nwhere ˆW = [W V] and b′= Vc+ bis the context-\ndependent bias, formed by adding a linear projection\nof the context embedding. We refer to this adapta-\ntion approach as the ConcatCell model.\nOur proposed model extends the ConcatCell\nby using a context-dependent weight matrix\nW′= W + A, in place of the generic weight ma-\ntrix W. (We refer to W as generic because it is\nshared across all context settings.) The adaptation\nmatrix, A, is generated by taking the product of the\ncontext embedding vector against a set of left and\nright basis tensors to produce a rank r matrix. The\nleft and right adaptation basis tensors are given as\n498\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nZL ∈ Rk×(e+d)×r and ZR ∈ Rr×d×k. The two\nbases tensors together can be thought of as holding\nk different rank r matrices, Aj = ZL,jZR,j, each\nthe size of W. By taking the product between cand\nthe corresponding tensor modes of ZL and ZR (us-\ning ×i to denote the mode-itensor product, i.e., the\nproduct with the i-th dimension of the tensor), the\ncontext determines the weighted combination of the\nkmatrices:\nA= (c×1 ZL)(ZR ×3 c⊺). (3)\nThe number of degrees of freedom ofAis controlled\nby the dimensionkof the context vector and the rank\nrof the kweight matrices. The rank is treated as a\nhyperparameter and controls the extent to which the\nmodel relies on the generic weight matrix W versus\nbehaves in a more context-speciﬁc manner.\nWe call this model the FactorCell because the\nweight matrix has been adapted by adding a factored\ncomponent. The ConcatCell model is a special case\nof the FactorCell where ZL and ZR are set to zero.\nIn summary, the proposed model is given by:\nht = σ(W′[wt,ht−1] +b′)\nW′= W + (c×1 ZL)(ZR ×3 c)\nb′= Vc+ b.\n(4)\nIf the context is known in advance,W′can be pre-\ncomputed, in which case applying the RNN at test\ntime requires no more computation than using an un-\nadapted RNN of the same size. This means that for\na ﬁxed sized recurrent layer, the FactorCell model\ncan have many more parameters than the Concat-\nCell model but hardly any increase in computational\ncost.\n2.3 Adapting the Softmax Bias\nThe last layer of the model predicts the probability\nof the next symbol in the sequence using the output\nfrom the recurrent layer using the softmax function\nto create a normalized probability distribution. The\noutput probabilities are given by\nyt = softmax(ELht + bout), (5)\nwhere E ∈ R|V |×e is the matrix of word embed-\ndings, L ∈Re×d is a linear projection to match the\ndimension of the recurrent layer (when e ̸= d), and\nbout ∈R|V |is the softmax bias vector. We tie the\nword embeddings in the input layer with the ones in\nthe output layer (Press and Wolf, 2017; Inan et al.,\n2017).\nIf sj is the indicator row vector for the jth word\nin the vocabulary then p(wt|w1:t−1) = styt and\nlog p(w1:T ) =∑\nt log swt yt.\nAdapting the softmax bias alters the unigram dis-\ntribution. There are two ways to accomplish this.\nWhen the values that the context can take are cate-\ngorical with low cardinality then context-dependent\nsoftmax bias vectors can be learned directly. This is\nequivalent to replacing cwith a one-hot vector. Oth-\nerwise, a projection of the context embedding, Qc\nwhere Q ∈R|V |×k, can be used to adapt the bias\nvector as in\nyt = softmax(ELht + Qc+ bout). (6)\nThe projection can be thought of as a low-rank ap-\nproximation to using the one-hot context vector.\nBoth strategies are explored, depending on the na-\nture of the original context space.\nAs noted in Section 5, adaptation of the softmax\nbias has been used in other studies. As we show in\nthe experimental work, it is useful for representing\nphenomena where unigram statistics are important.\n3 Data\nOur experiments make use of six datasets: four tar-\ngeting word-level sequences, and two targeting char-\nacter sequences. The character studies are moti-\nvated by the growing interest in character-level mod-\nels in both speech recognition and machine transla-\ntion (Hannun et al., 2014; Chung et al., 2016). By\nusing multiple datasets with different types of con-\ntext, we hope to learn more about what makes a\ndataset amenable to adaptation. The datasets range\nin size from over 100 million words of training data\nto 5 million characters of training data for the small-\nest one. When using a word-based vocabulary, we\npreprocess the data by lowercasing, tokenizing and\nremoving most punctuation. We also truncate sen-\ntences to be shorter than a maximum length of 60\nwords for AGNews and DBPedia and 150 to 200 to-\nkens for the remaining datasets. Summary informa-\ntion is provided in Table 1, including the training,\ndevelopment, and test data sizes in terms of number\n499\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nName Train Dev Test Vocab Docs. Context\nAGNews 4.6M 0.2M 0.3M 54,492 115K 4 Newspaper sections\nDBPedia 28.7M 0.3M 3.6M 84,341 555K 14 Entity categories\nTripAdvisor 127.2M 2.6M 2.6M 88,347 843K 3.5K Hotels/5 Sentiment\nYelp 91.5M 0.7M 7.1M 57,794 645K 5 Sentiment\nEuroTwitter∗ 5.3M 0.8M 1.0M 194 80K 9 Languages\nGeoTwitter∗ 51.7M 2.2M 2.2M 203 604K Latitude & Longitude\nTable 1: Dataset statistics: Dataset size in words (* or characters) of Train, Dev and Test sets, vocabulary size, number\nof training documents, and context variables.\nof tokens, vocabulary size, number of training doc-\numents (i.e. context samples), and the context vari-\nables (f1:n). The largest dataset, TripAdvisor, has\nover 800 thousand hotel review documents, which\nadds up to over 125 million words of training data.\nThe ﬁrst three datasets (AGNews, DBPedia, and\nYelp) have previously been used for text classiﬁca-\ntion (Zhang et al., 2015). These consist of newspa-\nper headlines, encyclopedia entries, and restaurant\nand business reviews, respectively. The context vari-\nables associated with these correspond to the news-\npaper section (world, sports, business, sci & tech)\nfor each headline, the page category on DBPedia\n(out of 14 options such as actor, athlete, building,\netc.), and the star rating on Yelp (from one to ﬁve).\nFor AgNews, DBPedia, and Yelp we use the same\ntest data as in previous work. Our fourth dataset,\nfrom TripAdvisor, was previously used for language\nmodeling and consists of two relevant context vari-\nables: an identiﬁer for the hotel and a sentiment\nscore from one to ﬁve stars (Tang et al., 2016). Some\nof the reviews are written in French or German but\nmost are in English. There are 4,333 different hotels\nbut we group all the ones that do not occur at least 50\ntimes in the training data into a single entity, leaving\nus with around 3,500. These four datasets use word-\nbased vocabularies.\nWe also experiment on two Twitter datasets: Eu-\nroTwitter and GeoTwitter. EuroTwitter consists\nof 80 thousand tweets labeled with one of nine\nlanguages: (English, Spanish, Galician, Catalan,\nBasque, Portuguese, French, German, and Italian).\nThe corpus was created by combining portions of\nmultiple published datasets for language identiﬁca-\ntion including Twitter70 (Jaech et al., 2016), Tweet-\nLID (Zubiaga et al., 2014), and the monolingual\nportion of tweets from a code-switching detection\nworkshop (Molina et al., 2016). The GeoTwit-\nter data contains tweets with latitude and longitude\ninformation from England, Spain, and the United\nStates.1 The latitude and longitude coordinates are\ngiven as numerical inputs. This is different from\nthe other ﬁve datasets that all use categorical con-\ntext variables.\n4 Experiments with Different Contexts\nThe goal of our experiments is to show that the\nFactorCell model can deliver improved performance\nover current approaches for multiple language\nmodel applications and a variety of types of con-\ntexts. Speciﬁcally, results are reported for context-\nconditioned perplexity and generative model text\nclassiﬁcation accuracy, using contexts that capture\na range of phenomena and dimensionalities.\nTest set perplexity is the most widely accepted\nmethod for evaluating language models, both for use\nin recognition/translation applications and genera-\ntion. It has the advantage that it is easy to measure\nand is widely used as a criteria for model ﬁt, but the\nlimitation that it is not directly matched to most tasks\nthat language models are directly used for. Text clas-\nsiﬁcation using the model in a generative classiﬁer is\na simple application of Bayes rule:\nˆω= arg max\nω\np(w1:T |ω)p(ω) (7)\nwhere w1:T is the text sequence, p(ω) is the class\nprior, which we assume to be uniform. Classiﬁca-\ntion accuracy provides additional information about\nthe power of a model, even if it is not being designed\nexplicitly for text classiﬁcation. Further, it allows\nus to be able to directly compare our model perfor-\n1Data was accessed from http://followthehashtag.com.\n500\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nmance against previously published text classiﬁca-\ntion benchmarks.\nNote that the use of classiﬁcation accuracy for\nevaluation here involves counting errors associated\nwith applying the generative model to independent\ntest samples. This differs from the accuracy criterion\nused for evaluating context-sensitive language mod-\nels for text generation based on a separate discrimi-\nnative classiﬁer trained on generated text (Ficler and\nGoldberg, 2017; Hu et al., 2017). We discuss this\nfurther in Section 5.\nThe experiments compare the FactorCell model\n(equations 4 and 6) to two popular alternatives,\nwhich we refer to as ConcatCell (equations 2 and\n6) and SoftmaxBias (equation 6). As noted earlier,\nthe SoftmaxBias method is a simpliﬁcation of the\nConcatCell model, which is in turn a simpliﬁcation\nof the FactorCell model. The SoftmaxBias method\nimpacts only the output layer and thus only uni-\ngram statistics. Since bag-of-word models provide\nstrong baselines in many text classiﬁcation tasks, we\nhypothesize that the SoftmaxBias model will cap-\nture much of the relative improvement over the un-\nadapted model for word-based tasks. However, in\nsmall vocabulary character-based models, the uni-\ngram distribution is unlikely to carry much infor-\nmation about the context, so adapting the recurrent\nlayer should become more important in character-\nlevel models. We expect that performance gains will\nbe greatest for the FactorCell model for sources that\nhave sufﬁcient structure and data to support learning\nthe extra degrees of freedom.\nAnother possible baseline would use models in-\ndependently trained on the subset of data for each\ncontext. This is the “independent component” case\nin (Yogatama et al., 2017). This will fail when a\ncontext variable takes on many values (or continu-\nous values) or when training data is limited, because\nit makes poor use of the training data, as shown in\nthat study. While we do have some datasets where\nthis approach is plausible, we feel that its limitations\nhave been clearly established.\n4.1 Implementation Details\nThe RNN variant that we use is an LSTM with cou-\npled input and forget gates (Melis et al., 2018). The\ndifferent model variants are implemented2 using the\nTensorﬂow library. The model is trained with the\nstandard negative log likelihood loss function, i.e.\nminimizing cross entropy. Dropout is used as a reg-\nularizer in the recurrent connections as described in\nSemeniuta et al. (2016). Training is done using the\nAdam optimizer with a learning rate of 0.001. For\nthe models with word-based vocabularies, a sampled\nsoftmax loss is used with a unigram proposal dis-\ntribution and sampling 150 words at each time-step\n(Jean et al., 2014). The classiﬁcation experiments\nuse a sampled softmax loss with a sample size of\n8,000 words. This is an order of magnitude faster to\ncompute with a minimal effect on accuracy.\nHyperparameter tuning was done based on min-\nimizing perplexity on the development set and us-\ning a random search. Hyperparameters included\nword embedding size e, recurrent state size d, con-\ntext embedding size k, and weight adaptation ma-\ntrix rank r, the number of training steps, recurrent\ndropout probability, and random initialization seed.\nThe selected hyperparameter values are listed in Ta-\nble 2. For any ﬁxed LSTM size, the FactorCell has a\nhigher count of learned parameters compared to the\nConcatCell. However, during evaluation both mod-\nels use approximately the same number of ﬂoating-\npoint operations because W′only needs to be com-\nputed once per sentence. Because of this, we believe\nlimiting the recurrent layer cell size is a fair way to\ncompare between the FactorCell and the ConcatCell.\n4.2 Word-based Models\nPerplexities and classiﬁcation accuracies for the four\nword-based datasets are presented in Table 3. In\neach of the four datasets, the FactorCell model gives\nthe best perplexity. For classiﬁcation accuracy, there\nis a bigger difference between the models, and the\nFactorCell model is the most accurate on three out of\nfour datasets and tied with the SoftmaxBias model\non AgNews. For DBPedia and TripAdvisor, most\nof the improvement in perplexity relative to the un-\nadapted case is achieved by the SoftmaxBias model\nwith smaller relative improvements coming from the\nincreased power of the ConcatCell and FactorCell\nmodels. For Yelp, the perplexity improvements are\nsmall; the FactorCell model is just 1.3% better than\n2Code available at http://github.com/ajaech/calm.\n501\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nAgNews DBPedia EuroTwitter GeoTwitter TripAdvisor Yelp\nWord Embed 150 114-120 35-40 42-50 100 200\nLSTM dim 110 167-180 250 250 200 200\nSteps 4.1-5.5K 7.5-8.0K 6.0-8.0K 6.0-11.1K 8.4-9.9K 7.2-8.8K\nDropout 0.5 1.00 0.95-1.00 0.99-1.00 0.97-1.00 1.00\nCtx. Embed 2 12 3-5 8-24 20-30 2-3\nRank 12 19 2 20 12 9\nTable 2: Selected hyperparameters for each dataset. When a range is listed it means that a different values were\nselected for the FactorCell, ConcatCell, SoftmaxBias or Unadapted models.\nAGNews DBPedia TripAdvisor Yelp\nModel PPL ACC PPL ACC PPL ACC PPL ACC\nUnadapted 96.2 – 44.1 – 51.6 – 67.1 –\nSoftmaxBias 95.1 90.6 40.4 95.5 48.8 51.9 66.9 51.6\nConcatCell 93.8 89.7 39.5 97.8 48.3 56.0 66.8 56.9\nFactorCell 92.3 90.6 37.7 98.2 48.2 58.2 66.2 58.8\nTable 3: Perplexity and classiﬁcation accuracy on the test set for the four word-based datasets.\nthe unadapted model.\nFrom Yogatama et al. (2017), we see that for AG-\nNews, much more so than for other datasets, the un-\nigram statistics capture the discriminating informa-\ntion, and it is the only dataset in that work where a\nnaive Bayes classiﬁer is competitive with the gener-\native LSTM for the full range of training data. The\nfact that the SoftmaxBias model gets the same ac-\ncuracy as the FactorCell model on this task suggests\nthat topic classiﬁcation tasks may beneﬁt less from\nadapting the recurrent layer.\nFor the DBPedia and Yelp datasets, the Factor-\nCell model beats previously reported classiﬁcation\naccuracies for generative models (Yogatama et al.,\n2017). However, it is not competitive with state-of-\nthe-art discriminative models on these tasks with the\nfull training set. With less training data, it probably\nwould be, based on the results in (Yogatama et al.,\n2017).\nThe numbers in Table 3 do not adequately convey\nthe fact that there are hyperparameters whose effect\non perplexity is greater than the sometimes small\nrelative differences between models. Even the seed\nfor the random weight initialization can have a “ma-\njor impact” on the ﬁnal performance of an LSTM\n(Reimers and Gurevych, 2017). We use Figure 1 to\nshow how the three classes of models perform across\na range of hyperparameters. The ﬁgure compares\nperplexity on the x-axis with accuracy on the y-axis\nwith both metrics computed on the development set.\nEach point in this ﬁgure represents a different in-\nstance of the model trained with random hyperpa-\nrameter settings and the best results are in the up-\nper right corner of each plot. The color/shape differ-\nences of the points correspond to the three classes of\nmodels: FactorCell, ConcatCell, and SoftmaxBias.\nFigure 1: Accuracy vs. perplexity for different classes of\nmodels on the four word-based datasets.\n502\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nWithin the same model class but across different\nhyperparameter settings, there is much more varia-\ntion in perplexity than in accuracy. The LSTM cell\nsize is mainly responsible for this; it has a much big-\nger impact on perplexity than on accuracy. It is also\napparent that the models with the lowest perplexity\nare not always the ones with the highest accuracy.\nSee Section 4.4 for further analysis.\nFigure 2 is a visualization of the per-word log\nlikelihood ratio between a model assuming a 5 star\nreview and the same model assuming a 1 star review.\nLikelihoods were computed using an ensemble of\nthree models to reduce variance. The analysis is re-\npeated for each class of model. Words highlighted\nin blue are given a higher likelihood under the 5 star\nassumption.\nUnigrams with strong sentiment such as “lovely”\nand “friendly” are well-represented by all three\nmodels. The reader may not consider the tokens\n“craziness” or “5-8pm” to be strong indicators of a\npositive review but the way they are used in this re-\nview is representative of how they are typically used\nacross the corpus.\nAs expected, the ConcatCell and FactorCell\nmodel capture the sentiment of multi-token phrases.\nAs an example, the unigram “enough” is 3% more\nlikely to occur in a 5 star review than in a 1 star re-\nview. However, “do enough” is 30 times more likely\nto appear in a 5 star review than in a 1 star review.\nIn this example, the FactorCell model does a better\njob of handling the word “enough.”\n4.3 Character-based Models\nNext, we evaluate the EuroTwitter and GeoTwitter\nmodels using both perplexity and a classiﬁcation\ntask. For EuroTwitter, the classiﬁcation task is to\nidentify the language. With GeoTwitter, it is less ob-\nvious what the classiﬁcation task should be because\nthe context values are continuous and not categori-\ncal. We selected six cities and then assigned each\nsentence the label of the closest city in that list while\nstill retaining the exact coordinates of the Tweet.\nThere are two cities from each country: Manchester,\nLondon, Madrid, Barcelona, New York City, and\nLos Angeles. Tweets from locations further than 300\nkm from the nearest city on the list were discarded\nwhen evaluating the classiﬁcation accuracy.\nPerplexities and classiﬁcation accuracies are pre-\nSoftmaxBias\nConcatCell\nFactorCell\nFigure 2: Log likelihood ratio between a model that as-\nsumes a 5 star review and the same model that assumes\na 1 star review. Blue indicates a higher 5 star likelihood\nand red is a higher likelihood for the 1 star condition.\nsented in Table 4. The FactorCell model has the\nlowest perplexity and the highest accuracy for both\ndatasets. Again, the FactorCell model clearly im-\nproves on the ConcatCell as measured by classiﬁ-\ncation accuracy. Consistent with our hypothesis,\nadapting the softmax bias is not effective for these\nsmall vocabulary character-based tasks. The Soft-\nmaxBias model has small perplexity improvements\n(<1%) and low classiﬁcation accuracies.\nEuroTwitter GeoTwitter\nModel PPL ACC PPL ACC\nUnadapted 6.35 – 4.64 –\nSoftmaxBias 6.29 43.0 4.63 29.9\nConcatCell 6.17 91.5 4.54 42.2\nFactorCell 6.07 93.3 4.52 63.5\nTable 4: Perplexity and classiﬁcation accuracies for the\nEuroTwitter and GeoTwitter datasets.\nFigure 3 compares perplexity and classiﬁcation\naccuracy for different hyperparameter settings of the\ncharacter-based models. Again, we see that it is pos-\n503\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nFigure 3: Accuracy vs. Perplexity for different classes of\nmodels on the two character-based datasets.\nsible to trade-off some perplexity for gains in classi-\nﬁcation accuracy. For EuroTwitter, if tuning is done\non accuracy rather than perplexity then the accuracy\nof the best model is as high as 95%.\nSometimes there can be little to no perplexity im-\nprovement between the unadapted model and the\nFactorCell model. This can be explained if the pro-\nvided context variables are mostly redundant given\nthe previous tokens in the sequence. To investigate\nthis further, we trained a logistic regression clas-\nsiﬁer to predict the language using the state from\nthe LSTM at the last time step on the unadapted\nmodel as a feature vector. Using just 30 labeled ex-\namples per class it is possible to get 74.6% accu-\nracy. Furthermore, we ﬁnd that a single dimension\nin the hidden state of the unadapted model is often\nenough to distinguish between different languages\neven though the model was not given any supervi-\nsion signal. This ﬁnding is consistent with previ-\nous work that showed that individual dimensions of\nLSTM hidden states can be strong indicators of con-\ncepts like sentiment (Karpathy et al., 2015; Radford\net al., 2017).\nFigure 4 visualizes the value of the dimension\nof the hidden layer that is the strongest indicator\nof Spanish on three different code-switched tweets.\nCode-switching is not a part of the training data but\nit provides a compelling visualization of the abil-\nity of the unsupervised model to quickly recognize\nthe language. The fact that it is so easy for the\nunadapted model to pick-up on the identity of the\ncontextual variable ﬁts with our explanation for the\nsmall relative gain in perplexity from the adapted\nmodels in these two tasks.\nFigure 4: The value of the dimension of the LSTM hidden\nstate in an unadapted model that is the strongest indicator\nfor Spanish text for three different code-switched tweets.\n4.4 Hyperparameter Analysis\nThe hyperparameter with the strongest effect on per-\nplexity is the size of the LSTM. This was consis-\ntent across all six datasets. The effect on classiﬁ-\ncation accuracy of increasing the LSTM size was\nmixed. Increasing the context embedding size gen-\nerally helped with accuracy on all datasets, but it\nhad a more neutral effect on TripAdvisor and Yelp\nand increased perplexity on the two character-based\ndatasets. For the FactorCell model, increasing the\nrank of the adaptation matrix tended to lead to in-\ncreased classiﬁcation accuracy on all datasets and\nseemed to help with perplexity on AGNews, DBPe-\ndia, and TripAdvisor.\nFigure 5: Comparison of the effect of LSTM parameter\ncount and FactorCell rank hyperparameters on perplexity\nfor DBPedia.\nFigure 5 compares the effect on perplexity of the\nLSTM parameter count and the FactorCell rank hy-\nperparameters. Each point in those plots represents\na separate instance of the model with varied hy-\n504\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nperparameters. In the right subplot of Figure 5,\nwe see that increasing the rank hyperparameter im-\nproves perplexity. This is consistent with our hy-\npothesis that increasing the rank can let the model\nadapt more. The variance is large because differ-\nences in other hyperparameters (such as hidden state\nsize) also have an impact.\nIn the left subplot we compare the performance of\nthe FactorCell with the ConcatCell as the size of the\nword embeddings and recurrent state change. The\nx-axis is the size of the W recurrent weight matrix,\nspeciﬁcally 3(e+ d)d for an LSTM with 3 gates.\nSince the adapted weights can be precomputed, the\ncomputational cost is roughly the same for points\nwith the same x-value. For a ﬁxed-size hidden state,\nthe FactorCell model has a better perplexity than the\nConcatCell.\nSince performance can be improved both by in-\ncreasing the recurrent state dimension and/or by in-\ncreasing rank, we examined the relative beneﬁts of\neach. The perplexity of a FactorCell model with an\nLSTM size of 120K will improve by 5% when the\nrank is increased from 0 to 20. To get the same de-\ncrease in perplexity by changing the size of the hid-\nden state would require 160K parameters, resulting\nin a signiﬁcant computational advantage for the Fac-\ntorCell model.\nUsing a one-hot vector for adapting the softmax\nbias layer in place of the context embedding when\nadapting the softmax bias vector tended to have a\nlarge positive effect on accuracy leaving perplexity\nmostly unchanged. Recall from Section 2.3 that if\nthe number of values that a context variable can take\non is small then we can allow the model to choose\nbetween using the low-dimensional context embed-\nding or a one-hot vector. This option is not avail-\nable for the TripAdvisor and the GeoTwitter datasets\nbecause the dimensionality of their one-hot vectors\nwould be too large. The method of adapting the soft-\nmax bias is the main explanation for why some Con-\ncatCell models performed signiﬁcantly above/below\nthe trendline for DBPedia in Figure 1.\nWe experimented with an additional hyperparam-\neter on the Yelp dataset, namely the inclusion of\nlayer normalization (Ba et al., 2016). (We had ruled-\nout using layer normalization in preliminary work\non the AGNews data before we understood that AG-\nNews is not representative, so only one task was\nexplored here.) Layer normalization signiﬁcantly\nhelped the perplexity on Yelp ( ≈ 2% relative im-\nprovement) and all of the top-performing models on\nthe held-out development data had it enabled.\n4.5 Analysis for Sparse Contexts\nThe TripAdvisor data is an interesting case because\nthe original context space is high dimensional (3500\nhotels ×5 user ratings) and sparse. Since the model\napplies end-to-end learning, we can investigate what\nthe context embeddings learn. In particular, we\nlooked at location (hotels are from 25 cities in the\nUnited States) and class of hotel, neither of which\nare input to the model. All of what it learns about\nthese concepts come from extracting information\nfrom the text of the reviews.\nTo visualize the embedding, we used a 2-\ndimensional PCA projection of the embeddings of\nthe 3500 hotels. We found that the model learns\nto group the hotels based on geographic region;\nthe projected embeddings for the largest cities are\nshown in Figure 6, plotting the 1.5σellipsoid of the\nGaussian distribution of the points. (Actual points\nare not shown to avoid clutter.) Not only are hotels\nfrom the same city grouped together, cities that are\nclose geographically appear close to each other in\nthe embedding space. Cities in the Southwest ap-\npear on the left of the ﬁgure, the West coast is on top\nand the East coast and Midwest is on the right side.\nThis is likely due in part to the impact of the region\non activities that guests may mention, but there also\nappears to be a geographic sampling bias in the hotel\nclass that may impact language use.\nClass is a rating from an independent agency that\nindicates the level of service and amenities that cus-\ntomers can expect to receive at a hotel. Whereas, the\nstar rating is the average score given to each estab-\nlishment by the customers who reviewed it. Hotel\nclass does not determine star rating although they\nare correlated ( r = 0.54). The dataset does not\ncontain a uniform sample of hotel classes from each\ncity. The hotels included from Boston, Chicago, and\nPhilly are almost exclusively high class and the ones\nfrom L.A. and San Diego happen to be low class, so\nthe embedding distributions also reﬂect hotel class:\nlower class hotels towards the top left and higher\nclass hotels towards the bottom right. The visual-\nization for the ConcatCell and SoftmaxBias models\n505\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nFigure 6: Distribution of a PCA projection of hotel em-\nbeddings PCA from the TripAdvisor FactorCell model\nshowing the grouping of the hotels by city.\nare similar.\nAnother way of understanding what the context\nembeddings represent is to compute the softmax bias\nprojection Qc and examine the words that experi-\nence the biggest increase in probability. We show\nthree examples in Table 5. In each case, the top\nwords are strongly related to geography and include\nnames of neighborhoods, local attractions, and other\nhotels in the same city. The top boosted words are\nrelatively unaffected by changing the rating. (Recall\nthat the hotel identiﬁer and the user rating are the\nonly two inputs used to create the context embed-\nding.) This table combined with the other visualiza-\ntions indicates that location effects tend to dominate\nin the output layer, which may explain why the two\nmodels adapting the recurrent network seem to have\na bigger impact on classiﬁcation performance.\n5 Prior Work\nThere have been many studies of neural language\nmodels that can be dynamically adapted based on\ncontext. Methods have been referred to as context-\ndependent models (Mikolov and Zweig, 2012),\ncontext-aware models (Tang et al., 2016), condi-\ntioned models (Ficler and Goldberg, 2017), and con-\ntrollable text generation (Hu et al., 2017). These\nmodels have been used in scoring word sequences\n(such as for speech recognition or machine trans-\nlation), for text classiﬁcation, and for generation.\nIn some work, context corresponds to the previous\nword history. Here, we instead consider known fac-\ntors such as user, location and domain metadata,\nthough the framework could be used with history-\nbased context.\nThe studies that most directly relate to our work\nare neural models that correspond to special cases of\nthe more general FactorCell model, including those\nthat leverage what we call the SoftmaxBias model\n(Dieng et al., 2017; Tang et al., 2016; Yogatama et\nal., 2017; Ficler and Goldberg, 2017) and others that\nuse the ConcatCell approach (Mikolov and Zweig,\n2012; Wen et al., 2013; Chen et al., 2015; Ghosh et\nal., 2016). One study (Ji et al., 2016) compares the\ntwo approaches, which they refer to as ccDCLM and\ncoDCLM. They ﬁnd that both approaches give sim-\nilar perplexities, but their ConcatCell style model\ndoes better at an auxiliary sentence ordering task.\nThis is consistent with our ﬁnding that adapting at\nthe recurrent layer can beneﬁt certain tasks while\nhaving only a minor impact on perplexity. They\ndo not test any models that adapt both the recurrent\nand output layers. Hoang et al. (2016) also consider\nadapting at the hidden layer vs. at the softmax layer,\nbut their architecture does not ﬁt cleanly into the\nframework of the SoftmaxBias model because they\nuse an extra perceptron layer; thus, it is difﬁcult to\ncompare the experimental ﬁndings with ours.\nThe FactorCell model is distinguished by hav-\ning an additive (factored) context-dependent trans-\nformation of the recurrent layer weight matrix. A\nrelated additive context-dependent transformation\nhas been proposed for log-bilinear sequence mod-\nels (Eisenstein et al., 2011; Hutchinson et al., 2015),\nbut these are less powerful than the RNN. A some-\nwhat different use of low-rank factorization has pre-\nviously been used to reduce the parameter count in\nan LSTM LM (Kuchaiev and Ginsburg, 2017), ﬁnd-\ning that the reduced number of parameters leads to\nfaster training.\nThere is a long history of adapting n-gram lan-\nguage models (see DeMori and Federico (1999) or\nBellegarda (2004) for a survey). One recent example\nis Chelba and Shazeer (2015) where a 34% relative\nimprovement in perplexity was obtained when us-\ning geographic features for adaptation. We hypoth-\nesize that the impressive improvement in perplexity\nis possible because the language in their dataset of\nGoogle mobile search queries is particularly sensi-\n506\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nHotel City Class Rating Top Boosted Words\nAmalﬁ Chicago 4.0 5 amalﬁ, chicago, allegro, burnham, sable, michi-\ngan, acme, conrad, talbott, wrigley\nBLVD Hotel Suites Los Angeles 2.5 3 hollywood, kodak, highland, universal, reseda,\ngrifﬁth, grauman’s, beverly, ventura\nFour Points Sheraton Seattle 3.0 1 seattle, pike, watertown, deca, needle, pikes,\npike’s monorail, uw, safeco\nTable 5: The top boosted words in the Softmax bias layer for different context settings in a FactorCell model.\ntive to location. Compared to n-gram based LMs,\nour model has two advantages in the way that it han-\ndles context. First, as we showed in our GeoTwitter\nexperiments, we can adapt to geography using GPS\ncoordinates as input without using predeﬁned geo-\ngraphic regions as in Chelba and Shazeer. Second,\nour model supports the joint effect of multiple con-\ntextual variables. Neural models have an advantage\nover discrete models as the number of context vari-\nables increases.\nMuch of the work on context-adaptive neural lan-\nguage models has focused on incorporating doc-\nument or topic information (Mikolov and Zweig,\n2012; Ji et al., 2016; Ghosh et al., 2016; Dieng\net al., 2017), where context is deﬁned in terms\nof word or n-gram statistics. Our work differs\nfrom these studies in that the context is deﬁned\nby a variety of sources, including discrete and/or\ncontinuous metadata, which is mapped to a con-\ntext vector in end-to-end training. Context-sensitive\nlanguage models for text generation tend to in-\nvolve other forms of context similar to the ob-\njective of our work, including speaker character-\nistics (Luan et al., 2016; Li et al., 2016), dialog act\n(Wen et al., 2015), sentiment and other factors\n(Tang et al., 2016; Hu et al., 2017), and style (Ficler\nand Goldberg, 2017). As noted earlier, some of\nthis work has used discriminative text classiﬁcation\nto evaluate generation. In preliminary experiments\nwith the Yelp data set, we found that the generative\nclassiﬁer accuracy of our model is highly correlated\nwith discriminative classﬁer accuracy ( r≈ 0.95).\nThus, by this measure, we anticipate that the model\nwould be useful for generation applications. Anec-\ndotally, we ﬁnd that the model gives more coherent\ngeneration results for DBPedia data, but further val-\nidation with human ratings is necessary to conﬁrm\nthe beneﬁts on more sources.\n6 Conclusions\nIn summary, this paper has introduced a new model\nfor adapting (or controlling) a language model de-\npending on contextual metadata. The FactorCell\nmodel extends prior work with context-dependent\nRNNs by using the context vector to generate a low-\nrank, factored, additive transformation of the recur-\nrent cell weight matrix. Experiments with six tasks\nshow that the FactorCell model matches or exceeds\nperformance of alternative methods in both perplex-\nity and text classiﬁcation accuracy. Findings hold\nfor a variety of types of context, including high-\ndimensional contexts, and the adaptation of the re-\ncurrent layer is particularly important for character-\nlevel models. For many contexts, the beneﬁt of\nthe FactorCell model comes with essentially no ad-\nditional computational cost at test time, since the\ntransformations can be pre-computed. Analyses of a\ndataset with a high-dimensional sparse context vec-\ntor show that the model learns context similarities to\nfacilitate parameter sharing.\nIn all six tasks that are explored here, all context\nfactors are available for all training and testing sam-\nples. In some scenarios, it may be possible for some\ncontext factors to be missing. A simple solution for\nhandling this is to use the expected value for the\nmissing variable(s), since this is equivalent to using\na weighted combination of the adaptation matrices\nfor the different possible values of the missing vari-\nables.\nIn this work, the experiment scenarios all used\nmetadata to specify context, since this type of con-\ntext can be more sensitive to data sparsity and has\nbeen less studied. In contrast, in many prior studies\nof language model adaptation, context is speciﬁed\nin terms of text samples, such as prior user queries,\nprior sentences in a dialog, other documents related\n507\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nin terms of topic or style, etc. The FactorCell frame-\nwork introduced here is also applicable to this type\nof context, but the best encoding of the text into an\nembedding (e.g. using bag of words, sequence mod-\nels, etc.) is likely to vary with the application. The\nFactorCell can also be used with online learning of\ncontext vectors, e.g. to take advantage of previous\ntext from a particular author (or speaker) (Jaech and\nOstendorf, 2018).\nThe models evaluated here were tuned to mini-\nmize perplexity, as is typical for language model-\ning. In analyses of performance with different hy-\nperparameter settings, we ﬁnd that perplexity is not\nalways positively correlated with accuracy, but the\ncriteria are more often correlated for approaches that\nadapt the recurrent layer. While not surprising, the\nresults raise concerns about using perplexity as the\nsole evaluation metric for context-aware language\nmodels. More work is needed to understand the rel-\native utility of these objectives for language model\ndesign.\nAcknowledgments\nThis research was supported in part by a Google\nFaculty Research Award.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nJerome R. Bellegarda. 2004. Statistical language model\nadaptation: Review and perspectives. Speech Commu-\nnication, 42(1):93–108.\nCiprian Chelba and Noam Shazeer. 2015. Sparse non-\nnegative matrix language modeling for geo-annotated\nquery session data. In Proc. IEEE Workshop on Auto-\nmatic Speech Recognition and Understanding, pages\n8–14.\nXie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,\nMoquan Wan, Mark J. F. Gales, and Philip C. Wood-\nland. 2015. Recurrent neural network language model\nadaptation for multi-genre broadcast speech recogni-\ntion. In Proc. Interspeech, pages 3511–3515.\nJunyoung Chung, Kyunghyun Cho, and Yoshua Bengio.\n2016. A character-level decoder without explicit seg-\nmentation for neural machine translation. Proc. An-\nnual Meeting of the Assoc. for Computational Linguis-\ntics (Proc. ACL), pages 1693–1703.\nRenato DeMori and Marcello Federico. 1999. Language\nmodel adaptation. In Computational Models of Speech\nPattern Processing, pages 280–303. Springer.\nAdji B. Dieng, Chong Wang, Jianfeng Gao, and John\nPaisley. 2017. TopicRNN: A recurrent neural network\nwith long-range semantic dependency. In Proc. Int.\nConf. Learning Representations (Proc. ICLR).\nJacob Eisenstein, Amr Ahmed, and Eric Xing. 2011.\nSparse additive generative models of text. InProc. Int.\nConf. Machine Learning (Proc. ICML).\nJessica Ficler and Yoav Goldberg. 2017. Control-\nling linguistic style aspects in neural language genera-\ntion. In Proc. Conf. on Empirical Methods in Natural\nLanguage Processing (EMNLP) Workshop on Stylistic\nVariation, pages 94–104.\nShalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,\nTom Dean, and Larry Heck. 2016. Contextual LSTM\nmodels for large scale NLP tasks. arXiv preprint\narXiv:1602.06291.\nAwni Hannun, Carl Case, Jared Casper, Bryan Catan-\nzaro, Greg Diamos, Erich Elsen, Ryan Prenger, San-\njeev Satheesh, Shubho Sengupta, Adam Coates, et al.\n2014. Deep speech: Scaling up end-to-end speech\nrecognition. arXiv preprint arXiv:1412.5567.\nCong Duy Vu Hoang, Trevor Cohn, and Gholamreza Haf-\nfari. 2016. Incorporating side information into re-\ncurrent neural network language models. In Proc.\nHuman Language Technology Conf. and Conf. North\nAmerican Chapter Assoc. for Computational Linguis-\ntics (HLT-NAACL), pages 1250–1255.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P. Xing. 2017. Controllable\ntext generation. Proc. ICML.\nBrian Hutchinson, Mari Ostendorf, and Maryam Fazel.\n2015. A sparse plus low-rank exponential language\nmodel for limited resource scenarios. IEEE Trans. Au-\ndio, Speech and Language Processing, 23(3):494–504.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A loss\nframework for language modeling. In Proc. ICLR.\nAaron Jaech and Mari Ostendorf. 2018. Personalized\nlanguage model for query auto-completion. In Proc.\nACL, pages 2–6.\nAaron Jaech, George Mulcaire, Shobhit Hathi, Mari Os-\ntendorf, and Noah A. Smith. 2016. Hierarchical\ncharacter-word models for language identiﬁcation. In\nProc. EMNLP Workshop on Natural Language Pro-\ncessing for Social Media.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and\nYoshua Bengio. 2014. On using very large target\nvocabulary for neural machine translation. In Proc.\nAnnual Meeting of the Association for Computational\nLinguistics and the International Joint Conference on\nNatural Language Processing, pages 1–10.\n508\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2016. Document context lan-\nguage models. In Proc. ICLR.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks. In\nProc. ICLR.\nOleksii Kuchaiev and Boris Ginsburg. 2017. Factoriza-\ntion tricks for LSTM networks. In Proc. ICLR.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A persona-based neural conver-\nsation model. In Proc. ACL, pages 994–1003.\nYi Luan, Yangfeng Ji, and Mari Ostendorf. 2016.\nLSTM based conversation models. arXiv preprint\narXiv:1603.09457.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In Proc. ICLR.\nTomas Mikolov and Geoffrey Zweig. 2012. Context de-\npendent recurrent neural network language model. In\nProc. IEEE Spoken Language Technology Workshop\n(SLT), pages 234–239.\nGiovanni Molina, Fahad AlGhamdi, Mahmoud\nGhoneim, Abdelati Hawwari, Nicolas Rey-Villamizar,\nMona Diab, and Thamar Solorio. 2016. Overview\nfor the second shared task on language identiﬁca-\ntion in code-switched data. In Proc. Workshop on\nComputational Approaches to Code Switching, pages\n40–49.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proc. Eu-\nropean Chapter Assoc. for Computational Linguistics\n(EACL), pages 157–1763.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of LSTM-networks for sequence tagging. In\nProc. EMNLP, pages 338–348.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt\nBarth. 2016. Recurrent dropout without memory loss.\nIn Proc. Int. Conf. Computational Linguistics (COL-\nING), pages 1757–1766.\nJian Tang, Yifan Yang, Sam Carton, Ming Zhang, and\nQiaozhu Mei. 2016. Context-aware natural language\ngeneration with recurrent neural networks. arXiv\npreprint arXiv:1611.09900.\nTsung-Hsien Wen, Aaron Heidel, Hung-yi Lee, Yu Tsao,\nand Lin-Shan Lee. 2013. Recurrent neural network\nbased language model personalization by social net-\nwork crowdsourcing. In Proc. Interspeech, pages\n2703–2707.\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao\nSu, David Vandyke, and Steve Young. 2015. Semanti-\ncally conditioned LSTM-based natural language gen-\neration for spoken dialogue systems. InProc. EMNLP,\npages 1711–1721.\nDani Yogatama, Chris Dyer, Wang Ling, and Phil Blun-\nsom. 2017. Generative and discriminative text classi-\nﬁcation with recurrent neural networks. arXiv preprint\narXiv:1703.01898.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nﬁcation. In Proc. Annu. Conf. Neural Inform. Process.\nSyst. (NIPS), pages 649–657.\nArkaitz Zubiaga, Inaki San Vicente, Pablo Gamallo,\nJos´e Ramom Pichel Campos, I ˜naki Alegr ´ıa Loinaz,\nNora Aranberri, Aitzol Ezeiza, and V ´ıctor Fresno-\nFern´andez. 2014. Overview of TweetLID: Tweet lan-\nguage identiﬁcation at SEPLN 2014. In TweetLID\nWorkshop at the Annual Conference of the Spanish\nSociety for Natural Language Processing (SEPLN),\npages 1–11.\nA LSTM FactorCell Equations\nOnly trivial changes are needed to use the Factor-\nCell method on an LSTM instead of a vanilla RNN.\nHere, we list the equations for an LSTM with cou-\npled input and forget gates, which is what was used\nin our experiments.\nThe weight matrixW from Equation 1 is now size\n3d×(e+ d) and bis dimension 3d, where 3 is the\nnumber of gates. Likewise, ZR from Equation 3 is\nmade to be of sizer×3d×k. The weight matrix W′\nis as deﬁned in Equation 4 and after computing it’s\nproduct with the input [wt,ht−1], the result is split\ninto three vectors of equal size: it, ft, and ot\n[it,ft,ot] =W′[wt,ht−1] +b, (8)\nwhere it, ft and ot are used in the input gate, the\nforget gate, and the output gate, respectively.\nUsing these three vectors we perform the gating\noperations to compute ht using the memory cell mt\nas follows:\nft ←sigmoid(ft + 1.0)\nmt = mt−1 ⊙ft + (1.0 −ft) ⊙tanh(it)\nht = tanh(mt) ⊙sigmoid(ot)\n(9)\nNote that equation 2, which shows that a con-\ntext vector concatenated with input is equivalent to\nan additive bias term, extends to equation 8. In\n509\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025\nother words, in the LSTM version of the ConcatCell\nmodel, the context vector effectively introduces an\nextra bias term for each of the 3 gates.\n510\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00035 by guest on 05 November 2025",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8823033571243286
    },
    {
      "name": "Computer science",
      "score": 0.881962239742279
    },
    {
      "name": "Language model",
      "score": 0.7690460085868835
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7396891117095947
    },
    {
      "name": "Context (archaeology)",
      "score": 0.64165198802948
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5828370451927185
    },
    {
      "name": "Metadata",
      "score": 0.5524128675460815
    },
    {
      "name": "Context model",
      "score": 0.5300920605659485
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5093573331832886
    },
    {
      "name": "Embedding",
      "score": 0.41389307379722595
    },
    {
      "name": "Natural language processing",
      "score": 0.37584516406059265
    },
    {
      "name": "Machine learning",
      "score": 0.3691716194152832
    },
    {
      "name": "Artificial neural network",
      "score": 0.29532039165496826
    },
    {
      "name": "World Wide Web",
      "score": 0.10994088649749756
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 21
}