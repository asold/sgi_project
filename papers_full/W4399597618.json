{
  "title": "Synthetic Query Generation using Large Language Models for Virtual Assistants",
  "url": "https://openalex.org/W4399597618",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5010658042",
      "name": "Sonal Sannigrahi",
      "affiliations": [
        "Instituto Superior TÃ©cnico"
      ]
    },
    {
      "id": "https://openalex.org/A4302456329",
      "name": "Thiago Fraga-Silva",
      "affiliations": [
        "Apple (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A321232085",
      "name": "Youssef Oualil",
      "affiliations": [
        "Apple (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2275890166",
      "name": "Christophe Van Gysel",
      "affiliations": [
        "Apple (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384652592",
    "https://openalex.org/W3160939593",
    "https://openalex.org/W2579131192",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W3196902142",
    "https://openalex.org/W2112006025",
    "https://openalex.org/W4296068409",
    "https://openalex.org/W3031797310",
    "https://openalex.org/W4384641573",
    "https://openalex.org/W4289123115",
    "https://openalex.org/W3102018197",
    "https://openalex.org/W3101067169"
  ],
  "abstract": "Virtual Assistants (VAs) are important Information Retrieval platforms that help users accomplish various tasks through spoken commands. The speech recognition system (speech-to-text) uses query priors, trained solely on text, to distinguish between phonetically confusing alternatives. Hence, the generation of synthetic queries that are similar to existing VA usage can greatly improve upon the VA's abilities -- especially for use-cases that do not (yet) occur in paired audio/text data. In this paper, we provide a preliminary exploration of the use of Large Language Models (LLMs) to generate synthetic queries that are complementary to template-based methods. We investigate whether the methods (a) generate queries that are similar to randomly sampled, representative, and anonymized user queries from a popular VA, and (b) whether the generated queries are specific. We find that LLMs generate more verbose queries, compared to template-based methods, and reference aspects specific to the entity. The generated queries are similar to VA user queries, and are specific enough to retrieve the relevant entity. We conclude that queries generated by LLMs and templates are complementary.",
  "full_text": "Synthetic Query Generation using Large Language Models\nfor Virtual Assistants\nSonal Sannigrahiâˆ—â€ \nsonal.sannigrahi@tecnico.ulisboa.pt\nInstituto Superior TÃ©cnico\nLisbon, Portugal\nThiago Fraga-Silva\ntfragadasilva@apple.com\nApple\nAachen, Germany\nYoussef Oualil\nyoualil@apple.com\nApple\nAachen, Germany\nChristophe Van Gyselâ€ \ncvangysel@apple.com\nApple\nCambridge, MA, USA\nABSTRACT\nVirtual Assistants (VAs) are important Information Retrieval plat-\nforms that help users accomplish various tasks through spoken com-\nmands. The speech recognition system (speech-to-text) uses query\npriors, trained solely on text, to distinguish between phonetically\nconfusing alternatives. Hence, the generation of synthetic queries\nthat are similar to existing VA usage can greatly improve upon the\nVAâ€™s abilitiesâ€”especially for use-cases that do not (yet) occur in\npaired audio/text data. In this paper, we provide a preliminary explo-\nration of the use of Large Language Models (LLMs) to generate syn-\nthetic queries that are complementary to template-based methods.\nWe investigate whether the methods (a) generate queries that are\nsimilar to randomly sampled, representative, and anonymized user\nqueries from a popular VA, and (b) whether the generated queries\nare specific. We find that LLMs generate more verbose queries, com-\npared to template-based methods, and reference aspects specific\nto the entity. The generated queries are similar to VA user queries,\nand are specific enough to retrieve the relevant entity. We conclude\nthat queries generated by LLMs and templates are complementary.\nCCS CONCEPTS\nâ€¢ Information systems â†’Search interfaces; Query log analy-\nsis; â€¢ Computing methodologies â†’Speech recognition.\nKEYWORDS\nvirtual assistants, synthetic query log generation\nACM Reference Format:\nSonal Sannigrahi, Thiago Fraga-Silva, Youssef Oualil, and Christophe Van\nGysel. 2024. Synthetic Query Generation using Large Language Models\nfor Virtual Assistants. In Proceedings of the 47th International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR â€™24),\nJuly 14â€“18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3626772.3661355\nâˆ—Work performed while an intern at Apple.\nâ€ Equal contribution.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s).\nThis is the authorâ€™s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of the\n47th International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR â€™24), July 14â€“18, 2024, Washington, DC, USA , https://doi.org/10.1145/\n3626772.3661355.\n1 INTRODUCTION\nVirtual Assistants (VAs) are important [9] Information Retrieval (IR)\nplatforms that help users accomplish various tasks. Users primarily\ninteract with VAs through voice commands, where users initiate a\nretrieval request by uttering a query.\nThe Automated Speech Recognition (ASR) component of the\nVA system transcribes the spoken user query, which is then sub-\nsequently processed by the retrieval engine. However, the ASR\nsystem is trained on audio/text pairs that are expensive and time-\nconsuming to obtain. During the recognition process, the ASR sys-\ntem employs a query prior trained solely on text to disambiguate\nbetween phonetically-similar recognition candidates. Hence, the\nquery prior is a powerful mechanism to modify the ASR systemâ€™s\nbehavior, and has been shown to be an effective manner to improve\nthe recognition of tail named entities [3, 8, 10, 14, 17].\nIn order to correctly recognize emerging entities [4], the ASR\nsystemâ€™s query prior is estimated using a mixture of usage-based\nand synthetic text data. Synthetic queries are typically generated\nusing a template-based approach [2, 13]. A query template, such\nas â€œplay music by $ARTISTâ€ , representing the generic intent of a\nuser wanting to play music by a specific artist, is instantiated using\na popularity-weighted list of entities. However, template-based\napproaches are stringent, may only represent a limited set of use-\ncases, and are not well-suited to generate synthetic queries for use-\ncases that are specific to particular entities. For example, the query\nâ€œplay Taylor Swiftâ€™s debut performance at the Grammyâ€™sâ€ represents\nthe userâ€™s intent to play the song â€œFifteenâ€ by Taylor Swift which\nwas Swiftâ€™s debut performance at the Grammyâ€™s in 2009. While\ncreating a template based on this query would be possible, it does\nnot generalize across entities: some entities may not have performed\nat the Grammyâ€™s and finding the relevant venue would require\nmanual curation. Hence, synthetic query generation methods that\ncan generate queries tailored to specific entities are necessary.\nRecent advances in Large Language Models (LLM) have shown\nimpressive improvements in language understanding tasks [ 5]\nthrough their emergent capabilities [ 16]. In IR, there have been\nvarious works focusing on the generation of queries using LLMs\n[1, 11, 15].\nIn this paper, we perform a preliminary analysis of the use of\nLLMs to produce query priors in VA ASR. We generate synthetic\nqueries by prompting LLMs using a description of the artist gath-\nered from Wikipedia. Then, we evaluate the generated queries in\narXiv:2406.06729v1  [cs.IR]  10 Jun 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Sonal Sannigrahi, Thiago Fraga-Silva, Youssef Oualil, and Christophe Van Gysel\nFigure 1: Proposed pipeline to generate queries for a VA via an LLM.\nterms of their similarity to randomly sampled, representative, and\nanonymized user queries from a popular VA, in addition to the\nqueriesâ€™ ability to retrieve the entity for which they were gener-\nated. More specifically, the research questions addressed are as\nfollows: (RQ1) Can LLMs generate VA queries that are similar to\nuser queries extracted from VA query logs (i.e., domain match)?\n(RQ2) Are the LLM-generated queries good at retrieving the entity\nfor which they were generated (i.e., specificity)?\nOur contributions are as follows: (1) We propose a prompt for\nLLMs to produce natural queries for the music domain for VAs,\nand perform extensive experiments comparing the LLM-generated\nqueries to queries generated using template-based methods, (2) We\nprovide insights through analysis into the differences between\nqueries generated using the various methods.\n2 METHODOLOGY\nFig. 1 shows an overview of our approach, which consists of the\nfollowing three main components: (1) entity descriptions extracted\nfrom Wikipedia to provide context for synthetic query generation,\n(2) the prompt which incorporates the entity description and for-\nmulates a request to the LLM to generate synthetic data, where we\nalso specify the intent the queries need to represent, and (3) the\nLLM, which takes the prompt as input and subsequently generates\na list of synthetic queries as output.\n2.1 Knowledge Base\nWe build our music artist knowledge base by linking Wikipedia data\nwith artist profiles on a popular streaming service. The paragraphs\nin the Wikipedia articles are used as contexts to generate synthetic\nqueries using LLMs (Â§2.2). The combination of the Wikipedia arti-\ncle, and the artist profile retrieved from the streaming service, are\nused to build a search engine to evaluate the end-to-end perfor-\nmance of the generated queries (Â§4.2). We obtained a catalog of mu-\nsic artist entities by downloading the list of most popular artists on\na streaming service in July 2023 and linking them to their respective\nWikipedia profile using property P2850 (i.e., â€œArtist ID Numberâ€ )\nthrough Wikidataâ€™s SPARQL query service1. We also use theMusic-\nGroup2 metadata object, embedded in the source of each artist page\non music.apple.com, with entries that include artist name, biogra-\nphy, an unique artist ID, as well as discography information. Be-\ntween both the Wikipedia dumps and the artist database, we main-\ntain a direct linking to produce a knowledge base of 14 161 artists.\n2.2 Prompt & LLMs\nOur prompt is depicted in Fig. 2. For each entity in the knowl-\nedge base (Â§2.1), we create prompts by populating the artist name\n1https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service\n2https://schema.org/MusicGroup\n[ ARTIST DESCRIPTION ]\nGenerate [K] queries based on the information\nabove about [ ARTIST NAME ] to play music or learn\nmore about [ ARTIST NAME ].\nHere are some examples : [ EXAMPLES ]\nFigure 2: LLM prompt used during our experiments. [ARTIST\nDESCRIPTION] and [ARTIST NAME] are replaced with an entity descrip-\ntion, and the entity name, resp. [EXAMPLES] are a list of example VA\nqueries for the specific intent. We fix [EXAMPLES] to the following\n\"play, queue, turn on, etc\". [K] is the number of queries.\nand use the lead section (i.e., the introduction) as their descrip-\ntion. For music artists, the lead section typically references no-\ntable audiography, collaborations and life events. The number of\nqueries, ğ¾, is set to 40 (Â§3.1) in this paper. We use the OpenAI API\nto generate queries with four model variants3. More specifically, we\nexperiment with babbage-002, gpt-3.5-turbo, gpt-3.5-turbo-\ninstruct, and gpt-4 (see Â§3.1 for details).\n3 EXPERIMENTAL SETUP\n3.1 Query generation methods under\ncomparison\nWe generate queries for all 14 161 entities from our knowledge\nbase (Â§2.1) using (a) the entity name by itself, (b) a template-based\napproach using the top-ğ¾ (according to prior probability) music\nquery templates released as part of [13] (excluding the templates\nthat consist of only the entity name by itself in order to differen-\ntiate from approach (a)), and (c) four different LLMs available via\nOpenAIâ€™s API using the prompt in Fig. 2 (Â§2.2) where we ask the\nLLM to generate ğ¾ queries: babbage-002, gpt-3.5-turbo (v0613),\ngpt-3.5-turbo-instruct, and gpt-4; with ğ¾ = 40. During our\nexperiments, we report evaluation measures at various values of\nğ¾ â‰¤40, in which case we extract the first ğ¾ queries from the list of\n40 queries (rather than issuing multiple requests to the LLM with\nvarying ğ¾). Generated queries that start with a VA wakeword (e.g.,\nâ€œhey V Aâ€where V Arefers to the name of the assistant), have the pre-\nfix corresponding to the wakeword removed. For example, a query\nâ€œhey VA play Moderatâ€ is normalized to â€œplay Moderatâ€ . This step\naims at avoiding biases towards methods that frequently generate\nthe wakeword during domain match evaluation (Â§4.1).\n3.2 Evaluation measures\nTo answerRQ1, we measure the likelihood of the generated queries\nunder a 4-gram back-off language model [6] estimated on randomly\nsampled and anonymized user queries over a span of 2 years from\na popular VA. We apply Good Turing smoothing, and N-grams\n3https://platform.openai.com/docs/models/overview\nSynthetic Query Generation using Large Language Models for Virtual Assistants SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nentity name templates babbage-002 gpt-3.5-turbo gpt-3.5-turbo-instruct gpt-4\n# entities 14161 14161 13848 14161 14156 14161\n# unique queries per entity 1.00 Â±0.00 39 .51 Â±0.50 9 .64 Â±14.74 41 .60 Â±3.61 40 .09 Â±2.27 39 .99 Â±0.13\nquery length per entity 1.58 Â±0.80 3 .78 Â±1.24 52 .30 Â±122.05 8 .11 Â±2.66 8 .42 Â±4.03 8 .31 Â±2.47\n% of queries with > 15 terms 0.01% 0.01% 40.85% 1.17% 1.44% 1.16%\nTable 1: Statistics of generated queries across the approaches under consideration (Â§3.1).\nthat occur infrequently (less than 3 times) in the data are filtered\nout. The negative log-likelihood (NLL) of a single query ğ‘ with\n|ğ‘|terms is defined as NLL (ğ‘)= âˆ’\n\u0010Ã|ğ‘|\nğ‘–=1 log ğ‘ƒ(ğ‘ğ‘– |ğ‘1 ...ğ‘ ğ‘–âˆ’1)\n\u0011\n,\nwhere ğ‘ƒ(ğ‘ğ‘– |ğ‘1 ...ğ‘ ğ‘–âˆ’1)represents the probability of the term ğ‘ğ‘–\nunder the 4-gram LM. Using a 4-gram LM, rather than looking for\nexact matches in query logs, provides us with a flexible approach to\nscore the likelihood of a query, while also having the ability to assign\na score to queries not present in the query logs. The lower the NLL,\nthe more a query is likely under VA user query behavior. We report\nmedian NLL across a query set of the first ğ¾ queries for each entity.\nFor RQ2, we measure to what capacity the generated queries\ncan retrieve the entity for which they were generated in order to\nmeasure query specificity. We build an index of our knowledge base\n14 161 entities where each entity is represented by its Wikipedia\npage and its profile on a music streaming service (Â§2.1), including\nbiography and most popular discography. Both indexed documents\nand queries are pre-processed by lower-casing, removing punctua-\ntion and non-alphanumeric characters, removing stopwords, and\napplying a Porter stemmer. We use the BM25-L retrieval model [12,\nÂ§3.2] with ğ‘˜1 = 1.5, ğ‘ = 0.75 and ğ›¿ = 0.5. Since for each query, there\nis only one relevant entity, we report reciprocal rank (RR), aver-\naged over the top-ğ¾ queries ğ‘and entities ğ‘’, with RR defined as\nRR (ğ‘,ğ‘’)= 1\nrank (ğ‘,ğ‘’)\nwhere rank (ğ‘,ğ‘’)equals the rank of the entity ğ‘’ for which query ğ‘\nwas generated under the BM25-L retrieval model. The higher the\nRR, the better a query is able to retrieve the entity it was generated\nfor (and, hence, the more specific an entity is). We report mean RR\nacross a query set for the first ğ¾ queries generated for each entity.\n4 RESULTS\nTable 1 shows statistics on the generated queries using the vari-\nous methods (Â§3.1). In Fig. 3a, we see that while the standalone\nentity-name and template-based methods generate relatively short\nqueries (1â€“4 terms), the LLM-based methods tend to be more ver-\nbose (âˆ¼8 terms). A sample of generated queries for entity Post Mal-\none (Q21621919) is depicted in Table 2. The babbage-002 LLM, a\nGPT base model not trained with instruction following [ 7], per-\nforms poorly and fails to generate reasonable queries. As expected,\nthe template-based approach generates queries that are stylistically\nsimple, since the template is independent from the entity for which\nthe queries are being generated. On the other hand, queries gener-\nated by LLM-based methods are able to refer to information present\nin the artist description that was fed as context to the LLM. We will\nnow answer the research questions raised in Â§1 and further defined\nin Â§3.2.\nMethod Sample of generated queries\nentity name â€œPost Maloneâ€\ntemplates\nâ€œplay Post Maloneâ€ , â€œplay the song Post Maloneâ€ ,\nâ€œplay Post Malone musicâ€\nbabbage-002 â€œStart with CTRL + Mâ€ , ...\ngpt-3.5\nâ€œplay White Iverson by Post Maloneâ€ ,\nâ€œqueue Congratulations by Post Maloneâ€ ,\nâ€œturn on Post Maloneâ€™s album Beerbongs & Bentleysâ€\ngpt-3.5\n(instruct)\nâ€œplay Post Maloneâ€™s debut single White Iversonâ€ ,\nâ€œplay Post Maloneâ€™s hit song Rockstarâ€ ,\nâ€œplay Post Maloneâ€™s song Sunflower from the\nSpider-Man Into the Spider-Verse soundtrackâ€\ngpt-4\nâ€œplay White Iverson by Post Maloneâ€ ,\nâ€œadd Rockstar by Post Malone to my playlistâ€ ,\nâ€œturn up the volume for Psycho by Post Maloneâ€\nTable 2: Example of queries generated by the various methods (Â§3.1).\n4.1 Similarity to VA usage queries\nFor RQ1, Fig. 3b shows the negative log likelihood (Â§3.1) for the\nmethods under consideration. The entity name by itself aligns\nclosest with user behavior, while the template-based approach is\na close second. This is not surprising, since the templates we used\nwere created by domain experts by analyzing high-frequency use-\ncases in a representative sample of VA usage [13, Â§3.1]. Hence, the\nentity name and template method represent frequent use-cases\nat the head of the query distribution.\nNext up, at approximately half the log-likelihood, queries gener-\nated by the LLMs seem to represent infrequent, tail use-cases. While\nnot entirely absent from VA usage, they are not as common as the\nstraight-forward templates. This is explained by the fact that the\nLLM-generated queries often reference specific songs or albums by\nthe artistâ€”extracted from the artistâ€™s descriptionâ€”resulting in less\ngeneric queries. However, this lack of generality yields queries that\nreference multiple entities and, hence, tend to be at most asâ€”and\noften, significantly lessâ€”likely as queries referencing only a single\nentity. Note that in our prompt (Fig. 2), we did not instruct the LLMs\nto exhibit this behavior. We answerRQ1 as follows: queries gener-\nated by LLMs trained with instruction following correlate with VA\nuser behavior, although they tend to be more specific than queries\ngenerated using template-based approaches. This raises the ques-\ntion whether template- and LLM-based approaches are complemen-\ntary when it comes to synthetic query generation. In fact, compar-\ning the query sets generated by the template-based method and\ngpt-3.5-turbo-instruct, the mean/std. dev of the Jaccard coef-\nficient across entities equals 0.0038 Â±0.0084, indicating very low\noverlap, and hence, complementarity.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Sonal Sannigrahi, Thiago Fraga-Silva, Youssef Oualil, and Christophe Van Gysel\n2 4 6 8 10 12 14\n0\n100000\n200000 entity name\ntemplates\nbabbage-002\ngpt-3.5-turbo\ngpt-3.5-turbo-instruct\ngpt-4\n(a) Distribution of generated query lengths across the approaches under consideration (Â§3.1). Lengths that exceed 15 tokens are not depicted, but are documented in\nTable 1.\n10 20 30 40\n# generated queries\n20\n30\n40\n50\n60NLL under query logs\n(b) Median NLL (Â§3.2; lower is better) for the various query generation\nmethods (except babbage-002 since it leads to very high NLL) for various\nquery cut-offs ( ğ¾ = 10,20,30,40). See Fig. 3a for the legend.\n10 20 30 40\n# generated queries\n0.700\n0.725\n0.750\n0.775\n0.800\nReciprocal Rank\n(c) Reciprocal rank (Â§3.2; higher is better) for the various query generation\nmethods (except babbage-002 since it generates non-sensical queries) for\nvarious query cut-offs ( ğ¾ = 10,20,30,40). See Fig. 3a for the legend.\nFigure 3\n4.2 Query specificity\nFor RQ2, Fig. 3c depicts the reciprocal rank for the various meth-\nods (Â§3.1) at various cut-offs of the list of generated queries. The\nentity name method performs best, since it does not contain any\nsuperfluous terms and matches directly the specific entity mention\ncontained within the entityâ€™s textual representation. Thetemplate-\nbased method performs nearly as well as the entity name method,\nsince it generates queries that contain the entity name padded with\ncarrier terms that are non-specific to the entity (e.g.,â€œplayâ€, â€œsongâ€).\nThe LLM-based methods score slightly worse than the entiy name\nand template methods, since the queries generated using LLMs are\nmore verbose and can include terms that match non-relevant enti-\nties. For example, song titles often consist of generic, non-specific\nterms and multiple songs can have the same title. Between the LLM-\nbased generated query collections, gpt-4 performs worst. When\nexamining the effect of query cut-off (ğ¾), we see that asğ¾increases,\nRR generally decreases. This is due to the fact that, as ğ¾ increases,\nqueries become more complex and can contain terms that con-\nfuse the retrieval model. We answer RQ2 as follows: entity-centric\nqueries generated by LLMs achieve an average reciprocal rank of\n0.70; indicating that the correct entity is often ranked highly. How-\never, since LLMs generate more verbose queries, there are more\nterms in the queries that can throw off the retrieval model.\n4.3 Complementarity\nFinally, following the conclusions to RQ1 and RQ2 above, and the\nqualitative examples in Table 2, we find that template- and LLM-\nbased methods are complementary as follows: (1) template-based\nmethods allow to generate synthetic queries for frequent use-cases\n(e.g., for tail entities) that apply across all entities (e.g., â€œplay mu-\nsic by $ARTISTâ€ ) and are computationally inexpensive, whereas\n(2) LLM-based methods can generate specialized/infrequent useâ€“\ncases (e.g., for popular/controversial entities) specific to the entity\nin question (e.g., â€œplay Taylor Swiftâ€™s duet with Ed Sheeranâ€ )â€”while\nhaving a higher computational cost. Hence, template- and LLM-\nbased methods can be combined to build a richer synthetic query col-\nlection with coverage for both (a) tail entities, and (b) tail use-cases.\n5 CONCLUSIONS\nIn this paper, we performed a preliminary analysis of the use of LLM-\nbased approaches for the generation of synthetic queries for training\na query prior used within a VA speech recognition system. We find\nthat template- and LLM-based approaches are complementary since\n(a) template-based methods can generate queries for frequent use-\ncases and infrequent entities, and (b) LLM-based methods are better\nsuited to target infrequent use-cases tailored to a specific entity. One\nlimitation of this work is that we relied on OpenAIâ€™s API for LLMs.\nHowever, we did not observe any significant differences in behavior\nbetween the LLMs we experimented with, and we believe that the\noverall conclusion that template- and LLM-based query generation\nmethods are complementary will remain valid. Another limitation\nis that the LLM training data can bias the generated query priors,\nhowever addressing this is out of the scope of the current work.\nFuture work includes approaches to mix together the results of the\nmultiple query generation methods, such that the final collection\naligns with user behavior; in addition to exploration of the prompt\nused to query the LLM, use of more advanced prompting techniques\n(e.g., chain of thought), and LLM fine-tuning.\nACKNOWLEDGMENTS\nThe authors would like to thank Manos Tsagkias, Lyan Verwimp,\nRuss Web, Sameer Badaskar, and the anonymous reviewers for their\ncomments and feedback.\nSynthetic Query Generation using Large Language Models for Virtual Assistants SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nSPEAKER BIOGRAPHY\nSonal Sannigrahi is a PhD student at Instituto Superior TÃ©cnico\nin Lisbon, Portugal working on multi-modal Natural Language\nProcessing. She previously worked on multilingual representation\nlearning and has published papers at EACL, ACL, amongst others.\nChristophe Van Gysel is a Staff Research Scientist working on\nthe Siri Speech language modeling team at Apple where he works\non the boundary between ASR and Search. Christophe obtained\nhis PhD in Computer Science from the University of Amsterdam\nin 2017. During his PhD, Christophe worked on neural ranking\nusing representation learning models with a focus on entities and\npublished at WWW, SIGIR, CIKM, WSDM, TOIS, amongst others.\nCOMPANY PROFILE\nApple revolutionised personal technology with the introduction of\nthe Macintosh in 1984. Today, Apple leads the world in innovation\nwith iPhone, iPad, Mac, Apple Watch, and Apple TV. Appleâ€™s five\nsoftware platforms â€” iOS, iPadOS, macOS, watchOS, and tvOS â€”\nprovide seamless experiences across all Apple devices and empower\npeople with breakthrough services including the App Store, Apple\nMusic, Apple Pay, and iCloud. Appleâ€™s more than 100,000 employees\nare dedicated to making the best products on earth, and to leaving\nthe world better than we found it.\nREFERENCES\n[1] Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, and Paul Thomas.\n2023. Can Generative LLMs Create Query Variants for Test Collections? An\nExploratory Study. In SIGIR. 1869â€“1873.\n[2] Ankur Gandhe, Ariya Rastrow, and Bjorn Hoffmeister. 2018. Scalable Language\nModel Adaptation for Spoken Dialogue Systems. In SLT. IEEE.\n[3] Sashank Gondala, Lyan Verwimp, Ernest Pusateri, Manos Tsagkias, and\nChristophe Van Gysel. 2021. Error-driven pruning of language models for virtual\nassistants. In ICASSP.\n[4] David Graus, Daan Odijk, and Maarten de Rijke. 2018. The birth of collective\nmemories: Analyzing emerging entities in text streams. JAIST 69, 6 (2018).\n[5] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n[6] Slava M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language\nModel Component of a Speech Recognizer. ASSP 35 (1987).\n[7] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. InNeurIPS,\nVol. 35.\n[8] Ernest Pusateri, Christophe Van Gysel, Rami Botros, Sameer Badaskar, Mirko\nHannemann, Youssef Oualil, and Ilya Oparin. 2019. Connecting and comparing\nlanguage model interpolation techniques. In Interspeech.\n[9] Juniper Research. 2019. Digital Voice Assistants in Use to Triple to 8 Billion by\n2023, Driven by Smart Home Devices. Press Release.\n[10] Mandana Saebi, Ernest Pusateri, Aaksha Meghawat, and Christophe Van Gysel.\n2021. A discriminative entity-aware language model for virtual assistants. In\nICASSP.\n[11] Hsuan Su, Ting-Yao Hu, Hema Swetha Koppula, Raviteja Vemulapalli, Jen-\nHao Rick Chang, Karren Yang, Gautam Varma Mantena, and Oncel Tuzel. 2024.\nCorpus Synthesis for Zero-shot ASR Domain Adaptation using Large Language\nModels. (2024).\n[12] Andrew Trotman, Antti Puurula, and Blake Burgess. 2014. Improvements to\nBM25 and language models examined. In Australasian Document Computing\nSymposium.\n[13] Christophe Van Gysel, Mirko Hannemann, Ernie Pusateri, Youssef Oualil, and Ilya\nOparin. 2022. Space-Efficient Representation of Entity-centric Query Language\nModels. In Interspeech.\n[14] Christophe Van Gysel, Manos Tsagkias, Ernest Pusateri, and Ilya Oparin. 2020.\nPredicting entity popularity to improve spoken entity recognition by virtual\nassistants. In SIGIR.\n[15] Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. 2023. Can\nChatGPT write a good boolean query for systematic review literature search?\n(2023).\n[16] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models. Transactions on Machine\nLearning Research (2022).\n[17] Youyuan Zhang, Sashank Gondala, Thiago Fraga-Silva, and Christophe Van Gysel.\n2023. Server-side Rescoring of Spoken Entity-centric Knowledge Queries for\nVirtual Assistants. arXiv preprint arXiv:2311.01398 (2023).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8157765865325928
    },
    {
      "name": "Query optimization",
      "score": 0.5462013483047485
    },
    {
      "name": "Query language",
      "score": 0.5305182337760925
    },
    {
      "name": "RDF query language",
      "score": 0.49118658900260925
    },
    {
      "name": "Programming language",
      "score": 0.43344253301620483
    },
    {
      "name": "Natural language processing",
      "score": 0.3557664155960083
    },
    {
      "name": "Information retrieval",
      "score": 0.3298412263393402
    },
    {
      "name": "Web search query",
      "score": 0.2746064364910126
    },
    {
      "name": "Web query classification",
      "score": 0.14709872007369995
    },
    {
      "name": "Search engine",
      "score": 0.07725223898887634
    }
  ]
}