{
  "title": "Looking to recognise: the pre-eminence of semantic over sensorimotor processing in human tool use",
  "url": "https://openalex.org/W3015661364",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2119496073",
      "name": "Giovanni Federico",
      "affiliations": [
        "Università degli Studi Suor Orsola Benincasa"
      ]
    },
    {
      "id": "https://openalex.org/A997907580",
      "name": "Maria A. Brandimonte",
      "affiliations": [
        "Università degli Studi Suor Orsola Benincasa"
      ]
    },
    {
      "id": "https://openalex.org/A2119496073",
      "name": "Giovanni Federico",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A997907580",
      "name": "Maria A. Brandimonte",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2605988653",
    "https://openalex.org/W2954389627",
    "https://openalex.org/W2077116199",
    "https://openalex.org/W2076576536",
    "https://openalex.org/W2113589953",
    "https://openalex.org/W2069826682",
    "https://openalex.org/W4214564655",
    "https://openalex.org/W2050601893",
    "https://openalex.org/W2161777499",
    "https://openalex.org/W1987027832",
    "https://openalex.org/W2010466979",
    "https://openalex.org/W2162876129",
    "https://openalex.org/W2136358489",
    "https://openalex.org/W2150522844",
    "https://openalex.org/W2900395181",
    "https://openalex.org/W1973092699",
    "https://openalex.org/W1981410850",
    "https://openalex.org/W2059043110",
    "https://openalex.org/W2006473412",
    "https://openalex.org/W2052612358",
    "https://openalex.org/W4212844288",
    "https://openalex.org/W1970243396",
    "https://openalex.org/W2472385446",
    "https://openalex.org/W2599639919",
    "https://openalex.org/W2027566210",
    "https://openalex.org/W2134927309",
    "https://openalex.org/W2020270671",
    "https://openalex.org/W2102022426",
    "https://openalex.org/W2116484132",
    "https://openalex.org/W2046510061",
    "https://openalex.org/W2293885148",
    "https://openalex.org/W2792598197",
    "https://openalex.org/W2899885232",
    "https://openalex.org/W2556590024",
    "https://openalex.org/W2910446113",
    "https://openalex.org/W1985270055",
    "https://openalex.org/W2162269770",
    "https://openalex.org/W2045543438",
    "https://openalex.org/W1991143829",
    "https://openalex.org/W2320219884",
    "https://openalex.org/W2055473086",
    "https://openalex.org/W2076207912",
    "https://openalex.org/W2015016650",
    "https://openalex.org/W2127445969",
    "https://openalex.org/W2135331973",
    "https://openalex.org/W2622243750",
    "https://openalex.org/W2982300096",
    "https://openalex.org/W2126672489",
    "https://openalex.org/W2108223231",
    "https://openalex.org/W2006535382",
    "https://openalex.org/W4230305990",
    "https://openalex.org/W2966763453",
    "https://openalex.org/W2037988383",
    "https://openalex.org/W2460367489",
    "https://openalex.org/W1965472719",
    "https://openalex.org/W2131544856",
    "https://openalex.org/W4249882522",
    "https://openalex.org/W2091301585",
    "https://openalex.org/W1990468088",
    "https://openalex.org/W1985957239",
    "https://openalex.org/W2021781432",
    "https://openalex.org/W2131956332",
    "https://openalex.org/W2115521440",
    "https://openalex.org/W2099679614",
    "https://openalex.org/W1982613181",
    "https://openalex.org/W1974044296",
    "https://openalex.org/W2136022845",
    "https://openalex.org/W2004210808",
    "https://openalex.org/W2913606428",
    "https://openalex.org/W2129208602",
    "https://openalex.org/W2079036675",
    "https://openalex.org/W2417025891",
    "https://openalex.org/W113673499",
    "https://openalex.org/W2574742418",
    "https://openalex.org/W2149989625",
    "https://openalex.org/W4232424142",
    "https://openalex.org/W1968316463",
    "https://openalex.org/W2149496207",
    "https://openalex.org/W1975849605",
    "https://openalex.org/W2087114659",
    "https://openalex.org/W2556809674",
    "https://openalex.org/W1488309867"
  ],
  "abstract": "Abstract Alongside language and bipedal locomotion, tool use is a characterizing activity of human beings. Current theories in the field embrace two contrasting approaches: “manipulation-based” theories, which are anchored in the embodied-cognition view, explain tool use as deriving from past sensorimotor experiences, whereas “reasoning-based” theories suggest that people reason about object properties to solve everyday-life problems. Here, we present results from two eye-tracking experiments in which we manipulated the visuo-perceptual context (thematically consistent vs. inconsistent object-tool pairs) and the goal of the task (free observation or looking to recognise). We found that participants exhibited reversed tools’ visual-exploration patterns, focusing on the tool’s manipulation area under thematically consistent conditions and on its functional area under thematically inconsistent conditions. Crucially, looking at the tools with the aim of recognising them produced longer fixations on the tools’ functional areas irrespective of thematic consistency. In addition, tools (but not objects) were recognised faster in the thematically consistent conditions. These results strongly support reasoning-based theories of tool use, as they indicate that people primarily process semantic rather than sensorimotor information to interact with the environment in an agent’s consistent-with-goal way. Such a pre-eminence of semantic processing challenges the mainstream embodied-cognition view of human tool use.",
  "full_text": "1Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreports\nLooking to recognise: the  \npre-eminence of semantic over \nsensorimotor processing in human \ntool use\nGiovanni federico* & Maria A. Brandimonte\nAlongside language and bipedal locomotion, tool use is a characterizing activity of human beings. \nCurrent theories in the field embrace two contrasting approaches: “manipulation-based” theories, \nwhich are anchored in the embodied-cognition view, explain tool use as deriving from past \nsensorimotor experiences, whereas “reasoning-based” theories suggest that people reason about \nobject properties to solve everyday-life problems. Here, we present results from two eye-tracking \nexperiments in which we manipulated the visuo-perceptual context (thematically consistent vs. \ninconsistent object-tool pairs) and the goal of the task (free observation or looking to recognise). \nWe found that participants exhibited reversed tools’ visual-exploration patterns, focusing on the \ntool’s manipulation area under thematically consistent conditions and on its functional area under \nthematically inconsistent conditions. Crucially, looking at the tools with the aim of recognising them \nproduced longer fixations on the tools’ functional areas irrespective of thematic consistency. In \naddition, tools (but not objects) were recognised faster in the thematically consistent conditions. These \nresults strongly support reasoning-based theories of tool use, as they indicate that people primarily \nprocess semantic rather than sensorimotor information to interact with the environment in an agent’s \nconsistent-with-goal way. Such a pre-eminence of semantic processing challenges the mainstream \nembodied-cognition view of human tool use.\nTool use represents a fundamental facet of the human intrinsic ability to interact with the environment. Alongside \nbipedal locomotion and language, tool use is a founding characteristic of human beings. Therefore, the study of \nthe cognitive mechanisms underlying the processing of tools is crucial in Cognitive Science. The critical relevance \nof the topic is well demonstrated by the large amount of research on perceptual and semantic processing of func-\ntional and motor properties of tools (also called “affordances”) that has been done in the last forty years\n1.\nIt should be noticed that the word “affordance” is probably one of the most ambiguous words in experimental \npsychology as it has acquired over time a multiplicity of meanings, hence becoming a term that generated con-\nfusion in the field of tool use, even among scholars. As an effort to reduce such an ambiguity, in this study, we \nendorsed the definition of the term as recently proposed by Osiurak and colleagues, i.e., as “an animal-relative, \nbiomechanical property specifying an action possibility within a body/hand-centered frame of reference” (p. 410)\n1. \nSuch an action possibility pertains to the physical but not to the neurocognitive domain, which, instead analyses \nhow affordances are perceived. To this end, eye-tracking research paradigms, which investigate individuals’ gaze \nbehaviour and their allocation of visuospatial attention, are good candidates to advance our understanding of the \ncognitive mechanisms associated with affordance perception\n2. Within the above framework, we used the word \n“affordance” or any synonym of it to refer to the action possibilities prompted by the visuo-perceptual context.\nAs a class of objects with intrinsic action and motor features3–5, tools are traditionally defined as handheld phys-\nical implementations that amplify the user’s sensorimotor capabilities. Note that, in this way, it is possible to use \nthe word “object” in order to refer to the plausible recipient of an action\n6. However, in everyday life, tools are rarely \nused in isolation. They are generally part of a broader scenario that includes other objects and the set of contextual \nand spatial relations\n7. Thus, a large number of researchers used paradigms with paired objects (i.e., object-tool \npairs) to investigate how the visuo-perceptual context modulates the functional and motor properties of tools8,9.\nSuor Orsola Benincasa University, Laboratory of Experimental Psychology, Naples, Italy. *email: research@\ngiovannifederico.net\nopen\n2Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nInterest in paired-object affordances has increased after the finding that visual extinction – a phenomenon \nin which patients with parietal damage fail to report stimuli presented on the contralesional visual field when \ntwo objects are simultaneously prompted – is reduced for objects co-located for action10,11. Using various para-\ndigms, a series of studies corroborated the specificity of the paired-object affordance effect. Specifically, experi-\nments with healthy participants investigated the cognitive and neural mechanisms underlying the facilitation that \nco-located-for-action and functionally linked objects provide to the perception of paired objects\n8,12,13. To date, \nspeeded classification responses to paired objects emerge when objects are positioned in a standard co-location \nfor right-handed actions\n8. The extraction of potential interactions between objects takes place automatically, \nwith an affordance-related activation for objects that are “active” (for action purposes) in a visual scene and an \naffordance-related inhibition for “passive” objects in a visual scene\n14.\nThe majority of studies related to the paired-object affordance effect used compatibility paradigms to verify \nwhether “action features” associated with an object (e.g., “graspability”) or with the visuo-perceptual context \n(e.g., functional or spatial relations with the objects of an object-tool pair) may have an impact on a task for \nwhich those action features are not relevant (e.g., categorization tasks). Less is known about the relative role of the \nvisuo-perceptual context and action-related information on object recognition. Indirect suggestions come from \npriming studies in which a higher naming accuracy was achieved when a single object (target) was preceded by \na different object (prime) with similar motor interactions. Intriguingly, the effect disappeared when the prime \nwas a word, suggesting an action representation based on visual object information\n15. However, it has long been \nknown that object naming and phonological retrieval rely on several distinct processes involving, among others, \nbut not exclusively, object recognition. In turn, object recognition is based on the perception of form and colour, \non visual analysis of the figure-ground relation and on the activation of stored semantic memories\n16. Therefore, \nobject naming should not be taken as a synonym of object recognition.\nFrom a strictly perceptual point of view, it has been suggested that an observer decodes a visual scene by incor-\nporating functional information derived from relations between objects17. In particular, it appears that observers \ntend to perceptually group objects following familiar functional relations (e.g., a pitcher and a glass), as objects \nand their functional relations interact for object identification\n12. More recently, in a behavioural experiment, \nBorghi and colleagues9 used black-and-white images displaying two manipulable objects linked by either a func-\ntional (e.g., knife–butter) or a spatial (e.g., knife–coffee mug) relation. Results showed faster relatedness responses \nwhen objects were functionally rather than spatially linked.\nObjects’ functional properties pertain to a kind of semantic knowledge (i.e., functional knowledge) associated \nwith the object identity (“What is it?”) and with the goals (“What can I do with?”) attainable by using the object, \nwhereas manipulation knowledge (i.e., sensorimotor knowledge) is related to the proper handling of an object\n18. \nIt appears that objects’ functional knowledge can be conceived as a component of objects’ conceptual representa-\ntion19. However, a long-established neuropsychological research tradition situates manipulation knowledge as \ncentral in tool use 20,21. The so-called “manipulation-based” hypothesis generated strong resonance within the \nembodied cognition approach, which suggests that object knowledge is constituted by information inscribed \nwithin the motor and sensory systems\n20–25. In the embodied cognition perspective, it seems that the main point \nabout tool use is to know how to manipulate it (i.e., using stored sensorimotor knowledge), rather than to reason \nabout how the tool can be used alone or in interaction with other objects. Such a manipulation-based approach \nappears to be rather simplistic, especially if one considers that humans use tools to solve everyday problems, i.e., \nas a problem-solving situation sustained by technical reasoning skills\n26. Accordingly, recent lines of research con-\ntrasted the well-established manipulation-based approach and proposed a reasoning-based perspective whose \nbasic assumption is that people reason about the physical object properties to solve everyday-life problems. Thus, \nupon seeing a tool, people do not automatically activate manipulation knowledge. Rather, they are confronted \nwith everyday issues (e.g., hanging a picture on the wall), so that they use mechanical knowledge (i.e., technical \nreasoning) to reason about how to use a tool (e.g., a hammer) and solve the problem (e.g., pounding a nail in the \nwall). In other words, the reasoning-based theoretical framework supports the idea that people do not passively \nlearn the relationship between objects (manipulation-based approach), but they dynamically generate it in order \nto “act” within a context\n1,6,27–32.\nClassical neuropsychological models posit a dissociation between the visual processing streams associ-\nated with object recognition and those associated with object-directed action. It is generally accepted that the \nventral stream (vision-for-perception system) is involved in object identification and recognition whereas the \ndorsal stream (vision-for-action system) deals with action-related and visuospatial object information mainly \ninvolved in the localization of objects in the space\n33. However, recent lines of research challenged the idea of \nfunctionally-separated processing streams for object recognition and object-directed action, assuming a joint \nand flexible involvement of ventral and dorsal brain areas in affordance processing 1,34–36. In particular, as an \nattempt to overcome the dichotomy between manipulation-based and reasoning-based approaches, the so-called \nThree Action-System model (3AS) has been recently proposed\n1. On the basis of the dorsal-system partition34, the \nthree neurocognitive systems of the 3AS that underlie the perception of affordances, mechanical knowledge and \nfunction knowledge are supposed to be, respectively, the dorso-dorsal system (i.e., the motor control system, in \nparticular the bilateral superior parietal cortex and the intraparietal sulcus), the ventro-dorsal system (mainly the \nleft inferior parietal cortex) and the ventral system (mainly the left temporal cortex). Thus, on the one hand, the \nreasoning-based approach to human tool use is supported by classical developmental studies that considered tool \nuse as a problem-solving occurrence supported by technical reasoning\n26; on the other hand, the reasoning-based \napproach appears to be consistent with neuropsychological evidence that highlights the involvement of a wide \nand complex fronto-parietal and occipito-temporal brain network in tool use and affordance processing\n35.\nIn a most recent eye-tracking study, Federico and Brandimonte 2, using an ecological experimental task \n(i.e., looking at 3D colour images depicting single tools or object-tool pairs), highlighted peculiar differences \nin participants’ visual exploration patterns as the degree of “action readiness” evoked by the visuo-perceptual \n3Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\ncontext changed. To manipulate action readiness, the authors used thematically consistent (e.g., hammer-nail, \nboth in the peri-personal space), thematically inconsistent (e.g., hammer-steel pot, both in the peri-personal \nspace) and spatially inconsistent (e.g., hammer-nail, with the hammer in the peri-personal space and the nail in \nthe extra-personal space) object-tool pairs. Results showed that single tools and tools of object-tool pairs were \ninitially fixated longer on their functional area. However, extending the time-window of analysis, tools of the-\nmatically consistent object-tool pairs were visually encoded in a more suited-for-action way. Indeed, the fixation \npattern focused on the manipulation area of the tool (e.g., the handle of a hammer) more than on its functional \narea (e.g., the head of a hammer). Conversely, tools of thematically and spatially inconsistent pairs obtained a \nreversed visuo-attentional pattern, with the functional area fixated longer than the manipulation area. It should \nbe noticed that the experimental paradigm devised by Federico and Brandimonte\n2 involved an ecological task \nin which participants were asked to look at the visual scene in a natural way. Such a freely-look-at task might \nimplicitly activate the goal of searching for potential mechanical actions between tools and objects. Hence, differ-\nences in visuo-attentional patterns might be evocative of a function-to-mechanical-to-motor “cascade” cognitive \nmechanism through which participants initially visually explore the scene (object-tool pairs) to gather the tool’s \nfunction knowledge (“What is it?”), then they try to solve first the mechanical knowledge issue (“How to use \nthe tool with the object?”) and, finally, the motor control issue (“How to grasp and manipulate the tool?”). For \ninstance, when the visuo-perceptual context is easy to decode in terms of action readiness (e.g., hammer-nail), the \nmechanical knowledge issue is promptly solved and the motor control instantiated, as indicated by the increase \nin the fixation time of the tool’s manipulation area. Conversely, when the visuo-perceptual context does not pro-\nmote action readiness (e.g., bottle-cap), the mechanical knowledge issue may not be so quickly solved, so that the \nmotor control may not be activated, as highlighted by the increment in the fixation time of the tool’s functional \narea. Those results were interpreted by the authors within a reasoning-based theoretical perspective, as suggest-\ning that the flexible visuo-attentional patterns observed in the study might reflect the engagement of different \ntool-use neurocognitive systems (i.e., the Three Action-System)\n1. Within that theoretical frame of reference, \nFederico and Brandimonte2 introduced the concept of “action reappraisal” to refer to the cognitive processing of \nmultiple sources of information (e.g., affordances, mechanical knowledge, functional knowledge, abstract knowl-\nedge, etc.) that can be used by an agent in order to reason about the possibility to act within and upon a context, \nin a proper and agent’s consistent-with-intention way\n27,28. The action reappraisal idea appears to be supported by \nrecent neuropsychological evidence indicating the inferior parietal cortex and the middle temporal brain areas \nas regions where a multimodal integration of action and semantic information takes place to generate high-level \ncognitive representations about tools\n35,37–39.\nDespite the intrinsic appeal of the action reappraisal idea, though, many questions still remain unanswered. \nOne basic issue refers to the nature of the processing required by the task. In fact, Federico and Brandimonte 2 \nused an implicit low-level task (i.e., free visual exploration of 3D images) in which the “to look at” instruction \nwas self-sufficient for the task to be performed, with no further elaborative processing involved. However, to go \na step further in the knowledge about the role of reasoning-based processing in human tool use, one should test \nthe action reappraisal idea by introducing a kind of task that needs higher-level processing to be performed. The \nbest candidate for such a research question is a simple short-term recognition task, which explicitly requires par-\nticipants to look at the images with the higher-level purpose of recognising them as being present or not in the \npreviously seen pair. The joint measures of visual exploration patterns and recognition performance should help \ndisentangle the relative role of reasoning vs. manipulation-based processes. Therefore, in the present article, we \ninvestigated the effects of the action readiness prompted by the visuo-perceptual context on performance in both \na lower level (free visual exploration) and a higher level (object short-term recognition) task.\nWe run two experiments. The first experiment was aimed at replicating the effects reported in a previous \nwork\n2 and extending them to a new set of stimuli with a simplified paradigm. Hence, we analysed by eye-tracking \nthe visuospatial attentional patterns of participants looking at 3D images depicting object-tool pairs that could \nbe thematically consistent or thematically inconsistent, with the object on the left and the tool on the right, both \nin the person’s peri-personal space. The crucial independent variable was the thematic consistency between the \nstimuli composing the object-tool pairs, with the assumption that affordance perception should be facilitated in \nthe thematically consistent condition by virtue of the higher action readiness elicited by the visuo-perceptual \ncontext\n2. In particular, we analysed the fixation patterns related to the tools of the object-tool pairs. The Areas \nof Interest considered in Experiment 1 were the ones related to the functional (middle-top area) and to the \nmanipulation (middle-bottom area) parts of the tool. In accordance with previous results\n2, we expected differ-\nent tool’s visual exploration patterns as the action readiness prompted by the visuo-perceptual context changed. \nSpecifically, a longer fixation duration on the manipulation areas of the tools was expected in the thematically \nconsistent condition.\nIn Experiment 2, we used eye-tracking during a yes-no short-term recognition task, in which new participants \nwere first presented with the same object-tool pairs as in Experiment 1 (thematically consistent and thematically \ninconsistent pairs) and then, after each pair presentation, asked to decide whether a subsequent single object (or a \nsingle tool) was present in the original, just seen, pair. The main reason for using a short-term object recognition \ntask was that, due to its explicit, high-level, semantic nature, such a task is instrumental in analysing differences \nin visual exploration patterns under object-tool thematically consistent or inconsistent conditions. In addition, \na short-term recognition task should prevent participants from using recoding strategies of object-tool relations \nthat might bias responses at test. Hence, Experiment 2 explored the novel, specific hypothesis that the activation \nof higher cognitive processes prompted by the goal-directed nature of the recognition task should influence the \nprocessing of the visual scene. Indeed, if the concept of action reappraisal is correct, then, contrasting a sim-\nple, spontaneous behaviour (looking at; Experiment 1) with a goal-directed behaviour (looking to recognise; \nExperiment 2) should make the functional-to-mechanical-to-motor cascade mechanism emerge more clearly. \nIn particular, in Experiment 1, the activation of the implicit goal of searching for potential mechanical actions \n4Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nbetween tools and objects should produce a visuo-attentional pattern that, under higher action readiness con-\nditions (i.e., in the thematically consistent conditions), favours the tool’s manipulation area, as a consequence of \nthe motoric nature of the implicit task (i.e., a simpler resolution of the cascade mechanism). In Experiment 2, the \nhigher level, goal-directed nature of the recognition task should promote functional/semantic processing over \nmotor activations, with more fixations on the functional areas of the tools also in the thematically consistent con-\ndition. In other words, the cognitive nature of the recognition task should prevent a reasoning-based agent from \nproceeding toward the mechanical and then the motor processing, hence lingering in the functional/semantic \nprocessing.\nFurthermore, in order to investigate the temporal dynamics of tool’s visual exploration, for both experi-\nments we used two different time windows of eye-tracking data analysis (500 ms and 1000 ms). We expected \na visual-attentional pattern initially (first 500 ms of visual exploration) focussing on the tools’ functional area \nin both the experiments and in all experimental conditions\n2. Finally, as regards recognition performance, in \naccordance with some recent literature9,12,15,17,18,37, faster recognition was predicted under thematically consistent \nconditions.\nResults\nExperiment 1. Experiment 1 was aimed to assess whether the action readiness evoked by the visual scene \ncould modify tools’ fixation patterns. Data related to the mean fixation time spent by participants to look at the \nmanipulation and functional AOIs of the tool, within a time window of analysis of 1000 ms, are summarized in \nTable 1. Data related to the first 500 ms of visual exploration are instead reported in Table 2.\nAs regards the extended time window of analysis (1000 ms), a repeated-measure ANOV A revealed an inter-\naction effect of Thematic Consistency and AOIs, F(1, 14) = 30.47, p < 0.001, η\np\n2 = 0.69. Post-hoc pairwise com-\nparisons revealed that fixation duration for the manipulation AOI was longer in the thematically consistent than \nthematically inconsistent condition (p < 0.01). Instead, the functional AOI was fixated longer in the themati-\ncally inconsistent than thematically consistent condition (p < 0.05). In the thematically consistent condition, the \nmanipulation AOI of the tool was fixated longer than its functional AOI (p < 0.05), whereas in the thematically \ninconsistent condition the functional AOI was fixated longer than the manipulation AOI (p < 0.01). The interac-\ntion effect is shown in Fig. 1. No main effects of Thematic Consistency or AOIs were found.\nAs regards the initial tool’s visual exploration (i.e., with a time window of analysis of 500 ms), a second \nrepeated-measure ANOV A revealed a main effect of the AOIs (Functional vs. Manipulation) on the tool’s fixation \nduration, F(1, 14) = 30.35, p < 0.001, η\np\n2 = 0.68. The functional AOI (M = 76.65 ms, SD = 49.97) obtained longer \nfixations than the Manipulation AOI (M = 20 ms, SD = 23.31). No main effect of Context or interaction effects \nwere found.\nAs hypothesised, participants’ visuo-attentional patterns focused on the manipulable part (e.g., the handle of a \nhammer) more than on the functional part (e.g., the head of a hammer) of the tools in the thematically consistent \ncondition. Conversely, the functional area of the tools obtained more fixations in the thematically inconsistent \ncondition (Figs.  1 and 2). Thus, tools’ specific areas obtained a reversed allocation of visuospatial attention as \nthe action readiness evoked by the visuo-perceptual context changed. Importantly, the tool’s visual exploration \nstarted from the tools’ functional areas, as emerged from the analysis of a time window of 500 ms. These results \nconfirm and extend previous findings regarding the emergence of peculiar tools’ fixation patterns as an effect of \naction reappraisal\n2.\nExperiment 2. In Experiment 2 we used a higher level, goal-directed short-term object recognition task \nwith the aim to explore whether functional/semantic processing may take over motor activations, and how that \nreverberates on tool’s visual-exploration. In addition, Experiment 2 was aimed to assess differences in object rec-\nognition performance as the action readiness of the visual context changed.\nEye-tracking data. Data related to the mean fixation time spent by participants to look at the manipulation and \nfunctional AOIs of the tool within the extended time window of analysis (1000 ms) are summarized in Table 3, \nwhereas data related to the restricted time window of analysis (500 ms) are reported in Table 4.\nAreas of Interest (Mean Fixation Time - mean and SD)\nManipulation AOI Functional AOI\nThematic Consistency\nThematically consistent 148.02 ms (38.87) 82.87 ms (38.18)\nThematically inconsistent 68.50 ms (36.21) 139.02 ms (41.63)\nTable 1. Experiment 1 – Mean Fixation Time of Manipulation and Functional AOIs of the Tool. Time window \nof analysis: 1000 ms.\nAreas of Interest (Mean Fixation Time - mean and SD)\nManipulation AOI Functional AOI\nThematic Consistency\nThematically consistent 20.36 ms (18.52) 63.8 ms (42.91)\nThematically inconsistent 19.65 ms (27.97) 89.49 ms (54.57)\nTable 2. Experiment 1 – Mean Fixation Time of Manipulation and Functional AOIs of the Tool. Time window \nof analysis: 500 ms.\n5Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nA repeated-measure ANOV A revealed a main effect of the AOIs on tool fixation duration within the extended \ntime window of analysis (1000 ms), F(1, 28) =  66.62, p < 0.001, η2\np = 0.70. This main effect was due to longer \ntool’s fixation duration on the Functional AOI (M = 247.29 ms, SD = 107.35) than on the Manipulation AOI \n(M = 79.86 ms, SD = 57.37). This effect is shown in Fig. 3. A main effect of Thematic Consistency on tool’s fixation \nduration was also found, F(1, 28) = 5.02, p = 0.033, η2\np = 0.15. This main effect was due to longer tool’s fixation \nduration in the thematically inconsistent condition (M = 183 ms, SD = 117.25) than the thematically consistent \ncondition (M = 152.78 ms, SD = 123.49). No interaction was found.\nA second repeated-measure ANOV A revealed a main effect of the AOIs (Functional vs. Manipulation) on tool \nfixation duration within the first 500 ms of visual exploration, F(1, 28) = 82.88, p < 0.001, ηp\n2 = 0.75. Functional \nAOIs (M = 82.18 ms, SD = 47.24) were fixated longer than Manipulation AOIs (M = 19.39 ms, SD = 23.2). No \nmain effect of Context or interaction effects were found.\nAs predicted, these results highlight a visuo-attentional pattern that emphasises tool’s functional areas in all \nexperimental conditions (Figs.  3 and 4). Participants’ looked at the functional part of the tools longer in both \nFigure 1. Experiment 1 – Tool’s visual exploration. In the thematically consistent condition, tools were fixated \nlonger on their manipulation area whereas in the thematically inconsistent condition they were fixated longer \non their functional area.\nFigure 2. Experiment 1 – Visual exploration heatmaps. Example of heatmaps related to participants’ visual \nexploration of object-tool pairs used in Experiment 1. (A) A heatmap of a thematically consistent object-tool \npair (bowl-whip) that highlights a visuo-attentional focus on the manipulable part of the tool. (B) A heatmap \nof a thematically inconsistent object-tool pair (shoe-whip) that highlights a visuo-attentional focus on the \nfunctional part of the tool. For both (A) and (B) the time window of the eye-tracking analysis was 1000 ms.\n6Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nthematically consistent and inconsistent conditions. In addition, within the thematically inconsistent condition, \ntool’s mean fixation time significantly increased. As regards the tool’s initial visual exploration (time window of \nanalysis of 500 ms), no differences with Experiment 1 were found, with the tools’ functional area fixated longer \nthan the manipulation area.\nBehavioural data. Reaction times and accuracy are summarized in Table 5.\nHits. A repeated-measure ANOV A revealed a main effect of Thematic Consistency on RTs, F (1, 25) =  6.93, \np = 0.014, η\np\n2 = 0.22. A significant interaction between Thematic Consistency and Object Type was also found, F \n(1, 25) = 4.27, p = 0.049, ηp\n2 = 0.15. Post-hoc pair-wise comparisons revealed that reaction times for tools were \nhigher in the thematically inconsistent than the thematically consistent conditions (p < 0.05). The effect is shown \nin Fig. 5a. No main effect of Object Type was found.\nCorrect Rejections. A repeated-measure ANOV A revealed a main effect of Object Type on RTs, F (1, 25) = 5.01, \np = 0.034, ηp\n2 = 0.17. The effect is shown in Fig. 5b. No effects of Thematic Consistency or interaction were found.\nThe analysis of the behavioural data showed a main effect of Thematic Consistency on object recognition. \nWhen a thematically consistent pair preceded a single tool or object, participants were faster at recognising the \nsecond stimulus as part of the pair. In particular, single tools, but not objects, were recognised faster after the pres-\nentation of thematically consistent object-tool pairs (Fig. 5a). Importantly, a mirror effect of Object Type emerged \nfor the Correct Rejections, with objects being rejected faster (Fig. 5b) than tools.\nOverall, the results of Experiment 2 showed that the tool’s functional area was fixated longer irrespectively of \nthe experimental condition. In addition, tools of thematically consistent object-tool pairs were fixated shorter \nand recognised faster.\nDiscussion\nIn two experiments, we explored whether and, if so, how the visuo-perceptual context and the goal of the task \nmay influence both the tool’s visual exploration patterns and object recognition performance. In particular, in \nExperiment 1, we analysed by eye-tracking the fixation patterns of tools that were part of object-tool pairs by \nAreas of Interest (Mean Fixation Time - mean and SD)\nManipulation AOI Functional AOI\nThematic Consistency\nThematically consistent 72.6 ms (48.6) 217 ms (91.4)\nThematically inconsistent 94.7 ms (64.5) 239 ms (86.7)\nTable 3. Experiment 2 – Mean Fixation Time of Manipulation and Functional AOIs of the Tool. Time window \nof analysis: 1000 ms.\nAreas of Interest (Mean Fixation Time - mean and SD)\nManipulation AOI Functional AOI\nThematic Consistency\nThematically consistent 18.47 ms (19.69) 72.88 ms (45.5)\nThematically inconsistent 20.3 ms (26.58) 91.48 ms (47.9)\nTable 4. Experiment 2 – Mean Fixation Time of Manipulation and Functional AOIs of the Tool. Time window \nof analysis: 500 ms.\nFigure 3. Experiment 2 – Mean Fixation Time of tool’s functional and manipulation AOIs. In Experiment 2, \ntools were fixated longer on their Functional AOI in all experimental condition (p < 0.001). Vertical bars denote \n0.95 confidence intervals.\n7Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nusing an implicit, low level, “looking at” visual-exploration task, while in Experiment 2 we combined eye-tracking \nwith an explicit, higher level, “looking to recognise” short-term yes-no recognition task. By contrasting these two \ndistinct kinds of tasks, we aimed to highlight differences in the processing of the visual scene as the action readi-\nness elicited by the visuo-perceptual context and the goal directedness of the task changed.\nThe results of Experiment 1 showed that when the visuo-perceptual context prompted high action readiness \n(i.e., under thematically consistent conditions), a distinctive fixation pattern of the part of the tool involved in its \nuse (i.e., manipulation area) emerged. Conversely, when the context elicited lower action readiness (i.e., in the \nthematically inconsistent scenes), tools were fixated longer on their functional part. The results of Experiment \n1 confirm and extend previous findings\n2, by using a new set of stimuli, a simplified paradigm and a different \ntime-window of analysis.\nRecent evidence has indicated that the initial visual exploration of a tool is at least in part aimed at gain-\ning its function, with longer fixations to the tool’s functional area (“What is it?”) 2,40. Then, a mental simula-\ntion of the action can be produced as an effect of mechanical knowledge (“How to use it?”). Hence, visual \nattention shifts towards the tool’s manipulation area\n1,2,27,28. In line with these claims, when we considered a \nrestricted time window of analysis, results highlighted a visuo-attentional pattern that focussed on the tools’ \nfunctional areas during the first 500 ms, in both experiments, regardless of the experimental conditions. This \nkind of function-to-mechanical-to-motor cascade mechanism is reflected in a distinctive fixation pattern rea-\nsonably generated by the interactions between functional knowledge, mechanical knowledge and motor control \nFigure 4. Experiment 2 – Visual exploration heatmaps. Example of heatmaps related to participants’ visual \nexploration of object-tool pairs used in Experiment 2. In both thematically consistent (A; e.g., bowl-whip) and \nthematically inconsistent (B; e.g., shoe-whip) conditions, the visuo-attentional focus was on the functional part \nof the tool. For both (A) and (B) the time window of the eye-tracking analysis was 1000 ms.\nObject-tool pairs\nHits Correct Rejections\nTool s Objects Tool s Objects\nThematically consistent\nRT (mean and SD, milliseconds)\n600 (122) 638 (107) 660 (134) 618 (115)\nAccuracy (%)\n> 0.95 > 0.97 > 0.96 > 0.97\nThematically inconsistent\nRT (mean and SD, milliseconds)\n667 (162) 645 (138) 665 (114) 653 (132)\nAccuracy (%)\n> 0.96 > 0.96 > 0.95 > 0.95\nTable 5. Experiment 2 – Object recognition: mean reaction times and accuracy.\n8Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nneurocognitive systems, as suggested by the Three Action-System model 1. Such a flexible and dynamic mecha-\nnism pertains to a neurocognitive level of environment-information gathering that engages peculiar operational \nstrategies, hence producing different cognitive outputs.\nIn Experiment 1, the looking-at task presumably activated the implicit goal of searching for potential mechan-\nical actions between objects. Such a goal promoted a visuo-attentional pattern consistent with its implicit motoric \nnature. Hence, when the visuo-perceptual context suggested higher action readiness (i.e., thematically consist-\nent condition) the simplest resolution of the cascade mechanism was reflected in a visuo-attentional pattern \nthat emphasised the tool’s manipulation area in order to actualise the action (i.e. using the tool on the object). \nConversely, the core assumption of Experiment 2 was that the higher level, goal-directed, short-term recognition \ntask should prevent an agent from proceeding toward the mechanical and then the motor processing, hence \npersisting in the functional/semantic processing. Indeed, as predicted, participants exhibited a fixation pattern \nfocused on the tool’s functional area in all experimental conditions. Additionally, in contrast with the results \nof Experiment 1, in Experiment 2 the tool’s mean fixation time was significantly longer under the thematically \ninconsistent condition. The most straightforward interpretation of this result is that thematic inconsistency made \nextraction of information harder\n41.\nAs regards object recognition performance, in Experiment 2, the analysis of the hits revealed that \naction-prompting tools (i.e., tools of the thematically consistent object-tool pairs) were recognised faster. \nConversely, the analysis of the correct rejections showed an object-type effect, with tools being rejected slower \nthan objects. Such an effect cannot be attributed to an alignment effect given by the spatial disposition of the \nobjects\n42. Indeed, if that were the case, the effect should emerge also for the hits. The presence of an object-type \neffect only for the correct-rejection responses indicates that action and/or motoric information associated with \nthe tools is costly in terms of rejection performance. In contrast, when the visual context stimulates the action as \nin the thematically consistent condition, the same tools’ action-related information improves recognition per -\nformance. Thus, a hammer is recognised faster when it is seen after a hammer-nail pair as compared to when \nit is seen after a hammer-scarf pair, probably because the “hammering” action possibility prompted by the \nvisuo-perceptual context is congruent with the action-related information conveyed by the vision of the tool. \nIn contrast, when a screwdriver is seen after the same hammer-nail pair, the “hammering” information – which \nconflicts with the “screwing” information prompted by the screwdriver – is reasonably no longer useful to dis-\ncriminate the previous stimulus, hence producing a cost in terms of rejection performance. This process would \nalso explain the absence of effects for the objects of the pairs.\nOverall, these results suggest that action readiness and the cognitive nature of task may influence the way in \nwhich the information is collected by an observer as well as object recognition performance. These results sup -\nport previous evidence in the literature that emphasised the relevance of objects’ functional relations in object \nrecognition\n9,12,15,17, while indicating – within a cognitive theoretical framework – how an agent can utilise the set \nof available information in order to interact with the environment in a reasoning-based way1,2,27,28.\nRecently, Federico and Brandimonte2 introduced the concept of action reappraisal to refer to a multidimen-\nsional cognitive process that utilises multiple sources of information and distinct neurocognitive systems (e.g., \nfunction knowledge, mechanical and technical knowledge, abstract knowledge, motor system, etc.) to exploit the \nenvironment in terms of action\n2. In that theoretical perspective, tools are seen as manipulable, physical imple -\nments that amplify the user’s sensorimotor capabilities6 in order to solve everyday problems. Tool use is thereby \nconceptualised as an instance of a problem-solving situation sustained by mechanical knowledge and technical \nreasoning\n1,2,26–28,43. Accordingly, here we endorse a theoretical approach for which individuals use tools to deal \nFigure 5. Experiment 2 – Mean RTs for Hits and Correct Rejections. (A) The interaction between Object Type \nand Thematic Consistency. Tools of object-tool pairs, but not objects, were recognised faster in the thematically \nconsistent than in the thematically inconsistent condition. Vertical bars denote 0.95 confidence intervals. \n(B) The main effect of Object Type, with tools being rejected slower than objects. Vertical bars denote 0.95 \nconfidence intervals.\n9Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nwith everyday circumstances, thus reasoning about the most appropriate use of them within a context, rather \nthan to passively learn and actualise the actions (also called “gesture engrams”) that can be performed with them \n(i.e., the manipulation-based approach\n20–24). This theoretical perspective is in line with the working memory \nhypothesis of affordances as regards the claim that the nature of the task modifies the cognitive workload and, \nas a consequence, affordance perception\n44–46. Furthermore, taking into account the affordance-competition \nhypothesis47 according to which various affordances are “pre-activated” before being selected, action reappraisal \nwould be explained as a cognitive process – presumably supported by the frontal lobes – that, from the multiple \nenvironment-available affordances, selects only those that are relevant to the individual’s intentions through an \ninhibitory mechanism sustained by high-level executive functions\n2,27.\nThe assumptions underlying action reappraisal appear to be substantiated by most recent evidence revealing a \nmultiplicity of distinct neurocognitive systems involved in tool use1,35. In particular, the recently proposed Three \nAction-System model1 provides a theoretical framework within which the idea of action reappraisal can be easily \nincorporated. For instance, the differences observed in the fixation patterns of the two experiments reported \nhere and the facilitation effects in tool recognition found in Experiment 2 might be interpreted as reflecting the \ninteractions between functional knowledge, mechanical knowledge and motor control neurocognitive systems. \nNamely, the ecological nature of the task in Experiment 1 (looking at) may have implicitly activated the cascade \nmechanism such that participants first solved the functional knowledge issue (“What is it?”), then the mechan-\nical knowledge issue (“How to use it?”) and finally they activated the motor system (i.e., mental simulation of \nthe action), hence finalizing the whole cascade process. On the other hand, in Experiment 2, the goal-oriented \nnature of task (looking to recognise) may have allowed participants to skip the mechanical/motor processing and \nto keep focused on the functional/semantic aspects of the recognition task. Coherent with this interpretation, \nthe behavioural data revealed that the same mechanical/motor information (i.e., action readiness) evoked by the \nvisuo-perceptual context had a facilitation effect (shorter RTs) on hits under thematically consistent conditions, \nbut an interference (longer RTs) on correct rejections.\nMechanical knowledge (the ventro-dorsal system) might be considered as a bridge system connect-\ning higher-level semantic information associated with object function and identity (the ventral system) and \nthe motor-control system (the dorso-dorsal system). This bridge system would generate a simulation of an \naction related to tool use, hence handling the perception of the related affordances in a proper and agent’s \nconsistent-with-intention way\n2. Such a kind of dynamic synergy among the neurocognitive systems, on the \none hand, produces an effect on the way action-related information is visually encoded, coherently with the \ndirect-visual-route-to-action theoretical view for which vision guides action;\n33 on the other hand, it substantiates \nhow this effect might reverberate on such high-level processes as object recognition.\nRecent evidence indicates that human tool use relies on a large and composite interplay of brain areas pertain-\ning to the fronto-parietal and occipito-temporal networks 3–5,35,36,48–55. In particular, the mechanical knowledge \nseems to be stored in the left inferior parietal cortex, specifically within the PF cytoarchitectonic area of the supra-\nmarginal gyrus (SMG), whereas the neurocognitive system associated to affordance perception (i.e., the motor \ncontrol system) appears to be the one composed by the bilateral superior parietal cortex and the intraparietal \nsulcus (putative human anterior intraparietal sulcus area and the anterior dorsal IPS). The left anterior portion \nof SMG, extending to the PFt cytoarchitectonic area of SMG seems to be an integrative neurocognitive layer \nbetween mechanical knowledge and the motor control system\n35,56,57. Despite the neural correlates of functional \nknowledge are still a debated issue in the literature49,50,58, recent evidence indicates the left temporal cortex, the left \nposterior middle temporal gyrus (pMTG) and the lateral occipital complex (LOC) as plausible neural substrates \nof functional knowledge\n35,36,52,58.\nThe left inferior parietal lobule might represent a neural substrate largely implicated in forming object-related \naction representations. In fact, a recent fMRI study highlighted compulsory access to abstract action informa-\ntion in the left inferior parietal lobe for object-directed actions, irrespective of task context\n53. Coherently, in the \ncontext of tool-related action processing and in the epistemological domain of others’ action understanding59, a \nmost recent meta-analysis54 reported the activations of both the PF cytoarchitectonic area (within the left infe-\nrior parietal lobe) and the left inferior frontal gyrus in observational tool-use contexts. As we detailed before, \nthe PF area is involved in the storage of mechanical knowledge (i.e., the ability to reason about physical object \nproperties). Hence, it appears that observing tools to use them and observing others’ tool-use actions share such \na specific neural counterpart. These findings clearly suggest that even observing others’ tool-use actions requires \nsupplementary cognitive skills and support the idea that tool-use action understanding might be much more \n“dis-embodied” than habitually supposed\n54.\nIn both experiments, we used object-tool pairs that were thematically consistent or inconsistent. Notably, \nincreasing evidence from studies using thematically related vs. unrelated pairs of objects suggests an involvement \nof the posterior parieto-temporal cortex (specifically, the temporo-parietal junction, the inferior parietal lobe, and \nthe middle and superior temporal gyri) in objects’ thematic relations processing\n60–63. Moreover, an increased left \noccipital cortex activation (ventral stream) has been reported when objects are correctly positioned for action, \nwhile the anterior regions of the dorsal stream (e.g., supplementary motor area) have been reported to be acti-\nvated when the task required an action decision but objects were not in the correct position for the action\n36. \nCrucially, a recent TMS study has suggested that the action readiness evoked by the visuo-perceptual context \nfacilitates the elaboration of taxonomic semantic relations among objects, indicating the inferior parietal cortex \nand the middle temporal areas as regions where a multimodal integration of action and semantic information \ntakes place to generate high-level cognitive representations about tools\n37. In the same direction, a recent fMRI \nstudy used both verbal stimuli and video to assess whether specific brain areas are involved in tool-related action \nprocessing, independently of the stimulus type. By using a multi-voxel pattern analysis, that study provided com-\npelling evidence in favour of the identification of the lateral posterior temporal cortex as a crucial brain region \nwhere a cross-modal integration of action-related information is executed. Conversely, unimodal representations \n10Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nproduced widespread and overlapped activations in the fronto-parietal network, in the regions that were not \nimplicated in the cross-modal integration of action-related information39.\nThe overlap of activation in the temporal and parietal brain areas reported by the above-mentioned studies \nappears to be particularly intriguing when analysed from the perspective of the recently proposed Hub-and-Spoke \nhypothesis of Semantic Memory\n38,64. As an attempt to bridge the gap between embodied and unembodied the-\nories of conceptual knowledge (see Meteyard, Cuadrado, Bahrami and Vigliocco65 for a strong-to-weak embod-\nied theories comparison), the Hub-and-Spoke hypothesis assumes that semantic memories originate from the \ninteraction of modality-specific sources of information, called spokes (e.g., visual features, praxis or somatosen-\nsory information, sounds, etc.), with a trans-modal semantic hub that provides a further modality-invariant rep-\nresentational resource. Spokes rely on distributed modality-specific cortical regions (e.g., motor areas for objects’ \nmotor properties), whereas the cortical regions within the anterior temporal region (ATL) underpin the central \nrepresentational hub. The interplay between spokes and hub gives rise to consistent and generalizable concepts.\nDeveloped in the epistemological domain of semantic memory studies, the Hub-and-Spoke hypothesis is \nconsistent with the concept of action reappraisal as an effect of multidimensional, information-gathering pro -\ncesses. Following this theoretical perspective, object-related action information might be conceptualised as \nmodality-specific motoric information that adds to the representational resources typically involved in object \nrecognition. Importantly, such a kind of motoric information potentially emanates from some (though not all) \nspecific spokes included in the Hub-and-Spoke hypothesis\n38.\nWe considered human tool use as a kind of problem-solving situation actively sustained by technical and \nmechanical reasoning skills2,26–28. In this sense, neuroimaging evidence of a left inferior prefrontal cortex involve-\nment in action planning and execution appears to be rather promising for interpreting our results35. These cortical \nregions are extensively implicated in executive functions as well as in motor timing, sequencing and simula-\ntion\n66–68. In particular, the rostro-lateral prefrontal cortex appears to be involved in reasoning tasks such as rela-\ntional integration, i.e., considering different relations simultaneously 69. The involvement of the frontal lobes in \nhuman tool use might also signal an inhibitory mechanism that modulates the selection of the most appropriate \naffordance among those that are simultaneously available in the environment. Such a selection mechanism would \noccur in accordance with the intentions and the action possibilities of the agent\n27,47. However, given the lack of \nneuropsychological studies on the correlation between frontal lesions and tool-use impairments, the role of the \nfrontal cortex in human tool use requires further investigation in future research.\nTo sum up, by extending previous findings\n2, the present results converge towards recent lines of research \nthat support a reasoning-based approach to human tool use. As we detailed above, the convergence between \nlower-level (Experiment 1) and higher-level (Experiment 2) measures plausibly reflects the involvement of distinct \nand complex neurocognitive systems engaged in human tool-use processing. Although the present results do not \nspeak directly to questions about the neural counterparts of these effects, we believe that applying the present \nmethods to patient studies or combining them with neuroimaging procedures may help address those questions.\nThe concept of action reappraisal – conceived as a wide-range cognitive theoretical perspective that links \ntogether different epistemological reservoirs (e.g., psychology of perception, memory studies, neuropsycholog-\nical studies, etc.) – emphasises how adopting a transversal approach might be extremely prolific to study such a \nhuman-characterizing and complex activity as tool use. It is worth noticing that our results add to the growing \nliterature suggesting that higher-level, semantic information is activated earlier than lower-level perceptual infor-\nmation and can affect visual perception, although the magnitude of top down processes is modulated by the con-\ntext and expectations\n30–32,70,71. To this respect, one might argue that the emphasis given in this article to semantic \nprocessing would seem in contrast with the neuropsychological literature that has shown how patients with \nsemantic deficits can still use tools\n55,72,73. However, we examined healthy participants engaged in free-observation \nand object-recognition ecological tasks, rather than to evaluate tool-use tasks in clinical settings. Therefore, the \nresults of those studies\n55,72,73 are hardly comparable to the present findings.\nAs we discussed, mechanical knowledge appears to be necessary to use tools (in terms of actual utiliza-\ntion)1,2,6,27–29, whereas functional/semantic knowledge might be acting as a complementary addendum in order to \nconstruct object-related action representations12,13,15–19,30–32,39. Thus, for patients with semantic deficits, the integrity \nof the neurocognitive systems associated with mechanical knowledge would guarantee for their adequate tool use \nabilities\n55,72,73. However, while, on the one hand, functional/semantic knowledge might be neither necessary nor \nsufficient in order to reason about the technical properties of a tool or to actualise the actions linked to its use, on \nthe other hand, it might be indispensable in order to create generalizable and abstract concepts (representations) \nlinked to tools and objects\n37–39,64. Such representations might be usable in everyday life by an agent, in the context \nof a cognitive-oriented functioning, as we have recently suggested. Here, we wish to emphasise the dynamicity and \nautomaticity of higher-level cognitive processes trough which a reasoning-based agent may elaborate multiple \nobjects-related information (i.e., action reappraisal). Hence, we investigated how the magnitude of such high-level \ncognitive processes may be modulated by the visuo-perceptual context and the individuals’ goals. Considering tool \nuse as a human ability that stands at the intersection of multiple cognitive processes, the action reappraisal idea \nmight constitute a useful starting point in order to provide broader answers regarding the mechanisms that under-\nlie tool processing in everyday life. While recent and converging evidence seem to support the action reappraisal \nidea\n1,2,6,27–32,35–39,44–46,53,54,59,64,71, further studies are clearly necessary in order to explore its deepest implications.\nMethods\nThe experiments were conducted in the Laboratory of Experimental Psychology at Suor Orsola Benincasa \nUniversity (Naples, Italy). The experiments were performed following the ethical standards laid down in the \n1964 Declaration of Helsinki. The study received approval from the Ethics Committee of the Department of \nEducational, Psychological and Communication Sciences of Suor Orsola Benincasa University.\n11Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nExperiment 1. Participants. Fifteen participants (8 females; mean age = 23.07 years, S.D. = 2.19) with nor-\nmal or corrected-to-normal vision took part in the experiment. All were right-handed based on the Edinburgh \nHandedness Inventory74, had no history of neurological or psychiatric disorders and gave informed consent on \ntheir participation.\nMaterials. Twenty three-dimensional (3D) computer-graphics generated stimuli were used in Experiment 1. \nTwo different classes of stimuli were used. The first group of stimuli was composed by 3D colour images depict-\ning pairs of objects (a tool on the right – e.g., a screwdriver – and an object on the left, e.g., a screw) that were \nthematically consistent, placed on the part of a table closest to the observer, in the participant’s peri-personal \nspace. There were the following ten object-tool pairs: nail-hammer, bowl-whip, carton box-cutter, bottle-bottle \nopener, screw-screwdriver, salami-knife, coffee cup-teaspoon, notebook-pen, glass-bottle, padlock-key. The sec-\nond group of stimuli was composed by 3D colour images depicting pairs of thematically-inconsistent objects \n(a tool on the right – e.g., a hammer – and an object on the left, e.g., a scarf) placed on the part of the table \nthat was closest to the observer, in the participant’s peri-personal space. This group comprised ten object-tool \npairs: scarf-hammer, women shoe-whip, alarm clock-cutter, notebook-bottle opener, nut-screwdriver, bolt-knife, \nChristmas ball-teaspoon, men shoe-pen, cap-bottle, baseball-key.\nBoth the objects of the object-tool pairs (Fig. 6a,b) appeared placed directly on a table, with a mean perceived \ndistance between object and tool (calculated from the centres) of approximately 20 cm and an angle of approxi-\nmately 180 deg (centre-to-centre, by considering the horizontal line of the table). Some examples of the stimuli \nare illustrated in Fig. 6(a,b).\nProcedure. The air-conditioned room used in the experiment was maintained constant at a temperature of 24 °C \nduring the entire duration of the study. Light conditions of the room were kept stable for all participants and for \nthe entire duration of the experiment. Before starting, the participants signed informed consent. They were asked \nto self-report their right-handedness, their adequate visual acuity and the absence of any neurological and psy-\nchiatric diseases at the date of the experiment. The Edinburgh Handedness Inventory\n74 was administered to par-\nticipants in order to verify that they were actually right-handers. Then, a classic optometric test with participants \nplaced three meters away from the test stimuli was administered to evaluate visual acuity. The participants were \nseated on a chair and a headrest was used to prevent head movements in order to allow a precise eye-tracking \nrecording. Participants seated at the distance of 54 cm from the monitor (23 inches, with a horizontal viewing \nangle of approximately 55 deg and a vertical viewing angle of approximately 15 deg) and were asked to keep \ntheir right hand motionless on the desk. In this way, the right hand was resting on the right side of the monitor, \nbecoming peripherally visible to the participants in the context of the visual scene. More specifically, the right \nhand was located at an angle between 35 deg and 40 deg of the right visual space (mid-peripheral vision). Then, \nthe experimental instructions were given. Participants were asked to complete an eye-tracking software calibra-\ntion procedure by following with their eyes a white cross that sequentially appeared on nine parts of the screen \n(black background). Afterwards, participants were asked to “observe what appeared on the screen in the most \nnatural way as possible” and the experiment started. A single trial of ten images related to each experimental con-\ndition was administered. Thus, twenty images were randomly presented according to the experimental visual flow \nFigure 6. Example of stimuli used in both the experiments. (A) Thematically consistent object-tool pair (a bowl \nand a whip). (B) Thematically inconsistent object-tool pair (a shoe and a whip).(C) Single tool (a whip). (D) \nSingle object (a shoe). Experiment 1 included (A) and (B). Experiment 2 included all stimuli.\n12Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\n(Fig. 7): before each stimulus, a fixation point (white cross over black background in the centre area of the screen) \nof 500 ms duration was shown. Then, the stimulus appeared for 3000 ms. After each stimulus, a black screen \nappeared for 4000 ms in order to permit retina relaxation. Each single presentation lasted 7.5 seconds (500 ms + \n3000 ms + 4000 ms). Globally, the stimuli presentation lasted 150 seconds (7500 ms x 20 stimuli). At the end of \nthe stimulation, a reachability task was administered using the same stimuli as those used during the experiment, \nwhich required participants to indicate if tools and objects in the visual scene were graspable with their right \nhand, according to their perspective. Participants correctly reported that both the tools and the objects were \nreachable with their right hand. Most relevant, in the thematically consistent condition, participants reported \nthat the tool was potentially usable on the object (e.g., the hammer was effectively usable on the nail), whereas, \nin the thematically inconsistent condition, tools and objects were reported to be not immediately usable together \nin a proper way (e.g., the bottle opener was not considered usable on the glass despite their spatial proximity). \nFor each participant, the overall duration of the experiment was 13 minutes. At the end of the experiment, par-\nticipants were debriefed regarding the purposes of the study and the methods used. No participant was excluded \nfrom the sample.\nApparatus and software paradigms. We used a Full-HD Webcam (Logitech HD Pro C920, sampling rate: 30 Hz) \nas eye-tracking hardware and custom Python/JavaScript software to manage the experiment and to acquire gaze \nbehaviour data. The eye-tracking technology used in the present study is based on WebGazer, an eye-tracking \nJavaScript library\n75. All the software paradigms were conceived and programmed using the Python programming \nlanguage and the PsychoPy framework76 with custom code optimization. In order to extract and analyse the data \ncollected through the software paradigms, distinct ad-hoc, custom-made scripts were engineered and developed \nusing PHP programming language and the MySQL Database Management System. All stimuli were presented on \na 23-inches monitor (Dell S2319H) at a resolution of 1920*1080px. All experimental software was executed by \nusing an Apple MacBook Pro (13-inch, 2017) running macOS Catalina (version 10.15).\nGaze-behaviour data. Participants’ visual-exploration patterns were analysed in terms of mean fixation dura-\ntion (milliseconds) on different Areas of Interest (AOIs). We defined two AOIs: the manipulation area of the \ntool (i.e., the middle-bottom area where to put the hand in order to use it) and the functional area of the tool \n(i.e., middle-top area of the tool through which it is possible to understand its function; Fig.  8). Taking into \naccount the technical limits of the eye-tracking technology used, both the AOIs were computed with a perimeter \nincreased by 64 pixels in all directions. Mean fixation durations to the AOIs were averaged per condition. For \neach stimulus, the first 250 ms of eye-tracking data were excluded from the gaze-behaviour analysis in order to \nreduce the error produced by the fixation point in participants’ visual-exploration patterns. Only the first 1000 ms \nof visual-exploration data for each stimulus were analysed in order to reduce data dispersal effects due to par -\nticipants’ visual-scene exploration. Within the first 1000 ms of visual exploration, two different time windows of \nanalysis were considered (first 500 ms and first 1000 ms). An at-a-glance qualitative indication of differences in \nparticipants’ fixation patterns may be appreciated in the visuo-attentional heatmaps (Fig. 2).\nData analysis. To analyse how participants looked at the tools of the object-tool pairs as the visuo-perceptual \ncontext changed, we performed a 2 × 2 repeated measure ANOV A with AOIs (manipulation vs. functional area) \nas a 2-level factor and Thematic Consistency (thematically consistent vs. thematically inconsistent pairs) as a \n2-level factor on tool fixation duration (milliseconds) for each time window of analysis (500 ms and 1000 ms). An \nalpha level of 0.05 was used for all the analyses. We used Bonferroni correction for multiple comparisons. All the \nanalyses were conducted by using the open-source statistical software “R” (v. 3.6.1; GUI v. 1.70; build 7684) for \nApple macOS operating system (Catalina, v. 10.5).\nExperiment 2.  Participants. Twentynine participants (11 females; mean age =  21.16 years, S.D. = 3.62) \nwith normal or corrected-to-normal vision were included in this experiment. All were right-handed based on the \nEdinburgh Handedness Inventory\n74 and had no history of neurological or psychiatric disorders. Participants gave \ntheir informed written consent.\nFigure 7. Experiment 1 - Experimental flow. For each trial, a fixation point appeared for 500 ms, then an object-\ntool pair appeared for 3000 ms. This pair could be thematically consistent or thematically inconsistent and it was \nfollowed by a black screen that appeared for 4000 ms to permit retina relaxation.\n13Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nMaterials. Forty three-dimensional (3D) computer-graphics images generated stimuli were used in the experi-\nment. Four different classes of stimuli were used. The first two groups of stimuli were the same as in Experiment \n1 (10 thematically consistent and 10 thematically inconsistent object-tool pairs). The third group of stimuli was \ncomposed by 3D colour images of a single tool placed at the centre of a table in the participant’s peri-personal \nspace. This group was composed of ten stimuli: hammer, whip, cutter, bottle opener, screwdriver, knife, teaspoon, \npen, bottle, key. The fourth group of stimuli was composed by 3D colour images of a single object placed in the \ncentre of a table in the participant’s peri-personal space. This group comprised ten objects: nail, bowl, carton box, \nbottle, screw, salami, coffee cup, notebook, glass, padlock. Single objects and tools (Fig. 6c,d) appeared placed on \nthe centre of a table, with the centre of the stimulus aligned with the centre of the table. Some examples of the \nstimuli are illustrated in Fig. 6(a–d).\nProcedure. The experimental setting and general procedure were the same as in Experiment 1 except that par-\nticipants were engaged in a short-term recognition task. Namely, participants seated at a distance of 54 cm from \nthe monitor (23 inches, with a horizontal viewing angle of approximately 55 deg and a vertical viewing angle of \napproximately 15 deg) and were asked to keep their right hand motionless on the desk and the left hand on a \nkeyboard with the index finger placed on the “E” key and the middle finger placed on the “W” key. In this way, \nthe right hand was resting on the right side of the monitor, becoming peripherally visible to the participants in \nthe context of the visual scene. More specifically, the right hand was located at an angle between 35 deg and 40 deg \nof the right visual space (mid-peripheral vision). Then, the experimental instructions were given. Participants \nwere asked to complete an eye-tracking software calibration procedure by following with their eyes a white cross \nthat sequentially appeared on nine parts of the screen (black background), then the experiment started. Each \ntrial began with a fixation point (+ ) that remained in view for 500 ms. Then, participants had to observe a first \nstimulus consisting of an object-tool pair that remained in view for 2000ms. while their gaze behaviour for the \nstimulus was analysed through eye-tracking technology. Then, a second fixation point (+) was shown for 500 ms, \nfollowed by the target stimulus consisting of a single object or a single tool. Participants had to indicate, by \npressing the appropriate key on the keyboard (“W” key for yes responses; “E” key for no responses), whether \nthe latter stimulus (single tool or single object) was present in the previously observed object-tool pair. At the \nend of each trial a black screen (blank) appeared for 4000 ms in order to permit retina relaxation. Stimuli were \nrandomly selected. The experimental paradigm was composed by a 2 (Thematic Consistency: 10 thematically \nconsistent vs. 10 thematically inconsistent object-tool pairs) x 2 (Object Type: tools vs. objects) x 2 (Response \nType: yes vs. no) within-factor design. Overall, the experiment consisted of 80 trials (10 ×  2 × 2 × 2) without \nrepetitions. The reachability task administered at the end of the experiment once again showed that participants \ncorrectly reported that both the tool and the object of the object-tool pairs were reachable using their right hand. \nIn addition, the tool was considered usable on the object only in the thematically consistent condition. The exper-\niment lasted approximately 30 minutes per participant. At the end of the experiment, participants were debriefed \nregarding the purpose of the study and the methods used as in Experiment 1. No participant was excluded from \nthe sample. The experimental flow is summarised in Fig. 9.\nApparatus and software paradigms. In Experiment 2 we used the same apparatus and software paradigms as in \nExperiment 1.\nGaze-behaviour data. The same criteria as in Experiment 1 guided the analysis of data related to participants’ \ngaze-behaviour. Differences in participants’ visuo-attentional patterns may be qualitatively appreciated in the \nfixation heatmaps (Fig. 4).\nData analyses. First, we analysed eye-tracking data in the same way as in Experiment 1. Secondly, we analysed \nbehavioural data related to participants’ recognition performance. Three male participants were excluded from \nFigure 8. Areas of Interest considered in the experiments. The AOIs considered in the experiments were the \nmanipulation area of the tool (circled in red, labelled as “M”) and the functional area of the tool (circled in \ngreen, labelled as “F”).\n14Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nthe behavioural data analysis as their object recognition performance was above 2.5 S.D. from the mean (outliers). \nAn alpha level of 0.05 was used for all the analyses. We used Bonferroni correction for multiple comparisons. \nAll the analyses were performed using R (v. 3.6.1; GUI v. 1.70; build 7684) for Apple macOS operating system \n(Catalina, version 10.5). For all the analyses, an alpha level of 0.05 was used.\nEye-tracking data analysisWe performed a 2 × 2 repeated-measure ANOV A with AOIs (manipulation vs. func-\ntional area) as a 2-level factor and Thematic Consistency (thematically consistent vs. thematically inconsistent \nobject-tool pairs) as a 2-level factor on tool fixation duration (expressed in milliseconds).\nBehavioural data analysisMean reaction times and accuracy were calculated. As typically observed with this kind \nof recognition task, accuracy was above 0.95 in all conditions, hence we analysed only mean RTs. For the mean \nRTs analysis, we performed two (Hits and Correct Rejections) distinct 2 × 2 repeated-measure ANOV As with the \n2-level factor Thematic Consistency (thematically consistent vs. thematically inconsistent object-tool pairs) and \nthe 2-level factor Object Type (tools vs. objects) on RTs.\nData availability\nThe data that support the findings of this study are available from the corresponding author upon reasonable \nrequest.\ncode availability\nSoftware code and scripts are available on reasonable request to the corresponding author.\nReceived: 16 December 2019; Accepted: 24 March 2020;\nPublished: xx xx xxxx\nReferences\n 1. Osiurak, F ., Rossetti, Y . & Badets, A. What is an affordance? 40 years later. Neuroscience & Biobehavioral Reviews 77, 403–417 (2017).\n 2. Federico, G. & Brandimonte, M. A. Tool and object affordances: An ecological eye-tracking study. Brain and Cognition 135, 103582 \n(2019).\n 3. Chao, L. L. & Martin, A. Representation of manipulable man-made objects in the dorsal stream. Neuroimage 12(4), 478–484 (2000).\n 4. Johnson-Frey, S. H. The neural bases of complex tool use in humans. Trends in Cognitive Sciences 8(2), 71–78 (2004).\n 5. Króliczak, G. & Frey, S. H. A common network in the left cerebral hemisphere represents planning of tool use pantomimes and \nfamiliar intransitive gestures at the hand-independent level. Cerebral Cortex 19(10), 2396–2410 (2009).\n 6. Osiurak, F ., Jarry, C. & Le Gall, D. Grasping the affordances, understanding the reasoning: toward a dialectical theory of human tool \nuse. Psychological Review 117(2), 517 (2010).\nFigure 9. Experiment 2 – Experimental flow. A fixation point appeared for 500 ms, then an object-tool pair \nappeared for 2000ms. This pair could be either thematically consistent or thematically inconsistent and it was \nfollowed by a second fixation point of 500 ms. Finally, a single tool or a single object appeared. Participants \npressed the “yes” key or the “no” key to indicate whether the single tool or the single object was present or not in \nthe object-tool pair. At the end of each trial, a black screen appeared for 4000 ms to permit retina relaxation.\n\n15Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\n 7. Mizelle, J. C. & Wheaton, L. A. Why is that hammer in my coffee? A multimodal imaging investigation of contextually based tool \nunderstanding. Frontiers in Human. Neuroscience 4, 233 (2010).\n 8. Y oon, E. Y ., Humphreys, G. W . & Riddoch, M. J. The paired-object affordance effect. Journal of Experimental Psychology: Human \nPerception and Performance 36(4), 812 (2010).\n 9. Borghi, A. M., Flumini, A., Natraj, N. & Wheaton, L. A. One hand, two objects: Emergence of affordance in contexts. Brain and \nCognition 80(1), 64–73 (2010).\n 10. Riddoch, M. J., Humphreys, G. W ., Heslop, J. & Castermans, E. Dissociations between object knowledge and everyday action. \nNeurocase 8(1), 100–110 (2002).\n 11. Riddoch, M. J. et al. I can see what you are doing: Action familiarity and affordance promote recovery from extinction. Cognitive \nNeuropsychology 23(4), 583–605 (2006).\n 12. Green, C. & Hummel, J. E. Familiar interacting object pairs are perceptually grouped. Journal of Experimental Psychology: Human \nPerception and Performance 32(5), 1107 (2006).\n 13. Roberts, K. L. & Humphreys, G. W . Action relationships concatenate representations of separate objects in the ventral visual system. \nNeuroimage 52(4), 1541–1548 (2010).\n 14. Xu, S., Humphreys, G. W . & Heinke, D. Implied actions between paired objects lead to affordance selection by inhibition. Journal of \nExperimental Psychology: Human Perception and Performance 41(4), 1021 (2015).\n 15. Helbig, H. B., Graf, M. & Kiefer, M. The role of action representations in visual object recognition. Experimental Brain Research \n174(2), 221–228 (2006).\n 16. Price, C. J., Moore, C. J., Humphreys, G. W ., Frackowiak, R. S. J. & Friston, K. J. The neural regions sustaining object recognition and \nnaming. Proceedings of the Royal Society of London. Series B: Biological Sciences 263(1376), 1501–1507 (1996).\n 17. Green, C. & Hummel, J. E. Functional interactions affect object detection in non-scene displays. In Proceedings of the Annual \nMeeting of the Cognitive Science Society (Vol. 26, No. 26) (2004).\n 18. Garcea, F . E. & Mahon, B. Z. What is in a tool concept? Dissociating manipulation knowledge from function knowledge. Memory & \nCognition 40(8), 1303–1313 (2012).\n 19. Ni, L., Liu, Y . & Yu, W . The dominant role of functional action representation in object recognition. Experimental Brain Research \n237(2), 363–375 (2019).\n 20. Buxbaum, L. J. Ideomotor apraxia: a call to action. Neurocase 7(6), 445–458 (2001).\n 21. Thill, S., Caligiore, D., Borghi, A. M., Ziemke, T. & Baldassarre, G. Theories and computational models of affordance and mirror \nsystems: an integrative review. Neuroscience & Biobehavioral Reviews 37(3), 491–521 (2013).\n 22. Buxbaum, L. J. & Kalénine, S. Action knowledge, visuomotor activation, and embodiment in the two action systems. Annals of the \nNew York Academy of Sciences 1191, 201 (2010).\n 23. Borghi, A. M. Object concepts and action: Extracting affordances from objects parts. Acta Psychologica 115(1), 69–96 (2004).\n 24. Borghi, A. M. & Riggio, L. Sentence comprehension and simulation of object temporary, canonical and stable affordances. Brain \nResearch 1253, 117–128 (2009).\n 25. Barsalou, L. W . Grounded cognition. Annual Review of Psychology 59, 617–645 (2008).\n 26. Beck, S. R., Apperly, I. A., Chappell, J., Guthrie, C. & Cutting, N. Making tools isn’t child’s play. Cognition 119(2), 301–306 (2011).\n 27. Osiurak, F . & Badets, A. Tool use and affordance: Manipulation-based versus reasoning-based approaches. Psychological Review  \n123(5), 534 (2016).\n 28. Osiurak, F . & Badets, A. Use of tools and misuse of embodied cognition: Reply to Buxbaum (2017). Psychological Review 124(3), \n361–368 (2017).\n 29. Osiurak, F . What neuropsychology tells us about human tool use? The four constraints theory (4CT): mechanics, space, time, and \neffort. Neuropsychology Review 24(2), 88–115 (2014).\n 30. Bar, M. Visual objects in context. Nature Reviews Neuroscience 5(8), 617 (2004).\n 31. Bar, M. et al. The contribution of context to visual object recognition. Journal of Vision 5(8), 88–88 (2005).\n 32. Bar, M. et al. Top-down facilitation of visual recognition. Proceedings of the National Academy of Sciences 103(2), 449–454 (2006).\n 33. Milner, A. D. & Goodale, M. A. Two visual systems re-viewed. Neuropsychologia 46(3), 774–785 (2008).\n 34. Rizzolatti, G. & Matelli, M. Two different streams form the dorsal visual system: anatomy and functions. Experimental Brain \nResearch 153(2), 146–157 (2003).\n 35. Reynaud, E., Lesourd, M., Navarro, J. & Osiurak, F . On the neurocognitive origins of human tool use: A critical review of \nneuroimaging data. Neuroscience & Biobehavioral Reviews 64, 421–437 (2016).\n 36. Roux-Sibilon, A., Kalénine, S., Pichat, C. & Peyrin, C. Dorsal and ventral stream contribution to the paired-object affordance effect. \nNeuropsychologia 112, 125–134 (2018).\n 37. De Bellis, F . et al. Left inferior parietal and posterior temporal cortices mediate the effect of action observation on semantic \nprocessing of objects: evidence from rTMS. Psychological Research, 1-14 (2018).\n 38. Lambon Ralph, M. A. L., Jefferies, E., Patterson, K. & Rogers, T. T. The neural and computational bases of semantic cognition. Nature \nReviews Neuroscience 18(1), 42 (2017).\n 39. Wurm, M. F . & Caramazza, A. Distinct roles of temporal and frontoparietal cortex in representing actions across vision and \nlanguage. Nature Communications 10(1), 289 (2019).\n 40. Ambrosini, E. & Costantini, M. Body posture differentially impacts on visual attention towards tool, graspable, and non-graspable \nobjects. Journal of Experimental Psychology: Human Perception and Performance 43(2), 360 (2017).\n 41. Just, M. A. & Carpenter, P . A. Eye fixations and cognitive processes. Cognitive Psychology 8(4), 441–480 (1976).\n 42. Tucker, M. & Ellis, R. On the relations between seen objects and components of potential actions. Journal of Experimental \nPsychology: Human Perception and Performance 24(3), 830 (1998).\n 43. Mounoud, P ., Duscherer, K., Moy, G. & Perraudin, S. The influence of action perception on object recognition: a developmental \nstudy. Developmental Science 10(6), 836–852 (2007).\n 44. Randerath, J., Li, Y ., Goldenberg, G. & Hermsdörfer, J. Grasping tools: effects of task and apraxia. Neuropsychologia 47(2), 497–505 \n(2009).\n 45. Randerath, J., Goldenberg, G., Spijkers, W ., Li, Y . & Hermsdörfer, J. From pantomime to actual use: how affordances can facilitate \nactual tool-use. Neuropsychologia 49(9), 2410–2416 (2011).\n 46. Randerath, J., Martin, K. R. & Frey, S. H. Are tool properties always processed automatically? The role of tool use context and task \ncomplexity. Cortex 49(6), 1679–1693 (2013).\n 47. Cisek, P . Cortical mechanisms of action selection: the affordance competition hypothesis. Philosophical Transactions of the Royal \nSociety B: Biological Sciences 362(1485), 1585–1599 (2007).\n 48. Beauchamp, M. S. & Martin, A. Grounding object concepts in perception and action: evidence from fMRI studies of tools. Cortex \n43(3), 461–468 (2007).\n 49. Boronat, C. B. et al . Distinctions between manipulation and function knowledge of objects: evidence from functional magnetic \nresonance imaging. Cognitive Brain Research 23(2-3), 361–373 (2005).\n 50. Canessa, N. et al. The different neural correlates of action and functional knowledge in semantic memory: an FMRI study. Cerebral \nCortex 18(4), 740–751 (2007).\n 51. Creem-Regehr, S. H. & Lee, J. N. Neural representations of graspable objects: are tools special? Cognitive Brain Research 22(3), \n457–469 (2005).\n16Scientific  RepoRtS  |         (2020) 10:6157  | https://doi.org/10.1038/s41598-020-63045-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\n 52. Orban, G. A. & Caruana, F . The neural basis of human tool use. Frontiers in Psychology 5, 310 (2014).\n 53. Chen, Q., Garcea, F . E., Jacobs, R. A. & Mahon, B. Z. Abstract representations of object-directed action in the left inferior parietal \nlobule. Cerebral Cortex 28(6), 2162–2174 (2018).\n 54. Reynaud, E., Navarro, J., Lesourd, M. & Osiurak, F . To Watch is to Work: a Review of NeuroImaging Data on Tool Use Observation \nNetwork. Neuropsychology Review, 1–14 (2019).\n 55. Goldenberg, G. & Spatt, J. The neural basis of tool use. Brain 132(6), 1645–1655 (2009).\n 56. Caspers, S. et al. The human inferior parietal lobule in stereotaxic space. Brain Structure and Function 212(6), 481–495 (2008).\n 57. Caspers, S. et al. The human inferior parietal cortex: cytoarchitectonic parcellation and interindividual variability. Neuroimage 33(2), \n430–448 (2006).\n 58. Goldenberg, G. Apraxia: The cognitive side of motor control. (Oxford University Press, 2013).\n 59. Thompson, E. L., Bird, G. & Catmur, C. Conceptualizing and testing action understanding. Neuroscience & Biobehavioral Reviews. \n105, 106–114 (2019).\n 60. Kalénine, S. et al. The sensory-motor specificity of taxonomic and thematic conceptual relations: A behavioral and fMRI study. \nNeuroimage 44(3), 1152–1162 (2009).\n 61. Kalénine, S. & Buxbaum, L. J. Thematic knowledge, artifact concepts, and the left posterior temporal lobe: Where action and object \nsemantics converge. Cortex 82, 164–178 (2016).\n 62. Sass, K., Sachs, O., Krach, S. & Kircher, T. Taxonomic and thematic categories: Neural correlates of categorization in an auditory-to-\nvisual priming task using fMRI. Brain Research 1270, 78–87 (2009).\n 63. Tsagkaridis, K., Watson, C. E., Jax, S. A. & Buxbaum, L. J. The role of action representations in thematic object relations. Frontiers in \nHuman Neuroscience 8, 140 (2014).\n 64. Rogers, T. T. & McClelland, J. L. Semantic cognition: A parallel distributed processing approach. (MIT press, 2004).\n 65. Meteyard, L., Cuadrado, S. R., Bahrami, B. & Vigliocco, G. Coming of age: A review of embodiment and the neuroscience of \nsemantics. Cortex 48(7), 788–804 (2012).\n 66. Koechlin, E. & Summerfield, C. An information theoretical approach to prefrontal executive function. Trends in Cognitive Sciences \n11(6), 229–235 (2007).\n 67. Bortoletto, M. & Cunnington, R. Motor timing and motor sequencing contribute differently to the preparation for voluntary \nmovement. Neuroimage 49(4), 3338–3348 (2010).\n 68. Stadler, W . et al. Predicting and memorizing observed action: differential premotor cortex involvement. Human Brain Mapping  \n32(5), 677–687 (2011).\n 69. Christoff, K. et al. Rostrolateral prefrontal cortex involvement in relational integration during reasoning. Neuroimage  14(5), \n1136–1149 (2001).\n 70. Meteyard, L., Bahrami, B. & Vigliocco, G. Motion detection and motion verbs: Language affects low-level visual perception. \nPsychological Science 18(11), 1007–1013 (2007).\n 71. Bar, M. A cortical mechanism for triggering top-down facilitation in visual object recognition. Journal of Cognitive Neuroscience  \n15(4), 600–609 (2003).\n 72. Buxbaum, L. J., Schwartz, M. F . & Carew, T. G. The role of semantic memory in object use. Cognitive Neuropsychology 14(2), 219–254 \n(1997).\n 73. Osiurak, F . et al. Object utilization and object usage: A single-case study. Neurocase 14(2), 169–183 (2008).\n 74. Oldfield, R. C. The assessment and analysis of handedness: the Edinburgh inventory. Neuropsychologia 9(1), 97–113 (1971).\n 75. Papoutsaki, A. et al. Webgazer: Scalable webcam eye tracking using user interactions. In Proceedings of the Twenty-Fifth \nInternational Joint Conference on Artificial Intelligence-IJCAI 2016 (2016).\n 76. Peirce, J. et al. PsychoPy2: Experiments in behavior made easy. Behavior Research Methods 51(1), 195–203 (2019).\nAuthor contributions\nG.F . and M.A.B. conceived and designed the study and the experiments; G.F . developed the experimental software \nand scripts, conducted the experiments, pilots and setup, analysed the data and prepared the figures; G.F . wrote \nthe paper; M.A.B. revised the manuscript and provided critical comments and theoretical contribution.\ncompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to G.F .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons license, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s) 2020",
  "topic": "Embodied cognition",
  "concepts": [
    {
      "name": "Embodied cognition",
      "score": 0.7102019786834717
    },
    {
      "name": "Cognitive science",
      "score": 0.5454607009887695
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5249046087265015
    },
    {
      "name": "Perception",
      "score": 0.5224446654319763
    },
    {
      "name": "Cognition",
      "score": 0.5216537117958069
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5004229545593262
    },
    {
      "name": "Mainstream",
      "score": 0.4987006187438965
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4646444022655487
    },
    {
      "name": "Computer science",
      "score": 0.449631929397583
    },
    {
      "name": "Psychology",
      "score": 0.4305873513221741
    },
    {
      "name": "Semantic memory",
      "score": 0.4138086140155792
    },
    {
      "name": "Object (grammar)",
      "score": 0.41175681352615356
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3316759467124939
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27193355560302734
    },
    {
      "name": "Neuroscience",
      "score": 0.08751854300498962
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Theology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802142324",
      "name": "Università degli Studi Suor Orsola Benincasa",
      "country": "IT"
    }
  ],
  "cited_by": 42
}