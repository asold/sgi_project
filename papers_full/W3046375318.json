{
    "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
    "url": "https://openalex.org/W3046375318",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2011452039",
            "name": "Gu Yu",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4221579193",
            "name": "Tinn, Robert",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2035056196",
            "name": "Cheng Hao",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4209124084",
            "name": "Lucas, Michael",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4221580594",
            "name": "Usuyama, Naoto",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1971393576",
            "name": "Liu XiaoDong",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4221580601",
            "name": "Naumann, Tristan",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2119363152",
            "name": "Gao Jian-feng",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4221580603",
            "name": "Poon, Hoifung",
            "affiliations": [
                "Microsoft (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2145383760",
        "https://openalex.org/W2736047977",
        "https://openalex.org/W2174775663",
        "https://openalex.org/W2964022985",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2136437513",
        "https://openalex.org/W2793862696",
        "https://openalex.org/W2913340405",
        "https://openalex.org/W2743028754",
        "https://openalex.org/W2900758626",
        "https://openalex.org/W2034269086",
        "https://openalex.org/W2170189740",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W2295072214",
        "https://openalex.org/W6676573207",
        "https://openalex.org/W2075201173",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2955483668",
        "https://openalex.org/W2154142897",
        "https://openalex.org/W2735784619",
        "https://openalex.org/W2951036431",
        "https://openalex.org/W2114315281",
        "https://openalex.org/W2765742249",
        "https://openalex.org/W2964179635",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W154351976",
        "https://openalex.org/W4238634189",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W4287813862",
        "https://openalex.org/W3039677769",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2158049734",
        "https://openalex.org/W4301409532",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2949460468",
        "https://openalex.org/W1905522558",
        "https://openalex.org/W2962859618",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2047782770",
        "https://openalex.org/W2112227057",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W4234482043",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2888041867",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3017003177",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W4237040408",
        "https://openalex.org/W3035763680"
    ],
    "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning Benchmark) at https://aka.ms/BLURB .",
    "full_text": "1\nDomain-Specific Language Model Pretraining for Biomedical\nNatural Language Processing\nYU GU‚àó, ROBERT TINN‚àó, HAO CHENG‚àó, MICHAEL LUCAS, NAOTO USUYAMA, XIAODONG\nLIU, TRISTAN NAUMANN, JIANFENG GAO, and HOIFUNG POON, Microsoft Research\nPretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing\n(NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing\nassumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this\npaper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine,\npretraining language models from scratch results in substantial gains over continual pretraining of general-domain language\nmodels. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available\ndatasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical\nNLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling\nchoices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with\nBERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a\nleaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at\nhttps://aka.ms/BLURB.\nCCS Concepts: ‚Ä¢ Computing methodologies‚ÜíNatural language processing; ‚Ä¢ Applied computing‚ÜíBioinformat-\nics.\nAdditional Key Words and Phrases: Biomedical, NLP, Domain-specific pretraining\nACM Reference Format:\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung\nPoon. 2021. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. 1, 1, Article 1\n(January 2021), 24 pages. https://doi.org/10.1145/3458754\n1 INTRODUCTION\nIn natural language processing (NLP), pretraining large neural language models on unlabeled text has proven\nto be a successful strategy for transfer learning. A prime example is Bidirectional Encoder Representations\nfrom Transformers (BERT) [16], which has become a standard building block for training task-specific NLP\nmodels. Existing pretraining work typically focuses on the newswire and Web domains. For example, the original\nBERT model was trained on Wikipedia1 and BookCorpus [62], and subsequent efforts have focused on crawling\nadditional text from the Web to power even larger-scale pretraining [39, 50].\n‚àóThese authors contributed equally to this research.\n1http://wikipedia.org\nAuthors‚Äô address: Yu Gu, Aiden.Gu@microsoft.com; Robert Tinn, Robert.Tinn@microsoft.com; Hao Cheng, chehao@microsoft.com; Michael\nLucas, Michael.Lucas@microsoft.com; Naoto Usuyama, naotous@microsoft.com; Xiaodong Liu, xiaodl@microsoft.com; Tristan Naumann,\ntristan@microsoft.com; Jianfeng Gao, jfgao@microsoft.com; Hoifung Poon, hoifung@microsoft.com, Microsoft Research, One Microsoft\nWay, Redmond, WA, 98052.\n¬© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author‚Äôs version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was\npublished in , https://doi.org/10.1145/3458754.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\narXiv:2007.15779v6  [cs.CL]  16 Sep 2021\n1:2 ‚Ä¢ Gu, Tinn, Cheng, et al.\nMixed-Domain Pretraining\nVocab\nGeneral\nBERT\nBiomed\nBERT\nVocab\nBiomed\nBERT\nDomain-Specific Pretraining from Scratch\nText Source\nFig. 1. Two paradigms for neural language model pretraining. Top: The prevailing mixed-domain paradigm assumes that\nout-domain text is still helpful and typically initializes domain-specific pretraining with a general-domain language model and\ninherits its vocabulary. Bottom: Domain-specific pretraining from scratch derives the vocabulary and conducts pretraining\nusing solely in-domain text. In this paper, we show that for domains with abundant text such as biomedicine, domain-specific\npretraining from scratch can substantially outperform the conventional mixed-domain approach.\nIn specialized domains like biomedicine, past work has shown that using in-domain text can provide additional\ngains over general-domain language models [8, 34, 45]. However, a prevailing assumption is that out-domain text\nis still helpful and previous work typically adopts a mixed-domain approach, e.g., by starting domain-specific\npretraining from an existing general-domain language model (Figure 1 top). In this paper, we question this\nassumption. We observe that mixed-domain pretraining such as continual pretraining can be viewed as a form of\ntransfer learning in itself, where the source domain is general text, such as newswire and the Web, and the target\ndomain is specialized text such as biomedical papers. Based on the rich literature of multi-task learning and\ntransfer learning [4, 13, 38, 59], successful transfer learning occurs when the target data is scarce and the source\ndomain is highly relevant to the target one. For domains with abundant unlabeled text such as biomedicine, it is\nunclear that domain-specific pretraining can benefit by transfer from general domains. In fact, the majority of\ngeneral domain text is substantively different from biomedical text, raising the prospect of negative transfer that\nactually hinders the target performance.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:3\nWe thus set out to conduct a rigorous study on domain-specific pretraining and its impact on downstream\napplications, using biomedicine as a running example. We show that domain-specific pretraining from scratch\nsubstantially outperforms continual pretraining of generic language models, thus demonstrating that the prevailing\nassumption in support of mixed-domain pretraining is not always applicable (Figure 1).\nTo facilitate this study, we compile a comprehensive biomedical NLP benchmark from publicly-available\ndatasets, and conduct in-depth comparisons of modeling choices for pretraining and task-specific fine-tuning\nby their impact on domain-specific applications. Our experiments show that domain-specific pretraining from\nscratch can provide a solid foundation for biomedical NLP, leading to new state-of-the-art performance across a\nwide range of tasks. Additionally, we discover that the use of transformer-based models, like BERT, necessitates\nrethinking several common practices. For example, BIO tags and more complex variants are the standard label\nrepresentation for named entity recognition (NER). However, we find that simply using IO (in or out of entity\nmentions) suffices with BERT models, leading to comparable or better performance.\nTo help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-\nspecific models for the community, and created a leaderboard featuring our comprehensive benchmark at\nhttps://aka.ms/BLURB.\n2 METHODS\n2.1 Language Model Pretraining\nIn this section, we provide a brief overview of neural language model pretraining, using BERT [16] as a running\nexample.\n2.1.1 Vocabulary. We assume that the input consists of text spans, such as sentences separated by special\ntokens [SEP]. To address the problem of out-of-vocabulary words, neural language models generate a vocabulary\nfrom subword units, using Byte-Pair Encoding (BPE) [51] or variants such as WordPiece [32]. Essentially, the\nBPE algorithm tries to greedily identify a small set of subwords that can compactly form all words in the given\ncorpus. It does this by first shattering all words in the corpus and initializing the vocabulary with characters and\ndelimiters. It then iteratively augments the vocabulary with a new subword that is most frequent in the corpus\nand can be formed by concatenating two existing subwords, until the vocabulary reaches the pre-specified size\n(e.g., 30,000 in standard BERT models or 50,000 in RoBERTa [39]). In this paper, we use the WordPiece algorithm\nwhich is a BPE variant that uses likelihood based on the unigram language model rather than frequency in\nchoosing which subwords to concatenate. The text corpus and vocabulary may preserve the case ( cased) or\nconvert all characters to lower case (uncased).\n2.1.2 Model Architecture. State-of-the-art neural language models are generally based on transformer archi-\ntectures [55], following the recent success of BERT [16, 39]. The transformer model introduces a multi-layer,\nmulti-head self-attention mechanism, which has demonstrated superiority in leveraging GPU-based parallel\ncomputation and modeling long-range dependencies in texts, compared to recurrent neural networks, such as\nLSTMs [22]. The input token sequence is first processed by a lexical encoder, which combines a token embed-\nding, a (token) position embedding and a segment embedding (i.e., which text span the token belongs to) by\nelement-wise summation. This embedding layer is then passed to multiple layers of transformer modules [55].\nIn each transformer layer, a contextual representation is generated for each token by summing a non-linear\ntransformation of the representations of all tokens in the prior layer, weighted by the attentions computed using\nthe given token‚Äôs representation in the prior layer as the query. The final layer outputs contextual representations\nfor all tokens, which combine information from the whole text span.\n2.1.3 Self-Supervision. A key innovation in BERT [16] is the use of a Masked Language Model (MLM)for\nself-supervised pretraining. Traditional language models are typically generative models that predict the next\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:4 ‚Ä¢ Gu, Tinn, Cheng, et al.\ntoken based on the preceding tokens; for example, n-gram models represent the conditional probability of\nthe next token by a multinomial of the preceding n-gram, with various smoothing strategies to handle rare\noccurrences [43]. Masked Language Model instead randomly replaces a subset of tokens by a special token (e.g.,\n[MASK]), and asks the language model to predict them. The training objective is the cross-entropy loss between the\noriginal tokens and the predicted ones. In BERT and RoBERTa, 15% of the input tokens are chosen, among which\na random 80% are replaced by [MASK], 10% are left unchanged and 10% are randomly replaced by a token from the\nvocabulary. Instead of using a constant masking rate of 15%, a standard approach is to gradually increase it from\n5% to 25% with 5% increment for every 20% of training epochs, which makes pretraining more stable [37]. The\noriginal BERT algorithm also usesNext Sentence Prediction (NSP), which determines for a given sentence pair\nwhether one sentence follows the other in the original text. The utility of NSP has been called into question [39],\nbut we include it in our pretraining experiments to enable a head-to-head comparison with prior BERT models.\n2.1.4 Advanced Pretraining Techniques. In the original formulation of BERT [16], the masked language model\n(MLM) simply selects random subwords to mask. When a word is only partially masked, it is relatively easy to\npredict the masked portion given the observed ones. In contrast, whole-word masking (WWM) enforces that the\nwhole word must be masked if one of its subwords is chosen. This has been adopted as the standard approach\nbecause it forces the language model to capture more contextual semantic dependencies.\nIn this paper, we also explore adversarial pretraining and its impact on downstream applications. Motivated by\nsuccesses in countering adversarial attacks in computer vision, adversarial pretraining introduces perturbations\nin the input embedding layer that maximize the adversarial loss, thus forcing the model to not only optimize the\nstandard training objective (MLM), but also minimize adversarial loss [37].\n2.2 Biomedical Language Model Pretraining\nIn this paper, we will use biomedicine as a running example in our study of domain-specific pretraining. In\nother words, biomedical text is considered in-domain, while others are regarded as out-domain. Intuitively, using\nin-domain text in pretraining should help with domain-specific applications. Indeed, prior work has shown that\npretraining with PubMed text leads to better performance in biomedical NLP tasks [8, 34, 45]. The main question\nis whether pretraining should include text from other domains. The prevailing assumption is that pretraining can\nalways benefit from more text, including out-domain text. In fact, none of the prior biomedical-related BERT\nmodels have been pretrained using purely biomedical text [8, 34, 45]. Here, we challenge this assumption and\nshow that domain-specific pretraining from scratch can be superior to mixed-domain pretraining for downstream\napplications.\n2.2.1 Mixed-Domain Pretraining. The standard approach to pretraining a biomedical BERT model conducts\ncontinual pretraining of a general-domain pretrained model, as exemplified by BioBERT [34]. Specifically, this\napproach would initialize with the standard BERT model [ 16], pretrained using Wikipedia and BookCorpus.\nIt then continues the pretraining process with MLM and NSP using biomedical text. In the case of BioBERT,\ncontinual pretraining is conducted using PubMed abstracts and PubMed Central full text articles. BlueBERT [45]\nuses both PubMed text and de-identified clinical notes from MIMIC-III [26].\nNote that in the continual pretraining approach, the vocabulary is the same as the original BERT model, in this\ncase the one generated from Wikipedia and BookCorpus. While convenient, this is a major disadvantage for this\napproach, as the vocabulary is not representative of the target biomedical domain.\nCompared to the other biomedical-related pretraining efforts, SciBERT [8] is a notable exception as it generates\nthe vocabulary and pretrains from scratch, using biomedicine and computer science as representatives for scientific\nliterature. However, from the perspective of biomedical applications, SciBERT still adopts the mixed-domain\npretraining approach, as computer science text is clearly out-domain.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:5\n2.2.2 Domain-Specific Pretraining from Scratch. The mixed-domain pretraining approach makes sense if the\ntarget application domain has little text of its own, and can thereby benefit from pretraining using related domains.\nHowever, this is not the case for biomedicine, which has over thirty million abstracts in PubMed, and adds\nover a million each year. We thus hypothesize that domain-specific pretraining from scratch is a better strategy for\nbiomedical language model pretraining.\nBiomedical Term Category BERT SciBERT PubMedBERT (Ours)\ndiabetes disease ‚úì ‚úì ‚úì\nleukemia disease ‚úì ‚úì ‚úì\nlithium drug ‚úì ‚úì ‚úì\ninsulin drug ‚úì ‚úì ‚úì\nDNA gene ‚úì ‚úì ‚úì\npromoter gene ‚úì ‚úì ‚úì\nhypertension disease hyper-tension ‚úì ‚úì\nnephropathy disease ne-ph-rop-athy ‚úì ‚úì\nlymphoma disease l-ym-ph-oma ‚úì ‚úì\nlidocaine drug lid-oca-ine] ‚úì ‚úì\noropharyngeal organ oro-pha-ryn-ge-al or-opharyngeal ‚úì\ncardiomyocyte cell card-iom-yo-cy-te cardiomy-ocyte ‚úì\nchloramphenicol drug ch-lor-amp-hen-ico-l chlor-amp-hen-icol ‚úì\nRecA gene Rec-A Rec-A ‚úì\nacetyltransferase gene ace-ty-lt-ran-sf-eras-e acetyl-transferase ‚úì\nclonidine drug cl-oni-dine clon-idine ‚úì\nnaloxone drug na-lo-xon-e nal-oxo-ne ‚úì\nTable 1. Comparison of common biomedical terms in vocabularies used by the standard BERT, SciBERT and PubMedBERT\n(ours). A ‚úì indicates the biomedical term appears in the corresponding vocabulary, otherwise the term will be broken into\nword pieces as separated by hyphen. These word pieces often have no biomedical relevance and may hinder learning in\ndownstream tasks.\nA major advantage of domain-specific pretraining from scratch stems from having an in-domain vocabulary.\nTable 1 compares the vocabularies used in various pretraining strategies. BERT models using continual pretraining\nare stuck with the original vocabulary from the general-domain corpora, which does not contain many common\nbiomedical terms. Even for SciBERT, which generates its vocabulary partially from biomedical text, the deficiency\ncompared to a purely biomedical vocabulary is substantial. As a result, standard BERT models are forced to divert\nparametrization capacity and training bandwidth to model biomedical terms using fragmented subwords. For\nexample, naloxone, a common medical term, is divided into four pieces ([na, ##lo, ##xon, ##e]) by BERT, and\nacetyltransferase is shattered into seven pieces ([ace, ##ty, ##lt, ##ran, ##sf, ##eras, ##e]) by BERT.2 Both terms\nappear in the vocabulary of PubMedBERT.\nAnother advantage of domain-specific pretraining from scratch is that the language model is trained using\npurely in-domain data. For example, SciBERT pretraining has to balance optimizing for biomedical text and\ncomputer science text, the latter of which is unlikely to be beneficial for biomedical applications. Continual\npretraining, on the other hand, may potentially recover from out-domain modeling, though not completely. Aside\nfrom the vocabulary issue mentioned earlier, neural network training uses non-convex optimization, which means\n2Prior work also observed similar shattering for clinical words [52].\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:6 ‚Ä¢ Gu, Tinn, Cheng, et al.\nthat continual pretraining may not be able to completely undo suboptimal initialization from the general-domain\nlanguage model.\nIn our experiments, we show that domain-specific pretraining with in-domain vocabulary confers clear\nadvantages over mixed-domain pretraining, be it continual pretraining of general-domain language models, or\npretraining on mixed-domain text.\n2.3 BLURB: A Comprehensive Benchmark for Biomedical NLP\nBioBERT [34] SciBERT [8] BLUE [45] BLURB\nBC5-chem [35] ‚úì ‚úì ‚úì ‚úì\nBC5-disease [35] ‚úì ‚úì ‚úì ‚úì\nNCBI-disease [18] ‚úì ‚úì - ‚úì\nBC2GM [53] ‚úì - - ‚úì\nJNLPBA [27] ‚úì - - ‚úì\nEBM PICO [44] - ‚úì - ‚úì\nChemProt [31] ‚úì ‚úì ‚úì ‚úì\nDDI [21] ‚úì - ‚úì ‚úì\nGAD [11] ‚úì - - ‚úì\nBIOSSES [54] - - ‚úì ‚úì\nHoC [20] - - ‚úì ‚úì\nPubMedQA [25] - - - ‚úì\nBioASQ [42] ‚úì - - ‚úì\nTable 2. Comparison of the biomedical datasets in prior language model pretraining studies and BLURB.\nThe ultimate goal of language model pretraining is to improve performance on a wide range of downstream\napplications. In general-domain NLP, the creation of comprehensive benchmarks, such as GLUE [56, 57], greatly\naccelerates advances in language model pretraining by enabling head-to-head comparisons among pretrained\nlanguage models. In contrast, prior work on biomedical pretraining tends to use different tasks and datasets for\ndownstream evaluation, as shown in Table 2. This makes it hard to assess the impact of pretrained language\nmodels on the downstream tasks we care about. To the best of our knowledge, BLUE [45] is the first attempt\nto create an NLP benchmark in the biomedical domain. We aim to improve on its design by addressing some\nof its limitations. First, BLUE has limited coverage of biomedical applications used in other recent work on\nbiomedical language models, as shown in Table 2. For example, it does not include any question-answering task.\nMore importantly, BLUE mixes PubMed-based biomedical applications (six datasets such as BC5, ChemProt,\nand HoC) with MIMIC-based clinical applications (four datasets such as i2b2 and MedNLI). Clinical notes differ\nsubstantially from biomedical literature, to the extent that we observe BERT models pretrained on clinical notes\nperform poorly on biomedical tasks, similar to the standard BERT. Consequently, it is advantageous to create\nseparate benchmarks for these two domains.\nTo facilitate investigations of biomedical language model pretraining and help accelerate progress in biomedical\nNLP, we create a new benchmark, the Biomedical Language Understanding & Reasoning Benchmark (BLURB) .\nWe focus on PubMed-based biomedical applications, and leave the exploration of the clinical domain, and other\nhigh-value verticals to future work. To make our effort tractable and facilitate head-to-head comparison with\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:7\nprior work, we prioritize the selection of datasets used in recent work on biomedical language models, and will\nexplore the addition of other datasets in future work.\nDataset Task Train Dev Test Evaluation Metrics\nBC5-chem NER 5203 5347 5385 F1 entity-level\nBC5-disease NER 4182 4244 4424 F1 entity-level\nNCBI-disease NER 5134 787 960 F1 entity-level\nBC2GM NER 15197 3061 6325 F1 entity-level\nJNLPBA NER 46750 4551 8662 F1 entity-level\nEBM PICO PICO 339167 85321 16364 Macro F1 word-level\nChemProt Relation Extraction 18035 11268 15745 Micro F1\nDDI Relation Extraction 25296 2496 5716 Micro F1\nGAD Relation Extraction 4261 535 534 Micro F1\nBIOSSES Sentence Similarity 64 16 20 Pearson\nHoC Document Classification 1295 186 371 Micro F1\nPubMedQA Question Answering 450 50 500 Accuracy\nBioASQ Question Answering 670 75 140 Accuracy\nTable 3. Datasets used in the BLURB biomedical NLP benchmark. We list the numbers of instances in train, dev, and test\n(e.g., entity mentions in NER and PICO elements in evidence-based medical information extraction).\nBLURB is comprised of a comprehensive set of biomedical NLP tasks from publicly available datasets, including\nnamed entity recognition (NER), evidence-based medical information extraction (PICO), relation extraction,\nsentence similarity, document classification, and question answering. See Table 3 for an overview of the BLURB\ndatasets. For question answering, prior work has considered both classification tasks (e.g., whether a reference\ntext contains the answer to a given question) and more complex tasks such as list and summary [42]. The latter\ntypes often require additional engineering effort that are not relevant to evaluating neural language models.\nFor simplicity, we focus on the classification tasks such as yes/no question-answering in BLURB, and leave the\ninclusion of more complex question-answering to future work.\nTo compute a summary score for BLURB, the simplest way is to report the average score among all tasks.\nHowever, this may place undue emphasis on simpler tasks such as NER for which there are many existing datasets.\nTherefore, we group the datasets by their task types, compute the average score for each task type, and report\nthe macro average among the task types. To help accelerate research in biomedical NLP, we release the BLURB\nbenchmark as well as a leaderboard at http://aka.ms/BLURB.\nBelow are detailed descriptions for each task and corresponding datasets.\n2.3.1 Named Entity Recognition (NER).\nBC5-Chemical & BC5-Disease. The BioCreative V Chemical-Disease Relation corpus [ 35] was created for\nevaluating relation extraction of drug-disease interactions, but is frequently used as a NER corpus for detecting\nchemical (drug) and disease entities. The dataset consists of 1500 PubMed abstracts broken into three even splits\nfor training, development, and test. We use a pre-processed version of this dataset generated by Crichton et al.\n[14], discard the relation labels, and train NER models for chemical (BC5-Chemical) and disease (BC5-Disease)\nseparately.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:8 ‚Ä¢ Gu, Tinn, Cheng, et al.\nNCBI-Disease. The Natural Center for Biotechnology Information Disease corpus [18] contains 793 PubMed\nabstracts with 6892 annotated disease mentions linked to 790 distinct disease entities. We use a pre-processed set\nof train, development, test splits generated by Crichton et al. [14].\nBC2GM. The Biocreative II Gene Mention corpus [ 53] consists of sentences from PubMed abstracts with\nmanually labeled gene and alternative gene entities. Following prior work, we focus on the gene entity annotation.\nIn its original form, BC2GM contains 15000 train and 5000 test sentences. We use a pre-processed version of the\ndataset generated by Crichton et al. [14], which carves out 2500 sentences from the training data for development.\nJNLPBA. The Joint Workshop on Natural Language Processing in Biomedicine and its Applications shared\ntask [27] is a NER corpus on PubMed abstracts. The entity types are chosen for molecular biology applications:\nprotein, DNA, RNA, cell line, and cell type. Some of the entity type distinctions are not very meaningful. For\nexample, a gene mention often refers to both the DNA and gene products such as the RNA and protein. Following\nprior work that evaluates on this dataset [34], we ignore the type distinction and focus on detecting the entity\nmentions. We use the same train, development, and test splits as in Crichton et al. [14].\n2.3.2 Evidence-Based Medical Information Extraction (PICO).\nEBM PICO. The Evidence-Based Medicine corpus [44] contains PubMed abstracts on clinical trials, where each\nabstract is annotated with P, I, and O in PICO: Participants (e.g.,diabetic patients), Intervention (e.g.,insulin),\nComparator (e.g., placebo) and Outcome (e.g., blood glucose levels). Comparator (C) labels are omitted as\nthey are standard in clinical trials: placebo for passive control and standard of care for active control. There are\n4300, 500, and 200 abstracts in training, development, and test, respectively. The training and development sets\nwere labeled by Amazon Mechanical Turkers, whereas the test set was labeled by Upwork contributors with prior\nmedical training. EBM PICO provides labels at the word level for each PIO element. For each of the PIO elements\nin an abstract, we tally the F1 score at the word level, and then compute the final score as the average among\nPIO elements in the dataset. Occasionally, two PICO elements might overlap with each other (e.g., a participant\nspan might contain within it an intervention span). In EBM-PICO, about 3% of the PIO words are in the overlap.\nNote that the dataset released along with SciBERT appears to remove the overlapping words from the larger\nspan (e.g., the participant span as mentioned above). We instead use the original dataset [44] and their scripts for\npreprocessing and evaluation.\n2.3.3 Relation Extraction.\nChemProt. The Chemical Protein Interaction corpus [31] consists of PubMed abstracts annotated with chemical-\nprotein interactions between chemical and protein entities. There are 23 interactions organized in a hierar-\nchy, with 10 high-level interactions (including NONE). The vast majority of relation instances in ChemProt are\nwithin single sentences. Following prior work [8, 34], we only consider sentence-level instances. We follow the\nChemProt authors‚Äô suggestions and focus on classifying five high-level interactions ‚Äî UPREGULATOR (CPR : 3),\nDOWNREGULATOR (CPR : 4), AGONIST (CPR : 5), ANTAGONIST (CPR : 6), SUBSTRATE (CPR : 9)‚Äî as well as every-\nthing else (false). The ChemProt annotation is not exhaustive for all chemical-protein pairs. Following previous\nwork [34, 45], we expand the training and development sets by assigning a false label for all chemical-protein\npairs that occur in a training or development sentence, but do not have an explicit label in the ChemProt corpus.\nNote that prior work uses slightly different label expansion of the test data. To facilitate head-to-head comparison,\nwe will provide instructions for reproducing the test set in BLURB from the original dataset.\nDDI. The Drug-Drug Interaction corpus [21] was created to facilitate research on pharmaceutical information\nextraction, with a particular focus on pharmacovigilance. It contains sentence-level annotation of drug-drug\ninteractions on PubMed abstracts. Note that some prior work [45, 61] discarded 90 training files that the authors\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:9\nconsidered not conducive to learning drug-drug interactions. We instead use the original dataset and produce\nour train/dev/test split of 624/90/191 files.\nGAD. The Genetic Association Database corpus [11] was created semi-automatically using the Genetic Asso-\nciation Archive.3 Specifically, the archive contains a list of gene-disease associations, with the corresponding\nsentences in the PubMed abstracts reporting the association studies. Bravo et al. [ 11] used a biomedical NER\ntool to identify gene and disease mentions, and create the positive examples from the annotated sentences in the\narchive, and negative examples from gene-disease co-occurrences that were not annotated in the archive. We use\nan existing preprocessed version of GAD and its corresponding train/dev/test split created by Lee et al. [34].\n2.3.4 Sentence Similarity.\nBIOSSES. The Sentence Similarity Estimation System for the Biomedical Domain [54] contains 100 pairs of\nPubMed sentences each of which is annotated by five expert-level annotators with an estimated similarity score\nin the range from 0 (no relation) to 4 (equivalent meanings). It is a regression task, with the average score as\nthe final annotation. We use the same train/dev/test split in Peng et al . [45] and use Pearson correlation for\nevaluation.\n2.3.5 Document Classification.\nHoC. The Hallmarks of Cancer corpus was motivated by the pioneering work on cancer hallmarks [ 20]. It\ncontains annotation on PubMed abstracts with binary labels each of which signifies the discussion of a specific\ncancer hallmark. The authors use 37 fine-grained hallmarks which are grouped into ten top-level ones. We focus\non predicting the top-level labels. The dataset was released with 1499 PubMed abstracts [6] and has since been\nexpanded to 1852 abstracts [5]. Note that Peng et al. [45] discarded a control subset of 272 abstracts that do not\ndiscuss any cancer hallmark (i.e., all binary labels are false). We instead adopt the original dataset and report\nmicro F1 across the ten cancer hallmarks. Though the original dataset provided sentence level annotation, we\nfollow the common practice and evaluate on the abstract level [19, 60]. We create the train/dev/test split, as they\nare not available previously.4\n2.3.6 Question Answering (QA).\nPubMedQA. The PubMedQA dataset [25] contains a set of research questions, each with a reference text from\na PubMed abstract as well as an annotated label of whether the text contains the answer to the research question\n(yes/maybe/no). We use the original train/dev/test split with 450, 50, and 500 questions, respectively.\nBioASQ. The BioASQ corpus [42] contains multiple question answering tasks annotated by biomedical experts,\nincluding yes/no, factoid, list, and summary questions. Pertaining to our objective of comparing neural language\nmodels, we focus on the the yes/no questions (Task 7b), and leave the inclusion of other tasks to future work.\nEach question is paired with a reference text containing multiple sentences from a PubMed abstract and a yes/no\nanswer. We use the official train/dev/test split of 670/75/140 questions.\n2.4 Task-Specific Fine-Tuning\nPretrained neural language models provide a unifying foundation for learning task-specific models. Given an\ninput token sequence, the language model produces a sequence of vectors in the contextual representation. A\ntask-specific prediction model is then layered on top to generate the final output for a task-specific application.\n3http://geneticassociationdb.nih.gov/\n4The original authors used cross-validation for their evaluation.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:10 ‚Ä¢ Gu, Tinn, Cheng, et al.\nNeural Language Model (e.g., BERT)\nTransform Input (e.g., replace entities by dummy tokens)\n‚Ä¶ùë≤ùëπùë®ùë∫mutationisamediatorofùëªùíÇùíçùíÇùíõùíêùíëùíÇùíìùíäùíÉ resistance ‚Ä¶\n‚Ä¶$ùëÆùë¨ùëµùë¨mutationisamediatorof$ùë´ùëπùëºùëÆresistance ‚Ä¶\nContextual\nRepresentation\nPreprocessing\nFeaturizer (e.g. concatenation of entity vectors)\nPredict\nFig. 2. A general architecture for task-specific fine-tuning of neural language models, with a relation-extraction example.\nNote that the input goes through additional processing such as word-piece tokenization in the neural language model\nmodule.\nGiven task-specific training data, we can learn the task-specific model parameters and refine the BERT model\nparameters by gradient descent using backpropragation.\nPrior work on biomedical NLP often adopts different task-specific models and fine-tuning methods, which\nmakes it difficult to understand the impact of an underlying pretrained language model on task performance. In\nthis section, we review standard methods and common variants used for each task. In our primary investigation\ncomparing pretraining strategies, we fix the task-specific model architecture using the standard method identifed\nhere, to facilitate a head-to-head comparison among the pretrained neural language models. Subsequently, we\nstart with the same pretrained BERT model, and conduct additional investigation on the impact for the various\nchoices in the task-specific models. For prior biomedical BERT models, our standard task-specific methods\ngenerally lead to comparable or better performance when compared to their published results.\n2.4.1 A General Architecture for Fine-Tuning Neural Language Models. Figure 2 shows a general architecture\nof fine-tuning neural language models for downstream applications. An input instance is first processed by\na TransformInput module which performs task-specific transformations such as appending special instance\nmarker (e.g., [CLS]) or dummifying entity mentions for relation extraction. The transformed input is then\ntokenized using the neural language model‚Äôs vocabulary, and fed into the neural language model. Next, the\ncontextual representation at the top layer is processed by a Featurizer module, and then fed into the Predict\nmodule to generate the final output for a given task.\nTo facilitate a head-to-head comparison, we apply the same fine-tuning procedure for all BERT models and\ntasks. Specifically, we use cross-entropy loss for classification tasks and mean-square error for regression tasks.\nWe conduct hyperparameter search using the development set based on task-specific metrics. Similar to previous\nwork, we jointly fine-tune the parameters of the task-specific prediction layer as well as the underlying neural\nlanguage model.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:11\nTask Problem Formulation Modeling Choices\nNER Token Classification Tagging Scheme, Classification Layer\nPICO Token Classification Tagging Scheme, Classification Layer\nRelation Extraction Sequence Classification Entity/Relation Representation, Classification Layer\nSentence Similarity Sequence Regression Sentence Representation, Regression Loss\nDocument Classification Sequence Classification Document Representation, Classification Layer\nQuestion Answering Sequence Classification Question/Text Representation, Classification Layer\nTable 4. Standard NLP tasks and their problem formulations and modeling choices.\n2.4.2 Task-Specific Problem Formulation and Modeling Choices. Many NLP applications can be formulated as a\nclassification or regression task, wherein either individual tokens or sequences are the prediction target. Modeling\nchoices usually vary in two aspects: the instance representation and the prediction layer. Table 4 presents an\noverview of the problem formulation and modeling choices for tasks we consider and detailed descriptions are\nprovided below. For each task, we highlight the standard modeling choices with an asterisk (*).\nNER. Given an input text span (usually a sentence), the NER task seeks to recognize mentions of entities of\ninterest. It is typically formulated as a sequential labeling task, where each token is assigned a tag to signify\nwhether it is in an entity mention or not. The modeling choices primarily vary on the tagging scheme and\nclassification method. BIO is the standard tagging scheme that classifies each token as the beginning of an\nentity (B), inside an entity (I), or outside (O). The NER tasks in BLURB are only concerned about one entity type\n(in JNLPBA, all the types are merged into one). In the case when there are multiple entity types, theBI tags would\nbe further divided into fine-grained tags for specific types. Prior work has also considered more complex tagging\nschemes such as BIOUL, where U stands for the last word of an entity and L stands for a single-word entity. We\nalso consider the simpler IO scheme that only differentiates between in and out of an entity. Classification is\ndone using a simple linear layer or more sophisticated sequential labeling methods such as LSTM or conditional\nrandom field (CRF) [33].\n‚Ä¢TransformInput: returns the input sequence as is.\n‚Ä¢Featurizer: returns the BERT encoding of a given token.\n‚Ä¢Tagging scheme: BIO*; BIOUL; IO.\n‚Ä¢Classification layer: linear layer*; LSTM; CRF.\nPICO. Conceptually, evidence-based medical information extraction is akin to slot filling, as it tries to identify\nthe PIO elements in an abstract describing a clinical trial. However, it can be formulated as a sequential tagging\ntask like NER, by classifying tokens belonging to each element. A token may belong to more than one element,\ne.g., participant (P) and intervention (I).\n‚Ä¢TransformInput: returns the input sequence as is.\n‚Ä¢Featurizer: returns the BERT encoding of a given token.\n‚Ä¢Tagging scheme: BIO*; BIOUL; IO.\n‚Ä¢Classification layer: linear layer*; LSTM; CRF.\nRelation Extraction. Existing work on relation extraction tends to focus on binary relations. Given a pair of\nentity mentions in a text span (typically a sentence), the goal is to determine if the text indicates a relation for the\nmention pair. There are significant variations in the entity and relation representations. To prevent overfitting\nby memorizing the entity pairs, the entity tokens are often augmented with start/end markers or replaced by\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:12 ‚Ä¢ Gu, Tinn, Cheng, et al.\na dummy token. For featurization, the relation instance is either represented by a special [CLS]token, or by\nconcatenating the mention representations. In the latter case, if an entity mention contains multiple tokens, its\nrepresentation is usually produced by pooling those of individual tokens (max or average). For computational\nefficiency, we use padding or truncation to set the input length to 128 tokens for GAD and 256 tokens for\nChemProt and DDI which contain longer input sequences.\n‚Ä¢TransformInput: entity (dummification*; start/end marker; original); relation ([CLS]*; original).\n‚Ä¢Featurizer: entity (dummy token*; pooling); relation ([CLS]BERT encoding*; concatenation of the mention\nBERT encoding).\n‚Ä¢Classification layer: linear layer*; more sophisticated classifiers (e.g., MLP).\nSentence Similarity. The similarity task can be formulated as a regression problem to generate a normalized\nscore for a sentence pair. By default, a special [SEP]token is inserted to separate the two sentences, and a special\n[CLS]token is prepended to the beginning to represent the pair. The BERT encoding of [CLS]is used to compute\nthe regression score.\n‚Ä¢TransformInput: [CLS]ùëÜ1 [SEP]ùëÜ2 [SEP], for sentence pair ùëÜ1,ùëÜ2.\n‚Ä¢Featurizer: [CLS]BERT encoding.\n‚Ä¢Regression layer: linear regression.\nDocument Classification. For each text span and category (an abstract and a cancer hallmark in HoC), the goal\nis to classify whether the text belongs to the category. By default, a [CLS]token is appended to the beginning of\nthe text, and its BERT encoding is passed on by the Featurizer for the final classification, which typically uses a\nsimple linear layer.\n‚Ä¢TransformInput: [CLS]ùê∑ [SEP], for document ùê∑.\n‚Ä¢Featurizer: returns [CLS]BERT encoding.\n‚Ä¢Classification layer: linear layer.\nQuestion Answering. For the two-way (yes/no) or three-way (yes/maybe/no) question-answering task, the\nencoding is similar to the sentence similarity task. Namely, a[CLS]token is prepended to the beginning, followed\nby the question and reference text, with a [SEP]token to separate the two text spans. The [CLS]BERT encoding\nis then used for the final classification. For computational efficiency, we use padding or truncation to set the\ninput length to 512 tokens.\n‚Ä¢TransformInput: [CLS]ùëÑ [SEP]ùëá [SEP], for question ùëÑ and reference text ùëá.\n‚Ä¢Featurizer: returns [CLS]BERT encoding.\n‚Ä¢Classification layer: linear layer.\n2.5 Experimental Settings\nFor biomedical domain-specific pretraining, we generate the vocabulary and conduct pretraining using the latest\ncollection of PubMed5 abstracts: 14 million abstracts, 3.2 billion words, 21 GB. (The original collection contains\nover 4 billion words; we filter out any abstracts with less than 128 words to reduce noise.)\nWe follow the standard pretraining procedure based on the Tensorflow implementation released by NVIDIA.6\nWe use Adam [30] for the optimizer using a standard slanted triangular learning rate schedule with warm-up\nin 10% of steps and cool-down in 90% of steps. Specifically, the learning rate increases linearly from zero to the\npeak rate of 6 √ó10‚àí4 in the first 10% of steps, and then decays linearly to zero in the remaining 90% of steps.\nTraining is done for 62,500 steps with batch size of 8,192, which is comparable to the computation used in previous\n5https://pubmed.ncbi.nlm.nih.gov/; downloaded in Feb. 2020.\n6https://github.com/NVIDIA/DeepLearningExamples\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:13\nVocabulary Pretraining Corpus Text Size\nBERT Wiki + Books - Wiki + Books 3.3B words / 16GB\nRoBERTa Web crawl - Web crawl 160GB\nBioBERT Wiki + Books continual pretraining PubMed 4.5B words\nSciBERT PMC + CS from scratch PMC + CS 3.2B words\nClinicalBERT Wiki + Books continual pretraining MIMIC 0.5B words / 3.7GB\nBlueBERT Wiki + Books continual pretraining PubMed + MIMIC 4.5B words\nPubMedBERT PubMed from scratch PubMed 3.1B words / 21GB\nTable 5. Summary of pretraining details for the various BERT models used in our experiments. Statistics for prior BERT\nmodels are taken from their publications when available. The size of a text corpus such as PubMed may vary a bit, depending\non downloading time and preprocessing (e.g., filtering out empty or very short abstracts). Both BioBERT and PubMedBERT\nalso have a version pretrained with additional PMC full text; here we list the standard version pretrained using PubMed only.\nbiomedical pretraining.7 The training takes about 5 days on one DGX-2 machine with 16 V100 GPUs. We find\nthat the cased version has similar performance to the uncased version in preliminary experiments; thus, we focus\non uncased models in this study. We use whole-word masking (WWM), with a masking rate of 15%. We denote\nthe resulting BERT model PubMedBERT.\nFor comparison, we use the public releases of BERT [16], RoBERTa [39], BioBERT [34], SciBERT [8], Clinical-\nBERT [1], and BlueBERT [45]. See Table 5 for an overview. BioBERT and BlueBERT conduct continual pretraining\nfrom BERT, whereas ClinicalBERT conducts continual pretraining from BioBERT; thus, they all share the same\nvocabulary as BERT. BioBERT comes with two versions. We use BioBERT++ (v1.1), which was trained for a longer\ntime and performed better. ClinicalBERT also comes with two versions. We use Bio+Clinical BERT.\nPrior pretraining work has explored two settings: BERT-BASE with 12 transformer layers and 100 million\nparameters; BERT-LARGE with 24 transformer layers and 300 million parameters. Prior work in biomedical\npretraining uses BERT-BASE only. For head-to-head comparison, we also use BERT-BASE in pretraining Pub-\nMedBERT. BERT-LARGE appears to yield improved performance in some preliminary experiments. We leave an\nin-depth exploration to future work.\nFor task-specific fine-tuning, we use Adam [30] with the standard slanted triangular learning rate schedule\n(warm-up in the first 10% of steps and cool-down in the remaining 90% of steps) and a dropout probability of 0.1.\nDue to random initialization of the task-specific model and drop out, the performance may vary for different\nrandom seeds, especially for small datasets like BIOSSES, BioASQ, and PubMedQA. We report the average scores\nfrom ten runs for BIOSSES, BioASQ, and PubMedQA, and five runs for the others.\nFor all datasets, we use the development set for tuning the hyperparameters with the same range: learning rate\n(1e-5, 3e-5, 5e-5), batch size (16, 32) and epoch number (2‚Äì60). Ideally, we would conduct separate hyperparameter\ntuning for each model on each dataset. However, this would incur a prohibitive amount of computation, as we\nhave to enumerate all combinations of models, datasets and hyperparameters, each of which requires averaging\nover multiple runs with different randomization. In practice, we observe that the development performance is not\nvery sensitive to hyperparameter selection, as long as they are in a ballpark range. Consequently, we focus on\nhyperparameter tuning using a subset of representative models such as BERT and BioBERT, and use a common\nset of hyperparameters for each dataset that work well for both out-domain and in-domain language models.\n7For example, BioBERT started with the standard BERT, which was pretrained using 1M steps with batch size of 256, and ran another 1M\nsteps in continual pretraining.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:14 ‚Ä¢ Gu, Tinn, Cheng, et al.\n3 RESULTS\nIn this section, we conduct a thorough evaluation to assess the impact of domain-specific pretraining in biomedical\nNLP applications. First, we fix the standard task-specific model for each task in BLURB, and conduct a head-to-\nhead comparison of domain-specific pretraining and mixed-domain pretraining. Next, we evaluate the impact\nof various pretraining options such as vocabulary, whole-word masking (WWM), and adversarial pretraining.\nFinally, we fix a pretrained BERT model and compare various modeling choices for task-specific fine-tuning.\n3.1 Domain-Specific Pretraining vs Mixed-Domain Pretraining\nBERT RoBERTa BioBERT SciBERT ClinicalBERT BlueBERT PubMedBERT\nuncased cased cased cased uncased cased cased cased uncased\nBC5-chem 89.25 89.99 89.43 92.85 92.49 92.51 90.80 91.19 93.33\nBC5-disease 81.44 79.92 80.65 84.70 84.54 84.70 83.04 83.69 85.62\nNCBI-disease 85.67 85.87 86.62 89.13 88.10 88.25 86.32 88.04 87.82\nBC2GM 80.90 81.23 80.90 83.82 83.36 83.36 81.71 81.87 84.52\nJNLPBA 77.69 77.51 77.86 78.55 78.68 78.51 78.07 77.71 79.10\nEBM PICO 72.34 71.70 73.02 73.18 73.12 73.06 72.06 72.54 73.38\nChemProt 71.86 71.54 72.98 76.14 75.24 75.00 72.04 71.46 77.24\nDDI 80.04 79.34 79.52 80.88 81.06 81.22 78.20 77.78 82.36\nGAD 80.41 79.61 80.63 82.36 82.38 81.34 80.48 79.15 83.96\nBIOSSES 82.68 81.40 81.25 89.52 86.25 87.15 91.23 85.38 92.30\nHoC 80.20 80.12 79.66 81.54 80.66 81.16 80.74 80.48 82.32\nPubMedQA 51.62 49.96 52.84 60.24 57.38 51.40 49.08 48.44 55.84\nBioASQ 70.36 74.44 75.20 84.14 78.86 74.22 68.50 68.71 87.56\nBLURB score 76.11 75.86 76.46 80.34 78.86 78.14 77.29 76.27 81.16\nTable 6. Comparison of pretrained language models on the BLURB biomedical NLP benchmark. The standard task-specific\nmodels are used in the same fine-tuning process for all BERT models. The BLURB score is the macro average of average\ntest results for each of the six tasks (NER, PICO, relation extraction, sentence similarity, document classification, question\nanswering). See Table 3 for the evaluation metric used in each task.\nWe compare BERT models by applying them to the downstream NLP applications in BLURB. For each task,\nwe conduct the same fine-tuning process using the standard task-specific model as specified in subsection 2.4.\nTable 6 shows the results.\nBy conducting domain-specific pretraining from scratch, PubMedBERT consistently outperforms all the other\nBERT models in most biomedical NLP tasks, often by a significant margin. The gains are most substantial\nagainst BERT models trained using out-domain text. Notably, while the pretraining corpus is the largest for\nRoBERTa, its performance on biomedical NLP tasks is among the worst, similar to the original BERT model.\nModels using biomedical text in pretraining generally perform better. However, mixing out-domain data in\npretraining generally leads to worse performance. In particular, even though clinical notes are more relevant\nto the biomedical domain than general-domain text, adding them does not confer any advantage, as evident by\nthe results of ClinicalBERT and BlueBERT. Not surprisingly, BioBERT is the closest to PubMedBERT, as it also\nuses PubMed text for pretraining. However, by conducting domain-specific pretraining from scratch, including\nusing the PubMed vocabulary, PubMedBERT is able to obtain consistent gains over BioBERT in most tasks. A\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:15\nnotable exception is PubMedQA, but this dataset is small, and there are relatively high variances among runs\nwith different random seeds.\nCompared to the published results for BioBERT, SciBERT, and BlueBERT in their original papers, our results\nare generally comparable or better for the tasks they have been evaluated on. The ClinicalBERT paper does not\nreport any results on these biomedical applications [1].\n3.2 Ablation Study on Pretraining Techniques\nWiki + Books PubMed\nWord Piece Whole Word Word Piece Whole Word\nBC5-chem 93.20 93.31 92.96 93.33\nBC5-disease 85.00 85.28 84.72 85.62\nNCBI-disease 88.39 88.53 87.26 87.82\nBC2GM 83.65 83.93 83.19 84.52\nJNLPBA 78.83 78.77 78.63 79.10\nEBM PICO 73.30 73.52 73.44 73.38\nChemProt 75.04 76.70 75.72 77.24\nDDI 81.30 82.60 80.84 82.36\nGAD 83.02 82.42 81.74 83.96\nBIOSSES 91.36 91.79 92.45 92.30\nHoC 81.76 81.74 80.38 82.32\nPubMedQA 52.20 55.92 54.76 55.84\nBioASQ 73.69 76.41 78.51 87.56\nBLURB score 79.16 79.96 79.62 81.16\nTable 7. Evaluation of the impact of vocabulary and whole word masking on the performance of PubMedBERT on BLURB.\nTo assess the impact of pretraining options on downstream applications, we conduct several ablation studies\nusing PubMedBERT as a running example. Table 7 shows results assessing the effect of vocabulary and whole-\nword masking (WWM). Using the original BERT vocabulary derived from Wikipedia & BookCorpus (by continual\npretraining from the original BERT), the results are significantly worse than using an in-domain vocabulary from\nPubMed. Additionally, WWM leads to consistent improvement across the board, regardless of the vocabulary in\nuse. A significant advantage in using an in-domain vocabulary is that the input will be shorter in downstream\ntasks, as shown in Table 8, which makes learning easier. Figure 3 shows examples of how domain-specific\npretraining with in-domain vocabulary helps correct errors from mixed-domain pretraining.\nFurthermore, we found that pretraining on general-domain text provides no benefit even if we use the in-domain\nvocabulary; see Table 9. The first column corresponds to BioBERT, which conducted pretraining first on the\ngeneral domain and then on PubMed. The second column adopted the same continual pretraining strategy, except\nthat the in-domain vocabulary (from PubMed) was used, which actually led to slight degradation in performance.\nOn the other hand, by conducting pretraining from scratch on PubMed, we attained similar performance even\nwith half of the compute (third column), and attained significant gain with the same amount of compute (fourth\ncolumn; PubMedBERT). In sum, general-domain pretraining confers no advantage here in domain-specific\npretraining.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:16 ‚Ä¢ Gu, Tinn, Cheng, et al.\nFig. 3. Examples of how domain-specific pretraining helps correct errors from mixed-domain pretraining. Top: attention\nfor the leading word piece of the gene mention ‚Äúepithelial-restricted with serine box\" (abbreviation ‚ÄúESX\") in the BC2GM\ndataset. Bottom: attention for the [CLS] token in an instance of AGONIST relation between a pair of dummified chemical\nand protein. In both cases, we show the aggregate attention from the penultimate layer to the preceding layer, which tends\nto be most informative about the final classification. Note how BioBERT tends to shatter the relevant words by inheriting the\ngeneral-domain vocabulary. The domain-specific vocabulary enables PubMedBERT to learn better attention patterns and\nmake correct predictions.\nIn our standard PubMedBERT pretraining, we used PubMed abstracts only. We also tried adding full-text\narticles from PubMed Central (PMC),8 with the total pretraining text increased substantially to 16.8 billion words\n(107 GB). Surprisingly, this generally leads to a slight degradation in performance across the board. However, by\n8https://www.ncbi.nlm.nih.gov/pmc/\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:17\nVocab Wiki + Books PubMed\nBC5-chem 35.9 28.0\nBC5-disease 35.9 28.0\nNCBI-disease 34.2 27.4\nBC2GM 38.5 30.5\nJNLPBA 33.7 26.0\nEBM PICO 30.7 25.1\nChemProt 75.4 55.5\nDDI 106.0 75.9\nGAD 47.0 35.7\nBIOSSES 80.7 61.6\nHoC 40.6 31.0\nPubMedQA 343.1 293.0\nBioASQ 702.4 541.4\nTable 8. Comparison of the average input length in word pieces using general-domain vs in-domain vocabulary.\nPretraining Wiki + Books ‚ÜíPubMed PubMed (half time) PubMed\nVocab Wiki + Books PubMed PubMed PubMed\nBC5-chem 92.85 93.41 93.05 93.33\nBC5-disease 84.70 85.43 85.02 85.62\nNCBI-disease 89.13 87.60 87.77 87.82\nBC2GM 83.82 84.03 84.11 84.52\nJNLPBA 78.55 79.01 78.98 79.10\nEBM PICO 73.18 73.80 73.74 73.38\nChemProt 76.14 77.05 76.69 77.24\nDDI 80.88 81.96 81.21 82.36\nGAD 82.36 82.47 82.8 83.96\nBIOSSES 89.52 89.93 92.12 92.30\nHoC 81.54 83.14 82.13 82.32\nPubMedQA 60.24 54.84 55.28 55.84\nBioASQ 84.14 79.00 79.43 87.56\nBLURB score 80.34 80.03 80.23 81.16\nTable 9. Evaluation of the impact of pretraining corpora and time on the performance on BLURB. In the first two columns,\npretraining was first conducted on Wiki & Books, then on PubMed abstracts. All use the same amount of compute (twice as\nlong as original BERT pretraining), except for the third column, which only uses half (same as original BERT pretraining).\nextending pretraining for 60% longer (100K steps in total), the overall results improve and slightly outperform the\nstandard PubMedBERT using only abstracts. The improvement is somewhat mixed across the tasks, with some\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:18 ‚Ä¢ Gu, Tinn, Cheng, et al.\nPubMed PubMed + PMC PubMed + PMC (longer training)\nBC5-chem 93.33 93.36 93.34\nBC5-disease 85.62 85.62 85.76\nNCBI-disease 87.82 88.34 88.04\nBC2GM 84.52 84.39 84.37\nJNLPBA 79.10 78.90 79.16\nEBM PICO 73.38 73.64 73.72\nChemProt 77.24 76.96 76.80\nDDI 82.36 83.56 82.06\nGAD 83.96 84.08 82.90\nBIOSSES 92.30 90.39 92.31\nHoC 82.32 82.16 82.62\nPubMedQA 55.84 61.02 60.02\nBioASQ 87.56 83.43 87.20\nBLURB score 81.16 81.01 81.50\nTable 10. Evaluation of the impact of pretraining text on the performance of PubMedBERT on BLURB. The first result column\ncorresponds to the standard PubMedBERT pretrained using PubMed abstracts (‚ÄúPubMed‚Äù). The second one corresponds\nto PubMedBERT trained using both PubMed abstracts and PMC full text (‚ÄúPubMed+PMC‚Äù). The last one corresponds to\nPubMedBERT trained using both PubMed abstracts and PMC full text, for 60% longer (‚ÄúPubMed+PMC (longer training)‚Äù).\ngaining and others losing. We hypothesize that the reason for this behavior is two-fold. First, PMC inclusion is\ninfluenced by funding policy and differs from general PubMed distribution, and full texts generally contain more\nnoise than abstracts. As most existing biomedical NLP tasks are based on abstracts, full texts may be slightly\nout-domain compared to abstracts. Moreover, even if full texts are potentially helpful, their inclusion requires\nadditional pretraining cycles to make use of the extra information.\nAdversarial pretraining has been shown to be highly effective in boosting performance in general-domain\napplications [37]. We thus conducted adversarial pretraining in PubMedBERT and compared its performance\nwith standard pretraining (Table 11). Surprisingly, adversarial pretraining generally leads to a slight degradation\nin performance, with some exceptions such as sentence similarity (BIOSSES). We hypothesize that the reason\nmay be similar to what we observe in pretraining with full texts. Namely, adversarial training is most useful if\nthe pretraining corpus is more diverse and relatively out-domain compared to the application tasks. We leave a\nmore thorough evaluation of adversarial pretraining to future work.\n3.3 Ablation Study on Fine-Tuning Methods\nIn the above studies on pretraining methods, we fix the fine-tuning methods to the standard methods described\nin subsection 2.4. Next, we will study the effect of modeling choices in task-specific fine-tuning, by fixing the\nunderlying pretrained language model to our standard PubMedBERT (WWM, PubMed vocabulary, pretrained\nusing PubMed abstracts).\nPrior to the current success of pretraining neural language models, standard NLP approaches were often\ndominated by sequential labeling methods, such as conditional random fields (CRF) and more recently recurrent\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:19\nPubMedBERT + adversarial\nBC5-chem 93.33 93.17\nBC5-disease 85.62 85.48\nNCBI-disease 87.82 87.99\nBC2GM 84.52 84.07\nJNLPBA 79.10 79.18\nEBM PICO 73.38 72.92\nChemProt 77.24 77.04\nDDI 82.36 83.62\nGAD 83.96 83.54\nBIOSSES 92.30 94.11\nHoC 82.32 82.20\nPubMedQA 55.84 53.30\nBioASQ 87.56 82.71\nBLURB score 81.16 80.77\nTable 11. Comparison of PubMedBERT performance on BLURB using standard and adversarial pretraining.\nTask-Specific Model Linear Layer Bi-LSTM\nBC5-chem 93.33 93.12\nBC5-disease 85.62 85.64\nJNLPBA 79.10 79.10\nChemProt 77.24 75.40\nDDI 82.36 81.70\nGAD 83.96 83.42\nTable 12. Comparison of linear layers vs recurrent neural networks for task-specific fine-tuning in named entity recognition\n(entity-level F1) and relation extraction (micro F1), all using the standard PubMedBERT.\nTagging Scheme BIO BIOUL IO\nBC5-chem 93.33 93.37 93.11\nBC5-disease 85.62 85.59 85.63\nJNLPBA 79.10 79.02 79.05\nTable 13. Comparison of entity-level F1 for biomedical named entity recognition (NER) using different tagging schemes and\nthe standard PubMedBERT.\nneural networks such as LSTM. Such methods were particularly popular for named entity recognition (NER) and\nrelation extraction.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:20 ‚Ä¢ Gu, Tinn, Cheng, et al.\nWith the advent of BERT models and the self-attention mechanism, the utility of explicit sequential modeling\nbecomes questionable. The top layer in the BERT model already captures many non-linear dependencies across\nthe entire text span. Therefore, it‚Äôs conceivable that even a linear layer on top can perform competitively. We\nfind that this is indeed the case for NER and relation extraction, as shown in Table 12. The use of a bidirectional\nLSTM (Bi-LSTM) does not lead to any substantial gain compared to linear layer.\nWe also investigate the tagging scheme used in NER. The standard tagging scheme distinguishes words by their\npositions within an entity. For sequential tagging methods such as CRF and LSTM, distinguishing the position\nwithin an entity is potentially advantageous compared to the minimal IO scheme that only distinguishes between\ninside and outside of entities. But for BERT models, once again, the utility of more complex tagging schemes is\ndiminished. We thus conducted a head-to-head comparison of the tagging schemes using three biomedical NER\ntasks in BLURB. As we can see in Table 13, the difference is minuscule, suggesting that with self-attention, the\nsequential nature of the tags is less essential in NER modeling.\nInput text Classification Encoding ChemProt DDI\nENTITY DUMMIFICATION [CLS] 77.24 82.36\nENTITY DUMMIFICATION MENTION 77.22 82.08\nORIGINAL [CLS] 50.52 37.00\nORIGINAL MENTION 75.48 79.42\nENTITY MARKERS [CLS] 77.72 82.22\nENTITY MARKERS MENTION 77.22 82.42\nENTITY MARKERS ENTITY START 77.58 82.18\nTable 14. Evaluation of the impact of entity dummification and relation encoding in relation extraction, all using PubMedBERT.\nWith entity dummification, the entity mentions in question are anonymized using entity type tags such as $DRUG or $GENE.\nWith entity marker, special tags marking the start and end of an entity are appended to the entity mentions in question.\nRelation encoding is derived from the special [CLS]token appended to the beginning of the text or the special entity start\ntoken, or by concatenating the contextual representation of the entity mentions in question.\nThe use of neural methods also has subtle, but significant, implications for relation extraction. Previously,\nrelation extraction was generally framed as a classification problem with manually-crafted feature templates. To\nprevent overfitting and enhance generalization, the feature templates would typically avoid using the entities in\nquestion. Neural methods do not need hand-crafted features, but rather use the neural encoding of the given\ntext span, including the entities themselves. This introduces a potential risk that the neural network may simply\nmemorize the entity combination. This problem is particularly pronounced in self-supervision settings, such as\ndistant supervision, because the positive instances are derived from entity tuples with known relations. As a\nresult, it is a common practice to ‚Äúdummify‚Äù entities (i.e., replace an entity with a generic tag such as $DRUG or\n$GENE) [24, 58].\nThis risk remains in the standard supervised setting, such as in the tasks that comprise BLURB. We thus\nconducted a systematic evaluation of entity dummification and relation encoding, using two relation extraction\ntasks in BLURB.\nFor entity marking, we consider three variants: dummify the entities in question; use the original text; add start\nand end tags to entities in question. For relation encoding, we consider three schemes. In the [CLS]encoding\nintroduced by the original BERT paper, the special token [CLS]is prepended to the beginning of the text span,\nand its contextual representation at the top layer is used as the input in the final classification. Another standard\napproach concatenates the BERT encoding of the given entity mentions, each obtained by applying max pooling\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:21\nto the corresponding token representations. Finally, following prior work, we also consider simply concatenating\nthe top contextual representation of the entity start tag, if the entity markers are in use [7].\nTable 14 shows the results. Simply using the original text indeed exposes the neural methods to significant\noverfitting risk. Using [CLS]with the original text is the worst choice, as the relation encoding has a hard time to\ndistinguish which entities in the text span are in question. Dummification remains the most reliable method,\nwhich works for either relation encoding method. Interestingly, using entity markers leads to slightly better\nresults in both datasets, as it appears to prevent overfitting while preserving useful entity information. We leave\nit to future work to study whether this would generalize to all relation extraction tasks.\n4 DISCUSSION\nStandard supervised learning requires labeled examples, which are expensive and time-consuming to annotate.\nSelf-supervision using unlabeled text is thus a long-standing direction for alleviating the annotation bottleneck\nusing transfer learning. Early methods focused on clustering related words using distributed similarity, such as\nBrown Clusters [12, 36]. With the revival of neural approaches, neural embedding has become the new staple\nfor transfer learning from unlabeled text. This starts with simple stand-alone word embeddings [ 41, 46], and\nevolves into more sophisticated pretrained language models, from LSTM in ULMFiT [ 23] and ELMo [ 47] to\ntransformer-based models in GPT [ 48, 49] and BERT [ 16, 39]. Their success is fueled by access to large text\ncorpora, advanced hardware such as GPUs, and a culmination of advances in optimization methods, such as\nAdam [30] and slanted triangular learning rate [23]. Here, transfer learning goes from the pretrained language\nmodels to fine-tuning task-specific models for downstream applications.\nAs the community ventures beyond the standard newswire and Web domains, and begins to explore high-\nvalue verticals such as biomedicine, a different kind of transfer learning is brought into play by combining text\nfrom various domains in pretraining language models. The prevailing assumption is that such mixed-domain\npretraining is advantageous. In this paper, we show that this type of transfer learning may not be applicable when\nthere is a sufficient amount of in-domain text, as is the case in biomedicine. In fact, our experiments comparing\nclinical BERTs with PubMedBERT on biomedical NLP tasks show that even related text such as clinical notes\nmay not be helpful, since we already have abundant biomedical text from PubMed. Our results show that we\nshould distinguish different types of transfer learning and separately assess their utility in various situations.\nThere are a plethora of biomedical NLP datasets, especially from various shared tasks such as BioCreative [3, 29,\n40, 53], BioNLP [15, 28], SemEval [2, 9, 10, 17], and BioASQ [42]. The focus has evolved from simple tasks, such\nas named entity recognition, to more sophisticated tasks, such as relation extraction and question answering, and\nnew tasks have been proposed for emerging application scenarios such as evidence-based medical information\nextraction [44]. However, while comprehensive benchmarks and leaderboards are available for the general\ndomains (e.g., GLUE [57] and SuperGLUE [56]), they are still a rarity in biomedical NLP. In this paper, inspired\nby prior effort towards this direction [ 45], we create the first leaderboard for biomedical NLP, BLURB ‚Äî a\ncomprehensive benchmark containing thirteen datasets for six tasks.\n5 CONCLUSION\nIn this paper, we challenge a prevailing assumption in pretraining neural language models and show that domain-\nspecific pretraining from scratch can significantly outperform mixed-domain pretraining such as continual\npretraining from a general-domain language model, leading to new state-of-the-art results for a wide range\nof biomedical NLP applications. To facilitate this study, we create BLURB, a comprehensive benchmark for\nbiomedical NLP featuring a diverse set of tasks such as named entity recognition, relation extraction, document\nclassification, and question answering. To accelerate research in biomedical NLP, we release our state-of-the-art\nbiomedical BERT models and setup a leaderboard based on BLURB.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:22 ‚Ä¢ Gu, Tinn, Cheng, et al.\nFuture directions include: further exploration of domain-specific pretraining strategies; incorporating more\ntasks in biomedical NLP; extension of the BLURB benchmark to clinical and other high-value domains.\nREFERENCES\n[1] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Pub-\nlicly Available Clinical BERT Embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop . Association for\nComputational Linguistics, Minneapolis, Minnesota, USA, 72‚Äì78. https://doi.org/10.18653/v1/W19-1909\n[2] Marianna Apidianaki, Saif M. Mohammad, Jonathan May, Ekaterina Shutova, Steven Bethard, and Marine Carpuat (Eds.). 2018.Proceedings\nof The 12th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2018, New Orleans, Louisiana, USA, June 5-6, 2018 .\nAssociation for Computational Linguistics. https://www.aclweb.org/anthology/volumes/S18-1/\n[3] Cecilia N. Arighi, Phoebe M. Roberts, Shashank Agarwal, Sanmitra Bhattacharya, Gianni Cesareni, Andrew Chatr-aryamontri, Simon\nClematide, Pascale Gaudet, Michelle Gwinn Giglio, Ian Harrow, Eva Huala, Martin Krallinger, Ulf Leser, Donghui Li, Feifan Liu, Zhiyong\nLu, Lois J. Maltais, Naoaki Okazaki, Livia Perfetto, Fabio Rinaldi, Rune S√¶tre, David Salgado, Padmini Srinivasan, Philippe E. Thomas,\nLuca Toldo, Lynette Hirschman, and Cathy H. Wu. 2011. BioCreative III interactive task: an overview. BMC Bioinformatics 12, 8 (03 Oct\n2011), S4. https://doi.org/10.1186/1471-2105-12-S8-S4\n[4] Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain Adaptation via Pseudo In-Domain Data Selection. In Proceedings of the\n2011 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Edinburgh, Scotland,\nUK., 355‚Äì362. https://www.aclweb.org/anthology/D11-1033\n[5] Simon Baker, Imran Ali, Ilona Silins, Sampo Pyysalo, Yufan Guo, Johan H√∂gberg, Ulla Stenius, and Anna Korhonen. 2017. Cancer\nHallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer. Bioinformatics 33, 24\n(2017), 3973‚Äì3981.\n[6] Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan H√∂gberg, Ulla Stenius, and Anna Korhonen. 2015. Automatic semantic classification\nof scientific literature according to the hallmarks of cancer. Bioinformatics 32, 3 (2015), 432‚Äì440.\n[7] Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the Blanks: Distributional Similarity\nfor Relation Learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for\nComputational Linguistics, Florence, Italy, 2895‚Äì2905. https://doi.org/10.18653/v1/P19-1279\n[8] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3615‚Äì3620. https://doi.org/10.18653/v1/D19-1371\n[9] Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel M. Cer, and David Jurgens (Eds.). 2017. Proceedings\nof the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017 . Association for\nComputational Linguistics. https://www.aclweb.org/anthology/volumes/S17-2/\n[10] Steven Bethard, Daniel M. Cer, Marine Carpuat, David Jurgens, Preslav Nakov, and Torsten Zesch (Eds.). 2016. Proceedings of the 10th\nInternational Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA, USA, June 16-17, 2016 . The Association for\nComputer Linguistics. https://www.aclweb.org/anthology/volumes/S16-1/\n[11] √Älex Bravo, Janet Pi√±ero, N√∫ria Queralt-Rosinach, Michael Rautschka, and Laura I Furlong. 2015. Extraction of relations between genes\nand diseases from text and large-scale data analysis: implications for translational research. BMC bioinformatics 16, 1 (2015), 55.\n[12] Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L Mercer. 1992. Class-based n-gram models of natural\nlanguage. Computational linguistics 18, 4 (1992), 467‚Äì480.\n[13] Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997), 41‚Äì75.\n[14] Gamal Crichton, Sampo Pyysalo, Billy Chiu, and Anna Korhonen. 2017. A neural network multi-task learning approach to biomedical\nnamed entity recognition. BMC bioinformatics 18, 1 (2017), 368.\n[15] Dina Demner-Fushman, Kevin Bretonnel Cohen, Sophia Ananiadou, and Junichi Tsujii (Eds.). 2019. Proceedings of the 18th BioNLP\nWorkshop and Shared Task, BioNLP@ACL 2019, Florence, Italy, August 1, 2019 . Association for Computational Linguistics. https:\n//www.aclweb.org/anthology/volumes/W19-50/\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . 4171‚Äì4186.\n[17] Mona T. Diab, Timothy Baldwin, and Marco Baroni (Eds.). 2013. Proceedings of the 7th International Workshop on Semantic Evaluation,\nSemEval@NAACL-HLT 2013, Atlanta, Georgia, USA, June 14-15, 2013 . The Association for Computer Linguistics. https://www.aclweb.\norg/anthology/volumes/S13-2/\n[18] Rezarta Islamaj Doƒüan, Robert Leaman, and Zhiyong Lu. 2014. NCBI disease corpus: a resource for disease name recognition and\nconcept normalization. Journal of biomedical informatics 47 (2014), 1‚Äì10.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing ‚Ä¢ 1:23\n[19] Jingcheng Du, Qingyu Chen, Yifan Peng, Yang Xiang, Cui Tao, and Zhiyong Lu. 2019. ML-Net: multi-label classification of biomedical\ntexts with deep neural networks. Journal of the American Medical Informatics Association 26, 11 (06 2019), 1279‚Äì1285. https://doi.org/10.\n1093/jamia/ocz085 arXiv:https://academic.oup.com/jamia/article-pdf/26/11/1279/36089060/ocz085.pdf\n[20] Douglas Hanahan and Robert A Weinberg. 2000. The hallmarks of cancer. cell 100, 1 (2000), 57‚Äì70.\n[21] Mar√≠a Herrero-Zazo, Isabel Segura-Bedmar, Paloma Mart√≠nez, and Thierry Declerck. 2013. The DDI corpus: An annotated corpus with\npharmacological substances and drug‚Äìdrug interactions. Journal of biomedical informatics 46, 5 (2013), 914‚Äì920.\n[22] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735‚Äì1780.\n[23] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics,\nMelbourne, Australia, 328‚Äì339. https://doi.org/10.18653/v1/P18-1031\n[24] Robin Jia, Cliff Wong, and Hoifung Poon. 2019. Document-Level ùëÅ-ary Relation Extraction with Multiscale Representation Learning. In\nNAACL.\n[25] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research\nQuestion Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China,\n2567‚Äì2577. https://doi.org/10.18653/v1/D19-1259\n[26] Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li-wei H. Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter\nSzolovits, Leo Anthony Celi, and Roger G. Mark. 2016. MIMIC-III, a freely accessible critical care database. Scientific Data 3, 1 (24 May\n2016), 160035. https://doi.org/10.1038/sdata.2016.35\n[27] Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. 2004. Introduction to the Bio-entity Recognition\nTask at JNLPBA. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications\n(NLPBA/BioNLP). COLING, Geneva, Switzerland, 73‚Äì78. https://www.aclweb.org/anthology/W04-1213\n[28] Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011. Overview of Genia Event Task in BioNLP Shared Task 2011.\nIn Proceedings of the BioNLP Shared Task 2011 Workshop (Portland, Oregon) (BioNLP Shared Task ‚Äô11) . Association for Computational\nLinguistics, USA, 7‚Äì15.\n[29] Sun Kim, Rezarta Islamaj Dogan, Andrew Chatr-aryamontri, Mike Tyers, W. John Wilbur, and Donald C. Comeau. 2015. Overview of\nBioCreative V BioC Track.\n[30] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.).\nhttp://arxiv.org/abs/1412.6980\n[31] Martin Krallinger, Obdulia Rabal, Saber A Akhondi, Martƒ±n P√©rez P√©rez, Jes√∫s Santamar√≠a, GP Rodr√≠guez, et al. 2017. Overview of the\nBioCreative VI chemical-protein interaction Track. In Proceedings of the sixth BioCreative challenge evaluation workshop , Vol. 1. 141‚Äì146.\n[32] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural\nText Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations .\nAssociation for Computational Linguistics, Brussels, Belgium, 66‚Äì71. https://doi.org/10.18653/v1/D18-2012\n[33] John Lafferty, Andrew Mccallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and\nlabeling sequence data. In in Proceedings of the 18th International Conference on Machine Learning . 282‚Äì289.\n[34] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained\nbiomedical language representation model for biomedical text mining. Bioinformatics (09 2019). https://doi.org/10.1093/bioinformatics/\nbtz682\n[35] Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C\nWiegers, and Zhiyong Lu. 2016. BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database 2016\n(2016).\n[36] Percy Liang. 2005. Semi-supervised learning for natural language . Ph.D. Dissertation. Massachusetts Institute of Technology.\n[37] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. 2020. Adversarial Training for\nLarge Neural Language Models. arXiv preprint arXiv:2004.08994 (2020).\n[38] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. 2015. Representation Learning Using Multi-Task Deep\nNeural Networks for Semantic Classification and Information Retrieval. In Proceedings of the 2015 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . 912‚Äì921.\n[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).\n[40] Yuqing Mao, Kimberly Van Auken, Donghui Li, Cecilia N. Arighi, Peter McQuilton, G. Thomas Hayman, Susan Tweedie, Mary L.\nSchaeffer, Stanley J. F. Laulederkind, Shur-Jen Wang, Julien Gobeill, Patrick Ruch, Anh Tuan Luu, Jung jae Kim, Jung-Hsien Chiang,\nYu-De Chen, Chia-Jung Yang, Hongfang Liu, Dongqing Zhu, Yanpeng Li, Hong Yu, Ehsan Emadzadeh, Graciela Gonzalez, Jian-Ming\nChen, Hong-Jie Dai, and Zhiyong Lu. 2014. Overview of the gene ontology task at BioCreative IV. Database: The Journal of Biological\n, Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:24 ‚Ä¢ Gu, Tinn, Cheng, et al.\nDatabases and Curation 2014 (2014).\n[41] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781 (2013).\n[42] Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara, and Georgios Paliouras. 2019. Results of the Seventh Edition of the\nBioASQ Challenge. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 553‚Äì568.\n[43] Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependences in stochastic language modelling.\nComputer Speech & Language 8, 1 (1994), 1 ‚Äì 38. https://doi.org/10.1006/csla.1994.1001\n[44] Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain J Marshall, Ani Nenkova, and Byron C Wallace. 2018. A corpus with\nmulti-level annotations of patients, interventions and outcomes to support language processing for medical literature. In Proceedings of\nthe conference. Association for Computational Linguistics. Meeting , Vol. 2018. NIH Public Access, 197.\n[45] Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT\nand ELMo on Ten Benchmarking Datasets. In Proceedings of the 18th BioNLP Workshop and Shared Task . Association for Computational\nLinguistics, Florence, Italy, 58‚Äì65. https://doi.org/10.18653/v1/W19-5006\n[46] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural language processing (EMNLP) . 1532‚Äì1543.\n[47] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep\nContextualized Word Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers) . Association for Computational Linguistics, New\nOrleans, Louisiana, 2227‚Äì2237. https://doi.org/10.18653/v1/N18-1202\n[48] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-\ntraining.\n[49] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised\nmultitask learners. OpenAI Blog 1, 8 (2019), 9.\n[50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020),\n1‚Äì67. http://jmlr.org/papers/v21/20-074.html\n[51] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. InProceedings\nof the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational\nLinguistics, Berlin, Germany, 1715‚Äì1725. https://doi.org/10.18653/v1/P16-1162\n[52] Yuqi Si, Jingqi Wang, Hua Xu, and Kirk Roberts. 2019. Enhancing clinical concept extraction with contextual embeddings. Journal of the\nAmerican Medical Informatics Association (2019).\n[53] Larry Smith, Lorraine K Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger,\nChristoph M Friedrich, et al. 2008. Overview of BioCreative II gene mention recognition. Genome biology 9, S2 (2008), S2.\n[54] Gizem Soƒüancƒ±oƒülu, Hakime √ñzt√ºrk, and Arzucan √ñzg√ºr. 2017. BIOSSES: a semantic sentence similarity estimation system for the\nbiomedical domain. Bioinformatics 33, 14 (2017), i49‚Äìi58.\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all you need. In Advances in neural information processing systems . 5998‚Äì6008.\n[56] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019.\nSuperglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing\nSystems. 3266‚Äì3280.\n[57] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A MULTI-TASK BENCHMARK\nAND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING. InICLR.\n[58] Hai Wang and Hoifung Poon. 2018. Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision. In EMNLP.\n[59] Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, and Jianfeng Gao. 2019. Multi-task Learning with Sample Re-weighting for Machine\nReading Comprehension. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for Computational Linguistics, Minneapolis,\nMinnesota, 2644‚Äì2655. https://doi.org/10.18653/v1/N19-1271\n[60] M. Zhang and Z. Zhou. 2014. A Review on Multi-Label Learning Algorithms. IEEE Transactions on Knowledge and Data Engineering 26,\n8 (2014), 1819‚Äì1837. https://doi.org/10.1109/TKDE.2013.39\n[61] Yijia Zhang, Wei Zheng, Hongfei Lin, Jian Wang, Zhihao Yang, and Michel Dumontier. 2018. Drug‚Äìdrug interaction extraction via\nhierarchical RNNs on sequence and shortest dependency paths. Bioinformatics 34, 5 (2018), 828‚Äì835.\n[62] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books\nand movies: Towards story-like visual explanations by watching movies and reading books. In ICCV.\n, Vol. 1, No. 1, Article 1. Publication date: January 2021."
}