{
  "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models",
  "url": "https://openalex.org/W4389519189",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2366525249",
      "name": "Jiang, Yifan",
      "affiliations": [
        "Viterbo University",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2100488138",
      "name": "Ilievski, Filip",
      "affiliations": [
        "Viterbo University",
        "Vrije Universiteit Amsterdam",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2776341553",
      "name": "Ma Kaixin",
      "affiliations": [
        "Bellevue Hospital Center"
      ]
    },
    {
      "id": "https://openalex.org/A4311427828",
      "name": "Sourati, Zhivar",
      "affiliations": [
        "University of Southern California",
        "Viterbo University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3175270222",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4287888538",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2963272610",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W3175910413",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3165066581",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3213868621",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2999524812",
    "https://openalex.org/W3183822815",
    "https://openalex.org/W3176825161",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4362597839",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4285606726",
    "https://openalex.org/W4389523933",
    "https://openalex.org/W4330338194",
    "https://openalex.org/W3035172163",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W2528961568",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2946707294",
    "https://openalex.org/W2987669390",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2080940044",
    "https://openalex.org/W3184978204",
    "https://openalex.org/W3196645780",
    "https://openalex.org/W33677238",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3125491424",
    "https://openalex.org/W2889677365",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3211511509",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W3101056292"
  ],
  "abstract": "The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of reconstruction examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across reconstruction formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14317–14332\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBRAIN TEASER : Lateral Thinking Puzzles for Large Language Models\nYifan Jiang1, Filip Ilievski1,2, Kaixin Ma3∗, Zhivar Sourati1\n1Information Sciences Institute, Viterbi School of Engineering, University of Southern California\n2Department of Computer Science, Faculty of Science, Vrije Universiteit Amsterdam\n3Tencent AI Lab, Bellevue, W A\n{yifjia,ilievski,Souratih}@isi.edu, f.ilievski@vu.nl\nkaixinma@global.tencent.com\nAbstract\nThe success of language models has inspired\nthe NLP community to attend to tasks that\nrequire implicit and complex reasoning, rely-\ning on human-like commonsense mechanisms.\nWhile such vertical thinking tasks have been\nrelatively popular, lateral thinking puzzles have\nreceived little attention. To bridge this gap,\nwe devise BRAIN TEASER : a multiple-choice\nQuestion Answering task designed to test the\nmodel’s ability to exhibit lateral thinking and\ndefy default commonsense associations. We\ndesign a three-step procedure for creating the\nfirst lateral thinking benchmark, consisting of\ndata collection, distractor generation, and gen-\neration of reconstruction examples, leading to\n1,100 puzzles with high-quality annotations. To\nassess the consistency of lateral reasoning by\nmodels, we enrich BRAIN TEASER based on\na semantic and contextual reconstruction of\nits questions. Our experiments with state-of-\nthe-art instruction- and commonsense language\nmodels reveal a significant gap between hu-\nman and model performance, which is further\nwidened when consistency across reconstruc-\ntion formats is considered. We make all of our\ncode and data available to stimulate work on de-\nveloping and evaluating lateral thinking models.\n1 Introduction\nHuman reasoning processes comprise two types of\nthinking: vertical and lateral (Waks, 1997). Vertical\nthinking, also known as linear, convergent, or logi-\ncal thinking, is a sequential analytical process that\nis based on rationality, logic, and rules, typically\nassociated with the left-brain hemisphere. Vertical\nthinking, as illustrated in Figure 1 (top), is needed\nto create a reasoning path from flooding a room\nto filling it with water for physical reasoning, and\nfrom inanimate objects with five fingers to gloves in\nriddles. Meanwhile, lateral thinking (or “thinking\n* Work done when KM was at Carnegie Mellon University\nHow do you flood a room?\n(A) Fill it with objects.(B) Fill it with water.\nPIQA\ncover with water\nLateral Thinking\nSentence Puzzle\nA man shaves everyday, yet keeps his beard long.\nhis beard gets clean everyday\nhe is a barber and he shaves others\nWord PuzzleWhat type of cheese is made backwards?\nMozzarella\nFeta\n Edam\nRiddleSense\nI have five fingers, but I am not alive. What am I?\n(A) Glove. (B) Computer.\nitem like a hand\nVertical Thinking\nfive separate parts\nFigure 1: Contrasting existing Vertical Thinking tasks\n(PIQA (Bisk et al., 2020) and RiddleSense (Lin et al.,\n2021)) to our novel lateral thinking task called BRAIN -\nTEASER . While prior tasks require commonsense to\nbe injected, BRAIN TEASER ’s lateral thinking puzzles\nrequire default commonsense thinking to be deprecated.\noutside the box”) is a divergent and creative pro-\ncess that involves looking at a problem from a new\nperspective and defying preconceptions, associated\nwith the right-brain hemisphere (De Bono, 1970;\nWaks, 1997). Lateral thinking is required to solve\nthe puzzle in Figure 1 (bottom), by overwriting\nthe commonsense associations of man shaves to he\nshaves himself, and regarding the man as somebody\nwho shaves others all day (e.g., a barber).\nThe development of natural language processing\n(NLP) models and their evaluation has achieved\nmuch progress in vertical thinking. In particular,\nlarge language models (LLMs) (Devlin et al., 2019;\nLiu et al., 2019; Brown et al., 2020b) have achieved\nstrong performance across a variety of complex rea-\nsoning tasks (Talmor et al., 2019; Bisk et al., 2020;\n14317\nSap et al., 2019b), even with the complete absence\n(zero-shot) (Sanh et al., 2022) or limited provision\n(few-shot) of training time exemplars (Chung et al.,\n2022).1 To perform well on tasks such as reason-\ning over physical interactions (Bisk et al., 2020)\nand social implications (Sap et al., 2019b), LLMs\nexhibit better vertical thinking capabilities, includ-\ning commonsense association (Wei et al., 2022)\nand inference ability (Bosselut et al., 2019). While\nthe extent to which these models possess common\nsense is heavily discussed (Marcus, 2022; Bubeck\net al., 2023; Wei et al., 2023), we note that prior\nwork has not considered the lateral thinking ability\nof LLMs. Creative thinking problems in bench-\nmarks and knowledge bases are often filtered out\nas noise during preprocessing (Vajjala and Meur-\ners, 2012; Speer et al., 2017; Sap et al., 2019a), and\nonly kept if their resolution can be supported by\ncommonsense associations, as in the case of riddles\n(Figure 1) (Lin et al., 2021; Gao et al., 2018). As\nmany situations are novel, we expect that lateral\nthinking puzzles like those in Figure 1-bottom will\nbe hindered by default commonsense associations\nand cannot be easily solved by further adaptation\nand scaling of the existing LLM methods.\nTo bridge this gap, we propose to study the abil-\nity of state-of-the-art LLMs to reason on lateral\nthinking puzzles. We formulate lateral thinking puz-\nzles as multiple-choice Question Answering (QA)\ntasks, making them intuitive to answer by humans\nand easy to evaluate automatically. Following our\ntask definition, we create a novel BRAIN TEASER\nbenchmark with two tasks of different granularity:\nSentence Puzzles and Word Puzzles (cf. Figure 1).\nTo construct the dataset, we design a data collec-\ntion procedure, which crawls relevant puzzles from\nseveral publicly available websites, performs semi-\nautomatic filtering of irrelevant question categories\n(e.g., pun, dad jokes), and ensures high data quality.\nTo ensure fair and informative questions, we con-\nstruct distractors semi-automatically by manual an-\nnotation of the explicit and implicit (commonsense)\npremises that arise from each puzzle. To address\nconcerns of possible LLM memorization (Carlini\net al., 2022) and their lack of consistency (Gold-\nberg, 2023), we enrich BRAIN TEASER with two\nreconstruction strategies: semantic reconstruction\nand context reconstruction, which create variants\nof each puzzle without changing its original way of\n1In this paper, we use the terms language model and large\nlanguage model interchangeably.\ndefying default commonsense associations. This\nsystematic procedure results in a novel BRAIN -\nTEASER benchmark with 1.1K high-quality data\npoints and nearly 100% human evaluation results.\nUsing BRAIN TEASER as the benchmark, we con-\nduct comprehensive experiments involving differ-\nent model structures, model sizes, and prompting\nstrategies. The results reveal a huge gap between\nhuman performance and current LLMs, indicating\nthe great need to improve lateral thinking in LLMs.\nWe summarize our contributions as follows: 1)\nWe introduce lateral thinking puzzles, a multiple-\nchoice QA task designed to test the model’s ability\nto exhibit lateral thinking and defy default com-\nmonsense associations. 2) We design a three-step\nprocedure for creating the first lateral thinking\nbenchmark, BRAIN TEASER , consisting of data\ncollection, distractor generation, and generation\nof reconstruction examples, leading to 1,100 high-\nquality puzzles. 3) We conduct comprehensive\nexperiments with state-of-the-art LLMs. We make\nall of our code and data available to stimulate work\non developing and evaluating lateral thinking mod-\nels.2\n2 Related work\nWe review prior work on computational creativity,\ncommonsense reasoning, and model robustness.\nComputational Creativity Computational cre-\nativity work includes a broader set of tasks, some\nof which have been relatively popular, including\npun (Zou and Lu, 2019) and humor (Meaney et al.,\n2021) detection. A particular class of creative chal-\nlenges, called brain teasers (Draper, 2009; High-\nhouse et al., 2019), is designed to evaluate a wide\nrange of human intelligence skills, including strat-\negy development, planning, visual-spatial thinking,\ncreativity, and memory (Altun et al., 2016). Most\nsimilar to our task, Lin et al. (2021) collects riddles\nfrom public websites to challenge current models.\nWhile in principle computational creativity puz-\nzles and brain teasers combine vertical and lateral\nthinking, prior work has focused on the former cate-\ngory. Our BRAIN TEASER task complements these\nworks with word- and sentence-level lateral think-\ning puzzles. BRAIN TEASER can serve as a formal\nplatform to evaluate the creative skills of LLMs,\nwhich have been partially explored in recent work\n2The code is available at https://github.com/1171-\njpg/BrainTeaser\n14318\n(Franceschelli and Musolesi, 2023; Bubeck et al.,\n2023; Wang et al., 2023a).\nCommonsense Reasoning The task of common-\nsense reasoning has been popular in recent years\n(Rajani et al., 2019; Ma et al., 2019; Lourie et al.,\n2021; Maharana and Bansal, 2022), accompanied\nby the introduction of numerous challenging bench-\nmarks (Talmor et al., 2019; Sap et al., 2019b;\nSakaguchi et al., 2019) and availability of large-\nscale commonsense resources (Speer et al., 2017;\nHwang et al., 2021). While each of the existing\ndatasets focuses on different dimensions of com-\nmonsense knowledge (Ilievski et al., 2021a), most\nof them are constructed in the multiple-choice for-\nmat, due to the ease of evaluation. Some prior\nworks have focused on generative commonsense\nreasoning (Lin et al., 2020; Boratko et al., 2020).\nHowever, due to the vast plausible answer space,\nthe evaluation has been challenging and a large\namount of answer annotations have to be collected\nin order to ensure fairness (Boratko et al., 2020).\nCuriously, while possession of common sense has\nbeen a central goal of AI, its role in our BRAIN -\nTEASER task is as a distractor. Namely, successful\nsolutions of the lateral thinking puzzles in BRAIN -\nTEASER require the models to defy commonsense\nassociations and linear inference chains.\nRobustness Studies As a novel benchmark,\nBRAIN TEASER relates to other works that evaluate\nthe performance of LLMs. Since these models are\nsurpassing human performance on some existing\nbenchmarks (Xu et al., 2022), the NLP community\nhas shifted the focus towards robustness evalua-\ntion, i.e., whether the model can retain a similar\nperformance to semantically perturbed or adver-\nsarially constructed questions (Abdou et al., 2020;\nNie et al., 2020). Some recent works have adopted\nmodel adversarial approaches to generate datasets\nthat are challenging for models to solve (Zellers\net al., 2019; Sakaguchi et al., 2019), while oth-\ners combine multiple tasks to evaluate the model’s\nbehavioral consistency across semantic, logical,\nand factual categories (Jang et al., 2022). Be-\nsides dataset construction, analysis studies have\nalso shown that models easily learn shortcuts to\nsolve the datasets (Branco et al., 2021; Elazar et al.,\n2021) and their performance heavily depends on\nthe overlap of tokens between training and test data\n(Ma et al., 2021b). Different from prior works\nwhere associative resources are used to finetune\nthe model to improve robustness, we expect that\nthe lateral thinking puzzles in BRAIN TEASER re-\nquire unique associations and creative reasoning\npaths. In this way, BRAIN TEASER is designed to\nminimize the impact of confounding factors like\nmemorization in LLMs (Bang et al., 2023; Guo\net al., 2023; Goldberg, 2023).\n3 Construction of B RAIN TEASER\nIn this section, we first provide a definition of lat-\neral thinking puzzles in various granularities. We\nthen present a three-stage pipeline for constructing\nthe multiple-choice puzzles in the BRAIN TEASER\ndataset, consisting of data collection, distractor\nsampling, and reconstruction sample generation.\nFinally, we present key data statistics and quality\nvalidation results.\n3.1 Task Definition\nWhile lateral thinking puzzles are often presented\nto humans in an open-ended fashion, these are dif-\nficult to evaluate automatically and are difficult to\nsolve by humans.3 An additional complication is\nthat there may be multiple independent, yet correct,\npuzzle explanations. To alleviate these challenges,\nwe pose lateral thinking puzzles as a multiple-\nchoice QA task, a format frequently employed for\nreasoning tasks. We expect this approach to be both\nfacile for human comprehension and amenable to\nautomated evaluation. In general, each puzzle con-\ntains a question Q stating the context, and a lateral\nexplanation e from explanation space E that serves\nas the correct answer. Q can be decomposed into\nan atomic premise set P, which includes both ex-\nplicitly stated clauses and implicit clauses derived\nthrough default commonsense inferences or associ-\nations. For example, in the following puzzle: \"How\ncould a cowboy ride into town on Friday, stay two\ndays, and ride out on Wednesday?\", the set P in-\ncludes the following premises:\n• p1: Cowboy rides into town on Friday.\n• p2: Cowboy stays in town for two days.\n• p3: Cowboy rides out on Wednesday.\n• p4: Wednesday is the third day of the week.\n• p5: Sunday is two days after Friday.\n3Our small-scale user study shows that both humans and\nLLMs are unable to perform this open-ended task well, scor-\ning 2.64 and 2.62 on a 5-point scale, respectively (see Ap-\npendix A.5 for details).\n14319\nThe premises p1, p2, and p3 are explicitly provided\nby the context, and the premises p4 and p5 are\nimplicitly obtained by default commonsense asso-\nciation. The goal of a puzzle is to find an expla-\nnation that does not contradict the premise set P,\nE ∩¬P = ∅, as the premises are the target to\nexplain and support. With vertical thinking, the\nquestion appears impossible to answer because P\ncontains statements that conflict with each other.\nThe premises p3 and p4 are inconsistent with other\npremises, leading to an obstacle in explaining the\npuzzle. The default commonsense inference thus\nbecomes a logic stumper (Bar-Hillel et al., 2018),\npreventing one from creatively exploring additional\nexplanations in E.\nLateral thinking leads to a correct solution to this\npuzzle: “His horse is named Wednesday.”. This\ncreative solution defies the commonsense associa-\ntion of Wednesday as a third day of the week (p4).\nThus, the key point of a lateral thinking puzzle is\nthat some implicit premises generated through de-\nfault commonsense association incorrectly create\nan arbitrary “box” that wrongly excludes the possi-\nble solution from the explanation space (Bar-Hillel\net al., 2018).\nUpon careful exploration, we devise two granu-\nlarity variants of lateral thinking puzzles following\nour definition (Figure 1): sentence-based, where\nthe puzzle is centered on sentence premises (e.g.,\nWednesday is the third day of the week), and word-\nbased, where the answer violates the default mean-\ning of the word and focuses on the letter composi-\ntion of the target question (e.g., cheese made back-\nwards →edam).\n3.2 Data Collection\nWe collect over ten thousand lateral thinking puz-\nzles with answers from public websites such as\nriddles.com and rd.com using web crawlers. We\nmerge the data from different sources and re-\nmove (near-)duplicates based on sentence similar-\nity (Reimers and Gurevych, 2019). We conduct a\nsemi-automatic process that corrects typos by us-\ning an automatic library, Auto Correct,4 followed\nby human verification to ensure that the puzzles\npreserve their original meaning. We filter the re-\nmaining data manually to preserve QA pairs that fit\nthe definition of the sentence- or word-based lateral\nthinking puzzles. This process yields 373 unique\nlateral puzzles, formatted as QA pairs.\n4github.com/phatpiglet/autocorrect\nTable 1: Example of generated distractors.\nPremise Answer/Distractor\npw: Wednesday is the Answer: His horse is\nthird day of the week. named Wednesday.\np2: Cowboy stays in Distractor: While in town,\nin town for two days. he stays in bed for two days.\np5: Sunday is two days Distractor: Friday and\npast Friday. Saturday are holidays.\n3.3 Distractor Sampling\nWe convert each puzzle and its explanation into\na multiple-choice QA format to ensure a straight-\nforward evaluation process. A key challenge in\ncreating fair and informative multiple-choice ques-\ntions is sampling distractors that are simultaneously\nincorrect and challenging (Ma et al., 2021a). We\npropose a systematic approach for distractor sam-\npling that directly benefits from our premise-based\ndefinition of lateral thinking puzzles.\nFor sentence puzzles, we list possible premises\nP = {p1, p2, p3, . . .}from the question context\nmanually as the commonsense associations in the\ndata are obvious and straightforward, especially\nwhen the answers are provided, like the example in\nSection 3.1. We know the correct answer p′\nc is an\nunconventional overwriting of the wrong premise\n(logic stumper) pw generated by default common-\nsense association. We generate the distractors by\noverwriting other premises in P −pw. This pro-\ncedure guarantees that the distractors are incorrect\nbecause the misleading premise pw still remains in\nthe premise set and prevents one from reaching the\ncorrect explanation. We first use COMET (Hwang\net al., 2021) to generate the possible premise over-\nwriting candidates for the question as a head com-\nbined with inference relations (e.g., happens after,\nhindered by, cause). Then we pick the COMET-\ngenerated tails that are consistent with the question\ncontext as distractors and revise them by manual\nannotation. Table 1 shows example distractors for\nour running example puzzle from Section 3.1.\nFor word puzzles, as we focus on the literal mean-\ning rather than semantic meaning, distractors can\nshare similar semantic meaning as the correct an-\nswers and still exhibit similar commonsense as-\nsociations. We pick distractors from the correct\nanswer’s synonyms in WordNet (e.g., mozzarella\nfor edam in Figure 1) and Wikipedia entries that\nbelong to the same category (e.g., both edam and\ncheddar belong to the semi-hard cheese category).\nSince it is generally possible that none of the cre-\n14320\nTable 2: A sentence-based lateral thinking puzzle and its reconstruction variations. We present an analogous\nword-level puzzle in the Appendix A.3.\nAdv Strategy Question Answers\n-\nHis horse is named Wednesday.\nHow could a cowboy ride into town on Friday, stay While in town, he stays in bed for two days.\ntwo days, and ride out on Wednesday? Friday and Saturday are holidays.\nNone of the above.\nHis horse is named Wednesday.\nSemantic Re- How could a cowboy come into town on Friday, While in town, he stays in bed for two days.\nconstruction stay two days, and then ride away on Wednesday? Friday and Saturday are holidays.\nNone of the above.\nThe pilot’s airplane is named Tuesday.\nContext Re- How can a pilot take off in Los Angeles on Tuesday, He flies straight for 24h and flies quickly for hours left.\nconstruction fly for 48 hours, and land in Tokyo on Tuesday? There was a one-week long holiday.\nNone of the above.\native solutions will be sensible for some of the ques-\ntions, we also include the option None of the above\nin all questions’ candidates set. This answer candi-\ndate simulates the situation where humans cannot\noverwrite their commonsense inference and give\nup on explaining the lateral thinking puzzle. To cre-\nate puzzles where lateral thinking fails (i.e., with\nanswer None of the above), we replace the correct\nanswer with a distractor in 6% of the questions. Af-\nter this procedure, each question in BRAIN TEASER\nhas four answer candidates.\n3.4 Generating Reconstruction Examples\nSince the latest LLMs are pretrained on massive\nweb snapshots, it is possible that the data sources\nfor BRAIN TEASER are also included in their train-\ning data. Consequently, it is possible for LLMs to\nmemorize the correct answer without performing\nany reasoning. To ensure that our task evaluates\nlateral thinking ability rather than memorization,\nwe construct reconstruction versions of the original\ndata in two parallel ways (Table 2): (1) Seman-\ntic Reconstruction rephrases the original question\nwithout changing its answer, distractor, and any\npremises in P. To do so, we use an open-source\nrephrasing tool,5 after which human annotators re-\nfine and validate that all premises remain the same.\n(2) Context Reconstruction keeps the misleading\ncommonsense premise intact and changes both the\nquestion and the answer to a new situational con-\ntext. For this purpose, we prompt GPT-4 for initial\nreconstructions, which are then manually refined\nby human annotators. The new distractors are gen-\nerated following the same process as in Section 3.3.\nThe premise set and the corresponding distractors\nalso get translated to the new context. Intuitively, a\n5https://quillbot.com/\nTable 3: Key statistics of the BRAIN TEASER dataset.\nChoices combine the correct answer with all the distrac-\ntors. Standard deviation is computed without the None\nof the above choice, as its token length is fixed and not\nrelated to the question context.\nSentence Word\n# Puzzles 627 492\nAverage Question Tokens 34.88 10.65\n% Long Question (>30 tokens) 48.32% 2.23%\nAverage Answer Tokens 9.11 3.0\nStd of Choice Tokens 2.36 0.52\nmodel that learns to reason should be able to solve\nthese two reconstruction variants of the questions\neasily, whereas the model that memorizes the an-\nswer would stumble.\n3.5 Data Analysis and Validation\nKey Statistics BRAIN TEASER includes 1,119\ndata samples including its reconstruction variants.\nTable 3 reports key statistics of each subtask of\nBRAIN TEASER . The questions in the Sentence\nPuzzle category are much longer because they are\nin a narrative story format rather than simple short\nquestions, like those in the Word Puzzle category.\nThe difference between the standard deviation in\nthe number of choice tokens betweenSentence Puz-\nzle and Word Puzzlecan be ascribed to the different\nstrategies for generating distractors, i.e., overwrit-\ning various premises with new statements versus\ngenerating similar words from the synonym set.\nWe use ChatGPT prompting to extract the con-\ntext topic from each question and to analyze the\nmajor topics in each subtask. The topic distribution\nshows that both subtasks involve a large range of\n(more than 80) areas. Sentence Puzzle is denom-\ninated by math, physics, and nature while Word\nPuzzle is denominated particularly by the language\n14321\ntopic. For both tasks, there is a long tail of less\ncommon topics. The details of topic extraction and\nits obtained statistics are given in the Appendix A.1.\nThe data statistics and the topic analysis suggest\nthat, despite its limited size, BRAIN TEASER can\nfunction as a comprehensive benchmark for assess-\ning model performance across diverse topics and\nvarying lengths of context.\nHuman Validation To ensure the quality of our\ndataset, we invited three expert annotators to ver-\nify the validity of the QA pairs and their recon-\nstruction variants. We sampled 102 examples from\nBRAIN TEASER randomly and asked the annotators\nthe following two questions: 1) Does the original\npuzzle and correct answer make sense? 2) Are the\nreconstruction variants still consistent with the orig-\ninal questions in terms of the required reasoning\nprocess? On average, the human annotators rated\n99% of the original question-answering pairs as\nvalid. 100% of the semantic reconstructions and\n97% context reconstructions were marked as con-\nsistent with the original question-answer pair. The\noverall Fleiss (1971) kappa inter-annotator agree-\nment is 0.948, which is an almost perfect score.\n4 Experimental Setup\nWe describe the models selected for our experi-\nments and the metrics used to evaluate the reason-\ning accuracy and consistency of these models.\n4.1 Model selection\nInstruction-Based Models We evaluate the\ninstruction-finetuned LLMs in zero/few-shot set-\nting: 1) ChatGPT, a publicly available state-of-\nthe-art LLM from the GPT (Brown et al., 2020a)\nseries. 2) T0 (Sanh et al., 2022), a LLM trained\nwith multitasking instruction tuning that has strong\nzero-shot generalization ability. 3) FlanT5 (Chung\net al., 2022), an enhanced version of T5 (Raffel\net al., 2020) which is instruction-finetuned (Wei\net al., 2021) in both zero-shot and few-shot setting.\nFor a fair comparison with humans, while running\nzero-shot prompting on ChatGPT, we add a descrip-\ntion indicating that the question is a brain teaser\npuzzle that needs creative thinking to solve. For\nthe rest of the models, we use the same instruction\ntemplates as found in their training sets (for full\ndetails, please refer to Appendix A.2).\nCommonsense Models To understand the effect\nof commonsense knowledge on our task, we eval-\nuate the following models that are enhanced with\ncommon sense: 1) RoBERTa-L (CSKG) (Ma et al.,\n2021a), a model finetuned on the synthetic QA\npairs generated from a diverse set of commonsense\nknowledge graphs (CSKG) (Ilievski et al., 2021b).\n2) CAR (Wang et al., 2023b), a model finetuned in\na similar pipeline as Ma et al. (2021a) but with en-\nhanced negative sampling strategy and reportedly\nsuperior performance. For reference, we also in-\nclude the vanilla RoBERTa model (Liu et al., 2019)\nto understand the impact of commonsense knowl-\nedge. We evaluate all of the models in a zero-shot\nfashion, following the scoring method defined in\n(Ma et al., 2021a). We select RoBERTa because of\nits widespread usage of the commonsense task and\nimpressive zero-shot performance. RoBERTa-L\n(CSKG) achieve SOTA zero-shot result on multiple\ncommonsense tasks, while CAR even outperforms\nChatGPT on commonsense tasks.\nHuman Evaluation To assess the upper bound per-\nformance on BRAIN TEASER , we randomly sample\n102 questions from it and invite three experts an-\nnotator to solve the test. On average, it takes one\nhour for an annotator to complete the task.\n4.2 Evaluation Metrics\nAs accuracy is a fair evaluation metric for the\nMCQA format and it has been adopted by many\npopular commonsense reasoning tasks (Mihaylov\net al., 2018; Talmor et al., 2019; Bisk et al., 2020),\nwe evaluate model performance using two accuracy\nmetrics: Instance-based Accuracy considers each\n(original or reconstruction) question separately. We\nreport instance-based accuracy on the original puz-\nzles, and their semantic and context reconstructions.\nGroup-based Accuracy considers each original\npuzzle and its variants as a group. The model will\nscore 1 only when it successfully solves all three\npuzzles in the group, otherwise, its score is 0.\n5 Results\nOur experiments target five questions: 1) Can\nLLMs reason on lateral thinking puzzles similar to\nhumans? 2) How do LLMs perform on reconstruc-\ntion variants? 3) Are model predictions consistent\nacross partitions? 4) Does tuning on commonsense\nknowledge help to answer BRAIN TEASER puzzles\nbetter? 5) Can LLMs do better in the few-shot\nsetting with more demonstrations?\nOverall Performance The main results are\nshown in Table 4. For both word and sentence\nBRAIN TEASER puzzles, the performance of the\n14322\nTable 4: Main zero-shot results over two BRAIN TEASER subtasks across all models in all metrics: Ori = Original,\nSem = Semantic, Con = Context. The best performance among all models is in bold, and the best performance in\ncommonsense augmented models is underlined. The human evaluation (*) is computed over 102 randomly sampled\ndata. The random base is average over three different seeds.\nInstance-based Group-based OverallCategory Model Original Semantic Context Ori & Sem Ori & Sem & Con\nSentence Puzzle\nRandom - 25.52 24.88 22.81 5.58 1.44 24.40\nInstruction\nFlanT5(780M) 18.66 16.27 22.01 10.53 4.31 18.98\nFlanT5(3B) 26.79 25.36 35.41 20.10 12.92 29.19\nFlanT5(11B) 33.49 31.58 36.84 22.01 11.00 33.97\nT0(11B) 22.01 22.01 29.67 16.27 11.00 24.56\nT0P(11B) 23.92 22.49 34.93 17.70 11.96 27.11\nT0PP(11B) 26.32 27.27 37.80 19.14 11.96 30.46\nChatGPT 60.77 59.33 67.94 50.72 39.71 62.68\nCommonsense\nRoBERTa-L 43.54 40.19 46.41 33.01 20.10 43.38\nRoBERTa-L(CSKG) 35.41 36.84 44.98 28.71 18.18 39.07\nCAR 10.53 10.53 11.48 5.74 2.39 10.85\nHuman∗ - 90.74 90.74 94.44 90.74 88.89 91.98\nWord Puzzle\nRandom - 26.02 27.85 22.51 7.32 1.83 25.34\nInstruction\nFlanT5(780M) 22.56 17.68 28.66 9.15 3.66 22.97\nFlanT5(3B) 37.80 29.88 42.68 23.17 12.80 36.79\nFlanT5(11B) 42.68 32.93 43.90 28.66 20.12 39.84\nT0(11B) 17.07 14.02 23.17 9.76 6.10 18.09\nT0P(11B) 28.66 26.22 34.15 19.51 12.80 29.67\nT0PP(11B) 33.54 31.10 39.63 20.12 10.98 34.76\nChatGPT 56.10 52.44 51.83 43.90 29.27 53.46\nCommonsense\nRoBERTa-L 19.51 19.51 23.17 14.63 6.10 20.73\n- RoBERTa-L(CSKG) 18.90 16.46 30.49 12.80 6.10 21.95\nCAR 38.41 31.10 20.12 26.22 6.10 29.88\nHuman∗ - 91.67 91.67 91.67 91.67 89.58 91.67\nstrongest model, ChatGPT (53 and 63%) is halfway\nbetween random (25%) and human performance\n(92%). In general, neither type of model is able to\nperform consistently well across the two subtasks:\ninstruction-based models perform better on word\npuzzles, whereas commonsense models perform\nslightly better on sentence puzzles. The perfor-\nmance of the models is often close to random, with\naround a third of the models performing equal or\nworse than random guessing. As it can be expected,\nwe see that scaling up instruction-finetuned models\nleads to improved performance on both subtasks.\nYet, the large gap between human and model per-\nformance clearly shows that even the most power-\nful LLMs are unable to exhibit lateral thinking in\nmultiple-choice puzzles and confirms the challeng-\ning nature of our BRAIN TEASER dataset.\nOriginal vs Reconstruction Partitions In most\ncases, all models and humans perform the best on\nthe context reconstruction partition. We hypoth-\nesize that this is because original lateral thinking\npuzzles are designed to mislead humans to a wrong\nchoice based on commonsense associations, often\ninvolving rare words and unconventional sentence\nstructures. Meanwhile, we note that our contextual\nreconstruction mechanism yields puzzles that are\nmore familiar or easier to solve than the original\npuzzle, possibly because some of the commonsense\nassociations are relatively weaker. An exception\nto this trend is ChatGPT’s performance on word\npuzzles, where ChatGPT performs the best on the\noriginal examples. We believe that this is due to a\ncombination of two factors. First, the word puzzle\nreconstructions only have a limited impact on the\nvocabulary domain and sentence structure, because\nof the much shorter questions. Second, ChatGPT\nmay have memorized some of the word puzzles,\ne.g., given the question \"How do you spell COW\nin thirteen letters?\", its answer begins with \"The\nquestion seems to be a brain teaser . . .\" We pro-\nvide representative examples of the prevalent lateral\nthinking errors of memorization and commonsense\nassociations in Table 5.\nConsistency of Model Predictions We further\ncompare the performance on instance- and group-\nbased metrics to understand whether the models\ncan solve lateral thinking puzzles by following a\nconsistent reasoning path. A model understand-\n14323\nTable 5: Error analysis on memorization and commonsense association.\nQuestion Answer LLM choice\nMemorization\nThe man calls his dog on the other side of the river, and the dog The river was frozen. The river was frozen.\ncrosses the river without getting wet and using ant tools.\nThe man had to cross the rivers. He can’t swim or use any tools The river was frozen. He jumped a half-mile\nlike the bridge. How does the man succeed in the end? far to across the river.\nCommonsense Association\nWhat animal has no wings, but yet will fly? A caterpillar. An eagle.\nThere is no light on the road and the car’s headlight is broken. It was daytime. The driver is good\nHow can the driver see the black dog? at listening .\nHow can Jenny read in a totally no light house at night? The book is in Braille. It was daytime.\ning rather than memorizing the reasoning path of\nthe original brain teaser should be able to answer\nits adversarial reconstructions with ease. Notably,\nhuman performance only has a minimal drop on\ngroup-based metrics whereas all models suffer sig-\nnificant drops. Further analysis (see Appendix A.6)\nreveals that ChatGPT and RoBERTa-L fail to an-\nswer many (45 and 61%, respectively) of the origi-\nnal or semantically changed puzzles when contextu-\nally translated puzzles are solved correctly. These\nobservations suggest that the ability of the models\nto perform consistent lateral thinking is far from\nhuman ability.\nImpact of Commonsense Knowledge We ob-\nserve that commonsense knowledge has a salient\nnegative impact on the model’s performance on\nsentence puzzles. The best-performing model in\nthe commonsense category is the vanilla RoBERTa\nmodel, whose adaptation with commonsense\nknowledge leads to a significant drop in results, es-\npecially with the CAR method. This trend confirms\nour initial hypothesis that learning commonsense\nassociations is generally detrimental to complex lat-\neral thinking tasks. Commonsense knowledge has\na limited positive impact on the word-puzzle task,\npossibly because much of the commonsense associ-\nations learned by these models hold between words,\nincluding synonyms. Finally, given the apparent\nsimilarity of riddles and lateral thinking puzzles,\nwe finetuned a RoBERTa model on the Riddle-\nSense dataset and evaluated it on our task. Again,\nwe observe that the model struggles on solving the\npuzzles despite gaining better results compared to\nthe vanilla RoBERTa model (see Appendix A.7).\nImpact of Few-Shot Demonstrations As LLMs\nare good few-shot learners (Brown et al., 2020b),\nwe are interested to see if in-context learning can\nhelp them better solve our task. We experiment\nwith our two most powerful models: ChatGPT\nFigure 2: Few-shot prompting performance of ChatGPT\nand FlanT5(11B).\nand FlanT5 (11B). We randomly pick 8 puzzles\n(4 from each subtask) and create new context re-\nconstructions as demonstrations. We experiment\nwith few-shot prompting with 2, 4, 6, and 8 of these\ndemonstrations, balanced between the two subtasks.\nThe few-shot results are shown in Figure 2, and we\npresent the full results in Appendix A.4. The num-\nber of few-shot demonstrations has no clear impact\non sentence puzzles, which confirms that lateral\nthinking puzzles are unique and the models can\nhardly learn generalizable patterns from in-context\nexamples. Providing more few-shot demonstra-\ntions has a marginal positive impact for word puz-\nzles. Given this task’s focus on the letter compo-\nsition of each word, the in-context examples may\nbe used to teach the model to pay attention to the\nsurface form of the answer candidates. It’s worth\nnoting that, although few-shot examples might ex-\nhibit superficial resemblances, their contribution to\nmodel generalization for sentence puzzles remains\n14324\nminimal, given the abstract nature of reasoning\npattern in this subtask.\nQualitative Error Analysis We analyze two\nprevalent lateral thinking errors in the ChatGPT and\nFlanT5 (11b) LLMs: memorization and common-\nsense associations, both of which become more\napparent with scaling up (Carlini et al., 2022). We\nshow examples in Table 5.\nMemorization We find that memorization hap-\npens in both subtasks. Given the sentence puzzle\n\"The man calls his dog on the other side of the river,\ncrosses the river without getting wet and using ant\ntools.\" the LLMs picked the correct answer \" The\nriver was frozen.\" for both the original and its se-\nmantic reconstruction. However, when the question\nin a new context becomes \"The man had to cross\nthe rivers. He can’t swim or use any tools. like the\nbridge. How does the man succeed in the end? \",\nall LLMs failed to answer. Memorization is more\nfrequent in word puzzles. A semantic reconstruc-\ntion will cause confusion in the model, as is also\napparent from the gap between original accuracy\nand the ori&sem accuracy in Table 4.\nCommonsense association Similarly, we also\nfind that commonsense association often confuses\nLLMs. For example, for \" What animal has no\nwings, but yet will fly?\", the models associate the\nwords \"wings\" and \" fly” with birds and pick the\nwrong answer \"An eagle.\" despite the contradiction\nbetween \"eagle\" and \"no wings\". Meanwhile, the\ncorrect lateral thinking answer \"A caterpillar.\" is\nnot picked by the models. Interestingly, common-\nsense associations that mislead models in some\nexamples can be the needed hint in others. For\nexample, in one puzzle, \"There is no light on the\nroad and the car’s headlight is broken. How can\nthe driver see the black dog?\", the answer \"It was\ndaytime.\" is hindered by the commonsense associa-\ntion between mentioning no light and night. How-\never, in another example, \" How can Jenny read\nin a totally no light house at night? \", the same\ncommonsense association leads the model to the\ncorrect answer: \" The book is in Braille. \". In the\nsecond example, the answer is misled by another\ncommonsense association related to reading.\n6 Conclusions and Outlook\nWe defined the task of lateral thinking for LLMs,\nformulated as a multiple-choice QA with a\nsentence- and word-level puzzles. We developed\nBRAIN TEASER , a 1.1K lateral thinking benchmark\nthat combines original puzzles and their reconstruc-\ntion variants. Our experiments showed that Chat-\nGPT’s performance on this task is halfway between\nrandom and humans, whereas other models often\nperform close to random. While scaling up model\nsize improved performance, enriching with com-\nmon sense or providing few-shot demonstrations\nyielded limited benefits. Meanwhile, all models\ntend to solve the variants of the same puzzle in-\nconsistently. Our error analysis showed that the\nmodels’ lateral thinking is often hindered by memo-\nrization and misleading commonsense associations.\nIn the future, we intend to develop lateral thinking\nmodels, create additional lateral thinking evalua-\ntion tasks (e.g., relating to alteration (De Bono,\n1970)), and investigate flexible ways to combine\nlateral and vertical thinking.\nLimitations\nWhile our work focuses on both Sentence puzzles\nand Word puzzles, we intend to develop a compre-\nhensive lateral thinking benchmark according to\nde Bono’s four skills: awareness, random stimula-\ntion, alternatives, and alteration (De Bono, 1970).\nMoreover, while our paper tries to provide a clear\ndistinction between lateral and vertical thinking,\nit remains an open question to which extent other\nbrain teaser categories, e.g. puns and visual puz-\nzles, require lateral or vertical thinking. As these\ntasks are not the focus of our present paper, we\nleave it to future work to comprehensively evalu-\nate models’ ability to think out of the box on such\ntasks and to characterize the complementary and\nopposing aspects of vertical and lateral thinking.\nAlso, we opt for constructing the dataset in a\nmultiple-choice QA format to reduce the burden\nof the evaluation process. However, this inevitably\nreduces the difficulty of the task and permits the\nsituation where the models solve the questions cor-\nrectly for the wrong reasons. Future work should\nalso look into better evaluation metrics that are\nsuitable for creative and open-ended generations\nsuch that our task can also be adapted to an open-\nended setting. Finally, while our current puzzles\nare provided in a static manner, future work should\nalso investigate an interactive (multi-step) setup,\nwhere the model (or human) may ask clarification\nquestions or receive contextual hints.\n14325\nEthical Considerations\nAs our lateral thinking puzzles are “folk knowl-\nedge” and are published on a range set of websites,\nit is hard to check their original licenses compre-\nhensively. Yet, the website owners declare per-\nmission to print and download material for non-\ncommercial use without modification on the mate-\nrial’s copyright. Therefore, we provide the corre-\nsponding copyright statements and website URLs\nfor each original lateral thinking puzzle and its re-\nconstruction version. In addition, we will create a\nform to ask future dataset users to sign a document\nclaiming that the only aim of the data usage is re-\nsearch before providing them with the data. We\nnote that, despite our best efforts, the task data may\nstill contain bias in terms of gender or politics. We\nwill indicate that future research should use the task\ndata with caution.\nReferences\nMostafa Abdou, Vinit Ravishankar, Maria Barrett,\nYonatan Belinkov, Desmond Elliott, and Anders Sø-\ngaard. 2020. The sensitivity of language models\nand humans to Winograd schema perturbations. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7590–\n7604, Online. Association for Computational Lin-\nguistics.\nMeryem Altun, Muhsin Hazar, and Zekihan Hazar. 2016.\nInvestigation of the effects of brain teasers on atten-\ntion spans of pre-school children. International jour-\nnal of environmental and science education, 11:8112–\n8119.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nMaya Bar-Hillel, Tom Noah, and Shane Frederick.\n2018. Learning psychology from riddles: The\ncase of stumpers. Judgment and Decision Making,\n13(1):112–122.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nMichael Boratko, Xiang Li, Tim O’Gorman, Rajarshi\nDas, Dan Le, and Andrew McCallum. 2020. Pro-\ntoQA: A question answering dataset for prototypi-\ncal common-sense reasoning. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1122–1136,\nOnline. Association for Computational Linguistics.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for knowl-\nedge graph construction. In Association for Compu-\ntational Linguistics (ACL).\nRuben Branco, António Branco, João António Ro-\ndrigues, and João Ricardo Silva. 2021. Shortcutted\ncommonsense: Data spuriousness in deep learning\nof commonsense reasoning. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1504–1521, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020b. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nEdward De Bono. 1970. Lateral thinking. New York.\n14326\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nStephen W Draper. 2009. Catalytic assessment: under-\nstanding how mcqs and evs can foster deep learn-\ning. British Journal of Educational Technology ,\n40(2):285–293.\nYanai Elazar, Hongming Zhang, Yoav Goldberg, and\nDan Roth. 2021. Back to square one: Artifact detec-\ntion, training and commonsense disentanglement in\nthe Winograd schema. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10486–10500, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nGiorgio Franceschelli and Mirco Musolesi. 2023. On\nthe creativity of large language models.\nGe Gao, Eunsol Choi, Yejin Choi, and Luke Zettle-\nmoyer. 2018. Neural metaphor detection in context.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n607–613, Brussels, Belgium. Association for Com-\nputational Linguistics.\nYoav Goldberg. 2023. Two kinds of recall. arXiv\npreprint arXiv:2303.10527.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection.\nScott Highhouse, Christopher D Nye, and Don C Zhang.\n2019. Dark motives and elective use of brainteaser\ninterview questions. Applied Psychology, 68(2):311–\n340.\nJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2021. Comet-atomic 2020: On sym-\nbolic and neural commonsense knowledge graphs. In\nAAAI.\nFilip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin\nZhang, Deborah L McGuinness, and Pedro Szekely.\n2021a. Dimensions of commonsense knowledge.\nKnowledge-Based Systems.\nFilip Ilievski, Pedro Szekely, and Bin Zhang. 2021b.\nCskg: The commonsense knowledge graph. In The\nSemantic Web: 18th International Conference, ESWC\n2021, Virtual Event, June 6–10, 2021, Proceedings\n18, pages 680–696. Springer.\nMyeongjun Jang, Deuk Sin Kwon, and Thomas\nLukasiewicz. 2022. Becel: Benchmark for consis-\ntency evaluation of language models. In Proceedings\nof the 29th International Conference on Computa-\ntional Linguistics, pages 3680–3696.\nBill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee,\nand Xiang Ren. 2021. Riddlesense: Reasoning\nabout riddle questions featuring linguistic creativ-\nity and commonsense knowledge. arXiv preprint\narXiv:2101.00376.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavatula,\nand Yejin Choi. 2021. Unicorn on rainbow: A uni-\nversal commonsense reasoning model on a new mul-\ntitask benchmark. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 35, pages\n13480–13488.\nKaixin Ma, Jonathan Francis, Quanyang Lu, Eric Ny-\nberg, and Alessandro Oltramari. 2019. Towards gen-\neralizable neuro-symbolic systems for commonsense\nquestion answering. In Proceedings of the First\nWorkshop on Commonsense Inference in Natural Lan-\nguage Processing, pages 22–32, Hong Kong, China.\nAssociation for Computational Linguistics.\nKaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan\nBisk, Eric Nyberg, and Alessandro Oltramari. 2021a.\nKnowledge-driven data construction for zero-shot\nevaluation in commonsense question answering. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 13507–13515.\nKaixin Ma, Filip Ilievski, Jonathan Francis, Satoru\nOzaki, Eric Nyberg, and Alessandro Oltramari.\n2021b. Exploring strategies for generalizable\ncommonsense reasoning with pre-trained models.\nEMNLP 2021.\nAdyasha Maharana and Mohit Bansal. 2022. On cur-\nriculum learning for commonsense reasoning. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 983–992, Seattle, United States. Association\nfor Computational Linguistics.\nGary Marcus. 2022. Deep learning is hitting a wall.\nNautilus, Accessed, pages 03–11.\n14327\nJ. A. Meaney, Steven Wilson, Luis Chiruzzo, Adam\nLopez, and Walid Magdy. 2021. SemEval 2021 task\n7: HaHackathon, detecting and rating humor and\noffense. In Proceedings of the 15th International\nWorkshop on Semantic Evaluation (SemEval-2021),\npages 105–119, Online. Association for Computa-\ntional Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901, Online. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4932–4942, Florence, Italy. Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2019. Winogrande: An adver-\nsarial winograd schema challenge at scale.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\net al. 2022. Multitask prompted training enables\nzero-shot task generalization. In International Con-\nference on Learning Representations.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A. Smith, and Yejin Choi.\n2019a. Atomic: An atlas of machine commonsense\nfor if-then reasoning. In AAAI Conference on Artifi-\ncial Intelligence.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019b. Social iqa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 4463–4473.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI confer-\nence on artificial intelligence, volume 31.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of NAACL-HLT, pages 4149–\n4158.\nSowmya Vajjala and Detmar Meurers. 2012. On im-\nproving the accuracy of readability classification us-\ning insights from second language acquisition. In\nProceedings of the seventh workshop on building ed-\nucational applications using NLP, pages 163–173.\nShlomo Waks. 1997. Lateral thinking and technology\neducation. Journal of Science Education and Tech-\nnology, 6:245–255.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. V oyager: An open-ended\nembodied agent with large language models.\nWeiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan\nXu, Xin Liu, Yangqiu Song, and Antoine Bosse-\nlut. 2023b. Car: Conceptualization-augmented rea-\nsoner for zero-shot commonsense question answer-\ning. arXiv preprint arXiv:2305.14869.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2023. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. In Advances\nin Neural Information Processing Systems.\nYichong Xu, Chenguang Zhu, Shuohang Wang, Siqi\nSun, Hao Cheng, Xiaodong Liu, Jianfeng Gao,\nPengcheng He, Michael Zeng, and Xuedong Huang.\n2022. Human parity on commonsenseqa: Augment-\ning self-attention with external attention. Proceed-\nings of the Thirty-First International Joint Confer-\nence on Artificial Intelligence.\n14328\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\nYanyan Zou and Wei Lu. 2019. Joint detection and\nlocation of English puns. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2117–2123, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nA Appendix\nA.1 Puzzle topics\nWe use few-shot prompting in ChatGPT to extract\nthe context topic for each question. Table 6 shows\nthe top 10 topics for each subtask, for which the\nprompting template is as follows:\n\" Can you provide context environment in the fol-\nlowing brain teasers? Here are several examples:\n{examples}\"\nTable 6: Top-10 topics extracted for both subtasks.\nSentence Puzzle Word Puzzle\nTopic Frequency Topic Frequency\nMathematics 45 Language 79\nPhysics 41 Food 27\nNature 37 Mathematics 26\nTransportation 32 Animals 24\nAnimals 25 Science 22\nSports 24 Time 18\nFamily 23 Geography 16\nTime 19 Nature 13\nEducation 16 Entertainment 12\nLaw 16 Finance 11\nOthers 339 Others 244\nA.2 Prompting templates\nChatGPT We use the following instruction to\nprompt ChatGPT:\n\"Please pick the best choice for the brain teaser.\nEach brain teaser has only one possible solution,\nincluding the choice none of the above, answer\nshould only provide the choice:\"\nFlanT5 We use the instruction template for the AI2\nReasoning Challenge (ARC) (Clark et al., 2018):\n\"Question: {Question}\nWhat is the correct answer to the question\nfrom the following choices?\n(A) {choice}\n(B) {choice}\n(C) {choice}\n(D) {choice}\"\nT0PP We use the instruction template for the Com-\nmensenseQA task (Talmor et al., 2019):\n\"{Question}\nChoose the most suitable option to answer the\nabove question.\nOptions:\nA. {choice}\nB. {choice}\nC. {choice}\n14329\nD. {choice}\"\nA.3 Word puzzle example\nTable 7 presents a word puzzle with its reconstruc-\ntion examples.\nA.4 Few-shot prompting result\nTable 8 shows the few-shot result of ChatGPT and\nFlanT5 (11B) on the two BRAIN TEASER subtasks.\nA.5 Annotation Details\nHuman evaluation We give the following instruc-\ntion to human evaluation participants:\n\"Hi, welcome to the brain teaser test. Each brain\nteaser has only one possible solution (none of the\nabove is possible!). Please select the choice in the\nanswer column. Try to Think out of Box :)\"\nHuman validation We give the following instruc-\ntion:\n\"Congratulations on passing the brain teaser test.\nYou should notice that some brain teasers are\nsimilar to each other :)! Actually, the brain teasers\ncan be divided in groups like the following: In each\nbrain teaser group, we have an original question,\nsemantic reconstruction questions, and context\nreconstruction questions. Semantic reconstruction\nquestion rephrases the original question without\nchanging the correct answer and the distractors.\nContext reconstruction question keeps the original\nreasoning path but changes both the question and\nthe answer to describe a new situational context.\nPlease help with the following three tasks:\n1)Whether the original question and its answer\nmake sense. 2)Whether the semantic reconstruc-\ntion question rephrases the original question.\n3)Whether the context reconstruction question\nkeeps the original reasoning path.\"\nOpen-ended Human Performance We give the\nfollowing instruction:\n\"Please write down the answer of each brain teaser.\nAnything that makes sense is welcome!! Also, no\nanswer is acceptable!\"\nWe let both humans and ChatGPT write down\nthe most possible answer to 30 context reconstruc-\ntion questions based on their understanding. Three\nexperts score the answers on a scale of 5, based on\nthe following rubrics:\n• score 0: Fail to answer.\n• score 1: Try to answer the question, but the\nanswer doesn’t make sense.\n• score 2: The answer is wrong but related to\nthe golden label.\n• score 3: The answer is wrong, but the reason-\ning strategy is similar to the golden answer\nand may lack some keywords.\n• score 4: The answer is wrong but lacks minor\ninformation. Or the answer makes sense but\nis not the same as the golden answer.\n• score 5: The answer is correct.\nBoth humans and LLMs cannot perform this\ntask well, scoring 2.64 and 2.62 on a 5-point scale.\nHumans give up more often (18%) rather than gen-\nerating meaningless text like ChatGPT, making the\ncomparison harder if the task is in an open-end\nformat.\nA.6 Evidence of stronger distractors in the\noriginal puzzle\nThe barber example in Figure 1, \"shaves everyday\"\nand \"keep his beard long\" triggers a commonsense\nassociation that the man shaves himself every day.\nThe contextually reconstructed puzzle of the barber\nexample is \"How can a man go to football team ev-\nery day but doesn’t play football at all?\". This new\nquestion still aims to guide the model to think in the\ndefault commonsense way that \" He is a football\nplayer.\" but the correct answer \" He is a coach. \"\nis also highly probable, resulting in an inherent\ndecrease in difficulty.\nA.7 Fine-tuned on Riddle Sense\nWe finetuned RoBERTa-L on RiddleSense (Lin\net al., 2021) to analyze whether being aware of\nlinguistic creativity can enhance the model’s per-\nformance on BRAIN TEASER . We train RoBERTa-\nL (RS) on the training data of RiddleSense in 3\nepochs with a learning rate at 1e−6, batch size at\n4. RoBERTa-L (RS) reaches 59.95 on the valida-\ntion set, which is on par with the original paper\n(60.72). We then adapt Roberta-L (RS) to do the\nzero-shot evaluation on BRAIN TEASER . The re-\nsults are shown in Table 9.\nEven though Roberta-L (RS) already gains in-\nsight into creative thinking, it is still struggling on\nBRAIN TEASER . The better results show that en-\nhancing creative thinking during the training may\n14330\nTable 7: Overview of a word puzzle example and its reconstruction versions.\nAdv Strategy Question Answers\n-\nThe letter N.\nWhat part of London is in France? The letter O.\nThe letter L.\nNone of the above.\nThe letter N.\nSemantic Re- Which area of London is inside France? The letter O.\nconstruction The letter L.\nNone of the above.\nThe letter A.\nContext Re- What part of Korea is in China? The letter E.\nconstruction The letter R.\nNone of the above.\nTable 8: Main few-shot results of ChatGPT and FlanT5(11B) on two BRAIN TEASER subtasks. Ori = Original, Sem\n= Semantic, Con = Context. The best performance among all models is in bold.\nInstance-based Group-based OverallModel Original Semantic Context Ori & Sem Ori & Sem & Con\nSentence puzzle\nChatGPT(zero-shot) 60.77 59.33 67.94 50.72 39.71 62.68\nChatGPT(two-shot) 61.72 60.77 68.90 51.67 40.67 63.80\nChatGPT(four-shot) 59.33 55.98 62.20 47.85 32.06 59.17\nChatGPT(six-shot) 60.29 59.81 66.51 51.20 40.19 62.20\nChatGPT(eight-shot) 63.16 62.68 67.46 54.55 44.02 64.43\nFlanT5(zero-shot) 33.49 31.58 36.84 22.01 11.00 33.97\nFlanT5(two-shot) 37.80 33.49 38.76 26.79 13.40 36.68\nFlanT5(four-shot) 38.28 34.45 41.15 26.79 13.40 37.96\nFlanT5(six-shot) 38.28 34.45 41.63 27.27 13.88 38.12\nFlanT5(eight-shot) 38.76 33.01 41.63 26.79 14.35 37.80\nWord puzzle\nChatGPT(zero-shot) 56.10 52.44 51.83 43.90 29.27 53.46\nChatGPT(two-shot) 55.49 53.66 51.22 44.51 30.49 53.46\nChatGPT(four-shot) 54.27 53.66 51.83 43.90 28.05 53.25\nChatGPT(six-shot) 56.71 51.83 54.27 45.12 28.66 54.27\nChatGPT(eight-shot) 58.54 56.71 54.27 48.17 34.76 56.50\nFlanT5(zero-shot) 42.68 32.93 43.90 28.66 20.12 39.84\nFlanT5(two-shot) 44.51 34.76 45.73 30.49 18.90 41.67\nFlanT5(four-shot) 43.29 35.98 47.56 30.49 20.73 42.28\nFlanT5(six-shot) 44.51 36.59 47.56 29.88 17.68 42.89\nFlanT5(eight-shot) 45.73 33.54 46.95 27.44 16.46 42.07\nbe a possible solution to defying commonsense.\nYet, we note that the performance of this model\nalso declines on the group-based metrics. More-\nover, we point out the possible data distribution\noverlap between BRAIN TEASER and RiddleSense,\nas RiddleSense was collected publicly from similar\nonline websites and contains much more samples\n(5.7k) than BRAIN TEASER .\nA.8 Human Annotator Information\nOur human annotators major in computer science\ncome from East Asia, Europe and the Middle East.\nAll annotators all fluent in English.\n14331\nTable 9: RoBERTa-L (RS) zero-shot results over two BRAIN TEASER subtasks.\nInstance-based Group-based OverallModel Original Semantic Context Ori & Sem Ori & Sem & Con\nSentence Puzzle\nRoBERTa-L(RS) 42.11 45.93 54.55 37.32 27.75 47.53\nWord Puzzle\nRoBERTa-L(RS) 23.78 26.22 31.10 20.73 9.76 27.03\n14332",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7762445211410522
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.6823482513427734
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6456308364868164
    },
    {
      "name": "Adversarial system",
      "score": 0.6417063474655151
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5900624394416809
    },
    {
      "name": "Task (project management)",
      "score": 0.5733497142791748
    },
    {
      "name": "Language model",
      "score": 0.559318482875824
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5560135245323181
    },
    {
      "name": "Question answering",
      "score": 0.5184573531150818
    },
    {
      "name": "Language understanding",
      "score": 0.4884733259677887
    },
    {
      "name": "Natural language processing",
      "score": 0.435769259929657
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.43570923805236816
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}