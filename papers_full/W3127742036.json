{
  "title": "Transformers with Competitive Ensembles of Independent Mechanisms",
  "url": "https://openalex.org/W3127742036",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2984527647",
      "name": "Lamb, Alex",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098476997",
      "name": "He, Di",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223651720",
      "name": "Goyal, Anirudh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227149612",
      "name": "Ke, Guolin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4299748205",
      "name": "Liao, Chien-Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226317563",
      "name": "Ravanelli, Mirco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751323908",
      "name": "Bengio, Yoshua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3092792322",
    "https://openalex.org/W3135839756",
    "https://openalex.org/W171902450",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3032371044",
    "https://openalex.org/W2995849700",
    "https://openalex.org/W3037620288",
    "https://openalex.org/W2137440383",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W3038896498",
    "https://openalex.org/W2964118342",
    "https://openalex.org/W1983108229",
    "https://openalex.org/W3035810926",
    "https://openalex.org/W3048758284",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2194690080",
    "https://openalex.org/W2767175863",
    "https://openalex.org/W3046669506",
    "https://openalex.org/W2902227449",
    "https://openalex.org/W22861983",
    "https://openalex.org/W3016129867",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W2072128103",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2128653836",
    "https://openalex.org/W2988309730",
    "https://openalex.org/W3108981297",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3087365230",
    "https://openalex.org/W2153894152",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W3042128741",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3030102521",
    "https://openalex.org/W2976023236",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2559823555",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W3037655549",
    "https://openalex.org/W1552314771",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964067718",
    "https://openalex.org/W3027637851",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2121973264",
    "https://openalex.org/W2583761661",
    "https://openalex.org/W2563346237"
  ],
  "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated. This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed. For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically. In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation. This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms. To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention. Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent. We study TIM on a large-scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.",
  "full_text": "Transformers with Competitive Ensembles of Independent Mechanisms\nAlex Lamb 1 Di He 2 Anirudh Goyal 1 Guolin Ke 2 Chien-Feng Liao 1 3 Mirco Ravanelli1 Yoshua Bengio1\nAbstract\nAn important development in deep learning from\nthe earliest MLPs has been a move towards ar-\nchitectures with structural inductive biases which\nenable the model to keep distinct sources of infor-\nmation and routes of processing well-separated.\nThis structure is linked to the notion of indepen-\ndent mechanisms from the causality literature, in\nwhich a mechanism is able to retain the same\nprocessing as irrelevant aspects of the world are\nchanged. For example, convnets enable separa-\ntion over positions, while attention-based archi-\ntectures (especially Transformers) learn which\ncombination of positions to process dynamically.\nIn this work we explore a way in which the Trans-\nformer architecture is deﬁcient: it represents each\nposition with a large monolithic hidden repre-\nsentation and a single set of parameters which\nare applied over the entire hidden representation.\nThis potentially throws unrelated sources of in-\nformation together, and limits the Transformer’s\nability to capture independent mechanisms. To\naddress this, we propose Transformers with Inde-\npendent Mechanisms (TIM), a new Transformer\nlayer which divides the hidden representation and\nparameters into multiple mechanisms, which only\nexchange information through attention. Addi-\ntionally, we propose a competition mechanism\nwhich encourages these mechanisms to specialize\nover time steps, and thus be more independent.\nWe study TIM on a large-scale BERT model, on\nthe Image Transformer, and on speech enhance-\nment and ﬁnd evidence for semantically mean-\ningful specialization as well as improved perfor-\nmance.\n1Mila, University of Montreal 2Microsoft Research Asia\n3Research Center for Information Technology Innovation,\nAcademia Sinica. Correspondence to: Alex Lamb <lam-\nbalex@iro.umontreal.ca>, Di He <dihe@microsoft.com>.\n1. Introduction\nA major theme throughout the history of deep learning has\nbeen the introduction of inductive biases in neural architec-\ntures, more recently with a focus on the ability to dynam-\nically keep distinct types of information separated. While\nan MLP architecture has one large hidden representation at\neach layer, a convnet keeps different spatial positions’ rep-\nresentations separated by default. This separation enables\nmore appropriate reuse of parameters, improving general-\nization (e.g. compared with a fully connected MLP) by\nensuring that some parts of the hidden representation cap-\nturing some aspects of the data can remain unchanged when\nother aspects are changed. Additionally, it is important to be\nable to reuse parameters in all situations where the parame-\nters are relevant, and not use parameters in positions where\nthey are irrelevant, and this is where attention mechanisms\ncan be very useful.\nWhile dividing information between different positions (for\nexample time steps or spatial positions) is already very use-\nful, it has been recognized from the earliest deep learning\nwork on the notion of disentangling (Bengio, 2009; Glorot\net al., 2011; Rifai et al., 2012; Mathieu et al., 2016; Achille\n& Soatto, 2018) that other features of the data could ad-\nvantageously be kept well-separated, even over overlapping\nsets of positions. This has suggested the idea that a model\ncan be decomposed into multiple components, which are\noften called modules, each operating on a different set of\nfeatures. Modularity has been identiﬁed as an essential in-\ngredient for generalization in machine learning (Jacobs et al.,\n1991; Bottou & Gallinari, 1991; Ronco et al., 1997; Reed\n& De Freitas, 2015; Andreas et al., 2016; Rosenbaum et al.,\n2017; Fernando et al., 2017; Shazeer et al., 2017; Goyal\net al., 2019; 2020; Rahaman et al., 2020; Goyal & Bengio,\n2020). The motivating intuition is that if the relationship\nbetween the modules changes between training and evalua-\ntion, then a model which keeps these modules sufﬁciently\nseparate but can adapt how they are combined could be more\nrobust. It can even be robust to changes where the overall\ndata distribution differs between training and evaluation.\nThis has been studied in the causality literature through the\nnotion of “Independent Mechanisms” (Peters et al., 2018) or\ncausal modules, which can be ﬂexibly re-used, re-purposed\nand re-combined.\narXiv:2103.00336v1  [cs.LG]  27 Feb 2021\nTransformers with Competitive Ensembles of Independent Mechanisms\nWhile modularity and independent mechanisms ideas are\nclosely related, the latter has a special focus on the no-\ntion that mechanisms should have the ability to remain un-\nchanged when unrelated aspects of the world are changed.\nIn that sense it is a more speciﬁc idea which builds on the\nmore general concept of modularity. While the study of\nindependent mechanisms in the context of deep architec-\ntures is relatively recent (Goyal et al., 2019; Mittal et al.,\n2020; Goyal et al., 2020), a few ideas are considered central.\nOne is that mechanisms are separately parameterized (or\ndynamically parameterized, with the possibility of separa-\ntion), which means that the function computed by a module\nremains the same even as other mechanisms need to be\nchanged. Another central idea is specialization between\nmechanisms, which is the idea that mechanisms should seek\nto only model some parts of the world. One way to help\naccomplish this is by forcing the mechanisms to compete to\nexplain different positions (in time or space), such that some\nmechanisms would not be used by the model on positions\nwhere they are less relevant.\nIn this work we explore how the idea of independent mech-\nanisms can be beneﬁcial in the Transformer architecture.\nTransformers (Vaswani et al., 2017) are based on informa-\ntion sharing across positions controlled dynamically by a\nsoft-attention mechanism (Bahdanau et al., 2014), while\nstill using a fully-connected MLP to process the extracted\nfeature vectors (concatenated over a set of attention heads)\nat each position. An important way in which this improves\nover convnets is that if this attention becomes sufﬁciently\nsparse, then it gains the ability to keep information well-\nseparated between different positions. At the same time, at\neach position, the Transformer stores a single monolithic\nhidden representation, over which it applies its entire set of\nparameters. For example, if we consider a generative model\nof images of animals in a ﬁeld, then some of the parameters\nlike those describing how animals have symmetric eyes or\na certain number of feet, are only relevant for the positions\nin the image where the animal is present. A normal Trans-\nformer, however, would apply the same parameters to the\nentire hidden representation at all spatial positions. Addi-\ntionally, if sources of information need to be accessed over\nmultiple positions, it has no way to keep that information\nwell-separated between parts of the hidden representation,\nunless a large fraction of the parameters are set to exactly\nzero. In practice, models tend not to learn these sorts of\nhighly sparse parameter matrices as it is not necessary in\norder to ﬁt the training set. Thus different underlying factors\ntend to be freely blended together rather than disentangled:\nwe hypothesize and show empirically that this leads to dete-\nriorated generalization when something about some of these\nfactors changes.\nOur newly proposed technique, which we call Transform-\ners with Competitive Independent Mechanisms (TIM)seeks\nFigure 1.We show the TIM model with two positions and three\nmechanisms. First, position-wise attention shares information\nbetween positions while keeping the different mechanisms well-\nseparated (which can then be weighted by the competition between\nmechanisms). Next, attention is used to share information between\nthe mechanisms at each position. Finally, a position-wise and\nmechanism-wise feed-forward layer (including layer norm and\nresidual connections) produces the TIM layer’s ﬁnal output.\nto address this limitation of the Transformer by dividing\nthe hidden representation and parameters into multiple dis-\ntinct mechanisms. These mechanisms perform self-attention\n(over input elements) separately, and information is ex-\nchanged sparingly between the mechanisms using attention.\nThe model is naturally compelled to keep multiple informa-\ntion signals well-separated, even within a single position.\nThe process of selectively activating some mechanisms and\nnot others relies on competition between mechanisms, just\nlike in recurrent independent mechanism (RIMs) (Goyal\net al., 2019). We hypothesize and show empirically that\nthis provides an inductive bias encouraging the mechanisms\nto be more independent and specialized, more robust to\nchanges only affecting other mechanisms.\n2. Transformers with Competitive\nIndependent Mechanisms\n2.1. Preliminaries\nMultihead Self-attention sub-layer The attention mecha-\nnism can be formulated as querying a dictionary with key-\nvalue pairs (Bahdanau et al., 2014; Vaswani et al., 2017),\ne.g., Attention(Q,K,V ) =softmax(QKT /√dmodel) ·V,\nwhere dmodel is the dimensionality of the hidden represen-\ntations and Q(Query), K (Key), V (Value) are speciﬁed\nas the hidden representations of the previous layer in the\nso-called self-attention sub-layers in the Transformer archi-\ntecture. The multi-head variant of attention allows the model\nto jointly attend to information from different representa-\ntion subspaces, and is deﬁned as Multihead(Q,K,V ) =\nTransformers with Competitive Ensembles of Independent Mechanisms\nConcat(head1,··· ,headH)WO, with the heads deﬁned as:\nheadk = Attention(QWQ\nk ,KW K\nk ,VW V\nk ) where WQ\nk ∈\nRdmodel×dK ,WK\nk ∈ Rdmodel×dK ,WV\nk ∈ Rdmodel×dV , and\nWO ∈RHdV ×dmodel are project parameter matrices, H is\nthe number of heads, and dK and dV are the dimensionali-\nties of Key and Value.\nGroup Linear Layer: It takes multiple hidden repre-\nsentations, and applies a separately parameterized linear\ntransformation to each. This operation can be efﬁciently\nimplemented using batched-matrix multiplications. We\nset the numbers of groups ns and deﬁne a weight tensor\nW ∈Rns×din ×dout . If the input h is shaped as h∈Rns×din ,\nthen we can deﬁne the layer as: GroupLinear(h,W,ns) =\n[hjWj]ns\nj=1.\n2.2. Speciﬁcs of TIM\nWe ﬁrst lay out the parts of a TIM layer and then give\nmore detailed steps in Algorithm 1. The proposed method\nis a drop-in replacement for a standard Transformer layer,\nand converting an existing Transformer layer to TIM layer\ninvolves few changes:\n• A TIM layer ﬁrst shares information between positions.\nTIM is able to easily keep different streams of pro-\ncessing well-separated by splitting the hidden state and\nparameters into ns mechanisms.\n• Competition over which mechanisms can read from\nother positions is introduced to encourage to special-\nization over distinct patterns.\n• Attention is used to share information between differ-\nent mechanisms.\n• Position-wise, mechanism-wise feedforward layers are\nused to process the results of these attention layers.\nAn overall illustration of how the TIM layer is structured\ngiven in Figure 1.\nStep 1 Competition between different mechanisms.Aside\nfrom having separate parameters and only exchanging in-\nformation via inter-mechanism attention, we wanted to\ncreate a stronger inductive bias to encourage the mech-\nanisms to specialize. To do this, we created a competi-\ntion system in which each mechanism has a layer which\noutputs a single scalar value (as a function of the current\nlayer’s representation), and these are passed through a soft-\nmax over the different mechanisms (this softmax is applied\nposition-wise and separately for each layer). The value\nof this softmax is then used to weight how much each\nmechanism is allowed to update its representation after\nthe self-attention. This competition score is computed as\nc = softmax\n(\nGroupLinear(h,Wc,ns)\n)\n, where we note\nAlgorithm 1 A single TIM Encoder-Layer\nHyperparameters: Number of mechanisms ns, key size dk,\nvalue size dv, number of heads for self-attention H, number\nof heads for inter-mechanism attention Hc. We set dmech =\ndmodel/ns and dffn −m = dffn /ns\nInput: An input hidden representation hfor a single example.\nStep 1: Compute Competition Between Mechanisms\nWc ∈Rns×dmech×1\nc= softmax\n(\nGroupLinear(h,Wc,ns)\n)\nStep 2: Sharing Information Between Positions\nWQ\n2 ,WK\n2 ∈Rns×dmech×HdK\nWV\n2 ∈Rns×dmech×HdV ,\nWO\n2 ∈Rns×HdV ×dmech\nCompute query, key, and value:\nQ= GroupLinear(h,WQ\n2 ,ns)\nK = GroupLinear(h,WK\n2 ,ns)\nV = GroupLinear(h,WV\n2 ,ns)\nUpdate mechanism hidden states using attention result:\nM := PositionAttention(Q,K,V,n sH)\nM := GroupLinear(M,W O\n2 ,ns)\nh:= norm(h+ c⊙M,ns)\nStep 3: Sharing Information Between Mechanisms\nWQ\n3 ,WK ∈Rns×dmech×HcdK\nWV\n3 ∈Rns×dmech×HcdV\nWO\n3 ∈Rns×HcdV ×dmech\nCompute query, key, and value:\nQ= GroupLinear(h,WQ\n3 ,ns)\nK = GroupLinear(h,WK\n3 ,ns)\nV = GroupLinear(h,WV\n3 ,ns)\nUpdate mechanism hidden states using attention result:\nM := MechanismAttention(Q,K,V,H c)\nM := GroupLinear(M,W O\n3 ,ns)\nh:= norm(h+ M,ns)\nStep 4: Mechanism-wise, Position-Wise, FFN Sub-Layer\nW(1) ∈Rns×dmech×dﬀn−m\nW(2) ∈Rns×dﬀn−m ×dmech .\nF = GroupLinear(σ(GroupLinear(h,W(1))),W(2))\nh:= norm(h+ F,ns)\nthat each mechanism has its own parameters for the layer\n(hence the use of a Group Linear layer instead of a nor-\nmal linear layer). Thus the ns modules have a per-step\nweighting for how much they are able to read during the\nlater self-attention stage. Thus if one mechanism wants\nto perform attention on a given position, it suppresses the\nother mechanisms on that position. We found that this often\nimproved results and that these softmax scores are fairly\ninterpretable as a measure of specialization. Exact equa-\ntions for this step are given in Step 1 and used in Step 2 in\nAlgorithm 1.\nStep 2 Each mechanism shares information across posi-\ntions. This step allows each mechanism to have its own\nindependent dynamics, which are themselves similar to a\nTransformers with Competitive Ensembles of Independent Mechanisms\nnormal transformer layer. These independent dynamics al-\nlow each mechanism to read information from other time\nsteps using attention and process that information using FFN\nlayers. We modify the self-attention sub-layer and feed-\nforward sub-layers (FFN) to be mechanism-wise as well\nas position-wise, with separate parameters for each mecha-\nnism. Additionally, the layer-normalization is modiﬁed to\nbe performed separately for each mechanism. The projec-\ntions and FFN sub-layers can be modiﬁed by replacing the\nlinear layers with group linear layers. When performing\nthe self-attention itself, the mechanisms behave the same\nas heads, and thus we can use the same type of multi-head\nattention process, so long as the total number of heads is di-\nvisible by the number of mechanisms. One notable property\nis if mechanisms only consisted of this part of the model\n(independent dynamics) by itself, then each mechanism in\nTIM would be a completely independent transformer model\nwith its own forward pass and its own parameters. Steps 2\nand 4 in Algorithm 1 give more detail on this step.\nStep 3 Sharing of information between different mecha-\nnisms via attention.Although we allow each mechanism to\nremain independent and process information independently,\nit is also important to allow the different mechanisms in\nTIMs to share information between each other (in case the\nmechanisms are not truly fully independent). To do this\nwe use a standard multi-head attention sub-layer to share\ninformation between the mechanisms, which is done in a\nposition-wise fashion. We made this attention mechanism\nrelatively small, with just 2 heads with 32 units each. This\nis because we want the different mechanisms to be as inde-\npendent as possible, and thus only share small amounts of\nhigh level information. This can be thought of as another\nattention layer, where we treat the different mechanisms\nas positions, and perform this attention in parallel over the\ndifferent steps in the sequence. This attention could also\nbe allowed to look at previous layer’s mechanisms, which\nhas previously been shown to improve the specialization of\nlayers (Lamb et al., 2020). More details on this are given in\nStep 3 in the appendix’s Algorithm 1.\n2.3. Implementing and Integrating TIM\nThe TIM layer is a drop-in replacement for a standard Trans-\nformer layer and turning an existing Transformer layer into\na TIM layer is surprisingly straightforward. It is a drop-in\nreplacement for a single layer which can be ﬂexibly used\nin a variety of models and architectures (both encoders and\ndecoders). A simple strategy is if a normal hidden represen-\ntation is of shape (T,b,d model), then our TIM hidden rep-\nresentation should be reshape-able to (T,b,n s,dmodel/ns).\nFirst, each layer linear layer in the existing Transformer\nlayer should be replaced by a group-wise linear layer imple-\nmented using batch matrix multiplication. Second, so long\nas the number of heads is divisible by the number of mecha-\nMechanism-wise\nSelf-Attention\nand \nInter-mechanism\nAttention\nA single position in a\nTransformer layer\nwith 2 heads\nA single position in a\nTIMs layer with 2\nmechanisms\nHead-wise\nSelf-Attention\nMonolithic State\nHead 1\nHead 2\nMechanism 1\nMechanism 2\nhL hL+1FFN\nLayerAttentionProjection\nLayer\nFigure 2.We show a simpliﬁed version of the model at a single\nposition to illustrate the difference between heads and mechanisms.\nHeads allow for parallel attention, but the differentiation between\nheads is transient: it begins with the projection layer and ends\nimmediately following the attention. As a result, most of the pa-\nrameters are not head-speciﬁc. With independent mechanisms, the\ninformation is kept well-separated throughout the entire layer, and\nall of the layer’s parameters are speciﬁc to a single mechanism.\nnisms, the self-attention does not need to be changed, since\nmechanisms behave interchangeably with heads in this part\nof the model. Third, the inter-mechanism communication\ncan be added as a drop-in module into the Transformer layer.\nFinally, the competition layer is just a single layer with a\nsoftmax, which can easily be added.\nAlthough TIM is a drop-in replacement for a normal Trans-\nformer layer, there are a few subtleties that must be con-\nsidered for successful integration. First, if the total size\nof the hidden representation is kept the same, integrating\nTIM drastically reduces the total number of parameters in\nthe model because all of the linear layers are replaced by\ngrouped-linear layers (which can be thought of as having a\nblock-sparse structure). This step by itself reduces the num-\nber of parameters by a factor of ns, but a TIM layer also\nadds new parameters to the model through the addition of\nthe Inter-mechanism Attention Sub-Layer and Mechanism-\nCompetition Sub-Layers, although both of these are rather\nsmall. If the total number of hidden units is kept the same,\na TIM layer usually reduces the number of parameters by\nabout 30-40%, depending on the exact hyperparameters.\nAdditionally, while we initially thought that it would make\nsense to replace every Transformer layer with a TIM layer,\nwhen we analyzed the mechanism-competition, we found\nthat it was almost always completely ﬂat on the ﬁrst layer,\nwhich suggested to us that the ﬁrst two layers as well as the\nlast layer should be kept as normal Transformer layers.\n3. Related Work\nSpecialization and Competition over heads in Transform-\ners. Cui et al. (2019) proposed a mixed multi-head attention\nTransformers with Competitive Ensembles of Independent Mechanisms\nFigure 3.We trained a TIM version of Image Transformer (pixel-by-pixel, raster-order generative model) with ns = 2and show the\nactivation score for the ﬁrst mechanism on the bottom row. On CIFAR-10, we see that the activation of mechanisms is correlated with the\nbackground and foreground of the object. To more directly test the property of independent mechanisms, we constructed a dataset in\nwhich the left-half of each image is an MNIST digit and the right-half of the image is a random CIFAR example. These two sides of the\nimage are independent and follow different dynamics. We found that TIM learns to specialize over the two sides of the image, with one\nmechanism only activating on the side with the MNIST digit and one mechanism only activating on the side with the CIFAR example.\nmechanism which forces some heads to learn speciﬁc pat-\nterns, such as attending to precedent/local tokens only. Clark\net al. (2019) studied which positions attention heads focus\non and found that some heads have speciﬁc patterns, such as\nattending locally. Vig et al. (2020) showed that the heads in\na model of protein sequences are semantically meaningful.\nAn et al. (2020) considered adding a repulsive force to the\nheads in Transformers to try to make them more specialized.\nIn our view, this evidence for specialization over heads is\ncomplementary with our results.\nIndependent Mechanisms and Modularity in Transform-\ners. We’re not aware of any work which breaks a Trans-\nformer’s hidden representation into multiple mechanisms\nwith separate parameters which interact through attention,\nthough some works hint at this direction. The Group Trans-\nformer (Park et al., 2020) replaces the fully-connected layers\nwith group-linear layers and uses low-rank layers to pass\ninformation between the groups. The universal transformer\n(Dehghani et al., 2018) shared parameters between layers\nand updated using gating, and this gating could behave sim-\nilarly to the competition that we propose but lacks the idea\nof having multiple mechanisms. The Switch Transformer\n(Fedus et al., 2021) selects different experts for each ex-\nample but doesn’t decompose the hidden state per-position\ninto multiple mechanisms (which have both distinct state\nand distinct parameters) as we do in TIM. Combining the\nSwitch Transformer with TIM could be a fruitful area for\nfuture research, especially since the switching over mixture\nof experts could be done different for each mechanism, al-\nlowing the switching to be speciﬁc to speciﬁc sub-patterns\nwithin the data.\nIndependent Mechanisms in Recurrent Networks. The\nidea of independent mechanisms has seen a signiﬁcant\namount of focus in recurrent networks (Goyal et al., 2019).\nThe idea is to parameterize the model as an ensemble of\nmechanisms, having their own dynamics, but sparingly in-\nteracting with each other using a bottleneck of attention. In\nthe case of recurrent networks, dividing the hidden represen-\ntation into mechanisms has the advantage that at a particular\ntime-step, only a fraction of mechanisms can be active, and\nhence computation is sparse in time, where in the case of\ntransformers, imposing the idea of independent mechanisms\nin some higher layers has the added advantage that compu-\ntation can be sparse both in space (i.e., position ) as well as\ntime.\n4. Experiments\nWe seek to answer two questions in our experiments.\n• Do the mechanisms that we learn with TIM specialize\nin sensible and semantically meaningful ways? We\nanalyze this both on toy datasets where we have clearly\nindependent mechanisms by construction (Figure 3)\nand on large-scale realistic speech and NLP tasks (Fig-\nure 5 and Figure 4).\n• How using a model which learns these independent\nmechanisms leads to better quantitative performance,\nboth on the original task and on transfer learning,\nwhich we demonstrate in Figure 3 and Table 1. We\nshow substantial improvements on speech enhance-\nment, BERT masked language modeling, and the chal-\nlenging CATER visual-reasoning task.\n4.1. Image Transformer: Evidence of Specialization\nWe integrated TIM into the Image Transformer, which is\na generative model which generates an image pixel-by-\npixel, with a small-scale variant of the GPT-2 architecture\n(Karpathy, 2020; Radford et al., 2019). We ﬁrst consid-\nered a pedagogic task in which the dataset consists of two\nclearly independent mechanisms. Our synthetic task uses\nMNIST digits (LeCun & Burges, 1998) and CIFAR images\n(Krizhevsky, 2009) of small realistic images of animals and\nvehicles. Each example in our constructed dataset consists\nof an MNIST digit on its left-side and a CIFAR image on its\nright-side, with these two examples selected randomly. It is\nclear that two sides of the image are independent and have\ncompletely different types of content, and thus it is natural\nfor each mechanism to specialize over a single side.\nWhen training with TIM on this dataset, we found that we\nwere able to nearly exactly recover a competition pat-\nTransformers with Competitive Ensembles of Independent Mechanisms\ntern in which the mechanisms specialize over the two\nsides of the image (Fig. 3, right). Intriguingly, this special-\nization does not appear at the very beginning of training, in\nwhich the mechanisms mostly specialize over the lightness\nor darkness of the pixels. However as training progresses,\nthe two sides of the image become increasingly special-\nized to one mechanism or the other. We also visualized the\ncompetition pattern with TIM on CIFAR-10 and found a\nspecialization between foreground and background re-\ngions in the images (Fig. 3, left).\n4.2. Speech Enhancement\nSpeech enhancement aims to improve the quality of speech\nrecordings. A speech signal captured in real environments,\nin fact, is often corrupted by noise and reverberation that\nmight severely affect its intelligibility. Speech enhancement\nhas long been studied in the research community (Jacob Ben-\nesty & Chen, 2015). Traditional approaches were based on\nsignal processing techniques such as spectral-subtraction\nor Wiener ﬁltering (Boll, 1979; Ephraim & Malah, 1984;\nScalart & Filho, 1996). The idea behind these methods is\nto estimate the noise in non-speech segments and remove\nit from speech regions. End-to-end deep learning-based\nspeech enhancement has turned out to signiﬁcantly outper-\nform traditional signal processing methods, and recently\nusing Transformers has led to promising performance (Kim\net al., 2020). We believe that TIM ﬁts well with this task\nbecause the traditional technique of decomposing the signal\ninto speech and noisy parts and then analyzing these two\nsignals separately embodies the desiderata of independent\nmechanisms.\nTable 3.We trained TIM on the DNS speech enhancement dataset\nand evaluate on the DNS test-set (left). Results with (*) used\nadditional outside datasets for training. For external baselines (a)\nis (Choi et al., 2020), (b) is (Koyama et al., 2020), and (c) is (Isik\net al., 2020).\nModels Param DNS\n(Trained on DNS) (M) (PESQ)\nNoisy - no reverb n/a 1.582\nU-Net-MultiScale+ (a) 3.5 2.710\nConv-TasNet (b) 5.1 2.730\nPoCoNet (c) 50.0 2.722\nPoCoNet-SSL* (c) 50.0 2.748\nTransformer Baseline 6.1 2.727\nTIM-NoComp (ns= 2) 6.0 2.754\nTIM-Comp (ns= 2) 6.0 2.742\nTIM-Comp (ns= 4) 6.0 2.730\nTable 3 (left) compares the performance achieved by TIM\nwith other recent systems on the widely-studied Deep Noise\nSuppression (DNS) dataset (Reddy et al., 2020). DNS is\na large corpus composed of roughly 441 hours of clean\nand noisy speech samples. The clean speech is artiﬁcially\ncorrupted with noise sequences from the audioset database,\nwhich contains two million human-labeled clips drawn from\nYouTube videos and belong to about 600 audio events.\nNoise is added to the clean speech signal using a random\nsignal-to-noise-ratio (SNR) ranging from 0 to 40 dB. We\nreplaced all Transformer layers except for the ﬁrst two and\nthe last layer with TIM layers and we increased the total\nnumber of hidden units and heads (by about 20%) to match\nthe number of parameters of the baseline, and we used two\nmechanisms (but achieved slightly worse yet better-than-\nbaseline results with ns = 4). The systems are evaluated\nwith the Perceptual Evaluation of Speech Quality (PESQ)\nscore (Rix et al., 2001). To assess the generalization capabil-\nity of TIM, we tested our model on the V oicebank test-set as\nwell (see Table 3-right). V oicebank (Thiemann et al., 2013),\nin fact, is characterized by noisy conditions different from\nthat of the DNS dataset used for training.\nThe results, shown in Table 3, highlight that TIM slightly\noutperforms the recently-proposed PocoNet (Hu et al., 2020)\nmodel, which uses additional data and has 8 times the param-\neters of TIM. To the best of our knowledge, TIM achieves\nthe best PESQ performance so far published in the litera-\nture on the DNS dataset. Qualitatively, we found that the\ncompetition scores matches our intuition. Indeed, the two\nmechanisms clearly specialize over speech and non-speech\nparts of the audio sequence, as shown in Figure 5. More-\nover, we intriguingly found that this competition between\nmechanisms is consistent across layers, starts out with low\nconﬁdence, and becomes increasingly conﬁdent in later\nlayers. Compared to a standard Transformer, TIM shows\nsuperior generalization capabilities. This interesting feature\ncan be appreciated in Table 3 (right), where we tested our\nmodel on a different dataset (V oiceBank). In mismatch con-\nditions, the competition mechanism seems to play a crucial\nrole. This ﬁnding agrees with our intuition, according to\nwhich employing specialized and competing modules can\nmake the model less affected by irrelevant changes of the\ninput distribution.\n4.3. BERT Pre-training and Fine-Tuning\nBERT (Devlin et al., 2018) is one of the most popularly used\nmethods to learn the representation of natural language. The\nBERT model uses a multi-layer Transformer encoder and is\ntrained by the masked language modeling task using Web\ndata corpus (Liu et al., 2019). The pre-trained contextual\nsentence representations have been shown to be effective in\na large number of downstream tasks.\nFor BERT, we replaced all of the transformer layers except\nfor the ﬁrst two layers and the last layer with TIM layers\n(we also report a result where all layers are TIM layers,\nTransformers with Competitive Ensembles of Independent Mechanisms\nTable 1.We compare the baseline BERT models to TIM with and without competition, on both validation likelihood (perplexity) and NLP\nﬁne-tuning tasks (reported as accuracy, with median and standard deviation over ﬁve ﬁne-tuning trials with different seeds). We also show\nthat it is essential to make the ﬁrst and last layers normal Transformer layers and not TIM layers (TIM-All-Layers).\nResult BERT BERT-130M TIM-All-Layers TIM-NoComp TIM-Comp\nTIM Layers 0/12 0/12 12/12 9/12 9/12\nParameters 110M 130M 110M 130M 130M\nCompetition? \u0017 \u0017 \u0017 \u0017 ✓\nValid-NLL 2.096 2.040 2.112 2.033 2.027\nMNLI-M 84.93 ±0.15 85.37 ±0.29 84.19 ±0.34 85.89 ±0.17 85.28 ±0.22\nMNLI-MM 84.91 ±0.18 85.28 ±0.27 84.55 ±0.15 85.80 ±0.07 85.17 ±0.18\nQNLI 91.34 ±0.21 91.84 ±0.32 91.37 ±0.59 91.78 ±0.14 91.97 ±0.20\nSST-2 92.88 ±0.33 92.75 ±0.26 92.52 ±0.56 92.75 ±0.13 92.97 ±0.25\nSTS-B 89.43 ±0.25 89.34 ±0.15 88.20 ±0.32 88.52 ±0.28 89.63 ±0.05\nFigure 4.On a BERT model, we show the minimum, average, and maximum mechanism competition values of some selected common\ntokens (left). One mechanism clearly specializes over the period between sentences, yet the high difference between the minimum and\nmaximum values suggest that these differences are contextual and not a static function of the token. In particular we found that the\nmodular activation for a period depends on whether it is used to mark the end of a sentence or whether it is used as part of a number or a\nURL. Moreover, the mechanism activation is highly correlated between layers (scatter-plot in center, correlation matrix on right).\nFigure 5.An examples of a speech signal (left) with their respective competition patterns over ﬁve successive TIM layers (ordered from\ntop to bottom). In the early layers, the competition is uncertain, but becomes more certain in the deeper layers. This is further quantiﬁed\nin a correlation matrix of competition over layers (middle) and a plot showing that competition entropy drops in later layers, especially at\nthe lowest percentiles (right).\nTransformers with Competitive Ensembles of Independent Mechanisms\nTable 2.To assess zero-shot transfer generalization we evaluate\na model trained on the DNS Speech Enhancement train-set on\nthe voicebank test-set. The performance is reported on wideband\nPESQ (higher is better). TIM shows improved generalization\ncapabilities in mismatch conditions.\nModels VoiceBank\n(DNS →VoiceBank, Zero-Shot Transfer) (PESQ)\nNoisy - no reverb 1.970\nTransformer Baseline 2.517\nTIM-NoComp (ns=4) 2.503\nTIM-Comp (ns=2) 2.575\nTIM-Comp (ns=4) 2.540\nshowing that it leads to worse performance). We used two\nmechanisms and evenly increased the number of hidden\nunits and total number of heads across all layers to match\nthe number of parameters in the baseline model.\nPre-training. Following Devlin et al. (2018), we used En-\nglish Wikipedia corpus and BookCorpus for pre-training.\nBy concatenating these two datasets, we obtained a cor-\npus with roughly 3.4 billion words in total. We trained all\nmodel variants with the same procedure and hyperparam-\neters which were tuned on the BERT baseline model. All\nmodels were run on 16 NVIDIA Tesla V100 GPUs.\nFine-tuning. We used MNLI, MNLI-MM, QNLI,\nSST-2 and STS-B from the GLUE ( General Language\nUnderstanding Evaluation) dataset (Wang et al., 2018) as\nthe downstream tasks to evaluate the performance of the pre-\ntrained models. Ideally the features learned by BERT would\nremain useful on these distinct tasks which have relatively\nsmall training sets.\nResults. The overall comparison results are shown in Ta-\nble 1. We found that both TIM-NoComp and TIM-Comp\nachieve lower perplexities (masked language modeling loss)\non the validation dataset compared to the two BERT base-\nlines. We found generally better and more reliable (less vari-\nance between seeds) results when ﬁne-tuning experiments\nwith TIM. These empirical results show that our proposed\nTIM is a better model architecture in a wide range of natural\nlanguage applications.\n4.4. CATER Occluded Object Tracking\nThe CATER spatio-temporal reasoning video task (Girdhar\n& Ramanan, 2019) involves locating an object’s position at\nthe end of a video, even as that object is occluded by other\nmoving objects. For example, a ball can be hidden under a\ncup (this process could itself be occluded and not directly\nobserved), followed by the movement of the cup. Tracking\nthe location of the ball requires reasoning about how objects\nwill move even when they are not directly observable. We\nfocus on the localization task, in which the goal is to predict\nthe location of the target object in the ﬁnal frame. This\ntarget object may be occluded by other objects hence hiding\nit from direct visual observation. In this case, it necessary\nto reason about the movement of the objects to determine\nwhere the target is at the end of the scene.\nWe ﬁrst sample frames from the video at a sampling rate of\n6 images per second. We pass each sampled frame through\na resnet block to get a sequence of feature representations:\n{f1,f2,f3,..., fT }. We then pass this sequence through\na Transformer, an LSTM, or TIM. This task is setup as a\nclassiﬁcation task where we have to predict which cell in\nthe 6 ×6 grid contains the target image in the ﬁnal frame.\nWe present results in Table 4. We achieved substantial im-\nprovements over the transformer baseline and also achieved\nthe biggest improvements when using a large number of\nmechanisms.\nTable 4.Comparison on CATER Object Tracking . Here, we\ncompare the Top-1 and Top-5 accuracy of Transformers with TIM.\nModel Top-1 % Top-5 %\nLSTM 67.4 85.8\nTRANSFORMER 68.7 81.7\nTIM-C OMP (ns = 2) 68.4 85.7\nTIM-C OMP (ns = 4) 71.0 87.3\nTIM-C OMP (ns = 8) 71.1 87.2\n5. Conclusion\nScaling to extremely large Transformers with a very large\nnumber of hidden units for each position has become one\nof the dominant paradigms in applied machine learning.\nThis work explores a new direction in the structure of the\nTransformer architecture which will become increasingly\nimportant as models become larger and researchers seek to\nmodel more complex phenomena. Evidence suggests that\nthe Transformer’s success is a result of its use of attention\nto communicate information between positions, which al-\nlows for effective and precise transmission of information\neven over very long sequences (Kaplan et al., 2020). At\nthe same time, each position within a Transformer is still\nrepresented with a single monolithic hidden representation,\nand a set of parameters which is applied over the entire\nhidden representation. Our newly proposed technique, TIM,\nhas shown that it is possible to make the Transformer even\nmore dynamic by breaking the hidden representation and\nlayers into multiple mechanisms which interact via attention\nand have an inductive bias towards specialization. We show\nthat these mechanisms specialize over distinct parts of the\ndata and improve results across diverse types of data. These\nresults suggest that there is room to improve the structural\ninductive biases in the Transformer and point towards an\nincreasingly central area of future research as state-of-the-\nart Transformers, and the tasks they’re trained on, become\nlarger and more diverse.\nTransformers with Competitive Ensembles of Independent Mechanisms\nReferences\nAchille, A. and Soatto, S. Emergence of invariance and\ndisentanglement in deep representations. The Journal of\nMachine Learning Research, 19(1):1947–1980, 2018.\nAn, B., Lyu, J., Wang, Z., Li, C., Hu, C., Tan, F., Zhang,\nR., Hu, Y ., and Chen, C. Repulsive attention: Rethinking\nmulti-head attention as bayesian inference. arXiv preprint\narXiv:2009.09364, 2020.\nAndreas, J., Rohrbach, M., Darrell, T., and Klein, D. Neural\nmodule networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 39–48,\n2016.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.arXiv\npreprint arXiv:1409.0473, 2014.\nBengio, Y .Learning deep architectures for AI. Now Pub-\nlishers Inc, 2009.\nBoll, S. Suppression of acoustic noise in speech using\nspectral subtraction. IEEE Transactions on acoustics,\nspeech, and signal processing, 27(2):113–120, 1979.\nBottou, L. and Gallinari, P. A framework for the cooperation\nof learning algorithms. In Advances in neural information\nprocessing systems, pp. 781–788, 1991.\nChoi, H.-S., Heo, H., Lee, J. H., and Lee, K. Phase-aware\nsingle-stage speech denoising and dereverberation with\nu-net. arXiv preprint arXiv:2006.00687, 2020.\nClark, K., Khandelwal, U., Levy, O., and Manning, C. D.\nWhat does bert look at? an analysis of bert’s attention.\narXiv preprint arXiv:1906.04341, 2019.\nCui, H., Iida, S., Hung, P.-H., Utsuro, T., and Nagata, M.\nMixed multi-head self-attention for neural machine trans-\nlation. In Proceedings of the 3rd Workshop on Neural\nGeneration and Translation, pp. 206–214, 2019.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, Ł. Universal transformers. arXiv preprint\narXiv:1807.03819, 2018.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nEphraim, Y . and Malah, D. Speech Enhancement Using a\nMinimum Mean-Square Error Short-Time Spectral Am-\nplitude Estimator. IEEE Transactions on Audio, Speech,\nand Language Processing, 32(6):1109–1121, 1984.\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\ners: Scaling to trillion parameter models with simple and\nefﬁcient sparsity, 2021.\nFernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha, D.,\nRusu, A. A., Pritzel, A., and Wierstra, D. Pathnet: Evolu-\ntion channels gradient descent in super neural networks.\narXiv preprint arXiv:1701.08734, 2017.\nFu, S.-W., Liao, C.-F., Hsieh, T.-A., Hung, K.-H., Wang, S.-\nS., Yu, C., Kuo, H.-C., Zezario, R. E., Li, Y .-J., Chuang,\nS.-Y ., et al. Boosting objective scores of speech enhance-\nment model through metricgan post-processing. arXiv\npreprint arXiv:2006.10296, 2020.\nGirdhar, R. and Ramanan, D. CATER: A diagnostic dataset\nfor compositional actions and temporal reasoning. CoRR,\nabs/1910.04744, 2019. URL http://arxiv.org/\nabs/1910.04744.\nGlorot, X., Bordes, A., and Bengio, Y . Domain adaptation\nfor large-scale sentiment classiﬁcation: A deep learning\napproach. In ICML, 2011.\nGoyal, A. and Bengio, Y . Inductive biases for deep\nlearning of higher-level cognition. arXiv preprint\narXiv:2011.15091, 2020.\nGoyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine,\nS., Bengio, Y ., and Schölkopf, B. Recurrent independent\nmechanisms. arXiv preprint arXiv:1909.10893, 2019.\nGoyal, A., Lamb, A., Gampa, P., Beaudoin, P., Levine,\nS., Blundell, C., Bengio, Y ., and Mozer, M. Object\nﬁles and schemata: Factorizing declarative and proce-\ndural knowledge in dynamical systems. arXiv preprint\narXiv:2006.16225, 2020.\nHu, Y ., Liu, Y ., Lv, S., Xing, M., Zhang, S., Fu, Y ., Wu, J.,\nZhang, B., and Xie, L. Dccrn: Deep complex convolution\nrecurrent network for phase-aware speech enhancement.\narXiv preprint arXiv:2008.00264, 2020.\nIsik, U., Giri, R., Phansalkar, N., Valin, J.-M., Helwani,\nK., and Krishnaswamy, A. Poconet: Better speech en-\nhancement with frequency-positional embeddings, semi-\nsupervised conversational data, and biased loss. arXiv\npreprint arXiv:2008.04470, 2020.\nJacob Benesty, Jesper R. Jensen, M. G. C. and Chen, J.\nSpeech Enhancement–A Signal Subspace Perspective.\nAcademic Press, 2015.\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., Hinton, G. E.,\net al. Adaptive mixtures of local experts. Neural compu-\ntation, 3(1):79–87, 1991.\nTransformers with Competitive Ensembles of Independent Mechanisms\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKarpathy, A. mingpt. https://github.com/\nkarpathy/minGPT, 2020.\nKim, J., El-Khamy, M., and Lee, J. T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp.\n6649–6653. IEEE, 2020.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. CoRR, abs/1412.6980, 2014. URL http:\n//arxiv.org/abs/1412.6980.\nKoehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-\nerico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C.,\nZens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst,\nE. Moses: Open source toolkit for statistical machine\ntranslation. In ACL, 2007.\nKoyama, Y ., Vuong, T., Uhlich, S., and Raj, B. Exploring the\nbest loss function for dnn-based low-latency speech en-\nhancement with temporal convolutional networks. arXiv\npreprint arXiv:2005.11611, 2020.\nKrizhevsky, A. Learning multiple layers of features from\ntiny images. 2009.\nLamb, A., Goyal, A., Słowik, A., Mozer, M., Beaudoin, P.,\nand Bengio, Y . Neural function modules with sparse ar-\nguments: A dynamic approach to integrating information\nacross layers, 2020.\nLeCun, Y ., C. C. and Burges, C. The mnist database of hand-\nwritten digits. arXiv preprint arXiv:2001.08361, 1998.\nURL http://yann.lecun.com/exdb/mnist/.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nMathieu, M. F., Zhao, J. J., Zhao, J., Ramesh, A., Sprech-\nmann, P., and LeCun, Y . Disentangling factors of varia-\ntion in deep representation using adversarial training. In\nAdvances in neural information processing systems, pp.\n5040–5048, 2016.\nMittal, S., Lamb, A., Goyal, A., V oleti, V ., Shanahan, M.,\nLajoie, G., Mozer, M., and Bengio, Y . Learning to com-\nbine top-down and bottom-up signals in recurrent neural\nnetworks with attention over modules. arXiv preprint\narXiv:2006.16981, 2020.\nPark, S., Kim, G., Lee, J., Cha, J., and Lee, J.-H.\nK. H. Group-transformer: Towards a lightweight\ncharacter-level language model, 2020. URL https:\n//openreview.net/forum?id=rkxdexBYPB.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. arXiv preprint arXiv:1802.05365,\n2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019.\nRahaman, N., Goyal, A., Gondal, M. W., Wuthrich, M.,\nBauer, S., Sharma, Y ., Bengio, Y ., and Schölkopf, B.\nS2rms: Spatially structured recurrent modules. arXiv\npreprint arXiv:2007.06533, 2020.\nRavanelli, M. and Omologo, M. Contaminated speech train-\ning methods for robust DNN-HMM distant speech recog-\nnition. In Proc. of Interspeech, pp. 756–760, 2015.\nReddy, C. K., Gopal, V ., Cutler, R., Beyrami, E., Cheng,\nR., Dubey, H., Matusevych, S., Aichner, R., Aazami, A.,\nBraun, S., et al. The interspeech 2020 deep noise suppres-\nsion challenge: Datasets, subjective testing framework,\nand challenge results. arXiv preprint arXiv:2005.13981,\n2020.\nReed, S. and De Freitas, N. Neural programmer-interpreters.\narXiv preprint arXiv:1511.06279, 2015.\nRifai, S., Bengio, Y ., Courville, A., Vincent, P., and Mirza,\nM. Disentangling factors of variation for facial expression\nrecognition. In European Conference on Computer Vision,\npp. 808–822. Springer, 2012.\nRix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,\nA. P. Perceptual evaluation of speech quality (pesq)-a\nnew method for speech quality assessment of telephone\nnetworks and codecs. In IEEE International Conference\non Acoustics, Speech, and Signal Processing (ICASSP),\nvolume 2, pp. 749–752, 2001.\nRonco, E., Gollee, H., and Gawthrop, P. J. Modular neural\nnetworks and self-decomposition. Technical Report CSC-\n96012, 1997.\nRosenbaum, C., Klinger, T., and Riemer, M. Routing net-\nworks: Adaptive selection of non-linear functions for\nmulti-task learning. arXiv preprint arXiv:1711.01239,\n2017.\nScalart, P. and Filho, J. V . Speech enhancement based on\na priori signal to noise estimation. In Acoustics, Speech,\nand Signal Processing, 1996. ICASSP-96. Conference\nProceedings., 1996 IEEE International Conference on,\nvolume 2, pp. 629–632. IEEE, 1996.\nTransformers with Competitive Ensembles of Independent Mechanisms\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. In ACL,\n2016.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\nQ., Hinton, G., and Dean, J. Outrageously large neural\nnetworks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538, 2017.\nThiemann, J., Ito, N., and Vincent, E. The diverse environ-\nments multi-channel acoustic noise database: A database\nof multichannel environmental noise recordings. The\nJournal of the Acoustical Society of America, 133(5):\n3591–3591, 2013.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nVig, J., Madani, A., Varshney, L. R., Xiong, C., Socher, R.,\nand Rajani, N. F. Bertology meets biology: Interpreting\nattention in protein language models. arXiv preprint\narXiv:2006.15222, 2020.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. GLUE: A multi-task benchmark and anal-\nysis platform for natural language understanding. CoRR,\nabs/1804.07461, 2018. URL http://arxiv.org/\nabs/1804.07461.\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books\nand movies: Towards story-like visual explanations by\nwatching movies and reading books. In arXiv preprint\narXiv:1506.06724, 2015.\nTransformers with Competitive Ensembles of Independent Mechanisms\nA. Experiment Details\nA.1. Image Transformer Details\nFor the Image Transformer, we used a baseline with 6 Trans-\nformer layers. As in the other experiments, we made the\nﬁrst 2 layers use TIMs as well as the last layer. We ran each\nexperiment for 30 epochs with a batch size of 24, and other-\nwise used the same training hyperparameters as the minGPT\nrepository (Karpathy, 2020). We used the Adam optimizer\nwith warmup, with Betas = (0.9, 0.95). Our baseline model\nhad 8 heads (total per-layer) and a layer hidden size of 184.\nWhen using TIMs, we increased this (for all layers) to 10\nheads and a hidden size of 200. This led to the baseline and\nthe TIM model having roughly the same number of total\nparameters.\nA.2. Speech Enhancement Details\nDatasets Neural speech enhancement systems are trained\nusing a parallel corpus of noise and clean examples, which\nare generated by artiﬁcially contaminating clean speech\nwith disturbances such as additive noise and reverberation\n(Ravanelli & Omologo, 2015). The speech enhancement\nmodels considered in this work are trained with the DNS\ndataset (noisy, no reverb) (Reddy et al., 2020), which is a\nsynthetic corpus recently made publicly available by Mi-\ncrosoft. This corpus is extremely suitable for our study\nbecause it is quite big (441 hours) and contains a large vari-\nety of possible noises (from 600 different categories). To the\nbest of our knowledge, it is the biggest open-source speech\nenhancement dataset. Moreover, it has been the object of an\ninternational challenge on speech enhancement1. This gave\nus the possibility to compare our TIM with the best systems\nsubmitted to this competition.\nFor evaluation, we used the test sets of the DNS and V oice-\nbank datasets (Thiemann et al., 2013). The latter has been\nadopted to study a transfer learning scenario, where differ-\nent datasets are used for training and evaluation purposes.\nV oicebank, in fact, is generated with noisy sequences dif-\nferent from the one contained in the DNS corpus. Since\nV oicebank is released at 48 kHz, the original raw waveforms\nwere downsampled from 48kHz to 16kHz.\nModel Architecture The proposed TIM is fed with noisy\nspeech and estimates clean speech at the output. More\nprecisely, we estimate the log-spectral magnitude of the\nclean signal. Mean Squared Error (MSE) between the clean\nand the corresponding noisy signal is used as cost function.\nThe input waveform is transformed with the Short-Term\nFourier Transform (STFT) based on 512 frequency points\nand window length of 32 ms with 16 ms overlap.\n1https://dns-challenge.azurewebsites.net/\nInterspeech2020\nBefore adding the transformer layers, we employ four 1D\nconvolutional layers that act as a pre-encoder module. This\nis done to replace positional encoding from the original\ntransformers and inject relative location information to the\nframes in the sequence (Kim et al., 2020; Fu et al., 2020).\nThe four convolutional layers are based on 1024, 512,128,\nand 256 channels, respectively. The kernel size is 3. Af-\nter the convolution, each layer applies layernorm followed\nby LeakyReLU. The Transformer part is composed of 8\nencoder blocks with a hidden size of 512. In order to em-\nploy approximately the same number of parameters (i.e, 6\nmillion), the baseline transformers used a hidden size of\n256. We used 16 attention heads, a dropout rate of 0.1,\nand LeakyReLU activations. We kept the number of heads\nthe same as in the baseline model. To follow the real-time\nprocessing restriction in DNS challenge, a causal setting is\nadopted to all our models with access to 32 ms of future\nframes. Attention masks are also applied to the self-attention\nlayers to prevent using the future information.\nTraining We followed the exact same training procedure\nfor the baseline model and the TIMs model, with both\ntrained for 50 epochs. We used the standard variant of the\nAdam optimizer with a batch size of 16. The initial learning\nrate was set to 0.0002 and halved when the validation score\ndecreased for 5 epochs. We reported test set performance at\nthe epoch with the best validation score, which in practice\nwas near the end of training. Both models train for about 50\nhours on a single Nvidia V100 GPU.\nA.3. BERT Pre-Training and Fine-Tuning Details\nBERT (Devlin et al., 2018) is one of the most popularly used\nmethods to learn the representation of natural language. The\nBERT model uses a multi-layer Transformer encoder and is\ntrained by the masked language modeling task using Web\ndata corpus (Liu et al., 2019). The pre-trained contextual\nsentence representations have been shown to be effective in\na large number of downstream tasks.\nTo validate our proposed architecture, we conduct experi-\nments to compare TIM with Transformer on the language\npre-training task. For our model, we replace all of the trans-\nformer layers except for the ﬁrst two layers and the last layer\nwith TIM layers (we also report a result where all layers\nare TIM layers, showing that it leads to worse performance).\nWe scaled the dimensionality of the hidden nodes and the\ninner-layer of the FFN sub-layer are set to, the number of\nmechanisms is set to 2 and the number of heads is set to 16.\nWe mainly test two TIM variants, TIM without competition\n(TIM-NoComp) and TIM with competition (TIM-Comp).\nFor a fair comparison, we set one baseline as a 12-layer\nTransformer with 130M parameters (BERT-130M). The size\nof hidden nodes and the inner-layer of the FFN sub-layer are\nTransformers with Competitive Ensembles of Independent Mechanisms\nset to 768/4096, and the number of heads is set to 12. We\nalso use the standard BERT-Base model (110M parameters)\nas another baseline.\nDataset Following (Devlin et al., 2018), we use English\nWikipedia corpus2 and BookCorpus3 for pre-training. By\nconcatenating these two datasets, we obtain a corpus with\nroughly 3400M words in total. We follow a couple of con-\nsecutive pre-processing steps: segmenting documents into\nsentences by Spacy 4, normalizing, lower-casing, and tok-\nenizing the texts by Moses decoder (Koehn et al., 2007),\nand ﬁnally, applying byte pair encoding(BPE) (Sennrich\net al., 2016) with setting the vocabulary size |V|as 32,678.\nOptimization Following the standard settings used in\nmany previous works (Devlin et al., 2018; Liu et al., 2019),\nwe train the models for 1000ksteps with setting the batch\nsize as 256 and the maximum sequence length as 512. For\nall the models to compare, we set the masked probability p\nto be 0.15. We follow previous works to replace 80% of the\nmasked positions by [MASK], 10% by randomly sampled\nwords, and keep the remaining positions unchanged. We\nchoose the most widely used Adam (Kingma & Ba, 2014) as\nthe optimizer, and set the hyper-parameter βas (0.9,0.98).\nThe learning rate is set as 1e-4 with a 10 k-step warm-up\nstage and then decays linearly to zero. We set the dropout\nprobability as 0.1. All models are run on 8 NVIDIA Tesla\nV100 GPUs.\nFine-tuning We use the GLUE ( General Language\nUnderstanding Evaluation) dataset (Wang et al., 2018) as\nthe downstream tasks to evaluate the performance of the pre-\ntrained models. Reporting large-scale task performance or\naveraged performance over all tasks depends on our choice.\nSame to the pre-training, we use Adam as the optimizer\nand set the hyper-parameter β as (0.9,0.98). Following\nall previous works, we apply the hyper-parameter search\n(over β and learning rate) during the ﬁne-tuning for each\ndownstream task. Each conﬁguration was run for ﬁve times\nwith different random seeds, and the median and standard\ndeviation over these ﬁve results on the development set was\nbe used as the performance of one conﬁguration.\nResults The overall comparison results are shown in Table 1.\nIt is easy to ﬁnd that both TIM-NoComp and TIM-Comp\nachieve lower perplexities (masked language modeling loss)\non the validation dataset compared to the two BERT base-\nlines. On the downstream tasks, the two TIM variants are\n2https://dumps.wikimedia.org/enwiki\n3As the dataset BookCorpus (Zhu et al., 2015) is no longer\nfreely distributed, we follow the suggestions from Devlin et al.\n(2018) to crawl from smashwords.com and collect BookCor-\npus by ourselves.\n4https://spacy.io\nalso slightly better than the BERTs on all tasks. Those em-\npirical results show that our proposed TIM is a better model\narchitecture in a wide range of natural language applica-\ntions.\nSimilar to previous analysis, we further study the competi-\ntion patterns in the TIM-Comp model to investigate how the\ncompetitive module behaves.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5619544386863708
    },
    {
      "name": "Computer science",
      "score": 0.3575519919395447
    },
    {
      "name": "Engineering",
      "score": 0.2436676025390625
    },
    {
      "name": "Electrical engineering",
      "score": 0.18710744380950928
    },
    {
      "name": "Voltage",
      "score": 0.10844197869300842
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I84653119",
      "name": "Academia Sinica",
      "country": "TW"
    }
  ]
}