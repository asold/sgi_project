{
    "title": "A Survey of Vision-Language Pre-Trained Models",
    "url": "https://openalex.org/W4225683910",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5001058167",
            "name": "Yifan Du",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5008547683",
            "name": "Zikang Liu",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5100363220",
            "name": "Junyi Li",
            "affiliations": [
                "Université de Montréal",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5037145565",
            "name": "Wayne Xin Zhao",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3207750165",
        "https://openalex.org/W4221145109",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3035652667",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W4312877428",
        "https://openalex.org/W3210084825",
        "https://openalex.org/W3104152799",
        "https://openalex.org/W3209274285",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3198196812",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2963115613",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3133825286",
        "https://openalex.org/W2962964995",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W3184784418",
        "https://openalex.org/W4287240280",
        "https://openalex.org/W4210352519",
        "https://openalex.org/W2951529591",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W2563399268",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3167118264",
        "https://openalex.org/W4312910992",
        "https://openalex.org/W4225700096",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3208314443",
        "https://openalex.org/W3177224328",
        "https://openalex.org/W3212610063",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W3129576130"
    ],
    "abstract": "As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.",
    "full_text": "A Survey of Vision-Language Pre-Trained Models\nYifan Du1,3† , Zikang Liu1† , Junyi Li1,2 and Wayne Xin Zhao1,3∗\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2DIRO, Universit´e de Montr´eal\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\n{yifandu1999, jasonlaw8121, batmanfly}@gmail.com, junyi.li@umontreal.ca\nAbstract\nAs transformer evolves, pre-trained models have\nadvanced at a breakneck pace in recent years. They\nhave dominated the mainstream techniques in nat-\nural language processing (NLP) and computer vi-\nsion (CV). How to adapt pre-training to the field\nof Vision-and-Language (V-L) learning and im-\nprove downstream task performance becomes a fo-\ncus of multimodal learning. In this paper, we re-\nview the recent progress in Vision-Language Pre-\nTrained Models (VL-PTMs). As the core con-\ntent, we first briefly introduce several ways to en-\ncode raw images and texts to single-modal embed-\ndings before pre-training. Then, we dive into the\nmainstream architectures of VL-PTMs in modeling\nthe interaction between text and image representa-\ntions. We further present widely-used pre-training\ntasks, and then we introduce some common down-\nstream tasks. We finally conclude this paper and\npresent some promising research directions. Our\nsurvey aims to provide researchers with synthesis\nand pointer to related research.\n1 Introduction\nWe now live in a world with various modalities (voice, vision,\nodors, etc.), among which vision and language are two critical\nones. In academia, there exist large amounts of works focus-\ning on V-L tasks. These tasks require the agent to jointly pro-\ncess information from these two modalities and utilize them\nto answer complex questions. For example, visual question\nanswering (VQA) [Antol et al., 2015] takes an image and the\ncorresponding question as input and gives the correct answer;\nimage captioning [Lin et al., 2014] generates a description for\na given image.\nDeep learning has revolutionized the fields of artificial in-\ntelligence. Various deep models have been applied to solve\nV-L tasks, such as recurrent neural network (RNN) [Arevalo\net al., 2017], convolutional neural network (CNN) [Huang et\nal., 2020 ] and transformer [Vaswani et al., 2017]. Despite\n†Equal Contribution.\n∗Corresponding Author.\nthe success of deep learning, most of these models are de-\nsigned for specific tasks, which leads to poor transferability.\nPre-training a huge model on large-scale general datasets and\nthen fine-tuning it on specific downstream tasks is one tech-\nnique to increase transferability. Pre-training is first discov-\nered to be effective in the field of CV [Simonyan and Zis-\nserman, 2014]. After the proposal of transformer [Vaswani\net al., 2017] and BERT [Devlin et al., 2018], the paradigm\nof pre-training and fine-tuning becomes prevalent in the field\nof NLP. With its powerful capability to model long-range de-\npendency, transformer has become the backbone of most Pre-\ntrained Language Models (PLMs). BERT and GPT-3[Brown\net al., 2020] are typical PLMs that significantly outperform\nprevious methods and achieve new state-of-the-art results on\nvarious downstream tasks.\nDue to the success of pre-trained models in the field of\nCV and NLP, many works have tried to pre-train large-\nscale models on both vision and language modalities, called\nVision-Language Pre-Trained Models (VL-PTMs). By pre-\ntraining on large-scale image-text corpora, VL-PTMs can\nlearn universal cross-modal representations, which are ben-\neficial for achieving strong performance in downstream V-L\ntasks [Zellers et al., 2019; Tan and Bansal, 2019]. For exam-\nple, LXMERT [Tan and Bansal, 2019] employs a dual-stream\nfusion encoder to learn V-L representations, significantly out-\nperforming traditional models on VQA [Antol et al., 2015],\nNLVR2 [Suhr et al., 2018] tasks via pre-training on 9.18M\nimage-text pairs. Besides, VL-PTMs achieve strong results\non many other V-L tasks like visual commonsense reason-\ning [Zellers et al., 2019] and image captioning [Lin et al.,\n2014]. These VL-PTMs utilize different single-modal en-\ncoders, elaborate V-L interaction schemes, and devise various\npre-training tasks.\nHowever, there lacks a comprehensive survey to summa-\nrize the recent progress in this field. Mogadala et al.[2021]\nmainly reviews existing V-L tasks, datasets, and traditional\nsolutions but rarely introduces V-L pre-training methods.\nRuan and Jin [2022] focus on Video-Language PTMs instead\nof Vision-Language PTMs. Different from them, our survey\naims to present a thorough review of VL-PTMs, which sum-\nmarizes recent research progress and provides pointers to re-\nlated research. We present the recent mainstream VL-PTMs\nin Table 1.\nBasically, there are three steps to pre-train a VL-PTM: 1)\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5436\nencode images and texts into latent representations preserving\ntheir semantics (Section 2); 2) design a performant architec-\nture to model the interaction between two modalities (Sec-\ntion 3); and 3) devise effective pre-training tasks to train the\nVL-PTMs (Section 4). After learning universal vision and\nlanguage features, VL-PTMs can be fine-tuned on various\ndownstream V-L tasks (Section 5). Finally, we conclude this\nsurvey and point out some promising research directions in\nSection 6.\n2 Learning Vision-Language Representation\nAs discussed in Section 1, encoding images and texts as em-\nbeddings preserving input semantics is the first step in pre-\ntraining a VL-PTM. The ways of encoding images and texts\nare quite different because of the discrepancy between the two\nmodalities. Almost all VL-PTMs utilize a transformer-based\nPTM as a text encoder, but how to learn visual representations\nbased on visual contents is still an open problem. In what fol-\nlows, we introduce several methods to encode the images and\ntexts into single-modal embeddings before feeding them into\na cross-modal transformer.\nPre-training Dataset. The initial step in pre-training VL-\nPTMs is to construct large-scale image-text pairs. We for-\nmally define the pre-training dataset as D = {(W, V)}N\ni=1,\nwhere W and V denote the text and image, respectively, and\nN is the number of image-text pairs. Specifically, each text\nwill be tokenized as a sequence of tokens W = ⟨w1, ..., wn⟩.\nSimilarly, each image will be also transformed into a se-\nquence of object features (or grid features, or patch features),\ndenoted as V = ⟨v1, ..., vm⟩. In Table 2 we list several\nwidely-used or recently proposed pre-training datasets.\nText Representation. Most of existing studies on VL-\nPTMs follow BERT [Devlin et al., 2018] to preprocess the\nraw text. The text sequence is first split into tokens and\nconcatenated with “[CLS]” and “[SEP]” tokens, denoted as\nW = ⟨[CLS], w1, ..., wn, [SEP]⟩. Each token wj will be\nmapped to a word embedding. Besides, a positional embed-\nding indicating the position and a segment embedding indi-\ncating the modality type are added with the word embedding\nto obtain the final embedding of wj.\nImage Representation. To align with the sequence of em-\nbeddings of the paired text, the image V will also be repre-\nsented as a sequence of embedding vectors. In this way, we\ncan unify input representation as a sequence of embeddings\nfor both modalities. Unlike relationships among words in\ntext, relationships among visual concepts in images are crit-\nical for V-L tasks but difficult to capture. For example, in\norder to generate a description of an image, the model is ex-\npected to infer the complex relationships among various ob-\njects in the image. Therefore, many works elaborate different\nvision encoders to model these relationships and the attributes\nof objects. Early works like ViLBERT [Lu et al., 2019 ]\nand LXMERT [Tan and Bansal, 2019 ] first utilize Faster R-\nCNN [Ren et al., 2015] to detect a sequence of object re-\ngions from an image and then encode them as a sequence\nof Region-Of-Interest (ROI) features. Besides, some VL-\nPTMs get rid of the bounding box and encode an image into\npixel-level grid features. For example, pixel-BERT [Huang\net al., 2020] and SOHO [Huang et al., 2021] abandon Faster\nR-CNN in favor of ResNet so that the visual encoder could\nview an image as a whole, avoiding the risk of neglecting\nsome critical regions. Apart from these methods, many works\ntry to follow the success of ViT [Dosovitskiy et al., 2020] to\nutilize a transformer to extract vision features. In this sce-\nnario, the transformer in VL-PTMs is tasked with the objec-\ntive of modeling object relationships within an image. An\nimage is firstly split into several flattened 2D patches. Then\nthe embeddings of image patches are arranged in a sequence\nto represent the original image. ALBEF [Li et al., 2021a]\nand SimVLM [Wang et al., 2021b] feed patches to an ViT\nencoder to extract vision features, which lead the way to a\nfull-transformer VL-PTM.\n3 Modeling Vision-Language Interaction\nAfter encoding images and texts into single-modal embed-\ndings, the next step is to design an encoder to integrate infor-\nmation from both vision and language modalities. For exam-\nple, to answer a question about an image, the model needs to\ncombine the linguistic information from both questions and\nanswers, then localize the corresponding region in the paired\nimages, and lastly align linguistic meanings with visual clues.\nBased on the way of aggregating information from different\nmodalities, we categorize the encoder into fusion encoder,\ndual encoderand the combination of both.\n3.1 Fusion Encoder\nThe fusion encoder takes text embeddings and image features\nas input and designs several fusion approaches to model V-L\ninteraction. After self-attention or cross-attention operation,\nthe hidden states of the last layer will be treated as the fused\nrepresentation of different modalities. There are mainly two\ntypes of fusion schemes for modeling the cross-modal inter-\naction: single stream and dual stream.\nSingle-stream Architecture. The single-stream architec-\nture assumes that the potential correlation and alignment be-\ntween two modalities are simple, which can be learned by a\nsingle transformer encoder. Therefore, the text embeddings\nand image features are concatenated together, adding some\nspecial embeddings to indicate position and modalities, and\nfed into a transformer-based encoder.\nAlthough different V-L tasks require different input for-\nmats (e.g., ⟨caption, image⟩ for image captioning, ⟨question,\nanswer, image⟩ for VQA), single-stream architecture can\nhandle them in a unified framework due to the unordered rep-\nresentation nature of transformer attention. VisualBERT [Li\net al., 2019] and V-L BERT [Su et al., 2019] utilize segment\nembedding to indicate input elements from different sources.\nInstead of simply using image-text pair, OSCAR [Li et al.,\n2020c] adds object tags detected from the image and repre-\nsents the image-text pair as a⟨word, tag, image⟩ triple to help\nthe fusion encoder better align different modalities. Since the\nsingle-stream architecture performs self-attention directly on\ntwo modalities, they may neglect intra-modality interaction.\nThus some works propose to employ dual-stream architecture\nto model V-L interaction.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5437\nVL-PTM T\next encoder Vision encoder Fusion scheme Pre-training tasks Multimodal datasets for pre-training\nFusion Encoder\nV\nisualBERT [2019] BERT Faster R-CNN Single stream MLM+ITM COCO\nUniter [2020] BERT Faster R-CNN Single stream MLM+ITM+WRA+MRFR+MRC CC+COCO+VG+SBU\nOSCAR [2020c] BERT Faster R-CNN Single stream MLM+ITM CC+COCO+SBU+Flickr30k+VQA\nInterBert [2020] BERT Faster R-CNN Single stream MLM+MRC+ITM CC+COCO+SBU\nViLBERT [2019] BERT Faster R-CNN Dual stream MLM+MRC+ITM CC\nLXMERT [2019] BERT Faster R-CNN Dual stream MLM+ITM+MRC+MRFR+VQA COCO+VG+VQA\nVL-BERT [2019] BERT Faster R-CNN+ ResNet Single stream MLM+MRC CC\nPixel-BERT [2020] BERT ResNet Single stream MLM+ITM COCO+VG\nUnified VLP [2020] UniLM Faster R-CNN Single stream MLM+seq2seq LM CC\nUNIMO [2020b] BERT, RoBERTa Faster R-CNN Single stream MLM+seq2seq LM+MRC+MRFR+CMCL COCO+CC+VG+SBU\nSOHO [2021] BERT ResNet + Visual Dictionary Single stream MLM+MVM+ITM COCO+VG\nVL-T5 [2021] T5, BART Faster R-CNN Single stream MLM+VQA+ITM+VG+GC COCO+VG\nXGPT [2021] transformer Faster R-CNN Single stream IC+MLM+DAE+MRFR CC\nVisual Parsing [2021] BERT Faster R-CNN + Swin transformer Dual stream MLM+ITM+MFR COCO+VG\nALBEF [2021a] BERT ViT Dual stream MLM+ITM+CMCL CC+COCO+VG+SBU\nSimVLM [2021b] ViT ViT Single stream PrefixLM C4+ALIGN\nWenLan [2021] RoBERTa Faster R-CNN + EffcientNet Dual stream CMCL RUC-CAS-WenLan\nViLT[2021] ViT Linear Projection Single stream MLM+ITM CC+COCO+VG+SBU\nDual Encoder\nCLIP [2021] GPT2 V\niT, ResNet CMCL self-collected\nALIGN [2021] BERT EffcientNet CMCL self-collected\nDeCLIP [2021b] GPT2, BERT ViT, ResNet, RegNetY-64GF CMCL+MLM+CL CC+self-collected\nFusion Encoder+\nDual Encoder\nVLMo [2021a] BERT ViT Single stream MLM+ITM+CMCL CC+COCO+VG+SBU\nFLA V A[2021] ViT ViT Single stream MMM+ITM+CMCL CC+COCO+VG+SBU+RedCaps\nTable 1: Glossary of Representative VL-PTMs. MLM/MVM: (Cross-Modal) Masked Language/Vision Modeling. ITM: Image-Text Match-\ning. MRC: Masked Region Classification. MRFR: Masked Region Feature Regression. VG: Visual Grounding. GC: Grounded Captioning.\nWRA: Word-Region Alignment. CMCL: Cross-Modal Contrastive Learning. DAE: Denoising AutoEncoding\nDataset Size Reference\nCOCO 328,124 [Lin et al., 2014]\nVG 108,077 [Krishna et al., 2017]\nCC 3.1M [Sharma et al., 2018]\nSBU 1M [Ordonez et al., 2011]\nLAION 400M https://laion.ai/laion-400-open-dataset/\nRedCaps 12M [Desai et al., 2021]\nTable 2: Widely Used Pre-training Datasets\nDual-stream Architecture. Different from self-attention\noperation in single-stream architectures, dual-stream archi-\ntectures adopt a cross-attention mechanism to model V-L in-\nteraction, where the query vectors are from one modality\nwhile the key and value vectors are from the other. A cross-\nattention layer usually contains two unidirectional cross-\nattention sub-layers: one from language to vision and another\nfrom vision to language. They are responsible for exchang-\ning information and aligning the semantics between the two\nmodalities.\nDual-stream architectures assume that the intra-modal\ninteraction and cross-modal interaction need to be sepa-\nrated to obtain better multimodal representations. ViL-\nBERT [Lu et al., 2019] utilizes two transformers to further\nmodel intra-modality interaction after the cross-modal mod-\nule. LXMERT [Tan and Bansal, 2019 ] does not use ex-\ntra transformers but appends a self-attention sub-layer after\ncross-attention sub-layer to further build internal connections.\nIn the cross-modal sub-layers, the parameters of the atten-\ntion module are shared between the two streams. In this\ncase, the model learns a single function to contextualize im-\nage and text embeddings. ALBEF [Li et al., 2021a] employs\ntwo separate transformers before cross-attention for images\nand texts, which better decouples intra-modal interaction and\ncross-modal interaction. This type of architecture helps to en-\ncode the input in a more comprehensive way. However, the\nextra feature encoder makes it parameter-inefficient.\n3.2 Dual Encoder\nAlthough the fusion encoder could model cross-modal inter-\naction at different levels and achieves state-of-the-art results\non many V-L tasks, it relies on a heavy transformer network to\nmodel V-L interaction. When performing cross-modal match-\ning tasks like Image-Text Retrieval, the fusion encoder has to\njointly encode all possible image text pairs, which leads to a\nquite slow inference speed.\nIn contrast, a dual encoder utilizes two single-modal en-\ncoders to encode two modalities separately. Then, it adopts\nstraightforward methods such as shallow attention layer [Lee\net al., 2018] or dot product [Radford et al., 2021; Jia et al.,\n2021] to project the image embedding and text embedding to\nthe same semantic space for computing V-L similarity scores.\nWithout the complex cross-attention in transformer, the V-\nL interaction modeling strategy in the dual encoder is much\nmore efficient. Thus, the feature vectors of images and text\ncan be pre-computed and stored, which is more effective for\nretrieval tasks than the fusion encoder. Although dual en-\ncoder models like CLIP [Radford et al., 2021] have shown\nsurprising performance on image-text retrieval tasks, they fail\nin some hard V-L understanding tasks such as NLVR[Suhr et\nal., 2018 ]. This is attributed to the shallow interaction be-\ntween the two modalities.\n3.3 Combination of Fusion Encoder and Dual\nEncoder\nBased on the observation that fusion encoder performs bet-\nter on V-L understanding tasks while dual encoder performs\nbetter on retrieval tasks, it is natural to combine the bene-\nfits of the two types of architectures. FLA V A [Singh et al.,\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5438\n2021] first adopts a dual encoder to obtain single-modal rep-\nresentations. Then the single-modal embeddings are sent to a\nfusion encoder to obtain cross-modal representation. Apart\nfrom its model design, FLA V A conducts several unimodal\npre-training tasks to improve the quality of single-modal rep-\nresentations. VLMo [Wanget al., 2021a] introduces Mixture-\nof-Modality-Expert (MoME) and unifies a dual encoder and\na fusion encoder into a single framework. After pre-training\non images, texts, and image-text pairs by stage, VLMo can\nnot only be fine-tuned on V-L understanding tasks, but also\nbe applied to efficient image-text retrieval.\n4 Cross-Modal Pre-training Tasks\nAccording to Section 1, after the input images and texts are\nencoded as vectors and fully interacted, the next step is to\ndesign pre-training tasks for VL-PTMs. The designed pre-\ntraining tasks have a great impact on what VL-PTM can learn\nfrom the data. In this section, we introduce some widely-used\npre-training tasks.\n4.1 Cross-Modal Masked Language\nModeling (MLM)\nCross-modal MLM is similar to MLM in the BERT model.\nIn cross-modal MLM, VL-PTMs predict masked tokens not\nonly based on unmasked tokens, but also by taking vision fea-\ntures into account. The dependency on vision modality differ-\nentiates cross-modal MLM from MLM in NLP. This task has\nbeen proven to be quite effective for pre-training VL-PTMs\nbecause it helps the model to align vision and text by consid-\nering the relationship between image and text. Formally, the\nobjective can be defined as:\nLMLM = −E(W,V )∈D log Pθ\n(\nwm|w\\m, V\n)\n, (1)\nwhere wm, w\\m represent the masked tokens and unmasked\ntokens respectively, and(W, V) ∈ Drepresents a text W and\nan image V sampled from dataset D.\nDue to the distinction between cross-modal MLM and\nMLM in NLP, an effective masking strategy is necessary for\ncross-modal MLM. If the method is too simple, the model\nmay be able to predict the masked tokens purely on the ba-\nsis of their surrounding tokens. By masking some tokens that\nrely on the image, VL-PTMs will take into account the image\nfeatures, thus aligning tokens and their corresponding objects\nin the image. ViLT[Kim et al., 2021] utilizes the Whole Word\nMasking strategy, which prevents the model from predicting\ntokens solely by words co-occurrence; InterBERT[Lin et al.,\n2020] masks several consecutive segments of text to make\nthis pre-training task more difficult and improves its perfor-\nmance on downstream tasks further.\n4.2 Cross-Modal Masked Region\nPrediction (MRP)\nSimilar to the cross-modal MLM, cross-modal MRP masks\nsome RoI features with zeros and predicts them based on\nother image features. The model learns object relationships\nby inferring from other unmasked regions and learns V-L\nalignments by inferring from the text. There are two kinds\nof learning objectives: Masked Region Classification (MRC)\nand Masked Region Feature Regression (MRFR).\nMasked Region Classification (MRC). MRC learns to\npredict the semantic class of each masked region. This task is\nmotivated by the observation that VL-PTMs just learn high-\nlevel semantics of images instead of raw pixels from the lan-\nguage side. To predict the region class, the hidden state hvi\nof the masked region vi from VL-PTMs is fed into a fully-\nconnected (FC) layer, followed by a softmax function to form\na predicted distribution on K object classes. The final objec-\ntive is to minimize the cross-entropy (CE) loss between the\npredicted distribution and the detected object category, which\ncan be formally defined as:\nLMRC = E(W,V )∈D\nl∑\ni=1\nCE\n(\nsoftmax(FC(hvi )), c(vi)\n)\n, (2)\nwhere l is the amount of masked regions, and c (vi) repre-\nsents the true label of the masked region, such as the object\ndetection output or the (pre-defined) visual tokens.\nThrough the cross-modal attention in VL-PTMs, the hid-\nden state hvi contains information from both vision and lan-\nguage modality, which makes it possible to predict visual\nsemantic class from the text. As for the ground-truth label\nc (vi), an intuitive method is to regard object tags (with the\nhighest confidence score) detected from object dector as the\ntrue labels[Tan and Bansal, 2019; Su et al., 2019]. However,\nthese labels are pseudo, which highly relies on the quality of\nthe pre-trained object detectors, thus there are some variants\nof this task. ViLBERT [Lu et al., 2019] and UNITER [Chen\net al., 2020] propose to consider the raw output of the detector\nas soft labels, which is a distribution of object classes. In this\nscenario, the objective becomes the KL-divergence between\ntwo distributions. SOHO [Huang et al., 2021] first maps the\nCNN-based grid features to visual tokens, and then predicts\nthe masked visual tokens based on their surrounding tokens.\nMasked Region Feature Regression (MRFR). MRFR\nlearns to regress the masked region feature hvi to its corre-\nsponding original region feature ˆEV (vi), which can be writ-\nten as:\nLMRFR = E(W,V )∈D\nl∑\ni=1\n∥FC(hvi ) −ˆEV (vi)∥2. (3)\nIn this formula, the region feature ˆEV (vi) of vi is com-\nputed based on an unmasked image and l represents the\namount of masked regions. MRFR requires the model to\nreconstruct the high-dimensional vectors instead of seman-\ntic class. When images are represented as a sequence of\nregion features by faster R-CNN, simple masking strategies\nlike random masking can give satisfying performances [Tan\nand Bansal, 2019; Chen et al., 2020; Liet al., 2020b]. How-\never, random masking will not be so effective when images\nare converted into grid features or patch features, because\nthe model will directly duplicate neighbor features as the pre-\ndicted features. Visual parsing [Xue et al., 2021] uses patch\nfeatures to represent an image and assumes that visual to-\nkens (region features) of high attention weights have simi-\nlar semantics. It first randomly masks a visual token as a\npivot token, and continues to maskk tokens with top-k atten-\ntion weights. SOHO [Huang et al., 2021] pre-trains a vision\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5439\ndictionary and masks all the features sharing the same visual\nindex to avoid information leakage.\n4.3 Image-Text Matching (ITM)\nCross-modal MLM and MRP help VL-PTMs learn the fine-\ngrained correlation between images and texts, while ITM em-\npowers VL-PTMs with the ability to align them at a coarse-\ngrained level. ITM is similar to the Next Sentence Predic-\ntion (NSP) task in NLP, which requires the model to deter-\nmine whether an image and a text are matched. Given an\nimage-text pair, a score function sθ measures the alignment\nprobability between the image and text. The objective func-\ntion is:\nLITM = −E(W,V )∈D\n[\ny log sθ\n(\nhw[CLS] , hv[IMG]\n)\n+(1 −y) log\n(\n1 −sθ\n(\nhw[CLS] , hv[IMG]\n))]\n,\n(4)\nwhere y ∈ {0, 1} represents whether W and V are matched\nwith each other or not, and hw[CLS] and hv[IMG] are the repre-\nsentations of w[CLS] and v[IMG], respectively.\nThe key to this task is how to represent an image-text pair\nin a single vector so that the score function sθ could output\na probability. UNITER [Chen et al., 2020], Unicoder [Li et\nal., 2020a] and SOHO [Huang et al., 2021] concatenate the\nword sequence W and the object sequence V and take the\nfinal hidden state of the “[CLS]” token as the fused represen-\ntation. By feeding it into a fully-connected layer layer, they\ncan reduce the dimension to predict the alignment probability.\nWhile, ViLBERT [Lu et al., 2019] adopts the representation\nof the “[IMG]” and “[CLS]” tokens to represent image and\ntext respectively, and the fused representation is computed by\nelement-wise product between them.\n4.4 Cross-Modal Contrastive Learning (CMCL)\nCMCL aims to learn universal vision and language represen-\ntation under the same semantic space by pushing the embed-\ndings of matched image-text pairs together while pushing the\nnon-matched ones apart. The image-to-text contrastive loss\ncan be formulated as:\nLi2t = −E(W,V )∈D\n\nlog\nsθ\n(\nhv[IMG] , hw[CLS]\n)\n∑\nW′ sθ\n(\nhv[IMG] , hw′\n[CLS]\n)\n\n, (5)\nwhere W′ belongs to the negative samples set of V , hw[CLS] ,\nhv[IMG] and hw′\n[CLS]\nare the representations of w[CLS], v[IMG]\nand w′\n[CLS], respectively, and sθ is a score function to justify\nhow similar a given image-text pair is. It is worth noting that\nthe contrastive loss in CMCL is symmetrical, and the text-to-\nimage contrastive loss is formulated similarly.\nCLIP [Radford et al., 2021] and ALIGN [Jia et al., 2021]\nleverage large-scale image-text pairs to learn transferable vi-\nsual representations and exhibit surprising zero-shot transfer\nto image classification tasks. ALBEF [Li et al., 2021a] pro-\nposes to adopt momentum distillation to facilitate contrastive\nlearning on massive noisy image-text pairs. WenLan [Huo\net al., 2021] employs MoCo [He et al., 2020] and maintains\na queue to store negative samples, which has been proven\nto be effective for contrastive learning. UNIMO [Li et al.,\n2020b] incorporates a large volume of unimodal data during\ncontrastive learning, allowing vision and language to enhance\neach other. It outperforms previous works on both multi-\nmodal and unimodal downstream tasks. [Yang et al., 2022]\nclaims that CMCL does not guarantee similar inputs from the\nsame modality stay close by, so they introduce intra-modal\ncontrastive learning to benefit representation learning.\n5 Adapting VL-PTMs to Vision-Language\nDownstream Tasks\nPre-training tasks are able to help VL-PTMs to learn general\nvisual and linguistic features, which can be applied to various\ndownstream tasks. In this section, we introduce several com-\nmon vision-language integration tasks and how VL-PTMs are\nadapted to them. Basically, we categorised these downstream\ntasks into cross-modal matching, cross-modal reasoning and\nvision and language generation.\n5.1 Cross-Modal Matching\nCross-modal matching requires VL-PTMs to learn cross-\nmodal correspondences between different modalities. We\nintroduce two commonly-used cross-modal matching tasks:\nimage text retrieval and visual referring expression.\nImage Text Retrieval (ITR). ITR is a typical cross-modal\nmatching task. This task requires retrieving an image that\nmatches a given sentence most and vice versa. Early VL-\nPTMs that utilize a fusion-encoder architecture obtain a fused\nvector representation which is later projected to a similarity\nscore [Lu et al., 2019; Liet al., 2019; Liet al., 2020c]. Dual-\nencoder architectures such as CLIP [Radford et al., 2021]\nand ALBEF [Li et al., 2021a] are more efficient for ITR as\nthey can pre-compute and store the embeddings of images\nand texts before retrieval.\nVisual Referring Expression (VRE). VRE is an extension\nof the referring expression task in NLP. The goal is to localize\nthe region in an image that corresponds to a specific textual\ndescription. Most VL-PTMs (e.g. [Lu et al., 2019]) take the\nfinal representation of the extracted region proposals as in-\nput and learn a linear projection to predict a matching score,\nwhich is the same strategy as ITR during fine-tuning.\n5.2 Cross-Modal Reasoning\nCross-modal reasoning requires VL-PTMs to perform lan-\nguage reasoning based on visual information. Ignoring any\nmodality gives poor performance. Here we present two\ncommonly-used cross-modal reasoning tasks.\nVisual Question Answering (VQA). VQA is a widely-\nused cross-modal reasoning task. Different from text-based\nQA, VQA requires answering questions about images. Most\nresearchers consider VQA as a classification task and require\nthe model to select a correct answer from an answer pool.\nVL-PTMs with a fusion-encoder architecture usually map the\nfinal cross-modal representation (usually corresponds to the\ninput [CLS] token) to the distribution of answer labels. How-\never, VL-PTMs with a dual-encoder architecture are not so\neffective for VQA tasks because the interaction between the\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5440\ntwo modalities is too shallow to conduct cross-modal reason-\ning. There are also some works modeling VQA as a gener-\nation task [Cho et al., 2021; Wanget al., 2021b], which can\ngeneralize better to real-world open-ended scenarios.\nNatural Language for Visual Reasoning (NLVR). NLVR\nprovides an image pair and a textual statement as input and\nrequires the model to decide whether the statement is true\nabout the image pair, thus can be considered as a binary clas-\nsification task. Most VL-PTMs first encode the given two\nimage-text pairs separately, then a classifier is trained over\nthe concatenation of the two embeddings to make a predic-\ntion [Tan and Bansal, 2019; Chen et al., 2020].\nVisual Commonsense Reasoning (VCR). VCR is consid-\nered to be another kind of VQA task. The main difference\nbetween VCR and VQA is that VCR’s questions pay more\nattention to visual common sense. Different from VQA, the\nVCR task can be decomposed into two multi-choice sub-\ntasks: question answering (Q → A) and answer justification\n(Q + A → R). Most VL-PTMs utilize the same approach in\nVQA to solve these two sub-tasks. For the question answer-\ning sub-task, the procedure is the same as VQA. For the an-\nswer justification sub-task, the concatenations of question and\nanswer are treated as the new questions and the rationales be-\ncome the options. A linear layer is trained to predict a score\nfor each possible option [Lu et al., 2019].\n5.3 Vision and Language Generation\nBased on the source modal and target modal, the generation\ntask can be divided into text-to-image generation and image-\nto-text generation (multimodal text generation).\nText-to-Image Generation. Text-to-Image generation is\nthe task of generating a corresponding image from a descrip-\ntive text. X-LXMERT[Cho et al., 2020] first converts contin-\nuous visual representations to discrete cluster centroids and\nthen ask the model to predict the cluster ids of masked re-\ngions. DALL-E [Ramesh et al., 2021] trains a codebook to\ntokenize images and formulates text-to-image generation task\nas an autoregressive generative task. It achieves new state-of-\nthe-art results on MS-COCO [Lin et al., 2014] in zero-shot\nsetting.\nMultimodal Text Generation. Multimodal text generation\ncan be regarded as a special type of conditional text genera-\ntion, where the condition includes not only texts but also im-\nages. Usually, a decoder is needed for the generation process.\nImage captioning is a typical image-to-text generation task\nthat requires the model to generate a description of an image.\nXGPT [Xia et al., 2021] and VL-T5 [Cho et al., 2021] en-\ncode the images first and then employ a decoder to generate\nthe captions autoregressively. Multimodal machine transla-\ntion is another generation task that aims to introduce images\nto improve translation quality. VL-T5[Cho et al., 2021] tack-\nles this task using the same strategy as in image captioning.\nAs for the connection between VL-PTMs architecture and\ndownstream tasks, fusion encoder is more suitable than dual\nencoder on cross-modal reasoning tasks for its powerful abil-\nity to model interaction. The dual encoder is more suitable\nfor cross-modal retrieval tasks since it keeps a similar perfor-\nmance as fusion encoder does while being more efficient.\n6 Conclusion and Future Directions\nIn this paper, we present an overview of VL-PTMs. We re-\nview the commonly-used architectures and discuss their ad-\nvantages and disadvantages. We also introduce several main-\nstream approaches to pre-training a VL-PTM and adapt it to\ndownstream tasks. Though VL-PTMs have made significant\nprogress on V-L tasks compared to traditional methods, there\nare still several challenges that could be the directions of fu-\nture research.\nUnified Model Architecture. Transformer-based models\nhave shown surprising performance on NLP, CV , and multi-\nmodal tasks. The success of transformer-based models in var-\nious domains indicates the possibility of using a single trans-\nformer model to learn a representation of different modali-\nties and building a general agent to handle tasks in different\ndomains. UNIMO [Li et al., 2020b] and FLA V A[Singh et\nal., 2021] make some inspiring attempts in this direction, but\ntheir performance on some tasks is much worse than the task-\nspecific baselines. Data2vec [Baevski et al., 2022] adopts\nself-supervised learning to unify vision, speech , and lan-\nguage. This model achieves competitive results to predom-\ninant methods on several tasks, which paves the way for a\npowerful unified model.\nModel Compression and Acceleration. Despite the great\nsuccess achieved by VL-PTMs in various fields, it is diffi-\ncult to deploy such a huge model in real-life scenarios, thus\nleading to a direction of VL-PTM compression and accel-\neration. Knowledge distillation has been used to compress\nVL-PTM [Fang et al., 2021], but some other traditional com-\npression methods such as quantization and pruning for VL-\nPTMs are yet to be explored. As for model acceleration, Li\net al.[2021b] construct a data-efficient paradigm for V-L pre-\ntraining. Despite all these achievements, only a few efforts\nfocus on improving VL-PTM’s inference speed.\nAdvanced Pre-training Methods. Though the current pre-\ntraining method seems quite effective, the potential of ad-\nvanced pre-training methods is yet to be explored. Using ad-\nversarial samples to enhance pre-training has been shown to\nbe effective[Gan et al., 2020], which helps VL-PTMs to over-\ncome the overfitting issue. Stage-wise pre-training [Wang et\nal., 2021a] has been proposed for better single-modal repre-\nsentation. With that ahead, the potential of pre-training meth-\nods are not fully developed, which is worth further studies.\nReaching the Limit of VL-PTMs. Nowadays, with the\nsuccess of large-scale PLMs in NLP, many researchers have\nalso tried to build a deeper model or use a larger dataset for\nV-L pre-training. ALIGN [Jia et al., 2021] has a number of\n675.4 million parameters and collects a huge dataset consist-\ning of 1.8 billion image-text pairs for pre-training. It achieves\nstate-of-the-art results on almost all downstream tasks. Wen-\nlan [Fei et al., 2021 ] expands the dataset to 650 million\nimage-text pairs and shows astonishing performance on both\nvision-language understanding and generation tasks. In the\nfuture, VL-PTMs will need more high-quality data and more\nparameters to reach a higher recognition level.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5441\nReferences\n[Antol et al., 2015] Stanislaw Antol, Aishwarya Agrawal, Ji-\nasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-\nnick, and Devi Parikh. Vqa: Visual question answering. In\nICCV, pages 2425–2433, 2015.\n[Arevalo et al., 2017] John Arevalo, Thamar Solorio,\nManuel Montes-y G ´omez, and Fabio A Gonz ´alez. Gated\nmultimodal units for information fusion. arXiv preprint\narXiv:1702.01992, 2017.\n[Baevski et al., 2022] Alexei Baevski, Wei-Ning Hsu,\nQiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli.\nData2vec: A general framework for self-supervised\nlearning in speech, vision and language. URL https://ai.\nfacebook. com/research/data2veca-general-framework-\nfor-self-supervi sed-learning-in-speech-vision-and-la\nnguage/. Accessed, 2022.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot\nlearners. NeurIPS, pages 1877–1901, 2020.\n[Chen et al., 2020] Yen-Chun Chen, Linjie Li, Licheng Yu,\nAhmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representation\nlearning. In ECCV, pages 104–120. Springer, 2020.\n[Cho et al., 2020] Jaemin Cho, Jiasen Lu, Dustin Schwenk,\nHannaneh Hajishirzi, and Aniruddha Kembhavi. X-\nlxmert: Paint, caption and answer questions with multi-\nmodal transformers. arXiv preprint arXiv:2009.11278,\n2020.\n[Cho et al., 2021] Jaemin Cho, Jie Lei, Hao Tan, and Mohit\nBansal. Unifying vision-and-language tasks via text gen-\neration. arXiv preprint arXiv:2102.02779, 2021.\n[Desai et al., 2021] Karan Desai, Gaurav Kaul, Zubin\nAysola, and Justin Johnson. Redcaps: Web-curated image-\ntext data created by the people, for the people. arXiv\npreprint arXiv:2111.11431, 2021.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Fang et al., 2021] Zhiyuan Fang, Jianfeng Wang, Xiaowei\nHu, Lijuan Wang, Yezhou Yang, and Zicheng Liu. Com-\npressing visual-linguistic model via knowledge distilla-\ntion. In CVPR, pages 1428–1438, 2021.\n[Fei et al., 2021] Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guox-\ning Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua\nSong, Xin Gao, Tao Xiang, et al. Wenlan 2.0: Make\nai imagine via a multimodal foundation model. arXiv\npreprint arXiv:2110.14378, 2021.\n[Gan et al., 2020] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen\nZhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\ntraining for vision-and-language representation learning.\nNeurIPS, pages 6616–6628, 2020.\n[He et al., 2020] Kaiming He, Haoqi Fan, Yuxin Wu, Sain-\ning Xie, and Ross Girshick. Momentum contrast for un-\nsupervised visual representation learning. In CVPR, pages\n9729–9738, 2020.\n[Huang et al., 2020] Zhicheng Huang, Zhaoyang Zeng, Bei\nLiu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning\nimage pixels with text by deep multi-modal transformers.\narXiv preprint arXiv:2004.00849, 2020.\n[Huang et al., 2021] Zhicheng Huang, Zhaoyang Zeng, Yu-\npan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. See-\ning out of the box: End-to-end pre-training for vision-\nlanguage representation learning. In CVPR, pages 12976–\n12985, 2021.\n[Huo et al., 2021] Yuqi Huo, Manli Zhang, Guangzhen Liu,\nHaoyu Lu, Yizhao Gao, Guoxing Yang, Jingyuan Wen,\nHeng Zhang, Baogui Xu, Weihao Zheng, et al. Wenlan:\nBridging vision and language by large-scale multi-modal\npre-training. arXiv preprint arXiv:2103.06561, 2021.\n[Jia et al., 2021] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting\nChen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and\nvision-language representation learning with noisy text su-\npervision. arXiv preprint arXiv:2102.05918, 2021.\n[Kim et al., 2021] Wonjae Kim, Bokyung Son, and Ildoo\nKim. Vilt: Vision-and-language transformer with-\nout convolution or region supervision. arXiv preprint\narXiv:2102.03334, 2021.\n[Krishna et al., 2017] Ranjay Krishna, Yuke Zhu, Oliver\nGroth, Justin Johnson, Kenji Hata, Joshua Kravitz,\nStephanie Chen, Yannis Kalantidis, Li-Jia Li, David A\nShamma, et al. Visual genome: Connecting language\nand vision using crowdsourced dense image annotations.\nIJCV, pages 32–73, 2017.\n[Lee et al., 2018] Kuang-Huei Lee, Xi Chen, Gang Hua,\nHoudong Hu, and Xiaodong He. Stacked cross attention\nfor image-text matching. In ECCV, pages 201–216, 2018.\n[Li et al., 2019] Liunian Harold Li, Mark Yatskar, Da Yin,\nCho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\nand performant baseline for vision and language. arXiv\npreprint arXiv:1908.03557, 2019.\n[Li et al., 2020a] Gen Li, Nan Duan, Yuejian Fang, Ming\nGong, and Daxin Jiang. Unicoder-vl: A universal encoder\nfor vision and language by cross-modal pre-training. In\nAAAI, pages 11336–11344, 2020.\n[Li et al., 2020b] Wei Li, Can Gao, Guocheng Niu, Xinyan\nXiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang.\nUnimo: Towards unified-modal understanding and gener-\nation via cross-modal contrastive learning. arXiv preprint\narXiv:2012.15409, 2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5442\n[Li et al., 2020c] Xiujun Li, Xi Yin, Chunyuan Li,\nPengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan\nWang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar:\nObject-semantics aligned pre-training for vision-language\ntasks. In ECCV, pages 121–137. Springer, 2020.\n[Li et al., 2021a] Junnan Li, Ramprasaath Selvaraju,\nAkhilesh Gotmare, Shafiq Joty, Caiming Xiong, and\nSteven Chu Hong Hoi. Align before fuse: Vision\nand language representation learning with momentum\ndistillation. NeurIPS, 2021.\n[Li et al., 2021b] Yangguang Li, Feng Liang, Lichen Zhao,\nYufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and\nJunjie Yan. Supervision exists everywhere: A data ef-\nficient contrastive language-image pre-training paradigm.\narXiv preprint arXiv:2110.05208, 2021.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Be-\nlongie, James Hays, Pietro Perona, Deva Ramanan, Pi-\notr Doll ´ar, and C Lawrence Zitnick. Microsoft coco:\nCommon objects in context. In ECCV, pages 740–755.\nSpringer, 2014.\n[Lin et al., 2020] Junyang Lin, An Yang, Yichang Zhang,\nJie Liu, Jingren Zhou, and Hongxia Yang. Interbert:\nVision-and-language interaction for multi-modal pretrain-\ning. arXiv preprint arXiv:2003.13198, 2020.\n[Lu et al., 2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and\nStefan Lee. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. arXiv\npreprint arXiv:1908.02265, 2019.\n[Mogadala et al., 2021] Aditya Mogadala, Marimuthu\nKalimuthu, and Dietrich Klakow. Trends in integration of\nvision and language research: A survey of tasks, datasets,\nand methods. JAIR, pages 1183–1317, 2021.\n[Ordonez et al., 2011] Vicente Ordonez, Girish Kulkarni,\nand Tamara Berg. Im2text: Describing images using 1\nmillion captioned photographs. NeurIPS, 2011.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris\nHallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al. Learning transferable visual mod-\nels from natural language supervision. arXiv preprint\narXiv:2103.00020, 2021.\n[Ramesh et al., 2021] Aditya Ramesh, Mikhail Pavlov,\nGabriel Goh, Scott Gray, Chelsea V oss, Alec Radford,\nMark Chen, and Ilya Sutskever. Zero-shot text-to-image\ngeneration. In ICML, pages 8821–8831, 2021.\n[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross Gir-\nshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. NeurIPS, 2015.\n[Ruan and Jin, 2022] Ludan Ruan and Qin Jin. Survey:\nTransformer based video-language pre-training. AI Open,\n2022.\n[Sharma et al., 2018] Piyush Sharma, Nan Ding, Sebastian\nGoodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic\nimage captioning. In ACL, pages 2556–2565, 2018.\n[Simonyan and Zisserman, 2014] Karen Simonyan and An-\ndrew Zisserman. Very deep convolutional networks\nfor large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[Singh et al., 2021] Amanpreet Singh, Ronghang Hu,\nVedanuj Goswami, Guillaume Couairon, Wojciech\nGaluba, Marcus Rohrbach, and Douwe Kiela. Flava: A\nfoundational language and vision alignment model. arXiv\npreprint arXiv:2112.04482, 2021.\n[Su et al., 2019] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li,\nLewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training\nof generic visual-linguistic representations. arXiv preprint\narXiv:1908.08530, 2019.\n[Suhr et al., 2018] Alane Suhr, Stephanie Zhou, Ally Zhang,\nIris Zhang, Huajun Bai, and Yoav Artzi. A corpus for rea-\nsoning about natural language grounded in photographs.\narXiv preprint arXiv:1811.00491, 2018.\n[Tan and Bansal, 2019] Hao Tan and Mohit Bansal. Lxmert:\nLearning cross-modality encoder representations from\ntransformers. arXiv preprint arXiv:1908.07490, 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, 2017.\n[Wang et al., 2021a] Wenhui Wang, Hangbo Bao, Li Dong,\nand Furu Wei. Vlmo: Unified vision-language pre-\ntraining with mixture-of-modality-experts. arXiv preprint\narXiv:2111.02358, 2021.\n[Wang et al., 2021b] Zirui Wang, Jiahui Yu, Adams Wei Yu,\nZihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Sim-\nple visual language model pretraining with weak supervi-\nsion. arXiv preprint arXiv:2108.10904, 2021.\n[Xia et al., 2021] Qiaolin Xia, Haoyang Huang, Nan Duan,\nDongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon\nBharti, and Ming Zhou. Xgpt: Cross-modal generative\npre-training for image captioning. In NLPCC, pages 786–\n797, 2021.\n[Xue et al., 2021] Hongwei Xue, Yupan Huang, Bei Liu,\nHouwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo.\nProbing inter-modality: Visual parsing with self-attention\nfor vision-and-language pre-training. NeurIPS, 2021.\n[Yang et al., 2022] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu,\nSampath Chanda, Liqun Chen, Belinda Zeng, Trishul\nChilimbi, and Junzhou Huang. Vision-language pre-\ntraining with triple contrastive learning. arXiv preprint\narXiv:2202.10401, 2022.\n[Zellers et al., 2019] Rowan Zellers, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. From recognition to cognition:\nVisual commonsense reasoning. In CVPR, pages 6720–\n6731, 2019.\n[Zhou et al., 2020] Luowei Zhou, Hamid Palangi, Lei\nZhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Uni-\nfied vision-language pre-training for image captioning and\nvqa. In AAAI, pages 13041–13049, 2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5443"
}