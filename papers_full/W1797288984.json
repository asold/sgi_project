{
  "title": "Entropy-based Pruning of Backoff Language Models",
  "url": "https://openalex.org/W1797288984",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Stolcke, A.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2157140289",
    "https://openalex.org/W1903115690",
    "https://openalex.org/W32217939",
    "https://openalex.org/W2162995740",
    "https://openalex.org/W1757803263",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W2099111195",
    "https://openalex.org/W2611071497"
  ],
  "abstract": "A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance.",
  "full_text": "arXiv:cs/0006025v1  [cs.CL]  11 Jun 2000\nEntropy-based Pruning of Backoff Language Models\nAndreas Stolcke\nSpeech Technology And Research Laboratory\nSRI International\nMenlo Park, California\nABSTRACT\nA criterion for pruning parameters from N-gram backoff language\nmodels is developed, based on the relative entropy between the orig-\ninal and the pruned model. It is shown that the relative entropy re-\nsulting from pruning a single N-gram can be computed exactlyand\nefﬁciently for backoff models. The relative entropy measure can be\nexpressed as a relative change in training set perplexity. This leads\nto a simple pruning criterion whereby all N-grams that change per-\nplexity by less than a threshold are removed from the model. Exper-\niments show that a production-quality Hub4 LM can be reducedto\n26% its original size without increasing recognition error. We also\ncompare the approach to a heuristic pruning criterion by Seymore\nand Rosenfeld [9], and show that their approach can be interpreted\nas an approximation to the relative entropy criterion. Experimen-\ntally, both approaches select similar sets of N-grams (about 85%\noverlap), with the exact relative entropy criterion givingmarginally\nbetter performance.\n1. Introduction\nN-gram backoff models [5], despite their shortcomings,\nstill dominate as the technology of choice for state-of-the-\nart speech recognizers [4]. Two sources of performance\nimprovements are the use of higher-order models (several\nDARPA-Hub4 sites now use 4-gram or 5-gram models) and\nthe inclusion of more training data from more sources (Hub4\nmodels typically include Broadcast News, NABN and WSJ\ndata). Both of these approaches lead to model sizes that are\nimpractical unless some sort of parameter selection technique\nis used. In the case of N-gram models, the goal of parame-\nter selection is to chose which N-grams should have explicit\nconditional probability estimates assigned by the model, so\nas to maximize performance (i.e., minimize perplexity and/or\nrecognition error) while minimizing model size. As pointed\nout in [6], pruning (selecting parameters from) a full N-gram\nmodel of higher order amounts to building avariable-length\nN-gram model, i.e., one in which training set contexts are not\nuniformly represented by N-grams of the same length.\nSeymore and Rosenfeld [9] showed that selecting N-grams\nbased on their conditional probability estimates and fre-\nquency of use is more effective than the traditional absolute\nfrequency thresholding. In this paper we revisit the problem\nof N-gram parameter selection by deriving a criterion that sat-\nisﬁes the following desiderata.\n• Soundness: The criterion should optimize some well-\nunderstood information-theoretic measure of language\nmodel quality.\n• Efﬁciency: An N-gram selection algorithm should be\nfast, i.e., take time proportional to the number of N-\ngrams under consideration.\n• Self-containedness: As a practical consideration, we\nwant to be able to prune N-grams from existing language\nmodels. This means a pruning criterion should be based\nonly on information contained in the model itself.\nIn the remainder of this paper we describe our pruning algo-\nrithm based on relative entropy distance between N-gram dis-\ntributions (Section 2), investigate how the quantities required\nfor the pruning criterion can be obtained efﬁciently and ex-\nactly (Section 3), show that the criterion is highly effective\nin reducing the size of state-of-the-art language models with\nnegligible performance penalties (Section 4), investigate the\nrelation between our pruning criterion and that of Seymore\nand Rosenfeld (Section 5), and draw some conclusions (Sec-\ntion 6).\n2. N-gram Pruning Based on Relative\nEntropy\nAn N-gram language model represents a probability distri-\nbution over wordsw, conditioned on(N − 1)-tuples of pre-\nceding words, or historiesh. Only a ﬁnite set of N-grams\n(w, h ) have conditional probabilities explicitly represented in\nthe model. The remaining N-grams are assigned a probability\nby the recursive backoff rule\np(w|h) = α (h)p(w|h′)\nwhere h′ is the historyh truncated by the ﬁrst word (the one\nmost distant fromw), andα (h) is abackoff weight associated\nwith historyh, determined so that∑\nw p(w|h) = 1.\nThe goal of N-gram pruning is to remove explicit estimates\np(w|h) from the model, thereby reducing the number of pa-\nrameters, while minimizing the performance loss. Note that\nafter pruning, the retained explicit N-gram probabilitiesare\nunchanged, but backoff weights will have to be recomputed,\nthereby changing the values of implicit (backed-off) proba-\nbility estimates. Thus, the pruning approach chosen is con-\nceptually independent of the estimator chosen to determine\nthe explicit N-gram estimates.\nSince one of our goals is to prune N-gram models without\naccess to any statistics not contained in the model itself, a\nnatural criterion is to minimize the ‘distance’ between the\ndistribution embodied by the original model and that of the\npruned model. A standard measure of divergence between\ndistributions isrelative entropy orKullback-Leibler distance\n(see, e.g., [2]). Although not strictly a distance metric, it is a\nnon-negative, continuous function that is zero if and only if\nthe two distributions are identical.\nLet p(·|·) denote the conditional probabilities assigned by\nthe original model, andp′(·|·) the probabilities in the pruned\nmodel. Then, the relative entropy between the two models is\nD(p||p′) = −\n∑\nwi,hj\np(wi, h j )[log p′(wi |hj ) − log p(wi|hj )]\n(1)\nwhere the summation is over all wordswi and histories (con-\ntexts)hj .\nOur goal will be to select N-grams for pruning such that\nD(p||p′) is minimized. However, it would not be feasible\nto maximize over all possible subsets of N-grams. Instead,\nwe will assume that the N-grams affect the relative entropy\nroughly independently, and computeD(p||p′) due to each in-\ndividual N-gram. We can then rank the N-grams by their ef-\nfect on the model entropy, and prune those that increase rela-\ntive entropy the least.\nTo choose pruning thresholds, it is helpful to look at a more\nintuitive interpretation ofD(p||p′) in terms ofperplexity, the\naverage branching factor of the language model. The per-\nplexity of the original model (evaluated on the distribution it\nembodies) is given by\nPP = e\n−\n∑\nh,w p(h,w) logp(w|h)\n,\nwhereas the perplexity of the pruned model on the original\ndistribution is\nPP ′ = e\n−\n∑\nh,w p(h,w) logp′(w|h)\nThe relative change in model perplexity can now be expressed\nin terms of relative entropy:\nPP ′ − PP\nPP = eD(p||p′) − 1\nThis suggests a simple thresholding algorithm for N-gram\npruning:\n1. Select a thresholdθ .\n2. Compute the relative perplexity increase due to pruning\neach N-gram individually.\n3. Remove all N-grams that raise the perplexity by less than\nθ , and recompute backoff weights.\nRelation to Other W ork Our choice of relative entropy\nas an optimization criterion is by no means new. Relative\nentropy minimization (sometimes in the guise of likelihood\nmaximization) is the basis of many model optimization tech-\nniques proposed in the past, e.g., for text compression [1],\nMarkov model induction [10, 7]. Kneser [6] ﬁrst suggested\napplying it to backoff N-gram models, although, as shown in\nSection 5, the heuristic pruning algorithm of Seymore and\nRosenfeld [9] amounts to an approximate relative entropy\nminimization. The algorithm described in the next section\nis novel in that it removes some of the approximations em-\nployed in previous approaches. Speciﬁcally, the algorithmof\n[6] assumes that backoff weights are unchanged by the prun-\ning, and [9] does not consider the effect that a changed back-\noff weight has on N-gram probabilities other than the pruned\none (this effect is discussed in more detail in Section 5).\nThe main approximation that remains in our algorithm is the\ngreedy aspect: we do not consider possible interactions be-\ntween selected N-grams, and prune based solely on relative\nentropy due to removing a single N-gram, so as to avoid\nsearching the exponential space of N-gram subsets.\n3. Computing Relative Entropy\nWe now show how the relative entropyD(p||p′) due to prun-\ning a single N-gram parameter can be computed exactly and\nefﬁciently. Consider the effect of removing an N-gram con-\nsisting of historyh and wordw. This entails two changes to\nthe probability estimates.\n• The backoff weightα (h) associated with historyh is\nchanged, affecting all backed-off estimates involving\nhistoryh. We use the notationBO(wi, h ) to denote this\ncase, i.e., that the original model does not contain an\nexplicit N-gram estimate for(wi, h ). Letα (h) be the\noriginal backoff weight, andα ′(h) the backoff weight in\nthe pruned model.\n• The explicit estimatep(w|h) is replaced by a backoff\nestimate\np′(w|h) = α ′(h)p(w|h′)\nwhere h′ is the history obtained by dropping the ﬁrst\nword inh.\nAll estimates not involving historyh remain unchanged, as\ndo all estimates for whichBO(wi, h ) is not true.\nSubstituting in (1), we get\nD(p||p′) = −\n∑\nwi\np(wi, h )[log p′(wi |h) − log p(wi|h)] (2)\n= − p(w, h )[log p′(w|h) − log p(w|h)]\n−\n∑\nwi : BO(wi , h)\np(wi, h )[log p′(wi|h) − log p(wi|h)]\n= − p(h)\n{\np(w|h)[log p′(w|h) − log p(w|h)]\n+\n∑\nwi : BO(wi , h)\np(wi|h)[log p′(wi|h) − log p(wi|h)]\n}\nAt ﬁrst it seems as if computingD(p||p′) for a given N-\ngram requires a summation over the vocabulary, something\nthat would be infeasible for large vocabularies and/or mod-\nels. However, by plugging in the terms for the backed-off\nestimates, we see that the sum can be factored so as to allow\na more efﬁcient computation.\nD(p||p′)\n= − p(h)\n{\np(w|h) logp(w|h′) + logα ′(h) − log p(w|h)]\n+\n∑\nwi : BO(wi , h)\np(wi|h)[log α ′(h) − log α (h)]\n}\n= − p(h)\n{\np(w|h)[log p(w|h′) + logα ′(h) − log p(w|h)]\n+[log α ′(h) − log α (h)]\n∑\nwi : BO(wi , h)\np(wi|h)\n}\nThe sum in the last line represents the total probability mass\ngiven to backoff (the numerator for computingα (h)); it needs\nto be computed only once for eachh, which is done efﬁciently\nby summing over allnon-backoff estimates:\n∑\nwi :BO(wi ,h)\np(wi|h) = 1 −\n∑\nwi :¬BO(wi ,h)\np(wi|h)\nThe marginal history probabilitiesp(h) are obtained by mul-\ntiplying conditional probabilitiesp(h1)p(h2|h1) . . . .\nFinally, we need to be able to compute the revised backoff\nweightsα ′(h) efﬁciently, i.e., in constant time per N-gram.\nRecall that\nα (h) =\n1 − ∑\nwi :¬BO(wi ,h) p(wi|h)\n1 − ∑\nwi :¬BO(wi ,h) p(wi|h′)\nα ′(h) is obtained by dropping the term for the pruned N-gram\n(w, h ) from the summation in both numerator and denomina-\ntor. Thus, we compute the original numerator and denomi-\nnator once per historyh, and then addp(w|h) and p(w|h′),\nrespectively, to obtainα ′(h) for each prunedw.\n4. Experiments\nWe evaluated relative entropy-based language model pruning\nin the Broadcast News domain, using SRI’s 1996 Hub4 eval-\nuation system [8]. N-best lists generated with a bigram lan-\nguage model were rescored with various pruned versions of a\nlarge four-gram language model.1\n1We used the 1996 system, partly due to time constraints, partly be-\ncause the 1997 system generated N-best lists using a large trigram language\nmodel, which makes rescoring experiments with smaller language models\nless meaningful.\nθ bigrams trigrams 4-grams PP WER\n0 11093357 14929826 3266900 163.0 32.6\n10−9 7751596 9634165 1938343 163.9 32.6\n10−8 3186359 3651747 687742 172.3 32.6\n10−7 829827 510646 62481 202.3 33.9\n0 11093357 14929826 0 172.5 32.9\nTable 1: Perplexity (PP) and word error rate (WER) as a\nfunction of pruning threshold and language model sizes.\nAs noted in Section 2, the pruning algorithm is applicable ir-\nrespective of the particular N-gram estimator used. We used\nGood-Turing smoothing [3] throughout and did not investi-\ngate possible interactions between smoothing methods and\npruning.\nTable 1 shows model size, perplexity and word error results as\ndetermined on the development test set, for various pruning\nthresholds. The ﬁrst and last rows of the table give the per-\nformance of the full four-gram and the pure trigram model,\nrespectively. Note that perplexity here refers to the indepen-\ndent test set, not to the training set perplexity that underlies\nthe pruning criterion.\nAs shown, pruning is highly effective. Forθ = 10−8, we ob-\ntain a model that is 26% the size of the original model with-\nout degradation in recognition performance and less than 6%\nperplexity increase. Comparing the pruned four-gram model\nto the full trigram model, we see that it is better to include\nnon-redundant four-grams than to use a much larger number\nof trigrams. The pruned (θ = 10−8) four-gram has the same\nperplexity and lower word error (p < 0. 07) than the full tri-\ngram.\n5. Comparison to Seymore and Rosenfeld’s\nApproach\nIn [9], Seymore and Rosenfeld proposed a different pruning\nscheme for backoff models (henceforth called the “SR crite-\nrion,” as opposed to the relative entropy, or “RE criterion”).\nIn the SR approach, N-grams are ranked by a weighted differ-\nence of the log probability estimate before and after pruning,\nN (w, h )[log p(w|h) − log p′(w|h)] (3)\nwhere N (w, h ) is the discounted frequency with which N-\ngram (w, h ) was observed in training. Comparing (3) with\nthe expansion ofD(p||p′) in (2), we see that the two crite-\nria are related. First, we can assume thatN (w, h ) is roughly\nproportional top(w, h ), so for ranking purposes the two are\nequivalent. The difference of the log probabilities in (3) cor-\nresponds to the same quantity in (2). Thus, the major dif-\nference between the two approaches is that the SR criterion\ndoes not include the effect on N-grams other than the one be-\ning considered, namely, those due to changes in the backoff\nNo. Trigrams SR RE\n1000 238.1 237.9\n10000 225.1 223.9\n100000 207.3 205.2\n1000000 186.4 184.7\nTable 2: Comparison of Seymore and Rosenfeld (SR) and\nRelative Entropy (RE) pruning: perplexities as a function of\nthe number of trigrams.\nweightα (h).\nTo evaluate the effect of ignoring backed-off estimates in the\npruning criterion we compared the performance of the SR and\nthe RE criterion on the Broadcast News development test set,\nusing the same N-best rescoring system as described before.\nTo make the methods comparable we adopted Seymore and\nRosenfeld’s approach of ranking the N-grams according to\nthe criterion in question, and to retain a speciﬁed number of\nN-grams from the top of the ranked list. For the sake of sim-\nplicity we used a trigram-only version of the Hub4 language\nmodel used earlier, and restricted pruning to trigrams.\nWe also veriﬁed that the discounted frequencyN (w, h ) in\n(3) could be replaced with the model’s N-gram probability\np(w, h ) without changing the ranking signiﬁcantly: over 99%\nof the chosen N-grams were the same. This means the SR cri-\nterion can also be based entirely on information in the model\nitself, making it more convenient for model post-processing.\nTables 2 and 3 show model perplexity and word error rates,\nrespectively, for the two pruning methods as a function of the\nnumber of trigrams in the model. In terms of perplexity, we\nsee a very small, albeit consistent, advantage for the relative\nentropy method, as expected given the optimized criterion.\nHowever, the difference is negligible when it comes to recog-\nnition performance, where results are identical or differ only\nnon-signiﬁcantly. We can thus conclude that, for practical\npurposes, the SR criterion is a very good approximation to\nthe RE criterion.\nNo. Trigrams SR RE\n0 35.8\n1000 35.5 35.5\n10000 34.8 34.8\n100000 34.3 34.2\n1000000 33.2 33.1\nAll 32.9\nTable 3: Comparison of Seymore and Rosenfeld (SR) and\nRelative Entropy (RE) pruning: word error rate as a function\nof the number of trigrams.\nNo. Trigrams No. shared trigrams\n1000 883\n10000 8721\n100000 85599\n1000000 852016\nTable 4: Overlap of selected trigrams between SR and RE\nmethods.\nFinally, we looked at the overlap of the N-grams chosen by\nthe two criteria, shown in Table 4. The percentage of common\ntrigrams ranges from 88.3% to 85.2%, and seems to decrease\nas the model size increases. We can expect the most frequent\nN-grams to be among those that are shared, making it no sur-\nprise that both methods perform so similarly.\n6. Conclusions\nWe developed an algorithm for N-gram selection for backoff\nN-gram language models, based on minimizing the relative\nentropy between the full and the pruned model. Experiments\nshow that the algorithm is highly effective, eliminating all but\n26% of the parameters in a Hub4 four-gram model without\nsigniﬁcantly affecting performance. The pruning criterion of\nSeymore and Rosenfeld is seen to be an approximate version\nof the relative entropy criterion; empirically, the two methods\nperform about the same.\nAcknowledgments\nThis work was sponsored by DARPA through the Naval Com-\nmand and Control Ocean Surveillance Center under contract\nN66001-94-C-6048. I thank Roni Rosenfeld and Kristie Sey-\nmore for clariﬁcations and discussions regarding their paper\n[9]. Thanks also to Hermann Ney and Dietrich Klakow for\npointing out similarities to [6].\nReferences\n1. T. C. Bell, J. G. Cleary, and I. H. Witten.T ext Compression.\nPrentice Hall, Englewood Cliffs, N.J., 1990.\n2. T. M. Cover and J. A. Thomas.Elements of Information The-\nory. John Wiley and Sons, Inc., New York, 1991.\n3. I. J. Good. The population frequencies of species and the es-\ntimation of population parameters.Biometrika, 40:237–264,\n1953.\n4. F. Jelinek. Up from trigrams! The struggle for improved lan-\nguage models. InProc. EUROSPEECH, pp. 1037–1040, Gen-\nova, Italy, 1991.\n5. S. M. Katz. Estimation of probabilities from sparse data for\nthe language model component of a speech recognizer.IEEE\nASSP, 35(3):400–401, 1987.\n6. R. Kneser. Statistical language modeling using a variable con-\ntext length. In H. T. Bunnell and W. Idsardi, editors,Proc. IC-\nSLP, vol. 1, pp. 494–497, Philadelphia, 1996.\n7. D. Ron, Y . Singer, and N. Tishby. The power of amnesia. In\nJ. Cowan, G. Tesauro, and J. Alspector, editors,NIPS-5, pp.\n176–183. Morgan Kaufmann, San Mateo, CA, 1994.\n8. A. Sankar, L. Heck, and A. Stolcke. Acoustic modeling for\nthe SRI Hub4 partitioned evaluation continuous speech recog-\nnition system. InProceedings DARP A Speech Recognition\nW orkshop, pp. 127–132, Chantilly, V A, 1997. Morgan Kauf-\nmann.\n9. K. Seymore and R. Rosenfeld. Scalable backoff language\nmodels. In H. T. Bunnell and W. Idsardi, editors,Proc. IC-\nSLP, vol. 1, pp. 232–235, Philadelphia, 1996.\n10. A. Stolcke and S. Omohundro. Hidden Markov model induc-\ntion by Bayesian model merging. In S. J. Hanson, J. D. Cowan,\nand C. L. Giles, editors,NIPS-5, pp. 11–18. Morgan Kauf-\nmann, San Mateo, CA, 1993.\nErratum\nThe published paper had an error in the second equation for\nD(p||p′) in Section 3. In two instances, the quantitylog α ′(h)\nhad been mistakenly typeset aslog α (h′). Also, the informa-\ntion in reference [6] was incorrect.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.7975538969039917
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.5766453146934509
    },
    {
      "name": "Computer science",
      "score": 0.4748245179653168
    },
    {
      "name": "Mathematics",
      "score": 0.4735885262489319
    },
    {
      "name": "Language model",
      "score": 0.4707741439342499
    },
    {
      "name": "Approximation error",
      "score": 0.4531148672103882
    },
    {
      "name": "Pruning",
      "score": 0.4174349904060364
    },
    {
      "name": "Kullback–Leibler divergence",
      "score": 0.41583770513534546
    },
    {
      "name": "Algorithm",
      "score": 0.3691869378089905
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3367769718170166
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}