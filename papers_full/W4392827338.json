{
  "title": "A Semantic and Motion-Aware Spatiotemporal Transformer Network for Action Detection",
  "url": "https://openalex.org/W4392827338",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5041300663",
      "name": "Matthew Korban",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A5081562360",
      "name": "Peter Youngs",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A5034452294",
      "name": "Scott T. Acton",
      "affiliations": [
        "University of Virginia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2156983007",
    "https://openalex.org/W6681511049",
    "https://openalex.org/W1797109199",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W3040942941",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3100950981",
    "https://openalex.org/W4362606112",
    "https://openalex.org/W2019245255",
    "https://openalex.org/W2044291182",
    "https://openalex.org/W2611596598",
    "https://openalex.org/W3048602385",
    "https://openalex.org/W3173407577",
    "https://openalex.org/W6682864246",
    "https://openalex.org/W2342662179",
    "https://openalex.org/W2751445731",
    "https://openalex.org/W3030265329",
    "https://openalex.org/W3186835416",
    "https://openalex.org/W2962722947",
    "https://openalex.org/W2963529931",
    "https://openalex.org/W4319300031",
    "https://openalex.org/W3206778864",
    "https://openalex.org/W4377229961",
    "https://openalex.org/W6798038150",
    "https://openalex.org/W3173459793",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W4296894930",
    "https://openalex.org/W3110666852",
    "https://openalex.org/W6600983433",
    "https://openalex.org/W4220967417",
    "https://openalex.org/W6798086466",
    "https://openalex.org/W6840410254",
    "https://openalex.org/W1539249290",
    "https://openalex.org/W2259424905",
    "https://openalex.org/W6749916090",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2618799552",
    "https://openalex.org/W3109173645",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W2951702519",
    "https://openalex.org/W4312372834",
    "https://openalex.org/W4312772544",
    "https://openalex.org/W3206930349",
    "https://openalex.org/W6810265253",
    "https://openalex.org/W3170837227",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W3204116406",
    "https://openalex.org/W3175859344",
    "https://openalex.org/W4312509322",
    "https://openalex.org/W4312312750",
    "https://openalex.org/W4319299890",
    "https://openalex.org/W2883275382",
    "https://openalex.org/W3009812836",
    "https://openalex.org/W4213160450",
    "https://openalex.org/W2949343965",
    "https://openalex.org/W2981808500",
    "https://openalex.org/W2962688385",
    "https://openalex.org/W3096824106",
    "https://openalex.org/W3131080139",
    "https://openalex.org/W4313193611",
    "https://openalex.org/W4225849788",
    "https://openalex.org/W6797613833",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W6797263693",
    "https://openalex.org/W3173621652",
    "https://openalex.org/W4312658081",
    "https://openalex.org/W3109174848",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W24089286"
  ],
  "abstract": "This paper presents a novel spatiotemporal transformer network that introduces several original components to detect actions in untrimmed videos. First, the multi-feature selective semantic attention model calculates the correlations between spatial and motion features to model spatiotemporal interactions between different action semantics properly. Second, the motion-aware network encodes the locations of action semantics in video frames utilizing the motion-aware 2D positional encoding algorithm. Such a motion-aware mechanism memorizes the dynamic spatiotemporal variations in action frames that current methods cannot exploit. Third, the sequence-based temporal attention model captures the heterogeneous temporal dependencies in action frames. In contrast to standard temporal attention used in natural language processing, primarily aimed at finding similarities between linguistic words, the proposed sequence-based temporal attention is designed to determine both the differences and similarities between video frames that jointly define the meaning of actions. The proposed approach outperforms the state-of-the-art solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24, and EPIC-Kitchens.",
  "full_text": "JOURNAL OF LATEX CLASS FILES 1\nA Semantic and Motion-Aware Spatiotemporal\nTransformer Network for Action Detection\nMatthew Korban, Senior Member, IEEE, Peter Y oungs, Scott T. Acton,Fellow, IEEE\nAbstract—This paper presents a novel spatiotemporal transformer network that introduces several original components to detect\nactions in untrimmed videos. First, the multi-feature selective semantic attention model calculates the correlations between spatial and\nmotion features to model spatiotemporal interactions between different action semantics properly. Second, the motion-aware network\nencodes the locations of action semantics in video frames utilizing the motion-aware 2D positional encoding algorithm. Such a\nmotion-aware mechanism memorizes the dynamic spatiotemporal variations in action frames that current methods cannot exploit.\nThird, the sequence-based temporal attention model captures the heterogeneous temporal dependencies in action frames. In contrast\nto standard temporal attention used in natural language processing, primarily aimed at finding similarities between linguistic words, the\nproposed sequence-based temporal attention is designed to determine both the differences and similarities between video frames that\njointly define the meaning of actions. The proposed approach outperforms the state-of-the-art solutions on four spatiotemporal action\ndatasets: AVA 2.2, AVA 2.1, UCF101-24, and EPIC-Kitchens.\nIndex Terms—Human action detection, transformer network, spatiotemporal attention, action semantics, positional encoding.\n✦\n1 I NTRODUCTION\nS\nPATIOTEMPORAL action detection aims to localize action\nclass instances in untrimmed videos in both spatial and\ntemporal dimensions [1]. However, such a spatiotemporal\naction detection faces several challenges, including (1) com-\nplex spatiotemporal interactions between action semantics,\n(2) dynamic spatiotemporal variations in action semantics,\nand (3) heterogeneous temporal dependencies between ac-\ntion frames. We will explain our proposed solutions to solve\nthe challenges above as follows:\nThe semantics are the meaningful components of actions\nthat can be categorized into two fundamental types: persons\nand objects. [2] is one of the first methods incorporating\naction semantics into spatiotemporal action detection. How-\never, their method was limited to finding the motion tra-\njectory of localized objects over video frames, which alone\nmight not be enough to model various actions. [3] addressed\nsuch a limitation by including persons and objects that\nbetter represent human actions. Yet, they did not consider\nthe interactions between semantics, which are crucial parts\nof actions. To solve this issue, [4] proposed a video action\ntransformer network that captures the correlations between\na central person and the surrounding pixels. However, they\ndid not explicitly model the interactions between all the\naction semantics in video frames. However, many actions\nare defined based on the spatiotemporal interactions be-\ntween action semantics. For example, the action “kicking\na ball” is characterized by the interaction between a “per-\nThis material is based upon work supported by the National Science\nFoundation under Grant No. 2000487 and partly supported by NSF\nunder Grant No. 2322993. (Corresponding author: Scott T. Acton.)\nMatthew Korban and Scott T. Acton are with the Depart-\nment of Electrical and Computer Engineering, University of Vir-\nginia, Charlottesville, VA 22904 (e-mail: {acw6ze@virgnia.edu, ac-\nton@virginia.edu}).\nPeter Youngs is with the Department of Curriculum, Instruction and\nSpecial Education at the University of Virginia, Charlottesville, VA\n22903 (email: pay2n@virginia.edu).\nson” and a moving “ball”, in both spatial and temporal\ndomains. These domains are associated with the spatial\nand motion properties of action semantics, respectively.\nMore examples will be illustrated in Fig. 3. To address this\nissue, we propose a multi-feature semantic attention model\nthat enables the transformer network to selectively capture\nthe spatiotemporal interactions between action semantics\nbased on the correlations between their motion and spa-\ntial features. More details about the multi-feature selective\nattention model are explained in Section 3.3.2.\nThe positional encoding of the transformer network\nextends its capability to encode the order information in\nthe data [5]. As an alternative to the sequential connection\nin RNN and LSTM, the positional encoding allows the\ntransformer network to process sequential data more effec-\ntively and efficiently. The positional encoding was initially\nproposed to represent 1D temporal order information in\nlinguistic words [6]. It is also used to encode spatial order\ninformation in computer vision [7], [8] recently. Two main\nstrategies to incorporate positional information in computer\nvision have been ordered pixels [7] and patches of pixels [8].\nNevertheless, the current positional encoding strategies\nhave two issues when dealing with video frames: First, they\nare still based on standard 1D temporal positional encoding\ndesigned for linguistic words, which might not work well\nfor images due to their 2D nature. Furthermore, existing\npositional encoding algorithms are limited to static spatial\norder, which cannot accurately represent dynamic action\nsemantics. Specifically, the positional information of action\nsemantics changes based on their spatiotemporal variations\ncaused by the dynamic movements of action semantics in\nvideos. These spatiotemporal variations will be illustrated\nlater in Fig. 2. Hence, we propose a motion-aware 2D\npositional encoding algorithm, which is more effective than\nthe standard methods in modeling the positions of action\nsemantics considering their spatiotemporal variations. The\narXiv:2405.08204v1  [cs.CV]  13 May 2024\nJOURNAL OF LATEX CLASS FILES 2\nmotion-aware 2D positional encoding will be discussed in\nmore detail in Section 3.3.1.\nTemporal order information is essential in modeling\nactions since human action is a sequential process over\ndifferent times [9]. Human actions are, however, temporally\nheterogeneous, making modeling the temporal dependen-\ncies between action frames challenging. For example, in\nsome actions such as “running”, the temporal dependencies\nbetween similar and adjacent frames are essential. On the\nother hand, in many actions such as “jumping”, the critical\ntemporal dependencies are between distinctive and non-\nadjacent frames, so-called keyframes such as “start”, “mid-\ndle”, and “end” of the jump [10].\nTraditional sequential methods such as Recurrent Neural\nNetwork (RNN) encounter difficulty computing heteroge-\nneous temporal dependencies in actions because of limited\ntemporal receptive fields and bias toward adjacent action\nframes. A better way to model temporal dependencies in\nactions is using a transformer network, which can more\neffectively process non-adjacent frames due to a larger tem-\nporal receptive field. However, the standard transformer\nnetwork was initially designed for natural language pro-\ncessing in which the highest temporal dependencies are\nbetween the exact linguistic words, followed by words from\nsimilar categories [6]. Hence, while the transformer net-\nwork can include the temporal dependencies between non-\nadjacent frames, it is still heavily biased toward similar and\noften adjacent action frames. Consequently, the transformer\nnetwork struggles to capture the heterogeneous temporal\ndependencies between distinctive and non-adjacent frames.\nTherefore, to resolve this issue, we propose a sequence-\nbased temporal attention model to capture heterogeneous\ntemporal dependencies within the transformer network ef-\nfectively. More details about the above are discussed in\nSection 3.3.4.\nIn summary, the main contributions of this paper are as\nfollows:\n• A novel spatiotemporal transformer network is pro-\nposed that includes several original components to\ndetect actions in untrimmed videos by properly\nmodeling the action semantics, their interactions, and\nmovements. The new design solves the fundamental\nissues of the standard transformer network in ac-\ntion modeling and understanding. To do this, the\ntransformer network and the attention mechanism\nare renovated in both spatial and temporal domains.\n• We are the first to develop a multi-feature selective\nsemantic attention model to capture the important\nspatiotemporal interactions between action seman-\ntics based on their spatial and motion properties\ncorrelations. The multi-feature attention handles the\nlimitation of the standard self-attention, which is\nrestricted to a single feature space. The proposed\ntransformer also uses a selective attention model,\nwhich, by selecting informative inputs, is more ef-\nfective and efficient than the standard self-attention\nin modeling action semantics.\n• A novel motion-aware 2D positional encoding is\nintroduced that uses a 2D motion memory module to\nmodel dynamic spatiotemporal semantic variations\nin action frames. This overcomes the drawback of the\nstandard 1D positional encoding, which is limited\nto static spatial positions, making it ineffective in\ndealing with 2D images and movements in videos.\n• A sequence-based temporal attention model is sug-\ngested that, along with the sequence-based temporal\npositional encoding, can effectively capture heteroge-\nneous temporal dependencies in action frames that\nmight exist in distinctive and often non-adjacent\nframes. This model eliminates the limitations of tra-\nditional temporal attention, which is biased toward\ntemporal dependencies between similar and com-\nmonly adjacent frames in action videos.\n• The proposed method outperforms state-of-the-art\nmethods on four public spatiotemporal action bench-\nmarks, AVA (V2.2 and V2.1), UCF101-24, and EPIC-\nKitchens.\n2 R ELATED WORK\nThere are three relevant topics covered in this section, in-\ncluding methods based on (1) action semantics, (2) multiple\nfeatures for action modeling, (3) spatiotemporal action de-\ntection.\n2.1 Action Semantics\nTo represent action semantics, earlier approaches used\nhandcrafted features, such as shape descriptors extracted\nfrom human silhouettes [11] and local binary patterns\ncaptured from human parts [12]. With advances in deep\nlearning, action semantics can now be analyzed more ef-\nfectively. Several studies in the literature used deep net-\nworks for action semantics that focus either on persons\n[13] or objects [14]. In [13], a convolutional feature-based\naction tubelet detector is presented for modeling persons\nover different time periods. [14] suggested an object-centric\nfeature alignment mechanism to signify critical objects in\naction videos. However, various actions in the wild rely\non complex interactions between persons and objects. To\naccommodate this complexity, [15] introduced a transformer\nnetwork to compute the interactions between persons and\nobjects based on their spatial features. As of yet, no effective\nstrategy has been developed to capture the spatiotemporal\nrelationship between action semantics based on spatial and\nmotion features.\n2.2 Multi-feature Networks in Action Modeling\n[16] introduced one of the first deep networks that used\nboth RGB images and optical flow fields to extract spatial\nand motion features for action modeling and recognition.\n[17] redesigned its architecture, including softmax and pool-\ning layers, in order to improve a multi-feature network.\nDespite taking advantage of multiple features in [16], [17],\nthey did not suggest any explicit solution to employ mul-\ntiple features effectively in action recognition. Therefore,\n[18] proposed directly enhancing the individual and hybrid\nrepresentations of multiple features via a spatiotemporal\npyramid pooling and a fusion mechanism, respectively.\n[19] improved this multi-feature representation by using\nregion proposals rather than the whole frames suggested\nJOURNAL OF LATEX CLASS FILES 3\nin [16], [17], [18], to emphasize more informative parts of\nthe optical flow and RGB images. [20] further strengthened\nthe joint representation of multiple features by computing\nthe relationship between feature vectors with a self-attention\nmechanism. Still, calculating the relationship between action\nsemantics based on the correlations between their spatial\nand motion features remains unresolved.\n2.3 Spatiotemporal Action Detection\nIn one of the earliest spatiotemporal action detection ap-\nproaches, [1] proposed codebooks in which one codeword\nrepresented spatial and temporal information about each\naction class. A major weakness of [1] is its inability to\nsuccessfully detect actions in complex backgrounds due to\nusing handcrafted features. [3] addressed this shortcoming\nby developing a deep CNN that could more accurately de-\ntect spatiotemporal action instances in complex background\nenvironments. [21] enhanced such as a deep network in [3]\nby detecting multiple spatial action instances per frame with\na more efficient single-shot multi-box detector. Before now,\neach action proposal was based on a fixed model, accumu-\nlating the error over time. In [22], this issue was tackled by\nproposing a progressive learning framework that adapts to\nnew relevant action contexts as they arise. As an extension of\nthe previous work, [23] used the track-of-interest alignment\ntechnique to cope with large spatial variations of in-the-wild\naction videos. A more effective strategy to tackle both large\nspatial and temporal variations is the transformer network,\nwhich has increasingly been used for action detection and\nrecognition.\n3 M ETHODOLOGY\n3.1 Method Overview\nFig. 1 shows the overview of the proposed pipeline for\naction detection. Given a sequence of RGB frames, IRGB =\n{Irgb\nt ∈ RH×W×3, t= 0 , 1, ..., τ}, the goal is to find the\naction class scores, ˆY , and the start of the end of action,\nts and te, respectively. Here, τ is the length of the action\nsequence; and H and W indicate the size of the full image\n(height and width). The suggested pipeline includes two\nmain stages: preprocessing (Section 3.2), in which the in-\nput data are prepared, and the spatiotemporal transformer\nnetwork (Section 3.3), which models and detects the action\nsequence.\nIn the prepossessing stage, the spatial action semantics\nSG = {ZG, OG} are detected that includes the geometries\nof persons ZG = {zg\ni ∈ Rhi×wi×3, i= 0 , 1, ..., N} and\nobjects OG = {og\ni ∈ Rh′\ni×w′\ni×3, i= 0 , 1, ..., N′}, where\nN, N′, og, and zg are the numbers of persons, number of\nobjects, individual detected persons, and detected objects in\nan action frame, respectively. hi and wi indicate the sub-\nimage size of the detected persons; and h′\ni and w′\ni shows\nthe sub-image size of the detected objects. Note that the\nspatial action semantics represent the spatial information of\npersons and objects in RGB images. We used [7] to detect\nthe action semantics, including persons and objects. Apart\nfrom the spatial features, motion features also are impor-\ntant properties of action semantics. So, to incorporate the\nmotion features, the optical flow fields IFLOW = {Iflow\nt ∈\nRH×W×3, i= 0, 1, ..., τ}, are computed using [24], a highly\nefficient optical flow extraction algorithm. The optical flow\nfields result from converting 2D optical flow motion vectors\nto three-channel images to enable computing the multi-\nfeature attention between spatial and motion features (more\ndetails are in Section 3.3.2). Then, for each frame, the motion\nsemantics SM = {ZM , OM } ∈Iflow\nt , including the motions\nof persons ZM = {zm\ni ∈ Rhi×wi×3, i= 0, 1, ..., N} and ob-\njects OM = {om\ni ∈ Rh′\ni×w′\ni×3, i= 0, 1, ..., N′} are enhanced\nand segmented to make it invariant to camera movements\n(Section 3.2.1). The bounding boxes BG, obtained from\nthe semantic detection algorithm [7], are used for motion\nsegmentation. Here BG = {BZ, BO}, where BZ = {bz\ni ∈\nR4, i= 0 , 1, ..., N} and BO = {bo\ni ∈ R4, i= 0 , 1, ..., N′};\nand bp and bo are the corresponding bounding boxes for\neach motion semantic.\nThe proposed spatiotemporal transformer network in-\ncludes several modules to model the spatial and motion\nsemantics obtained from the preprocessing step and detect\nthe action sequence. The multi-feature selective semantic\nattention model (Section 3.3.2) captures the critical relation-\nship between action semantics based on the correlations\nbetween their spatial and motion features. To handle the\nspatiotemporal variations in action semantics, the motion\nmemory module (Section 3.3.1) utilizes the semantic motion\nvectors (SV ) and updates the semantic positional encoding\nof the transformer network using the 2D horizontal and\nvertical semantic motion memory offsets ∆px\ni ∈ ∆PX and\n∆py\ni ∈ ∆PY , i ∈ {0, 1, , , , , N+ N′}. Here, SV is computed\nfrom the optical flow motion vectors before conversion to\nimages (more details are in Section 3.2.1).\nThe motion memory module also outputs the 2D se-\nmantic motion memory features FX\nM and FY\nM . The output\nof the multi-feature selective attention is the multi-head\nsemantic attention, AH, which represents the most infor-\nmative selection of correlative patterns between spatial and\nmotion semantics extracted by the heads of the transformer\nnetworks. The multi-feature fusion module (Section 3.3.3)\ncombines the motion memory and the multi-feature seman-\ntic features and directs the dataflow in multiple layers of\nthe deep network. The output of the multi-feature fusion\nmodule is the final set of semantic features, fs\nt , that are\ncaptured in different frames to form the sequence of final\nsemantic , Xs = {fs\nt , t= 0 , 1, ..., τ}. Subsequently, Xs is\nprocessed in the sequence-based temporal attention model\n(Section 3.3.4) to extract the heterogeneous temporal depen-\ndencies between different frames, fs\nt . The sequence-based\ntemporal attention values, ˆA, proceed to the classification\nand regression stages to detect the action sequence. The\nimplementation details of the proposed pipeline are in Sec-\ntion 4.1. We will discuss the aforementioned components of\nthe proposed method thoroughly in the following sections.\nOur algorithm summary is shown in Table 1. The extended\nversion of the algorithm summary is in the Supplementary\nMaterial.\n3.2 Preprocessing\nIn the proposed pipeline, the preprocessing stage includes\nsemantic detection, optical flow extraction, motion enhance-\nment, and segmentation to prepare the inputs for the\nJOURNAL OF LATEX CLASS FILES 4\nFig. 1. The pipeline for the proposed method includes the preprocessing stage and the transformer network. Given the sequence of RGB images,\nfirst, the spatial semantics and optical flow fields are extracted in the preprocessing stage. The motion enhancement and segmentation algorithm\nextracts the motion semantics that are invariant to camera movement. The multi-feature selective attention model captures the correlative patterns\nbetween spatial and motion semantics. The motion memory module updates the semantic positional encoding (S-positional encoding) of the\ntransformer network and makes it semantically motion-aware. The multi-feature fusion combines the extracted features and directs them to the\ndeep network. The sequence-based temporal attention model captures the heterogeneous temporal dependencies between different times that are\nthen used to detect the action sequence in the classification and regression stage.\nTABLE 1\nPipeline algorithm summary in a hierarchical order indicating each\nphase (in bold), the summary of each phase, and the inputs and\noutputs of each phase.\nPhase/Summary Inputs Outputs\nSemantic Detection IRGB SG\nextracts action semantics\nOptical Flow Extraction IRGB SM, SV\nextracts optical flow\nMotion Enhancement BG, IV SM, SV\nimproves motion features\nFeature Embedding SG, SM XG, XM\nextract spatiotemporal features\nMotion Memory Module SV ∆PX, ∆PY\nprovides motion information\nMA 2D Positional Encoding ∆PX, ∆PY PX\nA , PY\nAfor motion-aware transformer\nMF Semantic Attention SG, SM AH\nfor correlations in multi-features\nMulti-Feature Fusion AH Xscombines features in layers\nSB Temporal Attention Xs ˆAcomputes temporal relations\nClassification and Regression ˆA ˆY , ts, teclassifies actions and frames\ntransformer network. We exploited [7], a well-established\nstate-of-the-art object/person detection algorithm, to extract\nspatial (RGB) action semantics, including persons and ob-\njects for each frame as SG = {ZG\nt , OG\nt }. In this stage, the\ncorresponding bounding boxes for persons and objects are\nBG\nt = {BZ\nt , BO\nt } ∈R(N+N′)×4.\nThe optical flow fields are estimated from RGB images\nutilizing [24], a highly efficient state-of-the-art optical flow\nestimation algorithm. Optical flow is a powerful and pop-\nular modality to represent motions in actions. However,\noptical flow is sensitive to camera movement, a common\nissue in videos captured in the wild. In other words, the\ncamera movements can remarkably distort the motion in-\nformation depicted in the optical flow fields. To address this\nproblem, a semantic motion enhancement and segmentation\nalgorithm is developed to use semantic motion vectors in\nthe transformer network effectively.\n3.2.1 Semantic Motion Enhancement and Segmentation\nGiven the semantic bounding boxes BG = {BZ, BO}\nobtained from the semantic detection algorithm, the dis-\ntorted optical flow fields Iflow\nt (x, y) (affected by cam-\nera movements) and the corresponding motion vectors,\nIV (u, v) ∈ RH×W×2, the goal of the semantic motion\nenhancement and segmentation algorithm is to extract the\nenhanced motion semantics including persons and objects\nSM = {ZM , OM }; and the corresponding semantic motion\nvectors, SV = {ZV , OV }, which are invariant to camera\nmovements. Here, u and v are scalar units that define the\nmotion displacement between the image pixels in the time\nt, as (x(t), y(t)), and the time t + ω, as (x(t+ω), y(t+ω)). The\nsemantic motion enhancement and segmentation algorithm\nis designed based on the dominant motions of persons\nin action frames. Consequently, we consider persons as\nthe foreground and the remaining portions of the frame\nas the background, affected mainly by camera movements\n[25]. The semantic motion enhancement and segmentation\nalgorithm consists of two steps: motion modeling and mo-\ntion restoration, which are explained in the Supplementary\nMaterial.\nJOURNAL OF LATEX CLASS FILES 5\n3.3 Semantic and Motion-Aware Spatiotemporal\nTransformer Network\nBefore delivering the action semantics to the transformer\nnetwork, they are embedded in the feature space. To do\nsuch, first, the outputs of the prepossessing step, includ-\ning each spatial action semantic, sg\ni ∈ SG, are resized as\nRhi×wi×3 → Rˆh× ˆw×2, and each motion semantic, sm\ni ∈ SM\nas Rh′\ni×w′\ni×3 → Rˆh× ˆw×2. Here, ˆh and ˆw represent the fixed\nimage size. So, the resized spatial and motion semantics\nare ˆSG and ˆSM , respectively. Next, the action semantics\nare converted to spatial and motion semantic features as\nXG = {XG\nZ , XG\nO } and XM = {XM\nZ , XM\nO }, respectively.\nThis feature embedding is performed using the convolu-\ntional layers, ConvG, and ConvM as:\nXG = ConvG( ˆSG, WG) : R\nˆN×ˆh× ˆw×3 → R\nˆN×df ,\nXM = ConvM ( ˆSM , WM ) : R\nˆN×ˆh× ˆw×3 → R\nˆN×df ,\n(1)\nwhere ˆN = N + N′ is the total number of action semantics,\ndf is the size of feature embedding, and WG and WM\nare the kernel weights. The embedded feature set includes\nXG\nZ and XG\nO , which are the spatial features of persons and\nobjects; and XM\nZ , and XM\nO , which are the motion features\nof persons and objects, respectively. The aforementioned\nfeature embedding is illustrated in Fig. 4.\n3.3.1 Motion-Aware 2D Positional Encoding.\nThe transformer network has several advantages over tra-\nditional temporal networks because of the concurrent pro-\ncessing of inputs [6] by using positional encoding that adds\nthe order information. As an integral part of the transformer\nnetwork, positional encoding was initially designed for nat-\nural language processing, which was mainly a time-related\nproblem. The standard 1D temporal positional encoding [6]\nis illustrated as follows.\nP(pt, 2n) = sin( pt\nψ\n2n\ndm\n),\nP(pt, 2n + 1) = cos( pt\nψ\n2n\ndm\n),\n(2)\nwhere pt, n, and dm are the temporal order of the input,\nthe dimension index of the positional embedding, and the\nsemantic model size, respectively. ψ is a large integer as\nsuggested by the original work [6] to accommodate high-\ndimensional embedding features.\nThe positional encoding was later applied to spatial\nproblems, such as object detection [7], and spatiotemporal\nproblems, such as action detection [26], without effective\nadaptation to these areas. Two main issues are (1) the 1D\nnature of the standard positional encoding, which leaves it\nless effective in handling 2D images; and (2) its incapabil-\nity to handle spatiotemporal variations in spatiotemporal\ninputs such as action sequences. The proposed positional\nencoding has two characteristics that address the issues\nabove. Firstly, the proposed positional encoding is “2D”,\nmaking it more adept in dealing with 2D images. Secondly,\nand more importantly, the suggested positional encoding is\nmotion-aware, which makes it more effective in handling\nspatiotemporal variations in action sequences.\nCurrently, the most common method for encoding spa-\ntial positions in images is to segment them into patches\n[8], [27], [28]. In this regard, the current methods treat 2D\nimages as 1D inputs and assign the positional labels to each\npatch following the standard temporal positional encoding\nindicated in (2). Nevertheless, the current methods em-\nploy fixed patches that cannot accommodate spatiotemporal\nvariations in video frames. Fig. 2 shows examples of\nsuch spatiotemporal variations and compares the proposed\nmotion-aware positional encoding to the standard patch-\nbased approach in dealing with this issue. In this example,\nthe image is divided into 18 labeled between p1 to p18. Here,\nthe red and green basketball players change their positions\nwhen moving from time t (in Fig. 2 (a)) to t + ω (in Fig.\n2 (b)). So, now, as a defender, the green player switches\nhis position with the red player, on offense at position\np11. Consequently, when spatiotemporal changes occur in\naction semantics, the transformer network cannot model the\nsemantic movements at different action frames when using\nthe standard patch-based positional encoding. By contrast,\nthe proposed motion-aware 2D positional encoding mem-\norizes the position changes of each set of action semantics\nat different times and adaptively updates their positional\ninformation within the transformer network as shown in\nFig. 2 (c). We later also numerically compare the proposed\nmotion-aware 2D positional encoding to the standard one in\nSection 4.4.1 and Table 7. The motion-aware 2D positional\nencoding is formulated as follows:\nPX\nA (px\ni , 2n) = sin(px\ni + ∆px\ni\nψ\n2n\ndm\n),\nPX\nA (px\ni , 2n + 1) = cos(px\ni + ∆px\ni\nψ\n2n\ndm\n),\n(3)\nPY\nA (py\ni , 2n) = sin(py\ni + ∆py\ni\nψ\n2n\ndm\n),\nPY\nA (py\ni , 2n + 1) = cos(py\ni + ∆py\ni\nψ\n2n\ndm\n),\n(4)\nwhere px\ni and py\ni are initial horizontal and vertical positions\nfor each action semantics si ∈ {SG, SM }, respectively\nthat are obtained by the standard patch-base approach [8].\n∆px\ni ∈ ∆PX, and ∆py\ni ∈ ∆PY , i = {0, 1, ...,ˆN} are\nhorizontal and vertical semantic motion memory offsets for\neach action semantics. ∆PX and ∆PY include the motion\noffsets for all the action semantics. ω is the duration of\nthe motion. In this work, we define the index of positional\nembedding as n ∈ {0, 1, ..., df /2}.\n∆px\ni and ∆py\ni adaptively update the positions of ac-\ntion semantics according to their motion during the action\nsequence. A set of concatenated embedded 2D positional\nencoding PA = {PX\nA , PY\nA } is delivered to the transformer\nnetwork. To do such, the embedded positional encoding is\nadded to the inputs as:\nˆXG = PA + XG, ˆXM = PA + XM , (5)\nwhere ˆXG = { ˆXG\nZ , ˆXG\nO } and ˆXM = { ˆXM\nZ , ˆXM\nO } are\npositional encoded spatial and motion semantic features (for\npersons and objects), respectively. Two new components of\nJOURNAL OF LATEX CLASS FILES 6\nthe motion-aware positional encoding, ∆px\ni , and ∆py\ni are\ncomputed using the motion memory module. Details of this\ncomputation are found below.\nMotion Memory Module. Using a memory module has\neffectively modeled the varying temporal information [29].\nThe input to the motion memory module is the sequence of\nsemantic motion vectors as SV\nseq = {SV\nt , t= 0, 1, ..., τ}. The\noutputs are (1) the semantic motion memory offsets, ∆PX\nand ∆PY , that are used for our motion-aware 2D positional\nencoding; and (2) 2D semantic motion memory features,\nFM = {FX\nM , FY\nM } utilized in the multi-feature fusion stage\nto enrich the feature representation. The motion memory\nmodule memorizes the changes in semantic motion vectors\nusing the motion memory network as follows:\nupdate: zt = σ(Wzxt + Uzht−ˆω),\nreset: rt = σ(Wrxt + Urht−ˆω),\ncurrent: nt = tanh\n\u0010\nWhxt + rt ⊙ Uhht−ˆω\n\u0011\n,\noutput: ht = zt ⊙ ht−ˆω + (1 − zt) ⊙ nt,\n(6)\nwhere xt is the input, ⊙ represents the element-wise mul-\ntiplication, and σ is a sigmoid function. zt, rt, nt, and ht\nare the update gate, reset gate, current gate, and output,\nrespectively. Wz, Wr, and Wh ∈ R ˆNˆh ˆw×dg ; and Uz, Ur,\nand Uh ∈ Rdg×dg are parameter matrices, where dg is\nthe state size. The above formulation is inspired by the\nlearning steps of the gated recurrent unit [30]. Moreover,\nˆω is a dilated temporal value representing the time when\nsignificant motion exists in the action sequence based on a\nthreshold value, Th. Given the inputs that are horizontal\nand vertical components of the semantic motion vectors,\nSV\nX, and SV\nY , the memorized horizontal and vertical outputs\nof the networks are hX and hY , respectively. The motion\nmemory networks memorize the movements of action se-\nmantics during action sequences. As a result, the motion\nmemory networks update the positional encoding, making\nit motion-aware. The final outputs of the motion memory\nnetworks are computed by using an averaging pooling layer\n(AvPool) as:\n∆PX = AvP ool(hX) : R\nˆN×dg → R\nˆN ,\n∆PY = AvP ool(hY ) : R\nˆN×dg → R\nˆN ,\n(7)\nThe motion memory module also outputs the semantic\nmotion memory features, FM = {FX\nM , FY\nM }, as:\nFX\nM = ConvX(hX\nZ , WX), F X\nM = ConvY (hX\nZ , WY ). (8)\nNote that ConvX and ConvY : RN×dg → RN×df and\nWX and WY are the kernel weights. Only the part of\nthe network outputs corresponding to persons as hX\nZ , and\nhX\nZ ∈ RN×dg are selected in the feature selection. This\nis because persons’ movements are more critical in action\nsequences than objects’ movements.FM is used in the multi-\nfeature fusion module to improve the feature representation\nof actions.\n3.3.2 Multi-feature Selective Semantic Attention\nBased on their spatial and motion features, many actions\nare characterized by spatiotemporal interactions between\ntheir semantics, including persons and objects. Transformer\nnetworks are designed to capture the interactions between\ndifferent inputs using an attention mechanism [6]. There-\nfore, we propose a multi-feature selective attention model\nto extract such interactions between action semantics. There\nare three differences between the proposed multi-feature\nattention and the standard “cross-attention” mechanism.\nFirst, the proposed attention is selective, meaning only\ninformative queries are considered. Second, as opposed to\nthe existing methods that include the whole frame/input,\nthe suggested strategy focuses only on action semantics.\nFinally, four multi-feature attention types are introduced to\nenrich the action feature representation.\nIn our view, four types of multi-feature semantic inter-\nactions occur in human actions, which are illustrated in\nFig. 3. The first scenario happens when all the interacting\naction semantics are stationary, such as the “person” and\n“cake/candle” in the action ”blowing a candle” in Fig. 3\n(a). As a result, the spatial-to-spatial attention, AGG, rep-\nresents such a stationary-only interaction between action\nsemantics. In the second case, the entire action semantics\nare in motion, such as the ”person” and the ”jet ski” in\nthe action ”jet skiing” illustrated in Fig. 3 (b). Consequently,\nthe motion-to-motion attention, AMM , serves such motion-\nonly interactions between action semantics. As for the third\nscenario, the spatial-to-motion attention, AGM , provides\nthe interaction between the moving semantics, the “person\npitching the ball”, and the stationary one, the “person\nwaiting for the ball” in Fig. 3 (c). Finally, the motion-to-\nspatial attention, AMG , represents the interactions between\nmoving and stationary semantics, in this example, basket-\nball “player” and “net”, respectively (shown in Fig. 3 (d)).\nAmong the above multi-feature attentions types, AGG and\nAMM are intra-feature attention, while AGM and AMG are\ninter-feature attention. In the aforementioned examples, the\naction semantics are highlighted by bounding boxes. The\ngreen motion vectors, which are obtained from optical flow\nfields, visualize the motion semantics.\nThe architecture of the multi-feature selective seman-\ntic attention model is illustrated in Fig. 4. The attention\nmechanism is to find the correlations between the query\n(Q) and keys (K) and then map them to values (V ). The\nfour suggested attention types to represent the correlations\nbetween spatial and motion semantic features are shown in\nFig. 4 and are formulated as:\nAGM = Softmax (QG(KM )T\n√dh\n)V M ,\nAMG = Softmax (QM (KG)T\n√dh\n)V G,\nAGG = Softmax (QM (KG)T\n√dh\n)V G,\nAMM = Softmax (QM (KM )T\n√dh\n)V M ,\n(9)\nThe multi-feature semantic queries, keys, and values are\nillustrated as follows:\nJOURNAL OF LATEX CLASS FILES 7\n(a) (b) (c)\nFig. 2. The motion-aware positional encoding, (c), compared to the standard one, (a) and (b), in dealing with spatiotemporal action semantic\nvariations: two basketball players, red and green, change their positions from timet, (a) to t + τ, (b). So the red player (p11 as the offensive player)\nswitched his position to the green player (now p11 the defender). The proposed motion-aware positional encoding, (c), can memorize the position\nchanges of two basketball players using the motion memory offsets, ∆pgreen and ∆pred. The green and red arrows show the motion vectors\nobtained from the optical flow fields.\n(a) (b) (c) (d)\nFig. 3. Some examples of our multi-feature attention types that capture the spatiotemporal semantic interactions in action samples. (a): spatial-\nto-spatial attention between the “sitting person” and the “stationary cake” ; (b): motion-to-motion attention between the “moving person” and the\n“moving jet ski” ; (c): spatial-to-motion attention between the “stationary waiting player” and the “moving pitching player”; (d): motion-to-spatial\nattention between the “moving jumping player” and the “stationary net“. The green arrows show the motion vectors obtained from the optical flow\nfields. The action samples are collected from the UCF101 dataset [31].\nFig. 4. The proposed transformer network includes several modules to capture the multi-feature semantic features. The feature embedding converts\nthe motion and spatial semantics to features. The motion memory module memorizes the semantic position changes and includes them as the\nmotion-aware positional encoding in the semantic multi-feature extraction. The multi-feature selective attention represents the correlations between\npersons with other persons and the most relevant objects. In this action example, “teacher using an instructional tool”, these correlations represent\nthe interactions between the “teacher” and the ‘students”, and the“teacher” and relevant objects such as the “handheld whiteboard.”\nJOURNAL OF LATEX CLASS FILES 8\nQG = ˆXG\nZ WG\nQ , KG = ˆXGWG\nK , VG = ˆXGWG\nV ,\nQM = ˆXM\nZ WM\nQ , KM = ˆXM WM\nK , VM = ˆXM WM\nV ,\n(10)\nwhere WG\nQ , WM\nQ ∈ RN×df ×dh , and WG\nK , WM\nK , WG\nV , WM\nV ∈\nR ˆN×df ×dh are projecting weights. dh = d f/Nh is the size of\nattention head, Nh is number of attention heads, and T is a\ntranspose operation.\nThe compound semantic attention head, AC, is defined\nby combining the four multi-feature attention types and\nthen applying a convolutional operation as:\nAC′\n= ConcatC(AGM , AMG , AGG, AMM ),\nAC = ConvC(AC′\n, WC),\n(11)\nwhere ConvC : R4×N×dh → RN×dh , ConcatC is a con-\ncatenation operator that stacks the inputs as ConcatC :\n4 ×RN×dh → R4×N×dh , WC is the kernel weights, and AC′\nis the concatenated head. To extract features more effectively\nthrough different heads of the transformer network, the\nmulti-head semantic attention, AH ∈ RN×df is computed\nby concatenating each of the Nh compound semantic atten-\ntion heads as:\nAH = ConcatH(AC\ni , i= {0, 1, ..., Nh}), (12)\nwhere ConcatH : Nh × RN×dh → RN×Nh×dh = RN×df .\nSelective Attention. As shown in Fig. 4 and (10), in\nthe proposed attention model, the queries and keys/values\nare defined differently. The queries are only persons, ˆXZ,\nwhile the keys/values include both persons and objects,\nˆX = { ˆXZ, ˆXO}. However, in the standard self-attention\nmodel, queries and keys/values are derived from the same\nsource. We argue that the proposed selective attention\nmodel is more effective and efficient than the standard self-\nattention in modeling action semantics. In particular, the\nselective attention model computes the interactive corre-\nlations between persons-to-persons and persons-to-objects,\nwhich are essential to various actions. Therefore, we exclude\nthe computation of correlations between objects-to-objects,\nwhich are often irrelevant to actions, thereby reducing the\nexpressive power of action features. Specifically, among a\nlarge number of background objects in action videos, only\na few are relevant to the action. An example is shown in\nFig. 4, where the most relevant object is the “Handheld\nwhiteboard” in the action “teacher using an instructional\ntool”. By contrast, the “teacher” is correlated with all the\nother persons, ”students”, in this activity. We will later\nnumerically compare the performance of the selective at-\ntention model to the standard one in Section 4.4.3 and\nTable 9. Moreover, the selective semantic attention is now\nrepresented in RN×dh , and comparing that expression to\nthe standard self-attention space of R(N+N′)×dh , leads to a\nreduced set of computations. Specifically, the computational\ncost of our selective semantic attention isO(N·(N+N′)·dh),\nwhich is more efficient than a regular semantic self-attention\nwith a computational cost that is O((N + N′)2 · dh). The\nmathematical proof is provided in the supplementary mate-\nrial.\n3.3.3 Multi-feature Fusion\nThe proposed transformer network consists of multiple\nlayers with a defined relationship between the successive\nlayers l − 1 and l, as illustrated as follows:\nˆBl\nO = MF SSA(Bl−1\nO ) + Bl−1\nO , l ∈ {2, ..., L},\nBl\nO = MLP (Norm( ˆBl)) + ˆBl, l ∈ {2, ..., L},\n(13)\nwhere MF SSAis the multi-feature selective semantic at-\ntention, BO is the layer output, ˆBO is the intermediate\nlayer output, L is the number of layers, and Norm is a\nnormalization layer.\nFor the first layer, the relation between the inputs and\noutput is shown as follows:\nˆB1\nO = MF SSA( ˆXG, ˆXM ) + FM.A, (14)\nwhere FM.A ∈ RN×dh is our motion-aware features defined\nas:\nFM.A = ConvA(ConcatA(XG\nZ , XM\nZ , FX\nM , FY\nM ), WA). (15)\nwhere ConvA : R4×N×df → RN×df , ConcatA : 4 ×\nRN×df → R4×N×df , and WA are the kernel weights.\nHere, MF SSA( ˆXG, ˆXM ) = AH. To properly update the\nprojecting weights for the keys and values in the layers\nl ≥ 2 during the training, the selected inputs for the multi-\nfeature keys and values are converted to their original size\nof the first layer as R ˆN×df . The final output of the multi-\nfeature fusion for frame t is the final semantic features, fs\nt is\nillustrated as:\nfs\nt = ConvF (BL\nO, WO) : RN×df → Rdf , (16)\nwhere WO are the kernel weights. The sequence of fs\nt is\nused as the multi-feature representation of semantics for\neach action frame in the sequence-based temporal attention\nmodel.\n3.3.4 Sequence-based Temporal Attention\nGiven the sequence of final semantic features for different\nframes, Xs = {fs\nt , t= 0, 1, ..., τ}, the goal of the sequence-\nbased temporal attention model, ˆA ∈ Rτ×df is to compute\nthe temporal dependencies between semantic features in\ndifferent frames t ∈ τ, effectively. Temporal attention is\ncapable of modeling long sequences without discriminating\nagainst older data, which makes it suitable for processing\nlong, untrimmed action sequences. Nevertheless, the stan-\ndard temporal attention was designed for natural language\nprocessing [6]. Accordingly, the standard temporal attention\ntends to assign the highest attention to words themselves\nand followed by words from similar categories. [32]. How-\never, such a homogeneous attention model is unsuitable for\nmodeling heterogeneous temporal dependencies between\naction frames. There are many actions in which the tempo-\nral dependencies between distinctive frames, the so-called\nkeyframes, provide essential information. For example, in\nthe action sequence “triple jump” the temporal dependen-\ncies between the three distinctive steps “hop”, “step”, and\n“jump” define the meaning of the action [10]. However,\nstandard homogeneous temporal attention mainly focuses\nJOURNAL OF LATEX CLASS FILES 9\non the temporal dependencies between similar and often ad-\njacent frames, discarding essential temporal dependencies\nbetween distinctive frames. To illustrate this phenomenon,\nwe will show some examples in Section 4.4.4, and in Fig. 5.\nOn the other hand, the proposed sequence-based tem-\nporal attention model can compute the heterogeneous tem-\nporal dependencies between action frames without dis-\ncriminating based on their similarity. It is because the\nsequence-based temporal attention model focuses on the\ntemporal dependencies that are relevant for actions, rather\nthan individual frames themselves. Some conceptual poof\nis provided in the supplementary material. We will later\nnumerically compare the sequence-based temporal attention\nto the standard one in Section 4.4.4 and Table 10.\nSome researchers suggested using a probabilistic ap-\nproach to enhance the standard attention mechanism. [33]\nproposed exploiting a mixture model whose parameters are\nupdated following the observed queries. [34] presented a\nmixture of Gaussian keys as an effective replacement for\nredundant heads in a transformer network. These meth-\nods, however, are based on expectation maximization (EM),\nwhich has several issues, especially when used in a deep\nlearning framework. This includes high computational cost,\nsensitivity to initialization, model complexity, and slow\nconvergence [35]. On the other hand, our sequence-based\ntemporal attention is simple yet effective and does not\nrequire any complex and iterative optimization algorithms\nsuch as EM.\nThe standard temporal attention [6] is shown as:\nA(QΓ, KΓ, VΓ) = Softmax (QΓKT\nΓp\ndf\n)VΓ, (17)\nwhere the temporal queries, keys, and values are QΓ,\nKΓ, and VΓ ∈ Rτ×df ; and τ is the number of frames.\nAcorr(QΓ, KΓ) = QΓKT\nΓ ∈ Rτ×τ is the temporal atten-\ntion correlation matrix that indicates the frame-by-frame\ntemporal dependencies. In the standard approach, QΓ =\n(Xs + P)WQ, KΓ = (Xs + P)WK, and VΓ = (Xs + P)WV ,\nwhere WQ, WK, and WQ ∈ Rτ×df are the temporal\nprojecting weights. P is the temporal positional encoding\nfollowing (2) that is added to the input sequence, Xs to\ninclude the temporal order information. So, let’s start with\nthe definition of the standard temporal attention correlation\nmatrix, Acorr(QΓ, VΓ), that only focuses on the relationship\nbetween individual inputs (queries and keys):\nAcorr(QΓ, KΓ) = (Xs + P)WQ(Xs + P)T WT\nK, (18)\nThe above can be rewritten as:\nAcorr(QΓ, KΓ) =\n(a)\nz }| {\nXsWQXT WT\nK +\n(b)\nz }| {\nP WQPT WT\nK +\nXsWQPT WT\nK| {z }\n(c)\n+ P WQXT\ns WT\nK| {z }\n(d)\n,\n(19)\nWe argue that in the above, the expressions (c) and (d)\ndo not contain useful information. This is because, during\nthe learning process, they do not lead to meaningful gra-\ndient updates since they are the results of multiplying the\ninput sequence and the position information. So, a more ef-\nficient temporal attention correlation matrix ˜Acorr(QΓ, KΓ)\ncan be shown as follows:\n˜Acorr(QΓ, KΓ) = XsWQXT WT\nK + P WQPT WT\nK, (20)\nThe above can be rewritten as:\n˜Acorr(QΓ, KΓ) = QΓKT\nΓ + PQPT\nK. (21)\nConsequently, the following defines the efficient tempo-\nral attention:\n˜A = Softmax (\n˜Acorr\np\ndf\n)VΓ (22)\nUntil now, the temporal dependencies between QΓ and\nKΓ are calculated between individual frames without in-\ncluding their relation to the action sequence. To resolve\nthis issue, the sequence-based temporal attention correlation\nmatrix, ˆAcorr, revises (21) to provide the temporal depen-\ndencies between the inputs, QΓ and KΓ with respect to the\ndistribution of the action sequence, Xs, as shown as:\nˆAcorr(QΓ, KΓ; Xs) = Iτ −\n(QΓ − KΓ)S−1\nX (QΓ − KΓ)T /NX−\n(PQ − PK)S−1\nX (PQ − PK)T /NP ,\n(23)\nwhere SX ∈ Rdf ×df is the covariance matrix of the dis-\ntribution Xs, and Iτ ∈ Rτ×τ is matrix of ones. NX and\nNP are normalizing terms. The above also presents a more\neffective sequence-based temporal positional encoding than\nthe original PQ and PK. (23) is based on the intuition of the\nMahalanobis distance explained in Supplementary Material.\nFinally, the sequence-based temporal attention is computed\nas follows:\nˆA = Softmax (\nˆAcorr\np\ndf\n)VΓ, (24)\n3.3.5 Classification and Regression\nThe final stage of the proposed pipeline is classification\nand regression. The multi-label classification layer includes\ntwo frame-level and sequence-level predictions shown as\nfollows:\nˆY S = Softmax (Convs( ˆAP , Ws)),\nˆY F = Softmax (Convf ( ˆAP , Wf )),\n(25)\nwhere ˆY S, and ˆY F are the sequence and frame predic-\ntion scores, respectively. Convs : Rτ×d f→ RCl , Convf :\nRτ×d f→ Rτ×Cl , and Ws and Wf are the kernel weights.\nNote that Cl is the number of classes. The set of class scores\ncan be defined as ˆY = {ˆY S, ˆY F }. Finally, the regression\nlayer calculates the start and end of the action sequence, ts,\nand te, for a given class c so that the selected time interval\nsatisfies: (ts, te; c) = argmax\nt\n( 1\nτ\nP\nt( ˆY F\nts,te ) + ˆY S\nts,te ).\nThe loss function of the proposed network is shown as\nfollows:\nJOURNAL OF LATEX CLASS FILES 10\nL = −\nτX\nt=1\nClX\nc=1\ny(c)\nt logˆy(c)\nt − α\nClX\nc=1\nY (c)log ˆY (c), (26)\nwhere y and ˆy are the ground truth and predicted values\nfor each frame t of class c at time t. Y and ˆY are the\nground truth and predicted values for each action sequence,\nrespectively. Note that α is a loss adjustment parameter.\n4 E XPERIMENTAL RESULTS\n4.1 Implementation Details\nThe implementation details of the proposed pipeline are\nsummarized in Table 2. In this table, all the hyperparam-\neters mentioned in the paper are described. The Model\nZoo with the DETR architecture [7] and R101 backbone\nis used for object/person detection to balance efficiency\nand performance. The object/person detection threshold is\n0.5. The optical flow is extracted by the FastFlowNet [24]\nfine-tuned on FlyingThings3D [36] with using 320 × 448\npatches during data augmentation. The learnable semantic\nand temporal projecting weights WG\nK , WM\nK , WG\nV , WM\nV , WQ,\nWK, and WV are initialized with random values. For the\nEPIC-Kitchens [37] dataset, only the classification layer is\nused. The network weights are initialized from the Kinetics\npre-trained model. The focal loss [38] was used during the\ntraining.\nSV are required to update our motion-aware positional\nencoding, which is also 2D. On the other hand, SM helps\ncalculate multi-feature attention between motion and spatial\nfeatures that are also derived from images. Moreover, the\nmotion feature embedding in our pipeline is computed by a\nconvolutional layer requiring images as the inputs.\nAll the experiments are conducted using PyTorch 1.7\non a server PC with dual Nvidia RTX 3090 GPUs (24GB\nVRAM), AMD Ryzen Threadripper 3990X 64-Core Proces-\nsor, and 256GB of RAM.\nTABLE 2\nImplementation details of the proposed pipeline with corresponding\nsections.\nParameter Value Section\nSemantic detection confidence threshold 0.5 3.2.1\nNumber of GMM distributions (K) 16 3.2.1\nNumber of action semantic ( ˆN) 10 3.3\nSemantic image size (ˆh × ˆw) 128 × 128 3.3\nSize of feature embedding (df ) 2048 3.3\nPositional encoding integer (ψ) 1e4 3.3.1\nDuration of motion (τ) 32 3.3.1\nSize of motion network state ( dg) 1024 3.3.1\nMotion memory module threshold (Th) 2.35 3.3.1\nNumber of attention heads (Nh) 4 3.3.2\nSize of attention head (dh) 512 3.3.2\nNumber of multi-feature layers (L) 6 3.3.3\nInput normalizing term (NX) 1 e3 3.3.4\nPosition normalizing term (NP ) 2 e3 3.3.4\nLoss adjustment parameter (α) 2.4 3.3.5\nOptimizer SGD 3.3.5\nNumber of training epochs 100 3.3.5\nNumber of videos per batch 16 3.3.5\nLearning rate 1e−4 3.3.5\nLearning rate decay (every 30 epochs) 0.1 3.3.5\nWeight decay 1e−6 3.3.5\nMulti-label classification threshold 0.5 3.3.5\n4.2 Datasets\nThe proposed pipeline is evaluated using four public\nspatiotemporal action datasets, AVA (version 2.1 and 2.2)\n[39], UCF101-24 [31], and EPIC-Kitchens [37]. Short descrip-\ntions of these datasets are given in the following.\nA V Adataset comprises 15-minute video segments of 80\naction classes. The AVA dataset is suitable for evaluating\nour proposed pipeline as it covers a notable variety of\ninteractions between action semantics, including person-to-\nperson and person-to-object interactions. Both AVA versions\n(v2.1 and v2.2) include 235 videos for training and 64 videos\nfor validation. The annotations for bounding boxes and class\nlabels are provided for every second of the video segments.\nAVA 2.2 revises the label annotation by adding more frame-\nlevel labels. Following the instruction of the benchmark,\n[39] and the previous studies [15], [40], [41], we used 60\naction classes with the measurement metrics ofmean Average\nPrecision (mAP)with an IoU threshold of 0.5.\nUCF101-24 is a subset of the UCF101 dataset [31] that\nincludes 3’207 untrimmed videos from 24 action classes. As\na part of THUMOS Challenge 2015, this dataset includes\nthe annotation for bounding boxes and frames. It covers\na variety of spatial and temporal action instances in each\nvideo, which is well-suited for our experiments. An mAP\nwith an IoU threshold of 0.5 is used for evaluation on the\nfirst split of the data following the previous work [15], [40].\nEPIC-Kitchens comprises 55 hours of daily activities,\nprimarily focusing on the interaction between persons and\nobjects. This dataset features the most diverse objects among\nspatiotemporal action benchmarks, making it suitable to\ntest the effectiveness of the proposed pipeline in modeling\naction semantics. Following a standard setting [40], [42], we\nused 22’675 videos for training and 5’886 videos for testing.\nAccording to the literature [43], [44], [45], the top-1 accuracy\nmetric is adopted to evaluate the suggested method on\nthe EPIC-Kitchens dataset on “action”, “noun”, and “verb”\nclasses. A top-1 accuracy is obtained by comparing the\nhighest predicted results with the ground truth.\n4.3 Comparative Results\nThe comparative results of four public benchmarks are as\nfollows.\n4.3.1 Results on AVA 2.2\nTable 3 illustrates the comparison between our method,\nSMAST, and the state-of-the-art approaches on the AVA\n2.2 dataset. In [46], a video-masked autoencoder using\npre-trained models achieved the highest transfer learning\nperformance when using Kinetic-700. Overall, our pro-\nposed pipeline (SMAST) outperforms the state-of-the-art\napproaches on the AVA 2.2 dataset. Specifically, with the\nmAP@0.5 of 40.2%, the suggested method surpasses the best\ncurrent benchmark [46] by 0.9.\n4.3.2 Results on AVA 2.1\nTable 4 compares The proposed method, SMAST, to the\nstate-of-the-art strategies on the AVA 2.1 dataset. In con-\nclusion, the proposed SMAST surpassed the state-of-the-art\nmethods previously used on the AVA 2.1 dataset with the\nmAP@0.5 of 33.1%, exceeding the highest existing bench-\nmark [52] by 1.1%.\nJOURNAL OF LATEX CLASS FILES 11\nTABLE 3\nComparison of our proposed method (SMAST) with the state-of-the-art\nstrategies on the AVA 2.2 dataset.\nTeam Method Pub/Year mAP@0.5 (%)\nTang et al. [40] AIA ECCV 2020 34.4\nFeichten al. [47] MoCo CVPR 2021 20.3\nFan et al. [48] MVT ICCV 2021 27.3\nFeichten al. [49] X3D CVPR 2022 27.4\nChen et al. [50] WOO ICCV 2021 28.3\nWu et al. [51] TLVU CVPR 2021 31.0\nPan et al. [15] ACARNet CVPR 2021 31.7\nLiu et al. [45] ORVT CVPR 2022 26.6\nZhao et al. [52] Tuber CVPR 2022 33.6\nWu et al. [44] MeMViT CVPR 2022 35.4\nWei et al. [53] MaskFeat CVPR 2022 38.8\nTong et al. [46] VideoMAE NIPS 2022 39.3\nFaure et al. [54] HIT WCACV 2023 32.6\nKorban et al. SMAST - - - - - 40.2\nTABLE 4\nComparison of our suggested method (SMAST) with the\nstate-of-the-art approaches on the AVA 2.1 dataset.\nTeam Method Pub/Year mAP@0.5 (%)\nSun et al. [55] ACRN ECCV 2018 17.4\nGirdhar et al. [4] VT CVPR 2019 27.6\nWu et al. [41] LTFB CVPR 2019 27.7\nStroud et al. [56] D3D CVPR 2020 23.0\nTang et al. [40] AIA ECCV 2020 31.2\nWu et al. [51] TLVU CVPR 2021 27.8\nChen et al. [50] WOO ICCV 2021 28.0\nPan et al. [15] ACARNet CVPR 2021 30.0\nShah et al. [57] PGA CVPR 2022 28.4\nZhao et al. [52] TubeR CVPR 2022 32.0\nKorban et al. SMAST - - - - - 33.1\n4.3.3 Results on UCF101-24\nTable 5 illustrates the comparative results on the UCF101-\n24 benchmark. SMAST, outperformed the other strategies\non the UCF101-24 benchmark with the mAP@0.5 of 85.5%,\nexceeding the highest performance [54] by 0.7.\nTABLE 5\nComparison of our pipeline (SMAST) with the state-of-the-art methods\non the UCF101-24 dataset.\nTeam Method Pub/Year mAP@0.5 (%)\nSong et al. [58] Tacnet CVPR 2019 72.1\nPramono et al. [59] HSAN CVPR 2019 73.7\nYang et al. [22] STEP CVPR 2019 75.0\nZhang et al. [60] ASM CVPR 2019 77.9\nTang et al. [40] AIA ECCV 2020 78.8\nLi et al. [61] MOC ECCV 2020 78.0\nSu et al. [19] PCSC PAMI 2020 79.2\nLiu et al [62] ACDnet PRL 2021 70.9\nPan et al. [15] ACARNet CVPR 2021 84.3\nKumar et al. [63] ESSL CVPR 2022 69.9\nLi et al. [64] DSRM SIVP 2022 81.2\nZhao et al. [52] TubeR CVPR 2022 81.3\nFaure et al. [54] HIT WCACV 2023 84.8\nKorban et al. SMAST - - - - - 85.5\n4.3.4 Results on EPIC-Kitchens\n6 compares the proposed pipeline, SMAST, and state-of-\nthe-art methods on the EPIC-Kitchens benchmark. Our pro-\nposed method outperformed the existing approaches with\nthe top-1 accuracy for “action”, ”verb”, and “noun” of\n50.9%, 70.1%, and 64.8%, respectively.\nTABLE 6\nComparative results on the EPIC-Kitchens dataset showing top-1\naccuracy (%) for different classes of action, noun, and verb.\nTeam Method Pub/Year action verb noun\nTang et al. [40] AIA ECCV 2020 27.7 60.0 37.2\nNagrani et al. [65] MBT NIPS 2021 43.4 64.8 58.0\nArnab et al. [66] Vivit ICCV 2021 44.0 66.4 56.8\nPatrick et al. [67] Mformer NIPS 2021 44.5 67.0 58.5\nKondrat et al. [68] Movinets CVPR 2021 47.7 72.2 57.3\nLiu et al. [45] ORVT CVPR 2022 45.7 68.4 58.7\nWu et al. [44] MeMViT CVPR 2022 48.4 71.4 60.3\nGirdhar et al. [43] Omnivore CVPR 2022 49.9 69.5 61.7\nYan et al. [69] MVT CVPR 2022 50.5 69.9 63.9\nKorban et al. SMAST - - - - - 50.9 70.1 64.8\n4.4 Ablation Study\nIn this section, we present an ablation study that evaluates\nthe impact of each module in the proposed action detection\nsolution.\n4.4.1 Motion-Aware 2D Positional Encoding and Motion\nEnhancement\nTable 7 indicates the impact of the proposed motion-aware\n(MA) 2D positional encoding and the proposed motion\nenhancement algorithm on the overall action detection per-\nformance. Our findings show that the MA 2D positional\nencoding is ineffective without the motion enhancement\nalgorithm. Specifically, it only boosted the overall perfor-\nmance by 0.1 % (from 30.9 % to 31.0%). The reason is that,\nwithout motion enhancement, the motion awareness of 2D\npositional encoding is impacted by the camera movement,\na common issue in action videos captured in the wild.\nNotably, the camera movement causes incorrect extraction\nof motion vectors, resulting in imprecise calculations of the\nmotion memory offsets, ∆PX and ∆PY , in the 2D posi-\ntional encoding. In contrast, when the motion enhancement\nalgorithm is used, the MA 2D positional encoding increases\nthe overall performance by 2.2% (from 30.9% to 33.1%). The\n1D positional encoding is computed following [8].\nTABLE 7\nThe impact of the motion-aware (MA) 2D positional encoding and\nmotion enhancement algorithm on the overall action detection\nperformance. The evaluation is conducted on the AVA 2.1 benchmark.\nScenario mAP@0.5\nStandard 1D positional encoding 30.9\nMA 2D positional encoding (no motion enhancement) 31.0\nMA 2D positional encoding (with motion enhancement) 33.1\n4.4.2 Multi-Feature Semantic Attention\nTable 8 illustrates the impact of various multi-feature atten-\ntion types and their combinations on the overall action de-\ntection performance (tested on the AVA 2.1 dataset). When\na single attention type is used, the highest performance\nis achieved by the intra-feature attention types, including\nAGG with a mAP@0.5 of 26.5% followed by AMM , with a\nmAP@0.5 of 25.6%. The inter-feature attention types, AMG ,\nand AGM , alone did not result in competitive performance.\nHowever, combined with AGG and AMM , the inter-feature\nattention types, AMG , and AGM , boosted the overall ac-\ntion detection performance. The results show that although\nJOURNAL OF LATEX CLASS FILES 12\nmany actions can still be effectively modeled independently,\nmany depend on the interaction between spatial and motion\nfeatures. The highest performance is obtained when all the\nmulti-feature attention types, AGG, AMM , AGM , and AMG\nare used, yielding a mAP@0.5 of 33.1%.\nTABLE 8\nThe impact of different multi-feature (MF) attention types, AGG\n(spatial-to-spatial), AMM (motion-to-motion), AGM (spatial-to-motion),\nand AMG (motion-to-spatial) on the overall action detection\nperformance. Different attention types represent standard one-stream\nand two-stream baselines and our single, double, combined, and full\nMF attentions.\nAttention type mAP@0.5 (%)\nAGG (one-stream RGB baseline) 26.5\nAMM (one-stream FLLOW baseline) 25.6\nAGM (single MF attention) 24.7\nAMG (single MF attention) 24.4\nAGG+ AMM (two-stream RGB+FLOW baseline) 28.0\nAGM+ AMG (double MF attention) 27.1\nAGG+ AMM + AGM (combined MF attention) 31.8\nAGG+ AMM + AMG (combined MF attention) 30.9\nAGG + AMM + AGM + AMG (full model) 33.1\n4.4.3 Selective Attention\nTable 9 compares various forms of self-attention and the\nproposed selective attention model characterized by two\ntypes of action semantics, persons ( Z) and objects ( O).\nThe lowest performance is due to the self-attention mech-\nanism that defined queries and keys as objects (mAP@0.5 of\n24.3%). In contrast, the maximum performance among the\nself-attention types is achieved with the queries and keys\nspecified as persons + objects ( Z + O) with a mAP@0.5 of\n31.8%. Overall, the proposed selective attention model with\nthe queries defined as persons ( Z) and the keys as persons\n+ objects (Z + O) yields a mAP@0.5 of 33.1%.\nTABLE 9\nThe comparison between different self-attention types and the\nproposed selective attention model based on two action semantics,\npersons (Z), and objects (O). Here the self-attention follows the\nstandard attention mechanism in the baseline [6].\nAttention category Inputs (Semantics) mAP@0.5 (%)\nSelf-attention Query ( O) , Key (O) 24.3\nSelf-attention Query ( Z) , Key (Z) 27.5\nSelf-attention Query ( Z + O) , Key (Z + O) 31.8\nSelective attention Query (Z) , Key (Z + O) 33.1\n4.4.4 Sequence-based Temporal Attention\nTable 10 compares the suggested sequence-based temporal\nattention with the efficient and standard ones on the AVA\n2.1 dataset. In this table, the standard temporal positional\nencoding (TPE) was presented in (18), the efficient temporal\nattention (ETPE) in (21), and the sequence-based temporal\npositional encoding (PTPE) in (23). Moreover, A, ˜A, and ˆA\nwere shown in (17), (22), and (24), respectively.\nThe results illustrated that the efficient temporal atten-\ntion ( ˜A) + ETPE with a mAP@0.5 of 30.9% slightly per-\nformed better than the standard temporal attention ( A) +\nTPE with a mAP@0.5 of 30.4%. Using sequence-based mod-\nels for both temporal attention and positional encoding, ˆA +\nTABLE 10\nThe comparison between standard temporal attention, efficient\ntemporal attention, and the proposed sequence-based temporal\nattention. Different types of positional encodings are the standard\ntemporal positional encoding (TPE), efficient temporal positional\nencoding (ETPE), and the proposed sequence-based temporal\npositional encoding (PTPE).\nModules mAP@0.5 (%)\nStandard temporal attention (A) + TPE (baseline [6]) 30.4\nEfficient temporal attention ( ˜A) + ETPE 30.9\nsequence-based temporal attention ( ˆA) + ETPE 32.5\nsequence-based temporal attention ( ˆA) + PTPE 33.1\nPTPE, yielded the maximum performance with a mAP@0.5\nof 33.1.\nFig. 5 compares the proposed sequence-based temporal\nattention correlation and the standard one on an action\nsequence, “triple jump”, with 10 sampled frames. Note that\nAcorr and ˆAcorr were explained in (18) and (21), respec-\ntively. Here, Acorr\ni−j and ˆAcorr\ni−j are the temporal correlation\nbetween the frame i ∈ τ and j ∈ τ, Investigation of\nFig. 5 more closely indicates that the standard temporal\nattention model leans toward producing greater values to\nsimilar frames, such as t = 1 and t = 2 . On the other\nhand, The standard temporal attention sets lower values\nto distinctive and non-adjacent frames, such as t = 3 and\nt = 5 or t = 6 and t = 9 . By contrast, the proposed\nsequence-based temporal attention does not discriminate\namong frames based on similarity. In other words, ˆAcorr can\nmore effectively provide the temporal relationship between\ndistinctive and non-adjacent frames so-called keyframes.\n4.4.5 Enhancement of the current frameworks\nWe integrated the proposed selective multi-feature attention\n(MFA) into several state-of-the-art spatiotemporal action\ndetection frameworks based on the transformer network.\nThis includes the Tublet attention in TubeR [52], actor-\ncontext attention in ACARNet [15], and interaction attention\nin AIA [40]. The results for the AVA 2.1 dataset are shown\nin Table 11. The outcomes indicated that all the methods\nimproved when the new MFA was used, with the highest\nenhancement of 1.2%.\nTABLE 11\nThe results of enhancing the state-of-the-art spatiotemporal action\ndetection frameworks using the proposed selective multi-feature\nattention (MFA).\nMethod mAP@0.5% (baseline) mAP@0.5% (MFA)\nAIA [40] 31.2 32.4\nACARNet [15] 30.0 30.1\nTubR [52] 32.0 32.5\n4.4.6 Error Analysis and Failure Cases\nFig. 6 (and Fig. 1 and Fig. 2 in the Supplementary Material)\nshow some examples of success and failure cases from\nthree sequences selected from the validation set of the AVA\ndataset. The true positives (TP), false positives (FP), and\nfalse negatives (FN) are the correct, incorrect, and missed\naction class predictions. Some limitations of the current\npipeline that contributed to failure cases are (1)2D restriction\nJOURNAL OF LATEX CLASS FILES 13\nFig. 5. An example of action “triple jump” illustrates the comparison between the proposed sequence-based temporal attention correlation, ˆAcorr,\nand the standard temporal attention correlation, Acorr. Here, Acorr\ni−j and ˆAcorr\ni−j represents the temporal correlation between the frame i and j. The\nstandard temporal attention tends to give higher values to similar frames, such as t = 1 and t = 2, and lower values to distinctive non-adjacent\nframes, such as t = 3 and t = 5 or t = 6 and t = 9. In contrast, the proposed temporal attention does not discriminate against frames based\non their similarities. Hence, ˆAcorr is more effective than Acorr in representing the temporal dependencies between distinctive and non-adjacent\nframes that represent keyframes.\nof videosis a cause of error. For example, the action “walking\nalong” was incorrectly detected (FP) as “talk to” or “listen\nto” due to the lack of 3D perception of face orientations.\n(2) Occlusion between persons and objects can distract the\nnetwork from detecting the correct relations between action\nsemantics. For example, a background object occluded by\npersons led to the incorrect conclusion (FP) that the persons\n“give” objects to others. On the other hand, occlusion can\nalso cause an object to be hidden, preventing it from detect-\ning (FN) the action class label “carry/hold” an object. (3)\nThe similarity between different classesalso has contributed to\nfailure cases. For instance, the action “sit” could be confused\nwith “bend/bow”.\n4.4.7 Computational Efficiency Analysis\nTable 12 shows the efficiency analysis of different pipeline\nmodules, based on running our algorithm on 30 frames sam-\npled from 5 seconds of an action video. Furthermore, Table\n13 indicates the efficiency analysis of different combinations\nof multi-feature attention types.\nTABLE 12\nEfficiency analysis of different pipeline modules.\nModule time (ms)\nAction semantic detection 401\nOptical Flow extraction 233\nMotion Memory Module 106\nMulti-feature selective semantic attention 160\nMulti-feature regular semantic attention 235\nTemporal Attention 41\nClassification and Regression 35\nTotal pre-processing 634\nTotal network modules 342\nTotal runtime 976\nTABLE 13\nEfficiency analysis of different multi-feature attention (MFA) modules\nand their combinations.\nMFA Module time (ms)\nAGG 101\nAMM 108\nAGM + AMG 143\nAGG + AMM + AGM + AMG 160\n5 C ONCLUSIONS AND FUTURE WORK\nThis paper presents a novel spatiotemporal transformer\nnetwork to model action semantics and their interactions\neffectively. In terms of contributed components, we intro-\nduce, first, a motion-aware 2D positional encoding that, in\ncontrast to the standard one, can handle spatiotemporal\nvariations in videos, especially geared toward action se-\nmantics. Second, a multi-feature attention model captures\ncomplicated multi-feature interactions between action se-\nmantics based on their spatial and motion properties. This\nmodel is accompanied by a special-purpose selective atten-\ntion mechanism, which is more effective and efficient than\nthe standard self-attention mechanism. Third, the sequence-\nbased temporal attention model effectively captures hetero-\ngeneous temporal dependencies between action frames. In\ncontrast to the traditional temporal attention mechanism\nthat focuses on the relationship between individual action\nframes, the sequence-based temporal attention model pri-\noritizes the temporal dependencies between frames based\non their contribution to the action sequence. The pro-\nposed pipeline outperformed the state-of-the-art methods\non four spatiotemporal action benchmarks: AVA 2.2, AVA\n2.1, UCF101-24, and EPIC-Kitchens.\nThe current pipeline detects action semantics with a pre-\ntrained network, which is not end-to-end. A seamless learn-\ning process within the transformer network that encodes\naction semantics directly may be more desirable. The same\nJOURNAL OF LATEX CLASS FILES 14\nvideo id: “7T5G0CmwTPo”\nFig. 6. Examples of success and failure cases on a sequence of the AVA validation set (video id: “7T5G0CmwTPo”). The results show the true\npositive (TP), false positive (FP), and false negative (FN) predicted classes with their confidence scores.\ncriticism could also be levied toward our current method\nof encoding optical flow fields. As possible future work, we\nsuggest extracting or simulating the optical flow fields end-\nto-end within the deep network.\n6 S UPPLEMENTARY MATERIAL\n6.1 Extended Algorithm Summary\nTable 14 indicates the extended algorithm summary.\n6.2 Semantic Motion Enhancement and Segmentation\nMotion modeling: In this step, the background motion\nSV\nB (λ) = {λn, n ∈ 0, ..., H} is modeled, where λ =\n(u, v), and H is the size of SV\nB (λ). We assume that\nSV\nB (λ) = SV (λ) − SV\nZ (λ), where SV\nZ (λ) ∈ SV are the\nsemantic motion vectors for persons that are segmented\nfrom SV (λ) using the corresponding bounding boxes for\npersons, BZ. The Gaussian mixture models (GMMs) with\nK distributions is used to model the background motion\nas P(λ) = PK\nk=1 πkN(λ|µk, Σk), where N(λ|µk, Σk) is a\nGaussian density with the mean of µk and the covariance\nof Σm, and mixing coefficient of πk. SV\nB (λ) is modeled by\noptimizing the GMMs parameters by way of the maximum\nlikelihood estimation for the GMMs.\nMotion restoration: In this step, the semantic motions\nvectors SV = {SV\nZ , SV\nO } ∈SV , which includes the vectors\nfor persons, SV\nZ , and objects, SV\nO are restored. Each motion\nvector is restored as: λn ∈ SV = λn −µm, where λn belongs\nto one of the k ∈ K GMMs distributions. The final enhanced\nsegmented semantic motion vectors SM\nA = {SV , SM } are\nused as reliable motion representations of action semantics\nin the spatiotemporal transformer network, whereSM is the\ncorresponding segmented semantic motion images that are\nconverted from vectors.\nSome examples are shown in Fig. 7 (raw images are\nselected from the AVA dataset). In this example, the per-\nsons walk to their right as the camera moves in the same\ndirection slightly faster. Such spatiotemporal inconsistency\nleads to incorrect estimation of motion vectors showing the\nwrong motion direction to the left (red motion vectors). As\na result, the enhanced segmented semantic motion vectors\nare illustrated in Fig. 7, in which the distorted and resorted\nmotion vectors are shown in red and green, respectively.\n6.3 Proof for Efficiency of Selective Attention\nThe following show the matrix multiplications between\nquery (Q), key ( K) and value ( V ) of our selective semantic\nattention (AS) that are illustrated with a size operation ( d):\nFor the selective semantic attention, AS, the dimension-\nality and computational cost are given by:\nd(AS) = d(Q) × d(K)T × d(V )\n= RN×dh × (R(N+N′)×dh )T × R(N+N′)×dh\n= RN×dh × Rdh×(N+N′) × R(N+N′)×dh\n= RN×dh .\n(27)\nThis makes the computational cost of our selective semantic\nattention O(N · (N + N′) · dh).\nJOURNAL OF LATEX CLASS FILES 15\nTABLE 14\nPipeline algorithm summary in a hierarchical order indicating each phase (in bold), the summary of each phase, inputs and outputs of each phase,\nand the corresponding manuscript section.\nPhase/Summary Inputs Outputs Section\nsemantic detection IRGB SG 3.2\nextracts action semantics (people and objects) RGB frames spatial action semantics\noptical flow extraction IRGB SM, SV 3.2\nextracts optical flow fields for action semantics RGB frames motion semantics images, and vectors\nmotion enhancement and segmentation BG, IV SM, SV 3.2.1\nimproves motion features (w.r.t camera) bounding boxes, motion vectors enhanced motion semantics, and vectors\nfeature embedding SG, SM XG, XM 3.3\nextract spatial and motion features using Conv spatial, and motion semantics spatial, and motion feature embeddings\nmotion memory module SV ∆PX, ∆PY 3.3.1\nprovides motion information to the transformer semantic motion vectors semantic motion offsets\nmotion-aware 2D Positional Encoding ∆PX, ∆PY PX\nA , PX\nY 3.3.1\nmakes the transformer aware of movements semantic motion offsets 2D motion-aware positional encoding\nmulti-feature semantic attention SG, SM AH 3.3.2\ncomputes the correlations between multi-features spatial, and motion features multi-head semantic attention\nmulti-feature fusion AH Xs 3.3.3\ncombines and flows the features in different layers multi-head semantic attention, temporal semantic features\nsequence-based temporal attention Xs ˆA 3.3.4\ncomputes heterogeneous temporal relations temporal semantic features temporal attention\nclassification and regression ˆA ˆY , ts, te 3.3.5\nclassifies actions and start and end frames temporal attention class predictions, start and end frames\ntime: t time: t + ω distorted segmented our enhanced segmented\nsemantic motion vectors semantic motion vectors\nFig. 7. An example of the semantic motion enhancement and segmentation algorithm tested on an action sample from the AVA dataset. For two\nconsecutive sampled frames of t and t + ω, the distorted semantic motion vectors (red arrows) and the enhanced semantic motion vectors (green\narrows) are shown. In this example, the segmented semantics (persons) walk to the right. At the same time, the camera also moves to the right\n(marginally faster), causing incorrect extraction of semantic motion vectors. The proposed motion enhancement and segmentation algorithm fixes\nthis issue.\nFor regular semantic self-attention, AR, the dimensions\nare:\nd(AR) = d(Q) × d(K)T × d(V )\n= R(N+N′)×dh × (R(N+N′)×dh )T × R(N+N′)×dh\n= R(N+N′)×dh × Rdh×(N+N′) × R(N+N′)×dh\n= R(N+N′)×dh .\n(28)\nSo, the computational cost of the regular semantic self-\nattention is O((N + N′)2 · dh).\n6.4 Sequence-based temporal Attention\n6.4.1 Conceptual Proof\nLet’s think of the action frames as employees and the action\nas a company. Our approach is to value the professional\nrelationships (temporal dependencies) between employees\n(frames) that benefit the company (action), such as effective\nteamwork, rather than personal relationships, such as liking\nthe same foods. We also can prove the above conclusion by\nway of maximum a posteriori estimation. Specifically, given a\nclass c and observation x that could be a frame fs\nt ∈ Xs\nor an action sequence Xs, the goal of the action detection\nnetwork is to maximize the posterior ofp(c|x) = p(x|c)\np(x) ·p(c).\nWe can say that the likelihood p(Xs|c) > p(fs\nt |c) since a\nsequence better represents an action than a frame. On the\nother hand, p(Xs) < p(fs\nt ) since having a unique combina-\ntion of frames is less common than individual frames. So,\np(c|Xs) > p(fs\nt ).\n6.4.2 Mathematical Proof\nThe sequence-based temporal attention is based on the intu-\nition of the Mahalanobis distance, which determines the dis-\ntance between two vectors, q and k, with respect to the data\ndistribution Xs as D(q, k; Xs) =\np\n(q − k)S−1(q − k)T .\nAssuming the correlations between two vectors, corr(q, k)\ncan be defined as q.kT = 1 − D2(q2, k2)/2N, by replacing\nit in (21), we can justify (23). We, however, instead of\nthe vector product used in computing the Mahalanobis\ndistance, perform a matrix multiplication between vectors\nJOURNAL OF LATEX CLASS FILES 16\nto calculate the frame-by-frame sequence-based attention\ncorrelation matrix, ˆAcorr.\n6.5 Visualization of Predicted Results\nFig. 8 and Fig. 9 show some examples of success and failure\ncases from three sequences selected from the validation set\nof the AVA dataset. The true positives (TP), false positives\n(FP), and false negatives (FN) are the correct, incorrect, and\nmissed action class predictions.\nREFERENCES\n[1] Antonios Oikonomopoulos, Ioannis Patras, and Maja Pantic.\nSpatiotemporal localization and categorization of human actions\nin unsegmented image sequences. IEEE transactions on Image\nProcessing, 20(4):1126–1140, 2010.\n[2] Du Tran and Junsong Yuan. Max-margin structured output re-\ngression for spatio-temporal action localization. Advances in neural\ninformation processing systems, 25, 2012.\n[3] Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid.\nLearning to track for spatio-temporal action localization. In\nProceedings of the IEEE international conference on computer vision,\npages 3164–3172, 2015.\n[4] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\npages 244–253, 2019.\n[5] Bin Li, Jian Tian, Zhongfei Zhang, Hailin Feng, and Xi Li. Multi-\ntask non-autoregressive model for human motion prediction.IEEE\nTransactions on Image Processing, 30:2562–2574, 2020.\n[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. Advances in neural information processing\nsystems, 30, 2017.\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end\nobject detection with transformers. In European conference on\ncomputer vision, pages 213–229. Springer, 2020.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[9] Matthew Korban and Xin Li. Ddgcn: A dynamic directed graph\nconvolutional network for action recognition. In Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XX 16, pages 761–776. Springer, 2020.\n[10] Matthew Korban and Xin Li. Semantics-enhanced early action\ndetection using dynamic dilated convolution. Pattern Recognition,\n140:109595, 2023.\n[11] Kai Guo, Prakash Ishwar, and Janusz Konrad. Action recognition\nfrom video using feature covariance matrices. IEEE Transactions on\nImage Processing, 22(6):2479–2494, 2013.\n[12] Fahad Shahbaz Khan, Joost Van De Weijer, Rao Muhammad\nAnwer, Michael Felsberg, and Carlo Gatta. Semantic pyramids\nfor gender and action recognition. IEEE transactions on image\nprocessing, 23(8):3633–3645, 2014.\n[13] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, and\nCordelia Schmid. Action tubelet detector for spatio-temporal ac-\ntion localization. In Proceedings of the IEEE International Conference\non Computer Vision, pages 4405–4413, 2017.\n[14] Xiaohan Wang, Linchao Zhu, Yu Wu, and Yi Yang. Symbiotic at-\ntention for egocentric action recognition with object-centric align-\nment. IEEE transactions on pattern analysis and machine intelligence,\n2020.\n[15] Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao, and\nHongsheng Li. Actor-context-actor relation network for spatio-\ntemporal action localization. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages 464–474,\n2021.\n[16] Karen Simonyan and Andrew Zisserman. Two-stream convolu-\ntional networks for action recognition in videos. Advances in neural\ninformation processing systems, 27, 2014.\n[17] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Con-\nvolutional two-stream network fusion for video action recogni-\ntion. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 1933–1941, 2016.\n[18] Xuanhan Wang, Lianli Gao, Peng Wang, Xiaoshuai Sun, and Xian-\nglong Liu. Two-stream 3-d convnet fusion for action recognition\nin videos with arbitrary size and length. IEEE Transactions on\nMultimedia, 20(3):634–644, 2017.\n[19] Rui Su, Dong Xu, Luping Zhou, and Wanli Ouyang. Progressive\ncross-stream cooperation in spatial and temporal domain for ac-\ntion localization. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 43(12):4477–4490, 2020.\n[20] Weiming Hu, Haowei Liu, Yang Du, Chunfeng Yuan, Bing Li, and\nStephen Maybank. Interaction-aware spatio-temporal pyramid\nattention networks for action classification. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 44(10):7010–7028, 2022.\n[21] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip HS Torr, and\nFabio Cuzzolin. Online real-time multiple spatiotemporal action\nlocalisation and prediction. In Proceedings of the IEEE International\nConference on Computer Vision, pages 3637–3646, 2017.\n[22] Xitong Yang, Xiaodong Yang, Ming-Yu Liu, Fanyi Xiao, Larry S\nDavis, and Jan Kautz. Step: Spatio-temporal progressive learning\nfor video action detection. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages 264–272,\n2019.\n[23] Gurkirt Singh, Vasileios Choutas, Suman Saha, Fisher Yu, and Luc\nVan Gool. Spatio-temporal action detection under large motion.\nIn Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 6009–6018, 2023.\n[24] Lingtong Kong, Chunhua Shen, and Jie Yang. Fastflownet: A\nlightweight network for fast optical flow estimation. In 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages\n10310–10316. IEEE, 2021.\n[25] Matthew Korban, Peter Youngs, and Scott T Acton. A multi-modal\ntransformer network for action detection. Pattern Recognition,\n142:109713, 2023.\n[26] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei Zhang,\nSong Bai, and Xiang Bai. End-to-end temporal action detection\nwith transformer. IEEE Transactions on Image Processing, 31:5427–\n5441, 2022.\n[27] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas\nZamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers\nin vision: A survey. ACM computing surveys (CSUR), 54(10s):1–41,\n2022.\n[28] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and\nYunhe Wang. Transformer in transformer. Advances in Neural\nInformation Processing Systems, 34:15908–15919, 2021.\n[29] Matthew Korban, Peter Youngs, and Scott T Acton. Taa-gcn: A\ntemporally aware adaptive graph convolutional network for age\nestimation. Pattern Recognition, 134:109066, 2023.\n[30] Amulya Arun Ballakur and Arti Arya. Empirical evaluation of\ngated recurrent neural network architectures in aviation delay\nprediction. In 2020 5th International Conference on Computing,\nCommunication and Security (ICCCS), pages 1–7. IEEE, 2020.\n[31] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos in\nthe wild. arXiv preprint arXiv:1212.0402, 2012.\n[32] Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivane-\nsan Sangeetha. Ammu: a survey of transformer-based biomedical\npretrained language models. Journal of biomedical informatics, page\n103982, 2021.\n[33] Prasad Gabbur, Manjot Bilkhu, and Javier Movellan. Probabilistic\nattention for interactive segmentation. Advances in Neural Informa-\ntion Processing Systems, 34:4448–4460, 2021.\n[34] Tam Minh Nguyen, Tan Minh Nguyen, Dung DD Le, Duy Khuong\nNguyen, Viet-Anh Tran, Richard Baraniuk, Nhat Ho, and Stanley\nOsher. Improving transformers with probabilistic attention keys.\nIn International Conference on Machine Learning, pages 16595–16621.\nPMLR, 2022.\n[35] Wael Abd-Almageed, Aly El-Osery, and Christopher E Smith.\nNon-parametric expectation maximization: a learning automata\napproach. In SMC’03 Conference Proceedings. 2003 IEEE Interna-\ntional Conference on Systems, Man and Cybernetics. Conference Theme-\nSystem Security and Assurance (Cat. No. 03CH37483), volume 3,\npages 2996–3001. IEEE, 2003.\n[36] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel\nCremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to\nJOURNAL OF LATEX CLASS FILES 17\nvideo id: “CZ2NP8UsPuE”\nFig. 8. Some examples of success and failure scenarios from an action sequence of the validation set of the AVA dataset (video id:\n“CZ2NP8UsPuE”). The results indicate the true positive (TP), false positive (FP), and false negative (FN) predicted classes with their confidence\nscores.\nvideo id: “Hscyg0vLKc8”\nFig. 9. An example sequence of the AVA validation set showing the success and failure cases (video id: “Hscyg0vLKc8”). The results are true\npositive (TP), false positive (FP), and false negative (FN) predicted classes with their confidence scores.\nJOURNAL OF LATEX CLASS FILES 18\ntrain convolutional networks for disparity, optical flow, and scene\nflow estimation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4040–4048, 2016.\n[37] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja\nFidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti,\nJonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric\nvision: The epic-kitchens dataset. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 720–736, 2018.\n[38] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr\nDoll´ar. Focal loss for dense object detection. In Proceedings of the\nIEEE international conference on computer vision, pages 2980–2988,\n2017.\n[39] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caro-\nline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George\nToderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video\ndataset of spatio-temporally localized atomic visual actions. In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6047–6056, 2018.\n[40] Jiajun Tang, Jin Xia, Xinzhi Mu, Bo Pang, and Cewu Lu. Asyn-\nchronous interaction aggregation for action detection. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part XV 16, pages 71–87. Springer, 2020.\n[41] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He,\nPhilipp Krahenbuhl, and Ross Girshick. Long-term feature banks\nfor detailed video understanding. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 284–\n293, 2019.\n[42] Fabien Baradel, Natalia Neverova, Christian Wolf, Julien Mille,\nand Greg Mori. Object level visual reasoning in videos. In\nProceedings of the European Conference on Computer Vision (ECCV),\npages 105–121, 2018.\n[43] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der\nMaaten, Armand Joulin, and Ishan Misra. Omnivore: A single\nmodel for many visual modalities. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 16102–\n16112, 2022.\n[44] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan,\nBo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit:\nMemory-augmented multiscale vision transformer for efficient\nlong-term video recognition. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages 13587–13597,\n2022.\n[45] Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar,\nGal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson.\nObject-region video transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 3148–\n3159, 2022.\n[46] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Video-\nmae: Masked autoencoders are data-efficient learners for self-\nsupervised video pre-training. Advances in neural information\nprocessing systems, 35:10078–10093, 2022.\n[47] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and\nKaiming He. A large-scale study on unsupervised spatiotemporal\nrepresentation learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3299–3309, 2021.\n[48] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng\nYan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale\nvision transformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 6824–6835, 2021.\n[49] Christoph Feichtenhofer. X3d: Expanding architectures for effi-\ncient video recognition. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 203–213, 2020.\n[50] Shoufa Chen, Peize Sun, Enze Xie, Chongjian Ge, Jiannan Wu, Lan\nMa, Jiajun Shen, and Ping Luo. Watch only once: An end-to-end\nvideo action detection framework. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 8178–8187, 2021.\n[51] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form\nvideo understanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 1884–1894, 2021.\n[52] Jiaojiao Zhao, Yanyi Zhang, Xinyu Li, Hao Chen, Bing Shuai,\nMingze Xu, Chunhui Liu, Kaustav Kundu, Yuanjun Xiong, Da-\nvide Modolo, et al. Tuber: Tubelet transformer for video action\ndetection. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13598–13607, 2022.\n[53] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille,\nand Christoph Feichtenhofer. Masked feature prediction for self-\nsupervised visual pre-training. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 14668–\n14678, 2022.\n[54] Gueter Josmy Faure, Min-Hung Chen, and Shang-Hong Lai.\nHolistic interaction transformer network for action detection. In\nProceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 3340–3350, 2023.\n[55] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy,\nRahul Sukthankar, and Cordelia Schmid. Actor-centric relation\nnetwork. In Proceedings of the European Conference on Computer\nVision (ECCV), pages 318–334, 2018.\n[56] Jonathan Stroud, David Ross, Chen Sun, Jia Deng, and Rahul Suk-\nthankar. D3d: Distilled 3d networks for video action recognition.\nIn Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 625–634, 2020.\n[57] Anshul Shah, Shlok Mishra, Ankan Bansal, Jun-Cheng Chen,\nRama Chellappa, and Abhinav Shrivastava. Pose and joint-aware\naction recognition. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision, pages 3850–3860, 2022.\n[58] Lin Song, Shiwei Zhang, Gang Yu, and Hongbin Sun. Tacnet:\nTransition-aware context network for spatio-temporal action de-\ntection. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11987–11995, 2019.\n[59] Rizard Renanda Adhi Pramono, Yie-Tarng Chen, and Wen-Hsien\nFang. Hierarchical self-attention network for action localization in\nvideos. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 61–70, 2019.\n[60] Yubo Zhang, Pavel Tokmakov, Martial Hebert, and Cordelia\nSchmid. A structured model for action detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 9975–9984, 2019.\n[61] Yixuan Li, Zixu Wang, Limin Wang, and Gangshan Wu. Actions\nas moving points. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVI\n16, pages 68–84. Springer, 2020.\n[62] Yu Liu, Fan Yang, and Dominique Ginhac. Acdnet: An action\ndetection network for real-time edge computing based on flow-\nguided feature approximation and memory aggregation. Pattern\nRecognition Letters, 145:118–126, 2021.\n[63] Akash Kumar and Yogesh Singh Rawat. End-to-end semi-\nsupervised learning for video action detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 14700–14710, 2022.\n[64] Hui Li, Wenjun Hu, Ying Zang, and Shuguang Zhao. Action recog-\nnition based on attention mechanism and depthwise separable\nresidual module. Signal, Image and Video Processing, pages 1–9,\n2022.\n[65] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia\nSchmid, and Chen Sun. Attention bottlenecks for multimodal\nfusion. Advances in Neural Information Processing Systems, 34:14200–\n14213, 2021.\n[66] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun,\nMario Lu ˇci´c, and Cordelia Schmid. Vivit: A video vision\ntransformer. In Proceedings of the IEEE/CVF international conference\non computer vision, pages 6836–6846, 2021.\n[67] Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan\nMisra Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi,\nand Jo ˜ao F. Henriques. Keeping your eye on the ball: Trajectory\nattention in video transformers. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2021.\n[68] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingx-\ning Tan, Matthew Brown, and Boqing Gong. Movinets: Mobile\nvideo networks for efficient video recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 16020–16030, 2021.\n[69] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang,\nChen Sun, and Cordelia Schmid. Multiview transformers for video\nrecognition. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3333–3343, 2022.\nJOURNAL OF LATEX CLASS FILES 19\nMatthew Korban (Senior Member, IEEE) re-\nceived his BSc and MSc degrees in Electrical\nEngineering from the University of Guilan, where\nhe worked on sign language recognition in video.\nHe received his Ph.D. in Computer Engineering\nfrom Louisiana State University. Matthew has led\nseveral groundbreaking projects, such as LSU\nCAVE2, one of the world’s largest and most\nadvanced Cave2 VR. He has published several\npublications in top-tier computer vision confer-\nences and journals such as ECCV and Pattern\nRecognition. He is currently a Postdoc Research Associate at the\nUniversity of Virginia, working with Prof. Scott T. Acton. His research\ninterests include human action recognition, early action recognition,\nmotion synthesis, and human geometric modeling.\nPeter Youngs is a Professor and Chair in the\nDepartment of Curriculum, Instruction, and Spe-\ncial Education at the University of Virginia. He\nstudies how neural networks can automatically\nclassify instructional activities in videos of ele-\nmentary mathematics and reading instruction.\nHe currently serves as co-editor of American\nEducational Research Journal.\nScott T. Acton is a Professor and Chair of Elec-\ntrical and Computer Engineering at the Univer-\nsity of Virginia. He is also appointed in Biomed-\nical Engineering. For the last three years, he\nwas Program Director in the Computer and In-\nformation Sciences and Engineering directorate\nof the National Science Foundation. He received\nthe M.S. and Ph.D. degrees at the University of\nTexas at Austin, and he received his B.S. degree\nat Virginia Tech. Professor Acton is a Fellow of\nthe IEEE “for contributions to biomedical image\nanalysis.” Professor Acton’s laboratory at UVA is called VIVA - Virginia\nImage and Video Analysis. They specialize in biological/biomedical\nimage analysis problems. The research emphases of VIVA include\nimage analysis in neuroscience, tracking, segmentation, representation,\nretrieval, classification and enhancement. Recent theoretical interests\ninclude machine learning, active contours, partial differential equation\nmethods, scale space methods, and graph signal processing. Professor\nActon has over 300 publications in the image analysis area including\nthe books Biomedical Image Analysis: Tracking and Biomedical Image\nAnalysis: Segmentation. He was the 2018 Co-Chair of the IEEE Interna-\ntional Symposium on Biomedical Imaging. Professor Acton was Editor-\nin-Chief of the IEEE Transactions on Image Processing (2014-2018)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7679916024208069
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6639701128005981
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5212719440460205
    },
    {
      "name": "Transformer",
      "score": 0.5124137997627258
    },
    {
      "name": "Motion (physics)",
      "score": 0.504496693611145
    },
    {
      "name": "Exploit",
      "score": 0.4846540093421936
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4205652177333832
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4080355763435364
    },
    {
      "name": "Computer vision",
      "score": 0.3688538372516632
    },
    {
      "name": "Natural language processing",
      "score": 0.32228684425354004
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    }
  ]
}