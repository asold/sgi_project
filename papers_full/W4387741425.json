{
  "title": "A Multimodal Large Language Model for Forestry in pest and disease recognition",
  "url": "https://openalex.org/W4387741425",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103921800",
      "name": "Maolin Zhang",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A2323523758",
      "name": "Xianyong Li",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A2128274528",
      "name": "Yajun Du",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A4281346502",
      "name": "Zihan Rao",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A2104308516",
      "name": "Songlin Chen",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A2097097663",
      "name": "Weicheng Wang",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098788672",
      "name": "Xiaoliang Chen",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098329421",
      "name": "Dong Huang",
      "affiliations": [
        "Xihua University"
      ]
    },
    {
      "id": "https://openalex.org/A2109040337",
      "name": "Shumin Wang",
      "affiliations": [
        "China National Institute of Standardization"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389519587",
    "https://openalex.org/W2949960976",
    "https://openalex.org/W2963569233",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4386185600",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4312810944",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4297676396",
    "https://openalex.org/W2952557657",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4378498682",
    "https://openalex.org/W2964031251",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W4386148428",
    "https://openalex.org/W2058641082",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4382142077",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2090958120",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W4308902180",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W2964035320",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4322716276",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4224246420",
    "https://openalex.org/W3158697128",
    "https://openalex.org/W1996901117",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W2888867175",
    "https://openalex.org/W4361229539",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4376167553",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3184735396"
  ],
  "abstract": "<title>Abstract</title> In the context of generative multimodal dialogue, the amalgamation of image and text forms the fundamental basis for a wide array of visual and language tasks. Despite the proliferation of large language models (LLMs), research on training LLMs for multimodal reasoning has remained relatively scarce. The progression of existing multimodal models has been propelled by using expansive image-text datasets in broader domains. Nevertheless, these models need refinement when comprehending and engaging in dialogue concerning Chinese forestry biological images. This paper proposes an efficient and adaptive multimodal large language model for forestry (MLLM-For) in pest and disease recognition. The central concept involves harnessing our meticulously constructed, expansive Chinese forestry biological image caption dataset. We utilize open-ended instruction-following data to master the intricacies of open-ended dialogue semantics, facilitating the comprehensive assimilation of knowledge within forestry biology. In particular, the Querying Transformer (Q-Former) module is a conduit linking pre-trained visual and language models. Using image-text pairs, the China Open Source Chat Language Model (ChatGLM) can now align vocabulary specific to forestry pests and diseases. By aligning image features with the pre-trained language model, we undertake concurrent Low-Rank Adaptive (LoRA) fine-tuning of the ChatGLM model. Furthermore, we engage in high-caliber instruction data fine-tuning to 1 tailor it to the proposed multimodal task. The proposed MLLM-For model show-cases impressive proficiency in comprehending and engaging in dialogue regarding Chinese forestry pests and disease images. It also displays exceptional capabilities in multimodal dialogue and is adept at adhering to open-ended directives for retrieving information about forestry pests and diseases. Empirical findings affirm the competitive performance of the MLLM-For model within the realm of Chinese multimodal dialogue, surpassing five leading multimodal large language models.",
  "full_text": "A Multimodal Large Language Model for Forestry in\npest and disease recognition\nMaolin Zhang \nXihua University\nXianyong Li  (  xian-yong@163.com )\nXihua University\nYajun Du \nXihua University\nZihan Rao \nXihua University\nSonglin Chen \nXihua University\nWeicheng Wang \nXihua University\nXiaoliang Chen \nXihua University\nDong Huang \nXihua University\nShumin Wang \nChina National Institute of Standardization\nResearch Article\nKeywords: Large language models, Multimodal large language model,China Open Source Chat Language\nModel, Pest and Disease Recognition\nPosted Date: October 18th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3444472/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nA Multimodal Large Language Model for Forestry\nin pest and disease recognition\nMaolin Zhang1, Xianyong Li1*, Yajun Du1, Zihan Rao1,\nSonglin Chen1, Weicheng Wang1, Xiaoliang Chen1, Dong Huang1,\nShumin Wang2\n1*School of Computer and Software Engineering, Xihua University,\nChengdu, 610039, Sichuan, China.\n2China National Institute of Standardization, Beijing, 100191, Country.\n*Corresponding author(s). E-mail(s): lixy@mail.xhu.edu.cn;\nContributing authors: zhangml@stu.xhu.edu.cn;\nduyajun@mail.xhu.edu.cn; raozh@stu.xhu.edu.cn;\nsonglinchen@stu.xhu.edu.cn; weichengwang@stu.xhu.edu.cn;\nchenxl@mail.xhu.edu.cn; huangd@gzu.edu.cn; wangshm@cnis.ac.cn;\nAbstract\nIn the context of generative multimodal dialogue, the amalga mation of image and\ntext forms the fundamental basis for a wide array of visual and langu age tasks.\nDespite the proliferation of large language models (LLMs), researc h on training\nLLMs for multimodal reasoning has remained relatively scarce. Th e progression\nof existing multimodal models has been propelled by using exp ansive image-text\ndatasets in broader domains. Nevertheless, these models need reﬁnement when\ncomprehending and engaging in dialogue concerning Chinese fore stry biological\nimages. This paper proposes an eﬃcient and adaptive multimod al large language\nmodel for forestry (MLLM-For) in pest and disease recognition. The central con-\ncept involves harnessing our meticulously constructed, expan sive Chinese forestry\nbiological image caption dataset. We utilize open-ended in struction-following\ndata to master the intricacies of open-ended dialogue semant ics, facilitating the\ncomprehensive assimilation of knowledge within forestry biolo gy. In particular,\nthe Querying Transformer (Q-Former) module is a conduit linking pre -trained\nvisual and language models. Using image-text pairs, the Chin a Open Source Chat\nLanguage Model (ChatGLM) can now align vocabulary speciﬁc to forestry pests\nand diseases. By aligning image features with the pre-trained la nguage model, we\nundertake concurrent Low-Rank Adaptive (LoRA) ﬁne-tuning of t he ChatGLM\nmodel. Furthermore, we engage in high-caliber instruction data ﬁne-tuning to\n1\ntailor it to the proposed multimodal task. The proposed MLLM-F or model show-\ncases impressive proﬁciency in comprehending and engaging in d ialogue regarding\nChinese forestry pests and disease images. It also displays exc eptional capabil-\nities in multimodal dialogue and is adept at adhering to open- ended directives\nfor retrieving information about forestry pests and diseases. Empiri cal ﬁndings\naﬃrm the competitive performance of the MLLM-For model within th e realm of\nChinese multimodal dialogue, surpassing ﬁve leading multim odal large language\nmodels.\nKeywords: Large language models, Multimodal large language model,Ch ina Open\nSource Chat Language Model, Pest and Disease Recognition\n1 Introduction\nLarge Language Models (LLMs) [\n1],[2],[3],[4],[5],[6] have exhibited remarkable perfor-\nmance across diverse domains, garnering substantial interest from both the academic\nand industrial spheres. LLMs represent a category of natural language proce ssing\nmodels founded upon deep learning principles. These models lear n natural language’s\ngrammatical and semantic attributes through extensive training on expan sive textual\ncorpora. Their exceptional competencies, including comprehension , logical inference,\nand linguistic synthesis, underscore the expansive prospects for their use in a multi-\ntude of alternative domains. While language models have demonstrated com mendable\nperformance, it is evident that possessing solely textual ”comprehe nsion” often inad-\nequately communicates information with vividness. The integration of information\nfrom divergent modalities, notably textual and visual elements, coup led with the\ninclusion of perceptual acumen linked to ”visual” comprehension, fac ilitates the attain-\nment of a more comprehensive grasp of phenomena and ampliﬁes our express ive\nproﬁciencies. Consequently, this expansion broadens the horizons of language model\napplications. Multimodal fusion provides us with richer ways to e xplore and solve\ncomplex problems. Artiﬁcial General Intelligence (AGI) [ 7] possesses eﬃcient multi-\nmodal information processing capability. Multimodal artiﬁcial inte lligence (AI) not\nonly tackles tasks involving single data types but also establishes c onnections and\nfuses information across diverse data types, thus oﬀering support for addressing intri-\ncate challenges. Hence, we contemplate the feasibility of transposing large language\nmodels into cross-modal domains to emulate human cognitive abilities . An open and\ntransparent open-source base language model, ChatGLM [ 8], has caught our attention.\nEven with the notable performance showcased by the LLaMA [ 9] model through\nﬁne-tuning, its aptitude within the domain of the Chinese language rem ains con-\nstrained, stemming from its limited exposure to Chinese corpora d uring the pre-\ntraining stage. In contrast, the ChatGLM [ 8] model by Tsinghua University stands\nfor its exceptional performance in the realm of the Chinese language. It i s based\non the General Language Model (GLM) architecture, featuring 6.2 billion p arame-\nters. ChatGLM-6B employs methodologies analogous to those employed by Ch atGPT,\ntailored to optimize performance within Chinese question-answeri ng and dialogic\n2\ninteractions. However, it is noteworthy that frequent observation pe rtains to the\nsuboptimal performance of numerous expansive language models in scenari os neces-\nsitating a profound grasp of domain-speciﬁc intricacies. Our supposit ion posits that\nthis phenomenon may trace its origins to the paucity of data originating f rom distinct\nspecialized domains during the formative pre-training phase of the m odel. Within\nan industrial purview, universal expansive models undertake the role akin to ”com-\nprehensive compendiums”, typiﬁed by exemplars like GPT-3 [ 10], PaLM [ 11], MOSS\n[12], and ERNIE Bot[ 13], characterized by their capacity to furnish comprehensive\nresponses. Meanwhile, domain-speciﬁc expansive models assume th e semblance of\nmono-disciplinary experts, reﬁned within the conﬁnes of their sp eciﬁc niches.\nThe expansion of LLMs into visual-linguistic multimodality and their judicious\nutilization constitute a subject of considerable signiﬁcance. Befor e the emergence of\nextensive multimodal models, the prevalent approaches encompasse d manual image\nand video annotation, commonly called tagging. When confronted with more ext en-\nsive and intricate datasets, sole reliance on manual tagging does not meet d ata quality\nand tagging eﬃciency requirements. The progression of multimodal tec hnologies facil-\nitates the conversion of heterogeneous multimodal data, spanning images an d videos,\ninto textual representations by expansive models. This transfor mation contributes to\na more exhaustive and eﬃcacious assimilation of information. OpenAI’s rel ease of\nthe multimodal version of GPT-3 [ 10], known as DALL.E [ 14], showcases remarkable\ncapabilities in generating images from text. This achievement eﬀect ively bridges visual\nand linguistic domains, underscoring the potency of pre-trained mul timodal models.\nAddressing the eﬃcient synchronization of models with multimodal images and Chi-\nnese linguistic directives has spurred a resurgence of focus wit hin the community\ntoward reﬁning Chinese language-enriched foundational visual models [15],[16]. Con-\ncurrently, copious parallel image-text datasets across diverse domain s provide a proliﬁc\nresource. Empirical ﬁndings substantiate the capacity of generative p re-training to\nharness this parallel data for self-supervised visual-language model ing adeptly. Eviden-\ntial validation of this phenomenon has been showcased through open-source initiatives\nsuch as multimodal GPT-4[ 17] and LLaVA [ 18]. Through the utilization of multimodal\ninputs to guide the model following human intentions, the ensuin g Large Multimodal\nModels (LMMs) have exhibited resilient zero-shot completion pro wess spanning a\ngamut of user-oriented visual-linguistic assignments, thereby lay ing the groundwork for\nthe advancement of comprehensive multimodal conversational aides [ 15],[16],[9]. The\nendeavor to equip ChatGLM with image recognition capabilities poses a for midable\nchallenge, considering that LLMs do not integrate images during their pr e-training\nphase.\nTo enable the interaction between text and image modalities, we have employed a\nQuerying Transformer ( Q-Former) [ 19]. Q-Former, a lightweight transformer, utilizes a\nset of trainable query vectors to extract visual features from the image encoder, serving\nas an information bottleneck between the image and text encoders. It su pplies the most\npertinent visual information required for text generation by LLMs. Q-F ormer predomi-\nnantly consists of learned queries for both modalities, Cross-Atten tion, Self-Attention,\nand Feed Forward components. Concretely, during the training phase, we learn visual-\nlanguage representation by feeding vectors processed through the image and text\n3\nencoders into Q-Former, which bridges these two distinct modal ities. Although natural\nimage-text multimodal pre-training has shown impressive eﬃcacy i n numerous down-\nstream applications, its seamless adaptation to the forestry domain faces challenges\ndue to fundamental domain distinctions. Furthermore, collecting an notated datasets\nfor forestry-related diseases and pests typically necessitates su bstantial domain-speciﬁc\nknowledge and resources, thereby making implementing our model i n the specialized\nﬁeld of forestry a feasible proposition. Given the inherent constrai nts associated with\nthe modest scale of ChatGLM-6B, substantial opportunities exist for f urther reﬁning\nand optimizing the model.\nIn the past, the LLaMA-Adapter V2 [ 20] model eﬀectively broadened the scope of\nlanguage models to encompass multimodality by implementing adapter- based strate-\ngies. Nevertheless, the utilization of adapter-based methodologies cou ld lead to the\nintroduction of latency. Consequently, to alleviate the challenges associated with\nthe deployment of voluminous models—speciﬁcally, the need for ex tensive memory\nresources and notable storage utilization, coupled with the latency ste mming from\nthe integration of adapters within the model—the Low-Rank Adaptation (LoRA) [ 21]\ntechnique has been introduced. It provides signiﬁcant beneﬁts in terms of resource\nminimization while avoiding any supplementary inference latency . The LoRA model\nalleviates the storage requirements for parameters and virtual random acc ess mem-\nory (VRAM). When the rank(r) is markedly smaller than the model’s, the LoRA\nmodel obviates the necessity to retain optimizer state parameters, culminating in\na noteworthy decrease in VRAM consumption. LoRA eﬀectively showcases m ultiple\nsubstantial merits in the context of task adaptation. In order to enhance computa-\ntional eﬃciency, this model employs a strategy that involves swappi ng only the weights\nrather than modifying all parameters. This focused tactic arises fr om the superﬂuous\nnature of gradients for most parameters, leading to a marked reduction in computa-\ntional demands. By utilizing an adaptive mechanism, the LoRA architect ure attains\ncomprehensive performance without necessitating cumulative gradi ent adjustments to\nthe weight matrices. Aligning the rank(r) of all weight matrices in the LoRA model\nwith that of the pre-trained weight matrices facilitates the model’ s achievement of\nperformance akin to the original model throughout the ﬁne-tuning proc ess. The incor-\nporation of LoRA into the domain of multimodality facilitates a considerabl e reduction\nin the dimensionality of image data, eﬀectively transforming intricat e high-dimensional\nimage data into simpliﬁed low-dimensional representations. Acknow ledging the poten-\ntial of information loss when deploying LoRA for multimodal applications, we utilize\nmultiple parallel LoRA instances as a mitigation strategy. It entails amalgam ating the\nultimate LoRA outputs to alleviate this inherent limitation.\nThe expansiveness of large models does not imply an equivalence to gen eral arti-\nﬁcial intelligence; instead, the limits of their application are an ongoi ng sphere of\nexploration. The pervasive trend of ﬁne-tuning models in speciali zed vertical domains\nhas emerged as an inexorable trajectory, driving the advancement of AI sc enarios.\nWithin this manuscript, our focus converges on forestry-associate d ailments and pests.\nLeveraging the ChatGLM large model as a foundational cornerstone, we embark u pon\na trajectory characterized by swift iterations within scenario-bas ed applications. In\ndoing so, we seamlessly infuse artiﬁcial intelligence into each face t of safeguarding\n4\nagainst forestry diseases and pests. This process bears semblance to the subtle culti-\nvation depicted in ”it moistens things ﬁnely, without a sound.” We pr opel the forestry\ndomain into a new epoch by employing AI-propelled computations, sign ifying an era\nof transformative change. Our primary emphasis resides in the progress ion of the\nChatGLM model through the integration of domain-speciﬁc Chinese data con cern-\ning forestry diseases and pests. This inclusion is intended to fac ilitate meticulous\nadjustment during the pre-training phase. The resultant shift ai ms to guide the founda-\ntional language model towards alignment with a reﬁned corpus speciﬁc to the forestry\ndomain.\nAmidst the backdrop of climate change, the gravity of forest pest and dise ase\ncalamities in China is escalating. The forestry ecosystem is confront ing unparalleled\ntrials, prompting an imperative need to advance automated and intellige nt pest and\ndisease detection technologies. These progressions are pivotal in guidi ng the biologi-\ncal and scientiﬁc strategies for preventing and managing forestry pest s and diseases,\nthereby safeguarding ecological integrity. Consequently, the proﬁci ent management\nof detrimental organisms within forestry has acquired paramount signiﬁc ance and\nimmediate attention. Certain pernicious entities exhibit decep tive characteristics in\ntheir external features despite harboring substantial peril. The forest-related biologi-\ncal catastrophes these detrimental organisms induce are frequently l ikened to ”latent\nforest conﬂagrations”. This analogy is drawn due to their shared resemblan ce with\nnatural disasters and the distinctive intricacies and protracted ch allenges involved in\ntheir management.\nTo better serve grassroots communities, cater to societal needs, an d align with the\npractical requirements of the general populace, we introduce our mod el, the ”Forestry\nPest and Disease Doctor”. This initiative aims to disseminate knowl edge related to\nforestry, promote awareness of forest pest and disease control, and serv e as an eﬀective\nvehicle for disseminating advanced prevention and control technologie s. Ultimately,\nour goal is to contribute to the furtherance of rapid and high-quality dev elopment in\nmodern forestry. Forestry stands as a highly signiﬁcant discipline that has evolved from\n”empirical forestry” to ”evidence-based forestry” and, now, to ”digi tal forestry”. With\nthe ﬂourishing advancement of artiﬁcial intelligence technologies, a new revolution is\nunderway in the scientiﬁc research paradigm. Therefore, large-scal e AI models known\nas ”agricultural brains” will play a pivotal role in the practical implem entation of pest\nand disease management solutions in the vertical domain of forestry. Fine -tuning in\nvertical domains will inevitably become a prevailing trend.\nIn this paper, we will introduce a multimodal large language model for f orestry\n(MLLM-For) in pest and disease recognition. The primary purpose of this m odel is to\nexpand the unadulterated language instruction model, ChatGLM, into a m ultimodal\narchitecture, thereby endowing LLMs with the capability for generat ive production\nin alignment with visual modalities. This model represents the i naugural and success-\nful initiative to incorporate multimodal instruction extension wi thin the purview of\nforestry biology. The objective is directed at the comprehensive tr aining of a forestry\nbio multimodal conversational assistant through an end-to-end approach, s treamlining\nits implementation across practical application contexts. The eﬃcacy of domain-\nspeciﬁc pre-training has been demonstrated to be pertinent for For estry Bio Natural\n5\nLanguage Processing (NLP) applications and Forestry Bio Visual Language (VL) tasks\nalike.\n• A multimodal Chinese-language forestry pest and disease (FPD) dataset is con-\nstructed. The dataset comprises forestry pest and disease images and their textual\ndescriptions.\n• A multimodal large language model for forestry ( MLLM-For) in pest and diseas e\nrecognition is proposed. It is an extension of the ChatGLM model using th e LoRA\nﬁne-tuning technique. Using Q-Former, it successfully transf orms from linguistic\nunimodality to image-linguistic multimodality.\n• Experiments shows that the proposed MLLM-For model outperforms ﬁve s tate-of-\nthe-art models, including VisualGLM [\n22], Ziya-BLIP2-14B-Visual [ 23], MiniGPT\n[24],VisCPM [ 25] and Qwen [ 26] models.\nOur model\n• Using Q-Former, a successful transformation from linguistic unimod ality to image-\nlinguistic multimodality has been realized.\nThe rest of this paper is organized as follows. In Section 2, we present related\nwork. Subsequently, we provide a detailed exposition of our primary contributions in\nSection 3. Finally, in Section 4, we compare our model with relevant models. Section\n5 oﬀers conclusions and outlines directions for future work.\n2 Related Work\n2.1 Language Models\nThe pre-training frameworks can be classiﬁed into three categories: autoregressive,\nautoencoding, and encoder-decoder models. Contemporary pre-trai ned language mod-\nels, built upon the Transformer architecture like the GPT series [\n27],[28],[29], BERT,\nand others, employ autoregressive Transformer models to pre-train e xpansive lan-\nguage models on extensive textual corpora. This practice demonstrates t heir prowess\nin few-shot learning capacities [ 28, 30]. Large Language Models (LLMs) have shown\nremarkable progress through training on extensive text corpora, gradually ﬁnding\nutility across diverse domains. The emergence of LLMs has initiated a te chnologi-\ncal paradigm shift, and a lineup of open-source large models, including LLaMA [ 9],\nBLOOM [ 31], and ChatGLM [ 8], has substantially propelled the advancement of the\nNatural Language Processing (NLP) ﬁeld. In contrast, ChatGLM is a bilingual c on-\nversational language model proﬁcient in accommodating Chinese and Englis h. Having\nundergone training involving around 1 trillion tokens in both languages, b olstered by\ntechniques like supervised ﬁne-tuning and self-feedback, th e ChatGLM model, boast-\ning 6.2 billion parameters, demonstrates the capacity to generate resp onses that closely\nalign with human preferences.\nBased on the ChatGLM model, we enhance LLMs with the capacity to capture\nimage features through ﬁne-tuning. This endeavor lays the foundation f or creating\nan open-source multimodal model. Within this study, we integrate d omain-speciﬁc\nknowledge of forestry diseases and pests into the ChatGLM model, the reby reorienting\n6\nthe foundational language model towards a dedicated corpus speciﬁc to th e ﬁeld of\nforestry.\n2.2 Vision-Language Models\nIn light of the emergence of expansive language models, scholarly invest igations\nhave been fervently delving into the application of LLMs for addressi ng multimodal\nchallenges [\n20],[32], thereby culminating in the conception of Multimodal Large Lan-\nguage Model (MLLM) [ 17],[33],[34],[19],[35],[36]. A variety of methodologies have\nentailed the infusion of visual data into LLMs and the meticulous reﬁne ment of\nthese models through instructional directives. This strategic augm entation has facil-\nitated their adeptness in generating textual content from visual inp uts and has been\nshown to improve the generalization of language models to unknown tasks . In recent\ntimes, an evident transition has transpired within the landscape of i mage-language\nresearch, wherein the focus has shifted from expansive language mode ls to substan-\ntial vision-language models. The Generative Pre-trained Transformer 4 (GPT-4) [ 17]\nhas impressively showcased its prowess by adeptly handling of inp uts originating from\ndiverse modalities, including images and text, fulﬁlling a wide spectrum of tasks. This\nexceptional adeptness has acted as a catalyst, giving rise to a fresh su rge of inves-\ntigation that extends the scope from singular language instruction models towards\nthe realm of multimodal instructional models. Analogous to the princip les behind\nLLaMA-Adapter [ 20], this emerging paradigm empowers LLMs with the faculty of\nvisual reasoning, culminating in the proposal of LLaMA-Adapter V2 [ 20]. Conversely,\nBLIP2 [ 37] capitalizes on integrating Q-Former to facilitate the mapping of acqui red\nimage representations onto the textual embedding domain of LLMs.\nIn the pursuit of cultivating a directive comprehension akin to th at exhibited by\nGPT-4, endeavors such as MiniGPT-4 [ 24] and LLaVA [ 18] have surfaced, embracing\nthe utilization of datasets focused on image-guided tracking to cultiv ate the capacities\nof image-guided tracking within LMMs. MiniGPT-4 [ 24] embarks upon a trajectory\nof pre-training, encompassing a corpus of 134 million image-text pairs, to establish a\nconnection between the static visual encoder and the LLM. This connec tion is sub-\nsequently reinforced through the ﬁne-tuning of the model using i mage-text datasets\nthat are well-aligned. LLaVA [ 18], in a similar vein, leverages the pairings of image\nand text to serve as a conduit for achieving congruence between visual models and\nLMMs. Video-Chat [ 32] facilitates further expansion of the realm of comprehension,\nwhich extends the boundaries of image encoders to empower expansiv e models with\nthe competence to decode the visual constituents embedded with in videos. Although\nthese methodologies have showcased commendable aptitude in compreh ending mul-\ntiple modalities, they require the adjustment of billions of mode l parameters and\nthe assiduous aggregation of substantial quantities of training data encompas sing\nmultiple modalities. This dataset is sourced from human annotations or ou tputs pro-\nduced by the OpenAI API. Furthermore, these models are predominant ly designed for\ngeneric domains and have yet to be ﬁne-tuned for the speciﬁc context of forestry pest\nmanagement, diminishing precision in their generated responses.\nOur endeavor is directed towards endowing foundational LLMs with the ability\nto comprehend visual attributes. In this context, our model intro duces an innovative\n7\nLoRA ﬁne-tuning strategy, encompassing the immobilization of parameter s inherent\nto the initial pre-trained model. Additionally, augmentation is achie ved by integrating\nan auxiliary matrix to replicate the comprehensive ﬁne-tuning of mo del parameters.\nThis strategic implementation curtails computational requirements and orchestrates\na gradual infusion of image-based visual attributes into the pre-exist ing ChatGLM\nmodel, facilitated by low-rank adaptive. The outcome is a model that s howcases robust\ngeneralization capacities. Moreover, existing antecedent models h ave yet to attain\nthe desired level of adeptness within forestry biology. Our model w ill demonstrate a\nhigh level of competitiveness in the forestry domain compared to prev ious multimodal\nmodels.\n2.3 Querying Transformer\nVisual and linguistic modalities represent two fundamental channel s human beings\napprehend and comprehend their external environment. The central predicament con-\nfronting image-language models revolves around the harmonious amalgamation of dat a\nfrom these heterogeneous modalities into a feature space that expansi ve language mod-\nels can eﬀectively apprehend. At present, the adoption of the Transf ormer architecture\nhas ascended as the predominant methodology in the realm of multimodal algor ithms\nfor achieving the harmonious integration of information derived from dive rse modal-\nities into a feature space intelligible to LLMs and streamlining the process of feature\nfusion, owing to its remarkable aptitude in this regard. A new visual -language repre-\nsentation learning framework, Align before Fuse (ALBEF) [\n38], has been introduced,\nintegrating multimodal contrastive learning into the domain of multi modal models.\nALBEF encompasses an image encoder, a text encoder, and a multimodal en coder. It\npresents a straightforward, end-to-end, and highly proﬁcient framew ork for acquiring\nvisual-language representation skills.\nAn enhanced iteration of ALBEF, Q-Former, has been introduced. Q-Forme r is a\nstreamlined model consisting of two transformer sub-modules. In contrast to ALBEF,\nthe most salient divergence within Q-Former lies in the integration of Learned Queries.\nThese Queries actively interact with image attributes through Cross -Attention and\ntextual attributes through Self-Attention. Derived from informati on in both modal-\nities, these Queries yield feature outputs of query length, irre spective of the visual\nBackbone’s scale, thereby substantially diminishing computational complexity. The\nimage transformer is predominantly dedicated to extracting visual fe atures, and the\nText Transformer encompasses the roles of text encoder and text deco der. Q-Former\nincorporates three distinct training tasks, namely Image-Text Contr astive Learning\n(ITC), Image-grounded Text Generation (ITG), and Image-Text Matching (ITM) [ 19].\nThese tasks collaboratively enable the extraction and fusion of features .\nBLIP-2 [ 37] eﬃciently utilizes frozen image encoders and frozen LLMs to achieve a\nrange of visual-language tasks, yielding improved performance while mi nimizing com-\nputational overhead. Drawing from the Q-Former framework advanced in t he BLIP-2\nmodel, InstructBLIP [ 39] presents an instruction-aware visual feature extraction\nmethod. Ultimately, the model performs better than GPT-4, attainin g cutting-edge\noutcomes across diverse tasks. Recent research also has highlighted th e potential of\nQ-Former in integrating audio-visual signals, denoted as Audio Q-Former [40]. Our\n8\nmodel employs a strategy that involves encoding and decoding images and text, fol-\nlowed by their fusion within the Q-Former framework. Q-Former exc els in extracting\nvisual representations that are most informative for textual content. S ubsequently, the\ncombined data is fed into a language model, ensuring the model’s adapt ability with\ndynamic adjustments and enhanced learning capabilities. This approac h aims to reﬁne\nthe training process for improved alignment.\n2.4 Low-Rank Adaptation\nFor large models, full ﬁne-tuning of all parameters of the retrained mo del becomes\ninfeasible, and ﬁne-tuning large models and large model deployment s is also infeasible\ndue to the massive number of parameters. The approach commonly used to ad apt pre-\ntrained models to multiple downstream tasks is ﬁne-tuning, but ﬁ ne-tuning involves\nupdating all parameters with the trained model. The Low-rank struct ure is widespread\nin machine learning, and many machine learning algorithms have some inhe rent low-\nrank structure [\n41],[42],[43]. Moreover, it is well known that for many deep learning\ntasks, especially those with heavily overparameterized neural ne tworks, the learned\nneural networks will have low-rank properties after training [ 44]. Some previous work\nhas even explicitly imposed low-rank constraints when training th e original neural\nnetworks [45],[46],[47],[48],[49]. However, it was found that none of these works consid-\nered low-rank updates to the frozen model to adapt to downstream task s. Therefore,\nLoRA [ 21] was proposed to indirectly train some dense layers in the neural ne twork by\noptimizing the rank-decomposition matrix of the dense layers as they change during\nadaptation while keeping the pre-trained weights constant.\nAs such, we will adopt LoRA’s adaptive strategy to enhance the eﬀectivene ss of\nthe LLM ﬁne-tuning for downstream tasks. It maintains high-quality mo del perfor-\nmance without introducing inference latency or reducing input s equence length. It\ncan maintain high-quality model performance without introducing in ference delays or\nreducing the length of input sequences. It also demonstrates its e xcellent capability\nin service deployment scenarios, achieving the goal of fast task switc hing by sharing\nmost of the model parameters. The framework successfully optimizes the performance\nby approximating global training, thus eﬀectively reducing the wast e of resources.\nIn achieving the best overall performance, LoRA cleverly employs at tention-related\nmatrices, including W Q and W V , while taking W K into account. Experimental evi-\ndence from a related study [ 21] shows that the utility of the top singular vector\ndirection is high when the matrix rank is set to 8, because the other di rections usually\ncontain most of the accumulated random noise during training. Therefore, during the\ntraining of the LoRA model, the rank is set to 8. The study shows that th e neural\nnetwork performs well when the underlying model concept has a low -rank structure.\nThe most signiﬁcant advantage of LoRA is that it is faster and uses less memory .\nTherefore, it can be run on consumer-grade hardware.\n9\n3 The Multimodal Large Language Model for\nForestry\n3.1 Descriptions of Pertinent Symbols and Parameters\nTable\n1 summarizes the pertinent symbols utilized in the multimodal lar ge language\nmodel for forestry (MLLM-For), accompanied by their respective eluc idations. In order\nto maintain a state of stability throughout the model training procedu re, the vec-\ntor dimensions resulting from the residual connections after the in put of image-text\npairs are consistently preserved. For the initially trained images, they can be system-\natically transcribed into a matrix array comprising n matrices, each delineated by\n197 feature column vectors denoted as Aj′\na = [ a\n′\n0, a\n′\n1, a\n′\n2, a\n′\n3, . . . , a\n′\ni, . . . , a\n′\n196]. Herein,\nAj′\na signiﬁes the amalgamation of all feature vectors associated with the j-th training\nimage. After applying residual connections, the initial training tex tual content can be\nprojected to generate a matrix table mirroring the images. Each attrib ute delineated\nwithin the images possesses equivalent dimensions. In this paradigm , each matrix is\nalso characterized by 197 text-based feature vectors, denoted as the augm ented matrix\nBj\na = [ b0, b1, b2, b3, . . . , b196], representing the text-based feature vectors correspond-\ning to the j-th image. Subsequently, an alignment procedure is executed betw een\nthe image-text vector pairs. Positive samples are maximized to achiev e optimal sim-\nilarity alignment, whereas negative samples undergo supplementary c ross-attention\nmechanisms for ﬁne-grained realignment. This supplementary step am pliﬁes the align-\nment eﬃcacy, enabling the model to furnish more precise response s throughout the\ntext-generation process.\nThe MLLM-For model embarks on the LoRA ﬁne-tuning of the foundational Visu-\nalGLM model. The training process employs image-text data about fores try pest\noccurrences. Throughout this process, pertinent parameters wit hin the image encoder\nand language model remain ﬁxed, while LoRA-associated parameters in both com -\nponents undergo reﬁnement. Simultaneously, relevant parameters of Q-Former also\nundergo tuning. As a result, the reﬁned multimodal large language model tailored\nfor forestry applications is equipped to tackle issues surrounding forestry pest infesta-\ntions. This development contributes to advancing research in the s pecialized forestry\ndomain, facilitated by integrating multimodal large language models.\n3.2 The Framework of the Proposed MLLM-For Model\nThe MLLM-For model comprises ﬁve principal constituents encompassi ng LoRA ﬁne-\ntuning training, image encoding, text encoding, ﬁne-grained hard samp le alignment\n(ﬁne-grained HSA), and answer testing, as shown in Figure\n1. During the LoRA\nﬁne-tuning training, the critical action involves freezing paramet ers within the image\nencoder and the extensive language model. Following this, Q-Former is utilized to\nestablish a connection between the image encoder and the frozen large l anguage model.\nGiven the suboptimal outcomes achieved with alternative ﬁne-tunin g methodologies,\nthis study exclusively adopts LoRA for ﬁne-tuning training. As a resu lt, the ﬁne-\ntuned parameters encompass LoRA-related parameters in both the image enco der and\n10\nTable 1 Symbols\nSymbols Symbol interpretation\nAj Vector matrix of image features\nBj Vector matrix of text features\nAj\na An augmented matrix formed by splicing the overall vector\nof the picture\nAj′\na Augmented matrix embedding location features\nBj\na Text augmentation matrix formed by splicing text topics\nAIS Fine-tuned image feature skill matrix\nBT S Coded text feature skill matrix\nHj Input description encoded feature matrix\nHj\na An augmented matrix formed by splicing text features\nin the text encoding stage\nHj′\na Augmented matrix formed by self-attention\nthe large language model and the pertinent parameters within Q-Former. This train-\ning process culminates in acquiring and retaining multimodal proﬁ ciencies within the\nMLLM-For Model. Notably, this training approach substantially curtails t he consump-\ntion of hardware resources throughout the training endeavor. During im age encoding,\nthe features of the image are subjected to representation learning, cu lminating in\nencoding image attributes into a feature vector. The primary object ive is to engage\nin comparative learning with the vector derived from the text encod ing phase and to\nperform cross-attention fusion analysis with subsequent components .\nIn the text encoding phase, image text descriptions are encoded to generate\na text vector. Subsequently, the vector dimensions undergo normal ization through\nresidual layers, aligning them with the dimensions of the image vect or to facilitate\nconvenient comparative learning. Within the Fine-grained HSA phase, an introduced\ncross-attention mechanism serves as the primary function, enabling the model to focus\nconcurrently on the feature sequences of an alternative image while pr ocessing the\ninput sequence. This endeavor is aimed at attaining a more meticulou s alignment of\nimage-text vectors. In the section dedicated to image description, t he process encom-\npasses the extraction of image vectors, succeeded by integrating cross -attention fusion\nwith vectors derived from the text encoder. This process culmi nates in the task of\ntext generation. During the answer testing phase, questions and image s are fed into\nour well-trained MLLM-For model. Subsequently, the new model del ivers exemplary\nresponses grounded in the acquired skills and input image attributes . The following\nsections will give a detailed elucidation of the ﬁve constituents of t he MLLM-For\nmodel.\n3.3 LoRA Fine-Tuning Training\nThe LoRA ﬁne-tuning training procedure can be delineated into two distinct stages,\nas shown in Figure\n2. During the initial stage, the primary emphasis lies on training\ninput images. The images undergo encoding, extracting features, and c onverting from\nmulti-dimensional representations into one-dimensional column ve ctors. This extrac-\ntion sequence proceeds from left to right and top to bottom, forming col umn vectors.\n11\nEncoder\nSelf Attention\nFeed Forward\n...a1 a2 a8 a9\nImage-Text\nContrastive \n...b1 b2 b3 bn\n一只名叫丝带凤蝶\n的蝴蝶······\n(A butterfly named Ribbon\nCrested Butterfly ......)\nImage\nEncoder\nFine-grained\nHSA\nAnswer\nTesting\n...b1 b2 b3 bn\nText Encoder\nQ-Former\nBi-Self Attention\nFeed Forward\nx N\nBi-Self Attention\nCross Attention\nFeed Forward\nFull\nConnected\n...a2a1 a8 a9\nImage-grounded\nText Encoder\nImage\nEncoder\nLoRA\nInput Image\nQ-Former ChatGLM LoRA\nb1 b2 b3 bn...\nInput Image\n...\n\"一只丝带凤蝶正停在一片树叶上。\"\n(A Sericinus montela is resting on a leaf.)\nfine-grained HSA\n一只黑色的蝴停在一\n片树叶上。\n(A black butterfly\nrests on a leaf.)\n这张照片展示了一只黑色的蝴蝶，它正坐在一片绿叶上。\n这只蝴蝶的体型非常小，只有几毫米大小。它的身体呈现\n出深棕色和黑色相间的颜色，给人一种神秘而优雅的感觉。\n翅膀的形状十分独特，具有独特的花纹和图案。这些花纹\n在光线下闪烁着迷人的光芒，仿佛一幅精美的艺术品。蝴\n蝶的翅膀是椭圆形的，边缘略微弯曲，与周围的树叶形成\n了鲜明的对比。\n（This photo shows a black butterfly that is sitting on a\ngreen leaf. This butterfly is very small, only a few\nmillimeters in size. Its body has a dark brown and black\ncolor that gives it a mysterious and elegant look. The\nshape of its wings was very unique, with distinctive\npatterns and motifs. These patterns shimmer charmingly\nin the light, as if they were a fine work of art. The\nbutterfly's wings are oval with slightly curved edges,\ncontrasting with the surrounding leaves. ）\n描述这张图的背景。\n（Describe the context of this image.）\nImage\nEncoder\n...\nfine-gained HSA\nImage-Text Alignment \nFull\nConnected\n...\n...\nInput Image\n这张照片的背景是一棵\n高大的绿绿的树木，\n树上有一些树枝。\n（In the background of\nthis photo is a tall green tree\nwith some branches on it.)\n N x\nQ-Former\nText\nEncoder\nFig. 1 The architecture of the MLLM-For model.\nEncoder\nSelf Attention\nFeed Forward\n...a1 a2 a8 a9\nImage-Text\nContrastive \n...b1 b2 b3 bn\n一只名叫丝带凤蝶\n的蝴蝶······\n...b1 b2 b3 bn\nText Encoder\nQ-Former\nBi-Self Attention\nFeed Forward\nx N\nBi-Self Attention\nCross Attention\nFeed Forward\nFull\nConnected\n...a2a1 a8 a9\nImage-grounded\nText Encoder\nImage\nEncoder\nLoRA\nInput Image\nQ-Former ChatGLM LoRA\nb1 b2 b3 bn...\nInput Image\n...\n\"一只丝带凤蝶正停在一片树叶上。\"\n(A Sericinus montela is resting on a leaf.)\nfine-gained HSA\n一只黑色的蝴停在一\n片树叶上。\n(A black butterfly\nrests on a leaf.)\n这张照片展示了一只黑色的蝴蝶，它正坐在一片绿叶上。\n这只蝴蝶的体型非常小，只有几毫米大小。它的身体呈现\n出深棕色和黑色相间的颜色，给人一种神秘而优雅的感觉。\n翅膀的形状十分独特，具有独特的花纹和图案。这些花纹\n在光线下闪烁着迷人的光芒，仿佛一幅精美的艺术品。蝴\n蝶的翅膀是椭圆形的，边缘略微弯曲，与周围的树叶形成\n了鲜明的对比。\n（This photo shows a black butterfly that is sitting on a\ngreen leaf. This butterfly is very small, only a few\nmillimeters in size. Its body has a dark brown and black\ncolor that gives it a mysterious and elegant look. The\nshape of its wings was very unique, with distinctive\npatterns and motifs. These patterns shimmer charmingly\nin the light, as if they were a fine work of art. The\nbutterfly's wings are oval with slightly curved edges,\ncontrasting with the surrounding leaves. ）\n描述这张图的背景。\n（Describe the context of this image.）\nImage\nEncoder\n...\nfine-gained HSA\nImage-Text Alignment \n Full\nConnected\n...\n...\nInput Image\n这张照片的背景是一棵\n高大的绿绿的树木，\n树上有一些树枝。\n（In the background of\nthis photo is a tall green tree\nwith some branches on it.)\nImage\nEncoder\n N x\nFig. 2 The ﬂow chart of the LoRA ﬁne-tuning training.\nEach column vector is denoted as ai, where i = 1 ,2,3, . . . ,196. The features of the j-\nth image are encapsulated within the feature matrix Aj = [ a1, a2, a3, . . . , ai, . . . , a196]\n(j = 1 ,2,3, . . . , n). Following the transformation of image vectors’ dimensions,\nthey concatenate with the comprehensive information vector a0, which maintains\nan equivalent dimensionality as that of the image feature vectors. This am algama-\ntion yields an argmented matrix Aj\na = [ a0, a1, a2, a3, . . . , ai, . . . , a196]. After this,\npositional feature embedding is implemented on the amalgamated feature vectors,\nwhich is succeeded by introducing a residual connection layer. This iterative proce-\ndure culminates in the generation of a novel augmented feature matrix, denoted by\nAj′\na = [ a\n′\n0, a\n′\n1, a\n′\n2, a\n′\n3, . . . , a\n′\ni, . . . , a\n′\n196]. Subsequently, it is subject to a normalization\nprocess.\nDuring the training process, the parameters in the graph encoder ar e frozen and\nﬁne-tuned in conjunction with LoRA. This process involves subject ing the image fea-\ntures to processing by both the image encoder model and LoRA, resulti ng in the estab-\nlishment of the image feature skills matrix AIS = [ A1′\na , A2′\na , A3′\na , . . . , Aj′\na , . . . , A196′\na ].\nAfter this step, a phase of comparative learning between Q-Former and t he image-\ntext pairs ensued, aligning them with the text vectors. In this con text, the text vector\nmatrix Bj encapsulates the textual description of the features for the j-th image, for-\nmulated as Bj = [ b1, b2, b3, . . . , b196]. It is followed by the concatenation with the topic\n12\nvector b0, ensuring its dimensionality matches the text vectors. Conseque ntly, this pro-\ncess forms the augmented text matrix Bj\na = [ b0, b1, b2, b3, . . . , b196]. In Q-Former, text\nencoding leads to the formation of text features, resulting in a tex t feature skills matrix\nBT S = [ B1′\na , B2′\na , B3′\na , . . . , Bj′\na , . . . , Bn′\na ]. The skills matrices align within Q-Former,\nemploying cosine similarity for the alignment analysis. Subsequen tly, the aligned vec-\ntor features are fed into the ChatGLM language model for training. Throughout this\ntraining process, the parameters of the ChatGLM language model remain ﬁx ed while\nﬁne-tuning is carried out in conjunction with LoRA. The mathematical pr ocedure for\nthe LoRA ﬁne-tuning is delineated as follows.\nW = Wpm + tWLoRA = Wpm + tELoRA−zeros × FLoRA−gaussian, (1)\nwhere t is a random variable with an absolute value not exceeding 1, and W , Wpm, and\nWLoRA represent the weight matrices of the trained model, the frozen mod el, and the\nLoRA ﬁne-tuning process, respectively. During the model train ing, the FLoRA−gaussian\nmatrix is initialized using a normal distribution, while the ELoRA−zeros matrix is\ninitialized with zeros. It ensures that the bypass of the frozen mod el remains a zero\nmatrix at the beginning of the training process.\nIn the process of ﬁne-tuning LoRA, when LoRA is applied to the mapping m atrices\nW of attention’s Query and Value, the ﬁne-tuning eﬀect is further en hanced. The cal-\nculation process for the weights of the Query and Value mapping matrice s in attention\nis illustrated as follows.\nW Q = W Q\npm + tW Q\nLoRA, (2)\nW V = W V\npm + T WV\nLoRA. (3)\nWhen ﬁne-tuning LoRA and passing through the multi-head self-attent ion layer\nfor a training image-text data Y , the corresponding mapping produces calculation\nformulas for the Query matrix Q, Key matrix K, and Value matrix V . These formulas\nare shown below.\nQuery : Y × W Q = Y × W Q\npm + tY × W Q\nLoRA, (4)\nKey : Y × W k = Y × W k\npm + tY × W k\nLoRA, (5)\nV alue: Y × W V = Y × W V\npm + tY × W V\nLoRA. (6)\nWhen Softmax is utilized, the computational inference for matrices Q and K with\nLoRA layers can be expressed as follows.\nSoftmax(Q, KT ) = softmax( Y WQ\npm(W k\npm)T Y T + tY WQ\npm(W k\nLoRA)T Y T\n+tY WQ\nLoRA(W k\npm)T Y T + t2Y WQ\nLoRA(W k\nLoRA)T yT\n)\n.\n(7)\nThe ﬁnal attention calculation can be represented as follows.\nHead = softmax(Q, KT )Y WV\npm + T × softmax(Q, KT )Y WV\nLoRA. (8)\n13\nAfter undergoing ﬁne-tuning with LoRA training, a ﬁnal image-text unde rstanding\nskill matrix is formed as C = [ AIS , BT\nT S]. The corresponding understanding skills are\nstored in the newly trained model, achieving the LoRA ﬁne-tuning p rocess.\n3.4 Image Encoding\nThe image encoding process commences with the initial segmentation of input images\ninto smaller blocks, each signifying a distinct feature of the image . All images are\nuniformly divided into 14 × 14 blocks. The feature of each diminutive block is then\nembedded to formulate a comprehensive feature vector. Subsequen tly, a residual con-\nnection mapping is executed through a self-attention layer. The pr ocess further engages\nin residual connection mapping through a feedforward network, creatin g a feature col-\numn vector, denoted as d\n′\ni, where i = 1 ,2,3,· · ·,196. In this context, d\n′\n0 represents the\namalgamated vector encompassing the entirety of the image’s information alon g with\npositional data. These feature vectors from the j-th image eventually amalgamate to\nform a freshly augmented feature matrix Dj′\na = [ d\n′\n0, d\n′\n1, d\n′\n2, d\n′\n3, . . . , d\n′\ni, . . . , d\n′\n196]. The\nvectors contained within this feature matrix undergo alignment with subsequent-stage\ntext encoding vectors. This alignment process is implemented t o prevent the occur-\nrence of erroneous correlated alignments. To achieve this, the featur e vectors of the\nimages undergo a fusion analysis using a cross-attention mechanism in c onjunction\nwith the text. The primary objective is to validate further wheth er the image and\ntext convey identical information. This approach aims to minimize align ment errors\nthroughout the process. Additionally, these features oﬀer an augmented representa-\ntion of feature vectors for text generation within the model. Through th e process of\ncross-attention fusion, it contributes to the generation of enhanced t ext descriptions.\nConsequently, this optimization ensures the best expression of th e training eﬀect of the\nmodel in text generation, yielding answers that more precisely align with the desired\noutcomes.\n3.5 Text Encoding\nInitially, the textual description of the image is inputted. Durin g text encoding, the\nthematic content from the image description is assimilated and transfor med into a\nvector representation mirroring the dimensionality of the image enco ding. This vec-\ntor is designated as h0. The text gives rise to a vector representation, denoted by hi\n(i = 1 ,2,3, . . . ,196). The complete j-th image description is aggregated into a feature\nmatrix of text descriptions, denoted as Hj = [ h1, h2, h3, . . . , hi, . . . , h196]. Ultimately,\nthis feature matrix is merged with the thematic vector, resulting in a text augmented\nmatrix Hj\na = [ h0, h1, h2, h3, . . . , h196]. After encoding, a residual connection is estab-\nlished through a self-attention mechanism within the encoding se ction. A residual\nconnection is created through a feedforward network involving the vec tor before the\nfeedforward network layer. This procedure results in the generat ion of a novel aug-\nmented feature matrix for the text, denoted as Hj′\na = [ h\n′\n0, h\n′\n1, h\n′\n2, h\n′\n3, . . . , h\n′\nj , . . . , h\n′\n196].\nIt promotes facilitative analysis in comparative learning. In comparative learning,\n14\ncosine similarity analysis is employed as follows.\ncos\n⟨\nd′\ni, h′\nj\n⟩\n=\n− →d′\ni ·− →h′\nj\n⏐\n⏐\n⏐\n− →d′\ni\n⏐\n⏐\n⏐\n⏐\n⏐\n⏐\n− →h′\nj\n⏐\n⏐\n⏐\n(9)\nwhere − →d′\ni and − →h′\nj (i, j∈ (1,2,3, . . . ,196)) represent the i-th feature vector of an image\nand the j-th feature vector of the corresponding text, respectively. This calculation\nassesses the similarity between text and image features, where the closer the value of\ntheir similarity is to 1, the more they are similar. Nonetheless, mis alignments with\nnegative samples can arise in the context of similarity contrastive lear ning. Conse-\nquently, the model undertakes additional measures to address such challenges through\nﬁne-grained Hard Sample Alignment.\n3.6 Fine-Grained Hard Sample Alignment\nIn the ﬁne-grained Hard Sample Alignment process, we commence with th e encod-\ning of the j-th text to generate a matrix of text feature vectors, denoted as\nHj = [ h1, h2, h3, . . . , hi, . . . , h196], where hi represents the i-th content feature vec-\ntor of the text. Furthermore, the encoding process is separately ap plied to the\nthematic content of the text, resulting in a singular feature vector k0. Ultimately,\nthe feature vector corresponding to the text’s theme is concatenate d with the\nmatrix of text feature vectors to create a new augmented feature matrix , denoted\nby H\n′\nj = [ k0, h1, h2, h3, . . . , hi, . . . , h196]. This matrix undergoes additional cross-\nfusion and alignment with the previously inputted image feature vect ors. The goal\nis to achieve a ﬁner-grained matching between text and images, ensur ing that\nthe maximal correlation between positive and negative samples is lever aged during\nthe matching process. In the image description phase of the model, t he aug-\nmented matrix formed by the feature vectors of the images, denoted by Dj′\na =\n[d\n′\n0, d\n′\n1, d\n′\n2, d\n′\n3, . . . , d\n′\ni,· · ·, d\n′\n196], is synchronized with the inputted text feature matrix\nand subsequently input into the language model. Based on the feature v ectors of the\n15\nimages, the language model elaborates on the textual inference descript ion. Algo-\nrithm 1 outlines the procedure of encoding images to serve as prompts for the model.\nAlgorithm 1: Image encoding and model prompt process\nInput: Image vector feature matrix Dj′\na and corresponding text feature\nvector matrix Hj′\na\nOutput: Model’s description of the picture\n1 function: Image feature matrix Dj′\na and text feature vector matrix Hj′\na\nembedding alignment\n2 initialize Dj\na, Hj\na\n3 for − →di in Dj\na do\n4 for − →hi in Hj\na do\n5\n− →di : Count the ﬁrst section on relevant features.\n6\n− →hi : Count corresponding features in the text.\n7 end\n8 Dj\na.append(− →di )\n9 Hj\na.append(− →hi )\n10 end\n11 initialize Dj′\na , Hj′\na\n12 repeat\n13 for − →di in Dj\na\n14 for − →hi in Hj\na\n15 after residual computation to obtain d\n′\ni, h\n′\ni\n16 vector alignment using cosine similarity\n17 until convergence\n18 using ﬁne-grained hard sample alignment\n19 return Dj′\na and Hj′\na embedding aligned feature vectors.\n20 end function\n3.7 Answer Testing\nFigure\n3 presents a comprehensive overview of the answer testing phase. T he pre-\ntrained MLLM-For model is initially supplied with the test image dur ing answer\ntesting. The image undergoes encoding through the image encoder, res ulting in the\ngeneration of image feature vectors. These feature vectors, produced i n the answer\ntesting phase, are denoted as jk for k = 0 ,1,2, . . . ,196. The individual features\nare then aggregated to construct the feature matrix for the image, represe nted as\nJ = [ j0, j1, j2, . . . , jk, . . . , j196]. At the same time, inquiries aimed at extracting infor-\nmation from the image are inputted. These queries are encoded to form a question\nvector. Subsequently, the image feature matrix and the question vec tor are fed into the\nQ-Former. A fresh vector matrix is created by extracting image-text information vec-\ntors. The dimensions of the feature matrix are then adjusted via a ful ly connected layer.\nThis adapted matrix is subsequently input into the language model, w hich proceeds\n16\nEncoder\nSelf Attention\nFeed Forward\n...a1 a2 a8 a9\nImage-Text\nContrastive \n...b1 b2 b3 bn\n一只名叫丝带凤蝶\n的蝴蝶······\n...b1 b2 b3 bn\nText Encoder\nQ-Former\nBi-Self Attention\nFeed Forward\nx N\nBi-Self Attention\nCross Attention\nFeed Forward\nFull\nConnected\n...a2a1 a8 a9\nImage-grounded\nText Encoder\nImage\nEncoder\nLoRA\nInput Image\nQ-Former ChatGLM LoRA\nb1 b2 b3 bn...\nInput Image\n...\n\"一只丝带凤蝶正停在一片树叶上。\"\n(A Sericinus montela is resting on a leaf.)\nfine-gained HSA\n一只黑色的蝴停在一\n片树叶上。\n(A black butterfly\nrests on a leaf.)\n这张照片展示了一只黑色的蝴蝶，它正坐在一片绿叶上。\n这只蝴蝶的体型非常小，只有几毫米大小。它的身体呈现\n出深棕色和黑色相间的颜色，给人一种神秘而优雅的感觉。\n翅膀的形状十分独特，具有独特的花纹和图案。这些花纹\n在光线下闪烁着迷人的光芒，仿佛一幅精美的艺术品。蝴\n蝶的翅膀是椭圆形的，边缘略微弯曲，与周围的树叶形成\n了鲜明的对比。\n（This photo shows a black butterfly that is sitting on a\ngreen leaf. This butterfly is very small, only a few\nmillimeters in size. Its body has a dark brown and black\ncolor that gives it a mysterious and elegant look. The\nshape of its wings was very unique, with distinctive\npatterns and motifs. These patterns shimmer charmingly\nin the light, as if they were a fine work of art. The\nbutterfly's wings are oval with slightly curved edges,\ncontrasting with the surrounding leaves. ）\n描述这张图的背景。\n（Describe the context of this image.）\nImage\nEncoder\n...\nfine-gained HSA\nImage-Text Alignment \nFull\nConnected\n...\n...\nInput Image\n这张照片的背景是一棵\n高大的绿绿的树木，\n树上有一些树枝。\n（In the background of\nthis photo is a tall green tree\nwith some branches on it.)\nImage\nEncoder\n N x\nQ-Former\nFig. 3 Answer Testing Formation.\nto respond to the presented questions. Throughout this response pr ocess, the language\nmodel generates answers based on the feature information extracted from the input\nimage, resulting in a textual representation. Algorithm 2 provides the pseudo-code\noutlining the pertinent process.\nAlgorithm 2: Answer Testing\nInput: Image feature vector matrix J and problem vector matrix for the\ndesired problem\nOutput: Modelling targeted responses to questions\n1 function: Alignment of the image feature matrix J with the desired problem\nvector\n2 initialize J\n3 for ∀− →\nJkinJ do\n4 for problem vector in problem matrixdo\n5\n− →jk : Count Relevant features of responses to questions\n6 problem vector: Count computing key features in problem vectors\n7 end\n8 J.append( − →jk )\n9 problem matrix.append (problem vector)\n10 end\n11 Alignment of features of computational questions with picture response s\n12 Extract the required response feature vectors to pass to the language m odel\n13 Language modeling for targeted responses to questions asked\n4 Experiments\n4.1 Dataset\nDue to the absence of multimodal forestry-related datasets for model training, we\nconstruct a multimodal Chinese-language forestry pest and disease (F PD) dataset\ncomprising forestry pest and disease images and their textual descri ptions. To facilitate\ncomprehensive learning from the images, 3, 4, or more relevant questions were gen-\nerated for each image. The answers chieﬂy involve the image’s presente d content, the\nfeatured species, and the morphological attributes of said species. Th e FPD dataset\nencompasses approximately 4620 color forestry pest and disease images conce rning\n17\n豆天蛾\n(Clains) 红天蛾\n(Deilephila\nelpenor)\n蓝美凤蝶\n(Papilio\nprotenor\nCramer)\n星天牛\n(Anoplophora\nchinensis)\n柳天蛾\n(Smerinthus\nplanus\nWalker)\n碧凤蝶\n(Papilio\nbianor)\n菜蝽\n(Scopoli)\n幼虫(Larva)\n名字(name)\n危害(Jeopardize)\n寄主(Host)\n外形(Shapes)\n防治方法\n(Preventive Measures)\n分布(Distributions)\n成虫(Adult)\n幼虫(Larva)\n科(Family)\n纲(Class)\n别名(Nickname)\n类(Subclass)\n别名(Nickname)\n名字(name)\n危害(Jeopardize)\n纲(Class) 科(Family)\n外形(Shapes)\n防治方法\n(Preventive Measures)\n寄主(Host)\n分布(Distributions)\n纲(Class)\n分布(Distributions)\n目(Order)\n类(Subclass)\n外形(Shapes)\n别名(Nickname) 名字(name)\n分布(Distributions)\n生活习性(Habits)\n防治方法\n(Preventive Measures)\n名字(name)\n类(Subclass)\n科(Family)\n目(Order)\n分布(Distributions)\n生活习性(Habits)\n危害(Jeopardize)\n防治方法\n(Preventive Measures)\n外形(Shapes)\n蛹(Pupa)\n卵(Egg)\n名字(name)\n成虫(Adult)\n别名(Nickname)\n科(Family)\n危害(Jeopardize)\n分布(Distributions)\n别名(Nickname)\n幼虫(Larva)\n外形(Shapes)\n蛹(Pupa)\n防治方法\n(Preventive Measures)\n危害(Jeopardize)\n科(Family)\n分布(Distributions)\n纲(Class)\n名字(name)\n目(Order)\n名字(name)\n目(Order)\n界(Kingdom)\n纲(Class)\n别名(Nickname)\n防治方法\n(Preventive Measures)\n幼虫(Larva)\n分布(Distributions) 蛹(Pupa)\n分布(Distributions)\n生活习性(Habits)\n危害(Jeopardize)\nData\n杨扇舟蛾\n(Clostera\nanachoreta\nDenis)\n类(Subclass)\n名字(name)\n别名(Nickname)\n纲(Class)\n目(Order)\n门(Phylum)\n界(Kingdom)\n蛹(Pupa)\n防治方法\n(Preventive Measures)\n危害(Jeopardize)\n外形(Shapes)\n防治方法\n(Preventive Measures)\n. . .\n. . .\n. . .\n. . .\n. . .\nFig. 4 Overview of the dataset.\n80 insect categories. This dataset includes information on pest categoriz ation, tem-\nporal features, geographical distribution, damage caused, and pest control m ethods,\nas shown in Figure 4. To facilitate comprehensive learning from the images, 3, 4, or\nmore relevant questions were generated for each image. The answers chie ﬂy involve\nthe image’s presented content, the featured species, and the morph ological attributes\nof said species.\nTo bolster the base model’s conversational adeptness in forestry pes ts and diseases,\nwe collect around 50,000 entries concerning various concepts within fore stry pests and\ndiseases. These entries are meticulously selected to constitu te a pre-training corpus.\nAdditionally, we perform web scraping to extract content related to f orestry pests and\ndiseases from Baidu Baike, incorporating it as an extra reservoir for our pre-training\ncorpus.\n4.2 Comparison Models\nWe evaluate ﬁve leading open-source Chinese MLLM models, including VisualGLM-\n6B, Ziya-BLIP2-14B-Visual, MiniGPT, VisCPM, and Qwen-VL, as follows:\n• VisualGLM-6B [\n22] is an open-source multimodal conversational language model\nthat supports images, Chinese, and English. The language model is based on\nChatGLM-6B, with a total of 6.2 billion parameters. The visual component is estab-\nlished by training the BLIP2-Qformer, bridging the gap between the v isual and\nlanguage models, resulting in a combined model with a total of 7.8 billi on parame-\nters. Pre-training is conducted on 30 million high-quality Chinese text-image pairs\n18\nfrom the CogView dataset and 300 million carefully curated English text-im age\npairs.\n• Ziya-BLIP2-14B-Visual [23] is crafted by the ”Fengshen List” research team\nthrough the process of training on a subset of meticulously curated hi gh-quality\ndata sourced from open-access datasets. Ultimately, the Ziya-Visual mod el is fash-\nioned using an extensive corpus of around 20 million high-ﬁdelity data i nstances\ndesignated for training.\n• MiniGPT [24] originates from the King Abdullah University of Science and Tech-\nnology. The team integrates a static visual encoder (Q-Former & ViT) wit h an\nimmobile, extensive-scale text generation model, resulting in t he development of\nMiniGPT-4.\n• VisCPM [25] is a family of open-source large multimodal models that support mul-\ntimodal conversational capabilities (VisCPM-Chat model) and text-to- image gen-\neration capabilities (VisCPM-Paint model) in both Chinese and Englis h, achieving\nthe state-of-the-art performance among Chinese open-source multimo dal models.\nVisCPM is trained based on the large language model CPM-Bee with 10B parame-\nters, fusing visual encoder (Muﬃn) and visual decoder (Diﬀusi on-UNet) to support\nvisual inputs and outputs. Thanks to the good bilingual capability of CP M-Bee,\nVisCPM can be pre-trained with English multimodal data only and well ge neralized\nto achieve promising Chinese multimodal capabilities.\n• Qwen-VL [\n26] is a large-scale Vision Language Model (LVLM) developed by\nAlibaba Cloud. Qwen-VL can take images, text, and detection boxes as inpu t, and\nproduce text and detection boxes as output. The distinguishing fe atures of the\nQwen-VL series models oﬀer advanced capabilities in ﬁne-grained rec ognition and\nunderstanding with robust performance.\n4.3 Experimental Results\nExtensive experiments are conducted on our benchmark. All comparativ e experiments\nare performed on an NVIDIA A100 GPU. We set the parameters for beam search to 1,\nset the temperature to 0.8, and set the top p to 0.4. For experimental testing, including\nthe cognitive task, question and answer, count task, stages cognitive rec ognition, and\ncommon sense reasoning, we select images from our FPD dataset and non-dataset\nimages for experiments.\n4.3.1 Cognitive Task\nWe assess to ascertain whether MLLMs can engage in advanced logical reasoning f ol-\nlowing the perception of visual stimuli. For MLLMs to deduce accurate responses, they\nmust adhere to instructions, comprehend the contents of visual sti muli, align text with\nimages, and draw upon knowledge from LLM. This task presents a more formid able\nchallenge compared to a singular perceptual assignment. The application of MLLMs\nto address specialized issues necessitates a profound exploration of this domain. It\nencompasses identifying fundamental problems and determining sp eciﬁc data essen-\ntial for resolving them, a ”modal completion” process. Prompt ident iﬁcations of pest\nand disease types and categories in forestry pests and diseases are paramou nt. Early\n19\ndetection and warning, facilitated through vigilant monitoring and swift identiﬁca-\ntion, enable the timely revelation of pests and diseases, the issuanc e of alerts, and\nthe implementation of control measures to contain their propagation. Swif t and accu-\nrate identiﬁcations pinpoint the pest or disease types, facilitati ng targeted control\ninterventions.\nBecause many AI image recognition datasets are cultivated within Petri di shes, the\nmodel settings are only conducive to real-world production require ments. As such, we\nrandomly select 100 images from AI image recognition datasets and research libr aries\non agricultural pests and diseases from the IDADP dataset 1. MLLMs are tasked with\ndiscerning the species depicted in each image. AI insect recogniti on entails a conven-\ntional object detection task, primarily discerning the positions and classiﬁcations of\nseven types of insects within monochromatic containers.\nPerception is a fundamental capability of MLLMs, and its absence may lead to\nperceptual illusions. In this experiment, a single-turn dialogue format was employed,\nwherein the model was directly presented with an image and the promp t: ”What\nspecies is shown in the picture?”. Figure 5 depicts the outcomes of the baselines\nand MLLM-For model. Within this task, none of the three comparative mode ls\ncould deﬁnitively ascertain the species depicted in the image. Notab ly, VisualGLM-6B\ndemonstrates a laudable proﬁciency in scene depiction. It precise ly delineates species\npatterns, spots, and the ambient milieu. Particularly striking are its portrayals of the\nimage’s backdrop, the habitat of the depicted animal, and ﬁner features s uch as anten-\nnae, stripes, and spots on the animal. However, VisualGLM-6B occasionally r efrains\nfrom answering queries or provides proverbial responses. The Ziy a-BLIP2-14B-Visual\nmodel could discern the image’s content but could not specify the ex act species’ name.\nSimilarly to VisualGLM-6B, MiniGPT could render an account of the overal l environ-\nment and provide certain details about the insects. Nevertheless, MiniGPT exhibits\nlimited proﬁciency in adapting to Chinese; it responds in English to Chinese prompts,\nshowing relatively superior performance when queries in English . The MLLM-For\nmodel exhibits superior performance in this task. While it manages t o identify some\nspecies, a discernible gap exists compared to seasoned human expert s on an aggregate\nscale. Our model accommodates questions in Chinese and furnishes c orresponding and\naccurate responses. Furthermore, upon further examination, we disc over that when we\npersistently inquire: ”What is presented in the picture?”, the model capably generates\ncorresponding descriptions for the provided image, encompassing par ticulars such as\nantenna length, the insect’s developmental and growth stages, distr ibution regions,\nand more.\nFor the task of species recognition shown in Table 2, VisualGLM-6B, Ziya-\nBLIP2-14B-Visual, MiniGPT, VisCPM, and Qwen-VL exhibit minimal capabil ity in\nidentifying the species in the images. Our MLLM-For model outperfor ms these coun-\nterparts in this task. However, when it manages to identify some spec ies, there is still\na discernible gap compared to experienced human experts on an overall basis.\n1https://www.heywhale.com/mw/dataset/63e50cfea2c1716e14fb9db6\n20\nOurs\n这张照片上共有两只。\n(There are two of them\nin this photo.)\n图片上展示的是什么物种？\n（What species is shown in the picture?）\n这张图片中所示的物种叫黑点虎斑蝶，属于鳞翅目、虎斑蝶科的一种昆虫。别名：红\n凤蝶、黑缘凤蝶、黑点虎斑蝶等。\n（The species in this picture is called the Black-spotted Tiger Butterfly, an insect belonging to the Lepidoptera and Tiger \nButterfly families. Alias: Red Crested Butterfly, Black-margined Crested Butterfly, Black-spotted Tiger Butterfly, etc.）\n这只蝴蝶是橙色和黑色的，身体呈棕色。它坐在一片绿色的叶子上，叶子的末端有一\n些白色粉末。\n（This butterfly is orange and black with a brown body. It was sitting on a green leaf that had some white powder on the \nend of it.）\n图像上展示的是一只蝴蝶。\n（The image shows a butterfly.）\nThe image shows a butterfly sitting on a leaf in a green background. The butterfly has orange and black markings on \nits wings and is perched on a leaf with small white flowers on it. The butterfly’s body is brown and its antennae are white. \nThe background is made up of green leaves and small white flowers.\n光肩星天牛幼虫体长毫米，乳白色，无足；前胸背板有凸形纹，蛹体长毫米；裸蛹，黄白色。成虫，\n体黑色，有光泽，触角鞭状自第三节开始各节基部呈灰蓝色。前胸两侧各有个刺状突起，鞘翅上各有\n大小不等的由白色绒毛组成的斑纹个左右。\n(Light-shouldered stargazer larvae are millimeters long, creamy-white, and without legs; the dorsal plate of the \nprothorax has a convex pattern, and the pupa is millimeters long; naked pupa, yellowish-white. Adult, body black, glossy, \nantennae whip-like from the third segment onwards the base of each segment is gray-blue.There is a spiny protuberance \non each side of the prothorax, and the sheathed wings each have about one spot of varying sizes composed of white \ndowny hairs.）\n图像上显示了一只美丽的蝴蝶坐在绿色植物上。\n（The image shows a beautiful butterfly sitting on a green plant.）\n图片中有一只橙色的蝴蝶停在了树叶上，翅膀上有黑色的斑点。\n（The picture has an orange butterfly resting on a leaf with black spots on its wings.）\n图中处于什么阶段？\n（What stage is the diagram in?）\n这张照片展示了一只光肩星天牛成虫，它的体长为49~61毫米。\n(This photo shows an adult light-shouldered stargazer, which is 49 to 61 millimeters long.）\n我们可以推断出图中展示的是光肩星天牛的成虫。\n(We can deduce that the picture shows the adult of the light-shouldered stargazer.）\n我们可以推断出图中展示的是光肩星天牛的成虫。\n(We can deduce that the figure shows the adults of the light-shouldered stargazer.）\n这个图片显示了一个昆虫，它是一种昆虫。\n(This image shows an insect which is an insect.）\n图中处于什么阶段？\n(What stage is the diagram in?）\n图中处于幼虫阶段。\n(The figure is in the larval stage.）\nQ:这张图上有几只昆虫？\n(How many insects are in this picture?)\n这张图片展示了两只动物在\n草地上跳跃。\n(This picture shows two\nanimals jumping in a grassy\nfield.）\nVisualGLM\n这张图上有两只小昆虫。\n(There are two small\ninsects in this picture.）\nMinigpt\n这张图上有几只昆虫？\n(How many insects are \nin this picture?)\nVisCPM\n Qwen-VL\nZiya\n这张图上有两只昆虫。\n(There are two insects in\nthis picture.）\n有两只小动物。\n（There are two little\nanimals.）\nOurs\nVisualGML\nVisualGML\nZiya\nZiya\nOurs\nMiniGPT\nMiniGPT\nVisCPM\nVisCPM\nQwen\nQwen\nFig. 5 Cognitive Task Experiment Comparison Chart. In this ﬁgure, VisualGLM-6B, Ziya-BLIP2-\n14B-Visual, and Qwen-VL are shortened to VisualGLM, Ziya, a nd Qwen, respectively (The model\nabbreviations of the following ﬁgures are the same as those i n this ﬁgure).\nTable 2 The Accuracy Scores of\nEvaluation Models in Cognitive Task.\nModel Acc\nVisualGLM-6B 0.13\nZiya-BLIP2-14B-Visual 0\nMiniGPT 0\nViscpm 0\nQwen-VL 0\nMLLM-For (Ours) 0.21\n4.3.2 Question and Answer\nThe recommendations for preventing and controlling forestry pests an d diseases should\nexhibit a scientiﬁc and rational foundation. They will furnish inval uable guidance for\npractical forestry production, mitigating the environmental reper cussions of prevention\nand control measures, reducing costs, and bolstering economic eﬃcien cy. Further-\nmore, they will direct the reﬁnement and advancement of prevention and control\ntechnologies, establishing a robust framework for incorporating multi modal models in\nthe vertical domain. Conversations concerning the prevention and cont rol of forestry\npests and diseases play a pivotal role in large-scale models. We curate 100 forestry\nprevention and control questions from pertinent professional literat ure. The model\nundergoes assessment through a multi-turn dialogue, inquiring about its approach to\ncontrolling speciﬁc species. The experimental outcomes are depic ted in Figure\n6. In\nthis endeavor, we employ GPT-4 for answer evaluation. Notably, even GP T-4 cannot\n21\n图片上展示的是什么物种？\n（What species is shown in the picture?）\n......\n虎斑蝶的防治方法有哪些？\n（What are some ways to control tiger butterflies?）\n虎斑蝶是一种重要的害虫，其主要危害对象是树木和花卉。以下是一些常用的防治方法:\n1.农业措施：种植抗虫植物，如柑橘、茶树等，可以减少虎斑蝶的分布和繁殖机会；合理施肥和管理，促进植物生长，\n提高植物的抗性。\n2.物理控制：使用捕食工具或粘板捕捉幼虫，可以有效减少虎斑蝶的数量。\n3.化学防治：可以使用含有苯丙胺（PBO)或其他有害化学物质的喷雾剂进行喷洒，杀死幼虫和蛹。\n4.药剂熏蒸：使用烟雾机燃烧器将熏蒸剂加热并释放到空间中，以杀死幼虫和蛹。这种方法适用于大规模的虫害控制。\n(The Tiger Butterfly is an important pest that mainly affects trees and flowers. The following are some common control methods.\n1. Agricultural measures: Planting insect-resistant plants, such as citrus and tea trees, can reduce the distribution and \nreproduction opportunities of tiger butterflies; rational fertilization and management to\nPromote plant growth and improve plant resistance.\n2. Physical control: Using predatory tools or sticky boards to catch larvae can effectively reduce the number of tiger butterflies.\n3. Chemical control: Sprays containing phenylbutazone (PBO) or other harmful chemicals can be used to kill larvae and pupae.\n4. Pharmaceutical fumigation: Fumigants are heated and released into the space using smoke machines or burners to kill \nlarvae and pupae. This method is suitable for large scale pest control.)\n虎斑蝶是一种广泛分布的蝴蝶，其防治方法包括：\n1.农业控制：通过使用抗虫农药、生物农药或化学农药来杀死害虫。这些农药可以用于植物和昆虫之间的接触区域，从\n而有效地减少害虫的数量。\n2.物理防御：安装防虫或其他捕食性陷阱，以捕捉并防止它们从栖息地中逃脱。\n3.人工管理：利用各种诱饵吸引野生个体，然后将它们引入种植区。这有助于降低种群数量。\n4.环境改善：加强植被覆盖率增加土壤湿度促进土壤微生物的生长，从而减少护虎斑蝶所需的食物来源。\n(The Tiger Butterfly is a widely distributed butterfly and its control methods include:\n1. Agricultural control: killing the pests through the use of insect-resistant pesticides, biopesticides or chemical pesticides. \nThese pesticides can be used in the contact area between plants and insects, thus effectively reducing the number of pests.\n2. Physical defense: Installation of insect-proof or other predatory traps to catch and prevent them from escaping from their \nhabitats.\n3. Artificial management: using various baits to attract wild individuals and then introducing them to the planted area. This helps \nto reduce the population size.\n4. Environmental Improvement: Enhance vegetation cover to increase soil moisture to promote the growth of soil \nmicroorganisms, which reduces the food source required by the tiger lily butterfly.)\n虎斑蝶的防治方法包括在植物上寻找食物和水源。它们可以从叶子、花朵或果实中获得营养。此外，虎斑蝶还需要避\n免天敌。例如鸟类和其他昆虫，这些动物会捕捉并吃掉它们。虎斑蝶也可能通过在树木或灌木丛中栖息来保护自己。\n(Tiger butterfly control involves finding food and water sources on plants. They can get nutrients from leaves, flowers or fruits. \nIn addition, tiger butterflies need to avoid from natural enemies. Examples include birds and other insects that catch and eat them. \nTiger butterflies may also protect themselves by roosting in trees or bushes.)\n虎斑蝶是一种危害茅草的昆虫，它会感染植物，并且可能导致植物死亡。蝴蝶斑的防治方法包括：\n1.使用杀虫剂：可以用杀虫剂杀死虎斑蝶。\n2.使用抗斑蝶药物：可以用抗斑蝶药物来防止虎斑蝶感染植物。\n3.使用害虫剂：可以用害虫剂来防止虎斑蝶感染植物。\n4.使用环境改善：可以通过改造环境来防止虎斑蝶感染植物。\n(The Tiger Butterfly is an insect that harms thatch, infects plants, and can cause them to die. Methods of control of tiger \nbutterfly include:\n1. using insecticides: insecticides can be used to kill tiger spot butterflies.\n2. using anti-butterfly medication: anti-butterfly medication can be used to prevent tiger butterflies from infecting plants.\n3. Use of pests: Pests can be used to prevent tiger butterflies from infecting plants.\n4. Use environmental improvement: You can prevent tiger butterflies from infecting plants by modifying the environment.)\n➕\nOur Visualglm Ziya MiniGPT VisCPM Qwen-VL\n88.5 83.5 37.5 42.0\n为保护蝴蝶及其栖息地，可采取以下措施：\n1.绿化环境：种植更多的树木可以提供额外的栖息地，并吸引不同种类的蝴蝶。\n2.创造水景：在蝴蝶经常出没的地方，安装喷泉等可以吸引它们。\n3.使用环保型害虫剂：选择对环境影响小的杀虫剂来减少对蝴蝶种群的负面影响。\n(The following measures can be taken to protect butterflies and their habitats:\n1. Greening the environment: planting more trees can provide additional habitat and attract different species of butterflies.\n2. Create water features: installing fountains etc. in areas frequented by butterflies can attract them.\n3. Use environmentally friendly pests: Choose insecticides that have a low impact on the environment to minimize the \nnegative effects on butterfly populations.)\n采用化学防治方法：虎斑蝶主要危害农作物，可以通过喷杀虫剂进行防治。再作物开花期、幼虫期和成虫期可以采取\n三次防治，可以有效地减少虎斑蝶的危害。\n采用生物防治方法：可以通过引进天敌来控制虎斑蝶的数量。\n采用生物天敌防治方法：可以种植一些植物，利用植物的天敌来防治虎斑蝶。\n采用生态防治方法：可以通过改善农田和园林环境，减少适合虎斑蝶生长的环境，从而减少其数量。\n采用农业措施：在作物开花期，可以采取套种、间种等农业措施，以减少虎斑蝶的危害。\n(Adopting chemical control methods: The tiger butterfly mainly harms crops and can be controlled by spraying insecticides. \nCrops can be controlled by spraying insecticides during the flowering, larval and adult stages.\nThree times control can effectively reduce the damage of tiger butterfly.\nAdopting biological control methods: The number of tiger butterflies can be controlled by introducing natural enemies.\nAdopting biological natural enemy control methods: Some plants can be planted to control tiger butterflies by utilizing the \nnatural enemies of plants.\nAdoption of ecological control methods: The number of tiger butterflies can be reduced by improving the environment of \nfarmland and gardens to reduce the environment suitable for tiger butterflies to grow.\nAdoption of agricultural measures: During the flowering period of crops, agricultural measures such as hedging and \ninterplanting can be adopted to reduce the harm of tiger butterflies.)\nScore\nOurs\nVisualGML\nZiya\nMiniGPT\nVisCPM\nQwen\nFig. 6 Comparison Chart of Question and Answer Experiments.\ndeﬁnitively ascertain the correctness of prevention and control meas ures. Therefore,\nwe will manually assess the model’s prevention and control methodologi es, drawing\nfrom pertinent literature. We adopt a weighting ratio 3:7 between GP T-4 and human\njudgment, resulting in the ultimate scores delineated in Table 3.\nIn this task, VisualGLM-6B exhibits a laudable proﬁciency in respondi ng to\ninstructions, furnishing lucid responses, and proposing speciﬁ c preventative measures.\n22\nTable 3 The Accuracy Scores of Evaluation\nModels in Question and Answer task.\nModel GPT4 Human\nVisualGLM-6B 75.6 80\nZiya-BLIP2-14B-Visual 68.7 60\nMiniGPT 53.1 65\nVisCPM 67.5 60\nQwen-VL 73.7 75\nMLLM-For (Ours) 83.2 85\nNonetheless, the method tends to be overly general, lacking focus ed approaches tai-\nlored to speciﬁc insect species. Moreover, upon scrutiny, the c hemical control methods\nit advocates manifest certain inaccuracies and potentially misleadin g information, ren-\ndering them unsuitable for guiding purposes. Ziya-BLIP2-14B-Visual de monstrates\ncomparatively diminished performance in this context. It can only p roﬀer a single\ncontrol method, and the strategies for diﬀerent species are similarl y broad, needing\nmore detailed explications and instructive value. VisCPM furnishe s comprehensive\nresponses, yet it lacks speciﬁcity. It cannot be construed as presc riptive. MiniGPT\nfollows a parallel pattern and cannot present targeted control methods c ontingent on\ninsect species. While our model draws on the foundation of VisualGLM-6B , we are\nproﬁcient in delivering more precise control strategies. The subs tance and range of\nour suggested control measures are characterized by greater precision and compre-\nhensiveness, signifying a notable enhancement. As the GPT-4 evalu ation, our model\nalso attains superior scores for the proposed control methods. Our mode l performs the\nbest in this regard. For some species, it can give speciﬁc, targeted re commendations\nfor control.\n4.3.3 Count Task\nIn multimodal studies, the recognition of quantities assumes paramoun t signiﬁcance.\nWithin the specialized domain of forestry, as applied to vertical cont exts, the assess-\nment of pest quantities within images serves to ascertain the gravit y of the infestation,\ntrack alterations in its progression, and scrutinize the trends in pe st evolution. To\nthis end, we meticulously curated a subset of images from the IDADP dat aset, each\nfeaturing two or more insects, to serve as our testing cohort. While q uantity recogni-\ntion constitutes a fundamental aptitude for MLLMs, it does not represe nt a singular\nspecialized task. Our manually chosen test set exclusively encomp asses images por-\ntraying 2 or 3 insects, which are all distinctly discernible. Th is selection deliberately\neschews the complexities of recognizing intricate or densely popul ated insect imagery.\nThe testing protocol is elucidated in Figure\n7, and the ensuing results are meticulously\ntabulated in Table 4.3.3.\nIn this task, Ziya-BLIP2-14B-Visual, VisualGLM-6B, and our MLLM-For model\nall demonstrate accurate judgments regarding the number of insects in the images.\nZiya-BLIP2-14B-Visual and our model provide concise responses, strict ly adhering to\n23\nOurs\n这张照片上共有两只。\n(There are two of them\nin this photo.)\n图片上展示的是什么物种？\n（What species is shown in the picture?）\n这张图片中所示的物种叫黑点虎斑蝶，属于鳞翅目、虎斑蝶科的一种昆虫。别名：红\n凤蝶、黑缘凤蝶、黑点虎斑蝶等。\n（The species in this picture is called the Black-spotted Tiger Butterfly, an insect belonging to the Lepidoptera and Tiger \nButterfly families. Alias: Red Crested Butterfly, Black-margined Crested Butterfly, Black-spotted Tiger Butterfly, etc.）\n这只蝴蝶是橙色和黑色的，身体呈棕色。它坐在一片绿色的叶子上，叶子的末端有一\n些白色粉末。\n（This butterfly is orange and black with a brown body. It was sitting on a green leaf that had some white powder on the\nend of it.）\n图像上展示的是一只蝴蝶。\n（The image shows a butterfly.）\nThe image shows a butterfly sitting on a leaf in a green background. The butterfly has orange and black markings on\nits wings and is perched on a leaf with small white flowers on it. The butterfly’s body is brown and its antennae are white. \nThe background is made up of green leaves and small white flowers.\n光肩星天牛幼虫体长毫米，乳白色，无足；前胸背板有凸形纹，蛹体长毫米；裸蛹，黄白色。成虫，\n体黑色，有光泽，触角鞭状自第三节开始各节基部呈灰蓝色。前胸两侧各有个刺状突起，鞘翅上各有\n大小不等的由白色绒毛组成的斑纹个左右。\n(Light-shouldered stargazer larvae are millimeters long, creamy-white, and without legs; the dorsal plate of the \nprothorax has a convex pattern, and the pupa is millimeters long; naked pupa, yellowish-white. Adult, body black, glossy,\nantennae whip-like from the third segment onwards the base of each segment is gray-blue.There is a spiny protuberance \non each side of the prothorax, and the sheathed wings each have about one spot of varying sizes composed of white \ndowny hairs.）\n图像上显示了一只美丽的蝴蝶坐在绿色植物上。\n（The image shows a beautiful butterfly sitting on a green plant.）\n图片中有一只橙色的蝴蝶停在了树叶上，翅膀上有黑色的斑点。\n（The picture has an orange butterfly resting on a leaf with black spots on its wings.）\n图中处于什么阶段？\n（What stage is the diagram in?）\n这张照片展示了一只光肩星天牛成虫，它的体长为49~61毫米。\n(This photo shows an adult light-shouldered stargazer, which is 49 to 61 millimeters long.）\n我们可以推断出图中展示的是光肩星天牛的成虫。\n(We can deduce that the picture shows the adult of the light-shouldered stargazer.）\n我们可以推断出图中展示的是光肩星天牛的成虫。\n(We can deduce that the figure shows the adults of the light-shouldered stargazer.）\n这个图片显示了一个昆虫，它是一种昆虫。\n(This image shows an insect which is an insect.）\n图中处于什么阶段？\n(What stage is the diagram in?）\n图中处于幼虫阶段。\n(The figure is in the larval stage.）\nQ:这张图上有几只昆虫？\n(How many insects are in this picture?)\n这张图片展示了两只动物在\n草地上跳跃。\n(This picture shows two\nanimals jumping in a grassy\nfield.）\nVisualGLM\n这张图上有两只小昆虫。\n(There are two small\ninsects in this picture.）\nMiniGPT\n这张图上有几只昆虫？\n(How many insects are \nin this picture?)\nVisCPM\n Qwen\nZiya\n这张图上有两只昆虫。\n(There are two insects in\nthis picture.）\n有两只小动物。\n（There are two little\nanimals.）\nOurs\nVisualGML\nVisualGML\nZiya\nZiya\nOurs\nMiniGPT\nMiniGPT\nVisCPM\nVisCPM\nQwen\nQwen\nFig. 7 Comparison chart of Count experiment.\nTable 4 The Accuracy Scores of\nEvaluation Models in Count task.\nmodel Acc\nVisualGLM-6B 0.83\nZiya-BLIP2-14B-Visual 0.91\nMiniGPT 0.85\nVisCPM 0.75\nQwen-VL 0.6\nMLLM-For (Ours) 0.93\nthe instructions by outputting the insect count. Furthermore, ou r MLLM-For model\nobtains the best performance. Conversely, VisualGLM-6B and MiniGPT te nd to be\nmore verbose, oﬀering descriptions of the images in addition to the prom pt. In certain\ntest samples, some insects may be misjudged by Ziya-BLIP2-14B-Visual d ue to their\npositioning or if only half of their body is within the image frame. When an image\ncontains butterﬂy and larval (caterpillar) stages, VisualGLM-6B and Ziya-B LIP2-14B-\nVisual make incorrect quantity determinations. It is diﬃcult for Min iGPT to respond\nsatisfactorily to slightly complex images or moderately intricate Chi nese instructions.\nWe attribute this issue to the absence of similar ﬁne-tuning inst ructions in the training\nset, resulting in signiﬁcant discrepancies in insect quantity j udgments for images con-\ntaining multiple stages. Qwen-VL performs poorly in tasks related to specimen image\nrecognition, but it excels in natural photography images and adhering to in structions.\nMeanwhile, VisCPM slightly outperforms Qwen-VL.\n4.3.4 Stages Cognitive Recognition\nWe examine whether MLLMs exhibit enhanced logical inference capacity when per-\nceiving visual stimuli. MLLMs diﬀer signiﬁcantly from conventional m ethodologies.\n24\nOurs\n这张照片上共有两只。\n(There are two of them\nin this photo.)\n图片上展示的是什么物种？\n（What species is shown in the picture?）\n这张图片中所示的物种叫黑点虎斑蝶，属于鳞翅目、虎斑蝶科的一种昆虫。别名：红\n凤蝶、黑缘凤蝶、黑点虎斑蝶等。\n（The species in this picture is called the Black-spotted Tiger Butterfly, an insect belonging to the Lepidoptera and Tiger \nButterfly families. Alias: Red Crested Butterfly, Black-margined Crested Butterfly, Black-spotted Tiger Butterfly, etc.）\n这只蝴蝶是橙色和黑色的，身体呈棕色。它坐在一片绿色的叶子上，叶子的末端有一\n些白色粉末。\n（This butterfly is orange and black with a brown body. It was sitting on a green leaf that had some white powder on the \nend of it.）\n图像上展示的是一只蝴蝶。\n（The image shows a butterfly.）\nThe image shows a butterfly sitting on a leaf in a green background. The butterfly has orange and black markings on \nits wings and is perched on a leaf with small white flowers on it. The butterfly’s body is brown and its antennae are white. \nThe background is made up of green leaves and small white flowers.\n光肩星天牛幼虫体长毫米，乳白色，无足；前胸背板有凸形纹，蛹体长毫米；裸蛹，黄白色。成虫，\n体黑色，有光泽，触角鞭状自第三节开始各节基部呈灰蓝色。前胸两侧各有个刺状突起，鞘翅上各有\n大小不等的由白色绒毛组成的斑纹个左右。\n(Light-shouldered stargazer larvae are millimeters long, creamy-white, and without legs; the dorsal plate of the \nprothorax has a convex pattern, and the pupa is millimeters long; naked pupa, yellowish-white. Adult, body black, glossy, \nantennae whip-like from the third segment onwards the base of each segment is gray-blue.There is a spiny protuberance \non each side of the prothorax, and the sheathed wings each have about one spot of varying sizes composed of white \ndowny hairs.）\n图像上显示了一只美丽的蝴蝶坐在绿色植物上。\n（The image shows a beautiful butterfly sitting on a green plant.）\n图片中有一只橙色的蝴蝶停在了树叶上，翅膀上有黑色的斑点。\n（The picture has an orange butterfly resting on a leaf with black spots on its wings.）\n图中处于什么阶段？\n（What stage is the diagram in?）\n这张照片展示了一只光肩星天牛成虫，它的体长为49~61毫米。\n(This photo shows an adult light-shouldered stargazer, which is 49 to 61 millimeters long.）\n我们可以推断出图中展示的是光肩星天牛的成虫。\n(We can deduce that the picture shows the adult of the light-shouldered stargazer.）\n我们可以推断出图中展示的是光肩星天牛的成虫。\n(We can deduce that the figure shows the adults of the light-shouldered stargazer.）\n这个图片显示了一个昆虫，它是一种昆虫。\n(This image shows an insect which is an insect.）\n图中处于什么阶段？\n(What stage is the diagram in?）\n图中处于幼虫阶段。\n(The figure is in the larval stage.）\nQ:这张图上有几只昆虫？\n(How many insects are in this picture?)\n这张图片展示了两只动物在\n草地上跳跃。\n(This picture shows two\nanimals jumping in a grassy\nfield.）\nVisualGLM\n这张图上有两只小昆虫。\n(There are two small\ninsects in this picture.）\nMinigpt\n这张图上有几只昆虫？\n(How many insects are \nin this picture?)\nVisCPM\n Qwen-VL\nZiya\n这张图上有两只昆虫。\n(There are two insects in\nthis picture.）\n有两只小动物。\n（There are two little\nanimals.）\nOurs\nVisualGML\nVisualGML\nZiya\nZiya\nOurs\nMiniGPT\nMiniGPT\nVisCPM\nVisCPM\nQwen\nQwen\nFig. 8 Experimental Comparison Chart for Stages Cognitive Recogn ition.\nIn order to derive accurate conclusions, MLLMs must adhere to directi ves, grasp\nvisual representations, and tap into the knowledge reservoir withi n LLMs. They pose\na heightened challenge in contrast to solitary perceptual tasks. With in our special-\nized domain, discerning various developmental phases within a spec iﬁc insect species\nfacilitates the judicious selection of suitable control techniques , corresponding inter-\nventions, and control agents. It contributes to the surveillance and t imely forewarning\nof pestilence and disease outbreaks. Through regular scrutiny of insec t development\nstages, we can promptly identify upswings in pest populations or the di ssemination\nof pathogens. Furthermore, we can make appropriate measures to forestall t he pro-\nliferation of pests and diseases, ultimately furnishing more preci se and informative\nrecommendations.\nIn cognitive tasks, we utilize a format combining knowledge prompts w ith ques-\ntions. Initially, we curate a subset of the dataset and enlist human ex perts to describe\nvarious facets of the insects within the images comprehensively. Th ese descriptions\nencompass adult insects, larvae, and insect eggs. Subsequently, we p robe the develop-\nmental stage of the insects portrayed in the images. To mitigate the poss ibility of the\nmodel generating conjectural responses triggered by prompt words, we design prompts\nencompassing a minimum of three distinct stages of insects. For the s ake of evaluation,\nmirroring the approach in the initial experiment, we instruct the model to produce\nsolely ”yes” or ”no” responses. The correct answers are evenly balanced b etween ”yes”\nand ”no”. The experimental ﬁndings are graphically illustrated in Figure 8. This task\npredominantly evaluates the reasoning aptitude and coarse-grained recogn ition proﬁ-\nciency of MLLMs. The results of the assessments are succinctly summ arized in Table\n5.\n25\nTable 5 The accuracy scores of\nevaluation models in Stages\nCognitive Recognition task.\nmodel Acc\nVisualGLM-6B 0.53\nZiya-BLIP2-14B-Visual 0.65\nMiniGPT 0.51\nVisCPM 0.67\nQwen-VL 0.62\nMLLM-For (Ours) 0.71\nFrom the results of the assessments, Ziya-BLIP2-14B-Visual could not grasp the\ninstructions, resulting in an inability to carry out comprehensiv e task inference based\non the given prompts. Instead, it rigidly identiﬁes key terms with in the knowledge and\nproduces pertinent content associated with those keywords. Visual GLM-6B and our\nMLLM-For models showcase enhanced comprehension and task fulﬁllment capabili-\nties. Our model slightly outperforms VisualGLM-6B by accurately disc erning distinct\nstages of the same insect. Moreover, it can furnish more relevant infor mation and\nextract accurate data for inference, leading to reasoned and precise r esponses. Vis-\nCPM deviates from strict adherence to instructions, furnishing binary responses with\na suboptimal accuracy rate. Qwen-VL tends to generate illusions, pres uming the pres-\nence of both larval and adult stages in the image upon encountering these te rms in the\nprompts, rendering it comparatively weaker in inference. MiniGP T performs poorly,\nhard formulating responses in line with the provided instruction s. When instructions\ninvolve describing a non-existent stage of an insect, MLLMs conjure th e existence\nof such a stage and respond accordingly, indicating an illusion inﬂuence d by the\ninstructions.\n4.3.5 Common Sense Reasoning\nIn our investigation of ﬁne-tuning and training utilizing the propose d MLLM-For\nmodel within speciﬁc vertical domains, we also scrutinize the pot ential inﬂuence on\nthe model’s performance in broader domains. To this end, we adhere to the methodol-\nogy referenced in prior studies and administer assessments of common- sense reasoning\nto the model. Common-sense reasoning encompasses fundamental knowl edge applica-\nble to everyday situations. For example, when we present an image of an i ndividual\ndonning a down jacket, we inquire whether the model would be deem ed appropriate\nattire in cold (or hot) weather conditions. These queries entail rudi mentary knowledge\nthat individuals can promptly address without necessitating intri cate, step-by-step\ndeliberation. The test experiment is shown in Figure\n9.\nIn our experiments, most MLLMs exhibit a deﬁciency in reasoning abil ities. For\nexample, when we query the suitability of wearing a down jacket in t he summer,\nMLLMs logically deduce that it oﬀers substantial insulation; however, i t still gener-\nates an aﬃrmative response. This discrepancy signiﬁes a breakdown in the model’s\ncognitive process. Consequently, we incorporate CoT prompts in our in quiries, such\n26\n不是。(No)\nMiniGPT:\nVisCPM:\n这张图上的衣服是一件含有夹\n克的夹克。(The dress in this\npicture is a jacket containing\na blazer.)\n不，图片中的衣服不适合在\n夏天穿。(No, the dress in\nthe picture is not suitable\nfor summer.)\nQwen: 不是。(No)\nVisualGML:\nZiya:\n这张图上的衣服适合在夏天穿吗？只回答是或者不是。\nplease think step by step.\n(Is the dress in this picture suitable for summer? Just\nanswer yes or no. please think step by step.)\nOurs:答案是不适合。(The answer is not suitable.)\n这件黑色连帽夹克适合在夏季穿着。\n(This black hooded jacket is\nperfect for the summer season. )\nMiniGPT:\nVisCPM:\n是。(Yes)\nThe image shows a USB-C to\n HDMI cable.\n是的，图片中的物品可以用\n来给手机充电。(No, the\ndress in the picture is not\nsuitable for summer.)\nVisualGML:\nZiya:\nQwen: 不是。(No)\n这张图上的衣服适合在夏天穿吗？只回答是或者不是。\nplease think step by step.\n(Is the dress in this picture suitable for summer? Just\nanswer yes or no. please think step by step.)\nOurs:不能。(No.)\n这幅图像显示了一条黑色的HDMI电缆。\n(This image shows a black HDMI cable.)\nMiniGPT:\nVisCPM:\nThe image shows a bottle of\ngreen tea with a white label on it.\n不，图片中的物品不是用\n来驱蚊的。(No, the items\nin the picture are not for\nrepelling mosquitoes.)\nQwen: 是。(Yes)\nVisualGML:\nZiya:\n这张图片展示了一瓶绿色的液体，瓶\n口呈圆形。(This picture shows a\nbottle of green liquid with the bottle\nmouth is rounded. )\n是。(Yes)\n这张图上的衣服适合在夏天穿吗？只回答是或者不是。\nplease think step by step.\n(Is the dress in this picture suitable for summer? Just\nanswer yes or no. please think step by step.)\nOurs:\n是。(Yes)\nMiniGPT:\n不是。这张图片描述的是一个男人和女\n人在雨中笑着。(NO. This picture depicts\na man and woman laughing in the rain.)\nZiya:\n这张图片描述的是泰坦尼克号。\n(This picture depicts the Titanic.)\nVisCPM:是的，这张图片描述的是\n泰坦尼克号。(Yes, this\npicture depicts the Titanic.)\nQwen: 是。(Yes)\nVisualGML:图片展示了一对穿着礼服的情侣，\n其中男性戴着帽子和眼镜。(Image\nshows a couple in dresses, with\nthe male wearing a hat and glasses. )\n这张图上的衣服适合在夏天穿吗？只回答是或者不是。\nplease think step by step.\n(Is the dress in this picture suitable for summer? Just\nanswer yes or no. please think step by step.)\nOurs:\n 不是。(No)\nFig. 9 Experimental comparison chart for Common Sense Reasoning\nas ”please engage in step-by-step thinking”, aiming to direct the mode l toward a\nmarginally enhanced outcome. From Table 6, while VisualGLM-6B and our MLLM-For\nmodel display heightened adherence to instructions in this cont ext, their perceptual\ncapacities were relatively inferior compared to Ziya-BLIP2-14B-Visual. Ziya-BLIP2-\n14B-Visual and Qwen-VL demonstrate proﬁciency in providing more reﬁne d responses\nguided by prompts within the chain of thought. Our model performs sli ghtly below\nQwen-VL. Qwen-VL exhibits complete adherence to instructions and e xcels in\ncommon-sense reasoning by delivering binary ”yes” or ”no” responses. Ne vertheless,\nQwen-VL still exhibits a minor performance disparity compared to hu man experts.\n5 Conclusions\nThis paper presents a multimodal large language model for forestry (MLLM- For) in\npest and disease recognition. We establish a multimodal Chinese-lan guage forestry pest\nand disease dataset with image-text instruction adherence within for estry pest and\ndisease contexts. Based on the dataset, we embark on a comprehensive pr e-training\n27\nTable 6 The scores of evaluation models in\nCommon Sense Reasoning task.\nmodel Acc Human\nVisualGLM-6B 0.3 76\nZiya-BLIP2-14B-Visual 0.80 72\nMiniGPT 0.62 70\nViscpm 0.67 78\nQwen-VL 0.76 90\nMLLM-For (Ours) 0.65 78\nprocess for MLLM-For to enhance the understanding of forestry visuals and linguistics.\nMLLM-For showcases a robust repository of domain-speciﬁc knowledge. We implement\nsupplementary ﬁne-tuning procedures to optimize its performan ce, further reﬁning the\nmodel. MLLM-For is a substantial advancement in creating a practical for estry aide,\nsignifying an expansion of MLLMs into the vertical domain, particularly i n the Chinese\ncontext. Nevertheless, we acknowledge ample opportunity for enhance ment in tasks\ndemanding deep reasoning capabilities. The experiments compare ﬁv e currently out-\nstanding open-source MLLMs in Chinese and English. Our model outperf orms others\nin forestry pest and disease control, surpassing the baseline model , VisualGLM, to a\ncertain extent. However, there is still signiﬁcant room in pest and disease recognition.\nIn the future, we plan to expand the annotated dataset, address pest and disease recog-\nnition issues, and overcome challenges associated with model halluc inated knowledge.\nBesides, we hope to improve the quality and reliability of MLLMs.\nDeclarations\n• This work is partially supported by the Sichuan Science and Technology Program\n(Nos. 2022YFG0378, 2023YFS0424, 2023YFH0058 and 2023YFQ0044), and Engi-\nneering Research Center for ICH Digitalization and Multi-source Inf ormation Fusion\n(Fujian Polytechnic Normal University), Fujian Province Universit y (G3-KF2022).\n• The authors declare that they have no known competing ﬁnancial inter ests or per-\nsonal relationships that could have appeared to inﬂuence the work repor ted in this\npaper.\n• All sources of funding are our research projects. There are no potenti al conﬂicts of\ninterest. Human Participants and Animals do not involve in this researc h. Informed\nconsent for data used has been included in this study.\n• Availability of data and materials\n• The datasets generated during and analyzed during the current study are available\nfrom the corresponding author upon reasonable request. Our code is open at\nhttps:\n//github.com/motuomumu/MLLM-For.\n• Maolin Zhang : Methodology, Software, Data curation, Writing-original draft.\nXianyong Li: Supervision, Writing-review & editing, Funding acquisition, F ormal\nanalysis. Yajun Du: Funding acquisition, Investigation, Validation. Zihan Rao:\n28\nSoftware. Songlin Chen: Data curation, Software. Weicheng Wang: Software.\nDong Huang : Formal analysis, Software. Xiaoliang Chen : Funding acquisi-\ntion, Formal analysis. Shumin Wang: Formal analysis, Software. All authors\ncontributed to the manuscript revision and read and approved the sub mitted\nversion.\nReferences\n[1] Zhao, W.X., Zhou, K., Li, J.: A survey of large language models. CoRR\nabs/2303.18223 (2023) https://doi.org/10.48550/arXiv.2303.18223\n[2] Haotian Liu, Q.W.Y.J.L. Chunyuan Li: Visual instruction tuning. CoRR\nabs/2304.08485 (2023) https://doi.org/10.48550/arXiv.2304.08485\n[3] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi-\nrectional transformers for language understanding. In: Burstein, J., D oran, C.,\nSolorio, T. (eds.) Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\nume 1 (Long and Short Papers), pp. 4171–4186. Association for Computational\nLinguistics, ??? (2019).\nhttps://doi.org/10.18653/v1/n19-1423 . https://doi.org/\n10.18653/v1/n19-1423\n[4] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I ., et al.:\nLanguage models are unsupervised multitask learners. OpenAI blog 1(8), 9\n(2019)\n[5] Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou , Y., Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a uni ﬁed text-to-text\ntransformer. J. Mach. Learn. Res. 21, 140–114067 (2020)\n[6] Zhang, S., Roller, S., Goyal, N., and, M.A.: OPT: open pre-trained tr ansformer\nlanguage models. CoRR abs/2205.01068 (2022)\n[7] Everitt, T., Lea, G., Hutter, M.: AGI safety literature review. In: Lang, J. (ed.)\nProceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial\nIntelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pp. 5441–5449.\nijcai.org, ??? (2018)\n[8] Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng,\nW., Xia, X., Tam, W.L., Ma, Z., Xue, Y., Zhai, J., Chen, W., Liu, Z., Zhang,\nP., Dong, Y., Tang, J.: GLM-130b: An open bilingual pre-trained model. In: Th e\nEleventh International Conference on Learning Representations (ICLR ) (2023).\nhttps://openreview.net/forum?id=-Aw0rrrPUF\n[9] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T.,\nRozi` ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A. , Grave,\n29\nE., Lample, G.: Llama: Open and eﬃcient foundation language models. CoRR\nabs/2302.13971 (2023)\nhttps://doi.org/10.48550/arXiv.2302.13971\n[10] Chen, Z., Balan, M.M., Brown, K.: Language models are few-shot learne rs for\nprognostic prediction. CoRR abs/2302.12692 (2023) https://doi.org/10.48550/\narXiv.2302.12692\n[11] Chowdhery, A., Narang, S., ., J.D., et al.: Palm: Scaling language model ing with\npathways. CoRR abs/2204.02311 (2022) https://doi.org/10.48550/arXiv.2204.\n02311\n[12] Sun, T., Zhang, X., et al.: Moss: Training conversational language models from\nsynthetic data (2023)\n[13] ERNIBOT (2023)\n[14] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n1(2), 3 (2022)\n[15] Li, C., Liu, H., et al.: ELEVATER: A benchmark and toolkit for evaluating\nlanguage-augmented visual models. In: NeurIPS (2022)\n[16] Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., Gao, J.: Vision-language pre-traini ng:\nBasics, recent advances, and future trends. Found. Trends Comput. Graph. Vis.\n14(3-4), 163–352 (2022)\nhttps://doi.org/10.1561/0600000105\n[17] OpenAI: GPT-4 technical report. CoRR abs/2303.08774 (2023) https://doi.\norg/10.48550/arXiv.2303.08774\n[18] Askell, A., Bai, Y., Chen, A., et al.: A general language assistant as a laborator y\nfor alignment. CoRR abs/2112.00861 (2021)\n[19] Li, J., Li, D., Savarese, S., et al.: BLIP-2: bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. In: Inter national\nConference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\nUSA. Proceedings of Machine Learning Research, vol. 202, pp. 19730–19742.\nPMLR, ??? (2023)\n[20] Zhang, R., Han, J., et al.: Llama-adapter: Eﬃcient ﬁne-tuning of language m od-\nels with zero-init attention. CoRR abs/2303.16199 (2023)\nhttps://doi.org/10.\n48550/arXiv.2303.16199\n[21] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang , L.,\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021)\n[22] Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., Tang, J.: Glm: Ge neral\n30\nlanguage model pretraining with autoregressive blank inﬁlling. In: Pr oceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 320–335 (2022)\n[23] Zhang, J., Gan, R., Wang, J., et al.: Fengshenbang 1.0: Being the foundat ion of\nchinese cognitive intelligence. CoRR abs/2209.02970 (2022)\n[24] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enh ancing\nvision-language understanding with advanced large language models. CoRR\nabs/2304.10592 (2023)\nhttps://doi.org/10.48550/arXiv.2304.10592\n[25] Hu, J., Yao, Y., Wang, C., Wang, S., Pan, Y., Chen, Q., et al.: Large multilin gual\nmodels pivot zero-shot multimodal learning across languages (2023)\n[26] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,\nJ.: Qwen-vl: A frontier large vision-language model with versatile abi lities. arXiv\npreprint arXiv:2308.12966 (2023)\n[27] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Imp roving\nlanguage understanding by generative pre-training (2018)\n[28] Liu, X., McDuﬀ, D., Kovacs, G., Galatzer-Levy, I.R., et al.: Large langu age models\nare few-shot health learners. CoRR abs/2305.15525 (2023) https://doi.org/10.\n48550/arXiv.2305.15525\n[29] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I ., et al.:\nLanguage models are unsupervised multitask learners. OpenAI blog 1(8), 9\n(2019)\n[30] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I ., et al.:\nLanguage models are unsupervised multitask learners. OpenAI blog 1(8), 9\n(2019)\n[31] Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ili´ c, S., Hesslow, D. , et al.: Bloom: A\n176b-parameter open-access multilingual language model (2022)\n[32] Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., Qiao,\nY.: Videochat: Chat-centric video understanding. CoRR abs/2305.06355 (2023)\nhttps://doi.org/10.48550/arXiv.2305.06355\n[33] Alayrac, J., Donahue, J., Luc, P., et al.: Flamingo: a visual language model for\nfew-shot learning. In: NeurIPS (2022)\n[34] Huang, S., Dong, L., Wang, W., et al.: Language is not all you need: Aligning\nperception with language models. CoRR abs/2302.14045 (2023) https://doi.\norg/10.48550/arXiv.2302.14045\n[35] Driess, D., Xia, F., Sajjadi, M.S.M., Lynch, C., et al.: Palm-e: An embodied\n31\nmultimodal language model. In: International Conference on Machine Learn ing,\nICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA. Proceedings of Machine\nLearning Research, vol. 202, pp. 8469–8488. PMLR, ??? (2023)\n[36] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J.,\nZheng, X., et al.: Mme: A comprehensive evaluation benchmark for multi modal\nlarge language models. arXiv preprint arXiv:2306.13394 (2023)\n[37] Li, J., Li, D., et al.: BLIP: bootstrapping language-image pre-training for uni-\nﬁed vision-language understanding and generation. In: International Conf erence\non Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA.\nProceedings of Machine Learning Research, vol. 162, pp. 12888–12900. PMLR,\n??? (2022)\n[38] Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before\nfuse: Vision and language representation learning with momentum distil lation.\nAdvances in neural information processing systems 34, 9694–9705 (2021)\n[39] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P.,\nHoi, S.: InstructBLIP: Towards General-purpose Vision-Language Models w ith\nInstruction Tuning (2023)\n[40] Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visu al\nlanguage model for video understanding. arXiv preprint arXiv:2306.02858 (2023)\n[41] Li, Y., Liang, Y., Risteski, A.: Recovery guarantee of weighted low-rank approx-\nimation via alternating minimization. In: International Conference on Machine\nLearning, pp. 2358–2367 (2016)\n[42] Li, Y., Ma, T., Zhang, H.: Algorithmic regularization in over-parameteriz ed\nmatrix sensing and neural networks with quadratic activations. In: Con ference\nOn Learning Theory, pp. 2–47 (2018)\n[43] Grasedyck, L., Kressner, D., Tobler, C.: A literature survey of low-rank tensor\napproximation techniques. GAMM-Mitteilungen 36(1), 53–78 (2013)\n[44] Oymak, S., Fabian, Z., Li, M., Soltanolkotabi, M.: Generalization guarant ees for\nneural networks via harnessing the low-rank structure of the jacobi an. arXiv\npreprint arXiv:1906.05392 (2019)\n[45] Sainath, T.N., Kingsbury, B., Sindhwani, V., Arisoy, E., Ramabhadran , B.: Low-\nrank matrix factorization for deep neural network training with high-di mensional\noutput targets. In: 2013 IEEE International Conference on Acoustics, Spee ch and\nSignal Processing, pp. 6655–6659 (2013). IEEE\n[46] Povey, D., Cheng, G., Wang, Y., Li, K., Xu, H., Yarmohammadi, M., Khudanp ur,\nS.: Semi-orthogonal low-rank matrix factorization for deep neural network s. In:\n32\nInterspeech, pp. 3743–3747 (2018)\n[47] Zhang, Y., Chuangsuwanich, E., Glass, J.: Extracting deep neural ne twork bot-\ntleneck features using low-rank matrix factorization. In: 2014 IEEE In ternational\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 185–189\n(2014). IEEE\n[48] Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up convolution al neural\nnetworks with low rank expansions. arXiv preprint arXiv:1405.3866 (2014)\n[49] Khodak, M., Tenenholtz, N., Mackey, L., Fusi, N.: Initialization and regularization\nof factorized neural layers. arXiv preprint arXiv:2105.01029 (2021)\n33",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7272500395774841
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4804513156414032
    },
    {
      "name": "Context (archaeology)",
      "score": 0.46769285202026367
    },
    {
      "name": "Natural language processing",
      "score": 0.45253002643585205
    },
    {
      "name": "Vocabulary",
      "score": 0.41499584913253784
    },
    {
      "name": "Linguistics",
      "score": 0.2840953469276428
    },
    {
      "name": "Geography",
      "score": 0.11591038107872009
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210146077",
      "name": "China National Institute of Standardization",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210118093",
      "name": "Xianyang Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I102345215",
      "name": "Xihua University",
      "country": "CN"
    }
  ],
  "cited_by": 4
}