{
  "title": "Enhancing medical coding efficiency through domain-specific fine-tuned large language models",
  "url": "https://openalex.org/W4410008104",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2359014107",
      "name": "Hou Zhen",
      "affiliations": [
        "Indiana University – Purdue University Indianapolis"
      ]
    },
    {
      "id": "https://openalex.org/A1994211135",
      "name": "Liu Hao",
      "affiliations": [
        "Montclair State University",
        "Indiana University – Purdue University Indianapolis"
      ]
    },
    {
      "id": "https://openalex.org/A2101338964",
      "name": "Bian, Jiang",
      "affiliations": [
        "Indiana University Health",
        "Indiana University – Purdue University Indianapolis",
        "Regenstrief Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2054023128",
      "name": "He Xing",
      "affiliations": [
        "Indiana University – Purdue University Indianapolis",
        "Regenstrief Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1974453172",
      "name": "Zhuang Yan",
      "affiliations": [
        "Indiana University – Purdue University Indianapolis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1997057722",
    "https://openalex.org/W4390889597",
    "https://openalex.org/W2892312135",
    "https://openalex.org/W2318106356",
    "https://openalex.org/W4307138871",
    "https://openalex.org/W2152587002",
    "https://openalex.org/W2004910511",
    "https://openalex.org/W3013605954",
    "https://openalex.org/W2788599015",
    "https://openalex.org/W2146417320",
    "https://openalex.org/W2007666495",
    "https://openalex.org/W3015257991",
    "https://openalex.org/W2997101676",
    "https://openalex.org/W2104752900",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2096664202",
    "https://openalex.org/W1775813496",
    "https://openalex.org/W3212490246",
    "https://openalex.org/W3103901889",
    "https://openalex.org/W4402185117",
    "https://openalex.org/W4287854971",
    "https://openalex.org/W3035294872",
    "https://openalex.org/W2585785990",
    "https://openalex.org/W4307638650",
    "https://openalex.org/W2929849795",
    "https://openalex.org/W2770445088",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4397022293",
    "https://openalex.org/W4394943312",
    "https://openalex.org/W4400823526",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3030177116",
    "https://openalex.org/W4406642447",
    "https://openalex.org/W2052941242",
    "https://openalex.org/W2006642610",
    "https://openalex.org/W4406266037",
    "https://openalex.org/W3115258265",
    "https://openalex.org/W2088263677",
    "https://openalex.org/W3082278284",
    "https://openalex.org/W4392711449",
    "https://openalex.org/W2156235098",
    "https://openalex.org/W2167357104",
    "https://openalex.org/W2100590447",
    "https://openalex.org/W4402667093",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3175870271",
    "https://openalex.org/W3101475827",
    "https://openalex.org/W3112785303"
  ],
  "abstract": "Abstract Medical coding is essential for healthcare operations yet remains predominantly manual, error-prone (up to 20%), and costly (up to $18.2 billion annually). Although large language models (LLMs) have shown promise in natural language processing, their application to medical coding has produced limited accuracy. In this study, we evaluated whether fine-tuning LLMs with specialized ICD-10 knowledge can automate code generation across clinical documentation. We adopted a two-phase approach: initial fine-tuning using 74,260 ICD-10 code–description pairs, followed by enhanced training to address linguistic and lexical variations. Evaluations using a proprietary model (GPT-4o mini) on a cloud platform and an open-source model (Llama) on local GPUs demonstrated that initial fine-tuning increased exact matching from &lt;1% to 97%, while enhanced fine-tuning further improved performance in complex scenarios, with real-world clinical notes achieving 69.20% exact match and 87.16% category match. These findings indicate that domain-specific fine-tuned LLMs can reduce manual burdens and improve reliability.",
  "full_text": "npj |health systems Article\nhttps://doi.org/10.1038/s44401-025-00018-3\nEnhancing medical coding efﬁciency\nthrough domain-speciﬁc ﬁne-tuned large\nlanguage models\nCheck for updates\nZhen Hou1, Hao Liu1,2, Jiang Bian1,3,4,5,X i n gH e3,4 & Yan Zhuang1\nMedical coding is essential for healthcare operations yet remains predominantly manual, error-prone\n(up to 20%), and costly (up to $18.2 billion annually). Although large language models (LLMs) have\nshown promise in natural language processing, their application to medical coding has produced\nlimited accuracy. In this study, we evaluated whetherﬁne-tuning LLMs with specialized ICD-10\nknowledge can automate code generation across clinical documentation. We adopted a two-phase\napproach: initialﬁne-tuning using 74,260 ICD-10 code–description pairs, followed by enhanced\ntraining to address linguistic and lexical variations. Evaluations using a proprietary model (GPT-4o\nmini) on a cloud platform and an open-source model (Llama) on local GPUs demonstrated that initial\nﬁne-tuning increased exact matching from <1% to 97%, while enhancedﬁne-tuning further improved\nperformance in complex scenarios, with real-world clinical notes achieving 69.20% exact match and\n87.16% category match. Theseﬁndings indicate that domain-speciﬁc ﬁne-tuned LLMs can reduce\nmanual burdens and improve reliability.\nMedical coding is a critical component o f healthcare operations, involving\nthe translation of clinical documentation into standardized codes essential\nfor patient care, billing, resea rch, and public health reporting 1–3.M u l t i p l e\ncoding systems exist—each with a unique design, hierarchical structure, and\npurpose. For instance, the International Classiﬁcation of Diseases (ICD) is\nwidely used globally for coding diseases and morbidity, encompassing over\n74,260 unique codes in the ICD 10th revision (ICD-10)1,4. In current practice,\nphysicians often assign preliminary codes during patient visits and medical\ncoders subsequently verify or correct these codes by reviewing the entire\nchart including unstructured clinical notes. Despite technological\nadvancements, medical coding remains predominantly manual\n4–6,c h a l -\nlenged by the immense variability in documentation styles and the sheer\nbreadth of possible codes\n5,7,8. Consequently, this labor-intensive process is\nprone to human errors, reported to be as high as 20% 2,9,10,c o n t r i b u t i n gt o\ninefﬁciencies within the substantial medical coding market, which totals\napproximately $18.2 billion annually in the US11.\nMany prominent efforts have been made to enhance this process.\nEarly efforts in the use of rule-based systems 12–15 and terminology ser-\nvices like the Uni ﬁed Medical Language System (UMLS) to map clinical\nterms to codes 16. Researchers advanced to employing Natural Language\nProcessing (NLP) techniques and machine learning approaches 17–21 to\nautomate medical coding tasks 5. Notably, the Pretrained Language\nModels for ICD coding achieved a 59.8% match rate on ICD coding\ntasks involving approximately 8,900 codes, while the Label-Attention-\nAware Transformer model reached 71.5% on the top 50 most frequently\nused codes\n22,23.\nDespite these advancements, existing methods face signiﬁcant limita-\ntions. They struggle with accurate named entity recognition (NER), the\nprocess of recognizing and extractin g key medical terms for coding from\nclinical documentation5,24,25. This task is complicated by linguistic and lexical\nvariations5,26,27, including informal shorthand (e.g.,“HTN” for hypertension,\n“DM2” for type 2 diabetes), typo graphical errors (e.g., “malignnt” for\nmalignant), and varied phrasing (e.g., “diabetic nephropathy” vs. “kidney\ndisease due to diabetes ”). Multiple interrelated conditions within a single\nnote further complicate scaling to the full ICD-10 code set (e.g., “Type 2\ndiabetes with peripheral angiopathy with gangrene ” requires both E11.52\nand I70.26 codes). Traditional NLP approaches struggle with these com-\nplexities, particularly for context-driven reasoning\n5. Meanwhile, large lan-\nguage models (LLMs) have demonst rated strong natural language\nunderstanding and reasoning abilities28–30, offering promising potential31 to\ntransform medical coding by interpre ting complex medical contexts and\ngenerating accurate medical codes32.\n1Department of Biomedical Engineering and Informatics, Luddy School of Informatics, Computing, and Engineering, Indiana University, Indianapolis, IN, USA.\n2School of Computing, College of Science and Mathematics, Montclair State University, Montclair, NJ, USA.3Department of Biostatistics and Health Data Science,\nSchool of Medicine, Indiana University, Indianapolis, IN, USA.4Regenstrief Institute, Indiana University, Indianapolis, IN, USA.5Indiana University Health, India-\nnapolis, IN, USA. e-mail: ynzhuang@iu.edu\nnpj Health Systems|            (2025) 2:14 1\n1234567890():,;\n1234567890():,;\nRecent ﬁndings, however, indicate that LLMs have yet to meet the\nanticipated performance in medical coding, with accuracy below 50%33.K e y\nchallenges include: (1) generating non -existent codes, (2) struggling with\nless frequently used codes, and (3) a ssigning the same code to different\nclinical contexts, even when subtle distinctions warrant different codes. One\napproach uses prompt engineering to guide LLMs to select from a pre-\ndeﬁned subset of codes, but token limits prevent incorporating the entire\nICD-10 set\n34. Another strategy employs Retrieval-Augmented Generation\n(RAG) to dynamically retrieve candidate codes, although the variability and\nerrors in clinical documentation complicate accurate retrieval\n35,36.\nPrevious studies have predominantly focused on evaluating the para-\nmetric coding knowledge of LLMs using standardized code descriptions33,\nwhich do not suf ﬁciently reﬂect the complexity of real-world clinical doc-\numentation. To address this limitation and the above challenges compre-\nhensively, our study systematically e v a l u a t e sf o u rc o m m o nl i n g u i s t i ca n d\nlexical variations encountered in clinical practice that affect coding\naccuracy\n27,37–39: (1) reordered diagnostic expressions, (2) typographical\nerrors, (3) medical abbreviations, and (4) multiple interrelated conditions.\nBuilding on these, we further exami ne two more complex scenarios: (5)\nsentences with a single embedded diagnostic detail, and (6) full real-world\nclinical notes33. Our proposed methods target these variations to enhance the\nrobustness and reliability of automatedICD-10 coding in clinical settings.\nWe hypothesize that LLMs’ limitations in medical coding stem from\ninsufﬁcient specialized knowledge. To test this hypothesis, we employed\nﬁne-tuning (training LLMs on domain-speci ﬁc data to adapt them to a\nspeciﬁct a s k40) using the complete ICD-10 code set and prompt engineering\nto avoid common errors. We experimented with both proprietary (e.g.,\nOpenAI’sG P T - 4 o - m i n i41) and open-source LLMs (e.g., Meta’sL l a m a - 3 . 2 - 1\nB, Llama-3.2-3B, and Llama-3.1-8B42). This approach bypasses token lim-\nitations and enables LLMs to handle co mplex medical contexts. Rigorous\nevaluations using full real-world clinical notes evaluate the GPT of our\napproach in medical coding tasks and its potential to be adapted to other\ncoding systems.\nResults\nInitial ﬁne-tuning performance\nWe evaluated both pre-trained and ﬁne-tuned models on standard ICD\ncoding and linguistic and lexical var iations. Fine-tun ing dramatically\nimproved performance, with the exact code-matching rate increasing from\n3.35% to 97.48% for GPT-4o mini, and from 0.01 –0.85% to 98.80–98.83%\nfor Llama models in the base scenario. Among pre-trained models, GPT-4o\nmini (3.43–3.6%) outperformed Llama models (0.01–0.85%) across all tasks.\nThe model size was particularly inﬂuential in managing linguistic and\nlexical variations. In the Llama series, larger models consistently performed\nbetter after ﬁne-tuning. For instance, in reordered descriptions, the exact\ncode-matching rate rose from 36.19 % (Llama-3.2-1B) to 78.23% (Llama-\n3.2-8B). A similar pattern was observed for sentence integration tasks,\nimproving from 49.48% to 87.99%. All ﬁne-tuned models handled medical\nabbreviations (92.86–95.27%) and typographical errors (58.43 –83.84%)\nrelatively well but struggled with multiple concurrent conditions\n(3.85–10.90%) and full real-world clinical notes (0.01%), indicating these\nremain challenging.\nEnhanced Fine-tuning Performance\nEnhanced ﬁne-tuning on GPT-4o mini and Llama-3.2-1B yielded sub-\nstantial improvements across all variations (Table 1). For multiple con-\ncurrent conditions, Llama-3.2-1B went from 3.85% to 98.04%, and GPT-4o\nmini from 10.9% to 94.07%. Both models achieved an exact code-matching\nrate of 96.59% and 95.57% for medical abbreviations, 94.18% and 93.98% for\ntypographical errors, and 97.32% and 93.92% for sentences with single\nembedded diagnostic elements in Llama and GPT-4o mini, respectively.\nFor the full real-world clinical notes, exact ICD code matching showed\npromising results: Top-1 accuracy reached 69.20% (95% CI: 67.42–71.09%) and\nTop-4 accuracy was 64.27% (95% CI: 63.25–8%). At the category level, the model\ndemonstrated stronger performance with 87.16% (95% CI: 85.70-88.49%)\nTable 1 | Performance comparison of LLMs across pre-training, initialﬁne-tuning, and enhancedﬁne-tuning stages (n = 10,000)a\nModel Type Enhance Fine-tuned\nModels\nInitial Fine-tuned Models Pre-train Models\nModel Llama-3.2-1B GPT-4o mini Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B GPT-4o mini Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B GPT-4o mini\nReordered Diagnostic Expressions 87.51 85.02 36.19 67.92 78.23 61.30 0.01 0.13 0.69 3.31\nTypographical Errors 94.18 93.98 58.43 80.31 69.10 83.84 0.01 0.07 0.79 3.58\nMedical Abbreviations 96.59 95.57 93.80 92.86 95.27 93.06 0.01 0.06 0.07 3.55\nMultiple Concurrent Conditions 98.04 94.07 3.85 4.18 5.24 10.90 0.00 0.06 0.27 3.43\nSentences with Single Embedded Diagnostic\nInformation\n97.32 93.92 49.48 76.85 87.99 64.61 0.00 0.10 0.85 3.60\nFull Real-world Clinical Notes 69.20\nc NAb 0.00 0.01 0.01 NA b 0.00 0.10 2.36 NA b\naAll values represent the exact code-matching rate in percentages (%). Each model was evaluated on a test set ofn = 10,000 samples.\nbNA indicates data not available due to PhysioNet licensing restrictions, which explicitly prohibit sharing access to MIMIC data with external systems, such as GPT-4o. According to the license terms,“The LICENSEE will not share access to PhysioNet restricted data with\nanyone else”48.\ncRepresents Top-1 exact code-matching rate.\nhttps://doi.org/10.1038/s44401-025-00018-3 Article\nnpj Health Systems|            (2025) 2:14 2\nTop-1 matching rate and 75.81% (95% CI: 74.90–76.71%) Top-4 matching\nrate. The MRR was 0.753, indicating an effective ranking of predictions.\nTable 2 presents detailed performance metrics across different ranking\npositions. All prediction accuracies were signi ﬁcantly better than random\nprediction (p < 0.001).\nError Analysis\nFurthermore, we performed an error analysis focusing on the recognized\nmost frequently occurring errors, categorized into four types: (1) Non-\nexistence Errorsas generating invalid ICD codes; (2)Hierarchy-Level Errors\nrelated to incorrect speci ﬁcity in the ICD hierarchy; (3) Quantity Mis-\nmatches, which means generating an incorrect number of codes; and (4)\nCharacter Errorssuch as typos or transposed digits 33.A ss h o w ni nF i g .1,\nmost error types decreased signiﬁcantly after ﬁne-tuning compared to the\npre-trained models, although hierarchy-level errors increased in some\ninstances due to attempts at more speci ﬁc coding rather than defaulting to\nbroader categories. This trend was consistent across all model variants,\nunderscoring the effectiveness of our approach.\nIn the Llama series (Fig. 1b and c), non-existence errors generally\ndeclined as model size increased from 1B to 8B, though the 3B model\nshowed a small deviation. In contrast, differences in quantity mismatches\nand character errors were less scale dependent. GPT-4o mini followed\nsimilar error patterns (Fig. 1a), with all error types decreasing further after\nenhanced ﬁne-tuning. The consistency across different LLMs suggests the\nrobustness of our training approach in reducing various error types\n43.\nFor full real-world clinical notes, analysis of 535 incorrect codes from\n200 random samples revealed four distinct error patterns: (1) Information\nAbsence (42%), where no relevant documentation supported the assigned\ncode (e.g., a tobacco dependence code with no mention of smoking); (2)\nDiagnostic Criteria Insufﬁciency (38%), where basic symptoms a were pre-\nsent but lack speci ﬁc metrics needed for de ﬁnitive diagnosis (e.g., men-\ntioning renal insuf ﬁciency without echocardiography con ﬁrmation for\nmitral insufﬁciency staging)\n44;( 3 ) Clinical Context Misinterpretation(8%)\nwhere the model fails to properly understand clinical context, such as in a\nsample where“history of cervical cancer s/p hysterectomy with no evidence\nof recurrence” was incorrectly coded as current cervical cancer; and (4)\nCoding Rule Violation(12%) where ICD guidelines were incorrectly applied,\nsuch as using separate codes for hypertension and chronic kidney disease\ninstead of the combination code (I12.9).\nDiscussion\nWe hypothesized that the limitations of LLMs in medical coding stem\nprimarily from insufﬁcient domain-speciﬁc coding knowledge rather than\ninherent model de ﬁciencies. Our experimental results support this\nhypothesis through a two-stage validation process. Initial ﬁne-tuning with\nthe complete ICD-10 dataset improved the exact code-matching rate from\nless than 1% to over 97% in the base scenario. Enhancedﬁne-tuning further\nenabled the models to manage various documentation challenges, achieving\nhigh performance on reordered diagno stic expressions (97.32%), medical\nabbreviations (95.57%), and typographical errors (94.18%). For full real-\nworld clinical notes, our enhanced ﬁne-tuned model reached 69.20% on\nexact code matching and 88.85% at the category level.\nError analysis revealed distinct pa tterns across different evaluation\nscenarios. In the base scenario, most error types decreased afterﬁne-tuning,\nalthough hierarchy-level errors rose slightly as models attempted more\nprecise coding. Balanced hierarchical training may help reﬁne the matching\naccuracy at the granular code level. In full clinical notes, challenges emerged\nin synthesizing scattered clinical detai ls (chief complaints, present illness,\nphysical examination, etc.) and performing complex clinical reasoning. This\nlimitation could be addressed by integrating additional external information\nsuch as lab results and imaging reports. Clinical reasoning errors manifested\nin dif ﬁculties with implicit information understanding, temporal logic\nprocessing, multi-condition synthesis, and uncertainty interpretation. For\ninstance, the models sometimes failed to infer that “stable vital signs” in a\npostoperative patient indicate good recovery or that“mild elevation of blood\npressure” in a pregnant woman could suggest preeclampsia risk. Further-\nmore, they also struggled with tempor al relationships (history, current\nepisode, treatment process), accurat ely differentiating symptoms from\nvarious periods, and handling multi-condition logic. Additionally, doctors’\nuncertainty phrases (e.g., “consider,”“ to be excluded ”) were often mis-\ninterpreted. Addressing these deeper reasoning challenges will require\nstructured reasoning templates, enhanced training data capturing complex\ntemporal and conditional relationship s, standardized outputs (e.g., con-\nﬁdence levels), and layered evaluation mechanisms.\nDespite the promising results, the approach has limitations, including\nthe substantial computational resources and time required for ﬁne-tuning,\nas well as the need for extensive traini ng data to achieve optimal perfor-\nmance. Although the models performwell on many tasks, manual review by\ndomain experts remains essential, pa rticularly for complex or nuanced\ncases. Importantly, our approach is intended to support human coders,\nreduce manual burdens, and allow coders to focus on veri ﬁcation rather\nthan exhaustive coding tasks. Another limitation of our study is that,\nalthough we now include a signi ﬁcant portion of less frequently assigned\nICD-10 codes appearing in discharge s ummaries, explicit evaluation on\ntruly rare codes remains unexplored.\nFuture work will aim to address these limitations and broaden the\napplicability of our approach. We plan to evaluate the proposed approach\non other LLMs, develop advanced prompt engineering techniques, and\nevaluate these systems in real-world clinical environments. We also plan to\nfurther investigate the capabilities of generative LLMs in handling genuinely\nrare codes, leveraging their contextual reasoning advantages over traditional\nsupervised methods that require substantial training examples. This direc-\ntion could further enhance the generali zability and clinical applicability of\nautomated medical coding systems. Additionally, exploring interoperability\nacross healthcare standards, such as Fast Healthcare Interoperability\nResources (FHIR)45 and Observational Medical Outcomes Partnership\n(OMOP)46,47, to automate data translation and integration. By enabling\nLLMs to seamlessly handle diverse heal thcare data formats, this approach\nhas the potential to transform how informationﬂows across the healthcare\necosystem.\nTable 2 | Performance of enhancedﬁne-tuned Llama-3.2-1B on 10,000 full real-world clinical notes by ICD-10 code\nTest\nCodes\nTotal\nCodes\nExact Matched\nCodes (Rate)\nExact Match 95% CIa | P-valuee Categorical Matched\nCodes (Rate)\nCategorical Match 95% CIa | P-valuee\nTop-1b 2211 1530 (69.2%) [67.42, 71.09] | p < 0.001 1,927 (87.16%) [85.70, 88.49] | p < 0.001\nTop-2 4410 2860 (64.85) [63.43, 66.25] | p < 0.001 3,484 (79.02%) [77.79, 80.20] | p < 0.001\nTop-3 6554 4219 (64.37) [63.21, 65.52] | p < 0.001 5,042 (76.94%) [75.91, 77.95] | p < 0.001\nTop-4 8590 5521 (64.27) [63.25, 65.28] | p < 0.001 6,510 (75.81%) [74.90, 76.71] | p < 0.001\naCI indicates conﬁdence interval, calculated using the Wilson score interval method.\nbTop-1 indicates the model’s ﬁrst prediction matches the ground truth.\ncExact Code Match requires complete ICD-10 code matching.\ndCategory Match only requires matching of theﬁrst three characters of the ICD-10 code.\neThe P-value indicates the statistical signiﬁcance of the accuracy being better than expected random. A smallerp-value suggests stronger evidence of non-random prediction performance.\nhttps://doi.org/10.1038/s44401-025-00018-3 Article\nnpj Health Systems|            (2025) 2:14 3\nIn this study, we introduced a novel two-stage ﬁne-tuning approach\nthat enables LLMs to effectively automate medical coding tasks across\ndiverse real-world clinical documentation scenarios. Compared to\nexisting works using LLMs for medical coding, which primarily evaluate\nmodels based on parametric knowledge rather than real-world clinical\ncomplexity, our method provides a more comprehensive framework that\nsystematically addresses realistic medical coding challenges frequently\nencountered in clinical practice. Our ﬁndings validate our hypothesis that\nspecialized medical coding knowledge can be effectively imparted to\nLLMs through ﬁne-tuning, and overcoming challenges such as token\nlimitations, reliance on pre-selected code sets, and character-level hal-\nlucination. Moreover, our approach highlights the capability of ﬁne-\ntuned LLMs to accurately map clinical entities to standardized codes,\naligning closely with the goals of interoperable electronic health record\nsystems. Moreover, our methodology lays a robust foundation for\nextending these techniques to other coding systems and diverse health-\ncare datasets, thereby supporting more precise, ef ﬁcient, and interoper-\nable healthcare data management.\nFig. 1 | Error distribution pattern comparison across model scales and training\nstages. Part a details errors for GPT-4o mini, Part b for Llama-3.2-1B, and Part\nc compares Llama-3.2-3B and Llama-3.2-8B. Non-existence errors potentially cause\nclaim rejections and billing failures in healthcare systems. Hierarchy-level errors\naffect reimbursement amounts and clinical decision support by missing diagnostic\nspeciﬁcity. Quantity mismatches fail to capture critical comorbidities that in ﬂuence\ntreatment plans. Character errors involve transposed digits or typographical mis-\ntakes in codes that may lead to incorrect billing or documentation, affecting data\nintegrity. The higher error counts in Llama models are due to the inclusion of 10,000\nadditional MIMIC test samples in their evaluation. All models demonstrate similar\nerror reduction trends, with enhanced ﬁne-tuning delivering further improvements\nacross all error categories.\nhttps://doi.org/10.1038/s44401-025-00018-3 Article\nnpj Health Systems|            (2025) 2:14 4\nMethod\nTo effectively impart medical coding knowledge to LLMs, we designed a\ntwo-stepﬁne-tuning framework (Fig.2). In the initial stage, we leveraged the\ncomplete ICD-10 code set comprising74,260 code-description pairs across\nA–Z classiﬁcations and structured the training process as input-output pairs\n(e.g., pairing code “E11.9” with “Type 2 diabetes mellitus without compli-\ncations”. The enhanced ﬁne-tuning stage targets four coding challenges\narising from linguistic and lexical variations in clinical notes. For each\nvariation type, specialized training objectives and targeted data augmenta-\ntion strategies were implemented.\nData preparation\nWe prepared two datasets to be used in the initialﬁne-tuning and enhanced\nﬁne-tuning procedures. For the base scenario, we utilized the complete ICD-\n10-CM code-description pairs for trai ning and testing, ensuring full cov-\nerage of all ICD-10 codes.\nAs illustrated in Fig.2 (left), for linguistic and lexical variation scenarios\n(excluding clinical notes), we developed a pipeline to generate clinically\nrelevant test data. First, for each vari ation type, we created 10 high-quality\nexample pairs using Claude (e.g., “Generate ICD-10 descriptions that\nincorporate one or more standard medical abbreviations, such as converting\n‘unspeciﬁed’ to ‘unspec.’, while maintaining word order and meaning ”).\nThese examples were then used directly as training samples toﬁne-tune the\nmodel (see Supplementary Table 1, Supplementary Table 2, Supplementary\nTable 3, Supplementary Table 4, and Supplementary Table 5 for detailed\nprompts)\n48. Using these curated examples as templates, we then generated\n20,000 samples via Application Progr amming Interface (API) calls, pre-\nserving clinical authenticity and coding complexity (see Supplementary\nTable 6 for detailed prompts and gen eration methodology); the samples\nwere equally split into 10,000 each for training and testing, For each var-\niation scenario, we used completely d istinct ICD-10 code sets between the\ntraining and testing datasets to robustly assess generalization performance.\nThe data generated encompasses several variations:\n1. Reordered Diagnostic Expressio ns: Rearranging word order in stan-\ndard ICD-10 descriptions while preserving semantic meaning, to\nevaluate the model’s adaptability to varied phrasing.\n2. Typographical Errors: Injecting sp elling errors based on text length\n38\n(shorter descriptions <10 words receive 1–2 errors, while longer ones\n>20 words may receive up to 4 errors 39), to assess robustness to\nmisspellings.\n3. Medical Abbreviations: Replacing standard medical terms with com-\nmon abbreviations (e.g., “unspeciﬁed” to “unspec.”), testing the\nmodel’s capacity to interpret frequently used clinical shorthand.\n4. Multiple Concurrent Conditions: Combining two to ﬁve ICD-10\ndescriptions to reﬂect comorbidities to evaluate the model’s ability to\nprocess comorbidities.\n5. Sentences with Single Embedded Diagnostic Information: Embedding\nICD-10 descriptions into approximately 23 words, a length sufﬁcient to\neffectively represent a complete diagnostic context while maintaining\nefﬁciency49,t ot e s tt h em o d e l’s ability to extract relevant entities and\ngenerate corresponding codes.\nTable 3 presents examples of each test type along with their corre-\nsponding standard codes, which aim to comprehensively evaluate the\nmodel’s capability to handle diverse and challenging clinical scenarios10,50–52.\nFor the full real-world clinical notes coding scenario, we utilized discharge\nsummaries from the Medical Information Mart for Intensive Care\n(MIMIC)-IV dataset—a large, publicly available database containing de-\nidentiﬁed electronic health records from intensive care units at the Beth\nIsrael Deaconess Medical Center in Boston. Due to MIMIC data use\nrestrictions prohibiting data sha ring with external services such as\nChatGPT, this evaluation used only open-source Llama models in a local\nsetting\n48. To assess the accuracy of our coding approach, we concentrated on\nthe top four ICD codes ordered by their priority in each discharge summary,\nas these codes typically represent th e most critical diagnoses for each\nadmission documented by MIMIC\n53. To ensure robustness and avoid\nFig. 2 | Two-stageﬁne-tuning framework initial knowledge integration and\nenhanced clinical variation processing.This ﬁgure outlines our ﬁne-tuning pro-\ncess. The left side focuses on preparing data to handle variations in how clinical text\nis written. The middle section shows the training process, starting with learning from\nthe full ICD-10 dataset and then ﬁne-tuning further for speci ﬁc clinical variations.\nThe right side provides examples that illustrate how accuracy in coding improves\nacross different medical scenarios.\nhttps://doi.org/10.1038/s44401-025-00018-3 Article\nnpj Health Systems|            (2025) 2:14 5\noverﬁtting, we randomly selected 10,000 discharge summaries from the\nMIMIC-IV dataset for training. For evaluation, we speci ﬁcally tested on\n2211 discharge summaries from a separate set of 10,000 randomly selected\nrecords, restricting our test cases to a dmissions primarily related to circu-\nlatory system diseases, given their high prevalence and clinical signiﬁcance\nin intensive care settings\n54. We performed statistical checks to compare the\ntraining and testing sets. At the diagnostic category level ( ﬁrst three ICD\ncharacters), chi-square tests revealed signi ﬁcant differences ( χ² = 985.5,\np < 0.001). Low-frequency codes (occurring in <0.1% of records) constitute\napproximately 34.60% of unique codes, ensuring evaluation across both\ncommon and uncommon diagnostic patterns. A detailed code distribution\nacross training and testing datasets are provided in the Supplementary Fig. 1\nand Supplementary Note 1. These results con ﬁrm that the training and\ntesting sets capture distinct real-wor ld variability in diagnostic patterns,\nsupporting the generalizability of our work.\nExperiment setup\nThis study evaluated both open-source and proprietary LLMs to examine\nhow different architectures and model sizes respond to ﬁne-tuning for\nmedical coding tasks. From the open-source Llama series, we selected\nLlama3 models with one billion, three billion, and eight billion parameters to\ninvestigate the impact of model size. For proprietary models, we employed\nthe GPT-4o mini (released July 2024), which is based on the latest GPT-4\narchitecture. Although GPT-4 demo nstrated superior performance, its\nrestricted API led us to choose GPT-4o mini for its sufﬁcient performance\n41.\nFor enhancedﬁne-tuning, we selected GPT-4o mini and Llama-3.2-1B; the\nlatter’s minimal memory footprint (2GB GPU), is ideal for local deployment\nin resource-constrained environments, while GPT-4o mini serves as our\ncloud-based option, balancing performance and computational cost55,56.\nExperiments were conducted in two environments: GPT-4o mini\nwas ﬁne-tuned on OpenAI ’s cloud platform with a learning rate multi-\nplier of 1.8, while Llama models were locally ﬁne-tuned on four NVIDIA\nA100 GPUs (80GB each) using Llama Factory 57 with DeepSpeed58 opti-\nmization. Following Soroush et al.‘s ﬁndings that ICD-10 descriptions can\nbe effectively processed with limited token length, we used full parameter\nﬁne-tuning with a maximum sequence length of 50 tokens\n33. The number\nof epochs was set to 10, and the base batch size was set to one per device\nwith dynamic adjustment based on memory utilization\n59. We employed\nthe AdamW optimizer with a learning rate of 1e -5 and a cosine learning\nrate scheduler60.\nEvaluation prompting and metrics\nWe adopted a two-stage prompt structure (Fig. 2). The system prompt sets\nthe model ’sr o l e : “You are a medical coding specialist responsible for\nassigning ICD-10 codes to clinical documentation”, and the input prompt\nspeciﬁes the task type ( “Generate appropriate ICD-10 codes from clinical\nnotes” or “Generate appropriate ICD-10 codes based on standard\ndescriptions”), depending on the scenario.\nWe rigorously assessed performance with exact code matching,\nrequiring correct identiﬁcation of both the medical entity (NER match) and\nits corresponding ICD-10 code (code match) and category matching with the\ncategory code level (e.g.,“E11” for diabetes) to determine whether the model\nidentiﬁed the correct disease category. We calculated the exact match accu-\nracy to statistically analyze model performance. The P-value indicates the\nstatistical signiﬁcance of the accuracy being better than random prediction.\nWe additionally employed the Top‑N accuracy (with N ranging from one to\nfour based on prediction rank priority\n53) and Mean Reciprocal Rank (MRR)\nin full real‑world clinical note scenarios, where medical codes are assigned in\npriority order. For each metric, we present both the accuracy percentage and\nits 95% conﬁdence interval (CI). The Top-N metric reﬂects the percentage of\ninstances in which the correct code appears within the top N predicted codes,\nwhile MRR helps evaluate the overall ranking quality of our predictions.\nData availability\nData from this study were derived from the MIMIC-IV discharge sum-\nmaries, which are available upon ap proved request through PhysioNet\nhttps://physionet.org/content/mimiciv/. Our implementation code, data\nanalysis scripts, and data index used for training and testing are available at\nhttps://github.com/hzvictor/LLMCoder.\nCode availability\nOur implementation code and data analysis scripts used for training and\ntesting are available at https://github.com/hzvictor/LLMCoder.\nReceived: 1 March 2025; Accepted: 11 April 2025;\nTable 3 | Test scenario examples with standard ICD-10 codes\nTest Type Standard Description Example Description Code\nBase Type 2 diabetes mellitus without complications Type 2 diabetes mellitus without complications E119\nReordered Diagnostic\nExpressions\nBurkitt lymphoma, intrathoracic lymph nodes Intrathoracic lymph nodes Burkitt lymphoma C8372\nTypographical Errors Foreign body granuloma of soft tissue, not elsewhere\nclassiﬁed, unspeciﬁed hand\nForiegn body granuloma of soft tisue, not classifed\nelsewhere, unspeciﬁd hand\nM6024\nMedical Abbreviations Acute respiratory failure, unspeci ﬁed whether with\nhypoxia or hypercapnia\nAcute resp. failure, unspec. whether w/ hypoxia or\nhypercap.\nJ9600\nMultiple Concurrent\nConditions\nExternal constriction of vagina and vulva, initial\nencounter. Phocomelia, unspeciﬁed limb(s)\nExternal constriction of vagina and vulva, initial\nencounter. Phocomelia, unspeciﬁed limb(s)\nS30844A, Q731\nSentences with Single\nEmbedded Diagnostic\nInformation\nTerrorism involvingﬁrearms, public safety ofﬁcial\ninjured, sequela\nPublic safety ofﬁcial, Mike, was injured as a result of\nterrorism involvingﬁrearms, and he has sequela\nrelated to that incident.\nY384X1S\nFull Real-world\nClinical Notes\nAnemia, unspeciﬁed. Hyperlipidemia, unspeciﬁed.\nEssential (primary) hypertension. Portal vein\nthrombosis. Diverticulosis of intestine, part\nunspeciﬁed, without perforation or abscess without\nbleeding. Phlebitis of portal vein. Other speciﬁed\ndiseases of the liver. Atopic dermatitis, unspeciﬁed.\nOther microscopic hematuria. Personal history of\nnicotine dependence. Presence of intraocular lens.\n___ man with HTN, HLD, atopic dermatitis [omitted\nother past medical history - 82 words] presenting with\nabdominal bloating and nausea for several weeks. CT\nshowed portal vein thrombi [omitted detailed imaging\nﬁndings - 45 words]. [omitted detailed lab results -\n156 words] Diagnosed with pylephlebitis, treated\nwith anticoagulation (rivaroxaban) and antibiotics\n(Cipro/Flagyl) [omitted speciﬁc dosing - 38 words].\n[omitted detailed physical exams - 112 words]\nDischarged home with 4-week antibiotic course and\nfollow-up plan [omitted detailed follow-up\ninstructions - 218 words].\nI10, D649, I81, E785,\nK5790, K751, K7689,\nL209, R3129,\nZ87891, Z961\nhttps://doi.org/10.1038/s44401-025-00018-3 Article\nnpj Health Systems|            (2025) 2:14 6\nReferences\n1. Organization, W. H. International Statistical Classiﬁcation of Diseases\nand related health problems: Alphabetical index. Vol. 3 (World Health\nOrganization, 2004).\n2. O ’malley, K. J. et al. Measuring diagnoses: ICD code accuracy.Health\nServ. Res.40, 1620–1639 (2005).\n3. Herland, M., Khoshgoftaar, T. M. & Bauder, R. A. Big Data fraud detection\nu s i n gm u l t i p l em e d i c a r ed a t as o u r c e s .Big Data J.5, 29 (2018).\n4. Jetté, N., et al. The development, evolution, and modiﬁcations of ICD-\n10: challenges to the international comparability of morbidity data.\nMed. Care48, 1105–1110 (2010).\n5. Dong, H. et al. Automated clinical coding: what, why, and where we\nare?. NPJ Digit. Med.5, 159 (2022).\n6. Gooch, P. & Roudsari, A. Computerization of workﬂows, guidelines,\nand care pathways: a review of implementation challenges for\nprocess-oriented health information systems.J. Am. Med. Inf. Assoc.\n18, 738–748 (2011).\n7. Jensen, P. B., Jensen, L. J. & Brunak, S. Mining electronic health\nrecords: towards better research applications and clinical care.Nat.\nRev. Genet.13, 395–405 (2012).\n8. Spasic, I. & Nenadic, G. Clinical Text Data in Machine Learning:\nSystematic Review.JMIR Med. Inf.8, e17984 (2020).\n9. Tseng, P., Kaplan, R. S., Richman, B. D., Shah, M. A. & Schulman, K. A.\nAdministrative costs associated with physician billing and insurance-\nrelated activities at an academic health care system.JAMA 319,\n691–697 (2018).\n10. Burns, E. M. et al. Systematic review of discharge coding accuracy.J.\nPublic Health34, 138–148 (2011).\n11. Grand View Research, I. U.S. Medical Coding Market Size, Share &\nTrends Analysis Report By Classiﬁcation System (ICD, HCPCS, CPT),\nBy Component, By Medical Specialty, And Segment Forecasts, 2023 -\n2030, https://www.grandviewresearch.com/industry-analysis/us-\nmedical-coding-market(2022).\n12. Farkas, R. & Szarvas, G. Automatic construction of rule-based ICD-9-\nCM coding systems.BMC Bioinforma.9, S10 (2008).\n13. Z h o u ,L . ,C h e n g ,C . ,O u ,D .&H u a n g ,H .C o n s t r u c t i o no fas e m i - a u t o m a t i c\nICD-10 coding system.BMC Med Inf. Decis. Mak.20, 67 (2020).\n14. Jamian, L., Wheless, L., Crofford, L. J. & Barnado, A. Rule-based and\nmachine learning algorithms identify patients with systemic sclerosis\naccurately in the electronic health record.Arthritis Res. Ther.21, 305\n(2019).\n15. Mykowiecka, A., Marciniak, M. & Kupść,A .R u l e - b a s e di n f o r m a t i o n\nextraction from patients’\nclinical data.J. Biomed. Inf.42,9 2 3–936 (2009).\n16. Bodenreider, O. The Uniﬁed Medical Language System (UMLS):\nintegrating biomedical terminology.Nucleic Acids Res.32,\nD267–D270 (2004).\n17. Perotte, A. et al. Diagnosis code assignment: models and evaluation\nmetrics. J. Am. Med Inf. Assoc.21, 231–237 (2013).\n18. Kavuluru, R., Rios, A. & Lu, Y. An empirical evaluation of supervised\nlearning approaches in assigning diagnosis codes to electronic\nmedical records.Artif. Intell. Med.65, 155–166 (2015).\n19. Hu, S., Teng, F., Huang, L., Yan, J. & Zhang, H. An explainable CNN\napproach for medical codes prediction from clinical text.BMC Med.\nInf. Decis. Mak.21, 256 (2021).\n20. Zhang, Z. et al. BERT-XML: Large scale automated ICD coding using\nBERT pretraining. In Proc. 3rd Clinical Natural Language Processing\nWorkshop, 24–34. https://doi.org/10.18653/v1/2020.clinicalnlp-1.3\n(2020).\n21. Mustafa, A. & Rahimi Azghadi, M. Clustered Automated Machine\nLearning (CAML) model for clinical coding multi-label classiﬁcation.\nInt. J. Mach. Learn. Cybern.16, 1507–1529 (2025).\n22. Huang, C.-W., Tsai, S.-C. & Chen, Y.-N. PLM-ICD: Automatic ICD\nCoding with Pretrained Language Models. In Proceedings of the 4th\nClinical Natural Language Processing Workshop.Edition edn 10-20\n(Association for Computational Linguistics, Published, 2022).\n23. Vu, T., Nguyen, D. Q. & Nguyen, A. A label attention model for ICD\ncoding from clinical text.in Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artiﬁcial Intelligence.Edition edn\n(Published, 2021).\n24. Kundeti, S. R., Vijayananda, J., Mujjiga, S. & Kalyan, M. Clinical named\nentity recognition: Challenges and opportunities. In 2016 IEEE\nInternational Conference on Big Data.Edition edn 1937-1945\n(Published, 2016).\n25. Kühnel, L. & Fluck, J. We are not ready yet: limitations of state-of-the-\nart disease named entity recognizers.J. Biomed. Semant.13,2 6\n(2022).\n26. Hanauer, D. A. et al. Complexities, variations, and errors of\nnumbering within clinical notes: the potential impact on information\nextraction and cohort-identiﬁcation. B M CM e d .I n f .D e c i s .M a k .19,\n75 (2019).\n27. Sohn, S. et al. Clinical documentation variations and NLP system\nportability: a case study in asthma birth cohorts across institutions.J.\nAm. Med. Inf. Assoc.25, 353–359 (2017).\n28. Thirunavukarasu, A. J. et al. Large language models in medicine.Nat.\nMed. 29, 1930–1940 (2023).\n29. Lee, P., Bubeck, S. & Petro, J. Beneﬁts, limits, and risks of GPT-4 as an\nAI chatbot for medicine.N. Engl. J. Med.388, 1233–1239 (2023).\n30. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620\n, 172–180 (2023).\n31. Yang, R. et al. Large language models in health care: Development,\napplications, and challenges.Health Care Sci.2, 255–263 (2023).\n32. Ji, S. et al. A uniﬁed review of deep learning for automated medical\ncoding. ACM Comput. Surv.56, 306 (2024).\n33. Soroush, A. et al. Large language models are poor medical coders—\nbenchmarking of medical code querying.NEJM AI1, AIdbp2300040\n(2024).\n34. Li, Y., Wang, H., Yerebakan, H. Z., Shinagawa, Y. & Luo, Y. FHIR-GPT\nenhances health interoperability with large language models.NEJM AI\n1, AIcs2300301 (2024).\n35. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks.Adv. Neural Inf. Process. Syst.33, 9459–9474\n(2020).\n36. Teng, F., Ma, Z., Chen, J., Xiao, M. & Huang, L. Automatic medical\ncode assignment via deep learning approach for intelligent\nhealthcare. IEEE J. Biomed. Health Inf.24, 2506–2515 (2020).\n37. Mustafa, A., Naseem, U. & Rahimi Azghadi, M. Large language models\nvs human for classifying clinical documents.Int. J. Med. Inform.195,\n105800 (2025).\n38. Wong, W. & Glance, D. Statistical semantic and clinician conﬁdence\nanalysis for correcting abbreviations and spelling errors in clinical\nprogress notes.Artif. Intell. Med.53, 171–180 (2011).\n39. Lai, K. H., Topaz, M., Goss, F. R. & Zhou, L. Automated misspelling\ndetection and correction in clinical free-text records.J. Biomed. Inf.\n55, 188–195 (2015).\n40. Xia, Y. et al. Understanding the Performance and Estimating the Cost\nof LLM Fine-Tuning.in 2024 IEEE International Symposium on\nWorkload Characterization.Edition edn 210-223 (Published, 2024).\n41. OpenAi. GPT-4o Mini: Advancing cost-efﬁcient intelligence,https://\nopenai.com/index/gpt-4o-mini-advancing-cost-efﬁcient-\nintelligence/ (2025).\n42. Meta. Llama Models: Open-Source AI for Multilingual and Multimodal\nApplications, https://www.meta.com/llama (2024).\n43. Howe, N. H. R. et al. Exploring Scaling Trends in LLM Robustness.in\nICML 2024 Next Generation of AI Safety Workshop.Edition edn\n(Published, 2024).\n44. Otto Catherine, M. et al. 2020 ACC/AHA Guideline for the\nManagement of Patients With Valvular Heart Disease.J. Am. Coll.\nCardiol. 77, e25–e197 (2021).\n45. International, H. L. S.FHIR Overview, https://hl7.org/fhir/overview.\nhtml (2023).\nhttps://doi.org/10.1038/s44401-025-00018-3 Article\nnpj Health Systems|            (2025) 2:14 7\n46. Stang, P. E. et al. Advancing the science for active surveillance:\nrationale and design for the observational medical outcomes\npartnership. Ann. Intern. Med. 153, 600–606 (2010).\n47. Zhuang, Y. & Zhang, L. Promoting TEFCA with Blockchain\ntechnology: a decentralized approach to patient-centered healthcare\ndata management. In AMIA Annu Symp Proc.Edition edn 824-833\n(Published, 2023).\n48. PhysioNet. PhysioNet Credentialed Health Data Use Agreement,\nhttps://physionet.org/content/mimiciii/view-dua/1.4/ (2025).\n49. Moen, H. et al. Assisting nurses in care documentation: from\nautomated sentence classiﬁcation to coherent document structures\nwith subject headings.J. Biomed. Semant.11, 10 (2020).\n50. Shahbodaghi, A., Moghaddasi, H., Asadi, F. & Hosseini, A.\nDocumentation errors and deﬁciencies in medical records: a\nsystematic review.J. Health Manag.26, 351–368 (2024).\n51. Leaman, R., Khare, R. & Lu, Z. Challenges in clinical natural language\nprocessing for automated disorder normalization.J. Biomed. Inf.57,\n28–37 (2015).\n52. Kuhn, T., Basch, P., Barr, M. & Yackel, T. Clinical Documentation in the\n21st Century: Executive Summary of a Policy Position Paper From the\nAmerican College of Physicians.Ann. Intern Med.162,3 0 1–303 (2015).\n53. Physiology, M. I. T. L. f. C.MIMIC-IV Hospital: diagnoses_icd, https://\nmimic.mit.edu/docs/iv/modules/hosp/diagnoses_icd/ (2023).\n54. Ford, E. S., Greenlund, K. J. & Hong, Y. Ideal cardiovascular health and\nmortality from all causes and diseases of the circulatory system\namong adults in the United States.Circulation 125, 987–995 (2012).\n55. Pal, A. Performance Comparison: Llama-3 Models in Medical and\nHealthcare AI Domains, https://huggingface.co/blog/aaditya/llama3-\nin-medical-domain (2024).\n56. Meta. Llama-3.2-1B-Instruct Model, https://huggingface.co/meta-\nllama/Llama-3.2-1B-Instruct (2025).\n57. Zheng, Y. et al. 400–410 (Association for Computational Linguistics,\nPublished, 2024).\n58. Rasley, J., Rajbhandari, S., Ruwase, O. & He, Y. DeepSpeed: System\nOptimizations Enable Training Deep Learning Models with Over 100\nBillion Parameters. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining.Edition edn\n3505–3506 (Association for Computing Machinery, Published, 2020).\n59. Zhou, Y. & Srikumar, V. A Closer Look at How Fine-tuning Changes\nBERT. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics.Edition edn 1046–1061 (Association for\nComputational Linguistics, Published, 2022).\n60. Loshchilov, I. & Hutter, F. Decoupled Weight Decay Regularization. In\nthe International Conference on Learning Representations.Edition\nedn (Published, 2019).\nAcknowledgements\nThis work was supported in part by the Indiana University Faculty Research\nSupport Program 005962, the National Institute on Aging under grants\nU01AG088076, R01AG083039, and R01AG080991.\nAuthor contributions\nZ.H. implemented the system, including code development and\nexperimental setup, and wrote the main manuscript. H.L. provided technical\nguidance and contributed to method design. J.B. offered operational\nguidance and advised on clinical relevance. X.H. provided additional\ntechnical support and assisted with data analysis. Y.Z. guided strategic\ndirection and co-wrote the manuscript. All authors reviewed and approved\nthe ﬁnal manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s44401-025-00018-3\n.\nCorrespondenceand requests for materials should be addressed to\nYan Zhuang.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s44401-025-00018-3 Article\nnpj Health Systems|            (2025) 2:14 8",
  "topic": "Coding (social sciences)",
  "concepts": [
    {
      "name": "Coding (social sciences)",
      "score": 0.5652649998664856
    },
    {
      "name": "Computer science",
      "score": 0.5432272553443909
    },
    {
      "name": "Domain-specific language",
      "score": 0.44298121333122253
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4164814054965973
    },
    {
      "name": "Programming language",
      "score": 0.15198394656181335
    },
    {
      "name": "Sociology",
      "score": 0.11461776494979858
    },
    {
      "name": "Mathematics",
      "score": 0.09705018997192383
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55769427",
      "name": "Indiana University – Purdue University Indianapolis",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I166088655",
      "name": "Montclair State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1283055418",
      "name": "Indiana University Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1324682000",
      "name": "Regenstrief Institute",
      "country": "US"
    }
  ]
}