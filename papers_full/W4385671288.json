{
  "title": "14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon",
  "url": "https://openalex.org/W4385671288",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5027355573",
      "name": "Kevin Maik Jablonka",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5005389429",
      "name": "Qianxiang Ai",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5092171855",
      "name": "Alexander Al‐Feghali",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5052466374",
      "name": "Shruti Badhwar",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5000741143",
      "name": "Joshua D. Bocarsly",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5055903035",
      "name": "Andres M Bran",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5004932646",
      "name": "Stefan Bringuier",
      "affiliations": [
        "Film Independent"
      ]
    },
    {
      "id": "https://openalex.org/A5021656287",
      "name": "L. Catherine Brinson",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A5019215236",
      "name": "Kamal Choudhary",
      "affiliations": [
        "Material Measurement Laboratory",
        "National Institute of Standards and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5064747379",
      "name": "Defne Çırcı",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A5101971357",
      "name": "Sam Cox",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A5034688782",
      "name": "Wibe A. de Jong",
      "affiliations": [
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5074236095",
      "name": "Matthew L. Evans",
      "affiliations": [
        "Biogen (Belgium)",
        "UCLouvain"
      ]
    },
    {
      "id": "https://openalex.org/A5022678098",
      "name": "Nicolas Gastellu",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5092155963",
      "name": "Jérôme Genzling",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5009486833",
      "name": "M.V. Gil",
      "affiliations": [
        "Instituto Nacional del Carbón"
      ]
    },
    {
      "id": "https://openalex.org/A5039602411",
      "name": "Ankur K. Gupta",
      "affiliations": [
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5100672320",
      "name": "Zhi Hong",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5109614288",
      "name": "Alishba Imran",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5000470186",
      "name": "Sabine Kruschwitz",
      "affiliations": [
        "Federal Institute For Materials Research and Testing"
      ]
    },
    {
      "id": "https://openalex.org/A5019983551",
      "name": "Anne Labarre",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5092171854",
      "name": "Jakub Lála",
      "affiliations": [
        "The Francis Crick Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5069382481",
      "name": "Tao Liu",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5107973662",
      "name": "Steven Ma",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5015180232",
      "name": "Sauradeep Majumdar",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5044384097",
      "name": "G. Merz",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A5035012955",
      "name": "Nicolas Moitessier",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5087448657",
      "name": "Elias Moubarak",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5029963125",
      "name": "Beatriz Mouriño",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5069095834",
      "name": "Brenden G. Pelkie",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5029968236",
      "name": "Michael Pieler",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017606395",
      "name": "Mayk Caldas Ramos",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A5039010572",
      "name": "Bojana Ranković",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5061828831",
      "name": "Samuel G. Rodriques",
      "affiliations": [
        "The Francis Crick Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5026511975",
      "name": "Jacob N. Sanders",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A5028051805",
      "name": "Philippe Schwaller",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5083396472",
      "name": "Marcus Schwarting",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5052476870",
      "name": "Jiale Shi",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5075317126",
      "name": "Berend Smit",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5101639493",
      "name": "Ben E. Smith",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5057838760",
      "name": "Joren Van Herck",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5102858793",
      "name": "Christoph Völker",
      "affiliations": [
        "Federal Institute For Materials Research and Testing"
      ]
    },
    {
      "id": "https://openalex.org/A5021403239",
      "name": "Logan Ward",
      "affiliations": [
        "Argonne National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5019656467",
      "name": "Sean Warren",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5009364117",
      "name": "Benjamin Weiser",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5100755612",
      "name": "Sylvester Zhang",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A5100755613",
      "name": "Xiaoqi Zhang",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5003777448",
      "name": "Ghezal Ahmad Zia",
      "affiliations": [
        "Federal Institute For Materials Research and Testing"
      ]
    },
    {
      "id": "https://openalex.org/A5075908481",
      "name": "Aristana Scourtas",
      "affiliations": [
        "Argonne National Laboratory",
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5001393137",
      "name": "K. J. Schmidt",
      "affiliations": [
        "Argonne National Laboratory",
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5032231503",
      "name": "Ian Foster",
      "affiliations": [
        "Argonne National Laboratory",
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5103115676",
      "name": "Andrew Dickson White",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A5033890428",
      "name": "Ben Blaiszik",
      "affiliations": [
        "Argonne National Laboratory",
        "University of Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2884430236",
    "https://openalex.org/W3104073206",
    "https://openalex.org/W3022530152",
    "https://openalex.org/W2963784900",
    "https://openalex.org/W2968923792",
    "https://openalex.org/W3208687975",
    "https://openalex.org/W3000754666",
    "https://openalex.org/W3203573929",
    "https://openalex.org/W4366140592",
    "https://openalex.org/W2988055229",
    "https://openalex.org/W4225405705",
    "https://openalex.org/W2883583109",
    "https://openalex.org/W2150099651",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4226050570",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4330336443",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4225405251",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W3198449425",
    "https://openalex.org/W4320727261",
    "https://openalex.org/W4319996831",
    "https://openalex.org/W4365211638",
    "https://openalex.org/W4377096651",
    "https://openalex.org/W1971044734",
    "https://openalex.org/W4283031227",
    "https://openalex.org/W4306179830",
    "https://openalex.org/W4378942305",
    "https://openalex.org/W2039827935",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W2955551200",
    "https://openalex.org/W4214842232",
    "https://openalex.org/W2970235642",
    "https://openalex.org/W2143981217",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2809828967",
    "https://openalex.org/W4323030078",
    "https://openalex.org/W2795337745",
    "https://openalex.org/W2953641512",
    "https://openalex.org/W3115677442",
    "https://openalex.org/W3128465392",
    "https://openalex.org/W4281482733",
    "https://openalex.org/W4380552032",
    "https://openalex.org/W4322759378",
    "https://openalex.org/W2068110511",
    "https://openalex.org/W4376166811",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W2769423117",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4365597205",
    "https://openalex.org/W4365211632",
    "https://openalex.org/W1992985800",
    "https://openalex.org/W2140047797",
    "https://openalex.org/W2029667189",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W4312198153",
    "https://openalex.org/W3134045958",
    "https://openalex.org/W4324308135",
    "https://openalex.org/W3128474010",
    "https://openalex.org/W4225009204",
    "https://openalex.org/W4310797001",
    "https://openalex.org/W4311409687",
    "https://openalex.org/W4367365599",
    "https://openalex.org/W3209726219",
    "https://openalex.org/W3168661259",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4324387101",
    "https://openalex.org/W2027788760",
    "https://openalex.org/W4365449354",
    "https://openalex.org/W3206265774",
    "https://openalex.org/W2892094244",
    "https://openalex.org/W4361985736",
    "https://openalex.org/W2985753184",
    "https://openalex.org/W4296553914",
    "https://openalex.org/W4360793310",
    "https://openalex.org/W4307468223",
    "https://openalex.org/W2801555904",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W4304185479",
    "https://openalex.org/W4366736203",
    "https://openalex.org/W4205185581",
    "https://openalex.org/W3102096994",
    "https://openalex.org/W3189385021",
    "https://openalex.org/W3100221827",
    "https://openalex.org/W1596790865"
  ],
  "abstract": "We report the findings of a hackathon focused on exploring the diverse applications of large language models in molecular and materials science.",
  "full_text": "14 examples of how LLMs can transform materials\nscience and chemistry: a reﬂection on a large\nlanguage model hackathon†\nKevin Maik Jablonka, *a Qianxiang Ai, ‡b Alexander Al-Feghali, ‡c\nShruti Badhwar, ‡d Joshua D. Bocarsly, ‡e Andres M. Bran, ‡fg\nStefan Bringuier, ‡h L. Catherine Brinson, ‡i Kamal Choudhary, ‡j\nDefne Circi, ‡i Sam Cox, ‡k Wibe A. de Jong, ‡l Matthew L. Evans, ‡mn\nNicolas Gastellu, ‡c Jerome Genzling, ‡c Mar´ıa Victoria Gil, ‡o\nAnkur K. Gupta, ‡l Zhi Hong, ‡p Alishba Imran,‡q Sabine Kruschwitz, ‡r\nAnne Labarre, ‡c Jakub L´ala, ‡s Tao Liu, ‡c Steven Ma, ‡c\nSauradeep Majumdar, ‡a Garrett W. Merz, ‡t Nicolas Moitessier, ‡c\nElias Moubarak, ‡a Beatriz Mouriño, ‡a Brenden Pelkie, ‡u Michael Pieler, ‡vw\nMayk Caldas Ramos, ‡k Bojana Rankovi´c, ‡fg Samuel G. Rodriques, ‡s\nJacob N. Sanders, ‡x Philippe Schwaller, ‡fg Marcus Schwarting,‡y Jiale Shi, ‡b\nBerend Smit, ‡a Ben E. Smith, ‡e Joren Van Herck, ‡a Christoph Völker, ‡r\nLogan Ward, ‡z Sean Warren, ‡c Benjamin Weiser, ‡c Sylvester Zhang, ‡c\nXiaoqi Zhang, ‡a Ghezal Ahmad Zia, ‡r Aristana Scourtas, aa K. J. Schmidt, aa\nIan Foster, ab Andrew D. White k and Ben Blaiszik *aa\nLarge-language models (LLMs) such as GPT-4 caught the interest of many scientists. Recent studies\nsuggested that these models could be useful in chemistry and materials science. To explore these\npossibilities, we organized a hackathon. This article chronicles the projects built as part of this\nhackathon. Participants employed LLMs for various applications, including predicting properties of\nCite this:Digital Discovery,2 0 2 3 ,2,\n1233\nReceived 12th June 2023\nAccepted 8th August 2023\nDOI: 10.1039/d3dd00113j\nrsc.li/digitaldiscovery\naLaboratory of Molecular Simulation (LSMO), Institut des Sciences et Ing´enierie\nChimiques, Ecole Polytechnique F ´ed´erale de Lausanne (EPFL), Sion, Valais,\nSwitzerland. E-mail: mail@kjablonka.com\nbDepartment of Chemical Engineering, Massachusetts Institute of Technology,\nCambridge, Massachusetts 02139, USA\ncDepartment of Chemistry, McGill University, Montreal, Quebec, Canada\ndReincarnate Inc., USA\neYusuf Hamied Department of Chemistry, University of Cambridge, Lenseld Road,\nCambridge, CB2 1EW, UK\nfLaboratory of Arti cial Chemical Intelligence (LIAC), Institut des Sciences et\nIng´enierie Chimiques, Ecole Polytechnique F ´ed´erale de Lausanne (EPFL),\nLausanne, Switzerland\ngNational Centre of Competence in Research (NCCR) Catalysis, Ecole Polytechnique\nF´ed´erale de Lausanne (EPFL), Lausanne, Switzerland\nhIndependent Researcher, San Diego, CA, USA\niMechanical Engineering and Materials Science, Duke University, USA\njMaterial Measurement Laboratory, National Institute of Standards and Technology,\nMaryland, 20899, USA\nkDepartment of Chemical Engineering, University of Rochester, USA\nlApplied Mathematics and Computational Research Division, Lawrence Berkeley\nNational Laboratory, Berkeley, CA 94720, USA\nmInstitut de la Mati`ere Condens´ee et des Nanosciences (IMCN), UCLouvain, Chemin\ndes ´Etoiles 8, Louvain-la-Neuve, 1348, Belgium\nnMatgenix SRL, 185 Rue Armand Bury, 6534 Goz´ee, Belgium\noInstituto de Ciencia y Tecnolog´ıa del Carbono (INCAR), CSIC, Francisco Pintado Fe\n26, 33011 Oviedo, Spain\npDepartment of Computer Science, University of Chicago, Chicago, Illinois 60637,\nUSA\nqComputer Science, University of California, Berkeley, CA 94704, USA\nrBundesanstalt für Materialforschung und -prüfung, Unter den Eichen 87, 12205\nBerlin, Germany\nsFrancis Crick Institute, 1 Midland Rd, London NW1 1AT, UK\ntAmerican Family Insurance Data Science Institute, University of Wisconsin –\nMadison, Madison, WI 53706, USA\nuDepartment of Chemical Engineering, University of Washington, Seattle, WA 98105,\nUSA\nvOpenBioML.org, UK\nwStability.AI, UK\nxDepartment of Chemistry and Biochemistry, University of California, Los Angeles,\nCA 90095, USA\nyDepartment of Computer Science, University of Chicago, Chicago, IL 60490, USA\nzData Science and Learning Division, Argonne National Lab, USA\naaGlobus, University of Chicago, Data Science and Learning Division, Argonne\nNational Lab, USA. E-mail: blaiszik@uchicago.edu\nabDepartment of Computer Science, University of Chicago, Data Science and Learning\nDivision, Argonne National Lab, USA\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d3dd00113j\n‡ These authors contributed equally.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1233\nDigital\nDiscovery\nPERSPECTIVE\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nmolecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data,\nand developing new educational applications. The diverse topics and the fact that working prototypes could\nbe generated in less than two days highlight that LLMs will profoundly impact the future of ourﬁelds. The\nrich collection of ideas and projects also indicates that the applications of LLMs are not limited to materials\nscience and chemistry but oﬀer potential beneﬁts to a wide range of scientiﬁc disciplines.\n1. Introduction\nThe intersection of machine learning (ML) with chemistry and\nmaterials science has witnessed remarkable advancements in\nrecent years.\n1–9 Much progress has been made in using ML to,\ne.g., accelerate simulations10,11 or to directly predict properties or\ncompounds for a given application. 12 Thereby, developing\ncustom, hand-craed models for any given application is still\ncommon practice. Since science rewards doing novel things for\nthe rst time, we now face a deluge of tools and machine-learning\nmodels for various tasks. These tools commonly require input\ndata in their ownrigid, well-dened form(e.g., a table with specic\ncolumns or images from a specic microscope with specic\ndimensions). Further, they typically also report their outputs in\nnon-standard and sometimes proprietary forms.\nThis rigidity sharply contrasts the standard practice in the\n(experimental) molecular and materials sciences, which is\nintrinsically fuzzy and highly context-dependent.\n13 For instance,\nresearchers have many ways to refer to a molecule (e.g., IUPAC\nname, conventional name, simpli ed molecular-input line-\nTable 1 Overview of the developed tools and links to source code repositories. Full descriptions of the projects can be found in the ESI\nName Authors Links\nPredictive modeling\nAccurate molecular\nenergy predictions\nAnkur K. Gupta, Garrett W. Merz,\nAlishba Imran, Wibe A. de Jong\nhttps://doi.org/10.5281/zenodo.8104930\nText2Concrete Sabine Kruschwitz, Christoph Völker,\nGhezal Ahmad Zia\nhttps://ghezalahmad/LLMs-for-the-Design-of-\nSustainable-Concretes\nhttps://doi.org/10.5281/zenodo.8091195\nMolecule discovery by context Zhi Hong, Logan Ward\n https://globuslabs/ScholarBERT-XL\nhttps://doi.org/10.5281/zenodo.8122087\nGenetic algorithm without genes Benjamin Weiser, Jerome\nGenzling, Nicolas Gastellu,\nSylvester Zhang, Tao Liu,\nAlexander Al-Feghali,\nNicolas Moitessier, Anne Labarre,\nSteven Ma\nhttps://BenjaminWeiser/LLM-Guided-GA\nhttps://doi.org/10.5281/zenodo.8125541\nText-template paraphrasing Michael Pieler\n https://micpie/text-template-paraphrasing-chemistry\nhttps://doi.org/10.5281/zenodo.8093615\nAutomation and novel interfaces\nBOLLaMa Bojana Rankovi ´c, Andres M. Bran,\nPhilippe Schwaller\nhttps://doncamilom/BOLLaMa\nhttps://doi.org/10.5281/zenodo.8096827\nsMolTalk Jakub L ´ala, Sean Warren,\nSamuel G. Rodriques\nhttps://jakublala/smoltalk-legacy\nhttps://doi.org/10.5281/zenodo.8081749\nMAPI-LLM Mayk Caldas Ramos, Sam Cox,\nAndrew White\nhttps://maykcaldas/MAPI_LLM\nhttps://maykcaldasMAPI_LLM\nhttps://doi.org/10.5281/zenodo.8097336\nConversational electronic lab\nnotebook (ELN) interface (\n )\nJoshua D. Bocarsly, Matthew\nL. Evans and Ben E. Smith\nhttps://the-grey-group/datalab\nhttps://doi.org/10.5281/zenodo.8127782\nKnowledge extraction\nInsightGraph Defne Circi, Shruti Badhwar\nhttps://defnecirci/InsightGraph\nhttps://doi.org/10.5281/zenodo.8092575\nExtracting structured data from\nfree-form organic synthesis text\nQianxiang Ai, Jacob N. Sanders,\nJiale Shi, Stefan Bringuier,\nBrenden Pelkie, Marcus Schwarting\nhttps://qai222LLM_organic_synthesis\nhttps://doi.org/10.5281/zenodo.8091902\nTableToJson: structured information\nfrom scientic data in tables\nMar´ıa Victoria Gil\n https://vgvinter/TableToJson\nhttps://doi.org/10.5281/zenodo.8093731\nAbstractToTitle & TitleToAbstract: text\nsummarization and generation\nKamal Choudhary\nhttps://usnistgov/chemnlp\nhttps://doi.org/10.5281/zenodo.8122419\nEducation\nI-Digest Beatriz Mouriño, Elias Moubarak,\nJoren Van Herck,\nSauradeep Majumdar, Xiaoqi Zhang\nhttps://XiaoqZhang/i-Digest\nhttps://doi.org/10.5281/zenodo.8080962\n1234 | Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nentry system (SMILES)14) and to report results and procedures.\nIn particular, for the latter, it is known that small details such as\nthe order of addition or the strength of stirring (e.g., “gently”vs.\n“strongly”) are crucial in determining the outcome of reactions.\nWe do not have a natural way to deal with this fuzziness, and\noen a conversion into structured tabular form (the conven-\ntional input format for ML models) is impossible. Our current\n“solution” is to write conversion programs and chain many\ntools with plenty of application-specic “glue code” to enable\nscientic workows. However, this fuzziness of chemistry and\nheterogeneity of tools have profound consequences: a never-\nending stream of newle formats, interfaces, and interopera-\nbility tools exists, and users cannot keep up with learning.\n15 In\naddition, almost any transformation of highly context-\ndependent text (e.g., description of a reaction procedure) into\nstructured, tabular form will lead to a loss of information.\nOne of the aims of this work is to demonstrate how large\nlanguage models (LLMs) such as the generative pretrained\ntransformer (GPT)-4,\n16–21 can be used to address these chal-\nlenges. Foundation models such as GPTs are general-purpose\ntechnologies\n22 that can solve tasks they have not explicitly\nbeen trained on,23,24 use tools,25–27 and be grounded in knowl-\nedge bases.28,29 As we also show in this work, they provide new\npathways of exploration, new opportunities forexible inter-\nfaces, and may be used to eﬀectively solve certain tasks them-\nselves; e.g., we envision LLMs enabling non-experts to program\n(“malleable so ware”) using natural language as the\n“programming language”,30 extract structured information, and\ncreate digital assistants that make our tools interoperable— all\nbased on unstructured, natural-language inputs.\nInspired by early reports on the use of these LLMs in\nchemical research,31–34 we organized a virtual hackathon event\nfocused on understanding the applicability of LLMs to mate-\nrials science and chemistry. The hackathon aimed to explore\nthe multifaceted applications of LLMs in materials science and\nchemistry and encourage creative solutions to some of the\npressing challenges in theeld. This article showcases some of\nthe projects (Table 1) developed during the hackathon.\nOne of the conclusions of this work is that without these\nLLMs, such projects would take many months. The diversity of\ntopics these projects address illustrates the broad applicability\nof LLMs; the projects touch many diﬀerent aspects of materials\nscience and chemistry, from the wet lab to the computational\nchemistry lab, soware interfaces, and even the classroom.\nWhile the examples below are not yet polished products, the\nsimple observation that such capabilities could be created in\nhours underlines that we need to start thinking about how\nLLMs will impact the future of materials science, chemistry, and\nbeyond.\n35 The diverse applications show that LLMs are here to\nstay and are likely a foundational capability that will be inte-\ngrated into most aspects of the research process. Even so, the\npace of the developments highlights that we are only beginning\nto scratch the surface of what LLMs can do for chemistry and\nmaterials science.\nTable 1 lists the diﬀerent projects created in this collabora-\ntive eﬀort across eight countries and 22 institutions (ESI Section\nV†). One might expect that 1.5 days of intense collaborations\nwould, at best, allow a cursory exploration of a topic. However,\nthe diversity of topics and the diversity in the participants'\nexpertise, combined with the need to deliver a working proto-\ntype (within a short window of time) and the ease of prototyping\nwith LLMs, generated not only many questions but also prag-\nmatic prototypes. The projects were typically carried out in an\nexploratory way and without any evaluation of impact. In the\nremainder of this article, we focus on the insights we obtained\nfrom this collective eﬀort. For the details of each project, we\nrefer to the ESI.† While diﬀerent challenges were explored\nduring this hackathon, the results were preliminary. Digital\nDiscovery did not peer review the soundness of each study.\nInstead, the peer review for this perspective was to scope the\npotential of LLMs in chemistry and materials science.\nWe have grouped the projects into four categories: (1)\npredictive modeling, (2) automation and novel interfaces, (3)\nknowledge extraction, and (4) education. The projects in the\npredictive modeling category use LLMs for classication and\nregression tasks— and also investigate ways to incorporate\nestablished concepts such asD-ML\n36 or novel concepts such as\n“fuzzy” context into the modeling. Theautomation and novel\ninterfaces projects show that natural language might be the\nuniversal “glue” connecting our tools— perhaps in the future,\nwe will need not to focus on new formats or standards but\nrather use natural language descriptions to connect across the\nexisting diversity and diﬀerent modalities.\n35\nLLMs can also help make knowledge more accessible, as the\nprojects in the“knowledge extraction” category show; they can\nextract structured information from unstructured text. In\naddition, as the project in the “education” category shows,\nLLMs can also oﬀer new educational opportunities.\n1.1 Predictive modeling\nPredictive modeling is a common application of ML in chem-\nistry. Based on the language-interfaced ne-tuning (LIFT)\nframework,\n37 Jablonka et al.32 have shown that LLMs can be\nemployed to predict various chemical properties, such as solu-\nbility or HOMO–LUMO gaps based on line representations of\nmolecules such as self-referencing embedded strings (SELF-\nIES)\n38,39 and SMILES. Taking this idea even further, Ramos\net al.34 used this framework (with in-context learning (ICL)) for\nBayesian optimization — guiding experiments without even\ntraining models. These few-shot learning abilities have also\nbeen benchmarked by Guoet al.\n40\nThe projects in the following build on top of those initial\nresults and extend them in novel ways as well as by leveraging\nestablished techniques from quantum machine learning.\nGiven that these encouraging results could be achieved with\nand without ne-tuning (i.e., updates to the weights of the\nmodel) for the language-interfaced training on tabular datasets,\nwe use the term LIFT also for ICL settings in which structured\ndata is converted into text prompts for an LLM.\n1.1.1 Molecular energy predictions. Ac r i t i c a lp r o p e r t yi n\nquantum chemistry is the atomization energy of a molecule,\nwhich gives us the basic thermochemical data used to determine\na molecule's stability or reactivity. State-of-the-art quantum\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1235\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nchemical methods (i.e.,G 4 ( M P 2 )41) can predict this energy with\nan accuracy of 0.034 eV (or 0.79 kcal mol−1).42,43 This accuracy is\nsimilar to, and in some cases even better than, the accuracy that\ncan be reached experimentally. This motivated Ramakrishnan\net al.\n42 and Narayanan et al.43 to compute these atomization\nenergies for the 134 000 molecules in the QM9-G4MP2 dataset.\nThe Berkeley–Madison team (Ankur Gupta, Garrett Merz,\nAlishba Imran, and Wibe de Jong) used this dataset tone-tune\ndiﬀerent LLMs using the LIFT framework. The team investi-\ngated if they could use an LLM to predict atomization energies\nwith chemical accuracy. Jablonkaet al.32 emphasized that these\nLLMs might be particularly useful in the low-data limit. Here,\nwe have a relatively large dataset, so it is an ideal system to\ngather insights into the performance of these models for data-\nsets much larger than those used by Jablonkaet al.\n32\nThe Berkeley–Madison team showed that the LIFT frame-\nwork based on simple line representations such as SMILES and\nSELFIES38,39 can yield good predictions (R2 > 0.95 on a holdout\ntest set), that are, however, still inferior to dedicated models\nthat have access to 3D information.\n44,45 An alternative approach\nto achieve chemical accuracy with LLMs tuned only on string\nrepresentations is to leverage aD-ML scheme\n46 in which the\nLLM is tuned to predict the diﬀerence between G4(MP2) and\nB3LYP47 energies. Table 2 shows that good agreement could be\nachieved for the D-ML approach. This showcases how tech-\nniques established for conventional ML on molecules can also\nbe applied with LLMs.\nImportantly, this approach is not limited to the OpenAI\napplication programming interface (API). With parameter eﬃ-\ncient ne-tuning (PEFT) with low-rank adaptors (LoRA)48 of the\nGPT-2 model,49 one can also obtain comparable results on\nconsumer hardware. These results make the LIFT approach\nwidely more accessible.\n1.1.2 Text2Concrete. Concrete is the most used construc-\ntion material, and the mechanical properties and climate\nimpact of these materials are a complex function of the pro-\ncessing and formulation. Much research is focused on formu-\nlations of concrete that are less CO\n2 intensive.50 To expedite the\ndesign process, e.g., by prioritizing experiments using ML-\npredictions, data-driven methods have been investigated by\nVölker et al.\n51 The Text2Concrete team (Sabine Kruschwitz,\nChristoph Völker, and Ghezal Ahmad Zia) explored, based on\ndata reported by Rao and Rao,52 whether LLMs can be used for\nthis task. This data set provides 240 alternative, more sustain-\nable, concrete formulations and their respective compressive\nstrengths. From a practical point of view, one would like to have\na model that can predict the compressive strength of the\nconcrete as a function of its formulation.\nInterestingly, the largest LLMs can already give predictions\nwithout anyne-tuning. These models can“learn”from the few\nexamples provided by the user in the prompt. Of course, such\na few-shot approach (or ICL,\n20) does not allow for the same type\nof optimization asne-tuning, and one can therefore expect it\nto be less accurate. However, Ramoset al.34 showed that this\nmethod could perform well— especially if only so few data\npoints are available such that ne-tuning is not a suitable\napproach.\nFor their case study, the Text2Concrete team found\na predictive accuracy comparable to a Gaussian process\nregression (GPR) model (but inferior to a random forest (RF)\nmodel). However, one signicant advantage of LLMs is that one\ncan easily incorporate context. The Text2Concrete team used this\nto include well-established design principles like the inuence\nof the water-to-cement ratio on strength (Fig. 1) into the\nmodeling by simply stating the relationship between the\nfeatures in natural language (e.g., “high water/cement ratio\nreduces strength”). This additional context reduced the outliers\nand outperformed the RF model ( R\n2 of 0.67 and 0.72,\nrespectively).\nThe exciting aspect is that this is a typical example of domain\nknowledge that cannot be captured with a simple equation\nincorporable into conventional modeling work ows. Such\n“fuzzy”domain knowledge, which may sometimes exist only in\nthe minds of researchers, is common in chemistry and mate-\nrials science. With the incorporation of such“fuzzy”knowledge\ninto LIFT-based predictions using LLMs, we now have a novel\nand very promising approach to leverage such domain expertise\nthat we could not leverage before. Interestingly, this also may\nprovide a way to test“fuzzy”hypotheses, e.g., a researcher could\ndescribe the hypothesis in natural language and see how it\naﬀects the model accuracy. While the Text2Concrete example\nhas not exhaustively analyzed how“fuzzy” context alterations\nTable 2 LIFT for molecular atomization energies on the QM9-G4MP2 dataset. Metrics for models tuned on 90% of the QM9-G4MP2 dataset\n(117 232 molecules), using 10% (13 026 molecules) as a holdout test set. GPTChem refers to the approach reported by Jablonkaet al.,32 GPT-2-\nLoRA to PEFT of the GPT-2 model using LoRA. The results indicate that the LIFT framework can also be used to build predictive models for\natomization energies, that can reach chemical accuracy using aD-ML scheme. Baseline performance (mean absolute error reported by Ward\net al.45): 0.0223 eV for FCHL-based prediction of GP4(MP2) atomization energies and 0.0045 eV (SchNet) and 0.0052 eV (FCHL) for theD-ML\nscheme\nMol. repr. & framework\nG4(MP2) atomization energy (G4(MP2)-B3LYP) atomization energy\nR2 Median absolute deviation (MAD)/eV R2 MAD/eV\nSMILES: GPTChem 0.984 0.99 0.976 0.03\nSELFIES: GPTChem 0.961 1.18 0.973 0.03\nSMILES: GPT2-LoRA 0.931 2.03 0.910 0.06\nSELFIES: GPT2-LoRA 0.959 1.93 0.915 0.06\n1236\n| Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\naﬀect LLM performance, we recognize this as a key area for\nfuture research.\n1.1.3 Molecule discovery by context. Much context is\navailable in the full text of scientic articles. This has been\nexploited by Tshitoyanet al.53 who used a Word2Vec54 approach\nto embed words into a vector space. Word2Vec does so by\ntasking a model to predict for a word the probability for all\npossible next words in a vocabulary. In this way, word embed-\ndings capture syntactic and semantic details of lexical items\n(i.e., words). When applied to material science abstracts, the\nword embeddings of compounds such as Li\n2CuSb could be used\nfor materials discovery by measuring their distance (cosine\nsimilarity) to concepts such as “thermoelectric”.55 However,\ntraditional Word2Vec, as used by Tshitoyan et al. ,53 only\nproduces static embeddings, which remain unchanged aer\ntraining. Word embeddings extracted from an LLM, on the\nother hand, are contextualized on the specic sequence (sen-\ntence) in which they are used and, therefore, can more eﬀec-\ntively capture the contexts of words within a given corpus.\n56\nInspired by this, the GlobusLabs team (Zhi Hong, Logan Ward)\ninvestigated if similar embeddings could be used to discover\nhydrogen carrier molecules, that are relevant for energy storage\napplications. For this, they leverage the ScholarBert model\n57\ntrained on a large corpus of scientic articles collected by the\npublic.resource.org nonpro t organization. For di ﬀerent\ncandidate molecules, they searched for sentences in the pub-\nlic.resource.org corpus and used the average of the embeddings\nof these sentences as angerprint of the molecules. Given those\nngerprints, they could rank molecules by how close their\nngerprints are to the ones of known hydrogen carrier mole-\ncules. Visual inspection indicates that the selected molecules\nbear similarities to known hydrogen carrier molecules. Note\nthat in this case, molecules are not generatedde novo (as, for\nexample, in Liet al.\n58) but retrieved from existing databases.\n1.1.4 Text template paraphrasing. In the LIFT\nframework used in the examples above, the data are embedded in\nso-called prompt templates that can have a form like\nwhere the texts in chevrons are placeholders that are replaced with\nactual values such as“solubility”and “2-acetyloxybenzoic acid”.I n\nthe low-data regime, data points are “wasted” by the model\nneeding to learn the syntax of the prompt templates. In the big-\ndata regime, in contrast, one might worry that the model loses\nsome of its general language modeling abilities by always dealing\nwith the same template. This naturally raises the question if one\ncan augment the dataset to mitigate these problems— thereby\nleveraging again, similar toD-ML, a technique that has found use\nin conventional ML previously. However, text-based data are\nchallenging to augment due to their discrete nature and the fact\nthat the augmented text still needs to be syntactically and\nsemantically valid. Interestingly, as Michael Pieler ( https://\nwww.openbioml.org and Stability.AI) shows (and as has been\nexplored by Daiet al.\n59), it turns out that LLMs can also be used\nto address this problem by simply prompting an LLM (e.g.,G P T -\n4 or Anthropic's Claude) to paraphrase a prompt template (see\nESI Section ID†).\nThis approach will allow us to automatically create new\nparaphrased high-quality prompts for LIFT-based training very\neﬃciently— to augment the dataset and reduce the risk of\novertting to a specic template. Latter might be particularly\nimportant if one still wants to retain general language abilities\nof the LLMs aer netuning on chemistry or material science\ndata.\n1.1.5 Genetic algorithm using an LLM.Genetic algorithms\nare popular methods for generating new structures; they are\nevolutionary algorithms in which building blocks (e.g., frag-\nments of SMILES strings) are iteratively crossed over, mutated,\nand subjected to other genetic operations to evolve structures\nFig. 1 Using LLMs to predict the compressive strength of concretes. An illustration of the conventional approach for solving this task,i.e., training\nclassical prediction models using ten training data points as tabular data (left). Using the LIFT framework LLMs can also use tabular data and\nleverage context information provided in natural language (right). The context can be“fuzzy” design rules often known in chemistry and materials\nscience but hard to incorporate in conventional ML models. Augmented with this context and ten training examples, ICL with LLM leads to\na performance that outperforms baselines such as RFs or GPR.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1237\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nwith better performance (such as catalysts with higher conver-\nsion).60 The eﬃciency of such a genetic algorithm oen depends\non how well the genes and genetic operations match the\nunderlying chemistry. For example, if the algorithm replaces\natom by atom, it may take several generations before a complete\nfunctional group is replaced.\nOne might hypothesize that LLMs can make the evolution\nprocess more eﬃcient, e.g., by using an LLM to handle the\nreproduction. One might expect that inductive biases in the\nLLM help create recombined molecules which are more chem-\nically viable, maintaining the motifs of the two parent mole-\ncules better than a random operation.\nThe team from McGill University (Benjamin Weiser, Jerome\nGenzling, Nicolas Gastellu, Sylvester Zhang, Tao Liu, Alexander\nAl-Feghali, Nicolas Moitessier) set out therst steps to test this\nhypothesis (Fig. 2). In initial experiments, they found that GPT-\n3.5, without anynetuning, can fragment molecules provided\nas SMILES at rotatable bonds with a success rate of 70%. This\nindicates that GPT-3.5 understands SMILES strings and aspects\nof their relation to the chemical structures they represent.\nSubsequently, they asked the LLMs to fragment and recombine\ntwo given molecules. The LLM frequently created new\ncombined molecules with fragments of each species which were\nreasonable chemical structures more o en than a random\nSMILES string combining operation (two independent organic\nchemists judged the LLM-GA-generated molecules to be\nchemically reasonable in 32/32 cases, but only in 21/32 cases for\nthe random recombination operation).\nEncouraged by thesendings, they prompted an LLM with\n30 parent molecules and their performance scores (Tanimoto\nsimilarity to vitamin C) with the task to come up withn new\nmolecules that the LLM “believes” to improve the score. A\npreliminary visual inspection suggests that the LLM might\nproduce chemically reasonable modications. Future work will\nneed to systematically investigate potential improvements\ncompared to conventional GAs.\nThe importance of the results of the McGill team is that they\nindicate that these LLMs (when suitably conditioned) might not\nonly reproduce known structures but generate new structures\nthat make chemical sense.\n32,61\nA current limitation of this approach is that most LLMs still\nstruggle to output valid SMILES without explicitne-tuning.33\nWe anticipate that this problem might be mitigated by building\nfoundation models for chemistry (with more suitable tokeni-\nzation\n62,63), as, for instance, the ChemNLP project of open-\nbioml.org attempts to do ( https://github.com/OpenBioML/\nchemnlp). In addition, the context length limits the number\nof parent molecules that can be provided as examples.\nOverall, we see that theexibility of the natural language\ninput and the in-context learning abilities allows using LLMs in\nvery diﬀerent ways— to very eﬃciently build predictive models\nor to approach molecular and material design in entirely\nunprecedented ways, like by providing context— such as“fuzzy”\ndesign rules— or simply prompting the LLM to come up with\nnew structures. However, we alsond that some“old” ideas,\nsuch as D-ML and data augmentation, can also be applied in\nthis new paradigm.\nFig. 2 GA using an LLM. Thisﬁgure illustrates how diﬀerent aspects of a GA can be performed by an LLM. GPT-3.5 was used to fragment,\nreproduce, and optimize molecules represented by SMILES strings. Theﬁrst column illustrated how an LLM can fragment a molecule represented\nby a SMILES string (input molecule on top, output LLM fragments below). The middle column showcases how an LLM can reproduce/mix two\nmolecules as is done in a GA (input molecule on top, output LLM below). The right column illustrates an application in which an LLM is used to\noptimize molecules given their SMILES and an associated score. The LLM suggested potential modiﬁcations to optimize molecules. The plot\nshows best (blue) and mean (orange) Tanimoto similarity to vitamin C per LLM produced generations.\n1238 | Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n1.2 Automation and novel interfaces\nYao et al.64 and Schicket al.25 have shown that LLMs can be used\nas agents that can autonomously make use of external tools\nsuch as Web-APIs — a paradigm that some call MRKL\n(pronounced “miracle”) systems— modular reasoning, knowl-\nedge, and language systems.\n26 By giving LLMs access to tools\nand forcing them to think step-by-step,65 we can thereby convert\nLLMs from hypercondent models that oen hallucinate to\nsystems that can reason based on observations made by\nquerying robust tools. As the technical report for GPT-4 high-\nlighted,\n66 giving LLMs access to tools can lead to emergent\nbehavior, i.e., enabling the system to do things that none of its\nparts could do before. In addition, this approach can make\nexternal tools more accessible— since users no longer have to\nlearn tool-specic APIs. It can also make tools more interoper-\nable— by using natural language instead of “glue code” to\nconnect tools.\nThis paradigm has recently been used by Branet al.\n67 to\ncreate digital assistants that can call and combine various tools\nsuch as Google search and the IBM RXN retrosynthesis tool\nwhen prompted with natural language. Boiko et al.\n68 used\na similar approach and gave LLMs access to laboratoriesvia\ncloud lab APIs. In their system, the LLM could use external tools\nto plan a synthesis, which it could execute using the cloud lab.\n1.2.1 MAPI-LLM. Electronic structure calculations have\nreached such a high level of accuracy that one can answer\nquestions like “Is the material AnByCz stable?” Indeed, the\nMaterials Project\n69 stores thermodynamic data on many\ncomponents from which one can obtain a reasonable estimate of\nthe stability of a given material. Or, if the material is not in the\ndatabase, one can do a simulation instead. Similarly, to answer\nprompts such as“Give me a reaction to produce CaCO\n3”,t h e r ei s\na lot of helpful information in the Materials Project database and\nthe internet that can help to come up with an answer.\nTo answer these questions, state-of-the-art computational tools\nor existing databases can be used. However, their use oen\nrequires expert knowledge. To use existing databases, one must\nchoose which database to use, how to query the database, and\nwhat representation of the compound is used (e.g.,i n t e r n a t i o n a l\nchemical identier (InChI), SMILES,etc.). Otherwise, if the data is\nnot in a database, one must run calculations, which requires\na deep understanding of technical details. LLMs can simplify this\nprocess. By typing in a question, we can prompt the LLM to\ntranslate this question into a workow that leads to the answer.\nThe MAPI-LLM team (Mayk Caldas Ramos, Sam Cox, Andrew\nWhite) made therst steps towards developing such a system\n(MAPI-LLM) and created a procedure to convert a text prompt\ninto a query of the Materials Project API (MAPI) to answer\nquestions such as“Is the material AnByCz stable?”In addition,\nMAPI-LLM is capable of handling classication queries, such as\n“Is Fe\n2O3 magnetic?”, as well as regression problems, such as\n“What is the band gap of Mg(Fe2O3)2?”.\nBecause an LLM is used to create the workow, MAPI-LLM\ncan process even more complex questions. For instance, the\nquestion “If Mn\n23FeO32 is not metallic, what is its band gap?”\nshould create a two-step workow rst to check if the material is\nmetallic and then obtain its band gap if it is not.\nMoreover, MAPI-LLM applies ICL if the data for a material's\nproperty is unavailablevia the MAPI. MAPI-LLM generates an\nICL prompt, building context based on the data for similar\nFig. 3 Schematic overview of the MAPI-LLM workﬂow. It uses LLMs to process the user's input and decide which available tools (e.g., Materials\nProject API, the Reaction-Network package, and Google Search) to use following an iterative chain-of-thought procedure. In this way, it can\nanswer questions such as“Is the material AnByCz stable?”.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1239\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nmaterials available in Materials Project database. This context is\nthen leveraged by an LLM to infer properties for the unknown\nmaterial. This innovative use of ICL bridges data gaps and\nenhances MAPI-LLM's robustness and versatility (Fig. 3).\n1.2.2 sMolTalk. The previous application already touches\non the problem that soware for chemical applications requires\nscientists to invest a signicant amount of time in learning even\nthe most basic applications. An example of this is visualization\nsoware. Depending on the package and its associated docu-\nmentation, chemists and materials scientists might spend\nhours to days learning the details of specic visualization so-\nware that is sometimes poorly documented. And in particular,\nfor occasional use, if it takes a long time to learn the basics, it\nwon't be used.\nAs the sMolTalk-team (Jakub L´ala, Sean Warren, Samuel G.\nRodriques) showed, one can use LLMs to write code for visu-\nalization tools such as\nto address this ineﬃciency.70\nInterestingly, few-shot prompting with several examples of user\ninput with the expected JavaScript code that manipulates the\nviewer is all that is needed to create a prototype of an\ninterface that can retrieve protein structures from the protein\ndata bank (PDB) and create custom visualization solutions,e.g.,\nto color parts of a structure in a certain way (Fig. 4). The beauty\nof the language models is that the user can write the prompt in\nmany diﬀerent (“fuzzy”) ways: whether one writes “color” or\n“colour”, or terms like“light yellow” or “pale yellow” the LLM\ntranslates it into something the visualization so ware can\ninterpret.\nHowever, this application also highlights that further\ndevelopments of these LLM-based tools are needed. For\nexample, a challenge the sMolTalk tool faces is robustness. For\ninstance, fragments from the prompt tend to leak into the\noutput and must be handled with more involved mechanisms,\nsuch as retries (in which one gives the LLMs access to the error\nmessages) or prompt engineering. Further improvement can\nalso be expected if the application leverages a knowledge base\nsuch as the documentation of\nAs the work of Hocky and White shows,71 an LLM-interface\nfor soware can also be used with other programs such as\n,72 and extended with speech-to-text models (such as\nWhisper73) to enable voice control of such programs. In partic-\nular, such an LLM-based agent approach might be imple-\nmented for the\nprogram, where various tools for protein\nengineering could be interfaced through a chat interface,\nlowering the barrier to entry for biologists to use recent\nadvancements within in silico protein engineering (such as\nRosettaFold\n74 or RFDiﬀusion75).\n1.2.2.1 ELN interface:\n . In addition to large, highly\ncurated databases with well-dened data models76 (such as those\naddressed by the MAPI-LLM project), experimental materials and\nchemistry data is increasingly being captured using digital tools\nsuch as ELNs and laboratory information systems (LIMS).\nImportantly, these tools can be used to record both structured\nand unstructured lab data in a manner that is actionable by both\nhumans and computers. However, one challenge in developing\nthese systems is that it is diﬃcult for a traditional user interface to\nhave enoughexibility to capture the richness and diversity of\nreal, interconnected, experimental data. Interestingly, LLMs can\ninterpret and contextualize both structured and unstructured\nd a t aa n dc a nt h e r e f o r eb eu s e dt oc r e a t ean o v e lt y p eo fexible,\nconversational interface to such experimental data. The\nteam (Joshua D. Bocarsly, Matthew L. Evans, and Ben\nE. Smith) embedded an LLM chat interface within\n an\nopen source materials chemistry data management system,\nwhere the virtual LLM-powered assistant can be“attached” to\na given sample. The virtual assistant has access to responses from\nthe JavaScript object notation (JSON) API of\n(containing\nboth structured and unstructured/free text data) and can use\nthem to perform several powerful tasks:rst, it can contextualize\nexisting data by explaining related experiments from linked\nresponses, resolving acronyms/short-hand notations used by\nexperimentalists, or creating concise textual summaries of\ncomplex and nested entries. Second, it can reformat or render the\ndata, for instance, by creating (\n) owcharts or\n(Markdown) tables (Fig. 5). Third, it can use its generic reasoning\nabilities to suggest future experiments, for instance, related\nmaterials to study, synthesis protocols to try, or additional char-\nacterization techniques. This is shown in the examples given in\nESI Section 2C,† where\nwas able to provide hints about\nwhich NMR-active nuclei can be probed in the given sample.\nFig. 4 The sMolTalk interface. Based on few-shot prompting LLMs\ncan create code for visualization tools such as\n that can\ncreate custom visualization based on a natural-language description\nof the desired output. The top left box is the inputﬁeld where users can\nenter commands in natural language. The top right box prints the code\nthe LLM generates. This code generates the visualization shown in the\nlower box. In this example, the user entered a sequence of four\ncommands: the LLM (1) generates code for retrieving the structure, (2)\ncolors the carbons blue, (3) displays the hydrogens as red spheres, and\n(4) reduces the size of the spheres.\n1240\n| Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nIt is easy to envision that this tool could be even more helpful\nby ne-tuning or conditioning it on a research group's knowl-\nedge base (e.g., group Wiki or standard operating procedures)\nand communication history (e.g., a group's Slack history). An\nimportant limitation of the current implementation is that the\nsmall context window of available LLMs limits the amount of\nJSON data one can directly provide within the prompt, limiting\neach conversation to analyzing a relatively small number of\nsamples. Therefore, one needs to either investigate the use of\nembeddings to determine which samples to include in the\ncontext or adopt an “agent” approach where the assistant is\nallowed to query the API of the ELN (interleaved with extraction\nand summarization calls).\n1.2.3 BOLLaMa: facilitating Bayesian optimization with\nlarge language models.Bayesian optimization (BO) is a power-\nful tool for optimizing expensive functions, such as mapping of\nreaction conditions to the reaction yield. Chemists would\ngreatly benet from using this method to reduce the number of\ncostly experiments they need to run.\n77,78 However, BO faces an\ninterface and accessibility problem, too. The existing\nFig. 5 Using an LLM as an interface to an ELN/data management system/data management system. LLM-based assistants can provide powerful\ninterfaces to digital experimental data. Theﬁgure shows a screenshot of a conversation with\n in the\n data management system\n(https://github.com/the-grey-group/datalab). Here,\n is provided with data from the JSON API of\n of an experimental battery\ncell. The user then prompts (green box) the system to build aﬂowchart of the provenance of the sample. The assistant responds with\nmarkdown code, which the\n interface automatically recognizes and translates into a visualization.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1241\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nframeworks require signi cant background knowledge and\ncoding experience not conventionally taught in chemistry\ncurricula. Therefore, many chemists cannot benet from tools\nsuch as BO. The BOLLaMa-team (Bojana Rankovi´c, Andres M.\nBran, Philippe Schwaller) showed that LLMs can lower the\nbarrier for the use of BO by providing a natural language chat-\nlike interface to BO algorithms. Fig. 6 shows a prototype of\na chat interface in which the LLM interprets the user request,\ninitializes a BO run by suggesting initial experimental condi-\ntions, and then uses the feedback of the user to drive the BO\nalgorithm and suggest new experiments. The example used data\non various additives for a cooperative nickel-photoredox cata-\nlyzed reaction\n79 and the BO code from Rankovi´c et al.80 This\nideally synergizes with an LLM interface to a data management\nsolution (as discussed in the previous project) as one could\ndirectly persist the experimental results and leverage prior\nrecords to“bootstrap” BO runs.\nA st h ee x a m p l e si nt h i ss e c t i o ns h o w ,w end that LLMs have\nthe potential to greatly enhance the eﬃciency of a diverse array of\nprocesses in chemistry and materials science by providing novel\ninterfaces to tools or by completely automating their use. This can\nhelp streamline workows, reduce human error, and increase\nproductivity— oen by replacing “glue code ” with natural\nlanguage or familiarising oneself with a soware library by chat-\nting with an LLM.\n1.3 Knowledge extraction\nBeyond proving novel interfaces for tools, LLMs can also serve\nas powerful tools for extracting knowledge from the vast\namount of chemical literature. With LLMs, researchers can\nrapidly mine and analyze large volumes of data, enabling them\nto uncover novel insights and advance the frontiers of chemical\nknowledge. Tools such as paper-qa\n28 can help to dramatically\ncut down the time required for literature search by automati-\ncally retrieving, summarizing, and contextualizing relevant\nfragments from the entire corpus of the scientic literature— for\nexample, answering questions (with suitable citations) based on\na library of hundreds of documents.\n35 As the examples in the\nprevious section indicated, this is particularly useful if the\nmodel is given access to search engines on the internet.\n1.3.1 InsightGraph. To facilitate downstream use of the\ninformation, LLMs can also convert unstructured data— the\ntypical form of these literature reports— into structured data.\nThe use of GPT for this application has been reported by Dunn\net al.\n81 and Walker et al.,82 who used an iterativene-tuning\napproach to extract data structured in JSON from papers. In\ntheir approach, initial (zero-shot) completions of the LLM are\ncorrected by domain experts. Those corrected completions are\nthen used tonetune LLMs, showing improved performance on\nthis task.\nHowever, for certain applications, one can construct powerful\nprototypes using only careful prompting. For instance, the\nInsightGraph team (Defne Circi, Shruti Badhwar) showed that\nGPT-3.5-turbo, when prompted with an example JSON containing\na high-level schema and information on possible entities (e.g.,\nmaterials) and pairwise relationships (e.g., properties of mate-\nr i a l s ) ,c a n ,a sF i g .7i l l u s t r a t e s ,p r o v i d eak n o w l e d g eg r a p hr e p r e -\nsentation of the entities and their relationships in a text\ndescribing the properties and composition of polymer nano-\ncomposites. A further optimized version of this tool might oﬀer\na concise and visual means to understand and compare material\ntypes quickly and uses across sets of articles— a task that currently\nis very laborious. An advanced potential application is the creation\nof structured, materials-specic datasets for fact-based question-\nanswering and downstream machine-learning tasks.\n1.3.2 Extracting structured data from free-form organic\nsynthesis text. Unstructured text is commonly used for\ndescribing organic synthesis procedures. Due to the large\ncorpus of literature, manual conversion from unstructured text\nto structured data is unrealistic. However, structured data are\nneeded for building conventional ML models for reaction\nprediction and condition recommendation. The Open Reaction\nDatabase (ORD)\n84 is a database of curated organic reactions. In\nthe ORD, while reaction data are structured by the ORD schema,\nmany of their procedures are also available as plain text.\nInterestingly, an LLM (e.g., OpenAI's\n) can,\naer netuning on only 300 prompt–completion pairs, extract\n93% of the components from the free-text reaction description\ninto valid JSONs (Fig. 8). Such models might signi cantly\nincrease the data available for training models on tasks such as\npredicting reaction conditions and yields. In contrast to\nprevious approaches, such as the one of Guoet al.,\n85 the use of\nLLM does not require a specialized modeling setup but can be\ncarried out with relatively little expertise. It is worth noting that\nall reaction data submitted to ORD are made available under\nthe CC-BY-SA license, which makes ORD a suitable data source\nFig. 6 Schematic overview of BoLLama. An LLM can act as an inter-\nface to a BO algorithm. An experimental chemist can bootstrap an\noptimization and then, via a chat interface, update the state of the\nsimulation to which the bot responds with the recommended next\nsteps.\n1242\n| Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nfor ne-tuning or training an LLM to extract structured data\nfrom organic procedures. A recent study on gold nanorod\ngrowth procedures also demonstrated the ability of LLM in\na similar task.\n82 In contrast to the LIFT-based prediction of\natomization energies reported in therst section by the Ber-\nkeley–Madison team, parameter-e ﬃcient ne-tuning of the\nopen-source Alpaca model 86–88 using LoRA 48 did not yield\na model that can construct valid JSONs.\n1.3.3 TableToJson: structured information from tables in\nscientic papers. The previous example shows how structured\ndata can be extracted from plain text using LLMs. However,\nrelevant information in the scientic literature is not only found\nin text form. Research papers oen contain tables that collect\ndata on material properties, synthesis conditions, and results of\ncharacterization and experiments. Converting table informa-\ntion into structured formats is essential to enable automated\ndata analysis, extraction, and integration into computational\nworkows. Although some techniques could help in the process\nof extracting this information (performing OCR or parsing\nXML), converting this information in structured data following,\nfor example, a specic JSON schema with models remains\na challenge. The INCAR-CSIC team (Mar´ıa Victoria Gil) showed\nthat the OpenAI\nmodel, when prompted\nwith a desired JSON schema and the HyperText Markup\nLanguage (HTML) of a table contained in a scientic paper, can\ngenerate structured JSON with the data in the table.\nFirst, the OpenAI\nmodel was directly\nused to generate JSON objects from the table information. This\napproach was applied to several examples using tables collected\nfrom papers on diﬀerent research topics within the eld of\nchemistry.89–95 The accuracy for those diﬀerent examples, calcu-\nlated as the percentage of schema values generated correctly, is\nshown in Fig. 9. When the OpenAI model was prompted with the\ntable and desired schema to generate a JSON object, it worked\nremarkably well in extracting the information from each table cell\nand inserting it at the expected place in the schema. As output, it\nprovided a valid JSON object with a 100% success rate of error-free\ngenerated values in all the studied examples. However, in some\nexamples, the model did not follow the schema.\nTo potentially address this problem the team utilized the\napproach. This tool reads the keys from the JSON\nschema and only generates the value tokens, guaranteeing the\ngeneration of a syntactically valid JSON (corresponding to the\ndesired schema) by the LLM.\n96,97 Using an LLM without such\na decoding strategy cannot guarantee that valid JSON outputs are\nproduced. With the\n approach, in most cases, by\nusing a simple descriptive prompt about the type of input text,\nstructured data can be obtained with 100% correctness of the\nFig. 7 The InsightGraph interface. A suitably prompted LLM can create knowledge graph representations of scientiﬁc text that can be visualized\nusing tools such as neo4j's visualization tools.83\nFig. 8 The organic synthesis parser interface. The top box shows text\ndescribing an organic reaction (https://open-reaction-database.org/\nclient/id/ord-1f99b308e17340cb8e0e3080c270fd08), which the\nﬁnetuned LLM converts into structured JSON (bottom). A demo\napplication can be found at https://qai222.github.io/\nLLM_organic_synthesis/.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1243\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\ngenerated values. In one example, an accuracy of 80% was ob-\ntained due to errors in the generation of numbers in scientic\nnotation. For a table with more complex content (long molecule\nnames, hyphens, power numbers, subscripts, and super-\nscripts,.) the team achieved an accuracy of only 46%. Most of\nthese issues could be solved by adding a specice x p l a n a t i o ni n\nthe prompt, increasing the accuracy to 100% in most cases.\nOverall, both approaches performed well in generating the\nJSON format. The OpenAI\nmodel could\ncorrectly extract structured information from tables and give\na valid JSON output, but it cannot guarantee that the outputs\nwill always follow the provided schema.\nmay\npresent problems when special characters need to be generated,\nbut most of these issues could be solved with careful prompt-\ning. These results show that LLMs can be a useful tool to help to\nextract scientic information in tables and convert it into\na structured form with axed schema that can be stored in\na database, which could encourage the creation of more topic-\nspecic databases of research results.\n1.3.4 AbstractToTitle & TitleToAbstract: text summariza-\ntion and text generation.Technical writing is a challenging task\nthat oen requires presenting complex abstract ideas in limited\nspace. For this, frequent rewrites of sections are needed, in\nwhich LLMs could assist domain experts. Still, evaluating their\nability to generate text such as a scientic paper is essential,\nespecially for chemistry and materials science applications.\nLarge datasets of chemistry-related text are available from\nopen-access platforms such as arXiv and PubChem. These\narticles contain titles, abstracts, and oen complete manu-\nscripts, which can be a testbed for evaluating LLMs as these\ntitles and abstracts are usually written by expert researchers.\nIdeally, an LLM should be able to generate a title of an abstract\nclose to the one developed by the expert, which can be consid-\nered a specialized text-summarization task. Similarly, given\na title, an LLM should generate text close to the original abstract\nof the article, which can be considered a specialized text-\ngeneration task.\nThese tasks have been introduced by the AbstractToTitle &\nTitleToAbstract team (Kamal Choudhary) in the JARVIS-\nChemNLP package.\n98 For text summarization, it uses a pre-\ntrained Text-to-Text Transfer Transformer (T5) model developed\nby Google99 that is furtherne-tuned to produce summaries of\nabstracts. On the arXiv condensed-matter physics (cond-mat)\ndata, the team found that ne-tuning the model can help\nimprove the performance (Recall-Oriented Understudy for Gist-\ning Evaluation (ROUGE)-1 score of 39.0% which is better than an\nuntrained model score of 30.8% for an 80/20 split).\nFor text generation, JARVIS-ChemNLPnetunes the pretrained\nGPT-2-medium\n49 model available in the HuggingFace library.100\nAer netuning, the team found a ROUGE score of 31.7%, which\nis a good starting point for pre-suggestion text applications. Both\ntasks with well-den e dt r a i na n dt e s ts p l i t sa r en o wa v a i l a b l ei nt h e\nJARVIS-Leaderboard platformfor the AI community to compare\nother LLMs and systematicallyimprove the performance.\nIn the future, such title to abstract capabilities can be\nextended to generating full-length dra s with appropriate\ntables, gures, and results as an initial start for the human\nresearcher to help in the technical writing processes. Note that\nthere have been recent developments in providing guidelines\nfor using LLM-generated text in technical manuscripts,\n101 so\nsuch an LLM model should be considered as an assistant of\nwriting and not the master/author of the manuscripts.\nFig. 9 TableToJson. Results of the structured JSON generation of tables contained in scientiﬁc articles. Two approaches are compared: (i) the\nuse of an OpenAI model prompted with the desired JSON schema, and (ii) the use of an OpenAI model together with\nIn both cases,\nJSON objects were always obtained. The output of the OpenAI model did not always follow the provided schema, although this might be solved\nby modifying the schema. The accuracy of the results from the\n approach used with OpenAI models could be increased (as shown\nby the blue arrows) by solving errors in the generation of power numbers and special characters with a more detailed prompt. The results can be\nvisualized in this demo app:https://vgvinter-tabletojson-app-kt5aiv.streamlit.app/.\n1244\n| Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n1.4 Education\nGiven all the opportunities LLM open for materials science and\nchemistry, there is an urgent need for education to adapt.\nInterestingly, LLMs also provide us with entirely novel educa-\ntional opportunities,\n102 for example, by personalizing content or\nproviding almost limitless varied examples.\nThe I-Digest (Information-Digestor) hackathon team (Beatriz\nMouriño, Elias Moubarak, Joren Van Herck, Sauradeep\nMajumdar, Xiaoqi Zhang) created a path toward such a new\neducational opportunity by providing students with a digital\ntutor based on course material such as lecture recordings. Using\nthe Whisper model,\n73 videos of lecture recordings can be tran-\nscribed to text transcripts. The transcripts can then be fed into\nan LLM with the prompt to come up with questions about the\ncontent presented in the video (Fig. 10). In the future, these\nquestions might be shown to students before a video starts,\nallowing them to skip parts they already know or aer the video,\nguiding students to the relevant timestamps or additional\nmaterial in case of an incorrect answer.\nImportantly, and in contrast to conventional educational\nmaterials, this approach can generate a practically in nite\nnumber of questions and could, in the future, continuously be\nimproved by student feedback. In addition, it is easy to envision\nextending this approach to consider lecture notes or books to\nguide the students further or even recommend specice x e r c i s e s .\n2. Conclusion\nThe fact that the groups were able to present prototypes that\ncould do quite complex tasks in such a short time illustrates the\npower of LLMs. Some of these prototypes would have taken\nmany months of programming just a few months ago, but the\nfact that LLMs could reduce this time to a few hours is one of\nthe primary reasons for the success of our hackathon.\nCombined with the time-constrained environment in teams\n(with practically zero cost of“failure”), we found more energy\nand motivation. The teams delivered more results than in most\nother hackathons we participated in.\nThrough the LIFT framework, one can use LLMs to address\nproblems that could already be addressed with conventional\napproaches— but in a much more accessible way (using the\nsame approach for di ﬀerent problems), while also reusing\nestablished concepts such asD-ML. At the same time, however,\nwe can use LLMs to model chemistry and materials science in\nnovel ways; for example, by incorporating context information\nsuch as “fuzzy” design rules or directly operating on unstruc-\ntured data. Overall, a common use case has been to use LLMs to\ndeal with “fuzziness” in programming and tool development.\nWe can already see tools like Copilot and ChatGPT being used to\nconvert “fuzzy abstractions” or hard-to-dene tasks into code.\nThese advancements may soon allow everyone to write small\napps or customize them to their needs (end-user programming).\nAdditionally, we can observe an interesting trend in tool\ndevelopment: most of the logic in the showcased tools is written\nin English, not in Python or another programming language.\nThe resulting code is shorter, easier to understand, and has\nfewer dependencies because LLMs are adept at handling fuzz-\niness that is diﬃcult to address with conventional code. This\nsuggests that we may not need more formats or standards for\ninteroperability; instead, we can simply describe existing solu-\ntions in natural language to make them interoperable.\nExploring this avenue further is exciting, but it is equally\nFig. 10 The I-digest interface. (a) A video (e.g., of a lecture recording) can be described using the Whisper model. Based on the transcript, an LLM\ncan generate questions (and answers). Those can assist students in their learning. (b) The LLM can also detect mentions of chemicals and link to\nfurther information about them (e.g., on PubChem\n103–105).\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1245\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nimportant to recognize the limitations of LLMs, as they\ncurrently have limited interpretability and lack robustness.\nIt is interesting to note that none of the projects relied on the\nknowledge or understanding of chemistry by LLMs. Instead,\nthey relied on general reasoning abilities and provided chem-\nistry information through the context orne-tuning. However,\nthis also brings new and unique challenges. All projects used\nthe models provided by OpenAI's API. While these models are\npowerful, we cannot examine how they were built or have any\nguarantee of continued reliable access to them.\nAlthough there are open-source language models and tech-\nniques available, they are generally more di ﬃcult to use\ncompared to simply using OpenAI's API. Furthermore, the\nperformance of language models can be fragile, especially for\nzero- or few-shot applications. To further investigate this, new\nbenchmarks are needed that go beyond the tabular datasets we\nhave been using for ML for molecular and materials science—\nwe simply have no frameworks to compare and evaluate\npredictive models that use context, unstructured data, or tools.\nWithout automated tests, however, it is diﬃcult to improve\nthese systems systematically. On top of that, consistent bench-\nmarking is hard because de-duplication is ill-dened even if the\ntraining data are known. To enable a scientic approach to the\ndevelopment and analysis of these systems, we will also need to\nrevisit versioning frameworks to ensure reproducibility as\nsystems that use external tools depend on the exact versions of\ntraining data, LLM, as well as of the external tools and\nprompting setup.\nThe diversity of the prototypes presented in this work shows\nthat the potential applications are almost unlimited, and we can\nprobably only see the tip of the iceberg— for instance, we didn't\neven touch modalities other than text thus far. In addition, we\nalso want to note that the projects in the workshop mostly\nexplored the use of LLMs as tools or oracles but not as muses.\n106\nFrom techniques such as rubber duck debugging (describing\nthe problem to a rubber duck),107 we know that even simple—\nnon-intelligent— articulation or feedback mechanisms can help\novercome roadblocks and create creative breakthroughs.\nInstead of explaining a problem to an inanimate rubber duck,\nwe could instead have a conversation with an LLM, which could\nprobe our thinking with questions or aid in brainstorming by\ngenerating diverse new ideas. Therefore, one should expect an\nLLM to be as good as a rubber duck— if not drastically more\neﬀective.\nGiven these new ways of working and thinking, combined\nwith the rapid pace of developments in theeld, we believe that\nwe urgently need to rethink how we work and teach. We must\ndiscuss how we ensure safe use,\n108 standards for evaluating and\nsharing those models, and robust and reliable deployments.\nBut we also need to discuss how we ensure that the next\ngeneration of chemists and materials scientists are procient\nand critical users of these tools— that can use them to work\nmore eﬃciently while critically reecting on the outputs of the\nsystems. This work showcased some potential applications of\nLLMs that will benet from further investigation. We believe\nthat to truly leverage the power of LLMs in the molecular and\nmaterial sciences, however, we need a community e ﬀort—\nincluding not only chemists and computer scientists but also\nlawyers, philosophers, and ethicists: the possibilities and\nchallenges are too broad and profound to tackle alone.\nData availability\nThe code and data for the case studies reported in this article\ncan be found in the GitHub repositories linked in Table 1.\nConﬂicts of interest\nThere are no conicts to declare.\nAcknowledgements\nWe would like to specically thank Jim Warren (NIST) for his\ncontributions to discussions leading up to the hackathon and\nhis participation as a judge during the event. We would also like\nto thank Anthony Costa and Christian Dallago (NVIDIA) for\nsupporting the hackathon. B. B., I. T. F, and Z. H. acknowledge\nsupport from the the National Science Foundation awards\n#2226419 and #2209892. This work was performed under the\nfollowing nancial assistance award 70NANB19H005 from the\nU.S. Department of Commerce, National Institute of Standards\nand Technology as part of the Center for Hierarchical Materials\nDesign (CHiMaD). K. J. S, A. S. acknowledge support from the\nthe National Science Foundation award #1931306. K. M. J.,\nS. M., J. v. H., X. Z., B. M., E. M., and B. S. were supported by the\nMARVEL National Centre for Competence in Research funded\nby the Swiss National Science Foundation (grant agreement ID\n51NF40-182892) and the USorb-DAC Project, which is funded by\na grant from The Grantham Foundation for the Protection of\nthe Environment to RMI's climate tech accelerator program,\nThird Derivative. B. M. was further supported by the European\nUnion's Horizon 2020 research and innovation programme\nunder the Marie Sk łodowska-Curie grant agreement No.\n945363. M. C. R., S. C., and A. D. W. were supported by the\nNational Science Foundation and the National Institute of\nGeneral Medical Sciences under Grant No. 1764415 and award\nnumber R35GM137966, respectively. Q. A.'s contribution to this\nwork was supported by the National Center for Advancing\nTranslational Sciences of the National Institutes of Health\nunder award number U18TR004149. The content is solely the\nresponsibility of the authors and does not necessarily represent\nthe oﬃcial views of the National Institutes of Health. M. V. G.\nacknowledges support from the Spanish National Research\nCouncil (CSIC) through the Programme for internationalization\ni-LINK 2021 (Project LINKA20412), and from the Spanish\nAgencia Estatal de Investigaci ´on (AEI) through the Grant\nTED2021-131693B-I00 funded by MCIN/AEI/10.13039/\n501100011033 and by the “European Union NextGener-\nationEU/PRTR” and through the Ram´on y Cajal Grant RYC-\n2017-21937 funded by MCIN/AEI/10.13039/501100011033 and\nby “ESF Investing in your future”. The\nproject (M. L. E.,\nB. E. S. and J. D. B.) has received funding from the European\nUnion's Horizon 2020 research and innovation programme\nunder grant agreement 957189 (DOI: 10.3030/957189), the\n1246 | Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nBattery Interface Genome – Materials Acceleration Platform\n(BIG-MAP), as an external stakeholder project. M. L. E. addi-\ntionally thanks the BEWARE scheme of the Wallonia-Brussels\nFederation for funding under the European Commission's\nMarie Curie-Skłodowska Action (COFUND 847587). B. E. S.\nacknowledges support from the UK's Engineering and Physical\nSciences Research Council (ESPRC). B. P. acknowledges support\nfrom the National Science Foundation through NSF-CBET Grant\nNo. 1917340. The authors thank Phung Cheng Fei, Hassan\nHarb, and Vinayak Bhat for their helpful comments on this\nproject. D. C. and L. C. B. thank NSF DGE-2022040 for the aiM\nNRT funding support. K. C. thank the National Institute of\nStandards and Technology for funding, computational, and\ndata-management resources. Please note certain equipment,\ninstruments, soware, or materials are identied in this paper\nin order to specify the experimental procedure adequately. Such\nidentication is not intended to imply recommendation or\nendorsement of any product or service by NIST, nor is it\nintended to imply that the materials or equipment identied\nare necessarily the best available for the purpose. A. K. G.,\nG. W. M., A. I., and W. A. d. J. were supported by the U.S.\nDepartment of Energy, Oﬃce of Science, Basic Energy Sciences,\nMaterials Sciences and Engineering Division under Contract\nNo. DE-AC02-05CH11231, FWP No. DAC-LBL-Long, and by the\nU.S. Department of Energy, Oﬃce of Science, Oﬃce of High\nEnergy Physics under Award Number DE-FOA-0002705. M. B, B.\nR., and P. S. were supported by the NCCR Catalysis (grant\nnumber 180544), a National Centre of Competence in Research\nfunded by the Swiss National Science Foundation. S. G. R. and\nJ. L. acknowledge the generous support of Eric and Wendy\nSchmidt, and the core funding of the Francis Crick Institute,\nwhich receives its funding from Cancer Research UK, the UK\nMedical Research Council, and the Wellcome Trust.\nReferences\n1 K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev and\nA. Walsh, Machine learning for molecular and materials\nscience, Nature, 2018,559, 547–555.\n2 S. M. Moosavi, K. M. Jablonka and B. Smit, The Role of\nMachine Learning in the Understanding and Design of\nMaterials, J. Am. Chem. Soc., 2020,142, 20273–20287.\n3 D. Morgan and R. Jacobs, Opportunities and Challenges for\nMachine Learning in Materials Science,Annu. Rev. Mater.\nRes., 2020,50,7 1–103.\n4 R. Ramprasad, R. Batra, G. Pilania, A. Mannodi-\nKanakkithodi and C. Kim, Machine learning in materials\ninformatics: recent applications and prospects, npj\nComput. Mater., 2017,3, 54.\n5 J. Schmidt, M. R. G. Marques, S. Botti and M. A. L. Marques,\nRecent advances and applications of machine learning in\nsolid-state materials science, npj Comput. Mater., 2019, 5,\n83.\n6 K. Choudhary, B. DeCost, C. Chen, A. Jain, F. Tavazza,\nR. Cohn, C. W. Park, A. Choudhary, A. Agrawal,\nS. J. Billinge, et al., Recent advances and applications of\ndeep learning methods in materials science,npj Comput.\nMater., 2022,8, 59.\n7 K. M. Jablonka, D. Ongari, S. M. Moosavi and B. Smit, Big-\nData Science in Porous Materials: Materials Genomics and\nMachine Learning,Chem. Rev., 2020,120, 8066–8129.\n8 J. Shi, M. J. Quevillon, P. H. Amorim Valença and\nJ. K. Whitmer, Predicting Adhesive Free Energies of\nPolymer–Surface Interactions with Machine Learning,ACS\nAppl. Mater. Interfaces, 2022,14, 37161–37169.\n9 J. Shi, F. Albreiki, Y. J. Col ´on, S. Srivastava and\nJ. K. Whitmer, Transfer Learning Facilitates the Prediction\nof Polymer–Surface Adhesion Strength, J. Chem. Theory\nComput., 2023, 4631–4640.\n10 F. No´e, A. Tkatchenko, K.-R. Müller and C. Clementi,\nMachine Learning for Molecular Simulation, Annu. Rev.\nPhys. Chem., 2020,71, 361–390.\n11 S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa,\nM. Kornbluth, N. Molinari, T. E. Smidt and B. Kozinsky,\nE(3)equivariant graph neural networks for data eﬃcient\nand accurate interatomic potentials,Nat. Commun., 2022,\n13, 2453.\n12 B. Sanchez-Lengeling and A. Aspuru-Guzik, Inverse\nmolecular design using machine learning: generative\nmodels for matter engineering,Science, 2018,361, 360–365.\n13 J. F. Gonthier, S. N. Steinmann, M. D. Wodrich and\nC. Corminboeuf, Quanti cation of “fuzzy” chemical\nconcepts: a computational perspective, Chem. Soc. Rev.,\n2012, 41, 4671.\n14 D. Weininger, SMILES, a chemical language and\ninformation system. 1. Introduction to methodology and\nencoding rules,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n15 K. M. Jablonka, L. Patiny and B. Smit, Making the collective\nknowledge of chemistry open and machine actionable,Nat.\nChem., 2022,14, 365–376.\n16 R. Bommasani,et al., On the Opportunities and Risks of\nFoundation Models,CoRR 2021, abs/2108.07258.\n17 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez,Ł . Kaiser and I. Polosukhin, Attention is all\nyou need, Adv. Neural Inf. Process. Syst., 2017, 30, 6000–\n6010.\n18 A. Chowdhery, et al., PaLM: Scaling Language Modeling\nwith Pathways, arXiv, 2022, preprint, arXiv:2204.02311,\nDOI: 10.48550/arXiv.2204.02311.\n19 J. Hoﬀmann, et al. , Training Compute-Optimal Large\nLanguage Models, arXiv, 2022, preprint, arXiv:2203.15556,\nDOI: 10.48550/arXiv.2203.15556.\n20 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al., Language models are few-shot learners,\nAdv. Neural Inf. Process. Syst., 2020,33, 1877–1901.\n21 C. N. Edwards, T. Lai, K. Ros, G. Honke and H. Ji,\nTranslation between Molecules and Natural Language,\nConference On Empirical Methods In Natural Language\nProcessing, 2022.\n22 T. Eloundou, S. Manning, P. Mishkin and D. Rock, GPTs are\nGPTs: An Early Look at the Labor Market Impact Potential\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1247\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nof Large Language Models, arXiv, 2023, preprint,\narXiv:2303.10130, DOI:10.48550/arXiv.2303.10130.\n23 A. Srivastava, et al. , Beyond the Imitation Game:\nQuantifying and extrapolating the capabilities of language\nmodels, arXiv, 2022, preprint, arXiv:2206.04615, DOI:\n10.48550/arXiv.2206.04615.\n24 S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke,\nE. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg,\nH. Nori, H. Palangi, M. T. Ribeiro and Y. Zhang, Sparks of\nArticial General Intelligence: Early experiments with\nGPT-4, arXiv, 2023, preprint, arXiv:2303.12712, DOI:\n10.48550/arXiv.2303.12712.\n25 T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli,\nL. Zettlemoyer, N. Cancedda and T. Scialom, Toolformer:\nLanguage Models Can Teach Themselves to Use Tools,\narXiv, 2023, preprint, arXiv:2302.04761, DOI: 10.48550/\narXiv.2302.04761.\n26 E. Karpas,et al., MRKL Systems: a modular, neuro-symbolic\narchitecture that combines large language models, external\nknowledge sources and discrete reasoning, arXiv, 2022,\npreprint, arXiv:2205.00445, DOI: 10.48550/\narXiv.2205.00445.\n27 Y. Shen, K. Song, X. Tan, D. Li, W. Lu and Y. Zhuang,\nHuggingGPT: Solving AI Tasks with ChatGPT and its\nFriends in HuggingFace, arXiv, 2023, preprint,\narXiv:2303.17580, DOI:10.48550/arXiv.2303.17580.\n28 A. White, paper-qa, 2022, https://github.com/whitead/\npaper-qa.\n29 J. Liu, LlamaIndex, 2022, https://github.com/jerryjliu/\nllama_index, last accessed 2023-05-30.\n30 A. Karpathy, The Hottest New Programming Language Is\nEnglish, 2023, https://twitter.com/karpathy/status/\n1617979122625712128, last accessed 2023-05-11.\n31 G. M. Hocky and A. D. White, Natural language processing\nmodels that automate programming will transform\nchemistry research and teaching, Digit. Discov., 2022, 1,\n79–83.\n32 K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero and\nB. Smit, Is GPT-3 all you need for low-data discovery in\nchemistry?, ChemRxiv, 2023, preprint, DOI: 10.26434/\nchemrxiv-2023-fw8n4.\n33 A. D. White, G. M. Hocky, H. A. Gandhi, M. Ansari, S. Cox,\nG. P. Wellawatte, S. Sasmal, Z. Yang, K. Liu, Y. Singh,et al.,\nAssessment of chemistry knowledge in large language\nmodels that generate code,Digit. Discov., 2023, 368–376.\n34 M. C. Ramos, S. S. Michtavy, M. D. Porosoﬀ and A. D. White,\nBayesian Optimization of Catalysts With In-context\nLearning, arXiv, 2023, preprint, arXiv:2304.05341, DOI:\n10.48550/arXiv.2304.05341\n.\n35 A. D. White, The future of chemistry is language,Nat. Rev.\nChem., 2023,7, 457–458.\n36 R. Ramakrishnan, P. O. Dral, M. Rupp and O. A. Von\nLilienfeld, Big data meets quantum chemistry\napproximations: the D-machine learning approach, J.\nChem. Theory Comput., 2015,11, 2087–2096.\n37 T. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput,\nJ.-Y. Sohn, D. Papailiopoulos and K. Lee, Language-\nInterfaced Fine-Tuning for Non-Language Machine\nLearning Tasks, arXiv, 2022, preprint, arXiv:2206.06565,\nDOI: 10.48550/arXiv.2206.06565.\n38 M. Krenn, F. H¨ase, A. Nigam, P. Friederich and A. Aspuru-\nGuzik, Self-referencing embedded strings (SELFIES):\na 100% robust molecular string representation, Mach.\nLearn.: Sci. Technol., 2020,1, 045024.\n39 M. Krenn, Q. Ai, S. Barthel, N. Carson, A. Frei, N. C. Frey,\nP. Friederich, T. Gaudin, A. A. Gayle, K. M. Jablonka,\net al. , SELFIES and the future of molecular string\nrepresentations, Patterns, 2022,3, 100588.\n40 T. Guo, K. Guo, B. Nan, Z. Liang, Z. Guo, N. V. Chawla,\nO. Wiest and X. Zhang, What indeed can GPT models do\nin chemistry? A comprehensive benchmark on eight\ntasks, arXiv, 2023, preprint, arXiv:2305.18365, DOI:\n10.48550/arXiv.2305.18365.\n41 L. A. Curtiss, P. C. Redfern and K. Raghavachari, Gaussian-4\ntheory using reduced order perturbation theory,J. Chem.\nPhys., 2007,127, 124105.\n42 R. Ramakrishnan, P. O. Dral, M. Rupp and O. A. Von\nLilienfeld, Quantum chemistry structures and properties\nof 134 kilo molecules,Sci. Data, 2014,1,1 –7.\n43 B. Narayanan, P. C. Redfern, R. S. Assary and L. A. Curtiss,\nAccurate quantum chemical energies for 133000 organic\nmolecules, Chem. Sci., 2019,10, 7449–7455.\n44 A. K. Gupta and K. Raghavachari, Three-Dimensional\nConvolutional Neural Networks Utilizing Molecular\nTopological Features for Accurate Atomization Energy\nPredictions, J. Chem. Theory Comput., 2022,18, 2132–2143.\n45 L. Ward, B. Blaiszik, I. Foster, R. S. Assary, B. Narayanan\nand L. Curtiss, Machine learning prediction of accurate\natomization energies of organic molecules from low-\ndelity quantum chemical calculations, MRS Commun.,\n2019, 9, 891–899.\n46 R. Ramakrishnan, P. O. Dral, M. Rupp and O. A. von\nLilienfeld, Big Data Meets Quantum Chemistry\nApproximations: The D-Machine Learning Approach, J.\nChem. Theory Comput., 2015,\n11, 2087–2096.\n47 A. D. Becke, Density-functional thermochemistry. III. The\nrole of exact exchange,J. Chem. Phys., 1993,98, 5648–5652.\n48 E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,\nL. Wang and W. Chen, Low-Rank Adaptation of Large\nLanguage Models, arXiv, 2021, preprint, arXiv:2106.09685,\nDOI: 10.48550/arXiv.2106.09685.\n49 A. Radford, J. Wu, R. Child, D. Luan, D. Amodei and\nI. Sutskever, Language Models are Unsupervised Multitask\nLearners, 2019, https://d4mucfpksywv.cloudfront.net/better-\nlanguage-models/language_models_are_unsupervised_\nmultitask_learners.pdf.\n50 K. L. Scrivener, V. M. John and E. M. Gartner, Eco-eﬃcient\ncements: Potential economically viable solutions for a low-\nCO2 cement-based materials industry, Cem. Concr. Res.,\n2018, 114,2 –26.\n51 C. Völker, B. M. Torres, T. Rug, R. Firdous, G. Ahmad, J. Zia,\nS. Lüders, H. L. Scaﬃno, M. Höpler, F. Böhmer, M. Pfaﬀ,\nD. Stephan and S. Kruschwitz, Green building materials:\n1248 | Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\na new frontier in data-driven sustainable concrete design ,\n2023, DOI DOI:10.13140/RG.2.2.29079.85925.\n52 G. M. Rao and T. D. G. Rao, A quantitative method of\napproach in designing the mix proportions ofy ash and\nGGBS-based geopolymer concrete,Aust. J. Civ. Eng., 2018,\n16,5 3–63.\n53 V. Tshitoyan, J. Dagdelen, L. Weston, A. Dunn, Z. Rong,\nO. Kononova, K. A. Persson, G. Ceder and A. Jain,\nUnsupervised word embeddings capture latent knowledge\nfrom materials science literature,Nature, 2019,571,9 5–98.\n54 T. Mikolov, K. Chen, G. Corrado and J. Dean, Eﬃcient\nEstimation of Word Representations in Vector Space,\nInternational Conference On Learning Representations, 2013.\n55 E. A. Olivetti, J. M. Cole, E. Kim, O. Kononova, G. Ceder,\nT. Y.-J. Han and A. M. Hiszpanski, Data-driven materials\nresearch enabled by natural language processing and\ninformation extraction,Appl. Phys. Rev., 2020,7, 041317.\n56 S. Selva Birunda and R. Kanniga Devi, A review on word\nembedding techniques for text classi cation, Innovative\nData Communication Technologies and Application:\nProceedings of ICIDCA 2020, 2021, pp. 267–281.\n57 Z. Hong, A. Ajith, G. Pauloski, E. Duede, C. Malamud,\nR. Magoulas, K. Chard and I. Foster, Bigger is Not Always\nBetter, arXiv, 2022, preprint, arXiv:2205.11342, DOI:\n10.48550/arXiv.2205.11342.\n58 J. Li, Y. Liu, W. Fan, X.-Y. Wei, H. Liu, J. Tang and Q. Li,\nEmpowering Molecule Discovery for Molecule-Caption\nTranslation with Large Language Models: A ChatGPT\nPerspective, arXiv, 2023, preprint, arXiv: 2306.06615, DOI:\n10.48550/arXiv.2306.06615.\n59 H. Dai,et al., AugGPT: Leveraging ChatGPT for Text Data\nAugmentation, arXiv, 2023, preprint, arXiv:2302.13007,\nDOI: 10.48550/arXiv.2302.13007.\n60 V. Venkatasubramanian, K. Chan and J. M. Caruthers,\nComputer-aided molecular design using genetic\nalgorithms, Comput. Chem. Eng., 1994,18, 833–844.\n61 D. Flam-Shepherd and A. Aspuru-Guzik, Language models\ncan generate molecules, materials, and protein binding\nsites directly in three dimensions as XYZ, CIF, and PDB\nles, arXiv, 2023, preprint, arXiv:2305.05708, DOI:\n10.48550/arXiv.2305.05708.\n62 R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn,\nE. Saravia, A. Poulton, V. Kerkez and R. Stojnic, Galactica: A\nLarge Language Model for Science,arXiv, 2022, preprint,\narXiv:2211.09085, DOI:10.48550/arXiv.2211.09085.\n63 P. Schwaller, T. Gaudin, D. L´anyi, C. Bekas and T. Laino,\n“Found in Translation”: predicting outcomes of complex\norganic chemistry reactions using neural sequence-to-\nsequence models,Chem. Sci., 2018,9, 6091–\n6098.\n64 S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan and\nY. Cao, ReAct: Synergizing Reasoning and Acting in\nLanguage Models, arXiv, 2023, preprint, arXiv:2210.03629,\nDOI: 10.48550/arXiv.2210.03629.\n65 J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, F. Xia,\nQ. Le and D. Zhou, Chain-of-Thought Prompting Elicits\nReasoning in Large Language Models,Neural. Inf. Process.\nSyst., 2022, 24824–24837.\n66 OpenAI, GPT-4 Technical Report, arXiv, 2023, preprint,\narXiv:2303.08774v3, DOI:10.48550/arXiv.2303.08774.\n67 A. M. Bran, S. Cox, A. D. White and P. Schwaller,\nChemCrow: Augmenting large-language models with\nchemistry tools, arXiv, 2023, preprint, arXiv:2304.05376,\nDOI: 10.48550/arXiv.2304.05376.\n68 D. A. Boiko, R. MacKnight and G. Gomes, Emergent\nautonomous scienti c research capabilities of large\nlanguage models, arXiv, 2023, preprint, arXiv:2304.05332,\nDOI: 10.48550/arXiv.2304.05332.\n69 A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards,\nS. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder and\nK. A. Persson, Commentary: The Materials Project: A\nmaterials genome approach to accelerating materials\ninnovation, APL Mater., 2013,1, 011002.\n70 N. Rego and D. Koes, 3Dmol.js: molecular visualization\nwith WebGL,Bioinformatics, 2014,31, 1322–1324.\n71 A. White and G. Hocky,marvis – VMD Audio/Text control with\nnatural language, 2022,https://github.com/whitead/marvis.\n72 W. Humphrey, A. Dalke and K. Schulten, VMD: Visual\nmolecular dynamics,J. Mol. Graphics, 1996,14,3 3–38.\n73 A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey and\nI. Sutskever, Robust speech recognition via large-scale weak\nsupervision, arXiv, 2022, preprint, arXiv:2212.04356, DOI:\n10.48550/arXiv.2212.04356.\n74 M. Baek, et al., Accurate prediction of protein structures\nand interactions using a three-track neural network,\nScience, 2021,373, 871–876.\n75 J. L. Watson,et al., Broadly applicable and accurate protein\ndesign by integrating structure prediction networks and\ndiﬀusion generative models,bioRxiv, 2022, preprint, DOI:\n10.1101/2022.12.09.519842.\n76 C. W. Andersen,et al., OPTIMADE, an API for exchanging\nmaterials data,Sci. Data, 2021,8, 217.\n77 A. A. Volk, R. W. Epps, D. T. Yonemoto, B. S. Masters,\nF. N. Castellano, K. G. Reyes and M. Abolhasani,\nAlphaFlow: autonomous discovery and optimization of\nmulti-step chemistry using a self-drivenuidic lab guided\nby reinforcement learning,Nat. Commun., 2023,14, 1403.\n78 B. J. Shields, J. Stevens, J. Li, M. Parasram, F. Damani,\nJ. I. M. Alvarado, J. M. Janey, R. P. Adams and A. G. Doyle,\nBayesian reaction optimization as a tool for chemical\nsynthesis, Nature, 2021,590,8 9–96.\n79 C. N. Prieto Kullmer, J. A. Kautzky, S. W. Krska, T. Nowak,\nS. D. Dreher and D. W. MacMillan, Accelerating reaction\ngenerality and mechanistic insight through additive\nmapping, Science, 2022,376, 532–539.\n80 B. Rankovi´c, R.-R. Griﬃths, H. B. Moss and P. Schwaller,\nBayesian optimisation for additive screening and yield\nimprovements in chemical reactions – beyond one-hot\nencodings, ChemRxiv, 2022, preprint DOI: 10.26434/\nchemrxiv-2022-nll2j.\n81 A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S. Rosen,\nG. Ceder, K. A. Persson and A. Jain, Structured\ninformation extraction from complex scientic text with\nne-tuned large language models, arXiv, 2022, preprint,\narXiv:2212.05238, DOI:10.48550/arXiv.2212.05238.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 | 1249\nPerspective Digital Discovery\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n82 N. Walker, J. Dagdelen, K. Cruse, S. Lee, S. Gleason,\nA. Dunn, G. Ceder, A. P. Alivisatos, K. A. Persson and\nA. Jain, Extracting Structured Seed-Mediated Gold\nNanorod Growth Procedures from Literature with GPT-3,\narXiv, 2023, preprint, arXiv:2304.13846, DOI: 10.48550/\narXiv.2304.13846.\n83 Neo4j, Neo4j – The World's Leading Graph Database, 2012,\nhttp://neo4j.org/.\n84 S. M. Kearnes, M. R. Maser, M. Wleklinski, A. Kast,\nA. G. Doyle, S. D. Dreher, J. M. Hawkins, K. F. Jensen and\nC. W. Coley, The Open Reaction Database,J. Am. Chem.\nSoc., 2021,143, 18820–18826.\n85 J. Guo, A. S. Ibanez-Lopez, H. Gao, V. Quach, C. W. Coley,\nK. F. Jensen and R. Barzilay, Automated Chemical\nReaction Extraction from Scienti c Literature, J. Chem.\nInf. Model., 2021,62, 2035–2045.\n86 R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,\nC. Guestrin, P. Liang and T. B. Hashimoto, Stanford\nAlpaca: An Instruction-following LLaMA model , 2023,\nhttps://github.com/tatsu-lab/stanford_alpaca.\n87 Alpaca-LoRA, https://github.com/tloen/alpaca-lora.\n88 H. Touvron, T. Lavril, G. Izacard, X. Martinet,\nM.-A. Lachaux, T. Lacroix, B. Rozi `ere, N. Goyal,\nE. Hambro, F. Azhar, et al., Llama: open and eﬃcient\nfoundation language models, arXiv, 2023, preprint,\narXiv:2302.13971, DOI:10.48550/arXiv.2302.13971.\n89 Z. G. Mamaghani, K. A. Hawboldt and S. MacQuarrie,\nAdsorption of CO\n2 using biochar– review of the impact of\ngas mixtures and water on adsorption,J. Environ. Chem.\nEng., 2023,11, 109643.\n90 Y. Peng, V. Krungleviciute, I. Eryazici, J. T. Hupp,\nO. K. Farha and T. Yildirim, Methane Storage in Metal–\nOrganic Frameworks: Current Records, Surprise Findings,\nand Challenges,J. Am. Chem. Soc., 2013,135, 11887–11894.\n91 B. Sahoo, V. Pandey, A. Dogonchi, P. Mohapatra, D. Thatoi,\nN. Nayak and M. Nayak, A state-of-art review on 2D\nmaterial-boosted metal oxide nanoparticle electrodes:\nSupercapacitor applications, J. Energy Storage, 2023, 65,\n107335.\n92 D. D. Suppiah, W. M. A. W. Daud and M. R. Johan,\nSupported Metal Oxide Catalysts for CO\n2 Fischer–Tropsch\nConversion to Liquid Fuels-A Review,Energy Fuels, 2021,\n35, 17261–17278.\n93 M. Gonz´alez-V´azquez, R. Garc´ıa, M. Gil, C. Pevida and\nF. Rubiera, Comparison of the gasication performance\nof multiple biomass types in a bubbling uidized bed,\nEnergy Convers. Manage., 2018,176, 309–323.\n94 M. Mohsin, S. Farhan, N. Ahmad, A. H. Raza, Z. N. Kayani,\nS. H. M. Jafri and R. Raza, The electrochemical study of\nNixCe1−xO2−d electrodes using natural gas as a fuel,New\nJ. Chem., 2023,47, 8679–8692.\n95 P. Kaur and K. Singh, Review of perovskite-structure related\ncathode materials for solid oxide fuel cells, Ceram. Int.,\n2020, 46, 5521–5535.\n96 R. Sengottuvelu,jsonformer, 2018,https://github.com/1rgs/\njsonformer.\n97 R. Sengottuvelu, jsonformer, 2018, https://github.com/\nmartinezpl/jsonformer/tree/add-openai.\n98 K. Choudhary and M. L. Kelley, ChemNLP: A Natural\nLanguage Processing based Library for Materials\nChemistry Text Data, arXiv, 2022, preprint,\narXiv:2209.08203, DOI:10.48550/arXiv.2209.08203.\n99 C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li and P. J. Liu, Exploring the\nlimits of transfer learning with a uni ed text-to-text\ntransformer, J. Mach. Learn. Res., 2020,21, 5485–5551.\n100 T. Wolf, et al. , Transformers: State-of-the-Art Natural\nLanguage Processing, Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing:\nSystem Demonstrations, 2020.\n101 N. Editorials, Tools such as ChatGPT threaten transparent\nscience; here are our ground rules for their use,Nature,\n2023, 613,1 0–1038.\n102 E. R. Mollick and L. Mollick, Using AI to Implement\nEﬀective Teaching Strategies in Classrooms: Five\nStrategies, Including Prompts, SSRN Electron. J. , 2023,\nDOI: 10.2139/ssrn.4391243.\n103 S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li,\nB. A. Shoemaker, P. A. Thiessen, B. Yu, L. Zaslavsky,\nJ. Zhang and E. E. Bolton, PubChem 2023 update,Nucleic\nAcids Res., 2022,51, D1373–D1380.\n104 S. Kim, P. A. Thiessen, T. Cheng, B. Yu and E. E. Bolton, An\nupdate on PUG-REST: RESTful interface for programmatic\naccess to PubChem, Nucleic Acids Res., 2018, 46, W563–\nW570.\n105 S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li,\nB. A. Shoemaker, P. A. Thiessen, B. Yu, L. Zaslavsky,\nJ. Zhang and E. E. Bolton, PubChem 2019 update:\nimproved access to chemical data, Nucleic Acids Res. ,\n2018, 47, D1102\n–D1109.\n106 M. Krenn, R. Pollice, S. Y. Guo, M. Aldeghi, A. Cervera-\nLierta, P. Friederich, G. dos Passos Gomes, F. H ¨ase,\nA. Jinich, A. Nigam, Z. Yao and A. Aspuru-Guzik, On\nscientic understanding with articial intelligence, Nat.\nRev. Phys., 2022,4, 761–769.\n107 A. Hunt and D. Thomas,The Pragmatic programmer : from\njourneyman to master, Addison-Wesley, Boston, 2000.\n108 Q. Campbell, J. Herington and A. D. White, Censoring\nchemical data to mitigate dual use risk, arXiv, 2023,\npreprint, arXiv:2304.10510, DOI: 10.48550/\narXiv.2304.10510.\n1250 | Digital Discovery,2 0 2 3 ,2,1 2 3 3–1250 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Perspective\nOpen Access Article. Published on 08 August 2023. Downloaded on 11/5/2025 1:40:54 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online",
  "topic": "Reflection (computer programming)",
  "concepts": [
    {
      "name": "Reflection (computer programming)",
      "score": 0.5375987887382507
    },
    {
      "name": "Computer science",
      "score": 0.3485226035118103
    },
    {
      "name": "Chemistry",
      "score": 0.3438108563423157
    },
    {
      "name": "Nanotechnology",
      "score": 0.32082295417785645
    },
    {
      "name": "Materials science",
      "score": 0.25123414397239685
    },
    {
      "name": "Programming language",
      "score": 0.068383127450943
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5124864",
      "name": "École Polytechnique Fédérale de Lausanne",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210121988",
      "name": "Film Independent",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210147263",
      "name": "Material Measurement Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1321296531",
      "name": "National Institute of Standards and Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I148283060",
      "name": "Lawrence Berkeley National Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210149682",
      "name": "Biogen (Belgium)",
      "country": "BE"
    },
    {
      "id": "https://openalex.org/I95674353",
      "name": "UCLouvain",
      "country": "BE"
    },
    {
      "id": "https://openalex.org/I4210110490",
      "name": "Instituto Nacional del Carbón",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1330165540",
      "name": "Federal Institute For Materials Research and Testing",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2801081054",
      "name": "The Francis Crick Institute",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1282105669",
      "name": "Argonne National Laboratory",
      "country": "US"
    }
  ],
  "cited_by": 165
}