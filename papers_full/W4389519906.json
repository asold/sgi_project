{
  "title": "Do Language Models Learn about Legal Entity Types during Pretraining?",
  "url": "https://openalex.org/W4389519906",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5039102170",
      "name": "Claire Barale",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004220968",
      "name": "Michael Rovatsos",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084342011",
      "name": "Nehal Bhuta",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3204342617",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385570803",
    "https://openalex.org/W3177382889",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2573492843",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4385571145",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W4375958664",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3034340683",
    "https://openalex.org/W4287020173",
    "https://openalex.org/W4283810944",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4287208198",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W4287889082",
    "https://openalex.org/W3176589722",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4385572404",
    "https://openalex.org/W4385571079",
    "https://openalex.org/W3154903254",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W2890026792"
  ],
  "abstract": "Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures.",
  "full_text": "Proceedings of the Natural Legal Language Processing Workshop 2023, pages 25–37\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nDo Language Models Learn about Legal Entity Types during Pretraining?\nClaire Barale and Michael Rovatsos\nSchool of Informatics\nThe University of Edinburgh\n{claire.barale,michael.rovatsos}@ed.ac.uk\nNehal Bhuta\nSchool of Law\nThe University of Edinburgh\nnehal.bhuta@ed.ac.uk\nAbstract\nLanguage Models (LMs) have proven their\nability to acquire diverse linguistic knowledge\nduring the pretraining phase, potentially\nserving as a valuable source of incidental\nsupervision for downstream tasks. However,\nthere has been limited research conducted on\nthe retrieval of domain-specific knowledge,\nand specifically legal knowledge. We propose\nto explore the task of Entity Typing, serving\nas a proxy for evaluating legal knowledge as\nan essential aspect of text comprehension, and\na foundational task to numerous downstream\nlegal NLP applications. Through systematic\nevaluation and analysis and two types of\nprompting (cloze sentences and QA-based\ntemplates) and to clarify the nature of these\nacquired cues, we compare diverse types\nand lengths of entities both general and\ndomain-specific entities, semantics or syntax\nsignals, and different LM pretraining corpus\n(generic and legal-oriented) and architectures\n(encoder BERT-based and decoder-only with\nLlama2). We show that (1) Llama2 performs\nwell on certain entities and exhibits potential\nfor substantial improvement with optimized\nprompt templates, (2) law-oriented LMs show\ninconsistent performance, possibly due to\nvariations in their training corpus, (3) LMs\ndemonstrate the ability to type entities even in\nthe case of multi-token entities, (4) all models\nstruggle with entities belonging to sub-domains\nof the law (5) Llama2 appears to frequently\noverlook syntactic cues, a shortcoming\nless present in BERT-based architectures.\nThe code of the experiments is available\nat https://github.com/clairebarale/\nprobing_legal_entity_types.\n1 Introduction\nDuring the initial phase of pretraining, language\nmodels (LMs) are exposed to an extensive corpus\nof textual data, allowing them to acquire the ca-\npacity to represent the probabilistic structure of\nlanguage. In this process, it has been theorized that\nthey incidentally learn various linguistic signals\nand patterns, both syntactic and semantic. Work by\nPetroni et al. (2019) and subsequent studies (Jiang\net al., 2020b) make the hypothesis that a side ef-\nfect of the pretraining stage is that LMs also learn\nfactual knowledge. On the other hand, Gururangan\net al. (2020) research demonstrates the significance\nof both model pretraining and task-specific pretrain-\ning; pretraining a model with a specific focus on a\nparticular task or a limited domain corpus yields no-\ntable advantages in enhancing model performance\nand adaptability.\nEntity typing and extraction are crucial tasks\nfor a range of use cases including named-entity\nrecognition (NER), relation extraction, summariza-\ntion, structuring raw data, and most specifically\nin law, legal search, and past cases retrieval. To\ngain more insights into entity typing and extraction,\nentity probing tasks have been designed for bidi-\nrectional LSTM conditional random field models\n(Augenstein et al., 2017), masked language mod-\nels (Petroni et al., 2019; Jiang et al., 2020b) and\nautoregressive LMs (Epure and Hennequin, 2022),\nusing GPT-2.\nConversely, one notable bottleneck of the appli-\ncation of NLP within the legal domain is the lack\nof resources and annotated datasets. Thereby, it is\nof particular interest to explore the extent to which\nLMs, during their pretraining phase, acquire a suf-\nficient understanding of legal entities, serving as\na surrogate for legal knowledge. Ultimately, LMs\ncould be exploited as a source of weak and indi-\nrect supervision in downstream tasks such as legal\nNER or question answering (QA), as they consti-\ntute a good proxy to use natural text incidentally\nthanks to their pretraining stage. Indeed, humans\ndo not exclusively rely on exhaustive supervision\nbut instead make use of occasional feedback and\nlearn from incidental signals originating from var-\nious sources. This approach holds potential for\n25\nPretrained Language ModelPretraining corpus # Parameters # Tokens Corpus size # Vocab\nLegal\nCaseHOLD(Zheng et al., 2021)Harvard Case Law 110M 43B 37 GB 32K\nPile of law(Henderson et al., 2022)US, Canadian, ECthR 340M 130B 256 GB 32K\nLexLM(Chalkidis et al., 2023)US, Canada, EU, UK, India 125 2T + 256B 175 GB 50K\nGeneric\nBookCorpus (Zhu et al., 2015) - 16GB -\nCC_news (Nagel, 2016) - 76GB -\nOpenWebText (Radford et al., 2019) - 38GB -\nStories (Trinh and Le, 2018) - 31GB -\nRoBERTa(Liu et al., 2019) 125M 2T 160GB 50K\nDeBERTa(He et al., 2023) 86M 2T 160GB 128K\nLlama 2(Touvron et al., 2023)Data from publicly available sources 7B 2T - 32K\nTable 1: Overview of the models used. The table reports the description of the pretraining corpora, the number of\nparameters, the total number of tokens, the size of the corpus, and the vocabulary size\nincreased flexibility in terms of entity types, in con-\ntrast to supervised methods, and presents an alter-\nnative to existing automated annotation extraction\napproaches (Tedeschi and Navigli, 2022; Savelka,\n2023) which hold limitations in the set of entity\ntypes. It presents several advantages: it does not\nrequire human annotation, it can be easily com-\nbined with other sources of supervision such as le-\ngal knowledge bases, and it would support an open\nset of entities and user queries. It would offer the\nadvantage of seamless and fast application to new\ndatasets while facilitating transfers of knowledge\nbetween datasets and even potentially between dif-\nferent domains. In this paper, we study the inter-\nsection of entity knowledge and legal knowledge\nembedded within LMs, evaluated on a AsyLex, a\ndataset of Canadian Refugee Decisions.\n1.1 Research questions\nWe are interested in evaluating the quality of the\nentity knowledge learned during pretraining in off-\nthe-shelf LMs, specifically domain-specific enti-\nties, such as those pertinent to the legal field. How\nproficient are Language Models at acquiring\nknowledge about domain-specific entities like le-\ngal entities during pretraining? Can this acquired\nknowledge be considered sufficiently reliable for\ntasks such as annotating new datasets or serving as\nan indirect source of supervision? How does the\nchoice of prompt type impact the results obtained\nfrom knowledge queries? To what extent does the\nvariation in acquired knowledge differ across en-\ntity types? What categories of factual knowledge\ncan LMs retrieve, and in what instances do they\nmake errors? Does domain-specific pretraining\nand jurisdiction-specific pretraining enhance the\namount of factual knowledge compared to generic\npretraining? To what degree does knowledge ac-\nquisition in the legal domain overlap with that of\ngeneral language models?\n1.2 Contributions\nDiffering from the research objectives of Petroni\net al. (2019), which focuses on relation extraction,\nand Chalkidis et al. (2023) which investigates eight\ndistinct legal knowledge probing tasks with a fo-\ncus on legislation and legal terminology, we focus\non Legal Entity Types. To be clear, we ask the\nLM to predict the entity type, similarly to Epure\nand Hennequin (2022), and not the actual entity.\nFor example, in the prompt <Mask> is the capital\nof Germany, we expect the answer to be City or\nLocation and not Berlin.\nIn addition, we adopt a comprehensive interpre-\ntation of entity types, aligned with the work of\nBarale et al. (2023), encompassing both essential\nfactual knowledge (e.g., location and dates) and\nmore abstract legal concepts, such as the credibil-\nity of a claimant and the rationale behind a judg-\nment. Moreover, our approach diverges by allow-\ning longer entities to be masked (Figure 4), where\nprevious work was limited either to single token\n(Petroni et al., 2019) or 2-tokens entities (Jiang\net al., 2020a; Chalkidis et al., 2023). Where most\nprevious work focuses on masked language mod-\neling objective models (MLM), we introduce the\nuse of autoregressive LM (Llama2) in a zero-shot\nsetting, similar to the approach employed in Epure\nand Hennequin (2022).\nWe make the hypothesis that pretrained LMs\ninherently contain structured knowledge about spe-\ncific domains, which could be leveraged to generate\nincidental training instances. We seek to investi-\ngate the depth of a model’s knowledge, its nature,\nand whether it predominantly acquires knowledge\nfrom semantic or syntactic cues.\n26\nWe first conduct in section 3 an analysis of the\npretraining corpus of selected models both generic\nand legal LMs. We then prompt the LM with two\ndifferent styles of prompts, cloze text and question-\nbased, for the task of Entity Typing (section 5).\nAfter evaluating the experimental results in section\n6, we analyze the type of failure cases (6.5) to high-\nlight the strengths and weaknesses of the learning\nprocess and to draft directions for future work.\nOur contributions are as follows:\n• We propose two new experiments on the task of\nLegal Entity Typing in a zero-shot setting on a\nlarge set of entity types: Experiment MLMthat\nevaluates generic and legal BERT-based LM on\ncloze sentences, and Experiment Llama2which\nevaluates Llama2 on QA-style prompts.\n• We report the results for both experiments and\nshow that Llama2 exhibits good performance\non specific entities and has the potential for im-\nprovement with optimized prompts. However,\nlaw-oriented LMs display inconsistent results,\nlikely influenced by training corpus variations\nand struggle with Refugee Law-specific vocabu-\nlary.\n• We propose an in-depth analysis of the failure\nmodes of the models on this task, opening the\nway for future work.\n2 Background and related work\n2.1 Legal NLP and Legal LMs\nA range of tasks and use cases have been investi-\ngated in legal NLP (Zhong et al., 2018), including\nsummarization, information retrieval, and extrac-\ntion, or question answering. It is worth emphasiz-\ning that entity typing is foundational for many of\nthese tasks.\nThe legal domain presents numerous challenges\nfor self-supervised learning, primarily due to the\nspecificity of legal language in contrast to ordinary\nlanguage. This can lead to ambiguity in contex-\ntual meaning (that we aim to assess in this paper),\npotential implicit meanings, and variations in the\nsignificance of a term. A term that may be decisive\nin a legal context, such as \"appeal,\" might not carry\nthe same weight in a generic domain.\nGiven these specific challenges and the demon-\nstrated benefits of pretraining LM on legal text to\nachieve better performance on downstream tasks\nGen 100.0 34.8 \nCH 34.8 \nPol 1 46.2 55.7 \nLexLM 41.2 44.5 \n00� CJ� \n46.2 41.2 \n55.7 44.5 \n55.1 \nq_& �� 'v0 \nFigure 1: V ocabulary overlap (%) between the pretrain-\ning corpora. Gen stands for Generic and is sampled\nfrom sources similar to RoBERTa’s pretraining corpus,\npresented in Table 1. V ocabularies are created with the\ntop 10K most frequent tokens in a sample of 50K docu-\nments per model\n(Barale et al., 2023), there has been interest in pre-\ntraining models on legal texts (Zhong et al., 2018;\nXiao et al., 2021). These models typically use an\nencoder-only architecture based on the BERT ar-\nchitecture: LegalBERT (Chalkidis et al., 2020),\nCaseHOLD (Zheng et al., 2021), Pile of Law (Hen-\nderson et al., 2022), and LexLM (Chalkidis et al.,\n2023), that we use in our first experiment (details\nin Table 1). To the best of our knowledge, there is\nno decoder-only legal LM, which is why we limit\nour second experiment to a Llama2.\n2.2 Probing LMs for Entity Typing\nThe idea of latent language representations derived\nfrom pre-trained LMs holds promise as a source\nof structured knowledge. Similar to human learn-\ning, LMs accumulate domain-specific and linguis-\ntic knowledge, along with the development of gen-\neral pattern recognition capabilities through their\npretraining experiences (Brown et al., 2020). As\nnoted in the introduction, our work follows Petroni\net al. (2019)’s LAnguage Models Analysis frame-\nwork (LAMA) and LegalLAMA (Chalkidis et al.,\n2023). Several probing methods have been inves-\ntigated (Yin et al., 2023), evaluating multilingual\nextraction (Jiang et al., 2020a) as well as effec-\ntive prompting for factual knowledge extraction\n(Haviv et al., 2021; Qin and Eisner, 2021; Blevins\net al., 2023). Various types of tasks have been tar-\ngeted by these works, including relation extraction,\nNER, or entity typing (Shen et al., 2023), our task\nof interest. Concurrently, there have been efforts\n27\nto enhance entity typing pipelines, particularly to\nexpand the range of entities beyond traditional cate-\ngories like location or dates (Choi et al., 2018; Dai\net al., 2021) or to entities unseen during training\n(Epure and Hennequin, 2022; Lin et al., 2020), and\napproaches leveraging supervision from other tasks\nsuch as QA (Zhang et al., 2022). However, to the\nbest of our knowledge, there has been no work con-\nducted in the legal domain specifically addressing\nentity typing, and no prior research on entity typing\nin this domain has made use of prompts in the form\nof questions.\n3 Pretraining corpus analysis\n3.1 Vocabulary\nTo understand the difference between pretraining\ncorpora across LMs, we conduct an exploratory\nvocabulary analysis inspired by Gururangan et al.\n(2020) that investigates the impact of domain-\nspecific pretraining on a range of downstream tasks.\nThis preliminary study is destined to clarify and\noffer insights that will help explain the results of\nthe experiments presented in section 5. We select\na total of fifty thousand documents for each lan-\nguage model, perform basic cleaning, tokenize the\ntext, and remove stopwords, which gives us a list\nof tokens per LM. From this list, we select the most\ncommon ten thousand tokens, that constitute the\nfinal vocabulary for a given LM.\nFor the three legal LMs, as the datasets used for\npretraining are directly available, we randomly se-\nlect the fifty thousand documents. To construct a\ngeneric pretraining corpus, we reconstitute a vocab-\nulary based on the RoBERTa and DeBERTa corpus\nas indicated in Table 1. As for the other models,\nwe gathered fifty thousand entries, selected pro-\nportionally to the size of each corpus. That is to\nsay, we select 5,000 documents from BookCorpus\nwhich constitutes 10% of RoBERTa pretraining\ndata, 23,750 entries from CC_news, 11,875 entries\nfrom OpenWebText(using the open source version:\n(Gokaslan and Cohen, 2019)), and 9,688 entries\nfrom CC_stories. Given our limited knowledge of\nthe precise composition of Llama2’s pretraining\ncorpus, we propose utilizing the generic vocabular-\nies of RoBERTa and DeBERTa as suitable proxies\nfor our analysis.\n3.2 Vocabulary Overlap\nThe vocabulary overlap is represented in percent-\nage in the matrix in Figure 1. As anticipated, the\nMLM clozed-style prompt\n The appellant has spent the last two years in <MASK>\n[Predefined]\nlist_entity_types = [city, location, tribunal, affidavit, convention,\nreport...]\nentity = China\n Answer: A Location\nFigure 2: Experiment MLM prompt example\nQA Prompt example\nThis is an entity typing task. If you don't know the   \nanswer, just say that you don't know, don't try to make\nup an answer. Answer with the correct entity type only. \nThere is a list of entity types:\n{list_entity_types=list_entity_types}.\nQuestion: What entity type is {entity=Nice}?\n[Predefined]\nlist_entity_types = [city, country, tribunal, affidavit, convention,\nreport...]\nentity = Nice\n Answer: A City\nFigure 3: Experiment Llama2 QA prompt example\nlegal LMs exhibit a greater overlap in vocabulary\ncompared to their counterparts with generic train-\ning. However, significant disparities emerge among\nthe legal LMs. For example, CaseHOLD shares\nonly 44.5% of its vocabulary with LexLM. This\nobservation may be attributed to the more extensive\nand more diverse set of jurisdictions included in\nthe LexLM pretraining corpus. This aligns with\nthe fact that LexLM shows a higher percentage\nof vocabulary overlap with Pile of Law, which,\nin contrast to CaseHOLD which is limited to the\nUnited States, also includes legal documents from\na broader range of jurisdictions.\n4 Dataset\nWe useAsyLex, a dataset of refugee decisions from\nCanada curated for entity typing and extraction 1.\nThis publicly available dataset comprises 19,115\nhuman annotated instances, encompassing 20 dis-\ntinct categories of entities that hold legal relevance\nas explained in Barale et al. (2023). These cate-\ngories have been identified as categories of inter-\nest with the collaboration of legal professionals\nand experts in the field of refugee law. AsyLex\ncomprises 59,112 historical decision documents,\n1https://huggingface.co/datasets/clairebarale/\nAsyLex\n28\nEntity Type # 1t # 2t # 3t # 3+\nLOCATION 8970 694 214 122\nDATE 5269 3414 709 608\nNORP 9849 122 4 25\nORG 8473 338 228 961\nCREDIBILITY 4181 2732 1470 1617\nDETERMINATION 448 378 3161 6013\nCLAIMANT_INFO 2603 3531 2633 1233\nPROCEDURE 5390 2673 1103 834\nDOC_EVIDENCE 5192 3711 561 476\nEXPLANATION 1142 1963 1061 5834\nLEGAL_GROUND 2801 4142 868 2189\nLAW 688 2783 1511 5018\nLAW_CASE 4562 2136 663 2639\nLAW_REPORT 2327 537 2542 459\n8970\n5269\n9849\n8473\n4181\n448\n2603\n5390 5192\n1142\n2801\n688\n4562\n2327\n122\n608\n25\n961\n1617\n6013\n1233\n834\n476\n5834\n2189\n5018\n2639\n459\n# 1t # 2t # 3t # 3+\nFigure 4: Length of the target masked entities, in number of tokens, for all entity types\nspanning from 1996 to 2022. These documents\nare derived from the online repository of cases of\nthe Canadian Legal Information Institute 2. The\ndocuments encompass both initial determinations\nand subsequent appeals on whether the claimant is\ngranted refugee status or not. It is important to note\nthat the dataset contains entities of varying gen-\nerality. Some entities, such as geographical loca-\ntions, possess broad applicability and could be per-\ntinent to any text (generic entity types: location,\ndate, norp). Others are more specialized within\nthe legal domain, such as procedural steps (generic\nlegal entity types: org, law, claimant_info,\nprocedure, doc_evidence, law_case). Finally,\ncertain entities are highly specific to refugee law,\nsuch as the assessment of credibility, which fre-\nquently determines the acceptance or rejection\nof a refugee claim (Refugee Law entity types:\ncredibility, determination, explanation,\nlegal_ground, law_report ). This diversity in\nentity scope presents an opportunity for assessing\nthe impact of pretraining, particularly in scenarios\nwhere entity types have received various exposures\nduring the pretraining phase.\n4.1 Legal Entity Types\nThe selection of legal entity types within this\ndataset is intended to encapsulate characteristics\nthat can reflect similarities among various refugee\ncases (see Appendix A for an exhaustive descrip-\ntion of the types) and for which we have precise\ngold-standard annotations (Barale et al., 2023). The\nset of 14 entity types is pre-defined and closed for\nboth experiments. To extend the coverage of each\nentity type and extract specific entities, we use a\nsynonym generator to give synonyms for each of\nour 14 entity types. As a result, when prompted, the\nmodel would have to choose between a total of 151\n2Canlii: https://www.canlii.org/en/\nentity types, increasing the difficulty but also the\ninterest of the task. For example,location accepts\ncity or country. The complete list of synonyms gen-\nerated per entity type is available in Appendix B.\nIn our evaluation process, we assess predictions\nacross the 14 entity types. For instance, if a predic-\ntion is country, it will be categorized as location\nand evaluated against a gold answer that specifies\nlocation. Contrary to previous work, we do not\nlimit the entities’ length to a single token (Petroni\net al., 2019) or to entities spanning only two tokens\n(Jiang et al., 2020a). On the contrary, one of the\nobjectives is to use entity types in a broader way\nfor extracting information from text. Thereby we\nare interested in identifying multi-token entities\nthat have short spans of text and can be longer than\ntwo tokens, which is often the case for explaining\na decision for instance (entity type: explanation).\nThe length of the entity per entity type is presented\nin Figure 4, which provides explicit numerical val-\nues for both single-token entities and entities longer\nthan three tokens.\n5 Proposed Entity Typing Methodology\n5.1 Task description\nThe goal of this task is to classify legal entities men-\ntioned in text documents or sentences into specific\ntypes. Legal entities can include various organi-\nzations, companies, government bodies (org), or\nmore abstract concepts such as the credibility as-\nsessment made in the context of a refugee claim\n(credibility). The task involves extracting and\ncategorizing these entities based on their attributes\nor context within the text. As input, we use text\ndocuments split by sentences that contain mentions\nof legal entities. We then categorized legal entity\ntypes for each mention found in the input text.\nLet E be the set of possible entity types: E =\n{e1, e2, . . . , en}, S the set of sentences or text seg-\n29\nments, T the set of masked tokens within the sen-\ntences and P(ei|tj, sk) represents the conditional\nprobability that masked token tj in sentence sk be-\nlongs to entity type ei. The goal is to find the entity\ntype ei that maximizes the conditional probability\nfor each masked token tj in each sentence sk:\ne∗\ni = arg max\nei∈E\nP(ei|tj, sk)\nIn other words, the objective is to find the entity\ntype that is most likely for each masked token in\neach sentence.\n5.2 Language models used\nFor the first experiment, Experiment MLM with\nBERT-based LMs, we experiment with two generic\nmodels optimized for MLM, RoBERTa, and De-\nBERTa, and three legal-oriented LMs (see Table\n1). For the second experiment, Experiment Llama2\nwe use the open-source model Llama2, optimized\nfor dialogue use cases. Both tasks take the list\nof entity types as an input argument, making it a\nmultiple-choice task.\n5.3 Cloze prompts with BERT-based models\nFor the first experimental setting, we use cloze-\nstyle prompts that perfectly fit masked language\nmodels (MLM). We replace the entities in the sen-\ntences with a masked token and use BERT-based\nmodels with an MLM objective. Multi-token en-\ntities are substituted with a single masked token.\nIf multiple entities appear in the same sentence,\nonly the initial entity occurring in the sentence is\nconsidered. The model’s answers are limited to the\npredefined list of 151 entities. We do not provide\nmore context than what is contained in the input\nsentence. We randomly select ten thousand sen-\ntences per entity type, for which we have ground\ntruth annotations (the actual number of prompts\nafter cleaning is given in the column # prompts in\nTable 2). An example of a cloze-style prompt is\ngiven in Figure 2. With this Experiment MLM, our\nobjective is to assess whether the models can make\npredictions about the type of entity to expect based\non contextual and syntactic cues. For instance, in\nthe example presented in Figure 2, we assume that\na human reader could deduce from the context that\na location is the expected entity to fill the masked\nportion. Can an LM do the same?\n5.4 QA prompts with Llama2\nFor the second experimental setting, Experiment\nLlama2, we use a template that briefly explains the\ntask to the model and we input the predefined list of\n151 entity types. To provide a simple task framing,\nwe prompt the language model according to the\nfollowing template: \"What entity types is {entity}?\",\nto which the model is asked to answer with the most\nprobable entity type. Because of the format of the\nprompt, we use a text generation objective with an\nopen-source, state-of-the-art auto-regressive LM,\nLlama2. We use the smallest available version\nof the model (7B parameters, to spare computing\nresources) and its fine-tuned version Llama2-chat,\nwhich is optimized for dialogue use cases. An\nexample of this QA-style prompt is presented in\nFigure 3. In that experiment, the prompt explicitly\nmentions the entity, for example, here the question\nis \"What entity type is Nice?\" which makes it a\nsimpler task compared to the task of Experiment\nMLM.\n6 Experimental Results\nWe evaluate the results in terms of recall since we\nwant to ensure capturing as many true positives as\npossible, and F1 score to assess the overall perfor-\nmance on the task. In this section, we compare the\nresults in terms of LM used, length of the input\nentity, prompt type, and entity type, before con-\nducting an error analysis in the section 6.5. The\nresults of Experiment MLM are presented in Table\n2 and the results of Experiment Llama2 in Table 3.\n6.1 Language Models Comparison\nGiven the high difficulty of the task, the choice\nbetween 151 entity types when accounting for the\nsynonyms list, and the lack of description of the\nentities and extra context given, it is no surprise\nthat the scores are relatively low. However, the\ngoal of this work is not to reach the best accuracy,\nbut rather to explore where the models succeed or\nfail. On Experiment MLM, results are generally\nlower than in Experiment Llama2 which is firstly\nexplained by the greater difficulty of the task of\nExperiment MLM and the relative lack of context\nprovided for this task. In this experiment, Pile of\nLaw is the model that performs the best on average,\nin terms of F1, retrieving 16.36% of entity types,\nwith 9.47% in recall. The second best performing\nmodel is CaseHOLD with 15.29% average F1 and\n8.58% average recall. This is despite LexLM’s big-\n30\nType\nModel RoBERTa DeBERTa CaseHOLD PoL LexLM # prompts\nR F1 R F1 R F1 R F1 R F1\nLOCATION 0.058 0.110 0.108 0.194 0.070 0.131 0.336 0.503 0.055 0.104 9,913\nDATE 0.036 0.069 0.100 0.183 0.034 0.065 0.025 0.048 0.071 0.133 9,442\nNORP 0.037 0.072 0.035 0.067 0.031 0.060 0.032 0.062 0.065 0.122 9,986\nORG 0.088 0.161 0.066 0.123 0.081 0.149 0.018 0.036 0.074 0.138 9,947\nCREDIBILITY 0.028 0.054 0.026 0.051 0.123 0.219 0.056 0.106 0.028 0.055 9,527\nDETERMINATION 0.384 0.555 0.079 0.147 0.070 0.131 0.142 0.249 0.071 0.133 7,242\nCLAIMANT_INFO 0.080 0.149 0.060 0.114 0.081 0.150 0.079 0.147 0.045 0.085 9,666\nPROCEDURE 0.128 0.227 0.080 0.148 0.078 0.145 0.207 0.344 0.228 0.129 9,716\nDOC_EVIDENCE 0.128 0.228 0.125 0.223 0.188 0.317 0.048 0.092 0.056 0.105 9,814\nEXPLANATION 0.013 0.026 0.013 0.026 0.009 0.018 0.088 0.161 0.008 0.015 8,825\nLEGAL_GROUND 0.029 0.056 0.048 0.091 0.045 0.087 0.041 0.079 0.061 0.116 9,640\nLAW 0.093 0.170 0.203 0.337 0.237 0.383 0.061 0.115 0.066 0.124 9,128\nLAW_CASE 0.079 0.146 0.071 0.133 0.058 0.109 0.106 0.191 0.091 0.167 9,290\nLAW_REPORT 0.057 0.107 0.075 0.140 0.098 0.178 0.087 0.160 0.089 0.164 8,601\nTable 2: Entity type prediction scores in a zero-shot setting, on cloze sentences, measured in Recall and F1 score\nacross 2 generic LMs (RoBERTa and DeBERTa-V3), and 3 legal LMs (CaseHOLD, Pile of Law and LexLM)\nType R F1\nLOCATION 0.956 0.916\nDATE 0.730 0.575\nNORP 0.211 0.118\nORG 0.098 0.051\nLAW 0.100 0.053\nCREDIBILITY 0.219 0.123\nDETERMINATION 0.357 0.217\nCLAIMANT_INFO 0.627 0.456\nPROCEDURE 0.259 0.149\nDOC_EVIDENCE 0.653 0.485\nEXPLANATION 0.006 0.003\nLEGAL_GROUND 0.022 0.011\nLAW_CASE 0.034 0.017\nLAW_REPORT 0.048 0.025\nTable 3: Entity type prediction scores in a zero-shot\nsetting, on QA-style prompts, measured in Recall and\nF1 score, with Llama2, on 10K prompts per entity\nger size, LexLM being the model that performs the\nworst across all, being outperformed by generic\nLMs RoBERTa and DeBERTa. For all entities ex-\ncept one, the model that achieved the best recall\nalso achieved the best F1, highlighting the consis-\ntency in the precision. The only exception is the\ntype procedure for which the best F1 is reached\nwith Pile of Law and the best recall with LexLM.\n6.2 Single-token vs Multi-token\nAn interesting point is that we did not impose\nany restrictions on the length of entities; the en-\ntities that tend to be longer are typically more\nabstract and closer to a piece of legal common-\nsense knowledge and reasoning, for example,\nexplanation, determination, credibilty and\nlegal ground. Interestingly the best overall F1\nscore in Experiment MLM is achieved for the type\ndetermination, reaching an F1 score of 55.5%\n(RoBERTa). For instance, an entity flagged as\ndetermination can as long as: claimants are not\nconvention refugees and not persons in need of\nprotection. While the other models achieve lower\nscores for this entity type, it is to note that the dis-\nparity between these relatively lengthy multi-token\nentities and those that are typically single tokens\nis not substantial (refer to Figure 4). This may be\ndue to the nature of the task, which may mitigate\nsuch disparities compared to tasks like NER where\nthe model has to retrieve the actual entity. In Ex-\nperiment Llama2, shorter entities (that are also the\nmost generic ones) are well recognized (location,\ndate), with also good scores achieved on the types\ndetermination, claimant_info, procedure .\nOverall for both experiments and certainly due to\nthe nature of the task of entity typing, it seems that\nthe length of the initial entity to categorize does not\nhave an impact on the results.\n6.3 Prompt Templates Comparison\nThe scores are on average higher in the Exper-\niment Llama2 with a total average F1 score of\n30.86% when Experiment MLM reaches an average\nof 14.46% across all types. However, as noted in\n31\nError Type Prompt example Prediction Gold %MLM\nRandom Prediction under <mask> of the Republic of China, they cannot take on a second citizenship lawsuitlaw 70.71Contextually Accurate the applicant has not returned to <mask> since 2008 employmentlocation 12.43Closely Related my colleague relied on this <mask> in her conclusion ngo reportdoc_evidence 16.86\nLlama2\nRandom Prediction What isSubsection 648? country law 22.22Closely Related What isvietnamese? country nationality ( norp) 18.52False Negative What isfemale claimant? female claimant gender ( claimant_info) 33.33Prompt Error What isremoval order? It is a type of judicial decision. procedure 25.93\nTable 4: Error cases and the ratio of the different error types for both experiments, across all tested models\nGen CH PoL LexLM Llama2\nGeneric 11.59 8.51 20.42 11.97 63.26\nGen Legal 17.98 20.89 15.40 12.48 29.52\nRefugee Law 10.015.93 4.68 4.92 13.03\nTable 5: Entity types prediction scores averaged on 3\ngroups: generic (location, date, norp), legal enti-\nties applicable to most legal domains (Gen Legal: org,\nlaw, claimant_info, procedure, doc_evidence,\nlaw_case), and legal entities specific to refugee\nlaw (Refugee Law: credibility, determination,\nexplanation, legal_ground, law_report) Gen\ngroups the results of RoBERTa and DeBERTa-v3,CH\nrefers to CaseHOLD\nthe task description (5.1) the QA-based experiment\nis a relatively easier task, making the comparison\ndifficult.\nBased on the predicted entity types, it appears\nthat the template suggested for Experiment Llama2\nis not consistently well comprehended, resulting\nin a lack of clarity regarding the task. In some\ncases, it returns not just one entity type, but multi-\nple, leading to incorrect predictions. It seems that,\ninstead of relying on manually crafted prompts and\ntemplates, which have been acknowledged to be\nsu-optimal as mentioned by Jiang et al. (2020a),\nthere is significant room for improvement in this\nregard.\n6.4 Entity Types Comparison\nFor this evaluation, we categorize the type of entity\ninto three groups: those that can be encountered\nin any text with the same meaning, those that are\ncommonly found in legal texts, and those that are\nhighly specific to the domain of the dataset, refugee\nlaw. The combined results are summarized in Ta-\nble 5. Entities related to refugee law tend to yield\nthe lowest performance across all models and set-\ntings. Pile of Law outperforms other models even\non generic entities. At the same time, RoBERTa\nand DeBERTa surpass models specifically trained\non legal data for generic legal entities, possibly due\nto larger exposure and a larger vocabulary.\n6.5 Failure Cases Analysis\nWe identify four types of errors across the two\nexperiments:\n1. Random Prediction: this refers to cases where\nthe predicted entity type is entirely random and\nunrelated to the context.\n2. Contextually Accurate: this describes situations\nwhere the predicted entity type is incorrect, but\nwithin the context of the sentences, it is plausi-\nble in terms of syntax and meaning.\n3. Closely Related: instances where the predicted\nentity class is incorrect, yet it is closely related\nto the actual gold entity type. For example, it\nmisclassifies a legal ground (which is very pre-\ncisely one of the 5 reasons for being granted\nrefugee status, see Appendix A) for an explana-\ntion of the decision (which is more generic).\n4. False Negative, it predicts an entity type that is\nnot in the list of entity types given as input.\n5. Prompt Error, if the answer provided deviates\nfrom the prompt instruction, we categorize it\nas incorrect; we consistently consider that an\nanswer with more than five tokens is incorrect,\nas it signifies that the response extends beyond\nproviding just the entity type.\nExperiment MLM errors To assess the occur-\nrence of error types, we sample 10 errors per entity\ntype per model, i.e. a total of 700 errors for this\nexperiment. Table 4 presents the findings and an\nexample per error type. There is no instance of a\nFalse Negative error; the models never predict an\nentity type that is not in the input predefined list as\nwe constraint the model to a multiple-choice task,\nfrom our pre-defined list of entity types. The most\ncommon error is simply an incorrect prediction,\nwith the second most frequent error being the pre-\ndiction of a closely related entity. This may be due\nto the choice of categories, some of which express\nsubtle legal nuances. Another positive sign is the\n32\npresence of more than 10% of incorrect predictions\nthat are nevertheless accurate in the context of the\ninput sentence.\nExperiment Llama2errors Similarly, we sam-\nple 10 errors per entity type, i.e. a total of 135\nerrors. Table 4 presents the findings. It’s worth\nnoting that the use of QA-style prompts leads to a\nsignificant number of prompt errors and false nega-\ntives, which we believe could be mitigated to some\nextent by improving the initial prompt template in\nfuture work. Additionally, a common misclassi-\nfication pattern occurs with norp entities, which\nare always adjectives, but are misclassified as their\nnoun counterparts, as illustrated in the example\nprovided in Table 4. Similarly, acronyms for tri-\nbunals (e.g., RPD for Refugee Protection Division)\nare classified as units of length, an issue that might\nbe rectified by providing more contextual informa-\ntion. Finally, entities likeconsistent explanationare\noccasionally misclassified as explanation when\nthey should be categorized as a credibility as-\nsessment, possibly due to missing adjectives or\nentity length-related challenges.\n7 Conclusion\nOur investigation includes LM selection, input en-\ntity length, prompt types, and entity types, in an\nattempt to understand model strengths and limita-\ntions. In summary, our study shows that Llama2\nperforms best on specific entities and displays\npotential for improvement with better prompting\nstrategies. However, it also seems that Llama2\nrepeatedly overlooks syntactic cues. Masked lan-\nguage models mostly appear to be lacking suffi-\ncient context within our experimental setup, where\nthey are confronted with a highly challenging task.\nLaw-oriented LMs exhibit varying results, possibly\ninfluenced by training corpus differences, and the\nPile of Law model shows the best performance on\nour AsyLex dataset. Despite inherent challenges,\nLMs can accurately identify certain entity types, in-\ncluding multi-token ones, but encounter difficulties\nwith legal sub-domains like Refugee Law. Future\nresearch may explore optimized prompts and few-\nshot learning strategies. Furthermore, assessing\nthe average precision of the entity type ranking\npredictions generated by the LM, and conducting\nexperiments on additional datasets, would also be\nnecessary.\nReferences\nIsabelle Augenstein, Leon Derczynski, and Kalina\nBontcheva. 2017. Generalisation in named entity\nrecognition: A quantitative analysis. Computer\nSpeech & Language, 44:61–83.\nClaire Barale, Michael Rovatsos, and Nehal Bhuta.\n2023. Automated refugee case analysis: A NLP\npipeline for supporting legal practitioners. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2023, pages 2992–3005, Toronto, Canada.\nAssociation for Computational Linguistics.\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer. 2023.\nPrompting language models for linguistic structure.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 6649–6663, Toronto, Canada.\nAssociation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nIlias Chalkidis, Nicolas Garneau, Catalina Goanta,\nDaniel Katz, and Anders Søgaard. 2023. LeXFiles\nand LegalLAMA: Facilitating English multinational\nlegal language model development. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15513–15535, Toronto, Canada. Association\nfor Computational Linguistics.\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle-\nmoyer. 2018. Ultra-fine entity typing. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 87–96, Melbourne, Australia. Association for\nComputational Linguistics.\nHongliang Dai, Yangqiu Song, and Haixun Wang. 2021.\nUltra-fine entity typing with weak supervision from a\nmasked language model. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1790–1799, Online. Association\nfor Computational Linguistics.\nElena V . Epure and Romain Hennequin. 2022. Prob-\ning pre-trained auto-regressive language models for\nnamed entity typing and recognition. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 1408–1417, Marseille, France.\nEuropean Language Resources Association.\n33\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nAdi Haviv, Jonathan Berant, and Amir Globerson. 2021.\nBERTese: Learning to speak to BERT. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 3618–3623, Online. Association\nfor Computational Linguistics.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDeBERTav3: Improving deBERTa using ELECTRA-\nstyle pre-training with gradient-disentangled embed-\nding sharing. In The Eleventh International Confer-\nence on Learning Representations.\nPeter Henderson, Mark S. Krass, Lucia Zheng, Neel\nGuha, Christopher D. Manning, Dan Jurafsky, and\nDaniel E. Ho. 2022. Pile of law: Learning respon-\nsible data filtering from the law and a 256gb open-\nsource legal dataset.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5943–5959, On-\nline. Association for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nBill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan\nMoreno, Xiao Huang, Prashant Shiralkar, and Xi-\nang Ren. 2020. TriggerNER: Learning with entity\ntriggers as explanations for named entity recogni-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n8503–8511, Online. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSebastian Nagel. 2016.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJaromir Savelka. 2023. Unlocking practical applica-\ntions in legal domain: Evaluation of gpt for zero-shot\nsemantic annotation of legal texts. In Proceedings\nof the Nineteenth International Conference on Artifi-\ncial Intelligence and Law, ICAIL ’23, page 447–451,\nNew York, NY , USA. Association for Computing\nMachinery.\nYongliang Shen, Zeqi Tan, Shuhui Wu, Wenqi Zhang,\nRongsheng Zhang, Yadong Xi, Weiming Lu, and\nYueting Zhuang. 2023. PromptNER: Prompt locating\nand typing for named entity recognition. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12492–12507, Toronto, Canada. Association\nfor Computational Linguistics.\nSimone Tedeschi and Roberto Navigli. 2022. MultiN-\nERD: A multilingual, multi-genre and fine-grained\ndataset for named entity recognition (and disambigua-\ntion). In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, pages 801–812,\nSeattle, United States. Association for Computational\nLinguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\n34\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nChaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu,\nand Maosong Sun. 2021. Lawformer: A pre-trained\nlanguage model for chinese legal long documents. AI\nOpen, 2:79–84.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don’t know? In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 8653–8665, Toronto,\nCanada. Association for Computational Linguistics.\nWenzheng Zhang, Wenyue Hua, and Karl Stratos. 2022.\nEntqa: Entity linking as question answering. In In-\nternational Conference on Learning Representations\n(ICLR).\nLucia Zheng, Neel Guha, Brandon R. Anderson, Peter\nHenderson, and Daniel E. Ho. 2021. When does\npretraining help? assessing self-supervised learning\nfor law and the casehold dataset. In Proceedings\nof the 18th International Conference on Artificial\nIntelligence and Law . Association for Computing\nMachinery.\nHaoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao,\nZhiyuan Liu, and Maosong Sun. 2018. Legal judg-\nment prediction via topological learning. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 3540–3549,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, ICCV, pages\n19–27.\nAppendix\nA Legal Entity Types Description\nB Detail of the Extended Entity Types\nList\nC Error Types Detail per LM for\nExperiment MLM\n35\nType Description Examples\nLOCATION cities, countries, regions \"toronto, ontario\"\nDATE absolute or relative dates or periods \"june, 4th 1996\", \"two years\"\nNORP adjectives of nationalities, religious, politi-\ncal or ethnic groups or communities\n\"hutu\", \"nigerian\", \"christian\"\nORG tribunals, companies, NGOs \"immigration appeal division\", \"human\nrights watch\"\nCREDIBILITY mentions of credibility \"lack of evidence\", \"inconsistencies\"\nDETERMINATION outcome of the decision (accept/reject) \"appeal is dismissed\", \"not a convention\nrefugee\"\nCLAIMANT_INFO age, gender, citizenship, occupation \"28 year old\", \"citizen of Iran\", \"female\"\nPROCEDURE steps in the claim and legal procedure\nevents\n\"removal order\", \"sponsorship for applica-\ntion\"\nDOC_EVIDENCE pieces of evidence, proofs, supporting doc-\numents\n\"passport\", \"medical record\", \"marriage\ncertificate\"\nEXPLANATION reasons given by the panel for the determi-\nnation\n\"fear of persecution\", \"no protection by\nthe state\"\nLEGAL_GROUND referring to the Convention, refugee status\nis granted for reasons of race, religion, na-\ntionality, membership of a particular social\ngroup or political opinion\n\"homosexual\", \"christian\"\nLAW citations: legislation and international con-\nventions\n\"section 1(a) of the convention\"\nLAW_CASE citations: case law and past decided cases \"xxx v. minister of canada, 1994\"\nLAW_REPORT country reports written by NGOs or the\nUnited Nations\n\"amnesty international: police and military\ntorture of women in mexico, 2016\"\nTable 6: Pre-defined list of legal entity types\n36\nType Extended List\nLOCATION city, country, region, state, province, area, nation, land, republic, district, territory,\ndivision, zone\nDATE date, day of the month, appointment, particular date, date stamp, time, timestamp,\ncalendar date, schedule\nNORP nationality, religious community, political group, ethnic groups, community, racial\ngroup, party, faction, ideological group, belief community\nORG tribunal, firm, ngo, company, corporation, business, nonprofit, association, charity,\ncourt, judicial body\nCREDIBILITY plausibility, authenticity, integrity, trustworthiness, reliability, credibility, believabil-\nity, credibility, credibleness\nDETERMINATION verdict\", result, resolution, judgment, approval, denial, decline, rejection, approval,\ndetermination, finding, conclusion, decision, grant, refusal, positive decision, nega-\ntive decision\nCLAIMANT_INFO data, employment, resident, national, inhabitant, information, gender, age, citizen,\ncitizenship, sex, job, occupation, profession\nPROCEDURE affidavit, documentary evidence, proof, testimony, exhibit, record, file, paperwork,\noperation, procedure, legal procedure, legal process, judicial procedure, legal steps,\njudicial process\nDOC_EVIDENCE proof, evidence, document, written document, written evidence, written proof, written\nrecord, written report, written statement, written testimony, written witness statement\nEXPLANATION explanation, clarification, interpretation\nLEGAL_GROUND reason, ground, legal ground, justification, rationale, foundation, legal basis, legal\njustification\nLAW convention, international convention, law, legislation, legal code, treaty, agreement,\nprotocol, statute\nLAW_CASE citation, jurisprudence, case, law, case law, legal case, lawsuit, legal matter, legal\nprecedent, judicial decisions, legal rulings\nLAW_REPORT country report, report, official report, written report, ngo report, national report,\nstate report, regional report, nonprofit report, non-governmental organization report,\ncharity report\nTable 7: Extended pre-defined list of legal entity types (151 types)\nError Type RoBERTa DeBERTa CaseHOLD PoL LexLM # Total %\nRandom Prediction 109 81 90 112 103 495 70.71\nContextually Accurate 7 27 25 16 12 87 12.43\nClosely Related 24 32 25 12 25 118 16.86\nFalse Negative - - - - - - 0.00\nTotal 140 140 140 140 140 700 100\nTable 8: Error types figures per (number of occurrences) and in percentage for all studied LM\n37",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8376915454864502
    },
    {
      "name": "Natural language processing",
      "score": 0.6835065484046936
    },
    {
      "name": "Syntax",
      "score": 0.5769705772399902
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5369278192520142
    },
    {
      "name": "Task (project management)",
      "score": 0.5182592868804932
    },
    {
      "name": "Security token",
      "score": 0.4918496608734131
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.45946595072746277
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4103240370750427
    },
    {
      "name": "Programming language",
      "score": 0.1512610912322998
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": []
}