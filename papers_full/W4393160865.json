{
  "title": "Preparing Lessons for Progressive Training on Language Models",
  "url": "https://openalex.org/W4393160865",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2106581543",
      "name": "Yu Pan",
      "affiliations": [
        "Harbin Institute of Technology",
        "Shenzhen Institute of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1988620429",
      "name": "Ye Yuan",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2163935963",
      "name": "Yichun Yin",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2122229269",
      "name": "Jiaxin Shi",
      "affiliations": [
        "Huawei Technologies (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2607121186",
      "name": "Zenglin Xu",
      "affiliations": [
        "Harbin Institute of Technology",
        "Peng Cheng Laboratory",
        "Shenzhen Institute of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1994445760",
      "name": "Ming Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2151842933",
      "name": "Lifeng Shang",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1988620429",
      "name": "Ye Yuan",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2163935963",
      "name": "Yichun Yin",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2122229269",
      "name": "Jiaxin Shi",
      "affiliations": [
        "Huawei Technologies (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2607121186",
      "name": "Zenglin Xu",
      "affiliations": [
        "Shenzhen Institute of Information Technology",
        "Peng Cheng Laboratory",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1994445760",
      "name": "Ming Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2151842933",
      "name": "Lifeng Shang",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6846577953",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W2766207105",
    "https://openalex.org/W2178031510",
    "https://openalex.org/W6753640285",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6762715600",
    "https://openalex.org/W6784673708",
    "https://openalex.org/W4297950310",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W4221141790",
    "https://openalex.org/W3088112198",
    "https://openalex.org/W2901014608",
    "https://openalex.org/W4387796543",
    "https://openalex.org/W6795770965",
    "https://openalex.org/W2798836595",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W6766152879",
    "https://openalex.org/W6810509939",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2979450969",
    "https://openalex.org/W4321393046",
    "https://openalex.org/W3105020640",
    "https://openalex.org/W4323112125",
    "https://openalex.org/W3101845875",
    "https://openalex.org/W2991140127",
    "https://openalex.org/W3012776236",
    "https://openalex.org/W6789902073",
    "https://openalex.org/W3207296158",
    "https://openalex.org/W3097132740",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3138562265",
    "https://openalex.org/W4306886919",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3129418925",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2966234499",
    "https://openalex.org/W4225512894",
    "https://openalex.org/W4303939357",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2963628712",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3170925726",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3108384060",
    "https://openalex.org/W2979055841",
    "https://openalex.org/W2964088127",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4313043552",
    "https://openalex.org/W4308900169",
    "https://openalex.org/W2945667196",
    "https://openalex.org/W3123799706"
  ],
  "abstract": "The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prepares lessons for expanding operations by learning high-layer functionality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even rivaling methods using pretrained models, making it a universal and efficient solution for training deep models while reducing time, financial, and environmental costs.",
  "full_text": "Preparing Lessons for Progressive Training on Language Models\nYu Pan1*, Ye Yuan3,4 *, Yichun Yin5, Jiaxin Shi6, Zenglin Xu1,2† ,\nMing Zhang3,4† , Lifeng Shang5, Xin Jiang5, Qun Liu5\n1Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China\n2Pengcheng Laboratory, Shenzhen, China\n3School of Computer Science, Peking University, Beijing, China\n4Peking University-Anker Embodied AI Lab\n5Huawei Noah’s Ark Lab, Shenzhen, Guangdong, China\n6Cloud BU, Huawei Technologies\nAbstract\nThe rapid progress of Transformers in artificial intelligence\nhas come at the cost of increased resource consumption\nand greenhouse gas emissions due to growing model sizes.\nPrior work suggests using pretrained small models to im-\nprove training efficiency, but this approach may not be suit-\nable for new model structures. On the other hand, training\nfrom scratch can be slow, and progressively stacking lay-\ners often fails to achieve significant acceleration. To address\nthese challenges, we propose a novel method called Apollo,\nwhich prepares lessons for expanding operations by learning\nhigh-layer functionality during training of low layers. Our\napproach involves low-value-prioritized sampling (LVPS) to\ntrain different depths and weight sharing to facilitate effi-\ncient expansion. We also introduce an interpolation method\nfor stable model depth extension. Experiments demonstrate\nthat Apollo achieves state-of-the-art acceleration ratios, even\nrivaling methods using pretrained models, making it a uni-\nversal and efficient solution for training deep models while\nreducing time, financial, and environmental costs.\nIntroduction\nTransformers (Vaswani et al. 2017) have recently achieved a\nsignificant impact on the field of artificial intelligence (Wang\net al. 2020; Li et al. 2020; Dosovitskiy et al. 2021; Cao et al.\n2021; Wang et al. 2023a). Nevertheless, the training cost is\nincreasing in terms of the growing model size, which causes\nan amount of resourcing consumption and greenhouse gases\nemission (Schwartz et al. 2019; Pan et al. 2019). Addressing\nthis problem, recent work (Chen et al. 2022; Chen, Goodfel-\nlow, and Shlens 2016; Wang et al. 2023b; Pan et al. 2023)\nsuggests to improve training efficiency by reusing a pre-\ntrained small model as an initialization method that con-\ntains knowledge prior. However, the requirement of a pre-\ntrained model can cause fatal obstacles, especially for a\n*These authors contributed equally. This work was done when\nYu Pan and Ye Yuan were interns at Huawei Noah’s Ark Lab.\n†Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nnew-designed model structure, which affects the applicabil-\nity of these studies as general strategies for training. On the\nother hand, the studies of training from scratch (Gong et al.\n2019; Yang et al. 2020) generally stack layers to train Trans-\nformer progressively. Nevertheless, these methods usually\ncannot achieve significant acceleration in training, and are\nthus slower than training from pretrained models. Against\nthis background, it is an emergency to design a universal\nmethod to efficiently train the models with reduced time and\nfinancial costs, while benefiting the ecological environment.\nTo achieve this goal, the progressive expansion of mod-\nels in depth proves to be a crucial aspect in training from\nscratch. One noteworthy example is StackBERT (Gong et al.\n2019), where a stacking learning strategy contributes to im-\nproved training efficiency through two merits: (1) Fewer lay-\ners in the initial stages of training require fewer computa-\ntional resources, leading to faster training; (2) Lower trained\nweights provide a usable prior that benefits the training of\nstacked higher weights. While StackBERT undeniably ac-\ncelerates training from the second merit, there are still two\nconcerns that need to be addressed. Firstly, the suitability\nof the stacking method is questionable. For instance, di-\nrectly stacking the 1-st layer onto the 7-th layer of a 12-\nlayer Transformer is not intuitive due to the clear differ-\nences in semantic functionality between them (Rogers, Ko-\nvaleva, and Rumshisky 2020). Moreover, even though there\nmight be some similarities across the entire model, it has\nbeen pointed out that most of the layers are different from\neach other (Chen et al. 2022). Consequently, it raises doubts\nabout whether the normally trained weights are sufficiently\nprepared well to be expanded effectively, given the lack of\nknowledge in higher layers.\nMotivated by this consideration, we introduce “Apollo” -\na novel approach to preparing lessons for low-layer weights\nto learn the high-layer functionality in the training process.\nThis strategy proves beneficial in further extending the ca-\npabilities of the model. In essence, Apollo involves two\nkey components. Firstly, we employ a low-value-prioritized\nsampling (LVPS) technique, which randomly selects a depth\nfor training at each step. This helps to ensure a diverse\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18860\nTraining Step \nStage 1\nLayer 1\nLayer 1\nLayer 2\nLayer \nLayer 1\nLayer 2\nStage Stage \nLayer \nLayer \nLayer 1\nLayer 2\nLayer \nLayer \nLayer 1\nLayer \nLayer 1\nLayer \nLayer '\nLayer \nLayer \nInterpolation Interpolation Interpolation\nSampling  with LVPS algorithm.\nFigure 1: An illustration of the Apollo for training an L-layered model within T steps. We divide this training process into S\nstages. In the t-th step at the s-th stage, the model weights are N(s) layers (the left layers in each stage in the figure). To let the\nN(s) layers learn functionality in high layers in advance, we constructL(t) layers (the right layers in each stage in the figure) by\nsharing the N(s) weights through an interpolation method, where N(s) ≤ L(t). As shown in the figure, the same color denotes\nthe same weight. We randomly chooseL(t) at t-th step through a probability function Low-Value-Prioritized Sampling (LVPS).\nSince LVPS tends to select shallower layers, it can greatly save computation costs. Furthermore, we progressively increase\nthe N(s) weights when stepping into the next stage. Since weights in the early stage can learn the properties of higher layers,\nApollo can significantly contribute to the training efficiency.\ntraining experience. Subsequently, we share the low-layer\nweights, enabling them to adapt to the selected layers by\nLVPS. These shared weights are well-prepared not only for\nlearning high-layer functionality but also for recurrent trans-\nformation, as supported by previous work (Lan et al. 2020;\nDehghani et al. 2019; Yang et al. 2021). It is worth noting\nthat while weight-sharing strategies have been explored pre-\nviously to facilitate information exchange across layers, our\napproach is novel in its dynamic application to sample lay-\ners, which is important to learn high-layer property to im-\nprove training efficiency. Furthermore, we address the issue\nof training stability by introducing an interpolation method\nto extend the depth of the model. This is essential since di-\nrectly stacking layers can cause large gradients, which can\nbe detrimental to training stability. In all, we summarize our\ncontribution as:\n• Through sharing weights in the early stage to learn the\nfunctionality of high layers, Apollo effectively expands\nthe depth of networks, resulting in remarkable training\nacceleration.\n• Through LVPS, Apollo achieves a substantial reduction\nin training FLOPs by predominantly sampling low depth\nlayers, while retaining the benefit of expanding depth.\n• Through replacing layer stacking with layer interpola-\ntion, Apollo further enhances the stability of the ex-\npanded model.\n• Experiments show that Apollo attains state-of-the-art\ntraining efficiency, surpassing even the methods reliant\non pretrained models.\nRelated Work\nEfficient training from scratches. Training from scratch\nmeans training without any prior knowledge. Some ap-\nproaches (Gong et al. 2019; Yang et al. 2020; Li et al. 2022;\nGu et al. 2021; Shen et al. 2022) are known as “progres-\nsive training,” which involves initially pretraining a smaller\nscratch model and then gradually increasing its size, result-\ning in accelerated training. There are various training strate-\ngies for such models that are universally applicable and in-\ndependent of our own method. For instance, employing op-\ntimization techniques like Adam (Kingma and Ba 2015)\ncan accelerate the learning process by considering the op-\ntimizer’s perspective. Shoeybi et al. (2019) have success-\nfully utilized mixed precision training to enhance training\nefficiency, while low-rank methods provide a viable option\nfor memory and time-efficient training (Kamalakara et al.\n2022). Additionally, Wu et al. (2021) have demonstrated im-\nproved data efficiency by taking note of rare words. Other\neffective training methods include dropping layers (Zhang\nand He 2020), knowledge inheritance (Qin et al. 2022), and\nmerging tokens (Bolya et al. 2022). Our method embraces\nthe concept of progressive training and attains noteworthy\ntraining efficiency by preparing instructive lessons to facili-\ntate the expansion of layer depth.\nEfficient training by reusing pretrained models. Recent\nstudies have demonstrated the great potentials of develop-\ning large pre-trained models via expanding a small model.\nThe pioneering work of Net2Net (Chen, Goodfellow, and\nShlens 2016) introduced the concept of function-preserving\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18861\ntransformations which increase the width via neuron split-\nting and the depth via identity layers. However, the randomly\nchosen neurons for splitting in Net2Net are not promised\nto be well performed. Addressing this challenge, a series\nof studies (Wu, Wang, and Liu 2019; Wu et al. 2020b;\nWang et al. 2019b; Wu et al. 2020a) have adopted func-\ntional steepest descent to select the optimal subset of neu-\nrons for splitting. Following the idea of function preserva-\ntion, bert2BERT (Chen et al. 2022) expanded small trans-\nformers. More recently, LiGO (Wang et al. 2023b) intro-\nduced a trainable linear operator to learn an effective ex-\npansion formula. Mango (Pan et al. 2023) utilized a ten-\nsor ring matrix product operator (TR-MPO) to grow a small\npretrained model to a large counterpart for efficient train-\ning. In the past, the utilization of pretrained methods con-\nsistently yielded greater acceleration than that achieved by\ntraining from scratch. Remarkably, our approach demon-\nstrates highly promising results, surpassing even the swift-\nness of training from a pretrained model.\nMethod\nIn this section, we will introduce our method to sample the\ninformation of the higher layer to accelerate training.\nNotations\nTo describe the implementation of Apollo, we introduce re-\nlated notations here. We denote an L-layered network by\nf(L)(·), and denote the function of the l-th layer of f(L) as\nfl(·), where l ∈ [L]. Thus, given an input x, f(L)(x) can be\nformulated as\nf(L)(x) = fL(fL−1(. . . fl(. . . f1(x)))). (1)\nWe denote the set of N weights in f as {θi}N\ni=1, and the\nweights of l-the layer as Θ(fl). As our method samples in-\nformation of higher layers through weight sharing, here we\nformally consider a mapping function g(·) to arrange the\ng(l)-th weight to the l-th layer as\nΘ(fl) = θg(l), (2)\nwhere g(·) ∈ [N]. Moreover, for the convenience of express-\ning the process of layer expansion, we denote the layer size\nof the t-th step as L(t), where t ∈ T and T is the total steps.\nThus, a network at the t-th step can be denoted by f(L(t)).\nTransformer Architecture.Here, we introduce the struc-\nture of the Transformer, each block of which mainly contains\ntwo basic types of layers, i.e., the multi-head self-attention\n(MHSA) layer, and the feed-forward neural network (FFN)\nlayer. Assuming inputs of the l-th layer are query Ql ∈ Rd,\nkey Kl ∈ Rd, and value Vl ∈ Rd, where d is the dimension,\nthe l-th layer can be formulated as\nfl(Ql, Kl, Vl) = FFN(MHSA(Ql, Kl, Vl)). (3)\nThe parameters of the MHSA layer of the l-th layer are de-\nnoted by W{Q,K,V,O}\nl ∈ Rd×d. The weights of the FFN\nlayer are WIN\nl ∈ Rd×αd and WOUT\nl ∈ Rαd×d, where α\nis an expanding ratio that is often set to 4. Note that, we ne-\nglect biases in the formulation for simplification, which does\nAlgorithm 1: Process of Apollo\nRequire: the input data x, the ground-truth y, the stage set-\nting {st, t∈ [1, T], st ∈ [1, S]}: a non-decreasing list,\nindicating the stage of each step.\n1: for t = 1 to T do\n2: if t >1 and st > st−1 then\n3: for n = 1 to N(s) do\n4: θn := COPY\n\u0012\nθgN(sprev):N(s)\ninterpolation (n)\n\u0013\n5: end for\n6: end if\n7: L(t) = LVPS(N(s), L)\n8: for l = 1 to L(t) do\n9: Θ(fl) := SHARE\n\u0012\nθgN(s):L(t)\ninterpolation (l)\n\u0013\n10: end for\n11: L = Loss\n\u0010\nf(L(s))(x), y\n\u0011\n12: {∆θi}N(t)\n= L. backward()\n13: Update all the weights {θi}N(s)\nthrough {∆θi}N(s)\n14: end for\nEnsure: The trained model f(L) with {θi}L\nnot affect the generality of the proposed method. Following\nthe above formulation, all weights of the l-th layer can be\ndenoted as θl = W{Q,K,V,O,IN,OUT }\nl .\nEfficient Training by Apollo\nGiven the potential similarities between high and low layers,\nthe gradual expansion of layers during training can lead to\nenhanced acceleration, as opposed to the direct approach of\ntraining from scratch (Gong et al. 2019). However, as previ-\nously discussed, lower layers struggle to effectively capture\nthe intricacies of higher-level features, particularly when uti-\nlizing a stacking methodology which can cause training in-\nstability. To address this challenge, we introduce Apollo, an\ninnovative approach that facilitates progressive model train-\ning by leveraging weight sharing within the training process.\nThis approach enables the accession of high-level function-\nality prior to layer expansion, thus mitigating the aforemen-\ntioned issue.\nProgressive Training. Specifically in an L-layer net-\nwork, Apollo divides the whole training process into S\nstages. We useN(s), s∈ [S] to denote the weight number of\nthe s-th stage, satisfying\nN(s) < N(s+1). (4)\nAs restricted in Eq. (4), Apollo increases actual weights\nwhen stages go on. This progressive training way is efficient\nfor accelerating language models.\nPreparing Lessons. In order to provide lessons for N(s)\nweights within the s-th stage, thereby facilitating the pre-\nlearning of higher-layer functionalities, Apollo employs a\nstrategic weight-sharing approach. This involves distribut-\ning the N(s) weights across L(t) layers, drawn from the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18862\n1 2 3 4 5 6\nL(t)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPLVPS\n(\nL(t))\nk=0\nk=1\nk=2\nk=3\nk=4\nk=5\nFigure 2: A case of choosing hyper-parametersk of LVPS to\nsample 1-6 layer number.\nrange [N(s), L], thereby establishing a connection to higher-\nlayer elements. The determination of the appropriate L(t) is\nrealized through the utilization of a sampling function de-\nnoted as P(L(t)) at each training step. The core idea of this\nsampling function is the prioritization of shallower depths\n– specifically, N(s) in this context – to strike a balance be-\ntween computational efficiency and sustained performance.\nGrounded in this consideration, we introduce a novel ap-\nproach coined as Low-Value-Prioritized Sampling (LVPS).\nIn the development of LVPS, we employ an inverse pro-\nportional function as the cumulative distribution function for\nlayer selection with a formulation as\nFLVPS(x) = c − b\nx + k , c >0, b >0, x∈ [N(s), L], (5)\nwhere b, kand c are the hyper-parameters. This func-\ntion is congruent with our sampling objective, which fa-\nvors the selection of shallower depths. Then, by setting\nFLVPS\n\u0000\nN(s)\u0001\n= 0 and FLVPS (L) = 1. The probability den-\nsity function PLVPS can be solved as\nPLVPS(L(t)) =\n(\nb\n(L(t)+k)2 , if L(t) ∈ [N(s), L],\n0, otherwise, (6)\nw.r.t\nZ\nPLVPS(L(t)) dL(t) = 1, (7)\nwhere b and c can be solved in terms of Eq. (7) as\nb = (N(s) + k) ∗ (L + k)\nL − N(s) , c = L + k\nL − N(s) . (8)\nTherefore, we only need to adjust k to derive different sam-\npling settings as shown in Fig. 2. In this paper, we set\nk = 0 in every experiment to obtain the least computation\ncomplexity. We employ the notation LVPS(α, β) to signify\nthe process of sampling a value within the interval [α, β].\nPLVPS(L(t)) is the probability function of LVPS(N(s), L).\nA comparison with other sampling methods can be found\nin Table 1 and Fig. 3. In detail, Uniform Sampling (US)\nsamples the layers equally, thereby mitigating the potential\nlayer bias, while Edge Sampling (ES) tends to sample layers\n1 2 3 4 5 6\nL(t)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(L(t))\nUS\nFS\nES\nLVPS\nFigure 3: Comparison among US, FS, ES, and LVPS to sam-\nple 1-6 layer number.\nMethod Probability Density Function\nLVPS PLVPS(L(t)) = b\n(L(t)+k)2\nES PES\n\u0000\nL(t)\u0001\n= 1\nk ∗\n\u0010\n1\nL(t)−N(s)−b + 1\nL+b−L(t)\n\u0011\nUS PUS(L(t)) = 1\nL−N(s)\nFS PFS(L) = 1\nTable 1: An overview of sampling methods: (1) Low-Value-\nPrioritized Sampling (LVPS), (2) Uniform Sampling (US),\n(3) Edge Sampling (ES), and (4) Full Sampling (FS). L(t)\ncan only derive values in [N(s), L]. Since the integration of\nthe probability density function is 1, b can be solved when\nk is determined. In this paper, we set k = 0 and k = 10 for\nLVPS and ES, respectively. FS always samplesL(t) = L.\nin low and high positions to learn the high-layer informa-\ntion while maintaining efficiency. Alternatively, Full Sam-\npling (FS) always samples the deepest depth, helping early-\ntrained weights adequately acquire the functionality of each\nlayer, which, however, demands significant computational\nresources due to its selection of the maximum layer num-\nber at each step. Among these sampling methods, LVPS can\nachieve the highest efficiency in progressive training.\nStack V .S. Interpolation\nIn the training process of Apollo, it is important to select a\nfeasible method for expanding the layer for sharing in each\nstep and initializing weights of the next stage. As shown in\nFig. 4, the common-used methods are stacking and interpo-\nlating layers. Given a target to expandL1 to L2, the stacking\nmethod can be formed as\ngL1:L2\nstack (l2) = l2 mod L1, (9)\nwhere l2 ∈ [L2] is the index of L2. The stacking method is\nusually adopted in language models, e.g., BERT (Gong et al.\n2019). By contrast, the interpolation method is often used in\nthe computer vision field, e.g., ResNet (Chang et al. 2018),\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18863\nLayer 4\nLayer 5\nLayer 6\nLayer 1\nLayer 2\nLayer 3\nLayer 1\nLayer 2\nLayer 3\nLayer 1\nLayer 3\nLayer 6\nLayer 2\nLayer 4\nLayer 5\nStack Interpolation \nFigure 4: A case of expanding 3 layers to 6 layers. The same\ncolor denotes the same weight. The stacking method recur-\nrently arranges the layers, e.g., the 1-st layer → the 4-th\nlayer. By contrast, the interpolation method arranges the lay-\ners in a neighbor, e.g., the 1-st layer → the 2-nd layer.\nand can be formulated as\ngL1:L2\ninterpolation(l2) = ⌊l2 ∗ L1\nL2\n⌉, (10)\nwhere ∗ means dot production, and ⌊·⌉ denotes the rounding\noperation. Although the two expanding methods have both\nshown good performance in their fields, there is still a lack\nof analysis on the comparison between them, especially in\nthe applicability of language models. In this paper, we in-\nvestigate the influence of the stability and performance of\nthese methods. We defer an analysis experiment to the exper-\niment section. Here, we would like to give the conclusion:\n(1) There is only a small performance gap between them;\n(2) interpolation can reach better stability than the stacking\nmethod. As a result, we adopt interpolation instead of the\nstacking method as the expanding method of the proposed\nApollo training for language models.\nExperiment\nIn this section, we conduct experiments to validate the per-\nformance of our proposed method.\nCommon setting: We implement a series of experiments\non BERT and GPT for validation. We use AdamW as the\noptimizer with a learning rate of 10−4 and weight decay of\n10−2 in all the experiments. We chose the training batch\nsizes of 768 and 512 for BERT (Devlin et al. 2019) and\nGPT (Radford et al. 2019) models, respectively. We use the\nScratch model, StackBERT, bert2BERT, and LiGO as base-\nlines. Layer numbers of Apollo are [1, 3, 6, 12] and change\nat epoch [2, 4, 10]. LiGO is warmly trained for 100 steps\nas claimed in the original paper (Wang et al. 2023b). The\ntraining dataset is a concatenation of English Wikipedia and\nToronto Book Corpus (Zhu et al. 2015).\nExperiment on Expanding Method\nWe implement the experiment to show the influence of\nstacking and interpolating layers. We use BERT as the back-\nbone. We train a 6-layer BERT model called BERT-Base/2\nwhich is the half of BERT-Base for 10 epochs with a 768\nbatch size. Then, we expand the trained BERT-Base/2 into\nthe 12-layer BERT-Base through the stacking (Eq. (9)) and\n3\n 2\n 1\n 0 1 2 3 4 5\nValue\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Density\nBERT-Base/2\nBERT-Base/2-S\nBERT-Base/2-I\nFigure 5: Distribution of output activations. BERT-Base/2\nis half of BERT-Base. BERT-Base/2-S and BERT-Base/2-I\ndenote to stack and interpolate BERT-Base/2 to BERT-Base,\nrespectively. After stacking BERT-Base/2, the distribution\nof output activations changes a lot, while the interpolation\nmethod keeps the distribution well.\nModel Trainable Layer\nLoss Gradient (1e-3)\nBERT-Base Ra.\n- 12 10.56 11.45±26.46\nBERT-Base/2 -\n6 1.82 1.66±4.71\nBERT-Base/2-S ✗ 12 7.32 55.28±241.52\nBERT-Base/2-I ✓ 12 3.97 44.01±225.76\nTable 2: Results of the analysis of expanding methods.\nBERT-Base Ra. means a randomly initialized BERT-Base.\nWhen expanding BERT-Base/2 from 6 to 12, the loss and\ngradients of the stacking method rise sharply, which causes\nfailure in further training. By contrast, the interpolation\nmethod achieves smaller gradients and loss and is thus train-\nable later. However, both of the two methods cause higher\ngradients than a random model, indicating an unstable state.\ninterpolating (Eq. (10)) methods. We use 500 data samples\nfor validation. Moreover, we apply Apollo to train BERT-\nBase with stacking and interpolation methods.\nIllustrated in Fig.5, directly stacking BERT-Base/2 ex-\nhibits a notable alteration in the distribution of output ac-\ntivations, whereas the interpolation method preserves this\ndistribution. This distinction results in interpolation yielding\nsmaller losses and gradients in comparison to the stacking\ntechnique. This advantage contributes to the enhanced train-\nability of BERT-Base-I, as evidenced in Table2. Addition-\nally, in Table 3, where expanding BERT-Base/2 increases\nboth loss and gradients, Apollo showcases a reverse trend.\nHere, layer expansion within Apollo leads to a reduction\nin the loss and gradient for the lessons on the functionality\nof 12 layers, ensuring robust stability in progressive learn-\ning. Notably, interpolation further diminishes gradient val-\nues and improves the acceleration ratio, measured in terms\nof FLOPs. Therefore, we adopt interpolation as the preferred\nexpansion method for Apollo considering these findings.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18864\n0 2 4 6 8\nFLOPs (1e19)\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3MLM Loss\n4.0 4.5 5.0 5.5 6.0 6.5 7.0\n1e19\n1.44\n1.46\n1.48\n1.50\n35.6%41.6% 29.5% 0%\nbert2BERT\nLiGO\nStackBERT\nScratch\nApollo\n(a) On BERT-Base (FLOPs).\n0 40 80\nWall Time (hrs)\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3MLM Loss\n180000 200000 220000 240000 260000 280000 300000 320000\n1.44\n1.46\n1.48\n1.50\n41.1% 35.2% 28.9% 0%\nbert2BERT\nLiGO\nStackBERT\nScratch\nApollo (b) On BERT-Base (Wall Time).\n0 2 4 6 8\nFLOPs (1e19)\n2.45\n2.50\n2.55\n2.60\n2.65\n2.70\n2.75\n2.80\n2.85\n2.90GEN Loss\n4 5 6 7 8\n1e19\n2.48\n2.50\n2.52\n43.2%47.9 36.0% 0%\nbert2BERT\nLiGO\nStackBERT\nScratch\nApollo (c) On GPT-Base (FLOPs).\nFigure 6: Results of BERT-Base and GPT-Base. Apollo achieves the highest acceleration on BERT-Base and GPT-Base in terms\nof FLOPs at 41.6% and 47.9%, respectively. In addition, Apollo can keep the best training efficiency on wall time for BERT-\nBase at 41.1%. Apollo surpasses methods (i.e., bert2BERT and LiGO) relying on pretrained models in all cases.\nModel Acc. Ratio Layer\nLoss Gradient (1e-3)\nApollo-S 39.7% 6 1.82 1.65\n±3.72\n12 1.76 1.42±3.50\nApollo-I 41.6% 6 1.82 1.59\n±3.33\n12 1.76 1.39±3.01\nTable 3: Results of expanding methods for Apollo. “S” and\n“I” mean the stacking and the interpolation methods, respec-\ntively. Apollo can decrease the gradient values loss after ex-\npanding layers since learning the functionality of a 12-layer\nnetwork. Moreover, the interpolation method can derive a\ncomparably better acceleration ratio in terms of FLOPs and\nsmaller gradient values.\nExperiment on Sampling Method\nWe construct the experiment on BERT-Base to investigate\nthe influence of the sampling methods including ES, US, FS,\nand LVPS, with a comparison to the w/o sampling case.\nResults are shown in Fig. 7. The LVPS sampling method\nleads with the highest acceleration ratio, attaining an im-\npressive 41.6%. Both ES and LVPS attach the top two po-\nsitions for acceleration, underscoring the utility of sampling\nlower layers. Significantly, LVPS outperforms Apollo w/o\nsampling by a substantial margin of +9.3%, affirming the\npronounced benefits of extracting high-layer functionalities\nthrough sampling. US performing normally may indicate\nthat sampling middle layers is not really helpful. FS demon-\nstrates the least acceleration due to its consistent sampling\nof the maximum layers, incurring a substantial FLOP cost.\nIn summation, LVPS strategically harnesses the ability to\ncapture high-layer functionalities while simultaneously min-\nimizing FLOP usage by primarily sampling lower layers.\nExperiment on BERT\nWe construct this experiment to show the training efficiency\nof BERT. We train Apollo and StackBERT from scratch,\n0 2 4 6 8\nFLOPs (1e19)\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3MLM Loss\n4.0 4.5 5.0 5.5 6.0 6.5 7.0\n1e19\n1.44\n1.46\n1.48\n1.50\n37.5%41.6% 24.5% 0%\nScratch\nw/o Sampling\nApollo (ES)\nApollo (US)\nApollo (FS)\nApollo (LVPS)\nFigure 7: Results of the analysis on sampling methods.\nLVPS achieves the highest acceleration ratio at 41.6%, sur-\npassing ES and FS at 4.1% and +17.1%, respectively. Most\nsampling methods are faster than Apollo w/o Sampling. FS\nperforms the lowest acceleration by always sampling the\ndeepest depth, which is resource-consuming.\nwhile training bert2BERT and LiGO from BERT-Small to\nBERT-Base for 40 epochs. Additionally, we also implement\nshort training on BERT-Large for 5 epochs.\nAs shown in Fig. 6, Apollo achieves the highest ac-\nceleration rate of 41.6% in FLOPs saving, which outper-\nforms bert2BERT by an additional +6.0%. Through train-\ning from a pretrained BERT-Small, bert2BERT and LiGO\nexhibit superior performance compared to the progressive\ntraining approach StackBERT, resulting in acceleration ra-\ntios of 29.5% with corresponding improvements of +6.1%\nand +4.0%, respectively. However, the convergence speed\nof both bert2BERT and LiGO is found to be slower in com-\nparison to Apollo, which shows the accelerated ability of\nthe Apollo method through the “lessons” of higher layers.\nTo substantiate the effectiveness of Apollo, comprehensive\nevaluations were conducted using the SQuAD and GLUE\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18865\nModel Saving\n(FLOPs)\nSaving\n(Wall\nTime)\nSQuADv1.1\n(F1)\nSQuADv2.0\n(F1)\nSST-2\n(Acc)\nMNLI\n(Acc)\nMRPC\n(Acc)\nCOLA\n(Mcc)\nQNLI\n(Acc)\nSTS-B\n(Acc)\nQQP\n(Acc)\nGLUE\nAvg.\nSQuAD\nAvg.\nScratch - - 89.05\n77.49 92.04 84.05 87.65 56.95 91.39 89.16 91.17 84.63 83.27\nTraining from the\nPretrained Model: BERT-Small→BERT-Base\nbert2BERT 35.6% 35.2%\n90.02 78.99 92.89 84.92 86.91 60.32 91.81 88.11 90.72 85.10 84.50\nLiGO 33.5% 33.2% 90.09 78.34 92.75 84.99 87.44 61.10 91.33 87.94 90.42 85.14 84.22\nProgressive T\nraining form Scratch\nStackBERT 29.5% 28.9%\n89.82 78.21 92.94 84.63 87.65 61.61 90.95 87.13 90.20 85.01 84.01\nApollo 41.6% 41.1% 89.87 78.42 92.28 84.81 87.06 60.57 91.43 88.27 90.69 85.02 84.15\nTable 4: Experiments on downstream tasks of BERT-Base on GLUE (Wang et al. 2019a), SQuADv1.1 (Rajpurkar et al. 2016),\nand SQuADv2.0 (Rajpurkar, Jia, and Liang 2018) dataset. The terms “Training from the Pretrained Model” and “Progressive\nTraining from Scratch” denote the pretraining type of the methods in the table. Compared with baselines, Apollo can achieve\nthe highest FLOPs saving under similar downstream performance, even better than training from the pretrained model.\n0 1 2\nFLOPs (1e19)\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0MLM Loss\n55.1% 0%\nScratch\nApollo\nFigure 8: Results of BERT-Large.\nbenchmark datasets, as illustrated in Table 4. For a larger\nBERT-Large in Fig. 8, Apollo also shows a significant ac-\nceleration ratio at 55.1%. The results clearly demonstrate the\nability of Apollo to exhibit proficient transfer learning capa-\nbilities, coupled with faster convergence rates, thus making\nit promising for practical applications.\nExperiment on GPT\nWe also implement the experiment to validate the perfor-\nmance of GPT. We train Apollo, StackBERT and Scratch\nfrom a random initialization, and bert2BERT and LiGO\nfrom GPT-Small. The training epoch is 35.\nAs shown in Fig. 6(c), we construct a comparison among\nthe Scratch model, StackBERT, bert2BERT, LiGO, and\nApollo. Notably, our proposed Apollo method exhibits a re-\nmarkable 47.9% acceleration ratio. Despite the structural\ndisparities between GPT and BERT, encompassing dis-\ntinct features such as the mask method, and the position\nof layer normalization, Apollo consistently maintains the\nhighest performance across the evaluated models. Specifi-\ncally, Apollo shows a significant advancement over Stack-\nBERT, achieving an improvement of +11.9% for the pre-\npared lessons before expanding. Furthermore, compared to\nbert2BERT and LiGO, Apollo attains a considerable acceler-\nation advantage with +6.8% and +9.9% higher performance,\nrespectively. These findings highlight the substantial accel-\neration capabilities of the Apollo method, affirming its ef-\nfectiveness for broader applications.\nConclusion\nTraining language models imposes a substantial demand on\ncomputational resources. Compared to training from pre-\ntrained models, progressive training from scratch offers a\nuniversal and flexible solution to accelerate training, as it\ndoes not require a pretrained model. However, previous pro-\ngressive methods suffered from inefficient layer expansion,\nleading to suboptimal training efficiency. In light of these\nchallenges, we propose Apollo, a novel approach that im-\nparts lessons for weights in the early stage, achieving a more\nnatural and efficient expansion. Additionally, we conduct a\nthorough analysis of the influence between stacking and in-\nterpolating methods for expanding model depth, advocating\nthe use of interpolation for improved stability in progres-\nsive training. Experimental results consistently demonstrate\nthat Apollo achieves remarkable acceleration across various\nlanguage models, even surpassing methods that rely on pre-\ntrained models. This implies the significant effectiveness of\nApollo in enhancing training efficiency. In the future, we ex-\npect that Apollo will contribute to the realization of green AI\nby significantly reducing the cost of training Transformers.\nAcknowledgments\nThis work was partially supported by the National Nat-\nural Science Foundation of China (No. 62276002), a\nkey program of fundamental research from Shenzhen\nScience and Technology Innovation Commission (No.\nJCYJ20200109113403826), the Major Key Project of PCL\n(No. 2022ZD0115301), and an Open Research Project of\nZhejiang Lab (NO.2022RC0AB04).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18866\nReferences\nBolya, D.; Fu, C.; Dai, X.; Zhang, P.; Feichtenhofer, C.; and\nHoffman, J. 2022. Token Merging: Your ViT But Faster.\nCoRR, abs/2210.09461.\nCao, H.; Wang, Y .; Chen, J.; Jiang, D.; Zhang, X.; Tian, Q.;\nand Wang, M. 2021. Swin-Unet: Unet-like Pure Transformer\nfor Medical Image Segmentation. CoRR, abs/2105.05537.\nChang, B.; Meng, L.; Haber, E.; Tung, F.; and Begert, D.\n2018. Multi-level Residual Networks from Dynamical Sys-\ntems View. In ICLR (Poster). OpenReview.net.\nChen, C.; Yin, Y .; Shang, L.; Jiang, X.; Qin, Y .; Wang, F.;\nWang, Z.; Chen, X.; Liu, Z.; and Liu, Q. 2022. bert2BERT:\nTowards Reusable Pretrained Language Models. InACL (1),\n2134–2148. Association for Computational Linguistics.\nChen, T.; Goodfellow, I. J.; and Shlens, J. 2016. Net2Net:\nAccelerating Learning via Knowledge Transfer. In ICLR.\nDehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and\nKaiser, L. 2019. Universal Transformers. In ICLR (Poster).\nOpenReview.net.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT (1), 4171–4186.\nAssociation for Computational Linguistics.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR. OpenReview.net.\nGong, L.; He, D.; Li, Z.; Qin, T.; Wang, L.; and Liu, T. 2019.\nEfficient Training of BERT by Progressively Stacking. In\nICML, volume 97 of Proceedings of Machine Learning Re-\nsearch, 2337–2346. PMLR.\nGu, X.; Liu, L.; Yu, H.; Li, J.; Chen, C.; and Han, J. 2021.\nOn the Transformer Growth for Progressive BERT Training.\nIn NAACL-HLT, 5174–5180. Association for Computational\nLinguistics.\nKamalakara, S. R.; Locatelli, A.; Venkitesh, B.; Ba, J.; Gal,\nY .; and Gomez, A. N. 2022. Exploring Low Rank Training\nof Deep Neural Networks. CoRR, abs/2209.13569.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In ICLR (Poster).\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;\nand Soricut, R. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations. In ICLR.\nOpenReview.net.\nLi, C.; Zhuang, B.; Wang, G.; Liang, X.; Chang, X.; and\nYang, Y . 2022. Automated Progressive Learning for Effi-\ncient Training of Vision Transformers. In CVPR, 12476–\n12486. IEEE.\nLi, N.; Pan, Y .; Chen, Y .; Ding, Z.; Zhao, D.; and Xu, Z.\n2020. Heuristic Rank Selection with Progressively Search-\ning Tensor Ring Network. CoRR, abs/2009.10580.\nPan, Y .; Xu, J.; Wang, M.; Ye, J.; Wang, F.; Bai, K.; and\nXu, Z. 2019. Compressing Recurrent Neural Networks with\nTensor Ring for Action Recognition. In AAAI, 4683–4690.\nAAAI Press.\nPan, Y .; Yuan, Y .; Yin, Y .; Xu, Z.; Shang, L.; Jiang, X.; and\nLiu, Q. 2023. Reusing Pretrained Models by Multi-linear\nOperators for Efficient Training. In NeurIPS.\nQin, Y .; Lin, Y .; Yi, J.; Zhang, J.; Han, X.; Zhang, Z.; Su, Y .;\nLiu, Z.; Li, P.; Sun, M.; and Zhou, J. 2022. Knowledge In-\nheritance for Pre-trained Language Models. InNAACL-HLT,\n3921–3937. Association for Computational Linguistics.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRajpurkar, P.; Jia, R.; and Liang, P. 2018. Know What You\nDon’t Know: Unanswerable Questions for SQuAD. In ACL\n(2), 784–789. Association for Computational Linguistics.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100, 000+ Questions for Machine Comprehension\nof Text. In EMNLP, 2383–2392. The Association for Com-\nputational Linguistics.\nRogers, A.; Kovaleva, O.; and Rumshisky, A. 2020. A\nPrimer in BERTology: What We Know About How BERT\nWorks. Trans. Assoc. Comput. Linguistics, 8: 842–866.\nSchwartz, R.; Dodge, J.; Smith, N. A.; and Etzioni, O. 2019.\nGreen AI. CoRR, abs/1907.10597.\nShen, S.; Walsh, P.; Keutzer, K.; Dodge, J.; Peters, M. E.;\nand Beltagy, I. 2022. Staged Training for Transformer Lan-\nguage Models. In ICML, volume 162 of Proceedings of Ma-\nchine Learning Research, 19893–19908. PMLR.\nShoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper,\nJ.; and Catanzaro, B. 2019. Megatron-LM: Training Multi-\nBillion Parameter Language Models Using Model Paral-\nlelism. CoRR, abs/1909.08053.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In NIPS, 5998–6008.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019a. GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understanding.\nIn ICLR (Poster). OpenReview.net.\nWang, D.; Li, M.; Wu, L.; Chandra, V .; and Liu, Q. 2019b.\nEnergy-Aware Neural Architecture Optimization with Fast\nSplitting Steepest Descent. CoRR, abs/1910.03103.\nWang, M.; Pan, Y .; Yang, X.; Li, G.; Xu, Z.; and Cichocki,\nA. 2023a. Tensor Networks Meet Neural Networks: A Sur-\nvey and Future Perspectives. CoRR, abs/2302.09019.\nWang, M.; Su, Z.; Luo, X.; Pan, Y .; Zheng, S.; and Xu, Z.\n2020. Concatenated Tensor Networks for Deep Multi-Task\nLearning. In ICONIP (5), volume 1333 ofCommunications\nin Computer and Information Science, 517–525. Springer.\nWang, P.; Panda, R.; Hennigen, L. T.; Greengard, P.; Kar-\nlinsky, L.; Feris, R.; Cox, D. D.; Wang, Z.; and Kim, Y .\n2023b. Learning to Grow Pretrained Models for Efficient\nTransformer Training. In ICLR. OpenReview.net.\nWu, L.; Liu, B.; Stone, P.; and Liu, Q. 2020a. Firefly Neu-\nral Architecture Descent: a General Approach for Growing\nNeural Networks. In NeurIPS.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18867\nWu, L.; Wang, D.; and Liu, Q. 2019. Splitting Steepest\nDescent for Growing Neural Architectures. In NeurIPS,\n10655–10665.\nWu, L.; Ye, M.; Lei, Q.; Lee, J. D.; and Liu, Q. 2020b.\nSteepest Descent Neural Architecture Optimization: Escap-\ning Local Optimum with Signed Neural Splitting. CoRR,\nabs/2003.10392.\nWu, Q.; Xing, C.; Li, Y .; Ke, G.; He, D.; and Liu, T. 2021.\nTaking Notes on the Fly Helps Language Pre-Training. In\nICLR. OpenReview.net.\nYang, C.; Wang, S.; Yang, C.; Li, Y .; He, R.; and Zhang,\nJ. 2020. Progressively Stacking 2.0: A Multi-stage Layer-\nwise Training Method for BERT Training Speedup. CoRR,\nabs/2011.13635.\nYang, S.; Hou, L.; Song, X.; Liu, Q.; and Zhou, D. 2021.\nSpeeding up Deep Model Training by Sharing Weights and\nThen Unsharing. CoRR, abs/2110.03848.\nZhang, M.; and He, Y . 2020. Accelerating Training\nof Transformer-Based Language Models with Progressive\nLayer Dropping. In NeurIPS.\nZhu, Y .; Kiros, R.; Zemel, R. S.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning Books and\nMovies: Towards Story-Like Visual Explanations by Watch-\ning Movies and Reading Books. In ICCV, 19–27. IEEE\nComputer Society.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18868",
  "topic": "Training (meteorology)",
  "concepts": [
    {
      "name": "Training (meteorology)",
      "score": 0.663123369216919
    },
    {
      "name": "Computer science",
      "score": 0.45353153347969055
    },
    {
      "name": "Psychology",
      "score": 0.35982975363731384
    },
    {
      "name": "Linguistics",
      "score": 0.3429068922996521
    },
    {
      "name": "Geography",
      "score": 0.12683901190757751
    },
    {
      "name": "Philosophy",
      "score": 0.06843730807304382
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ]
}