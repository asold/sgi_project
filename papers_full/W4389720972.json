{
  "title": "TREE-GPT: MODULAR LARGE LANGUAGE MODEL EXPERT SYSTEM FOR FOREST REMOTE SENSING IMAGE UNDERSTANDING AND INTERACTIVE ANALYSIS",
  "url": "https://openalex.org/W4389720972",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2277494045",
      "name": "S Q Du",
      "affiliations": [
        "Shenzhen University",
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality"
      ]
    },
    {
      "id": "https://openalex.org/A2136354482",
      "name": "S.-J. Tang",
      "affiliations": [
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality",
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2101428097",
      "name": "W. X. Wang",
      "affiliations": [
        "Shenzhen University",
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality"
      ]
    },
    {
      "id": "https://openalex.org/A2100419765",
      "name": "X M Li",
      "affiliations": [
        "Shenzhen University",
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality"
      ]
    },
    {
      "id": "https://openalex.org/A2432205109",
      "name": "R. Z. Guo",
      "affiliations": [
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality",
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2277494045",
      "name": "S Q Du",
      "affiliations": [
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality",
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2136354482",
      "name": "S.-J. Tang",
      "affiliations": [
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality",
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2101428097",
      "name": "W. X. Wang",
      "affiliations": [
        "Shenzhen University",
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality"
      ]
    },
    {
      "id": "https://openalex.org/A2100419765",
      "name": "X M Li",
      "affiliations": [
        "Shenzhen University",
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality"
      ]
    },
    {
      "id": "https://openalex.org/A2432205109",
      "name": "R. Z. Guo",
      "affiliations": [
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality",
        "Shenzhen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4323313924",
    "https://openalex.org/W4292828962",
    "https://openalex.org/W6730267373",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2953857783",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W6608025442",
    "https://openalex.org/W4283591366",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3012111773",
    "https://openalex.org/W2127593414",
    "https://openalex.org/W6851092083",
    "https://openalex.org/W4313898799",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W2086118991",
    "https://openalex.org/W2991424587",
    "https://openalex.org/W2028716078",
    "https://openalex.org/W2091782809",
    "https://openalex.org/W2040945936",
    "https://openalex.org/W2896206172",
    "https://openalex.org/W2912067133",
    "https://openalex.org/W3011477772",
    "https://openalex.org/W6602512215",
    "https://openalex.org/W4213059695",
    "https://openalex.org/W6600445788",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2952142982",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4362679702",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2992737141",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2962766617"
  ],
  "abstract": "Abstract. This paper introduces a novel framework, Tree-GPT, which incorporates Large Language Models (LLMs) into the forestry remote sensing data workflow, thereby enhancing the efficiency of data analysis. Currently, LLMs are unable to extract or comprehend information from images and may generate inaccurate text due to a lack of domain knowledge, limiting their use in forestry data analysis. To address this issue, we propose a modular LLM expert system, Tree-GPT, that integrates image understanding modules, domain knowledge bases, and toolchains. This empowers LLMs with the ability to comprehend images, acquire accurate knowledge, generate code, and perform data analysis in a local environment. Specifically, the image understanding module extracts structured information from forest remote sensing images by utilizing automatic or interactive generation of prompts to guide the Segment Anything Model (SAM) in generating and selecting optimal tree segmentation results. The system then calculates tree structural parameters based on these results and stores them in a database. Upon receiving a specific natural language instruction, the LLM generates code based on a thought chain to accomplish the analysis task. The code is then executed by an LLM agent in a local environment and . For ecological parameter calculations, the system retrieves the corresponding knowledge from the knowledge base and inputs it into the LLM to guide the generation of accurate code. We tested this system on several tasks, including Search, Visualization, and Machine Learning Analysis. The prototype system performed well, demonstrating the potential for dynamic usage of LLMs in forestry research and environmental sciences.",
  "full_text": "TREE-GPT: MODULAR LARGE LANGUAGE MODEL EXPERT SYSTEM FOR FOREST\nREMOTE SENSING IMAGE UNDERSTANDING AND INTERACTIVE ANALYSIS\nS. Q. Dua,b,c, S.J. Tanga,b,c ∗, W. X. Wanga,b,c, X. M. Lia,b,c, R. Z. Guoa,b,c\na Research Institute for Smart Cities, School of Architecture and Urban Planning, Shenzhen University, Shenzhen, P.R. China\nb State Key Laboratory of Subtropical Building and Urban Science, Shenzhen, P.R. China\nc Guangdong–Hong Kong-Macau Joint Laboratory for Smart Cities, P.R. China\nKEY WORDS:Remote Sensing, Deep Learning, Forestry, Large Language Model, Individual Tree Segmentation, Tree Factor\nEstimation, Segment Anything Model.\nABSTRACT:\nThis paper introduces a novel framework, Tree-GPT, which incorporates Large Language Models (LLMs) into the forestry remote\nsensing data workflow, thereby enhancing the efficiency of data analysis. Currently, LLMs are unable to extract or comprehend\ninformation from images and may generate inaccurate text due to a lack of domain knowledge, limiting their use in forestry\ndata analysis. To address this issue, we propose a modular LLM expert system, Tree-GPT, that integrates image understanding\nmodules, domain knowledge bases, and toolchains. This empowers LLMs with the ability to comprehend images, acquire accurate\nknowledge, generate code, and perform data analysis in a local environment. Specifically, the image understanding module extracts\nstructured information from forest remote sensing images by utilizing automatic or interactive generation of prompts to guide the\nSegment Anything Model (SAM) in generating and selecting optimal tree segmentation results. The system then calculates tree\nstructural parameters based on these results and stores them in a database. Upon receiving a specific natural language instruction, the\nLLM generates code based on a thought chain to accomplish the analysis task. The code is then executed by an LLM agent in a local\nenvironment and . For ecological parameter calculations, the system retrieves the corresponding knowledge from the knowledge\nbase and inputs it into the LLM to guide the generation of accurate code. We tested this system on several tasks, including Search,\nVisualization, and Machine Learning Analysis. The prototype system performed well, demonstrating the potential for dynamic\nusage of LLMs in forestry research and environmental sciences.\n1. INTRODUCTION\nAnalysis of forest remote sensing data is essential for various\napplications in forestry and environmental sciences(Turner et\nal., 2006, Suratno et al., 2009, Indirabai et al., 2019, Sun et\nal., 2019). The analysis of forest remote sensing data is often\nsubject to temporal constraints, necessitating the exploration of\nmore efficient method. Recent advancements in artificial in-\ntelligence have given rise to Large Language Models (LLMs),\nwhich are machine learning algorithms capable of comprehend-\ning human natural language instruction and generating coherent\ntext (Zhao et al., 2023). By utilizing natural language as an in-\nterface, researchers can employ LLMs to generate data analysis\ncode in a matter of minutes, a task that previously required sev-\neral tens of minutes to complete. Consequently, the integration\nof LLMs into the analysis of forest remote sensing data is be-\ning considered as a means of enhancing efficiency in a similar\nmanner.\nLMMs like GPT series have left a profound impression due to\ntheir powerful intelligence (Ouyang et al., 2022). Naturally,\nwe are curious about whether we can employ similar models to\nperform intelligent analysis of forest remote sensing data. How-\never, the limited ability of LLMs to comprehend images and\ngenerate accurate domain-specific knowledge hinders their ap-\nplication in forest remote sensing data analysis. Vision serves as\nthe primary source of information for human cognition. How-\never, the majority of language models struggle to comprehend\nvisual information, limiting their applicability. In recent years,\nresearchers have investigated methods to enable LLMs to pro-\n∗ Corresponding author\ncess multimodal information. Among these studies, GPT-4 has\nemerged as the leading model, exhibiting remarkable perform-\nance in understanding and accurately describing natural im-\nages (OpenAI, 2023). Additionally, open-source models such\nas MiniGPT4 have proposed their own solutions (Zhu et al.,\n2023). However, these approaches exhibit suboptimal perform-\nance on remote sensing imagery due to the fact that the majority\nof these models are trained on natural images. To address this\nlimitation, researchers have proposed multimodal remote sens-\ning datasets and trained models that capable of understanding\nremote sensing images and performing natural language-based\nquestion answering (Lobry et al., 2020, Chappuis et al., 2022).\nThese models can understand natural language and provide an-\nswers regarding the presence of specific concepts in the im-\nages. However, due to their complexity, these model are still\nnot capable of precise quantitative analysis or executing data\nanalysis tasks through code generation. Furthermore, most lan-\nguage models exhibit limited performance when generating text\nin specific professional domains due to a lack of fine-tuning\nfor those domains. A phenomenon known as “hallucination” is\nalso commonly observed in language models, where they gen-\nerate information that does not exist in reality, such as providing\nincorrect interpretations of natural laws (Ji et al., 2023). Con-\nsequently, the lack of remote sensing image understanding abil-\nity and accurate professional knowledge are significant issues\nthat restrict the application of general language models in spe-\ncialized domains.\nTo address the first issue and enable LLMs to understand forest\nremote sensing images, we are considering the integration of\na separate image understanding module. This module is built\nto convert the implicit information contained in the image into\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1729\nexplicit, structured information that LLMs can understand. For\nforest remote sensing data analysis, it typically involves two\nprimary tasks: the individual tree crown (ITC) segmentation\nand calculation of tree structural parameters. Among them,\nthe ITC segmentation remains a challenging problem. Current\nresearch has explored various methods based on point clouds\nor images for ITC segmentation. Among them, image-based\nsegmentation methods tend to employ supervised learning al-\ngorithms like Mask R-CNN for accurate instance segmenta-\ntion(Beloiu et al., 2023). These methods require a large num-\nber of high-precision training samples. To reduce the cost of\nour approach, we are considering whether unsupervised or in-\nteractive methods can be used for ITC segmentation to generate\nstructured information that LLMs can comprehend.\nRecently, image representation learning (Vaswani et al., 2017,\nDosovitskiy et al., 2020) and self-supervised learning (He et al.,\n2022) have advanced significantly, along with the availability\nof large-scale data and computational resources (Kirillov et al.,\n2023). These developments have enabled the learning of uni-\nversal features that can be transferred across different domains\nfrom massive image collections. In this context, a Large Vision\nModel named Segment Anything Model (SAM) was proposed\n(Kirillov et al., 2023), which can perform zero-shot image seg-\nmentation guided by prompts. However, SAM still faces two\nchallenges in individual tree segmentation task: 1) the segment-\nation quality of SAM is highly sensitive to the input prompt, and\nthe segmentation granularity is not controllable without prior\nknowledge. 2) SAM generates many redundant masks in its\nsegmentation output, which hinders the identification of indi-\nvidual trees. This paper aims to overcome these challenges\nand leverage the potential of SAM for single-tree segmentation\ntasks by proposing a framework that does not require retraining\nand can be easily adapted to new scenarios.\nTo address the second issue of empowering LLMs with domain-\nspecific knowledge and task execution capabilities, we are con-\nsidering building a specialized knowledge base and an execu-\ntion agent for LLMs. The specialized knowledge base is a\nwidely used solution to tackle the issue of hallucination in LLMs.\nMeanwhile, the execution agent acts as a framework that con-\nnects LLMs with the local execution environment. LLMs can\nutilize the execution agent to execute generated code in the local\ndevelopment environment, perform data manipulation, and gen-\nerate visualization or data analysis results. Currently, there are\nexisting frameworks for knowledge base construction and task\nexecution for LLMs. However, designing the optimal frame-\nwork is still an open question. For the specific task of forest\nremote sensing data analysis, it is worth discussing how to ad-\napt the existing frameworks, such as Prompt Engineering, to\nsuit the characteristics of this particular task.\nFollowing above ideas, we propose a novel framework, Tree-\nGPT. The purpose of Tree-GPT is to build an expert system cap-\nable of understanding forest remote sensing images, possessing\ndomain-specific knowledge in forest ecology, and generating\ncode based on prompts to automatically execute data analysis\ntasks. The core of this system consists of an LLM as the reas-\noning engine, an image understanding module to convert image\ninformation into text, a Domain Knowledge Base to store pro-\nfessional knowledge for LLM retrieval, and an LLM execution\nagent for code execution. More specifically:\n• LLM: The LLM and its accompanying backend execution\nframework are pivotal components of Tree-GPT. In terms\nof model selection, the study utilizes the cloud-deployed\nOpenAI GPT-4 model, known for its high performance.\nHowever, in theory, an LLM deployed locally would also\nbe suitable.\n• Image Understanding:As the second key module of Tree-\nGPT, the image understanding module encompasses two\nfunctions: tree segmentation and calculation of tree struc-\ntural parameters. To enable low-cost algorithm transfer,\nthe study employs the Structure-Aware Masking (SAM)\nas the core and designs a new tree segmentation method to\naddress the challenges associated with SAM. This method\nstarts by employing a pre-trained tree detection model to\nlocate and outline trees (or uses an interactive approach) to\ngenerate an initial prompt. Subsequently, the study inputs\nthe initial prompt and a grid prompt into SAM, resulting\nin redundant tree segmentation outcomes. To refine the\nsegmentation results, a bipartite graph matching model is\nutilized, leveraging the initial prompt as a guide. After ob-\ntaining the segmentation results, additional remote sensing\ndata is incorporated to compute tree structural parameters,\nwhich are stored in a relational database for subsequent\ndata analysis access.\n• Domain Knowledge Base: As the third key module of\nTree-GPT, the Domain Knowledge Base is a vector data-\nbase that stores forest ecology domain-specific knowledge\nin embedded text form. This knowledge base allows for\nretrieval of crucial information through keyword searches\nand utilizes it as prompts to guide the LLM in generating\naccurate professional knowledge outputs.\n• LLM Execution Agent: Lastly, as the execution mod-\nule of Tree-GPT, the LLM execution agent serves as a\nbackend framework that connects the LLM’s code outputs\nwith the local runtime environment. The study has tailored\nthe Prompt Engineering framework to address the specific\ntasks of forest remote sensing data analysis, including in-\nformation retrieval, result visualization, and data analysis.\nTask-oriented prompt design ensures the generation of pre-\ncise code, facilitating efficient and effective data analysis.\nIn conclusion, Tree-GPT offers a user-friendly approach that\nenables real-time data updates and analysis. This reduces the\ntime required for the processing workflow and allows research-\ners to allocate more resources towards data interpretation and\nthe understanding of ecological patterns. With its understand-\ning and generation capabilities provided by Large Language\nModels, Tree-GPT offers a pertinent contribution to the field\nof tree ecological parameter extraction.\n2. RELATED WORKS\n2.1 Individual Tree Crown (ITC) Segmentation\nEfficient and automated ITC segmentation methods are crucial\nfor calculating tree factors (Yang et al., 2019). Existing meth-\nods for ITC segmentation can be classified based on the type\nof data used, including point cloud based methods and image\nbased method. Current UA V point cloud based methods often\nutilize unsupervised clustering techniques. Since the highest\npoint of a tree (which can be regarded as the tree’s center)\nis easily identifiable in the point cloud, these methods typic-\nally determine the tree center by leveraging the elevation dif-\nferences. They then perform point cloud clustering using the\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1730\nPrompt\nStructure Factors Estimation\nMatching\nImage\n PointCloud\nHeight\nArea\nLocation\nCrown \nwidth\nBbox\nMask\nGeo-Database Tree Factor Database Text\nEmbedding\nChatGPT or GPT-4\nSimilarity Search & Prompt Input\nDevelop\nEnvironment\nText\nGenerate\nWebUI: for Interaction and Visualization\nVisualization\nResults\nText\nEmbedding\nChatGPT or GPT-4\nSimilarity Search & Prompt Input\nDevelop\nEnvironment\nText\nGenerate\nWebUI: for Interaction and Visualization\nVisualization\nResults\n2D Masks 3D Masks\nSAM\nVit-H Masks\nLLM & Execution AgentImage Understanding Module\nDeepForest\nInteraction\nOr\nBbox PromptImage\nPrompt Generation\nForestry Knowledge\nChroma Vector Database\nOpenAI Embedding & Spliting\nDomain Knowledge Base\nAgent\nFigure 1. Overall Framework of the Tree-GPT.\ncenter as a seed point to segment the tree contour (Strˆımbu and\nStrˆımbu, 2015). However, accurately describing tree bound-\naries becomes challenging due to the lack of texture informa-\ntion in point clouds. As a result, most methods rely on strong\nprior assumptions such as the conical shape assumption to dif-\nferentiate between points at the boundaries of trees (Str ˆımbu\nand Strˆımbu, 2015, Yang et al., 2020, Qian et al., 2023). These\nmethods generally achieve acceptable results in most scenarios.\nHowever, in subtropical or tropical regions, trees tend to have\nbroader and more expansive shapes with large crown areas (Lutz\net al., 2012). As a result, many trees’ growth patterns deviate\nfrom the aforementioned prior assumptions, which limits the\ngeneralization capability of these methods when applied to new\nscenarios, necessitating repeated parameter tuning.\nBased on image data, tree segmentation methods primarily util-\nize the color and texture differences of different trees in RGB\nimages or the tree height differences in the canopy height model\n(CHM) to segment individual trees. Some studies have em-\nployed traditional morphological image segmentation methods,\nsuch as watershed algorithms and their variants, to achieve tree\nsegmentation in RGB or CHM images (Tochon et al., 2015,\nWagner et al., 2018, Yang et al., 2019). However, the lim-\nited utilization of morphological image features hinders the ac-\ncuracy of these segmentation methods. In recent years, re-\nsearch has focused on deep learning-based single-tree segment-\nation methods due to the advancement of deep learning tech-\nniques. Compared to traditional image segmentation methods,\ndeep learning networks possess stronger feature extraction cap-\nabilities and can learn features with certain generalization prop-\nerties, thus enhancing the generalization of the methods (LeCun\net al., 2015). Currently, most deep learning-based single-tree\nsegmentation methods are based on the Mask-RCNN network\n(He et al., 2017, Beloiu et al., 2023). Some methods have im-\nproved edge extraction by enhancing the loss function (Zhang\net al., 2022), while others have considered incorporating multi-\nmodal feature extraction techniques within the Mask-RCNN\nframework. They simultaneously use texture, color, and el-\nevation information from both RGB and CHM images to im-\nprove tree segmentation accuracy (Li et al., 2022). Although\ndeep learning-based instance segmentation methods for single-\ntree segmentation show significant improvements over tradi-\ntional image segmentation methods in terms of robustness and\ngeneralization, most supervised deep learning methods suffer\nfrom a fundamental limitation: their performance is heavily\nconstrained by the training data. Since deep learning is es-\nsentially about finding a set of factors that best fit the distri-\nbution of the training samples, the performance of the network\nis determined by the extent to which the probability distribution\nof the training samples aligns with the real-world distribution\n(Goodfellow et al., 2016). For single-tree segmentation tasks,\nhigh-precision open-source tree contour datasets are severely\nlacking. Moreover, distinguishing and annotating tree contours\nis extremely costly compared to natural images, making it dif-\nficult to rapidly annotate a large amount of data to train a tree\nsegmentation network with strong generalization and zero-shot\ntransfer capability.\n3. METHOD\nTree-GPT is a specialized system designed to process remote\nsensing data from forests. It comprises a GPT-4-based LLM, an\nimage understanding module, a Domain Knowledge Base, and\nan LLM execution agent. The Tree-GPT workflow operates as\nfollows: given a natural language instruction L and a remote\nsensing image I of the study area, the image understanding\nmodule processes I and collaborates with other remote sensing\ndata from the study area to generate tree structure parameters\nthat can be used as inputs for the LLM and for data analysis.\nThese parameters are stored in a relational database R. Sim-\nultaneously, the professional domain knowledge base module\nassesses L to determine if it is a knowledge-based question and\nwhether to invoke the professional knowledge base module to\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1731\nanswer it. If L is not a knowledge-based question, it proceeds\nto the LLM execution agent module.\nIn the LLM execution module, if L does not involve code ex-\necution tasks, the output result of the LLM is directly returned\nin the FastChat frontend. If the result contains code execution\ntasks, the corresponding code is generated based on prompt\ntemplates and executed using the LangChain agent module in\na local runtime environment. During execution, data visualiz-\nation or analysis results are generated by calling image I and\ndatabase R.\n3.1 Image Understanding Module\nMask Generation:The Tree-GPT model utilizes a tree target\ndetection framework and a bipartite graph matching method to\ncontrol the segmentation granularity of SAM. This generates\ntree segmentation results that correspond one-to-one with the\ntree position. Specifically, the single-tree segmentation mod-\nule uses points and target frames as SAM’s Prompt at the same\ntime to guide the generation of tree Masks. Among them, the\ntarget detection module can use automatic or manual methods\nto generate tree target detection frames, which are used to guide\nSAM for tree segmentation and assign semantic information\nto the segmentation results. The automatic target frame gen-\neration method uses RetinaNet (Lin et al., 2017) as the target\ndetection framework with ResNet50 (He et al., 2016) as the\nbackbone network and DeepForest (Weinstein et al., 2019) pre-\ntrained weights. Manual annotation of tree target frames can be\nused as a supplement to automatic tree target detection to im-\nprove the accuracy of tree detection in an interactive manner.\nIn addition to the target frame, we use a 48 × 48 grid sampling\npoint as the basic Prompt for SAM to deal with situations where\nBBox Prompt guidance fails. This generates enough Masks for\nsubsequent matching with the target frame and obtains optimal\nsegmentation results of the tree. After obtaining the Prompt, we\ninput Point and BBox Prompt into SAM respectively and merge\nthe two sets of tree segmentation results to obtain redundant tree\nsegmentation results.\nBest Mask Matching:After obtaining the redundant tree seg-\nmentation results, we consider how to select the optimal tree\nsegmentation results that correspond one-to-one with the real\ntrees. Since SAM’s segmentation results do not contain se-\nmantic information, we consider using the automatically or manu-\nally obtained tree target detection results as the real position of\nthe tree. We match the redundant Mask with the target detection\nresults to obtain the tree segmentation results corresponding to\nthe tree position. Specifically, we model this matching problem\nas a bipartite graph matching problem in mathematics. Bipartite\ngraph matching, also known as bipartite matching, is a model\nin graph theory where given two disjoint sets of vertices A and\nB, and a set of edges E ⊆ A × B, a matching M is a subset\nof E such that no two edges share an endpoint. A maximum\nmatching is a matching M of maximum cardinality (maximum\nnumber of edges). This problem can be defined as:\nmin:\nX\ni∈A\nX\nj∈B\ncijxij (1)\ns.t.\nX\ni∈A\nxij = 1, for all j ∈ B\nX\nj∈B\nxij = 1, for all i ∈ A\nxij ∈ {0, 1}, for all (i, j) ∈ E\n(2)\nWhere the matchings are represented by variables xij for each\nedge E(i, j). cij is the cost of matching vertex i to vertex j in\nthe bipartite graph. The constraints ensure that each vertex is\nmatched exactly once.\nIn short, the bipartite graph matching problem is used to solve\na set of elements in set B that correspond one-to-one with ele-\nments in set A. This correspondence is called maximum match-\ning. When solving bipartite graph matching, it is necessary\nto define the cost of adding each edge, that is, the Cost mat-\nrix. In this article, we aim to maximize the overlap between\nthe segmentation result and our detected target frame. We can\nuse GIoU (Rezatofighi et al., 2019) to characterize this degree\nof overlap. The GIoU is defined as IoU minus the area of the\nsmallest enclosing box that covers both bounding boxes P and\nG, excluding the union area of P and G and can be expressed as\nfollows:\nIoU = Area of Intersection\nArea of Union (3)\nGIoU = IoU − Ac − Au\nAc\n(4)\nAfter obtaining the Cost matrix, we use the classic Hungarian\nalgorithm to solve bipartite graph matching and obtain the op-\ntimal segmentation result with the highest overlap with tree po-\nsition.\nTree Structure Factor Estimation:After obtaining the tree\nsegmentation results, we use the 2D segmentation results to as-\nsign labels to the point cloud within the corresponding range.\nThis allows us to obtain the 3D single-tree segmentation res-\nults. After obtaining the 3D segmentation, we calculate the\ntree height, crown width, support height and crown area. We\nstore them in the tree database together with the 2D segmenta-\ntion results, external target frame and tree position information.\nAmong them, the tree position is stored in the database in pixel\ncoordinates. The 2D tree contour is stored in the database in\nMS COCO’s (Lin et al., 2014) compressed storage format.\n3.2 Domain Knowledge Base\nThe Domain Knowledge Base is a vector database that stores\nembedded text of ecological knowledge in the field of forestry.\nThe module first uses the OpenAIEmbedding API to embed the\ntext of ecological knowledge in the forestry field and convert it\ninto numerical vectors. The text is then segmented using the\nLangChain framework’s text segmentation tool with a block\nsize of 4,000 tokens, and the resulting knowledge vectors are\nsplit into several blocks and stored in the Chroma vector data-\nbase. When Tree-GPT receives a natural language instruction,\nit is first embedded using the OpenAIEmbedding API and con-\nverted into a numerical vector. The module then uses the Face-\nbook AI Similarity Search (FAISS) algorithm to retrieve relev-\nant information from the Chroma database, obtaining retrieval\nresults denoted as SS. These results are ranked, and text blocks\nwith a similarity greater than 0.6 are selected. These selected\ntext blocks are then used as new inputs to the LLM using the\nprompt template ’Given context, could you please explain the\nmeaning of query?’ to obtain the final inference result. This\nallows the model to accurately answer knowledge-based ques-\ntions in the professional domain.\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1732\nArea-1 2D Masks Area-2 2D Masks Area-3 2D Masks\nArea-1 3D Masks Area-3 3D Masks\nArea-2 3D Masks\nFigure 2. Visualization of individual tree crown(ITC) segmentation results. The first line shows tree crown segmentation results\n(segmentation in 2D). The second line shows individual tree point cloud segmentation results (3D segmentation by projecting 2D\nresults to point cloud).\n3.3 LLM Execution Agent\nThe LLM Execution Agent is a backend framework in Tree-\nGPT that bridges the LLM’s generated code structure with the\nlocal runtime environment. Built on the LangChain toolchain,\nthe LLM Execution Agent executes instructions in two steps:\nTask Planning and Code Execution. In Task Planning, the LLM\ndecomposes natural language instructions into subtasks using\nprompt templates and generates corresponding content for each\nsubtask. By breaking down tasks, the LLM is more likely to\ngenerate accurate code, as the subtasks are simpler. TreeGPT\nemploys task planning templates, as shown in the Table 1.\nPrompt\nTemplete Input Output\n1. Thought Natural Language\nInstruction\nTask\nDecomposition\n2. Action Task\nDecomposition\nType of\nSub-Tasks\n3. Action Input Sub-Tasks Code\n4. Observation Code Running\nResults\n5. Thought Running Results +\nInstruction Output Text\n6. Final Result -\nOutput Text+\nRunning Results\n(Visualization, etc.)\nTable 1. Process of LLM Execution Agent.\n4. EXPERIMENT RESULTS\nWe use data collected from Shenzhen, Guangdong Province,\nChina to verify the effectiveness of our method. The data was\ncollected in 2018, with an RGB orthoimage spatial resolution\nof 0.025m and a Lidar resolution of 100 points per m2. In the\nexperiment, we cropped three 3000×3000 pixel areas to verify\nthe effectiveness of the method. In terms of language model\nselection, we use the ChatGPT API to access the database to\nmaximize the accuracy of code generation.\nOur experiments on Tree-GPT revolve around two tasks: tree\nsegmentation and data visualization and analysis based on nat-\nural language. The results are presented in Figure 2 and 3. As\nshown in Figure 2, in the tree segmentation task, Tree-GPT’s\nsegmentation results are basically consistent with manual an-\nnotations. It can even be said that in some cases the manual an-\nnotation results may not be as accurate as SAM’s results. In ad-\ndition, based on natural language-guided tree parameter query,\nvisualization and analysis results are presented in Figures 3 re-\nspectively. The operating results of Tree-GPT are demonstrated\nin various tasks, including simple visualization (directly gen-\nerating scatter plots), information retrieval (finding the tallest\ntree), complex visualization (drawing a tree growth diagram\nbased on the position and crown width of the tree and gener-\nating a box plot of trees grouped by height), code generation,\nand statistical learning-based analysis and testing of tree ecolo-\ngical parameters (estimating the Gaussian distribution paramet-\ners of tree height and using RMSE as a confidence measure).\nAs shown in the figure3, during the query, Tree-GPT can give\ncorrect query results and visualize them in the figure. In the data\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1733\nPlease plot a scatter plot with tree \nheight on the x-axis and crown width \non the y-axis.\nNo problem! This is a scatter plot with \ntree height as the x-axis and tree \ncanopy width as the y-axis.\nPlease draw an illustration of tree \ngrowth using the center point of each \ntree as the circle center and half of the \ntree crown width as the radius. Please \nuse red, green, and blue colors to \ndifferentiate the neighboring trees.\nYes, of course! Here is an illustration \nof tree growth, using the center point \nof each tree as the circle center and \nhalf of the tree crown width as the \nradius:\nPlease group the data based on tree \nheight in the following ranges: \n(0<h<=4m), (4<h<=7m), (7<h<=9m), \n(9<h<=11m), (11<0=<13m), \n(h>=13m). Then, plot a boxplot for \neach group and present all the \nboxplots in one plot.\nThis is a box plot that has been \ncreated by grouping the data \naccording to your specifications:\nPlease help me fit a Gaussian \ndistribution to the tree height in the \nInput image and visualize the \ndistribution curve overlaid on the \nscatter plot of the data, indicating the \nparameters and confidence intervals \nof the Gaussian distribution.\nThis is an example result where I have \nused numpy, scipy, and matplotlib to \nfit and visualize a Gaussian \ndistribution, and labeled parameters.\nPlease help me find the tallest tree in \nthis image and mark it in the image, \nindicating its height next to it.\nThis is the visualization result of the \ntallest tree and its location queried \nfrom the data.\nPlease provide the code you used to \ngenerate this plot.\nThis code generates box plots for \neach group of data by classifying the \ninput data based on given grouping \nranges:\n# Define grouping ranges\ngroup_ranges = [(0, 4), (4, 7), (7, 9), \n(9, 11), (11, 13), (13, np.inf)]\n# Initialize data lists for each group\ngroup_data = [[] for _ in range(len(group_ranges))]\n# Group the data\nfor value, h in zip(crow, height):\n    for i, (lower, upper) in enumerate(group_ranges):\n        if lower <= h < upper:\n            group_data[i].append(value)\n            break\n# Generate box plot\ncolor = ['steelblue']\nbp = plt.boxplot(group_data, patch_artist=True)\n# Set colors for each box plot\nfor patch in bp['boxes']:\n    patch.set_facecolor(color)\nfor median in bp['medians']:\n    median.set(color='black')\n# Add x-axis labels\nlabels = [f\"{lower}-{upper}\" for (lower, upper) in group_ranges]\nplt.xticks(range(1, len(group_ranges) + 1), labels)\n# Add y-axis label\nplt.ylabel('Tree Canopy Width')\nplt.xlabel('Height')\nplt.savefig('box.png', bbox_inches='tight')\n# Show plot\nplt.show()\nFigure 3. The results of Tree-GPT’s operation.\nvisualization task, Tree-GPT can give basically correct results\nin the first round. However, it may require multiple manual\nguidance to achieve the best visualization effect. In the data\nanalysis task, Tree-GPT can implement simple machine learn-\ning code for various tasks.\n5. CONCLUSION\nIn conclusion, Tree-GPT offers a user-friendly approach that\nenables real-time data updates and analysis, thereby reducing\nthe time required for the processing workflow and allowing re-\nsearchers to allocate more resources towards data interpretation\nand the understanding of ecological patterns. With its under-\nstanding and generation capabilities provided by Large Lan-\nguage Models, Tree-GPT offers a pertinent contribution to the\nfield of tree ecological parameter extraction.\nACKNOWLEDGEMENTS\nThis work was supported in part by the National Key Research\nand Development Program of China (Project No. 2022 YFB\n3903700), a Shenzhen Science and Technology Program (Pro-\nject No. JCYJ 20210324093012033), the Natural Science Found-\nation of Guangdong Province (Project No. 2121A1515012574),\nthe National Natural Science Foundation of China (Project Nos.\n71901147, 41901329, 41971354, and 41971341,42001331), Shen-\nzhen Key Laboratory of Digital Twin Technologies for Cities\n(Project No: ZDSYS20210623101800001).\nREFERENCES\nBeloiu, M., Heinzmann, L., Rehush, N., Gessler, A., Griess,\nV . C., 2023. Individual Tree-Crown Detection and Species Iden-\ntification in Heterogeneous Forests Using Aerial RGB Imagery\nand Deep Learning. Remote Sensing, 15(5), 1463.\nChappuis, C., Zermatten, V ., Lobry, S., Le Saux, B., Tuia, D.,\n2022. Prompt-rsvqa: Prompting visual context to a language\nmodel for remote sensing visual question answering. Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 1372–1381.\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1734\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D.,\nZhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold,\nG., Gelly, S. et al., 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929.\nGoodfellow, I., Bengio, Y ., Courville, A., 2016.Deep learning.\nMIT press.\nHe, K., Chen, X., Xie, S., Li, Y ., Doll´ar, P., Girshick, R., 2022.\nMasked autoencoders are scalable vision learners. Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 16000–16009.\nHe, K., Gkioxari, G., Doll´ar, P., Girshick, R., 2017. Mask r-cnn.\nProceedings of the IEEE international conference on computer\nvision, 2961–2969.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learn-\ning for image recognition. Proceedings of the IEEE conference\non computer vision and pattern recognition, 770–778.\nIndirabai, I., Nair, M. H., Nair, J. R., Nidamanuri, R. R., 2019.\nEstimation of forest structural attributes using ICESat/GLAS-\nspaceborne laser altimetry data in the western ghats region of\nIndia. Journal of Geovisualization and Spatial Analysis, 3, 1–\n13.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E., Bang,\nY . J., Madotto, A., Fung, P., 2023. Survey of hallucination in\nnatural language generation. ACM Computing Surveys, 55(12),\n1–38.\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gust-\nafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y . et al.,\n2023. Segment anything. arXiv preprint arXiv:2304.02643.\nLeCun, Y ., Bengio, Y ., Hinton, G., 2015. Deep learning.nature,\n521(7553), 436–444.\nLi, Y ., Chai, G., Wang, Y ., Lei, L., Zhang, X., 2022. Ace r-cnn:\nAn attention complementary and edge detection-based instance\nsegmentation algorithm for individual tree species identification\nusing uav rgb images and lidar data. Remote Sensing, 14(13),\n3035.\nLin, T.-Y ., Goyal, P., Girshick, R., He, K., Doll ´ar, P., 2017.\nFocal loss for dense object detection. Proceedings of the IEEE\ninternational conference on computer vision, 2980–2988.\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., Doll ´ar, P., Zitnick, C. L., 2014. Microsoft coco:\nCommon objects in context. Computer Vision–ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-\n12, 2014, Proceedings, Part V 13, Springer, 740–755.\nLobry, S., Marcos, D., Murray, J., Tuia, D., 2020. RSVQA:\nVisual question answering for remote sensing data.IEEE Trans-\nactions on Geoscience and Remote Sensing, 58(12), 8555–\n8566.\nLutz, J. A., Larson, A. J., Swanson, M. E., Freund, J. A.,\n2012. Ecological Importance of Large-Diameter Trees in a\nTemperate Mixed-Conifer Forest. PLOS ONE, 7(5), 1-15. ht-\ntps://doi.org/10.1371/journal.pone.0036131.\nOpenAI, 2023. Gpt-4 technical report.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. et al.,\n2022. Training language models to follow instructions with hu-\nman feedback. Advances in Neural Information Processing Sys-\ntems, 35, 27730–27744.\nQian, C., Yao, C., Ma, H., Xu, J., Wang, J., 2023. Tree species\nclassification using airborne lidar data based on individual tree\nsegmentation and shape fitting. Remote Sensing, 15(2), 406.\nRezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I.,\nSavarese, S., 2019. Generalized intersection over union: A met-\nric and a loss for bounding box regression. Proceedings of the\nIEEE/CVF conference on computer vision and pattern recogni-\ntion, 658–666.\nStrˆımbu, V . F., Strˆımbu, B. M., 2015. A graph-based segmenta-\ntion algorithm for tree crown extraction using airborne LiDAR\ndata. ISPRS Journal of Photogrammetry and Remote Sensing,\n104, 30–43.\nSun, Y ., Xin, Q., Huang, J., Huang, B., Zhang, H., 2019. Char-\nacterizing tree species of a tropical wetland in southern china at\nthe individual tree level based on convolutional neural network.\nIEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing, 12(11), 4415–4425.\nSuratno, A., Seielstad, C., Queen, L., 2009. Tree species iden-\ntification in mixed coniferous forest using airborne laser scan-\nning. ISPRS Journal of Photogrammetry and Remote Sensing,\n64(6), 683–693.\nTochon, G., Feret, J.-B., Valero, S., Martin, R. E., Knapp, D. E.,\nSalembier, P., Chanussot, J., Asner, G. P., 2015. On the use of\nbinary partition trees for the tree crown segmentation of trop-\nical rainforest hyperspectral images. Remote sensing of envir-\nonment, 159, 318–331.\nTurner, J. A., Buongiorno, J., Zhu, S., 2006. An economic\nmodel of international wood supply, forest stock and forest area\nchange. Scandinavian Journal of Forest Research, 21(1), 73–\n86.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all\nyou need. Advances in neural information processing systems,\n30.\nWagner, F. H., Ferreira, M. P., Sanchez, A., Hirye, M. C.,\nZortea, M., Gloor, E., Phillips, O. L., de Souza Filho, C. R.,\nShimabukuro, Y . E., Arag˜ao, L. E., 2018. Individual tree crown\ndelineation in a highly diverse tropical forest using very high\nresolution satellite images. ISPRS journal of photogrammetry\nand remote sensing, 145, 362–377.\nWeinstein, B. G., Marconi, S., Bohlman, S., Zare, A., White, E.,\n2019. Individual tree-crown detection in RGB imagery using\nsemi-supervised deep learning neural networks. Remote Sens-\ning, 11(11), 1309.\nYang, J., Kang, Z., Cheng, S., Yang, Z., Akwensi, P. H., 2020.\nAn individual tree segmentation method based on watershed\nalgorithm and three-dimensional spatial distribution analysis\nfrom airborne LiDAR point clouds. IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing, 13,\n1055–1067.\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1735\nYang, Q., Su, Y ., Jin, S., Kelly, M., Hu, T., Ma, Q., Li, Y .,\nSong, S., Zhang, J., Xu, G. et al., 2019. The influence of veget-\nation characteristics on individual tree segmentation methods\nwith airborne LiDAR data. Remote Sensing, 11(23), 2880.\nZhang, C., Zhou, J., Wang, H., Tan, T., Cui, M., Huang, Z.,\nWang, P., Zhang, L., 2022. Multi-species individual tree seg-\nmentation and identification based on improved mask R-CNN\nand UA V imagery in mixed forests.Remote Sensing, 14(4), 874.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y ., Min,\nY ., Zhang, B., Zhang, J., Dong, Z. et al., 2023. A survey of large\nlanguage models. arXiv preprint arXiv:2303.18223.\nZhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.,\n2023. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models. arXiv preprint\narXiv:2304.10592.\nThe International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLVIII-1/W2-2023 \nISPRS Geospatial Week 2023, 2–7 September 2023, Cairo, Egypt\nThis contribution has been peer-reviewed. \nhttps://doi.org/10.5194/isprs-archives-XLVIII-1-W2-2023-1729-2023 | © Author(s) 2023. CC BY 4.0 License.\n \n1736",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8043261170387268
    },
    {
      "name": "Tree (set theory)",
      "score": 0.6333125829696655
    },
    {
      "name": "Modular design",
      "score": 0.5582209229469299
    },
    {
      "name": "Workflow",
      "score": 0.5201342701911926
    },
    {
      "name": "Code (set theory)",
      "score": 0.47926566004753113
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.43964749574661255
    },
    {
      "name": "Segmentation",
      "score": 0.41892439126968384
    },
    {
      "name": "Knowledge base",
      "score": 0.418545126914978
    },
    {
      "name": "Domain knowledge",
      "score": 0.41164594888687134
    },
    {
      "name": "Data science",
      "score": 0.40538299083709717
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3861556351184845
    },
    {
      "name": "Information retrieval",
      "score": 0.36467480659484863
    },
    {
      "name": "Data mining",
      "score": 0.3634946346282959
    },
    {
      "name": "Machine learning",
      "score": 0.35653483867645264
    },
    {
      "name": "Database",
      "score": 0.2719775438308716
    },
    {
      "name": "Programming language",
      "score": 0.14491727948188782
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}