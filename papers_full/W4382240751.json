{
    "title": "OctFormer: Efficient Octree-Based Transformer for Point Cloud Compression with Local Enhancement",
    "url": "https://openalex.org/W4382240751",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2112680921",
            "name": "Mingyue Cui",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2888902266",
            "name": "Junhua Long",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2683994054",
            "name": "Mingjian Feng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2124185167",
            "name": "Boyang Li",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2103501441",
            "name": "Huang Kai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112680921",
            "name": "Mingyue Cui",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2888902266",
            "name": "Junhua Long",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2683994054",
            "name": "Mingjian Feng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2124185167",
            "name": "Boyang Li",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2103501441",
            "name": "Huang Kai",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2965803762",
        "https://openalex.org/W2926206832",
        "https://openalex.org/W6635487051",
        "https://openalex.org/W2594519801",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2891062133",
        "https://openalex.org/W3049297040",
        "https://openalex.org/W4221156617",
        "https://openalex.org/W2966379582",
        "https://openalex.org/W3014200484",
        "https://openalex.org/W3114151084",
        "https://openalex.org/W2187463565",
        "https://openalex.org/W4287231913",
        "https://openalex.org/W2991443709",
        "https://openalex.org/W3024593601",
        "https://openalex.org/W2167527592",
        "https://openalex.org/W3168032844",
        "https://openalex.org/W3100435238",
        "https://openalex.org/W2890148670",
        "https://openalex.org/W6765299845",
        "https://openalex.org/W6646913887",
        "https://openalex.org/W3173179510",
        "https://openalex.org/W3098518823",
        "https://openalex.org/W3158136698",
        "https://openalex.org/W6648982606",
        "https://openalex.org/W2917690002",
        "https://openalex.org/W2976304245",
        "https://openalex.org/W3034310863",
        "https://openalex.org/W6791943378",
        "https://openalex.org/W3136926086",
        "https://openalex.org/W2954258401",
        "https://openalex.org/W3179630053",
        "https://openalex.org/W4312788649",
        "https://openalex.org/W3012494314",
        "https://openalex.org/W2905544027",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3185759054",
        "https://openalex.org/W2993383518",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W2991216808",
        "https://openalex.org/W3194206809",
        "https://openalex.org/W3035428054",
        "https://openalex.org/W3131373811",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1986473362"
    ],
    "abstract": "Point cloud compression with a higher compression ratio and tiny loss is essential for efficient data transportation. However, previous methods that depend on 3D convolution or frequent multi-head self-attention operations bring huge computations. To address this problem, we propose an octree-based Transformer compression method called OctFormer, which does not rely on the occupancy information of sibling nodes. Our method uses non-overlapped context windows to construct octree node sequences and share the result of a multi-head self-attention operation among a sequence of nodes. Besides, we introduce a locally-enhance module for exploiting the sibling features and a positional encoding generator for enhancing the translation invariance of the octree node sequence. Compared to the previous state-of-the-art works, our method obtains up to 17% Bpp savings compared to the voxel-context-based baseline and saves an overall 99% coding time compared to the attention-based baseline.",
    "full_text": "OctFormer: Efficient Octree-Based Transformer for Point Cloud Compression\nwith Local Enhancement\nMingyue Cui1, Junhua Long1, Mingjian Feng 1, Boyang Li 1, Kai Huang 1,2*\n1 School of Computer Science and Engineering, Sun Yat-sen University\n2 Shenzhen Institute, Sun Yat-sen University\n{cuimy,longjh7,fengmj8,liby3}@mail2.sysu.edu.cn, huangk36@mail.sysu.edu.cn\nAbstract\nPoint cloud compression with a higher compression ratio and\ntiny loss is essential for efficient data transportation. How-\never, previous methods that depend on 3D convolution or fre-\nquent multi-head self-attention operations bring huge compu-\ntations. To address this problem, we propose an octree-based\nTransformer compression method called OctFormer, which\ndoes not rely on the occupancy information of sibling nodes.\nOur method uses non-overlapped context windows to con-\nstruct octree node sequences and share the result of a multi-\nhead self-attention operation among a sequence of nodes.\nBesides, we introduce a locally-enhance module for exploit-\ning the sibling features and a positional encoding generator\nfor enhancing the translation invariance of the octree node\nsequence. Compared to the previous state-of-the-art works,\nour method obtains up to 17% Bpp savings compared to the\nvoxel-context-based baseline and saves an overall 99% cod-\ning time compared to the attention-based baseline.\nIntroduction\nIn the past few decades, the LiDAR sensor has proven to be\ncrucial for various types of intelligent robot applications (Li\nand Ibanez-Guzman 2020; Zou et al. 2022). It can accu-\nrately capture the 3D geometry information of scenes, which\nmakes it widely used in the detection, segmentation, plan-\nning, and other downstream perception tasks. However, stor-\ning and transmitting point cloud data require high costs. For\nexample, a single Velodyne LiDAR of HDL64 generates\nover 100,000 points per sweep, and about 2.88 million points\nare generated per second. Hence, it is necessary to develop\nan efficient point cloud compression method.\nDue to the disorder and sparsity of the point cloud, com-\npressing the point cloud into a tiny capacity is a tough chal-\nlenge compared to its well-studied image and video counter-\nparts. Fortunately, several schemes for point cloud geometry\nand attribute compression (Google 2017; Chou, Koroteev,\nand Krivoku´ca 2019; Guarda, Rodrigues, and Pereira 2020)\nhave been explored in prior research works. (Limuti, Polo,\nand Milani 2018) presents a voxelized dynamic point cloud\ncoding scheme that combines a cellular automata block re-\nversible transform for geometric data with a region adap-\n*Corresponding author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ntive transform for color data. (Quach, Valenzise, and Du-\nfaux 2019) designs a model for learning analysis and synthe-\nsis transforms suitable for voxel-based point cloud geometry\ncompression. Further, (Nguyen et al. 2021) proposes a deep\nauto-regressive generative method to estimate the occupancy\nprobability of each voxel. In general, these methods do not\nfully use spatial context information and also require a huge\namount of computing power.\nCompared with point cloud coding using voxel grids, oc-\ntree has more advantages benefiting from its higher coding\nefficiency (Huang et al. 2006; Liu et al. 2019). An octree\nnode uses an 8-bit occupancy to represent the distribution\nof children nodes. By combining the deep entropy model\nalong with an arithmetic encoder, the octree encoded point\ncloud is further compressed into a compact bitstream. Actu-\nally, in octree encoding process, the main problem faced is\nhow to efficiently extract strong prior information between\nspatial neighboring nodes, especially for the relationship be-\ntween sibling nodes. Sibling nodes can provide low-level lo-\ncal geometry features, which are significant for exploiting\ngeometry redundancy. Unlike ancestor nodes’ relationships,\nit is difficult to obtain the neighbor geometric relationship\nof sibling nodes by traversing the octree directly. Therefore,\na highly parallel sophisticated method for point cloud com-\npression is needed, and spatial context information is fully\nconsidered.\nIn this paper, we propose a Transformer-based entropy\nmodel called OctFormer, which can compress the point\ncloud efficiently. OctFormer uses non-overlapped context\nwindows to construct sequences and shares the results of the\nexpensive multi-head self-attention (MSA) operation for a\nsequence of nodes, which significantly reduces time over-\nhead. Subsequently, we propose an octree-based locally-\nenhanced feed-forward network (OctLeFF) and octree-\nbased positional encoding generator (OctPEG) to help the\nmodel fit the octree node sequence data better. We use\nOctPEG instead of traditional absolute positional encod-\ning (APE) to avoid unnecessary positional features, which\nenhances the translation-invariance of the octree node se-\nquence. Our method only needs prior knowledge informa-\ntion of the traversal format from sibling and ancestor nodes,\nnot additional coarse neighborhood information which is not\nreadily available, such as the occupancy information of sib-\nling nodes or local voxel context.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n470\nWe compare the proposed model with state-of-the-art\nmethods (Graziosi et al. 2020; Que, Lu, and Xu 2021; Huang\net al. 2020; Fu et al. 2022) on 3D point cloud datasets Se-\nmanticKITTI (Behley et al. 2019) and ScanNet (Dai et al.\n2017). The experiments show that our method outperforms\nthese methods, which are only designed for a specific cate-\ngory of point clouds. The contributions of our work can be\nsummarized as:\n• We propose a novel octree-based entropy model called\nOctFormer for point cloud geometry compression, which\nuses non-overlapped context windows to construct se-\nquences.\n• Our OctFormer shares the MSA results between a se-\nquence of traversal node information and does not need\nto use the occupancies of sibling nodes, which can be ap-\nplied to efficient point cloud encoding and decoding.\n• We propose a local-enhancing module replacing the orig-\ninal feed-forward network in Transformer. This simple\ndesign exploits the local features and improves the com-\npression performance.\n• To avoid unnecessary positional features caused by tra-\nditional APE, we introduce the OctPEG module which\nenhances the translation-invariance of the octree nodes\nwhen generating the sequence data.\nRelated Work\nStructured Point Cloud Compression\nStructured data formats intuitively present the point cloud\ndata and fit for convolution operations naturally. There are\nmainly voxel-based (Limuti, Polo, and Milani 2018; Quach,\nValenzise, and Dufaux 2019; Kaya, Schwarz, and Tabus\n2021) and image-based methods (Houshiar and N ¨uchter\n2015; Sun et al. 2019; Feng, Liu, and Zhu 2020). The video\npoint cloud compression standard (VPCC) (Schwarz et al.\n2018) converts the 3D input point cloud into a set of 2D\npatches followed by a packing process. Such a way allows\ncompressing the patches utilizing the existing video coding\nstandards. (Houshiar and N ¨uchter 2015) proposes mapping\nthe floating point measured ranges onto a three-channel im-\nage and then compressing it. The performance of image-\nbased methods is limited due to the loss of spatial informa-\ntion inherent. (Wang et al. 2021) voxelizes point clouds into\nnon-overlapped 3D cubes, and designs a 3D-convolution-\nbased variational autoencoder to compress the point clouds.\nHowever, the voxel-based method is sensitive to the density\nvariation and fails for the sparse point clouds. High compu-\ntational costs are also prohibitive.\nUnstructured Point Cloud Encoding\nOctree-based methods have higher coding efficiency\nthrough hierarchical representation. The MPEG point cloud\ngeometry compression standard (GPCC) (Graziosi et al.\n2020) compresses the point cloud geometry information by\nexploiting an octree-based encoding strategy. It predicts at-\ntributes and encodes or transmits them in a scalable man-\nner. (Dricot, Pereira, and Ascenso 2018) optimizes the oc-\ntree partitioning decisions based on a rate-distortion opti-\nmization process to adapt the content. (Wen et al. 2020)\n⊕\nlocal voxel context\nnode features\n……\n22\n3\n?\n?\n?\n?\n……\n231\n?\n?\n?\n147\n……\n?\nundecoded\n(a) V oxelContext-Net method\ndecoded\nundecoded\n……\n12\n?\n5\n……\n12\n?\n5\n9\n9\n……\n223\n12\n?\n?\n5\n……\n231\n44\n23\n11\n147\n……\n9\ns\nliding window\n(b) OctAttention method\n……\n?\n?\n?\n……\n?\n?\n?\n?\n?\nundecoded\n……\n223\n?\n?\n?\n?\n……\n231\n?\n?\n?\n147\n……\n?\nnon\n-\noverlapped window\n(c) Our proposed OctFormer\nFigure 1: Comparison between different methods of extract-\ning node features, where the occupancy is represented by\ndecimal digits in the node. For a sequence of octree nodes,\nour method can share the MSA results.\nadopts an adaptive patch generation module through adap-\ntive octree-based decomposition and clustering processing\nto avoid the reconstruction error prorogation. (Garcia et al.\n2020) develops a super-resolution technique to generate pos-\nsible contexts based on octrees that can be arithmetically\nencoded. OctSqueeze (Huang et al. 2020) is the first octree-\nbased deep learning entropy model by gathering context in-\nformation about conditions on ancestor nodes. However, the\nprior octree-based methods seldom focus on the neighbor-\nhood contexts from the sibling nodes, which can not fully\nextract the local spatial information of the nodes.\nRecently, V oxelContext-Net (Que, Lu, and Xu 2021) pro-\nposes introducing voxel context in the tree-structured deep\nmodel for a more precise prediction of the node’s occupancy\ndistribution. It employs 3D convolution on the generated lo-\ncal voxel context to encode the neighboring spatial informa-\ntion for each node in the constructed octree. However, this\nmethod has a limited receptive field of neighborhood infor-\nmation in fixed-size voxels and causes a lot of computations\ndue to introducing voxel encoding, especially for higher res-\nolution. Furthermore, OctAttention (Fu et al. 2022) designs\na conditional entropy model with a large receptive field that\nmodels the sibling and ancestor contexts to exploit the strong\ndependencies among the neighboring nodes. Additionally,\nOctAttention uses the mask operation to encode multiple\nnodes in parallel. Although OctAttention obtains excellent\ncompression performance, the resulting high time overhead\nis unacceptable. OctAttention directly uses the occupancy\nas features from the sibling nodes, so decoding the current\n471\nInput, N ×6\nOctPEG  \nEmbedding, N ×256\nLayerNorm\nMLP, N ×256\nLayerNorm\nMSA\nOctLeFF\n⊕\n×6\nInput Point Cloud Octree Construction & Entropy Encoding Process OctFormer Structure\nOccupied Octant\nEmpty Octant\nElement-wise Add\nOctFormer\nCoordinate: x, y, z\nDepth: 3\nParent Occupancy: 235\nIndex: 0\nN = 4 N = 4 N = 4\noccupancy\nprobability×N Arithmetic \nEncoder\nBitstream Linear\nConv1D (256, 3)\nLinear\nDepthwise\nConv1D (256, 3)\nFigure 2: The overview of our proposed method. The input point cloud is first constructed as an octree. Each non-leaf node\ncontains the feature of its xyz coordinate, depth, parent occupancy, and index. Non-overlapped context windows are constructed\nfor generating input sequences. For example, we set the context window size N = 4and take one sequence (orange) as input.\nThe input features (N∗6) are firstly embedded into 256 dimension vectors (N∗256). After adding the positional encoding using\nOctPEG, the features (N ∗ 256) are fed into Transformer blocks with our OctLeFF for feature extraction. Finally, a multi-layer\nperceptron (MLP) is employed to generate the occupancy distribution (N ∗ 256) for N nodes in the context window.\noctree node requires previous sibling nodes to be decoded.\nFrequent MSA operation for every prediction for one node\nbrings expensive time costs.\nAs shown in Fig.1, we visualize the differences in feature\nextraction between different methods. Overall, our proposed\nOctFormer has the following advantages:\n1. Compared with V oxelContext-Net, our proposed method\nconsiders higher resolution features (i.e., from sibling\nnodes) and does not need to introduce computationally\nexpensive 3D convolution on generated voxel grids.\n2. Compared with OctAttention, our method does not intro-\nduce sibling occupancy as a feature. Our method elimi-\nnates the decoding dependencies between the nodes and\nshares the result of a MSA operation within the con-\ntext window, which achieves several times to hundreds\nof times the time performance improvement.\n3. We design OctLeFF and OctPEG modules to further im-\nprove the compression performance, which is also veri-\nfied in both offline open source datasets and the practical\ndownstream task (i.e., segmentation).\nThe Proposed Method\nAs shown in Fig.2, we propose an octree-based Transformer\nwith the local enhancement for point cloud compression\ncalled OctFormer. We organize point cloud data with octrees\nand then design a Transformer-based deep entropy model\nalong with an arithmetic encoder to compress the octree\nnode sequences. The octree node sequences are constructed\nby non-overlapped context windows. By not introducing sib-\nling occupancy as features, our OctFormer eliminates the de-\ncoding dependencies between the nodes and shares the re-\nsult of a multi-head self-attention operation (MSA) within\nthe context window, which hugely reduces the computa-\ntions. Considering that the sibling nodes tend to have similar\noccupancies, we introduce an octree locally-enhanced feed-\nforward network to better capture the local features. Besides,\nwe propose octree-based positional encoding instead of ab-\nsolute positional encoding to keep the translation invariance\nof the input sequence. Finally, we predict the distribution of\nthe node’s occupancy based on our proposed OctFormer.\nOctree Building\nThe octree is an efficient representation structure, especially\nfor the sparse point cloud where most of the space is empty.\nWe construct an octree from an input point cloud by recur-\nsively partitioning the space into 8 cubes of the same size\nuntil the max depth is reached or the cube is empty. The oc-\ntree node uses an 8-bit occupancy to indicate whether the\nchild node is empty or not. We represent the octree by us-\ning these non-leaf nodes’ occupancies. For example, for the\noctree built in Fig.2, there are 15 non-leaf octree nodes in\ntotal and each non-leaf node requires 8 bits to store the occu-\npancy. Thus, it requires 120 bits to store the octree if there is\nno entropy model to compress further. Using a breadth-first\ntraversal, we serialize the octree into a bitstream and recon-\nstruct the octree structure from the bitstream losslessly. By\ntaking each leaf node’s center as a point, we reconstruct the\npoint cloud from the octree. Octree construction introduces\nquantization error, which is a lossy stage for the octree-based\nmethods. As shown in Fig.3, we visualize octree occupan-\ncies with different depths. A deeper octree obtains more fine-\ngrained leaf nodes and discovers precise geometric informa-\ntion. This means that the quality of the reconstructed point\ncloud depends on the max depth of the octree.\nOur Deep Entropy Model\nSuppose a sequence of occupancies for octree nodes are rep-\nresented as o = [o1, o2, ..., oi, ..., on], where oi represents\nthe occupancy of the i-th octree node Oi. oi can be rep-\nresented using an 8-bit code, which indicates whether a\n472\nFigure 3: The visualization of octree occupancies at depths of 8, 9, 10, and 11. The color of each point is set to the value of the\nnode’s occupancy and similar color denotes similar occupancy. Points in the octree leaf nodes decrease with increased depth.\nFigure 4: An illustration of the octree node sequences gener-\nating process. The nodes of a sequence come from different\nparts of the space, which is not friendly to the APE mecha-\nnism.\nchild node is empty or not. For example, suppose oi =\n[1, 0, 0, 0, 0, 1, 0, 0], that means child node of Oi at index 0\nand 5 is not empty, while the others are all empty.\nSuppose the actual distribution of the sequence o is P(o),\nand the predicted distribution is Q(o). According to the in-\nformation theory (Shannon 1948), the closer the estimated\ndistribution Q(o) is to the actual distributionP(o), the fewer\nbits are needed to encode the sequence o. Thus the goal\nof the entropy model is to minimize the cross-entropy loss\nEo∼P [−log2Q(o)] between the P(o) and Q(o). We factor-\nize Q(o) into a product of predicted probability distributions\nof every octant occupancy oi as follows:\nQ(o) =\nY\ni\nqi(oi|fi−j, ..., fi, ...fi+k; w) (1)\nwhere qi(oi|fi−j, ..., fi, ...fi+k; w) is the estimated proba-\nbility distribution of octree node occupancy oi. w is the\nweight of the entropy model. fi denotes the feature of the\noctree node Oi, which contains the xyz coordinates, in-\ndex (0-7), depth (1-12), and parent occupancy (1-255). As-\nsuming the distribution of the occupancy oi only depends\non the local context window of octree node Oi, we con-\nstruct non-overlapped context window with size of N to\nform sequence, and use all the features from the sequence\n(fi−j, ..., fi, ...fi+k) to predict the distribution qi, where\nj + k − 1 =N.\nOctree-based Transformer Block\nThe structure of OctFormer is illustrated in Fig.2. We first\nemploy an embedding layer to increase the feature dimen-\nsion from 6 to 256. Then we introduce the octree positional\nencoding generator (OctPEG) to generate positional encod-\ning for the input sequence. We use several Transformer\nblocks to extract features for every node, where an octree\nlocally-enhanced feed-forward network (OctLeFF) is used\nto help capture the local features. Finally, a MLP is em-\nployed to generate the occupancy distribution of every node.\nThe OctFormer block can be formally defined as:\nX0 = OctPEG(Emb) + Emb,\nX\n′\nl = MSA(LN (Xl−1)) +Xl−1,\nXl = OctLeFF (LN(X\n′\nl )) +X\n′\nl\n(2)\nwhere Emb is the output of the Embedding layer. X0 de-\nnotes the input before the first transformer block.Xl and X\n′\nl\nare the outputs of the MSA module and OctLeFF module of\nthe l-th transformer block, respectively. LN represents the\nlayer normalization (Ba, Kiros, and Hinton 2016).\nComputation Complexity Analysis Our OctFormer\neliminates the decoding dependencies between the nodes\nand shares the results of the MSA operation within our\nconstructed non-overlapped window, which hugely re-\nduces the computation cost. Our encoding and decoding\nprocesses are identical. Suppose there are n nodes to be\nencoding/decoding, the context window size is N and the\nfeature dimension is d. The computation complexity is\nO(N2 ∗ d ∗ n\nN ) for us. OctAttention (Fu et al. 2022) can\nachieve O(N2 ∗ d ∗ n\nN ) for encoding as well by applying\ntheir proposed masked operation and shrinking the average\nreceptive field to (N + 1)/2. However, because of taking\nthe sibling occupancy as a feature, OctAttention is N\ntimes slower when decoding, which brings O(N2 ∗ d ∗ n)\ncomplexity. We can achieveN times less computation when\ndecoding compared with OctAttention, which is a huge\nimprovement because the context window size N is usually\nset to a big number such as 512 or 1024.\nOctree Locally-enhanced Feed-Foward Network It has\nbeen proved that the vanilla Transformer can extract global\nfeatures effectively but have limitations in capturing the lo-\ncal features (Li et al. 2021; Wu et al. 2021). As shown in\n473\n0 1 2 3 4 5 6 7\nBpp\n45\n50\n55\n60\n65Point-to-Point PSNR(dB)\nPoint-to-Point PSNR on ScanNet\nGPCC\nOctSqueeze\nVoxelContex-Net\nOurs\n0 1 2 3 4 5 6 7\nBpp\n45\n50\n55\n60\n65\n70Point-to-Plane PSNR(dB)\nPoint-to-Plane PSNR on ScanNet\nGPCC\nOctSqueeze\nVoxelContex-Net\nOurs\n0 1 2 3 4\nBpp\n55\n60\n65\n70\n75\n80Point-to-Point PSNR(dB)\nPoint-to-Point PSNR on SemanticKITTI\nGPCC\nOctSqueeze\nVoxelContex-Net\nOctAttention\nOurs\n0 1 2 3 4\nBpp\n60\n65\n70\n75\n80\n85Point-to-Plane PSNR(dB)\nPoint-to-Plane PSNR on SemanticKITTI\nGPCC\nOctSqueeze\nVoxelContex-Net\nOctAttention\nOurs\nFigure 5: The quantitative results of different methods on ScanNet and SemanticKITTI, which is presented in the form of\nrate-distortion curves. It should be noted that we use the results of V oxelContext-Net not containing the coordinate refinement\nmodule, which is an algorithm to improve the PSNR value after point cloud reconstruction. Besides, for a fair comparison, we\nonly compare with OctAttention on SemanticKITTI dataset, in which data in their paper is used.\nGround Truth (ScanNet) Ours: PSNR:64.99 Bpp:5.54 GPCC: PSNR:64.86 Bpp:6.83 Ours: PSNR: 58.96 Bpp: 2.58 GPCC: PSNR:58.18 Bpp:2.69 \nGround Truth (SemanticKITTI) Ours: PSNR:83.06 Bpp:3.32 GPCC: PSNR:82.83 Bpp:5.17 Ours: PSNR:76.98 Bpp:1.67 GPCC: PSNR:75.82 Bpp:1.97 \n0.00040.00020 0.0001 0.0003\nError Colormap\nFigure 6: The visualization of compression results of our method and GPCC on ScanNet (top) and SemanticKITTI (bottom)\nunder different Bpps.\nFig.3, we visualize octree occupancies at different depths.\nWe can observe that octree nodes tend to have similar oc-\ncupancy among the siblings. Therefore, we propose using a\n1D convolution layer to replace the feed-forward network in\nthe vanilla Transformer to better capture the local features\nof the octree node. The structure of OctLeFF module is il-\nlustrated in Fig.2. We first use a linear projection layer to\nproject the feature to another dimension and then apply a\n1D convolution operation on the feature. Finally, we use an-\nother linear projection layer to project the feature back. Dif-\nferent from the original element-wise feed-forward network,\nthe 1D convolution aggregates the features from the siblings,\nwhich enables our OctLeFF to capture the local features of\nthe sibling nodes.\nOctree Positional Encoding Generator The sequence\ngenerated by an octree should have translation-invariance.\nAs shown in Fig.4, when generating the sequence data, the\nempty node is jumped over and the consequent N nodes are\norganized as a sequence. That means the sequence should\nhave an order in space but the nodes at the same position of\ndifferent sequences have totally different relative geometric\nlocations in space. However, the absolute positional encod-\ning (APE) scheme (Vaswani et al. 2017; Dosovitskiy et al.\n2021) performs badly on the tasks which require translation-\ninvariance because it adds unique positional encodings to\neach token. Inspired by (Chu et al. 2021), we propose using\nOctPEG to generate the positional encoding according to the\nlocal features. As shown in Fig.2, the key to our OctPEG is\nadopting a depth-wise convolution operation to extract the\nlocal feature before the transformer block. The OctPEG is\napplied in input embedding features E ∈ RN×256 to pro-\nduce the position encodings E′ ∈ RN×256. Compared with\nAPE, the designed convolution introduces learnable parame-\nters to enhance the translation-invariance for the node’s em-\nbedding features and makes the position encodings the same\nsize as the embedding features.\nLearning\nWe optimize our deep entropy model using the cross entropy\nbetween the real and predicted occupancy of the non-leaf\n474\nnode. The loss function ℓ is defined as follows:\nℓ = −\nX\ni\nlog qi(oi|fi−j, ..., fi, ...fi+k; w) (3)\nwhere qi(oi|fi−j, ..., fi, ...fi+k; w) is the estimated occu-\npancy distribution of octree node occupancy oi as defined\nin Eq.(1).\nExperiments\nDatasets\nSemanticKITTI SemanticKITTI (Behley et al. 2019) is a\nlarge-scale point cloud dataset under different densities. It\nprovides 22 sequences, 43,504 scans in total, with each scan\ncontaining over 120,000 points. We use the official train/test\nsplit, which is using sequences 00 to 10 for training and 11\nto 21 for testing. Overall, we use 23,201 scans for training\nand 20,351 scans for testing.\nScannet ScanNet (Dai et al. 2017) is a popular indoor\n3D point cloud dataset containing more than 1,500 scans.\nTrain/test splits are the same as ScanNet’s benchmark tasks,\nthat is, we select 1,045 as the training set, 156 as the vali-\ndation set, and 312 as the test set. We sample 50,000 points\nfrom each scan the same as (Que, Lu, and Xu 2021).\nExperimental Details\nBaseline Methods We compare our method against meth-\nods GPCC (Graziosi et al. 2020), OctSqueeze (Huang et al.\n2020), V oxelContext-Net (Que, Lu, and Xu 2021), and Oc-\ntAttention (Fu et al. 2022). They are also only designed for a\nspecific category of point clouds. Since the source codes of\nsome methods are not publicly available, we keep our train-\ning/testing setting consistent with them and use the results\nin their paper.\nImplementation Details We set the maximum depth of\nthe octree at 9 and 12 for ScanNet and SemanticKITTI re-\nspectively. At the training stage, we use all nodes of the oc-\ntree to optimize the model. At the testing stage, we test the\nbitrate at a certain depth of the octree by directly truncating\nthe octree at the depth and evaluate it.\nOur model is implemented in Pytorch and trained/tested\non a machine with Xeon Gold 6134 CPU and a single\nNVIDIA Tesla V100 GPU (32GB Memory). In the training\nprocedure, the Adam optimizer is adopted and the learning\nrate is set to 1e-4 for the entropy model. It takes 2 days to\ntrain the model on ScanNet and 3 days on SemanticKITTI.\nWe set the embedding size to 256 and the feed-forward di-\nmension in the Transformer block to 1024. We use 6 layers\nand 8 heads for the MSA. By default, the context window\nsize N is set to 1,024. We test the encoding/decoding time\non 1,000 dummy octree nodes for the experiment below.\nEvaluation Metrics We use bits per point (Bpp) as the\ncompression ratio metric. As for evaluating the point cloud\nreconstruction quality, we use the point-to-point PSNR (D1\nPSNR) and point-to-plane PSNR (D2 PSNR) proposed by\nthe MPEG standards (Schwarz et al. 2018). Specifically, we\nuse the official metric calculating tool pc\nerror provided by\nMethod Bpp on SemanticKITTI Time (s)\nD=8 D=10 D=12 En/Decode\nV oxelContext-Net - 0.951 4.497 0.426/0.419\nOctAttention 0.139 0.939 3.740 0.002/2.060\nOurs 0.146 0.963 3.708 0.005/0.007\nTable 1: Comparison with prior state-of-the-art methods on\nSemanticKITTI dataset. ’D’ is the max depth of octree. For\neach method, we choose the best compression results for fair\ncomparison.\nMPEG’s GPCC. We set the PSNR peak value r = 1 follow-\ning (Que, Lu, and Xu 2021) and (Fu et al. 2022). We normal-\nize the point cloud data to [0, 1]3. For a fair comparison, we\ncorrect other papers’ results by eliminating inconsistencies\nin the PSNR formula.\nExperiment Results\nResults on SemanticKITTI and ScanNet Datasets The\nrate-distortion curves of LiDAR compression are shown in\nFig.5. Overall, one can observe that our method achieves\nbetter performance compared with other baselines, as ex-\npected. Specifically, compared with GPCC and OctSqueeze,\nwe can achieve up to 54% and 21% bitrate savings on Se-\nmanticKITTI and 22% and 10% bitrate savings on Scan-\nNet. The state-of-the-art method V oxelContext-Net is close\nto ours on ScanNet. The main reason is that the point\ncloud in ScanNet is much denser, which means there’s much\nless information to discover in the octree. For sparse point\ncloud in SemanticKITTI, we achieve 17% Bpp savings com-\npared with V oxelContext-Net. Considering that octree-based\nmethods have the same reconstruction qualities under the\nsame octree depth, we only visualize the quantitative results\nof ours and GPCC, shown in Fig.6. The visualization results\nalso demonstrate the effectiveness of our method.\nComparison with Prior State-of-the-Art Methods As\nshown in Table.1, we further compare our method with\nstate-of-the-art V oxelContext-Net (Que, Lu, and Xu 2021)\nand OctAttention (Fu et al. 2022). Compared with\nV oxelContext-Net, our method has up to 17% Bpp savings\nand achieves dozens of times performance improvement.\nThe reason is that V oxelcontext-Net has a limited recep-\ntive field and expensive computation in higher resolution.\nCompared with OctAttention, we achieve about 300 times\nof time performance improvement, while maintaining sim-\nilar compression effect. Although introducing sibling occu-\npancy as the feature can greatly improve the compression\nperformance, it will bring unacceptable time consumption\nbecause decoding the current octree node requires previous\nsibling nodes to be decoded. Our method eliminates the de-\ncoding dependencies between the nodes and shares the result\nof a MSA operation within the context window. Therefore,\nour decoding process is identical to the encoding process,\nwhich is hundreds of times faster than OctAttention.\nAblation Study and Analysis\nContext Window Size As shown in Table.2, we perform\nablation study on context window size. We set the different\n475\nSize N Bpp on ScanNet\nD=5 D=6 D=7 D=8 D=9\n32 0.062 0.238 1.049 3.353 6.416\n128 0.057 0.220 0.986 3.192 6.135\n256 0.057 0.219 0.985 3.183 6.113\n512 0.057 0.218 0.980 3.169 6.086\n1024 0.056 0.211 0.958 3.123 6.021\nTable 2: Performance of our methods when setting different\ncontext window sizes N.\nAPE OctLeFF OctPEG Bpp Params\n✗ ✗ ✗ 6.212 5.66 M\n✓ ✗ ✗ 6.213 5.66 M\n✗ ✓ ✗ 6.144 4.28 M\n✗ ✗ ✓ 6.161 5.66 M\n✗ ✓ ✓ 6.135 4.28 M\nTable 3: Different compression performance when using dif-\nferent modules on ScanNet dataset with setting the depth to\n9. ✗ denotes removing and ✓ denotes retaining.\ncontext window sizes and train different models on ScanNet.\nFrom the table, we can observe that the method achieves up\nto 0.395 Bpp savings when setting the depth of the octree\nto 9. The model with larger context window size obtains\nbetter results, for larger window size has more context fea-\ntures. The experiment results demonstrate the effectiveness\nof large receptive field contexts.\nEffectiveness of Proposed Modules As shown in Table.3,\nwe perform ablation study on our proposed OctLeFF and\nOctPEG module. In the experiments, both the hidden size\nand channel size are set to 256. One can find that the origi-\nnal APE has little effect for the model, as we expected. The\ncomparison shows that the OctLeFF and OctPEG modules\neffectively improve compression performance and parame-\nters reduce 1.38M.\nRuntime Analysis As shown in Table.4, we compare the\nruntime performance of different methods under different\nparameter settings. We can observe that both the encoding\nand decoding time in V oxelContext-Net increase with the\nvoxel size, due to the more computationally expensive 3D\nconvolution on voxel grids. But for transformer-based meth-\nods, when the context window size is larger, the efficiency\nof parallelism in the transformer is higher. OctAttention de-\ncoding one octree node needs the previous sibling nodes to\nbe decoded, while our method can achieve much faster de-\ncoding since our encoding and decoding processes are the\nsame for feature extraction and parallel computing. Exper-\nimental results show that our method achieves a significant\nimprovement in runtime performance. We can observe that\nthe method can be applied to efficient point cloud encoding\nand decoding.\nPerformance on Segmentation Application\nThe effect of the compression method on downstream tasks\nis another important metric. As shown in Fig.7, we evalu-\nMethods Time (s)\nEncode Decode\nV oxelContext-Net (V=5) 0.3889 0.3741\nV oxelContext-Net (V=7) 0.4010 0.4148\nV oxelContext-Net (V=9) 0.4265 0.4199\nV oxelContext-Net (V=11) 0.4523 0.4495\nOctAttention (N=8) 0.1672 1.2710\nOctAttention (N=32) 0.0476 1.3635\nOctAttention (N=128) 0.0114 1.3737\nOctAttention (N=512) 0.0030 1.4091\nOctAttention (N=1024) 0.0023 2.0602\nOurs (N=8) 0.4457 0.4561\nOurs (N=32) 0.0765 0.0866\nOurs (N=128) 0.0283 0.0369\nOurs (N=512) 0.0096 0.0107\nOurs (N=1024) 0.0052 0.0073\nTable 4: The comparison of time performance of different\nmethods. ’V’ is the voxel size for V oxelContext-Net and ’N’\nrepresents the context window size for both our method and\nOctAttention.\nOurs: IOU: \n43.91 \nBpp\n: \n4.8\n0\n(a) Segmentation visualization.\n (b) Comparison with others.\nFigure 7: Quality performance of semantic segmentation\nof different point cloud compression methods on Se-\nmanticKITTI dataset.\nate the effectiveness of our proposed method on semantic\nsegmentation, which is a basic task for point cloud (Nguyen\nand Le 2013; Hu et al. 2022). We use RandLA-Net (Hu et al.\n2020) as the evaluative semantic segmentation method and\nintersection-over-union (IOU) as the metric. Experimental\nresults show that our method can achieve segmentation per-\nformance close to the raw data when the Bpp is 4.8. At any\ngiven Bpp, we outperform GPCC and obtain higher IOU.\nOverall, experimental results demonstrate the effectiveness\nof our method on such downstream tasks (i.e., semantic seg-\nmentation).\nConclusion\nWe propose an octree-based transformer with local enhance-\nment called OctFormer, which can compress point clouds\nefficiently. Specifically, we use non-overlapped context win-\ndows to construct sequences and share the results of the\nmulti-head self-attention operation to reduce time overhead.\nWe further design OctLeFF and OctPEG models to improve\nthe compression performance. The experimental results on\nboth datasets and the segmentation algorithm verify the ef-\nfectiveness of the proposed method.\n476\nAcknowledgments\nThis work was supported by the Shenzhen Basic Research\n(JCYJ20180508152434975), the National Key Research and\nDevelopment Program of China (2018YFB1802405), the\nScience and Technology Planning Project of Guangzhou\n(202007050004).\nReferences\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450.\nBehley, J.; Garbade, M.; Milioto, A.; Quenzel, J.; Behnke,\nS.; Stachniss, C.; and Gall, J. 2019. Semantickitti: A dataset\nfor semantic scene understanding of lidar sequences. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 9297–9307.\nChou, P. A.; Koroteev, M.; and Krivoku´ca, M. 2019. A vol-\numetric approach to point cloud compression—Part I: At-\ntribute compression. IEEE Transactions on Image Process-\ning, 29: 2203–2216.\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia, H.; and\nShen, C. 2021. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882.\nDai, A.; Chang, A. X.; Savva, M.; Halber, M.; Funkhouser,\nT.; and Nießner, M. 2017. Scannet: Richly-annotated 3d\nreconstructions of indoor scenes. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 5828–5839.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nDricot, A.; Pereira, F.; and Ascenso, J. 2018. Rate-Distortion\nDriven Adaptive Partitioning for Octree-Based Point Cloud\nGeometry Coding. In 2018 25th IEEE International Confer-\nence on Image Processing, 2969–2973.\nFeng, Y .; Liu, S.; and Zhu, Y . 2020. Real-Time Spatio-\nTemporal LiDAR Point Cloud Compression. In 2020\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems, 10766–10773.\nFu, C.; Li, G.; Song, R.; Gao, W.; and Liu, S. 2022. OctAt-\ntention: Octree-Based Large-Scale Contexts Model for Point\nCloud Compression. In Proceedings of the AAAI Conference\non Artificial Intelligence.\nGarcia, D. C.; Fonseca, T. A.; Ferreira, R. U.; and\nde Queiroz, R. L. 2020. Geometry Coding for Dynamic V ox-\nelized Point Clouds Using Octrees and Multiple Contexts.\nIEEE Transactions on Image Processing, 29: 313–322.\nGoogle. 2017. Draco 3d data compresison. https://github.\ncom/google/draco. Accessed: 2022-05-21.\nGraziosi, D.; Nakagami, O.; Kuma, S.; Zaghetto, A.; Suzuki,\nT.; and Tabatabai, A. 2020. An overview of ongoing point\ncloud compression standardization activities: video-based\n(V-PCC) and geometry-based (G-PCC). APSIPA Transac-\ntions on Signal and Information Processing, 9.\nGuarda, A. F.; Rodrigues, N. M.; and Pereira, F. 2020. Deep\nlearning-based point cloud geometry coding with resolution\nscalability. In 2020 IEEE 22nd International Workshop on\nMultimedia Signal Processing, 1–6.\nHoushiar, H.; and N ¨uchter, A. 2015. 3D point cloud com-\npression using conventional image compression for efficient\ndata transmission. In 2015 XXV International Conference on\nInformation, Communication and Automation Technologies,\n1–8.\nHu, Q.; Yang, B.; Fang, G.; Guo, Y .; Leonardis, A.; Trigoni,\nN.; and Markham, A. 2022. Sqn: Weakly-supervised seman-\ntic segmentation of large-scale 3d point clouds. InEuropean\nConference on Computer Vision, 600–619. Springer.\nHu, Q.; Yang, B.; Xie, L.; Rosa, S.; Guo, Y .; Wang, Z.;\nTrigoni, N.; and Markham, A. 2020. Randla-net: Efficient\nsemantic segmentation of large-scale point clouds. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 11108–11117.\nHuang, L.; Wang, S.; Wong, K.; Liu, J.; and Urtasun, R.\n2020. Octsqueeze: Octree-structured entropy model for lidar\ncompression. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 1313–1323.\nHuang, Y .; Peng, J.; Kuo, C.-C. J.; and Gopi, M.\n2006. Octree-Based Progressive Geometry Coding of Point\nClouds. In PBG@ SIGGRAPH, 103–110.\nKaya, E. C.; Schwarz, S.; and Tabus, I. 2021. Refining The\nBounding V olumes for Lossless Compression of V oxelized\nPoint Clouds Geometry. In 2021 IEEE International Con-\nference on Image Processing, 3408–3412.\nLi, Y .; and Ibanez-Guzman, J. 2020. Lidar for Autonomous\nDriving: The Principles, Challenges, and Trends for Auto-\nmotive Lidar and Perception Systems. IEEE Signal Pro-\ncessing Magazine, 37(4): 50–61.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707.\nLimuti, S.; Polo, E.; and Milani, S. 2018. A Transform Cod-\ning Strategy for V oxelized Dynamic Point Clouds. In 2018\n25th IEEE International Conference on Image Processing ,\n2954–2958.\nLiu, Z.; Tang, H.; Lin, Y .; and Han, S. 2019. Point-V oxel\nCNN for Efficient 3D Deep Learning. In Wallach, H.;\nLarochelle, H.; Beygelzimer, A.; d'Alch ´e-Buc, F.; Fox, E.;\nand Garnett, R., eds., Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc.\nNguyen, A.; and Le, B. 2013. 3D point cloud segmenta-\ntion: A survey. In 2013 6th IEEE conference on robotics,\nautomation and mechatronics, 225–230. IEEE.\nNguyen, D. T.; Quach, M.; Valenzise, G.; and Duhamel, P.\n2021. Lossless coding of point cloud geometry using a deep\ngenerative model. IEEE Transactions on Circuits and Sys-\ntems for Video Technology, 31(12): 4617–4629.\nQuach, M.; Valenzise, G.; and Dufaux, F. 2019. Learning\nConvolutional Transforms for Lossy Point Cloud Geometry\nCompression. In 2019 IEEE International Conference on\nImage Processing, 4320–4324.\n477\nQue, Z.; Lu, G.; and Xu, D. 2021. V oxelcontext-net: An oc-\ntree based framework for point cloud compression. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 6042–6051.\nSchwarz, S.; Preda, M.; Baroncini, V .; Budagavi, M.; Cesar,\nP.; Chou, P. A.; Cohen, R. A.; Krivoku ´ca, M.; Lasserre, S.;\nLi, Z.; et al. 2018. Emerging MPEG standards for point\ncloud compression. IEEE Journal on Emerging and Selected\nTopics in Circuits and Systems, 9(1): 133–148.\nShannon, C. E. 1948. A mathematical theory of communi-\ncation. The Bell system technical journal, 27(3): 379–423.\nSun, X.; Ma, H.; Sun, Y .; and Liu, M. 2019. A Novel Point\nCloud Compression Algorithm Based on Clustering. IEEE\nRobotics and Automation Letters, 4(2): 2132–2139.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, J.; Zhu, H.; Liu, H.; and Ma, Z. 2021. Lossy Point\nCloud Geometry Compression via End-to-End Learning.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 31(12): 4909–4923.\nWen, X.; Wang, X.; Hou, J.; Ma, L.; Zhou, Y .; and Jiang,\nJ. 2020. Lossy Geometry Compression Of 3d Point Cloud\nData Via An Adaptive Octree-Guided Network. In 2020\nIEEE International Conference on Multimedia and Expo, 1–\n6.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 22–31.\nZou, Q.; Sun, Q.; Chen, L.; Nie, B.; and Li, Q. 2022. A\nComparative Analysis of LiDAR SLAM-Based Indoor Nav-\nigation for Autonomous Vehicles. IEEE Transactions on In-\ntelligent Transportation Systems, 23(7): 6907–6921.\n478"
}