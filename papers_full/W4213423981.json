{
  "title": "Improving Mandarin End-to-End Speech Recognition With Word N-Gram Language Model",
  "url": "https://openalex.org/W4213423981",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2364673641",
      "name": "Tian, Jinchuan",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2232452073",
      "name": "Yu Jianwei",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2349809231",
      "name": "Weng Chao",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2115955093",
      "name": "Zou, Yuexian",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2096754558",
      "name": "Yu, Dong",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964539095",
    "https://openalex.org/W2794289187",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W2963362078",
    "https://openalex.org/W6640059789",
    "https://openalex.org/W2888779557",
    "https://openalex.org/W3095376166",
    "https://openalex.org/W3164692279",
    "https://openalex.org/W6794920533",
    "https://openalex.org/W3200601846",
    "https://openalex.org/W2787663903",
    "https://openalex.org/W2886180730",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W6754473786",
    "https://openalex.org/W3015190365",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2046932483",
    "https://openalex.org/W2408623794",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4200629210",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2889282842",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3015611708",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W3167533889",
    "https://openalex.org/W3163793923",
    "https://openalex.org/W3197478142",
    "https://openalex.org/W4327845728",
    "https://openalex.org/W4299649720",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W4310658830",
    "https://openalex.org/W2889048668",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W3156902660",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Despite the rapid progress of end-to-end (E2E) automatic speech recognition\\n(ASR), it has been shown that incorporating external language models (LMs) into\\nthe decoding can further improve the recognition performance of E2E ASR\\nsystems. To align with the modeling units adopted in E2E ASR systems,\\nsubword-level (e.g., characters, BPE) LMs are usually used to cooperate with\\ncurrent E2E ASR systems. However, the use of subword-level LMs will ignore the\\nword-level information, which may limit the strength of the external LMs in E2E\\nASR. Although several methods have been proposed to incorporate word-level\\nexternal LMs in E2E ASR, these methods are mainly designed for languages with\\nclear word boundaries such as English and cannot be directly applied to\\nlanguages like Mandarin, in which each character sequence can have multiple\\ncorresponding word sequences. To this end, we propose a novel decoding\\nalgorithm where a word-level lattice is constructed on-the-fly to consider all\\npossible word sequences for each partial hypothesis. Then, the LM score of the\\nhypothesis is obtained by intersecting the generated lattice with an external\\nword N-gram LM. The proposed method is examined on both Attention-based\\nEncoder-Decoder (AED) and Neural Transducer (NT) frameworks. Experiments\\nsuggest that our method consistently outperforms subword-level LMs, including\\nN-gram LM and neural network LM. We achieve state-of-the-art results on both\\nAishell-1 (CER 4.18%) and Aishell-2 (CER 5.06%) datasets and reduce CER by\\n14.8% relatively on a 21K-hour Mandarin dataset.\\n",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nImproving Mandarin End-to-End Speech\nRecognition with Word N-gram Language Model\nJinchuan Tian, Jianwei Yu, Chao Weng, Yuexian Zou, Senior Member, IEEEand Dong Yu, Fellow, IEEE\nAbstract—Despite the rapid progress of end-to-end (E2E)\nautomatic speech recognition (ASR), it has been shown that\nincorporating external language models (LMs) into the decoding\ncan further improve the recognition performance of E2E ASR\nsystems. To align with the modeling units adopted in E2E\nASR systems, subword-level (e.g., characters, BPE) LMs are\nusually used to cooperate with current E2E ASR systems.\nHowever, the use of subword-level LMs will ignore the word-level\ninformation, which may limit the strength of the external LMs\nin E2E ASR. Although several methods have been proposed to\nincorporate word-level external LMs in E2E ASR, these methods\nare mainly designed for languages with clear word boundaries\nsuch as English and cannot be directly applied to languages like\nMandarin, in which each character sequence can have multiple\ncorresponding word sequences. To this end, we propose a novel\ndecoding algorithm where a word-level lattice is constructed on-\nthe-ﬂy to consider all possible word sequences for each partial\nhypothesis. Then, the LM score of the hypothesis is obtained\nby intersecting the generated lattice with an external word N-\ngram LM. The proposed method is examined on both Attention-\nbased Encoder-Decoder (AED) and Neural Transducer (NT)\nframeworks. Experiments suggest that our method consistently\noutperforms subword-level LMs, including N-gram LM and\nneural network LM. We achieve state-of-the-art results on both\nAishell-1 (CER 4.18%) and Aishell-2 (CER 5.06%) datasets\nand reduce CER by 14.8% relatively on a 21K-hour Mandarin\ndataset.\nIndex Terms—speech recognition, language model\nI. I NTRODUCTION\nE\nND-TO-END automatic speech recognition (ASR) has\nmade great progress in the past few years. Although E2E\nASR frameworks such as attention-based encoder-decoder\n(AED) [1], [2] and neural transducer (NT) [3] can directly map\nspeech to token sequences through a single neural network, it\nhas been found that incorporating an external language model\ninto an E2E ASR system sometimes is crucial to utilize a large\namount of text corpus. To this end, several approaches have\nbeen proposed to effectively integrate external LMs in E2E\nASR systems [4], [5], [6], [7]. One of the most widely used\napproaches is called shallow fusion [5].\nIn shallow fusion, the log probabilities of each candidate\nhypothesis obtained from the E2E model and the external\nLM are interpolated during the decoding. To allow effective\nJinchuan Tian and Yuexian Zou are with the Advanced data and signal\nprocessing laboratory, School of Electric and Computer Science, Peking\nUniversity, Shenzhen Graduate School, Shenzhen, China. This work was done\nwhen Jinchuan Tian was an intern at Tencent AI Lab.\nJianwei Yu, Chao Weng and Dong Yu are with Tencent AI LAB. Jianwei\nYu and Chao Weng are also from Tencent ASR Oteam.\nJianwei Yu is the corresponding author. (tomasyu@tencent.com)\nPaper submitted to IEEE Signal Processing Letter (SPL)\non-the-ﬂy interpolation, the external LMs used in shallow\nfusion are usually constructed using the same subword level\nmodeling units (e.g., characters, BPE [8]) with E2E ASR\nsystems, especially for languages without clear word boundary\nsuch as Mandarin and Japanese. However, the use of subword\nlevel LMs will drop the word level information, which will\ncompromise the strength of external LMs. To overcome this\nissue, several approaches have been proposed to incorporate\nword-level external LMs during inference under the paradigm\nof shallow fusion [9], [10] and achieved signiﬁcant perfor-\nmance improvement. In [9], each hypothesis is ﬁrstly scored by\nthe character-level LM until a word boundary is encountered.\nThen the known words are further rescored using the word-\nlevel LM while the character-level LM provides the scores for\nout-of-vocabulary tokens. In [10], a look-ahead mechanism\nis proposed to get rid of character-level LMs and score\nthe hypothesis using a word-level RNN-LM only. However,\nmost of the previous methods require clear word boundaries\nto compute the word-level LM scores, which limits their\napplication for those languages with blurred word boundaries,\nsuch as Mandarin and Japanese. Therefore, we argue that the\nway to effectively incorporate word-level LMs into an E2E\nASR system for languages without clear word boundaries\nneeds further investigation.\nTo utilize the word-level LM during decoding, a subword-\nto-word conversion is a prerequisite. However, a character-\nlevel sequence can be mapped to different word sequences.\nTherefore, the main issue that needs to be addressed in\nincorporating word-level LM in E2E ASR for languages like\nMandarin is: how to consider all candidate word sequences\nof a partial character hypothesis. In this work, we propose\na new method to integrate word N-gram LMs into the de-\ncoding of E2E ASR systems without the requirement of clear\nword boundaries. During decoding, each character-level partial\nhypothesis generated by the ASR system is ﬁrstly converted\ninto a word lattice. Each path of the lattice stands for one\npossible subword-to-word conversion result. Then the score\nof each path is obtained by intersecting the word lattice with\nan external weighted ﬁnite-state acceptor (WFSA) word-level\nN-gram LM. Finally, the best score among all the paths (or\nthe averaged score of all the paths) in the lattice is used as\nthe score computed with word-level LM for the given partial\nhypothesis. In this way, the proposed method is free from\nword boundary and considers all possible subword-to-word\nconversion results.\nWe examine the proposed method on two of the most\npopular E2E ASR frameworks: AED and NT. Exhaustive\nexperiments show that our method consistently outperforms\narXiv:2201.01995v1  [cs.CL]  6 Jan 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nits subword-level counterparts, including both the N-gram\nLM and neural network LM. Superior robustness to hyper-\nparameters is also observed in our method. Experimentally, we\nachieve the character error rate (CER) of 4.18% and 5.06%\non Aishell-1 [11] test set and Aishell-2 [12] test-ios set. Our\nmethod is also compatible with large-scale ASR tasks: up to\n2% absolute, or 14.8% relative, CER reduction is observed on\na 21k-hour Mandarin corpus.\nThe main contributions of this work are concluded below:\n1) we propose a novel method to incorporate external word-\nlevel N-gram language models into E2E ASR systems for\nlanguages without clear word boundaries; 2) To the best of\nour knowledge, the proposed method achieves state-of-the-art\nresults on two of the widely used Mandarin datasets: Aishell-1\nand Aishell-2.\nII. O N-THE -FLY DECODING WITH WORD N-GRAM\nA. Shallow fusion in AED and NT\nShallow fusion [5] is a common method for LM adaptation\nof E2E ASR systems. In this work, we still follow the\nparadigm of shallow fusionto integrate the word-level N-gram\nLM. Without loss of generality, we assume a character-level\npartial hypothesis Cm\n1 is proposed by the E2E ASR system,\nwhere Cm−1\n1 is the history context and cm is the newly\nproposed character. In shallow fusion, the LM log-posterior\nlog p(cm|Cm−1\n1 ) is required during decoding for various E2E\nASR systems. Speciﬁcally, for the decoding algorithm[2] of\nAED, assume δ(Cm\n1 ) is the score of partial hypothesis Cm\n1 .\nIn shallow fusion, the score is computed recursively:\nδ(Cm\n1 ) =δ(Cm−1\n1 ) + [sAED\ncm + βAED ·log p(cm|Cm−1\n1 )] (1)\nLikewise, for the decoding algorithm ALSD[13] of NT system,\nassume δt(Cm\n1 ) is the score of partial hypothesis Cm\n1 with\ntime stamp t and the LM log-posterior is adopted only when\na non-blank character is generated:\nδt(Cm\n1 ) =δt(Cm−1\n1 ) + [sNT\ncm,t + βNT ·log p(cm|Cm−1\n1 )] (2)\nsAED\ncm and sNT\ncm,t are the scores for character cm generated by\nAED and NT systems respectively while βAED and βNT are the\nhyper-parameters in decoding.\nB. Word-level N-gram LM integration\nFor character-level LMs, the log-posterior log p(cm|Cm−1\n1 )\nused in shallow fusionis always provided. For word-level LM,\nhowever, this log-posterior cannot be accessed directly. In-\nstead, we compute the character-level log-posterior as follow:\nlog p(cm|Cm−1\n1 ) = logP(Cm\n1 ) −log P(Cm−1\n1 ) (3)\nAs suggested in the equation, the word-level LM can be inte-\ngrated into shallow fusionas long as the sequential probability\nP(Cm\n1 ) of any character sequence Cm\n1 can be computed by\na word-level LM. Below we explain how P(Cm\n1 ) is obtained\nby a word-level N-gram LM.\nTo assess character sequence by a word-level LM, a\ncharacter-to-word conversion is ﬁrstly required. For languages\nwith explicit word boundary (a.k.a., blank) such as English,\nthe word sequence converted from the corresponding char-\nacter sequence is unique. However, for other languages like\nMandarin, the absence of clear word boundary means a\nFig. 1: Lattice built from character-level sequence 孙悟空\nand vocabulary V= {孙, 悟，空, 悟空, 孙悟空}. All input\nsymbols are in word-level.\ncharacter sequence can be converted into multiple different\nword sequences. E.g., the Mandarin character sequence 孙悟\n空 can be mapped to word sequences [孙,悟,空], [孙,悟空] and\n[孙悟空]. Note we assume word 孙悟 is not a valid Mandarin\nword so the sequence [孙悟,空] is invalid and not considered.\nFormally, note f as a function that converts character-level\nsequence Cm\n1 into a word-level sequence Wnf\n1 :\nWnf\n1 = f(Cm\n1 ) = [w1,...,w nf ] ∀wi ∈V (4)\nwhere Vis a known word vocabulary and nf is the length of\nthe word-level sequence that depends on f. Thus, the probabil-\nity of the character-level sequence P(Cm\n1 ) is formulated as the\nweighted sum probability of all possible word-level sequence\nWnf\n1 :\nP(Cm\n1 ) =\n∑\nf∈F\nP(f)P(f(Cm\n1 )) =\n∑\nf∈F\nP(f)P(Wnf\n1 ) (5)\nwhere ∑\nf P(f) = 1and Fis the set of all valid character-\nto-word conversion functions.\nTo fully explore all possible word sequences Wnf\n1 and their\ncorresponding character-to-word conversion functions f in Eq.\n5, a lattice-based method is proposed. The character sequence\nCm\n1 is ﬁrstly transformed into a lattice Q(which is also a\nWFSA). For any word sequence Wnf\n1 , there is one and only\none path in the lattice whose input label sequence is exactly\nWnf\n1 . Thus, the whole lattice is a representation of all possible\ncharacter-to-word conversion results. An example lattice of\ncharacter sequence 孙悟空 is shown in Fig.1.\nGiven the character sequence Cm\n1 and word vocabulary V,\nthe detailed building process of the lattice is in Algorithm 1.\nFor any continuous r characters Cs+r−1\ns , if they can form a\nword in V(a.k.a, w= Cs+r−1\ns ∈V), an arc from state s−1\nto state s+ r −1 is added to the arc set A. The weights\non all these arcs are 0. Once all valid s and r are explored,\nthe lattice is built from the arc set A. In addition, we ensure\nthat any character predicted by the ASR systems is also in the\nword vocabulary V(∀ci ∈V) so that all the out-of-vocabulary\n(OOV) words can always be considered as the sequence of\nsingle characters and the end state (state m−1) is always\nreachable.\nBefore decoding, the word-level N-gram LM is trained\nfollowing [14], [15] and is transformed into a WFSA (termed\nas G). In the decoding stage, once the lattice Qfor character\nsequence Cm\n1 is built, the character-level probability in eq.5 is\ncomputed through an FST-based method. A new WFSA QG\nis generated by the word lattice Qand the word N-gram G:\nQG= Q◦G (6)\nwhere ◦ denotes intersect operation[16] in WFSA. Before\nintersect, we add self-loops with 0 weight and epsilon input\nlabel to explore all backoff paths in G. Subsequently, the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nAlgorithm 1 Build lattice for character-level sequence Cm\n1\nRequire: character-level sequence Cm\n1 , word-level vocabu-\nlary V, maximum word length l.\narc set A←∅\nfor r= 1,..,l do\nfor s= 1,...,m −r do\nif Cs+r−1\ns ∈V then\nA←A∪ arc(s−1,s + r−1,Cs+r−1\ns ,0)\nend if\nend for\nend for\nreturn WFSA(A)\ncharacter-level sequence probability P(Cm\n1 ) is equivalent to\nthe forward score of the end state in QG.\nlog P(Cm\n1 ) = log\n∑\nf∈F\nP(f)P(Wnf\n1 ) =Forward Score(QG)\n(7)\nIn FST literature[17], semirings are the algebraic structures\nthat deﬁne the rule of forward score computation. In this work,\nwe consider two well-known semirings: tropical-semiring and\nLog-semiring. Tropical-semiring only considers the path with\nthe best accumulated score, which is equivalent to set:\nP(f) =\n{1 if f= arg max\nf′∈F\nP(f′(Cm\n1 ))\n0 otherwise\n}\n(8)\nOn the other hand, Log-semiring equally accumulates the score\nof every path that can be accepted by QG, which is equivalent\nto set:\nP(f) = 1\n|F| (9)\nIII. E XPERIMENTS\nIn this section, we ﬁrstly describe the experimental setup.\nThen the experimental results and analysis on three Mandarin\ndatasets are presented.\nA. Experimental setup\n1) Datasets: We examine our method on two popular open-\nsource Mandarin datasets: Aishell-1 (178 hours), Aishell-2 (1k\nhours). To further evaluate the effectiveness of the proposed\nmethod on large-scale ASR tasks, we also apply our method to\na 21k-hour Mandarin dataset and report its performance under\n6 test scenarios: reading, translation, guild, television, music,\neducation.\n2) E2E ASR baselines : We verify the effectiveness of\nour method on two of the most popular E2E ASR sys-\ntems: Attention-based Encoder-Decoder (AED) and Neural\nTransducer (NT). For AED, a Conformer[18] encoder and a\nTransformer[19] decoder are adopted, which consumes 46M\nparameters; for NT, a Conformer encoder, an LSTM prediction\nnetwork and an MLP joint network are used (89M parameters).\nFollowing our previous work [20], the LF-MMI criterion[21],\n[22] is adopted in the training stage of the two systems for\nbetter performance. In the decoding stage, we adopt the MMI\nPreﬁx Score and MMI Alignment Score algorithms for AED\nand NT respectively [20]. The beam size of decoding is ﬁxed\nto 10 and the decoding algorithms used in AED and NT\nsystems are described in [2] and [13]. The E2E ASR baselines\nare mainly implemented by Espnet [23].\n3) Language Models : We compare the performance of three\ntypes of LMs: word-level N-gram LM, character-level N-gram\nLM and character-level neural network LM (NNLM). All LMs\nused in Aishell-1 and Aishell-2 are trained from transcriptions\nof the training set and adopt no external resources. An addi-\ntional text corpus is used in the 21k-hour experiments. Before\ntraining the word-level LM, the transcriptions are segmented\ninto word-level using jieba 1. All N-gram LMs are trained by\nkaldi lm2 while all NNLMs use a standard 1-layer LSTM\nwith the hidden size of 650. Unless otherwise speciﬁed, the\ndefault settings for the decoding are described as follows. The\nLM weight of shallow fusion method (a.k.a, βAED and βNT)\nis ﬁxed to 0.4 when using word-level N-gram LM. However,\nwe enumerate the LM weights of character-level N-gram LMs\nand NNLMs in set {0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 2.0 }and\nreport the best results. Also, the default order of word-level\nN-gram LM is 3 and the log-semiring is used. The WFSA-\nrelated operations are implemented by K2 3.\nB. Results on Aishell-1 and Aishell-2\nWe present our experimental results on Aishell-1 and\nAishell-2 in table I. Our main observations are listed below.\n1) LM weight of shallow fusion : The LM weight of shallow\nfusion is the ﬁrst hyper-parameter we investigate. As shown in\nexp.1-6 and exp.12-17, LM weight of 0.4 consistently shows\nthe best improvement on both ASR systems, which explains\nour default choice of this weight. Also, our method is robust to\nthe choice of LM weight: compared with the baseline systems\n(exp.1, exp.12), CER reduction is consistently observed when\nthe LM weight is smaller or equal to 0.8.\n2) Semiring: For both E2E ASR frameworks, the performance\ndifference caused by the choice of the semiring is negligible\n(exp.3 vs. exp.7; exp.14 vs. exp.18).\n3) Order of word-level N-gram LM : We ﬁnd word-level\nN-gram LMs of higher order do not show further perfor-\nmance improvement (exp.3 vs. exp.8; exp.14 vs. exp.19). One\nexplanation of this is that: since we only use the training\ntranscriptions of Aishell datasets, the higher-order terms (e.g.,\n4-gram terms) seen in LM training are rarely seen in the test\nset, as the training transcriptions are small in volume and\nachieve poor coverage.\n4) Comparison with Word N-gram Rescore : The sequence-\nlevel LM scores computed by Eq.5 can also be used for\nrescoring after the N-best hypotheses are proposed by ASR\nsystems. However, slight performance degradation is observed\nif the word N-gram LM is adopted in rescoring rather than in\non-the-ﬂy decoding (exp.3 vs. exp.9; exp.14 vs. exp.20).\n5) Comparison with character-level LM : We compare the\nperformance of our method with those character-level LMs,\nincluding both the N-gram LM and the neural network LM\n(exp.3 vs. exp.10-11; exp.14 vs. exp.21-22). Although the\ncharacter-level LMs do provide some improvement on the\n1https://github.com/fxsjy/jieba\n2https://github.com/danpovey/kaldi lm\n3https://github.com/k2-fsa/k2\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nNo. System LM type N-gram order Semiring LM weight Aishell-1 Aishell-2 RTF(CPU)4\ndev test ios android mic Aishell2-ios\n1\nAED\n- - - 0.0 4.55 5.10 5.93 7.04 6.79 1.45\n2\nWord N-gram\n3 log (Eq.9)\n0.2 4.22 4.62 5.43 6.48 6.21 2.03\n3 0.4 4.08 4.45 5.26 6.22 5.92 2.02\n4 0.6 4.09 4.43 5.27 6.22 5.99 2.00\n5 0.8 4.20 4.58 5.56 6.53 6.26 2.03\n6 1.0 4.42 4.80 6.00 6.94 6.77 2.04\n7 3 tropical (Eq.8) 0.4 4.08 4.45 5.26 6.22 5.92 2.01\n8 4 log (Eq.9) 0.4 4.08 4.46 5.26 6.22 5.93 2.08\n9 Word N-gram Rescore 3 log (Eq.9) (0, 2.0] 4.11 4.47 5.31 6.27 6.03 1.45\n10 Char. N-gram 6 - (0, 2.0] 4.24 4.63 5.25 6.17 5.94 1.60\n11 Char. NN - - (0, 2.0] 4.41 4.64 5.79 6.77 6.51 1.87\n12\nNT\n- - - 0.0 4.20 4.60 5.41 6.56 6.39 0.50\n13\nWord N-gram\n3 log (Eq.9)\n0.2 3.97 4.29 5.13 6.15 6.05 4.12\n14 0.4 3.87 4.18 5.06 6.08 5.98 4.48\n15 0.6 3.87 4.22 5.16 6.23 6.09 4.43\n16 0.8 4.00 4.33 5.37 6.36 6.23 4.25\n17 1.0 4.25 4.55 5.65 6.71 6.55 4.26\n18 3 tropical (Eq.8) 0.4 3.87 4.18 5.06 6.08 5.98 4.59\n19 4 log (Eq.9) 0.4 3.87 4.18 5.06 6.08 6.00 4.50\n20 Word N-gram Rescore 3 log (Eq.9) (0, 2.0] 3.93 4.27 5.11 6.08 6.02 0.50\n21 Char. N-gram 6 - (0, 2.0] 4.11 4.23 5.28 6.36 6.27 0.90\n22 Char. NN - - (0, 2.0] 4.09 4.43 5.33 6.41 6.28 1.62\nTABLE I: Results on Aishell-1 and Aishell-2 datasets. CER and RTF are reported. All LMs are adopted using shallow fusion.\nrecognition performance, our word-level N-gram LM outper-\nforms the character-level counterparts in most test sets and\nframeworks. This observation validates our claim that the\nword-level information is beneﬁcial during decoding.\n6) Different E2E ASR frameworks : The results suggest that\nthe proposed method is compatible with both AED and NT\nsystems and achieves performance improvement consistently\n(exp.1,3 vs. exp.12,14).\n7) Real-time factor (RTF) : Besides the recognition accuracy\n(CER%), the real-time factor (RTF) is also concerned. We ﬁnd\nthat hyper-parameters like LM weight, LM order and semiring\nhave negligible impact on the computational overhead. For\nAED, the computational overhead is close to that of NNLM\n(2.02 to 1.87). For NT, the RTF of our method is 4.48. The\ncomputational overhead on the NT system is much higher than\nthat of the AED system since the NT systems assess the partial\nhypotheses for each frame and each word sequence length. The\nacceleration of our method on NT is left for future work.\nSystem Aishell-1 Aishell-2\ndev/test ios/android/mic\nSpeechBrain [24] - / 5.58 - / - / -\nEspnet [25] 4.4 / 4.7 6.8 / 7.6 / 7.4\nWenet [26] - / 4.36 5.35 / - / -\nIcefall - / 4.26 - / - / -\nOurs\nAED 4.55 / 5.10 5.93 / 7.04 / 6.79\n+ Word N-gram 4.08 / 4.45 5.26 / 6.22 / 5.92\nNT 4.20 / 4.60 5.41 / 6.56 / 6.39\n+ Word N-gram 3.86 / 4.18 5.06 / 6.08 / 5.98\nTABLE II: Results on Aishell-1 and Aishell-2 datasets. All\nresults are in CER%. No external resources are used.\nWe ﬁnally conclude our experimental results and compare\nthem with other competitive frameworks in table II. As sug-\ngested in the table, signiﬁcant CER reduction is consistently\nobserved in both E2E ASR frameworks and various test sets.\nThe maximum and minimum absolute CER reduction are\n4single thread; Pytorch implementation; Intel Xeon 8255C CPU, 2.5GHz\n0.87% (AED, Aishell-2 mic) and 0.34% (NT, Aishell-1 dev)\nrespectively. The NT model with our word-level N-gram LM\nachieves all of the best results except that on the Aishell-2\nmic set. Our best results on Aishell-1 test set and Aishell-2\ntest-ios set are 4.18% and 5.06% respectively. To the best of\nour knowledge, these are the state-of-the-art results on these\ntwo datasets. The success of our method emphasizes that the\nadoption of word-level N-gram LM is promising.\nC. Results on 21k-hour Mandarin dataset\nWe show that the proposed method is still effective for large-\nscale ASR tasks and external text corpus. Speciﬁcally, we train\nan NT model on the 21k-hour speech data and the word-level\nLM (3-gram, with the size of 3.4G) is constructed from an\nexternal text corpus. As shown in table III, incorporating word-\nlevel N-gram LM based on the proposed method consistently\nachieves better recognition performance on all 6 test sets.\nThe absolute CER reduction is 1.1% on average while the\nmaximum relative CER reduction is 14.8% (in reading set).\nTest set re tr gu tv mu ed Mean\nNT 5.4 17.8 19.0 9.2 14.5 11.8 13.0\n+ Word N-gram 4.6 16.4 17.0 8.4 13.3 11.4 11.9\nTABLE III: Experimental results on a 21k-hour Mandarin\nwith w/o external word-level N-gram LM. re: reading; tr:\ntranslation; gu: guild; tv: television; mu: music; ed: education.\nIV. C ONCLUSION\nIn this work, we propose a new method to integrate the\nword-level N-gram language models into the decoding process\nof end-to-end automatic speech recognition systems. Com-\npared with the language models that work in subword-level,\nour method consistently achieves better performance. Also, our\nmethod is robust to various hyper-parameters and is applicable\nto large-scale ASR tasks. Experimentally, we achieve state-of-\nthe-art results on two of the most popular Mandarin datasets.\n5This research is partially supported by NSFC (No:6217021843)\nand Shenzhen Science & Technology Fundamental Research Programs\n(No:JCYJ20180507182908274 & JSGG20191129105421211).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nREFERENCES\n[1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A\nneural network for large vocabulary conversational speech recognition,”\nin 2016 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2016, pp. 4960–4964.\n[2] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hy-\nbrid ctc/attention architecture for end-to-end speech recognition,” IEEE\nJournal of Selected Topics in Signal Processing, vol. 11, no. 8, pp.\n1240–1253, 2017.\n[3] A. Graves, “Sequence transduction with recurrent neural networks,”\narXiv preprint arXiv:1211.3711, 2012.\n[4] S. Toshniwal, A. Kannan, C.-C. Chiu, Y . Wu, T. N. Sainath, and\nK. Livescu, “A comparison of techniques for language model integration\nin encoder-decoder speech recognition,” in 2018 IEEE Spoken Language\nTechnology Workshop (SLT), 2018, pp. 369–375.\n[5] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y . Bengio, “On using monolingual\ncorpora in neural machine translation,”arXiv preprint arXiv:1503.03535,\n2015.\n[6] A. Sriram, H. Jun, S. Satheesh, and A. Coates, “Cold fusion: Train-\ning seq2seq models together with language models,” arXiv preprint\narXiv:1708.06426, 2017.\n[7] W. Michel, R. Schl ¨uter, and H. Ney, “Early stage LM integration\nusing local and global log-linear combination,” in Interspeech 2020,\n21st Annual Conference of the International Speech Communication\nAssociation, Virtual Event, Shanghai, China, 25-29 October 2020, 2020,\npp. 3605–3609.\n[8] Y . Shibata, T. Kida, S. Fukamachi, M. Takeda, A. Shinohara, T. Shino-\nhara, and S. Arikawa, “Byte pair encoding: A text compression scheme\nthat accelerates pattern matching,” 1999.\n[9] T. Hori, S. Watanabe, and J. R. Hershey, “Multi-level language modeling\nand decoding for open vocabulary end-to-end speech recognition,” in\n2017 IEEE Automatic Speech Recognition and Understanding Workshop\n(ASRU), 2017, pp. 287–293.\n[10] T. Hori, J. Cho, and S. Watanabe, “End-to-end speech recognition with\nword-based rnn language models,” in 2018 IEEE Spoken Language\nTechnology Workshop (SLT), 2018, pp. 389–396.\n[11] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An open-\nsource mandarin speech corpus and a speech recognition baseline,”\nin 2017 20th Conference of the Oriental Chapter of the International\nCoordinating Committee on Speech Databases and Speech I/O Systems\nand Assessment (O-COCOSDA), 2017, pp. 1–5.\n[12] J. Du, X. Na, X. Liu, and H. Bu, “Aishell-2: Transforming mandarin asr\nresearch into industrial scale,” arXiv preprint arXiv:1808.10583, 2018.\n[13] G. Saon, Z. T ¨uske, and K. Audhkhasi, “Alignment-length synchronous\ndecoding for rnn transducer,” inICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2020,\npp. 7804–7808.\n[14] C. Manning and H. Schutze, Foundations of statistical natural language\nprocessing. MIT press, 1999.\n[15] A. Stolcke, “Srilm-an extensible language modeling toolkit,” in Seventh\ninternational conference on spoken language processing, 2002.\n[16] M. Mohri, F. Pereira, and M. Riley, “Weighted ﬁnite-state transducers\nin speech recognition,” Computer Speech & Language, vol. 16, no. 1,\npp. 69–88, 2002.\n[17] T. Hori and A. Nakamura, “Speech recognition algorithms using\nweighted ﬁnite-state transducers,” Synthesis Lectures on Speech and\nAudio Processing, vol. 9, no. 1, pp. 1–162, 2013.\n[18] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y . Wu et al., “Conformer: Convolution-augmented\ntransformer for speech recognition,” 2020.\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems, 2017, pp. 5998–6008.\n[20] J. Tian, J. Yu, C. Weng, S.-X. Zhang, D. Su, D. Yu, and Y . Zou,\n“Consistent training and decoding for end-to-end speech recognition\nusing lattice-free mmi,” 2021.\n[21] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, V . Manohar, X. Na,\nY . Wang, and S. Khudanpur, “Purely sequence-trained neural networks\nfor asr based on lattice-free mmi,” in Interspeech 2016, 2016, pp.\n2751–2755. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.\n2016-595\n[22] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, “End-to-end speech\nrecognition using lattice-free mmi,” in Proc. Interspeech 2018, 2018,\npp. 12–16. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.\n2018-1423\n[23] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y . Unno,\nN. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen,\nA. Renduchintala, and T. Ochiai, “ESPnet: End-to-end speech processing\ntoolkit,” in Proceedings of Interspeech, 2018, pp. 2207–2211. [Online].\nAvailable: http://dx.doi.org/10.21437/Interspeech.2018-1456\n[24] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lu-\ngosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou,\nS.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris,\nH. Na, Y . Gao, R. D. Mori, and Y . Bengio, “SpeechBrain: A general-\npurpose speech toolkit,” 2021, arXiv:2106.04624.\n[25] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y . Higuchi, H. Inaguma,\nN. Kamo, C. Li, D. Garcia-Romero, J. Shi, J. Shi, S. Watanabe, K. Wei,\nW. Zhang, and Y . Zhang, “Recent developments on espnet toolkit\nboosted by conformer,” in ICASSP 2021 - 2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2021,\npp. 5874–5878.\n[26] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen,\nL. Xie, and X. Lei, “Wenet: Production oriented streaming and non-\nstreaming end-to-end speech recognition toolkit,” in Proc. Interspeech.\nBrno, Czech Republic: IEEE, 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7721073627471924
    },
    {
      "name": "Mandarin Chinese",
      "score": 0.756956160068512
    },
    {
      "name": "Language model",
      "score": 0.6610949635505676
    },
    {
      "name": "Speech recognition",
      "score": 0.6566889882087708
    },
    {
      "name": "Word (group theory)",
      "score": 0.6340386867523193
    },
    {
      "name": "Decoding methods",
      "score": 0.5851013660430908
    },
    {
      "name": "Encoder",
      "score": 0.5168431401252747
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41868114471435547
    },
    {
      "name": "Algorithm",
      "score": 0.15290164947509766
    },
    {
      "name": "Mathematics",
      "score": 0.12352865934371948
    },
    {
      "name": "Linguistics",
      "score": 0.0675070583820343
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 13
}