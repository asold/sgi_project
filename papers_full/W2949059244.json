{
  "title": "Compressing Neural Language Models by Sparse Word Representations",
  "url": "https://openalex.org/W2949059244",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5049517695",
      "name": "Yunchuan Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024821632",
      "name": "Lili Mou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100378208",
      "name": "Yan Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100447691",
      "name": "Ge Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5049100391",
      "name": "Zhi Jin",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2085400714",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2217098601",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2950967261",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2295800168",
    "https://openalex.org/W2140610559",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2250189634",
    "https://openalex.org/W1512874001",
    "https://openalex.org/W2384495648",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2952822287"
  ],
  "abstract": "Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.",
  "full_text": "Compressing Neural Language Models by Sparse Word Representations\nYunchuan Chen,1,2 Lili Mou,1,3 Yan Xu,1,3 Ge Li,1,3 Zhi Jin1,3,∗\n1Key Laboratory of High Conﬁdence Software Technologies (Peking University), MoE, China\n2University of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn\n3Institute of Software, Peking University, doublepower.mou@gmail.com,\n{xuyan14,lige,zhijin}@pku.edu.cn ∗Corresponding author\nAbstract\nNeural networks are among the state-of-\nthe-art techniques for language modeling.\nExisting neural language models typically\nmap discrete words to distributed, dense\nvector representations. After information\nprocessing of the preceding context words\nby hidden layers, an output layer estimates\nthe probability of the next word. Such ap-\nproaches are time- and memory-intensive\nbecause of the large numbers of parame-\nters for word embeddings and the output\nlayer. In this paper, we propose to com-\npress neural language models by sparse\nword representations. In the experiments,\nthe number of parameters in our model in-\ncreases very slowly with the growth of the\nvocabulary size, which is almost imper-\nceptible. Moreover, our approach not only\nreduces the parameter space to a large ex-\ntent, but also improves the performance in\nterms of the perplexity measure.1\n1 Introduction\nLanguage models (LMs) play an important role\nin a variety of applications in natural language\nprocessing (NLP), including speech recognition\nand document recognition. In recent years, neu-\nral network-based LMs have achieved signiﬁ-\ncant breakthroughs: they can model language\nmore precisely than traditional n-gram statistics\n(Mikolov et al., 2011); it is even possible to gen-\nerate new sentences from a neural LM, beneﬁt-\ning various downstream tasks like machine trans-\nlation, summarization, and dialogue systems (De-\nvlin et al., 2014; Rush et al., 2015; Sordoni et al.,\n2015; Mou et al., 2015b).\n1Code released on https://github.com/chenych11/lm\nExisting neural LMs typically map a discrete\nword to a distributed, real-valued vector repre-\nsentation (called embedding) and use a neural\nmodel to predict the probability of each word\nin a sentence. Such approaches necessitate a\nlarge number of parameters to represent the em-\nbeddings and the output layer’s weights, which\nis unfavorable in many scenarios. First, with a\nwider application of neural networks in resource-\nrestricted systems (Hinton et al., 2015), such ap-\nproach is too memory-consuming and may fail to\nbe deployed in mobile phones or embedded sys-\ntems. Second, as each word is assigned with a\ndense vector—which is tuned by gradient-based\nmethods—neural LMs are unlikely to learn mean-\ningful representations for infrequent words. The\nreason is that infrequent words’ gradient is only\noccasionally computed during training; thus their\nvector representations can hardly been tuned ade-\nquately.\nIn this paper, we propose a compressed neural\nlanguage model where we can reduce the number\nof parameters to a large extent. To accomplish this,\nwe ﬁrst represent infrequent words’ embeddings\nwith frequent words’ by sparse linear combina-\ntions. This is inspired by the observation that, in a\ndictionary, an unfamiliar word is typically deﬁned\nby common words. We therefore propose an op-\ntimization objective to compute the sparse codes\nof infrequent words. The property of sparseness\n(only 4–8 values for each word) ensures the efﬁ-\nciency of our model.\nBased on the pre-computed sparse codes, we\ndesign our compressed language model as follows.\nA dense embedding is assigned to each common\nword; an infrequent word, on the other hand, com-\nputes its vector representation by a sparse combi-\nnation of common words’ embeddings. We use\nthe long short term memory (LSTM)-based recur-\nrent neural network (RNN) as the hidden layer of\nIn Proc. ACL, pages 226–235, 2016.\narXiv:1610.03950v1  [cs.CL]  13 Oct 2016\nour model. The weights of the output layer are\nalso compressed in a same way as embeddings.\nConsequently, the number of trainable neural pa-\nrameters is a constant regardless of the vocabulary\nsize if we ignore the biases of words. Even con-\nsidering sparse codes (which are very small), we\nﬁnd the memory consumption grows impercepti-\nbly with respect to the vocabulary.\nWe evaluate our LM on the Wikipedia corpus\ncontaining up to 1.6 billion words. During train-\ning, we adopt noise-contrastive estimation (NCE)\n(Gutmann and Hyv ¨arinen, 2012) to estimate the\nparameters of our neural LMs. However, dif-\nferent from Mnih and Teh (2012), we tailor the\nNCE method by adding a regression layer (called\nZRegressoion) to predict the normalization\nfactor, which stabilizes the training process. Ex-\nperimental results show that, our compressed LM\nnot only reduces the memory consumption, but\nalso improves the performance in terms of the per-\nplexity measure.\nTo sum up, the main contributions of this paper\nare three-fold. (1) We propose an approach to rep-\nresent uncommon words’ embeddings by a sparse\nlinear combination of common ones’. (2) We pro-\npose a compressed neural language model based\non the pre-computed sparse codes. The memory\nincreases very slowly with the vocabulary size (4–\n8 values for each word). (3) We further introduce a\nZRegression mechanism to stabilize the NCE\nalgorithm, which is potentially applicable to other\nLMs in general.\n2 Background\n2.1 Standard Neural LMs\nLanguage modeling aims to minimize the joint\nprobability of a corpus (Jurafsky and Martin,\n2014). Traditional n-gram models impose a\nMarkov assumption that a word is only depen-\ndent on previous n−1 words and independent of\nits position. When estimating the parameters, re-\nsearchers have proposed various smoothing tech-\nniques including back-off models to alleviate the\nproblem of data sparsity.\nBengio et al. (2003) propose to use a feed-\nforward neural network (FFNN) to replace the\nmultinomial parameter estimation inn-gram mod-\nels. Recurrent neural networks (RNNs) can also be\nused for language modeling; they are especially\ncapable of capturing long range dependencies in\nsentences (Mikolov et al., 2010; Sundermeyer et\nFigure 1: The architecture of a neural network-\nbased language model.\nal., 2015).\nIn the above models, we can view that a neural\nLM is composed of three main parts, namely the\nEmbedding, Encoding, and Prediction\nsubnets, as shown in Figure 1.\nThe Embedding subnet maps a word to a\ndense vector, representing some abstract features\nof the word (Mikolov et al., 2013). Note that this\nsubnet usually accepts a list of words (known as\nhistory or context words) and outputs a sequence\nof word embeddings.\nThe Encoding subnet encodes the history of a\ntarget word into a dense vector (known as context\nor history representation). We may either leverage\nFFNNs (Bengio et al., 2003) or RNNs (Mikolov\net al., 2010) as the Encoding subnet, but RNNs\ntypically yield a better performance (Sundermeyer\net al., 2015).\nThe Prediction subnet outputs a distribu-\ntion of target words as\np(w= wi|h) = exp(s(h,wi))∑\nj exp(s(h,wj)), (1)\ns(h,wi) =W⊤\ni h+ bi, (2)\nwhere h is the vector representation of con-\ntext/history h, obtained by the Encoding subnet.\nW = (W1,W2,..., WV) ∈RC×V is the output\nweights of Prediction; b= (b1,b2,...,b V) ∈\nRC is the bias (the prior). s(h,wi) is a scoring\nfunction indicating the degree to which the context\nhmatches a target word wi. ( V is the size of vo-\ncabulary V; Cis the dimension of context/history,\ngiven by the Encoding subnet.)\n2.2 Complexity Concerns of Neural LMs\nNeural network-based LMs can capture more pre-\ncise semantics of natural language than n-gram\nmodels because the regularity of the Embedding\nsubnet extracts meaningful semantics of a word\nand the high capacity of Encoding subnet en-\nables complicated information processing.\nDespite these, neural LMs also suffer from sev-\neral disadvantages mainly out of complexity con-\ncerns.\nTime complexity. Training neural LMs is typi-\ncally time-consuming especially when the vocab-\nulary size is large. The normalization factor in\nEquation (1) contributes most to time complex-\nity. Morin and Bengio (2005) propose hierar-\nchical softmax by using a Bayesian network so\nthat the probability is self-normalized. Sampling\ntechniques—for example, importance sampling\n(Bengio and Sen´ecal, 2003), noise-contrastive es-\ntimation (Gutmann and Hyv¨arinen, 2012), and tar-\nget sampling (Jean et al., 2014)—are applied to\navoid computation over the entire vocabulary. In-\nfrequent normalization maximizes the unnormal-\nized likelihood with a penalty term that favors nor-\nmalized predictions (Andreas and Klein, 2014).\nMemory complexity and model complexity. The\nnumber of parameters in the Embedding and\nPrediction subnets in neural LMs increases\nlinearly with respect to the vocabulary size, which\nis large (Table 1). As said in Section 1, this is\nsometimes unfavorable in memory-restricted sys-\ntems. Even with sufﬁcient hardware resources, it\nis problematic because we are unlikely to fully\ntune these parameters. Chen et al. (2015) pro-\npose the differentiated softmax model by assign-\ning fewer parameters to rare words than to fre-\nquent words. However, their approach only han-\ndles the output weights, i.e., W in Equation (2);\nthe input embeddings remain uncompressed in\ntheir approach.\nIn this work, we mainly focus on memory and\nmodel complexity, i.e., we propose a novel method\nto compress the Embedding and Prediction\nsubnets in neural language models.\n2.3 Related Work\nExisting work on model compression for neural\nnetworks. Buciluˇa et al. (2006) and Hinton et al.\n(2015) use a well-trained large network to guide\nthe training of a small network for model compres-\nsion. Jaderberg et al. (2014) compress neural mod-\nels by matrix factorization, Gong et al. (2014) by\nquantization. In NLP, Mou et al. (2015a) learn an\nembedding subspace by supervised training. Our\nwork resembles little, if any, to the above methods\nas we compress embeddings and output weights\nusing sparse word representations. Existing model\nSub-nets RNN-LSTM FFNN\nEmbedding V E V E\nEncoding 4(CE + C2 + C) nCE + C\nPrediction V (C + 1) V (C + 1)\nTOTAL † O((C + E)V ) O((E + C)V )\nTable 1: Number of parameters in different neural\nnetwork-based LMs. E: embedding dimension;\nC: context dimension; V: vocabulary size. †Note\nthat V ≫C(or E).\ncompression typically works with a compromise\nof performance. On the contrary, our model im-\nproves the perplexity measure after compression.\nSparse word representations. We leverage\nsparse codes of words to compress neural LMs.\nFaruqui et al. (2015) propose a sparse coding\nmethod to represent each word with a sparse vec-\ntor. They solve an optimization problem to ob-\ntain the sparse vectors of words as well as a dic-\ntionary matrix simultaneously. By contrast, we do\nnot estimate any dictionary matrix when learning\nsparse codes, which results in a simple and easy-\nto-optimize model.\n3 Our Proposed Model\nIn this section, we describe our compressed lan-\nguage model in detail. Subsection 3.1 formal-\nizes the sparse representation of words, serving\nas the premise of our model. On such a basis,\nwe compress theEmbedding and Prediction\nsubnets in Subsections 3.2 and 3.3, respectively.\nFinally, Subsection 3.4 introduces NCE for pa-\nrameter estimation where we further propose\nthe ZRegression mechanism to stabilize our\nmodel.\n3.1 Sparse Representations of Words\nWe split the vocabularyVinto two disjoint subsets\n(Band C). The ﬁrst subset Bis a base set, con-\ntaining a ﬁxed number of common words (8k in\nour experiments). C= V\\Bis a set of uncommon\nwords. We would like to useB’s word embeddings\nto encode C’s.\nOur intuition is that oftentimes a word can be\ndeﬁned by a few other words, and that rare words\nshould be deﬁned by common ones. Therefore,\nit is reasonable to use a few common words’ em-\nbeddings to represent that of a rare word. Follow-\ning most work in the literature (Lee et al., 2006;\nYang et al., 2011), we represent each uncommon\nword with a sparse, linear combination of com-\nmon ones’ embeddings. The sparse coefﬁcients\nare called a sparse code for a given word.\nWe ﬁrst train a word representation model like\nSkipGram (Mikolov et al., 2013) to obtain a set of\nembeddings for each word in the vocabulary, in-\ncluding both common words and rare words. Sup-\npose U = (U1,U2,..., UB) ∈ RE×B is the\n(learned) embedding matrix of common words,\ni.e., Ui is the embedding of i-th word in B. (Here,\nB = |B|.)\nEach word in Bhas a natural sparse code (de-\nnoted as x): it is a one-hot vector withBelements,\nthe i-th dimension being on for the i-th word in B.\nFor a wordw∈C, we shall learn a sparse vector\nx = (x1,x2,...,x B) as the sparse code of the\nword. Provided that x has been learned (which\nwill be introduced shortly), the embedding of wis\nˆw=\nB∑\nj=1\nxjUj = Ux, (3)\nTo learn the sparse representation of a certain\nword w, we propose the following optimization\nobjective\nmin\nx\n∥Ux−w∥2\n2 + α∥x∥1 + β|1⊤x−1|\n+ γ1⊤max{0,−x}, (4)\nwhere max denotes the component-wise maxi-\nmum; wis the embedding for a rare word w∈C.\nThe ﬁrst term (called ﬁtting loss afterwards)\nevaluates the closeness between a word’s coded\nvector representation and its “true” representation\nw, which is the general goal of sparse coding.\nThe second term is an ℓ1 regularizer, which en-\ncourages a sparse solution. The last two regular-\nization terms favor a solution that sums to 1 and\nthat is nonnegative, respectively. The nonnegative\nregularizer is applied as in He et al. (2012) due to\npsychological interpretation concerns.\nIt is difﬁcult to determine the hyperparameters\nα, β, and γ. Therefore we perform several tricks.\nFirst, we drop the last term in the problem (4), but\nclip each element in xso that all the sparse codes\nare nonnegative during each update of training.\nSecond, we re-parametrize αand β by balanc-\ning the ﬁtting loss and regularization terms dy-\nnamically during training. Concretely, we solve\nthe following optimization problem, which is\nslightly different but closely related to the concep-\ntual objective (4):\nmin\nx\nL(x) +αtR1(x) +βtR2(x), (5)\nwhere L(x) =∥Ux−w∥2\n2, R1(x) =∥x∥1, and\nR2(x) =|1⊤x−1|. αtand βtare adaptive param-\neters that are resolved during training time. Sup-\npose xt is the value we obtain after the update of\nthe t-th step, we expect the importance of ﬁtness\nand regularization remain unchanged during train-\ning. This is equivalent to\nαtR1(xt)\nL(xt) = wα ≡const, (6)\nβtR2(xt)\nL(xt) = wβ ≡const. (7)\nor\nαt = L(xt)\nR1(xt)wα and βt = L(xt)\nR2(xt)wβ,\nwhere wα and wβ are the ratios between the regu-\nlarization loss and the ﬁtting loss. They are much\neasier to specify than αor βin the problem (4).\nWe have two remarks as follows.\n•To learn the sparse codes, we ﬁrst train the\n“true” embeddings by word2vec2 for both\ncommon words and rare words. However,\nthese true embeddings are slacked during our\nlanguage modeling.\n•As the codes are pre-computed and remain\nunchanged during language modeling, they\nare not tunable parameters of our neural\nmodel. Considering the learned sparse codes,\nwe need only 4–8 values for each word on av-\nerage, as the codes contain 0.05–0.1% non-\nzero values, which are almost negligible.\n3.2 Parameter Compression for the\nEmbedding Subnet\nOne main source of LM parameters is the\nEmbedding subnet, which takes a list of words\n(history/context) as input, and outputs dense, low-\ndimensional vector representations of the words.\nWe leverage the sparse representation of words\nmentioned above to construct a compressed\nEmbedding subnet, where the number of param-\neters is independent of the vocabulary size.\nBy solving the optimization problem (5) for\neach word, we obtain a non-negative sparse code\nx ∈RB for each word, indicating the degree to\nwhich the word is related to common words in\nB. Then the embedding of a word is given by\nˆw= Ux.\n2https://code.google.com/archive/p/word2vec\nWe would like to point out that the embedding\nof a word ˆwis not sparse becauseUis a dense ma-\ntrix, which serves as a shared parameter of learn-\ning all words’ vector representations.\n3.3 Parameter Compression for the\nPrediction Subnet\nAnother main source of parameters is the\nPrediction subnet. As Table 1 shows, the out-\nput layer contains V target-word weight vectors\nand biases; the number increases with the vocabu-\nlary size. To compress this part of a neural LM, we\npropose a weight-sharing method that uses words’\nsparse representations again. Similar to the com-\npression of word embeddings, we deﬁne a base set\nof weight vectors, and use them to represent the\nrest weights by sparse linear combinations.\nWithout loss of generality, we let D = W:,1:B\nbe the output weights of Bbase target words, and\nc= b1:B be bias of the Btarget words.3 The goal\nis to use D and cto represent W and b. How-\never, as the values ofWand bare unknown before\nthe training of LM, we cannot obtain their sparse\ncodes in advance.\nWe claim that it is reasonable to share the\nsame set of sparse codes to represent word vec-\ntors in Embedding and the output weights in\nthe Prediction subnet. In a given corpus, an\noccurrence of a word is always companied by\nits context. The co-occurrence statistics about a\nword or corresponding context are the same. As\nboth word embedding and context vectors cap-\nture these co-occurrence statistics (Levy and Gold-\nberg, 2014), we can expect that context vec-\ntors share the same internal structure as embed-\ndings. Moreover, for a ﬁne-trained network, given\nany word w and its context h, the output layer’s\nweight vector corresponding to w should spec-\nify a large inner-product score for the context h;\nthus these context vectors should approximate the\nweight vector of w. Therefore, word embed-\ndings and the output weight vectors should share\nthe same internal structures and it is plausible to\nuse a same set of sparse representations for both\nwords and target-word weight vectors. As we shall\nshow in Section 4, our treatment of compressing\nthe Prediction subnet does make sense and\nachieves high performance.\nFormally, the i-th output weight vector is esti-\nmated by\nˆWi = Dxi, (8)\n3 W:,1:B is the ﬁrst B columns of W.\nFigure 2: Compressing the output of neural LM.\nWe apply NCE to estimate the parameters of the\nPrediction sub-network (dashed round rectan-\ngle). The SpUnnrmProb layer outputs a sparse,\nunnormalized probability of the next word. By\n“sparsity,” we mean that, in NCE, the probability\nis computed for only the “true” next word (red)\nand a few generated negative samples.\nThe biases can also be compressed as\nˆbi = cxi. (9)\nwhere xi is the sparse representation of the i-th\nword. (It is shared in the compression of weights\nand biases.)\nIn the above model, we have managed to com-\npressed a language model whose number of pa-\nrameters is irrelevant to the vocabulary size.\nTo better estimate a “prior” distribution of\nwords, we may alternatively assign an indepen-\ndent bias to each word, i.e., bis not compressed.\nIn this variant, the number of model parameters\ngrows very slowly and is also negligible because\neach word needs only one extra parameter. Exper-\nimental results show that by not compressing the\nbias vector, we can even improve the performance\nwhile compressing LMs.\n3.4 Noise-Contrastive Estimation with\nZRegression\nWe adopt the noise-contrastive estimation (NCE)\nmethod to train our model. Compared with the\nmaximum likelihood estimation of softmax, NCE\nreduces computational complexity to a large de-\ngree. We further propose the ZRegression\nmechanism to stablize training.\nNCE generates a few negative samples for each\npositive data sample. During training, we only\nneed to compute the unnormalized probability of\nthese positive and negative samples. Interested\nreaders are referred to (Gutmann and Hyv ¨arinen,\n2012) for more information.\nFormally, the estimated probability of the word\nwi with history/context his\nP(w|h; θ) = 1\nZh\nP0(wi|h; θ)\n= 1\nZh\nexp(s(wi,h; θ)), (10)\nwhere θ is the parameters and Zh is a context-\ndependent normalization factor. P0(wi|h; θ) is\nthe unnormalized probability of the w (given by\nthe SpUnnrmProb layer in Figure 2).\nThe NCE algorithm suggests to take Zh as pa-\nrameters to optimize along with θ, but it is in-\ntractable for context with variable lengths or large\nsizes in language modeling. Following Mnih and\nTeh (2012), we set Zh = 1 for all h in the base\nmodel (without ZRegression).\nThe objective for each occurrence of con-\ntext/history his\nJ(θ|h) = log P(wi|h; θ)\nP(wi|h; θ) +kPn(wi)+\nk∑\nj=1\nlog kPn(wj)\nP(wj|h; θ) +kPn(wj),\nwhere Pn(w) is the probability of drawing a nega-\ntive samplew; kis the number of negative samples\nthat we draw for each positive sample.\nThe overall objective of NCE is\nJ(θ) =Eh[J(θ|h)] ≈ 1\nM\nM∑\ni=1\nJ(θ|hi),\nwhere hi is an occurrence of the context and M is\nthe total number of context occurrences.\nAlthough setting Zh to 1 generally works well\nin our experiment, we ﬁnd that in certain sce-\nnarios, the model is unstable. Experiments show\nthat when the true normalization factor is far away\nfrom 1, the cost function may vibrate. To com-\nply with NCE in general, we therefore propose a\nZRegression layer to predict the normalization\nconstant Zh dependent on h, instead of treating it\nas a constant.\nThe regression layer is computed by\nZ−1\nh = exp(W⊤\nZ h+ bZ),\nPartitions Running words\nTrain (n-gram) 1.6 B\nTrain (neural LMs) 100 M\nDev 100 K\nTest 5 M\nTable 2: Statistics of our corpus.\nwhere WZ ∈RC and bZ ∈R are weights and bias\nfor ZRegression. Hence, the estimated proba-\nbility by NCE with ZRegression is given by\nP(w|h) = exp(s(h,w)) ·exp(W⊤\nZ h+ bZ).\nNote that the ZRegression layer does not\nguarantee normalized probabilities. During val-\nidation and testing, we explicitly normalize the\nprobabilities by Equation (1).\n4 Evaluation\nIn this part, we ﬁrst describe our dataset in Subsec-\ntion 4.1. We evaluate our learned sparse codes of\nrare words in Subsection 4.2 and the compressed\nlanguage model in Subsection 4.3. Subsection 4.4\nprovides in-depth analysis of the ZRegression\nmechanism.\n4.1 Dataset\nWe used the freely available Wikipedia 4 dump\n(2014) as our dataset. We extracted plain sen-\ntences from the dump and removed all markups.\nWe further performed several steps of preprocess-\ning such as text normalization, sentence splitting,\nand tokenization. Sentences were randomly shuf-\nﬂed, so that no information across sentences could\nbe used, i.e., we did not consider cached language\nmodels. The resulting corpus contains about 1.6\nbillion running words.\nThe corpus was split into three parts for train-\ning, validation, and testing. As it is typically time-\nconsuming to train neural networks, we sampled a\nsubset of 100 million running words to train neu-\nral LMs, but the full training set was used to train\nthe backoff n-gram models. We chose hyperpa-\nrameters by the validation set and reported model\nperformance on the test set. Table 2 presents some\nstatistics of our dataset.\n4.2 Qualitative Analysis of Sparse Codes\nTo obtain words’ sparse codes, we chose 8k com-\nmon words as the “dictionary,” i.e., B = 8000.\n4http://en.wikipedia.org\nFigure 3: The sparse representations of selected\nwords. The x-axis is the dictionary of 8k common\nwords; the y-axis is the coefﬁcient of sparse cod-\ning. Note that algorithm, secret, and debate are\ncommon words, each being coded by itself with a\ncoefﬁcient of 1.\nWe had 2k–42k uncommon words in different set-\ntings. We ﬁrst pretrained word embeddings of\nboth rare and common words, and obtained 200d\nvectors U and win Equation (5). The dimension\nwas speciﬁed in advance and not tuned. As there\nis no analytic solution to the objective, we opti-\nmized it by Adam (Kingma and Ba, 2014), which\nis a gradient-based method. To ﬁlter out small co-\nefﬁcients around zero, we simply set a value to 0\nif it is less than 0.015 ·max{v∈x}. wα in Equa-\ntion (6) was set to 1 because we deemed ﬁtting loss\nand sparsity penalty are equally important. We set\nwβ in Equation (7) to 0.1, and this hyperparameter\nis insensitive.\nFigure 3 plots the sparse codes of a few selected\nwords. As we see, algorithm, secret, and debate\nare common words, and each is (sparsely) coded\nby itself with a coefﬁcient of 1. We further notice\nthat a rare word like algorithms has a sparse rep-\nresentation with only a few non-zero coefﬁcient.\nMoreover, the coefﬁcient in the code of al-\ngorithms—corresponding to the base word algo-\nrithm—is large ( ∼0.6), showing that the words\nalgorithm and algorithms are similar. Such phe-\nnomena are also observed with secret and debate.\nThe qualitative analysis demonstrates that our\napproach can indeed learn a sparse code of a word,\nand that the codes are meaningful.\n4.3 Quantitative Analysis of Compressed\nLanguage Models\nWe then used the pre-computed sparse codes to\ncompress neural LMs, which provides quantita-\ntive analysis of the learned sparse representations\nof words. We take perplexity as the performance\nmeasurement of a language model, which is de-\nﬁned by\nPPL = 2−1\nN\n∑N\ni=1 log2 p(wi|hi)\nwhere N is the number of running words in the\ntest corpus.\n4.3.1 Settings\nWe leveraged LSTM-RNN as theEncoding sub-\nnet, which is a prevailing class of neural networks\nfor language modeling (Sundermeyer et al., 2015;\nKarpathy et al., 2015). The hidden layer was 200d.\nWe used the Adam algorithm to train our neural\nmodels. The learning rate was chosen by valida-\ntion from {0.001,0.002,0.004,0.006,0.008}. Pa-\nrameters were updated with a mini-batch size of\n256 words. We trained neural LMs by NCE, where\nwe generated 50 negative samples for each pos-\nitive data sample in the corpus. All our model\nvariants and baselines were trained with the same\npre-deﬁned hyperparameters or tuned over a same\ncandidate set; thus our comparison is fair.\nWe list our compressed LMs and competing\nmethods as follows.\n•KN3. We adopted the modiﬁed Kneser-Ney\nsmoothing technique to train a 3-gram LM;\nwe used the SRILM toolkit (Stolcke and oth-\ners, 2002) in out experiment.\n•LBL5. A Log-BiLinear model introduced in\nMnih and Hinton (2007). We used 5 preced-\ning words as context.\n•LSTM-s. A standard LSTM-RNN language\nmodel which is applied in Sundermeyer et al.\n(2015) and Karpathy et al. (2015). We im-\nplemented the LM ourselves based on Theano\n(Theano Development Team, 2016) and also\nused NCE for training.\n•LSTM-z. An LSTM-RNN enhanced with\nthe ZRegression mechanism described in\nSection 3.4.\n•LSTM-z,wb. Based on LSTM-z, we com-\npressed word embeddings in Embedding\nand the output weights and biases in\nPrediction.\n•LSTM-z,w. In this variant, we did not com-\npress the bias term in the output layer. For\neach word in C, we assigned an independent\nbias parameter.\n4.3.2 Performance\nTables 3 shows the perplexity of our compressed\nmodel and baselines. As we see, LSTM-based\nLMs signiﬁcantly outperform the log-bilinear\nV ocabulary 10k 22k 36k 50k\nKN3† 90.4 125.3 146.4 159.9\nLBL5 116.6 167.0 199.5 220.3\nLSTM-s 107.3 159.5 189.4 222.1\nLSTM-z 75.1 104.4 119.6 130.6\nLSTM-z,wb 73.7 103.4 122.9 138.2\nLSTM-z,w 72.9 101.9 119.3 129.2\nTable 3: Perplexity of our compressed language\nmodels and baselines. †Trained with the full cor-\npus of 1.6 billion running words.\nV ocabulary 10k 22k 36k 50k\nLSTM-z,w 17.76 59.28 73.42 79.75\nLSTM-z,wb 17.80 59.44 73.61 79.95\nTable 4: Memory reduction (%) by our proposed\nmethods in comparison with the uncompressed\nmodel LSTM-z. The memory of sparse codes are\nincluded.\nFigure 4: Fine-grained plot of performance\n(perplexity) and memory consumption (including\nsparse codes) versus the vocabulary size.\nmodel as well as the backoff 3-gram LM, even if\nthe 3-gram LM is trained on a much larger cor-\npus with 1.6 billion words. The ZRegression\nmechanism improves the performance of LSTM\nto a large extent, which is unexpected. Subsec-\ntion 4.4 will provide more in-depth analysis.\nRegarding the compression method proposed\nin this paper, we notice that LSTM-z ,wb and\nLSTM-z,w yield similar performance to LSTM-z.\nIn particular, LSTM-z,w outperforms LSTM-z in\nall scenarios of different vocabulary sizes. More-\nover, both LSTM-z,wb and LSTM-z,w can reduce\nthe memory consumption by up to 80% (Table 4).\nWe further plot in Figure 4 the model perfor-\nmance (lines) and memory consumption (bars) in\na ﬁne-grained granularity of vocabulary sizes. We\nsee such a tendency that compressed LMs (LSTM-\nz,wb and LSTM-z ,w, yellow and red lines) are\ngenerally better than LSTM-z (black line) when\nwe have a small vocabulary. However, LSTM-\nz,wb is slightly worse than LSTM-z if the vocabu-\nlary size is greater than, say, 20k. The LSTM-z ,w\nremains comparable to LSTM-z as the vocabulary\ngrows.\nTo explain this phenomenon, we may imagine\nthat the compression using sparse codes has two\neffects: it loses information, but it also enables\nmore accurate estimation of parameters especially\nfor rare words. When the second factor dominates,\nwe can reasonably expect a high performance of\nthe compressed LM.\nFrom the bars in Figure 4, we observe that tra-\nditional LMs have a parameter space growing lin-\nearly with the vocabulary size. But the number\nof parameters in our compressed models does not\nincrease—or strictly speaking, increases at an ex-\ntremely small rate—with vocabulary.\nThese experiments show that our method can\nlargely reduce the parameter space with even per-\nformance improvement. The results also verify\nthat the sparse codes induced by our model indeed\ncapture meaningful semantics and are potentially\nuseful for other downstream tasks.\n4.4 Effect of ZRegression\nWe next analyze the effect of ZRegression for\nNCE training. As shown in Figure 5a, the training\nprocess becomes unstable after processing 70% of\nthe dataset: the training loss vibrates signiﬁcantly,\nwhereas the test loss increases.\nWe ﬁnd a strong correlation between unsta-\nbleness and the Zh factor in Equation (10), i.e.,\nthe sum of unnormalized probability (Figure 5b).\nTheoretical analysis shows that theZhfactor tends\nto be self-normalized even though it is not forced\nto (Gutmann and Hyv ¨arinen, 2012). However,\nproblems would occur, should it fail.\nIn traditional methods, NCE jointly estimates\nnormalization factor Z and model parameters\n(Gutmann and Hyv ¨arinen, 2012). For language\nmodeling, Zh dependents on context h. Mnih\nand Teh (2012) propose to estimate a separate Zh\nbased on two history words (analogous to 3-gram),\nbut their approach hardly scales to RNNs because\nof the exponential number of different combina-\ntions of history words.\nWe propose the ZRegression mechanism in\nSection 3.4, which can estimate the Zh factor well\n(Figure 5d) based on the history vector h. In\nthis way, we manage to stabilize the training pro-\ncess (Figure 5c) and improve the performance by\n(a) Training/test loss vs. training time w/o\nZRegression.\n(b) The validation perplexity and normalization factor Zh w/o\nZRegression.\n(c) Training loss vs. training time w/\nZRegression of different runs.\n(d) The validation perplexity and normalization factor Zh w/\nZRegression.\nFigure 5: Analysis of ZRegression.\na large margin, as has shown in Table 3.\nIt should be mentioned that ZRegression is\nnot speciﬁc to model compression and is generally\napplicable to other neural LMs trained by NCE.\n5 Conclusion\nIn this paper, we proposed an approach to repre-\nsent rare words by sparse linear combinations of\ncommon ones. Based on such combinations, we\nmanaged to compress an LSTM language model\n(LM), where memory does not increase with the\nvocabulary size except a bias and a sparse code\nfor each word. Our experimental results also show\nthat the compressed LM has yielded a better per-\nformance than the uncompressed base LM.\nAcknowledgments\nThis research is supported by the National Ba-\nsic Research Program of China (the 973 Pro-\ngram) under Grant No. 2015CB352201, the Na-\ntional Natural Science Foundation of China under\nGrant Nos. 61232015, 91318301, 61421091 and\n61502014, and the China Post-Doctoral Founda-\ntion under Grant No. 2015M580927.\nReferences\n[Andreas and Klein2014] Jacob Andreas and Dan\nKlein. 2014. When and why are log-linear models\nself-normalizing. In Proceedings of the Annual\nMeeting of the North American Chapter of the\nAssociation for Computational Linguistics , pages\n244–249.\n[Bengio and Sen´ecal2003] Yoshua Bengio and Jean-\nS´ebastien Sen´ecal. 2003. Quick training of proba-\nbilistic neural nets by importance sampling. In Pro-\nceedings of the Ninth International Workshop on Ar-\ntiﬁcial Intelligence and Statistics.\n[Bengio et al.2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Jauvin. 2003. A neu-\nral probabilistic language model. The Journal of\nMachine Learning Research, 3:1137–1155.\n[Buciluˇa et al.2006] Cristian Bucilu ˇa, Rich Caruana,\nand Alexandru Niculescu-Mizil. 2006. Model com-\npression. In Proceedings of the 12th ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining, pages 535–541.\n[Chen et al.2015] Welin Chen, David Grangier, and\nMichael Auli. 2015. Strategies for training large\nvocabulary neural language models. arXiv preprint\narXiv:1512.04906.\n[Devlin et al.2014] Jacob Devlin, Rabih Zbib,\nZhongqiang Huang, Thomas Lamar, Richard M\nSchwartz, and John Makhoul. 2014. Fast and robust\nneural network joint models for statistical machine\ntranslation. In Proceedings of the 52rd Annual\nMeeting of the Association for Computational\nLinguistics, pages 1370–1380.\n[Faruqui et al.2015] Manaal Faruqui, Yulia Tsvetkov,\nDani Yogatama, Chris Dyer, and Noah A. Smith.\n2015. Sparse overcomplete word vector represen-\ntations. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1491–1500.\n[Gong et al.2014] Yunchao Gong, Liu Liu, Ming Yang,\nand Lubomir Bourdev. 2014. Compressing deep\nconvolutional networks using vector quantization.\narXiv preprint arXiv:1412.6115.\n[Gutmann and Hyv¨arinen2012] Michael Gutmann and\nAapo Hyv ¨arinen. 2012. Noise-contrastive estima-\ntion of unnormalized statistical models, with appli-\ncations to natural image statistics. The Journal of\nMachine Learning Research, 13(1):307–361.\n[He et al.2012] Zhanying He, Chun Chen, Jiajun Bu,\nCan Wang, Lijun Zhang, Deng Cai, and Xiaofei He.\n2012. Document summarization based on data re-\nconstruction. In Proceedings of the 26th AAAI Con-\nference on Artiﬁcial Intelligence, pages 620–626.\n[Hinton et al.2015] Geoffrey Hinton, Oriol Vinyals, and\nJeff Dean. 2015. Distilling the knowledge in a neu-\nral network. arXiv preprint arXiv:1503.02531.\n[Jaderberg et al.2014] Max Jaderberg, Andrea Vedaldi,\nand Andrew Zisserman. 2014. Speeding up convo-\nlutional neural networks with low rank expansions.\nIn Proceedings of the British Machine Vision Con-\nference.\n[Jean et al.2014] S ´ebastien Jean, Kyunghyun Cho,\nRoland Memisevic, and Yoshua Bengio. 2014. On\nusing very large target vocabulary for neural ma-\nchine translation. arXiv preprint arXiv:1412.2007.\n[Jurafsky and Martin2014] Dan Jurafsky and James H.\nMartin. 2014. Speech and Language Processing .\nPearson.\n[Karpathy et al.2015] Andrej Karpathy, Justin Johnson,\nand Fei-Fei Li. 2015. Visualizing and un-\nderstanding recurrent networks. arXiv preprint\narXiv:1506.02078.\n[Kingma and Ba2014] Diederik P Kingma and Jimmy\nBa. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980.\n[Lee et al.2006] Honglak Lee, Alexis Battle, Rajat\nRaina, and Andrew Y Ng. 2006. Efﬁcient sparse\ncoding algorithms. In Advances in Neural Informa-\ntion Processing Systems, pages 801–808.\n[Levy and Goldberg2014] Omer Levy and Yoav Gold-\nberg. 2014. Linguistic regularities in sparse and ex-\nplicit word representations. In Proceedings of the\nEighteenth Conference on Natural Language Learn-\ning, pages 171–180.\n[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ ´at,\nLukas Burget, Jan Cernock `y, and Sanjeev Khudan-\npur. 2010. Recurrent neural network based lan-\nguage model. In INTERSPEECH, pages 1045–\n1048.\n[Mikolov et al.2011] Tomas Mikolov, Anoop Deoras,\nDaniel Povey, Lukas Burget, and Jan Cernock ´y.\n2011. Strategies for training large scale neural net-\nwork language models. In Proceedings of the IEEE\nWorkshop on Automatic Speech Recognition and\nUnderstanding, pages 196–201.\n[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg\nCorrado, and Jeffrey Dean. 2013. Efﬁcient estima-\ntion of word representations in vector space. arXiv\npreprint arXiv:1301.3781.\n[Mnih and Hinton2007] Andriy Mnih and Geoffrey\nHinton. 2007. Three new graphical models for sta-\ntistical language modelling. In Proceedings of the\n24th International Conference on Machine learning,\npages 641–648.\n[Mnih and Teh2012] Andriy Mnih and Yee-Whye Teh.\n2012. A fast and simple algorithm for training neu-\nral probabilistic language models. arXiv preprint\narXiv:1206.6426.\n[Morin and Bengio2005] Fr ´ederic Morin and Yoshua\nBengio. 2005. Hierarchical probabilistic neural net-\nwork language model. In Proceedings of the In-\nternational Workshop on Artiﬁcial Intelligence and\nStatistics, pages 246–252.\n[Mou et al.2015a] Lili Mou, Ge Li, Yan Xu, Lu Zhang,\nand Zhi Jin. 2015a. Distilling word embed-\ndings: An encoding approach. arXiv preprint\narXiv:1506.04488.\n[Mou et al.2015b] Lili Mou, Rui Yan, Ge Li, Lu Zhang,\nand Zhi Jin. 2015b. Backward and forward lan-\nguage modeling for constrained natural language\ngeneration. arXiv preprint arXiv:1512.06612.\n[Rush et al.2015] Alexander M Rush, Sumit Chopra,\nand Jason Weston. 2015. A neural attention model\nfor abstractive sentence summarization. In Proceed-\nings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 379–389.\n[Sordoni et al.2015] Alessandro Sordoni, Michel Gal-\nley, Michael Auli, Chris Brockett, Yangfeng Ji, Mar-\ngaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and\nBill Dolan. 2015. A neural network approach\nto context-sensitive generation of conversational re-\nsponses. In Proceedings of the 2015 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 196–205.\n[Stolcke and others2002] Andreas Stolcke et al. 2002.\nSRILM—An extensible language modeling toolkit.\nIn INTERSPEECH, pages 901–904.\n[Sundermeyer et al.2015] Martin Sundermeyer, Her-\nmann Ney, and Ralf Schl ¨uter. 2015. From feed-\nforward to recurrent LSTM neural networks for lan-\nguage modeling. IEEE/ACM Transactions on Au-\ndio, Speech and Language Processing , 23(3):517–\n529.\n[Theano Development Team2016] Theano Develop-\nment Team. 2016. Theano: A Python framework\nfor fast computation of mathematical expressions.\narXiv preprint arXiv:1605.02688.\n[Yang et al.2011] Meng Yang, Lei Zhang, Jian Yang,\nand David Zhang. 2011. Robust sparse coding for\nface recognition. In Proceedings of the 2011 IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 625–632.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9062126278877258
    },
    {
      "name": "Language model",
      "score": 0.746179461479187
    },
    {
      "name": "Computer science",
      "score": 0.7361118793487549
    },
    {
      "name": "Word (group theory)",
      "score": 0.6883853673934937
    },
    {
      "name": "Vocabulary",
      "score": 0.6093448400497437
    },
    {
      "name": "Context (archaeology)",
      "score": 0.575805127620697
    },
    {
      "name": "Artificial intelligence",
      "score": 0.562813937664032
    },
    {
      "name": "Artificial neural network",
      "score": 0.5605127215385437
    },
    {
      "name": "Layer (electronics)",
      "score": 0.5124085545539856
    },
    {
      "name": "Natural language processing",
      "score": 0.47537413239479065
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.4106738865375519
    },
    {
      "name": "Speech recognition",
      "score": 0.3590719699859619
    },
    {
      "name": "Mathematics",
      "score": 0.1701691448688507
    },
    {
      "name": "Linguistics",
      "score": 0.09085482358932495
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}