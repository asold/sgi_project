{
  "title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments",
  "url": "https://openalex.org/W4399177289",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2739209109",
      "name": "Abhinav Rajvanshi",
      "affiliations": [
        "SRI International"
      ]
    },
    {
      "id": "https://openalex.org/A2131104076",
      "name": "Karan Sikka",
      "affiliations": [
        "SRI International"
      ]
    },
    {
      "id": "https://openalex.org/A2096005872",
      "name": "Xiao Lin",
      "affiliations": [
        "SRI International"
      ]
    },
    {
      "id": "https://openalex.org/A2156202025",
      "name": "Bhoram Lee",
      "affiliations": [
        "SRI International"
      ]
    },
    {
      "id": "https://openalex.org/A4214029737",
      "name": "Han-Pang Chiu",
      "affiliations": [
        "SRI International"
      ]
    },
    {
      "id": "https://openalex.org/A2119933826",
      "name": "Alvaro Velasquez",
      "affiliations": [
        "University of Colorado Boulder",
        "Defense Advanced Research Projects Agency"
      ]
    },
    {
      "id": "https://openalex.org/A2739209109",
      "name": "Abhinav Rajvanshi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131104076",
      "name": "Karan Sikka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096005872",
      "name": "Xiao Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156202025",
      "name": "Bhoram Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214029737",
      "name": "Han-Pang Chiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119933826",
      "name": "Alvaro Velasquez",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384268338",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2962887844",
    "https://openalex.org/W3009928773",
    "https://openalex.org/W3004691725",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4282981845",
    "https://openalex.org/W3101465773",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W1931877416",
    "https://openalex.org/W4288616852",
    "https://openalex.org/W2990129662",
    "https://openalex.org/W2884565639",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W2125814131",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4312707839",
    "https://openalex.org/W4221151084",
    "https://openalex.org/W3211462570",
    "https://openalex.org/W4365460440",
    "https://openalex.org/W4226236972",
    "https://openalex.org/W4364353501",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3081567831",
    "https://openalex.org/W4287112297",
    "https://openalex.org/W4382334278",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3040041096",
    "https://openalex.org/W4390873494",
    "https://openalex.org/W3183042936",
    "https://openalex.org/W4306705192",
    "https://openalex.org/W3035154952",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4383109211",
    "https://openalex.org/W4366999541",
    "https://openalex.org/W2967353954",
    "https://openalex.org/W4386075839",
    "https://openalex.org/W3182243998",
    "https://openalex.org/W4293566037"
  ],
  "abstract": "Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on multi-object navigation (MultiON) task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. We also introduce a benchmark dataset for MultiON task employing ProcTHOR framework that provides large photo-realistic indoor environments with variety of objects. SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than 8% in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments. The code, benchmark dataset and demonstration videos are accessible at https://www.sri.com/ics/computer-vision/saynav.",
  "full_text": "SayNav: Grounding Large Language Models\nfor Dynamic Planning to Navigation in New Environments\nAbhinav Rajvanshi1, Karan Sikka1, Xiao Lin1, Bhoram Lee1, Han-Pang Chiu1, Alvaro Velasquez2,3\n1 SRI International, 201 Washington Rd, Princeton, NJ 08540, USA\n2 University of Colorado Boulder, Boulder, CO 80309, USA\n3 Defense Advanced Research Projects Agency (DARPA)\n{abhinav.rajvanshi, karan.sikka, xiao.lin, bhoram.lee, han-pang.chiu}@sri.com, alvaro.velasquez@colorado.edu\nAbstract\nSemantic reasoning and dynamic planning capabilities are\ncrucial for an autonomous agent to perform complex nav-\nigation tasks in unknown environments. It requires a large\namount of common-sense knowledge, that humans possess,\nto succeed in these tasks. We present SayNav, a new approach\nthat leverages human knowledge from Large Language Mod-\nels (LLMs) for efficient generalization to complex naviga-\ntion tasks in unknown large-scale environments. SayNav uses\na novel grounding mechanism, that incrementally builds a\n3D scene graph of the explored environment as inputs to\nLLMs, for generating feasible and contextually appropriate\nhigh-level plans for navigation. The LLM-generated plan is\nthen executed by a pre-trained low-level planner, that treats\neach planned step as a short-distance point-goal navigation\nsub-task. SayNav dynamically generates step-by-step instruc-\ntions during navigation and continuously refines future steps\nbased on newly perceived information. We evaluate SayNav\non multi-object navigation (MultiON) task, that requires the\nagent to utilize a massive amount of human knowledge to ef-\nficiently search multiple different objects in an unknown en-\nvironment. We also introduce a benchmark dataset for Mul-\ntiON task employing ProcTHOR framework that provides\nlarge photo-realistic indoor environments with variety of ob-\njects. SayNav achieves state-of-the-art results and even out-\nperforms an oracle based baseline with strong ground-truth\nassumptions by more than 8% in terms of success rate, high-\nlighting its ability to generate dynamic plans for successfully\nlocating objects in large-scale new environments. The code,\nbenchmark dataset and demonstration videos are accessible\nat https://www.sri.com/ics/computer-vision/saynav.\n1 Introduction\nFinding multiple target objects in a novel environmentis a\nrelatively easy task for a human but a daunting task for an\nautonomous agent. Given such a task, humans are able to\nleverage common-sense priors like room layouts and plausi-\nble object placement to infer likely locations of objects. For\nexample, there are higher chances of finding a pillow on the\nbed in the bedroom and a spoon on the dining table or in\nthe kitchen. Humans are also capable of dynamically plan-\nning and adjusting their search strategies and actions based\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\non new visual observations during exploration in a new en-\nvironment. For example, a human would search for a spoon\nfirst instead of a pillow if entering a kitchen.\nSuch reasoning and dynamic planning capabilities are\nessential for an autonomous agent to accomplish complex\nnavigation tasks in novel settings, such as searching and\nlocating specific objects in new houses. However, current\nlearning-based methods, with the most popular being deep\nreinforcement learning (DRL) (Anderson et al. 2018; Chap-\nlot et al. 2020; Khandelwal et al. 2022), require massive\namounts of training for the agent to achieve reasonable per-\nformance even for simpler navigation tasks, such as finding\na single object (object-goal navigation) or reaching a single\ntarget point (point-goal navigation) (Anderson et al. 2018).\nMoreover, significant computational resources are needed to\nreplicate human ability to generalize to new environments.\nSuch computational demands impede the development of an\nautonomous agent to efficiently conduct complex tasks at\nunknown places.\nIn this paper, we propose SayNav – a new approach to\nleverage common-sense knowledge from Large Language\nModels (LLMs) for efficient generalization to complicated\nnavigation tasks in unknown large-scale environments. Re-\ncently, agents equipped with LLM-based planners have\nshown remarkable capabilities to conduct complex manip-\nulation tasks with only a few training samples (Ahn et al.\n2022; Song et al. 2022). SayNav follows this trend of utiliz-\ning LLMs in developing generalist planning agents specifi-\ncally for navigation tasks. To fully demonstrate and validate\nSayNav’s capabilities, we choose a complex navigation task,\nmulti-object navigation (MultiON). For this task, the agent\nneeds to efficiently explore a new 3D environment to locate\nmultiple different objects given the names of these objects.\nThis task requires a large amount of prior knowledge and\ndynamic planning capabilities (similar to humans) for suc-\ncess.\nMultiON task has emerged recently as a generalization of\nthe Object-goal Navigation task. It was introduced by (Wani\net al. 2020) as a task of “navigation to an ordered sequence\nof objects” and most of the other works on MultiON fol-\nlow the same definition. (Gireesh et al. 2023) dropped the\nconstraint of having a ordered sequence of the given objects\nand defined the goal as to localize certain number of objects\nin any order. This is how we also define our MultiON task\nProceedings of the Thirty-Fourth International Conference on Automated Planning and Scheduling (ICAPS2024)\n464\nFigure 1: A SayNav example: The robot uses LLM-based planner to efficiently find one target object (laptop) in a new house.\nas it poses much larger planning challenges and task com-\nplexities than the previous definition. Following the trend in\nMultiON task, we will be using three different objects as the\ntargets for each episode for experiments. Note that SayNav\nis capable of searching for any number of objects in the en-\nvironment.\nThe key innovation of SayNav is to incrementally build\nand expand a 3D scene graph of the new environment\nusing perceived information during exploration. It then\ngrounds feasible and contextually appropriate knowledge\nfrom LLMs which is used by the agent for navigation. This\nnew grounding mechanism ensures that LLMs adhere to the\nphysical constraints in the new environment, including the\nspatial layouts and geometric relationships among perceived\nentities. 3D scene graphs (Armeni et al. 2019; Kim et al.\n2019; Rosinol et al. 2021; Hughes et al. 2022; Wald et al.\n2020; Wu et al. 2021) have recently emerged as powerful\nhigh-level representations of 3D large-scale environments to\nsupport real-time decisions in robotics. A 3D scene graph is\na layered graph which represents spatial concepts (nodes)\nat multiple levels of abstraction (such as objects, places,\nrooms, and buildings) with their relations (edges). We uti-\nlize this 3D scene graph representation to ground LLMs in\nthe current environment, which is used to continuously build\nand refine the search plan for the agent during navigation.\nSpecifically, SayNav utilizes LLMs to generate step-by-\nstep instructions on the fly, for locating target objects dur-\ning navigation. To ensure feasible and effective planning in\na dynamic manner, SayNav continuously extracts and con-\nverts a subgraph (from the current 3D scene graph) into a\ntextual prompt to be fed to the LLMs. This extracted sub-\ngraph includes spatial concepts in the local region centered\naround the current position of the agent. The LLM then plans\nnext steps based on this subgraph, such as inferring likely\nlocations for the target object and prioritizing them. This\nplan also includes conditional statements and fallback op-\ntions when any of the steps is unable to achieve the goal.\nFor example, if the agent is not able to find the laptop on the\ndesk, it will go to a next likely location (bed) in a bedroom.\nSayNav also leverages LLMs to augment and refine the\nscene graph during navigation, such as annotating the room\ntype based on current perceived objects. This improves the\nhierarchical organization of semantic information in the\nscene graph, that can support better planning. SayNav com-\nputes the feasibility of completing the current goal based on\nthe room type which helps in better optimization of plan. For\nexample, it can skip the restroom when looking for a spoon,\nbut can come back later if needed.\nSayNav only requires a few examples via in-context learn-\ning (Brown et al. 2020; Ouyang et al. 2022) for configur-\ning LLMs to conduct high-level dynamic planning to com-\nplicated MultiON tasks in new environments. The LLM-\ngenerated plan is then executed by a pre-trained low-level\nplanner that treats each planned step as a short-distance\npoint-goal navigation sub-task (such as moving to a per-\nceived object). This decomposition reduces the planning\ncomplexity of the navigation task, because the sub-tasks\nplanned by LLMs are simple enough for low-level planners\nto execute successfully.\nFigure 1 illustrates an example of SayNav utilizing LLMs\nto efficiently explore a new environment and locate one (lap-\ntop) of the three target objects. The agent first looks around\n(i.e., observes to build the scene graph) and identifies what\ntype of room it starts from. After checking potential loca-\ntions of the target objects in the room, the agent does not\nfind any target. Then it decides to go through the door to\nmove to another room. The agent continuously expands the\nscene graph during exploration and realizes that the neigh-\nbor room is a living room. There, it finds one target on the\ntable and continues searching for other two objects.\nThe main contributions are summarized as follows.\n1. We present, to the best of our knowledge, the first\nLLM-based high-level planner specifically for naviga-\ntion tasks in large-scale unknown photo-realistic environ-\nments. The proposed LLM planner incrementally gener-\nates step-by-step instructions in a dynamic manner dur-\ning navigation. The instructions generated from LLMs\nduring navigation are consistent and non-redundant.\n2. We propose a novel grounding mechanism to LLMs for\nnavigation in new large-scale environments. SayNav in-\ncrementally builds and expands a 3D scene graph during\nexploration. Next-step plans are generated from LLMs,\nby utilizing text prompts based on a selected portion\n465\n(subgraph) of the scene graph. Parts of the scene graph\nare also continuously refined and updated by LLMs.\n3. We introduce a benchmark dataset for MultiON task\nacross different houses for our evaluation and future use\nby researchers.\n2 Related Work\nIn this section, we provide a brief review on related works\nin visual navigation, high-level planning with LLMs for au-\ntonomous agents and MultiON.\nVisual Navigation in New Environmentsis a fundamen-\ntal capability for many applications for autonomous agents.\nRecent learning-based approaches with DRL methods have\nshown great potential to outperform classical approaches\nbased on SLAM (simultaneous localization and mapping)\nand path planning techniques, on different visual naviga-\ntion tasks (Mishkin et al. 2019). These navigation tasks in-\nclude point-goal navigation (Wijmans et al. 2019), image-\ngoal navigation (Zhu et al. 2017), and object-goal navigation\n(Chaplot et al. 2020).\nHowever, these methods generally require at least hun-\ndreds of millions of iterations (Wijmans et al. 2019) for\ntraining agents to generalize in new environments. This en-\ntails high cost in terms of both data collection and computa-\ntion. In addition, it hinders the development of autonomous\nagents that can conduct more complex navigation tasks, such\nas multi-object navigation and cordon and search, that re-\nquires the ability to exploit common-sense knowledge and\nplan dynamically in novel environments.\nLeveraging common-sense knowledge from LLMs allows\nus to avoid the high cost of training as in the previous\nlearning-based methods. By effectively grounding LLMs\n(such as ChatGPT) via text prompting, our approach enables\nefficient high-level planning for visual navigation in un-\nknown environments. To better demonstrate and evaluate our\nproposed method, we use multi-object navigation, which is\nmore complex than previous navigation tasks such as object-\ngoal navigation. The MultiON task demands common-sense\nknowledge, as humans do, to efficiently search for multiple\ndifferent objects in large-scale unknown environments.\nHigh-Level Planning with LLMs has become an emer-\ngent trend in the robotics field. LLMs by virtue of both train-\ning on internet scale data and instruction tuning have demon-\nstrated excellent capabilities to perform zero/few shot learn-\ning for unseen tasks (Zhao et al. 2023; Brown et al. 2020).\nRecent instruction tuned models such as ChatGPT have fur-\nther shown strong capabilities to follow natural instructions\nexpressed as prompts (Chung et al. 2022; Peng et al. 2023).\nRecent works in autonomy have used LLMs and demon-\nstrated significant progress (Ahn et al. 2022; Song et al.\n2022; Huang et al. 2022; Liu et al. 2023; Driess et al.\n2023; Brown et al. 2020; Ouyang et al. 2022) in incorpo-\nrating human knowledge, that enables efficient training of\nautonomous agents for tasks such as mobile manipulation.\nThese works reduce the learning complexity by using a two-\nlevel planning architecture. For each assigned task, they uti-\nlize LLMs to generate a high-level step-by-step plan. Each\nplanned step, formulated as a much simpler sub-task, can be\nexecuted by an oracle (ground truth) or a pre-trained low-\nlevel planner that maps one step into a sequence of primitive\nactions. Agents with these LLM-based planners are able to\nperform a new task with only a few training examples via\nin-context learning (Brown et al. 2020; Ouyang et al. 2022).\nHowever, these LLM-based planners have two major lim-\nitations for visual navigation tasks in new large-scale envi-\nronments. First, the grounding mechanisms in these meth-\nods (Ahn et al. 2022; Song et al. 2022; Huang et al. 2022;\nLiu et al. 2023) are designed for small-scale environments.\nFor example, works such as (Song et al. 2022; Singh et al.\n2023) have focused on the AI-THOR based environment\nthat consists of only a single room. Moreover, these meth-\nods only rely on detection of specific objects. They do not\nconsider room layout and the topological arrangement of\nperceived entities inside the room, which are important to\nground LLMs in the physical environment for visual naviga-\ntion tasks. Therefore, knowledge extracted from LLMs us-\ning these methods might not be contextually appropriate to\nan agent for navigation in large-scale settings, such as multi-\nroom houses.\nSecond, some of these LLM-based planners typically gen-\nerate a multi-step long-horizon plan in the beginning for\nthe assigned task, which is not feasible for navigating in\nunknown environments. They also lack the capability to\nchange the plan during task execution. In contrast, an effec-\ntive search plan for navigation in new places is required to\nbe incrementally generated and updated during exploration.\nFuture actions are decided based on current perceived scenes\nwith the memory of previously-visited regions.\nSayPlan (Rana et al. 2023) addresses the first issue by\nusing a pre-built ground-truth 3D scene graph of a known\nlarge-scale place, to ground LLMs for high-level task plan-\nning. However, the planning complexity for SayPlan is sim-\nplified due to the availability of the entire ground-truth scene\ngraph prior to task execution. In other words, SayPlan can-\nnot be used for task planning in unknown environments.\nOur approach, SayNav, is designed to leverage LLMs\nspecifically for visual navigation in unknown large-scale en-\nvironments. We propose a new grounding mechanism that\nincrementally builds a 3D scene graph of the explored en-\nvironment as inputs to LLMs, for generating the high-level\nplans. SayNav also dynamically generates step-by-step in-\nstructions during navigation. It continuously refines future\nsteps based on newly perceived information via LLMs.\nThe only work we found to leverage LLMs specifically\nfor navigation tasks in unknown environments is L3MVN\n(Yu et al. 2023). It uses LLMs to find the nearby semantic\nfrontier based on detected objects, for expanding the explo-\nration area to eventually find the target object. For exam-\nple, moving to the (sofa, table) region which is more likely\nto have TV . In other words, it utilizes LLMs to hint to the\nnext exploration direction. It does not use LLMs as a full\nhigh-level planner, that generates step-by-step instructions.\nIn contrast, our SayNav uses the 3D scene graph to ground\nLLMs as a high-level planner. Our LLM-based planner gen-\nerates the instructions in a dynamic manner, and considers\nits prior planned steps to generate better future plans.\nMultiON task has attracted attention of researchers in the\n466\nFigure 2: The overview of our SayNav framework.\nlast few years. As already mentioned, most of them (Chen\net al. 2022; Zeng et al. 2023; Marza et al. 2022, 2023) have\nconsidered pre-defined sequence of objects to be localized\nwhich simplifies the task by a great extent. Moreover, they\nemploy pure DRL-based approaches and hence suffer from\nissues discussed before. (Gireesh et al. 2023) is the only\nwork to consider MultiON without any sequence of objects\nbut again makes use of a DRL-based approach.\n3 SayNav\nWe now describe SayNav’s framework as well as the multi-\nobject navigation task.\n3.1 Task Definition\nWe choose Multi-Object Navigation task, to validate Say-\nNav. The goal of this task is to navigate the agent in a\nlarge-scale unknown environment in order to find an in-\nstance for each of three predefined object categories (such\nas ”laptop”, ”tomato”, and ”bread”). The agent is initialized\nat a random location in the environment and receives the\ngoal object categories (o i, oj, ok) as input. At each time\nstep t during navigation, the agent receives environment\nobservations et and takes control actions at. The observa-\ntions include RGBD images, semantic segmentation maps,\nand the agent’s pose (location and orientation). The action\nspace includes five control commands: turn-left, turn-\nright, move-forward , stop, and look-around. Both turn-\nleft and turn-right actions rotate the agent by 90 degrees.\nThe move-forward action moves the agent by 25 centime-\nters. The task execution is successful if the agent locates (by\ndetection) all three objects within a time period.\nNote that MultiON task poses much larger planning chal-\nlenges and task complexities than previous navigation tasks,\nwhich either look for a single object (Chaplot et al. 2020)\nor reach a single target point (Wijmans et al. 2019). For\nexample, the agent needs to dynamically set up the search\nplan based on the prioritized order among three different ob-\njects. This plan can also be changed during exploration in a\nnew house with unknown layouts. As shown in Figure 1, the\nagent first realizes that it is in the bedroom and then decides\nto prioritize places (such as the table) to locate the laptop in-\nside this room. On the other hand, if the agent had started in\nthe kitchen, it would have been more efficient to search for\nthe fork and spoon first. Therefore, this new task requires\nextensive semantic reasoning and dynamic planning capa-\nbilities, as what humans possess, for an autonomous agent\nto explore in large-scale unknown environments.\n3.2 Overview\nSayNav’s framework is illustrated in Figure 2. The cor-\nresponding pseudo-code is in Algorithm 1. It includes\nthree modules: (1) Incremental Scene Graph Generation,\n(2) High-Level LLM-based Dynamic Planner, and (3) Low-\nLevel Planner. The Incremental Scene Graph Generation\nmodule accumulates observations received by the agent to\nbuild and expand a scene graph, which encodes semantic en-\ntities (such as objects and furniture) from the areas the agent\nhas explored. The High-Level LLM-based Dynamic Planner\ncontinuously converts relevant information from the scene\ngraph into text prompts to a pre-trained LLM, for dynam-\nically generating short-term high-level plans. Each LLM-\nplanned step is executed by the Low-Level Planner to gen-\nerate a series of control commands for execution.\n3.3 Incremental Scene Graph Generation\nThis module continuously builds and expands a 3D scene\ngraph of the environment being explored. A 3D scene graph\nis a layered graph which represents spatial concepts (nodes)\nat multiple levels of abstraction with their relations (edges).\nThis representation has recently emerged as a powerful\nhigh-level abstraction for 3D large-scale environments in\nrobotics. Here we define four levels in the 3D scene graph:\nsmall objects, large objects, rooms, and house. Each object\nnode is associated with its 3D coordinate and room node is\nassociated with its bounds. Every door is treated as an edge\nbetween two rooms, which also has an associated 3D coor-\ndinate. All other edges reveal the topological relationships\namong semantic concepts across different levels. Figure 3\nshows one example of our scene graph. Mathematically, our\nscene graph can be represented as a set of 4 kinds of triplets:\n{(sh, ‘near’, li), (li, ‘in’, rj), (rj, djk , rk), (rj, ‘in’, H)}\nwhere, sh: small object, li: large object, rj, rk: rooms (i ̸=\nj), djk : door between rj & rk, H: house (root node).\nLarge objects act as landmarks which are visible from far\naway distance. LLM uses these large objects to reason if a\nsmall object can be found near them. For example, it may\nbe logical to walk towards a dining tableto find a knife. The\ndecision of small vs. large objects is made based on its di-\nmensions and LLM’s understanding about the object class.\nThe scene graph is built using environmental observations\nreceived by the agent during exploration. The depth of each\nsegmented object can be obtained based on RGBD images\nand semantic segmentation images. The 3D coordinate of\neach perceived object can then be estimated by combining\nits depth information at multiple timestamps and the corre-\nsponding agent’s poses.\nWe also utilize LLMs to augment and refine high-level\nabstractions of the scene graph. For example, we use LLMs\n467\nAlgorithm 1: SayNav\nInput : Start location of robot start location\nhouse ID house id\nTarget Objects target objects\n1 unfound objects ← target objects\n2 spawn robot(start location)\n3 SceneGraph ←\ncreate scene graph(house id)\n4 while len(unfound objects) > 0 do\n5 objs found, observations←\nlook around()\n6 update unfound objects(objs found)\n7 room type ←\nidentify room type(observations)\n8 SceneGraph.update(room type, observations)\n9 plan needed ← is feasible(room type)\n10 if plan needed then\n11 subgraph ← SceneGraph.\n12 extract subgraph(room type)\n13 plan ←\nquery llm for plan(subgraph,\nunfound objects)\n14 for action in plan do\n15 if action.type = ’navigate’ then\n16 navigate to(action.target location)\n17 end\n18 else if action.type = ’look’ then\n19 objs found, observations←\nlook around()\n20 SceneGraph.update(observations)\n21 update unfound objects(objs found)\n22 end\n23 end\n24 end\n25 if len(unfound objects) > 0 then\n26 if SceneGraph.all doors explored() then\n27 return ’Task Failed’\n28 end\n29 door ←\nfind next unexplored door()\nnavigate to(door)\n30 end\n31 end\n32 return ’Success’\nto annotate and identify the spatial entity (room type) at the\nroom level of the graph based on its connected objects at\nlower levels. For instance, a room is probably a bedroom if\nit includes a bed. The bounds of a room are calculated based\non the detection of walls and floor of the room.\nSayNav also uses the 3D scene graph to support mem-\nory for future planning. For example, it automatically anno-\ntates the room nodes that have been investigated. Therefore,\nit will not generate repeated plans when the agent revisits\nthe same room during exploration.\nFigure 3: An example of our scene graph.\n3.4 High-Level LLM-based Dynamic Planner\nSimilar to previous works in LLM-based planning, SayNav\nutilizes a two-level planning architecture to reduce the learn-\ning complexity of the assigned task. However, instead of\ngenerating a complete high-level plan for the entire task in\nthe beginning, SayNav utilizes LLMs to incrementally gen-\nerate a short-term plan regularly, based on current obser-\nvations and the memory of previously-visited regions. This\nhigh-level planner can be set-up using only a few training\nexamples via typical in-context learning procedures (Brown\net al. 2020; Ouyang et al. 2022) (as shown in Figure 4).\nOur high-level LLM-based dynamic planner extracts a\nsubgraph from the full 3D scene graph and converts it into\ntext prompts, which are fed to an LLM. The extracted sub-\ngraph includes spatial concepts in the local region centered\naround the current position of the agent. We implemented\nthe LLM prompts similar to (Singh et al. 2023), which con-\nstructs programming language prompts based on the text la-\nbels in the extracted subgraph. Once prompts are received,\nthe LLM planner outputs short-term step-by-step instruc-\ntions, as pseudo code. The generated plan provides an effi-\ncient search strategy within the current perceived area based\non human knowledge, prioritizing locations to visit inside\nthe room based on the likelihoods of target objects being\ndiscovered. For example, LLM may provide a plan to first\ncheck the desk and then the bed to find the laptop in the bed-\nroom. Figure 4 shows the prompt structure used to gener-\nate the plan. We provide two in-context examples inside the\nprompt to constrain the LLM-generated plans. For instance,\nwe constrain each step to generate a navigate or look func-\ntion call with arguments and a high-level comment. Note\nthat we also make LLM output a descriptive comment asso-\nciated with each step in the generated plan, for minimizing\nthe prevailing issue of hallucinations in LLMs.\nThe LLM-based planner also extends and updates the plan\nwhen the previous plan fails or the task goal (finding three\nobjects) is not achieved after the execution of previous short-\nterm plan. It attempts to update the plan either by (i) gen-\nerating a plan based on new information received from the\nenvironment, (ii) putting the low-level planner into an ex-\nploratory mode (ex: try to go to next room), or (iii) refining\nthe scene graph by instructing the low-level planner to ran-\ndomly move around and collect more observations.\nBefore generating the high-level plan for a local region\n(room), LLM also reasons about the feasibility of locating\nthe target object(s) in the detected room-type. It may decide\n468\nFigure 4: Prompt used to create the search plan for a partic-\nular room.\nFigure 5: Prompts used to compute the feasibility of finding\nan object in a room-type and to identify the room-type.\nto skip searching a specific room and come back later if the\nfeasibility is low. In this case, it would mark the correspond-\ning room-node in the scene graph to come back later. Fig-\nure 5 shows the structure of prompts used to identify the\nroom type and determine the feasibility of locating target\nobject in a room.\n3.5 Low-Level Planner\nThe Low-Level Planner converts each LLM-planned step\ninto a series of control commands for execution. To inte-\ngrate two planners, SayNav formulates each LLM-planned\nstep as a short-distance point-goal navigation (P OINT NAV)\nsub-task for the low-level planner. The target point for each\nsub-task, such as moving from the current position to the ta-\nble in the current room, is assigned by the 3D coordinate of\nthe object (e.g. table) described in each planned step.\nSayNav’s low-level planner takes the RGBD images (res-\nolution 320 × 240) and the agent’s pose (location and\norientation) as inputs, and it outputs move\nforward,\nturn left and turn right actions to control the robot\nfollowing standard P OINT NAV settings. Note that large-\nscale DRL approaches typically take 108 to 109 simula-\ntion steps during training to solve P OINT NAV tasks in sim-\nulation environments (Wijmans et al. 2019; Weihs et al.\n2020), which poses serious training requirements and com-\nputation demands. In SayNav, however, the two-level plan-\nning architecture simplifies the job of the low-level planner.\nThe low-level planner mostly outputs control commands for\nshort-range movements. The LLM-based high-level planner\nis also robust to failures in the low-level planner, by making\nregular plan updates. In this way, the training load required\non the low-level planner can be greatly reduced.\nEncouraged by the success of imitation learning (IL)\non navigation tasks under resource constraints (Ramrakhya\net al. 2022, 2023; Shah et al. 2023), we investigate a sam-\nple efficient IL-based method to train the low-level planner\nfor the agent in SayNav. This low-level planner is trained\nfrom scratch (without pretraining) on only 2800 episodes or\n7 × 105 simulation steps. Specifically, the low-level planner\nis trained using the DAGGER algorithm (Ross et al. 2011) to\nfollow a shortest path oracle as the expert. Despite the fact\nthat the shortest path oracle lacks the exploration behavior\nrequired to solve the P OINT NAV task in complex environ-\nments (e.g. multiple rooms), we find that it helps the agent\nto learn short-distance navigation skills very quickly, with-\nout human-in-the-loop.\nWe first implement a shortest path oracle using A* algo-\nrithm on the grid of reachable positions in the house, and\nthen train a POINT NAV agent using the DAGGER (Ross et al.\n2011) algorithm with the A* oracle as the expert. We per-\nform DAGGER dataset aggregation over two rounds. In the\nfirst round, 2,000 episodes were collected using a random\nagent. In the second round, 800 additional episodes were\ncollected using an agent trained using episodes from the\nfirst round. The aggregated dataset contains a total of 2,800\nepisodes or 7×105 simulator steps for IL. For each episode,\nwe choose a scene from the ProcThor-10k train split, ran-\ndomly sample a start location and sample a random object\nfrom the scene as the goal location. The robot then performs\nthe task by taking expert action with p = 0.2 and agent ac-\ntion from p = 0.8. The robot’s observations and expert ac-\ntions are stored for behavior cloning.\nFor behavior cloning, the objective function is to mini-\nmize cross entropy loss of predicted action against expert\naction at every step. For agent architecture, following (Wij-\nmans et al. 2019), we use a standard architecture shown in\nFigure 6. As mentioned, our agent receives an RGBD image\nand the agent’s pose with respect to the goal location as the\ninput. A GroupNorm (Wu et al. 2018) ResNet18 (He et al.\n2016) encodes the input RGBD image, and a 2-layer 512\nhidden size gated recurrent unit (GRU) (Cho et al. 2014)\ncombines history and sensor inputs to predict the next ac-\ntion. We use the Adamax optimizer with lr10−4 and weight\ndecay 10−4. We optimize the objective function with batch\nsize of 16 episodes for 50 epochs.\nWe evaluate the P OINT NAV performance of the agent\n469\nFigure 6: Low-level PointNav architecture.\non the publicly available AI2Thor O BJECT NAV dataset 1\nProcThor-10k-val split, using the ground truth location of\nthe object as the goal for P OINT NAV evaluation. The evalu-\nation split contains 1550 episodes. When multiple instances\nof the target object are available, we arbitrarily select the\nfirst instance as the P OINT NAV goal. Our low-level planner\nachieves 84.5% SR (success radius 1.5m, max 300 steps)\nand 0.782 SPL. On the subset of episodes where the starting\nposition and the goal position are in the same room, perfor-\nmance increases to 98.5% SR and 0.930 SPL.\n4 Experimental Results\n4.1 Multi-Object Navigation Dataset\nMost prior Embodied AI simulators such as AI2-THOR\n(Kolve et al. 2017) or Habitat (Szot et al. 2021) are either\nbased on environments with single rooms or lack the nat-\nural placement of objects within the environment or lack\nthe ability to interact with the objects. For our experiments,\nwe opted for the recently introduced ProcTHOR framework\n(Deitke et al. 2022), which is built on top of the AI2-THOR\nsimulator. ProcTHOR is capable of procedurally generating\nfull floor plans of a house given a room specification (ex: a\nhouse with 2 bedrooms, 1 kitchen and 1 bathroom). It also\npopulates each floorplan with 108 object types, with realis-\ntic, physically plausible, and natural placements. The scenes\nin ProcTHOR are interactive which allows to change the\nstate, lighting and texture of objects, posing a bigger chal-\nlenge for perception and a broader scope for future work.\nWe build a benchmark dataset of 132 episodes using 132\ndifferent houses with 3-10 rooms each and select 3 objects\nfor each house to conduct MultiON task. Each episode in the\ndataset is described using the following properties:\n1. data\ntype : This corresponds to either ‘val’ or ‘test’ based\non which set of data was used from ProcThor-10K to con-\nstruct the episode.\n2. house\nidx : This corresponds to index of the specific house\nin the ProcThor-10K dataset.\n3. num\nrooms : Number of rooms in the house\n4. num targets : Number of targets in the house (Currently\nwe have limited the dataset to 3 targets)\n5. targets : List of unique targets in the house along with\ntheir ground truth locations\n1https://github.com/allenai/object-nav-eval\n6. start position : Start position randomly sampled from all\nreachable positions in the house\n7. start heading : Start heading randomly sampled from 0,\n90, 180 and 270 degrees\n8. shortest path targets order : The order of targets that re-\nsults in shortest path using A* planner. This is evaluated by\nrunning A* planner on all possible target orders.\n9. shortest\npath length : Length of the shortest path com-\nputed via A* planner along the shortest path targets order\n4.2 Metrics\nWe report two standard metrics that are used for evaluating\nnavigation tasks: Success Rate (SR) and Success Weighted\nby Path Length (SPL)(Anderson et al. 2018). SR measures\nthe percentage of cases where the agent is able to find all the\nthree objects successfully, while SPL normalizes the success\nby ratio of the shortest path to actual path taken. We use the\nminimum of the shortest path from the starting point to per-\nmutations of all the target objects. In addition to these two\nmetrics, we measure the similarity between the object or-\ndering obtained by the agent and that by the ground-truth.\nThe ground-truth object ordering gives an idea of how a per-\nfect agent would have explored the space by first identifying\nobjects that are highly probably in current room/scene-graph\nand then exploring other rooms. We use the Kendall distance\nmetric (Lapata 2006), which computes the distance between\ntwo rankings based on the number of disagreeing pairs. We\nuse the Kendall Tauthat converts this distance into a corre-\nlation coefficient, and report it over the successful episodes\n(all three targets are located).\n4.3 Implementation Details\nWe use the default robot with head-mounted RGBD camera\nin the AI2-Thor simulator. The camera has 320 × 240 reso-\nlution with 90°field-of-view (FoV). The details of the robot\nobservations and actions can be referred to the section 3.1.\nWe conduct experiments with 2 different LLMs: gpt-3.5-\nturbo and gpt-4. For training the low-level planner, we used\nthe IL method, described in the section 3.5. It achieves\n84.5% SR (success radius 1.5m, max 300 steps) and 0.782\nSPL in unseen ProcThor-10k-val scenes with random start\nand goal locations. For short-range movements within a sin-\ngle room, performance increases to 98.5% SR and 0.930\nSPL.\nNote that SayNav consists of three modules – incremen-\ntal scene graph generator, LLM-based planner, and a low-\nlevel planner. The major goal of our experiments is to fully\nvalidate and verify the LLM-based planning capabilities in\nSayNav for MultiON. Therefore, for each of the other two\nmodules, we implemented an alternative option which uses\nground truth information to avoid any error within that mod-\nule. This allows us to conduct ablation study for determining\nthe impact of each module on the overall performance.\nFirst, we allow the scene graph to be generated using\nground truth (GT) instead of visual observations (VO). We\nhave described the generation of scene graph using VO in the\nsection 3.3. The GT option directly uses the ground truth in-\nformation of surrounding objects, including 3D coordinates\nand geometric relationships among objects, to incrementally\n470\nScene LL SR SPL Kendall\nGraph Planner (%) Tau\nBaseline 56.06 0.49\nGT OrNav 95.35 0.43 0.70\nSayNav GT PNav 80.62 0.32 0.72\n(gpt-3.5) VO OrNav 71.32 0.48 0.56\nVO PNav 60.32 0.34 0.62\nGT OrNav 93.93 0.46 0.76\nSayNav GT PNav 84.09 0.36 0.78\n(gpt-4) VO OrNav 69.80 0.47 0.76\nVO PNav 64.34 0.33 0.78\nTable 1: Results of SayNav on multi-object navigation task.\nBaseline uses a PNav agent to navigate along the short-\nest route among targets based on ground-truth positions;\nGT and VO build the scene graph from ground-truth ob-\nject/room locations and visual observations provided by the\nsimulator respectively; OrNav and PNav use oracle and IL-\nlearned low-level (LL) planner respectively for navigating\nbetween the points assigned by the high-level planner.\nbuild the scene graph during exploration. This option avoids\nany association and computation ambiguity from processing\non visual observations, such as computing 3D coordinates\nfor each object based on RGBD image and its segmentation.\nSecond, we use an oracle planner (OrNav) as the low-\nlevel planner instead of our efficiently-trained IL agent\n(PNav). We have described the implementation of PNav in\nthe section 3.5. For OrNav, we use an A* planner which has\naccess to the map of the environment. Given a target loca-\ntion, it can plan the shortest path from the agent’s current\nlocation to the target.\n4.4 Baseline\nMost existing LLM-based approaches for robot navigation\noperate in known and/or small-scale environments. How-\never, there do exist RL / IL based approaches (ex: (Gireesh\net al. 2023)) and traditional SLAM-based approaches that\ncan work in our problem setting. Many previous works (ex:\n(Savva et al. 2019)) have shown that RL/IL based Point-\ngoal Navigation (PointNav) outperforms SLAM-based ap-\nproaches in unknown environments by a large extent (80%\nvs 62% in the mentioned example). Due to lack of open-\nsource datasets/code from existing works, we chose to im-\nplement the strongest possible baseline method which em-\nploys IL-based PointNav policy that could potentially re-\nflect the upper bound of the performance of a learning-based\nagent using the same amount of training data as ours. The\nbaseline agent uses two privileged information that SayNav\ndoesn’t have access to. First, it has access to the optimal or-\nder of target objects which achieves the shortest path. This\nsimplifies the MultiON task to become a series of ObjectNav\ntasks. Second, as a PointNav policy typically performs bet-\nter than an ObjectNav policy, we also provide the baseline\nagent access to the ground truth coordinate of each target\nobject, which then simplifies the task to a series of Point-\nNav tasks. In other words, we implement a PointNav agent\nFigure 7: Visualization of an episode with SayNav (OrNav\n+ GT) for multi-object object navigation task.\nto navigate through ground truth points of the objects in the\noptimal order.\n4.5 Quantitative Results\nTable 1 shows the results of the baseline along with differ-\nent implementation choices in SayNav. Note for the baseline\nmethod, even after using ground-truth object locations in op-\ntimal order, SR is only 56.06%, which indicates the diffi-\nculty of MultiON task. It is because of the fact that PointNav\npolicy observes a loss in performance in large-scale environ-\nments. Although, the baseline has access to the locations of\nthe goal objects, finding each object still requires it to plan a\nlong-range path across multiple rooms. In comparison, Say-\nNav, without using any ground truth, achieves a higher SR\n(60.32% and 64.34% with gpt-3.5-turbo and gpt-4 respec-\ntively). This improvement highlights the superiority of Say-\nNav in navigating in large-scale unknown environments.\nSR: With SayNav, we observe that the best performance is\nachieved when using scene graph generated by ground-truth\nobject/room location from the simulator (GT) along with\nOrNav. Note that SR of 95.35% with gpt-3.5 and 93.93%\nwith gpt-4 in this setting reflects the strength of our LLM-\nbased high-level planner which is the only imperfect mod-\nule in the experiment. When we replaced GT with VO, we\ndo observe a loss in performance. We found that the drop in\nSR can be associated with various challenges encountered in\nany perception based algorithms. The inaccurate estimation\nof objects’ 3D positions due to partial observations can lead\nto failures in detecting targets and navigation. In addition,\n471\nwe remove objects less than 20 pixels on semantic segmen-\ntation observations for more practical behavior. Therefore,\nvery small objects can also be missed-out while building the\nscene graph from visual observations. We also observed a\nspecific challenge in VO associated with estimating the lo-\ncation of glass doors. In the depth map, the depths of visible\nobjects behind the glass door represent the depths of the ac-\ntual physical door, which fails the estimation of the location\nof the door. A similar trend can be found from results with\nGT & PNav and with VO & PNav. When replacing OrNav\nwith PNav, we also observe a fall in performance. This is\nobvious as PNav doesn’t access any ground truth informa-\ntion, as compared to OrNav.\nHowever, even with all these challenges, SayNav outper-\nforms the oracle-based baseline and succeeds in MultiON\ntasks. We believe it is due to LLM-based dynamic planning\ncapabilities, with the grounding mechanism based on incre-\nmental scene graph generation. It leverages common-sense\nknowledge, as humans do, to efficiently search multiple dif-\nferent objects in an unknown environment. It also refines or\ncorrects the plan in case of any failure in a planned step.\nSPL: Looking at the SPL metric, we see a drop in Say-\nNav as compared to the baseline. Note that SPL reflects the\nlength of the path taken by the agent as compared to the\nshortest possible path. For example, SPL=1 for an episode\nwould mean that the agent, starting from initial position,\ngoes straight to the targets along the shortest path in the op-\ntimal order with zero exploration which is practically im-\npossible in an unknown environment. As a result of access\nto the ground-truth object locations in optimal order, it be-\ncomes obvious for the baseline to have higher SPL. From\nthe results, we also observe that the low-level planner has\nthe major impact on SPL. The system achieves higher SPL\nwith OrNav as compared to PNav.\nKendall-Tau: The Kendall-Tau (τ ) metric measures the\nsimilarity between the order of objects as located by the\nagent and the optimal ordering based on the ground-truth.\nIt shows the importance of the knowledge provided by the\nLLMs, for finding the optimal plan. We observe that the or-\ndering of the objects is not affected much by the low-level\nplanner. This is reasonable since the ordering should ma-\njorly depend on the plans generated by the high-level plan-\nner. As expected, the score drops when we replace GT with\nVO since LLM uses scene-graph to generate the plan. The\nconsiderable improvement in the score with gpt-4 (vs gtp-\n3.5-turbo) shows that using a better LLM enables an im-\nprovement in use of common-sense priors and yields more\noptimal ordering. Also, note that Kendall Tau metric doesn’t\napply to the baseline since it already has access to the opti-\nmal order of targets.\n4.6 Qualitative Results\nWe show an example of a typical episode in Figure 7 where\nthe agent is asked to locate an alarm-clock, a laptop, and a\ncellphone in an unknown house. The agent happens to start\nin the kitchen (determined by LLM based on perceived ob-\njects). The planner reasons that it is unlikely to find either of\nthe objects there, so it decides to go to another room through\na door. Then, it comes to a living-room where it is able to lo-\nScene LL SR SPL Kendall\nGraph Planner (%) Tau\nGT OrNav 77.86 0.37 0.72\nSayNav GT PNav 61.83 0.24 0.76\n(gpt-3.5) VO OrNav 58.73 0.40 0.72\nVO PNav 46.77 0.29 0.82\nGT OrNav 93.93 0.43 0.69\nSayNav GT PNav 86.36 0.35 0.77\n(gpt-4) VO OrNav 72.09 0.44 0.73\nVO PNav 61.60 0.35 0.77\nTable 2: Results of SayNav on MultiON task using LLM\nMemory.\ncate the laptop and cellphone. The third object still remains\nunfound, so it again decides to go to another room via a door.\nEventually, it locates the alarm-clock in the final room. The\ncomplete demo-video for this example can be found at our\nproject link.\n4.7 Memory via LLM\nAs mentioned in the section 3.3, SayNav uses the 3D scene\ngraph to support memory for future planning. For instance,\nit automatically annotates the room nodes that have already\nbeen explored and won’t plan for the room if the agent hap-\npens to visit the same room again. This type of implemen-\ntation to support memory works perfectly for our chosen\ntask. However, we also wanted to explore the possibility of\nmaking the LLM track its own plans. Hence, we also imple-\nmented SayNav’s memory via LLM by using Conversational\nChains module of the LangChain framework2. Here we use\ntwo separate instances of identical LLM models, one to gen-\nerate the high level plans and another to track the generated\nplans. The LLM tracking the generated plan, uses theRoom\nTracking Prompt in Figure 8 and is also equipped with a\nconversational memory. The LLM responsible for generat-\ning the plans receives detailed description of the surround-\ning environment while the other LLM instance only receives\nthe minimal information necessary to do the tracking. A key\nadvantage of this framework is that it avoids hitting the max-\nimum token limit on the LLM by not relying on the entire\nconversational history (as is usually done). We believe that\nLLM-based tracking might be able to generalize better to\nother tasks since it eliminates the need of a module for track-\ning plan history, which would require different implementa-\ntions for different task (we will test this in our future work).\nTable 2 shows the performance of SayNav using LLM\nMemory via gpt-3.5-turbo and gpt-4. Note that we use the\nsame model for both the instances of LLM in our experi-\nments. We do observe substantial drop in SR and SPL met-\nrics by using LLM Memory in the case of gpt-3.5-turbo as\ncompared to the results reported in Table 1. However, with\ngpt-4, the LLM Memory is able to achieve similar results as\ncompared to Table 1. This shows that it is feasible to hand-\nover the task of tracking to the better LLM models.\n2https://python.langchain.com/docs/modules/chains\n472\nFigure 8: Prompt used for room tracking via LLM.\n4.8 Real-World Demonstration\nWe also ported SayNav to a real robot and tested under var-\nious settings, for showcasing the efficient generalization of\nSayNav to real environments. The demonstration video for\na cafeteria environment is available at our project link.\n5 Limitations and Discussion\nThis section discusses the limitations of our work and some\nideas to improve SayNav in the future. As mentioned be-\nfore, our scene graph generation faces various challenges\nencountered in any perception-based algorithms. In addition\nto the glass door issue that we described earlier, Figure 10\nshows a failure case for another issue due to visual obser-\nvations. Note, in our experiments, the agent is not equipped\nwith arms to open/close the door. Therefore, it only can go\nthrough open doors to move to other rooms. In this episode,\nthe agent from most of the positions in the room cannot ob-\nserve that the door is open (which connects to the other room\nthat has the target object). The robot repeatedly tries to go\ntowards the center of the current room and refine the scene\ngraph. However, it is still not able to identify the “open” sta-\ntus for the door, and therefore, fails to achieve the goal.\nA better mechanism to verify attributes (open/close) as-\nsociated with the object node (door) in the scene graph can\nhelp to alleviate this case. For example, the agent can move\ncloser to the door, verify visual observations from all possi-\nble angles, and compare the depth information of the door\nto that of the wall (closed doors shall have nearly identical\nFigure 9: SayNav on a real robot.\nFigure 10: A failure example: The left picture shows the\nRGB image from the camera mounted on the agent and the\nright picture shows the top-down view of the house. Due to\ngeometry of the room, agent is unable to observe that the\ndoor is open and hence, unable to navigate through the door\n(marked with yellow rectangle).\ndepths as the connected wall). In the future, we would like\nto develop verification and feedback mechanisms to validate\nthe plans generated by LLMs for improving SayNav’s per-\nformance. We also plan to explore smaller LLMs (instead of\nGPT-3.5 and GPT-4), which can run locally on a real robot.\n6 Conclusion\nWe present SayNav, a new approach for efficient general-\nization to complex navigation tasks in unknown large-scale\nenvironments. SayNav incrementally builds and converts a\n3D scene graph of the explored environment to LLMs, for\ngenerating dynamic and contextually appropriate high-level\nplans for navigation. We evaluate SayNav on the MultiON\ntask, that requires the agent to efficiently search multiple\ndifferent objects in an unknown environment. Our results\ndemonstrate that SayNav even outperforms a strong oracle\nbased Point-nav baseline (64.34% vs 56.06%), for success-\nfully locating objects in large-scale new environments.\nAcknowledgements\nThis material is, in part, based upon work supported by the\nDefense Advanced Research Projects Agency (DARPA) un-\nder Contract No. HR001123C0089. Any opinions, findings,\nconclusions or recommendations expressed in this material\nare those of the author(s) and do not necessarily reflect the\nviews of the DARPA. We would like to thank Tixiao Shan\n473\nfor experiments and demonstrations in the real world. We\nalso thank Rakesh “Teddy” Kumar, Ajay Divakaran, and\nSupun Samarasekera for their valuable feedback to this ma-\nterial.\nReferences\nAhn, M.; et al. 2022. Do as i can, not as i say: Grounding\nlanguage in robotic affordances. In arXiv:2204.01691.\nAnderson, P.; et al. 2018. On evaluation of embodied navi-\ngation agents. arXiv:1807.06757.\nArmeni, I.; et al. 2019. 3d scene graph: A structure for uni-\nfied semantics, 3d space, and camera. In CVPR.\nBrown, T.; et al. 2020. Language models are few-shot learn-\ners. NeurIPS, 33: 1877–1901.\nChaplot, D.; et al. 2020. Object goal navigation using goal-\noriented semantic exploration. NeurIPS, 33: 4247–4258.\nChen, P.; et al. 2022. Learning Active Camera for Multi-\nObject Navigation. In NeurIPS.\nCho, K.; et al. 2014. Learning phrase representations using\nRNN encoder-decoder for statistical machine translation. In\narXiv:1406.1078.\nChung, H. W.; et al. 2022. Scaling instruction-finetuned lan-\nguage models. arXiv:2210.11416.\nDeitke, M.; et al. 2022. ProcTHOR: Large-Scale Embodied\nAI Using Procedural Generation. NeurIPS, 35: 5982–5994.\nDriess, D.; et al. 2023. Palm-e: An embodied multimodal\nlanguage model. In arXiv:2303.03378.\nGireesh, N.; et al. 2023. Sequence-Agnostic Multi-Object\nNavigation. arXiv:2305.06178.\nHe, K.; et al. 2016. Deep residual learning for image recog-\nnition. In CVPR.\nHuang, W.; et al. 2022. Inner monologue: Embodied\nreasoning through planning with language models. In\narXiv:2207.05608.\nHughes, N.; et al. 2022. Hydra: A real-time spatial percep-\ntion engine for 3d scene graph construction and optimiza-\ntion. In Robotics: Science and Systems.\nKhandelwal, A.; et al. 2022. Simple but effective: Clip em-\nbeddings for embodied ai. In CVPR, 14829–14838.\nKim, U.; et al. 2019. 3-D scene graph: A sparse and seman-\ntic representation of physical environments for intelligent\nagents. IEEE Transactions on Cybernetics, 50(12): 4921–\n4933.\nKolve, E.; et al. 2017. Ai2-thor: An interactive 3d environ-\nment for visual ai. arXiv preprint arXiv:1712.05474.\nLapata, M. 2006. Automatic evaluation of information or-\ndering: Kendall’s tau. Computational Linguistics, 32(4):\n471–484.\nLiu, B.; et al. 2023. LLM+P: Empowering large lan-\nguage models with optimal planning proficiency. In\narXiv:2304.11477.\nMarza, P.; et al. 2022. Teaching Agents how to Map: Spatial\nReasoning for Multi-Object Navigation. In IROS.\nMarza, P.; et al. 2023. Multi-Object Navigation with dynam-\nically learned neural implicit representations. In ICCV.\nMishkin, D.; et al. 2019. Benchmarking classic and\nlearned navigation in complex 3d environments. In\narXiv:1901.10915.\nOuyang, L.; et al. 2022. Training language models to follow\ninstructions with human feedback. In arXiv:2203.02155.\nPeng, B.; et al. 2023. Instruction tuning with gpt-4.\narXiv:2304.03277.\nRamrakhya, R.; et al. 2022. Habitat-web: Learning embod-\nied object-search strategies from human demonstrations at\nscale. In CVPR, 5173–5183.\nRamrakhya, R.; et al. 2023. Pirlnav: Pretraining with imita-\ntion and rl finetuning for objectnav. InCVPR, 17896–17906.\nRana, K.; et al. 2023. SayPlan: Grounding Large Language\nModels using 3D Scene Graphs for Scalable Task Planning.\nIn arXiv:2307.06135.\nRosinol, A.; et al. 2021. Kimera: From slam to spatial per-\nception with 3d dynamic scene graphs. The International\nJournal of Robotics Research, 40(12–14): 1510–1546.\nRoss, S.; et al. 2011. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In AIS-\nTATS. JMLR Workshop and Conference Proceedings.\nSavva, M.; et al. 2019. Habitat: A Platform for Embodied\nAI Research. In ICCV.\nShah, D.; et al. 2023. ViNT: A Foundation Model for Visual\nNavigation. arXiv:2306.14846.\nSingh, I.; et al. 2023. Progprompt: Generating situated robot\ntask plans using large language models. In ICRA.\nSong, C. H.; et al. 2022. Llm-planner: Few-shot grounded\nplanning for embodied agents with large language models.\nIn arXiv:2212.04088.\nSzot, A.; et al. 2021. Habitat 2.0: Training home assistants\nto rearrange their habitat. NeurIPS, 34: 251–266.\nWald, J.; et al. 2020. Learning 3D semantic scene graphs\nfrom 3D indoor reconstructions. In CVPR.\nWani, S.; et al. 2020. MultiON: Benchmarking Semantic\nMap Memory using Multi-Object Navigation. In NeurIPS.\nWeihs, L.; et al. 2020. Allenact: A framework for embodied\nai research. arXiv preprint arXiv:2008.12760.\nWijmans, E.; et al. 2019. Dd-ppo: Learning near-\nperfect pointgoal navigators from 2.5 billion frames. In\narXiv:1911.00357.\nWu, S.; et al. 2021. SceneGraphFusion: Incremental 3D\nscene graph prediction from RGB-D sequences. In CVPR.\nWu, Y .; et al. 2018. Group normalization. InECCV, 3–19.\nYu, B.; et al. 2023. Leveraging Large Language Models for\nVisual Target Navigation. In arXiv:2304.05501.\nZeng, H.; et al. 2023. Multi-Object Navigation Using Poten-\ntial Target Position Policy Function. IEEE Transactions on\nImage Processing, 32: 2608 – 2619.\nZhao, W. X.; et al. 2023. A survey of large language models.\narXiv:2303.18223.\nZhu, Y .; et al. 2017. Target-driven visual navigation in in-\ndoor scenes using deep reinforcement learning. In ICRA.\n474",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7927985787391663
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5851254463195801
    },
    {
      "name": "Planner",
      "score": 0.5575135946273804
    },
    {
      "name": "Task (project management)",
      "score": 0.5535557270050049
    },
    {
      "name": "Oracle",
      "score": 0.5203754305839539
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5003459453582764
    },
    {
      "name": "Human–computer interaction",
      "score": 0.47345632314682007
    },
    {
      "name": "Scene graph",
      "score": 0.4529031217098236
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.4410780966281891
    },
    {
      "name": "Code (set theory)",
      "score": 0.43398404121398926
    },
    {
      "name": "Machine learning",
      "score": 0.36337679624557495
    },
    {
      "name": "Software engineering",
      "score": 0.15745985507965088
    },
    {
      "name": "Programming language",
      "score": 0.11567756533622742
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1298353152",
      "name": "SRI International",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1280581677",
      "name": "Defense Advanced Research Projects Agency",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I188538660",
      "name": "University of Colorado Boulder",
      "country": "US"
    }
  ],
  "cited_by": 37
}