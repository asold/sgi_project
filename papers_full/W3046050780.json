{
  "title": "Transformers on Sarcasm Detection with Context",
  "url": "https://openalex.org/W3046050780",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2120314922",
      "name": "Amardeep Kumar",
      "affiliations": [
        "Indian Institute of Technology Dhanbad"
      ]
    },
    {
      "id": "https://openalex.org/A2201950551",
      "name": "Vivek Anand",
      "affiliations": [
        "Indian Institute of Technology Hyderabad"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2575367545",
    "https://openalex.org/W2888643222",
    "https://openalex.org/W2607623312",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2510141903",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2508865106",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963635943"
  ],
  "abstract": "Sarcasm Detection with Context, a shared task of Second Workshop on Figurative Language Processing (co-located with ACL 2020), is study of effect of context on Sarcasm detection in conversations of Social media. We present different techniques and models, mostly based on transformer for Sarcasm Detection with Context. We extended latest pre-trained transformers like BERT, RoBERTa, spanBERT on different task objectives like single sentence classification, sentence pair classification, etc. to understand role of conversation context for sarcasm detection on Twitter conversations and conversation threads from Reddit. We also present our own architecture consisting of LSTM and Transformers to achieve the objective.",
  "full_text": "Proceedings of the Second Workshop on Figurative Language Processing, pages 88–92\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n88\nTransformers on Sarcasm Detection with Context\nAmardeep Kumar\nIndian Institute of Technology\n(ISM), Dhanbad\nadkr6398@gmail.com\nVivek Anand\nIIIT Hyderabad, India\nvivek.a@research.iiit.ac.in\nAbstract\nSarcasm Detection with Context, a shared task\nof Second Workshop on Figurative Language\nProcessing (co-located with ACL 2020), is\nstudy of effect of context on Sarcasm detection\nin conversations of Social media. We present\ndifferent techniques and models, mostly based\non transformer for Sarcasm Detection with\nContext. We extended latest pre-trained trans-\nformers like BERT, RoBERTa, spanBERT on\ndifferent task objectives like single sentence\nclassiﬁcation, sentence pair classiﬁcation, etc.\nto understand role of conversation context for\nsarcasm detection on Twitter conversations\nand conversation threads from Reddit. We\nalso present our own architecture consisting of\nLSTM and Transformers to achieve the objec-\ntive.\n1 Introduction\nWith advent of Internet and Social media platforms,\nit is important to know actual sentiments and be-\nliefs of its users, and recognizing Sarcasm is very\nimportant for this. We can’t always decide if a sen-\ntence is sarcastic or not without knowing its context.\nFor example, consider below two sentences S1 and\nS2.\nS1: ”What you love on weekends?”\nS2: ”I love going to the doctor.”\nJust by looking at the ’S2’ sentence we can tag\nthe sentence ’S2’ as ”not sarcastic”, but imagine\nthis sentence as a reply to the sentence ’S1’ , now\nwe would like to tag the sentence ’S2’ as”sarcas-\ntic”. Hence it is necessary to know the context of a\nsentence to know sarcasm.\nWe were provided with conversation threads\nfrom two of popular social media, Reddit and Twit-\nter. For this objective We used different pre-trained\nlanguage model and famous transformer architec-\nture like BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019) and spanBERT (Joshi et al., 2020). We\nalso propose our own architecture made of Trans-\nformers (Vaswani et al., 2017) and LSTM (Hochre-\niter and Schmidhuber, 1997).\n2 Datasets\nTwo types of Datasets were used, corpus from Twit-\nter conversations and conversation threads from\nReddit.\nTwitter Corpus Ghosh et al. (2018) introduced\na self label twitter conversations corpus. The sar-\ncastic tweets were collected by relying upon hash-\ntags, like sarcasm, sarcastic, etc., that users assign\nto their sarcastic tweets. For non-sarcastic they\nadopted a methodology, according to which Non-\nsarcastic tweet doesn’t contain sarcasm hashtag\ninstead they were having sentiments hashtag like\nhappy, positive, sad, etc.\nReddit Corpus Khodak et al. (2018) collected\n1.5 million sarcastic statement and many of non-\nsarcastic statement from Reddit. They self anno-\ntated all of these Reddit corpus manually.\nFor both datasets, the training and testing data\nwas provided in json format where each utterance\ncontains the following ﬁelds: 1) ”label” : SAR-\nCASM or NOT SARCASM. For test data, label\nwas not provided. 2) ”response” : the sarcastic\nresponse, whether a sarcastic Tweet or a Reddit\npost. 3) ”context” : the conversation context of the\n”response”. 4) ”id” : unique id to identify and label\neach data point in test dataset.\nTwitter data set is of 5,000 English Tweets\nbalanced between the ”SARCASM” and\n”NOT SARCASM” classes and Reddit dataset\nis of 4,400 Reddit posts balanced between the\n”SARCASM” and ”NOT SARCASM” classes.\n3 Pre-Process\nWe used different text pre-processing technique to\nremove noise from text provided to us. We removed\nunwanted punctuation, multiple spaces, URL tags,\netc. We changed different abbreviations to their\n89\nproper format, for example: ”I’m” was changed to\n”I am”, ”idk” to ”I don’t know”, etc.\n4 Experiments\nWe experimented with different transformers and\npretrained models like BERT , RoBERTa, span-\nBERT and our own architecture built over these\nTransformers.\nFor both datasets, each training and testing ut-\nterance contains two major ﬁelds: ”response” (i.e,\nthe sarcastic response, whether a sarcastic Tweet\nor a Reddit post), ”context” (i.e., the conversation\ncontext of the ”response”). The ”context” is an or-\ndered list of dialogue, i.e., if the ”context” contains\nthree elements, ”c1”, ”c2”, ”c3”, in that order, then\n”c2” is a reply to ”c1” and ”c3” is a reply to ”c2”.\nFurther, if the sarcastic ”response” is ”r”, then ”r”\nis a reply to ”c3”. For instance, for the following\nexample, ”label”: ”SARCASM”, ”response”: ”Did\nKelly just call someone else messy? Baaaahaaa-\nhahahaha”, ”context”: [”X is looking a First Lady\nshould”, ”didn’t think it was tailored enough it\nlooked messy”]. The response tweet, ”Did Kelly...”\nis a reply to its immediate context ”didn’t think it\nwas tailored...” which is a reply to ”X is looking...”.\nFor each utterance in datasets, We deﬁned ’re-\nsponse’ as response string and concatenation of all\nthe ’context’ in reverse order as contextstring.\nresponse string = ”response”\ncontext string = ”c3” + ”c2” + ”c1”\nWe approached this classiﬁcation task in two\nways, ﬁrst as Single sentence classiﬁcation task\nand second as Sentence pair classiﬁcation tasks.\nWe also experimented single sentence classiﬁca-\ntion only with response string. Throughout the\nexperiment we used ’transformers’ library by Hug-\nging Face (Wolf et al., 2019) for experimenting\nwith BERT and RoBERTa models and for span-\nBERT we used their ofﬁcial released code, and\nincorporated new methods to suit our task.\n4.1 Single sentence Classiﬁcation Task\nAs name indicates, to obtain a single sentence for\nclassiﬁcation, we concatenated response string and\ncontext string.\nFigure 1 represents general architecture of mod-\nels used in subsection 4.1.1, 4.1.2 and 4.1.3, for\nsingle sentence classiﬁcation where:\n• Input : response string + context string\nFigure 1: Transformer model for single sentence classi-\nﬁcation\n• Transformer: layer could be any of the model\nfrom BERT, RoBERTa or spanBERT as trans-\nformer.\n• Embedding output: is representation of\n”[CLS]” token by transformer, used for classi-\nﬁcation task.\n• Feed Forward Network : has multiple dense\nand dropout layer.\n• Softmax: classiﬁer for binary classiﬁcation.\n4.1.1 BERT\nDevlin et al. (2019) introduced Bidirectional En-\ncoder Representations from Transformers(BERT).\nBERT’s key technical innovation is applying bidi-\nrectional training of Transformers to language mod-\neling. BERT is pre-trained on two objectives,\nMasked language modeling (MLM) and next sen-\ntence prediction (NSP).\nWe used ’bert-base-uncased’ and ’bert-large-\nuncased’ pretrained model in transformer layer.\n’bert-base-uncased’ has 12-layers, 768-hidden state\nsize, 12-attention heads and 110M parameters,\nwith each hidden state of (max seq len, 768) size\nand embedding output of 768 length. ’bert-large-\nuncased’ has 24-layers, 1024-hidden state size, 16-\nattention heads and 340M parameters. it has each\nhidden state of (max seq len, 1024) size and em-\nbedding output of 1024 length. ’bert-large-uncased’\ngave better results than ’bert-base-uncased’ on both\ndatasets.\n4.1.2 SpanBERT\nJoshi et al. (2020) introduced pretraining method to\nrepresent and predict span instead of words. This\napproach is different from BERT based pretraining\nmethods in two ways:\n1. Masking contiguous random spans instead of\nmasking random tokens.\n90\n2. Span Boundary Objective: Predicting entire\ncontent of masked span with help of hidden\nstates of boundary token of masked span.\nWe used ’spanbert-base-cased’ and ’spanbert-large-\ncased’ pretrained model as transformer layer.\n’spanbert-base-cased’ has 12-layers, 768-hidden\nstate size, 12-attention heads and 110M parame-\nters, with each hidden state of (max seq len, 768)\nsize and embedding output of 768 length. ’spanbert-\nlarge-cased’ has 24-layers, 1024-hidden state size,\n16-attention heads and 340M parameters. It has\neach hidden state of (max seq len, 1024) size\nand embedding output of 1024 length. ’spanbert-\nlarge-cased’ gave better results than ’spanbert-\nbase-cased’, ’bert-base-uncased’ and ’bert-large-\nuncased’ respectively on both datasets .\n4.1.3 RoBERTa\nLiu et al. (2019) presented a replication study of\nBERT pre-training, related to impact of key hyper-\nparameter and size of training data on which it\nwas pre-trained, and found BERT as signiﬁcantly\nuntrained.\nWe tried only roberta large models, which has 24-\nlayers, 1024-hidden state size, 24-attention heads\nand 355M parameters. it has each hidden state of\n(max seq len, 1024) size and embedding output of\n1024 length. ’roberta-large’ gave better results than\nall previous models.\n4.1.4 LSTM over Transformer\nFigure 2: Architecture of model 4.1.4\nTo improvise, We modiﬁed previously used\nmodel architecture. Figure 2 represents architec-\nture of our successful improvised model, where:\n• HS[-1], HS[-2]: represent last hidden state\nand second last hidden state output by trans-\nformer respectively.\n• Concatenation: layer concatenate two or more\ntensors along suitable axis.\n• LSTM: Hochreiter and Schmidhuber (1997)\nIn this model, last two hidden states are concate-\nnated and passed through LSTM to get more con-\ntextual representation of text. Later output of\nLSTM and embedding output of transformer is\nconcatenated and fed through feed forward Neural\nnetwork for classiﬁcation.\nWe tried ’bert-large-uncased’ and ’Roberta large’\nas transformer layer in this architecture. ’Roberta\nlarge’ gave best f1-score among all. This model\nalso gave best result on classiﬁcation using only\n’response string’ as input on both datsets.\n4.2 Sentence pair Classiﬁcation task\nIn this Sentence Pair classiﬁcation task, we give a\npair of text as input for binary classiﬁcation. We\npresent following two models:\n4.2.1 Siamese Transformer\nOur architecture was inspired from two things, ﬁrst\nis intuition that it may be a case that only ’response’\nis Sarcastic but not concatenation of ’response’ and\n’context’, and second, Siamese network (Mueller\nand Thyagarajan, 2016).\nFigure 3: Architecture of Proposed Siamese Trans-\nformer\nFigure 3 represents our Siamese Transformer,\nwhere: ’input 1’ is response string, ’input 2’ is\nresponse string + context string, ’Softmax’ is last\nsoftmax layer intuitively work as ’OR’ logical gate.\nWe expected improvement in result over previ-\nous models, but it didn’t happen. This also estab-\nlishes that context is necessary for Sarcasm Detec-\ntion.\n4.2.2 Dual Transformer\nLength of context string is larger than re-\nsponse string so it might be that their combined\ncontextual representation is dominated by ’con-\ntext string’. To overcome this, we pass them\nthrough different transformers to get their individ-\nual representation of equal size. These represen-\ntation are then concatenated and passed through\nBi-LSTM to get contextual representation of the\n91\nFigure 4: Architecture of Dual Transformer model\ncombination. Figure 4 represents our architec-\nture of Dual transformer, where: ’input 1’ is re-\nsponse string, ’input 2’ is context string, ’BiL-\nSTM’ is bidirectional LSTM (Schuster and Paliwal,\n1997)\nLast hidden state output of both transformers are\nconcatenated and passed over Bi-LSTM to get a\nbetter contextual, output of which is passed through\na classiﬁcation layer. This model didn’t give better\nresults as expected. We guessed lack of training\ndata as one of the possible reason.\n5 Results\nModel P R f1\nresponse onlySS 68.7 71.8 67.50\nbert-baseSS 74.7 74.8 74.62\nbert-largeSS 75.1 75.1 75.03\nroberta-largeSS 76.1 76.1 76.05\nspanbert-baseSS 74.9 75.2 74.89\nspanbert-largeSS 75.7 75.9 75.68\nbert-largeLoT 76.5 76.7 76.44\nroberta-largeLoT 77.3 77.4 77.22\nroberta-largeST 75.5 75.5 75.49\nroberta-largeDT 75.9 76.2 75.88\nTable 1: Result on Twitter\nModel P R f1\nresponse onlySS 64.2 64.7 63.83\nbert-baseSS 66.5 66.6 66.47\nbert-largeSS 67.3 67.3 67.27\nroberta-largeSS 67.5 67.5 67.49\nspanbert-baseSS 66.9 67.3 66.75\nspanbert-largeSS 67.4 67.4 67.36\nbert-largeLoT 68.1 68.1 68.0\nroberta-largeLoT 69.3 69.9 69.11\nroberta-largeST 67.9 68.1 67.86\nroberta-largeDT 68.1 68.1 68.1\nTable 2: Result on Reddit\nTable 1 and Table 2 depict results of all mod-\nels and tasks on Twitter and Reddit datasets re-\nspectively. In both table ’SS’ denotes single sen-\ntence classiﬁcation task, ’LoT’ denotes LSTM\nover Transformer(4.1.4), ’DT’ denotes Dual Trans-\nformer(4.2.2) and ’ST’ denotes Siamese Trans-\nformer (4.2.1).\nUsing only ’response string’ (i.e without using\ncontext) we got best f1-score of 67.50 and 63.2\non Twitter and Reddit datsets respectively. Using\nresponse as well as context, LSTM over Trans-\nformer model (sub-section 4.1.4) with ’robert-large’\nas transformer layer performed best. We tried dif-\nferent maximum sequence legth, 126 on Twitter\nconversation and 80 on Reddit Conversation text\ngave the best results. We didn’t benchmark our re-\nsults with Ghosh et al. (2018), Zhang et al. (2016),\netc. related works, becuase those models were\ntrained on different datasets. To do a fair compar-\nison, we would have to re-train those models on\nour dataset, but due to computational constraints\nwe were unable to do this.\n6 Related Work\nMost of the existing works are on detecting sarcasm\nwithout considering context. Joshi et al. (2016) ,\nZhang et al. (2016) , Ghosh et al. (2018) have con-\nsidered context and utterances separately for sar-\ncasm detection and showed how context is helpful\nin sarcasm detection.\n7 Conclusion\nTo conclude, we showed effective method for sar-\ncasm detection and how much context is necessary\nfor it. We didn’t use any dataset (reddit and twitter)\nspeciﬁc pre-processing or hyperparameter tuning\nin order to evaluate effectiveness of models across\nvarious types of data. In future, we would like to\nexperiment with supplementing external data or\nmerging different types of data on this task.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n92\nDebanjan Ghosh, Alexander R. Fabbri, and Smaranda\nMuresan. 2018. Sarcasm analysis using conversa-\ntion context. Computational Linguistics, 44(4):755–\n792.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nAditya Joshi, Vaibhav Tripathi, Pushpak Bhat-\ntacharyya, and Mark J. Carman. 2016. Harnessing\nsequence labeling for sarcasm detection in dialogue\nfrom TV series ‘Friends’. In Proceedings of The\n20th SIGNLL Conference on Computational Natu-\nral Language Learning, pages 146–155, Berlin, Ger-\nmany. Association for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanbert: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nMikhail Khodak, Nikunj Saunshi, and Kiran V odra-\nhalli. 2018. A large self-annotated corpus for sar-\ncasm. In Proceedings of the Eleventh International\nConference on Language Resources and Evalua-\ntion (LREC-2018), Miyazaki, Japan. European Lan-\nguages Resources Association (ELRA).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nJonas Mueller and Aditya Thyagarajan. 2016. Siamese\nrecurrent architectures for learning sentence similar-\nity. In thirtieth AAAI conference on artiﬁcial intelli-\ngence.\nMike Schuster and Kuldip K Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE transactions\non Signal Processing, 45(11):2673–2681.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nMeishan Zhang, Yue Zhang, and Guohong Fu. 2016.\nTweet sarcasm detection using deep neural network.\nIn Proceedings of COLING 2016, the 26th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 2449–2460, Osaka, Japan.\nThe COLING 2016 Organizing Committee.",
  "topic": "Sarcasm",
  "concepts": [
    {
      "name": "Sarcasm",
      "score": 0.9951098561286926
    },
    {
      "name": "Conversation",
      "score": 0.7358157634735107
    },
    {
      "name": "Computer science",
      "score": 0.714743971824646
    },
    {
      "name": "Sentence",
      "score": 0.6756082773208618
    },
    {
      "name": "Transformer",
      "score": 0.6554743051528931
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5224534273147583
    },
    {
      "name": "Natural language processing",
      "score": 0.5121722221374512
    },
    {
      "name": "Literal and figurative language",
      "score": 0.4978470802307129
    },
    {
      "name": "Speech recognition",
      "score": 0.3807157874107361
    },
    {
      "name": "Linguistics",
      "score": 0.25893181562423706
    },
    {
      "name": "Communication",
      "score": 0.21308964490890503
    },
    {
      "name": "Psychology",
      "score": 0.20741313695907593
    },
    {
      "name": "Irony",
      "score": 0.11526989936828613
    },
    {
      "name": "Engineering",
      "score": 0.11208590865135193
    },
    {
      "name": "Electrical engineering",
      "score": 0.07828262448310852
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}