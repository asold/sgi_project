{
  "title": "UDALM: Unsupervised Domain Adaptation through Language Modeling",
  "url": "https://openalex.org/W3155145134",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5053198720",
      "name": "Constantinos Karouzos",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026032406",
      "name": "Georgios Paraskevopoulos",
      "affiliations": [
        "National Technical University of Athens"
      ]
    },
    {
      "id": "https://openalex.org/A5084949286",
      "name": "Alexandros Potamianos",
      "affiliations": [
        "National Technical University of Athens"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891975605",
    "https://openalex.org/W2963261224",
    "https://openalex.org/W2963326042",
    "https://openalex.org/W2994806031",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W2511131004",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3114610051",
    "https://openalex.org/W2594718649",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2101210369",
    "https://openalex.org/W3013522441",
    "https://openalex.org/W2953217208",
    "https://openalex.org/W2138382875",
    "https://openalex.org/W1597032530",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2892202790",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2131953535",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W2153353890",
    "https://openalex.org/W3089183975",
    "https://openalex.org/W3083515246",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963463240",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2998635301",
    "https://openalex.org/W3090047754",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2140398982",
    "https://openalex.org/W2120587290",
    "https://openalex.org/W2803777992",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2133556223",
    "https://openalex.org/W2962970380",
    "https://openalex.org/W2964068917",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2998172865",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W3152368098",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W2963777311",
    "https://openalex.org/W3030416750",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3035431747",
    "https://openalex.org/W2891691791"
  ],
  "abstract": "In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding $91.74\\%$ accuracy, which is an $1.11\\%$ absolute improvement over the state-of-the-art.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 2579–2590\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n2579\nUDALM: Unsupervised Domain Adaptation through Language Modeling\nConstantinos Karouzos1, Georgios Paraskevopoulos1,4, Alexandros Potamianos1,2,3\n1 School of ECE, National Technical University of Athens, Athens, Greece\n2 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA, USA\n3 Behavioral Signal Technologies, Los Angeles, CA, USA\n4 Institute for Language and Speech Processing, Athena Research Center, Athens, Greece\nckarouzos@gmail.com, geopar@central.ntua.gr, potam@central.ntua.gr\nAbstract\nIn this work we explore Unsupervised Do-\nmain Adaptation (UDA) of pretrained lan-\nguage models for downstream tasks. We intro-\nduce UDALM, a ﬁne-tuning procedure, using\na mixed classiﬁcation and Masked Language\nModel loss, that can adapt to the target do-\nmain distribution in a robust and sample efﬁ-\ncient manner. Our experiments show that per-\nformance of models trained with the mixed\nloss scales with the amount of available tar-\nget data and the mixed loss can be effectively\nused as a stopping criterion during UDA train-\ning. Furthermore, we discuss the relationship\nbetween A-distance and the target error and ex-\nplore some limitations of the Domain Adver-\nsarial Training approach. Our method is eval-\nuated on twelve domain pairs of the Amazon\nReviews Sentiment dataset, yielding 91.74%\naccuracy, which is an1.11% absolute improve-\nment over the state-of-the-art.\n1 Introduction\nDeep architectures have achieved state-of-the-art\nresults in a variety of machine learning tasks. How-\never, real world deployments of machine learning\nsystems often operate under domain shift, which\nleads to performance degradation. This introduces\nthe need for adaptation techniques, where a model\nis trained with data from a speciﬁc domain, and\nthen can be optimized for use in new settings. Efﬁ-\ncient techniques for model re-usability can lead to\nfaster and cheaper development of machine learn-\ning applications and facilitate their wider adoption.\nEspecially techniques for Unsupervised Domain\nAdaptation (UDA) can have high real world im-\npact, because they do not rely on expensive and\ntime-consuming annotation processes to collect la-\nbeled data for domain-speciﬁc supervised training,\nfurther streamlining the process.\nUDA approaches in the literature can be grouped\nin three major categories, namely pseudo-labeling\ntechniques (e.g. Yarowsky, 1995; Zhou and Li,\n2005), domain adversarial training (e.g. Ganin\net al., 2016) and pivot-based approaches (e.g.\nBlitzer et al., 2006; Pan et al., 2010). Pseudo-\nlabeling approaches use a model trained on the\nsource labeled data to produce pseudo-labels for\nunlabeled target data and then train a model for\nthe target domain in a supervised manner. Do-\nmain adversarial training aims to learn a domain-\nindependent mapping for input samples by adding\nan adversarial cost during model training, that min-\nimizes the distance between the source and target\ndomain distributions. Pivot-based approaches aim\nto select domain-invariant features (pivots) and use\nthem as a basis for cross-domain mapping. This\nwork does not fall under any of these categories,\nrather we aim to optimize the ﬁne-tuning procedure\nof pretrained language models (LMs) for learning\nunder domain-shift.\nTransfer learning from language models pre-\ntrained in massive corpora (Howard and Ruder,\n2018; Devlin et al., 2019; Yang et al., 2019; Liu\net al., 2019; Brown et al., 2020) has yielded signif-\nicant improvements across a wide variety of NLP\ntasks, even when small amounts of data are used\nfor ﬁne-tuning. Fine-tuning a pretrained model is\na straightforward framework for adaptation to tar-\nget tasks and new domains, when labeled data are\navailable. However, optimizing the ﬁne-tuning pro-\ncess in UDA scenarios, where only labeled out-of-\ndomain and unlabeled in-domain data are available\nis challenging.\nIn this work, we propose UDALM, a ﬁne-tuning\nmethod for BERT (Devlin et al., 2019) in order to\naddress the UDA problem. Our method is based\non simultaneously learning the task from labeled\ndata in the source distribution, while adapting to\nthe language in the target distribution using multi-\ntask learning. The key idea of our method is that\nby simultaneously minimizing a task-speciﬁc loss\non the source data and a language modeling loss\non the target data during ﬁne-tuning, the model\n2580\nwill be able to adapt to the language of the target\ndomain, while learning the supervised task from\nthe available labeled data.\nOur key contributions are: (a) We introduce\nUDALM, a novel, simple and robust unsuper-\nvised domain adaptation procedure for downstream\nBERT models based on multitask learning, (b) we\nachieve state-of-the-art results for the Amazon re-\nviews benchmark dataset, surpassing more com-\nplicated approaches and (c) we explore how A-\ndistance and the target error are related and con-\nclude with some remarks on domain adversarial\ntraining, based on theoretical concepts and our em-\npirical observations. Our code and models are pub-\nlicly available1.\n2 Related Work\nTraditionally, UDA has been performed using\npseudo-labeling approaches. Pseudo-labeling tech-\nniques are semi-supervised algorithms that either\nuse the same model (self-training) (Yarowsky,\n1995; McClosky et al., 2006; Abney, 2007) or\nmultiple ensembles of models (tri-training) (Zhou\nand Li, 2005; Søgaard, 2010) in order to produce\npseudo-labels for the target unlabeled data. Saito\net al. (2017) proposed an asymmetric tri-training\napproach. Ruder and Plank (2018) introduced a\nmulti-task tri-training method. Rotman and Re-\nichart (2019) and Lim et al. (2020) study pseudo-\nlabeling with contextualized word representations.\nYe et al. (2020) combine self-training with XLM-\nR (Conneau et al., 2020) to reduce the produced\nlabel noise and propose CFd, class aware feature\nself-distillation.\nAnother line of UDA research includes pivot-\nbased methods, focusing on extracting cross-\ndomain features. Structural Correspondence Learn-\ning (SCL) (Blitzer et al., 2006) and Spectral Feature\nAlignment (Pan et al., 2010) aim to ﬁnd domain-\ninvariant features (pivots) to learn a mapping be-\ntween two domain distributions. Ziser and Reichart\n(2017, 2018, 2019) combine SCL with neural net-\nwork architectures and language modeling. Miller\n(2019) propose to jointly learn the task and pivots.\nLi et al. (2018b) learn pivots with hierarchical at-\ntention networks. Pivot-based methods have also\nbeen used in conjunction with BERT (Ben-David\net al., 2020).\nDomain adversarial training is a dominant ap-\nproach for UDA (Ramponi and Plank, 2020), in-\n1https://github.com/ckarouzos/slp_daptmlm\nspired by the theory for learning from different do-\nmains introduced in Ben-David et al. (2007, 2010).\nGanin et al. (2016); Ganin and Lempitsky (2015)\npropose to learn a task while not being able to dis-\ntinguish if samples come from the source or the\ntarget distribution, through use of an adversarial\ncost. This approach has been adopted for a diverse\nset of problems, e.g. sentiment analysis, tweet\nclassiﬁcation and universal dependency parsing (Li\net al., 2018a; Alam et al., 2018; Sato et al., 2017).\nDu et al. (2020) pose domain adversarial training\nin the context of BERT models. Zhao et al. (2018)\npropose multi-source domain adversarial networks.\nGuo et al. (2018) propose a mixture-of-experts ap-\nproach for multi-source UDA. Guo et al. (2020)\nexplore distance measures as additional losses and\nuse them to construct dynamic multi-armed ban-\ndit controller for the source domains. Shen et al.\n(2018) learn domain invariant features via Wasser-\nstein distance. Bousmalis et al. (2016) introduce do-\nmain seperation networks with private and shared\nencoders.\nUnsupervised pretraining on domain-speciﬁc\ncorpora can be an effective adaptation process. For\nexample BioBERT (Lee et al., 2020) and SciB-\nERT (Beltagy et al., 2019) are specialized BERT\nvariants, where pretraining is extended on large\namounts of biomedical and scientiﬁc corpora re-\nspectively. Sun et al. (2019) propose continuing the\npretraining of BERT with target domain data and\nmultitask learning using relevant tasks for BERT\nﬁne-tuning. Xu et al. (2019) introduce a review\nreading comprehension task and a post-training\napproach for BERT with an auxiliary loss on a\nquestion-answering task. Continuing pretraining\non multiple phases, from general to domain spe-\nciﬁc (DAPT) and task speciﬁc data (TAPT), further\nimproves performance of pretrained language mod-\nels, as shown by Gururangan et al. (2020). Han and\nEisenstein (2019) propose AdaptaBERT, which in-\ncludes a second phase of unsupervised pretraining,\nin order to use BERT in a unsupervised domain\nadaptation context.\nRecent works have highlighted the merits of us-\ning Language Modeling as an auxiliary task during\nﬁne-tuning. Chronopoulou et al. (2019) use an aux-\niliary LM loss to avoid catastrophic forgetting in\ntransfer learning and Jia et al. (2019) adopt this\napproach for cross-domain named-entity recogni-\ntion. We draw inspiration from these approaches\nand utilize auxiliary Language Modeling for UDA.\n2581\n(a)\n (b)\n (c)\nFigure 1: (a) BERT (Devlin et al., 2019) is pretrained on English Wikipedia and BookCorpus with the Masked\nLanguage Modeling (MLM) and the Next Sentence Prediction (NSP) tasks. (b) We continue the pretraining of\nBERT on unlabeled target domain data using the MLM task. (c) We train a task classiﬁer with source domain\nlabeled data, while we keep the MLM objective on unlabeled target domain data.\n3 Problem Deﬁnition\nLet X be the input space and Y the set of labels.\nFor binary classiﬁcation tasks Y = {0,1}. In do-\nmain adaptation there are two different distribu-\ntions over X ×Y, called the source domain DS\nand the target domain DT. In the unsupervised\nsetting labels are provided for samples drawn from\nDS, while samples drawn from DT are unlabeled.\nThe goal is to train a model that performs well\non samples drawn from the target distribution DT.\nThis is summarized in Eq. 1.\nS = (xi,yi)n\ni=1 ∼(DS)n\nT = (xi)n+m\ni=n+1 ∼(DX\nT )m (1)\nwhere DX\nT is the marginal distribution of DT over\nX, n is the number of samples from the source\ndomain and mis the number of samples from the\ntarget domain.\n4 Proposed Method\nFig. 1 gives an overview of the proposed Unsu-\npervised Domain Adaptation through Language\nModeling (UDALM). Starting from a model that is\npretrained in general corpora (Fig. 1a), we keep pre-\ntraining it on target domain data using the masked\nlanguage modeling task (Fig. 1b). On the ﬁnal\nﬁne-tuning step (Fig. 1c) we update the model\nweights using both a classiﬁcation loss on the la-\nbeled source data and Masked Language Modeling\nloss on the unlabeled target data.\nIn Fig. 1a we see the BERT general pretrain-\ning phase. BERT (Devlin et al., 2019) is based\non the Transformer architecture (Vaswani et al.,\n2017). During BERT pretraining, input tokens are\nrandomly selected to be masked. BERT is trained\nusing the Masked Language Modeling (MLM) ob-\njective, which consists of predicting the most proba-\nble tokens for the masked positions. Additionally it\nuses a Next Sentence Prediction (NSP) loss, which\nclassiﬁes whether the pair of input sentences are\ncontinuous or not. If a labeled dataset is available,\na pretrained BERT model can be ﬁne-tuned for the\ndownstream task in a supervised manner with the\naddition of an output layer.\nIn Fig. 1b we initialize a model using the weights\nof a generally pretrained BERT and continue pre-\ntraining on an unsupervised set of in-domain data,\nin order to adapt to the target domain. This step\ndoes not require use of supervised data, since we\nuse the MLM objective.\nFor the ﬁnal ﬁne-tuning step, shown in Fig. 1c\nwe perform supervised ﬁne-tuning on the source\ndata, while we keep the MLM objective on the\ntarget data as an auxiliary task. Following standard\npractice, we use the [CLS] token representation\nfor classiﬁcation. The classiﬁer consists of a single\nfeed-forward layer.\nDuring this procedure the model learns the task\nthrough the classiﬁcation objective using the la-\nbeled source domain samples, and simultaneously\n2582\nit adapts to the target domain data through the\nMLM objective. The model is trained on the source\ndomain labeled data for the classiﬁcation task and\ntarget domain unlabeled data for the masked lan-\nguage modeling task. We mask only the target\ndomain data. During training we interleave source\nand target data and feed them to the BERT encoder.\nFeatures extracted from the source data are then\nused for classiﬁcation, while target features are\nused for Masked Language Modeling.\nThe mixed loss used for the ﬁne-tuning step, is\nthe sum of the classiﬁcation lossLCLF and the aux-\niliary MLM loss LMLM. LCLF is a cross-entropy\nloss, calculated on labeled examples from source\ndomain, while LMLM is used to predict masked\ntokens for unlabeled examples from target domain.\nWe train the model over mixed batches, that include\nboth source and target data, used for the respective\ntasks. The mixed loss is presented in Eq. 2:\nL(s,t) = λLCLF(s) + (1−λ)LMLM(t) (2)\nWe process nlabeled source samples s ∼DS and\nm unlabeled target samples t ∼DT on a batch.\nThe weighting factor λis selected as the ratio of\nlabeled source data over the sum of labeled source\nand unlabeled target data, as stated in Eq. 3:\nλ= n\nn+ m (3)\n5 Experiments\n5.1 Dataset\nWe evaluate UDALM on the Amazon reviews\nmulti-domain sentiment dataset (Blitzer et al.,\n2007), a standard benchmark dataset for domain\nadaptation. Reviews with one or two stars are la-\nbeled as negative, while reviews with four or ﬁve\nstars are labeled as positive. The dataset contains\nreviews on four product domains:Books (B), DVDs\n(D), Electronics (E) and Kitchen appliances (K),\nyielding 12 adaptation scenarios of source-target\ndomain pairs. Balanced sets of 2000 labeled re-\nviews are available for each domain. We use20000\n(randomly selected) unlabeled reviews for (B), (D)\nand (E). For (K) 17805 unlabeled reviews are avail-\nable. For each of the 12 adaptation scenarios we\nuse 20% of both labeled source and unlabeled tar-\nget data for validation, while labeled target data are\nused for testing exclusively and are not seen during\ntraining or validation.\n5.2 Implementation Details\nWe use BERTBASE (uncased) as the Language\nModel on which we apply domain pretraining.\nThe BERTBASE original English model is a 12-\nlayer, 768-hidden, 12-heads, 110M parameter trans-\nformer architecture, trained on the BookCorpus\nwith 800M words and a version of the English\nWikipedia with 2500M words. We convert source\nand target sentences to WordPieces (Wu et al.,\n2016). For target sentences we randomly mask\n15% of WordPiece tokens, as in (Devlin et al.,\n2019). If a token in a speciﬁc position is selected\nto be masked 80% of the time is replaced with a\n[MASK] token, 10% of the time with a random\ntoken and 10% of the time remains unchanged.\nThe maximum sequence length is set to 512 by\ntruncation of inputs. During domain pretraining\nwe train with batch size of 8 for 3 epochs (2 hours\non two GTX-1080Ti cards). During the ﬁnal ﬁne-\ntuning step of UDALM we train with batch size 36,\nconsisting of n= 1 source sub-batch of 4 samples\nand m = 8 target sub-batches of 4 samples each.\nWe update parameters after every 5 accumulated\nsub-batches. We train for 10 epochs with early\nstopping on the mixed loss in Eq. 2. For all ex-\nperiments we use AdamW optimizer (Loshchilov\nand Hutter, 2018) with learning rate 10−5. Each\nadaptation scenario requires one hour on one GTX-\n1080Ti. For the domain adversarial experiments\nwe set λd = 0.01 in Eq. 4 2 and train for 10 epochs.\nModels are developed with PyTorch (Paszke et al.,\n2019) and HuggingFace Transformers (Wolf et al.,\n2019).\n5.3 Baselines - Compared methods\nWe select three state-of-the-art methods for compar-\nison. Each of the selected methods represents a dif-\nferent line of UDA research, namely domain adver-\nsarial training BERT-DAAT(Du et al., 2020), self-\ntraining XLM-R based p+CFd (Ye et al., 2020)\nand pivot-based R-PERL (Ben-David et al., 2020).\nWe report results for the following settings with\nBERT models:\nSource only (SO): We ﬁne-tune BERT on source\ndomain labeled data, without using target data.\nDomain Pretraining (DPT): We use the target do-\nmain unlabeled data in order to continue pretraining\nof BERT with MLM loss (as in Fig. 1b) and then\n2We also manually experimented with λd = 1 and\nlambdad = 0.1, and a sigmoid schedule for λd. We report\nbest results.\n2583\nR-PERL DAAT p+CFd SO BERT DAT BERT DPT BERT UDALM\nB→D 87.8 90 .9 87 .7 89.51 ±0.76 87 .31 ±2.14 90 .49 ±0.38 90.97 ±0.22\nB→E 87.2 88 .9 91 .3 90.51 ±0.51 86 .91 ±2.71 90 .38 ±1.59 91.69 ±0.31\nB→K 90.2 88 .0 92 .5 91.75 ±0.28 90 .59 ±1.17 92 .66 ±0.43 93.21 ±0.22\nD→B 85.6 89.7 91.5 90.26 ±0.64 86 .30 ±3.10 91 .02 ±0.75 91 .00 ±0.42\nD→E 89.3 90.1 91.6 88.71 ±1.48 87 .85 ±1.24 91 .03 ±0.82 92.30 ±0.47\nD→K 90.4 88.8 92.5 91.22 ±0.69 89 .95 ±1.53 92 .30 ±0.42 93.66 ±0.37\nE →B 90.2 89.6 88.7 87.96 ±0.89 85 .65 ±1.91 88 .52 ±0.55 90.61 ±0.30\nE →D 84.8 89.3 88.2 87.37 ±0.64 83 .99 ±1.31 87 .85 ±0.47 88 .83 ±0.61\nE →K 91.2 91.7 93.6 93.30 ±0.50 92 .45 ±1.35 94 .39 ±0.72 94.43 ±0.24\nK →B 83.0 90.8 89.8 88.15 ±0.64 85 .07 ±1.03 88 .83 ±0.81 90 .29 ±0.51\nK →D 85.6 90.5 87.8 87.23 ±0.49 84 .11 ±0.62 88 .52 ±0.69 89 .54 ±0.59\nK →E 91.2 93.2 92.6 93.23 ±0.34 92 .07 ±0.24 93 .42 ±0.40 94.34 ±0.26\nAverage 87.50 90.12 90.63 89.93 ±0.65 87 .68 ±1.53 90 .78 ±0.67 91.74 ±0.38\nTable 1: Accuracy of unsupervised domain adaptation on twelve domain pairs of Amazon Reviews Multi Domain\nSentiment Dataset.\nﬁne-tune the resulting model on source domain\nlabeled data.\nDomain Adversarial (DAT): Domain Adversarial\nTraining with BERT. Starting from the domain pre-\ntrained BERT (as in Fig. 1b), we then ﬁne-tune the\nmodel with domain adversarial training as in Ganin\net al. (2016). For a BERT model with parameters\nθ, with LCLF being a cross-entropy loss for super-\nvised task prediction, LADV being a cross-entropy\nloss for domain prediction and λd being a weight-\ning factor, domain adversarial training consists of\nthe minimization criterion described in Eq. 4.\nmin\nθ\nLCLF(θ; DS) −λdLADV(θ; DS,DT) (4)\nUDALM: The proposed method, where we ﬁne-\ntune the model created in the domain pretraining\nstep using the mixed loss in Eq. 2.\n6 Experimental Results\n6.1 Comparison to state-of-the-art\nWe present results for all 12 domain adaptation set-\ntings in Table 1. Results for SO BERT, DAT BERT,\nDPT BERT and UDALM are averaged over ﬁve\nruns and we include standard deviations The last\nline of Table 1 contains the macro-averaged accu-\nracy and deviations over all domain pairs. UDALM\nsurpasses all other techniques, yielding an absolute\nimprovement of 1.81% over the SO BERT baseline.\nFor fair comparison, we compare only with meth-\nods based on pretrained models, mostly BERT. We\nobserve that BERT ﬁne-tuned only with the source\ndomain labeled data, without any knowledge of\nthe target domain, is a competitive baseline. This\nsource-only model even surpasses state-of-the-art\nmethods developed for UDA, e.g. R-PERL (Ben-\nDavid et al., 2020).\nWe reproduce the domain adversarial training\nprocedure and present results in the DAT BERT\ncolumn of Table 1. Adversarial training proved to\nbe unstable in our experiments, even after careful\ntuning of the adversarial loss weighting factor λd.\nThis is evidenced by the high standard deviations\nin the DAT BERT experiments. We observe that\nadversarial training does not manage to outperform\nthe source-only baseline.3\nDomain pretraining increases the average accu-\nracy with an absolute improvement of 0.85% over\nthe source-only baseline. Continuing MLM pre-\ntraining on the target domain data leads to better\nmodel adaptation, and therefore improved perfor-\nmance, on the target domain. This is consistent\nwith previous works on supervised (Gururangan\net al., 2020; Xu et al., 2019; Sun et al., 2019) and\nunsupervised settings (Han and Eisenstein, 2019;\nDu et al., 2020).\nUDALM yields an additional 0.96% absolute\nimprovement of average accuracy over domain pre-\ntraining. Keeping the MLM loss during ﬁne-tuning\ntherefore, leads to better adaptation and acts as a\nregularizer that prevents the model from overﬁtting\non the source domain. We also observe smaller\nstandard deviations when using UDALM, which\nindicates that including the MLM loss during ﬁne-\ntuning can result to more robust training.\n6.2 Sample efﬁciency\nUDALM surpasses in terms of macro-average ac-\ncuracy all other approaches for unsupervised do-\nmain adaptation on the Amazon reviews multi-\ndomain sentiment dataset. Speciﬁcally, our method\nimproves on the state-of-the-art pseudo-labeling\n3Note that we did not have to perform extensive tuning for\nthe other methods, including UDALM.\n2584\nFigure 2: Average accuracy for different amount of target domain unlabeled samples of: (1) DPT BERT (2) DAT\nBERT and (3) UDALM.\n(p+CFd Ye et al., 2020), domain adversarial (DAAT\nDu et al., 2020) and pivot-based (R-PERL Ben-\nDavid et al., 2020) approaches by 1.11%, 1.62%\nand 4.24% respectively.\nWe further investigate the impact of using dif-\nferent amount of target domain unlabeled data on\nmodel performance, to study the sample efﬁciency\nof UDALM. We experiment with settings of 500,\n2000, 6000, 10000 and 14000 samples, by ran-\ndomly limiting the number of unlabeled target do-\nmain data. For each setting we conduct three exper-\niments with BERT models: (1) DPT, (2) DAT and\n(3) UDALM. When no target data are available, all\nmethods are equivalent to a source only ﬁne-tuned\nBERT. Again, we do not tune the hyper-parameters\nfor DPT or UDALM. Fig. 2 shows the average ac-\ncuracy on the twelve adaptation scenarios of the\nstudied dataset. We see that UDALM produces ro-\nbust performance improvement when we limit the\namount of target data, indicating that it can be used\nin low-resource settings. However, training BERT\nin a domain adversarial manner shows instabilities.\nThis is further discussed in Section 7.\n6.3 On the stopping criteria for UDA training\nA common problem when performing UDA is the\nlack of target labeled data that can be used for\nhyperparameter validation. For example, Ruder\nand Plank (2018) use a small set of labeled target\ndata for validation, putting the problem in a semi-\nsupervised setting. When training under a domain\nshift, optimization of model performance on the\nsource data may not result to optimal performance\nfor the target data.\nTo illustrate this, we examine if the minimiza-\ntion of the mixed loss can be used as a stopping\ncriterion for UDA training. We compare ﬁve stop-\nping criteria: (1) ﬁxed training for 1 epoch, (2)\nﬁxed training for 3 epochs, (3) ﬁxed training for 10\nepochs, (4) stop when the minimum classiﬁcation\nloss is reached for the source data and (5) stop when\nthe minimum mixed loss ( Eq. 2) is reached. For\n(4) and (5) we train for 10 epochs with patience\n3. We report average accuracy of the ﬁve stop-\nping criteria over the twelve adaptation scenarios\nof Amazon Reviews dataset on Table 2. Training\nfor a ﬁxed number of 10 epochs and stopping when\nthe minimum mixed loss perform best, yielding\ncomparable accuracies of 91.75% and 91.73% re-\nspectively. Note that stopping when the minimum\nsource loss stops the ﬁne-tuning process too soon\nand does not allow the model to learn the target\ndomain effectively. Overall, we observe that the\nmixed loss can be effectively used for early stop-\nping, regularizing the model and alleviating the\nneed for extensive search for the optimal number\nof training steps. This is an indication that the\nmixed loss could be used for model validation.\nStopping Criterion Epochs Av. Acc.\nFixed 1 90.98\nFixed 3 91.65\nFixed 10 91.75\nMin source loss 10, patience 3 91.30\nMin mixed loss 10, patience 3 91.74\nTable 2: Comparison of average accuracy for various\nvalidation settings.\n2585\nFigure 3: Comparison of average A-distance, average source error and average target error rate of different methods\nover all source - target pairs of the Amazon reviews dataset.\n7 Discussion\n7.1 Background Theory\nBen-David et al. (2007, 2010) provide a theory of\nlearning from different domains. A key outcome\nof this work is the following theorem:\nTheorem (Ben-David et al., 2007, 2010) Let H\nbe the hypothesis space and let DS,DT be the\ntwo domains and ϵS,ϵT be the corresponding error\nfunctions. Then for any h∈H:\nϵT(h) ≤ϵS(h) + 1\n2dH∆H(DS,DT) + C (5)\nwhere dH∆H(DS,DT) is the H∆H-divergence\n(Kifer et al., 2004) between two domains, that is a\nmeasure of distance between domains that can be\nestimated from ﬁnite samples.\nEq. 5 deﬁnes an upper bound for the expected\nerror ϵT(h) of a hypothesis hon the target domain\nas the sum of three terms, namely the expected\nerror on the source domain ϵS(h), the divergence\nbetween the source and target domain distributions\n1\n2 dH∆H(DS,DT) and the error of the ideal joint\nhypothesis C. When such an hypothesis exists, the\nterm is considered relatively small and in practice\nignored. The ﬁrst term, bounds the expected error\non the target domain by the expected error in the\nsource domain and is expected to be small, due\nto supervised learning on the source domain. The\nsecond term, gives a notion of distance between the\nsource and target domain extracted features. Intu-\nitively this equation states: “if there exists a hypoth-\nesis hthat has small error on the source data and\nthe source feature space is close to the target fea-\nture space, then this hypothesis will have low error\non the target data”. Domain Adversarial Training\naims to learn features that simultaneously result to\nlow source error and low distance between target\nand source feature spaces based on the combined\nloss in Eq. 4.\n7.2 A-distance only provides an upper bound\nfor target error\nAccording to Ben-David et al. (2007) the H∆H-\ndivergence can be approximated by proxy A-\ndistance, that is deﬁned by Eq. 6 given the domain\nclassiﬁcation error ϵD.\ndA = 2(1 −2ϵD) (6)\nWe calculate an approximation of the distance\nbetween domains. Following prior work (Ganin\net al., 2016; Saito et al., 2017) we create an SVM\ndomain classiﬁer. We feed the SVM with BERT’s\n[CLS] token representations, measure the domain\nclassiﬁcation error, and compute A-distance as in\nEq. 6. We train the domain classiﬁer on 2000 sam-\nples from each source and target domains. Fig. 3\nshows the A-distance along with the source and\nthe target error, averaged over the twelve available\ndomain pairs using representations obtained from\nfour methods, namely BERT SO, DAT BERT, DPT\nBERT and UDALM. DAT BERT minimizes the dis-\ntance between domains. DPT BERT also reduces\nthe A-distance, to similar levels with DAT, without\nusing an explicit loss to minimize A-distance. To\nour surprise we found that, although it achieves the\nlowest error rate, UDALM does not signiﬁcantly re-\nduce the proxy A-distance compared to the source-\nonly baseline. Additionally, we observe that the\nsource error is correlated to model performance on\nthe target task, i.e. models with lower source error\n2586\nhave also lower target error. UDALM speciﬁcally,\nachieves high accuracy on the source task and is\nable to transfer the task knowledge across domains,\nwhile DAT is able to bring domain representations\ncloser, but at the cost of achieving weaker perfor-\nmance on the task at hand.\nOverall, we do not observe a correlation between\nthe resulting A-distance and model performance\non target domain. Therefore, lower distance be-\ntween domains, achieved intentionally or not, is\nnot a necessary condition for good performance on\nthe target domain4, and our efforts could be better\nspent towards synergistic learning of the supervised\nsource task and the target domain distribution.\n7.3 Limitations of Domain Adversarial\nTraining\nDomain adversarial training (Ganin et al., 2016)\nfaces some critical limitations that make the\nmethod difﬁcult to be reproduced due to high hyper-\nparameter sensitivity and instability during train-\ning.\nSuch limitations have been highlighted by other\nauthors in the UDA literature. For example, ac-\ncording to Shen et al. (2018) when a domain clas-\nsiﬁer can perfectly distinguish target from source\nrepresentations, there will be a gradient vanishing\nproblem. Shah et al. (2018) state that domain adver-\nsarial training is unstable and needs careful hyper-\nparameter tuning for their experiments. Wang et al.\n(2020) report results over three multi-domain NLP\ndatasets, where domain adversarial training in con-\njunction with BERT under-performs. Ruder and\nPlank (2018) found that the domain adversarial loss\ndid not help for their experiments on the Amazon\nreviews dataset.\nIn our experiments we note that domain-\nadversarial training results to worse performance\nthan naive source only training. Furthermore, we\nexperienced the need for extensive tuning of theλd\nparameter from Eq. 4 every time the experimental\nsetting changed (e.g. when testing for different\namounts of available target data as in Section 6.2).\nThis motivated us to further investigate the behav-\nior of BERT ﬁne-tuned with the adversarial cost.\nFor visual inspection, we perform T-SNE (Maaten\nand Hinton, 2008) on representations extracted\n4Shu et al. (2018) state that feature distribution matching\nis a weak constraint when high-capacity feature extractors\nare used. Intuitively, a high-capacity feature extractor can\nperform arbitrary transformations to the input features in order\nto match the distributions.\nfrom BERT, under four UDA setings in Fig. 4. In\nFig. 4a we observe features extracted using BERT\nwith Domain Adversarial Training and we com-\npare it with features from SO BERT (Fig. 4b), DPT\nBERT (Fig. 4c) and UDALM (Fig. 4d). We ob-\nserve that domain adversarial training manages to\ngroup tightly target and source samples, especially\nin the case of positive samples. Nevertheless, in\nthe process, DAT introduces signiﬁcant distortion\nin the semantic space, which is reﬂected in model\nperformance5.\nWe can attribute this behavior to two factors.\nFirst, The formulation of the adversarial loss in\nEq. (4) can lead to trivial solutions. In order to\nmaximize the LADV term of Eq. (4), the model\ncan just ﬂip all domain labels, namely just pre-\ndict that source samples belong to the target do-\nmain and vice-versa. In this case the model can\nstill discriminate between domains and domain-\nindependent representations are not encouraged.\nWe empirically observed this behavior in our ex-\nperiments with DAT, and only extensive hyper-\nparameter tuning could alleviate this issue. Addi-\ntionally, Eq. (4) aims to minimize the upper bound\nof the target error ϵT(h) in Eq. (5). While this\nis desirable, reduction of the upper bound does\nnot necessarily result in reduction of the bounded\nterm in all scenarios. Furthermore, optimizing\nthe LADV(θ; DS,DT) term can lead to increas-\ning LCLF(θ; DS), and therefore one must ﬁnd a\nbalance between the two adversarial terms, again\nthrough careful hyper-parameter tuning. These is-\nsues could potentially be alleviated by including\nregularization terms that discourage trivial solu-\ntions and improve robustness. Therefore, given\nthe lack of guarantees for good performance and\nthe practical considerations, further investigation\nshould be conducted regarding the robustness and\nreproducibility of DAT for UDA.\n8 Conclusions and Future Work\nUnsupervised domain adaptation of pretrained lan-\nguage models is a challenging problem with direct\nreal world applications. In this work we propose\nUDALM, a robust, plug and play training strategy,\nwhich is able to improve performance in the target\ndomain, achieving state-of-the-art results across\n12 adaptation settings in the multi-domain Ama-\n5Note, we include this visualization for a single source-\ndomain pair as an example. We performed multiple runs\nof T-SNE over all 12 source-domain pairs and this behavior\nappeared consistently.\n2587\n(a) DAT BERT\n (b) BERT SO\n(c) DPT BERT\n (d) UDALM\nFigure 4: 2Drepresentations of BERT [CLS] features using t-SNE for the D→Ktask. The goal is to maximize\nseparation between target positive (blue) and target negative (yellow) samples.\nzon reviews dataset. Our method produces robust\nresults with little hyper-parameter tuning and the\nproposed mixed-loss can be used for model valida-\ntion, allowing for fast model development. Further-\nmore, UDALM scales with the amount of available\nunsupervised data from the target domain, allow-\ning for adaptation in low-resource settings. In our\nanalysis, we discuss the relationship between the\nA-distance and the target error. We observe that\nlow A-distance may not suggest low target error\nfor high capacity models. Additionally, we exam-\nine limitations of Domain Adversarial Training and\nhighlight that the adversarial cost may lead to dis-\ntortion of the feature space and negatively impact\nperformance.\nIn the future we plan to apply UDALM to other\ntasks under domain-shift, such as sequence classiﬁ-\ncation, question answering and part-of-speech tag-\nging. Furthermore, we plan to extend our method\nfor temporal and style adaptation, by adding more\nrelevant auxiliary tasks that model language shift\nover time and over different platforms. Finally, we\nwant to investigate the effectiveness of the proposed\nﬁne-tuning approach in supervised scenarios.\nAcknowledgements\n• This research has been co-ﬁnanced by the Eu-\nropean Regional Development Fund of the\nEuropean Union and Greek national funds\nthrough the Operational Program Competitive-\nness, Entrepreneurship and Innovation, under\nthe call RESEARCH – CREATE – INNO-\nV ATE (project safety4all with code:T1EDK-\n04248)\n• This work has been partially supported by\ncomputational time granted from the Greek\nResearch & Technology Network (GR-NET)\nin the National HPC facility - ARIS.\n• The authors would like to thank Efthymios\nGeorgiou for his comments and suggestions.\nReferences\nSteven Abney. 2007. Semisupervised learning for com-\nputational linguistics. CRC Press.\nFiroj Alam, Shaﬁq Joty, and Muhammad Imran. 2018.\nDomain adaptation with adversarial training and\n2588\ngraph embeddings. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1077–\n1087, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nEyal Ben-David, Carmel Rabinovitz, and Roi Reichart.\n2020. Perl: Pivot-based domain adaptation for\npre-trained deep contextualized embedding models.\nTransactions of the Association for Computational\nLinguistics, 8:504–521.\nShai Ben-David, John Blitzer, Koby Crammer, Alex\nKulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. 2010. A theory of learning from different\ndomains. Machine learning, 79(1-2):151–175.\nShai Ben-David, John Blitzer, Koby Crammer, and Fer-\nnando Pereira. 2007. Analysis of representations for\ndomain adaptation. In Advances in neural informa-\ntion processing systems, pages 137–144.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiﬁcation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation of Computational Linguistics , pages 440–\n447, Prague, Czech Republic. Association for Com-\nputational Linguistics.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 120–128, Sydney, Australia. As-\nsociation for Computational Linguistics.\nKonstantinos Bousmalis, George Trigeorgis, Nathan\nSilberman, Dilip Krishnan, and Dumitru Erhan.\n2016. Domain separation networks. In Advances in\nneural information processing systems , pages 343–\n351.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2089–2095, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nChunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, and\nJianxin Liao. 2020. Adversarial and domain-aware\nBERT for cross-domain sentiment analysis. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4019–\n4028, Online. Association for Computational Lin-\nguistics.\nYaroslav Ganin and Victor Lempitsky. 2015. Unsuper-\nvised domain adaptation by backpropagation. vol-\nume 37 of Proceedings of Machine Learning Re-\nsearch, pages 1180–1189, Lille, France. PMLR.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, François Lavi-\nolette, Mario Marchand, and Victor Lempitsky.\n2016. Domain-adversarial training of neural net-\nworks. The Journal of Machine Learning Research,\n17(1):2096–2030.\nHan Guo, Ramakanth Pasunuru, and Mohit Bansal.\n2020. Multi-source domain adaptation for text clas-\nsiﬁcation via distancenet-bandits. In AAAI, pages\n7830–7838.\nJiang Guo, Darsh Shah, and Regina Barzilay. 2018.\nMulti-source domain adaptation with mixture of ex-\nperts. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4694–4703, Brussels, Belgium. Association\nfor Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\n2589\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4238–4248, Hong Kong,\nChina. Association for Computational Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nChen Jia, Xiaobo Liang, and Yue Zhang. 2019. Cross-\ndomain NER using cross-domain language model-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2464–2474, Florence, Italy. Association for\nComputational Linguistics.\nDaniel Kifer, Shai Ben-David, and Johannes Gehrke.\n2004. Detecting change in data streams. In VLDB,\nvolume 4, pages 180–191. Toronto, Canada.\nJ Lee, W Yoon, S Kim, D Kim, S Kim, CH So, and\nJ Kang. 2020. Biobert: a pre-trained biomedical lan-\nguage representation model for biomedical text min-\ning. Bioinformatics (Oxford, England), 36(4):1234.\nYitong Li, Timothy Baldwin, and Trevor Cohn. 2018a.\nWhat’s in a domain? learning domain-robust text\nrepresentations using adversarial training. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 2 (Short Papers), pages 474–479, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nZheng Li, Ying Wei, Yu Zhang, and Qiang Yang.\n2018b. Hierarchical attention transfer network for\ncross-domain sentiment classiﬁcation. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence.\nKyungTae Lim, Jay Yoon Lee, Jaime Carbonell, and\nThierry Poibeau. 2020. Semi-supervised learning\non meta structure: Multi-task tagging and parsing\nin low-resource scenarios. 34:8344–8351.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(Nov):2579–2605.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Reranking and self-training for parser adapta-\ntion. In Proceedings of the 21st International Con-\nference on Computational Linguistics and 44th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 337–344, Sydney, Australia. As-\nsociation for Computational Linguistics.\nTimothy Miller. 2019. Simpliﬁed neural unsupervised\ndomain adaptation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 414–419, Minneapolis, Minnesota. As-\nsociation for Computational Linguistics.\nSinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang\nYang, and Zheng Chen. 2010. Cross-domain senti-\nment classiﬁcation via spectral feature alignment. In\nProceedings of the 19th international conference on\nWorld wide web, pages 751–760.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems, volume 32, pages 8026–8037. Cur-\nran Associates, Inc.\nAlan Ramponi and Barbara Plank. 2020. Neural unsu-\npervised domain adaptation in nlp—a survey. arXiv\npreprint arXiv:2005.14672.\nGuy Rotman and Roi Reichart. 2019. Deep contextual-\nized self-training for low resource dependency pars-\ning. Transactions of the Association for Computa-\ntional Linguistics, 7:695–713.\nSebastian Ruder and Barbara Plank. 2018. Strong base-\nlines for neural semi-supervised learning under do-\nmain shift. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1044–1054, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nKuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada.\n2017. Asymmetric tri-training for unsupervised do-\nmain adaptation. volume 70 of Proceedings of\nMachine Learning Research, pages 2988–2997, In-\nternational Convention Centre, Sydney, Australia.\nPMLR.\nMotoki Sato, Hitoshi Manabe, Hiroshi Noji, and Yuji\nMatsumoto. 2017. Adversarial training for cross-\ndomain Universal Dependency parsing. In Proceed-\nings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies ,\npages 71–79, Vancouver, Canada. Association for\nComputational Linguistics.\n2590\nDarsh Shah, Tao Lei, Alessandro Moschitti, Salva-\ntore Romeo, and Preslav Nakov. 2018. Adversar-\nial domain adaptation for duplicate question detec-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1056–1063, Brussels, Belgium. Association\nfor Computational Linguistics.\nJian Shen, Yanru Qu, Weinan Zhang, and Yong Yu.\n2018. Wasserstein distance guided representation\nlearning for domain adaptation. In Proceedings\nof the Thirty-Second AAAI Conference on Artiﬁ-\ncial Intelligence, (AAAI-18), the 30th innovative Ap-\nplications of Artiﬁcial Intelligence (IAAI-18), and\nthe 8th AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018 , pages 4058–\n4065. AAAI Press.\nRui Shu, Hung Bui, Hirokazu Narui, and Stefano Er-\nmon. 2018. A dirt-t approach to unsupervised do-\nmain adaptation. In International Conference on\nLearning Representations.\nAnders Søgaard. 2010. Simple semi-supervised train-\ning of part-of-speech taggers. In Proceedings of\nthe ACL 2010 Conference Short Papers, pages 205–\n208, Uppsala, Sweden. Association for Computa-\ntional Linguistics.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nChengyu Wang, Minghui Qiu, Jun Huang, and Xi-\naofeng He. 2020. Meta ﬁne-tuning neural lan-\nguage models for multi-domain text mining. arXiv\npreprint arXiv:2003.13003.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, et al. 2019. Huggingface’s transformers: State-\nof-the-art natural language processing. ArXiv, pages\narXiv–1910.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nHu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019. BERT\npost-training for review reading comprehension and\naspect-based sentiment analysis. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 2324–2335, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 189–196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.\nHai Ye, Qingyu Tan, Ruidan He, Juntao Li, Hwee Tou\nNg, and Lidong Bing. 2020. Feature Adap-\ntation of Pre-Trained Language Models across\nLanguages and Domains for Text Classiﬁcation.\narXiv:2009.11538 [cs]. ArXiv: 2009.11538.\nHan Zhao, Shanghang Zhang, Guanhang Wu, José\nM. F. Moura, Joao P Costeira, and Geoffrey J Gor-\ndon. 2018. Adversarial multiple source domain\nadaptation. In Advances in Neural Information Pro-\ncessing Systems, volume 31, pages 8559–8570. Cur-\nran Associates, Inc.\nZhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex-\nploiting unlabeled data using three classiﬁers. IEEE\nTransactions on knowledge and Data Engineering ,\n17(11):1529–1541.\nYftah Ziser and Roi Reichart. 2017. Neural structural\ncorrespondence learning for domain adaptation. In\nProceedings of the 21st Conference on Computa-\ntional Natural Language Learning (CoNLL 2017) ,\npages 400–410, Vancouver, Canada. Association for\nComputational Linguistics.\nYftah Ziser and Roi Reichart. 2018. Pivot based lan-\nguage modeling for improved neural domain adapta-\ntion. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers) , pages 1241–1251,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nYftah Ziser and Roi Reichart. 2019. Task reﬁnement\nlearning for improved accuracy and stability of un-\nsupervised domain adaptation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5895–5906, Florence,\nItaly. Association for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8043742775917053
    },
    {
      "name": "Domain adaptation",
      "score": 0.7718028426170349
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6208126544952393
    },
    {
      "name": "Language model",
      "score": 0.617290735244751
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5497435331344604
    },
    {
      "name": "Sample (material)",
      "score": 0.5288675427436829
    },
    {
      "name": "Machine learning",
      "score": 0.5037035346031189
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.48199453949928284
    },
    {
      "name": "Natural language processing",
      "score": 0.3641992211341858
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3355279564857483
    },
    {
      "name": "Mathematics",
      "score": 0.1104382872581482
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174458059",
      "name": "National Technical University of Athens",
      "country": "GR"
    }
  ],
  "cited_by": 5
}