{
    "title": "VD-BERT: A Unified Vision and Dialog Transformer with BERT",
    "url": "https://openalex.org/W3023074479",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1852389517",
            "name": "Wang Yue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221351219",
            "name": "Joty, Shafiq",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221719684",
            "name": "Lyu, Michael R.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2753127173",
            "name": "King, Irwin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751801181",
            "name": "Xiong, Caiming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224947216",
            "name": "Hoi, Steven C. H.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2964218959",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2001832483",
        "https://openalex.org/W2963623904",
        "https://openalex.org/W2970355596",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2967045987",
        "https://openalex.org/W2963643760",
        "https://openalex.org/W3016271098",
        "https://openalex.org/W2966158321",
        "https://openalex.org/W2108862644",
        "https://openalex.org/W2917061951",
        "https://openalex.org/W2185175083",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964004697",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2892245540",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2970869018",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2622980782",
        "https://openalex.org/W2750998636",
        "https://openalex.org/W2981902456",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963287297",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2990095214",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W3035103424",
        "https://openalex.org/W3035052826",
        "https://openalex.org/W2988023442",
        "https://openalex.org/W4249013746",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2768661419",
        "https://openalex.org/W3034291519",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2963224792",
        "https://openalex.org/W2967674528",
        "https://openalex.org/W2997035654",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3116651605",
        "https://openalex.org/W2963115613",
        "https://openalex.org/W2091158010",
        "https://openalex.org/W2997547717",
        "https://openalex.org/W3022310886"
    ],
    "abstract": "Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.",
    "full_text": "VD-BERT: A UniÔ¨Åed Vision and Dialog Transformer with BERT\nYue Wang1‚àó, ShaÔ¨Åq Joty2, Michael R. Lyu1, Irwin King1, Caiming Xiong2, and Steven C.H. Hoi2\n1 Department of Computer Science and Engineering\nThe Chinese University of Hong Kong, HKSAR, China\n2Salesforce Research\n1{yuewang,lyu,king}@cse.cuhk.edu.hk\n2{sjoty,cxiong,shoi}@salesforce.com\nAbstract\nVisual dialog is a challenging vision-language\ntask, where a dialog agent needs to answer a\nseries of questions through reasoning on the\nimage content and dialog history. Prior work\nhas mostly focused on various attention mech-\nanisms to model such intricate interactions. By\ncontrast, in this work, we propose VD-BERT,\na simple yet effective framework of uniÔ¨Åed\nvision-dialog Transformer that leverages the\npretrained BERT language models for Visual\nDialog tasks. The model is uniÔ¨Åed in that (1) it\ncaptures all the interactions between the image\nand the multi-turn dialog using a single-stream\nTransformer encoder, and (2) it supports both\nanswer ranking and answer generation seam-\nlessly through the same architecture. More\ncrucially, we adapt BERT for the effective fu-\nsion of vision and dialog contents via visually\ngrounded training. Without the need of pre-\ntraining on external vision-language data, our\nmodel yields new state of the art, achieving the\ntop position in both single-model and ensem-\nble settings ( 74.54 and 75.35 NDCG scores)\non the visual dialog leaderboard. Our code\nand pretrained models are released at https:\n//github.com/salesforce/VD-BERT.\n1 Introduction\nVisual Dialog (or VisDial) aims to build an AI\nagent that can answer a human‚Äôs questions about\nvisual content in a natural conversational setting\n(Das et al., 2017). Unlike the traditional single-turn\nVisual Question Answering (VQA) (Antol et al.,\n2015), the agent in VisDial requires to answer ques-\ntions through multiple rounds of interactions to-\ngether with visual content understanding.\nThe primary research direction in VisDial has\nbeen mostly focusing on developing various atten-\ntion mechanisms (Bahdanau et al., 2015) for a bet-\n*This work was mainly done when Yue Wang was an intern\nat Salesforce Research Asia, Singapore.\nVQ\nA(a) Most VQA\nV HQ\nA(b) Most VisDial\nV HQ\nA(c) Our VD-BERT\nFigure 1: Attention Ô¨Çow direction illustration. V: vi-\nsion, H: dialog history, Q: question, A: answer. The ar-\nrow denotes the attention Ô¨Çow direction and the dashed\nline represents an optional connection.\nter fusion of vision and dialog contents. Compared\nto VQA that predicts an answer based only on the\nquestion about the image (Figure 1(a)), VisDial\nneeds to additionally consider the dialog history.\nTypically, most of previous work (Niu et al., 2019;\nGan et al., 2019; Kang et al., 2019) uses the ques-\ntion as a query to attend to relevant image regions\nand dialog history, where their interactions are usu-\nally exploited to obtain better visual-historical cues\nfor predicting the answer. In other words, the atten-\ntion Ô¨Çow in these methods is unidirectional ‚Äì from\nquestion to the other entities (Figure 1(b)).\nBy contrast, in this work, we allow for bidirec-\ntional attention Ô¨Çow between all the entities using\na uniÔ¨Åed Transformer (Vaswani et al., 2017) en-\ncoder, as shown in Figure 1(c). In this way, all the\nentities simultaneously play the role of an ‚Äúinforma-\ntion seeker‚Äù (query) and an ‚Äúinformation provider‚Äù\n(key-value), thereby fully unleashing the potential\nof attention similar to Schwartz et al. (2019). We\nemploy the Transformer as the encoding backbone\ndue to its powerful representation learning capa-\nbility exhibited in pretrained language models like\nBERT (Devlin et al., 2019). Inspired by its recent\nsuccess in vision-language pretraining, we further\nextend BERT to achieve simple yet effective fusion\nof vision and dialog contents in VisDial tasks.\nRecently some emerging work has attempted to\nadapt BERT for multimodal tasks (Sun et al., 2019;\narXiv:2004.13278v3  [cs.CV]  2 Nov 2020\nLu et al., 2019; Tan and Bansal, 2019; Zhou et al.,\n2020). They often use self-supervised objectives to\npretrain BERT-like models on large-scale external\nvision-language data and then Ô¨Åne-tune on down-\nstream tasks. This has led to compelling results\nin tasks such as VQA, image captioning, image\nretrieval (Young et al., 2014), and visual reason-\ning (Suhr et al., 2019). However, it is still unclear\nhow visual dialog may beneÔ¨Åt from such vision-\nlanguage pretraining due to its unique multi-turn\nconversational structure. SpeciÔ¨Åcally, each image\nin the VisDial dataset is associated with up to 10\ndialog turns, which contain much longer contexts\nthan either VQA or image captioning.\nIn this paper, we present VD-BERT, a novel uni-\nÔ¨Åed vision-dialog Transformer framework for Vis-\nDial tasks. SpeciÔ¨Åcally, we Ô¨Årst encode the image\ninto a series of detected objects and feed them into\na Transformer encoder together with the image\ncaption and multi-turn dialog. We initialize the\nencoder with BERT for better leveraging the pre-\ntrained language representations. To effectively\nfuse features from the two modalities, we make\nuse of two visually grounded training objectives ‚Äì\nMasked Language Modeling (MLM) and Next Sen-\ntence Prediction (NSP). Different from the original\nMLM and NSP in BERT, we additionally take the\nvisual information into account when predicting\nthe masked tokens or the next answer.\nVisDial models have been trained in one of two\nsettings: discriminative or generative. In the dis-\ncriminative setting, the model ranks a pool of an-\nswer candidates, whereas the generative setting\nadditionally allows the model to generate the an-\nswers. Instead of employing two types of de-\ncoders like prior work, we rely on a uniÔ¨Åed Trans-\nformer architecture with two different self-attention\nmasks (Dong et al., 2019) to seamlessly support\nboth settings. During inference, our VD-BERT\neither ranks the answer candidates according to\ntheir NSP scores or generates the answer sequence\nby recursively applying the MLM operations. We\nfurther Ô¨Åne-tune our model on dense annotations\nthat specify the relevance score for each answer\ncandidate with a ranking optimization module.\nIn summary, we make the following contributions:\n‚Ä¢ To the best of our knowledge, our work serves\nas one of the Ô¨Årst attempts to explore pretrained\nlanguage models for visual dialog. We show-\ncase that BERT can be effectively adapted to this\ntask with simple visually grounded training for\ncapturing the intricate vision-dialog interactions.\nBesides, our VD-BERT is the Ô¨Årst uniÔ¨Åed model\nthat supports both discriminative and generative\ntraining settings without explicit decoders.\n‚Ä¢ Without pretraining on external vision-language\ndata, our model yields new state-of-the-art results\nin discriminative setting and promising results in\ngenerative setting on VisDial benchmarks (¬ß5.1).\n‚Ä¢ We conduct extensive experiments not only to\nanalyze how our model performs with various\ntraining aspects (¬ß5.2) and Ô¨Åne-tuning on dense\nannotations (¬ß5.3), but also to interpret it via at-\ntention visualization (¬ß5.4), shedding light on fu-\nture transfer learning research for VisDial tasks.\n2 Related Work\nVisual Dialog. The Visual Dialog task has been\nrecently proposed by Das et al. (2017), where a\ndialog agent needs to answer a series of questions\ngrounded by an image. It is one of the most chal-\nlenging vision-language tasks that require not only\nto understand the image content according to texts,\nbut also to reason through the dialog history. Pre-\nvious work (Lu et al., 2017; Seo et al., 2017; Wu\net al., 2018; Kottur et al., 2018; Jiang et al., 2020;\nYang et al., 2019; Guo et al., 2019a; Niu et al.,\n2019) focuses on developing a variety of attention\nmechanisms to model the interactions among enti-\nties including image, question, and dialog history.\nFor example, Kang et al. (2019) proposed DAN, a\ndual attention module to Ô¨Årst refer to relevant con-\ntexts in the dialog history, and then Ô¨Ånd indicative\nimage regions. ReDAN, proposed by Gan et al.\n(2019), further explores the interactions between\nimage and dialog history via multi-step reasoning.\nDifferent from them, we rely on the self-\nattention mechanism within a single-stream Trans-\nformer encoder to capture such complex interac-\ntions in a uniÔ¨Åed manner and derive a ‚Äúholistic‚Äù\ncontextualized representation for all the entities.\nSimilar to this, Schwartz et al. (2019) proposed\nFGA, a general factor graph attention that can\nmodel interactions between any two entities but in\na pairwise manner. There is recent work (Nguyen\net al., 2019; Agarwal et al., 2020) also applying\nthe Transformer to model the interactions among\nmany entities. However, their models neglect the\nimportant early interaction of the answer entity and\ncannot naturally leverage the pretrained language\nrepresentations from BERT like ours.\nDialog History\nAnswer!ùê¥!: ‚Äúbrown and tan‚Äù\nFollow-up QuestionùëÑ\": ‚Äúwhat coloris the giraffe?‚Äù\n[CLS]ùëù#\nùëú$\nùëù$\n‚Ä¶\nSegmentImageVD-BERT (Disc/Gen)PositionInput ùëú%\nùëù%\n[SEP]ùëù%&$\nùê∂‚Ä¶ [EOT]‚Ä¶ ùëÑ$ùê¥$\n‚Ä¶ [EOT]‚Ä¶ùëÑ'ùê¥'\n‚Ä¶\nT[CLS]‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶NSP MLM ‚Ä¶\nC: a man talking to a giraffe in an enclosure\nQ1: how many people are there? A1: 1Q2: is it a male of female? A2: MaleQ3: what is he doing? A3: looking at the giraffe\nùëÑ\"!ùê¥!\n‚Ä¶ [SEP]ùëù|)|\n‚Ä¶‚Ä¶ ‚Ä¶MLM\n‚Ä¶\nSelf-attention MasksGen: seq2seq\n*ùê¥!ùêºùêª\"ùëÑ\"\n*ùê¥!\nùêºùêª\"ùëÑ\"\n*ùê¥!ùêºùêª\"ùëÑ\"\n*ùê¥!\nùêºùêª\"ùëÑ\"\nDisc: bidirectional\nText\n‚Ä¶ MLM MLM1:!ùê¥!is correct0: !ùê¥!is incorrect\n1. brown and tan (1.0) 2. it is brownish (0.6) 3. brown (0.6) 4. golden brown (0.4) 5. brown tan (0.4)6. orange and white (0.2)7. medium brown (0.2) 8. ican't tell (0.0)‚Ä¶\n√óùëÅNSP Scores\nDenseAnnotation Fine-tuning\nùëØ#ùëØ#$%\nùëØ#ùëØ#$%\nInvisible for attending\nRanking Module\nFigure 2: The model architecture of our uniÔ¨Åed VD-BERT for both discriminative and generative settings.\nRegarding the architecture, our model mainly\ndiffers from previous work in two facets: Ô¨Årst, un-\nlike most prior work that considers answer candi-\ndates only at the Ô¨Ånal similarity computation layer,\nour VD-BERT integrates each answer candidate\nat the input layer to enable its early and deep fu-\nsion with other entities, similar to Schwartz et al.\n(2019); second, existing models adopt an encoder-\ndecoder framework (Sutskever et al., 2014) with\ntwo types of decoder for the discriminative and gen-\nerative settings separately, while we instead adopt\na uniÔ¨Åed Transformer encoder with two different\nself-attention masks (Dong et al., 2019) to seam-\nlessly support both settings without extra decoders.\nPretraining in Vision and Language. Pre-\ntrained language models like BERT (Devlin et al.,\n2019) have boosted performance greatly in a broad\nset of NLP tasks. In order to beneÔ¨Åt from the pre-\ntraining, there are many recent attempts to extend\nBERT for vision and language pretraining. They\ntypically employ the Transformer encoder as the\nbackbone with either a two-stream architecture\nto encode text and image independently such as\nViLBERT (Lu et al., 2019) and LXMERT (Tan\nand Bansal, 2019), or a single-stream architecture\nto encode both text and image together, such as\nB2T2 (Alberti et al., 2019), Unicoder-VL (Li et al.,\n2020), VisualBERT (Li et al., 2019), VL-BERT (Su\net al., 2020), and UNITER (Chen et al., 2019). Our\nVD-BERT belongs to the second group. These\nmodels yield prominent improvements mainly on\nvision-language understanding tasks like VQA, im-\nage retrieval (Young et al., 2014), and visual rea-\nsoning (Suhr et al., 2019; Zellers et al., 2019).\nMore recently, Zhou et al. (2020) proposed VLP\nwhich also allows generation using a uniÔ¨Åed Trans-\nformer with various self-attention masks (Dong\net al., 2019). Their model was proposed for VQA\nand image captioning. Our model is inspired by\nVLP and speciÔ¨Åcally tailored for the visual dia-\nlog task. Most closely related to this paper is the\nconcurrent work VisDial-BERT by Murahari et al.\n(2019), who also employ pretrained models (i.e.,\nViLBERT) for visual dialog. Our work has two\nmajor advantages over VisDial-BERT: Ô¨Årst, VD-\nBERT supports both discriminative and generative\nsettings while theirs is restricted to only the discrim-\ninative setting; second, we do not require to pretrain\non large-scale external vision-language datasets\nlike theirs and still yield better performance (¬ß5.1).\n3 The VD-BERT Model\nWe Ô¨Årst formally describe the visual dialog task.\nGiven a question Qt grounded on an image I at\nt-th turn, as well as its dialog history formulated\nas Ht = {C, (Q1, A1), ...,(Qt‚àí1, At‚àí1)}(where\nC denotes the image caption), the agent is asked\nto predict its answer At by ranking a list of 100\nanswer candidates {ÀÜA1\nt , ÀÜA2\nt , ...,ÀÜA100\nt }. In general,\nthere are two types of decoder to predict the answer:\na discriminative decoder that ranks the answer can-\ndidates and is trained with a cross entropy loss, or a\ngenerative decoder that synthesizes an answer and\nis trained with a maximum log-likelihood loss.\nFigure 2 shows the overview of our approach.\nFirst, we employ a uniÔ¨Åed vision-dialog Trans-\nformer to encode both the image and dialog history,\nwhere we append an answer candidate ÀÜAt in the\ninput to model their interactions in an early fusion\nmanner (¬ß3.1). Next, we adopt visually grounded\nMLM and NSP objectives to train the model for\neffective vision and dialog fusion using two types\nof self-attention masks ‚Äì bidirectional and seq2seq.\nThis allows our uniÔ¨Åed model to work in both dis-\ncriminative and generative settings (¬ß3.2). Lastly,\nwe devise a ranking optimization module to further\nÔ¨Åne-tune on the dense annotations (¬ß3.3).\n3.1 Vision-Dialog Transformer Encoder\nVision Features. Following previous work, we\nemploy Faster R-CNN (Ren et al., 2015) pre-\ntrained on Visual Genome (Krishna et al., 2017)\nto extract the object-level vision features. Let\nOI = {o1, ..., ok}denote the vision features for\nan image I, where each object feature oi is a\n2048-d Region-of-Interest (RoI) feature and k is\nthe number of the detected objects (Ô¨Åxed to 36 in\nour setting). As there is no natural orders among\nthese objects, we adopt normalized bounding box\ncoordinates as the spatial location. SpeciÔ¨Åcally,\nlet (x1, y1) and (x2, y2) be the coordinates of the\nbottom-left and top-right corner of the i-th object,\nits location information is encoded into a 5-d vec-\ntor: pi = (x1\nW , y1\nH , x2\nW , y2\nH , (x2‚àíx1)(y2‚àíy1)\nWH ), where\nW and H respectively denote the width and height\nof the input image, and the last element is the rela-\ntive area of the object. We extend pi with its class\nid and conÔ¨Ådence score for a richer representation.\nLanguage Features. We pack all the textual ele-\nments (caption and multi-turn dialog) into a long\nsequence. We employ WordPiece tokenizer (Wu\net al., 2016) to split it into a word sequence w,\nwhere each word is embedded with an absolute\npositional code following Devlin et al. (2019).\nCross-Modality Encoding. To feed both image\nand text into the Transformer encoder, we integrate\nthe image objects with language elements into a\nwhole input sequence. Similar to BERT, we use\nspecial tokens like [CLS] to denote the beginning\nof the sequence, and [SEP] to separate the two\nmodalities. Moreover, to inject the multi-turn di-\nalog structure into the model, we utilize a special\ntoken [EOT] to denote end of turn (Whang et al.,\n2019), which informs the model when the dialog\nturn ends. As such, we prepare the input sequence\ninto the format as x = ([CLS], o1, ..., ok, [SEP],\nC, [EOT], Q1A1, [EOT], ..., Qt ÀÜAt, [SEP]). To\nnotify the model for the answer prediction, we fur-\nther insert a [PRED] token between the Qt ÀÜAt pair.\nFinally, each input token embedding is combined\nwith its position embedding and segment embed-\nding (0 or 1, indicating whether it is image or text)\nwith layer normalization (Ba et al., 2016).\nTransformer Backbone. We denote the embed-\nded vision-language inputs as H0 = [e1, ...,e|x|]\nand then encode them into multiple levels of\ncontextual representations Hl = [ hl\n1, ...,hl\n|x|]\nusing L-stacked Transformer blocks, where the\nl-th Transformer block is denoted as Hl =\nTransformer(Hl‚àí1), l‚àà[1, L]. Inside each Trans-\nformer block, the previous layer‚Äôs outputHl‚àí1 ‚àà\nR|x|√ódh is aggregated using the multi-head self-\nattention (Vaswani et al., 2017):\nQ = Hl‚àí1WQ\nl , K = Hl‚àí1WK\nl , V = Hl‚àí1WV\nl ,\n(1)\nMij =\n{\n0, allow to attend,\n‚àí‚àû, prevent from attending, (2)\nAl = softmax(QKT\n‚àödk\n+ M)V, (3)\nwhere WQ\nl , WK\nl , WV\nl ‚àà Rdh√ódk are learnable\nweights for computing the queries, keys, and values\nrespectively, and M ‚ààR|x|√ó|x|is the self-attention\nmask that determines whether tokens from two lay-\ners can attend each other. Then Al is passed into a\nfeedforward layer to compute Hl for the next layer.\n3.2 Visually Grounded Training Objectives\nWe use two visually grounded training objectives‚Äî\nmasked language modeling (MLM) and next sen-\ntence prediction (NSP) to train our VD-BERT.\nParticularly, we aim to capture dense interactions\namong both inter-modality (i.e., image-dialog) and\nintra-modality (i.e., image-image, dialog-dialog).\nSimilar to MLM in BERT,15% tokens in the text\nsegment (including special tokens like [EOT] and\n[SEP]) are randomly masked out and replaced\nwith a special token [MASK]. The model is then\nrequired to recover them based not only on the\nsurrounding tokens w\\m but also on the image I:\nLMLM = ‚àíE(I,w)‚àºD log P(wm|w\\m, I), (4)\nwhere wm refers to the masked token and D de-\nnotes the training set. Following Zhou et al. (2020),\nwe do not conduct similar masked object/region\nmodeling in the image segment.\nAs for NSP, instead of modeling the relation-\nship between two sentences (as in BERT) or the\nmatching of an image-text pair (as in other vision-\nlanguage pretraining models like ViLBERT), VD-\nBERT aims to predict whether the appended answer\ncandidate ÀÜAt is correct or not based on the joint\nunderstanding of the image and dialog history:\nLNSP = ‚àíE(I,w)‚àºD log P(y|S(I, w)), (5)\nwhere y ‚àà{0, 1}indicates whether ÀÜAt is correct,\nand S(¬∑) is a binary classiÔ¨Åer to predict the proba-\nbility based on the [CLS] representation T[CLS]\nat the Ô¨Ånal layer. Below we introduce the discrimi-\nnative and generative settings of VD-BERT.\nDiscriminative Setting. For training in the dis-\ncriminative setting, we transform the task of select-\ning an answer into a pointwise binary classiÔ¨Åcation\nproblem. SpeciÔ¨Åcally, we sample an answer ÀÜAt\nfrom the candidate pool and append it to the in-\nput sequence, and ask the NSP head to distinguish\nwhether the sampled answer is correct or not. We\nemploy the bidirectional self-attention mask to al-\nlow all the tokens to attend to each other by setting\nthe mask matrix M in Eq. (2) to all 0s. To avoid\nimbalanced class distribution, we keep the ratio of\npositive and negative instances to 1:1 in each epoch.\nTo encourage the model to penalize more on neg-\native instances, we randomly resample a negative\nexample from the pool of 99 negatives w.r.t. every\npositive one at different epochs. During inference,\nwe rank the answer candidates according to the\npositive class score of their NSP heads.\nGenerative Setting. In order to autoregressively\ngenerate an answer, we also train VD-BERT with\nthe sequence-to-sequence (seq2seq) self-attention\nmask (Dong et al., 2019). For this, we divide the\ninput sequence to each Transformer block into two\nsubsequences, context and answer:\nx ‚âú (I, w) = (I, Ht, Qt,Ó¥ô Ó¥òÓ¥ó Ó¥ö\ncontext\nÀÜAt). (6)\nWe allow tokens in the context to be fully visible\nfor attending by setting the left part of M to all 0s.\nFor the answer sequence, we mask out (by setting\n‚àí‚àûin M) the ‚Äúfuture‚Äù tokens to get autoregressive\nattentions (see the red dots in Figure 2).\nDuring inference, we rely on the same uniÔ¨Åed\nTransformer encoder with sequential MLM opera-\ntions without an explicit decoder. SpeciÔ¨Åcally, we\nrecursively append a [MASK] token to the end of\nthe sequence to trigger a one-step prediction and\nthen replace it with the predicted token for the next\ntoken prediction. The decoding process is based\non greedy sampling and terminated when a [SEP]\nis emitted, and the resulting log-likelihood scores\nwill be used for ranking the answer candidates.\n3.3 Fine-tuning with Rank Optimization\nAs some answer candidates may be semantically\nsimilar (e.g., ‚Äúbrown and tan‚Äù vs ‚Äúbrown‚Äù in Figure\n2), VisDial v 1.0 additionally provides dense an-\nnotations that specify real-valued relevance scores\nfor the 100 answer candidates, [s1, ..., s100] with\nsi ‚àà[0, 1]. To Ô¨Åne-tune on this, we combine the\nNSP scores from the model for all answer candi-\ndates together into a vector [p1, ..., p100].\nAs dense annotation Ô¨Åne-tuning is typically a\nLearning to Rank (LTR) problem, we can make\nuse of some ranking optimization methods. Af-\nter comparing various methods in Table 3(c), we\nadopt ListNet (Cao et al., 2007) with the top-1 ap-\nproximation as the ranking module for VD-BERT:\nLListNet = ‚àí\nN‚àë\ni=1\nf(si) log(f(pi)), (7)\nf(xi) = exp (xi)‚àëN\nj=1 exp (xj)\n, i = 1, ..., N. (8)\nFor training efÔ¨Åciency, we sub-sample the candi-\ndate list and use only N = 30 answers (out of\n100) for each instance. To better leverage the con-\ntrastive signals from the dense annotations, the sub-\nsampling method Ô¨Årst picks randomly the candi-\ndates with non-zero relevance scores, and then it\npicks the ones from zero scores (about 12% of can-\ndidates are non-zero on average).\n4 Experimental Setup\nDatasets. We evaluate our model on the VisDial\nv0.9 and v1.0 datasets (Das et al., 2017). SpeciÔ¨Å-\ncally, v0.9 contains a training set of 82,783 images\nand a validation set of 40,504 images. The v 1.0\ndataset combines the training and validation sets of\nv0.9 into one training set and adds another 2,064\nimages for validation and 8,000 images for test-\ning (hosted blindly in the task organizers‚Äô server).\nEach image is associated with one caption and 10\nquestion-answer pairs. For each question, it is\npaired with a list of 100 answer candidates, one\nof which is regarded as the correct answer.\nFor the v1.0 validation split and a part of v 1.0\ntrain split (2,000 images), extra dense annotations\nfor the answer candidates are provided to make the\nevaluation more reasonable. The dense annotation\nspeciÔ¨Åes a relevance score for each answer candi-\ndate based on the fact that some candidates with\nsimilar semantics to the ground truth answer can\nalso be considered as correct or partially correct,\ne.g., ‚Äúbrown and tan‚Äù and ‚Äúbrown‚Äù in Figure 2.\nEvaluation Metric. Following Das et al. (2017),\nwe evaluate our model using the ranking metrics\nlike Recall@K (K ‚àà{1, 5, 10}), Mean Recipro-\ncal Rank (MRR), and Mean Rank, where only one\nanswer is considered as correct. Since the 2018\nVisDial challenge (after the acquisition of dense\nannotations), NDCG metric that considers the rele-\nvance degree of each answer candidate, has been\nadopted as the main metric to determine the winner.\nConÔ¨Ågurations. We use BERTBASE as the back-\nbone, which consists of 12 Transformer blocks,\neach with 12 attention heads and a hidden state di-\nmensions of 768. We keep the max input sequence\nlength (including 36 visual objects) to 250. We\nuse Adam (Kingma and Ba, 2015) with an initial\nlearning rate of 3e ‚àí5 and a batch size of 32 to\ntrain our model. A linear learning rate decay sched-\nule with a warmup of 0.1 is employed. We Ô¨Årst\ntrain VD-BERT for 30 epochs on a cluster of 4\nV100 GPUs with 16G memory using MLM and\nNSP losses (with equal coefÔ¨Åcients). Here we only\nutilize one previous dialog turn for training efÔ¨Å-\nciency. For instances where the appended answer\ncandidate is incorrect, we do not conduct MLM on\nthe answer sequence to reduce the noise introduced\nby the negative samples. After that, we train for\nanother 10 epochs with full dialog history using ei-\nther NSP in the discriminative setting or MLM on\nthe answer sequence in the generative setting. For\ndense annotation Ô¨Åne-tuning in the discriminative\nsetting, we train with the ListNet loss for 5 epochs.\n5 Results and Analysis\nWe Ô¨Årst compare VD-BERT with state-of-the-art\nmodels on VisDial datasets (¬ß5.1). Then we con-\nduct ablation studies to examine various aspects of\nour model (¬ß5.2), followed by an in-depth analysis\nof Ô¨Åne-tuning on dense annotations (¬ß5.3). Lastly,\nwe interpret how it attains the effective fusion of\nvision and dialog via attention visualization (¬ß5.4).\n5.1 Main Results\nWe report main quantitative comparison results on\nboth VisDial v1.0 and v0.9 datasets below.\nComparison. We consider state-of-the-art pub-\nlished baselines, including NMN (Hu et al., 2017),\nCorefNMN (Kottur et al., 2018), GNN (Zheng\net al., 2019), FGA (Schwartz et al., 2019),\nDV AN (Guo et al., 2019b), RvA (Niu et al.,\n2019), DualVD (Jiang et al., 2020), HACAN (Yang\net al., 2019), Synergistic (Guo et al., 2019a),\nDAN (Kang et al., 2019), ReDAN (Gan et al.,\n2019), CAG (Guo et al., 2020), Square (Kim et al.,\n2020), MCA (Agarwal et al., 2020), MReal-BDAI\nand P1 P2 (Qi et al., 2020). We further report re-\nModel NDCG‚ÜëMRR‚ÜëR@1‚ÜëR@5‚ÜëR@10‚ÜëMean‚Üì\nPublished Results\nÔ£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥\nNMN 58.10 58.80 44.15 76.88 86.88 4.81CorefNMN 54.70 61.50 47.55 78.10 88.80 4.40GNN 52.82 61.37 47.33 77.98 87.83 4.57FGA 52.10 63.70 49.58 80.97 88.55 4.51DV AN 54.70 62.58 48.90 79.35 89.03 4.36RvA 55.59 63.03 49.03 80.40 89.83 4.18DualVD 56.32 63.23 49.25 80.23 89.70 4.11HACAN 57.17 64.22 50.88 80.63 89.45 4.20Synergistic 57.32 62.20 47.90 80.43 89.95 4.17Synergistic‚Ä† 57.88 63.42 49.30 80.77 90.683.97DAN 57.59 63.20 49.63 79.75 89.35 4.30DAN‚Ä† 59.36 64.9251.28 81.6090.88 3.92ReDAN‚Ä† 64.47 53.73 42.45 64.68 75.68 6.64CAG 56.64 63.49 49.85 80.63 90.15 4.11Square‚Ä† 60.16 61.26 47.15 78.73 88.48 4.46MCA‚àó 72.47 37.68 20.67 56.67 72.12 8.89MReal-BDAI‚Ä†‚àó 74.02 52.62 40.03 68.85 79.15 6.76P1P2‚Ä†‚àó 74.9149.13 36.68 62.98 78.55 7.03\nLeaderboard Results\nÔ£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥\nLF 45.31 55.42 40.95 72.45 82.83 5.95HRE 45.46 54.16 39.93 70.45 81.50 6.41MN 47.50 55.49 40.98 72.30 83.30 5.92MN-Att 49.58 56.90 42.42 74.00 84.35 5.59LF-Att 49.76 57.07 42.08 74.82 85.05 5.41MS ConvAI 55.35 63.27 49.53 80.40 89.60 4.15UET-VNU‚Ä† 57.40 59.50 45.50 76.33 85.82 5.34MV AN 59.37 64.84 51.45 81.12 90.65 3.97SGLNs‚Ä† 61.27 59.97 45.68 77.12 87.10 4.85VisDial-BERT‚àó 74.47 50.74 37.95 64.13 80.00 6.28Tohoku-CV‚Ä†‚àó 74.88 52.14 38.93 66.60 80.65 6.53\nOurs\n{VD-BERT 59.96 65.44 51.63 82.2390.68 3.90VD-BERT‚àó 74.54 46.72 33.15 61.58 77.15 7.18VD-BERT‚Ä†‚àó 75.3551.17 38.90 62.82 77.98 6.69\nTable 1: Summary of results on the test-std split of\nVisDial v1.0 dataset. The results are reported by the\ntest server. ‚Äú ‚Ä†‚Äù denotes ensemble model and ‚Äú ‚àó‚Äù in-\ndicates Ô¨Åne-tuning on dense annotations. The ‚Äú ‚Üë‚Äù de-\nnotes higher value for better performance and ‚Äú ‚Üì‚Äù is\nthe opposite. The best and second-best results in each\ncolumn are in bold and underlined respectively.\nsults from the leaderboard1 for a more up-to-date\ncomparison, where some can be found in the arXiv,\nsuch as MV AN (Park et al., 2020), SGLNs (Kang\net al., 2020), VisDial-BERT (Murahari et al., 2019),\nand Tohoku-CV (Nguyen et al., 2019).\nResults on VisDial v1.0 test-std. We report the\ncomparison results on VisDial v1.0 test-std split in\nTable 1 and make the following observations.\n‚Ä¢New state of the art for both single-model and\nensemble settings. Our single-model VD-BERT\nsigniÔ¨Åcantly outperforms all of its single-model\ncounterparts across various metrics, even includ-\ning some ensemble variants such as Synergistic,\nDAN (except R@10), and ReDAN (except NDCG).\nWith further Ô¨Åne-tuning on dense annotations, the\nNDCG score increases quite sharply, from 59.96\nto 74.54 with nearly 15% absolute improvement,\nsetting a new state of the art in the single-model\nsetting. This indicates that dense annotation Ô¨Åne-\ntuning plays a crucial role in boosting the NDCG\n1https://evalai.cloudcv.org/web/\nchallenges/challenge-page/161/\nleaderboard/483#leaderboardrank-1\nModel MRR‚Üë R@1‚Üë R@5‚Üë R@10‚Üë Mean‚Üì\nDiscriminative/Generative\nLF 58.07/51.99 43.82/41.83 74.68/61.78 84.07/67.59 5.78/17.07HRE 58.46/52.37 44.67/42.29 74.50/62.18 84.22/67.92 5.72/17.07HREA 58.68/52.42 44.82/42.28 74.81/62.33 84.36/68.17 5.66/16.79MN 59.65/52.59 45.55/42.29 76.22/62.85 85.37/68.88 5.46/17.06HCIAE 62.22/54.67 48.48/44.35 78.75/65.28 87.59/71.55 4.81/14.23CoAtt 63.98/55.78 50.29/46.10 80.71/65.6988.81/71.74 4.47/14.43RvA 66.34/55.43 52.71/45.37 82.97/65.27 90.73/72.97 3.93/10.71DV AN 66.67/55.9453.62/46.5882.85/65.5090.72/71.253.93/14.79\nVD-BERT70.04/55.95 57.79/46.83 85.34/65.4392.68/72.054.04/13.18\nTable 2: Discriminative and generative results of vari-\nous models on the val split of VisDial v0.9 dataset.\nscores. Moreover, our designed ensemble version\nyields new state of the art (75.35 NDCG), outper-\nforming the 2019 VisDial challenge winner MReal-\nBDAI (74.02 NDCG) by over 1.3 absolute points.\n‚Ä¢Inconsistency between NDCG and other metrics.\nWhile dense annotation Ô¨Åne-tuning yields huge im-\nprovements on NDCG, we also notice that it has a\nsevere countereffect on other metrics, e.g., reducing\nthe MRR score from 65.44 to 46.72 for VD-BERT.\nSuch a phenomenon has also been observed in\nother recent models, such as MReal-BDAI, VisDial-\nBERT, Tohoku-CV Lab, and P1 P2, whose NDCG\nscores surpass others without dense annotation Ô¨Åne-\ntuning by at least around10% absolute points while\nother metrics drop dramatically. We provide a de-\ntailed analysis of this phenomenon in ¬ß5.3.\n‚Ä¢Our VD-BERT is simpler and more effective\nthan VisDial-BERT.VisDial-BERT is a concurrent\nwork to ours that also exploits vision-language pre-\ntrained models for visual dialog. It only reports the\nsingle-model performance of 74.47 NDCG. Com-\npare to that, our VD-BERT achieves slightly better\nresults (74.54 NDCG), however, note that we did\nnot pretrain on large-scale external vision-language\ndatasets like Conceptual Captions (Sharma et al.,\n2018) and VQA as VisDial-BERT does. Besides,\nwhile VisDial-BERT does not observe improve-\nments by ensembling, we endeavor to design an\neffective ensemble strategy (see Table 3(d)) to in-\ncrease the NDCG score to 75.35 for VD-BERT.\nResults on VisDial v0.9 val. We further show\nboth discriminative and generative results on v0.9\nval split in Table 2. For comparison, we choose LF,\nHRE, HREA, MN (Das et al., 2017), HCIAE (Lu\net al., 2017), CoAtt (Wu et al., 2018), RvA, and\nDV AN as they contain results in both settings on\nthe v0.9 val. These models employ dual decoders\nfor each setting separately. Our model continues to\nyield much better results in the discriminative set-\nting (e.g., 70.04 MRR compared to DV AN‚Äôs66.67)\nModel NDCG‚ÜëMRR‚ÜëR@1‚ÜëR@5‚ÜëR@10‚ÜëMean‚Üì\n(a)\nFrom scratch 56.20 62.25 48.16 79.57 89.01 4.31Init from VLP 61.79 66.67 53.23 83.60 91.97 3.66Init from BERT63.22 67.44 54.02 83.96 92.33 3.53‚Ü™‚Üíonly NSP 55.89 63.15 48.98 80.45 89.72 4.15\n(b)\nNo history 64.7062.93 48.70 80.42 89.73 4.30One previous turn 63.47 65.30 51.66 82.30 90.97 3.86Full history 63.22 67.44 54.02 83.96 92.33 3.53‚Ü™‚Üíonly text 54.32 62.79 48.48 80.12 89.33 4.27\n(c)\nCE 74.47 44.94 32.23 60.10 76.70 7.57ListNet 74.5446.72 33.15 61.58 77.157.18ListMLE 72.96 36.81 20.70 54.60 73.28 8.90ApproxNDCG 72.4549.88 37.88 62.90 77.407.26\n(d)\nEPOCH 74.84 47.40 34.30 61.58 77.78 7.12LENGTH 75.07 47.33 33.88 62.2078.50 7.01RANK 75.13 50.00 38.28 60.93 77.28 6.90DIVERSE 75.35 51.17 38.90 62.8277.98 6.69\nTable 3: Extensive ablation studies: training with (a)\nvarious settings and (b) contexts on v1.0 val; dense an-\nnotation Ô¨Åne-tuning with (c) varying ranking methods\nand (d) various ensemble strategies on v1.0 test-std.\nand comparable results with the state of the art in\nthe generative setting (e.g., 55.95 MRR score vs.\nDV AN‚Äôs55.94). This validates the effectiveness\nof our VD-BERT in both settings using a uniÔ¨Åed\nTransformer encoder. By contrast, VisDial-BERT\ncan only support the discriminative setting.\n5.2 Ablation Study\nWe Ô¨Årst study how different training settings in-\nÔ¨Çuence the results in Table 3(a). We observe that\ninitializing the model with weights from BERT\nindeed beneÔ¨Åts the visual dialog task a lot, increas-\ning the NDCG score by about 7% absolute over\nthe model trained from scratch. Surprisingly, the\nmodel initialized with the weights from VLP that\nwas pretrained on Conceptual Captions (Sharma\net al., 2018), does not work better than the one ini-\ntialized from BERT. It might be due to the domain\ndiscrepancy between image captions and multi-turn\ndialogs, as well as the slightly different experiment\nsettings (e.g., we extract 36 objects from image\ncompared to their 100 objects). Another possible\nreason might be that the VisDial data with more\nthan one million image-dialog turn pairs can pro-\nvide adequate contexts to adapt BERT for effective\nvision and dialog fusion. We also Ô¨Ånd that the vi-\nsually grounded MLM is crucial for transferring\nBERT into the multimodal setting, indicated by a\nlarge performance drop when using only NSP.\nWe then examine the impact of varying the dia-\nlog context used for training in Table 3(b). With\nlonger dialog history (‚ÄúFull history‚Äù), our model\nindeed yields better results in most of the ranking\nmetrics, while the one without using any dialog\nhistory obtains the highest NDCG score. This in-\n1. no (0.0) 2. yes(1.0) 3. no it is not (0.0) 4. it is not visible (0.0) 5. icannot tell ( 0.0)6. yes, it is (1.0) 7. it is (0.8) 8. ican't tell (0.0)\n1. yes(1.0) 2. yes it is (1.0) 3. yes, it is (1.0) 4. yep (0.8) 5. it is (0.8)6. yes some (0.6)7. I think so (0.6)8. definitely (0.6)W/ Fine-tuningNDCG=97.06Base ModelNDCG=41.31\nAn elephant eats large amounts of foliage as another elephant stands nearby\nQ1: is the elephant a baby? A1: noQ2: is he eating from a tree?A2: no the groundQ3: are they outside?A3: yesQ4: is the food in his mouth?A4: yes (GT)\n1. yes (0.0) 2. yes people (0.0) 3. no it's empty (0.4) 4. icannot tell (0.8) 5. yes a few (0.0)6. yes there are (0.0)7. no (0.4)8. yes for sure (0.0)\n1. icannot tell (0.8) 2. ican't tell (0.8) 3. can't tell (0.8) 4. not sure (0.8) 5. idon't know (0.8)6. icannot see any (0.8)7. not visible (0.6)8. not that ican see (0.6)W/ Fine-tuningNDCG=91.80Base ModelNDCG=42.19A double decker bus sits empty at the station\nQ1: are there any people? A1: yesQ2: are they on the bus?A2: no, the bus is emptyQ3: are there any other buses?A3: 1 other busQ4: are there people on bus?A4: no it's empty (GT)\nFigure 3: The effects of dense annotation Ô¨Åne-tuning in\nour VD-BERT for two examples. GT: ground truth.\ndicates that dense relevance scores might be anno-\ntated with less consideration of dialog history. If\nwe remove the visual cues from the ‚ÄúFull history‚Äù\nmodel, we see a drop in all metrics, especially, on\nNDCG. However, this version still obtains compa-\nrable results to the ‚ÄúNo history‚Äù variant, revealing\nthat textual information dominates the VisDial task.\nIn Table 3(c), we compare Cross Entropy (CE)\ntraining with a bunch of other listwise ranking op-\ntimization methods: ListNet (Cao et al., 2007),\nListMLE (Xia et al., 2008), and approxNDCG (Qin\net al., 2010). Among these methods, ListNet yields\nthe best NDCG and Mean Rank, while the approx-\nNDCG achieves the best MRR and Recall on Vis-\nDial v1.0 test-std. Therefore, we employ the List-\nNet as our ranking module.\nWe also explore ways to achieve the best en-\nsemble performance with various model selection\ncriteria in Table 3(d). We consider three criteria,\nEPOCH , LENGTH , and RANK that respectively re-\nfer to predictions from different epochs of a single\nmodel, from different models trained with varying\ncontext lengths and with different ranking methods\nin Table 3(b)-(c). We use four predictions from\neach criterion and combine their diverse predic-\ntions (DIVERSE ) by summing up their normalized\nranking scores. We observe thatEPOCH contributes\nthe least to the ensemble performance while RANK\nmodels are more helpful thanLENGTH models. The\ndiverse set of them leads to the best performance.\n5.3 Fine-tuning on Dense Annotations\nIn this section, we focus on the effect of dense an-\nnotation Ô¨Åne-tuning and try to analyze the reason of\nthe inconsistency issue between NDCG and other\nranking metrics (see Table 1) in the following.\nCase Study. We provide two examples to qual-\nitatively demonstrate how dense annotation Ô¨Åne-\ntuning results in better NDCG scores in Figure 3.\nFor the example at the top, Ô¨Åne-tuning helps our\nmodel to assign higher ranks to the answers that\nshare similar semantics with the ground truth an-\nswer and should also be regarded as correct (‚Äúyes,\nit is‚Äù and ‚Äúyep‚Äù vs. ‚Äúyes‚Äù). In the example at the\nbottom, we spot a mismatch between the sparse and\ndense annotations: the ground truth answer ‚Äúno, it‚Äôs\nempty‚Äù is only given a 0.4 relevance score, while\nuncertain answers like ‚Äúi don‚Äôt know‚Äù are consid-\nered to be more relevant. In this case, Ô¨Åne-tuning\ninstead makes our model fail to predict the correct\nanswer despite the increase of NDCG score.\nRelevance Score and Question Type Analysis.\nWe Ô¨Årst show how various metrics change for Ô¨Åne-\ntuning in Figure 4. For this experiment, we ran-\ndomly sample 200 instances from VisDial v1.0 val\nas the test data and use the rest for Ô¨Åne-tuning\nwith the ListNet ranking method. We observe\nthat NDCG keeps increasing with more epochs of\nÔ¨Åne-tuning, while other metrics such as Recall@K\nand MRR) drop. For further analysis, we clas-\nsify the 2, 064 instances in VisDial v1.0 val set\nbased on the ground-truth‚Äôs relevance score and\nquestion type (Table 4). We consider four bins\n{0.0, 0.2 ‚àº0.4, 0.6 ‚àº0.8, 1.0}for the relevance\nscore and four question types: Yes/no, Number,\nColor, and Others. We then analyze the NDCG\nscores assigned by DAN (Kang et al., 2019) and\nour VD-BERT with and without dense annotation\nÔ¨Åne-tuning. We choose DAN as it achieves good\nNDCG scores (Table 1) and provides the source\ncode to reproduce their predictions.\nBy examining the distribution of the relevance\nscores, we Ô¨Ånd that only 31% of them are aligned\nwell with the sparse annotations and 9% are totally\nmisaligned. As the degree of such mismatch in-\ncreases (relevance score changes 1.0 ‚Üí0.0), both\nDAN and our model witness a plunge in NDCG\n(63.29 ‚Üí43.86 and 70.25 ‚Üí48.07), while dense\nannotation Ô¨Åne-tuning signiÔ¨Åcantly boosts NDCG\nscores for all groups, especially for the most mis-\naligned one (48.07 ‚Üí82.84 for our model). These\nresults validate that the misalignment of the sparse\nand dense annotations is the key reason for the\ninconsistency between NDCG and other metrics.\nFor question types, we observe that Yes/no is the\nmajor type (76%) and also the easiest one, while\nNumber is the most challenging and least frequent\none (3%). Our model outperforms DAN by over\n10% in most of the question types except Color.\nFine-tuning on dense annotations gives our model\nhuge improvements across all the question types,\nespecially for Others with over 30% absolute gain.\n20406080100\n012345678\nNDCGMRRR@1R@5R@10\nEpoch Num\nRanking Score (%)\nFigure 4: Dense annotation Ô¨Åne-tuning on\nvarious metrics with the ListNet method.\nModels All Relevance Score Question Type\n1.0 0.6‚àº0.8 0.2‚àº0.4 0.0 Yes/no Number Color Others(31%) (35%) (25 %) (9 %) (76%) (3 %) (11%) (10%)\nDAN 58.28 63.29 61.02 53.29 43.8659.86 41.03 57.55 51.89Ours 63.55 70.25 65.18 58.40 48.0765.45 48.98 58.51 58.75Ours (w/ ft) 89.6295.38 89.76 84.63 82.8491.05 74.41 84.00 89.12\nTable 4: NDCG scores in VisDial v1.0 val split broken down into4\ngroups based on either the relevance score or the question type. The%\nvalue in the parentheses denotes the corresponding data proportion.\nLayer 5 Head 7\n(b)\n(a)Layer 1 Head 11Layer3 Head 1 Layer 8 Head 2Layer 5 Head 5\nFigure 5: Attention weight visualization in our VD-BERT for a sampled image-dialog example.\n5.4 Attention Visualization\nTo interpret our VD-BERT, we visualize the atten-\ntion weights on the top 10 detected objects from its\ncaption in Figure 5(a). We observe that many heads\nat different layers can correctly ground some enti-\nties like person and motorcycle in the image,\nand even reveal some high-level semantic correla-\ntions such as person‚Üîmotorcycle (at L8H2)\nand motorcycle‚Üîstreet (at L1H11). Be-\nsides, heads at higher layers tend to have a sharper\nfocus on speciÔ¨Åc objects like the man and the mo-\ntorcycles in the image.\nNext, we examine how our VD-BERT captures\nthe interactions between image and multi-turn di-\nalog. In contrast to other vision-language tasks,\nvisual dialog has a more complex multi-turn struc-\nture, thereby posing a hurdle for effective fusion.\nAs shown in Figure 5(b), VD-BERT can ground\nentities and discover some object relations, e.g.,\nhelmet is precisely related to the man and the mo-\ntorcycle in the image (see the rightmost red box).\nMore interestingly, it can even resolve visual pro-\nnoun coreference of he in the question to the man\nin the image (see the middle red box). We provide\nmore qualitative examples in Figure 6 and 7.\n6 Conclusion\nWe have presented VD-BERT, a uniÔ¨Åed vision-\ndialog Transformer model that exploits the pre-\ntrained BERT language models for visual dialog.\nVD-BERT is capable of modeling all the interac-\ntions between an image and a multi-turn dialog\nwithin a single-stream Transformer encoder and\nenables the effective fusion of features from both\nmodalities via simple visually grounded training.\nBesides, it can either rank or generate answers\nseamlessly. Without pretraining on external vision-\nlanguage datasets, our model establishes new state-\nof-the-art performance in the discriminative setting\nand shows promising results in the generative set-\nting on the visual dialog benchmarks.\nAcknowledgements\nWe thank Chien-Sheng Wu, Jiashi Feng, Jiaxin Qi,\nand our anonymous reviewers for their insightful\nfeedback on our paper. This work was partially sup-\nported by the Research Grants Council of the Hong\nKong Special Administrative Region, China (No.\nCUHK 14210717, General Research Fund; No.\nCUHK 2300174, Collaborative Research Fund).\nReferences\nShubham Agarwal, Trung Bui, Joon-Young Lee, Ioan-\nnis Konstas, and Verena Rieser. 2020. History for\nvisual dialog: Do we really need it? In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July\n5-10, 2020, pages 8182‚Äì8197. Association for Com-\nputational Linguistics.\nChris Alberti, Jeffrey Ling, Michael Collins, and David\nReitter. 2019. Fusion of detected objects in text\nfor visual question answering. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2131‚Äì2140. Association for\nComputational Linguistics.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: visual question an-\nswering. In 2015 IEEE International Conference on\nComputer Vision, ICCV 2015, Santiago, Chile, De-\ncember 7-13, 2015, pages 2425‚Äì2433.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization. CoRR,\nabs/1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nZhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and\nHang Li. 2007. Learning to rank: from pairwise ap-\nproach to listwise approach. In Machine Learning,\nProceedings of the Twenty-Fourth International Con-\nference (ICML 2007), Corvallis, Oregon, USA, June\n20-24, 2007, volume 227 ofACM International Con-\nference Proceeding Series, pages 129‚Äì136. ACM.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. UNITER: learning universal\nimage-text representations. CoRR, abs/1909.11740.\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi\nSingh, Deshraj Yadav, Jos ¬¥e M. F. Moura, Devi\nParikh, and Dhruv Batra. 2017. Visual dialog. In\n2017 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2017, Honolulu, HI, USA,\nJuly 21-26, 2017, pages 1080‚Äì1089.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171‚Äì4186.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, 8-14 December 2019, Vancouver, BC,\nCanada, pages 13042‚Äì13054.\nZhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li,\nJingjing Liu, and Jianfeng Gao. 2019. Multi-step\nreasoning via recurrent dual attention for visual dia-\nlog. In Proceedings of the 57th Conference of the As-\nsociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 6463‚Äì6474.\nDalu Guo, Chang Xu, and Dacheng Tao. 2019a. Image-\nquestion-answer synergistic network for visual dia-\nlog. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2019, Long Beach, CA,\nUSA, June 16-20, 2019, pages 10434‚Äì10443.\nDan Guo, Hui Wang, and Meng Wang. 2019b. Dual vi-\nsual attention network for visual dialog. In Proceed-\nings of the Twenty-Eighth International Joint Confer-\nence on ArtiÔ¨Åcial Intelligence, IJCAI 2019, Macao,\nChina, August 10-16, 2019 , pages 4989‚Äì4995. ij-\ncai.org.\nDan Guo, Hui Wang, Hanwang Zhang, Zheng-Jun\nZha, and Meng Wang. 2020. Iterative context-\naware graph inference for visual dialog. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 10052‚Äì10061. IEEE.\nRonghang Hu, Jacob Andreas, Marcus Rohrbach,\nTrevor Darrell, and Kate Saenko. 2017. Learning\nto reason: End-to-end module networks for visual\nquestion answering. In IEEE International Confer-\nence on Computer Vision, ICCV 2017, Venice, Italy,\nOctober 22-29, 2017 , pages 804‚Äì813. IEEE Com-\nputer Society.\nXiaoze Jiang, Jing Yu, Zengchang Qin, Yingying\nZhuang, Xingxing Zhang, Yue Hu, and Qi Wu. 2020.\nDualvd: An adaptive dual encoding model for deep\nvisual understanding in visual dialogue. In The\nThirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of ArtiÔ¨Åcial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in ArtiÔ¨Åcial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 11125‚Äì\n11132. AAAI Press.\nGi-Cheon Kang, Jaeseo Lim, and Byoung-Tak Zhang.\n2019. Dual attention networks for visual reference\nresolution in visual dialog. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2024‚Äì2033. Association for\nComputational Linguistics.\nGi-Cheon Kang, Junseok Park, Hwaran Lee, Byoung-\nTak Zhang, and Jin-Hwa Kim. 2020. Dialgraph:\nSparse graph learning networks for visual dialog.\nCoRR, abs/2004.06698.\nHyounghun Kim, Hao Tan, and Mohit Bansal. 2020.\nModality-balanced models for visual dialogue. In\nThe Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial In-\ntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of ArtiÔ¨Åcial Intelligence Confer-\nence, IAAI 2020, The Tenth AAAI Symposium on Ed-\nucational Advances in ArtiÔ¨Åcial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 ,\npages 8091‚Äì8098. AAAI Press.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nSatwik Kottur, Jos¬¥e M. F. Moura, Devi Parikh, Dhruv\nBatra, and Marcus Rohrbach. 2018. Visual corefer-\nence resolution in visual dialog using neural module\nnetworks. In Computer Vision - ECCV 2018 - 15th\nEuropean Conference, Munich, Germany, Septem-\nber 8-14, 2018, Proceedings, Part XV , pages 160‚Äì\n178.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Vi-\nsual genome: Connecting language and vision us-\ning crowdsourced dense image annotations. Interna-\ntional Journal of Computer Vision, 123(1):32‚Äì73.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In The Thirty-Fourth AAAI Conference\non ArtiÔ¨Åcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of ArtiÔ¨Åcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in ArtiÔ¨Åcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 11336‚Äì11344. AAAI Press.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. CoRR, abs/1908.03557.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, 8-\n14 December 2019, Vancouver, BC, Canada , pages\n13‚Äì23.\nJiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh,\nand Dhruv Batra. 2017. Best of both worlds: Trans-\nferring knowledge from discriminative learning to a\ngenerative visual dialog model. In Advances in Neu-\nral Information Processing Systems 30: Annual Con-\nference on Neural Information Processing Systems\n2017, 4-9 December 2017, Long Beach, CA, USA ,\npages 314‚Äì324.\nVishvak Murahari, Dhruv Batra, Devi Parikh, and Ab-\nhishek Das. 2019. Large-scale pretraining for visual\ndialog: A simple state-of-the-art baseline. CoRR,\nabs/1912.02379.\nVan-Quang Nguyen, Masanori Suganuma, and\nTakayuki Okatani. 2019. EfÔ¨Åcient attention mech-\nanism for handling all the interactions betweenn\nmany inputs with application to visual dialog.\nCoRR, abs/1911.11390.\nYulei Niu, Hanwang Zhang, Manli Zhang, Jianhong\nZhang, Zhiwu Lu, and Ji-Rong Wen. 2019. Recur-\nsive visual attention in visual dialog. In IEEE Con-\nference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 6679‚Äì6688.\nSungjin Park, Taesun Whang, Yeochan Yoon, and\nHueiseok Lim. 2020. Multi-view attention networks\nfor visual dialog. CoRR, abs/2004.14025.\nJiaxin Qi, Yulei Niu, Jianqiang Huang, and Hanwang\nZhang. 2020. Two causal principles for improv-\ning visual dialog. In 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020 , pages\n10857‚Äì10866. IEEE.\nTao Qin, Tie-Yan Liu, and Hang Li. 2010. A gen-\neral approximation framework for direct optimiza-\ntion of information retrieval measures. Inf. Retr.,\n13(4):375‚Äì397.\nShaoqing Ren, Kaiming He, Ross B. Girshick, and\nJian Sun. 2015. Faster R-CNN: towards real-time\nobject detection with region proposal networks. In\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 91‚Äì99.\nIdan Schwartz, Seunghak Yu, Tamir Hazan, and\nAlexander G. Schwing. 2019. Factor graph atten-\ntion. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2019, Long Beach, CA,\nUSA, June 16-20, 2019, pages 2039‚Äì2048.\nPaul Hongsuck Seo, Andreas M. Lehrmann, Bohyung\nHan, and Leonid Sigal. 2017. Visual reference reso-\nlution using attention memory for visual dialog. In\nAdvances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, 4-9 December 2017, Long\nBeach, CA, USA, pages 3719‚Äì3729.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 1: Long Papers ,\npages 2556‚Äì2565.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 6418‚Äì6428.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. Videobert: A\njoint model for video and language representation\nlearning. In 2019 IEEE/CVF International Confer-\nence on Computer Vision, ICCV 2019, Seoul, Ko-\nrea (South), October 27 - November 2, 2019 , pages\n7463‚Äì7472. IEEE.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems 27: Annual Conference on Neural Informa-\ntion Processing Systems 2014, December 8-13 2014,\nMontreal, Quebec, Canada, pages 3104‚Äì3112.\nHao Tan and Mohit Bansal. 2019. LXMERT: learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019 , pages\n5099‚Äì5110. Association for Computational Linguis-\ntics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nTaesun Whang, Dongyub Lee, Chanhee Lee, Kisu\nYang, Dongsuk Oh, and Heuiseok Lim. 2019. Do-\nmain adaptive training BERT for response selection.\nCoRR, abs/1908.04812.\nQi Wu, Peng Wang, Chunhua Shen, Ian D. Reid, and\nAnton van den Hengel. 2018. Are you talking to me?\nreasoned visual dialog generation through adversar-\nial learning. In 2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt\nLake City, UT, USA, June 18-22, 2018, pages 6106‚Äì\n6115.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google‚Äôs neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nFen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and\nHang Li. 2008. Listwise approach to learning to\nrank: theory and algorithm. In Machine Learning,\nProceedings of the Twenty-Fifth International Con-\nference (ICML 2008), Helsinki, Finland, June 5-9,\n2008, volume 307 of ACM International Conference\nProceeding Series, pages 1192‚Äì1199. ACM.\nTianhao Yang, Zheng-Jun Zha, and Hanwang Zhang.\n2019. Making history matter: History-advantage\nsequence training for visual dialog. In 2019\nIEEE/CVF International Conference on Computer\nVision, ICCV 2019, Seoul, Korea (South), October\n27 - November 2, 2019, pages 2561‚Äì2569. IEEE.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. TACL, 2:67‚Äì78.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Visual\ncommonsense reasoning. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019 ,\npages 6720‚Äì6731. Computer Vision Foundation /\nIEEE.\nZilong Zheng, Wenguan Wang, Siyuan Qi, and Song-\nChun Zhu. 2019. Reasoning visual dialogs with\nstructural and partial observations. In IEEE Con-\nference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 6669‚Äì6678.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason J. Corso, and Jianfeng Gao. 2020. Uni-\nÔ¨Åed vision-language pre-training for image caption-\ning and VQA. In The Thirty-Fourth AAAI Con-\nference on ArtiÔ¨Åcial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of ArtiÔ¨Åcial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in ArtiÔ¨Åcial In-\ntelligence, EAAI 2020, New York, NY, USA, Febru-\nary 7-12, 2020, pages 13041‚Äì13049. AAAI Press.\nL4H7L3H6\nL1H11 L4H9 L7H1 L9H12\nL2H7 L11H9L1H2 L5H3\n(d)\n(c)\nL5H1\n(b)\nLayer 4 Head 12\nAttend to previous word\nLayer 2 Head 12\nAttend to next word\nLayer 10 Head 2\nAttend to self-token\n(a)\nL2H4L1H4 L5H1\nFigure 6: More attention visualization examples showing that VD-BERT achieves the effective fusion of vision\nand dialog contents. L xHy: Layer x Head y (1 ‚â§x, y‚â§12). (a) It learns three apparent attention patterns for\nthe example in Figure 5: attentions that a token puts to its previous token, to itself, and to the next token. Besides,\nsome of its attention heads can precisely ground some entities between image and caption/multi-turn dialog: (b)\npizza, beer, and table; (c) horse, wild, and giraffe; (d) teenage girl, hair, and phone.\nImage + CaptionGT DialogDAN VD-BERT VD-BERT (w/ft)\na brown building stands at the corner of a street on a sunny day\nQ1: is there any people in the photo?A1: yes far awayQ2: are there any streets visible in the photo?A2: yesQ3: are there cars parked along the streets?A3: noQ4: what kind of buildings appear in the photo?A4: older architectural building like a libraryQ5: is there any signs visible in the photo?A5: there is a no entry signQ6: are there traffic lights visible?A6: noQ7: can you see the sidewalk?A7: yesQ8: are the sidewalks clean?A8: immaculateQ9: are there any trees planted near the building?A9: there is a tree next to the buildingQ10: is the tree short or tall?A10: tall\nQ1: is there any people in the photo?A1: yesQ2: are there any streets visible in the photo?A2: no on the streetQ3: are there cars parked along the streets?A3: noQ4: what kind of buildings appear in the photo?A4: yes shopsQ5: is there any signs visible in the photo?A5: yesQ6: are there traffic lights visible?A6: 1 light poleQ7: can you see the sidewalk?A7: yesQ8: are the sidewalks clean?A8: yes very cleanQ9: are there any trees planted near the building?A9: yesQ10: is the tree short or tall?A10: tall\nQ1: is there any people in the photo?A1: yesQ2: are there any streets visible in the photo?A2: yesQ3: are there cars parked along the streets?A3: noQ4: what kind of buildings appear in the photo?A4: older architectural building like a libraryQ5: is there any signs visible in the photo?A5: yesQ6: are there traffic lights visible?A6: noQ7: can you see the sidewalk?A7: yesQ8: are the sidewalks clean?A8: yesQ9: are there any trees planted near the building?A9: there is a tree next to the buildingQ10: is the tree short or tall?A10: tall\nQ1: is there any people in the photo?A1: yesQ2: are there any streets visible in the photo?A2: yesQ3: are there cars parked along the streets?A3: noQ4: what kind of buildings appear in the photo?A4: not sureQ5: is there any signs visible in the photo?A5: yesQ6: are there traffic lights visible?A6: noQ7: can you see the sidewalk?A7: yesQ8: are the sidewalks clean?A8: yes, they are cleanQ9: are there any trees planted near the building?A9: no there isn'tQ10: is the tree short or tall?A10: tall\nman pointing at tv display in a store\nQ1: is the picture in color?A1: yesQ2: how many people are in the picture?A2: isee 2 peopleQ3: how many tv's are in the picture?A3: 18 screens on the display but there are some in boxes as wellQ4: is 1 of the people a worker in the store?A4: no it doesn‚Äôt look like itQ5: is there an image on the tv that he is pointing at?A5: yes it isan cartoonQ6: is the man standing or sitting?A6: standingQ7: what is the brand of tv?A7: iam unable to tellQ8: can you tell what store it is?A8: noQ9: are there any other products in view?A9: ijust see tv's and boxesQ10: what coloris the floor?A10: blue\nQ1: is the picture in color?A1: yesQ2: how many people are in the picture?A2: 2Q3: how many tv's are in the picture?A3: 2Q4: is 1 of the people a worker in the store?A4: yes, but they are taking a breakQ5: is there an image on the tv that he is pointing at?A5: yes it isan cartoonQ6: is the man standing or sitting?A6: standingQ7: what is the brand of tv?A7: it's in japaneseQ8: can you tell what store it is?A8: yes it isa list of rulesQ9: are there any other products in view?A9: there is papers all over the desk so iwould say yesQ10: what coloris the floor?A10: blue\nQ1: is the picture in color?A1: yesQ2: how many people are in the picture?A2: 2Q3: how many tv's are in the picture?A3: 18 screens on the display but there are some in boxes as wellQ4: is 1 of the people a worker in the store?A4: there are no visible workersQ5: is there an image on the tv that he is pointing at?A5: yes it isan cartoonQ6: is the man standing or sitting?A6: standingQ7: what is the brand of tv?A7: it's in japaneseQ8: can you tell what store it is?A8: noQ9: are there any other products in view?A9: ijust see tv's and boxesQ10: what coloris the floor?A10: blue\nQ1: is the picture in color?A1: yes it isQ2: how many people are in the picture?A2: 1Q3: how many tv's are in the picture?A3: 18 screens on the display but there are some in boxes as wellQ4: is 1 of the people a worker in the store?A4: there are no visible workersQ5: is there an image on the tv that he is pointing at?A5: yesQ6: is the man standing or sitting?A6: standingQ7: what is the brand of tv?A7: ican't tellQ8: can you tell what store it is?A8: noQ9: are there any other products in view?A9: yesQ10: what coloris the floor?A10: blue\na woman is smiling next to a decorated microwave\nQ1: is there anyone in the room with the woman?A1: noQ2: is she young?A2: noQ3: what coloris her hair?A3: blackQ4: is it a big microwave?A4: yesQ5: is it sunny?A5: noQ6: is there any other appliances in the room?A6: noQ7: what is the woman wearing?A7: a shirtQ8: is her hair long?A8: noQ9: what coloris the microwave?A9: whiteQ10: is she wearing glasses?A10: no\nQ1: is there anyone in the room with the woman?A1: no, she is aloneQ2: is she young?A2: no about 18Q3: what coloris her hair?A3: brownQ4: is it a big microwave?A4: normal sizeQ5: is it sunny?A5: noQ6: is there any other appliances in the room?A6: noQ7: what is the woman wearing?A7: brown jacketQ8: is her hair long?A8: yesQ9: what coloris the microwave?A9: silverQ10: is she wearing glasses?A10: no\nQ1: is there anyone in the room with the woman?A1: noQ2: is she young?A2: yesQ3: what coloris her hair?A3: brownQ4: is it a big microwave?A4: yesQ5: is it sunny?A5: noQ6: is there any other appliances in the room?A6: noQ7: what is the woman wearing?A7: black jeans and a t shirtQ8: is her hair long?A8: noQ9: what coloris the microwave?A9: whiteQ10: is she wearing glasses?A10: no\nQ1: is there anyone in the room with the woman?A1: noQ2: is she young?A2: yesQ3: what coloris her hair?A3: brownQ4: is it a big microwave?A4: yesQ5: is it sunny?A5: ican't tellQ6: is there any other appliances in the room?A6: not that ican seeQ7: what is the woman wearing?A7: a black jacketQ8: is her hair long?A8: noQ9: what coloris the microwave?A9: whiteQ10: is she wearing glasses?A10: no\nan elephant sprays the person on its back\nQ1: is the elephant large?A1: yes, but doesn't look full grownQ2: how many people are on the elephant?A2: ican only see 1Q3: are there more elephants in the picture?A3: noQ4: what is the person wearing?A4: ithink it's shorts, hard to tell with the spray from the elephantQ5: where is the elephant getting the water from?A5: it's either a large puddle or maybe a river bank, not sureQ6: is the water clean or dirty?A6: it looks pretty muddy, so iwould say dirtyQ7: is there grass anywhere near the water or mainly mud?A7: mainly mudQ8: can you see the sky?A8: noQ9: are there any trees in the photo?A9: yesQ10: can you tell what type of trees?A10: no\nQ1: is the elephant large?A1: yesQ2: how many people are on the elephant?A2: four, 1 in each photoQ3: are there more elephants in the picture?A3: no, just the 2 birdsQ4: what is the person wearing?A4: a green button upQ5: where is the elephant getting the water from?A5: it's either a large puddle or maybe a river bank, not sureQ6: is the water clean or dirty?A6: it looks pretty muddy, so iwould say dirtyQ7: is there grass anywhere near the water or mainly mud?A7: there's a bit of grass, and a green reflection on the waterQ8: can you see the sky?A8: a very little part of if through the treesQ9: are there any trees in the photo?A9: yes, but there's hardly any leavesQ10: can you tell what type of trees?A10: iwould think late spring or summer cause all the trees are nice and green\nQ1: is the elephant large?A1: yesQ2: how many people are on the elephant?A2: 2Q3: are there more elephants in the picture?A3: noQ4: what is the person wearing?A4: a waterproof swimsuit thingQ5: where is the elephant getting the water from?A5: it's either a large puddle or maybe a river bank, not sureQ6: is the water clean or dirty?A6: it looks pretty muddy, so iwould say dirtyQ7: is there grass anywhere near the water or mainly mud?A7: mainly mudQ8: can you see the sky?A8: noQ9: are there any trees in the photo?A9: yesQ10: can you tell what type of trees?A10: no\nQ1: is the elephant large?A1: yesQ2: how many people are on the elephant?A2: 1Q3: are there more elephants in the picture?A3: noQ4: what is the person wearing?A4: he is wearing a jacketQ5: where is the elephant getting the water from?A5: cannot tell from the pictureQ6: is the water clean or dirty?A6: it looks pretty muddy, so iwould say dirtyQ7: is there grass anywhere near the water or mainly mud?A7: no, there is 0Q8: can you see the sky?A8: noQ9: are there any trees in the photo?A9: yesQ10: can you tell what type of trees?A10: no ican't\na man wearing a hat holds a small white dog\nQ1: age of man?A1: maybe 20sQ2: type of dog?A2: a white fluffy dogQ3: is it small?A3: yesQ4: can you see the head?A4: ican see the dogs head and the man's headQ5: is the mouth open on the dog?A5: noQ6: what else is the man wearing?A6: eyeglassesQ7: what colorhat is it?A7: tan hatQ8: what coloris his shirt?A8: hard to tell, possibly a grey vestQ9: what coloris his pants?A9: they are not shownQ10: is the man outside?A10: no, he is inside\nQ1: age of man?A1: 40sQ2: type of dog?A2: a white fluffy dogQ3: is it small?A3: average sizeQ4: can you see the head?A4: yesQ5: is the mouth open on the dog?A5: yesQ6: what else is the man wearing?A6: a jacket and a hatQ7: what colorhat is it?A7: brownQ8: what coloris his shirt?A8: blackQ9: what coloris his pants?A9: blackQ10: is the man outside?A10: no, he is inside\nQ1: age of man?A1: 40sQ2: type of dog?A2: a white fluffy dogQ3: is it small?A3: yesQ4: can you see the head?A4: ican see the dogs head and the man's headQ5: is the mouth open on the dog?A5: noQ6: what else is the man wearing?A6: a jacket and a hatQ7: what colorhat is it?A7: tan hatQ8: what coloris his shirt?A8: whiteQ9: what coloris his pants?A9: they are not shownQ10: is the man outside?A10: no, he is inside\nQ1: age of man?A1: 40sQ2: type of dog?A2: ican't tellQ3: is it small?A3: yesQ4: can you see the head?A4: yesQ5: is the mouth open on the dog?A5: nopeQ6: what else is the man wearing?A6: eyeglassesQ7: what colorhat is it?A7: it's brownQ8: what coloris his shirt?A8: his shirt is whiteQ9: what coloris his pants?A9: ican't see itQ10: is the man outside?A10: no\na small bedroom with a hall with laundry in the background\nQ1: do you see any people?A1: noQ2: is there a bed in the bedroom?A2: yesQ3: is it a big pile of laundry?A3: noQ4: what coloris the bedspread?A4: white and light grayQ5: is itdirty laundry or clean?A5: cleanQ6: is the hall carpeted or hard floor?A6: hard floorQ7: is the laundry hanging or piled or folded?A7: hangingQ8: is it a light or dark wood floor?A8: darkQ9: is the bedroom a child's room, guest room, or adult bedroom?A9: adultQ10: is there any other furniture in the bedroom?A10: yes\nQ1: do you see any people?A1: noQ2: is there a bed in the bedroom?A2: noQ3: is it a big pile of laundry?A3: yesQ4: what coloris the bedspread?A4: white and tan blanketsQ5: is itdirty laundry or clean?A5: it's cleanQ6: is the hall carpeted or hard floor?A6: woodQ7: is the laundry hanging or piled or folded?A7: hangingQ8: is it a light or dark wood floor?A8: lightQ9: is the bedroom a child's room, guest room, or adult bedroom?A9: hard to sayQ10: is there any other furniture in the bedroom?A10: a table and another bed\nQ1: do you see any people?A1: noQ2: is there a bed in the bedroom?A2: yesQ3: is it a big pile of laundry?A3: noQ4: what coloris the bedspread?A4: whiteQ5: is itdirty laundry or clean?A5: cleanQ6: is the hall carpeted or hard floor?A6: hard floorQ7: is the laundry hanging or piled or folded?A7: hangingQ8: is it a light or dark wood floor?A8: lightQ9: is the bedroom a child's room, guest room, or adult bedroom?A9: adultQ10: is there any other furniture in the bedroom?A10: a table and another bed\nQ1: do you see any people?A1: noQ2: is there a bed in the bedroom?A2: yesQ3: is it a big pile of laundry?A3: yesQ4: what coloris the bedspread?A4: white and light grayQ5: is itdirty laundry or clean?A5: cleanQ6: is the hall carpeted or hard floor?A6: hard floorQ7: is the laundry hanging or piled or folded?A7: hangingQ8: is it a light or dark wood floor?A8: lightQ9: is the bedroom a child's room, guest room, or adult bedroom?A9: adultQ10: is there any other furniture in the bedroom?A10: yes\nFigure 7: More qualitative examples in VisDial v1.0 val split for three model variants: DAN (Kang et al., 2019),\nVD-BERT, and VD-BERT with dense annotation Ô¨Åne-tuning. The second column is for ground truth (GT) dialog."
}