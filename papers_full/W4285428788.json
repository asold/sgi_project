{
  "title": "Multi-Behavior Hypergraph-Enhanced Transformer for Sequential Recommendation",
  "url": "https://openalex.org/W4285428788",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2122011417",
      "name": "Yuhao Yang",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2101072704",
      "name": "Chao Huang",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3034975406",
      "name": "Lianghao Xia",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2129628882",
      "name": "Yuxuan Liang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2085303801",
      "name": "Yanwei Yu",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2105534490",
      "name": "Chenliang Li",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3178835722",
    "https://openalex.org/W2892880750",
    "https://openalex.org/W2951369132",
    "https://openalex.org/W3199763349",
    "https://openalex.org/W3045200674",
    "https://openalex.org/W3206932362",
    "https://openalex.org/W2951570486",
    "https://openalex.org/W3093242741",
    "https://openalex.org/W3035287707",
    "https://openalex.org/W3152943193",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2997261254",
    "https://openalex.org/W2942947041",
    "https://openalex.org/W2171279286",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W2783272285",
    "https://openalex.org/W3035666843",
    "https://openalex.org/W4212931205",
    "https://openalex.org/W2899457523",
    "https://openalex.org/W3177890934",
    "https://openalex.org/W2964926209",
    "https://openalex.org/W3081304929",
    "https://openalex.org/W2788728386",
    "https://openalex.org/W3093862905",
    "https://openalex.org/W3122507327",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2963604523",
    "https://openalex.org/W2937556626",
    "https://openalex.org/W3101707147",
    "https://openalex.org/W4225412853",
    "https://openalex.org/W2964296635",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W3004578093",
    "https://openalex.org/W3208850925",
    "https://openalex.org/W3207682456",
    "https://openalex.org/W4299286960",
    "https://openalex.org/W4224983022",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3155496675",
    "https://openalex.org/W3100199015",
    "https://openalex.org/W3034773362",
    "https://openalex.org/W3124675547",
    "https://openalex.org/W4226110810"
  ],
  "abstract": "Learning dynamic user preference has become an increasingly important\\ncomponent for many online platforms (e.g., video-sharing sites, e-commerce\\nsystems) to make sequential recommendations. Previous works have made many\\nefforts to model item-item transitions over user interaction sequences, based\\non various architectures, e.g., recurrent neural networks and self-attention\\nmechanism. Recently emerged graph neural networks also serve as useful backbone\\nmodels to capture item dependencies in sequential recommendation scenarios.\\nDespite their effectiveness, existing methods have far focused on item sequence\\nrepresentation with singular type of interactions, and thus are limited to\\ncapture dynamic heterogeneous relational structures between users and items\\n(e.g., page view, add-to-favorite, purchase). To tackle this challenge, we\\ndesign a Multi-Behavior Hypergraph-enhanced Transformer framework (MBHT) to\\ncapture both short-term and long-term cross-type behavior dependencies.\\nSpecifically, a multi-scale Transformer is equipped with low-rank\\nself-attention to jointly encode behavior-aware sequential patterns from\\nfine-grained and coarse-grained levels. Additionally, we incorporate the global\\nmulti-behavior dependency into the hypergraph neural architecture to capture\\nthe hierarchical long-range item correlations in a customized manner.\\nExperimental results demonstrate the superiority of our MBHT over various\\nstate-of-the-art recommendation solutions across different settings. Further\\nablation studies validate the effectiveness of our model design and benefits of\\nthe new MBHT framework. Our implementation code is released at:\\nhttps://github.com/yuh-yang/MBHT-KDD22.\\n",
  "full_text": "Multi-Behavior Hypergraph-Enhanced Transformer\nfor Sequential Recommendation\nYuhao Yang\nUniversity of Hong Kong\nHong Kong, China\nyuhao-yang@outlook.com\nChao Huangâˆ—\nUniversity of Hong Kong\nHong Kong, China\nchaohuang75@gmail.com\nLianghao Xia\nUniversity of Hong Kong\nHong Kong, China\naka_xia@foxmail.com\nYuxuan Liang\nNational University of Singapore\nSingapore, Singapore\nyuxliang@outlook.com\nYanwei Yu\nOcean University of China\nQingdao, China\nyuyanwei@ouc.edu.cn\nChenliang Li\nWuhan University\nWuhan, China\ncllee@whu.edu.cn\nABSTRACT\nLearning dynamic user preference has become an increasingly im-\nportant component for many online platforms (e.g., video-sharing\nsites, e-commerce systems) to make sequential recommendations.\nPrevious works have made many efforts to model item-item transi-\ntions over user interaction sequences, based on various architec-\ntures, e.g., recurrent neural networks and self-attention mechanism.\nRecently emerged graph neural networks also serve as useful back-\nbone models to capture item dependencies in sequential recommen-\ndation scenarios. Despite their effectiveness, existing methods have\nfar focused on item sequence representation with singular type of\ninteractions, and thus are limited to capture dynamic heterogeneous\nrelational structures between users and items (e.g., page view, add-\nto-favorite, purchase). To tackle this challenge, we design a Multi-\nBehavior Hypergraph-enhanced Transformer framework (MBHT)\nto capture both short-term and long-term cross-type behavior de-\npendencies. Specifically, a multi-scale Transformer is equipped with\nlow-rank self-attention to jointly encode behavior-aware sequential\npatterns from fine-grained and coarse-grained levels. Additionally,\nwe incorporate the global multi-behavior dependency into the hy-\npergraph neural architecture to capture the hierarchical long-range\nitem correlations in a customized manner. Experimental results\ndemonstrate the superiority of our MBHT over various state-of-\nthe-art recommendation solutions across different settings. Further\nablation studies validate the effectiveness of our model design and\nbenefits of the new MBHT framework. Our implementation code\nis released at: https://github.com/yuh-yang/MBHT-KDD22.\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\nâˆ—Chao Huang is the corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nKDDâ€™22, August 14â€“18, 2022, Washington, DC, USA\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9385-0/22/08. . . $15.00\nhttps://doi.org/10.1145/3534678.3539342\nKEYWORDS\nSequential Recommendation, Graph Neural Networks, Hypergraph\nLearning, Multi-Behavior Recommendation\nACM Reference Format:\nYuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei Yu, and Chen-\nliang Li. 2022. Multi-Behavior Hypergraph-Enhanced Transformer for Se-\nquential Recommendation. In Proceedings of the 28th ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining (KDDâ€™22), August 14â€“18,\n2022, Washington, DC, USA. ACM, New York, NY, USA, 12 pages. https:\n//doi.org/10.1145/3534678.3539342\n1 INTRODUCTION\nRecommendation models have emerged as the core components of\nmany online applications [34], such as social media platforms [36],\nvideo streaming services [10] and online retail systems [24]. Due to\nthe highly practical value of sequential behavior modeling in vari-\nous online platforms, sequential recommendation has been widely\nadopted in online platforms, with the aim of forecasting future usersâ€™\ninteracted item based on their past behavior sequences [1, 19].\nGenerally, in the sequential recommendation scenario, the sys-\ntems rely on the item sequences to model time-evolving user prefer-\nences. Upon this learning paradigm, many sequential recommender\nsystems have been proposed to encode the item dependencies based\non various neural techniques and provided stronger performance,\ne.g., recurrent neural recommendation architecture-GRU4Rec [7]\nand convolution-based sequence encoder model-Caser [ 20]. In-\nspired by the Transformer framework, self-attention has been uti-\nlized to capture the item-item pairwise correlations in SASRec [13]\nand BERT4Rec [19]. Recently, graph neural networks (GNNs) have\nshown effectiveness in sequential recommender systems. Owing\nto strength of graph convolutions, GNN-based methods (e.g., SR-\nGNN [25], GCSAN [28]) are developed to learn item transitions\nwith message passing over the constructed item graph structures.\nDespite their effectiveness, most of existing works have thus far\nfocused on sequential behavior pattern encoding for singular type\nof interactions (e.g., clicks or purchases), without taking multi-typed\nuser-item relationships into consideration. However, in practical\nonline platforms, user behaviors often exhibit both time-dependent\nand multi-typed, involving different types of user-item interactions,\ne.g.,, page view, add-to-favorite, purchase. As illustrated in Figure 1,\ncustomers may click and add their interested products to their fa-\nvorite lists before purchasing them. Additionally, we also visualize\narXiv:2207.05584v2  [cs.IR]  20 Sep 2022\nKDDâ€™22, August 14â€“18, 2022, Washington, DC, USA Yuhao Yang et al.\nTime\nView\nFavorite\nCart\nPurchase\nBERT4RecBERT4RecMBHT\n(a)\n(b)\nFigure 1: (a) Illustration example of sequential recommen-\ndation with multi-behavior dynamics. (b) Learned behavior-\naware dependency weights for short-term item correlations\namong neighboring {[4],[5],[6]}and long-range dependen-\ncies among {[2],[13],[15]}by BERT4Rec and our MBHT.\nthe learned dependency weights by the baseline BERT4Rec and our\nMBHT method to capture behavior-aware short-term item correla-\ntions (among neighboring items {[4], [5], [6]}) and long-term item-\nwise dependencies (among {[2], [13], [15]}). We can observe that the\nglobal multi-behavior dependencies can be better distilled by our\nMBHT as compared to BERT4Rec. Hence, effectively enhancing\nthe user preference learning with the exploration of heterogeneous\nuser-item interactions in a dynamic environment, is also the key to\nmaking accurate sequential recommendations. Nevertheless, this\ntask is not trivial due to the following challenges:\nâ€¢Dynamic Behavior-aware Item Transitions . How to explic-\nitly capture the dynamic behavior-aware item transitions with\nmulti-scale temporal dynamics remains a challenge. There exist\ndifferent periodic behavior patterns (e.g., daily, weekly, monthly)\nfor different categories of items (e.g., daily necessities, seasonal\nclothing) [9, 17]. Therefore, it is necessary to explicitly capture\nmulti-scale sequential effects of behavior-aware item transitional\npatterns from fine-grained to coarse-grained temporal levels.\nâ€¢Personalized Global Multi-Behavior Dependencies. The im-\nplicit dependencies across different types of behaviors over time\nvary from user to user. For example, due to the personalized and\ndiverse user interaction preferences, some people would like to\nadd products to their favorite list if they show interested in the\nitems. Others may prefer to generate their favorite item list with\nproducts they are very likely to buy. That is to say, for different\nusers, multi-behaviors have various time-aware dependencies\non their interests. Moreover, multi-behavior item-wise depen-\ndencies are beyond pairwise relations and may exhibit triadic or\nevent higher-order. Hence, the designed model requires a tailored\nmodeling of diverse usersâ€™ multi-behavior dependencies with a\ndynamic multi-order relation learning paradigm.\nContribution. This work proposes a Multi-Behavior Hypergraph-\nenhanced Transformer (MBHT) to capture dynamic item dependen-\ncies with behavior type awareness. Our developed MBHT frame-\nwork consitens of two key learning paradigms to address the afore-\nmentioned challenges correspondingly. (1) Behavior-aware Se-\nquential Patterns. We propose a multi-scale Transformer module\nto comprehensively encode the multi-grained sequential patterns\nfrom fine-grained level to coarse-grained level for behavior-aware\nitem transitions. To improve the efficiency of our sequential pat-\ntern encoder, we equip our multi-scale Transformer with the self-\nattentive projection based on low-rank factorization. To aggregate\nscale-specific temporal effects, a multi-scale behavior-aware pattern\nfusion is introduced to integrate multi-grained item transitional\nsignals into a common latent representation space. (2)Global Mod-\neling of Diverse Multi-Behavior Dependencies . We generalize\nthe modeling of global and time-dependent cross-type behavior\ndependencies with a multi-behavior hypergraph learning paradigm.\nWe construct the item hypergraph structures by unifying the la-\ntent item-wise semantic relateness and item-specific multi-behavior\ncorrelations. Technically, to capture the global item semantics, we\ndesign the item semantic dependence encoder with metric learn-\ning. Upon the hypergraph structures, we design the multi-behavior\nhyperedge-based message passing schema for refining item embed-\ndings, which encourages the long-range dependency learning of\ndifferent types of user-item relationships. Empirically, MBHT is\nable to provide better performance than state-of-the-art methods,\ne.g., BERT4Rec [19], HyperRec [21], SURGE [1], MB-GMN [27].\nThe main contributions are summarized as follows:\nâ€¢This work proposes a new framework named MBHT for sequen-\ntial recommendation, which uncovers the underlying dynamic\nand multi-behavior user-item interaction patterns.\nâ€¢To model multi-grained item transitions with the behavior type\nawareness, we design a multi-scale Transformer which is empow-\nered with the low-rank and multi-scale self-attention projection,\nto maintain the evolving relation-aware user interaction patterns.\nâ€¢In addition, to capture the diverse and long-range multi-behavior\nitem dependencies, we propose a multi-behavior hypergraph\nlearning paradigm to distill the item-specific multi-behavior cor-\nrelations with global and customized sequential context injection.\nâ€¢We perform extensive experiments on three publicly available\ndatasets, to validate the superiority of our proposed MBHT over\nvarious state-of-the-art recommender systems. Model ablation\nand case studies further show the benefits of our model.\n2 PROBLEM FORMULATION\nIn this section, we introduce the primary knowledge and formulates\nthe task of multi-behavior sequential recommendation.\nBehavior-aware Interaction Sequence . Suppose we have a se-\nquential recommender system with a set of ğ¼ users ğ‘¢ğ‘– âˆˆU where\n|U|= ğ¼. For an individual user ğ‘¢ğ‘–, we define the behavior-aware\ninteraction sequence ğ‘†ğ‘– = [(ğ‘£ğ‘–,1,ğ‘ğ‘–,1),..., (ğ‘£ğ‘–,ğ‘—,ğ‘ğ‘–,ğ‘—),..., (ğ‘£ğ‘–,ğ½,ğ‘ğ‘–,ğ½ )]\nwith the consideration of item-specific interaction type, where ğ½\ndenotes the length of temporally-ordered item sequence. Here, we\ndefine ğ‘ğ‘–,ğ‘— to represent the behavior type of the interaction between\nuser ğ‘¢ğ‘– and ğ‘—-th item ğ‘£ğ‘— in ğ‘†ğ‘–, such as page view, add-to-favorite,\nadd-to-cart and purchase in e-commerce platforms.\nTask Formulation. In our multi-behavior sequential recommender\nsystem, different types of interaction behaviors are partitioned into\ntarget behaviors and auxiliary behaviors . Specifically, we regard\nthe interaction with the behavior type we aim to predict as target\nbehaviors. Other types of user behaviors are defined as auxiliary be-\nhaviors to provide various behaviour contextual information about\nusersâ€™ diverse preference, so as to assist the recommendation task\nMulti-Behavior Hypergraph-Enhanced Transformer\nfor Sequential Recommendation KDDâ€™22, August 14â€“18, 2022, Washington, DC, USA\non the target type of user-item interactions. For example, in many\nonline retail platforms, purchase behaviors can be considered as\nthe prediction targets, due to their highly relevance to the Gross\nMerchandise Volume (GMV) in online retailing to indicate the total\nsales value for merchandise [9, 24]. We formally present our studied\nsequential recommendation problem as follows:\nâ€¢Input: The behavior-aware interaction sequenceğ‘†ğ‘– = [(ğ‘£ğ‘–,1,ğ‘ğ‘–,1)\n,..., (ğ‘£ğ‘–,ğ‘—,ğ‘ğ‘–,ğ‘—),..., (ğ‘£ğ‘–,ğ½,ğ‘ğ‘–,ğ½ )]of each user ğ‘¢ğ‘– âˆˆU.\nâ€¢Output: The learning function that estimates the probability of\nuser ğ‘¢ğ‘– will interact with the item ğ‘£ğ½+1 with the target behavior\ntype at the future (ğ½ +1)-th time step.\n3 METHODOLOGY\nFigure 2 presents the overall architecture of our proposed MBHT\nmodel which consists of three key modules: i) Multi-scale model-\ning of behavior-aware transitional patterns of user preference; ii)\nGlobal learning of multi-behavior dependencies of time-aware user\ninteractions; iii) Cross-view aggregation with the encoded repre-\nsentations of sequential behavior-aware transitional patterns and\nhypergraph-enhanced multi-behavior dependencies.\n3.1 Multi-Scale Modeling of Behavior-aware\nSequential Patterns\nIn this section, we present the technical details of our MBHT in cap-\nturing the behavior-aware user interest with multi-scale dynamics.\n3.1.1 Behavior-aware Context Embedding Layer . To inject\nthe behavior-aware interaction context into our sequential learning\nframework, we design the behavior-aware context embedding layer\nto jointly encode the individual item information and the corre-\nsponding interaction behavior contextual signal. Towards this end,\ngiven an item ğ‘£ğ‘—, we offer its behavior-aware latent representation\nhğ‘— âˆˆRğ‘‘ with the following operation:\nhğ‘— = eğ‘— âŠ•pğ‘— âŠ•bğ‘— (1)\nwhere eğ‘— âˆˆRğ‘‘ represents the initialized item embedding.bğ‘— âˆˆRğ‘‘ is\nthe behavior type embedding corresponding to the interaction type\n(e.g., page view, add-to-favorite) between user ğ‘¢ğ‘– and item ğ‘£ğ‘—. Here,\npğ‘— âˆˆRğ‘‘ represents the learnable positional embedding of item ğ‘£ğ‘—\nwhich differentiates the temporally-ordered positional information\nof different interacted items. After this context embedding layer,\nwe can obtain the item representation matrix H âˆˆRğ½Ã—ğ‘‘ for the\nbehavior-aware interacted item sequence ğ‘†ğ‘– of user ğ‘¢ğ‘–.\n3.1.2 Multi-Scale Transformer Layer . In practical recommen-\ndation scenarios, user-item interaction preference may exhibit\nmulti-scale transitional patterns over time. For instance, users often\npurchase different categories of products ( e.g., daily necessities,\nclothes, digital devices) with different periodic trends, such as daily\nor weekly routines [10]. To tackle this challenge, we design a multi-\nscale sequential preference encoder based on the Transformer ar-\nchitecture to capture the multi-grained behavior dynamics in the\nbehavior-aware interaction sequence ğ‘†ğ‘– of users (ğ‘¢ğ‘– âˆˆU).\nLow-Rank Self-Attention Module . Transformer has shown its\neffectiveness in modeling relational data across various domains\n(e.g., language [31], vision [15]). In Transformer framework, self-\nattention serves as the key component to perform the relevance-\naware information aggregation among attentive data points (e.g.,\nwords, pixels, items). However, the high computational cost (i.e.,\nquadratic time complexity) of the self-attention mechanism limits\nthe model scalability in practical settings [ 14]. Motivated by the\ndesign of Transformer structure in [22], we design a low-rank-based\nself-attention layer without the quadratic attentive operation, to\napproximate linear model complexity.\nIn particular, different from the original scaled dot-product atten-\ntion for pairwise relation encoding, we generate multiple smaller\nattention operations to approximate the original attention with\nlow-rank factorization. We first define two trainable projection\nmatrices ğ‘¬ âˆˆR\nğ½\nğ¶Ã—ğ½ and ğ‘­ âˆˆR\nğ½\nğ¶Ã—ğ½ to perform the low-rank em-\nbedding transformation. Here, ğ¶ denotes the low-rank scale and\nğ½\nğ¶ represents the number of low-rank latent representation spaces\nover the input behavior-aware interaction sequence ğ‘†ğ‘–. Formally,\nwe represent our low-rank self-attention as follows:\nbH = softmax(H Â·ğ‘Šğ‘„(E Â·H Â·ğ‘Šğ¾)T\nâˆš\nğ‘‘\n)Â· F Â·H Â·ğ‘Šğ‘‰ (2)\nwhere ğ‘Šğ‘„, ğ‘Šğ¾, ğ‘Šğ‘‰ are learnable transformation matrices for em-\nbedding projection. In our low-rank self-attention module, ğ‘¬ and ğ‘­\nare utilized to project the (Rğ½Ã—ğ‘‘)-dimensional key and value trans-\nformed representationsHÂ·ğ‘Šğ¾ and HÂ·ğ‘Šğ‘‰ into (R\nğ½\nğ¶Ã—ğ‘‘)-dimensional\nlatent low-rank embeddings bH âˆˆR\nğ½\nğ¶Ã—ğ‘‘. In summary, with the low-\nrank factor decomposition over the original attention operations,\nwe calculate the context mapping matrix M = HÂ·ğ‘Šğ‘„(HÂ·ğ‘Šğ¾)ğ‘‡\nâˆš\nğ‘‘ with\nthe dimension ofRğ½Ã—ğ½\nğ¶ as compared to the original dimensionRğ½Ã—ğ½\nin the vanilla self-attention mechanism. By doing so, the computa-\ntional cost of our behavior sequence encoder can be significantly\nreduced from the ğ‘‚(ğ½Ã—ğ½)to ğ‘‚(ğ½Ã—ğ½\nğ¶)given that the low-rank pro-\njected dimension ğ½/ğ¶ is often much smaller than ğ½, i.e., ğ½/ğ¶ â‰ª ğ½.\nMulti-Scale Behavior Dynamics. To endow our MBHT model\nwith the effective learning of multi-scale behaviour transitional\npatterns, we propose to enhance our low-rank-based transformer\nwith a hierarchical structure, so as to capture granularity-specific\nbehavior dynamics. To be specific, we develop a granularity-aware\naggregator to generate granularity-specific representationgğ‘ which\npreserves the short-term behavior dynamic. Here, we define ğ‘ as\nthe length of sub-sequence for a certain granularity. We formally\npresent our granularity-aware emebdding generation with the ag-\ngregated representation ğšªğ‘ âˆˆR\nğ½\nğ‘Ã—ğ‘‘ and ğœ¸ âˆˆRğ‘‘ as follows:\nğšªğ‘ = {ğœ¸1,..., ğœ¸ ğ½\nğ‘\n}= [ğœ‚(h1,..., hğ‘); ...;ğœ‚(hğ½âˆ’ğ‘+1,..., hğ½)] (3)\nwhere ğœ‚(Â·)represents the aggregator to capture the short-term\nbehavior-aware dynamics. Here, we utilize the mean pooling to per-\nform the embedding aggregation. After that, we feed the granularity-\naware behavior representations into a self-attention layer for en-\ncoding granularity-specific behavior pattern as shown below:\nHğ‘ = softmax(\nğšªğ‘ Â·ğ‘Šğ‘„\nğ‘ (ğšªğ‘ Â·ğ‘Šğ¾ğ‘ )T\nâˆš\nğ‘‘\n)Â· ğšªğ‘ Â·ğ‘Šğ‘‰\nğ‘ (4)\nKDDâ€™22, August 14â€“18, 2022, Washington, DC, USA Yuhao Yang et al.\nBehavior-awareItemSequence\nitemtype\nâ€¦\nLow-rankProjection\ncontentmappingmatrix\nTsoftmax\n â€¦ â€¦ â€¦ â€¦â€¦ â€¦ â€¦ â€¦\nMulti-scale Behavior Dynamics\nMulti-ScalePattern Fusion\nItem-wise Semantic Hypergraph\nItem-wise Multi-Behavior Hypergraph\nHyperedge Embeddings\nHypergraph Convolution\n(a) (b) (c)\nFigure 2: MBHTâ€™s model flow. (a) We inject the behavior-aware interaction context into item embeddings h ğ‘— = eğ‘— âŠ•pğ‘— âŠ•bğ‘—. (b)\nMulti-scale transformer architecture to capture behavior-aware transitional patterns via low-rank self-attention and multi-\nscale sequence aggregation. Scale-specific behavior patterns are fused through the fusion function eH = ğ‘“(bH âˆ¥Hğ‘1 âˆ¥Hğ‘2 ). (c)\nWe capture the global and personalized multi-behavior dependency learning with our hypergraph neural architecture over G.\nHğ‘ âˆˆ R\nğ½\nğ‘Ã—ğ‘‘ encodes the short-term transitional patterns over\ndifferent item sub-sequences. In our MBHT framework, we design\na hierarchical Transformer network with two different scale settings\nğ‘1 and ğ‘2. Accordingly, our multi-scale Transformer can produce\nthree scale-specific sequential behavior embeddings bH âˆˆRğ½Ã—ğ‘‘,\nHğ‘1 âˆˆR\nğ½\nğ‘1 Ã—ğ‘‘, Hğ‘2 âˆˆR\nğ½\nğ‘2 Ã—ğ‘‘.\n3.1.3 Multi-Scale Behaviour Pattern Fusion. To integrate the\nmulti-scale dynamic behavior patterns into a common latent rep-\nresentation space, we propose to aggregate the above encoded\nscale-specific embeddings with a fusion layer presented as follows:\neH = ğ‘“(bH âˆ¥Hğ‘1 âˆ¥Hğ‘2 ) (5)\nHere ğ‘“(Â·)represents the projection function which transforms\nR(ğ½\nğ¶+ğ½\nğ‘1 +ğ½\nğ‘2 )Ã—ğ‘‘ dimensional embeddings into Rğ½Ã—ğ‘‘ dimensional\nrepresentations corresponding to different items (ğ‘£ğ‘— âˆˆğ‘†ğ‘–) in the\nbehavior-aware interaction sequence ğ‘†ğ‘– of user ğ‘¢ğ‘–. Here, âˆ¥denotes\nthe concatenation operation over different embedding vectors.\nMulti-Head-Enhanced Representation Spaces. In this part, we\npropose to endow our behavior-aware item sequence encoder with\nthe capability of jointly attending multi-dimensional interaction\nsemantics. In particular, our multi-head sequential pattern encoder\nprojects the H into ğ‘ latent representation spaces and performs\nhead-specific attentive operations in parallel.\neH = (head1 âˆ¥head2 âˆ¥Â·Â·Â·âˆ¥ headğ‘)ğ‘¾ğ· (6)\nheadğ‘› = ğ‘“(bHğ‘› âˆ¥Hğ‘1\nğ‘› âˆ¥Hğ‘2\nğ‘› )\nwhere bHğ‘›,Hğ‘1\nğ‘› and Hğ‘2\nğ‘› are computed with head-specific projec-\ntion matrices ğ‘Šğ‘„\nğ‘› ,ğ‘Šğ¾ğ‘› ,ğ‘Šğ‘‰ğ‘› âˆˆRğ‘‘Ã—ğ‘‘/ğ‘, and ğ‘¾ğ· âˆˆRğ‘‘Ã—ğ‘‘ is the out-\nput transformation matrix. The multiple attention heads allows our\nmulti-scale Transformer architecture to encode multi-dimensional\ndependencies among items in ğ‘†ğ‘–.\nNon-linearity Injection with Feed-forward Module. In our\nmulti-scale Transformer, we use the point-wise feed-forward net-\nwork to inject non-linearities into the new generated representa-\ntions. The non-linear transformation layer is formally represented:\nPFFN(eH(ğ‘™))= [FFN(Ëœh(ğ‘™)\n1 )T,Â·Â·Â· ,FFN(Ëœh(ğ‘™)\nğ‘¡ )T] (7)\nFFN(x)= GELU(xW(ğ‘™)\n1 +b(ğ‘™)\n1 )W(ğ‘™)\n2 +b(ğ‘™)\n2 ,\nIn our feed-forward module, we adopt two layers of non-linear\ntransformation with the integration of intermediate non-linear acti-\nvation GELU(Â·). In addition, W(ğ‘™)\n1 âˆˆRğ‘‘Ã—ğ‘‘â„,W(ğ‘™)\n2 âˆˆRğ‘‘â„Ã—ğ‘‘,b(ğ‘™)\n1 âˆˆ\nRğ‘‘,b(ğ‘™)\n2 âˆˆRğ‘‘ are learnable parameters of projection matrices and\nbias terms. Here, ğ‘™ denotes the ğ‘™-th multi-scale Transformer layer.\n3.2 Customized Hypergraph Learning of\nGlobal Multi-Behavior Dependencies\nIn our MBHT framework, we aim to incorporate long-range multi-\nbehavior dependency into the learning paradigm of evolving user\ninterests. However, it is non-trivial to effectively capture the person-\nalized long-range multi-behavior dependencies. To achieve our goal,\nwe propose to tackle two key challenges in our learning paradigm:\nâ€¢i) Multi-Order Behavior-wise Dependency . The item-wise\nmulti-behavior dependencies are no longer dyadic with the con-\nsideration of comprehensive relationships among different types\nof user behaviors. For example, when deciding to recommend\na specific item to users for their potential purchase preference,\nit would be useful to explore past multi-behavior interactions\n(e.g., page view, add-to-favorite) between users and this item.\nCustomers are more likely to add their interested products into\ntheir favorite item list before making final purchases.\nâ€¢ii) Personalized Multi-Behavior Interaction Patterns. Multi-\nbehavior patterns may vary by users with different correlations\nacross multi-typed user-item interactions. In real-life e-commerce\nsystems, some users like to add many items to their favorite list\nor cart, if they are interested in, but only a few of them will\nbe purchased later. In contrast, another group of users only tag\ntheir interested products as favorite only if they show strong\nwillingness to buy them. Hence, such complex and personalized\nmulti-behavior patterns require our model to preserve the diverse\ncross-type behaviour dependencies.\nTo address the above challenges, we build our global multi-\nbehavior dependency encoder upon the hypergraph neural architec-\nture. Inspired by the flexibility of hypergraphs in connecting multi-\nple nodes through a single edge [2, 26], we leverage the hyperedge\nstructure to capture the tetradic or higher-order multi-behavior\ndependencies over time. Additionally, given the behavior-aware\ninteraction sequence of different users, we construct different hy-\npergraph structures over the sequence ğ‘†ğ‘– (ğ‘¢ğ‘– âˆˆU), with the aim of\nencoding the multi-behavior dependency in a customized manner.\nMulti-Behavior Hypergraph-Enhanced Transformer\nfor Sequential Recommendation KDDâ€™22, August 14â€“18, 2022, Washington, DC, USA\n3.2.1 Item-wise Hypergraph Construction. In our hypergraph\nframework, we generate two types of item-wise hyperedge connec-\ntions corresponding to i) long-range semantic correlations among\nitems; ii) item-specific multi-behavior dependencies across time.\nItem Semantic Dependency Encoding with Metric Learning .\nTo encode the time-evolving item semantics and the underlying\nlong-range item dependencies based on the same user interest (e.g.,\nfood, outdoor activities), we introduce an item semantic encoder\nbased on a metric learning framework. Specifically, we design the\nlearnable metric ^ğ›½ğ‘›\nğ‘—,ğ‘—â€² between items with a multi-channel weight\nfunction ğœ(Â·)presented as follows:\nğ›½ğ‘—,ğ‘—â€² = 1\nğ‘\nğ‘âˆ‘ï¸\nğ‘›=1\n^ğ›½ğ‘›\nğ‘—,ğ‘—â€²; vğ‘— = eğ‘— âŠ•bğ‘— (8)\n^ğ›½ğ‘›\nğ‘—,ğ‘—â€² = ğœ(ğ’˜ğ‘› âŠ™vğ‘—,ğ’˜ğ‘› âŠ™vğ‘—â€²) (9)\nwhere ^ğ›½ğ‘›\nğ‘—,ğ‘—â€² represents the learnable channel-specific dependency\nweight between item ğ‘£ğ‘— and ğ‘£ğ‘—â€². We define the weight functionğœ(Â·)\nas the cosine similarity estimation based on the trainable ğ’˜ğ‘› of\nğ‘›-th representation channel. (ğ’˜ğ‘› âŠ™vğ‘—) represents the embedding\nprojection operation. In our item-wise semantic dependency, we\nperform metric learning under ğ‘ representation channels (indexed\nby ğ‘›). The mean pooling operation is applied to all learned channel-\nspecific item semantic dependency scores (e.g., ^ğ›½ğ‘›\nğ‘—,ğ‘—â€²), to obtain the\nfinal relevance ğ›½ğ‘—,ğ‘—â€² between item ğ‘£ğ‘— and ğ‘£ğ‘—â€².\nItem-wise Semantic Dependency Hypergraph . With the en-\ncoded semantic dependencies among different items, we generate\nthe item-wise semantic hypergraph by simultaneously connecting\nmultiple highly dependent items with hyperedges. In particular,\nwe construct a set of hyperedges Eğ‘, where |Eğ‘|corresponds to\nthe number of unique items in sequence ğ‘†ğ‘–. In the hypergraph\nGğ‘ of item-wise semantic dependencies, each unique item will be\nassigned with a hyperedge in ğœ– âˆˆEğ‘ to connect top-ğ‘˜ semantic\ndependent items according to the learned item-item semantic de-\npendency score ğ›½ğ‘—,ğ‘—â€²(encoded from the metric learning component).\nğ´ğ‘— represents the set of top-ğ‘˜ semantic correlated items of a spe-\ncific item ğ‘£ğ‘—. We define the connection matrix between items and\nhyperedges as Mğ‘ âˆˆRğ½Ã—|Eğ‘|in which each entry ğ‘šğ‘(ğ‘£ğ‘—,ğœ–ğ‘—â€²)is:\nğ‘šğ‘(ğ‘£ğ‘—,ğœ–ğ‘—â€²)=\n(\nğ›½ğ‘—,ğ‘—â€² ğ‘£ğ‘—â€² âˆˆğ´ğ‘—;\n0 ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ ; (10)\nwhere ğœ–ğ‘—â€² denotes the hyperedge which is assigned to item ğ‘£ğ‘—â€².\nItem-wise Multi-Behavior Dependency Hypergraph . To cap-\nture the personalized item-wise multi-behavior dependency in a\ntime-aware environment, we further generate a hypergraph struc-\nture Gğ‘ based on the observed multi-typed interactions (e.g., page\nview, add-to-cart) between userğ‘¢ğ‘– and a specific item ğ‘£ğ‘— at differ-\nent timestamps. Here, we define Eğ‘ to represent the set of items\nwhich have multi-typed interactions with user ğ‘¢ğ‘–. In hypergraph\nGğ‘, the number of hyperedges is equal to |Eğ‘|. Given that users\nhave diverse multi-behaviour patterns with their interacted items,\nthe constructed multi-behavior dependency hypergraphs vary by\nusers. To be specific, we generate item-hyperedge connection ma-\ntrix Mğ‘ âˆˆRğ½Ã—|Eğ‘|(ğ‘šğ‘ âˆˆMğ‘) for hypergraph Gğ‘ as follow:\nğ‘šğ‘(ğ‘£ğ‘\nğ‘—,ğœ–ğ‘â€²\nğ‘— )=\n(\n1 ğ‘£ğ‘\nğ‘— âˆˆEğ‘\nğ‘—;\n0 ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ ;\n(11)\nğ‘£ğ‘\nğ‘— represents that item ğ‘£ğ‘— is interacted with user ğ‘¢ğ‘– under the ğ‘-th\nbehavior type. Eğ‘\nğ‘— denotes the set of multi-typed ğ‘¢ğ‘–-ğ‘£ğ‘— interactions.\nWe further integrate our constructed hypergraph structures\nGğ‘ and Gğ‘ by concatenating connection matrices Mğ‘ and Mğ‘\nalong with the column side. As such, the integrated hypergraph\nGis constructed with the concatenated connection matrix Mâˆˆ\nRğ½Ã—(|Eğ‘|+|Eğ‘|), i.e., M= Mğ‘ âˆ¥Mğ‘. Different behavior-aware in-\nteraction sequences result in different hypergraph structures for\ndifferent users, which allow our MBHT model to encode the per-\nsonalized multi-behavior dependent patterns in a customized way.\n3.2.2 Hypergraph Convolution Module. In this module, we\nintroduce our hypergraph message passing paradigm with the\nconvolutional layer, to capture the global multi-behavior depen-\ndencies over time. The hypergraph convolutional layer generally\ninvolves two-stage information passing [2], i.e., node-hyperedge\nand hyperedge-node embedding propagation along with the hy-\npergraph connection matrix Mfor refining item representations.\nParticularly, we design our hypergraph convolutional layer as:\nX(ğ‘™+1)= Dâˆ’1\nğ‘£ Â·M Â·Dâˆ’1\nğ‘’ Â·MT Â·X(ğ‘™) (12)\nwhere X(ğ‘™) represents the item embeddings encoded from the ğ‘™-\nth layer of hypergraph convolution. Furthermore, Dğ‘£ and Dğ‘’ are\ndiagonal matrices for normalization based on vertex and edge de-\ngrees, respectively. Note that the two-stage message passing by\nM Â·MT takes ğ‘‚((|Eğ‘|+|E ğ‘|)Ã— ğ½2)calculations, which is quite\ntime-consuming. Inspired by the design in [ 33], we calculate a\nmatrix Mâ€²by leveraging pre-calculated ğ›½ğ‘—,ğ‘—â€² to obtain a close ap-\nproximation representation ofMÂ·MT and thus boost the inference.\nThe detailed process can be found in the supplementary material.\nWe also remove the non-linear projection following [ 6] to sim-\nplify the message passing process. Each item embedding x(0)in\nX(0)is initialized with the behavior-aware self-gating operation\nas: x(0)= (vğ‘— âŠ•bğ‘—)âŠ™ sigmoid((vğ‘— âŠ•bğ‘—)Â· w +r).\n3.3 Cross-View Aggregation\nIn the forecasting layer of MBHT framework, we propose to fuse\nthe learned item representations from different views: 1) multi-scale\nbehavior-aware sequential patterns with Transformer architecture;\n2) personalized global multi-behavior dependencies with Hyper-\ngraph framework. To enable this cross-view aggregation in an\nadaptive way, we develop an attention layer to learn explicit impor-\ntance for view-specific item embeddings. Formally, the aggregation\nprocedure is presented as follows:\nğ›¼ğ‘– = Attn(ğ’†ğ‘–)= exp(ğ’‚T Â·ğ‘¾ğ‘ğ’†ğ‘–)Ã\nğ‘– exp(ğ’‚T Â·ğ‘¾ğ‘ğ’†ğ‘–) (13)\nğ’†ğ‘– âˆˆ{Ëœhğ‘–,Ëœxğ‘–}; gğ‘– = ğ›¼1 Â·Ëœhğ‘– âŠ•ğ›¼2 Â·Ëœxğ‘–\nwhere Ëœhğ‘– and Ëœxğ‘– are embeddings from the two views separately\nfor item at the ğ‘–-th position, and ğ’‚ âˆˆRğ‘‘, ğ‘¾ğ‘ âˆˆRğ‘‘Ã—ğ‘‘ are trainable\nKDDâ€™22, August 14â€“18, 2022, Washington, DC, USA Yuhao Yang et al.\nTable 1: Statistical information of experimented datasets.\nStats. Taobao Retailrocket IJCAI\n# Users 147, 892 11, 649 200, 000\n# Items 99, 038 36, 223 808, 354\n# Interactions 7, 092, 362 87, 822 13, 072, 940\n# Average Length 48.23 14.55 78.58\n# Density 5Ã—10âˆ’6 1Ã—10âˆ’6 7Ã—10âˆ’7\n# Behavior Types[buy, cart, fav, pv][buy, cart, pv][buy, cart, fav, pv]\nparameters. Here, to eliminate the over-smooth effect of graph\nconvolution, Ëœxğ‘– is the average ofx(ğ‘™)across all convolutional layers.\nFinally, the probability of ğ‘–-th item in the sequence being item ğ‘£ğ‘— is\nestimated as: ^ğ‘¦ğ‘–,ğ‘— = gT\nğ‘– vğ‘—, where vğ‘— represents item ğ‘£ğ‘—â€™s embedding.\n3.4 Model Learning And Analysis\nTo fit our multi-behavior sequential recommendation scenario, we\nutilize the Cloze task [12, 19] as our training objective to model the\nbidirectional information of item sequence. We describe our Cloze\ntask settings as follows: Considering the multi-behavior sequential\nrecommender systems, we mask all items in the sequence with\nthe target behavior type ( e.g., purchase). To avoid the label leak\nissue, we replace the masked items as well as the corresponding\nbehavior type embeddings with the special token[mask], and leave\nout masked items in the hypergraph construction in Section 3.2.1.\nInstead, for masked items, we generate its hypergraph embedding\nxğ‘– in Equation 13 by adopting sliding-window average pooling\nfunction over hypergraph embeddings of the contextual neighbors\nsurrounding the mask position (ğ‘šâˆ’ğ‘1,ğ‘š +ğ‘2). The details are\npresented in Algorithm 1 in the supplementary material. Hence,\nour model makes prediction on masked items based on the encoded\nsurrounding context embeddings in the behavior interaction se-\nquence. Given the probability estimation function ^ğ‘¦ğ‘–,ğ‘— = gT\nğ‘– vğ‘—, we\ndefine our optimized objective loss with Cross-Entropy as below:\nL= 1\n|ğ‘‡|\nâˆ‘ï¸\nğ‘¡âˆˆğ‘‡,ğ‘šâˆˆğ‘€\nâˆ’log( exp ^ğ‘¦ğ‘š,ğ‘¡Ã\nğ‘—âˆˆğ‘‰ exp ^ğ‘¦ğ‘š,ğ‘—\n) (14)\nwhere ğ‘‡ is the set of ground-truth ids for masked items in each\nbatch, ğ‘€ is the set of masked positions corresponding to ğ‘‡, and ğ‘‰\nis the item set. The time complexity is analyzed in supplementary\nmaterial.\n4 EXPERIMENTS\nThis section aims to answer the following research questions:\nâ€¢RQ1: How does our MBHT perform as compared to various state-\nof-the-art recommendation methods with different settings?\nâ€¢RQ2: How effective are the key modules (e.g., multi-scale atten-\ntion encoder, multi-behavior hypergraph learning) in MBHT?\nâ€¢RQ3: How does MBHT perform to alleviate the data scarcity\nissue of item sequences when competing with baselines?\nâ€¢RQ4: How do different hyperparameters affect the model perfor-\nmance? (Evaluation results are presented in Appendix A.4).\n4.1 Experimental Settings\n4.1.1 Datasets. We utilize three recommendation datasets col-\nlected from real-world scenarios. i)Taobao. This dataset is collected\nfrom Taobao which is one of the largest e-commerce platforms in\nChina. Four types of user-item interactions are included in this\ndataset, i.e., target behaviors-purchase; auxiliary behaviors-add-to-\nfavorites, add-to-cart, page view . ii) Retailrocket. This dataset is\ngenerated from an online shopping site-Retailrocket over 4 months,\nto record three types of user behaviors,i.e., target behaviors-purchase;\nauxiliary behaviors-page view & add-to-cart . iii) IJCAI. This dataset\nis released by IJCAI Contest 2015 for the task of repeat buyers pre-\ndiction. It shares the same types of interaction behaviors with the\nTaobao dataset. The detailed statistical information of these exper-\nimented datasets are summarized in Table 1. Note that different\ndatasets vary by average sequence length and user-item interaction\ndensity, which provides diverse evaluation settings.\n4.1.2 Evaluation Protocols. In our experiments, closely follow-\ning the settings in [19, 20], we adopt the leave-one-out strategy for\nperformance evaluation. For each user, we regard the temporally-\nordered last purchase as the test samples, and the previous ones\nas validation samples. Additionally, we pair each positive sample\nwith 100 negative instances based on item popularity [ 19]. We\nutilize three evaluation metrics: Hit Ratio (HR@N), Normalized Dis-\ncounted Cumulative Gain (NDCG@N) and Mean Reciprocal Rank\n(MRR) [21, 25, 30]. Note that larger HR, NDCG and MRR scores\nindicate better recommendation performance.\n4.1.3 Baselines. We compare our MBHT with a variety of rec-\nommendation baselines to validate the performance superiority.\nGeneral Sequential Recommendation Methods.\nâ€¢GRU4Rec [7]. It utilizes the gated recurrent unit as sequence\nencoder to learn dynamic preference with a ranking-based loss.\nâ€¢SASRec [13]. The self-attention mechanism is leveraged in this\nmethod to encode the item-wise sequential correlations.\nâ€¢Caser [20]. This method integrates the convolutional neural\nlayers from both vertical and horizontal views to encode time-\nevolving user preference of item sequence.\nâ€¢HPMN [17]. It employs a hierarchically structured periodic mem-\nory network to model multi-scale transitional information of user\nsequential behaviors. The incremental updating mechanism is\nintroduced to retain behaviour patterns over time.\nâ€¢BERT4Rec [19]. It uses a bidirectional encoder for modeling se-\nquential information with Transformer. The model is optimized\nwith the Cloze objective, and has produced state-of-the-art per-\nformance among sequence learning-based baselines.\nGraph-based Sequential Recommender Systems.\nâ€¢SR-GNN [25]. It generates graph structures based on item-item\ntransitional relations in sequences, and conducts graph-based\nmessage passing to capture local and global user interests.\nâ€¢GCSAN [28]. It empowers the self-attention mechanism with\nwith a front-mounted GNN structure. The attentive aggregation\nis performed over the encoded graph embeddings.\nâ€¢HyperRec [21]. It designs sequential hypergraphs to capture\nevolving usersâ€™ interests and regards users as hyperedges to con-\nnect interacted items, so as to model dynamic user preferences.\nâ€¢SURGE [1]. It adopts metric learning to build personalized graphs\nand uses hierarchical attention to capture multi-dimensional user\ninterests in the graph.\nMulti-Behavior Recommendation Models.\nâ€¢BERT4Rec-MB [19]. We enhance the BERT4Rec method to han-\ndle the dynamic multi-behavior context by injecting behavior\ntype representations into the input embeddings for self-attention.\nMulti-Behavior Hypergraph-Enhanced Transformer\nfor Sequential Recommendation KDDâ€™22, August 14â€“18, 2022, Washington, DC, USA\nTable 2: The performance of our method and the best performed baseline are presented with bold and underlined, respectively.\nSuperscript âˆ—indicates the significant improvement between our MBHT and the best performed baseline with ğ‘value < 0.01.\nModel Taobao Retailrocket IJCAI\nHR@5 NDCG@5 HR@10 NDCG@10 MRRHR@5 NDCG@5 HR@10 NDCG@10 MRRHR@5 NDCG@5 HR@10 NDCG@10 MRR\nGeneral Sequential Recommendation Methods\nCaser 0.082 0.058 0.123 0.071 0.070 0.632 0.539 0.754 0.578 0.535 0.134 0.092 0.167 0.104 0.109\nHPMN 0.162 0.130 0.219 0.141 0.139 0.664 0.633 0.711 0.587 0.602 0.144 0.085 0.197 0.124 0.123\nGRU4Rec 0.147 0.105 0.209 0.125 0.118 0.640 0.575 0.708 0.597 0.572 0.141 0.100 0.200 0.119 0.113\nSASRec 0.150 0.110 0.206 0.128 0.123 0.669 0.644 0.689 0.650 0.645 0.146 0.110 0.191 0.124 0.122\nBERT4Rec 0.198 0.153 0.254 0.171 0.163 0.808 0.670 0.881 0.694 0.639 0.297 0.220 0.402 0.253 0.227\nGraph-based Sequential Recommender Systems\nSR-GNN 0.102 0.071 0.153 0.087 0.086 0.848 0.780 0.891 0.793 0.767 0.072 0.048 0.118 0.062 0.064\nGCSAN 0.217 0.160 0.305 0.188 0.173 0.872 0.846 0.890 0.851 0.842 0.119 0.086 0.175 0.104 0.101\nHyperRec 0.145 0.130 0.224 0.133 0.129 0.860 0.705 0.833 0.820 0.816 0.140 0.109 0.236 0.144 0.132\nSURGE 0.122 0.078 0.193 0.100 0.093 0.878 0.879 0.906 0.887 0.870 0.226 0.159 0.322 0.190 0.171\nMulti-Behavior Recommendation Models\nBERT4Rec-MB0.211 0.169 0.263 0.186 0.178 0.875 0.858 0.889 0.863 0.857 0.257 0.189 0.342 0.216 0.197\nMB-GCN 0.185 0.103 0.309 0.143 0.149 0.844 0.735 0.878 0.752 0.739 0.218 0.145 0.335 0.182 0.177\nNMTR 0.125 0.082 0.174 0.097 0.103 0.827 0.697 0.858 0.724 0.741 0.109 0.076 0.184 0.099 0.106\nMB-GMN 0.196 0.115 0.319 0.154 0.151 0.853 0.762 0.901 0.830 0.822 0.235 0.161 0.337 0.193 0.176\nMBHT 0.323âˆ— 0.257âˆ— 0.405âˆ— 0.283âˆ— 0.262âˆ— 0.931âˆ— 0.933âˆ— 0.956âˆ— 0.950âˆ— 0.929âˆ— 0.346âˆ— 0.268âˆ— 0.437âˆ— 0.297âˆ— 0.272âˆ—\n# Improve 48.84% 52.07% 26.95% 50.53% 47.19% 6.04% 6.14% 5.52% 7.10% 6.78% 16.50% 21.82% 8.71% 17.39% 19.82%\nâ€¢MB-GCN [11]. This model is built upon the graph convolutional\nlayer to refine user/item embeddings through the behavior-aware\nmessage passing on the user-item interaction graph.\nâ€¢NMTR [3]. It defines the behavior-wise cascading relationships\nto model the dependency among different types of behaviors\nunder a multi-task learning paradigm.\nâ€¢MB-GMN [27]. It employs a graph meta network to capture\npersonalized multi-behavior signals and model the diverse multi-\nbehavior dependencies. It generates state-of-the-art performance\namong different multi-behavior recommendation methods.\n4.1.4 Hyperparameter Settings. In MBHT model, we search\nthe number of hypergraph propagation layers from {1,2,3,4}. The\nnumber of multi-head channels is set as 2 for both self-attention\nand metric learning components. The value of ğ‘˜ for construct-\ning item-wise semantic dependency hypergraph is tuned from\n[4,6,8,10,12,14]. Considering that datasets vary by average se-\nquence length, the multi-scale setting parameters (ğ¶,ğ‘1,ğ‘2)are\nsearched amongst the value range of ([20,4,20],[20,8,40],[40,4,20],[40,8,40]).\n4.2 Performance Evaluation (RQ1)\nWe report the detailed performance comparison on different datasets\nin Table 2 and summarize the observations as followed:\nâ€¢The proposed MBHT consistently outperforms all types of base-\nlines by a significant margin on different datasets. The perfor-\nmance improvements can be attributed from: i) Through the\nmulti-scale behavior-aware Transformer, MBHT is able to capture\nthe behavior-aware item transitional patterns from fine-grained\nto coarse-grained time granularities. ii) With the hypergraph\nneural network for multi-behavior dependency learning, we en-\ndow MBHT with the capability of capturing long-range item\ncorrelations across behavior types over time.\nâ€¢By jointly analyzing the results across different datasets, we\ncan observe that our MBHT is robust to different recommenda-\ntion scenarios with various data characteristics, such as average\nsequence length and user-item interaction density, reflecting var-\nious user behaviour patterns in many online platforms.\nTable 3: Ablation study with key modules.\nModel Variants Taobao Retailrocket IJCAI\nHR@5 NDCG@5HR@5 NDCG@5HR@5 NDCG@5\nMBHT 0.323 0.257 0.956 0.950 0.346 0.268\n(-) MB-Hyper 0.261 0.206 0.883 0.861 0.320 0.249\n(-) ML-Hyper 0.271 0.212 0.898 0.874 0.328 0.256\n(-) Hypergraph 0.246 0.194 0.813 0.839 0.301 0.234\n(-) MS-Attention0.253 0.200 0.816 0.832 0.329 0.256\nâ€¢Graph-based sequential recommendation methods (e.g., SR-GNN,\nGCSAN, SURGE) perform worse than general sequential base-\nlines (e.g., BERT4Rec, HPMN) on IJCAI dataset with longer item\nsequences. The possible reason is that passing message between\nitems over the generated graph structures based on their directly\ntransitional relations can hardly capture the long-term item de-\npendencies. However, the performance superiority of GNN-based\nmodels can be observed on Retailrocket with shorter item se-\nquences. In such cases, modeling of short-term item transitional\nregularities is sufficient for capturing item dependencies.\nâ€¢BERT4Rec-MB outperforms BERT4Rec in most evaluation cases.\nIn addition, it can be found that multi-behavior recommenda-\ntion models (e.g., MB-GCN, MB-GMN) achieve comparable per-\nformance to other baselines. These observations indicate the\neffectiveness of incorporating multi-behavior context into the\nlearning process of user preference. With the effective modeling\nof dynamic multi-behaviour patterns from both short-term and\nlong-term perspectives, our MBHT is more effective than those\nstationary multi-behavior recommendation approaches.\n4.3 Ablation Study (RQ2)\n4.3.1 Effects of Key Components. We firstly investigate the ef-\nfectiveness of different components of our MBHT from both Trans-\nformer and Hypergraph learning views. Specifically, we generate\nfour variants and make comparison with our MBHT method:\nâ€¢(-) MB-Hyper. This variant does not include the hypergraph of\nitem-wise multi-behavior dependency to capture the long-range\ncross-type behavior correlations.\nâ€¢(-) ML-Hyper. In this variant, we remove the hypergraph mes-\nsage passing over the hyperedges of item semantic dependence\n(encoded with the metric learning component).\nKDDâ€™22, August 14â€“18, 2022, Washington, DC, USA Yuhao Yang et al.\nSequential View Graph View\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Attention Weight\n(a) Taobao\nSequential View Graph View\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Attention Weight\n (b) IJCAI\nSequential View Graph View\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Attention Weight (c) Retailrocket\nFigure 3: Distributions of the learned attentive view-specific\ncontributions. Green triangles and black line in the showed\nboxes denote the mean and median values, respectively.\n0 1 2 3 4\nSequence Group ID\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30NGCG@5\nMBHT\nBERT4Rec-MB\nBERT4Rec\nSURGE\nMB-GCN\n(a) Taobao\n0 1 2 3 4\nSequence Group ID\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25NGCG@5\nMBHT\nBERT4Rec-MB\nBERT4Rec\nSURGE\nMB-GCN (b) IJCAI\nFigure 4: Performance w.r.t different sequence lengths.\nâ€¢(-) Hypergraph. This variant disables the entire hypergraph item-\nwise dependency learning, and only relies on the multi-scale\nTransformer to model the sequential behavior patterns.\nâ€¢(-) MS-Attention. For this variant, we replace our multi-scale\nattention layer with the original multi-head attentional operation.\nFrom the reported results in Table 3, we summarize the following\nobservations to show the rationality of our model design. 1) With\nthe incorporation of hypergraph-based dependency learning on\neither item-wise latent semantic ((-) MB-Hyper) or dynamic multi-\nbehavior correlations ((-) ML-Hyper), MBHT can further boost\nthe recommendation performance. 2) Comparing with the vanilla\nmulti-head attention ((-) MS-Attention), the effectiveness of our\nmulti-scale low-rank self-attention can be validated.\n4.3.2 Contribution of Learning Views. We further investigate\nthe contribution of sequential and hypergraph learning views, by\npresenting the distributions of our learned importance scores of hğ‘–\nand xğ‘– in Figure 3. In particular, hğ‘– preserves multi-scale behavior\ndynamics of diverse user preference andxğ‘– encodes the global multi-\nbehavior dependencies. We can observe that hypergraph learning\nview contributes more to the effective modeling of dynamic multi-\nbehavior patterns with longer item sequences ( e.g., Taobao and\nIJCAI dataset). This further confirms the efficacy of our behavior-\naware hypergraph learning component in capturing the long-range\nitem dependencies in multi-relational sequential recommendation.\n4.4 Model Benefit Study (RQ3)\n4.4.1 Performance w.r.t Sequence Length. To further study\nthe robustness of our model, we evaluate MBHT on item sequences\nwith different length. Specifically, we split users into five groups in\nterms of their item sequences and conduct the performance com-\nparison on each group of users. From results presented in Figure 4,\nMBHT outperforms several representative baselines not only on\nthe shorter item interaction sequences, but also on the longer item\n1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4HR@5\nMBHT\nBERT4Rec-MB\nBERT4Rec\nGCSAN\nSURGE\n(a) Taobao\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4HR@5\nBERT4Rec-MB\nMBHT\nBERT4Rec\nGCSAN\nSURGE (b) IJCAI\nFigure 5: Training curves evaluated by testing Hit Rate.\nTarget BehaviorsAuxiliary \nBehaviors\nU2034U3072\nU0158U6714\nUser CasesScale-specificPatternsğ‘!\nğ‘\"\n Hypergraph Weights\nItem-wiseSemantics\nPageviewAdd-to-favAdd-to-cartPurchase\n(a) (c)\n(b) (d)\n(e)\n(f)\n(g)\n(h)\nFigure 6: Case studies with cross-type behavior dependen-\ncies.\nsequences. It indicates that our recommendation framework is en-\nhanced by injecting the behavior-aware short-term and long-term\ndependencies (from locally to globally) into the sequence embed-\ndings. Such data scarcity issue is hard to be alleviated purely from\nthe general and GNN-based sequential recommender systems.\n4.4.2 Model Convergence Study. We further investigate the\nconvergence property of our MBHT and various sequential and\ngraph-based temporal recommendation methods in Figure 5. Along\nthe model training process, MBHT achieves faster convergence rate\ncompared with most competitive methods. For example, MBHT\nobtains its best performance at epoch 2 and 8, while BERT4Rec\nand BERT4Rec-MB take 10 and 15 epochs to converge on Taobao\nand IJCAI datasets, respectively. This observation suggests that\nexploring the augmented multi-behavior information from both\nsequential and hypergraph views can provide better gradient to\nguide the model optimization in sequential recommendation.\n4.5 Case Study\nIn this section, we conduct further model analysis with case study\nto show the model interpretation for multi-behavior dependency\nmodeling. In particular, we show user-specific cross-type behavior\ndependencies in Figure 6 (a)-(d). Each 4 Ã—4 dependency matrix is\nlearned from our multi-scale Transformer. We compute the item-\nitem correlations by considering their behavior-aware interactions\nin the specific sequence ğ‘†ğ‘– of user ğ‘¢ğ‘–. Figure 6 (e)-(f) show the scale-\nspecific multi-behavior dependencies with the scales ofğ‘1 and ğ‘2 in\nour multi-scale modeling of behavior-aware sequential patterns. In\nFigure 6 (g), we show the overall relevance scores among different\ntypes of behaviors in making final forecasting on target behaviors.\nAdditionally, our hypergraph-based item dependencies (M Â·MT)\nMulti-Behavior Hypergraph-Enhanced Transformer\nfor Sequential Recommendation KDDâ€™22, August 14â€“18, 2022, Washington, DC, USA\nare shown in Figure 6 (h), such as hypergraph-based i) behavior-\naware item relevance; and ii) item-wise semantic dependence.\n5 RELATED WORK\nSequential Recommendation. Earlier studies solve the next-item\nrecommendation using the Markov Chain-based approaches to\nmodel item-item transitions [5, 18]. In recent years, many efforts\nhave been devoted to proposing neural network-enhanced sequen-\ntial recommender systems to encode the complex dependencies\namong items from different perspectives. For example, recurrent\nneural network in GRU4Rec [7] and convolutional operations in\nCaser [20]. Inspired by the strength of Transformer, SASRec [13]\nand BERT4Rec [19] are built upon the self-attention mechanism\nfor item-item relation modeling. Furthermore, recently emerged\ngraph neural networks produce state-of-the-art performance by\nperforming graph-based message passing among neighboring items,\nin order to capture sequential signals, e.g., SR-GNN [25], MTD [8],\nMA-GNN [16] and SURGE [1]. However, most of those methods\nare specifically designed for singular type of interaction behaviors,\nand cannot handle diverse user-item relationships.\nHypergraph Learning for Recommendation . Motivated by ex-\npressiveness of hypergraphs [2, 32], hypergraph neural networks\nare utilized in several recent recommender systems to model high-\norder relationships, such as multi-order item correlations in Hy-\nperRec [21], global user dependencies in HCCF [ 26], high-order\nsocial relationships in MHCN [ 33], and item dependencies with\nmulti-modal features in HyperCTR [4]. Following this research line,\nthis work integrates the hypergraph neural architecture with Trans-\nformer architecture to encode behavior-aware sequential patterns\nfrom local to global-level for comprehensive behavior modeling.\nMulti-Behavior Recommender Systems . There exist recently\ndeveloped multi-behavior recommender systems for modeling user-\nitem relation heterogeneity [3, 11, 23, 29, 35]. For example, NMTR [3]\nis a multi-task recommendation framework with the predefined\nbehavior-wise cascading relationships. Motivated by the strength\nof GNNs, MBGCN [11], MBGMN [27], MGNN [35] are developed\nbased on the graph-structured message passing over the generated\nmulti-relational user-item interaction graphs. Nevertheless, none\nof those approaches considers the time-evolving multi-behaviour\nuser preference. To fill this gap, our MBHT model is able to capture\nboth short-term and long-term multi-behavior dependencies with\na hypergraph-enhanced transformer architecture.\n6 CONCLUSION\nIn this paper, we present a new sequential recommendation frame-\nwork MBHT which explicitly captures both short-term and long-\nterm multi-behavior dependencies. MBHT designs a multi-scale\nTransformer to encode the behavior-aware sequential patterns at\nboth fine-grained and coarse-grained levels. To capture the global\ncross-type behavior dependencies, we empower MBHT with a\nmulti-behavior hypergraph learning component. Empirical results\non several real-world datasets validate the strengths of our MBHT\nwhen competing with state-of-the-art recommendation methods.\nACKNOWLEDGMENTS\nThis research is supported by the research grants from the Depart-\nment of Computer Science & Musketeers Foundation Institute of\nData Science at the University of Hong Kong.\nREFERENCES\n[1] Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, et al. 2021.\nSequential Recommendation with Graph Neural Networks. In SIGIR. 378â€“387.\n[2] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hy-\npergraph neural networks. In AAAI, Vol. 33. 3558â€“3565.\n[3] Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, et al. 2019. Neural multi-\ntask recommendation from multi-behavior data. In ICDE. IEEE, 1554â€“1557.\n[4] Li He, Hongxu Chen, Dingxian Wang, Shoaib Jameel, Philip Yu, et al. 2021. Click-\nThrough Rate Prediction with Multi-Modal Hypergraphs. In CIKM. 690â€“699.\n[5] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov\nchains for sparse sequential recommendation. In ICDM. IEEE, 191â€“200.\n[6] Xiangnan He, Kuan Deng, Xiang Wang, et al. 2020. Lightgcn: Simplifying and\npowering graph convolution network for recommendation. In SIGIR. 639â€“648.\n[7] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2016. Session-based recommendations with recurrent neural networks. In ICLR.\n[8] Chao Huang, Jiahui Chen, Lianghao Xia, Yong Xu, Peng Dai, Yanqing Chen,\nLiefeng Bo, Jiashu Zhao, and Jimmy Xiangji Huang. 2021. Graph-enhanced\nmulti-task learning of multi-level transition dynamics for session-based recom-\nmendation. In AAAI.\n[9] Chao Huang, Xian Wu, Xuchao Zhang, Chuxu Zhang, Jiashu Zhao, Dawei Yin,\nand Nitesh V Chawla. 2019. Online purchase prediction via multi-scale modeling\nof behavior dynamics. In KDD. 2613â€“2622.\n[10] Hao Jiang et al . 2020. What aspect do you like: Multi-scale time-aware user\ninterest modeling for micro-video recommendation. In MM. 3487â€“3495.\n[11] Bowen Jin, Chen Gao, Xiangnan He, Depeng Jin, et al . 2020. Multi-behavior\nrecommendation with graph convolutional networks. In SIGIR. 659â€“668.\n[12] Taegwan Kang, Hwanhee Lee, Byeongjin Choe, and Kyomin Jung. 2021. Entan-\ngled bidirectional encoder to autoregressive decoder for sequential recommenda-\ntion. In SIGIR. 1657â€“1661.\n[13] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In ICDM. IEEE, 197â€“206.\n[14] Nikita Kitaev et al. 2020. Reformer: The efficient transformer. In ICLR.\n[15] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, et al. 2021. Swin transformer:\nHierarchical vision transformer using shifted windows. In ICCV. 10012â€“10022.\n[16] Chen Ma, Liheng Ma, Yingxue Zhang, et al . 2020. Memory augmented graph\nneural networks for sequential recommendation. In AAAI, Vol. 34. 5045â€“5052.\n[17] Kan Ren, Jiarui Qin, et al. 2019. Lifelong sequential modeling with personalized\nmemorization for user response prediction. In SIGIR. 565â€“574.\n[18] Steffen Rendle, Christoph Freudenthaler, et al. 2010. Factorizing personalized\nmarkov chains for next-basket recommendation. In WWW. 811â€“820.\n[19] Fei Sun, Jun Liu, et al. 2019. BERT4Rec: Sequential recommendation with bidi-\nrectional encoder representations from transformer. In CIKM. 1441â€“1450.\n[20] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation\nvia convolutional sequence embedding. In WSDM. 565â€“573.\n[21] Jianling Wang, Kaize Ding, Liangjie Hong, Huan Liu, and James Caverlee. 2020.\nNext-item recommendation with sequential hypergraphs. In SIGIR. 1101â€“1110.\n[22] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.\nLinformer: Self-Attention with Linear Complexity. arXiv:2006.04768 [cs.LG]\n[23] Wei Wei, Chao Huang, Lianghao Xia, Yong Xu, Jiashu Zhao, and Dawei Yin. 2022.\nContrastive Meta Learning with Behavior Multiplicity for Recommendation. In\nWSDM. 1120â€“1128.\n[24] Liang Wu, Diane Hu, Liangjie Hong, et al. 2018. Turning clicks into purchases:\nRevenue optimization for product search in e-commerce. In SIGIR. 365â€“374.\n[25] Shu Wu, Yuyuan Tang, Yanqiao Zhu, et al. 2019. Session-based recommendation\nwith graph neural networks. In AAAI, Vol. 33. 346â€“353.\n[26] Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin, and Jimmy Xiangji\nHuang. 2022. Hypergraph Contrastive Collaborative Filtering. arXiv preprint\narXiv:2204.12200.\n[27] Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, and Liefeng Bo. 2021. Graph\nmeta network for multi-behavior recommendation. In SIGIR. 757â€“766.\n[28] Chengfeng Xu, Pengpeng Zhao, et al. 2019. Graph Contextualized Self-Attention\nNetwork for Session-based Recommendation.. In IJCAI, Vol. 19. 3940â€“3946.\n[29] Haoran Yang, Hongxu Chen, Lin Li, S Yu Philip, and Guandong Xu. 2021. Hyper\nMeta-Path Contrastive Learning for Multi-Behavior Recommendation. In ICDM.\nIEEE, 787â€“796.\n[30] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge\nGraph Contrastive Learning for Recommendation.arXiv preprint arXiv:2205.00976\n(2022).\n[31] Shaowei Yao and Xiaojun Wan. 2020. Multimodal transformer for multimodal\nmachine translation. In ACL. 4346â€“4350.\nKDDâ€™22, August 14â€“18, 2022, Washington, DC, USA Yuhao Yang et al.\n[32] Jaehyuk Yi and Jinkyoo Park. 2020. Hypergraph convolutional recurrent neural\nnetwork. In KDD. 3366â€“3376.\n[33] Junliang Yu, Hongzhi Yin, et al. 2021. Self-Supervised Multi-Channel Hypergraph\nConvolutional Network for Social Recommendation. In WWW. 413â€“424.\n[34] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-\nommender system: A survey and new perspectives. ACM Computing Surveys\n(CSUR) 52, 1 (2019), 1â€“38.\n[35] Weifeng Zhang, Jingwen Mao, Yi Cao, and Congfu Xu. 2020. Multiplex graph\nneural networks for multi-behavior recommendation. In CIKM. 2313â€“2316.\n[36] Xiangmin Zhou, Dong Qin, Xiaolu Lu, Lei Chen, and Yanchun Zhang. 2019.\nOnline social media recommendation over streams. In ICDE. IEEE, 938â€“949.\nMulti-Behavior Hypergraph-Enhanced Transformer\nfor Sequential Recommendation KDDâ€™22, August 14â€“18, 2022, Washington, DC, USA\nA SUPPLEMENTARY MATERIAL\nIn our supplemental material, we first summarize the learning pro-\ncess of our MBHT framework in Algorithm 1 and conduct the\nmodel time complexity analysis. Then, we present our strategy to\nsimplify the implementation of our hypergraph-based embedding\npropagation, so as to improve the model efficiency.\nA.1 The Learning Process of MBHT\nAlgorithm 1: The forward propagation flow of MBHT\nInput : The behavior-aware interaction sequence ğ‘†ğ‘– for\nuser ğ‘¢ğ‘– with mask tokens at positions ğ‘€ and with\ntrue labels ğ‘‡.\nğ‘†ğ‘– = [(ğ‘£ğ‘–,1,ğ‘ğ‘–,1),..., [mask],..., (ğ‘£ğ‘–,ğ½,ğ‘ğ‘–,ğ½ )].\nOutput: The estimated probability for user ğ‘¢ğ‘– interacting\nwith ground-truth items ğ‘‡ at the time step\npositions ğ‘€.\n1 Multi-Scale Transformer View;\n2 Inject positional and behavior signals into Transformer\ninputs: H â†[h1,..., hğ½+1], hğ‘— â†eğ‘— âŠ•pğ‘— âŠ•bğ‘—;\n3 Perform multi-scale attention under multi-scale settings\nğ¶,ğ‘1 and ğ‘2 according to Equation 2-4:\neH â†ğ‘“(bH âˆ¥Hğ‘1 âˆ¥Hğ‘2 );\n4 Perform point-wise feed-forward to inject non-linearity:\neH(ğ‘™)â†[FFN(Ëœh(ğ‘™)\n1 )T,Â·Â·Â· ,FFN(Ëœh(ğ‘™)\nğ‘¡ )T];\n5 Hypergraph View;\n6 Model item-wise semantic dependencies with multi-channel\nmetric learning: ğ›½ğ‘—,ğ‘—â€² â† 1\nğ‘\nÃğ‘\nğ‘›=1 ^ğ›½ğ‘›\nğ‘—,ğ‘—â€²;\n7 Construct customized hyperedges according to Equation\n10-11: Mâ†M ğ‘ âˆ¥Mğ‘;\n8 Apply hypergraph convolutional function to aggregate\ninformation from the graph:\nX(ğ‘™+1)â†Dâˆ’1ğ‘£ Â·M Â·Dâˆ’1ğ‘’ Â·MT Â·X(ğ‘™);\n9 Fusion and Prediction;\n10 foreach ğ‘š âˆˆğ‘€,ğ‘¡ âˆˆğ‘‡ do\n11 Generate hypergraph-view embedding for mask\nposition ğ‘šusing sliding-window contextual pooling:\nËœxğ‘š â†mean(Ëœxğ‘šâˆ’ğ‘1,..., Ëœxğ‘š+ğ‘2 );\n12 Apply attentive cross-view aggregation to fuse the\ninformation according to Equation 13:\ngğ‘š â†ğ›¼1 Â·Ëœhğ‘š âŠ•ğ›¼2 Â·Ëœxğ‘š;\n13 Calculate the probability of item at ğ‘šbeing ğ‘£ğ‘¡:\n^ğ‘¦ğ‘š,ğ‘¡ â†gT\nğ‘švğ‘¡;\n14 end\n15 return [^ğ‘¦ğ‘š1,ğ‘¡1,^ğ‘¦ğ‘š2,ğ‘¡2,...];\nA.2 Time Complexity Analysis\nThis section conducts the time complexity analysis of our MBHT\nframework as follows. (1) For the multi-scale Transformer view,\nwith our low-rank self-attention layer, we significantly reduce the\ncomputational cost from ğ‘‚(ğ¿Ã—ğ‘‘ Ã—(( ğ½\nğ¶)2 +( ğ½\nğ‘1\n)2 +( ğ½\nğ‘2\n)2))to\napproximate the linear time complexity ğ‘‚(3ğ¿ğ‘‘ğ½), given that ğ¶\n(low-rank scale), ğ‘1,ğ‘2 (resolution scales) â‰ªğ½ [22]. ğ½ denotes the\nlength of item sequence. (2) For the hypergraph learning view,\nthe semantic metric learning takes ğ‘‚(ğ½2ğ‘‘)complexity, and the\nhypergraph convolutional function takes ğ‘‚(ğ¿ğ½ğ‘‘2). Based on the\nabove discussion, the overall time complexity of our MBHT is\nğ‘‚(3ğ¿ğ½ğ‘‘ +ğ¿ğ½ğ‘‘2 +ğ½2ğ‘‘), which is comparable to state-of-the-art\nbaselines.\nA.3 Simplifying Hypergraph Message Passing\nWith the consideration of high computational cost during the mes-\nsage passing in our hypergraph learning framework, we propose to\nsimplify the propagation scheme with the learnable embedding pro-\njection. Formally, we rewrite the two-stage (node-hyperedge-node)\nmessage propagation process M Â·MT as follows:\n(M Â·MT)ğ‘–ğ‘— =\nğ‘›âˆ‘ï¸\nğ‘˜=1\nMğ‘–ğ‘˜MT\nğ‘˜ğ‘— = M(ğ‘–)MT(ğ‘—)\n= M(ğ‘–)M(ğ‘—) (15)\nwhere M(ğ‘–)and M(ğ’‹) denotes theğ‘–-th row and ğ‘—-th column vector,\nrespectively. Based on our item-wise semantic dependence and\nmulti-behavior correlations, our hypergraph-guided information\npropagation can be presented as follows:\nM(ğ‘–)M(ğ‘—)=\n|Eğ‘|âˆ‘ï¸\nğ‘˜=1\nMğ‘–,ğ‘˜Mğ‘—,ğ‘˜ +\n|Eğ‘|âˆ‘ï¸\nğ‘š=1\nMğ‘–,ğ‘šMğ‘—,ğ‘š (16)\nwhere Mğ‘–,ğ‘˜ and Mğ‘–,ğ‘š denote the value of hypergraph connection\nmatrix with item-wise semantic dependency and multi-behavior\ncorrelations of item ğ‘£ğ‘–, respectively. Here, we first simplify the mul-\ntiplication operations with the multi-behavior dependency hyper-\nedges. Specifically, the same item (ğ‘£ğ‘– = ğ‘£ğ‘—) interacted with different\nbehavior types over time will be connected to the same hyperedge\nğœ–ğ‘, i.e., Mğ‘–,ğœ–ğ‘ = Mğ‘—,ğœ–ğ‘ = 1. Since each item can only be connected\nto one hyperedge, for ğœ–â€²â‰  ğœ–ğ‘, Mğ‘–,ğœ–â€² = Mğ‘—,ğœ–ğ‘ = 0. This indicates\nthat if ğ‘£ğ‘– â‰  ğ‘£ğ‘—, there exists no multi-behavior dependency hyper-\nedge ğœ–â€²such that Mğ‘–,ğœ–â€²Mğ‘—,ğœ–â€² = 1, since different items cannot\nbe connected to the same hyperedge. Hence, we can isolate the\ninfluence of the multi-behavior dependency hyperedges as follows:\n|Eğ‘|âˆ‘ï¸\nğœ–â€²=1\nMğ‘–,ğœ–â€²Mğ‘—,ğœ–â€² =\n(\n1 if ğ‘£ğ‘– = ğ‘£ğ‘—\n0 otherwise (17)\nWe further investigate the item-wise semantic dependency hy-\nperedges in Equation 16 for simplifying hypergraph convolutional\noperations. Note that, Mğ‘–,ğ‘š denotes the cosine similarity between\nitem ğ‘£ğ‘– and ğ‘£ğ‘š that belong to the hyperedge ğ‘š. Generally, the\ncomputation Ã|Eğ‘|\nğ‘š=1 Mğ‘–ğ‘šMğ‘—ğ‘š can be divided into three cases de-\npending on the hyperedge ğ‘š: i) behavior-aware self-connection, if\nğ‘£ğ‘– = ğ‘£ğ‘— and ğ‘šis the hyperedge assigned to this item; ii) first-order\nsimilarity, if ğ‘£ğ‘– â‰  ğ‘£ğ‘— and ğ‘šis assigned to ğ‘£ğ‘– or ğ‘£ğ‘—; iii) second-order\nsimilarity, if ğ‘£ğ‘– â‰  ğ‘£ğ‘— and ğ‘šis not assigned to either ğ‘£ğ‘– or ğ‘£ğ‘—.\nWe leverage the pre-calculated behavior-aware semantic similar-\nities ğ›½ğ‘–,ğ‘— between items for the first and second cases. Here ğ›½ğ‘–,ğ‘— is\ntruncated from the top-ğ‘˜ value to be consistent with the semantic\ndependency hyperedges. We denote the values of the third case by\nğ‘¤ğ‘–,ğ‘—. Therefore, based on the above discussions, for the first case,\nwe have: Ã|Eğ‘|\nğ‘š=1 Mğ‘–ğ‘šMğ‘—ğ‘š = ğ›½ğ‘–,ğ‘— +ğ‘¤ğ‘–,ğ‘—; for the second case, we\nhave Ã|Eğ‘|\nğ‘š=1 Mğ‘–ğ‘šMğ‘—ğ‘š = ğ›½ğ‘–,ğ‘— +ğ›½ğ‘—,ğ‘– +ğ‘¤ğ‘–,ğ‘—. Since the computation\nKDDâ€™22, August 14â€“18, 2022, Washington, DC, USA Yuhao Yang et al.\nTable 4: Comparison between two implementation of hyper-\ngraph message passing schemes, i.e., the original embedding\npropagation based on hypergraph connection matrices, and\nthe simplified message passing schema. The recommenda-\ntion accuracy is measured by Recall@5 and model computa-\ntional cost is measured by the running time of each epoch.\nTaoabo Retailrocket IJCAI\nRecall@5 Epoch TimeRecall@5 Epoch TimeRecall@5 Epoch Time\nOrigin 0.326 86.43 0.959 4.58 0.352 133.28\nSimplified 0.323 38.05 0.956 2.11 0.346 65.2\n0.92\n0.94HR@5\nRetailrocket\n[40,4,20] [40,8,40] [20,4,20] [20,8,40]\nScales\n0.30\n0.35HR@5\n T aobao\nIJCAI\n(a) Sensitivity to attention scales\n0.92\n0.94HR@5\nRetailrocket\n4 6 8 10 12 14\nSim-Group Lengths\n0.32\n0.34HR@5\nT aobao\nIJCAI (b) Sensitivity to sim-group length\nFigure 7: Hyperparameter study of MBHT framework.\nof the second-order complexity is time-consuming, and a number\nof such values are slight due to the top- ğ‘˜ truncation, we replace\nğ‘¤ğ‘–,ğ‘— by a hyperparameter ğ‘¤0 to obtain a close approximation Mâ€²\nfor MMT. Formally, Mâ€²âˆˆRğ½Ã—ğ½ is generated as follows:\nMâ€²= ğ¶ âŠ•ğ´âŠ•ğ‘Š (18)\nwhere ğ¶ğ‘–,ğ‘— = 1 if ğ‘£ğ‘– = ğ‘£ğ‘—, otherwise ğ¶ğ‘–,ğ‘— = 0, and ğ´ğ‘–,ğ‘— = ğ›½ğ‘–,ğ‘— if\nğ‘£ğ‘– = ğ‘£ğ‘—, otherwise ğ´ğ‘–,ğ‘— = ğ›½ğ‘–,ğ‘— +ğ›½ğ‘—,ğ‘–. ğ‘Šğ‘–,ğ‘— = ğ‘¤0. In our experiments,\nğ‘¤0 is searched among [0.05,0.1,0.15,0.2]. Finally, we express the\nlight version of hypergraph convolution function below:\nX(ğ‘™+1)= Dâˆ’1 Â·Mâ€²Â·X(ğ‘™) (19)\nWe conduct empirical experiments to further evaluate the model\nperformance of our simplified hypergraph convolution mechanism.\nWe report the evaluation results in Table 4. The model training\nis performed on a single GTX3090 GPU for running time evalua-\ntion. We can observe that our simplified hypergraph-based message\npassing scheme only lead to slightly performance degradation and\nimprove the model efficiency with lower computational cost. The\npotential reasons are: i) a large number of second-order similarity\nare slight, since scores are truncated from top-ğ‘˜and ii) the convolu-\ntion process inherently takes into account high-order connectivity.\nAt the same time, simplifying the convolutional function brings a\nlot enhancement on inference speed, since it eliminates the original\nğ‘‚((|Eğ‘|+|E ğ‘|)Ã— ğ½2)calculations and Mâ€²can be easily built by\nleveraging pre-calculated values and data pre-processing.\nA.4 Hyperparameter Study (RQ4)\nWe conduct experiments to analyze the influence of key hyperpa-\nrameters in our MBHT framework and report results in Figure 7.\nImpact of Multi-Scale Settings . We search multi-scale setting pa-\nrameters (ğ¶,ğ‘1,ğ‘2)among the range{[20,4,20],[20,8,40],[40,4,20]\n[40,8,40]}. We present the observations as followed:\nâ€¢The best performance on Retailrocket and Taobao datasets can\nbe achieved with (ğ‘1,ğ‘2)= (4,20)and (ğ‘1,ğ‘2)= (8,40)given\nthe difference of average sequence length.\nâ€¢For the low-rank projection scale parameter ğ¶, we can notice\nthat projecting original self-attentive sequence embedding space\ninto ğ½\n20 channels can bring the best performance on IJCAI and\nRetailrocket datasets. For Taobao dataset, we can observe that ğ½\n40\nperforms better than ğ½\n20 , which indicates that dense user-item\ninteraction data may need less low-rank projection channels for\nbetter sequential pattern encoding.\nImpact of Item-wise Semantic Dependency Set . Our hyper-\ngraph item dependency encoder investigates the latent semantic\ncorrelations among different items. We search the top-ğ‘˜ semantic\ndependent items from {4,6,8,10,12,14} for global message passing\nthrough the item-wise semantic hyperedges. Observations are:\nâ€¢The best settings of ğ‘˜ is proportionally to the average sequence\nlength of different datasets. It indicates that larger hypergraph\npropagation scope is better for modeling longer item sequences.\nâ€¢Increasing the number of connected items through hyperedges\nmay firstly boost the performance at the early stage, and then\nlead to performance degradation by involving noise during the\nhypergraph-based embedding propagation.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8322626352310181
    },
    {
      "name": "Hypergraph",
      "score": 0.7894071340560913
    },
    {
      "name": "Dependency (UML)",
      "score": 0.4393923580646515
    },
    {
      "name": "ENCODE",
      "score": 0.429823637008667
    },
    {
      "name": "Exploit",
      "score": 0.4237942695617676
    },
    {
      "name": "Transformer",
      "score": 0.4149611294269562
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4016399085521698
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3788645267486572
    },
    {
      "name": "Machine learning",
      "score": 0.3610556125640869
    },
    {
      "name": "Data mining",
      "score": 0.3331548273563385
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Discrete mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I59028903",
      "name": "Ocean University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    }
  ]
}