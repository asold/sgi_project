{
  "title": "Practical Text Classification With Large Pre-Trained Language Models",
  "url": "https://openalex.org/W2903285529",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287599652",
      "name": "Kant, Neel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227412475",
      "name": "Puri, Raul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288982588",
      "name": "Yakovenko, Nikolai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2988481184",
      "name": "Catanzaro, Bryan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2805744755",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W2798132978",
    "https://openalex.org/W2994831951",
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W2772357980",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2952754453",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Multi-emotion sentiment classification is a natural language processing (NLP) problem with valuable use cases on real-world data. We demonstrate that large-scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets, including those with label class imbalance and domain-specific context. By training an attention-based Transformer network (Vaswani et al. 2017) on 40GB of text (Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our model achieves a 0.69 F1 score on the SemEval Task 1:E-c multi-dimensional emotion classification problem (Mohammad et al. 2018), based on the Plutchik wheel of emotions (Plutchik 1979). These results are competitive with state of the art models, including strong F1 scores on difficult (emotion) categories such as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive results on rare categories such as Anticipation (0.42) and Surprise (0.37). Furthermore, we demonstrate our application on a real world text classification task. We create a narrowly collected text dataset of real tweets on several topics, and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin. We also perform a variety of additional studies, investigating properties of deep learning architectures, datasets and algorithms for achieving practical multidimensional sentiment classification. Overall, we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on real-world sentiment classification.",
  "full_text": "Practical Text Classiﬁcation With Large Pre-Trained Language Models\nNeel Kant\nUniversity of California, Berkeley\nkantneel@berkeley.edu\nRaul Puri\nNVIDIA, Santa Clara CA\nraulp@nvidia.com\nNikolai Yakovenko\nNVIDIA, Santa Clara CA\nnyakovenko@nvidia.com\nBryan Catanzaro\nNVIDIA, Santa Clara CA\nbcatanzaro@nvidia.com\nAbstract\nMulti-emotion sentiment classiﬁcation is a natural language\nprocessing (NLP) problem with valuable use cases on real-\nworld data. We demonstrate that large-scale unsupervised\nlanguage modeling combined with ﬁnetuning offers a prac-\ntical solution to this task on difﬁcult datasets, including those\nwith label class imbalance and domain-speciﬁc context. By\ntraining an attention-based Transformer network (Vaswani\net al. 2017) on 40GB of text (Amazon reviews) (McAuley\net al. 2015) and ﬁne-tuning on the training set, our model\nachieves a 0.69 F1 score on the SemEval Task 1:E-c multi-\ndimensional emotion classiﬁcation problem (Mohammad et\nal. 2018), based on the Plutchik wheel of emotions (Plutchik\n1979). These results are competitive with state of the art mod-\nels, including strong F1 scores on difﬁcult (emotion) cate-\ngories such as Fear (0.73), Disgust (0.77) and Anger (0.78), as\nwell as competitive results on rare categories such as Antic-\nipation (0.42) and Surprise (0.37). Furthermore, we demon-\nstrate our application on a real world text classiﬁcation task.\nWe create a narrowly collected text dataset of real tweets\non several topics, and show that our ﬁnetuned model outper-\nforms general purpose commercially available APIs for sen-\ntiment and multidimensional emotion classiﬁcation on this\ndataset by a signiﬁcant margin. We also perform a variety\nof additional studies, investigating properties of deep learn-\ning architectures, datasets and algorithms for achieving prac-\ntical multidimensional sentiment classiﬁcation. Overall, we\nﬁnd that unsupervised language modeling and ﬁnetuning is a\nsimple framework for achieving high quality results on real-\nworld sentiment classiﬁcation.\nIntroduction\nRecent work has shown that language models – both RNN\nvariants like the multiplicative LSTM (mLSTM) (Krause et\nal. 2016), as well as the attention-based Transformer net-\nwork (Vaswani et al. 2017) – can be trained efﬁciently\nover very large datasets, and that the resulting models can\nbe transferred to downstream language understanding prob-\nlems, often matching or exceeding the previous state of the\nart approaches on academic datasets. However, how well do\nthese models perform on practical text classiﬁcation prob-\nlems, with real world data?\nCopyright c⃝ 2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nIn this work, we train both mLSTM and Transformer lan-\nguage models on a large 40GB text dataset (McAuley et\nal. 2015), then transfer those models to two text classiﬁca-\ntion problems: binary sentiment (including Neutral labels),\nand multidimensional emotion classiﬁcation based on the\nPlutchik wheel of emotions (Plutchik 1979). We examine\nour performance on these tasks, both against large academic\ndatasets, and on an original text dataset that we compiled\nfrom social media messages about several speciﬁc topics,\nsuch as video games.\nWe demonstrate that our approach matches the state of the\nart on the academic datasets without domain-speciﬁc train-\ning and without excessive hyper-parameter tuning. Mean-\nwhile on the social media dataset, our approach outperforms\ncommercially available APIs by signiﬁcant margins, even\nwhen those models are re-calibrated to the test set.\nFurthermore, we notice that 1) the Transformer model\ngenerally out-performs the mLSTM model, especially when\nﬁne-tuning on multidimensional emotion classiﬁcation, and\n2) ﬁne-tuning the model signiﬁcantly improves performance\non the emotion tasks, both for the mLSTM and the Trans-\nformer model. We suggest that our approach creates models\nwith good generalization to increasingly difﬁcult text classi-\nﬁcation problems, and we offer ablation studies to demon-\nstrate that effect.\nIt is difﬁcult to ﬁt a single model for text classiﬁcation\nacross domains, due to unknown words, specialized con-\ntext, colloquial language, and other differences between do-\nmains. For example, words such as war and sick are not\nnecessarily negative in the context of video games, which\nare signiﬁcantly represented in our dataset. By training a\nlanguage model across a large text dataset, we expose our\nmodel to many contexts. Perhaps a small amount of down-\nstream transfer is enough to choose the right context features\nfor emotion classiﬁcation in the appropriate setting.\nOur work shows that unsupervised language modeling\ncombined with ﬁnetuning offers a practical solution to spe-\ncialized text classiﬁcation problems, including those with\nlarge category class imbalance, and signiﬁcant human label\ndisagreement.\nBackground\nSupervised learning is difﬁcult to apply to NLP problems be-\ncause labels are expensive. Following (Radford, J´ozefowicz,\narXiv:1812.01207v1  [cs.CL]  4 Dec 2018\nand Sutskever 2017), (Radford et al. 2018) and (Dai and Le\n2015), we train unsupervised text models on large amounts\nof unlabelled text data, and transfer the model features to\nsmall supervised text problems. The supervised text classi-\nﬁcation problem used for transfer is binary sentiment on the\nStanford Sentiment Treebank (SST) (Socher et al. 2013).\nSome of these binary text examples are subtle. Prior\nworks show that unsupervised language models can learn\nnuanced features of text, such as word ordering and double\nnegation, just from the underlying task of next-word predic-\ntion. However, while this includes difﬁcult examples, it does\nnot necessarily represent sentiment on practical text prob-\nlems.\n• The source material (professionally written movie re-\nviews) does not include colloquial language.\n• The dataset excludes Neutral sentiment texts and those\nwith weak directional sentiment.\n• The dataset does not include dimensions of sentiment\napart from Positive and Negative.\nPlutchik’s Wheel of Emotions We focus our multi-\ndimension emotion classiﬁcation on Plutchik’s wheel of\nemotions (Plutchik 1979). This taxonomy, in use since 1979,\naims to classify human emotions as a combination of four\ndualities: Joy - Sadness, Anger - Fear, Trust - Disgust, and\nSurprise - Anticipation. According to the basic emotion\nmodel (Ekman 2013), while humans experience hundreds of\nemotions, some emotions are more fundamental than others.\nThe commercial general purpose emotion classiﬁcation\nAPI that we compare against, IBM’s Watson1, offers classi-\nﬁcation scores for the Joy, Sadness, Fear, Disgust and Anger\nemotions – all present in Plutchik’s wheel (Fig. 1).\nSemEval Multidimension Emotion Dataset The Se-\nmEval Task 1:E-c problem (Mohammad et al. 2018) offers a\ntraining set of 6,857 tweets, with binary labels for the eight\nPlutchik categories, plus Optimism, Pessimism, and Love.\nThis dataset was created through a process of text selection\nand human labeling. We show our results on this dataset and\ncompare it to the current state of the art performance.\nWhile it is not possible to report rater agreement on these\ncategories for the compilation of the dataset, the authors note\nthat 2 out of 7 raters had to agree for a positive label to be\napplied, as requiring larger agreement caused a scarcity of\n1https://www.ibm.com/watson/services/natural-language-\nunderstanding/\nFigure 1: Plutchik’s wheel of emotions (Plutchik 1979).\nlabels for some categories. This indicates that some of the\ncategories had signiﬁcant rater disagreement between the\nhuman raters. The dataset also included a substantial degree\nof label class imbalance, with some categories like Anger\n(37%), Disgust (38%), Joy (36%) and Sadness (29%) rep-\nresented often in the dataset, while others like Trust (5%)\nand Surprise (5%) present much less frequently (Fig.2). This\nclass imbalance and human rater disagreement is not uncom-\nmon for real world text classiﬁcation problems2.\nCompany Tweet Dataset In addition to the SemEval\ntweet dataset, we wanted to see how our model would per-\nform on a similar but domain-speciﬁc task: Plutchik emotion\nclassiﬁcation on tweets relevant to a particular company. We\ncollected tweets on a variety of topics, including:\n• Video game tweets\n• Tweets about the company stock\nWe submitted the ﬁrst batch of 4,000 tweets to human\n2We submitted the SemEval training set for re-labeling using\nour rater instructions. See Fig.3 for an estimate of rater disagree-\nment over the SemEval training set.\nTable 1: Difﬁcult video game tweets.\nTweet Watson Sad Joy Fear GCL Ours\nBinary Binary Binary\nEncouraging collaboration among players in Sea ofThieves<url> -0.302 0.229 0.194 0.150 -0.80 Pos\ngot my ﬁrstkillon Fortnite all by myself I’m geeked<emoji>perioddddd. -0.847 0.003 0.666 0.225 +0.60 Neu\nFarCry5 ”LostOn Mars” Gameplay Walkthrough - DLC2:<url>via @YouTube -0.909 0.047 0.015 0.873 +0.00 Neu\nNEW SUBMACHINEGUNIS INSANE! — Fortnite Best Moments 39 (Fortnite\nFunnyFails& WTF Moments)<url>\n-0.936 0.821 0.178 0.056 -0.10 Pos\nTable 2: Label class balance (as percent) for SemEval and company tweet datasets.\nSize Anger Anticipation Disgust Fear Joy Sad Surprise Trust Ave/None\nSemEval 6,858 37.2 14.3 38.0 18.2 36.2 29.4 5.3 5.2 23.0/2.9\n(Random) 4,021 7.8 14.7 5.2 1.7 21.9 3.4 4.3 6.6 8.2/52.1\n(Active) 5,024 22.0 10.2 12.3 5.6 19.7 6.3 7.1 6.5 11.2/35.6\n(All) 13,326 11.7 12.9 6.8 2.9 20.6 4.2 5.0 7.6 8.9/47.0\nraters on the FigureEight 3 platform, with rules similar to\nthose used by SemEval, which also used the FigureEight\nplatform for human labeling. Speciﬁcally, we veriﬁed that\nraters passed our golden set (answering 70% of test ques-\ntions correctly). We applied positive labels for each category\nwhere 2 out of 5 raters agreed. This is slightly less permis-\nsive than the 2 out of 7 raters used by SemEval, because we\ndid not have a budget for 7 raters per tweet.\nAfter the ﬁrst pass, we noticed that random sampling led\nto some categories being severely under-sampled, below 5%\nof tweets. Thus we employed a bootstrapping technique to\npre-classify tweets by category using our current model, and\nchoose tweets with more likely emotion tweets for classi-\nﬁcation. See Active Learning section for details. We also\nsampled 5,000 tweets balanced by source category, since\nvideo game tweets have much more emotion, thus domi-\nnated the bootstrapped selections.\nHenceforth, we refer to the combined company tweets\ndataset consisting of:\n• 4,021 random tweets\n• 5,024 tweets selected for higher emotion content\n• 4,281 tweets selected for source category balance\nTable 3: Inter-rater agreement. Humans don’t always agree,\neven on binary sentiment.\nDataset Judgments Binary Plutchik\n(3 choices) (8 choices)\nSemEval 20,514 77.3% 61.1%\nCompany (random) 20,005 80.7% 67.3%\nCompany (active) 25,017 79.0% 52.3%\nCompany (balanced) 23,812 80.0% 71.0%\nFinetuning Recent work has shown promising results us-\ning unsupervised language modeling, followed by transfer\nlearning to natural language tasks (Radford, J´ozefowicz, and\nSutskever 2017), (Radford et al. 2018). Furthermore, these\nmodels beneﬁt when the entire model is ﬁne-tuned on the\ntransfer task, as demonstrated in (Howard and Ruder 2018).\nSpeciﬁcally, these methods have beaten the state of the art\non binary sentiment classiﬁcation. These models have also\nattained the best overall score on the GLUE Benchmark 4\n(Wang et al. 2018), comprised of a variety of text under-\nstanding tasks, including entailment and question answer-\ning.\n3https://www.ﬁgure-eight.com/\n4https://gluebenchmark.com/leaderboard\nMethodology\nWe use a larger batch size with shorter sequence length,\nspeciﬁcally a global batch of 512 and sequence length 64\ntokens (tokenized with a 32,000 BPE vocabulary, as de-\ntailed in Characters and Subword Units . The shorter se-\nquence length works well because the transfer target are\ntweets, which are short pieces of text. We trained our lan-\nguage model on the Amazon Reviews dataset (McAuley et\nal. 2015) rather than other large datasets like BooksCorpus\n(Zhu et al. 2015), because reviews are rich in emotional con-\ntext.\nWe also train an mLSTM network on the same dataset,\nbased on the model from (Puri et al. 2018).\nWe chose to compare these particular models because\nthey work in fundamentally different ways and because they\ncollectively hold state of the art results on many signiﬁcant\nacademic NLP benchmarks. We wanted to test these models\non difﬁcult classiﬁcation problems with real-world data.\nUnsupervised Pretraining. The language modeling ob-\njective can be summarized as a maximum likelihood estima-\ntion problem for a sequence of tokens. We treat our model\nas a function with two parts: an encoder fe and decoder fd.\nThe encoder forms the bulk of the model, including the to-\nken embedding dictionary as the ﬁrst module. The decoder\nis simply a softmax linear layer that projects the encoder\noutput into the dimension equal to the vocabulary size. The\nobjective to maximize is as follows.\n−log p(x0, . . . , xn) =−\nn∑\nt=1\nlog p(xt|xt−1, . . . , x0)\np(xt|xt−1, . . . , x0) =fd(hl\nt)\nwhere hl\nt is a hidden layer activation in the ﬁnal layer of fe,\nindexed 1 . . . lfor timestep t.\nThe model is tasked with predicting the next token given\nall of the ones prior by outputting a probability distribution\nover the vocabulary of tokens. Doing this for each timestept\nproduces each term in the sum of the log-likelihood formu-\nlation, and so maximizing the correct probabilities is a way\nto understand the joint probability distribution of sequences\nin this corpus of text.\nCharacters and Subword Units. While (Radford,\nJ´ozefowicz, and Sutskever 2017), (Gray, Radford, and\nKingma 2017) and (Puri et al. 2018) have shown state\nof the art results for language modeling and task transfer\nwith character-level mLSTM models, we found that our\nTable 4: Hyperparameters for language modeling and ﬁne-\ntuning phases.\nLanguage Modeling Finetuning\nglobal\nbatch size\n512 (size 64 on 8 GPUs) 32\nsequence\nlength\n64 - kept short because tar-\ngeting tweet application\nmax(batch)\noptimizer ADAM\nlr\n(schedule)\n2×10−4\n(cosine decay after linear\nwarmup on 2000 iterations)\n1×10−5\n(constant after 1/2 epoch lin-\near warmup)\nDecoder\nmodule\nRdh×32000 Binary:MLP(1024→nc)\nwith PReLU and 0.3 dropout\nMulticlass:MLP(4096→\n2048→1024→nc) with\nPReLU and 0.3 dropout\n# Epochs 1 5\nLoss LLM= Softmax Cross En-\ntropy\nSigmoid Binary Cross En-\ntropy+0.02·LLM\nTransformer model beneﬁts from modeling language\nthrough subword units. Using a byte-pair-encoding (BPE)\n(Sennrich, Haddow, and Birch 2015) of various sized we\nnotice that a 32,000 word-piece vocabulary achieves a\nbetter bits per character (BPC) loss over one epoch of the\nAmazon Reviews dataset (McAuley et al. 2015) than a small\nvocabulary. We compute the BPC equivalent over word\npieces, following (Mikolov et al. 2012). For the remainder\nof this work, our Transformer models use 32,000 word\npieces5.\nRecent work (Al-Rfou et al. 2018) has shown that it is\npossible to train a character level Transformer that is up to\n64 layers deep and which beat state of the art BPC over\nlarge text datasets. However this requires intermediate layer\nlosses, and other auxiliary losses for optimal convergence.\nBy comparison, (Radford et al. 2018) uses a bytepair en-\ncoding vocabulary with 40,000 word pieces for their state of\nthe art results on language transfer tasks with a Transformer\nmodel. Our work closely follows their model.\nSupervised Finetuning. After the pretraining, we initial-\nize a new decoder f†\nd to be exclusively trained on the super-\nvised problem. Depending on the task, this decoder may be a\nsingle linear layer with activation or an MLP. We also retain\nthe original decoder fd and continue to train it by using lan-\nguage modeling as an auxiliary loss when ﬁnetuning on the\nnew corpus. Error signals from both decoders are backprop-\nagated into the language model. The differences between the\nhyperparameters for ﬁnetuning and language modeling are\ndescribed in Table 4.\nELMo Baseline We also compare our language models to\nELMo (Peters et al. 2018), a contextualized word represen-\ntation based on a deep bidirectional language model, trained\non large text corpus. We use a publicly available pretrained\n5Library for BPE available in open source:\nhttps://github.com/google/sentencepiece\nELMo model from the authors. During ﬁnetuning, text is\nembedded with ELMo before being passed into a decoder\nf†\nd. Error signals are backpropagated into the ELMo lan-\nguage model. Unlike our other models, we do not use an\nauxiliary language modeling loss during ﬁnetuning, as the\nELMo language model is bidirectional.\nFinetuning the ELMo model substantially improves ac-\ncuracy on our tasks, thus we include only ﬁnetuned ELMo\nresults.\nMultihead vs. Single Head Finetuning Decoders The\ntweet datasets are an example of a multilabel classiﬁcation\nproblem. We can formulate the problem for the ﬁnetuning\ndecoder, f†\nd as either a collection of single binary problems\nor multiple problems put together.\nThe single binary problem formulation allows for a focus\non one class and end-to-end optimization will only have one\nerror signal. However, because the label classes are imbal-\nanced in all categories, this may lead to a sparse gradient\nsignal for the positive label, which may impact recall and\nprecision. Increasing the size of f†\nd to more than one linear\nlayer leads to rapid overﬁtting and lower validation perfor-\nmance.\nThe combined binary problems formulation (henceforth\ndescribed as multihead) allows for a richer error signal that\npropagates more information through the encoder fe and\nsentiment representation in f†\nd. In this setup, constructing\na Multilayer network is far more useful, and can be thought\nof as speciﬁcally creating sentiment features to be used at\nthe ﬁnal layer to predict the presence of the individual emo-\ntions. We ﬁnd that the inclusion of easier, more balanced la-\nbel categories improves performance on harder ones in Table\n7. However, the easier categories have slightly lower perfor-\nmance because the network is not being optimized for only\nthose categories.\nThresholding Supervised Results For both the multihead\nMLP and the single linear layer instantiating off†\nd, we found\nthat thresholding predictions produced noticeably better re-\nsults than using a ﬁxed threshold value such as t∗ = 0.5.\nThis makes sense since the label classes for most categories\nare very imbalanced. For thresholding, we take a dataset of\ntweets and split it into training (70%), thresholding (10%)\nand validation (20%) sets. At each epoch of ﬁnetuning on\nthe training set, we calculate validation accuracy and save\npredictions on the threshold set on the epoch for which this\nis maximized.\nTo threshold, we search the discretized version of [0, 1]:\nthe linear space T = { i\n200 : 1 ≤i ≤200}for the positive\nlabel threshold for each category. We denoted the threshold\nwhich gave the best score on the threshold set as t∗.\nIBM Watson and Google NLP 6 both offer commercial\nAPIs for binary sentiment analysis, producing scalar val-\nues that correspond to a continuous [-1,+1] sentiment score.\nWe applied our thresholding procedure to these scores. In\n6https://cloud.google.com/natural-language/\nTable 5: Binary sentiment accuracy. The SST dataset includes Positive and Negative labels. Other datasets include Neutral\nlabels. Third party results (Watson and Google) thresholded on the test set.\nSST (acc) Company -/=/+\nTransformer (ﬁnetune) 90.9% 81.2% 88.2 /73.5/81.9\nmLSTM (ﬁnetune) 90.4% 78.2% 87.0/69.3/78.3\n8k mLSTM(Puri et al. 2018) 93.8% 77.3% 86.0/67.4/78.6\n(Gray, Radford, and Kingma 2017) 93.1% - -\nELMo (ﬁnetuned) 79.9% 71.4% 81.7/60.1/72.4\nELMo+BiLSTM+Attn (Wang et al. 2018) 91.6% - -\nWatson API 84.4% 56.7% 42.9/54.0/73.3\nGoogle Sentiment (GCL) API 81.3% 62.5% 69.6/54.0/63.8\nClass Balance 50.0/50.0 - 22.4/46.0/31.6\nthe case of classiﬁcation with neutrals we create two thresh-\nolds 0 < t∗\n1 < t∗\n2 < 1 which we individually optimized\njointly over T as well. With the ﬁnetuning procedure, we\nfound success with a decoder f†\nd = MLP(64, 2), whose two\noutput units ˆyp, ˆyn are probability estimates of the positive\nand negative labels yp, yn. These units both have sigmoid\nactivations, since we denote a neutral as yp = yn = 0. To\nthreshold these predictions, we searched the cartesian prod-\nuct T ×T to determine 0 < t∗\np, t∗\nn < 1.\nActive Learning We hypothesized that we could achieve\ngreater precision and recall on our datasets if our class label\nwere more equally balanced. To this end, we employed an\nactive learning procedure to select unlabeled tweets to be la-\nbeled. The algorithm consisted of ﬁrst ﬁnetuning a language\nmodel f = (fe, fd, f†\nd) on labeled tweets for 5 epochs. At\npeak validation accuracy, we obtain predictionsP ∈R8×nu ,\nfor Plutchik sentiment on the unlabeled tweets.\nFrom the labeled dataset, we calculate the negative class\npercentage for each category v ∈R8. Then we obtain cat-\negory a weighting parameter w = 10 ×(v −0.5) so that\nwi ∈[−5, 5] for i ∈1 . . .8. Then, we get scores for each\nunlabeled point as weighted features: s = ew⊤P ∈Rnu .\nThis way, positive predictions for sentiment categories are\nweighted by how much they would contribute towards bal-\nancing all of the class distributions. The scores s are used\nas weights in a weighted uniform random sampler, and from\nthis, we sampled 5,000 tweets to be labeled.\nWe found that overall, the method produced tweets with\nmore emotion. Not only was the positive class balance av-\neraged across label categories higher (11.2% compared to\n8.2% for random sampling), but the percentage of tweets\nwhich had no emotion was dramatically lower: 35.6% com-\npared to 52.1% for random sampling (Table 2). We hence\nachieved better class balance than the dataset prior to the\naugmentation.\nResults\nBinary Sentiment Tweets\nFor binary sentiment, we compare our model on two tasks:\nthe academic SST dataset, which consists of a balanced set\nof Positive and Negative labels, and the company tweets\ndataset, which consists of a balance between Positive, Neu-\ntral and Negative labels. See Table 5.\nWhile the Transformer gets close but does not exceed the\nstate of the art on the SST dataset, it exceeds both the mL-\nSTM and ELMo baseline as well as both Watson and Google\nSentiment APIs on the company tweets. This is despite op-\ntimally calibrating the API results on the test set.\nMulti-Label Emotion Tweets\nThe IBM Watson API offers multi-label emotion predictions\nfor ﬁve categories: Anger, Disgust, Fear, Joy and Sadness.\nWe compare our models to Watson on these categories for\nboth the SemEval dataset and the company tweets in Table 7.\nWe ﬁnd that our models outperform Watson on every emo-\ntion category.\nSemEval Tweets We submitted our ﬁnetuned Transformer\nmodel to the SemEval Task1:E-C challenge, as seen in Ta-\nble 6. These results were computed by the organizers on\na golden test set, for which we do not have access to the\ntruth labels. Our model achieved the top macro-averaged\nF1 score among all submission, with competitive but lower\nscores for the micro-average F1 an the Jaccard Index accu-\nracy 8. This suggests that our model out-performs the other\ntop submission on rare and difﬁcult categories, since macro-\naverage weighs performance on all classes equally, and the\nmost common categories of Joy, Anger, Disgust and Opti-\nmism get relatively higher F1 scores across all models.\n8SemEval 2018 results can be seen at\nhttp://alt.qcri.org/semeval2018/. Our entry is #1 in the post-\nevaluation period for Task1:E-C, as of October 2018.\nTable 6: Comparison on SemEval Task 1:E-c challenge. Of-\nﬁcial results on the golden test set [truth labels hidden]. 7\nAccuracy Micro F1 Macro F1(Jaccard)\nTransformer (ours) 0.577 0.690 0.561\n(Baziotis et al. 2018) 0.595 0.709 0.542\n(Meisheri and Dey 2018) 0.582 0.694 0.534\nTable 7: Transformer vs. mLSTM on Plutchik Tweet Categories (F1 Score). MH: Multi Head, SH: Single Head\nAnger Anticipation Disgust Fear Joy Sadness Surprise Trust Average\nCompany\nTransformer (MH) .684 .486 .441 .400 .634 .333 .269 .300 .443\nTransformer (SH) .679 .491 .371 .400 .675 .286 .210 .279 .424\nmLSTM (SH) .636 .426 .319 .232 .609 .260 .201 .284 .371\nELMo (MH) .515 .306 .325 .086 .489 .182 .161 .182 .281\nWatson .358 - .179 .086 .520 .096 - - -\nSemeval\nTransformer (MH) .779 .413 .769 .723 .850 .712 .360 .240 .606\nTransformer (SH) .774 .425 .765 .735 .832 .699 .373 .247 .606\nmLSTM (SH) .668 .189 .691 .535 .763 .557 .103 .000 .438\nELMo (MH) .506 .215 .351 .172 .540 .348 .164 .239 .317\nWatson .498 - .331 .149 .684 .359 - - -\nWe also compare the deep learning architectures of the\nTransformer and mLSTM on this dataset in Table 7 and\nﬁnd that the Transformer outperforms the mLSTM across\nPlutchik categories.\nThe winner of the Task1:E-c challenge (Baziotis et al.\n2018) trained a bidirectional LSTM with an 800,000 word\nembedding vocabulary derived from training word vectors\n(Mikolov et al. 2013) on a dataset of 550 million tweets.\nSimilarly, the second place winner of the SemEval leader-\nboard trained a word-level bidirectional LSTM with atten-\ntion, as well as including non-deep learning features into\ntheir ensemble (Meisheri and Dey 2018). Both submissions\nused training data across SemEval tasks, as well as addi-\ntional training data outside of the training set.\nIn comparison, we demonstrate that ﬁnetuning can be as\neffective on this task, despite training only on 7,000 tweets.\nFurthermore, out language modeling took place on the Ama-\nzon Reviews dataset, which does not contain emoji, hashtags\nor usernames. We would expect to see improvements if our\nunsupervised dataset contained emoji, for example.\nPlutchik on Company Tweets Our models gets lower F1\nscores on the company tweets dataset than on equivalent Se-\nmEval categories. As with the SemEval challenge tweets,\nthe Transformer outperformed the mLSTM. These results\nare shown in Table 7. Both models performed signiﬁcantly\nbetter than the Watson API on all categories for which Wat-\nson supplies predictions.\nWe could not conclusively determine whether the single-\nhead or the multihead Transformer will perform better on a\ngiven task. Thus we recommend trying both methods on a\nnew dataset.\nAnalysis\nClassiﬁcation Performance by Dataset Size We would\nhave liked to label more data for the company tweets dataset,\nand thus looked into how much extra labeling contributes to\nﬁnetuned model performance accuracy.\nFirst, let us explain the difference between micro and\nmacro averaging of the F1 scores. We can summarize the F1\nscores of categories c ∈C (or any other metric M) through\nmacro and micro averaging to obtainM. The macro method\nweights each class equally by averaging the metric calcu-\nlated on each individual class. The micro method accounts\nfor the class imbalances in each category by aggregating all\nof the true/false positives/negatives ﬁrst, and then calculat-\ning an overall metric.\nMmacro = 1\n|C|\n∑\nc ∈C\nM(TPc, TNc, FPc, FNc)\nMmicro = M(TP, TN, FP, FN )\nTP =\n∑\nc ∈C\nTPc, TN =\n∑\nc ∈C\nTNc . . .\nIn one experiment, we decreased the size of the training\ndataset and observed the resulting macro and micro averaged\nF1 scores across all categories on company tweets. The re-\nsults are shown in Fig. 2a. We observe that the macro aver-\nage is more sensitive to dataset size and falls more quickly\nthan the micro average. The interpretation of this is that cat-\negories with worse class imbalance (which consequently in-\nﬂuence macro more than micro average) beneﬁt more from\nhaving a larger training dataset size. This suggests that we\nmay obtain substantially better results with more data in the\nharder categories.\nWe conducted a related experiment that focused on the\ndifference in category performance when using a single head\nversus a multihead decoder f†\nd. We apply the two architec-\ntures at different training dataset sizes for three different la-\nbel categories: Anger, Anticipation and Trust, which we cat-\negorize as low, mediumand high difﬁculty, respectively. As\nseen in Fig. 2b it appears that the difference between the\nsingle and multihead becomes more pronounced for more\ndifﬁcult categories, as well as for smaller dataset sizes.\nWe do not have enough data to make a ﬁrm conclusion,\nbut this study suggests that we could get more out of the la-\nbeled data that we have, by studying which categories bene-\nﬁt from single head and multihead decoders. All categories\nbeneﬁt from more training data, but some categories beneﬁt\nfrom from marginal labeled data than others. This suggests\nfurther and more rigorous study of the boostrapping meth-\nods we used to select tweets for our human labeling budget,\nas described in the Active Learning section.\n(a)\n (b)\nFigure 2: a) Comparison of macro and micro averages of F1 scores across categories on the company tweets dataset. b) F1\nScores for different categories on different dataset sizes for single head vs. multihead decoder.\nDataset Quality and Human Rater Agreement The Se-\nmEval dataset (Mohammad et al. 2018) applies a positive\nlabel for every category where 2 out of 7 vetted raters agree.\nThe reason is for the dataset to contain difﬁcult and subtle\nexamples of sentiments, not just those examples where ev-\neryone agrees. The raters also have a tendency to under-label\ncategories, especially when presented multiple options.\nFollowing a similar process, we required 2 out of 5 raters\nfor a positive label, and in the case of binary sentiment la-\nbels (Positive, Neutral, Negative), we rounded toward polar-\nized sentiment and away from Neutral labels in the case of\na 2/3 split. Applying the SemEval-trained Transformer di-\nrectly to our company tweets dataset gets reasonably good\nresults (0.338 macro average), also validating that our label-\ning technique is similar to that of SemEval.\nLooking at rater agreement by dataset (Fig. 3), we see that\nPlutchik category labels contain large rater disagreement,\neven among vetted raters who passed the golden set test. Fur-\nthermore, datasets with more emotions (the SemEval dataset\nand our active learning sampled company tweets) contain\nhigher Plutchik disagreement than random company tweets.\nThis is likely because raters tend to apply the ”No Emo-\ntion” label when they are not sure about a category. As Table\n2 shows, the SemEval and active company tweets datasets\ncontain fewer no-emotion tweets than other datsets.\nIt would be interesting to analyze rater disagreement\nby category, how much this effects classiﬁer convergence,\nwhether getting 7+ ratings per tweet helps classiﬁer conver-\ngence, and also whether this work could beneﬁt from esti-\nmating rater quality via agreement with the crowd, as pro-\nposed in (Khetan, Lipton, and Anandkumar 2017). However\nthis analysis is not straightforward, as the truth data is itself\ncollected through human labeling.\nAlongside classiﬁer convergence by dataset size (Fig.2b),\nwe think that this could be an interesting area a future re-\nsearch.\nDifﬁcult tweets and challenging contexts. There is not\nsufﬁcient space for a thorough analysis, but we wanted to\nsuggest why general purpose APIs may not work well on our\ncompany tweets dataset. Table 1 samples the largest binary\nsentiment disagreements between human raters and the Wat-\nson API. For simplicity, we restrict examples to video game\ntweets, which comprise 19.1% of our test set. As we can see,\nall of these examples appear to ascribe negative emotion to\ngenerally negative terms which, in a video game context, do\nnot indicate negative sentiment.\nOur purpose is not to castigate the Watson or the GCL\nAPIs. Rather, we propose that it may not be possible to pro-\nvide context-independent emotion classiﬁcation scores that\nwork well across text contexts.\nIt may work better in practice, on some tasks, to train\na large unsupervised model and to use a small amount of\nlabeled data to ﬁnetune on the context present in the spe-\nciﬁc dataset. We would like to quantify this further in future\nwork.\nRecent work (Yang et al. 2017) shows that training an\nRNN with multiple softmax outputs leads to a much im-\nproved BPC on language modeling, especially for diverse\ndatasets and models with large vocabularies. This is because\nthe multiple softmaxes are able to capture a larger number\nof distinct contexts in the text than a single output.\nPerhaps our Transformer also captures the features rele-\nvant to a large number of distinct contexts, and the ﬁnetun-\ning is able to select the most signiﬁcant of these features,\nwhile ignoring those features that – while adding value in\ngeneral – are not appropriate in a video game setting.\nConclusion\nIn this work we demonstrate that unsupervised pretraining\nand ﬁnetuning provides a ﬂexible framework that is effec-\ntive for difﬁcult text classiﬁcation tasks. We noticed that the\nﬁnetuning was especially effective with the Transformer net-\nwork, when transferring to downstream tasks with noisy la-\nbels and specialized context.\nWe think that this framework makes it easy to customize\na text classiﬁcation model on niche tasks. Unsupervised lan-\nguage modeling can be done on general text datasets, and re-\nquires no labels. Meanwhile downstream task transfer works\nwell enough, even on small amounts of domain-speciﬁc la-\nbelled data, to be accessible to most academics and small\norganization.\nIt would be great to see this approach applied to a variety\nof practical text classiﬁcation problems, much as (Radford\net al. 2018) and (Devlin et al. 2018) have applied language\nmodeling and transfer to a variety of academic text under-\nstanding problems on the GLUE Benchmark.\nReferences\nAl-Rfou, R.; Choe, D.; Constant, N.; Guo, M.; and Jones,\nL. 2018. Character-level language modeling with deeper\nself-attention. CoRR abs/1808.044449.\nBaziotis, C.; Athanasiou, N.; Chronopoulou, A.; Kolovou,\nA.; Paraskevopoulos, G.; Ellinas, N.; Narayanan, S.; and\nPotamianos, A. 2018. NTUA-SLP at semeval-2018 task\n1: Predicting affective content in tweets with deep attentive\nrnns and transfer learning. CoRR abs/1804.06658.\nDai, A. M., and Le, Q. V . 2015. Semi-supervised sequence\nlearning. CoRR abs/1511.01432.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding.\nEkman, P. 2013. An argument for basic emotions.\nGray, S.; Radford, A.; and Kingma, D. P. 2017. Gpu kernels\nfor block-sparse weights.\nHoward, J., and Ruder, S. 2018. Fine-tuned language models\nfor text classiﬁcation. CoRR abs/1801.06146.\nKhetan, A.; Lipton, Z. C.; and Anandkumar, A. 2017. Learn-\ning from noisy singly-labeled data. CoRR abs/1712.04577.\nKrause, B.; Lu, L.; Murray, I.; and Renals, S. 2016.\nMultiplicative LSTM for sequence modelling. CoRR\nabs/1609.07959.\nMcAuley, J.; Targett, C.; Shi, Q.; and van den Hengel, A.\n2015. Image-based recommendations on styles and substi-\ntutes. SIGIR.\nMeisheri, H., and Dey, L. 2018. Tcs research at semeval-\n2018 task 1: Learning robust representations using multi-\nattention architecture. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, 291–299. Associ-\nation for Computational Linguistics.\nMikolov, T.; Sutskever, I.; Deoras, A.; Le, H.-S.; Kombrink,\nS.; and Cernocky, J. 2012. Subword language modeling with\nneural networks. Technical report, Unpublished Manuscript.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.; and Dean,\nJ. 2013. Distributed representations of words and phrases\nand their compositionality. CoRR abs/1310.4546.\nMohammad, S. M.; Bravo-Marquez, F.; Salameh, M.; and\nKiritchenko, S. 2018. Semeval-2018 Task 1: Affect in\ntweets. In Proceedings of International Workshop on Se-\nmantic Evaluation (SemEval-2018).\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized\nword representations. CoRR abs/1802.05365.\nPlutchik, R. 1979. Emotions: A general psychoevolutionary\ntheory. 1.\nPuri, R.; Kirby, R.; Yakovenko, N.; and Catanzaro, B. 2018.\nLarge scale language modeling: Converging on 40gb of text\nin four hours. CoRR abs/1808.01371.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nRadford, A.; J´ozefowicz, R.; and Sutskever, I. 2017. Learn-\ning to generate reviews and discovering sentiment. CoRR\nabs/1704.01444.\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma-\nchine translation of rare words with subword units. CoRR\nabs/1508.07909.\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A. Y .; and Potts, C. 2013. Recursive deep models\nfor semantic compositionality over a sentiment treebank. In\nProceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, 1631–1642. Stroudsburg,\nPA: Association for Computational Linguistics.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. CoRR abs/1706.03762.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2018. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. CoRR\nabs/1804.07461.\nYang, Z.; Dai, Z.; Salakhutdinov, R.; and Cohen, W. W.\n2017. Breaking the softmax bottleneck: A high-rank RNN\nlanguage model. CoRR abs/1711.03953.\nZhu, Y .; Kiros, R.; Zemel, R. S.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning books and\nmovies: Towards story-like visual explanations by watching\nmovies and reading books. CoRR abs/1506.06724.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8610113859176636
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6973491311073303
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6652353405952454
    },
    {
      "name": "SemEval",
      "score": 0.5996302962303162
    },
    {
      "name": "Natural language processing",
      "score": 0.5340449213981628
    },
    {
      "name": "Machine learning",
      "score": 0.5065587759017944
    },
    {
      "name": "Language model",
      "score": 0.5032314658164978
    },
    {
      "name": "Task (project management)",
      "score": 0.43512994050979614
    },
    {
      "name": "Emotion classification",
      "score": 0.4175848066806793
    },
    {
      "name": "Surprise",
      "score": 0.4167405366897583
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ]
}