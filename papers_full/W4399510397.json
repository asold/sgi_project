{
  "title": "Automating Patch Set Generation from Code Reviews Using Large Language Models",
  "url": "https://openalex.org/W4399510397",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5004815210",
      "name": "Md Tajmilur Rahman",
      "affiliations": [
        "Gannon University"
      ]
    },
    {
      "id": "https://openalex.org/A5101474357",
      "name": "Rahul Singh",
      "affiliations": [
        "Gannon University"
      ]
    },
    {
      "id": "https://openalex.org/A5102898897",
      "name": "Mir Yousuf Sultan",
      "affiliations": [
        "Gannon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4308643152",
    "https://openalex.org/W3161903544",
    "https://openalex.org/W2949805459"
  ],
  "abstract": "The advent of Large Language Models (LLMs) has revolutionized various domains of artificial intelligence, including the realm of software engineering. In this research, we evaluate the efficacy of pre-trained LLMs in replicating the tasks traditionally performed by developers in response to code review comments. We provide code contexts to five popular LLMs and obtain the suggested code-changes (patch sets) derived from real-world code-review comments. The performance of each model is meticulously assessed by comparing their generated patch sets against the historical data of human-generated patch-sets from the same repositories. This comparative analysis aims to determine the accuracy, relevance, and depth of the LLMs' feedback, thereby evaluating their readiness to support developers in responding to code-review comments. Novelty: This particular research area is still immature requiring a substantial amount of studies yet to be done. No prior research has compared the performance of existing Large Language Models (LLMs) in code-review comments. This in-progress study assesses current LLMs in code review and paves the way for future advancements in automated code quality assurance, reducing context-switching overhead due to interruptions from code change requests.",
  "full_text": "Automating Patch Set Generation from Code Review Comments\nUsing Large Language Models\nTajmilur Rahman, Rahul Singh, Mir Yousuf Sultan\n{rahman007,rahul041,mir002}@gannon.edu\nGannon University\nErie, PA, USA\nABSTRACT\nThe advent of Large Language Models (LLMs) has revolutionized\nvarious domains of artificial intelligence, including the realm of\nsoftware engineering. In this research, we evaluate the efficacy of\npre-trained LLMs in replicating the tasks traditionally performed\nby developers in response to code review comments. We provide\ncode contexts to five popular LLMs and obtain the suggested code-\nchanges (patch sets) derived from real-world code-review com-\nments. The performance of each model is meticulously assessed by\ncomparing their generated patch sets against the historical data of\nhuman-generated patch-sets from the same repositories. This com-\nparative analysis aims to determine the accuracy, relevance, and\ndepth of the LLMs’ feedback, thereby evaluating their readiness to\nsupport developers in responding to code-review comments. Nov-\nelty: This particular research area is still immature requiring a sub-\nstantial amount of studies yet to be done. No prior research has com-\npared the performance of existing Large Language Models (LLMs)\nin code-review comments. This in-progress study assesses current\nLLMs in code review and paves the way for future advancements\nin automated code quality assurance, reducing context-switching\noverhead due to interruptions from code change requests.\nCCS CONCEPTS\n• Software and its engineering→Automatic programming.\nKEYWORDS\nLarge Language Models, Automated Code Review, Software Engi-\nneering, Pull Requests, Code Quality.\nACM Reference Format:\nTajmilur Rahman, Rahul Singh, Mir Yousuf Sultan. 2024. Automating Patch\nSet Generation from Code Review Comments Using Large Language Models.\nIn Conference on AI Engineering Software Engineering for AI (CAIN 2024),\nApril 14–15, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 2 pages.\nhttps://doi.org/10.1145/3644815.3644981\n1 INTRODUCTION\nCode review stands as a critical quality assurance practice, ensuring\ncode correctness, maintainability, and adherence to coding stan-\ndards. Developers frequently transition to other tasks right after\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCAIN 2024, April 14–15, 2024, Lisbon, Portugal\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0591-5/24/04\nhttps://doi.org/10.1145/3644815.3644981\nfinalizing a pull request. This shift is driven by the demands of\nfast-paced development, where there is an increasing need for au-\ntomated assistance in the code review process. Frequent context\nswitches caused by pull-request review comments can slow down\ndevelopers. Recent advances in Artificial Intelligence, especially\nin Large Language Models (LLMs), present opportunities to auto-\nmate aspects of code review, enhancing the efficiency of human\nreviewers [7].\nThis research investigates the feasibility of employing pre-trained\nLLMs, such as OpenAI’s GPT-4, Google’s BARD, Microsoft’s Code-\nBERT, Deep Mind’s AlphaCode, and Salesforce’s CodeT5, and GitHub\nCopilot in the context of automating code review. The models were\ninitially designed for natural language processing tasks. They have\ndemonstrated remarkable capabilities in understanding and gen-\nerating human-like text. These capabilities can be leveraged to\nanalyze and critique code [1, 2]. By presenting these models with\ncode context and patch sets from real-world Pull Requests, we aim\nto explore the extent to which LLMs can replicate the nuanced\nevaluations typically performed by human reviewers.\nIn our approach, the performance of the LLMs will be rigorously\nassessed by comparing their understanding ability of code-review\ncomments and code-change suggestions against the historical data\nof human code reviews from existing repositories. Metrics such\nas accuracy, relevance, and actionability of feedback will serve as\nbenchmarks for evaluation [9].\n1.1 Research Questions\nOur study is structured around the following research questions\naimed at comprehending how various Language Models (LLMs)\nrespond to distinct code-review comments. The objective is to gain\ninsights into the diverse ways LLMs react to different types of code-\nreview feedback and compare the performance of today’s LLMs.\n(1) RQ1: How accurately can today’s LLMs interpret and under-\nstand code-review comments?\n(2) RQ2: Can today’s LLMs generate necessary code-changes\nbased on code-review comments?\n(3) RQ3: How acceptable are the generated code-changes to the\nhuman developers given the variations of contexts in differ-\nent software projects and what is the acceptance threshold?\n(4) RQ4: How much contextual information must be embedded\nwithin a code-review comment to achieve the minimum\nacceptance threshold?\n2 RELATED WORKS\nTufano et al. [8] partially automated the manual code review pro-\ncess to reduce the time spent by developers [4]. They created two\ndistinct deep-learning architectures. The first model learns code\narXiv:2406.04346v1  [cs.SE]  10 Apr 2024\nCAIN 2024, April 14–15, 2024, Lisbon, Portugal Tajmilur Rahman\nGit Repo\nApache-Kafka\nApache-Spark\nApache-Airflow\nFetch Pull Request\nData\nFetch Review\nComments\nFilter out pull-\nrequests without\nreview comments\nUnmatched\nmatched\nCompared code\nchanges\nLLM\nCode\nDeveloper\nCode Developer\nReview\nLLM O/P Code\nEvaluate\nFigure 1: Study Design\nchanges made by developers offering contributors a revised version\nof their code with recommended transformations before submission.\nThe second model assists reviewers by automatically implementing\ntheir comments expressed in natural language. While the perfor-\nmance of the models was promising, with the contributor-side\nmodel replicating code transformations for up to 16% of cases and\nthe reviewer-side model correctly implementing comments for up\nto 31% of cases, there are still plenty of areas for improvement that\nrequire further study.\nPalvannan and Brown [6] explored the integration of a bot named\n“SUGGESTION BOT” into the software development cycle to en-\nhance the efficiency of the peer code review process. The bot lever-\nages GitHub’s suggested changes functionality to automatically\nreview code, aiming to address challenges faced by developers with\nnon-comprehensive feedback and disruptive notifications from ex-\nisting bots.\nLi et al. [ 3] studied LLMs at a large scale that addresses the\ntime-consuming nature of manual code reviews in software devel-\nopment by proposing an automated approach. The study focuses on\nleveraging pre-training techniques and introduces a model called\n“CodeReviewer” which acts as a code reviewer. The results indicate\nthe effectiveness of CodeReviewer in automating various aspects\nof the code review process.\nCompared to the existing studies, our study aims to develop a\nmodel to generate code change suggestions in response to code-\nreview comments. Since code-review is a way of transferring system\nknowledge to the newer team members, our aim is not to intro-\nduce a code reviewer bot to eliminate human involvement from the\nentire code-review process. Instead, we would like to save the addi-\ntional time that it takes for a developer to address the code-review\ncomments.\n3 METHODOLOGY\nFigure 1 shows the study design at a glance. We started with down-\nloading 30K pull requests from three Apache projects: Kafka, Spark,\nand Airflow using GitHub APIs. We removed the pull requests\nthat didn’t have any review comments since we are interested in\ngenerating code changes based on code-review comments only.\nWe created various prompts to train five distinct language mod-\nels: GPT-4.0, CodeBERT, BARD, Copilot, and CodeT5. After stan-\ndardizing the prompt format across all models, the code, along with\ncorresponding comments and prompts, is individually submitted\nto each Language Model (LLM). The generated outputs are then\ncompared to existing code changes in repositories. If the suggested\ncode change review matches existing repository data by 80% or\nmore [5], it is automatically integrated into the evaluation phase.\nCode suggestions matched below the 80% threshold are directed to\ndevelopers for manual review. Based on developers’ feedback if a\npatch-set doesn’t pass then that is counted as a failing code sugges-\ntion. However, if human reviewers approve the code-suggestion\nas an acceptable change to address the corresponding code-review,\nit goes to the evaluation phase. In the evaluation phase, the LLM\nmodels are evaluated based on the number of code-suggestions\naccepted from a model.\nREFERENCES\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. 2020. Language models are few-shot learners. Advances in neural information\nprocessing systems 33 (2020), 1877–1901.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[3] Li et al. 2022. Automating code review activities by large-scale pre-training. In\nProceedings of the 30th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering . 1035–1047.\n[4] Shuai et al. 2023. CodeXGLUE: A Machine Learning Benchmark Dataset for Code\nUnderstanding and Generation. arXiv preprint arXiv:2305.06328 (2023).\n[5] Rabah Abdul Khalek. 2023. How to test the fairness of ML models The 80 percent\nrule to measure the disparate impact. (2023).\n[6] Nivishree Palvannan and Chris Brown. 2023. Suggestion Bot: Analyzing the\nImpact of Automated Suggested Changes on Code Reviews. arXiv preprint\narXiv:2305.06328 (2023).\n[7] J. Smith and L. Tan. 2020. Automated Code Review and the Future of Code Quality.\nJournal of Software Engineering Research 15(3) (2020), 112–119.\n[8] Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and Gabriele\nBavota. 2021. Towards automating code review activities. In 2021 IEEE/ACM 43rd\nInternational Conference on Software Engineering (ICSE) . IEEE, 163–174.\n[9] Wenxi Wang, Kaiyuan Wang, Mengshi Zhang, and Sarfraz Khurshid. 2019. Learn-\ning to optimize the alloy analyzer. In2019 12th IEEE Conference on Software Testing,\nValidation and Verification (ICST) . IEEE, 228–239.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6826931238174438
    },
    {
      "name": "Code review",
      "score": 0.6702131628990173
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6150562167167664
    },
    {
      "name": "Code (set theory)",
      "score": 0.6089472770690918
    },
    {
      "name": "Novelty",
      "score": 0.568561315536499
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.449318528175354
    },
    {
      "name": "Relevance (law)",
      "score": 0.4203588366508484
    },
    {
      "name": "Realm",
      "score": 0.4171903431415558
    },
    {
      "name": "Data science",
      "score": 0.3787725567817688
    },
    {
      "name": "Software quality",
      "score": 0.3764849901199341
    },
    {
      "name": "Software engineering",
      "score": 0.34085023403167725
    },
    {
      "name": "Software",
      "score": 0.2816199064254761
    },
    {
      "name": "Software development",
      "score": 0.24844053387641907
    },
    {
      "name": "Programming language",
      "score": 0.1958533525466919
    },
    {
      "name": "Psychology",
      "score": 0.10953563451766968
    },
    {
      "name": "Political science",
      "score": 0.10760879516601562
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I967637",
      "name": "Gannon University",
      "country": "US"
    }
  ],
  "cited_by": 1
}