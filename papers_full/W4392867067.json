{
  "title": "Application of generative language models to orthopaedic practice",
  "url": "https://openalex.org/W4392867067",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Caterson, Jessica",
      "affiliations": [
        "London School of Hygiene & Tropical Medicine"
      ]
    },
    {
      "id": null,
      "name": "Ambler, Olivia",
      "affiliations": [
        "Morriston Hospital"
      ]
    },
    {
      "id": null,
      "name": "Cereceda-Monteoliva, Nicholas",
      "affiliations": [
        "St Thomas' Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2602791941",
      "name": "Horner Matthew",
      "affiliations": [
        "University Hospital of Wales"
      ]
    },
    {
      "id": "https://openalex.org/A2140721585",
      "name": "Jones Andrew",
      "affiliations": [
        "University Hospital of Wales"
      ]
    },
    {
      "id": null,
      "name": "Poacher, Arwel Tomos",
      "affiliations": [
        "Cardiff University",
        "University Hospital of Wales"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1980479734",
    "https://openalex.org/W2084986427",
    "https://openalex.org/W4328049700",
    "https://openalex.org/W4319227726",
    "https://openalex.org/W4316671929",
    "https://openalex.org/W4322725838",
    "https://openalex.org/W4319332969",
    "https://openalex.org/W4321366933",
    "https://openalex.org/W4319341091",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4319301505",
    "https://openalex.org/W2070204206",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4361000349",
    "https://openalex.org/W6792161730",
    "https://openalex.org/W4385374425",
    "https://openalex.org/W4315498620",
    "https://openalex.org/W4381480701"
  ],
  "abstract": "Objective To explore whether large language models (LLMs) Generated Pre-trained Transformer (GPT)-3 and ChatGPT can write clinical letters and predict management plans for common orthopaedic scenarios. Design Fifteen scenarios were generated and ChatGPT and GPT-3 prompted to write clinical letters and separately generate management plans for identical scenarios with plans removed. Main outcome measures Letters were assessed for readability using the Readable Tool. Accuracy of letters and management plans were assessed by three independent orthopaedic surgery clinicians. Results Both models generated complete letters for all scenarios after single prompting. Readability was compared using Flesch-Kincade Grade Level (ChatGPT: 8.77 (SD 0.918); GPT-3: 8.47 (SD 0.982)), Flesch Readability Ease (ChatGPT: 58.2 (SD 4.00); GPT-3: 59.3 (SD 6.98)), Simple Measure of Gobbledygook (SMOG) Index (ChatGPT: 11.6 (SD 0.755); GPT-3: 11.4 (SD 1.01)), and reach (ChatGPT: 81.2%; GPT-3: 80.3%). ChatGPT produced more accurate letters (8.7/10 (SD 0.60) vs 7.3/10 (SD 1.41), p=0.024) and management plans (7.9/10 (SD 0.63) vs 6.8/10 (SD 1.06), p&lt;0.001) than GPT-3. However, both LLMs sometimes omitted key information or added additional guidance which was at worst inaccurate. Conclusions This study shows that LLMs are effective for generation of clinical letters. With little prompting, they are readable and mostly accurate. However, they are not consistent, and include inappropriate omissions or insertions. Furthermore, management plans produced by LLMs are generic but often accurate. In the future, a healthcare specific language model trained on accurate and secure data could provide an excellent tool for increasing the efficiency of clinicians through summarisation of large volumes of data into a single clinical letter.",
  "full_text": "1\nCaterson J, et al. BMJ Open 2024;14:e076484. doi:10.1136/bmjopen-2023-076484\nOpen access  \nApplication of generative language \nmodels to orthopaedic\n \npractice\nJessica Caterson,1 Olivia Ambler,2 Nicholas Cereceda- Monteoliva,3 \nMatthew Horner,4,5 Andrew Jones,6 Arwel Tomos Poacher    4,7\nTo cite: Caterson J, Ambler O, \nCereceda-\n \nMonteoliva N, et al.  \nApplication of generative \nlanguage models to \northopaedic practice. BMJ Open \n2024;14:e076484. doi:10.1136/\nbmjopen-2023-076484\n ► Prepublication history \nand additional supplemental \nmaterial for this paper are \navailable online. To view these \nfiles, please visit the journal \nonline (https://doi.org/10.1136/\n \nbmjopen-2023-076484).\nReceived 08 June 2023\nAccepted 08 Januar\ny 2024\n1London School of Hygiene & \nTropical Medicine, London, UK\n2Plastic Surgery, Morriston \nHospital, Swansea, UK\n3Guy's and St Thomas' Hospitals \nNHS Trust, London, UK\n4Trauma Department, University \nHospital of Wales, Cardiff, UK\n5Trauma and Orthopaedic \nSurgery, University Hospital of \nWales, Cardiff, UK\n6Orthopaedic Surgery, University \nHospital of Wales, Cardiff, UK\n7School of Biosciences, Cardiff \nUniversity, Cardiff, UK\nCorrespondence to\nDr Arwel Tomos Poacher;  \n \ndrarwelpoacher@\n \ngmail.\n \ncom\nOriginal research\n© Author(s) (or their \nemployer(s)) 2024. Re-\n use \npermitted under CC BY\n-\n NC.\n No \ncommercial re-\n use.\n See rights \nand permissions. Published by \nBMJ.\nABSTRACT\nObjective T o explore whether large language models \n(LLMs) Generated Pre-\n trained \nTransformer (GPT)-\n 3 and \nCha\ntGPT can write clinical letters and predict management \nplans for common orthopaedic scenarios.\nDesign\n F\nifteen scenarios were generated and ChatGPT \nand GPT-\n 3 prompted to write c\nlinical letters and separately \ngenerate management plans for identical scenarios with \nplans removed.\nMain outcome measures\n Letters were assessed for \nreadability using the Readable \nTool. Accuracy of letters and \nmanagement plans were assessed by three independent \northopaedic surgery clinicians.\nResults\n Both models genera\nted complete letters for \nall scenarios after single prompting. Readability was \ncompared using Flesch-\n Kincade Grade Level (Cha\ntGPT: \n8.77 (SD 0.918); GPT-\n 3:\n 8.47 (SD 0.982)), Flesch \nReadability Ease (ChatGPT: 58.2 (SD 4.00); GPT-\n 3:\n 59.3 \n(SD 6.98)), Simple Measure of Gobbledygook (SMOG) Index \n(ChatGPT: 11.6 (SD 0.755); GPT-\n 3:\n 11.4 (SD 1.01)), and \nreach (ChatGPT: 81.2%; GPT-\n 3:\n 80.3%). ChatGPT produced \nmore accurate letters (8.7/10 (SD 0.60) vs 7.3/10 (SD \n1.41), p=0.024) and management plans (7.9/10 (SD 0.63) \nvs 6.8/10 (SD 1.06), p<0.001) than GPT-\n 3.\n However, \nboth LLMs sometimes omitted key information or added \nadditional guidance which was at worst inaccurate.\nConclusions\n This stud\ny shows that LLMs are effective \nfor generation of clinical letters. With little prompting, \nthey are readable and mostly accurate. However, they \nare not consistent, and include inappropriate omissions \nor insertions. Furthermore, management plans produced \nby LLMs are generic but often accurate. In the future, \na healthcare specific language model trained on \naccurate and secure data could provide an excellent \ntool for increasing the efficiency of clinicians through \nsummarisation of large volumes of data into a single \nclinical letter.\nINTRODUCTION\nAccurate and readable clinical letters are \nan essential part of orthopaedic practice. \nThe British Orthopaedic Association (BOA) \nhas issued guidance about writing clinical \nletters, advising the inclusion of likely diag-\nnosis, investigations and management plan \nincluding any risk/benefits of such plan.\n1 \nHowever, effective documentation can be \ntime-\n consuming, and with rising caseloads \nand increasing workplace pressures, this \nguidance is often not adhered to. One study \nof \nfracture clinical letters reported only \n26% contained relevant information, 2 and a \nlater re-\n audit found an improvement only to \n48%.3 With a record 730  000 people waiting \non trauma and orthopaedic waiting lists as \nof March 2022, the burden of orthopaedic \ndocumentation, is only set to grow larger.\n4\nOne possible solution to the problem of \nincreasing demand, is part-\n automation of\n \nthe writing process using artificial intelli-\ngence (AI) technology. Natural Language \nProcessing (NLP) offers a solution. NLP \nis a broad term encompassing multiple \nmethods including text auto-\n completion\n \nand summarisation of large quantities \nof text.\n5 In November 2022, the public \nrelease of ChatGPT by OpenAI was seen \nas part of a greater paradigm shift in the \ncapabilities of this technology.\n6\nChat-  Generated Pre-  trained Transformer \n(ChatGPT) is a super\nvised learning model \n(GPT-\n 4, previously GPT\n-\n 3), reinforced by\n \nhuman feedback. 6 This model is designed \nto generate text responses by predicting \nthe next word or sequence of words from \nSTRENGTHS AND LIMITATIONS OF THIS STUDY\n ⇒ This is the first study of its kind evaluating the use of \nlanguage models in orthopaedic practice.\n ⇒ We have evaluated the use of a variety of language \nmodels and made comparisons between them a \nnovel advancement in the utilisation of artificial in-\ntelligence models in clinical practice.\n ⇒ This study quantified readability and accuracy \nthrough the use of the validated ‘Readable’ tool \nrather than just using Likert scales.\n ⇒ This study is limited by the variation of clinical man-\nagement provided by the language models which \nwas mitigated using specialist consultants’ agree-\nment on a management plan for each case.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 14 March 2024. 10.1136/bmjopen-2023-076484 on BMJ Open: first published as \n2\nCaterson J, et al. BMJ Open 2024;14:e076484. doi:10.1136/bmjopen-2023-076484\nOpen access  \nthe input it receives. 6 GPT-  3 and GPT -  4 have wide -\nspread functionality, from text summarisation and \ngeneration, to answering questions. Additionally, \nChatGPT additionally has human-\n led fine-\n tuning for\n \ncertain tasks or domains 6 such as to have conversa-\ntions with the user, as a chatbot or virtual assistant. \nBoth are trained on a large data set accessed from a \ndiverse range of sources from the internet.\nThe potential applications of ChatGPT in medical \nsettings are vast and continue to be explored in \nacademia, medical education and clinical practice. \nChatGPT has been used to identify new system-\natic review prompts and write scientific papers and \ncase reports.\n7 8  In education its use as a study tool \nto explain complex concepts, design scenarios for \nteaching and create multiple choice questions has \nbeen investigated.\n9–11  Researchers have also asked \nChatGPT to give antimicrobial advice, 12 select \nappropriate imaging resources for breast pathology \npresentations\n13 and generate documentation such as \ndischarge summaries and clinical letters in simulated \nclinical settings.\n14 15\nHowever, a frequent concern has been its accu-\nracy, including responses that contain oversimpli-\nfied, incomplete or falsified information, that could \nresult in incorrect medical advice.\n8 11 12  For medical \nresearch, education and clinical practice, such inac-\ncuracies could have damaging consequences.\nThe aim of this study is to explore the use of large \nlanguage models (LLMs) in text summarisation and \ngeneration in common orthopaedic clinic scenarios, \nusing GPT-\n 3 and ChatGPT as examples which could\n \nbe applied in this context.\nMETHODS\nFour elective and 11 fracture clinical scenarios were \nsimulated by authors ATP, MH and AJ (table\n \n1). Core \ndetails, such as basic patient demographics, mechanism \nof injury, relevant medical history and social history, \ninvestigations, examinations and a management plan \nwere composed in clinical note format (see online \nsupplemental appendix 1 for full set of prompts and \nresponses).\nThe first response for each scenario was collected to \navoid selection bias, as responses are not consistent for \nrepeated identical prompts. Each response was then \nassessed for readability and accuracy.\nReadability was assessed with the online tool ‘Read-\nable’.\n16 This tool, which has been used previously in \nsimilar readability studies, 17 18 provides validated metrics \nsuch as Flesch-\n Kincaid Grade Level 19, Flesch Reading \nEase,19 Simple Measure of Gobbledygook (SMOG) \nIndex20 and reach.21\nAccuracy was assessed by three independent senior \northopaedic clinicians using a Likert scale rating from 0 \nto 10; 0 indicating a completely inaccurate letter, and 10 \nbeing completely accurate. Outputs were also analysed \nfor general tone, and any omissions and insertions noted \nby the clinicians and authors JC and ATP. An additional \nanalysis was conducted assessing the ability of ChatGPT \nand GPT-\n 3 to create appropriate management plans for \neach \ncase. For each case, the following prompt was used: \n‘Write an appropriate management plan for the following \npatient seen in an orthopaedic clinic based on the infor -\nmation provided:’. Again, management plans were rated \nfor accuracy by three independent senior orthopaedic \nclinicians using the same Likert scale.\nTable 1 Common orthopaedic scenarios selected as pr ompts for clinical letters, covering both elective and fracture clinical \nsettings\nCategory Case vignette\nElective case Hip arthritis – total hip replacement.\nKnee arthritis – non-\n operative tr\neatment.\nCarpal tunnel syndrome – surgical decompression.\nLumbar disc prolapse – nerve root block.\nFracture clinic Distal radial fracture – non-\n operative.\nClavicle fractur\ne – non-\n operative.\nPr\noximal humerus fracture – surgical fixation.\nOlecranon fracture – surgical fixation.\nMidshaft ulna fracture – non-\n operative.\nBiceps ruptur\ne – for further imaging.\nBuckle fracture – discharge.\nQuadriceps rupture – for further imaging.\nMCL rupture – non-\n operative.\nW\neber A ankle fracture – discharge.\nBimalleolar ankle fracture – surgical fixation.\nChatGPT and GPT- 3 wer e then prompted to write clinical letters with the following prompts, for elective and fracture clinical cases, \nrespectively: ‘Write an outpatient clinic letter for the following patient to the patient and their GP:’ and ‘Write a letter to the patient and their \nGP about the following:’.\nGP , general practitioner; GPT, Generated Pre-\n trained T\nransformer; MCL, medial collateral ligament.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 14 March 2024. 10.1136/bmjopen-2023-076484 on BMJ Open: first published as \n3\nCaterson J, et al. BMJ Open 2024;14:e076484. doi:10.1136/bmjopen-2023-076484\nOpen access\nReadability and accuracy of the ChatGPT and GPT-  3 \nresponses were each compared with the original prompt \nusing the paired t-\n test for each of the listed metrics. Statis\n-\ntical significance was deemed as p<0.05. All statistical \nanalysis was conducted using R (V.4.2.2).\n22\nPublic and patient involvement\nPatients or the public were not involved in the design, \nor conduct, or reporting, or dissemination plans of our \nresearch given the nature of this study. Public and patient \ninvolvement was characterised as ‘none’.\nRESULTS\nChatGPT and GPT-\n 3 both created complete clinical letters \nwith a single prompt (see online supplemental appendix \n1 for all responses). Without any more specification than \nabove, the letters \nincluded blanks to fill in the patient’s \nname and clinician responsible for composing the letter. \nChatGPT also successfully wrote separate letters for the \npatient and general practitioner for single fracture clinic \nprompts.\nFor readability, the mean Flesch-\n Kincaid Grade Level \nwas 8.77 (SD 0.918) and 8.47 (SD 0.982) for ChatGPT \nand GPT\n-\n 3, respectively\n. This metric describes the approx-\nimate US school grade that would be expected to under -\nstand the letter, which in both prompts is equivalent to \nthe reading level expected of children aged 14–15. These \nscores equate to mean Flesch Readability Ease scores of \n58.2 (SD 4.00) and 59.3 (SD 6.98), respectively. SMOG \nIndex, which is a measure of the average number of years \nof education needed for someone to understand a piece \nof text, was greater for ChatGPT letters than GPT-\n 3 letters, \nwith mean index scores of 11.6 (SD 0.755) and 11.4 (SD \n1.01), respectively\n. GPT-\n 3 letters also had a higher mean \nreach than ChatGPT\n-\n 3 letters (81.2% vs 80.3%). Compar-\nison of scores using paired Student’\ns t-\n test showed no \nstatistically significant differences for any readability \nmetric assessed (\ntable\n \n2).\nThe quality of the written content from both ChatGPT \nand GPT-\n 3 was inconsistent. In some cases, the letters \nsummarised \nall content well, with good inference of some \nrelevant information. For example, it was able to infer \nthat some occupations were relevant as their work could \nbe impacted by their injury. However, in other cases, \nrelevant pieces of information were omitted, notably \nmedical histories in elective cases. Both language models \nconsistently added content unprompted. In some cases, \nthis was relevant and accurate, for example, instructing \na patient to follow nil by mouth instructions (assuming \nthey had been told these), and counselling another on \nthe risks of steroid use for muscular tendon rupture. \nHowever, sometimes these statements were inappro-\npriate, such as describing that a cast will be removed \nafter one\n week, \nwhen this would depend on a follow-\n up \nX-\n ray\n, and comments such as ‘Thank you for choosing \nour hospital’ and ‘return to my office’, which are not \napplicable in a UK setting. Both models were also incon-\nsistent in handling abbreviations used in the prompts, for \nexample ChatGPT described a ‘Web A’ fracture identical \nto the prompt, whereas GPT-\n 3 \nchanged this to ‘Weber \nA’. Conversely, ChatGPT changed ‘PMHx’ to medical \nhistory, whereas in some cases GPT-\n 3 did not. In some \ncases when abbreviations were unabbreviated inappropri\n-\nately, for example, a ‘high BMI’ was written as ‘we noted \nthat you have a large body habitus’, which may be consid-\nered inappropriate in a patient’s letter.\nThe management plans provided were generic and \nsometimes inaccurate. The mean accuracy score for \nChatGPT from three independent reviewers was 7.9/10 \n(SD 0.63) across all prompts, which was statistically signifi-\ncantly greater than for GPT-\n 3 (mean accuracy 6.8/10 (SD \n1.06, p<0.001)) (\nfigure\n \n1B).\nBoth ChatGPT and GPT-\n 3 were methodical in \nforma-\ntion of management plans, consistently including \npoints for pain management, physical therapy and \nlifestyle advice, as well as general advice of manage-\nment for the orthopaedic injuries. However, specific \nmanagement of the orthopaedic injury was generally \nunfocussed, often listing the most common conser -\nvative and surgical options, but failing to recognise \nthe management the prompt was clearly pointing \nto. For example, a severe case of carpal tunnel \nsyndrome requiring urgent decompression was first \nadvised to try a splint. There were also notable inac-\ncuracies, particularly duration of cast application, \nand in some cases outright incorrect suggestions, \nsuch as ‘[providing the] patient with written instruc-\ntions on how to change the cast’. However, in other \nTable 2 Mean r eadability scores for ChatGPT and GPT-  3 derived r esponses (SD=SD deviation) and p value for paired t-  test \ncomparing mean dif\nference. Statistical significance equates to p value <0.05\nMetric Mean ChatGPT response (SD) Mean GPT-  3 r esponse (SD) P value (significance)\nFlesch-\n Kincaid Grade Level 8.77 (0.918) 8.47 (0.982) 0.4023 (NS)\nFlesch Readability Ease 58.2 (4.00) 59.3 (6.98) 0.4861 (NS)\nSMOG Index 11.6 (0.755) 11.4 (1.01) 0.5870 (NS)\nGeneral Public Reach (%) 80.3 (4.20) 81.2 (5.78) 0.6218 (NS)\nChatGPT had a mean accuracy of 8.7/10 (SD 0.60) according to independent ratings from three senior orthopaedic clinicians. This compared \nwith a mean accuracy of 7.3/10 (SD 1.41) for GPT-\n 3 generated clinical letters. This dif\nference was statistically significant (p=0.024) (figure 1A).\nGPT, Generated Pre-\n trained T\nransformer; NS, not significant; SMOG, Simple Measure of Gobbledygook.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 14 March 2024. 10.1136/bmjopen-2023-076484 on BMJ Open: first published as \n4\nCaterson J, et al. BMJ Open 2024;14:e076484. doi:10.1136/bmjopen-2023-076484\nOpen access  \ncases, management plans were comprehensive and \ncomplete.\nDISCUSSION\nOur results demonstrate that currently available LLMs \nare capable of summarising text from clinician notes to \nreadable clinical letters. Generally, these letters required \nlimited prompting and had a readability score appro-\npriate for those educated to a level of 14 years and above. \nOur letters were less readable than previous studies.\n14 \nThis is likely because we did not include a specific \nreadability instruction in our prompts. In addition, the \nprompts contained medical jargon including acronyms \n(eg, ‘Web A’), which were frequently transferred to the \nletters produced by ChatGPT and GPT3. Such tech-\nnical language will be less readable to a general reader. \nHowever, if prompted, ChatGPT and GPT-\n 3 have both \ndemonstrated the ability to simplify written text, 14 and \nover time this can be standardised. Thus, while letters \nshould still be checked by clinicians before sending, \nLLMs could be a useful tool in summarising clinical notes \nstraight to readable patient clinical letters, with limited \nadditional time and resources needed.\nThe accuracy of the letters was inconsistent, with notable \nomissions and inappropriate insertions by both models. \nThis has been a recurring issue noted in multiple previous \nstudies.\n8 11 12 In our study, ChatGPT and GPT- 3 were able \nto infer information from the notes for some cases, but for \nothers, this nuance was overlooked. ChatGPT was more \naccurate overall for letters and management plans, which \nmay \nbe because it uses the more sophisticated GPT-\n 4, \nand \nit has been further refined with human reinforcement. \nBoth models added information to the letters. This could \nbe useful, such as including lifestyle advice on smoking \ncessation, weight loss, pain management and rehabili-\ntation following an injury without prompting. However, \nthe information added was not always correct or appro-\npriate. The addition of information, for example, when \ncasts will be changed or removed, despite the prompts \nincluding defined management plans, could prove unsafe \nif these language models were freely applied in real clin-\nical settings to give medical advice without appropriate \noversight from clinicians. Given that language models \ngenerate text by adding words or sequences of words with \nthe highest probability of following the prior text,\n23 the \nhighest probability text overall may not in every case be \nthe correct one. This is especially true when considering \na holistic approach to patient care.\nThe inaccuracies noted in this study may raise addi-\ntional concerns regarding the liability of the developer of \nthe LLM, and the clinician who uses the model to produce \na clinical letter.\n24 If incorrect medical advice is sent to the \npatient, this could potentially lead to harm. Attribution of \nthis harm may be disputed if the clinician did not include \nthe incorrect advice in their prompt, but signed off on \nthe final letter containing the incorrect information. \nWith the emergence of AI and automated processes, it is \nessential that, as well as improving the accuracy of these \nlanguage models, these possibilities are considered and \nappropriately addressed by legal regulations and guide-\nlines to ensure proper liability is set.\nUltimately, ChatGPT is not an approved medical \ndevice under the Health Insurance Portability and \nAccountability Act,\n24 25 and thus could not be used for \nFigure 1 Accuracy scor es for ChatGPT and GPT-  3 generate (A) letters and (B) management plans, independently scor ed by \nthree senior orthopaedic clinicians. Grey lines show paired prompts. Compared using paired t-\n test; *, p<0.05, ****, p<0.001. \nGPT\n, Generated Pre-\n trained T\nransformer.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 14 March 2024. 10.1136/bmjopen-2023-076484 on BMJ Open: first published as \n5\nCaterson J, et al. BMJ Open 2024;14:e076484. doi:10.1136/bmjopen-2023-076484\nOpen access\nthe purpose demonstrated. For a LLM to succeed in a \nclinical setting, it would need to be trained on robust \nmedical data, with appropriately qualified clinicians \nproviding feedback to ensure the accuracy of its \noutputs, and appropriately handle sensitive patient \ninformation.\nRobust, medical-\n specific data could improve the\n \naccuracy of the outputs provided. However, there \nremains a risk of biases from the training data trans-\nlating to the generated letters. Racial and sex biases \nwere reported for GPT-\n 3,26 although with the added \nhuman feedback loop, some of these were addressed \nfor ChatGPT. However, not all bias is as easy to detect, \nand there is an additional risk of an adversary inten-\ntionally introducing biases to favour or cause harm to \ncertain patient groups.\n27\nSenior clinician review and feedback of any LLM, \nas well as patient involvement in the development \nprocess is also essential to ensure implementation of \nany LLM is accurate, appropriate and accepted. Addi-\ntionally, clinicians should be provided with training \nto understand how to write prompts of a satisfactory \nquality, and to critically appraise any output gener -\nated. This may also assist with addressing any fears \naround ‘deskilling’ of clinicians; the LLM should aim \nto streamline the clinical letter writing process, but \nthe clinician should still know what is appropriate to \ninclude in the letter, and ensure the final letter is of \nthe expected standard.\nUnlike ChatGPT or GPT-\n 3, an LLM appropriate for\n \nclinical use would also need to have a secure database \nthat adheres to current data protection standards, \nand maintain the privacy of any patient who’s infor -\nmation is input to generate a clinical letter. ChatGPT \nhad a breach of privacy, where ~1.2% of user’s history \nor personal information could be accessed by other \nindividuals.\n28 If patient-  identifiable data were used,  \nthis would be a significant breach of confidentiality, 29 \nand could raise serious ethical and legal issues and \nhave widespread negative effects for the patient and \nthe healthcare provider using the technology. Patients \nwould also likely need to be appropriately informed \nof any use of an LLM, under the General Data Protec-\ntion Regulation.\n29 And, as part of its section on rights \non individual, be able to withdraw their data from \nany model, and have the right to an explanation for \nany automatic processes involving their data.\n30 31  It \nis possible that their informed consent may also be \nnecessary, given health is a protected characteristic \nunder the regulation.\n31 32  However, if any LLM is \ndemonstrated to be within the public’s interest, this \nmay not be necessary.\n31 32\nThis study compares 15 different orthopaedic \nscenarios in the elective and fracture clinical settings. \nWhile not entirely exhaustive, it provides a represen-\ntative sample of the types of commonly encountered \nscenarios which an LLM might be used to summarise \nclinical notes into patient letters. In doing so, it \nsufficiently demonstrates the utility of such an LLM \ntool in this clinical setting. This study focused on the \npotential of LLMs, and so the readability and accu-\nracy of the generated letters was not compared with \nletters written by actual clinicians for patients. Given \nthat there is scope for LLMs to aid in text summarisa-\ntion, if an appropriate LLM approved for clinical use \nwas generated, it would be important to compare this \nwith the current standard to ensure and understand \nany benefit that was attained.\nThe current strength of ChatGPT and language \nmodels lies within their creation of readable text, \nnot accurate text, and it is likely that the success of \nany LLM in a healthcare setting would be limited to \nthis. If the drawbacks of current LLMs and legal and \nethical issues could be addressed, it is clear that a \nhealthcare specific language model trained on accu-\nrate and secure data would provide an excellent tool \nfor increasing the efficiency of clinicians through \nusable summarisation of large volumes of data into a \nsingle clinical letter.\nFor fracture clinical documentation, such a tool \nwould likely prove beneficial, for this use. The possi-\nbility of converting clinical notes or dictations straight \nto highly readable letters by automation of the more \nrepetitive aspects, would be an attractive, time saving \nprospect. However, all clinical information should be \nspecified by the clinician in the initial prompt, and \nthe final letter checked for accuracy before sending \nany letter.\nTwitter Jessica Caterson @jess_caterson and Arwel Tomos Poacher @\narwelpoacher\nContributors\n JC:\n Study conceptualisation and design, data collection and analysis, \nfirst draft, edit and review. ATP: Study conceptualisation and design, data collection \nand analysis, first draft, edit, review, Guarantor. OA: First draft, edit and review. \nNC-\n M:\n review, data collection and data collection and analysis, first draft, edit and \nreview. MH: First draft, edit and review. AJ: First draft, edit and review.\nFunding\n The authors ha\nve not declared a specific grant for this research from any \nfunding agency in the public, commercial or not-\n for\n-\n profit sectors.\nCompeting interests\n None dec\nlared.\nPatient and public involvement\n P\natients and/or the public were not involved in \nthe design, or conduct, or reporting, or dissemination plans of this research.\nPatient consent for publication\n Not a\npplicable.\nEthics approval\n Not a\npplicable.\nProvenance and peer review\n Not commissioned; externally peer reviewed.\nData av\nailability statement\n Da\nta are available upon reasonable request.\nSupplemental material\n This content has been supplied by the author(s).\n It has \nnot been vetted by BMJ Publishing Group Limited (BMJ) and may not have been \npeer-\n reviewed.\n Any opinions or recommendations discussed are solely those \nof the author(s) and are not endorsed by BMJ. BMJ disclaims all liability and \nresponsibility arising from any reliance placed on the content. Where the content \nincludes any translated material, BMJ does not warrant the accuracy and reliability \nof the translations (including but not limited to local regulations, clinical guidelines, \nterminology, drug names and drug dosages), and is not responsible for any error \nand/or omissions arising from translation and adaptation or otherwise.\nOpen access\n This is an open access artic\nle distributed in accordance with the \nCreative Commons Attribution Non Commercial (CC BY-\n NC 4.0) license,\n which \npermits others to distribute, remix, adapt, build upon this work non-\n commercially\n, \nand license their derivative works on different terms, provided the original work is \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 14 March 2024. 10.1136/bmjopen-2023-076484 on BMJ Open: first published as \n6\nCaterson J, et al. BMJ Open 2024;14:e076484. doi:10.1136/bmjopen-2023-076484\nOpen access  \nproperly cited, appropriate credit is given, any changes made indicated, and the use \nis non-\n commercial.\n See: http://creativecommons.org/licenses/by-nc/4.0/.\nORCID iD\nArwel Tomos Poacher http://orcid.org/0000-0002-4200-4929\nREFERENCES\n 1  British Orthopaedic Association. Consultant advisory book. 2023. \nA\nvailable: https://www.boa.ac.uk/standards-guidance/consultant-\n \nadvisory-book.html [Accessed 13 Apr 2023].\n 2\n Hook SE, Banister GC, T\nopliss C, et al. Letters and notes in \northopaedic surgery. Ann R Coll Surg Engl 2006;88:292–6. \n 3\n Longworth A, Davies D, Amirfeyz R, \net al. Notes and Letters in \nOrthopaedic Surgery Revisited: Can Surgeons Change? Bulletin \n2010;92:86–8. \n 4\n British Orthopaedic Association. England and W\nales T&O Waiting \nTimes data for, March . 2022Available: https://www.boa.ac.uk/\n \nr\nesources/england-and-wales-t-o-waiting-times-data-for-march-\n \n2022.html#:~:text=Ther\ne [Accessed 13 Apr 2023].\n 5\n IBM. What is Natural Language Pr\nocessing? | IBM, Available: https://\nwww.ibm.com/uk-en/topics/natural-language-processing [Accessed \n13 Apr 2023].\n 6\n OpenAI. Intr\noducing ChatGPT, Available: https://openai.com/blog/\n \nchatgpt [Accessed 13 Apr 2023].\n 7\n Gupta R, Park JB, Bisht C, \net al. Expanding Cosmetic Plastic Surgery \nResearch With ChatGPT. Aesthetic Surgery Journal 2023;43:930–7. \n 8\n Manohar N, Prasad SS. Use of ChatGPT in Academic Publishing: \nA Rar\ne Case of Seronegative Systemic Lupus Erythematosus in A \nPatient With HIV Infection. Cureus \n 9\n Huh S. Ar\ne ChatGPT’s knowledge and interpretation ability \ncomparable to those of medical students in Korea for taking a \nparasitology examination?: a descriptive study. J Educ Eval Health \nProf 2023;20:1. \n 10\n Seney V\n, Desroches ML, Schuler MS. Using ChatGPT to Teach \nEnhanced Clinical Judgment in Nursing Education. Nurse Educ \n2023;48:124. \n 11\n Mogali SR. Initial impr\nessions of ChatGPT for anatomy education. \nAnat Sci Educ February 7, 2023. \n 12\n Howar\nd A, Hope W, Gerada A. ChatGPT and antimicrobial advice: \nthe end of the consulting infection doctor? The Lancet Infectious \nDiseases 2023;23:405–6. \n 13\n Rao A, Kim J, Kamineni M, \net al. Evaluating chatgpt as an adjunct for \nradiologic decision-\n \nmaking. Radiology and Imaging [Preprint]. \n 14\n Ali SR, Dobbs TD, Hutchings HA, \net al. Using ChatGPT to write \npatient clinic letters. The Lancet Digital Health 2023;5:e179–81. \n 15\n Patel SB, Lam K. ChatGPT\n: the future of discharge summaries? The \nLancet Digital Health 2023;5:e107–8. \n 16\n Readability scor\ne | Readability test | reading level Calculator | \nreadable. Available: https://readable.com/ [Accessed 13 Apr 2023].\n 17\n W\nang L-\n W\n, Miller MJ, Schmitt MR, et al. Assessing readability \nformula differences with written health information materials: \nApplication, results, and recommendations. Research in Social and \nAdministrative Pharmacy 2013;9:503–16. \n 18\n Burke V\n, Greenberg D, Commission on Adult Basic Education (U.S). \nAdult basic education: an interdisciplinary journal for adult literacy \neducators. Adult Basic Educ Lit J Commission on Adult Basic \nEducation 1990.\n 19\n Kincaid J, Fishbur\nne R, Rogers R, et al. Derivation Of New \nReadability Formulas (Automated Readability Index, Fog Count And \nFlesch Reading Ease Formula). Inst Simul Train\n 20\n McLaughlin G. SMOG grading–A new r\neadability formula in the \njournal of reading. 1969.\n 21\n What’\ns new in October 2019? Introducing Reach – Readable, \nAvailable: https://readable.com/blog/whats-new-in-october-2019/ \n[Accessed 13 Apr 2023].\n 22\n R Cor\ne Team. R: A language and environment for statistical \ncomputing. Vienna, Austria: R Foundation for Statistical Computing, \n2021.\n 23\n Ouyang L, W\nu J, Jiang X, et al. Training language models to follow \ninstructions with human feedback.\n 24\n Haupt CE, Marks M. AI-\n Generated Medical Advice-\n GPT and Beyond. \nJAMA\n 2023;329:1349–50. \n 25\n Harvey H, Pogose M. How to get ChatGPT r\negulatory approved as \na medical device, Available: https://www.hardianhealth.com/blog/\n \nhow-to-get-r\negulatory-approval-for-medical-large-language-models \n[Accessed 13 Apr 2023].\n 26\n Chiu K-\n L, Collins A, Alexander R. Detecting Hate Speech with GPT\n-\n 3.\n 27\n Zou A, W\nang Z, Kolter JZ, et al. Universal and Transferable \nAdversarial Attacks on Aligned Language Models.\n 28\n OpenAI. Mar\nch 20 Chatgpt outage: here’s what happened. Available: \nhttps://openai.com/blog/march-20-chatgpt-outage#technical-details \n[Accessed 13 Apr 2023].\n 29\n Art. 4 GDPR – Definitions - General Data Pr\notection Regulation \n(GDPR), Available: https://gdpr-info.eu/art-4-gdpr/ [Accessed 13 Apr \n2023].\n 30\n What is automated individual decision-\n making and pr\nofiling; 2018.\n 31\n Art. 22 GDPR - Automated individual decision-\n making, including \npr\nofiling - \n GDPR.\n eu, A\nvailable: https://gdpr.eu/article-22-automated-\n \nindividual-decision-making/ [Accessed 27 Nov 2022].\n 32\n What is valid consent?; In detail\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 14 March 2024. 10.1136/bmjopen-2023-076484 on BMJ Open: first published as ",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.9347435235977173
    },
    {
      "name": "Medicine",
      "score": 0.728516697883606
    },
    {
      "name": "Computer science",
      "score": 0.14343109726905823
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089966",
      "name": "London School of Hygiene & Tropical Medicine",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2801456533",
      "name": "Morriston Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1324925910",
      "name": "St Thomas' Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2800861664",
      "name": "University Hospital of Wales",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I79510175",
      "name": "Cardiff University",
      "country": "GB"
    }
  ],
  "cited_by": 19
}