{
  "title": "Evaluating the Utilities of Large Language Models in Single-cell Data Analysis",
  "url": "https://openalex.org/W4387526306",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099342772",
      "name": "Hongyu Zhao",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2102556518",
      "name": "Tianyu Liu",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2124917612",
      "name": "Ke-xing Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2115576035",
      "name": "Yuge Wang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A1984223526",
      "name": "Hongyu Li",
      "affiliations": [
        "Yale University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3013151148",
    "https://openalex.org/W2942678593",
    "https://openalex.org/W590063802",
    "https://openalex.org/W2514159238",
    "https://openalex.org/W2744040456",
    "https://openalex.org/W2797310920",
    "https://openalex.org/W4385572162",
    "https://openalex.org/W4362521490",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4366526178",
    "https://openalex.org/W6603094881",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W6600669965",
    "https://openalex.org/W6600018615",
    "https://openalex.org/W4225598893",
    "https://openalex.org/W6647548779",
    "https://openalex.org/W4385270787",
    "https://openalex.org/W6604662147",
    "https://openalex.org/W6784195601",
    "https://openalex.org/W6603321306",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W6600178264",
    "https://openalex.org/W6600007113",
    "https://openalex.org/W6601218445",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W4285175926",
    "https://openalex.org/W6633466687",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6827496332",
    "https://openalex.org/W613625395",
    "https://openalex.org/W3153732845",
    "https://openalex.org/W6853194184",
    "https://openalex.org/W4210583065",
    "https://openalex.org/W2069089843",
    "https://openalex.org/W2888113858",
    "https://openalex.org/W2727705276",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2540833380",
    "https://openalex.org/W2523246573",
    "https://openalex.org/W4311830298",
    "https://openalex.org/W4288257146",
    "https://openalex.org/W3139417289",
    "https://openalex.org/W4385955324",
    "https://openalex.org/W2096192437",
    "https://openalex.org/W2931036699",
    "https://openalex.org/W3210860076",
    "https://openalex.org/W3208940117",
    "https://openalex.org/W3106188259",
    "https://openalex.org/W4320719442",
    "https://openalex.org/W3003807737",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4376134478",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W4233698560",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W1492083415",
    "https://openalex.org/W4286425347",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4367628401",
    "https://openalex.org/W1963522244",
    "https://openalex.org/W4367602258",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4210881488",
    "https://openalex.org/W2739492614",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3147114346",
    "https://openalex.org/W4375947188",
    "https://openalex.org/W3117771127",
    "https://openalex.org/W4321855108",
    "https://openalex.org/W4321011699",
    "https://openalex.org/W2886347923",
    "https://openalex.org/W3091832721",
    "https://openalex.org/W4379958259",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W4289745500",
    "https://openalex.org/W3010683590",
    "https://openalex.org/W2763051133",
    "https://openalex.org/W4377865902",
    "https://openalex.org/W4384821687",
    "https://openalex.org/W3036137852",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4241787340",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4321605297",
    "https://openalex.org/W2638715417",
    "https://openalex.org/W4323697401",
    "https://openalex.org/W2792693509",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2561754210",
    "https://openalex.org/W2962681511",
    "https://openalex.org/W2050909725",
    "https://openalex.org/W2467604901",
    "https://openalex.org/W2799147145",
    "https://openalex.org/W2951506174",
    "https://openalex.org/W4316096017",
    "https://openalex.org/W4233578221",
    "https://openalex.org/W4361193485",
    "https://openalex.org/W4292121845",
    "https://openalex.org/W4378806473",
    "https://openalex.org/W3088401235",
    "https://openalex.org/W4389174580",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W4382022265",
    "https://openalex.org/W4378509498",
    "https://openalex.org/W2901677030",
    "https://openalex.org/W4319322519",
    "https://openalex.org/W2183781891",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W2069295116",
    "https://openalex.org/W2794764220",
    "https://openalex.org/W2971398276",
    "https://openalex.org/W4308028428",
    "https://openalex.org/W3002417351",
    "https://openalex.org/W2949177718",
    "https://openalex.org/W4382202847",
    "https://openalex.org/W2800392236",
    "https://openalex.org/W4283728503",
    "https://openalex.org/W4379879135",
    "https://openalex.org/W4295521014",
    "https://openalex.org/W4383473944",
    "https://openalex.org/W4319818513",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4312020017",
    "https://openalex.org/W2949067670",
    "https://openalex.org/W3199761064"
  ],
  "abstract": "<title>Abstract</title> Large Language Models (LLMs) have made significant strides in both industrial and scientific domains. In this paper, we evaluate the performance of LLMs in single-cell sequencing data analysis through comprehensive experiments across eight downstream tasks pertinent to single-cell data. By comparing seven different single-cell LLMs with task-specific methods, we found that single-cell LLMs may not consistently excel in all tasks than task-specific methods. However, the emergent abilities and the successful applications of cross-species/cross-modality transfer learning of LLMs are promising. In addition, we present a systematic evaluation of the effects of hyper-parameters, initial settings, and stability for training single-cell LLMs based on a proposed <bold>scEval</bold> framework, and provide guidelines for pre-training and fine-tuning. Our work summarizes the current state of single-cell LLMs, and points to their constraints and avenues for future developments.",
  "full_text": "Evaluating the Utilities of Large Language Models in\nSingle-cell Data Analysis\nHongyu Zhao \nYale University https://orcid.org/0000-0003-1195-9607\nTianyu Liu \nYale University\nKexing Li \nTsinghua University\nYuge Wang \nYale University\nHongyu Li \nYale University https://orcid.org/0000-0001-6525-9310\nAnalysis\nKeywords: single-cell data, large language model, deep learning, benchmark\nPosted Date: October 12th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3376641/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: There is NO Competing Interest.\nEvaluating the Utilities of Large Language\nModels in Single-cell Data Analysis\nTianyu Liu1,2, Kexing Li1,3, Yuge Wang1, Hongyu Li1,\nHongyu Zhao1,2*\n1Department of Biostatistics, Yale University, CT, USA.\n2Interdepartmental Program in Computational Biology &\nBioinformatics, Yale University, CT, USA.\n3School of Life Sciences, Tsinghua University, Beijing, China.\n*Corresponding author(s). E-mail(s): hongyu.zhao@yale.edu;\nContributing authors: tianyu.liu@yale.edu; kexing.li@yale.edu;\nyuge.wang@yale.edu; hongyu.li@yale.edu;\nAbstract\nLarge Language Models (LLMs) have made signiﬁcant strides in b oth industrial\nand scientiﬁc domains. In this paper, we evaluate the performanc e of LLMs in\nsingle-cell sequencing data analysis through comprehensive experiments across\neight downstream tasks pertinent to single-cell data. By comp aring seven diﬀer-\nent single-cell LLMs with task-speciﬁc methods, we found tha t single-cell LLMs\nmay not consistently excel in all tasks than task-speciﬁc me thods. However, the\nemergent abilities and the successful applications of cross-s pecies/cross-modality\ntransfer learning of LLMs are promising. In addition, we present a s ystematic\nevaluation of the eﬀects of hyper-parameters, initial setting s, and stability for\ntraining single-cell LLMs based on a proposed scEval framework, and provide\nguidelines for pre-training and ﬁne-tuning. Our work summarizes the current\nstate of single-cell LLMs, and points to their constraints an d avenues for future\ndevelopments.\nKeywords: single-cell data, large language model, deep learning, benchmark\n1\n1 Introduction\nSingle-cell sequencing technologies oﬀer high-throughout observation s into complex\nbiological systems at the cell level with multimodal data [\n1, 2]. They help elucidate\ndisease mechanisms and potential treatments [ 3–5]. In line with the central dogma of\nmolecular biology, these technologies enable the characterization of variou s molecules,\nsuch as DNA (e.g. scDNA-seq) [ 6], RNA (e.g. scRNA-seq) [ 7, 8], and proteins (e.g. Cite-\nseq) [ 9]. Furthermore, single-cell sequencing can facilitate epigeneti c studies, including\nchromatin accessibility (e.g. scATAC-seq) [ 10, 11] and methylation [ 12, 13]. These\ntechnologies have been rated as among the most impactful ones in recent y ears [ 14, 15].\nWith the development of single-cell technologies, expansive single -cell datasets have\nbeen collected, and they present challenges in analysis, integration , interpretation, and\ndownstream utilization. Similar to single-cell data, natural language pr ocessing (NLP)\nalso boasts extensive datasets, where pre-trained Large Language Models ( LLMs) have\nshown great success in addressing NLP tasks or multimodal tasks [ 16]. Numerous\nLLMs, including GPT-4 [ 17] and LLaMA [ 18], excel at diverse language-related tasks\nsuch as question answering and sentence generation, which has receiv ed widespread\nattention from both the AI community and society. Moreover, these LLMs s howcase\nimpressive performance in zero-shot learning, thereby enabling th em to address tasks\nbeyond their original training scope, such as solving mathematical probl ems [ 19].\nIndeed, there are notable parallels between studies based on single- cell sequenc-\ning data and those in NLP. Firstly, both have numerous downstream tasks that are\nintimately connected with the multimodal data [ 20, 21]. Secondly, both require con-\ntemplation of intra-data and inter-data relationships [ 22, 23]. Thirdly, both beneﬁt\nfrom high-quality databases [ 16, 24].\nWhile LLMs have seen marked success in the realms of DNA analysis [ 25] and\nbiomedical NLP [ 26, 27], their application in single-cell research remains largely\nuncharted. There is a limited number of robust pre-trained model s (known as single-\ncell LLMs) capable of managing multiple tasks in single-cell research. S ome single-cell\nLLMs focus on cell-type annotation or gene function prediction, includi ng scBERT\n[28], tGPT [ 29], CellLM [ 30], and Geneformer [ 31], while others aim to create a\nfoundation model in this area that can handle multiple tasks, includi ng scGPT [ 32],\nscFoundation [33] and SCimilarity [ 34]. Details of these models can be found in Appen-\ndices D and E. Furthermore, no studies to date have comprehensively evaluated the\nutility of these models and provide guidance for model training. Litt le has been done\nto compare NLP-focused LLMs with those used for single-cell research to gain insight\ninto emerging abilities and zero-shot (or few-shots) learning abilit ies of the latter.\nHere we present a framework for assessing various single-cell LLMs and tas ks\n(shown in Figure 1 (a)), termed Single-cell Large Language Model Evaluation\n(scEval), shown in Figure 1 (b). Using scEval, we identify critical parameters and\nstrategies for the ﬁne-tuning process of single-cell LLMs. We also ex amine the poten-\ntial emergent abilities of single-cell LLMs, substantiating that the l atter also possess\ndistinctive abilities. To help the audience better understand LLMs, we provide a glos-\nsary summary of common terms in Artiﬁcial Intelligence (AI) and Machine Le arning\n(ML) in Supplementary ﬁle 1.\n2\n.\n.\n.\nP=0.9\nP=0.05\nP=0.01\nB cell\nT cell\nNK cell\nCell\nDNA\nRNA\nProtein\n+\n+\nJoint space\nGene\nP=0.01\nP=0.99\nSensitive\nInsensitive\nG1 G2 G3\nG1 G2 G3\n0 ? ?\n2\n4\n7\nPerturb\nG1\nG1\nTF1\nTF2\nSimulate\nBase Real Data\nS1\nS2\nS3\nSettings\n+\nSpatial \nTranscriptomics\nSingle-cell \nTranscriptomics\nG1\nG2\nG3\nOverview of scLLMs\nPre-training\nEncoder Block\nDecoder Block\nE\nFine-tune Block\nFine-tune type1\nFine-tune \ntype2\nSub-task 1\nSub-task 2\nAn example of single-cell LLM\nLandscape of sc Eval\nPre-trained LLMs\nFine-tune model 1\nFine-tune model 2\nFine-tune model n\n.\n.\n.\nModels Settings\nZero-shot model 1\nZero-shot model m\n.\n.\n.\nHandli ng  Downstream tasks Evaluati on  Process\nOutput Evaluation\nInterpretation Guidel in e\na\nb\nEncoder Block\nDecoder Block\nHyper -parameters\n Dropout\n Loss weight\n Bins\n MR\n Epoch\n LR\n ECS \nInitial settings\n Zero-shot \n Layer freezing\n Pre-training\n Reconstruction\n Extra information\nLoss components\n Mask Loss\n Prob Loss\n GEPC Loss\n ECS Loss\nDesign ideas\n Pre-processing\n Architecture\n Corpus size\n Task-specific part\nFactors Affecting scLLMs c\nCell-type Annotation\nGene Function Prediction\nBatch Effect Correciton\nMulti-omics Integration Imputation\nPerturbation Prediction Gene Network Analysis Simulation\nFig. 1 : Overview of single-cell LLMs, landscape of scEval and factors aﬀecting s ingle-\ncell LLMs. (a): Overview of single-cell LLMs describing the typical s tructure of LLMs\nand general tasks for single-cell data analysis. The right two blocks re present two\ntypes of downstream tasks. Yellow block: Sub-task 1, including Ce ll-type Annotation\nand Gene Function Prediction (top to bottom). Blue block: Sub-task 2, including\nBatch Eﬀect Correction, Multi-omics Data Integration, Imputation (Fr om left to right,\ntop row), Perturbation Prediction, Gene Network Analysis, and Simulat ion (From\nleft to right, bottom row). (b): The landscape of scEval shows the work ﬂow of our\nsystematic evaluation analysis. (c): Factors which can aﬀect the perfor mance of single-\ncell LLMs. The known factors can be classiﬁed into four diﬀerent type s. Details of\nhyper-parameters can be found in Appendix A. Details of initial settings can be found\nin Appendix B.\n2 Results\nModel Open \nSource?\nBatch Effect \nCorrection\nCell-type \nAnnotation\nMulti-omics \nData \nIntegration\nImputation\nGene \nFunction \nPrediction\nPerturbation \nPrediction\nGene \nNetwork \nAnalysis\nSimulation\nscGPT \nGeneformer\nscBERT\nCellLM\ntGPT\nscFoundation\nSCimilarity\nFunctions from the original design\nMore versatile\nData for this table were collected until August 1st, 2023.\nFunctions added by scEval\n3\nTab. 1 : Criteria to consider when choosing a model based on the breadth of t asks\nand usability. The white blanks represent that selected models d o not ﬁll the criterion\nbecause they do not have such design for speciﬁc tasks. We do not evalu ate the models\nwhich are not open-source. Here the green circles represent functi ons implemented\nby the original model, and the purple circles represent fucntions added by scEval for\nevaluation. The top 3 methods are highlighted.\nOverview of our evaluations. We evaluated the performance of ﬁve open-source\nsingle-cell LLMs (scGPT, Geneformer, scBERT, CellLM and tGPT) by as sessing the\nperformance of LLM on eight tasks with 22 datasets. We did not evaluate all the\nmodels with all the datasets with the rationales of reasons provided in Supplementary\nﬁles 2 and 3. The tasks that can be performed for diﬀerent models as well as the\noverall ranks are summarized in Table\n1. We also compared their performance with\nstate-of-the-art (SOTA) task-speciﬁc methods. For each task, we dis cuss the eﬀect of\ndiﬀerent parameter settings on model performance and investigate the contribution\nof diﬀerent loss functions and initial settings by ablation tests. For t he emergent\nabilities, we consider the contribution of model size to the perfor mance of LLMs.\nFinally, we evaluate the stability and usability of diﬀerent single- cell LLMs and make\nrecommendations for preferred models. To ensure fairness, we ﬁxe d all models to be\nfrom versions before August 1, 2023.\nEvaluation based on cell-perspective tasks shows that single-cell L LMs\ncan reduce batch eﬀect and annotate cell types.\nBatch Eﬀect Correction. We considered scGPT, tGPT and ResPAN [ 35] for this\ntask. We also provide a detailed analysis of the inﬂuence of various hyp er-parameters\non the performance of scGPT in batch eﬀect correction. The descripti on of the eval-\nuation metric Sfinal can be found in Appendix C. As shown in Figure 2 (a), scGPT\noutperformed ResPAN in three of the nine datasets and outperformed tGP T in all\ndatasets for batch eﬀect correction, while ResPAN had an overall best cor rection. In\naddition, scGPT outperformed the scGPT full model overall, raisin g the issue of the\nneed for increasing the size of pre-training datasets. For the MHSP dat aset, batch\neﬀect correction using scGPT was worse than the raw dataset as reﬂecte d in the Sfinal .\nMoreover, scGPT had worse performance in reducing the batch eﬀect f or large-scale\ndatasets, as their biology conservation scores were lower than those of raw datasets\n(Extended Data Figure 1). Extended Data Figures 2 and 3 show the Uniform Manifold\nApproximation and Projection (UMAP) [ 36] plots for the raw data and the scGPT\nresults. We could still observe batch eﬀect in the output of scGPT on certain datasets.\nTherefore, single-cell LLMs do not show advantage in handling atlas-leve l datasets for\nthe batch eﬀect correction task.\nWe provide a detailed analysis of the impact of various hyper-parameter s on the\nperformance of scGPT in batch eﬀect correction based on Figure 2 (b) and Extended\nData Figure 4. A smaller learning rate tended to lead to better performance across\nall datasets. The optimal number of training epochs varied across dataset s, with a\nlarger number of epochs being beneﬁcial for most datasets. This resul t contradicts\nrecent research advocating for a single-epoch training approach [ 37], suggesting that\nthe optimal number of epochs might be context-dependent. Increasi ng the number\nof bins is generally associated with an increase in the ﬁnal score. The i mpact of the\n4\n0.4\n0.5\n0.6\n0.7\n0.8\n51 101 151 201 501 1001\nBins\nScore\n0.55\n0.60\n0.65\n0.70\n0.75\n1 5 10\nEpoch\nScore\n0.4\n0.6\n0.8\n5e−05 1e−04 1e-03 1e-02\nLR\nScore\nCellLM\nTOSICA\nscBERT\nscGPT\nGeneformer\nPancrm\nHumanPBMC\nCell Lines\nMCA\nDC\nPBMC368k\nImmue All Human\nMHSP\nMB spatial\nPancreas cross\nLiver cross\nDatasets\nMethods\n0.00\n0.25\n0.50\n0.75\nScore\nRanking\n5\n4\n3\n2\n1\n0.85\n0.81\n0.69\n0.5\n0.17\na\nb\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n1 5 10 50 100\nLoss Weight\n0.39\n0.4\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47\n0.1 0.3 0.4 0.5 0.7 0.9\nMR\n0.39\n0.4\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n1 5 10\nEpoch\nc d\ne\ng\ntraining by reward network\ncross−entro py\nno pretrain\nfreeze except decoder\nzeroshot\nmse loss\nfreeze except encoder\nSettings Score\nPancrm\nHumanPBMC\nCell Lines\nMCA\nDC\nPBMC368k\nImmue All Human\nMHSP\nMB spatial\nPancreas cross\nLiver cross\nAccuracy F1Precision Recall\nAll remove No mask loss No zero prob No gepc No ecs Default All remove No mask loss No zero prob No gepc No ecs Default All remove No mask loss No zero prob No gepc No ecs Default All remove No mask loss No zero prob No gepc No ecs Default\nScore\n0 0.2 0.4 0.8\n0.00\n0.25\n0.50\n0.75\n1.00\n5e−05 1e−04 1e-03 1e-02\nLR\nScore\n0.25\n0.50\n0.75\n1.00\n1 5 10\nEpoch\nScore\n0.25\n0.50\n0.75\n1.00\n0 0.2 0.4 0.6 0.8 1\nECS\nScore\nMethods\nGeneformer\nscBERT\nscGPT\nf\nPancrm\nHumanPBMC\nCell Lines\nMCA\nDC\nPBMC368k\nImmue All Human\nMHSP\nImmue atlas\nLung atlas\nHeart atlas\nPBMC Multiomics\nDataset Raw 0.71 scGPT  0.81 scGPT Full 0.47 tGPT 0.09 ResPAN 0.88\nScore\n0 0.2 0.4 0.8\nScore\nScore\nScore\nFig. 2 : Experimental results of single-cell LLMs and benchmarking method s for\ncell-level tasks. (a): An overall assessment of raw data and data after b atch eﬀect cor-\nrection based on diﬀerent methods. scGPT full represents scGPT model with larger\npre-training datasets comparing to scGPT. (b): The eﬀect of hyper- parameters includ-\ning Bins, Learning rate (LR), and Epoch for scGPT training in the Batch Eﬀect\nCorrection task. (c): Results of diﬀerent initial settings for the m ulti-omics data inte-\ngration task. (d): The eﬀect of hyper-parameters including loss weigh t, mask ratio,\nand epoch for scGPT training in the Multi-omics Data Integration task. (e): Compar-\nison among models in the Cell-type Annotation task. The scores on the lef t represent\nthe average accuracy of diﬀerent models across diﬀerent datasets. (f) : The eﬀect of\nhyper-parameters including LR, Epoch and ECS for scGPT training in the Cell-type\nAnnotation. LR and Epoch are shared hyper-parameters for Geneformer, scB ERT and\nscGPT. (g) Ablation tests of the loss function components for cell-type annotation.\nThe red component is signiﬁcant ( p − value < 0. 05).\n5\nmask ratio and dropout rate on model performance is unclear, suggesting th at further\ninvestigation is needed to understand their inﬂuence. These obse rvations may improve\nthe application of scGPT for batch eﬀect correction in single-cell data analysis and\nmay also inform ﬁne-tuning of other similar models.\nExtended Data Figure 5 (a) presents the comparison of scores across diﬀerent\ninitial settings for the batch eﬀect correction task using scGPT. We can see that\nscGPT is capable of performing zero-shot learning tasks. For the Cell Li nes dataset,\nthe zero-shot learning approach even achieved the highest score, indi cating that it\ncan be an eﬀective method for certain datasets. Moreover, pre-traini ng signiﬁcantly\ncontributes to the performance of scGPT in the batch eﬀect correcti on task. Without\npre-training, the model’s performance notably decreased. Using cros s-entropy as the\nloss function for gene expression reconstruction yielded better re sults than the mean\nsquare error (MSE) loss for most datasets. Freezing weights is not cruc ial for batch\neﬀect correction. Interestingly, the encoder structure appears to play a more important\nrole in the training process, as freezing the encoder layer led t o a larger decrease\nin score. Incorporating the cell type information as a human label into t he training\nprocess enhanced performance for most datasets. These observations su ggest that\neach dataset may require unique tuning, underscoring the importanc e of adaptable\nmethodologies in single-cell RNA-seq data analysis.\nExtended Data Figure 5 (b) shows the performance metrics versus the choices\nof diﬀerent optimizers for the batch eﬀect correction. Adam and AdamW [ 38] had\ncomparable performance in this task, while SGD [ 39], Sophia-G [ 40] (a novel optimizer\nwhich is designed for training LLMs) and Lion [ 41] (an optimized version of Adam)\nwere worse. Therefore, the optimizers in the Adam family are preferr ed.\nExtended Data Figure 5 (c) illustrates the impact of diﬀerent loss function compo-\nnents on the performance of batch eﬀect correction using scGPT. Using all components\nof the loss function did not always yield the best results, with the exceptions of the\nPancrm, MCA, and MHSP datasets. Using only the gradient reverse approach re sulted\nin the worst performance. The GEPC Loss seemed to play a crucial role i n the per-\nformance of the batch eﬀect correction task. These results suggest the need for a\ncareful composition of the loss function when training single-cell LLMs for batch eﬀect\ncorrection, with each loss function component contributing diﬀeren tly to the model\nperformance.\nMulti-omics Data Integration. For this task, we seek to integrate scRNA-\nseq datasets with scATAC-seq datasets. We assessed the integration q uality through\nthe same score as Batch Eﬀect Correction. The results presented in F igure 2 (c)\nsummarize the impact of initial setting choices on the performance of s cGPT for the\nmulti-omics integration task. As was the case with batch eﬀect correcti on, the cross-\nentropy loss function led to better performance compared to the MSE l oss for this\ntask. Interestingly, pre-training did not signiﬁcantly aﬀect the p erformance for this\ntask. The encoder part of the single-cell LLM played a more important rol e than the\ndecoder. Including cell types or human labels in the training pro cess proved beneﬁcial,\nlikely providing the model with more precise and useful informat ion for the task. The\nzero-shot learning approach did not perform as well for this task as it did f or batch\neﬀect correction.\n6\nWe illustrate the evaluation metrics versus diﬀerent parameter se ttings in Figure 2\n(d) and Extended Data Figure 6. scGPT did not perform well on this task as shown by\nthe low score (below 0.5). This is also shown in the UMAP results. Cer tain parameters\naﬀected the training process, with a smaller weight for the loss fun ction and mask\nratio, and more epochs, improved the model’s performance. Setting the learning rate\ntoo high caused the model to collapse.\nCell-type Annotation. We considered Geneformer, scGPT, scBERT, CellLM\nand TOSICA [ 23] for this task. We assessed the performance of diﬀerent single-cell\nLLMs in assigning cell types based on the four metrics discussed in App endix C.2.\nThe UMAPs for the raw data and scGPT are shown in Extended Data Figures 7 and\n8. We observed accurate annotation results on these UMAPs. Figure 2 (e) displays\nthe Accuracy for diﬀerent datasets for the ﬁve models. On average, mod els with pre-\ntraining performed better than those without pre-training. However , CellLM did not\nperform well across all the datasets. Moreover, for the intra-dataset pr ediction task,\nall the single-cell LLMs were comparable even if they had diﬀerent pre -training set-\ntings. For the inter-dataset prediction task, scGPT was worse than Gen eformer but\nbetter than scBERT. For cross-species cell type annotation, the per formance of Gene-\nformer, scGPT and scBERT were comparable for the MCA dataset, but scGP T did\nnot perform well for the MHSP and MB spatial datasets. Diﬀerent single-c ell LLMs\nalso had large divergences in performance. CellLM had running errors for Pancrm,\nHumanPBMC, PBMC368k, and Liver cross datasets, which raises the issue of t he\nreliability and usability of this model.\nIn Figure 2 (f) and Extended Data Figure 9, we compared the performance of\nmodels with diﬀerent hyper-parameter settings. Higher loss weight , learning rate, ECS\nthreshold, mask ratio and smaller epochs tended to lead to worse perf ormance of\nscGPT. There was little correlation between the number of bins and t he performance\nof scGPT. We observed the consistency in the performance of diﬀeren t single-cell\nLLMs under the condition of altering their shared hyper-parameters. F or Geneformer\nand scBERT, lower learning rate and higher epochs also tended to lead to better\nperformance.\nWe also considered diﬀerent initial settings for model training. Th e ﬁrst setting\nis Freeze all, where we froze all the weights of pre-trained layers. The second se tting\nis Default, where we used the default ﬁne-tuning settings. The third sett ing is From\nscartch, where we did not use the pre-trained weights. Extended Data Figur e 10 (a)\nshows the score versus initial settings across diﬀerent datasets. He re we considered\nscGPT and scBERT. We omitted Geneformer because it requires pre- training weights\nas input. It can be seen that pre-training always improved results for scGPT, especially\nfor the cross-dataset conditions. However, there was little beneﬁt of pre-training for\nscBERT. For both cases, freezing the pre-training layers and lett ing them not be\ninvolved in the ﬁne-tuning process was not recommended. In some cas es, the ﬁne-\ntuning performance of such freezing was worse than training from the scratch. Transfer\nlearning for diﬀerent species is possible because, for the MCA datas et, pre-training\nbased on human data can help predict cell types for the mouse. For the sam e type of\nGPU, the training process of scGPT was faster than scBERT and Geneform er, with\nmore GPU memory usage, according to Extended Data Figure 10 (b).\n7\nIn Extended Data Figure 11, we show the performance of scGPT based on diﬀerent\noptimizers across four datasets. Adam, AdamW, and Lion were comparable, while\nSGD was worse than them but better than Sophia-G, which was not stable.\nMoreover, we explored the contribution of diﬀerent loss function com ponents\ntowards the cell-type Annotation task, and the results are shown in Fi gure\n2 (g). Here\nwe included three extra metrics, and details can be found in Append ix C. Based on\nthe Accuracy of Figure 2 (g), the inclusion of mask loss is important. Moreover, the\ndefault setting is generally good across diﬀerent tasks. Based on prec ision and recall\nof Figure 2 (g), the eﬀect of diﬀerent loss function terms had less eﬀect on prec ision\nand more eﬀect on recall. Such diﬀerence could aﬀect the ﬁnal F1 score. Removing\nthe GEPC loss function terms improved the cell type prediction f or the DC, MHSP\nand MB Spatial datasets, and did not aﬀect the prediction performance for t he other\ndatasets.\nTherefore, single-cell LLMs can handle the Cell-type Annotation task wi th suit-\nable pre-training data and model structure, but one pre-training f ramework is not\nconsistently good across all the datasets.\nEvaluation based on gene-perspective tasks shows that single-cell LL Ms\ncan handle tasks related to functions of genes.\nGene Function Prediction. We considered Geneformer, scGPT and vanilla NN\nfor this task. The evaluation results with the same four metrics as the Cell-type Anno-\ntation task are shown in Figure 3 (a). On average, Geneformer and scGPT performed\nwell in this task, and there is a performance gap between single-cell LLMs and Vanilla\nNN.\nFigure 3 (b) and Extended Data Figure 12 show the accuracy of diﬀerent hyper-\nparameter settings. Smaller learning rate and loss weight tended to le ad more accurate\nresults. Geneformer was more sensitive to Epoch compared to scGPT . For scGPT, pre-\ntraining contributed more than ﬁne-tuning in this task as increasin g epochs did not\naﬀect the model performance. Only tuning the number of bins, mask r atio, dropout\nrate and ECS threshold did not aﬀect the prediction results.\nIn Extended Data Figure 13 (a), we considered diﬀerent initial settings for model\ntraining. It can be seen that pre-training always improved results for scGPT. Moreover,\nfreezing the whole model did not aﬀect the performance of scGPT.\nExtended Data Figure 13 (b) shows the performance of scGPT based on diﬀerent\noptimizers. Adam and AdamW were comparable, while Lion was worse than them bu t\nbetter than SGD and Sophia-G.\nExtended Data Figure 13 (c) shows the ablation test results of scGPT for this task.\nThere was no signiﬁcant diﬀerence by comparing the default setting and those without\ncertain components. Therefore, the task-speciﬁc loss function is t he most important\ndesign for this task.\nPerturbation Prediction. We considered scGPT and GEARS [ 42] for this task.\nIn the training process, we masked the genes under perturbation and tried to recon-\nstruct the expression levels of all the genes of the input cells rath er than only the\nmasked genes. We used Mean Pearson Correlation (MPC) as the metric to ev aluate\nthe performance of scGPT under diﬀerent hyper-parameters or initi al settings, and\ndetails can be found in Appendix C. The datasets include two perturbation conditions:\n8\nfreeze except encoder\nzeroshot\nfreeze except decoder\nno pretrain\nall default\nGEARS\nNorman\nAdamson\nDixit\nMethod\nMetrics\n0.4\n0.6\n0.8\nCorrelation\nRanking\n6\n5\n4\n3\n2\n1\n0.84\n0.77\n0.76\n0.69\n0.38\n0.34\nUMAP1\nUMAP2\nMarkers\nCD8+  T cells\nCD16+  Monocytes\nCD20+  B cells\nErythrocytes\nErythroid progenitors\nHSPCs\nMegakaryocyte progenitors\nMonocyte progenitors\nMonocyte-derived dendritic cells\nNK cells\nPlasma cells\nPlasmacytoid dendritic cells\nNA\na\nb\nc\nd\nUMAP1\nUMAP2\nleiden\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nDatasets\ne\nVanilla NN\nscGPT\nGeneformer\nT1\nT2\nT3\nFunctions\nMethods\n0.4\n0.6\n0.8\nScore\nRanking\n3\n2\n1\n0.91\n0.78\n0.54\n0.00\n0.25\n0.50\n0.75\n1.00\n5e−05 1e−04 1e-03 1e-02\nLR\nScore\n0.65\n0.70\n0.75\n0.80\n0.85\n1 5 10 50 100\nLoss weight\nScore\n0.8\n0.9\n1 5 10\nEpoch\nScore\nMethods\nGeneformer\nscGPT\nFig. 3 : Experimental results of single-cell LLMs and benchmarking method s for gene-\nlevel tasks. (a): Comparisions among Geneformer, scGPT and vanilla NN in the Gene\nFunction Prediction task. (b): The eﬀect of hyper-parameters incl uding Loss weight,\nBins and Learning rate for scGPT and Geneformer in the Gene Function Pr ediction\ntask. (c): Correlation of GEARs and scGPT under diﬀerent settings acros s diﬀerent\ndatasets. A higher correlation means lower rank and better performance. The numbers\ncorresponding to the settings represent the average value across two datasets. (d):\nDataset-level gene embeddings colored by the marker genes of diﬀerent cell types. (e):\nDataset-level gene embeddings colored by the Leiden cluster.\n9\na single-gene perturbation and a double-gene perturbation. Based on our e xperiments,\nscGPT could predict gene perturbation with lower MPC comparing wit h GEARS, but\nits default setting is the best design for its current structure .\nFigure 3 (c) summarizes results for diﬀerent initial settings of scGPT for t he\nNorman, Adamson and Dixit datasets. The default setting performed best f or these\ndatasets across diﬀerent settings. This indicates that the initial c onﬁguration of scGPT\nworks well for this task. The performance was comparable between traini ng from\nscratch and training from pre-trained weights. Freezing the decod er part of the model\nperformed better than freezing the encoder part. Interestingly , this option did not lead\nto a large increase in error, suggesting that the encoder carries a sign iﬁcant portion\nof the important information needed for the prediction task.\nRegarding the eﬀect of hyper-parameters, Extended Data Figure 14 shows that\nscGPT is sensitive to adjusting the learning rate and epochs. Dec reasing learning rate\nand increasing the number of epochs improved MPC. Increasing the dropout rate\ndecreased MPC. The rest of hyper-parameters did not contribute muc h in this task.\nExtended Data Figure 15 (a) shows that AdamW and Adam are comparable\nfor scGPT in perturbation prediction. The rest of the optimizers cou ld signiﬁcantly\nreduce the performance of scGPT. Moreover, based on the ablation tests shown in\nExtended Data Figure 15 (b), we found that diﬀerent loss function components,\nexcept the masked gene expression reconstruction loss, did not contr ibute much in\nthis task. Therefore, the task-speciﬁc loss component is the most im portant design for\nperturbation prediction.\nGene Network Analysis. The GRNs deﬁned in scGPT are GCNs, because the\nconstruction process is based on embeddings similarity. For the inf erence of GCNs\nusing scGPT, two types of GCNs can be deﬁned: 1. Type 1 GCN (Tissue- speciﬁc\nGCN): This type of GCN is generated by applying the scGPT model to th e entire\ndataset under a zero-shot learning framework to create gene embeddings . The Pearson\ncorrelation is then computed to infer gene-gene relationships based on these embed-\ndings. The quality of the GCN is evaluated based on the relationships be tween marker\ngenes for diﬀerent cell types. 2. Type 2 GCN (Cell-type speciﬁc G CN): This GCN is\ncreated by applying the scGPT model to generate cell-type-speci ﬁc gene embeddings\nunder the zero-shot learning framework, and cosine similarity is use d to infer gene-gene\nrelationships based on these embeddings [ 32]. The quality of this GCN is evaluated\nbased on the Gene Ontology Enrichment Analysis (GOEA) [ 43] for cell-type speciﬁc\ngene sets. These GCNs can provide valuable insights into the unders tanding of gene\ninteractions and regulation in speciﬁc tissues or cell types, which could have broad\napplications in biology and medicine [ 44, 45].\nWe used the Immune Human Atlas dataset to evaluate the performance of inf erring\nthese two types of GCNs. The known information including marker gene s [ 46], cell\ntypes [ 46], and Reactome pathways [ 47] was utilized to evaluate the performance of\nscGPT on the GCN inferences.\nIn the analysis of the Immune Human Atlas dataset, we also considered anoth er\napproach to deﬁning GCN similar to scGPT’s deﬁnition by ﬁnding the nearest neigh-\nbors of genes based on gene embeddings. The focus was initially on Type 1 G CN, with\nthe results presented in Figure 3 (d) and (e). The neighboring relationships within this\n10\ndata are colored according to the distribution of marker genes. We collec ted marker\ngenes based on the source paper of Immune Human Atlas dataset [ 46]. As this rela-\ntionship was determined based on k-nearest neighbors, it can be viewe d as a form of\ngene co-expression relationship. From Figure 3 (d), only marker genes from two cell\ntypes showed the co-embedded and isolated relationship. They are Mon ocyte-derived\ndendritic cells and Megakaryocyte progenitors. Figure 3 (e), on the other hand, repre-\nsents the cluster labels based on the Leiden clustering method [ 48]. These clusters can\nbe interpreted as groups of genes that share common functions, or “gene co-fu nction\nclusters”. For marker genes from other cell types, some of them are in di ﬀerent clusters\nshown in Figure 3 (e), and some genes are co-embedded with other cell types’ marker\ngenes. There are two isolated groups (9 and 12), but no marker genes are ide ntiﬁed in\nthese two groups.\nThese analyses highlight how scGPT can be used to create gene embeddi ngs\nto reveal important patterns of co-expression and co-functionality bet ween diﬀer-\nent genes, oﬀering insights into potential gene-gene interactions and regulatory\nrelationships.\nExtended Data Figure 16 (a) and (b) focus on gene embeddings categorized by\ncell types. Extended Data Figure 16 (a) shows that gene embeddings from diﬀerent\ncell types tended to be co-embedded and there was no apparent diﬀer ence. There was\nspeciﬁc gene enrichment in the genes from Erythroid progenitors and C D 16+ Mono-\ncytes. The distribution of the remaining genes on the UMAP results was relatively\nrandom, and this could be due to two reasons. One reason is that the qualit y of gene\nembeddings was unsatisfactory. Since scGPT adopts the zero-shot learn ing scheme\nembeddings, it does not incorporate cell-type-speciﬁc information . The other reason\nis that the complex biological network in the human immune system makes the com-\nmunication between cell-cell or gene-gene diﬃcult to decompose [ 49, 50]. Additional\nanalysis is needed for gene embeddings. Extended Data Figure 16 (b) shows the clus-\nter results based on the Leiden algorithm [ 48]. Most of the clusters contained marker\ngenes of diﬀerent cell types.\nThis analysis illustrates the complexity of cellular functionality and the diﬃculty\nof clearly deﬁning these relationships based on gene embeddings. Des pite these chal-\nlenges, the scGPT model still demonstrates its potential in ident ifying functional\nsimilarities between diﬀerent cell types.\nThis study also involved evaluating and exploring GCNs based on the hum an\nimmunology system, which is known for its complexity due to interac tions among\nvarious cell types. The original analysis of GCN from [ 32] focused on HLA genes and\nCD genes. We omitted the analysis for HLA genes because they carry a higher risk for\nmisreading based on previous research [ 51–54], and the network result is not reliable.\nTherefore, we selected genes with the preﬁxes CD-, and utilized the embeddings of the\nremaining genes to calculate the GCNs. The scGPT results for major CD - genes are\nshown in Extended Data Figure 16 (c), which shows the GCN for major CD- genes. The\nvalue above the edges represents the strength of correlation. The thr eshold of strong\ncorrelation here is set as 0.4. A publicly accessible database containing gene enrichment\ninformation- the Immune System R-HSA-168256 pathway from the Reactome 2022\ndatabase [\n47] - was used as a reference for validation. The GCN was constructed based\n11\non the correlation between the two genes. For the CD- genes (3rd gene set ), there was\nno overlap, indicating poor inference of GCN.\nOur results highlight the importance of critical evaluation and cross-r eferencing in\nthe development of gene regulatory networks, as well as the potential and l imitations\nof using machine learning models like scGPT for this purpose.\nEvaluation based on Imputation and Simulation Analysis shows that\nfurther improvement of single-cell LLMs is needed.\nImputation. We considered scGPT and Tangram [ 55] for this task. The evaluation\nmetrics for this task based on clustering and correlation are provided in Appendix\nC. We considered imputation for two diﬀerent type of datasets, known as scRNA-seq\ndatasets and spatial transcriptomic datasets. The imputation results f or scRNA-seq\nare summarized in Figure 4 (a), which suggest that the imputation function of scGPT\nfor scRNA-seq data introduced more noise into the original sequencing data, suggesting\nthe unreliability of the decoder’s output.\nAccording to Figure 4 (b), scGPT performed well in the spatial transcriptomic data\nimputation task compared to the SOTA spatial imputation method, Tangram [ 55, 56].\nBased on the evaluation of correlation and signiﬁcance proportion, the imput ation\nresults of scGPT are better than the results of Tangram. Moreover, the scores of these\ntwo metrics based on the zero-shot learning version were even bette r than the pre-\ntraining version with scRNA-seq data. However, based on the results of t he average\nbio score evaluation, the raw data had better scores. This could be caus ed by the\nsources of the spatial clustering labels, which were generated from ge ne expression\nclusters rather than expert annotation. Such methods could introdu ce bias before and\nafter imputation.\nExtended Data Figure 17 shows the results of Diﬀerentially Expressed Genes\n(DEGs) based on pre-imputation data and post-imputation data. Results i n Extended\nData Figure 17 (a) showed that scRNA-seq imputation was not reliable because the\nexpression patterns of all genes were similar after imputation based on scGPT. How-\never, based on Extended Data Figure 17 (b), no mitochondrial (MT) genes were\nincluded in the DEGs after imputation based on scGPT. These genes we re identiﬁed\nas DEGs in the raw dataset. High proportion of MT genes are indicative of low- quality\ndata, which means cells with such patterns are apoptotic or lysing [ 57]. Therefore, the\nMT genes should be omitted in the downstream analysis by ﬁltering[ 58, 59]. Moreover,\nbased on Extended Data Figure 17 (b) and (c), the DEG patterns after imputation\nbased on scGPT and Tangram are similar. Thus, scGPT has the potential to pr oduce\nbiologically meaningful imputation for spatial transcriptomic datasets.\nSimulation Analysis. We considered scGPT and scDesign3 [ 60] for this task. We\nevaluated the output of scGPT against the output of scDesign3 [ 60]. For conditions\nincorporating batch eﬀects, we employed the same metrics used in t he evaluation of\nbatch eﬀect correction. In scenarios without batch eﬀects, our metri cs are primarily\nfocused on assessing the preservation of biological information. As shown in Figure 4\n(c)-(e), scDesign3 outperformed scGPT across two conditions of the s imulation task.\nIn particular, scDesign3 had a more pronounced superiority in generati ng simulation\ndata without batch eﬀects, in comparison to scGPT. This is consisten t with the results\nshown in Figure 4 (d) and (e). The gene-gene correlation from scDesign3 was also\n12\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nscDesign3 scGPT\n0.4\n0.5\n0.6\n0.7\n0.8\nscDesign3 scGPT\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nCorrela on\nsigni ﬁ cance \npropor on\nAverage bio\nscDesign3-batch scDesign3-nobatch\nscGPT-batch scGPT-nobatch\nr=0.26 r=0.24\nr=0.05\nr=0.05\n0.0\n0.2\n0.4\n0.6\n0.8\nraw scGPT\nSc or e\na\ncSc or e\nbatch no batch\n**\n** ***\nb\nd\ne\nTangram scGPT (zero-shot) scGPT\nFig. 4 : Experimental results of the Imputation task and the Simulation task. The\nsigniﬁcance level was computed based on paired Student’s t-test. Th e number of stars\nrepresents the signiﬁcance level ( ∗ ∗ ∗ : p − value < 0. 005, ∗∗ : p − value < 0. 05).\n(a): Comparison of the average bio score between the raw data and imputed data by\nscGPT in the scRNA-seq imputation task. (b): Comparison of the average bi o score,\naverage correlation score, and average signiﬁcance level score among Tangram, scGPT,\nand scGPT (zero-shots) in spatial transcriptomics imputation task. (c ): Comparison\nof the average bio score between scDesign3 and scGPT for simulation. (d ): Gene-gene\ncorrelation heatmap from the raw HumanPBMC dataset. We select the subse t of the\ntop 100 highly variable genes. (e): Comparison of diﬀerent simulation meth ods by\ncorrelation. The heatmap represents the top 100 highly variable genes (for raw and\nscDesign3) or the subset of the top 100 highly variable genes (for scGPT) bas ed on the\nHumanPBMC dataset. The correlation “r” represents the Pearson correlati on between\nthe gene correlation of raw data and the gene correlation of simulation data.\n13\nmore similar to the gene-gene correlation of the raw data. Therefore, the simulation\ntask needs to be improved for single-cell LLMs. In addition, we prese nt UMAPs of\nthe output produced by diﬀerent methods in Extended Data Figure s 18 and 19 that\nillustrate the advantage of scDesign3. The embeddings of scGPT with the no batch\neﬀect settings tended to preserve the batch eﬀect, while the em beddings with batch\neﬀect tended to remove the batch eﬀect.\nSingle-cell LLMs have emergent abilities but the stability of sin gle-cell\nLLMs should be improved.\nEmergent Ability Analysis. In this section, we explored the emergent abilities\nof scBERT, Geneformer, and scGPT. We considered three scenarios t o investigate the\nemergent abilities: cross-data cell type prediction, cross-spec ies analysis, and spatial\ntranscriptomics analysis.\nBased on [ 33], we compared models with large parameter size of pre-training to\nmodels of small parameter size. For all tasks except spatial data batch eﬀ ect correc-\ntion, we compared the performance of the three examples with that of Vanilla neural\nnetworks (NNs) to identify any emergent abilities. As for the task of spat ial data batch\neﬀect correction, we examined the statistics derived from our corre ction evaluation to\ndetect any emergent abilities. Importantly, in the context of batch e ﬀect correction, we\nrestricted our evaluation to batch-related metrics. We could not acce ss the cell-type\nlabels for these datasets.\nIn the ﬁrst scenario, our anticipated emergent ability would be a signi ﬁcant\nimprovement in prediction accuracy for single-cell LLMs compared to vanilla NNs of a\nsmaller size. Figure 5 (a) provides an overview of diﬀerent model sizes. From Figure 5\n(b), it is evident that scGPT and Geneformer outperformed NNs in term s of accuracy\nin cross-data scenarios, suggesting the emergent abilities in the cr oss-data cell-type\nannotation task. The low accuracy of scBERT may be caused by its default ﬁ ne-tuning\nsetting and/or the pre-trained model weights.\nIn the second scenario, the desired emergent ability mirrors that of t he ﬁrst task.\nNonetheless, there was no enhancement in performance from Figure 5 (c). Figure 5\n(c) shows that vanilla NN outperformed scGPT for both the MCA and MB Spatial\ndatasets. vanilla NN was comparable to Geneformer for the MCA dataset and worse\nthan Geneformer for the MB spatial dataset. Moreover, vanilla NN was better than\nscBERT for the MCA dataset and worse than scBERT for the MB spatial dataset .\nIn the third scenario, we considered two emergent abilities. Firs tly, in line with\nthe batch eﬀect correction task, we postulated that using Human scRNA-se q data\nfor pre-training could aid in batch eﬀect correction for human spatial tr anscriptomic\ndata. Secondly, resonating with the cell-type annotation task, we hypot hesized that\npre-training on Human scRNA-seq data might assist in cell-type annotation for mouse\nspatial transcriptomic data. Figure 5 (c) shows that we did not detect any emergent\nability pertaining to the second component in the MB Spatial data annotat ion task.\nFigure 5 (d) suggests the emergent ability for batch eﬀect correction. The ﬁne -tuning\nprocess appeared beneﬁcial in reducing the batch eﬀect inherent in the spatial data,\nwhereas scenarios with model freezing except decoder yielded s ubpar results. The\nperformance of scGPT in the integration of spatial data was better than that of\n14\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nMCA\nMB \nSpatial\nGeneformer scBERT vanilla NN scGPT\nc d\n0\n50\n100\n150\n200\n250\n300\nNNs Geneformer scBERT scGPT CellLM\n Model Size (MB)\ne\n0 0.2 0.4 0.6 0.8 1\nscGPT\nG eneformer\nvanilla NN\nscBERT\na b\n10.201.15 8.38\nAccuracy\nAccuracy\n0.6\n0.7\n0.8\nCL HumanPBMC MCA Pancrm\nDatasets\nScore\nMethods\nResPAN\nscGPT\nscVI\n0.25\n0.50\n0.75\n1.00\nCL HumanPBMC MCA Pancrm\nDatasets\nMethods\nGeneformer\nscBERT\nscGPT\n41.87 55.07\nResults un der different initial settings\nResPAN\ncross-entropy\nfreeze except decoder\nmse loss\nzeroshot\nfreeze except decoder\nno pretrain\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nFig. 5 : Diﬀerent comparison groups for emergent ability analysis and stability analysis.\n(a): The model scale of diﬀerent methods. Number under the x-axis r epresents the\ntrainable parameter size (unit: Million). (b): Accuracy of LLMs and vanilla NN in Cell-\ntype Annotation task. The dataset here is the Pancreas cross dataset. (c): Accuracy of\nLLMs and vanilla NN in Cell-type Annotation task. The datasets here are MB Spatial\nand MCA. (d): Overall score comparison including ResPAN and diﬀerent s ettings of\nscGPT. The dataset here is the human spatial transcriptomic dataset. ( e): Diﬀerent\nbatch correction scores of diﬀerent models based on changing random seed s (left) and\ndiﬀerent average classiﬁcation scores of diﬀerent models based on chan ging random\nseeds (right). The bold black line represents the median value whi le the length of each\nbox can be interpreted as the variance level.\nResPAN. Therefore, we observed emergent abilities in the batch eﬀec t correction for\nsptail transcriptomic data.\nStability Analysis. To analyze the stability of single-cell LLMs, we selected Batch\nEﬀect Correction and Cell-type Annotation as two representative tasks and varied the\nseeds of single-cell LLMs to investigate the model stability. These two tasks are the\nmain tasks in single-cell data analysis and have solid metrics for evalu ation. Ideally,\n15\nthe results of diﬀerent single-cell LLMs should not vary substantiall y across diﬀerent\ndatasets. We also considered stability for other benchmarking tools. Our experiment\nresults summarized in Figure 5 showed that the stability for single-cell LLMs is task-\nspeciﬁc.\nBased on Figure 5 (e), the variance of scVI [ 61] and ResPAN was generally lower\nthan that of scGPT, and scVI and ResPAN also had a higher score on average. Ther e-\nfore, single-cell LLMs were not as stable as SOTA deep-learning-based me thods for\nthe Batch Eﬀect Correction task.\nFigure 5 (f) suggests that the variance of Geneformer was generally smaller than\nscGPT and scBERT. All three models had high median average scores. More over, the\nvariance of scBERT was relatively large in the experiments based on the MCA dataset,\nwhich implies that single-cell LLMs might fail under certain random se eds.\n3 Discussion\nIn this paper, we have evaluated the performance of single-cell LLMs on eight diﬀer-\nent tasks in single cell data analysis. The ranks of these single-cell LLMs are shown\nin Table\n1, where we not only consider the broader functions of the models, but\nalso their usability. For example, open-source models with tutorial s are more friendly\nto researchers and these models also receive a number of stars and li kes based on\nExtended Data Figure 20.\nFor the Cell-type Annotation task, we compared results among scGPT, scB ERT,\nGeneformer and CellLM. Geneformer was generally better than scGPT and scBERT\nin this task. Despite its smaller size, scBERT occasionally outper formed scGPT on\ncertain datasets. This suggests that continually seeking larger model s, driven purely\nby the scaling law, may not always be the preferred approach. For the G ene Function\nPrediction task, we showed that the pre-trained model can reach bet ter performance.\nMoreover, during the ﬁne-tuning process, the optimization of the encoder-decoder\nstructure did not seem to improve performance. Such ﬁndings sugges t that we may\nnot need to train the whole model for this task. For the data imputation t ask, we\nshowed that scGPT did not perform well in the scRNA-seq imputation tas k, but it\noutperformed Tangram in the spatial transcriptomic data imputation task b oth under\nzero-shot learning and ﬁne-tuning frameworks. Such ﬁnding suggests that single-cell\nLLMs can transfer knowledge across diﬀerent omic data.\nWe observed the emergent ability of single-cell LLMs. For the cross-dat a Cell-\ntype Annotation task, the performance of pre-trained scGPT is much bet ter than the\nperformance of neural networks with smaller parameter size. Moreover , scGPT can\nalso be used to analyze spatial transcriptomic data for batch eﬀect corre ction, which\nhave attracted much attention in recent years [ 62].\nThus, the contributions of single-cell LLMs are substantial, as they all ow for the\nextraction of meaningful biological information across a variety of tasks. The se models\nprovide valuable insights and have the potential to advance our unders tanding of\nsingle-cell biology.\nDespite the signiﬁcant contributions of single-cell LLMs, there are e vident limita-\ntions in their construction and training steps. A more comprehensiv e understanding\n16\nof such LLMs is necessary to address these issues. We employed an ablat ion testing\napproach to discern the relative importance of diﬀerent components in the loss func-\ntion during the ﬁne-tuning process. The scGPT loss function comp rises, at most, ﬁve\nparts, with four ﬁxed components and one task-speciﬁc term. However, our experi-\nmental results suggest that not all components are of equal importance. The GEPC\nloss term appears to be the least signiﬁcant, and the inclusion of the EC S Loss is not\nconsistently beneﬁcial across all the datasets we tested.\nFor the Batch Eﬀect Correciton task, scGPT did not outperform task-spe ciﬁc\nSOTA methods. Moreover, the correction results of atlas-level datas ets were worse than\nthe raw datasets. One possible solution is to incorporate extra inform ation, including\ncell types, in the pre-training or ﬁne-tuning process. Moreove r, for the Perturbation\nPrediction task, the function of model pre-training is not obvious. G EARS was gen-\nerally better than scGPT in this task. For the data simulation task, sc GPT did not\nperform very well. One possible reason is from [ 63], which suggests that current LLMs\nrely on non-transferable procedures for speciﬁc tasks and not all the ab ilities of LLMs\nare transferable and general.\nIn our stability analysis, we found that single-cell LLMs were not very s table in the\nbatch eﬀect correction task. For some datasets, the cell-type predic tion accuracy was\nalso not reliable. Moreover, in the evaluation for the Cell-type Annotat ion task, we\nfound that diﬀerent single-cell LLMs had diﬀerent variance levels. T he variance under\ndiﬀerent random seeds was driven by datasets. Considering the probl ems of stability,\nwe still have diﬃculty reﬁning it into a toolbox (for example, Se urat [ 64] or Scanpy\n[59]) with various functions.\nMoreover, we explored the parameter-eﬃcient ﬁne-tuning framewor k known as\nLoRA [ 65] for scGPT in Batch Eﬀect Correction and Cell-type Annotation. Extend ed\nData Figure 21 showed that LoRA slowed down the ﬁne-tuning process. One possibl e\nreason is from the trade-oﬀ between accelerating the forward process an d accelerating\nthe backward process, since incorporating LoRA decreases the acceler ating process\n[65]. LoRA also could not improve the performance of scGPT.\nThus, the training of single-cell LLMs is more nuanced than that of general LLMs.\nWhile it is important to consider the similarities between these two model types,\nwe must also consider the diﬀerences rooted in the datasets and domain -speciﬁc\nknowledge.\nWe also studied the impacts of the ﬁne-tuning process. The pre-t raining phase\nplays an important role across multiple tasks, particularly for Cell-t ype Annotation\nand Gene Function Prediction. We observed a considerable performanc e improvement\nfor these two tasks with pre-training. Moreover, even if the perfor mance of single-cell\nLLMs in batch integration tasks is not outstanding, they can still annotate ce ll types\nwith high accuracy. Thus, single-cell LLMs may be robust to batch eﬀec t.\nOur results suggest that scGPT’s performance is signiﬁcantly inﬂue nced by the\nlearning rate and the number of bins. The former plays a critical role during the\ntraining process, while the latter is set during the pre-proce ssing stages. These stages\ncorrespond to two essential aspects of task-speciﬁc ﬁne-tuning. Fu rthermore, the\npre-training phase can also boost the model’s performance on tasks inv olving cross-\nspecies datasets. As an example, pre-training strategy using human si ngle-cell datasets\n17\nenabled scGPT to handle Batch Eﬀect Correction and Cell-type Annotati on tasks on\nmouse datasets eﬀectively.\nInterestingly, we found that the optimal optimizer for single-cell LLM s was still\nAdam. When other factors were controlled, the performance derived from the SGD\noptimizer signiﬁcantly lagged behind Adam, highlighting the necessit y of tolerating\nthe additional memory consumption caused by Adam. Moreover, the contrib ution of\nthe encoder part is more important than that of the decoder part, which may be\nexplained by the fact that the noise in the single-cell data makes the encoder part of\nthe model take on more tasks. We note that this result is inconsistent with current\nresearch in general LLMs [ 66].\nAlthough some tasks were only evaluated based on scGPT, the patterns we fou nd\nin scGPT could be extended in the training of other single-cell LLMs b ased on Figure\n2 (f). We have identiﬁed several beneﬁcial factors that can improve t he performance\nof single-cell LLMs: 1. Employing a smaller learning rate (e.g., 1e-4) and a larger\nbin size (e.g., 501) can improve the ﬁnal score. A smaller learning rate ensures a more\nstable training process, while a larger bin size contributes more t o reducing batch\neﬀects before the training process. 2. During the ﬁne-tuning pr ocess, freezing parts\nof the model can retain performance while saving training time. The re is a higher\npriority for freezing the decoder than the encoder. Given that we have numerous\ntasks requiring the resolution of embeddings, freezing the encod er is not advisable. 3.\nThere is beneﬁt in integrating cell-type into the ﬁne-tuning process in multiple tasks.\n4. Adam is the most preferred optimizer for single-cell LLMs. 5. Possessing a GPU\nwith larger memory (e.g., RTX3090, A6000, and A100) is advantageous for the ﬁne-\ntuning process. 6. Cross-entropy loss is the optimal choice for masked gene expression\nreconstruction. We provide a table shown in Table 1 to evaluate the number of tasks\nthat diﬀerent single-cell LLMs can handle. Models with the scGPT framework achieve\nthe best score based on consideration of both usability and stability.\nThe application of LLMs to single-cell data remains a promising avenue of explo-\nration, given its impressive performance in Cell-type Annotation and P erturbation\nPrediction tasks. Since much has been done to optimize general LLMs [ 67–70] (includ-\ning eﬃcient tuning, model compression, and other research directi ons), we focus\nspeciﬁcally on how to better train and apply single-cell LLMs. We disc uss several\npotential research topics and future directions below:\nIn terms of data collection, we need high-quality data spanning diﬀer ent contexts,\nsuch as various cell types, disease states, genders, and even data from diﬀerent species\nfor pre-training datasets. High-quality pre-training datasets are imp ortant for general\nLLMs otherwise the performance may be reduced [ 71, 72]. The qualify of pre-training\ndata can also be veriﬁed with the online learning framework [ 73]. We can evaluate\nthe performance of single-cell LLMs on downstream tasks and then decide which\ndata sets to include. Moreover, the incorporation of biological informati on is crucial\nfor the success of a LLM in biology. With tissue information, we can also dev elop\ndomain-speciﬁc or tissue-speciﬁc LLMs for single-cell data analysis.\nAs for model training, both the pre-training and ﬁne-tuning steps of t he existing\nsingle-cell LLMs need improvement. During model pre-training, w e should focus on\nincorporating biological information or human feedback into the process, as opposed to\n18\njust relying on the conventional masked token prediction task. For ins tance, integrat-\ning cell-type or disease condition information into the pre-trainin g step is an intriguing\napproach, and let experts to evaluate the quality of model output durin g the training\nprocess is also meaningful. Moreover, we should also consider the se curity or trustwor-\nthiness of LLM training [ 74]. Using poisoned training single-cell datasets (for example,\nwrong data or make-up data) as an attack can test the robustness of single-cel l LLMs.\nWith better pre-training design, we may increase the scale of curr ent models to billion\nlevel. For model ﬁne-tuning, instruction tuning [ 75] is a potential direction to explore.\nIn this context, cells could be considered as prompts, as described in scGPT. Another\npossible direction is to focusing on generating uniﬁed embeddings for cells/genes and\ncombining them with task-speciﬁc models for downstream applicati ons, inspired by\n[33, 76].\nFor model evaluations, we need more eﬀective methods to assess resu lts for cer-\ntain tasks. For instance, evaluating the quality of gene embeddings sole ly based on\ngenes from a single pathway is too simplistic, and we may need to conﬁ rm the biolog-\nical function of genes using diﬀerent approaches, including veriﬁc ation from biology\nexperiments to avoid the harm of incorrect output of single-cell LLMs ( also known as\nLLM hallucinations [ 77]). Moreover, we should also account for the pre-training cost\n(such as training time and power consumption). This information can gui de users in\nselecting the most suitable single-cell LLM for their needs.\nWhen it comes to task selection, we should ﬁrst deﬁne the tasks rigorou sly.\nFor example, in the GRN inference tasks deﬁned by scGPT, we cannot t reat a co-\nexpression network based on gene embeddings the same as a gene regulatory n etwork.\nAlso from scGPT, using attention we can infer gene-gene correlation str ength with\ndirection, but the relation between the correlation of features and th e value in the\nattention map is in debate [ 78–80]. Moreover, we should consider more meaningful\nand representative tasks related to single-cell and spatial data for si ngle-cell LLMs.\nFor instance, tasks such as cell-type deconvolution, marker gene sele ction, and spatial\ntranscriptomics imputation warrant exploration.\nIn summary, the goal of studying single-cell data-based LLMs may be the de velop-\nment of a large-scale model that is capable of performing multiple tasks with stable and\nreliable results. Such a model should also be user-friendly wit h detailed tutorials and\nwebsites. Although our evaluation is subject to the number of current single-cell LLMs,\nthe scale of current single-cell LLMs and the size of pre-training datas ets, we hope\nthat our analysis can provide insights into best practice and guide the development of\nfuture LLMs for single-cell data analysis.\n4 Methods\nProblem deﬁnition. We consider a pre-trained LLM, denoted as M(x, θ ), which is\nbased on the single-cell dataset D. Here, θ embodies the set of both model param-\neters (e.g., network weights) and hyper-parameters (e.g., epochs an d learning rate).\nDiﬀerent LLMs have used distinct pre-training datasets. The mode l structure for the\nﬁne-tuning phase is deﬁned as M′(x, θ ′). Our objective is to ascertain the optimal set\nof θ′ for various sub-task. Formally, we denote the loss for task k as Lk(·, ·), and use\n19\nthe evaluation dataset Deval = {xi, y i}n\ni=1, to assess Lk. Our primary goal is to ﬁnd:\nθ∗ = arg min\nθ′\nEx,y ∈D [Lk(M′(x, θ ′), y )] .\nOur second goal is to evaluate the performance of diﬀerent single-cell LLM s, that\nis, we intend to ﬁnd:\nM∗ = arg min\nM′\nEx,y ∈D [Lk(M′(x, θ ∗), y )] .\nOur third goal is to assess other abilities of single-cell LLMs, includ ing: 1. zero-\nshot learning; 2. emergent ability [\n81]; 3. cross-species data analysis; 4. biological\nmechanism exploration; and 5. stability.\nParameters and tasks. Most single-cell LLMs share the pre-training process.\nBy considering the overlap among various single-cell LLMs, we have sel ected scGPT,\nscBERT, and Geneformer as representative examples for our analysis. W e also high-\nlight the downstream tasks in Figure 1 (a). We focus on eight ﬁne-tuning tasks in\ntotal: 1. Batch Eﬀect Correction; 2. Multi-omics Data Integration; 3. Cell -type Anno-\ntation; 4. Gene Function Prediction; 5. Perturbation Prediction; 6. Ge ne Network\nAnalysis; 7. Imputation; and 8. scRNA-seq Simulation. By compiling all the hyper-\nparameters across diﬀerent models, we present a list of factors that c an aﬀect the\nmodel performance categorized by types in Figure 1 (b). The deﬁnition of diﬀerent\nhyper-parameters is discussed in Appendix A. To analyze the eﬀect of diﬀerent hyper-\nparameters, initial settings and optimizers, we selected four repr esentative datasets:\nPancrm, HumanPBMC, Cell Lines and MCA because they cover various data t ypes.\nPancrm is from Pancreas tissue and has ﬁve batches. HumanPBMC is from PBM C\nand has nine cell types. Cell Lines has two cell types, as a binary labe l dataset. MCA\nis from Mus musculus.\nBatch Eﬀect Correction. Batch Eﬀect Correction is an essential step follow-\ning scRNA-seq data pre-processing. It primarily signiﬁes the dist ribution disparity in\nscRNA-seq datasets originating from the same tissue, which can be attrib uted to var-\nious factors [ 82]. The reduction of batch eﬀects is critical not only to allow research ers\nto discern genuine biological signals but also to facilitate integrated an alyses across\ndiﬀerent studies. The challenge of this task arises from the need to balance the removal\nof batch signals with the preservation of biological signals. We treat this t ask as a data\nintegration problem.\nFor the Batch Eﬀect Correction, the metrics we consider here are ins pired by scIB\n[46], including Normalized Mutual Information (NMI), Adjusted Rand Index (ARI)\nand Cell-type Average Silhouette Width (cell-type ASW) for the biologi cal conserva-\ntion score; and batch Average Silhouette Width (batch ASW), Principal Com ponent\nRegression (PCR) and Graph Connectivity (GC) for the batch eﬀect corr ection score.\nWe compute the weighted average of these metrics to represent the ﬁnal batch eﬀect\ncorrection score. Details of these metrics can be found in Appendix D.1. Let Sbio\ndonate the average biological conservation metric and Sbatch donate the average batch\n20\neﬀect correction metric as Sbatch, the ﬁnal model score is:\nSﬁnal = 0 . 6 ·Sbio + 0. 4 ·Sbatch.\nMulti-omics Data Integration. Multi-omics Data Integration is a key for multi-\nomics data analysis [ 21]. It is akin to an advanced form of batch eﬀect correction. If\nunpaired multi-omics data are present, the objective is to map diﬀe rent datasets into\na shared space for subsequent analysis. If paired multi-omics data are present, the\ngoal is to assess whether the use of multi-omics data can contribute to l earning a more\ncomprehensive representation of the data. A signiﬁcant challenge here is how to align\nomics at the feature level. For instance, the feature of the scRNA-seq d ata is a gene,\nthe feature of the scATAC-seq data is a peak, and the feature of the prot ein data is\na protein. The tokenization step can become complex given diﬀerent mo dalities. We\ntreat this task as a data integration problem. We use the same metrics for multi-omics\ndata integration as those for Batch Eﬀect Correction.\nCell-type Annotation. Cell-type Annotation is another key step following single-\ncell data pre-processing [ 83]. This step annotates each cell with its accurate cell type\nlabel, which can be achieved through prior knowledge [ 84] or computational methods\n[85]. These annotated cell-type labels can provide essential biological i nformation for\nfurther downstream analysis, such as cell-type speciﬁc network an alysis. In addition,\ndrug response prediction [ 33] or single-cell disease classiﬁcation [ 86] can also be treated\nas a variation of this task. A common approach employed by single-cell LLMs in dealing\nwith the cell-type annotation task is to use single-cell datasets for m odel training and\ntreat the unannotated datasets as testing datasets. The challenge lies i n predicting\nor annotating a set of cells which originate from studies diﬀerent from t he training\ndatasets. Moreover, the existence of cells with novel cell types (which are not included\nin the training datasets) further complicates the problem. We treat this task as a\nmulti-label classiﬁcation problem.\nIn this task, we choose datasets with batch eﬀect in two diﬀerent cas es. The intra-\ndataset case allows batch intersection, which means that the training and testing\ndatasets can contain cells from the same batch. Here the total dataset was spl it into\napproximately 70% as a training dataset and the rest as a testing dataset. T he inter-\ndataset case is cross-batch (cross-data) annotation, which means that the t raining and\ntesting datasets are from diﬀerent sources. We consider two datasets from the same\ntissue in this setting. More speciﬁcally, we consider Pancreas cros s from pancreas, and\nLiver cross from Liver. The main score for evaluation here is accuracy, whi ch is deﬁned\nas:\nScelltype = #corrected classiﬁed cells\n#Total cells .\nWe also consider Precision, Recall and F1 scores in the analysis for ablat ion test,\nand details can be found in Appendix D.2. Moreover, except the general comparision,\nwe consider four datasets for the eﬀect of diﬀerent hyper-parameters , initial settings\nand optimizers: Pancrm, HumanPBMC, Cell Lines, and MCA, which is from Mus\nmusculus rather than Homo sapiens.\nGene Function Prediction. Gene Function Prediction is important to identify\nthe properties of genes across diﬀerent conditions [ 31]. There are approximately 20,000\n21\nprotein-encoding genes for humans [ 87] and only some are annotated with functions.\nAccurate prediction of gene function can help us understand and infer t he role of genes\nin biological systems. Here we consider three types of functions for gen e. The ﬁrst one\nis dosage-sensitive or non-sensitive. Some genes are dosage-sensitive , which means that\nthey are signiﬁcant in the analysis of Copy Number Variants (CNVs) related t o genetic\ndiagnosis. The second one is Bivalent versus non-methylated. Bivalen t chromatin struc-\nture is important to identify key developmental genes in embryoni c stem cells (ESCs).\nTherefore, identifying bivalently marked genes versus unmethy lated genes is impor-\ntant. The third one is Bivalent versus Lys4-only methylated. Lys4-only -methylated\ngenes are also diﬀerent from bivalently marked gene. We compare the mod el output\nwith the true gene labels. We treat this task as a binary classiﬁcation p roblem. Here\nwe used the same metrics as the cell-type annotation task. We used a pu blic dataset\n[31] considering only labeled genes in the dataset for prediction and evalu ation.\nPerturbation Prediction. Perturbation Prediction [ 42] is a task based on gene\nediting and single-cell sequencing technologies. After silencin g some genes, we can\nobtain unperturbed and perturbed gene expression levels by seque ncing, which allows\nus to explore the interactions between genes. A well-known techn ique is Perturb-seq\n[88]. In perturbation prediction, we intend to predict the gene expr ession level after\ngene editing. This task contains may predict seen gene perturbation i n the testing\ndatasets (a easier one), or predict unseen gene perturbation in the te sting datasets (a\nmore diﬃcult one). We treat this task as a regression problem. The metr ic we used here\nis MPC, and the details can be found in Appendix D.4. In the perturbation prediction\ntask, we construct the paired input-target datasets by selecting th e cells with non-\ncontrol guide identity and then randomly sample cells under the cont rol condition,\nand then combine them as the training and testing datasets. Our Pertu rb-seq datasets\nare from GEARS [ 42], which contain cells with three conditions: control; one gene\nperturbation; and two genes perturbation. In the evaluation process, we combined case\n2 and case 3. Here we select GEARS [ 42] as a benchmarking tool.\nGene Network Analysis. Gene Network Analysis is a downstream task for\nsingle-cell datasets [ 89]. The objective is to infer speciﬁc gene networks (for exam-\nple, Gene Regulatory Network (GRN) or Gene Co-expression Network (GCN)) f rom\ndiﬀerent datasets. A GRN can assist in understanding the regularory relationships\nbetween genes and predicted perturbation outcomes. The challenge in this task stems\nfrom the Granger causal relationship or time-dependent correlation [ 90]. A GCN can\nbe used to analyze genes with similar functions or uncover the charact eristics of genes\nin some diseases [ 91]. GCN and GRN are two diﬀerent tasks because correlation does\nnot imply causal relation [ 92]. This limitation means that we cannot determine which\ngenes are the “causes” of expression level changes in other genes only base d on embed-\ndings similarity or correlation. We treat this task as a network inferen ce problem. In\nthe gene network analysis task, we consider using the overlap betwee n ground truth\ngenes and inferred genes as a metric. Details can be found in Appendix D.4.\nImputation. Imputation is a ﬁlling task related to missing data. Generally, we\nhave two targets: 1. Perform imputation for scRNA-seq data to reduce data noise\nand ﬁll in technical zeros with biologically meaningful values [ 93, 94]. 2. Perform\nimputation for spatial transcriptomic data because of unseen or unmeasu red genes\n22\n[55, 56]. According to [ 95], current spatial imputation methods do not show strong\nperformance across diﬀerent datasets. Using single-cell LLMs, we can ei ther use zero-\nshot learning to impute the unseen genes, or ﬁne-tune our model base d on reference\nscRNA-seq with more genes to perform imputation. We treat this task as a m atrix-\ncompletion problem. Details of the metrics we used here can be found i n Appendix\nD.5. In the imputation task, we used two public datasets from the mouse t issue to\nanalyze the performance of single-cell LLMs. One dataset is a scRNA-seq dat aset, and\nanother one is a spatial transcriptomic dataset. For the imputation of the s cRNA-seq\ndataset, we used the output of the model decoder as imputation result s. To evaluate\nthis task, we used the biology preservation score from the batch eﬀect c orrection and\ncompare it to the score from raw data. For the imputation of spatial transcri ptomic\ndata, we considered two diﬀerent settings to perform imputation. Th e ﬁrst setting uses\nscRNA-seq to perform ﬁne-tuning and inference based on spatial transc riptomic data.\nThe second setting uses a zero-shot learning framework to directly perform inference\nbased on spatial transcriptomic data. We considered using the correlati on between\nknown raw gene expression and known imputed gene expression as a met ric.\nSimulation. scRNA-seq Simulation is a data generation task. Leveraging the gen-\nerative pre-training process of scGPT, we can generate gene express ions based on real\ndatasets. Since a prevalent issue with scRNA-seq data simulation is t he considerable\ndivergence between simulation datasets and real datasets [ 96], direct generation from\nreal datasets is preferred. By arranging diﬀerent sequences of maski ng genes or alter-\ning diﬀerent seeds, we can generate new simulated scRNA-seq datasets from real ones.\nThe quality of our simulation datasets can be evaluated by comparing them w ith\nthe outputs of current simulation methods. We treat this task as a data- generation\nproblem.\nWe used the same metrics as Batch Eﬀect Correction task for evaluation. I t is pos-\nsible to produce diverse reconstruction outcomes from a single real dataset by varying\nthe random seeds. This feature enables us to create simulated single -cell datasets.\nNotably, these generated datasets retain the same gene sets as their inpu t counterparts.\nWe generate the simulation results by multiplying the output of the gene expression\ndecoder and Bernoulli decoder. We have the ﬂexibility to generat e datasets either with\nor without batch eﬀects. If we intend to produce datasets with batch eﬀects, the gra-\ndient reverse loss function is omitted. Conversely, to generate dat asets without batch\neﬀects, we either retain this function or utilize single-batch dat a.\nEmergent Ability. [81, 97] argue that due to the extensive number of parameters,\nLLMs can manage speciﬁc tasks that small-scale models cannot handle, based on few-\nshots learning. Such an attribute refers to Emergent Ability. For ins tance, GPT-4 can\ntackle some SAT questions even though it has not been trained on such datas ets with\nnovel scaling [ 17]. We hypothesize that single-cell LLMs may also possess this capabil-\nity. To test this, we devise diﬀerent scenarios similar to instan ces of emergent ability\nto assess the performance of single-cell LLMs. Such scenarios include : Cross-dataset\nCell-type Annotation, Cross-species Cell-type Annotation and Spatial t ranscriptomic\ndata analysis.\n23\nAblation tests. Given that there is no existing work investigating the signiﬁcance\nof diﬀerent loss function components of single-cell LLMs, we conducted a comprehen-\nsive analysis of the impact of various loss function components. These in clude the\nMasked Gene Expression loss (Mask Loss), Zero Log Probability Loss (Prob Los s),\nGene Expression Prediction for Cell Modelling Loss (GEPC Loss), and E lastic Cell\nSimilarity Loss (ECS Loss). We retain the task-speciﬁc loss in the ﬁne -tuning process\nas the baseline condition (All remove). Consequently, our null hypot hesis ( H0) is that\nthe removal of the component C will not degrade the performance, while the alter-\nnative hypothesis ( H1) is that the removal of component C will worsen the model’s\nperformance. Comparing the score after eliminating a speciﬁc compone nt to the score\nfrom the default setting allows us to determine whether to rejec t the null hypothesis.\nThe test we employ here is the paired Students’ t-test. The det ails of the diﬀerent loss\nfunction components are shown below.\n1. Mask Loss: In both pre-training and ﬁne-tuning, we mask the expressi on levels\nof some genes, denoted as Mmask, for gene expression prediction. The mask loss is\nmotivated from this setting and works as follows,\n˜x(i) = MLP\n(\nh(i)\nn\n)\n,\nLmask = 1\n|Mmask |\n∑\nj∈Mmask\nce\n(\n˜x(i)\nj , x (i)\nj\n)\n,\nwhere ˜x(i) represents the predicted gene expression levels for cell i, while ˜x(i)\nj represents\nthe ground truth. h(i)\nn represents the embeddings for cell i with n genes. MLP means\nthat we use linear Multi-Layer Perceptron (MLP) Neural Networks as the ou tput of\nsingle-cell LLMs under the setting of this loss function.\n2. Prob Loss: Since single-cell data can be treated as count data, we can use t he\nBernoulli distribution to model the occurrence of the expressi on in masked genes and\nuse the Maximum Likelihood Estimation (MLE) approach to estimate the p arameters\nof Bernoulli distribution. Such loss function can be used to determ ine whether the\ngiven masked gene position carries zero expression levels or not. Prob Los s works as\nfollows:\nProbi = MLP\n(\nh(i)\nn\n)\n,\nDisti = Bernoulli(Probi),\nLProb = − LogProbi(x(i) > 0),\nwhere Prob i represents the output of the model for the parameter estimation for ce ll i.\nThe estimation is based on one MLP model. Dist i represents the Bernoulli distribution\nbased on Prob i. LogProb i is the log probability based on the relationship between\ngene expression levels of cell i and zero, which can be computed based on Dist i.\n3. GEPC Loss: This loss function is similar to Mask Loss, but now we predi ct the\ngene expression levels based on cell embeddings or cell representat ion. For cell i with\ngene j, we create a query vector qj and represent the cell based on h(i)c. We can use\nthe inner product between these two terms to predict gene expr ession levels. That is,\n24\nqj = MLP\n(\nembg\n(\nt(i)\ng\n))\n,\n˜x(i)\nj = qj ·W h(i)\nc ,\nLGEPC = 1\n|Mmask |\n∑\nj∈Mmask\nce\n(\n˜x(i)\nj , x (i)\nj\n)\n,\nwhere emb g(t(i)\ng ) represents the embeddings of the gene token g in cell i. We also use\none MLP to generate the query embeddings. The following process is s imilar to the\nsteps for Lmask.\n4. ECS Loss: This loss function is used to control the similarity of the e mbeddings\nof cells in the same batch, which is deﬁned as:\nLECS = −\n(\nCosSim\n(\nh(i)\nc , h(i′)\nc\n)\n− β\n) 2\n,\nwhere CosSim represents the cosine similarity function, i and i′ are the indices of the\ntwo cells. The idea of this loss function is to ensure the similari ty between paired cells\nis higher than the predeﬁned threshold β. Moreover, dissimilar pairs should be more\ndissimilar, respectively.\nTask-speciﬁc ﬁne-tuning process. For the experiments we have in the Results\nsection, we load the pre-training weights based on the requirement of diﬀerent single-\ncell LLMs. The pre-training weights we used can be found in our Github folder. After\nthe ﬁne-tuning process, we recorded the related metrics and cond ucted more analy-\nsis. In the experiments for all tasks, we chose the ﬁrst version of sc GPT as a baseline\nand representative model for the following three reasons. Firstly , scGPT is an open-\nsource single-cell LLM with largest datasets for pre-training, and it is w ell-deﬁned\nwith detailed tutorials. In addition, the architecture of scGPT is e asy to adjust and\nincludes multiple loss function components. The functions of Gen e Function Predic-\ntion, Imputation and Simulation for scGPT were designed in scEval for ev aluation.\nMoreover, diﬀerent single-cell LLMs have overlaps and unique term s in the pre-training\nand ﬁne-tuning framework, but scGPT is the most general one. In each tas k, we also\nincluded task-speciﬁc methods as comparisons. Moreover, for Genef ormer, scBERT\nand CellLM, we also evaluated their performance based on shared hyper-p arameters\nwith scGPT to verify if our rules found in scGPT can be extended for other single-cell\nLLMs.\n5 Data availability and reproducibility\nWe used the resources from the Yale High Performance Center (Yale HPC) t o conduct\nall of the experiments. The version of GPU we used is NVIDIA A100 (40 GB). T he\nrandom seed of all experiments is the same as the default setting of the original papers.\nThe information of datasets and the download link can be found in Appendix\nF.\nThe codes of scEval can be found in https://github.com/HelloWorldLTY/scEvalwith\nMIT license.\n25\n6 Acknowledgements\nWe appreciate the comments, feedback, and model explanations from Yingx in Lin,\nHaotian Cui, Christina Theodoris, Rex Ying and Minsheng Hao.\n7 Author contributions\nT.L. designed the study with H.Z. and Y.W. T.L. run the experiments w ith K.L.\nH.L. and T.L. designed the website. T.L., K.L. and H.Z. wrote the manuscri pt. H.Z.\nsupervised this work.\nReferences\n[1] Han, X., Zhou, Z., Fei, L., Sun, H., Wang, R., Chen, Y., Chen, H., Wang, J.,\nTang, H., Ge, W., et al.: Construction of a human cell landscape at single-cell\nlevel. Nature 581(7808), 303–309 (2020)\n[2] Saliba, A.-E., Westermann, A.J., Gorski, S.A., Vogel, J.: Single-c ell rna-seq:\nadvances and future challenges. Nucleic acids research 42(14), 8845–8860 (2014)\n[3] Stubbington, M.J., Rozenblatt-Rosen, O., Regev, A., Teichmann, S .A.: Single-\ncell transcriptomics to explore the immune system in health and di sease. Science\n358(6359), 58–63 (2017)\n[4] Mathys, H., Davila-Velderrain, J., Peng, Z., Gao, F., Mohammadi, S., Young,\nJ.Z., Menon, M., He, L., Abdurrob, F., Jiang, X., et al.: Single-cell transcriptomic\nanalysis of alzheimer’s disease. Nature 570(7761), 332–337 (2019)\n[5] Zhang, L., He, C.H., Coﬀey, S., Yin, D., Hsu, I.-U., Su, C., Ye, Y., Zhang, C.,\nSpurrier, J., Nicholson, L., et al.: Single-cell transcriptomic atlas of alzheimer’s\ndisease middle temporal gyrus reveals region, cell type and sex spec iﬁcity of gene\nexpression with novel genetic risk for mertk in female. medRxiv, 2023–02 (2023)\n[6] Evrony, G.D., Hinch, A.G., Luo, C.: Applications of single-cell dna s equencing.\nAnnual review of genomics and human genetics 22, 171–197 (2021)\n[7] Hwang, B., Lee, J.H., Bang, D.: Single-cell rna sequencing technologi es and\nbioinformatics pipelines. Experimental & molecular medicine 50(8), 1–14 (2018)\n[8] Zheng, G.X., Terry, J.M., Belgrader, P., Ryvkin, P., Bent, Z.W ., Wilson, R.,\nZiraldo, S.B., Wheeler, T.D., McDermott, G.P., Zhu, J., et al.: Massively parallel\ndigital transcriptional proﬁling of single cells. Nature communications 8(1), 1–12\n(2017)\n[9] Stoeckius, M., Hafemeister, C., Stephenson, W., Houck-Loomis, B ., Chattopad-\nhyay, P.K., Swerdlow, H., Satija, R., Smibert, P.: Simultaneous e pitope and\n26\ntranscriptome measurement in single cells. Nature methods 14(9), 865–868\n(2017)\n[10] Cusanovich, D.A., Daza, R., Adey, A., Pliner, H.A., Christiansen, L. , Gunderson,\nK.L., Steemers, F.J., Trapnell, C., Shendure, J.: Multiplex s ingle-cell proﬁling of\nchromatin accessibility by combinatorial cellular indexing. Scienc e 348(6237),\n910–914 (2015)\n[11] Chen, X., Miragaia, R.J., Natarajan, K.N., Teichmann, S.A.: A rapid and rob ust\nmethod for single cell chromatin accessibility proﬁling. Nature commu nications\n9(1), 1–9 (2018)\n[12] Luo, C., Keown, C.L., Kurihara, L., Zhou, J., He, Y., Li, J., Castanon, R.,\nLucero, J., Nery, J.R., Sandoval, J.P., et al.: Single-cell methylomes iden-\ntify neuronal subtypes and regulatory elements in mammalian cortex. Sc ience\n357(6351), 600–604 (2017)\n[13] Mulqueen, R.M., Pokholok, D., Norberg, S.J., Torkenczy, K.A., Fie lds, A.J., Sun,\nD., Sinnamon, J.R., Shendure, J., Trapnell, C., O’Roak, B.J., et al.: Highly scal-\nable generation of dna methylation proﬁles in single cells. Nature biotec hnology\n36(5), 428–431 (2018)\n[14] Teichmann, S., Efremova, M.: Method of the year 2019: single-cell mu ltimodal\nomics. Nat. Methods 17(1), 2020 (2020)\n[15] Flynn, E., Almonte-Loya, A., Fragiadakis, G.K.: Single-cell multiom ics. Annual\nReview of Biomedical Data Science 6 (2023)\n[16] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,\nZhang, J., Dong, Z., et al.: A survey of large language models. arXiv preprint\narXiv:2303.18223 (2023)\n[17] OpenAI: GPT-4 Technical Report (2023)\n[18] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroi x, T.,\nRozi` ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,\nE., Lample, G.: LLaMA: Open and Eﬃcient Foundation Language Models\n(2023)\n[19] Imani, S., Du, L., Shrivastava, H.: MathPrompter: Mathematical reason ing\nusing large language models. In: Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics (Volume 5: Industry\nTrack), pp. 37–42. Association for Computational Linguistics, Toronto, Canada\n(2023).\nhttps://doi.org/10.18653/v1/2023.acl-industry.4 . https://aclanthology.\norg/2023.acl-industry.4\n[20] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ic hter, B., Wahid,\n27\nA., Tompson, J., Vuong, Q., Yu, T., et al.: Palm-e: An embodied multimo dal\nlanguage model. arXiv preprint arXiv:2303.03378 (2023)\n[21] Stuart, T., Butler, A., Hoﬀman, P., Hafemeister, C., Papalexi, E., M auck, W.M.,\nHao, Y., Stoeckius, M., Smibert, P., Satija, R.: Comprehensive inte gration of\nsingle-cell data. Cell 177(7), 1888–1902 (2019)\n[22] Kuzman, T., Ljubeˇ si´ c, N., Mozetiˇ c, I.: Chatgpt: beginning of an e nd of man-\nual annotation? use case of automatic genre identiﬁcation. arXiv preprint\narXiv:2303.03953 (2023)\n[23] Chen, J., Xu, H., Tao, W., Chen, Z., Zhao, Y., Han, J.-D.J.: Transformer for\none stop interpretable cell type annotation. Nature Communications 14(1), 223\n(2023)\n[24] Heumos, L., Schaar, A.C., Lance, C., Litinetskaya, A., Drost, F., Zappia, L.,\nL¨ ucken, M.D., Strobl, D.C., Henao, J., Curion, F., et al.: Best prac tices for\nsingle-cell analysis across modalities. Nature Reviews Genetics, 1–23 (2023)\n[25] Avsec, ˇZ., Agarwal, V., Visentin, D., Ledsam, J.R., Grabska-Barwinska, A.,\nTaylor, K.R., Assael, Y., Jumper, J., Kohli, P., Kelley, D.R.: Eﬀe ctive gene\nexpression prediction from sequence by integrating long-range inter actions.\nNature methods 18(10), 1196–1203 (2021)\n[26] Xu, H., Woicik, A., Poon, H., Altman, R.B., Wang, S.: Multilingual transl ation\nfor zero-shot biomedical classiﬁcation using biotranslator. Nature Commun ica-\ntions 14(1), 738 (2023)\n[27] Singhal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung, H.W ., Scales, N.,\nTanwani, A., Cole-Lewis, H., Pfohl, S., et al.: Large language models encode\nclinical knowledge. Nature, 1–9 (2023)\n[28] Yang, F., Wang, W., Wang, F., Fang, Y., Tang, D., Huang, J., Lu, H., Yao, J.:\nscbert as a large-scale pretrained deep language model for cell type annot ation\nof single-cell rna-seq data. Nature Machine Intelligence 4(10), 852–866 (2022)\n[29] Shen, H., Liu, J., Hu, J., Shen, X., Zhang, C., Wu, D., Feng, M., Yang, M ., Li,\nY., Yang, Y., et al.: Generative pretraining from large-scale transcript omes for\nsingle-cell deciphering. Iscience 26(5) (2023)\n[30] Zhao, S., Zhang, J., Nie, Z.: Large-scale cell representation learning via divide-\nand-conquer contrastive learning. arXiv preprint arXiv:2306.04371 (2023)\n[31] Theodoris, C.V., Xiao, L., Chopra, A., Chaﬃn, M.D., Al Sayed, Z.R., Hill , M.C.,\nMantineo, H., Brydon, E.M., Zeng, Z., Liu, X.S., et al.: Transfer learn ing enables\npredictions in network biology. Nature, 1–9 (2023)\n28\n[32] Cui, H., Wang, C., Maan, H., Wang, B.: scgpt: Towards building a foundati on\nmodel for single-cell multi-omics using generative ai. bioRxiv, 2023–04 ( 2023)\n[33] Hao, M., Gong, J., Zeng, X., Liu, C., Guo, Y., Cheng, X., Wang, T., Ma, J.,\nSong, L., Zhang, X.: Large scale foundation model on single-cell transcriptomi cs.\nbioRxiv, 2023–05 (2023)\n[34] Heimberg, G., Kuo, T.C., DePianto, D., Heigl, T., Diamant, N., Salem, O .,\nScalia, G., Biancalani, T., Rock, J., Turley, S., et al.: Scalable que rying of\nhuman cell atlases via a foundational model reveals commonalities across\nﬁbrosis-associated macrophages. bioRxiv (2023)\n[35] Wang, Y., Liu, T., Zhao, H.: Respan: a powerful batch correction model f or\nscrna-seq data through residual adversarial networks. Bioinformatics 38(16),\n3942–3949 (2022)\n[36] McInnes, L., Healy, J., Melville, J.: Umap: Uniform manifold approxi mation and\nprojection for dimension reduction. arXiv preprint arXiv:1802.03426 (2018)\n[37] Xue, F., Fu, Y., Zhou, W., Zheng, Z., You, Y.: To repeat or not to repeat: Insights\nfrom scaling llm under token-crisis. arXiv preprint arXiv:2305.13230 (2023)\n[38] Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: In terna-\ntional Conference on Learning Representations (ICLR), San Diega, CA, USA\n(2015)\n[39] Ruder, S.: An overview of gradient descent optimization algorithms. arXiv\npreprint arXiv:1609.04747 (2016)\n[40] Liu, H., Li, Z., Hall, D., Liang, P., Ma, T.: Sophia: A scalable stochas-\ntic second-order optimizer for language model pre-training. arXiv prepr int\narXiv:2305.14342 (2023)\n[41] Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X.,\nLuong, T., Hsieh, C.-J., Lu, Y., Le, Q.V.: Symbolic Discovery of Optimiz ation\nAlgorithms. arXiv (2023).\nhttps://arxiv.org/abs/2302.06675\n[42] Roohani, Y., Huang, K., Leskovec, J.: Predicting transcriptional out comes of\nnovel multigene perturbations with gears. Nature Biotechnology, 1–9 (2023)\n[43] Thomas, P.D., Ebert, D., Muruganujan, A., Mushayahama, T., Albou, L.-P .,\nMi, H.: Panther: Making genome-scale phylogenetics accessible to all. Protein\nScience 31(1), 8–22 (2022)\n[44] Badia-i-Mompel, P., Wessels, L., M¨ uller-Dott, S., Trimbour, R ., Ramirez Flores,\nR.O., Argelaguet, R., Saez-Rodriguez, J.: Gene regulatory network inf erence in\nthe era of single-cell multi-omics. Nature Reviews Genetics, 1–16 (2023)\n29\n[45] Sebastian, S., Roy, S., Kalita, J.: A generic parallel framework for i nferring\nlarge-scale gene regulatory networks from expression proﬁles: applicati on to\nalzheimer’s disease network. Brieﬁngs in Bioinformatics 24(1), 482 (2023)\n[46] Luecken, M.D., B¨ uttner, M., Chaichoompu, K., Danese, A., Inte rlandi, M.,\nM¨ uller, M.F., Strobl, D.C., Zappia, L., Dugas, M., Colom´ e-Tatch´ e, M., et\nal.: Benchmarking atlas-level data integration in single-cell genomics. Nature\nmethods 19(1), 41–50 (2022)\n[47] Fabregat, A., Jupe, S., Matthews, L., Sidiropoulos, K., Gillespie, M., Garapati,\nP., Haw, R., Jassal, B., Korninger, F., May, B., et al.: The reactome pathway\nknowledgebase. Nucleic acids research 46(D1), 649–655 (2018)\n[48] Traag, V.A., Waltman, L., Van Eck, N.J.: From louvain to leiden: guaranteein g\nwell-connected communities. Scientiﬁc reports 9(1), 5233 (2019)\n[49] Li, X.-h., Wang, Z.-x., Lu, T.-y., Che, X.-j.: Modelling immune s ystem: Prin-\nciples, models, analysis and perspectives. Journal of Bionic Engine ering 6(1),\n77–85 (2009)\n[50] Davis, M.M., Tato, C.M., Furman, D.: Systems immunology: just get ting started.\nNature immunology 18(7), 725–732 (2017)\n[51] Kang, J.B., Shen, A.Z., Sakaue, S., Luo, Y., Gurajala, S., Nathan, A., Rumk er,\nL., Aguiar, V.R., Valencia, C., Lagattuta, K., et al.: Mapping the dynamic\ngenetic regulatory architecture of hla genes at single-cell resolution . medRxiv,\n2023–03 (2023)\n[52] Meyer, D., C. Aguiar, V.R., Bitarello, B.D., C. Brandt, D.Y., Nunes, K.: A\ngenomic perspective on hla evolution. Immunogenetics 70, 5–27 (2018)\n[53] Brandt, D.Y., Aguiar, V.R., Bitarello, B.D., Nunes, K., Goudet, J., M eyer, D.:\nMapping bias overestimates reference allele frequencies at the h la genes in the\n1000 genomes project phase i data. G3: Genes, Genomes, Genetics 5(5), 931–941\n(2015)\n[54] Sakaue, S., Gurajala, S., Curtis, M., Luo, Y., Choi, W., Ishigaki, K., Kang, J.B.,\nRumker, L., Deutsch, A.J., Sch¨ onherr, S., et al.: Tutorial: a stati stical genetics\nguide to identifying hla alleles driving complex disease. Nature P rotocols, 1–17\n(2023)\n[55] Biancalani, T., Scalia, G., Buﬀoni, L., Avasthi, R., Lu, Z., Sanger, A., T okcan,\nN., Vanderburg, C.R., Segerstolpe, ˚\nA., Zhang, M., et al.: Deep learning and\nalignment of spatially resolved single-cell transcriptomes with tangr am. Nature\nmethods 18(11), 1352–1362 (2021)\n[56] Li, H., Zhou, J., Li, Z., Chen, S., Liao, X., Zhang, B., Zhang, R., Wang, Y., S un,\n30\nS., Gao, X.: A comprehensive benchmarking with practical guidelines for cellular\ndeconvolution of spatial transcriptomics. Nature Communications 14(1), 1548\n(2023)\n[57] Lun, A.T., McCarthy, D.J., Marioni, J.C.: A step-by-step workﬂo w for low-level\nanalysis of single-cell rna-seq data with bioconductor. F1000Research 5 (2016)\n[58] Satija, R., Farrell, J.A., Gennert, D., Schier, A.F., Regev, A.: S patial reconstruc-\ntion of single-cell gene expression data. Nature biotechnology 33(5), 495–502\n(2015)\n[59] Wolf, F.A., Angerer, P., Theis, F.J.: Scanpy: large-scale single-c ell gene expres-\nsion data analysis. Genome biology 19, 1–5 (2018)\n[60] Song, D., Wang, Q., Yan, G., Liu, T., Sun, T., Li, J.J.: scdesign3 gene rates\nrealistic in silico data for multimodal single-cell and spatial omics . Nature\nBiotechnology, 1–6 (2023)\n[61] Lopez, R., Regier, J., Cole, M.B., Jordan, M.I., Yosef, N.: Deep gene rative\nmodeling for single-cell transcriptomics. Nature methods 15(12), 1053–1058\n(2018)\n[62] Marx, V.: Method of the year: spatially resolved transcriptomics. Nat ure\nmethods 18(1), 9–14 (2021)\n[63] Wu, Z., Qiu, L., Ross, A., Aky¨ urek, E., Chen, B., Wang, B., Kim, N., Andreas,\nJ., Kim, Y.: Reasoning or reciting? exploring the capabilities and lim itations of\nlanguage models through counterfactual tasks. arXiv preprint arXiv:2307.02477\n(2023)\n[64] Hao, Y., Hao, S., Andersen-Nissen, E., Mauck, W.M., Zheng, S., Butler, A., Lee,\nM.J., Wilk, A.J., Darby, C., Zager, M., et al.: Integrated analysis of multimodal\nsingle-cell data. Cell 184(13), 3573–3587 (2021)\n[65] Hu, E.J., shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., C hen,\nW.: LoRA: Low-rank adaptation of large language models. In: International\nConference on Learning Representations (2022).\nhttps://openreview.net/forum?\nid=nZeVKeeFYf9\n[66] Wang, T., Roberts, A., Hesslow, D., Le Scao, T., Chung, H.W., Beltagy, I. , Lau-\nnay, J., Raﬀel, C.: What language model architecture and pretraining ob jective\nworks best for zero-shot generalization? In: International Conference on Machine\nLearning, pp. 22964–22984 (2022). PMLR\n[67] Hu, Z., Lan, Y., Wang, L., Xu, W., Lim, E.-P., Lee, R.K.-W., Bing, L., Poria,\nS.: Llm-adapters: An adapter family for parameter-eﬃcient ﬁne-tuning of large\nlanguage models. arXiv preprint arXiv:2304.01933 (2023)\n31\n[68] Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S., Chen, Y., C han,\nC.-M., Chen, W., et al.: Parameter-eﬃcient ﬁne-tuning of large-scale pre-trained\nlanguage models. Nature Machine Intelligence 5(3), 220–235 (2023)\n[69] Wang, Y., Si, S., Li, D., Lukasik, M., Yu, F., Hsieh, C.-J., Dhillon, I.S., Kumar,\nS.: Preserving in-context learning ability in large language model ﬁn e-tuning.\narXiv preprint arXiv:2211.00635 (2022)\n[70] Xu, R., Luo, F., Zhang, Z., Tan, C., Chang, B., Huang, S., Huang, F.: Raise a\nchild in large language model: Towards eﬀective and generalizable ﬁne-t uning.\narXiv preprint arXiv:2109.05687 (2021)\n[71] Chen, L., Zaharia, M., Zou, J.: How is chatgpt’s behavior changing over time?\narXiv preprint arXiv:2307.09009 (2023)\n[72] Ji, Y., Deng, Y., Gong, Y., Peng, Y., Niu, Q., Zhang, L., Ma, B., Li, X.: Explori ng\nthe impact of instruction data scaling on large language models: An empiric al\nstudy on real-world use cases. arXiv preprint arXiv:2303.14742 (2023)\n[73] Carta, T., Romac, C., Wolf, T., Lamprier, S., Sigaud, O., Oudeyer, P. -Y.:\nGrounding large language models in interactive environments with onli ne\nreinforcement learning. arXiv preprint arXiv:2302.02662 (2023)\n[74] Liang, W., Tadesse, G.A., Ho, D., Fei-Fei, L., Zaharia, M., Zhang, C., Zou,\nJ.: Advances, challenges and opportunities in creating data for trustw orthy ai.\nNature Machine Intelligence 4(8), 669–677 (2022)\n[75] Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai , A.M.,\nLe, Q.V.: Finetuned language models are zero-shot learners. In: Intern ational\nConference on Learning Representations (2022). https://openreview.net/forum?\nid=gEZrGCozdqR\n[76] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Le e, K., Zettle-\nmoyer, L.: Deep contextualized word representations. In: Proceed ings of the 2018\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227–\n2237. Association for Computational Linguistics, New Orleans, Louisiana (2018).\nhttps://doi.org/10.18653/v1/N18-1202 . https://aclanthology.org/N18-1202\n[77] Sun, W., Shi, Z., Gao, S., Ren, P., Rijke, M., Ren, Z.: Contrasti ve learning\nreduces hallucination in conversations. In: Proceedings of the AAAI Conf erence\non Artiﬁcial Intelligence, vol. 37, pp. 13618–13626 (2023)\n[78] Jain, S., Wallace, B.C.: Attention is not Explanation. In: Proceed ings of the\n2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long and\n32\nShort Papers), pp. 3543–3556. Association for Computational Linguistics, Min-\nneapolis, Minnesota (2019).\nhttps://doi.org/10.18653/v1/N19-1357 . https://\naclanthology.org/N19-1357\n[79] Wiegreﬀe, S., Pinter, Y.: Attention is not not explanation. In: Pro ceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pp. 11–20. Association for Computational Linguistics,\nHong Kong, China (2019).\nhttps://doi.org/10.18653/v1/D19-1002 . https://\naclanthology.org/D19-1002\n[80] Bibal, A., Cardon, R., Alfter, D., Wilkens, R., Wang, X., Fran¸ cois, T ., Watrin,\nP.: Is attention explanation? an introduction to the debate. In: Proc eedings\nof the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 3889–3900. Association for Computational Lin-\nguistics, Dublin, Ireland (2022).\nhttps://doi.org/10.18653/v1/2022.acl-long.269\n. https://aclanthology.org/2022.acl-long.269\n[81] Wei, J., Tay, Y., Bommasani, R., Raﬀel, C., Zoph, B., Borgeaud, S., Yogat ama,\nD., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O.,\nLiang, P., Dean, J., Fedus, W.: Emergent abilities of large language models .\nTransactions on Machine Learning Research (2022). Survey Certiﬁcation\n[82] Leek, J.T., Scharpf, R.B., Bravo, H.C., Simcha, D., Langmead, B., Joh nson,\nW.E., Geman, D., Baggerly, K., Irizarry, R.A.: Tackling the widespr ead and crit-\nical impact of batch eﬀects in high-throughput data. Nature Reviews Gene tics\n11(10), 733–739 (2010)\n[83] Huang, Q., Liu, Y., Du, Y., Garmire, L.X.: Evaluation of cell type annotation\nr packages on single-cell rna-seq data. Genomics, proteomics & bioinformat ics\n19(2), 267–281 (2021)\n[84] Pullin, J.M., McCarthy, D.J.: A comparison of marker gene selecti on methods\nfor single-cell rna sequencing data. bioRxiv, 2022–05 (2022)\n[85] Pliner, H.A., Shendure, J., Trapnell, C.: Supervised classiﬁ cation enables rapid\nannotation of cell atlases. Nature methods 16(10), 983–986 (2019)\n[86] Ravindra, N., Sehanobish, A., Pappalardo, J.L., Haﬂer, D.A., Dijk, D.: D isease\nstate prediction from single-cell data using graph attention networks . In: Pro-\nceedings of the ACM Conference on Health, Inference, and Learning, pp. 121–130\n(2020)\n[87] Salzberg, S.L.: Open questions: How many genes do we have? BMC biology\n16(1), 1–3 (2018)\n[88] Dixit, A., Parnas, O., Li, B., Chen, J., Fulco, C.P., Jerby-Arnon, L., Marjanovic,\n33\nN.D., Dionne, D., Burks, T., Raychowdhury, R., et al.: Perturb-seq: dissecting\nmolecular circuits with scalable single-cell rna proﬁling of pooled genetic screens.\ncell 167(7), 1853–1866 (2016)\n[89] Hecker, M., Lambeck, S., Toepfer, S., Van Someren, E., Guthke, R .: Gene\nregulatory network inference: data integration in dynamic models—a re view.\nBiosystems 96(1), 86–103 (2009)\n[90] Deshpande, A., Chu, L.-F., Stewart, R., Gitter, A.: Network infer ence with\ngranger causality ensembles on single-cell transcriptomics. Cell re ports 38(6),\n110333 (2022)\n[91] Su, C., Xu, Z., Shan, X., Cai, B., Zhao, H., Zhang, J.: Cell-type-spec iﬁc\nco-expression inference from single cell rna-sequencing data. Natur e Communi-\ncations 14(1), 4846 (2023)\n[92] Pearl, J., Mackenzie, D.: The Book of Why: the New Science of Cause an d Eﬀect.\nBasic books, ??? (2018)\n[93] Li, W.V., Li, J.J.: An accurate and robust imputation method scimpute for\nsingle-cell rna-seq data. Nature communications 9(1), 997 (2018)\n[94] Hou, W., Ji, Z., Ji, H., Hicks, S.C.: A systematic evaluation of single -cell rna-\nsequencing imputation methods. Genome biology 21, 1–30 (2020)\n[95] Marco Salas, S., Czarnewski, P., Kuemmerle, L.B., Helgadottir, S., Matts-\nson Langseth, C., Tiesmeyer, S., Avenel, C., Rehman, H., Tiklova, K., Anders-\nson, A., et al.: Optimizing xenium in situ data utility by quality as sessment and\nbest practice analysis workﬂows. bioRxiv, 2023–02 (2023)\n[96] Zappia, L., Phipson, B., Oshlack, A.: Splatter: simulation of single- cell rna\nsequencing data. Genome biology 18(1), 174 (2017)\n[97] Schaeﬀer, R., Miranda, B., Koyejo, S.: Are emergent abilities of lar ge language\nmodels a mirage? arXiv preprint arXiv:2304.15004 (2023)\n[98] Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amod ei, D.: Deep\nreinforcement learning from human preferences. Advances in neural information\nprocessing systems 30 (2017)\n[99] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Robert s, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K .,\nTsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.M.,\nPrabhakaran, V., Reif, E., Du, N., Hutchinson, B.C., Pope, R., Bradbury , J.,\nAustin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., G hemawat,\nS., Dev, S., Michalewski, H., Garc´ ıa, X., Misra, V., Robinson, K., Fed us, L.,\nZhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepas si,\n34\nR., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pe llat, M.,\nLewkowycz, A., Moreira, E.O., Child, R., Polozov, O., Lee, K., Zhou, Z ., Wang,\nX., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellste rn, K.S.,\nEck, D., Dean, J., Petrov, S., Fiedel, N.: Palm: Scaling language mode ling with\npathways. (2022)\n[100] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I ., et al.:\nLanguage models are unsupervised multitask learners. OpenAI blog 1(8), 9\n(2019)\n[101] Choromanski, K.M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl os,\nT., Hawkins, P., Davis, J.Q., Mohiuddin, A., Kaiser, L., Belanger, D .B., Colwell,\nL.J., Weller, A.: Rethinking attention with performers. In: Inte rnational Con-\nference on Learning Representations (2021). https://openreview.net/forum?id=\nUa6zuk0WRH\n[102] Du, J., Jia, P., Dai, Y., Tao, C., Zhao, Z., Zhi, D.: Gene2vec: distri buted\nrepresentation of genes based on co-expression. BMC genomics 20, 7–15 (2019)\n[103] Wu, C., Mark, A., Su, A.I.: Mygene. info: gene annotation query as a serv ice.\nbioRxiv, 009332 (2014)\n[104] Rubinsteyn, A., Nathanson, T., Kodysh, J., O’Donnell, T., Ahuja, A. , Ham-\nmerbacher, J., Aksoy, B., Bioinformatics, B., Grou` es, V., Hodes, I. : hammer-\nlab/pyensembl, version 1.1. 0. (Zenodo) (2017)\n[105] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farle y, D.,\nOzair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Ghahra-\nmani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K.Q. ( eds.)\nAdvances in Neural Information Processing Systems, vol. 27. Curran Asso-\nciates, Inc., ??? (2014).\nhttps://proceedings.neurips.cc/paper ﬁles/paper/2014/\nﬁle/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf\n[106] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville , A.C.: Improved\ntraining of wasserstein gans. Advances in neural information processin g systems\n30 (2017)\n[107] Doersch, C.: Tutorial on variational autoencoders. arXiv preprint\narXiv:1606.05908 (2016)\n[108] Misra, D.: Mish: A self regularized non-monotonic activation functi on. arXiv\npreprint arXiv:1908.08681 (2019)\n[109] Schmidt, T.: Coping with copulas. Copulas-From theory to applicati on in ﬁnance\n3, 1–34 (2007)\n[110] Tran, H.T.N., Ang, K.S., Chevrier, M., Zhang, X., Lee, N.Y.S., Goh, M., C hen,\n35\nJ.: A benchmark of batch-eﬀect correction methods for single-cell r na sequencing\ndata. Genome biology 21, 1–32 (2020)\n[111] Stephenson, E., Reynolds, G., Botting, R.A., Calero-Nieto, F.J., M organ, M.D.,\nTuong, Z.K., Bach, K., Sungnak, W., Worlock, K.B., Yoshida, M., et al.: Single-\ncell multi-omics analysis of the immune response in covid-19. Nature m edicine\n27(5), 904–916 (2021)\n[112] Sikkema, L., Ram´ ırez-Su´ astegui, C., Strobl, D.C., Gillett, T .E., Zappia, L.,\nMadissoon, E., Markov, N.S., Zaragosi, L.-E., Ji, Y., Ansari, M., et al.: An\nintegrated cell atlas of the lung in health and disease. Nature Medicine , 1–15\n(2023)\n[113] Litviˇ nukov´ a, M., Talavera-L´ opez, C., Maatz, H., Reichart, D., Worth, C.L., Lind-\nberg, E.L., Kanda, M., Polanski, K., Heinig, M., Lee, M., et al.: Cells of the\nadult human heart. Nature 588(7838), 466–472 (2020)\n[114] Palla, G., Spitzer, H., Klein, M., Fischer, D., Schaar, A.C., Kue mmerle, L.B.,\nRybakov, S., Ibarra, I.L., Holmberg, O., Virshup, I., et al.: Squidpy: a scalable\nframework for spatial omics analysis. Nature methods 19(2), 171–178 (2022)\n[115] Lin, Y., Cao, Y., Kim, H.J., Salim, A., Speed, T.P., Lin, D.M., Yang, P.,\nYang, J.Y.H.: scclassify: sample size estimation and multiscale classi ﬁcation of\ncells using single and multiple reference. Molecular systems bi ology 16(6), 9389\n(2020)\n[116] Franz´ en, O., Gan, L.-M., Bj¨ orkegren, J.L.: Panglaodb: a web serve r for explo-\nration of mouse and human single-cell rna sequencing data. Database 2019, 046\n(2019)\n[117] Stuart, T., Srivastava, A., Madad, S., Lareau, C.A., Satija, R.: Single- cell\nchromatin state analysis with signac. Nature methods 18(11), 1333–1341 (2021)\n[118] Li, Z., Zhou, X.: Bass: multi-scale and multi-sample analysis enable s accurate cell\ntype clustering and spatial domain detection in spatial transcriptomi c studies.\nGenome biology 23(1), 168 (2022)\n36\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nAppendixandExtendedDataFigures.pdf\nSupplementary\u0000le1KeywordsinAI.pdf\nSupplementary\u0000le2datasets.xlsx\nSupplementary\u0000le3models.xlsx\nSupplementary\u0000le4LLMstructure.pdf\nSupplementary\u0000le5datasetsource.xlsx",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.627355694770813
    },
    {
      "name": "Work (physics)",
      "score": 0.44370564818382263
    },
    {
      "name": "Computer science",
      "score": 0.44309982657432556
    },
    {
      "name": "Engineering",
      "score": 0.1701548993587494
    },
    {
      "name": "Economics",
      "score": 0.10727953910827637
    },
    {
      "name": "Management",
      "score": 0.0842922031879425
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}