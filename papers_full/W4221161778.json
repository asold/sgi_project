{
  "title": "Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut",
  "url": "https://openalex.org/W4221161778",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2309559497",
      "name": "Yangtao Wang",
      "affiliations": [
        "Institut polytechnique de Grenoble",
        "Centre National de la Recherche Scientifique",
        "Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A2102308878",
      "name": "Xi Shen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2937147546",
      "name": "Shell Xu Hu",
      "affiliations": [
        "Samsung (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2041852224",
      "name": "Yuan Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183168386",
      "name": "James L. Crowley",
      "affiliations": [
        "Université Grenoble Alpes",
        "Institut polytechnique de Grenoble",
        "Laboratoire d'Informatique de Grenoble",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2109119983",
      "name": "Dominique Vaufreydaz",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Institut polytechnique de Grenoble",
        "Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3070936185",
    "https://openalex.org/W2964352379",
    "https://openalex.org/W3152635971",
    "https://openalex.org/W3034315787",
    "https://openalex.org/W6697443983",
    "https://openalex.org/W2047670868",
    "https://openalex.org/W2964274719",
    "https://openalex.org/W3035725370",
    "https://openalex.org/W3110272085",
    "https://openalex.org/W2114030927",
    "https://openalex.org/W7746136",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6799515669",
    "https://openalex.org/W1920234547",
    "https://openalex.org/W6675869821",
    "https://openalex.org/W6767021658",
    "https://openalex.org/W6779397410",
    "https://openalex.org/W6780545476",
    "https://openalex.org/W6789505266",
    "https://openalex.org/W6742421098",
    "https://openalex.org/W21025885",
    "https://openalex.org/W2964028976",
    "https://openalex.org/W2559924584",
    "https://openalex.org/W2086052791",
    "https://openalex.org/W2962749812",
    "https://openalex.org/W2161185676",
    "https://openalex.org/W2002781701",
    "https://openalex.org/W6660344663",
    "https://openalex.org/W1963920598",
    "https://openalex.org/W2963441581",
    "https://openalex.org/W6760239191",
    "https://openalex.org/W6762226699",
    "https://openalex.org/W2984303785",
    "https://openalex.org/W3176171254",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W2807912089",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W1966601141",
    "https://openalex.org/W2088049833",
    "https://openalex.org/W6796634059",
    "https://openalex.org/W3167700966",
    "https://openalex.org/W3040134288",
    "https://openalex.org/W2926779152",
    "https://openalex.org/W2295160225",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6660241312",
    "https://openalex.org/W1919709169",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3035712793",
    "https://openalex.org/W2950557962",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6640860105",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6743428213",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6796761347",
    "https://openalex.org/W6786010853",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2254462240",
    "https://openalex.org/W6792919013",
    "https://openalex.org/W2765793020",
    "https://openalex.org/W6686164453",
    "https://openalex.org/W6775741425",
    "https://openalex.org/W2606520630",
    "https://openalex.org/W3204171527",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2107200795",
    "https://openalex.org/W3209709214",
    "https://openalex.org/W2616247523",
    "https://openalex.org/W6696994110",
    "https://openalex.org/W6677945368",
    "https://openalex.org/W2740667773",
    "https://openalex.org/W2963045696",
    "https://openalex.org/W2294182682",
    "https://openalex.org/W2979654309",
    "https://openalex.org/W4287760472",
    "https://openalex.org/W2121947440",
    "https://openalex.org/W3016163669",
    "https://openalex.org/W3130976481",
    "https://openalex.org/W2970642899",
    "https://openalex.org/W2186094539",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4287120947",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2167090521",
    "https://openalex.org/W3103376464",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035318183",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3107169861",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2746314669",
    "https://openalex.org/W4288375254",
    "https://openalex.org/W4239147634",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W2920182456",
    "https://openalex.org/W2105628432",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W4287727117",
    "https://openalex.org/W1952794764",
    "https://openalex.org/W2963087201",
    "https://openalex.org/W2039313011",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3106670560",
    "https://openalex.org/W4297776992",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3214096168"
  ],
  "abstract": "Transformers trained with self-supervised learning using self-distillation\\nloss (DINO) have been shown to produce attention maps that highlight salient\\nforeground objects. In this paper, we demonstrate a graph-based approach that\\nuses the self-supervised transformer features to discover an object from an\\nimage. Visual tokens are viewed as nodes in a weighted graph with edges\\nrepresenting a connectivity score based on the similarity of tokens. Foreground\\nobjects can then be segmented using a normalized graph-cut to group\\nself-similar regions. We solve the graph-cut problem using spectral clustering\\nwith generalized eigen-decomposition and show that the second smallest\\neigenvector provides a cutting solution since its absolute value indicates the\\nlikelihood that a token belongs to a foreground object. Despite its simplicity,\\nthis approach significantly boosts the performance of unsupervised object\\ndiscovery: we improve over the recent state of the art LOST by a margin of\\n6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The\\nperformance can be further improved by adding a second stage class-agnostic\\ndetector (CAD). Our proposed method can be easily extended to unsupervised\\nsaliency detection and weakly supervised object detection. For unsupervised\\nsaliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,\\nDUT-OMRON respectively compared to previous state of the art. For weakly\\nsupervised object detection, we achieve competitive performance on CUB and\\nImageNet.\\n",
  "full_text": "SELF -SUPERVISED TRANSFORMERS FOR UNSUPERVISED\nOBJECT DISCOVERY USING NORMALIZED CUT\nAUTHOR VERSION OF CVPR22 PAPER\nYangtao Wang1, Xi Shen2,3,∗, Shell Xu Hu4, Yuan Yuan5, James L. Crowley1, Dominique Vaufreydaz1\n1 Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n2 Tencent AI Lab 3 LIGM (UMR 8049) - Ecole des Ponts, UPE\n4 Samsung AI Center, Cambridge 5 MIT CSAIL\nAbstract\nTransformers trained with self-supervision using self-\ndistillation loss (DINO) have been shown to produce at-\ntention maps that highlight salient foreground objects. In\nthis paper, we show a graph-based method that uses the\nself-supervised transformer features to discover an object\nfrom an image. Visual tokens are viewed as nodes in\na weighted graph with edges representing a connectivity\nscore based on the similarity of tokens. Foreground ob-\njects can then be segmented using a normalized graph-\ncut to group self-similar regions. We solve the graph-cut\nproblem using spectral clustering with generalized eigen-\ndecomposition and show that the second smallest eigen-\nvector provides a cutting solution since its absolute value\nindicates the likelihood that a token belongs to a fore-\nground object.\nDespite its simplicity, this approach signiﬁcantly boosts\nthe performance of unsupervised object discovery: we\nimprove over the recent state-of-the-art LOST by a mar-\ngin of 6.9%, 8.1%, and 8.1% respectively on the VOC07,\nVOC12, and COCO20K. The performance can be further\nimproved by adding a second stage class-agnostic detec-\ntor (CAD). Our proposed method can be easily extended\nto unsupervised saliency detection and weakly supervised\nobject detection. For unsupervised saliency detection, we\nimprove IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,\nDUT-OMRON respectively compared to state-of-the-art.\nFor weakly supervised object detection, we achieve com-\npetitive performance on CUB and ImageNet. Our code is\navailable at: https://www.m-psi.fr/Papers/TokenCut2022/\n1 Introduction\nObject detection is a key enabling technology for real-\nworld vision systems for tasks such as robotics, au-\ntonomous driving, trafﬁc monitoring, manufacturing, and\nembodied artiﬁcial intelligence [21, 63, 64]. However,\n*Corresponding Author\n(a) DINO [6]\n (b) LOST [45].\n (c) TokenCut (ours)\n(d) Attention maps associated to different patches\nFigure 1: The attention map of the class token used in\nDINO [6] (a) and the map of the inverse degrees used in\nLOST [45] (b) are noisy for foreground / background sep-\naration. Our proposed method provides a clean attention\nmap that can be used to segment salient objects (c). Con-\nsidering the attention map associated to different patches\nhighlight different regions of the object (d), it is reason-\nable to build a graph computing attention maps from mul-\ntiple patches.\nthe performance of current state-of-the-art object detec-\ntors is limited by the high cost of annotating sufﬁcient\ntraining data [33] for supervised learning. This limitation\nbecomes even more apparent when using transfer learning\nto adapt a pre-trained object detector to a new application\ndomain. Approaches such as active learning [1], semi-\nsupervised learning [34], and weakly-supervised learn-\ning [39] have attempted to overcome this barrier by pro-\nviding more efﬁcient learning, but with only limited suc-\ncess.\narXiv:2202.11539v2  [cs.CV]  24 Mar 2022\nAuthor version of CVPR22 paper\nIn this work, we focus on object discovery in natural im-\nages with no human annotations. This is an important\nproblem and a critical step for many downstream applica-\ntions [56]. Poor object discovery can lead to poor overall\nsystem performance. Current approaches for this prob-\nlem adopt some forms of bounding box proposal mech-\nanism [11, 55, 56, 61] and formulate object discovery as\nan optimization problem. However, this approach can be\ncomputationally expensive [55] as each pair of bounding\nbox proposals across different images needs to be com-\npared, and the optimization may fail to scale to larger\ndatasets due to the quadratic computation overhead [57].\nTransformers have recently been shown to outperform\nconvolutional neural networks for visual recognition. Vi-\nsion Transformers, such as ViT [18] accept image patches\nas input tokens and use stacked layers of encoders with\nself-attention to map tokens to image-level class labels.\nRecent results with DINO [6] have shown that when\ntrained with self-distillation loss [25], the attention maps\nassociated to the class token from the last layer indicate\nsalient foreground regions. However, as illustrated in\nFig. 1a, such attention maps are noisy and it is not clear\nthat they can be used for unsupervised object discovery.\nWith LOST [45], the authors propose construction of a\ngraph and use the inverse degrees of nodes to segment ob-\njects. A heuristic seed expansion strategy is used to over-\ncome noise (Fig. 1b) and detect a single bounding box for\na foreground object. The attention maps associated with\ndifferent nodes often contain meaningful information, as\nillustrated in Fig. 1d. We have investigated whether it is\npossible to use the information in the entire graph by pro-\njecting the graph into a low dimensional subspace using\neigendecomposition. We have discovered that such a pro-\njection can be used with Normalized Cut [43] (Ncut) to\nsigniﬁcantly improve foreground / background segmenta-\ntion (Fig. 1c).\nIn this paper, we propose TokenCut, a simple but effective\ngraph-based approach for unsupervised object discovery.\nWe build on the self-supervised vision transformer trained\nwith DINO [6] as our backbone feature encoder and locate\nobjects with the resulting features. Instead of using only\nthe class token, we use all token features. We construct\nan undirected graph based on the token features in the last\nself-attention layer, where the visual tokens are viewed\nas graph nodes with edges representing a connectivity\nscore based on similarity of the features. We then use\na normalized graph-cut to group self-similar regions and\ndelimit the foreground objects. We solve the graph-cut\nproblem using spectral clustering with generalized eigen-\ndecomposition and show that the second smallest eigen-\nvector provides a cutting solution indicating the likelihood\nthat a token belongs to a foreground object. Our approach\ncan be considered as a run-time adaptation, which means\nthat the model is able to adapt to each speciﬁc test image\ndespite the shared training model.\nDespite its simplicity, our approach signiﬁcantly im-\nproves unsupervised object discovery. The method\nachieves 68.8%, 72.1% and 58.8% on VOC07 [19],\nVOC12 [20], COCO20K [33] respectively, thus outper-\nforming LOST [45] by a margin of 6.9%, 8.1% and 8.1%\nrespectively. TokenCut with second stage CAD further\nimproves the performance to 71.4%, 75.3% and 62.6%\non VOC07, VOC12, COCO20k respectively, which out-\nperforms LOST + CAD by 5.7%, 4.9% and 5.1% respec-\ntively.\nIn addition, we show that TokenCut can be easily ex-\ntended to weakly supervised object detection and unsuper-\nvised saliency detection. For weakly supervised object de-\ntection, the goal is to detect objects using only image-level\nannotations. We freeze the encoder and ﬁne-tune a linear\nclassiﬁer with weakly-supervised image labels. We then\napply TokenCut on the features extracted from the ﬁne-\ntuned encoder. Our approach produces clearly improved\nresults on the CUB dataset [59] and competitive perfor-\nmance on ImageNet-1K [14]. For unsupervised saliency\ndetection, we use the foreground region discovered by\nthe proposed approach and apply Bilateral Solver [5] as\na post-processing step to reﬁne edges of the foreground\nregion. In terms of results, our approach signiﬁcantly im-\nproves previous state-of-the-art methods on ECSSD [44],\nDUTS [60] and DUT-OMRON [67].\nIn summary, our main contributions are as follows:\n• We propose a simple and effective method to discover\nobjects in images without supervision based on the self-\nsupervised vision transformers. This method signif-\nicantly outperforms previous state-of-the-art methods\nfor unsupervised object discovery when tested on mul-\ntiple datasets;\n• We extend the proposed method to weakly-supervised\nobject detection and show that the simple approach can\nachieve competitive performance;\n• We also show that this method can be used for unsuper-\nvised saliency detection. The results demonstrate that\nTokenCut signiﬁcantly improves the previous state-of-\nthe-art performance on multiple datasets.\n2 Related Work\nSelf-supervised vision transformers. ViT [18] has\nshown that a transformer architecture [53] can be used as\nan effective encoder for images and provide useful fea-\ntures for supervised vision tasks. MoCo-v3 [8] demon-\nstrated that ViT can provide self-supervised representa-\ntion learning and achieve strong results using contrastive\nlearning. Recently, DINO [6] proposed to train transform-\ners with self-distillation loss [25], showing that ViT con-\ntains explicit information that can be used for semantic\nsegmentation of an image. Inspired by BERT [16], [32]\nproposed MST, which dynamically masks some tokens\nand learns to recover missing tokens using a global im-\nage decoder. Also motivated by BERT [16], BEIT [4] ﬁrst\ntokenizes the original image into visual tokens then ran-\ndomly mask some tokens and learn to recover them using\na transformer. Recently, MAE [23] masks a high propor-\n2\nAuthor version of CVPR22 paper\nFigure 2: An overview of the TokenCut approach. We construct a graph where the nodes are tokens and the edges\nare similarities between the tokens using transformer features. The foreground and background segmentation can be\nsolved by Ncut [43]. Performing bi-partition on the second smallest eigenvector allows to detect foreground object.\ntion of images and reconstructs the missing pixels with an\nasymmetric encoder-decoder.\nUnsupervised object discovery. Given a group of im-\nages, unsupervised object discovery seeks to discover and\ndelimit similar objects that appear in multiple images.\nSome methods [9, 26, 28, 29, 54] are designed to segment\ncommon repeated objects in an image collection, but rely\non strong assumptions about the frequency of appearance\nof an object. Other approaches [11, 50, 55, 56] use bound-\ning box proposals and formulate the object discovery as\nan optimization problem. [57] proposed a novel formula-\ntion of unsupervised object discovery as a ranking prob-\nlem and showed that discovery could be scaled to datasets\nwith more than 10K images. Recently, LOST [45] sig-\nniﬁcantly improved over state-of-the-art for unsupervised\nobject discovery. LOST extracts features using a self-\nsupervised transformer based on DINO [6] and designs a\nheuristic seed expansion strategy to obtain a single object\nregion. Our work is closely related to LOST [45], as we\nalso use self-supervised transformer features. However,\nrather than relying on the attention map of some speciﬁc\nnodes, we propose a graph-based method that employs\nthe attention scores of all the nodes and can be used with\nNcut [43] to obtain a more precise segmentation of the\nimage object.\nWeakly supervised object detection. Weakly super-\nvised object detection [22, 68, 69, 72] can be used to\nlocate image objects using only image-level annotation.\nEarly approaches [7, 41, 77] mainly relied on a Class Ac-\ntivation Map (CAM) which is introduced in [77] to gen-\nerate class-speciﬁc localization maps and ﬁnd discrimi-\nnant regions. Several methods [12, 13, 36, 46, 69, 75]\nhave been proposed to improve CAM by erasing the\ndiscriminant regions and forcing the networks to cap-\nture additional object regions. Data augmentation tech-\nniques such as Cutout [17] and CutMix [70] have been\nshown to provide improvement for both classiﬁcation and\nlocalization performance. Some methods achieve both\nclassiﬁcation and localization using two separate net-\nworks [22, 35, 71]. [71] trained the localization network\nusing pseudo bounding boxes generated by [61]. [71] ﬁrst\nlearns a classiﬁer, then freeze its weights and train another\ndetector. [22] learns a regressor and a classiﬁer using the\nconsistency of CAM between two transformations. Un-\nlike these approaches, which are speciﬁcally designed for\nweakly supervised object detection, we propose an uni-\nﬁed solution to both unsupervised object discovery and\nweakly supervised object detection based on transformer.\nUnsupervised saliency detection. Unsupervised\nsaliency detection seeks to segment a salient ob-\njects within an image. Earlier work on this problem\n[27, 31, 66, 78] used classical techniques such as\ncolor contrast [10], certain background priors [62], or\nsuper-pixels [31, 67]. More recently, unsupervised deep\nmodels [37, 60, 73] have incorporated heuristic saliency\nmethods as pseudo ground truth to train deep CNN\nmodels. However, these methods rely on a CNN model\npretrained with supervised training. [58] has proposed\nan unsupervised Large-Scale GAN that does not make\nuse of labels during training. In the following, we show\nthat incorporating a simple post-processing step into\nour unsupervised object discovery can provide a strong\nbaseline method for unsupervised saliency detection.\n3 Approach: TokenCut\nThe TokenCut algorithm can be used to predict bounding\nboxes that locate a salient object in an image. Our ap-\nproach, illustrated in Fig. 2, is based on a graph where the\nnodes are tokens and the edges are similarities between\nthe tokens using features based on the latent variables of\nthe transformer. In the following, we ﬁrst brieﬂy present\nvision transformers in Section 3.1.1 and Normalized Cut\nin Section 3.1.2. We then introduce our solution and the\nimplementation details in Section 3.2.\n3.1 Background\n3.1.1 Vision Transformers\nGiven an image of size H ×W, vision transformers\n(ViT) [18] take non-overlapping 2D image patches of res-\nolutions K ×K as inputs, with the number of patches\nN = HW/K2. Each patch is represented as a token, de-\nscribed by a vector of numerical features, referred to as an\nembedding. An extra learnable token, denoted as a class\ntoken CLS, is used to represent the aggregated informa-\ntion of the entire set of patches. This CLS token and the\n3\nAuthor version of CVPR22 paper\nset of patch tokens are fed to a standard transformer net-\nwork with a “pre-norm” layer normalization [2].\nThe vision transformer is composed of a multiple layers\nof encoders, each with feed-forward networks and multi-\nple attention heads for self-attention, paralleled with skip\nconnections. For the unsupervised object discovery task,\nwe use a vision transformer trained with self-supervised\nlearning using DINO [6] and extract latent variables from\nthe ﬁnal layer as the input features for our proposed\nmethod.\n3.1.2 Normalized Cut (Ncut)\nGraph partitioning. Ncut [43] can be used to partition\na graph into two disjoint sets A and B. The method parti-\ntions the graph so as to minimizing the Ncut energy [43]:\nNcut(A,B) = C(A,B)\nC(A,V) + C(A,B)\nC(B,V) (1)\nwhere C measures the degree of similarity between two\nsets. C(A,B) = ∑\nvi∈A,vj∈BEi,j and C(A,V) is the\ntotal connection from nodes in A to all nodes in the graph.\nAs shown by Shi and Malik [43], the optimization prob-\nlem in Eqn 1 is equivalent to:\nminxNcut(x) =miny\nyT(D−E )y\nyTDy . (2)\nWith the condition y ∈{1,−b}N, b satisﬁes yTD1 = 0.\nDis a diagonal matrix with di = ∑\njEi,j on its diagonal.\nNcut solution with the relaxed constraint.Taking z =\nD\n1\n2 y. Eqn 2 can be rewrite as:\nminz\nzTD−1\n2 (D−E )D−1\n2 z\nzTz . (3)\nIndicating in [43], the formulation in Eqn 3 is equivalent\nto the Rayleigh quotient [52], which is equivalent to solve\nD−1\n2 (D−E )D−1\n2 z = λz, where D−E is the Lapla-\ncian matrix and known to be positive semideﬁnite [38].\nTherefore z0 = D\n1\n2 1 is an eigenvector associated to the\nsmallest eigenvalue λ = 0. According to Rayleigh quo-\ntient [52], the second smallest eigenvector z1 is perpen-\ndicular to the smallest one ( z0) and can be used to mini-\nmize the energy in Eqn 3,\nz1 = argminzT z0\nzTD−1\n2 (D−E )D−1\n2 z\nzTz .\nTaking z = D\n1\n2 y,\ny1 = argminyT D1=0\nyT(D−E )y\nyTDy .\nThus, the second smallest eigenvector of the generalized\neigensystem (D−E )y = λDy is the real valued solution\nto the Ncut [43] problem.\n3.2 TokenCut Algorithm\nGraph construction. Our method uses the vision trans-\nformers as described in Section 3.1.1 to produce a vector\nof features for eachK×Kimage patch. A fully connected\nundirected graph G= ( V, E) of patches is constructed\nwhere each V represents a patch with a feature vector\n{vi}N\ni=1, and each patch is linked to adjacent patches by\nlabeled edges, E. Edge labels represent a similarity score\nS based on the cosine similarity of the feature vectors of\nthe two patches.\nEi,j =\n{1, if S(vi,vj) ≥τ\nϵ, else , (4)\nwhere τ is a hyper-parameter and ϵequals a small value\n1e−5 to assure that the graph is fully connected and Sis\nthe cosine similarity between features. Note that the spa-\ntial information has been implicitly included in the fea-\ntures via positional encoding in transformer.\nS(vi,vj) = vivj\n∥vi∥2∥vj∥2\n. (5)\nWe apply Ncut algorithm, as described in Section 3.1.2,\non constructed graph Gand obtain the second smallest\neigenvector of the generalized eigensystem, which can be\nseen as an attention map of the potential objects. We pro-\nvide visualization of this attention map in Section 4.\nDiscovering Salient Object with TokenCut. We as-\nsume that there is at least one object in the image and the\nobject occupies the foreground region. To successfully\nsegment the foreground objects from the image, we must\nsolve three problems: i) We must determine a means to\npartition the graph into two subgraphsand ii) given a bi-\npartition of the graph, we must determine which partition\nrepresents the foreground. iii) In case of detecting mul-\ntiple connected components in the foreground, we must\nidentify the most salient object.\nFor the ﬁrst problem, in our initial experiments we have\nused a simple average value of the projection onto the\nsecond smallest eigenvector to determine the similarity\nvalue for cutting the graph y1 = 1\nN\n∑\niyi\n1. Formally,\nA = {vi|yi\n1 ≤y1}and B = {vi|yi\n1 > y1}. We have\ncompared this to using the classical clustering algorithms\nof K-means and EM to cluster the second smallest eigen\nvector into 2 partitions. The comparison is available in the\nsupplementary material Table 7, indicating that the mean\ngenerally provides better results.\nFor the second problem, the foreground contains the\nsalient object and is assumed to be less connected to the\n4\nAuthor version of CVPR22 paper\n(a) DINO CLS\nToken Attention\n(b) DINO\nDetection\n(c) LOST Inverse\nDegree Attention\n(d) LOST\nDetection\n(e) Our Eigen\nAttention\n(f) Our\nDetection\nFigure 3: Visual results of unsupervised single object discovery on VOC12.In (a), we show the attention of the\nCLS token in DINO [6] which is used for detection (b). LOST [45] is mainly relied on the map of inverse degrees (c)\nto perform detection (d). For our approach, we illustrate the eigenvector in (e) and our detection in (f). Blue and Red\nbounding boxes indicate the ground-truth and the predicted bounding boxes respectively.\nentire graph. Intuitively, di <dj if vi belongs to the fore-\nground while vj is the background token. Therefore, the\neigenvector of the foreground object should have a larger\nabsolute value than the one of the background. We use the\nmaximum absolute value vmax to select the foreground\npartition and the most salient object. The partition that\ncontains vmaxis taken as foreground. As our graph has no\nexplicit spatial constraint, the foreground might contain\nmore than one connected regions. We simply select the\nlargest connected component existing in the foreground\ncontaining the maximum absolute value vmax as our ﬁnal\nobject region.\nIn summary, the TokenCut algorithm consists of the fol-\nlowing steps:\n1. Given an image, build a graph G= (V, E) accord-\ning to Equation 4 and 5.\n2. Solve the generalized eigensystem (D−E )y =\nλDy for the eigenvector associated to the second\nsmallest eigenvalue y1.\n3. Compute bi-partition using the average over y1:\ny1 = ∑\ni\nyi\n1\nN . A = {vi|yi\n1 ≤ y1}and B =\n{vi|yi\n1 >y1}\n4. Find the largest connected component associated\nto the maximum absolute value of y1.\nImplementation details. For our experiments, we use\nthe ViT-S/16 model [18] trained with self-distillation loss\n(DINO) [6] to extract features of patches. We employ the\nkeys features of the last layer as the input featuresv. Abla-\ntions on different features as well as transformers trained\nwith self-supervised learning are provided in the supple-\nmentary material Table 5. We set τ = 0.2 for all datasets,\nthe dependency on τ is provided in Section 4.4. In terms\nof running time, our un-optimised implementation takes\napproximately 0.32 seconds to detect a bounding box of a\nsingle image with resolution 480 ×480 on a single GPU\nQUADRO RTX 8000.\n4 Experiments\nWe evaluate our approach on three tasks: unsupervised\nsingle object discovery, weakly supervised object detec-\ntion and unsupervised saliency detection. We present\nresults of unsupervised single object discovery in Sec-\ntion 4.1. The results of weakly supervised object detection\nare in Section 4.2. The results of unsupervised saliency\ndetection in Section 4.3. We provide analysis of τ in Sec-\n5\nAuthor version of CVPR22 paper\nTable 1: Comparisons for unsupervised single object discovery. We compare TokenCut to state-of-the-art object\ndiscovery methods on VOC07 [19], VOC12 [20] and COCO20K [33, 56] datasets. Model performances are evaluated\nwith CorLoc metric. “Inter-image Simi.” means the model leverages information from the entire dataset and explores\ninter-image similarities to localize objects.\nMethod Inter-image Simi. DINO [6] Feat. VOC07 [19] VOC12 [20] COCO20K [33, 56]\nSelective Search [45, 51] \u0017 - 18.8 20.9 16.0\nEdgeBoxes [45, 79] \u0017 - 31.1 31.6 28.8\nKim et al. [30, 45] \u0013 - 43.9 46.4 35.1\nZhange et al. [45, 74] \u0013 - 46.2 50.5 34.8\nDDT+ [45, 61] \u0013 - 50.2 53.1 38.2\nrOSD [45, 56] \u0013 - 54.5 55.3 48.5\nLOD [45, 57] \u0013 - 53.6 55.1 48.5\nDINO-seg [6, 45] \u0017 ViT-S/16 [18] 45.8 46.2 42.1\nLOST [45] \u0017 ViT-S/16 [18] 61.9 64.0 50.7\nTokenCut \u0017 ViT-S/16 [18] 68.8 (↑6.9) 72.1 ( ↑8.1) 58.8 ( ↑8.1)\nLOD + CAD⋆[45] \u0013 - 56.3 61.6 52.7\nrOSD + CAD⋆[45] \u0013 - 58.3 62.3 53.0\nLOST + CAD⋆[45] \u0017 ViT-S/16 [18] 65.7 70.4 57.5\nTokenCut + CAD⋆[45] \u0017 ViT-S/16 [18] 71.4 (↑5.7) 75.3 ( ↑4.9) 62.6 ( ↑5.1)\n⋆ +CAD indicates to train a second stage class-agnostic detector with “pseudo-boxes” labels.\ntion 4.4, other ablation studies will be presented in sup-\nplementary material.\n4.1 Unsupervised Single Object Discovery\nEvaluation metric. We report performance using the\nCorLoc metric for precise localisation, as used by [11, 15,\n47, 55–57, 61]. CorLoc counts a predicted bounding box\nas correct if the intersection over union (IoU) score be-\ntween the predicted bounding box and one of the ground\ntruth bounding boxes is superior to 0.5.\nQuantitative Results. We evaluate our approach on\nthree commonly used benchmarks for unsupervised sin-\ngle object discovery: VOC07 [19] , VOC12 [20] and\nCOCO20K [33, 56]. The qualitative results are pro-\nvided in Tab. 1. We evaluate the CorLoc scores in\ncomparison with previous state-of-the-art single object\ndiscovery methods [30, 45, 51, 56, 57, 61, 74, 79] on\nVOC07, VOC12, and COCO20K datasets. These meth-\nods can be roughly divided into two groups based on\nwhether the model leverages information from the en-\ntire dataset and explores inter-image similarities or not.\nBecause of quadratic complexity of region comparison\namong images, models with inter-image similarities are\ngenerally difﬁcult to scale to larger datasets. The selective\nsearch [51], edge boxes [79], LOST [45] and TokenCut\ndo not require inter-image similarities and are thus much\nmore efﬁcient. As shown in the table, TokenCut consis-\ntently outperforms all previous methods on all datasets by\na large margin. Speciﬁcally, TokenCut improves the state-\nof-the-art by 6.9%, 8.1% and 8.1% in VOC07, VOC12\nand COCO20K respectively using the same ViT-S/16 fea-\ntures.\nWe also list a set of results that including a second stage\nunsupervised training strategy to boost the performance,\nThis is referred to as class-agnostic detection (CAD). A\nCAD is trained by assigning the same “foreground” cat-\negory to all the boxes produced by the ﬁrst stage single\nobject discovery model. As shown in Tab. 1, TokenCut +\nCAD outperforms the state-of-the-art by 5.7%, 4.9% and\n5.1% on VOC07, VOC12 and COCO20k respectively.\nQualitative Results. In Fig. 3, we provide visualization\nfor DINO-seg [6], LOST [45] and Tokencut. For each\nmethod, we visualize the heatmap that is used to perform\nobject detection. For DINO-seg, the heatmap is the at-\ntention map associated to the CLS token. For LOST, the\ndetection is mainly based on the map of inverse degree\n( 1\ndi\n). For TokenCut, we display the second smallest eigen-\nvector. The visual result demonstrates that Tokencut can\nextract a high quality segmentation of the salient object.\nComparing with DINO-seg and LOST, TokenCut is able\nto extract a more complete segmentation as can be seen\nin the ﬁrst and the third samples in Fig. 3. In some other\ncases, when all the methods have a high quality map, To-\nkenCut has the strongest intensity on the object, this phe-\nnomenon can be viewed in the last sample in Fig. 3. More\nvisual results can be found in the supplementary material\nFig. 7 and Fig. 8.\n4.2 Weakly Supervised Object Localization\nEvaluation metrics. We report three standard metrics:\nTop-1 Cls, GT Locand Top-1 Loc. Top-1 Cls represents\nthe top-1 accuracy of image classiﬁcation. GT Loc is sim-\nilar to CorLoc in which a predicted box is counted as cor-\nrect if the IoU score is superior to 0.5 between the pre-\ndicted bounding box and one of the ground-truth bound-\ning boxes. Top-1 Loc is the most important metric as it\n6\nAuthor version of CVPR22 paper\nTable 2: Comparisons for weakly supervised object localization.We report Top-1 Cls, GT Loc and Top-1 Loc\non CUB [59] and ImageNet-1K [14] datasets. Compared state-of-the-art methods are divided into two groups: with\nImageNet-1K supervised pretraining and with ImageNet-1K self-supervised pretraining.\nPretrained Dataset Method Backbone CUB [59], Acc. (%) ImageNet-1K [14], Acc. (%)\nTop-1 Cls GT Loc Top-1 Loc Top-1 Cls GT Loc Top-1 Loc\nImageNet-1K [14]\nsupervised pretrain\nCAM [77] GoogLeNet [48] 73.8 - 41.1 65 - 43.6\nHaS-32 [46] + [3] GoogLeNet [48] 75.4 61.1 47.4 68.9 60.6 44.6\nADL [13] + [3] ResNet50 [24] 75.0 77.6 59.5 75.8 62.2 49.4\nADL [13] InceptionV3 [49] 74.6 - 53.0 72.8 - 48.7\nI2C [76] InceptionV3 [49] - 72.6 56 73.3 68.5 53.1\nPSOL [71]‡ InceptionV3 [49] - - 65.5 - 65.2 54.8\nSLT-Net [22]⋆ InceptionV3 [49] 76.4 86.5 66.1 78.1 67.6 55.7\nImageNet-1K [14]\nself-supervised pretrain\nLOST [45] ViT-S/16 [18] 79.5 89.7 71.3 77.0 60.0 49\nTokenCut ViT-S/16 [18] 79.5 91.8 ( ↑2.1) 72.9 (↑1.6) 77.0 65.4 ( ↑5.4) 52.3 (↑3.3)\n⋆uses ten crop augmentations to get ﬁnal classiﬁcation results. ⋆and ‡learn a classifer and a detector separately.\nTable 3: Comparisons for unsupervised saliency detectionWe compare TokenCut to state-of-the-art unsupervised\nsaliency detection methods on ECSSD [44], DUTS [60] and DUT-OMRON [67]. TokenCut achieves better results\ncomparing with other competitive approaches.\nMethod ECSSD [44] DUTS [60] DUT-OMRON [67]maxFβ(%) IoU(%) Acc.(%) maxFβ(%) IoU(%) Acc.(%) maxFβ(%) IoU(%) Acc.(%)\nHS [66] 67.3 50.8 84.7 50.4 36.9 82.6 56.1 43.3 84.3wCtr [78] 68.4 51.7 86.2 52.2 39.2 83.5 54.1 41.6 83.8WSC [31] 68.3 49.8 85.2 52.8 38.4 86.2 52.3 38.7 86.5DeepUSPS [37] 58.4 44.0 79.5 42.5 30.5 77.3 41.4 30.5 77.9BigBiGAN [58] 78.2 67.2 89.9 60.8 49.8 87.8 54.9 45.3 85.6E-BigBiGAN [58] 79.7 68.4 90.6 62.4 51.1 88.2 56.3 46.4 86.0LOST [42, 45] 75.8 65.4 89.5 61.1 51.8 87.1 47.3 41.0 79.7LOST [42, 45]+Bilateral Solver [5] 83.7 72.3 91.6 69.7 57.2 88.7 57.8 48.9 81.8\nTokenCut 80.3 71.2 91.8 67.2 57.6 90.3 60.0 53.3 88.0TokenCut + Bilateral Solver [5] 87.4 (↑3.7) 77.2(↑4.9) 93.4 (↑1.8) 75.5(↑5.8) 62.4 (↑5.2) 91.4 (↑2.7) 69.7 (↑11.9) 61.8 (↑12.9) 89.7 (↑7.9)\nconsiders measuring both the classiﬁcation and the detec-\ntion: a predicted bounding box is counted as a true posi-\ntive if the class of the image is correctly predicted and the\nIoU is superior to 0.5 between the predicted bounding box\nand the ground-truth bounding box.\nResults. We use two datasets to evaluate model perfor-\nmances on weakly supervised object localization: CUB-\n200-2011 [59] (CUB) and ImangeNet-1k [40]. The ﬁne-\ntuning details can be found in supplementary material.\nIn Tab. 2, we compare TokenCut to the state-of-the-\nart weakly-supervised object localization approaches on\nCUB and ImageNet-1K datasets. The methods can be di-\nvided to two groups: models initialized with ImageNet-\n1K supervised pre-training [3, 13, 22, 46, 71, 76, 77] and\nmodels inilialized with ImageNet-1K self-supervised pre-\ntraining [45].\nOn the CUB dataset, TokenCut achieves the best per-\nformance over all methods, and outperforms the state-\nof-the-art LOST method by 2.1% and 1.6% on GT Loc\nand Top-1 Loc. Interestingly, all the ImageNet-1K self-\nsupervised pretraining models are better than the super-\nvised pretrained models. We believe that this is because\nsupervised pretraining learns a more discriminative rep-\nresentation of the pretrained dataset than self-supervised\npretraining, leading to a reduction in transferability to\ndownstream datasets such as CUB. In comparison, self-\nsupervised pretraining can learn a more general represen-\natation and thus provides better transferbility.\nOn the ImageNet-1K dataset, TokenCut outperforms\nLOST by 5.4% and 4.4% on GT Loc and Top-1 Loc, and\nachieves a comparable performance with the ImageNet-\n1K supervised pretrain model. If the downstream task\nis ImageNet-1K, then the supervised pretraining with\nImageNet-1K can provide discriminative features that im-\nprove the localization task as they are tuned to the dataset.\nTable 4: Analysis of τ. We report CorLoc for un-\nsupervised single object discovery on VOC07, VOC12,\nCOCO20K, and Top-1 Loc for weakly supervised object\ndetection on CUB and ImageNet-1K.\nτ CorLoc Top-1 LocVOC07 VOC12 COCO20K CUB ImageNet-1K\n0 67.4 71.3 56.1 73.0 53.80.1 68.6 72.1 58.2 73.2 53.40.2 68.8 72.1 58.8 72.9 52.30.3 67.7 72.1 58.2 70.8 50.4\n4.3 Unsupervised Saliency detection\nEvaluation Metrics We report three standard metrics:\nF-measure, IoU and Accuracy. F-measure is a standard\nmeasure in saliency detection. It is computed as Fβ =\n(1+β2)Precision×Recall\nβ2Precision+Recall , where the Precision and Recall\nare deﬁned based on the binarized predicted mask and\nground truth mask. The maxFβ is the maximum value\nof 255 uniformly distributed binarization thresholds. Fol-\n7\nAuthor version of CVPR22 paper\n(a) Input (b) Ours (c) Ours + BS (d) GT\nFigure 4: Visual results of unsupervised segments on\nECSSD [44]. In (a), we show the input image. TokenCut\ndetection result is presented in (b). TokenCut + Bilateral\nSolver results is shown in (c). (d) is the ground truth.\n(a) Input (b) Eigen Attention (c) Detection\nFigure 5: Visual results of images taken from the Inter-\nnet. We show the input images, our eigen attention and\nﬁnal detection in (a) (b) (c) respectively.\nlowing previous works [42, 58], we set β = 0.3 for con-\nsistency. IoU(Intersection over Union) score is computed\nbased on the binary predicted mask and the ground-truth,\nthe threshold is set to 0.5. Accuracy measures the pro-\nportion of pixels that have been correctly assigned to the\nobject/background. The binarization threshold is set to\n0.5 for masks.\nResults We further evaluate TokenCut on three com-\nmonly datasets for unsupervised saliency detection: EC-\nSSD [44], DUTS [60] and DUT-OMRON [67]. The\nqualitative results are in Tab. 3. TokenCut signiﬁcantly\noutperforms previous state-of-the-art. Adding Bilateral\nSolver [5] reﬁnes the boundary of an object and further\nboosts the performance over TokenCut, which can also be\nseen from the visual results presented in Fig. 4.\n4.4 Analysis and Discussion\nAnalysis ofτ. In Tab. 4, we provide an analysis onτde-\nﬁned in Equation 4. The results indicate that the effects of\nvariations in τ value are not signiﬁcant and that a suitable\nthreshold is τ = 0.2.\nInternet Images We further test TokenCut on Internet\nimages. The results are in Figure 5. We can see even\n(a) LOST\nInverse Attn.\n(b) LOST\nDetection\n(c) Our Eigen\nAttention\n(d) Our\nDetection\nFigure 6: Failure cases on VOC12 (1st and 2nd row)\nand COCO (3rd row).LOST [45] mainly relies on the\nmap of inverse degrees (a) to perform detection (b). For\nour approach, we illustrate the eigenvector in (c) and our\ndetection in (d). Blue and Red bounding boxes indicate\nthe ground-truth and the predicted bounding boxes respec-\ntively.\nthe inputs images are with noisy background, our algo-\nrithm can still provide precise attention map cover the ob-\nject leading to accurate bounding box prediction, which\ndemonstrates again the robustness of our approach.\nLimitations Despite the good performance of Token-\nCut, it has several limitations. Examples of failure cases\nare shown in Fig. 6: i) TokenCut focuses on the largest\nsalient part in the image, which may not be the desired\nobject (Fig. 6, 1st row). ii) Similar to LOST [45], To-\nkenCut assumes that a single salient object occupies the\nforeground. If multiple overlapping objects are present in\nan image, both LOST and our approach will fail to detect\none of the object (Fig. 6, 2nd row). iii) Neither LOST nor\nour approach can handle occlusion (Fig. 6, 3rd row).\n5 Conclusion\nWe have introduced TokenCut, a simple but effective ap-\nproach for unsupervised object discovery. TokenCut uses\nself-supervised learning with transformers to constructs a\ngraph where nodes are patches and edges represent simi-\nlarities between patches. We showed that salient objects\ncan be directly detected and delimited using Ncut. We\nevaluated this approach on unsupervised single object dis-\ncovery, weakly supervised object detection and unsuper-\nvised saliency detection, and showed that it provides a\nsigniﬁcant improvement over previous approaches. Our\nresults indicate that self-supervised transformers can pro-\nvide a rich and general set of features that may likely be\nused for a variety of computer vision problems.\n8\nAuthor version of CVPR22 paper\nAcknowledgements This work has been partially sup-\nported by the MIAI Multidisciplinary AI Institute at\nthe Univ. Grenoble Alpes (MIAI@Grenoble Alpes -\nANR-19-P3IA-0003), and by the EU H2020 ICT48\nproject Humane AI Net under contract EU #952026.\nReferences\n[1] Hamed H Aghdam, Abel Gonzalez-Garcia, Joost van de\nWeijer, and Antonio M L ´opez. Active learning for deep\ndetection neural networks. In ICCV, 2019. 1\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv, 2016. 4\n[3] Wonho Bae, Junhyug Noh, and Gunhee Kim. Rethinking\nclass activation mapping for weakly supervised object lo-\ncalization. In ECCV, 2020. 7, 16\n[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-\ntraining of image transformers. arXiv, 2021. 2\n[5] Jonathan T Barron and Ben Poole. The fast bilateral solver.\nIn ECCV, 2016. 2, 7, 8\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transform-\ners. In ICCV, 2021. 1, 2, 3, 4, 5, 6, 12, 13, 14\n[7] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader,\nand Vineeth N Balasubramanian. Grad-cam++: General-\nized gradient-based visual explanations for deep convolu-\ntional networks. In WACV, 2018. 3\n[8] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\nstudy of training self-supervised vision transformers. In\nICCV, 2021. 2, 12\n[9] Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-\nBin Huang. Show, match and segment: Joint weakly\nsupervised learning of semantic matching and object co-\nsegmentation. PAMI, 2020. 3\n[10] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang,\nPhilip HS Torr, and Shi-Min Hu. Global contrast based\nsalient region detection. TPAMI, 2014. 3\n[11] Minsu Cho, Suha Kwak, Cordelia Schmid, and Jean\nPonce. Unsupervised object discovery and localization in\nthe wild: Part-based matching with bottom-up region pro-\nposals. In CVPR, 2015. 2, 3, 6, 16\n[12] Junsuk Choe, Seong Joon Oh, Seungho Lee, Sanghyuk\nChun, Zeynep Akata, and Hyunjung Shim. Evaluating\nweakly supervised object localization methods right. In\nCVPR, 2020. 3\n[13] Junsuk Choe and Hyunjung Shim. Attention-based\ndropout layer for weakly supervised object localization. In\nCVPR, 2019. 3, 7, 16\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 7, 12, 14, 15, 16\n[15] Thomas Deselaers, Bogdan Alexe, and Vittorio Ferrari.\nLocalizing objects while learning their appearance. In\nECCV, 2010. 6\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT,\n2018. 2\n[17] Terrance DeVries and Graham W Taylor. Improved reg-\nularization of convolutional neural networks with cutout.\narXiv, 2017. 3\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. In ICLR,\n2020. 2, 3, 5, 6, 7, 12\n[19] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman. The PASCAL Visual Object Classes\nChallenge 2007 (VOC2007) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.html.\n2, 6, 13\n[20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman. The PASCAL Visual Object Classes\nChallenge 2012 (VOC2012) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\n2, 6\n[21] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. The\nInternational Journal of Robotics Research, 2013. 1\n[22] Guangyu Guo, Junwei Han, Fang Wan, and Dingwen\nZhang. Strengthen learning tolerance for weakly super-\nvised object localization. In CVPR, 2021. 3, 7\n[23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Doll ´ar, and Ross Girshick. Masked autoencoders are\nscalable vision learners. arXiv, 2015. 2\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 7\n[25] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling\nthe knowledge in a neural network. arXiv, 2015. 2\n[26] Kuang-Jui Hsu, Yen-Yu Lin, Yung-Yu Chuang, et al. Co-\nattention cnns for unsupervised object co-segmentation. In\nIJCAI, 2018. 3\n[27] Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu,\nNanning Zheng, and Shipeng Li. Salient object detection:\nA discriminative regional feature integration approach. In\nCVPR, 2013. 3\n[28] Armand Joulin, Francis Bach, and Jean Ponce. Discrim-\ninative clustering for image co-segmentation. In CVPR,\n2010. 3\n[29] Armand Joulin, Francis Bach, and Jean Ponce. Multi-class\ncosegmentation. In CVPR, 2012. 3\n[30] Gunhee Kim and Antonio Torralba. Unsupervised detec-\ntion of regions of interest using iterative link analysis. In\nNeurIPS, 2009. 6\n[31] Nianyi Li, Bilin Sun, and Jingyi Yu. A weighted sparse\ncoding framework for saliency detection. In CVPR, 2015.\n3, 7\n[32] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong\nZhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao,\nMing Tang, et al. Mst: Masked self-supervised transformer\nfor visual representation. In NeurIPS, 2021. 2\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects\nin context. In ECCV, 2014. 1, 2, 6, 13, 14, 16\n[34] Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo,\nKan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and\nPeter Vajda. Unbiased teacher for semi-supervised object\ndetection. In ICLR, 2021. 1\n[35] Weizeng Lu, Xi Jia, Weicheng Xie, Linlin Shen, Yicong\nZhou, and Jinming Duan. Geometry constrained weakly\nsupervised object localization. In ECCV, 2020. 3\n[36] Jinjie Mai, Meng Yang, and Wenfeng Luo. Erasing in-\ntegrated learning: A simple yet effective approach for\nweakly supervised object localization. In CVPR, 2020. 3\n[37] Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mum-\nmadi, Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu\nLou, and Thomas Brox. Deepusps: Deep robust unsuper-\n9\nAuthor version of CVPR22 paper\nvised saliency prediction via self-supervision. In NeurIPS,\n2019. 3, 7\n[38] Alex Pothen, Horst D Simon, and Kang-Pu Liou. Parti-\ntioning sparse matrices with eigenvectors of graphs. SIAM\njournal on matrix analysis and applications, 1990. 4\n[39] Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-\nYu Liu, Yong Jae Lee, Alexander G Schwing, and Jan\nKautz. Instance-aware, context-focused, and memory-\nefﬁcient weakly supervised object detection. In CVPR,\n2020. 1\n[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, et al. Ima-\ngenet large scale visual recognition challenge.IJCV, 2015.\n7\n[41] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-\ntra. Grad-cam: Visual explanations from deep networks\nvia gradient-based localization. In ICCV, 2017. 3\n[42] Xi Shen, Alexei A Efros, Armand Joulin, and Math-\nieu Aubry. Learning co-segmentation by segment\nswapping for retrieval and discovery. arXiv preprint\narXiv:2110.15904, 2021. 7, 8, 16\n[43] Jianbo Shi and Jitendra Malik. Normalized cuts and image\nsegmentation. TPAMI, 2000. 2, 3, 4\n[44] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical\nimage saliency detection on extended cssd. TPAMI, 2015.\n2, 7, 8, 16, 17\n[45] Oriane Sim ´eoni, Gilles Puy, Huy V . V o, Simon Roburin,\nSpyros Gidaris, Andrei Bursuc, Patrick P ´erez, Renaud\nMarlet, and Jean Ponce. Localizing objects with self-\nsupervised transformers and no labels. In BMVC, 2021.\n1, 2, 3, 5, 6, 7, 8, 12, 13, 14, 15, 16\n[46] Krishna Kumar Singh and Yong Jae Lee. Hide-and-seek:\nForcing a network to be meticulous for weakly-supervised\nobject and action localization. In ICCV, 2017. 3, 7, 16\n[47] Parthipan Siva, Chris Russell, Tao Xiang, and Lourdes\nAgapito. Looking beyond the image: Unsupervised learn-\ning for object saliency and detection. In CVPR, 2013. 6\n[48] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-\nmanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,\nVincent Vanhoucke, and Andrew Rabinovich. Going\ndeeper with convolutions. In CVPR, 2015. 7, 16\n[49] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, 2016. 7\n[50] Kevin Tang, Armand Joulin, Li-Jia Li, and Li Fei-Fei. Co-\nlocalization in real-world images. In CVPR, 2014. 3\n[51] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers,\nand Arnold WM Smeulders. Selective search for object\nrecognition. IJCV, 2013. 6\n[52] Charles F Van Loan and G Golub. Matrix computations.\nThe Johns Hopkins University Press, 1996. 4\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. InNeurIPS,\n2017. 2\n[54] Sara Vicente, Carsten Rother, and Vladimir Kolmogorov.\nObject cosegmentation. In CVPR, 2011. 3\n[55] Huy V V o, Francis Bach, Minsu Cho, Kai Han, Yann Le-\nCun, Patrick P ´erez, and Jean Ponce. Unsupervised image\nmatching and object discovery as optimization. In CVPR,\n2019. 2, 3, 6, 16\n[56] Huy V V o, Patrick P ´erez, and Jean Ponce. Toward unsu-\npervised, multi-object discovery in large-scale image col-\nlections. In ECCV, 2020. 2, 3, 6, 13, 14, 16\n[57] Huy V V o, Elena Sizikova, Cordelia Schmid, Patrick\nP´erez, and Jean Ponce. Large-scale unsupervised object\ndiscovery. arXiv, 2021. 2, 3, 6, 16\n[58] Andrey V oynov, Stanislav Morozov, and Artem Babenko.\nObject segmentation without labels with large-scale gener-\native models. In ICML, 2021. 3, 7, 8\n[59] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-\nlongie. The Caltech-UCSD Birds-200-2011 Dataset. Tech-\nnical report, California Institute of Technology, 2011. 2, 7,\n12, 14, 15, 16\n[60] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,\nDong Wang, Baocai Yin, and Xiang Ruan. Learning to de-\ntect salient objects with image-level supervision. InCVPR,\n2017. 2, 3, 7, 8, 16, 18\n[61] Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chunhua\nShen, and Zhi-Hua Zhou. Unsupervised object discovery\nand co-localization by deep descriptor transformation.Pat-\ntern Recognition, 2019. 2, 3, 6, 16\n[62] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian Sun.\nGeodesic saliency using background priors. In ECCV,\n2012. 3\n[63] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer.\nSqueezedet: Uniﬁed, small, low power fully convolu-\ntional neural networks for real-time object detection for\nautonomous driving. In CVPRW, 2017. 1\n[64] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge\nBelongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and\nLiangpei Zhang. Dota: A large-scale dataset for object\ndetection in aerial images. In CVPR, 2018. 1\n[65] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude\nOliva, and Antonio Torralba. Sun database: Large-scale\nscene recognition from abbey to zoo. In CVPR, 2010. 16\n[66] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical\nsaliency detection. In CVPR, 2013. 3, 7\n[67] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and\nMing-Hsuan Yang. Saliency detection via graph-based\nmanifold ranking. In CVPR, 2013. 2, 3, 7, 8, 16, 18\n[68] Yuan Yuan, Xiaodan Liang, Xiaolong Wang, Dit-Yan Ye-\nung, and Abhinav Gupta. Temporal dynamic graph lstm\nfor action-driven video object detection. In ICCV, 2017. 3\n[69] Yuan Yuan, Yueming Lyu, Xi Shen, Ivor Tsang, and Dit-\nYan Yeung. Marginalized average attentional network for\nweakly-supervised learning. In ICLR, 2019. 3\n[70] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Reg-\nularization strategy to train strong classiﬁers with localiz-\nable features. In ICCV, 2019. 3\n[71] Chen-Lin Zhang, Yun-Hao Cao, and Jianxin Wu. Rethink-\ning the route towards weakly supervised object localiza-\ntion. In CVPR, 2020. 3, 7\n[72] Dingwen Zhang, Junwei Han, Gong Cheng, and Ming-\nHsuan Yang. Weakly supervised object localization and\ndetection: a survey. IEEE transactions on pattern analysis\nand machine intelligence, 2021. 3\n[73] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi,\nand Richard Hartley. Deep unsupervised saliency detec-\ntion: A multiple noisy labeling perspective. In CVPR,\n2018. 3\n[74] Runsheng Zhang, Yaping Huang, Mengyang Pu, Jian\nZhang, Qingji Guan, Qi Zou, and Haibin Ling. Object dis-\ncovery from a single unlabeled image by mining frequent\nitemsets with multi-scale features. TIP, 2020. 6\n[75] Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, and\nThomas S Huang. Adversarial complementary learning for\nweakly supervised object localization. In CVPR, 2018. 3\n10\nAuthor version of CVPR22 paper\n[76] Xiaolin Zhang, Yunchao Wei, and Yi Yang. Inter-image\ncommunication for weakly supervised localization. In\nECCV, 2020. 7\n[77] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrim-\ninative localization. In CVPR, 2016. 3, 7, 16\n[78] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun.\nSaliency optimization from robust background detection.\nIn CVPR, 2014. 3, 7\n[79] C Lawrence Zitnick and Piotr Doll ´ar. Edge boxes: Locat-\ning object proposals from edges. In ECCV, 2014. 6\n11\nAuthor version of CVPR22 paper\nA Analysis of backbones.\nIn Tab. 5, we provide an ablation study on different transformer backbones. The “-S” and “-B” are ViT small[6, 18] and ViT\nbase[6, 18] architecture respectively. The “-16” and “-8” represents patch sizes 16 and 8 respectively. The “MocoV3” is another\npre-trained self-supervised transformer model [8]. The τ value is set to 0.3 for MoCov3, while for Dino the best tau value is 0.2.\nWe observe that although the result of MoCov3 is slightly worse than the results of TokenCut with Dino, MoCov3 still outperforms\nprevious state-of-the-art, indicating that TokenCut can provide similar results when used with other self-supervised Transformer\narchitectures. Besides, the results demonstrates that a patch size of 16 provides better results than a patch size of 8. Several insights\ncan be found: i) TokenCut outperforms LOST for different backbones. ii) As LOST relies on a heuristic seeds expansion strategy,\nthe performance varies signiﬁcantly using different backbones. While our approach is more robust.\nTable 5: Analysis of different backbones.We report CorLoc for unsupervised single object discovery on VOC07,\nVOC12, COCO20K.\nMethod Backbone VOC07 VOC12 COCO20K\nLOST [45] ViT-S/16 [6, 18] 61.9 64.0 50.7\nTokenCut MoCoV3-ViT-S/16 [8, 18] 66.2 66.9 54.5\nTokenCut ViT-S/16 [6, 18] 68.8 (↑6.9) 72.1 (↑8.1) 58.8 ( ↑8.1)\nLOST [45] ViT-S/8 [6, 18] 55.5 57.0 49.5\nTokenCut ViT-S/8 [6, 18] 67.3 ( ↑11.8) 71.6 ( ↑14.6) 60.7 (↑11.2)\nLOST [45] ViT-B/16 [6, 18] 60.1 63.3 50.0\nTokenCut ViT-B/16 [6, 18] 68.8 ( ↑8.7) 72.4 (↑9.1) 59.0 (↑9.0)\nWe provide another an ablation study on different backbones for weakly supervised object localization. Results are shown in Tab. 6.\nThe “-S” and “-B” designate ViT small [6, 18] and ViT base [6, 18] architecture respectively. The “-16” and “-8” indicate patch\nsizes 16 and 8 respectively. For our approach, we report results with τ = 0.2, which is the same on all the datasets. Note that\nLOST with ViT-S/8 achieves much worse results, because the seed expansion strategy in LOST relies on the top-100 patches which\nare with lowest degrees. When the total number of patches is large, the proposed seed expansion strategy is not able to cover entire\nobjects. While our approach provides more robust performance on different datasets across different backbones.\n+\nTable 6: Analysis of backbones for weakly supervised object localization.We report Top-1 Cls, GT Loc and Top-1\nLoc on CUB [59] and Imagenet-1k [14] datasets.\nMethod Backbone τ CUB [50], Acc. (%) ImageNet-1K [11], Acc. (%)\nTop-1 Cls GT Loc Top-1 Loc Top-1 Cls GT Loc Top-1 Loc\nLOST [45] ViT-S/16 [6, 18] - 79.5 89.7 71.3 77.0 60.0 49.0\nTokenCut ViT-S/16 [6, 18] 0.2 79.5 91.8 ( ↑2.1) 72.9 (↑1.6) 77.0 65.4 ( ↑5.4) 53.4 (↑4.4)\nLOST [45] ViT-S/8 [6, 18] - 82.3 78.0 64.4 79.4 45.8 38.1\nTokenCut ViT-S/8 [6, 18] 0.2 82.3 89.9 ( ↑11.9) 74.2 (↑9.8) 79.4 66.0 ( ↑20.2) 55.0 (↑16.9)\nLOST [45] ViT-B/16 [6, 18] - 80.3 90.7 72.8 78.3 58.6 48.3\nTokenCut ViT-B/16 [6, 18] 0.280.3 90.0 (↓0.7) 72.5 ( ↓0.3) 78.3 63.2 ( ↑4.8) 52.3 (↑4.0)\nB Analysis of bi-partition strategies.\nIn Tab. 7, we study different strategies to separate the nodes in our graph into two groups using the second smallest eigenvector.\nWe consider three natural methods: mean value (Mean), Expectation-Maximisation (EM), K-means clustering (K-means). We use\npython sklearn library for EM and K-means algorithm implementation. For EM algorithm, we set number of iteration to 300 and\neach component has its own general covariance matrix. The convergence threshold is set to 1e-3. For K-means algorithm, we use\n“k-means++” for initialization. The maximum number of iterations is set to 300. The convergence threshold is set to 1e-4. The\nresult suggests that the simple mean value as the splitting point performs well for most cases. We have also tried to search for\nthe splitting point based on the best Ncut(A,B) value. Due to the quadratic complexity, this approach requires substantially more\ncomputations. Thus, we ﬁnally obsolete it.\nTable 7: Analysis of different bi-partition methods.We report CorLoc for unsupervised single object discovery.\nBi-partition VOC07 VOC12 COCO20K\nMean 68.8 72.1 58.8\nEM 63.0 65.7 59.3\nK-means 67.5 69.2 61.6\n12\nAuthor version of CVPR22 paper\nC Analysis of Graph edge weight\nIn this section, we provide an ablation study on graph edge weight deﬁninig on equation 4. We have tested to directly use the\nsimilarity score as edge weights (i.e., Eij = S(xi,xj))). However, it is not possible because there may exist negative edge values,\nwhich violates the Normalized Cut algorithm assumption. Thus, we also tried thresholding the similarity score (i.e.,Eij = S(xi,xj)\nif S(xi,xj) >τ , else ϵ). We obtain 68.9% on VOC07 dataset and 72% on VOC12 dataset, which is similar to our reported results.\nD Visual results for unsupervised single object discovery on VOC07 and COCO12\nWe show visual results for unsupervised single object discovery on VOC07 [19] and COCO12 [33, 56], which are illustrated in\nFig. 7 and Fig. 8 respectively.\nFor each dataset, we compare both attention maps and bounding box predictions among DINO [6], LOST [45] and TokenCut. The\nattention map for DINO is extracted from the CLS token attention map of the last layer of key features. The attention map for\nLOST is the inverse degree map used in LOST for detection. The TokenCut attention map is the second smallest eigenvector of\nEquation 2. These results show that TokenCut provides clearly better segmentation of the object.\n(a) DINO CLS\nToken Attention\n(b) DINO\nDetection\n(c) LOST Inverse\nDegree Attention\n(d) LOST\nDetection\n(e) Our Eigen\nAttention\n(f) Our\nDetection\nFigure 7: Visual results of unsupervised single object discovery on VOC07 [19]In (a), we show the attention of\nthe CLS token in DINO [6] which is used for detection (b). LOST [45] is mainly relied on the map of inverse degrees\n(c) to perform detection (d). For our approach, we illustrate the eigenvector in (e) and our detection in (f). Blue and\nRed bounding boxes indicate the ground-truth and the predicted bounding boxes respectively.\n13\nAuthor version of CVPR22 paper\n(a) DINO CLS\nToken Attention\n(b) DINO\nDetection\n(c) LOST Inverse\nDegree Attention\n(d) LOST\nDetection\n(e) Our Eigen\nAttention\n(f) Our\nDetection\n+\nFigure 8: Visual results of unsupervised single object discovery on COCO20K [33, 56].In (a), we show the\nattention of the CLS token in DINO [6] used for detection (b). LOST [45] mainly relies on the map of inverse degrees\n(c) to perform detection (d). For TokenCut, we illustrate the eigenvector in (e) and the detection in (f). Blue and Red\nbounding boxes indicate the ground-truth and the predicted bounding boxes respectively.\nE Fine-tuning self-supervised transformers\nFor weakly supervised object localization, we use a pre-trained DINO model as our backbone and learn a linear classiﬁer on the\ntraining set where we only have access to the class labels. We freeze the backbone weights and ﬁne-tune a linear classiﬁer, as\nshown in Tab. 2. For CUB, We train with a SGD optimizer for 1000 epochs and set the batch size to 256 per GPU, distributed over\n4 GPUs. The learning rate is linearly warmed during the ﬁrst 50 epochs, then follows a cosine learning rate scheduler. We decay\nthe learning rate from batch size\n256 ×5e-4 to 1e-6. The weight decay is set to 0.005. For ImageNet-1K, we use the models released by\nDINO. Other training setups and details can be found in the supplementary material.\nF Visual results for weakly supervised object localizatio on CUB and Imagenet-1k\nWe present visual results for weakly supervised object localization on CUB [59] and Imagenet-1k [14] in Fig. 9 and Fig. 10\nrespectively.\n14\nAuthor version of CVPR22 paper\nFor each dataset, we compare the attention map and bounding box prediction with LOST [45] and our approach. The eigenvector\nof TokenCut provides better segmentation on objects and leads to better detection results.\n(a) LOST Inverse\nDegree Attention\n(b) LOST\nDetection\n(c) Ours Eigen\nAttention\n(d) Ours\nDetection\nFigure 9: Visual results for weakly supervised object localization on CUB [59].In (a), we show the map of\ninverse degrees used to perform detection with LOST (b) [45]. For TokenCut, we illustrate the eigenvector in (c)\nused for detection in (d). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes\nrespectively.\n(a) LOST Inverse\nDegree Attention\n(b) LOST\nDetection\n(c) Ours Eigen\nAttention\n(d) Ours\nDetection\n+\nFigure 10: Visual results of unsupervised single object discovery on Imagenet-1k [14].In (a), we show LOST\n[45] the map of inverse degrees, which is used to perform detection (b). For TokenCut, we illustrate the eigenvector in\n(c) and the detection in (d). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes\nrespectively.\n15\nAuthor version of CVPR22 paper\nG Failure cases on CUB and Imagenet-1k\nWe illustrate additional failure cases in Fig. 11. Those failure cases can be organised into three categories: 1) Where TokenCut\nfocus on the largest salient object, whereas the annotation is highlights a different object, shown in the ﬁrst and the second column\nin Fig. 11. 2) Similar to LOST, Tokencut is not able to differentiate the connected objects, such as the third and the fourth column\nin Fig. 11. 3) In case of occlusion, neither LOST nor our approach can’t detect the entire object, such as the last two columns in\nFig. 11.\n(a) LOST Inverse\nDegree Attention\n(b) LOST\nDetection\n(c) Ours Eigen\nAttention\n(d) Ours\nDetection\n+\nFigure 11: Failure cases on Imagenet-1k [14] and CUB [59].In (a), we show LOST [45] the map of inverse degrees,\nwhich is used to perform detection (b). For TokenCut, we illustrate the eigenvector in (c) and the detection in (d). Blue\nand Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively.\nH Datasets\nWe present in this section the details of the datasets used in our experiments:\n• VOC07 and VOC12correspond to the training and validation set of PASCAL-VOC07 and PASCAL-VOC12. VOC07 and\nVOC12 contain 5 011 and 11 540 images respectively which belong to 20 categories. They are commonly evaluated for unsuper-\nvised object discovery [11, 55–57, 61].\n• COCO20K consists of 19 817 randomly chosen images from the COCO2014 dataset [33]. It is used as a benchmark in [56] for\na large scale evaluation.\n• CUB consists of 200 bird species, including 6 033 and 5 755 images in training and test sets respectively, which is commonly\nused to evaluate weakly supervised object localization [3, 13, 46, 48, 77].\n• ImageNet [14] is a widely used benchmark for image classiﬁcation and object detection, which consists of 1 000 different\ncategories. The number of images in training and validation sets are 1.3 million and 50,000 respectively. Each image contains a\nsingle object supposed to be detected. During the training, only class labels are available.\n• ECSSD contains 1 000 real-world images of complex scenes for testing.\n• DUTS contains 10 553 train and 5 019 test images. The training set is collected from the ImageNet detection train/val set. The\ntest set is collected from ImageNet test, and the SUN dataset [65]. Following the previous works [42], we report the performance\non the DUTS-test subset.\n• DUT-OMRON[67] contains 5 168 images of high quality natural images for testing.\nI Visual results for unsupervised saliency detecion on ECSSD, DUTS and DUT-OMRON\nWe present visual results for unsupervised saliency detecion on ECSSD [44], DUTS [60] and DUT-OMRON [67] in Fig. 12, 13\nand 14 respectively.\n16\nAuthor version of CVPR22 paper\nFor each dataset, we compare LOST segmentation, LOST + Bilateral Solver and our approch. The TokenCut provides better\nsegmentation on objects. The performance is further improved with Bilateral Solver.\n(a) Input (b) LOST (c) LOST + BS (d) Ours (e) Ours + BS (f) GT\nFigure 12: Visual results of unsupervised segments on ECSSD [44]\n17\nAuthor version of CVPR22 paper\n(a) Input (b) LOST (c) LOST + BS (d) Ours (e) Ours + BS (f) GT\nFigure 13: Visual results of unsupervised segments on DUTS [60]\n(a) Input (b) LOST (c) LOST + BS (d) Ours (e) Ours + BS (f) GT\nFigure 14: Visual results of unsupervised segments on DUT-OMRON [67]\n18",
  "topic": "Pattern recognition (psychology)",
  "concepts": [
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6697840690612793
    },
    {
      "name": "Computer science",
      "score": 0.6621934175491333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6538589596748352
    },
    {
      "name": "Object detection",
      "score": 0.5684476494789124
    },
    {
      "name": "Transformer",
      "score": 0.5541566610336304
    },
    {
      "name": "Cluster analysis",
      "score": 0.5398333668708801
    },
    {
      "name": "Graph",
      "score": 0.5299065113067627
    },
    {
      "name": "Security token",
      "score": 0.47326138615608215
    },
    {
      "name": "Spectral clustering",
      "score": 0.4424845278263092
    },
    {
      "name": "Unsupervised learning",
      "score": 0.42146438360214233
    },
    {
      "name": "Theoretical computer science",
      "score": 0.12759974598884583
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I106785703",
      "name": "Institut polytechnique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210104430",
      "name": "Laboratoire d'Informatique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I899635006",
      "name": "Université Grenoble Alpes",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I142631665",
      "name": "École nationale des ponts et chaussées",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210101778",
      "name": "Samsung (United States)",
      "country": "US"
    }
  ],
  "cited_by": 125
}