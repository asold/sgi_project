{
    "title": "An Architecture for Accelerated Large-Scale Inference of Transformer-Based Language Models",
    "url": "https://openalex.org/W3169729304",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5091227381",
            "name": "Amir Ganiev",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5043691423",
            "name": "Colton Chapin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5002583029",
            "name": "Anderson de Andrade",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100322200",
            "name": "Chen Liu",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2550375374",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2402144811",
        "https://openalex.org/W638502264",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1954306935",
        "https://openalex.org/W2542459869",
        "https://openalex.org/W1789713128",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970443729",
        "https://openalex.org/W2072750586",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2953384591",
        "https://openalex.org/W2538998463"
    ],
    "abstract": "Amir Ganiev, Colton Chapin, Anderson De Andrade, Chen Liu. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers. 2021.",
    "full_text": "Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 163–169\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n163\nAn Architecture for Accelerated Large-Scale Inference of\nTransformer-Based Language Models\nAmir Ganiev∗ and Colt Chapin and Anderson de Andrade and Chen Liu∗\nWattpad\nToronto, ON, Canada\namir.ganiev@mail.utoronto.ca, {colt, anderson}@wattpad.com,\nceciliachen.liu@mail.utoronto.ca\nAbstract\nThis work demonstrates the development pro-\ncess of a machine learning architecture for in-\nference that can scale to a large volume of re-\nquests. In our experiments, we used a BERT\nmodel that was ﬁne-tuned for emotion analysis,\nreturning a probability distribution of emotions\ngiven a paragraph. The model was deployed as\na gRPC service on Kubernetes. Apache Spark\nwas used to perform inference in batches by\ncalling the service. We encountered some per-\nformance and concurrency challenges and cre-\nated solutions to achieve faster running time.\nStarting with 3.3 successful inference requests\nper second, we were able to achieve as high\nas 300 successful requests per second with the\nsame batch job resource allocation. As a result,\nwe successfully stored emotion probabilities\nfor 95 million paragraphs within 96 hours.\n1 Introduction\nAs data in organizations becomes more available\nfor analysis, it is crucial to develop efﬁcient ma-\nchine learning pipelines. Previous work (Al-Jarrah\net al., 2015) has highlighted the growing number\nof data centers and their energy and pollution reper-\ncussions. Machine learning models that require less\ncomputational resources to generate accurate re-\nsults reduce these externalities. On the other hand,\nmany machine learning applications also require\nresults in nearly real-time in order to be viable and\nmay also require results from as many data samples\nas possible in order to produce accurate insights.\nHence, there are also opportunity costs associated\nwith missed service-level objectives.\nAttention-based language models such as BERT\n(Devlin et al., 2019) are often chosen for their\nrelative efﬁciency, and empirical power. Com-\npared to recurrent neural networks (Hochreiter and\nSchmidhuber, 1997), each step in a transformer\nlayer (Vaswani et al., 2017) has direct access to all\n∗Work done while the author was working at Wattpad.\nother steps and can be computed in parallel, which\ncan make both training and inference faster. BERT\nalso easily accommodates different applications by\nallowing the ﬁne-tuning of its parameters on dif-\nferent tasks. Despite these beneﬁts, exposing these\nmodels and communicating with them efﬁciently\npossesses some challenges.\nMachine learning frameworks are often used to\ntrain, evaluate, and perform inference on predic-\ntive models. TensorFlow (Abadi et al., 2016) has\nbeen shown to be a reliable system that can operate\nat a large scale. A sub-component called Tensor-\nFlow Serving allows loading models as services\nthat handle inference requests concurrently.\nSystem architectures for inference have changed\nover time. Initial approaches favored ofﬂine set-\ntings where batch jobs make use of distributed plat-\nforms to load models and data within the same\nprocess and perform inference. For example, Ijari,\n2017 suggested an architecture that uses Apache\nHadoop (Hadoop, 2006) and Apache Pig for large-\nscale data processing, where results are written\nto a Hadoop Distributed File System (HDFS) for\nlater consumption. Newer distributed platforms\nsuch as Apache Spark (Zaharia et al., 2016) have\ngained prominence because of their memory opti-\nmizations and more versatile APIs, compared to\nApache Hadoop (Zaharia et al., 2012).\nAs part of this architecture, inference services\nwould often be reserved for applications that\nrequire faster responses. The batch-based and\nservice-based platforms have different use cases\nand often run in isolation. Collocating data and\nmodels in a batch job has some disadvantages.\nLoading models in the same process as the data\nforces them both to scale the same way. Moreover,\nmodels are forced to be implemented using the pro-\ngramming languages supported by the distributed\ndata platform. Their APIs often place some limita-\ntions on what can be done.\nWith the evolution of machine learning frame-\n164\nworks and container-orchestration systems such as\nKubernetes,1 it is now simpler to efﬁciently build,\ndeploy, and scale models as services. A scalable\narchitecture was presented in (Gómez et al., 2014)\nthat proposes the use of RESTful API calls exe-\ncuted by batch jobs in Hadoop to reach online ser-\nvices that provide real-time inference. Approaches\nlike this simplify the architecture and address the\nissues discussed previously.\nIn this work, we present an architecture for batch\ninference where a data processing task relies on ex-\nternal services to perform the computation. The\ncomponents of the architecture will be discussed in\ndetail along with the technical challenges and solu-\ntions we developed to accelerate this process. Our\napplication is a model for emotion analysis that\nproduces a probability distribution over a closed\nset of emotions given a paragraph of text (Liu et al.,\n2019). We present benchmarks to justify our ar-\nchitecture decisions and settings. The proposed\narchitecture is able to generate results for 95 mil-\nlion paragraphs within 96 hours.\n2 Architecture design\nWe deployed our model as a TensorFlow service\nin a Kubernetes cluster. A sidecar service prepro-\ncessed and vectorized paragraphs and forwarded\nrequests to this service. We used gRPC to commu-\nnicate with the services,2 which is an efﬁcient com-\nmunication protocol on HTTP/2. Both nearly real-\ntime and ofﬂine use cases made calls to these ser-\nvices. We used Apache Spark for batch processing,\nwhich we ran on Amazon’s AWS EMR service.3\nOur batch job was developed using Apache Spark’s\nPython API (PySpark). The batch job fetched a\ndataset of relevant paragraphs, called the inference\nservice, and stored the results. The job had two\nmodes: a backﬁll mode and a daily mode, which\nran on a subset of mutated and new paragraphs.\nThis batch job was part of a data pipeline, sched-\nuled using Apache AirFlow4 and Luigi.5 Figure 1\nshows the main components of this architecture.\n2.1 Kubernetes vs. Apache Spark\nOne of the key issues we faced in scaling up our\ninference services was the growing size of the mem-\nory footprint of an instance. A standard practice\n1https://kubernetes.io\n2https://grpc.github.io\n3https://aws.amazon.com/emr\n4https://airflow.apache.org\n5https://github.com/spotify/luigi\nwhen conducting model inference at scale in a\nMapReduce program such as Apache Spark is to\nbroadcast an instance of the model to each dis-\ntributed worker process to allow for parallel pro-\ncessing. However, when the footprint of these in-\nstances becomes too large, they begin to compete\nwith the dataset being processed for the limited\nmemory resources of the underlying cluster and, in\nmany cases, exceeding the capacity of the underly-\ning hardware.\nWhile this issue does not preclude the use of\nApache Spark for running inferences on large mod-\nels at scale, it does complicate the process of im-\nplementing the job in a cost-efﬁcient manner. It\nis possible to allocate more resources, but because\nthe clusters are static in size, a lot of work has to\ngo into properly calculating resource allocation to\navoid over or under-provisioning. This is where the\nidea of ofﬂoading the model to Kubernetes comes\ninto play.\nWhile our MapReduce clusters struggled to scale\nand accommodate the larger models being broad-\ncasted, by leveraging Kubernetes we were able to\nmonitor and optimize resource usage as well as\ndeﬁne autoscaling behaviors independently of this\ncluster. That said, while there are clear beneﬁts to\nisolating your model from your MapReduce job\nwe must now consider the added overhead of the\nnetwork calls and the effort to build and maintain\ncontainerized services.\n2.2 Kubernetes node pool\nTo ensure optimal resource usage, we provisioned\na segregated node pool dedicated to hosting in-\nstances of our models. A node pool is a collec-\ntion of similar resources with predeﬁned autoscal-\ning behaviors. We leveraged Kubernetes’ built-\nin taint/toleration functionality to establish the re-\nquired behavior. In Kubernetes, Taints designate\nresources as non-viable for allocation, unless de-\nployments are speciﬁcally annotated as having a\nToleration for said Taint. For this node pool, we\nselected instance types that offer faster CPUs, but\nprovide an adequate amount of memory to load our\nmodels.\n2.3 REST vs. gRPC\nOnce we made the decision to deploy our model\nas a service, we had to determine which network\nprotocol to use. While representational state trans-\nfer (REST) (Pautasso et al., 2013) is a well-known\nstandard, there were two aspects of our use case\n165\nFigure 1: Architecture overview.\nthat made us consider alternatives. The ﬁrst is that\narchitecturally, our use case was far more func-\ntional in nature than REST. Second, the nature of\nour data means that request messages can be large.\nIt was for this reason that we found the efﬁciency\noffered by the Protobuf protocol a natural ﬁt for\nour use case.6\nHaving decided to use gRPC and Protobuf, we\nencountered two issues. First, gRPC uses the\nHTTP/2 protocol which multiplexes requests over\na single persistent TCP connection. Because of this\npersistent connection, Layer-4 load balancers that\ncan only route connections are not able to recog-\nnize requests within them that could be balanced\nacross multiple replicas of a service. To enable\nthis, we rely on a Layer-7 load balancer which is\nable to maintain persistent connections with all de-\nvices, and identify and route requests within these\nchannels accordingly.\nThe second issue was organizational in nature.\nREST is a widely accepted standard, but more im-\nportantly, it is a protocol that developers are famil-\niar with. The introduction of a different API design\nhas led to signiﬁcant friction of adoption.\n2.4 A WS EMR cluster conﬁguration\nThe AWS EMR cluster needed to be conﬁgured\nto run a Apache Spark job that makes 95 million\ninference calls to our micro-service. Due to the\nunbounded nature of these paragraphs, which can\nbecome quite large in our use case, these 95 million\nrecords require a signiﬁcant amount of disk space\n(in the order of terabytes).\nTaking into account the cost constraints of this\nproject, we chose an AWS r5.xlarge instance with\n200 GiB of disk space as a master node, and 5 AWS\nr5.4xlarge instances with 1,000 GiB of disk space\neach, as worker nodes. This conﬁguration ensures\n6https://developers.google.com/\nprotocol-buffers\nthat there is enough disk capacity to process the\ndata and the number of cores is as high as possible\nwithout exceeding the cost constraints. Addition-\naly, we selected these to be memory-optimized to\nensure we provide the job with enough RAM to\nefﬁciently process our joins.\nThe EMR cluster conﬁguration is kept constant\nas a controlled variable throughout the project and\nin all of our experiments. This ensures that only\nthe implementation changes affect the performance\nof the inference job.\n2.5 Monitoring\nThere were two different solutions that monitor dif-\nferent aspects of the proposed architecture: Apache\nSpark console and DataDog. AWS EMR provided\naccess to the Apache Spark console for its running\ntasks and a history server for completed tasks. The\nconsole displays the execution plan, the running\nstage, the number of partitions completed in that\nstage, the number of stages left to execute, as well\nas statistics and message logs of our inference job.\nSuccess or failure of this job and its pipeline was re-\nported using DataDog.7 DataDog is a cloud based\nmonitoring service that provides helpful visualiza-\ntion tools to monitor applications.\nAdditionally, our services were instrumented to\nreport the number, latency, and status code of all\ncalls received. We made use of DataDog to aggre-\ngate and monitor these metrics. In our implementa-\ntion, the instrumentation was handled by functional\nwrappers around our endpoint handlers, as well as\na synchronous gRPC Interceptor for the client on\nour sidecar service. Figure 2 shows an example of\nour request count on our daily job.\n7https://www.datadoghq.com\n166\nFigure 2: DataDog visualization of a daily job, con-\nsisting of 600,000 paragraphs calls to the service. The\nX-axis is the local time starting at 1 a.m. The Y-axis is\nthe total number of calls executed within 1 minute. The\nhighlighted vertical bar shows that 18,000 calls were\nexecuted within 1 minute (300 per second) at 1:54 a.m.\nThe calls started at around 1 a.m. and reached a peak\nspeed at around 1:40 a.m.\n3 Architecture optimization\nOur initial approach, which used the conﬁguration\nin the previous section, was resilient to failures but\nperformed slowly at around 4 requests per second\nduring the inference step. With a backﬁll target\nof 95 million paragraphs, running this job was in-\ntractable. Our investigations concluded that the\nissues were rooted in a low request pressure on\nthe backend services. Thus, the sections below de-\nscribe the steps taken to address these issues and\nspeed up the inference process.\n3.1 Scaling model service\nOur autoscaling group consisted of instances with\nIntel’s Xeon Platinum 8175M CPUs and 64 GB\nof RAM. The use of GPUs is not cost-effective\nwithout a proper batching mechanism, which is\nconsidered to be outside of the scope of this work.\nEach instance had 8 physical cores and 16 logical\ncores. To reduce the memory footprint but also al-\nlow a ﬁne-grained resource allocation, Kubernetes\npods had a limit of 2 physical cores. In our ex-\nperiments, pods did not consume more than 4 GB\nof memory under heavy load. Network utilization\nremained well under 10 Gb/s. We set up an au-\ntoscaling policy with a target CPU utilization of\n70%.\nWith a maximum number of 100 pods (25 in-\nstances), we achieved a maximum of 300 requests\nper second, each request being a paragraph with at\nleast 15 characters. Our daily job usually ﬁnished\nwithin 60 minutes.\n3.2 Tuning TensorFlow Serving parameters\nWe evaluated the performance of TensorFlow Serv-\ning with multiple parameter conﬁgurations. The\nMKL OpenMP Intra-Op Req/Sec\nYes 2 2 5.207\nYes 2 4 5.931\nYes 4 2 4.786\nYes 4 4 5.714\nNo - 2 5.464\nNo - 4 6.452\nTable 1: Average requests per second of the service un-\nder different TensorFlow Serving settings: MKL, num-\nber of OpenMP threads, and number of intra-operation\nthreads. OpenMP is only used by MKL. Other conﬁgu-\nrations do not match the number of physical or logical\ncores available.\nonly settings that tangibly impacted performance\nincluded: enabling Intel’s Math Kernel Library\n(MKL), the OpenMP number of threads for MKL,\nand the thread pool size for TensorFlow intra-\noperations. We used TensorFlow Serving version\n2.3.0, which uses MKL-DNN version 0.21. Table\n1 illustrates performance under different conﬁgu-\nrations for pods with 2 CPU physical cores and 4\nlogical cores. In particular, we note that disabling\nMKL and allocating a thread pool the size of the\nnumber of logical cores gave us the best perfor-\nmance for this model.\n3.3 Spark job tuning\nConﬁguring parameters of TensorFlow serving and\nsuccessfully scaling up BERT micro-service al-\nlowed for a faster inference speed. However, adjust-\ning the service alone did not yield better results as\nthe speed remained relatively similar (3.3 complete\ncalls per second). Therefore, a PySpark job reached\nits limits in the proposed conﬁguration. The micro-\nservice was not receiving enough requests to trigger\nits autoscaling condition and capped out at 7 pods\n(far short of our max off 100). To address this, we\nsought to introduce more load by increasing the\nrate at which the client makes calls to the micro-\nservice.\nUsing synchronous calls, the number of requests\nthe batch job can make is bounded by the number\nof cores assigned to it. Since the computation is\ndone by the service, these cores will be mostly\nwaiting for the service responses.\nTo address Python’s synchronous nature limit-\ning the rate at which a single core can make calls\nto the service, we leveraged the AsyncIO library8\nwithin a PySpark User Deﬁned Function (UDF),\n8https://www.python.org\n167\nwhich allowed a single core to implement quasi-\nconcurrent calls and leverage the idle thread await-\ning a response. Since AsyncIO was utilized, the\ngRPC AsyncIO API9 was imported instead of de-\nfault gRPC. The async gRPC is compatible with\nAsyncIO and can create asynchronous channels.\nExceptions or errors returned by the call were ac-\ncessed with grpc.aio.AioRpcError method.\nEven with everything above implemented within\nthe PySpark UDF, it was not possible to take advan-\ntage of AsyncIO yet. By default, a vanilla PySpark\nUDF receives only one tabular row at a time con-\ntaining one paragraph. That means that the Asyn-\ncIO loop within the UDF was not be able to execute\nconcurrent calls if only one paragraph was avail-\nable. Apache Spark’s Vectorized UDFs (Pandas\nUDFs) allows us to process partitions in batches\nand achieve the desired level of concurrency. Each\nbatch is represented in memory using the Apache\nArrow format and accessible with the Pandas API.\nApache Arrow is an in-memory columnar data\nformat that facilitates the transference of data be-\ntween the JVM (Java virtual machine), which runs\nthe Apache Spark job, and Python processes (i.e.\nPandas UDFs). It offers zero-copy reads between\nprocesses for data access without serialization over-\nhead. In our work, a scalar Pandas UDF was de-\nﬁned to receive paragraphs as a Pandas Series and\nreturn a probability distribution of the emotion\nclasses for each paragraph, as a new Pandas Se-\nries.10\nWith Python AsyncIO, gRPC Async, and Pan-\ndas UDFs using Apache Arrow, the load created\nby the client (PySpark job) substantially increased.\nAsyncIO ensured that extra paragraphs were sent\nto the server while waiting to receive emotion prob-\nabilities for outstanding calls. However, as soon as\nthe ﬁrst one thousand calls were sent to the server\n(in a matter of seconds), the PySpark job failed.\nThe errors received by the client were canceled,\nunavailable or the deadline was exceeded (more\nabout gRPC errors in section 3.4). That indicated\nthat the client actually created too much load on\nthe server causing it to respond with errors. The\nKubernetes Deployment was overwhelmed and did\nnot have enough time to scale up the micro-service.\nThis led to unavailable and deadline errors.\nTo limit the maximum number of concur-\n9https://grpc.github.io/grpc/python/\ngrpc_asyncio.html\n10https://spark.apache.org/docs/latest/\napi/python/user_guide/arrow_pandas.html\nSemaphore Value Responses Per Second\n10 170\n25 256.7\n50 298.3\n75 Service upscaling fails\nTable 2: Achieved number of successful gRPC calls per\nsecond vs Semaphore value. EMR conﬁguration, table\npartitions, and paragraphs are kept constant.\nrent calls to the service we utilized Semaphore.\nSemaphore is a class in the AsyncIO library11 that\nimplements an internal counter (set by user) to limit\nthe number of concurrent requests as described by\nDijkstra, 1968. Number of concurrent requests run-\nning in each core can never exceed the maximum\nSemaphore counter value.\nTo identify the maximum Semaphore value that\nsuccessfully scales up the number of Kubernetes\npods without errors, we conducted tests. Results of\nthe experiments are shown in Table 2.\nWith the Semaphore value set to 50, PySpark ran\nsuccessfully and signiﬁcantly increased the load set\nby the client to the server. Table 3 summarizes all\nlibraries and tools used to increase the number of\ncalls per second.\n3.4 Errors during gRPC calls\nAs gRPC async calls were made to the service,\nerrors were returned. The most common gRPC re-\nsponse status code exceptions12 encountered were:\ncancelled, unavailable, and deadline exceeded. We\nexpected to receive errors when the service was\nscaling to process the received requests. When an\nerror was received by a running PySpark client, the\nrunning job would terminate. Thus, we were un-\nable to produce inference results without a solution\nthat handles errors and keeps running the Spark\njob.\nA Circuit Breaker (Nygard, 2018) was imple-\nmented to prevent clients from overwhelming ser-\nvices and a gRPC Interceptor was implemented to\nwait for services to be available and retry failing\ncalls. Table 4 shows the total number of requests\nmade and the number of errors that were handled\nby the circuit breaker.\n11https://docs.python.org/3/library\n12https://grpc.github.io/grpc/core/md_\ndoc_statuscodes.html\n168\nTool Deﬁnition Description\nAsync IO Python Library Write concurrent requests with coroutines\ngRPC AsyncIO Python Library GRPC client that works asynchronously\nPyArrow with Pandas Data Format and Python\nLibrary\nPySpark’s tabular data format to pass to\nUDF as a Pandas table\nSemaphore Class in Async IO Limits number of running requests\nAsync Circuit Breaker Asynchronous Design Pat-\ntern\nResends client calls on failure\nTable 3: All libraries, design patterns, and data formats imported to PySpark job to accelerate inference speed.\nCode Status Notes Amount\n0 OK Returned on success 97.84M\n1 CANCELLED The operation was cancelled, typically by the caller 666\n4 DEADLINE_EXCEEDED The time expired before the operation was complete 469.58K\n14 UNA V AILABLE The service is currently unavailable 27.90K\nTable 4: Number of status codes returned in gRPC responses for the entire batch job.\n3.5 Asynchronous circuit breaker\nA circuit breaker is a software design pattern that\nwas implemented to detect and act upon response\nfailures received by the PySpark client. As dis-\ncussed in Nygard, 2018, in a closed state the circuit\npasses through and all gRPC calls are being made.\nIf a number of consecutive failures are received,\nthe circuit opens and subsequent request attempts\nreturn a failure immediately. After a time period,\nthe circuit switches to a half-open state to test if\nthe underlying problem still exists. If a call fails\nin this half-open state, the breaker is once again\ntripped. When a call ﬁnally succeeds, the circuit\nbreaker resets back to the default closed state.\nThe circuit breaker implementation was taken\nfrom an open-source library.13 Modiﬁcations were\nmade to support AsyncIO, so calls running through\nit are sent concurrently. The state of the circuit\nbreaker is shared across requests that use the the\nsame gRPC client. To open or close the circuit,\nthe circuit breaker only considers the deadline ex-\nceeded, unavailable, and cancelled gRPC status\ncodes. Other errors are directly returned to the\nclient.\nFinally, a gRPC Interceptor uses this circuit\nbreaker to block requests until the circuit breaker\nis closed again and retry each request up to 4 times,\nafter which the data point is skipped and the batch\njob continues. The interceptor gets attached to\nthe gRPC channel on creation. This design pat-\n13https://github.com/fabfuel/\ncircuitbreaker\ntern allows clients to not overwhelm services with\nrequests and halts our batch job as the service de-\nployment scales up.\n4 Results\nAll steps in Section 3 improve the batch job speed\nand results in satisfactory performance. The data\npipeline is able to produce inference results for\nmore than 95 million paragraphs in around 96 hours\nwith an inference speed of around 300 requests per\nsecond. The semaphore value is set to 50.\n4.1 Daily runs of the batch job\nOnce the backﬁll data is stored, the data pipeline\nruns daily to ﬁnd new and updated paragraphs from\nour S3 datasets. Everyday, around 600,000 (varies\ndaily) paragraphs need to have their inference val-\nues stored. The graph in Figure 2 illustrates the typ-\nical daily run for the pipeline. It shows that it takes\nabout 40 minutes for the Kubernetes micro-service\npods to fully scale up. We limited the maximum\nnumber of pods for daily jobs to 100.\n4.2 Analytics platform\nInference results were stored in an AWS S3 bucket.\nThis dataset was registered in a AWS Glue Data\nCatalog.14 Amazon Athena15 is a query service that\nmade it possible to run SQL queries on this dataset.\nRedash16 is a cloud-based analytics dashboard that\nwe used to visualize insights from the inference\n14https://aws.amazon.com/glue\n15https://aws.amazon.com/athena\n16https://redash.io\n169\nresults. In includes a SQL client that makes calls\nto Amazon Athena and displays the query results.\nRedash was connected to Amazon Athena as a data\nsource, which enabled us to perform queries to all\ntables registered in AWS Glue.\n5 Conclusion\nThis paper discussed a successful machine learning\narchitecture for both online and ofﬂine inference\nthat centralizes models as services. We present\nsolutions that use concurrency to increase the infer-\nence speed of ofﬂine batch jobs in Apache Spark.\nBecause of this, the majority of resources are still\nassigned to these services, and the batch job re-\nsources grow at a much smaller rate in comparison.\nWe used a resource-intensive language model\nfor emotion classiﬁcation, where we demonstrated\nhow proper tuning of TensorFlow Serving and Ku-\nbernetes can improve the service’s performance.\nWe also showed that by parallelizing the calls made\nto the service in PySpark, we can signiﬁcantly im-\nprove inference speed.\nFinally, results were presented that provide use-\nful insights into the inference performance. To-\ngether, all these components resulted in a satisfac-\ntory architecture, which resulted in the emotion\nprobabilities of 95 million paragraphs to be stored\nwithin 96 hours. We hope the architecture can be\napplied to other language tasks or machine learning\nmodels.\nReferences\nMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\nManjunath Kudlur, Josh Levenberg, Rajat Monga,\nSherry Moore, Derek G. Murray, Benoit Steiner,\nPaul Tucker, Vijay Vasudevan, Pete Warden, Martin\nWicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-\nsorﬂow: A system for large-scale machine learning.\nIn 12th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 16), pages 265–\n283, Savannah, GA. USENIX Association.\nOmar Y . Al-Jarrah, Paul D. Yoo, Sami Muhaidat,\nGeorge K. Karagiannidis, and Kamal Taha. 2015.\nEfﬁcient machine learning for big data: A review.\nBig Data Research, 2(3):87–93.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nEdsger W Dijkstra. 1968. Cooperating sequential pro-\ncesses. In The origin of concurrent programming,\npages 65–138. Springer.\nA. Gómez, Esperanza Albacete, Y . Sáez, and P. I.\nViñuela. 2014. A scalable machine learning on-\nline service for big data real-time analysis. 2014\nIEEE Symposium on Computational Intelligence in\nBig Data (CIBD), pages 1–8.\nApache Hadoop. 2006. Apache hadoop.\nhttp://hadoop.apache.org.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nAbhish Ijari. 2017. The study of the large scale twitter\non machine learning. International Research Journal\nof Engineering and Technology (IRJET), 4:247–251.\nChen Liu, Muhammad Osama, and Anderson De An-\ndrade. 2019. Dens: a dataset for multi-class emotion\nanalysis. Proceedings of the EMNLP Conference.\nMichael T Nygard. 2018. Release it!: design and deploy\nproduction-ready software. Pragmatic Bookshelf.\nCesare Pautasso, Erik Wilde, and Rosa Alarcon. 2013.\nREST: advanced research topics and practical appli-\ncations. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nMatei Zaharia, Mosharaf Chowdhury, Tathagata\nDas, Ankur Dave, Justin Ma, Murphy Mccauley,\nM Franklin, Scott Shenker, and Ion Stoica. 2012.\nFast and interactive analytics over hadoop data with\nspark. Usenix Login, 37(4):45–51.\nMatei Zaharia, Reynold S Xin, Patrick Wendell, Tatha-\ngata Das, Michael Armbrust, Ankur Dave, Xian-\ngrui Meng, Josh Rosen, Shivaram Venkataraman,\nMichael J Franklin, et al. 2016. Apache spark: a\nuniﬁed engine for big data processing. Communica-\ntions of the ACM, 59(11):56–65."
}