{
  "title": "Patterns of Polysemy and Homonymy in Contextualised Language Models",
  "url": "https://openalex.org/W3212354382",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2948149483",
      "name": "Janosch Haber",
      "affiliations": [
        "The Alan Turing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A314690538",
      "name": "Massimo Poesio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4287632625",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2155499572",
    "https://openalex.org/W3185650949",
    "https://openalex.org/W3029942262",
    "https://openalex.org/W1972978214",
    "https://openalex.org/W4250570372",
    "https://openalex.org/W2996282670",
    "https://openalex.org/W2998443519",
    "https://openalex.org/W2141766660",
    "https://openalex.org/W2096819730",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2165086233",
    "https://openalex.org/W3201826573",
    "https://openalex.org/W1985544732",
    "https://openalex.org/W3034675880",
    "https://openalex.org/W2404032066",
    "https://openalex.org/W2950226244",
    "https://openalex.org/W2303034160",
    "https://openalex.org/W2480727",
    "https://openalex.org/W3146681443",
    "https://openalex.org/W2046121397",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2109688847",
    "https://openalex.org/W650074479",
    "https://openalex.org/W2908151936",
    "https://openalex.org/W1984866353",
    "https://openalex.org/W2043148667",
    "https://openalex.org/W2143376531",
    "https://openalex.org/W2950502294",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W1481930023",
    "https://openalex.org/W2944053932",
    "https://openalex.org/W3119854206",
    "https://openalex.org/W2100313834",
    "https://openalex.org/W2065157922",
    "https://openalex.org/W120214402",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1572948005",
    "https://openalex.org/W2499883748",
    "https://openalex.org/W4246716965",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W651567163",
    "https://openalex.org/W3177005832",
    "https://openalex.org/W3118471049",
    "https://openalex.org/W2042689703",
    "https://openalex.org/W2146158822",
    "https://openalex.org/W2170886006",
    "https://openalex.org/W3188660305",
    "https://openalex.org/W1999599827",
    "https://openalex.org/W2499802589"
  ],
  "abstract": "One of the central aspects of contextualised language models is that they should be able to distinguish the meaning of lexically ambiguous words by their contexts. In this paper we investigate the extent to which the contextualised embeddings of word forms that display multiplicity of sense reflect traditional distinctions of polysemy and homonymy. To this end, we introduce an extended, human-annotated dataset of graded word sense similarity and co-predication acceptability, and evaluate how well the similarity of embeddings predicts similarity in meaning. Both types of human judgements indicate that the similarity of polysemic interpretations falls in a continuum between identity of meaning and homonymy. However, we also observe significant differences within the similarity ratings of polysemes, forming consistent patterns for different types of polysemic sense alternation. Our dataset thus appears to capture a substantial part of the complexity of lexical ambiguity, and can provide a realistic test bed for contextualised embeddings. Among the tested models, BERT Large shows the strongest correlation with the collected word sense similarity ratings, but struggles to consistently replicate the observed similarity patterns. When clustering ambiguous word forms based on their embeddings, the model displays high confidence in discerning homonyms and some types of polysemic alternations, but consistently fails for others.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2663–2676\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2663\nPatterns of Lexical Ambiguity in Contextualised Language Models\nJanosch Haber and Massimo Poesio\nQueen Mary University of London and The Alan Turing Institute\n{j.haber|m.poesio}@qmul.ac.uk\nAbstract\nOne of the central aspects of contextualised\nlanguage models is that they should be able\nto distinguish the meaning of lexically ambigu-\nous words by their contexts. In this paper we\ninvestigate the extent to which the contextu-\nalised embeddings of word forms that display\nmultiplicity of sense reﬂect traditional distinc-\ntions of polysemy and homonymy. To this end,\nwe introduce an extended, human-annotated\ndataset of graded word sense similarity and\nco-predication acceptability, and evaluate how\nwell the similarity of embeddings predicts sim-\nilarity in meaning.\nBoth types of human judgements indicate that\nthe similarity of polysemic interpretations falls\nin a continuum between identity of meaning\nand homonymy. However, we also observe\nsigniﬁcant differences within the similarity rat-\nings of polysemes, forming consistent patterns\nfor different types of polysemic sense alterna-\ntion. Our dataset thus appears to capture a sub-\nstantial part of the complexity of lexical ambi-\nguity, and can provide a realistic test bed for\ncontextualised embeddings.\nAmong the tested models, BERT Large shows\nthe strongest correlation with the collected\nword sense similarity ratings, but struggles\nto consistently replicate the observed simi-\nlarity patterns. When clustering ambiguous\nword forms based on their embeddings, the\nmodel displays high conﬁdence in discerning\nhomonyms and some types of polysemic alter-\nnations, but consistently fails for others.\n1 Introduction\nCapturing lexical ambiguity has been a driving fac-\ntor in the development of contextualised language\nmodels (e.g. Peters et al., 2018; Devlin et al., 2019).\nEvaluating their performance, much of the focus\nhas been on homonymy, a variety of multiplicity\nof meaning exempliﬁed by word forms such match\nin (1), whose different meanings are entirely unre-\nlated.\n(1) a. The match burned my ﬁngers.\nb. The match ended without a winner.\nAnd indeed, research such as (Wiedemann et al.,\n2019; Loureiro and Jorge, 2019; Blevins and Zettle-\nmoyer, 2020) has achieved promising results on\nusing contextualised language models to disam-\nbiguate homonyms. But homonymy is not the\nonly form lexical ambiguity can take (Pinkal, 1995;\nCruse, 1995; Poesio, 2020): in polysemy, word\nforms like school in (2) can elicit different distinct\nbut related senses (Lyons, 1977).\n(2) a. They agreed to meet at the school.\nb. The school has prohibited drones.\nc. The school called Tom’s parents.\nPolysemy is in fact much more common than\nhomonymy, and most words can be consid-\nered polysemous to some degree (Rodd et al.,\n2004; Falkum and Vicente, 2015; Poesio, 2020)\n–however, the ability of contextualised language\nmodels to capture this phenomenon has been stud-\nied much less. In this paper, we shift the focus\nto polysemy proper, and investigate how well con-\ntextualised language models capture graded word\nsense similarity as observed in human annotations.\nIt is important to carefully distinguish polysemy\nfrom homonymy, as multiplicity of meaning and\nmultiplicity of sense have almost opposing seman-\ntic effects: while a homonym needs to be inter-\npreted correctly in order to arrive at the correct\nmeaning of an utterance, polysemes refer to differ-\nent aspects or facets of the same concept, and might\nnot even need to be completely speciﬁed to elicit a\ngood-enough interpretation of what is meant (Kle-\npousniotou, 2002; Pylkkänen et al., 2006; Recasens\net al., 2011; Frisson, 2015; Poesio, 2020). Evidence\nfrom psycholinguistic studies supports this distinc-\ntion, indicating that polysemes are processed very\ndifferently than homonyms (Frazier and Rayner,\n1990; Rodd et al., 2002; Klepousniotou et al., 2008,\n2012). A growing body of work recently also has\n2664\nstarted to challenge the uniform treatment of po-\nlysemic sense postulated by traditional theories\nsuch as the Generative Lexicon (Pustejovsky, 1991;\nAsher and Pustejovsky, 2006; Asher, 2011), and\nput forward proposals of a more structured mental\nrepresentation of polysemic sense (Ortega-Andrés\nand Vicente, 2019). Using co-predication tests,\nstudies such as Antunes and Chaves (2003); Traxler\net al. (2005); Schumacher (2013) show that not all\npolysemic interpretations can be co-predicated, and\nthat some sense interpretations lead to zeugma:1\n(3) # They took the door off its hinges and\nwalked through it.\nJoining a range of recent work seeking to provide\nempirical data of graded word use similarity, in\nHaber and Poesio (2020a,b) we recently released\nan experimental small-scale dataset of graded word\nsense similarity judgements for a highly controlled\nset of polysemic targets to investigate the notion of\nstructured sense representations. Analysing results\nfor ten seminal English polysemes, we observed\nsigniﬁcant differences among polysemic sense in-\nterpretations (Erk et al., 2013; Nair et al., 2020;\nTrott and Bergen, 2021) and found ﬁrst evidence of\na distance-based grouping of word senses in some\nof the targets.\nIn this paper we present a modiﬁed and extended\nversion of this initial dataset to i) provide additional\nannotated data to validate previous observations,\nand ii) include new targets allowing for the same\nalternations as the initial set. This expansion en-\nables us to carry out analyses not possible with the\noriginal dataset, including iii) investigating simi-\nlarity patterns and polysemy types, iii) performing\na detailed analysis the correlation between human\njudgements and sense similarities predicted by con-\ntextualised language models, and iv) obtaining pre-\nliminary insights on how well their ‘off-the-shelf’\nrepresentations of word sense can be used to cluster\ndifferent sense interpretations of polysemic targets.\nThe new data conﬁrms previous observations of\nvarying distances between polysemic word sense\ninterpretations, and provides tentative evidence for\nsimilarity patterns within targets of the same type\nof polysemy. These patterns can to some degree\nbe replicated by the similarities of embeddings ex-\ntracted from BERT Large, opening up potential\navenues of research utilising contextualised em-\nbeddings to proxy costly human annotations for\n1Example from Cruse (1995)\nthe collection of a large-scale repository of ﬁne-\ngrained word sense similarity. The collected data\nis publicly available online.2\n2 Methods\nThe data for this study was created by revising and\nextending the dataset of contextualised word sense\nsimilarity presented in Haber and Poesio (2020a,b),\nand contains annotated sample contexts for differ-\nent sense interpretations of 28 English polysemic\nnouns.\n2.1 Target words\nOur initial dataset contained one target word\nfor twelve frequently discussed types of logical\nmetonymy (Dölling, 2020). We focused on this\nform of regular (Apresjan, 1974; Moldovan, 2019)\nor inherent (Pustejovsky, 2008) polysemy as it al-\nlows us to investigate and analyse the same inter-\npretation patterns across a number of target word\nforms. We included the original data for eight tar-\ngets, excluded two proper noun samples because\ntheir vanilla embeddings pooling sub-token encod-\nings did not yield stable results under a simple\ncosine comparison, and re-collected annotations\nfor two others that exhibited a high degree of anno-\ntation noise in the ﬁrst collection effort. We then\nselected 18 additional targets for our second anno-\ntation effort, each allowing for the same alterna-\ntions as one of the initial ten in order to investigate\npotential patterns in their distribution of sense inter-\npretations. The new dataset contains the following\nset of seminal and experimental English polysemic\ntarget words:\nanimal/meat: lamb, chicken, pheasant, seagull;\nfood/event: lunch, dinner; container-for-content:\nglass, bottle, cup; content-for-container: beer,\nwine, milk, juice; opening/physical: window,\ndoor; process/result: building, construction, settle-\nment; physical/information: book, record; physi-\ncal/information/organisation: newspaper, maga-\nzine; physical/information/medium: CD, DVD;\nbuilding/pupils/directorate/institution: school,\nuniversity\n2.2 Sample sentences\nFollowing the approach detailed in Haber and Poe-\nsio (2020b), instead of collecting corpus samples\n2https://github.com/dali-ambiguity/\nPatterns-of-Lexical-Ambiguity\n2665\ncontaining the selected target words, custom sam-\nples were created such that i) the ambiguous target\nexpression is the subject of the sentence, ii) the\ncontext is kept as short as possible, and iii) the con-\ntext invokes a certain sense as clearly as possible\nwithout mentioning that sense explicitly. 3 With\nthis method, pairs of sample sentences can easily\nbe tested for target word similarity, as well as com-\nbined into co-predication structures to obtain ac-\nceptability judgements. As an example, polyseme\nnewspaper is traditionally assumed to allow for at\nleast three sense interpretations: (1) organisation,\n(2) physical object and (3) information content. In\nthe materials, each of these senses is invoked in\ntwo different contexts a and b:\n1a The newspaper ﬁred its editor in chief.\n1b The newspaper was sued for defamation.\n2a The newspaper lies on the kitchen table.\n2b The newspaper got wet from the rain.\n3a The newspaper wasn’t very interesting.\n3b The newspaper is rather satirical today.\nComparing targets with the same number identiﬁer\nresults in what traditionally would be considered a\nsame-sense scenario, and comparing targets with\ndifferent number identiﬁers results in a cross-sense\ncomparison. For co-predication, two contexts are\ncombined into a single sentence by conjunction\nreduction (Zwicky and Sadock, 1975). As an ex-\nample, contexts 1a and 1b are combined into co-\npredication sample 1ab as follows:\n1ab The newspaper ﬁred its editor in chief and was\nsued for defamation.\nBesides polysemic alternations, some of the tar-\ngets also allow for homonymic alternations (e.g.\nmagazine with different senses related to the print\nmedium, but also a homonymic interpretation as a\nstorage type). Feedback on homonymic interpreta-\ntions will allow us to better put into perspective the\nresults obtained for polysemic alternations.\nWe omitted a collection of additional word class\njudgements trialled in Haber and Poesio (2020a) as\nwe found that these judgements performed poorly\nin distinguishing polysemes from homonyms, and\ndid not seem to exhibit the degree of sensitivity\nrequired for our analysis.\n3As in “The school is an old building.\" for sense building.\nSee Haber and Poesio (2020b) for more details.\n2.3 Human Annotation\nWe collected human annotations online through\nAmazon Mechanical Turk (AMT). As a ﬁrst mea-\nsure of word sense similarity, we asked participants\nto rate the similarity in meaning of a target word\nshown in two different contexts –providing a meta-\nlinguistic signal. Like in the initial data collec-\ntion run, we did so by highlighting target expres-\nsions in bold font and asking annotators to rate the\nhighlighted expressions using a slider labelled with\n“The highlighted words have a completely differ-\nent meaning” on the left hand side and “The high-\nlighted words have completely the same meaning”\non the right. The submitted slider positions were\ntranslated to a 100-point similarity score, providing\nus with a graded word sense similarity judgement\n(Erk et al., 2013; Lau et al., 2014). As a second\nmeasure of word sense similarity, we asked partici-\npants to rate the acceptability of a co-predication\nstructure combining two contexts with the same\ntarget. We again used a slider, this time labelled\nwith “The sentence is absolutely unacceptable” on\nthe left and “The sentence is absolutely acceptable”\non the right. In the co-predication setting, the poly-\nsemic target was not highlighted, providing us with\na more ecological similarity judgement.\nAnnotators were paid 0.70 USD for a completed\nsurvey with 20 items, for an average expected\nhourly rate of 7.00 USD. To improve judgement\nquality, we required annotators to be located in\nthe US, and have completed at least 5000 previous\nsurveys with an acceptance rate of at least 90%.\nAnnotators judged items without any prior training\nbased on minimal guidelines only.4\n2.4 Contextualised Language Models\nModels of polysemy have previously been pro-\nposed in distributional semantics (see for example\nBoleda et al., 2012), but for the most part, such\nmodels found limited application in computational\nlinguistics. This changed with the emergence of\na new generation of contextualised language mod-\nels like ELMo (Peters et al., 2018), BERT (De-\nvlin et al., 2019) and GPT-2 (Radford et al., 2019),\nwhich led to impressive improvement in a num-\nber of NLP applications. In order to assess word\nsense similarity encoded in contextualised embed-\ndings, we extracted target word embeddings from\nthe different disambiguating contexts and calcu-\n4For full instructions and a screenshot of the annotation\ninterface see Appendix A\n2666\nlated their cosine similarity (1-cosine). For ELMo\nwe used the pretrained model on TensorFlow Hub5\nand extracted target word vectors from the LSTM’s\nsecond layer hidden state, which has previously\nbeen shown to encode the most semantic informa-\ntion (see e.g. Ethayarajh, 2019; Haber and Poesio,\n2020b). We used the pretrained BERT Base (12\nlayers, hidden state size of 768) and BERT Large\n(24 layers, hidden state size of 1024) from the Hug-\ngingface transformers package.6 As suggested by\nLoureiro and Jorge (2019), we experimented with\nboth the last hidden state and the sum of the last\nfour hidden states as contextualised representation\nof a target word.7 Lastly, we established a baseline\nby averaging over the static Word2Vec (Mikolov\net al., 2013) encodings of all words in a sample con-\ntext to create a naive contextualised embedding.\n3 Results\nIn our analyses we focused on three different as-\npects. First, we computed graded similarity and\nacceptability ratings based on the collected anno-\ntations, and investigated how these ratings relate\nto traditional distinctions of lexical ambiguity and\nrecent proposals of a more structured representa-\ntion of polysemic senses, especially considering\nthe patterns of word sense similarities observed\nacross different target words allowing for the same\nset of sense alternations. We then analysed how\nthe different contextualised language models’ tar-\nget embeddings correlate with either of the human\nannotations, and to what degree they replicate the\npatterns of word sense similarity observed in the\nhuman annotations. Lastly, we analysed the contex-\ntualised embbedings themselves, for a preliminary\nassessment of how well these ‘off-the-shelf’ word\nsense encodings fare in clustering samples based\non their sense interpretation.\n3.1 Word Sense Similarity Ratings\nIn our second annotation effort, we collected an\nadditional 8980 pairwise judgements from 220\nunique AMT participants rating the similarity of\nhighlighted target words in different contexts. Af-\nter ﬁltering, we retained a total of 5862 judgements\n5https://tfhub.dev/google/ELMo/3\n6https://huggingface.co/transformers/\npretrained_models.html\n7We also tested a pretrained implementation of GPT-2,\nbut excluded this model from our analysis, as due to its more\ntraditional left-to-right text processing, all of our samples intro-\nducing targets as \"The target....\", led to identical embeddings\nin different contexts.\nPolysemy\nCondition\nSame\nCross\n0.0 0.2 0.4 0.6 0.8 1.0\nWord Sense Similarity\nHomonymy\nPolysemy\nCondition\nSame\nCross\n0.0 0.2 0.4 0.6 0.8 1.0\nCo-predication Acceptability\nHomonymy\nFigure 1: Distributions of explicit word sense similarity\nratings and co-predication acceptability ratings given\nto same-sense (blue) and cross-sense (orange) samples\nwith polysemic and homonymic alternations.\n(including those of Haber and Poesio, 2020b), with\nan average of 16.5 annotations per item (minimum\n7)8 and a per-questionnaire inter-annotator agree-\nment rate of 0.62 (Krippendorff’s alpha, Artstein\nand Poesio, 2008) –which is relatively high consid-\nering the continuous rating scale provided to our\nannotators.\nWe ﬁrst investigated potential effects of predi-\ncate ordering by applying a Mann-Whitney U test\n(Mann and Whitney, 1947) to ratings for identical\ncontext pairs that were presented in a different or-\nder during annotation. Only 22 of 229 pairwise\ntests yielded p-values <0.05, and none passed Bon-\nferroni correction. We therefore concluded that –as\nexpected– predicate ordering effects are negligi-\nble for the explicit word sense similarity ratings\nbased on our materials, and combined results for\nfurther analysis. Figure 1 (left column) shows the\ndistributions of word sense similarity ratings col-\nlected across all target words, separated on whether\nor not there is a sense alternation in the sample,\nand whether this alternation is traditionally con-\nsidered to be polysemic or homonymic in nature.\nHomonymic cross-sense samples obtained a mean\nsimilarity rating of just 0.17, signiﬁcantly lower\nthan the overall same-sense mean of 0.89 (p-value\n<0.05). Polysemic cross-sense samples received a\nmean similarity score of 0.73, which is both sig-\nniﬁcantly lower than the same-sense mean, and\nsigniﬁcantly higher than the homonym mean (see\nTable 1, row 1). These results support the tradi-\ntional view that polysemy occupies a distinctive\nmiddle ground between identity of meaning and\nhomonymy (Pinkal, 1995).\n8See Appendix B for more details on ﬁltering\n2667\nPolysemy\nCondition\nSame\nCross\n0.0 0.2 0.4 0.6 0.8 1.0\nWord2Vec Cosine Similarity\nHomonymy\nPolysemy\nCondition\nSame\nCross\n0.0 0.2 0.4 0.6 0.8 1.0\nELMo Cosine Similarity\nHomonymy\nPolysemy\nCondition\nSame\nCross\n0.0 0.2 0.4 0.6 0.8 1.0\nBert Base Cosine Similarity\nHomonymy\nPolysemy\nCondition\nSame\nCross\n0.0 0.2 0.4 0.6 0.8 1.0\nBert Large Cosine Similarity\nHomonymy\nFigure 2: Distributions of embedding similarity scores obtained for same-sense (blue) and cross-sense (orange)\nsamples with polysemic and homonymic alternations. BERT results for summing over the last four hidden states.\nNext, we grouped the data based on target words,\nand performed pairwise comparisons on all ratings\ngiven to their cross-sense interpretations. A large\nnumber of signiﬁcant comparisons would indicate a\nhigh variance in the assigned ratings; a low percent-\nage of signiﬁcant differences indicates a consistent\nrating of samples. Due to the large number of tests,\nwe then carry out a Bonferroni correction on the\nobtained results to establish a corrected, more con-\nservative signiﬁcance level and determine a lower\nbound on this statistic. Comparing all combina-\ntions of same-sense pairings for example, 20 of 58\ntests yielded signiﬁcantly different results (p-values\n<0.05), but only 4 entries passed Bonferroni correc-\ntion (6.90%), indicating that same-sense samples\nare quite consistently rated to invoke very similar\ninterpretations. 14.71% of the 34 pairwise compar-\nisons of homonymic cross-sense samples passed\nBonferroni correction, as did 23.44% of the 337\npairwise comparisons between ratings for polyse-\nmic cross-sense samples. Ratings for cross-sense\nsamples therefore are less consistent than same-\nsense ratings, and polysemic alternations are rated\nmore inconsistently than homonymic ones. Ob-\nserving this variance in similarity scores justiﬁes\nour use of a continuous rating scale for the anno-\ntation experiments. With almost a quarter of the\nsimilarity ratings for polysemic sense alternations\nshowing signiﬁcant differences to those of other\nsenses, these results also provide a novel type of\nempirical evidence against a uniform treatment of\npolysemic senses.\n3.2 Co-Predication Acceptability Ratings\nBesides these explicit similarity ratings, we col-\nlected an additional 8640 judgements from 192 par-\nticipants rating the acceptability of co-predication\nSame-Sense Cross-Sense\nMeasure Pol. Hom. p Pol. Hom. p\nSimilarity 0.89 0.96 0.03 0.73 0.17 <0.05\nAcceptability 0.83 0.86 0.10 0.64 0.41 <0.05\nWord2Vec 0.60 0.65 0.12 0.55 0.58 0.06\nELMo 0.90 0.87 0.14 0.87 0.82 <0.05\nBERT Base 0.91 0.93 0.22 0.88 0.78 <0.05\nBERT Base (L4)0.93 0.95 0.27 0.91 0.82 <0.05\nBERT Large 0.79 0.85 0.15 0.72 0.44 <0.05\nBERT Large (L4)0.88 0.91 0.18 0.84 0.64 <0.05\nTable 1: Word sense similarity distribution means for\nthe different measures investigated in this study. p-\nvalues calculated through Mann-Whitney U.\nstructures created from our sample sentences. Af-\nter adding judgements for selected targets from the\ninitial data and ﬁltering noisy annotations, we re-\ntained a total of 7379 judgements, for an average of\n16.75 annotations per target word (minimum 12).\nCo-predication acceptability is meant to provide\na more ecological signal of word sense similarity\nthan the explicit similarity ratings, with partici-\npants less aware of the factors that inﬂuence the\nperceived acceptability of the evaluated sentence.\nPer-questionnaire inter-annotator agreement here\nonly reached a Krippendorff’s alpha rating of 0.34,\nindicating stronger individual differences in the\nparticipants’ use of the continuous rating scale.\nInvestigating order effects in our co-predication\nsamples revealed that only 1 of 229 pairwise com-\nparisons between the acceptability scores of co-\npredication structures with different predicate or-\nderings passed the Bonferroni corrected signiﬁ-\ncance level of 0.00021. We therefore argue that\nour samples are free from any secondary accept-\nability factors based on predication order (Murphy,\n2021), and therefore indeed primarily test for the\nacceptability of invoking different senses of the\ntarget words. Based on this observation, we again\n2668\nCombination Correlation Ordinary Least Squares (OLS) Regression Analysis\nFirst Measure Second Measurer p Coef. R2 F-stat. Prob. Omnib. Prob.\nSimilarity Acceptability 0.698 1.09E-25 0.484 0.487 156.571 1.09E-25 9.733 0.008\nAcceptability Similarity 0.698 1.09E-25 1.005 0.487 156.571 1.09E-25 0.967 0.617\nWord2Vec Similarity 0.206 0.008 0.675 0.042 7.309 0.008 31.562 0\nWord2Vec Acceptability 0.311 4.39E-05 0.707 0.097 17.625 4.39E-05 9.668 0.008\nELMo Similarity 0.515 1.11E-12 2.863 0.265 59.475 1.11E-12 10.43 0.005\nELMo Acceptability 0.523 4.39E-13 2.018 0.273 61.973 4.39E-13 6.552 0.038\nBERT Base Similarity 0.641 1.02E-20 4.070 0.411 115.185 1.02E-20 3.496 0.174\nBERT Base Acceptability 0.560 3.43E-15 2.469 0.314 75.521 3.43E-15 2.07 0.355\nBERT Large Similarity 0.687 1.22E-24 2.181 0.472 147.361 1.22E-24 15.96 0\nBERT Large Acceptability 0.550 1.40E-14 1.212 0.302 71.520 1.40E-14 5.324 0.07\nTable 2: Correlations between measures of contextualised word sense similarity. The ﬁrst set of columns displays\npairwise correlation based on Pearson’sr, the second set shows the key statistics obtained from an OLS regression\nanalysis. BERT results for summing over the last four hidden states.\ncombine results before further analysis. Figure 1\n(right column) shows the distributions of collected\nco-predication acceptability ratings split by sam-\nple condition and ambiguity type. The average\nacceptability rating for co-predication structures\ninvoking the same sense in both predications is\n0.83, the mean acceptability for homonymic cross-\nsense samples is 0.41, and the mean acceptability\nfor polysemic alternations is 0.64 –signiﬁcantly\nlower than the same-sense mean but signiﬁcantly\nhigher than the homonym mean (see Table 1,\nrow 2). These results support previous observa-\ntions of co-predication acceptability, too, being a\nnon-binary signal but rather forming a continuum\n(Lau et al., 2014) and provide an additional chal-\nlenge to co-predication as a linguistic test to dis-\ntinguish polysemy from homonymy. Same-sense\nand homonymic samples were rated quite consis-\ntently, with only 10.34% and 5.88% of pairwise\ncomparisons passing Bonferroni correction, respec-\ntively. Polyseme samples again show some de-\ngree of inconsistency, with 21.66% of comparisons\namong polysemic cross-sense samples passing the\ncorrected signiﬁcance threshold of 0.00015. These\nresults duplicate the observations made above, and\nprovide additional evidence for the non-uniformity\nin interpreting polysemic samples.\n3.3 Computational Ratings\nWe extracted contextualised embeddings of target\nword forms using the models described above, and\ndetermined pairwise similarity scores by calculat-\ning the embeddings’ cosine similarity (1-cosine).\nAs samples were encoded individually, there are no\npotential order effects here. Figure 2 visualises the\ndistribution of target embedding similarity scores,\nand the bottom part of Table 1 details their distribu-\n0.2 0.4 0.6 0.8 1.0\nMean Co-predication Acceptability Judgement\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Mean Sense Similarity Judgement\nCo-predication Judgements v Similarity Judgements\n0.6 0.7 0.8 0.9\nBERT Large Cosine Similarity (Last 4)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Mean Sense Similarity Judgement\nBERT Large WE Similarity v Similarity Judgements\nFigure 3: Correlations of co-predication v word sense\nsimilarity ratings (left) and BERT Large cosine simi-\nlarity scores v word sense similarity ratings (right), to-\ngether with the best linear ﬁt. Scaling of x-axis adjusted\nfor clarity. BERT results for summing over the last four\nhidden states.\ntion means. It is instantly noticeable that all com-\nputational models assign a much narrower range\nof similarity scores to the ambiguous samples –\nan observation already made in Ethayarajh (2019).\nHomonymic and polysemic cross-sense ratings do\nnot form signiﬁcantly different distributions in the\nembeddings of the static Word2Vec model (p-value\n0.06), and –even more problematic– homonymic\ncross-sense samples show no signiﬁcant difference\nto same-sense samples (p-value 0.09). ELMo sur-\nprisingly struggles with the same distinction (p-\nvalue 0.09), but all BERT models produce clearly\ndistinct distributions for polysemic, homonymic\nand same-sense samples (all p-values <0.05).\nIn order to establish a measure of correlation\nbetween the similarity scores predicted by the\ncontextualised models and the collected human\njudgements, we calculated their pairwise correla-\ntion (Pearson’sr), and performed an ordinary least\nsquares (OLS) regression for each combination of\ncontextualised language model and human sense\n2669\nsimilarity measure. The results of these calcula-\ntions are displayed in Table 2, and a selection of\nthe pairwise comparisons is visualised in Figure\n3. The non-contextualised Word2Vec baseline dis-\nplays a low but signiﬁcant correlation with both\nhuman similarity measures, and shows an overall\nlow goodness-of-ﬁt, with R 2 values of the OLS\nregression at 4% and 10%, respectively. ELMo\nclearly outperforms this baseline, both in terms of\ncorrelation with the human measures, as well as\nin its goodness-of-ﬁt in the OLS regression anal-\nysis. For both BERT models, summing over the\nlast 4 hidden states improved correlation with the\nsimilarity ratings by about 6 points, and the correla-\ntion with the co-predication acceptability ratings by\nabout 4 points. We will therefore report only results\non this version in the remainder of this paper. Both\nmodels show a similar performance in predicting\nco-predication acceptability ratings as ELMo, with\na slight lead by BERT Base, but BERT Large is\nclearly the best-performing model when predicting\nexplicit similarity scores, with a correlation of 0.69\nto the human annotation, and an R2 goodness-of-ﬁt\nof 47%. This high degree of correlation is also\nvisible in the scatter plot in Figure 3. These results\nsuggest that particularly BERT Large seems to be\nable to capture nuanced word sense distinctions in\na similar way as human annotators, and are in con-\ntrast to our initial ﬁndings reported in Haber and\nPoesio (2020b), where we measured a correlation\nof only 0.21 between BERT Base target embed-\nding cosine similarities and word sense similarity\njudgements. We suggest that this difference in cor-\nrelation is due to a number of factors, including i)\nthe omission of the unstable proper noun targets,\nii) the re-collection of annotations for particularly\nnoisy items, iii) the use of a signiﬁcantly larger\namount of data, and iv) the inclusion of a number\nof homonymic targets, which populate the lower\nend of the spectrum and facilitate a better ﬁt.\n3.4 Similarity Patterns\nOne of the key reasons for extending our initial\ndataset was to add more target words for each of the\ntested types of polysemic sense alternation in order\nto allow for an investigation of sense similarity pat-\nterns across targets. Utilising the extended dataset,\nwe established a set of similarity maps containing\nthe mean similarity ratings for each combination\nof senses a given target word can take on, and com-\npared these between targets of the same type. For\nPairwise Overall\nMeasure r p <0.05 r p\nSimilarity 0.44 3/24 (12.5%) 0.53 8.260e-10\nAcceptability0.44 4/24 (16.7%) 0.62 5.306e-14\nELMo 0.14 0/24 (0%) 0.21 0.025\nBERT Large 0.28 1/24 (4.2%) 0.27 0.003\nTable 3: Mean Pearson correlation of polysemic word\nsense similarity patterns across different target words\nallowing the same alternation of senses, number of sig-\nniﬁcant comparisons, and overall pattern correlation.\nCriterion t #C NMI F1 P R\nInconsistency<0.7 3.54 0.60 0.77 0.86 0.71\nDistance 31 4.21 0.75 0.75 0.90 0.64\nTable 4: Best-performing settings for inconsistency and\ndistance-based hierarchical Ward clustering of target\nword senses. #C is the average number of clusters pro-\nduced per target.\nexample, Figure 4 displays the similarity maps for\ntarget words newspaper and magazine. The corre-\nlation between these similarity maps reaches 0.89\n(p-value = 0.001) in human similarity ratings, and\n0.95 (p-value = 6.88e-05) for co-predication ac-\nceptability, indicating a clear pattern in the target’s\nsimilarity ratings. In the similarity maps based on\nthe cosines between BERT Large embeddings, the\ncorrelation reaches only 0.65 (p-value = 0.06), and\njust 0.34 (p-value = 0.37) in the ELMo similarity\nmaps. The overall pattern correlations across target\nwords of the same polysemy type can be found in\nTable 3. The ﬁrst set of scores are based on the\ncorrelations of all pairwise comparisons of poly-\nsemes that allow for the same alternations. Due to\nthe small number of senses tested in this study, in\nmost cases this comparison however does not allow\nfor signiﬁcant results. We therefore also calculated\na second score by appending all pairwise compar-\nisons into two separate lists and determining the\ncorrelation between these two lists. This is likely to\nrepresent a better estimate of overall pattern consis-\ntency, but might under-value inconsistent patterns.\nThe mean correlation between BERT Large’s sim-\nilarity maps and the human sense similarity maps\nis 0.49, with one signiﬁcantly similar pairing, and\n0.52 compared to co-predication similarity maps (4\nsigniﬁcant pairings) –rates comparable to the cor-\nrelation between the two human annotations (mean\nr = 0.54, 10 comparisons with p<0.05).\nA qualitative analysis of the similarity maps re-\nvealed that while some alternation types like ani-\nmal/meat do exhibit consistent similarity patterns\n2670\n1 2 3\n123\nSimilarity\n1 2 3\nAcceptability\n1 2 3\nBERT Large (Last 4)\n1 2 3\nELMo\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.6\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\nJudgement Patterns: Newspaper\n1 2 3\n123\nSimilarity\n1 2 3\nAcceptability\n1 2 3\nBERT Large (Last 4)\n1 2 3\nELMo\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.6\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\nJudgement Patterns: Magazine\nFigure 4: Similarity patterns in the sense similarity ratings for polysemes newspaper and magazine.\nSenses: 1-physical, 2-information, 3-organisation. Colour scales adjusted for computational measures.\n3b 1a 1b 2a 2b 3a\n0\n20\n40\n60\nNewspaper\n4b 4a 2b 2a 3b 3a 1a 1b\n0\n20\n40\n60\n80\nMagazine\nFigure 5: Dendograms of BERT Large contextualised\nembedding similarity for a selection of target words.\nNumbers indicate traditional sense distinctions.\nacross targets, others like the content-for-container\nalternation do not display any discernible similarity\npatterns at all. 9 These observations suggest that\nsense similarity patterns are best to be investigated\nwithin a given type of alternation, and further re-\nsearch will be needed to develop a more detailed\naccount of polysemy types and sense similarity\npatterns.\n3.5 Sense Clustering\nAs BERT Large displayed a high correlation with\nthe human judgements of word sense similarity,\nand some capability in replicating similarity pat-\nterns across target words, we next wanted to in-\nvestigate how well BERT’s contextualised embed-\ndings can be used to cluster our polysemous tar-\ngets according to their interpretation (McCarthy\net al., 2016; Garí Soler and Apidianaki, 2021). To\nprovide a tentative analysis, we grouped BERT\nLarge’s contextualised target encodings based on\n9See Figure 7 in the Appendix for the similarity maps of\nthe animal/meat targets\ntheir similarity using the hierarchical Ward clus-\ntering method implemented in SciPy.10 We opted\nfor hierarchical clustering as this method has to de-\ntermine the optimal number of clusters itself, and\ndoes not take this number as an argument like most\nclustering methods do. We experimented with two\ndifferent clustering criteria based only on a thresh-\nold parameter t. The quantitatively best-performing\nsettings are displayed in Table 4.11 Both settings\nproduce more clusters than the traditional grouping\nof the tested targets would assume, which indicates\nthat especially precision scores might be artiﬁcially\nhigh –but overall the clustering seems to produce\nsensible results. Figure 5 displays a selection of\ndendograms produced by the clustering. The group-\ning of newspaper interpretations clearly separates\nthe organisation sense 1 from the physical object\ninterpretation 3, but splits the information sam-\nples 2 among the two, indicating the similarity in\ntheir contextualised embeddings. For magazine,\nthe clustering of samples creates four clear group-\nings, with the organisation reading showing the\nmost similarity with theinformation interpretations,\nand clearly separating the three polysemic senses\nfrom the homonymic storage reading 4. The clus-\ntering of alternations like food/event, animal/meat\nand process/result appears work consistently well,\nwhile others like the content-for-container alterna-\ntion lead to consistently wrong sense groupings.\n10https://docs.scipy.org/doc/scipy/\nreference/generated/scipy.cluster.\nhierarchy.fcluster.html\n11See Appendix D for more detail on clustering\n2671\n4 Related Work\nMost work focused on the word sense disambigua-\ntion capabilities of contextualised language models\ninvestigates the classiﬁcation of homonyms with\nclear-cut evaluation criteria (see Loureiro et al.\n2021 for a recent summary). Polysemy proper adds\nanother dimension of difﬁculty, since related senses\ncan be perceived to be very similar to one another,\nand some form of graded relatedness criterion in\nnecessary to properly evaluate model predictions\n(Erk et al., 2013; Lau et al., 2014). Datasets that\ncapture graded similarity judgements usually do\nso for word pairs in isolation –often intended to\nevaluate static word sense embeddings (Taieb et al.,\n2019), or are conducted on a small number of items\n(Erk et al., 2013). Notable exceptions are the Word\nin Context (Wic) dataset by Pilehvar and Camacho-\nCollados (2019), which contains over 7,000 sen-\ntence pairs with an overlapping English word, but\nwas annotated based on a binary classiﬁcation task.\nThe CoSimLex dataset (Armendariz et al., 2020)\non the other hand collects graded similarity judge-\nments –but does so for different, related targets.\nIn parallel to our work, Nair et al. (2020) re-\ncently conducted an investigation of 32 polyse-\nmic and homonymic word types extracted from\nthe Semcor corpus (Miller et al., 1993) by compar-\ning the distances between a selection of cross-sense\nsamples as determined by participants arranging\nthem in a 2D spatial arrangement task. In line with\nour results, they reported polysemic senses to be\nrated signiﬁcantly more similar to one another in\nboth the human annotations and BERT Base em-\nbeddings, and found a strong correlation between\nthe cosine distance of BERT sense centroids and\naggregated relatedness judgements. In a similar ap-\nproach, Trott and Bergen (2021) recently presented\na novel dataset of 112 polysemes and homonyms,\nfor each of which a number of highly controlled\nsentence pairs were annotated for similarity of use.\nWhile their data is very similar to ours, one no-\nticeable difference can be found in the distribution\nof cross-sense polyseme ratings. Based on our\nsamples, different polyseme interpretations were\nrated to be mostly quite similar still, but their data\ndisplays an almost even distribution of similarity\nscores assigned to them. A closer inspection of the\ntargets used in their study revealed two main factors\nthat are likely to have contributed to this difference.\nFirstly, while all of our targets were speciﬁcally\nchosen to be regular, metonymic polysemes, a large\npart of Trott and Bergen’s polysemic targets are ex-\namples of metaphoric polysemy. Re-analysing their\ndata after distinguishing these different branches\nof polysemy might help to further investigate their\nrespective effects. And secondly, we noticed the\nuse of compound nouns (i.e. ice cone vs trafﬁc\ncone) to disambiguate target words. Considering\npolysemy as a form of under-speciﬁed language\nuse, we argue that these expressions might under-\nmine the raison d’être of polysemy proper as they\nover-specify the ambiguous target –but highlight\nan interesting additional facet of this research.\n5 Conclusion\nWe present a revised and extended dataset of graded\nword sense similarity for 28 seminal, lexically am-\nbiguous word forms. The collected data supports\nprevious observations of signiﬁcant similarity dif-\nferences between polysemic interpretations and led\nto the discovery of tentative patterns of word sense\nsimilarity for certain types of alternations. While\nmore work on this matter will be needed before def-\ninite conclusions can be drawn, both of these obser-\nvations can be taken as additional evidence against\nlinguistic models proposing a uniform treatment of\npolysemic senses. We also used the collected data\nto test how well different ‘off-the-shelf’ contextu-\nalised language models can predict human word\nsense similarity ratings. Among the tested models,\nespecially BERT Large seems to capture nuanced\nword sense distinctions in a similar way to human\nannotators, and to some degree is capable of group-\ning sense interpretations by their contextualised\nembeddings. We hope to further expand the dataset\npresented in this paper to create a novel, more com-\nplex benchmark for the word sense disambiguation\n(WSD) task. In this endeavour, contextualised lan-\nguage models could be used to automatically detect\nrelevant target word forms, and to collect corpus\nsamples exhibiting speciﬁc targets and interpreta-\ntions to be rated by human annotators for a more\nrealistic, real-world test bed.\nAcknowledgements\nThe work presented in this paper was supported by\nthe DALI project, ERC Grant 695662. Janosch\nHaber is now part of the Turing Enrichment\nScheme. The authors would like to thank Derya\nÇokal and Andrea Bruera for their input, and the\nanonymous reviewers for their feedback.\n2672\nReferences\nSandra Antunes and Rui Pedro Chaves. 2003. On the\nLicensing Conditions of Co-Predication. In Pro-\nceedings of the 2nd International Workshop on Gen-\nerative Approaches to the Lexicon.\nJuri D. Apresjan. 1974. Regular polysemy. Linguistics,\n12:5–32.\nCarlos Santos Armendariz, Matthew Purver, Matej\nUlˇcar, Senja Pollak, Nikola Ljubeši ´c, and Mark\nGranroth-Wilding. 2020. CoSimLex: A resource\nfor evaluating graded word similarity in context.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 5878–5886, Mar-\nseille, France. European Language Resources Asso-\nciation.\nRon Artstein and Massimo Poesio. 2008. Inter-coder\nagreement for computational linguistics. Comput.\nLinguist., 34(4):555–596.\nNicholas Asher. 2011. Lexical Meaning in Context: A\nWeb of Words. Cambridge University Press.\nNicholas Asher and James Pustejovsky. 2006. A type\ncomposition logic for generative lexicon. Journal of\nCognitive Science, 6(1).\nTerra Blevins and Luke Zettlemoyer. 2020. Moving\ndown the long tail of word sense disambiguation\nwith gloss informed bi-encoders. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 1006–1017, On-\nline. Association for Computational Linguistics.\nGemma Boleda, Sabine Schulte im Walde, and Toni\nBadia. 2012. Modeling regular polysemy: A study\non the semantic classiﬁcation of catalan adjectives.\nComputational Linguistics, 38(3):575–616.\nAlan D. Cruse. 1995. Polysemy and related phenom-\nena from a cognitive linguistic viewpoint. In Patrick\nSaint-Dizier and Evelyn Viegas, editors, Computa-\ntional Lexical Semantics , Studies in Natural Lan-\nguage Processing, page 33–49. Cambridge Univer-\nsity Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJohannes Dölling. 2020. Systematic Polysemy. In\nDaniel Gutzmann, Lisa Matthewson, Cécile Meier,\nHotze Rullmann, and Thomas Ede Zimmermann, ed-\nitors, The Wiley Blackwell Companion to Semantics.\nWiley.\nKatrin Erk, Diana McCarthy, and Nicholas Gaylord.\n2013. Measuring word meaning in context. Com-\nputational Linguistics, 39(3):511–554.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? comparing the geome-\ntry of bert, elmo, and gpt-2 embeddings. Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP).\nIngrid Lossius Falkum and Augustin Vicente. 2015.\nPolysemy: Current perspectives and approaches.\nLingua, 157:1–16.\nLyn Frazier and Keith Rayner. 1990. Taking on seman-\ntic commitments: Processing multiple meanings vs.\nmultiple senses. Journal of Memory and Language.\nSteven Frisson. 2015. About bound and scary books:\nThe processing of book polysemies. Lingua, 157:17\n– 35. Polysemy: Current Perspectives and Ap-\nproaches.\nAina Garí Soler and Marianna Apidianaki. 2021. Let’s\nPlay Mono-Poly: BERT Can Reveal Words’ Poly-\nsemy Level and Partitionability into Senses. Trans-\nactions of the Association for Computational Lin-\nguistics, 9:825–844.\nJanosch Haber and Massimo Poesio. 2020a. Assessing\npolyseme sense similarity through co-predication ac-\nceptability and contextualised embedding distance.\nIn Proceedings of the Ninth Joint Conference on Lex-\nical and Computational Semantics , pages 114–124,\nBarcelona, Spain (Online). Association for Compu-\ntational Linguistics.\nJanosch Haber and Massimo Poesio. 2020b. Word\nsense distance in human similarity judgements and\ncontextualised word embeddings. In Proceedings\nof the Probability and Meaning Conference (PaM\n2020), pages 128–145, Gothenburg. Association for\nComputational Linguistics.\nEkaterini Klepousniotou. 2002. The Processing of Lex-\nical Ambiguity: Homonymy and Polysemy in the\nMental Lexicon. Brain and Language, 81(1-3):205–\n223.\nEkaterini Klepousniotou, G. Bruce Pike, Karsten Stein-\nhauer, and Vincent Gracco. 2012. Not all ambigu-\nous words are created equal: An EEG investigation\nof homonymy and polysemy. Brain and Language.\nEkaterini Klepousniotou, Debra Titone, and Carolina\nRomero. 2008. Making sense of word senses: The\ncomprehension of polysemy depends on sense over-\nlap. Journal of Experimental Psychology: Learning,\nMemory, and Cognition, 34(6):1534–1543.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2014. Measuring Gradience in Speakers’ Grammat-\nicality Judgements. Proceedings of the 36th Annual\nMeeting of the Cognitive Science Society (CogSci\n2014).\n2673\nDaniel Loureiro and Alípio Jorge. 2019. Language\nmodelling makes sense: Propagating representations\nthrough WordNet for full-coverage word sense dis-\nambiguation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5682–5691, Florence, Italy. Associa-\ntion for Computational Linguistics.\nDaniel Loureiro, Kiamehr Rezaee, Mohammad Taher\nPilehvar, and Jose Camacho-Collados. 2021. Anal-\nysis and Evaluation of Language Models for Word\nSense Disambiguation. Computational Linguistics,\npages 1–55.\nJohn Lyons. 1977. Semantics, volume 2. Cambridge\nUniversity Press.\nH. B. Mann and D. R. Whitney. 1947. On a Test of\nWhether one of Two Random Variables is Stochasti-\ncally Larger than the Other. The Annals of Mathe-\nmatical Statistics, 18(1):50 – 60.\nDiana McCarthy, Marianna Apidianaki, and Katrin\nErk. 2016. Word sense clustering and clusterability.\nComput. Linguist., 42(2):245–275.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In 1st International Con-\nference on Learning Representations, ICLR 2013 -\nWorkshop Track Proceedings.\nGeorge A. Miller, Claudia Leacock, Randee Tengi, and\nRoss T. Bunker. 1993. A semantic concordance. In\nProceedings of the Workshop on Human Language\nTechnology, HLT ’93, page 303–308, USA. Associa-\ntion for Computational Linguistics.\nAndrei Moldovan. 2019. Descriptions and tests for po-\nlysemy. Axiomathes, pages 1–21.\nElliot Murphy. 2021. Predicate order and coherence in\ncopredication. Inquiry, 0(0):1–37.\nSathvik Nair, Mahesh Srinivasan, and Stephan Meylan.\n2020. Contextualized word embeddings encode as-\npects of human-like word sense knowledge. In Pro-\nceedings of the Workshop on the Cognitive Aspects\nof the Lexicon, pages 129–141, Online. Association\nfor Computational Linguistics.\nMarina Ortega-Andrés and Agustín Vicente. 2019. Po-\nlysemy and co-predication. Glossa: a journal of\ngeneral linguistics, 4(1).\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. CoRR, abs/1802.05365.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning represen-\ntations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1267–1273, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nManfred Pinkal. 1995. Logic and Lexicon. The seman-\ntics of the indeﬁnite . Kluwer Academic Publishers,\nDordrecht.\nMassimo Poesio. 2020. Ambiguity. In Daniel Gutz-\nmann, Lisa Matthewson, Cécile Meier, Hotze Rull-\nmann, and Thomas Ede Zimmermann, editors, The\nWiley Blackwell Companion to Semantics. Wiley.\nJames Pustejovsky. 1991. The Generative Lexicon.\nComput. Linguist., 17(4):409–441.\nJames Pustejovsky. 2008. From concepts to mean-\ning: The role of lexical knowledge. In Piet van\nSterkenburg, editor, Unity and Diversity of Lan-\nguages, pages 73–84. John Benjamins Publishing\nCompany.\nLiina Pylkkänen, Rodolfo Llinás, and Gregory L. Mur-\nphy. 2006. The representation of polysemy: Meg ev-\nidence. Journal of cognitive neuroscience, 18(1):97–\n109.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMarta Recasens, Eduard Hovy, and M. Antònia Martí.\n2011. Identity, non-identity, and near-identity: Ad-\ndressing the complexity of coreference. Lingua,\n121(6):1138–1152.\nJennifer Rodd, Gareth Gaskell, and William Marslen-\nWilson. 2002. Making sense of semantic ambiguity:\nSemantic competition in lexical access. Journal of\nMemory and Language, 46(2):245 – 266.\nJennifer M. Rodd, M. Gareth Gaskell, and William D.\nMarslen-Wilson. 2004. Modelling the effects of se-\nmantic ambiguity in word recognition. Cognitive\nScience, 28(1):89–104.\nPetra Schumacher. 2013. When combinatorial process-\ning results in reconceptualization: toward a new ap-\nproach of compositionality. Frontiers in Psychology,\n4:677.\nMohamed Ali Hadj Taieb, Torsten Zesch, and Mo-\nhamed Ben Aouicha. 2019. A survey of semantic\nrelatedness evaluation datasets and procedures. Arti-\nﬁcial Intelligence Review, pages 1–42.\nMatthew J. Traxler, Brian McElree, and Martin J.\nWilliams, Rihana S .and Pickering. 2005. Context\neffects in coercion: Evidence from eye movements.\nJournal of Memory and Language, 53(1):1–25.\nSean Trott and Benjamin Bergen. 2021. RAW-C: Re-\nlatedness of ambiguous words in context (a new lex-\nical resource for English). In Proceedings of the\n2674\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 7077–7087, Online. As-\nsociation for Computational Linguistics.\nGregor Wiedemann, Steffen Remus, Avi Chawla, and\nChris Biemann. 2019. Does BERT Make Any\nSense? Interpretable Word Sense Disambiguation\nwith Contextualized Embeddings.\nArnold M. Zwicky and Jerrold M. Sadock. 1975. Am-\nbiguity tests and how to fail them. In Syntax and\nSemantics volume 4, pages 1–36. Brill.\nA Annotation Instructions and Interface\nIn the word sense judgement task, participants\nwere given the following set of instructions:\n\"Carefully read each pair of sentences and\nspecify how similar the highlighted words are\nby using the slider. The slider ranges from ’The\nhighlighted words have a completely different\nmeaning’ on the far left to ’The highlighted words\nhave completely the same meaning’ on the far right.\nThere are 20 sentence pairs.\nThe survey contains a number of test items that\ncan be used to determine whether you are carefully\nreading the sentences or are submitting random\nanswers. Submissions that fail the test items will\nbe rejected.\"\nA screenshot of the the AMT interface for this\ntask is displayed in Figure 6. In the sentence\nacceptability task, the following instructions were\nshown to the participants:\n\"Carefully read each sentence and specify\nhow acceptable it is by using the slider. The\nslider ranges from ’The sentence is absolutely\nunacceptable’ on the far left to ’The sentence is\nabsolutely acceptable’ on the far right.\nThere are 20 sentences.\nThe survey contains a number of test items that\ncan be used to determine whether you are carefully\nreading the sentences or are submitting random\nanswers. Submissions that fail the test items will\nbe rejected.\"\nB Filtering\nIn order to reduce annotation noise, we ﬁltered\nout submissions from participants who failed to\nrate test items according to a set of custom criteria.\nSurveys in both experiments each contained two\ntest items.\nIn the word sense similarity annotation study,\none test item would show a (homonymic) target\ninterpreted in the same way in both sentences, with\nminimal changes to the context (test-same):\n(4) 1. The mole dug tunnels all throughout the\ngarden.\n2. The mole dug tunnels under the ﬂower\nbed.\nThe second test item would include two\nsentences with unrelated (homonymic) targets\n(test-random):\n(5) 1. The model wore a new dress designed\nby Versace.\n2. The seal indicated that the letter had\nnever been opened.\nSubmissions were excluded from analysis if ei-\nther the test-same item was rated below 0.7\nsimilarity, or the test-random item was rated\nabove 0.2 similarity.\nIn the co-predication study, the test-same\nitem would be no actual co-predication structure (to\nprevent any potential infelicitous co-predication),\nbut a similar-looking sentence with a conjunctive\nphrase:\n(6) A group of boys were playing Frisbee in\nthe park and a girl tried to balance on a\nslack line.\nThe test-random item would have the ﬁrst\npart of a conjunctive sentence, but end it a randomly\nscrambled phrase:\n(7) The match ended without a clear winner\nand the off the managed bass hook get to.\nSubmissions were excluded from analysis if\nboth, the test-same item was rated below 0.7\nsimilarity and the test-random item was rated\nabove 0.2 similarity.\nC Animal/Meat Similarity Maps\nFigure 7 shows the similarity maps for the tested\nanimal/meat alternation polyseme targets chicken,\n2675\nFigure 6: Screenshot of the the AMT interface for the explicit word sense similarity annotation task.\nlamb, pheasant and seagull. Both, chicken and\nlamb are common variants, pheasant is less fre-\nquent, and seagull would typically not be consid-\nered a member of this type, but still shows a similar\npattern in the co-predication acceptability ratings\nand BERT Large cosine similarity.\nD Clustering\nWe experimented with two clustering criteria: us-\ning node inconsistency, all leaf descendants of a\ncluster node belong to the same cluster if that node\nand all these descendants have an inconsistent value\nless than or equal to a threshold value t. Under the\ndistance criterion, clusters are formed so that the\nobservations in each cluster have no greater dis-\ntance than the set threshold value t. Figure 8 shows\nthe development of cluster purity, Normalised Mu-\ntual Information (NMI) and weighted F1 scores for\ndifferent values of threshold t using the inconsis-\ntency criterion (left) and distance criterion (centre).\nThe right graph plots the average number of clus-\nters produced by both measures with increasing\nthreshold t (gold mean: 3.0).\n2676\n1 2\n12\nSimilarity\n1 2\nAcceptability\n1 2\nBERT Large (Last 4)\n1 2\nELMo\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.6\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\nJudgement Patterns: Chicken\n1 2\n12\nSimilarity\n1 2\nAcceptability\n1 2\nBERT Large (Last 4)\n1 2\nELMo\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.6\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\nJudgement Patterns: Lamb\n1 2\n12\nSimilarity\n1 2\nAcceptability\n1 2\nBERT Large (Last 4)\n1 2\nELMo\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.6\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\nJudgement Patterns: Pheasant\n1 2\n12\nSimilarity\n1 2\nAcceptability\n1 2\nBERT Large (Last 4)\n1 2\nELMo\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.6\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\nJudgement Patterns: Seagull\nFigure 7: Similarity patterns in the sense similarity ratings for animal/meat alternation polysemes.\nSenses: 1-animal, 2-meat. Colour-scales adjusted for computational measures.\n0.6 0.7 0.8 0.9 1.0 1.1 1.2\nThreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClustering Performance (Inconsisitency)\nPurity\nNMI\nWeighted F1\n20 25 30 35 40 45 50\nThreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClustering Performance (Distance)\nPurity\nNMI\nWeighted F1\nThreshold\n1\n2\n3\n4\n5\nAverage Number of Clusters\nInconsistency\nDistance\nFigure 8: Clustering performance for the inconsistency (left) and distance (centre) criterion when grouping BERT\nLarge contextualised embeddings with linear Ward clustering based on clustering threshold t. Right: Average\nnumber of clusters produced by the clustering methods (gold mean: 3.0).",
  "topic": "Polysemy",
  "concepts": [
    {
      "name": "Polysemy",
      "score": 0.9492031335830688
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.7075929641723633
    },
    {
      "name": "Ambiguity",
      "score": 0.6044062376022339
    },
    {
      "name": "Meaning (existential)",
      "score": 0.6017544269561768
    },
    {
      "name": "Linguistics",
      "score": 0.5755066871643066
    },
    {
      "name": "Semantic similarity",
      "score": 0.5469215512275696
    },
    {
      "name": "Natural language processing",
      "score": 0.514987051486969
    },
    {
      "name": "Computer science",
      "score": 0.48082292079925537
    },
    {
      "name": "Word (group theory)",
      "score": 0.46914830803871155
    },
    {
      "name": "Alternation (linguistics)",
      "score": 0.4450515806674957
    },
    {
      "name": "Determiner",
      "score": 0.44154953956604004
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43418997526168823
    },
    {
      "name": "Psychology",
      "score": 0.29676371812820435
    },
    {
      "name": "Noun",
      "score": 0.1417028307914734
    },
    {
      "name": "Philosophy",
      "score": 0.07061752676963806
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210128584",
      "name": "The Alan Turing Institute",
      "country": "GB"
    }
  ],
  "cited_by": 13
}