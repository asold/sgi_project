{
  "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
  "url": "https://openalex.org/W4389524052",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098406446",
      "name": "Zhaoyang Wang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2612910427",
      "name": "Shaohan Huang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2103604808",
      "name": "Yuxuan Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125988827",
      "name": "Jiahai Wang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2143783680",
      "name": "Minghui Song",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2126310749",
      "name": "Zihan Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2102141881",
      "name": "Hai-zhen Huang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2114996910",
      "name": "Weiwei Deng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1997960622",
      "name": "Sun Feng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1964204209",
      "name": "Qi Zhang",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4378499145",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4385571831",
    "https://openalex.org/W4385571260",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4385681611",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4387075017",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385570498",
    "https://openalex.org/W4389524372",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3156420264",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4379089709",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1948–1966\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDemocratizing Reasoning Ability:\nTailored Learning from Large Language Model\nZhaoyang Wang†1 Shaohan Huang2 Yuxuan Liu†3 Jiahai Wang∗1 Minghui Song2\nZihan Zhang2 Haizhen Huang2 Furu Wei2 Weiwei Deng2 Feng Sun2 Qi Zhang2\nSchool of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China1\nMicrosoft2 Peking University3\nwangzhaoy22@mail2.sysu.edu.cn yx.liu@stu.pku.edu.cn\n{shaohanh,zihzha,fuwei,hhuang,zhang.qi}@microsoft.com\nwangjiah@mail.sysu.edu.cn\nAbstract\nLarge language models (LLMs) exhibit im-\npressive emergent abilities in natural language\nprocessing, but their democratization is hin-\ndered due to huge computation requirements\nand closed-source nature. Recent research on\nadvancing open-source smaller LMs by dis-\ntilling knowledge from black-box LLMs has\nobtained promising results in the instruction-\nfollowing ability. However, the reasoning abil-\nity which is more challenging to foster, is rel-\natively rarely explored. In this paper, we pro-\npose a tailored learning approach to distill such\nreasoning ability to smaller LMs to facilitate\nthe democratization of the exclusive reason-\ning ability. In contrast to merely employing\nLLM as a data annotator, we exploit the poten-\ntial of LLM as a reasoning teacher by building\nan interactive multi-round learning paradigm.\nThis paradigm enables the student to expose\nits deficiencies to the black-box teacher who\nthen can provide customized training data in\nreturn. Further, to exploit the reasoning po-\ntential of the smaller LM, we propose self-\nreflection learning to motivate the student to\nlearn from self-made mistakes. The learning\nfrom self-reflection and LLM are all tailored\nto the student’s learning status, thanks to the\nseamless integration with the multi-round learn-\ning paradigm. Comprehensive experiments and\nanalysis on mathematical and commonsense\nreasoning tasks demonstrate the effectiveness\nof our method. The code will be available at\nhttps://github.com/Raibows/Learn-to-Reason.\n1 Introduction\nLarge language models (LLMs) with emergent abil-\nities have achieved remarkable success across a\nwide range of tasks, deeply changed the landscape\nof both research and applications in natural lan-\nguage processing (Brown et al., 2020; Chen et al.,\n†Work done during internship at Microsoft.\n∗Corresponding author.\nStudent LM Teacher LLM \na) Annotated Data\nb\n) \nStudent’\ns\n \nFeedback\nc\n)\nS\ne\nl\nf\n-\nR\ne\nf\nl\ne\nc\nt\ni\no\nn\nFigure 1: Tailored learning from LLM. In contrast to\nprevious works merely adopt a), we propose b) and c)\nto further improve the reasoning distillation.\n2021; Chowdhery et al., 2022; OpenAI, 2023). And\nWei et al. (2022a,b) argue that emergent abilities\nparticularly in reasoning only exist in LLMs whose\nparameters are commonly larger than 100B. Never-\ntheless, a line of research (Touvron et al., 2023a,b;\nTaori et al., 2023; Zeng et al., 2023) has indicated\nthat smaller LMs with about 7B parameters after su-\npervised fine-tuning such as Vicuna (Chiang et al.,\n2023) can be comparable to LLMs in following hu-\nman instructions, while still falling short of reason-\ning. In this paper, we aim to harness the untapped\nreasoning potential of smaller LMs to democratize\nthis important emergent ability.\nChain-of-Thought (CoT) prompts LMs to gen-\nerate intermediate reasoning steps (i.e., rationale)\nto reach the final answer, significantly improving\nthe complex reasoning ability (Wei et al., 2022b;\nKojima et al., 2022a; Chung et al., 2022; Wang\net al., 2023a). However, it is challenging to prompt\nsmaller LMs to generate reasoning steps, since\nsuch ability appears to be exclusive to LLMs (Wei\net al., 2022a,b; Chowdhery et al., 2022), which\nindicates the necessity of utilizing data annotated\nwith rationales to cultivate smaller LMs’ reason-\ning ability. Unfortunately, most existing reason-\ning datasets lack high-quality rationale annota-\ntions, and manual labeling them can be costly. In-\nspired by the success of collecting instruction data\nfrom LLMs (e.g., ChatGPT) for instruction tuning\n1948\nsmaller LMs (Wang et al., 2023b; Taori et al., 2023;\nTouvron et al., 2023a,b), we propose to leverage\nthe rationales generated by LLMs to train smaller\nLMs to learn to use CoT towards reasoning.\nRecently, teaching smaller LMs towards reason-\ning with the help of LLMs has gained increasing\nattention. Most of these works (Ho et al., 2023;\nMagister et al., 2023; Fu et al., 2023b; Shridhar\net al., 2023) can be summarized in two main steps:\n(1) employing LLMs to generate rationales for an-\nnotating the training data. (2) Fine-tuning smaller\nLMs on these data to enable reasoning with CoT.\nThis approach can be viewed as a distant variant of\nblack-box knowledge distillation (Jianping et al.,\n2021). However, these methods only employ LLMs\nto annotate the data for training smaller LMs, with-\nout leveraging the smaller LMs to assist LLMs in\nreturn. As a consequence, the LLMs are not aware\nof the weaknesses of the smaller LMs, thereby hin-\ndering their powerful ability to analyze and provide\ntargeted feedback, which undermines the effective-\nness of the reasoning distillation.\nTo this end, we propose a multi-round interac-\ntive learning paradigm to exploit the potential of\nblack-box LLM as a reasoning teacher. In each\nround of learning, the student (i.e., smaller LM)\nfirst provides its learning status to the teacher LLM\nwho then can provide customized rationales as the\nfeedback to the student. The data annotated with\nthese rationales serves as our customized training\ndata. Such a paradigm is natural as it is in inline\nwith how we human beings learn from teachers.\nBeyond learning from the teacher, another cru-\ncial paradigm for human learning lies in self-\nreflection on self-made mistakes. In parallel, re-\ncent studies (Huang et al., 2022; Shinn et al., 2023;\nMadaan et al., 2023; Pan et al., 2023) have also\nshown that LLMs can self-improve by reflecting\non their own mistakes. Therefore, we exploit the\nreasoning potential of smaller LM by eliciting it\nto take self-reflection on the mistakes. These mis-\ntakes can complement correct rationales collected\nfrom the teacher LLM to teach the student LM to\ndistinguish bad and good reasoning steps, thereby\nenhancing its reasoning ability.\nPutting them together, as briefly presented in\nFig. 1, we propose a tailored multi-round learning\nparadigm based on the student’s learning status\nand deficiencies, including learning from LLM’s\ncustomized training data and self-reflection. In\nsummary, our contributions are three-fold:\n1) A multi-round learning paradigm is introduced\nto enable the student LM to provide feedback to\nthe teacher LLM who then can offer customized\ntraining data in response, building the interac-\ntion between smaller LM and black-box LLM.\n2) We propose self-reflection learning that moti-\nvates the student to learn from mistakes. To-\ngether with learning from customized training\ndata, it can be seamlessly integrated into the\nmulti-round learning paradigm.\n3) Experiments and analysis on mathematical and\ncommonsense reasoning tasks demonstrate the\neffectiveness of our method in distilling the rea-\nsoning ability from LLMs to smaller LMs.\n2 Related Work\nEmergence in LLM LLMs show emergent abil-\nities in a wide range of NLP tasks (Brown et al.,\n2020; Chowdhery et al., 2022; Wei et al., 2022a,b;\nOpenAI, 2023), among which the reasoning ability\nis the most noteworthy as it requires the model to\nperform multi-hop reasoning like human beings.\nSmaller LMs (<100B) are often considered to be\nfalling significantly short in reasoning, highlighting\nthe superiority of LLMs in this aspect (Wei et al.,\n2022a). In this paper, we aim to democratize such\nemergent reasoning ability to smaller LMs.\nCoT Prompting CoT prompts LMs to solve rea-\nsoning tasks by generating intermediate rationales\nto reach the answer, which has greatly improved the\nreasoning performance (Wei et al., 2022b; Kojima\net al., 2022b; Wang et al., 2023a). However, accord-\ning to the reasoning performance curve (Wei et al.,\n2022a), the CoT reasoning performance of smaller\nLMs is far from satisfactory, since the generation\nof rationales is challenging for them. Chung et al.\n(2022) reveal that smaller LMs can partially master\nthe CoT skill by training on data with rationales.\nWe show that the CoT performance of smaller LMs\ncan be further improved via tailored learning from\nLLM’s customized training data and self-reflection.\nDistilling Knowledge from LLM Fine-tuning\nsmaller LMs to follow instructions with high-\nquality data collected from LLMs shows the fea-\nsibility of distilling knowledge from LLMs (Taori\net al., 2023; Chiang et al., 2023; Xu et al., 2023).\nThis procedure can also be viewed as a distant vari-\nant of black-box distillation (Hinton et al., 2015;\nJianping et al., 2021). However, these works aim to\nimprove the instruction-following ability of smaller\n1949\nLMs, while the reasoning ability that we focus\non is often overlooked. Some recent studies (Ho\net al., 2023; Fu et al., 2023b; Shridhar et al., 2023)\npropose to employ LLMs to annotate rationales\nfor training smaller student LMs towards reason-\ning, not considering the student’s feedback to the\nteacher. In contrast, we exploit the potential of\nthe black-box LLM as the teacher instead of the\ndata annotator by proposing a multi-round learn-\ning paradigm. This paradigm enables the mutual\nfeedback between the LLM and smaller LM, thus\ncan make the teacher LLM offer training data tai-\nlored for the student LM’s learning status. Besides,\nwe propose self-reflection learning to motivate the\nstudent LM to learn from mistakes.\n3 Method\nAs shown in Fig. 2, we propose a multi-round learn-\ning paradigm that motivates the student LM and the\nteacher LLM to learn feedback from each other in\nan interactive manner. Specifically, each round of\nlearning consists of three key steps: (1) The student\nLM undergoes an “exam” on the training set for\ncollecting mistakes which are the wrong generated\nrationales. Existing works (Fu et al., 2023b; Ho\net al., 2023; Shridhar et al., 2023; Magister et al.,\n2023) merely provide the sample question for the\nLLM to collect annotated rationales, neglecting the\nimportance of the student’s feedback. However,\nthe student’s feedback is crucial in knowledge dis-\ntillation (Fu et al., 2021; Pham et al., 2021; Ren\net al., 2023). (2) Therefore, we propose to curate\na prompt integrated with the student’s wrong ratio-\nnale to ask the teacher LLM to generate customized\nfeedback for the student. (3) In the last step, the\nstudent learns to reason via training on the tailored\ntraining data collected from the LLM, and self-\nreflection on its self-made mistakes. These steps\nare iterated to improve the reasoning ability of the\nstudent LM until convergence.\n3.1 Undertaking an Exam\nGiven a dataset Dtrain = {(x,y)}, where xis the\nquestion and yis the answer, the correct rationale\nris often not provided. During inference of CoT,\nthe input is the question x, and the student LM’s\ngenerated output f(x) = [ˆr,ˆy] is the concatenation\nof the generated rationale ˆr and answer ˆy. The\nanswer is often at the end of the output.\nThe student LM undertakes an “exam” on the\ntraining set Dtrain for evaluating the learning sta-\ntus, and collecting the mistakes Dneg which are the\nsamples with wrong rationales and answers1:\nDneg = {(x,ˆr,ˆy) |ˆy̸= y,(x,y) ∈Dtrain}, (1)\nfor each question, we collect up to 4 wrong ratio-\nnales through the decoding with sampling strategy.\nThe collected mistake set Dneg reflecting the stu-\ndent’s learning status and weakness are used for\nthe following two purposes:\n(1) As the feedback for the teacher LLM to gener-\nate rationales tailored for the student.\n(2) As the negative contrastive samples for the stu-\ndent to learn from self-reflection.\n3.2 Student’s Feedback to LLM\nWe expect the black-box LLM to be a reasoning\nteacher instead of a data annotator. Thus, we pro-\npose to provide the student’s feedback to help the\nteacher LLM generate customized training data to\neffectively target the student’s weakness. In detail,\nwe devise a prompt template T shown in Fig. 3,\nwhich integrates both the question xand the stu-\ndent’s feedback (i.e., the wrong rationale ˆr). The\nstudent’s feedback can not only (1) assist teacher\nin identifying deficiencies in student’s reasoning,\nbut also (2) as the wrong demonstration example to\nhelp LLM increase the chance of generating correct\nrationales. Besides, to improve the LLM’s accuracy\nand reduce the costs of calling APIs, we follow Ze-\nlikman et al. (2022) by adding a hint to explicitly\ntell LLM the golden answer of the question.\nFor each sample (x,ˆr,ˆy) ∈Dneg, we request\nthe LLM with T(x,ˆr,ˆy) to generate 4 rationales,\nand only those containing correct answers are re-\ntained, since training with diverse reasoning paths\ncan boost the reasoning performance of smaller\nLMs (Ho et al., 2023; Fu et al., 2023b). The col-\nlected rationale together with its question and an-\nswer is denoted as (x,r,y ), which extends the orig-\ninal data to the customized training data Dtrain.\n3.3 Tailored Learning\nThe reasoning ability of student LM f can be\nimproved via tailored learning from both self-\nreflection and teacher’s customized training data.\nLearning from Self-Reflection We propose to\nlearn from the mistakes Dneg to simulate the self-\nreflection process of humans, which can help the\n1Following most existing works, we simply judge the qual-\nity of the generated rationale by the correctness of its answer.\n1950\nRound 3\nRound 2\nProblem: Tim has 30 less apples than Martha, \nand Harry has half as many apples as Tim. If \nMartha has 68 apples, how many apples does \nHarry have?\nWrong Rationale: Martha has 68 \napples. Harry has 68 –30 = 38 apples.\nCorrect Rationale: Martha has 68 \napples. Tim has 68 –30 = 38 apples. \nHarry has 38 / 2 = 19 apples.\nExam\nStudent’s \nFeedback to \nTeacher LLM\nLearn from \nCustomized Feedback\nLearn from\nSelf-Reflection\nFinish Round 1\nRound 1\nStudent LM\nFigure 2: Overview of the proposed multi-round learning paradigm. (1) The student LM first undertakes an “exam”\nto gather mistakes (i.e., wrong rationales) made by itself. (2) These mistakes are subsequently utilized as the\nstudent’s feedback to the teacher LLM, which in turn can generate training data (i.e., correct rationales) as the\nteacher’s customized feedback to the student. (3) Finally, the student learns to improve reasoning via self-reflection\non self-made mistakes, and assimilation of the customized training data from the teacher LLM. The trained student\nLM will initiate the next round of learning by repeating the three steps until the performance plateau is reached.\nQuestion:  ... How man apples does Harry have?\nWrong Solution:  Bob got 9 oranges…\nPlease correct the wrong solution by using \nbetter reasoning steps.\nHint: The final answer should be 19.\nBetter Reasoning:\nFigure 3: The prompt template T for asking the teacher\nLLM to generate customized rationales. The part col-\nored in golden is the integrated student feedback.\nstudent LM to identify the quality of different ra-\ntionales. The utilization can be defined in multi-\nple forms (e.g., likelihood ranking), here we adopt\na simple triplet-loss to encourage the model to\nlearn different representations for good and bad\nrationales. Specifically, the wrong reasoning path\n[x,ˆr,ˆy] ∈Dneg, and the correct reasoning path\n[x,r′,y] ∈Dtrain are utilized as the negative and\npositive contrastive samples, respectively. The hid-\nden state of the last token is used as the representa-\ntion of the whole reasoning path, which is denoted\nas h(r,y)\nx . Finally, the form of self-reflection learn-\ning is defined as follows:\nLcl = EDtrain max\n{\n0,ρ −cos(h(r,y)\nx ,h(r′,y)\nx )\n+ cos(h(r,y)\nx ,h(ˆr,ˆy)\nx )\n}\n, (2)\nwhere cos denotes the cosine similarity function,\nand ρset to 1.0 is the margin. (x,r,y ) ∈Dtrain is\nthe anchor sample whose positive and negative sam-\nples are randomly sampled from Dtrain and Dneg\nwith the same question x, respectively2.\nLearning from Customized Feedback LLM’s\ngenerated rationales are tailored to the student’s\nweakness, thanks to the previous student’s feed-\nback. These collected rationales merged into the\ntraining set Dtrain as the customized feedback for\nthe student, which is used to fine-tune the student\nLM f. In addition, we add several fixed demon-\nstrations “demo” listed in Table 15 to the prefix of\neach input sample, since recent research (Min et al.,\n2022; Zelikman et al., 2022; Fu et al., 2023b) have\nshown that training with demonstration examples\ncan improve the in-context learning ability of LMs.\nThe training objective is as follows:\nLlm = EDtrain log Pf ([demo,x,r,y]) , (3)\nwhere the square brackets represent the string con-\ncatenation. This process can directly help the stu-\ndent LM learn to generate intermediate reasoning\nsteps and master the CoT skill.\n2Recall that we collect up to 4 unique correct and wrong\nrationales for each question in Dtrain and Dneg, respectively.\n1951\nAlgorithm 1 Multi-round learning paradigm.\nRequire: the student LM f, the teacher LLM, the training\ndata Dtrain, the template T in Fig. 3\n1: Initialize f0 with pre-trained weights and set the learning\nround count r ←0\n2: repeat\n3: r ←r + 1; fr ←fr−1\n4: Infer on Dtrain with f and collects the mistakes\n(x, ˆr, ˆy) ∼Dneg by Eq. (1)\n5: if r ≤1 then\n6: Collect the rationale r for each sample of Dtrain\nfrom teacher LLM with T(x, null, y)\n7: else\n8: Collect the rationale r for each sample of Dneg\nfrom teacher LLM with T(x, ˆr, y)\n9: end if\n10: Optimize weights of fr using Eq. (4)\n11: until Converges\nJoint Learning The final optimization incorpo-\nrates the learning from both self-reflection and\nLLM’s customized feedback. The contrastive learn-\ning loss in Eq. (2) and the language modeling loss\nin Eq. (3) are combined as follows:\nL= Llm + λLcl, (4)\nwhere λ controls the impacts of self-reflection\nlearning, balancing the two learning objectives.\n3.4 Multi-round Learning\nAs depicted in Fig. 2, we adopt a multi-round learn-\ning paradigm to iteratively cultivate the reasoning\nability of the student LM. Multiple rounds of learn-\ning can assist the teacher LLM in staying updated\non the student’s learning status, and thus offer more\ncustomized training data. Based on the student’s\nlearning status, the customized training data and\nself-made mistakes are adjusted in each round and\ntailored to the student’s specific deficiencies.\nThe untrained student LM nearly has no reason-\ning ability, resulting in the noisy generations which\nare unhelpful as the feedback to the teacher LLM.\nConsequently, to prepare the data required by the\ninitial round, we directly request the teacher LLM\nto generate rationales for the entire training set ex-\ncluding the noisy feedback from the student. In the\nsubsequent rounds, we adhere to the procedures\noutlined in Sections 3.1 to 3.3: (1) the student LM\ntakes an “exam” to reveal self deficiencies and col-\nlect mistakes. (2) The teacher LLM is requested\nto generate customized training data based on the\nstudent’s feedback. (3) The student is trained via\nlearning both from self-reflection and teacher’s cus-\ntomized feedback. These steps are repeated until\nthe student’s performance reaches a plateau. The\nwhole paradigm is summarized in Algorithm 1.\n4 Experiments\n4.1 Tasks & Datasets\nMathematical Task We adopt three math word\nproblem datasets to evaluate the mathematical\nreasoning ability. GSM8k is a primary school\nlevel mathematical dataset (Cobbe et al., 2021).\nMultiArith is a multi-step arithmetic reasoning\ndataset (Roy and Roth, 2015). SV AMP is created\nby applying chosen variations over examples sam-\npled from existing datasets (Patel et al., 2021).\nCommonsense Task We use two closed-ended\nquestion answering datasets to evaluate the com-\nmonsense reasoning ability. CSQA (Talmor et al.,\n2019) is a multi-choice commonsense question an-\nswering dataset. StrategyQA dataset (Geva et al.,\n2021) which implicitly requires reasoning steps\nand strategies to answer the yes-no questions.\n4.2 Models & Baselines\nModels Following previous works (Ho et al.,\n2023; Zelikman et al., 2022; Hu et al., 2023),\nwe mainly utilize a publicly available LM GPT-\nJ (Wang and Komatsuzaki, 2021) as our student\nLM which has about 6B parameters. Considering\nthe pricing and availability, we select ChatGPT3, a\npopular black-box 175B LLM provided by OpenAI,\nas our teacher LLM.\nBaselines To demonstrate the effectiveness of\nour method, we compare with the following base-\nlines: (1) the teacher LLM and student LM (w/o\nfine-tuning), for showing the effectiveness of distill-\ning reasoning ability from the LLM. (2) Methods\nwithout the help of LLMs, including the student\nfine-tuned to directly generate answers without ra-\ntionales, and STaR (Zelikman et al., 2022) which\nself-iteratively trains the LM to generate rationales\nand answers with very few annotated data. They\nare compared to highlight the importance of high-\nquality rationales in teaching smaller LMs. (3)\nThree concurrent works which all use LLMs to\nhelp train smaller LMs to reason, including LM\nfine-tuned on CoT data (Magister et al., 2023),\nSpecializing smaller LMs for mathematical reason-\ning (Fu et al., 2023b), and the LLM-adapter (Hu\net al., 2023) which utilizes adapters for efficiently\n3https://chat.openai.com/chat. Most experiments are con-\nducted between February and April of 2023.\n1952\nMethod Distillation CoT # Params\nMathematical Reasoning Commonsense Reasoning\nGSM8K MultiArith SV AMP CSQA StrategyQA\nTeacher LLM No Yes 175B 62.2 95.5 78.0 76.0 68.6\nStudent (w/o Fine-tuning) No No 6B 2.7 9.0 20.7 34.5 47.2\nStudent (w/ Fine-tuning) No No 6B 7.2 18.0 32.3 66.7 63.9\nSTaR (Zelikman et al., 2022) No Yes 6B 10.7∗ 53.9 26.7 72.5∗ 60.0\nLLM-Adapter (Hu et al., 2023) Yes Yes 6B 10.6∗ 79.2∗ 45.0∗ - -\nSpecializing (Fu et al., 2023b) Yes Yes 11B 27.1∗ 63.0∗ 35.6∗ - -\nCoT Fine-tuned (Magister et al., 2023)Yes Yes 11B 18.4∗ - - - 63.8 ∗\nOne-Round Distillation Yes Yes 6B 15.6 81.5 47.7 68.1 63.8\n+ Multi-round Yes Yes 6B 32.0+16.4 83.1+1.6 51.3+3.6 70.2+2.1 65.5+1.7\n+ Self-Reflection Yes Yes 6B 33.1+1.1 85.4+2.3 55.0+3.7 71.3+1.1 65.9+0.4\nTable 1: Accuracy (%) on various reasoning tasks with different methods. “LLM-Adapter” refers to results of GPT-J\nusing LoRA adapter (Hu et al., 2022). “Specializing” refers to results of FlanT5-XXL (Chung et al., 2022) which\nhas about 11B parameters. “CoT Fine-tuned” refers to results of T5-11B (Raffel et al., 2020) fine-tuned on CoT\ndata from GPT-3 175B (Brown et al., 2020). ∗ denotes the results are from the original paper. Indentation means the\nmodifications are based on the up-level indentation. The best performance among small LMs are marked in bold.\ntuning on CoT data. (4) Our one-round distillation\nmethod, for demonstrating the superiority of the\nproposed multi-round learning paradigm.\n4.3 Experimental Setup\nThe student is fine-tuned with a learning rate of\n1e−6 in 10 epochs using AdamW (Loshchilov and\nHutter, 2019) in default. Without any heavy tuning,\nλin Eq. (4) is set to 0.5 to control the impact of\nself-reflection. The CoT prompt accompanied by a\nfixed 3-shot demonstration is used for most datasets\nto balance the efficiency and performance. Some\nprompts are referred to previous research (Zelik-\nman et al., 2022). And we use greedy decoding to\ngenerate the rationale and answer for evaluation.\nMore implementation details are in Appendix A.\n4.4 Main Results\nThe evaluation results are presented in Table 1.\nEffect of Distillation From the results of smaller\nLM with or without distillation, it is evident that the\nreasoning performance of smaller LM can be signif-\nicantly improved by distilling the reasoning ability\nfrom LLM. Although the student LM falls short in\nmathematical reasoning, it can achieve comparable\nperformance in commonsense reasoning with the\nteacher LLM while being 20x smaller in size.\nImportance of Rationales CoT can significantly\nimprove reasoning performance which shows the\nnecessity of high-quality rationales in teaching\nsmaller LMs. Though STaR performs well in\nCSQA which often only involves single-step rea-\nsoning, the self-generated rationales encounter dif-\nficulties when applied to other multi-step reasoning\ntasks. Conversely, nearly all distillation methods\ncan beat STaR in mathematical reasoning, which\nindicates that LLM’s generated rationales can often\nbetter guide the smaller LM to reason.\nComparison with Concurrent Works Com-\npared to concurrent distillation works (Hu et al.,\n2023; Fu et al., 2023b; Magister et al., 2023), our\nmethod consistently achieves better performance\nacross all datasets, which demonstrates the suc-\ncess of customized feedback from the black-box\nLLM. For GSM8K, in contrast to training an 11B\nmodel with 130k rationales used by Specializing,\nour method can yield better performance with a 6B\nmodel and only 54k rationales, significantly reduc-\ning the cost of model training and data collection.\nEffect of Multi-round & Self-reflection Com-\npared with our one-round distillation approach,\nmulti-round learning leads to an average improve-\nment of 5.1 in accuracy, indicating the success of\nbuilding the interaction between teacher and stu-\ndent. Besides, the self-reflection learning can fur-\nther exploit the reasoning potential of the student\nLM. Another advantage is that the self-reflection\ncan be seamlessly integrated into multi-round learn-\ning, pushing the boundaries of reasoning.\n5 Analysis\n5.1 Contribution of Student’s Feedback\nTo validate the contribution of student’s feedback\nto LLM, an ablation study is conducted by remov-\ning this feedback of the requesting prompt template\n1953\nGSM8K Example#1 Ground Truth: 196\nQuestion Olaf has 150 cars at first. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10\ntoy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars\ndoes Olaf have in total, finally?\nStudent’s Feedback Olaf has 150 toy cars. His family gives him twice as many toy cars as the uncle. The uncle gives Olaf\n10 toy cars. Dad gives Olaf 10 toy cars. Auntie gives Olaf 6 toy cars. So, Olaf has 150 + 10 + 10 + 6\n= 236 toy cars.\nTeacher’s Response Dad: 10 cars, 5 less than Mum; Mum: 10 + 5 = 15 cars; Aunt: 6 cars, 1 more than uncle; Uncle: 6 - 1\n= 5 cars; Grandpa: twice as many toy cars as the uncle, so 5 * 2 = 10 cars; Total: 150 + 15 + 10 + 6 +\n5 + 10 = 196 cars.\nTeacher’s Response\nw/o Feedback\nDad: 10 cars; Mum: 10 + 5 = 15 cars; Uncle: 6 - 1 = 5 cars; Grandpa: 2 * 5 = 10 cars; Total: 150 cars\n+ 10 cars + 15 cars + 5 cars + 10 cars = 190 cars.\nStrategyQA Example#1 Ground Truth: Yes\nQuestion Can you write a whole Haiku in a single tweet?\nStudent’s Feedback Haiku is a form of poetry that has three lines and a 5-7-5 syllable count. A tweet is 140 characters\nlong. So, the answer is No.\nTeacher’s Response A haiku can be written in a single tweet if it follows the 5-7-5 syllable pattern within 140 characters.\nFor example, \"Autumn moonlight. A worm digs silently into the chestnut. Dewdrops on a leaf.\" The\npoem is considered a Haiku even if it is shorted, so it’s possible to fit a whole Haiku in a single tweet.\nSo, the answer is Yes.\nTeacher’s Response\nw/o Feedback\nA Haiku is a traditional form of poetry that is comprised of three lines. The first line is five syllables,\nthe second line is seven syllables and the last line is five syllables. So, one cannot write a whole Haiku\nin a single tweet as it exceeds the character limit. So, the answer is No.\nTable 2: Case study of the impact of student’s feedback on the teacher LLM’s response. The incorrect reasoning\nsteps and answers are colored in red. More cases are listed in Appendix C.\nDataset # Request # Success Accuracy\nGSM8K 5701 5250 28.2\nw/o Feedback 5701 4641 26.5 −1.7\nSV AMP 168 166 51.3\nw/o Feedback 168 140 48.3 −3.0\nStrategyQA 328 317 65.5\nw/o Feedback 328 134 63.9 −1.6\nTable 3: The effect of student’s feedback to the teacher\nLLM for the 2nd round learning, based on the same 1st\nround. “w/o Feedback” indicates removing student’s\nfeedback in the prompt template shown in Fig. 3. #\nRequest and Success are the number of requests to LLM\nand response with correct rationales, respectively.\nDataset Method Distance Preference\nGSM8K Student 51.00 73.63\n+ Self-Reflection 65.08 79.11\nSQA Student 5.03 96.54\n+ Self-Reflection 24.78 98.91\nTable 4: Comparison of the student LM with and\nwithout self-reflection learning on GSM8K and SQA\ndatasets. “Distance” measures the Euclidean distance\nbetween correct and wrong reasoning paths in latent\nspace. “Preference” is the likelihood ratio of correct rea-\nsoning paths to wrong ones. Both are higher is better.\n(Fig. 3). Results in Table 3 show that student feed-\nback to LLM can first help the teacher LLM to gen-\nerate more accurate and tailored rationales (larger\n# Success), which is then beneficial to the student’s\nlearning (higher Accuracy). Note that cooperating\nwith our multi-round learning paradigm, the cumu-\nlative gains of student’s feedback can be substantial.\nFurther, we take a case study of the teacher LLM’s\ngenerated rationales in Table 2 which shows that\nthe LLM can often response improved rationales\nwhen the student’s feedback is taken into account.\nFor StrategyQA, the teacher LLM even gives a\ncounterexample to the student’s wrong answer, in-\ndicating the LLM can provide customized training\ndata based on the student’s feedback.\n5.2 Effect of Self-Reflection\nFirst, to intuitive understand the effect of self-\nreflection learning, Fig. 4 visualizes the latent space\nrepresentations of generated rationales. It shows\nthat the self-reflection could effectively cluster cor-\nrect rationales and wrong ones respectively, helping\nthe model to distinguish each other. Moreover, we\ncompare the distance and preference differences\nin Table 4 which indicates that the self-reflection\ncontributes to aligning the preference of the student\n1954\n5\n 0 5\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nw/o Self-Reflection\n10\n 5\n 0 5\nw/ Self-Reflection\nCorrect\nWrong\nFigure 4: The t-SNE visualization (van der Maaten\nand Hinton, 2008) of latent space representations of\nrationales generated on the GSM8K dataset.\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013\n/uni0000001a/uni00000018\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000014\n/uni0000001b/uni00000017\n/uni0000001b/uni0000001a/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000001b/uni00000015/uni00000011/uni00000013\n/uni0000001b/uni00000017/uni00000011/uni0000001c\n/uni0000001b/uni00000015/uni00000011/uni00000015\n/uni0000001a/uni0000001c/uni00000011/uni00000015\n/uni0000001a/uni0000001a/uni00000011/uni00000015\n/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000024/uni00000055/uni0000004c/uni00000057/uni0000004b\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013\n/uni00000019/uni00000015\n/uni00000019/uni00000016\n/uni00000019/uni00000017\n/uni00000019/uni00000018\n/uni00000019/uni00000019\n/uni00000019/uni00000016/uni00000011/uni00000019\n/uni00000019/uni00000017/uni00000011/uni0000001c\n/uni00000019/uni00000017/uni00000011/uni00000014\n/uni00000019/uni00000017/uni00000011/uni00000019\n/uni00000019/uni00000017/uni00000011/uni00000014\n/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024\nFigure 5: The effect of λin Eq. (4) on the initial round\nperformance of the student LM. λ= 0.0 indicates the\nabsence of self-reflection learning.\nLM with correct reasoning paths, while away from\nself-made wrong ones.\nFig. 5 illustrates the effect of the self-reflection\nlearning on the reasoning performance. The obser-\nvation is consistent with findings in Table 1 that\nself-reflection learning can help improve the rea-\nsoning ability when λ< 0.5. However, excessive\nemphasis on self-reflection learning (i.e., a larger\nvalue of λ) typically leads to poorer performance\nand instability, especially for the MultiArith dataset.\nWe conjecture that it has a negative impact on the\nlearning of teacher’s training data.\nTo verify the above hypothesis, we plot the loss\ncurve in Fig. 6. It shows that the excessive empha-\nsis on self-reflection learning (higher λ) can result\nin underfitting of the these training data within a\nlimited number of training steps. Consequently, the\nreasoning performance of the student LM could be\nsignificantly decreased due to not fully converged.\nIn general, a small value ofλis preferred to achieve\na balanced learning approach that incorporates both\nthe teacher’s rationales and self-made mistakes.\n/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000036/uni00000057/uni00000048/uni00000053\n/uni00000013/uni00000011/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000016\n/uni00000013/uni00000011/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000013/uni0000001c\n/uni00000013/uni00000011/uni00000014/uni00000015/uni0000002f/uni00000052/uni00000056/uni00000056\n= 0.0\n= 0.25\n= 0.5\n= 0.75\n= 1.0\nFigure 6: The training loss of Eq. (3) in the initial round\nof the student LM with different weight λon the Multi-\nArith dataset. We also observe that the loss of Eq. (2)\nwith different λcan all converge.\n5.3 Analysis of Multi-round Learning\nWe examine each learning round of the student LM,\nas detailed in Table 5. The error rate and accuracy\nare typically gradually decreased and increased\nwith the learning rounds, respectively. This is be-\ncause of each round of learning aims to enhance\nthe student LM in solving the questions that were\nnot learned well in previous round. Additionally,\ninspired by recent research on employing the LLM\nas the evaluator (Chiang and Lee, 2023; Fu et al.,\n2023a; Liu et al., 2023), we instruct GPT-4 (Ope-\nnAI, 2023) to automatically evaluate the quality of\ngenerated rationales. From the results in Table 6,\nwe find that there is an enhancement in the qual-\nity of both generated correct rationales and wrong\nones as the learning rounds progress. However, the\ngains in reasoning performance reach a plateau af-\nter several rounds of training. This can be attributed\nas follows: (1) For GSM8K, the most challenging\ntask, the student is reaching its capacity after 3\nrounds of learning, still not performing well (49.2\nER). (2) For SV AMP and CSQA, relatively easy\ntasks, the student achieves a good performance on\nthe training set after the 2 nd round, leading to a\nsmall ER. Consequently, the prepared data for the\nnext round will be relatively scarce, which is un-\nlikely to further help improve the student.\nWe conduct the 4th round learning on GSM8K\nfor justifying the above analysis, where the ER re-\nmains unsatisfactory (51.8 ER) despite a marginal\nimprovement (+1.4 ∆) in accuracy. Besides, the\nresults of the 3 rd round on SV AMP and CSQA\ndatasets show that there are no more gains after the\n2nd round. Thus, we suggest to take early stopping\n1955\nDataset Initial 1 st 2nd 3rd\nGSM8K\n# Data - 15k 16k 13k\nER 98.3 76.3 66.2 49.2\nAcc/∆ 2.7 +12.9 +12.6 +2.4\nSV AMP\n# Data - 2k 0.6k 0.3k\nER 76.0 24.0 16.7 17.6\nAcc./∆ 20.7 +27.0 +3.6 +1.0\nCSQA\n# Data - 26k 7k 3k\nER 67.8 18.9 7.6 9.2\nAcc./∆ 34.5 +31.8 +3.9 -0.6\nTable 5: Observation of the student LM in each round\nof learning. “Initial” refers to model w/o distillation.\n“#Data” represents the size of training samples. “ER”\nrefers to the error rate on train set. “Acc” denotes the\ninitial accuracy of the student LM, and “ ∆” indicates\nits performance change after each round.\nDataset Round Correct Wrong\nGSM8K\nInitial 2.59 ±0.27 1.02 ±0.07\n1st 4.50 ±0.18 1.15 ±0.20\n2nd 4.88 ±0.14 1.26 ±0.23\nSV AMP\nInitial 4.53 ±0.20 1.07 ±0.18\n1st 4.86 ±0.16 1.09 ±0.21\n2nd 4.90 ±0.24 1.11 ±0.20\nCSQA\nInitial 4.44 ±0.22 1.24 ±0.28\n1st 4.84 ±0.27 1.41 ±0.28\n2nd 4.96 ±0.12 1.55 ±0.33\nTable 6: Results of GPT-4 score for student LM’s gen-\nerated rationales in each round of learning. The score\nis given based on accuracy and quality of the reasoning\npath. “Correct” and “Wrong” stand for the rationales\nwith correct answers and wrong answers, respectively.\nin the multi-round learning if the student can nearly\nreach its plateau. By prior estimation of the task\ndifficulty and observing performance gains in each\nround, we can avoid excessive parameter tuning\non the number of learning rounds and balance the\nreasoning performance and training costs.\n5.4 Feasibility Study\nTo further benefit the community concerning about\nindividual affordable computation resources, we\nconduct a feasibility study by using different LMs\nspanning from 760M to 2.7B parameters. The\ntested models include two common LM architec-\ntures, i.e., encoder-decoder and decoder-only. The\nresults shown in Table 7 first suggest that the rea-\nsoning abilities of these small LMs can all be en-\nMethod 760M 770M 1.3B 2.7B\nSV AMP\nStudent 0.0 2.7 5.3 3.7\n+ Distillation 11.0 13.3 31.7 34.3\n+ Self-Reflection14.7+3.7 15.3+2.0 32.0+0.3 36.3+2.0\n+ Multi-round15.3+0.6 17.0+1.6 35.0+3.0 36.0−0.3\nSQA\nStudent 0.0 39.6 51.2 38.9\n+ Distillation 62.0 62.2 62.0 62.2\n+ Self-Reflection64.0+2.0 64.2+2.0 64.8+2.8 65.2+3.0\n+ Multi-round64.8+0.8 62.4−1.8 65.8+1.0 63.8−1.4\nTable 7: Results of our method with various LM\nsizes. “760M”, “770M”, “1.3B” and “2.7B” refer to\nT5-Large (Raffel et al., 2020), GPT-2 Large (Radford\net al., 2019), OPT-IML (Iyer et al., 2023) and GPT-\nNeo (Gao et al., 2020; Black et al., 2021), respectively.\nThe indentation means the modifications are based on\nthe up-level indentation.\nhanced with the proposed self-reflection learning.\nWith self-reflection, student LMs often achieve sat-\nisfying performance with just one round of learning\nfor commonsense tasks. Moreover, we find that our\nmulti-round learning can generally further improve\nthe performance in mathematical reasoning. How-\never, there are no more gains for StrategyQA, as\nit heavily relies on the memorization of common-\nsense knowledge mostly acquired from the pre-\ntraining stage, rather than on complex reasoning.\nAnother evidence is that increasing the model size\nseems not to have contribution to the performance\non this dataset. Besides, the relatively limited ca-\npacity of these smaller LMs may also restrict the\ngains from additional rounds of learning.\n6 Conclusion\nIn this paper, we propose a tailored learning ap-\nproach to cultivate the reasoning ability of the\nsmaller LM, aiming to democratize the emergent\nreasoning ability of the LLM. First, we propose\na multi-round interactive learning paradigm that\nenables the teacher LLM to provide customized\ntraining data according to the student’s feedback.\nNext, we propose the self-reflection learning to\nmotivate the student to distinguish correct ratio-\nnales from wrong ones. Further, the integration\nof learning from LLM’s customized feedback and\nself-reflection can complement each other within\nthe proposed multi-round learning paradigm. The\nempirical results from mathematical and common-\nsense reasoning tasks demonstrate the success of\nunleashing the reasoning potential of smaller LMs.\nWe believe that these findings can benefit the open-\nsource and NLP communities in the era of LLM.\n1956\nLimitations\nIn this section, we discuss the limitations of our\nmethod with integrity while offering potentially\nuseful advice for future research.\n1) Our experiments primarily utilize ChatGPT and\nGPT-J (Wang and Komatsuzaki, 2021) as the\nteacher LLM and student LM, respectively, due\nto the considerations of availability and costs.\nAlthough fine-tuning GPT-J on the outputs of\nChatGPT boosts their reasoning performance, a\nsubstantial gap still remains between them. It\nis valuable to validate our findings using more\npowerful LMs (e.g., LLaMA (Touvron et al.,\n2023a,b)). And training better foundation LMs\nshould be the primary task for the open-source\ncommunity, since imitating proprietary LLMs\nmay be a false promise (Gudibande et al., 2023).\n2) We have demonstrated the importance of stu-\ndent’s feedback in distilling the knowledge from\nthe black-box LLM, but without extensive engi-\nneering the feedback prompt templates (e.g., ex-\nplicitly instructing the LLM to act as a teacher).\nAnd the interactions (e.g., use reinforcement\nlearning to connect LLM and smaller LM) can\nbe explored in the future.\n3) Our self-reflection learning currently is defined\nin a straightforward triplet-loss form. However,\nthe core of self-reflection is learning from mis-\ntakes. Thus, the training objectives or forms can\nbe defined in various ways, such as ranking loss\nor verbal critic are expected to further help the\nsmaller LMs to reflect and learn from mistakes.\n4) Evaluating the correctness of generated ra-\ntionale is mainly based on the final answer.\nThough most existing works (Zelikman et al.,\n2022; Ho et al., 2023; Fu et al., 2023b; Shridhar\net al., 2023) in this field adopt this simple crite-\nrion, we call attention to develop more trustwor-\nthy criteria to evaluate the quality of rationales.\nPotential methods can be using GPT-4 (OpenAI,\n2023) or a process reward model (Lightman\net al., 2023) for automatic evaluation.\nEthics Statement\nRisk in using closed-source LLMs Though the\ndatasets used for evaluation is publicly available,\nthe annotated rationales in this paper are collected\nfrom close-source ChatGPT provided by OpenAI.\nOpen-source LLMs (e.g., LLaMA) have boomed\nin recent months, it is noteworthy that many of\nthem use the outputs from closed-source LLMs\n(e.g., Alpaca and Vicuna are trained on ChatGPT’s\noutputs) for further improvements. According to\nthe Sec. 2 \"Usage Requirements\", within OpenAI’s\nterms of use4, there exists a prohibition against \"use\noutput from the Services to develop models that\ncompete with OpenAI\". However, beyond its terms\nof use, the crucial matter lies in determining \"own-\nership of the copyright pertaining to the outputs\nof generative AI\". As of today, there remains an\nambiguity regarding the copyright status of genera-\ntive AI outputs, both in scholarly circles and legal\ncontexts. Compelling evidence indicates that these\nclosed-source LLMs undergo training using numer-\nous copyrighted materials, such as books, academic\npublishings, etc. Thus, we think at least the authors\nof the training data that directly supports LLM’s\noutputs hold the copyright, as opposed to the LLM\nservice provider. The prompt creators may also\nhold the copyright if their prompts substantially\ninfluence LLM’s outputs. For open-source and\nresearch communities, we call for a responsible\ndiscussion about data collection.\nSocial Impact This paper explores how to uti-\nlize the LLM as a teacher to enhance the reasoning\nperformance of smaller LMs, which can help de-\nmocratize these emergent abilities for the benefit of\nbroader communities (e.g., math education). Fur-\nthermore, we firmly believe that the utilization of\nLLMs can be a significant area of interest in natural\nlanguage processing applications and research.\nAcknowledgements\nWe thank the anonymous reviewers for their in-\nsightful and valuable comments. This work is\nsupported by the National Natural Science Foun-\ndation of China (62072483), and the Guangdong\nBasic and Applied Basic Research Foundation\n(2022A1515011690, 2021A1515012298).\nReferences\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n4https://openai.com/policies/terms-of-use\n1957\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCan-\ndlish, Ilya Sutskever, and Wojciech Zaremba. 2021.\nEvaluating large language models trained on code.\nabs/2107.03374.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607–15631, Toronto,\nCanada. Association for Computational Linguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An Open-\nSource Chatbot Impressing GPT-4 with 90%* Chat-\nGPT Quality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2022. Palm: Scaling language\nmodeling with pathways. arXiv, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv, abs/2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv, abs/2110.14168.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023a. Gptscore: Evaluate as you desire. arXiv,\nabs/2302.04166.\nShipeng Fu, Zhen Li, Zitao Liu, and Xiaomin Yang.\n2021. Interactive knowledge distillation for image\nclassification. Neurocomputing, 449:411–421.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023b. Specializing smaller language\nmodels towards multi-step reasoning. In Proceedings\nof the 40th International Conference on Machine\nLearning, volume 202 of Proceedings of Machine\nLearning Research, pages 10421–10430. PMLR.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv, abs/2101.00027.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346–\n361.\nArnav Gudibande, Eric Wallace, Charles Burton Snell,\nXinyang Geng, Hao Liu, P. Abbeel, S. Levine, and\nDawn Song. 2023. The False Promise of Imitating\nProprietary LLMs. arXiv, abs/2305.15717.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv,\nabs/1503.02531.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2023.\nLarge language models are reasoning teachers. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 14852–14882, Toronto, Canada.\nAssociation for Computational Linguistics.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nZhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-\nPeng Lim, Roy Ka-Wei Lee, Lidong Bing, and Sou-\njanya Poria. 2023. Llm-adapters: An adapter family\nfor parameter-efficient fine-tuning of large language\nmodels. arXiv, abs/2304.01933.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv,\nabs/2210.11610.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2023. Opt-iml: Scaling language\nmodel instruction meta learning through the lens of\ngeneralization. arXiv, abs/2212.12017.\nGou Jianping, Yu Baosheng, Stephen J Maybank, and\nTao Dacheng. 2021. Knowledge distillation: A\nsurvey. International Journal of Computer Vision,\n129(6):1789–1819.\n1958\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022a. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022b. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nHunter Lightman, V . Kosaraju, Yura Burda, Harrison\nEdwards, Bowen Baker, Teddy Lee, J. Leike, J. Schul-\nman, Ilya Sutskever, and Karl Cobbe. 2023. Let’s\nVerify Step by Step. arXiv, abs/2305.20050.\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan\nZhang, Haizhen Huang, Furu Wei, Weiwei Deng,\nFeng Sun, and Qi Zhang. 2023. Calibrating llm-\nbased evaluator. arXiv, abs/2309.13308.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv, abs/2303.17651.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2023.\nTeaching small language models to reason. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 1773–1781, Toronto, Canada. Associ-\nation for Computational Linguistics.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report. arXiv,\nabs/2303.08774.\nLiangming Pan, Michael Saxon, Wenda Xu, Deepak\nNathani, Xinyi Wang, and William Yang Wang. 2023.\nAutomatically correcting large language models: Sur-\nveying the landscape of diverse self-correction strate-\ngies. arXiv, abs/2308.03188.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094, Online.\nAssociation for Computational Linguistics.\nHieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le.\n2021. Meta pseudo labels. In Proceedings of the\nIEEE/CVF conference on computer vision and pat-\ntern recognition, pages 11557–11568.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20:\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1–\n16.\nYuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun\nYuan, and Mu Li. 2023. Tailoring instructions to stu-\ndent’s learning levels boosts knowledge distillation.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1990–2006, Toronto, Canada.\nAssociation for Computational Linguistics.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1743–1752, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning. arXiv, abs/2303.11366.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2023. Distilling reasoning capabilities into\nsmaller language models. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2023,\npages 7059–7073, Toronto, Canada. Association for\nComputational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\n1959\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. arXiv,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. arXiv, abs/2307.09288.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(86):2579–2605.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A\n6 Billion Parameter Autoregressive Language Model.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models. In\nInternational Conference on Learning Representa-\ntions.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508, Toronto, Canada. Association\nfor Computational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research. Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022b. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing. arXiv, abs/1910.03771.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv,\nabs/2304.01196.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. Star: Bootstrapping reasoning with rea-\nsoning. In Advances in Neural Information Process-\ning Systems, volume 35, pages 15476–15488.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130b: An open bilingual pre-trained model. In\nInternational Conference on Learning Representa-\ntions.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12697–12706.\nPMLR.\n1960\nA Implementation Details\nThe codes will be made publicly available after\nanonymous reviewing period.\nA.1 Data Preparation\nThe dataset statistics are shown in Table 8. Fol-\nlowing Ho et al. (2023), the data of SV AMP (Patel\net al., 2021), MultiArith (Roy and Roth, 2015) and\nStrategyQA (Geva et al., 2021) is split with a ratio\nof 70 : 30 for the training and evaluation, while\nGSM8K (Cobbe et al., 2021) and CSQA (Talmor\net al., 2019) datasets follow the original split. In\nmistakes collection, we use sampling decoding to\nprompt student LM to generate4 rationales for each\nsample, and only the wrong ones are collected. In\nrationales collection, the teacher LLM is requested\nto generate 4 diverse rationales for each question,\nand only the correct ones are collected. An exam-\nple of Fig. 3 for using student’s feedback to request\nthe LLM is shown in Table 12. The decoding gen-\neration configs are listed in Table 9.\nA.2 Training & Evaluation\nHyperparameter Experiments are performed\nwith the help of Transformers5 (Wolf et al., 2020)\nand Deepspeed6 (Rajbhandari et al., 2020) libraries.\nWe use 8 Tesla V100 GPUs with FP16 for training\nand evaluation. The adopted training hyperparame-\nter settings across all datasets are shown in Table 10.\nThe student LM is trained with a1e−6 learning rate\nfor the initial round learning, and 7e−7 for the fol-\nlowing rounds, to make the training more stable.\nAnd we set a random seed 42 for all experiments\nto ensure reproducibility.\nDemonstration Following Min et al. (2022); Ze-\nlikman et al. (2022); Fu et al. (2023b), we use sev-\neral fixed demonstrations selected from the train-\ning set as the prefix of each sample to improve\nthe in-context learning performance. Considering\nthe memory consumption and efficiency, we use 3-\nshot demonstrations for GSM8K, MultiArith, and\nSV AMP datasets. For CSQA and StrategyQA, we\nrespectively use 5-shot and 4-shot demonstrations\nto reduce the label bias (Zhao et al., 2021) since\nthey are essentially 5 (“a, b, c, d, e”) and 2 (“yes,\nno”) labels classification tasks. These demonstra-\ntions are listed in Table 15.\n5https://github.com/huggingface/transformers\n6https://github.com/microsoft/DeepSpeed\nDataset Type # Train # Test Split\nGSM8K Mathmatical 7473 1319 Original\nMultiArith Mathmatical 420 180 70:30\nSV AMP Mathmatical 700 300 70:30\nCSQA Commonsense 9741 1221 Original\nStrategyQA Commonsense 1603 687 70:30\nTable 8: Dataset statistics.\nArguments Mistakes LLM\nTemperature 1.0 1.0\nTop-p - 0.9\nTop-k 50 -\nMax Generation Len. 128 128\n# Return Sequences 4 4\nTable 9: Generation configs for collecting student’s self-\nmade mistakes and rationales from teacher LLM.\nHyperparameter Value\nEpoch 10\nBatch Size 16\nLearning Rate {1e−6,7e−7}\nβof AdamW (0.9, 0.999)\nϵof AdamW 1e−8\nWeight Decay 0.01\nWarmup Steps 100\nTable 10: Training hyperparameter settings.\nIn addition, from pilot experiments, we empir-\nically find that assigning less weights (0.1) to the\nfixed demonstration examples than the input sam-\nple helps the model focus on the input sample and\nyield better performance, which can be investigated\nin the future.\nEvaluation We use a simple-yet-effective CoT\nprompt template as follows:\nQuestion: x\\n Reasoning: r\\n Answer: y (5)\nwhere \\n is the line break symbol, xis the question,\nrand yare expected reasoning steps and answer, re-\nspectively. The greedy decoding is adopted for the\ngeneration of the student LM though beam search\nmay further improve the performance. The answer\nextraction of evaluation is simply using the first\nvalid token after the “Answer:”, which can avoid\ncomplex post-processing.\n1961\nB Generalization Results\nGeneralization experiments are conducted to evalu-\nate the generalization of the student LM, as shown\nin Table 11. The results reveal the following in-\nsights: (1) the in-domain generalization perfor-\nmance is enhanced after the reasoning distillation,\nwhile the out-of-domain (OOD) performance is\nusually slightly decreased. This finding is consis-\ntent with Fu et al. (2023b) although our method\nis better than theirs in terms of OOD performance.\n(2) The in-domain performance can be further im-\nproved by employing our multi-round learning\nparadigm. And we surprisingly find that, for some\ncases, the OOD performance can also be improved\nvia multi-round learning. This can be attributed to\nthat the customized training data of the following\nrounds may assists the model in generalizing its\nreasoning abilities to other domains. (3) The stu-\ndent LM trained on the GSM8K dataset exhibits the\nmost significant improvements in in-domain rea-\nsoning performance. Note that the GSM8K dataset\nis the most challenging one among these mathe-\nmatical datasets. Consequently, it is reasonable to\nexpect gains on the other datasets if the student can\nalready tackle the difficult problems.\nC Case Study\nContribution of Student’s Feedback Additional\nexamples of the LLM’s generated rationales are\npresented in Table 13. We observe that the teacher\nLLM, ChatGPT, is capable of generating more\ndetailed and precise reasoning steps when pro-\nvided with student’s feedback (i.e., wrong solution).\nThese detailed reasoning steps can help address the\nstudent’s deficiencies and thereby improve the rea-\nsoning performance in the subsequent round of\nlearning. Although both rationales, with and with-\nout feedback, are correct, their quality can vary.\nMore precise and customized rationales can help\nthe student better understand its own mistakes, es-\npecially coupled with our self-reflection learning,\nwhich is beneficial for student’s reasoning learning.\nMulti-round Learning To better understand the\nimpact of each learning round, we conduct a case\nstudy in Table 14. First, it is clear that the student\nLM initialized with pre-trained weights (i.e., the\n0th round) is powerless to generate meaningful an-\nswers for the mathematical reasoning task, which\nmay confuse the teacher LLM. Thus, we tend not to\nutilize these noisy feedback for preparing the train-\ning data of the initial round. Second, the LLM’s\ngenerated response is often tailored to student’s\ncurrent deficiencies, thus effectively improving stu-\ndent’s reasoning performance in the next round of\nlearning. Third, a single round of distillation may\nnot enable the student to solve challenging ques-\ntions. However, with the help of our multi-round\nlearning paradigm, the student can have the oppor-\ntunity to tackle such challenging questions.\n1962\nTrain on\nEvaluation on\nGSM8K MultiArith SV AMP CSQA StrategyQA\nNone 2.7 9.0 20.7 34.5 47.2\nGSM8K\n1st 15.6 46.6 25.3 28.4 38.3\nLast 32.0 80.3 42.3 30.0 38.3\nMultiArith\n1st 4.7 81.5 14.7 32.3 52.5\nLast 5.0 83.1 19.3 31.4 52.1\nSV AMP\n1st 4.0 12.4 47.7 29.6 45.4\nLast 5.4 14.6 51.3 34.0 44.7\nCSQA\n1st 2.6 5.1 12.3 68.1 48.0\nLast 2.3 5.1 14.3 70.2 51.1\nStrategyQA\n1st 3.8 9.0 19.0 33.3 63.8\nLast 9.6 9.6 17.3 33.5 65.5\nTable 11: Generalization results of our multi-round learning paradigm on in-domain and out-of-domain reasoning\ntasks. “None” refers to the student model initialized with pre-trained weights, without any fine-tuning. “1 st” refers\nto the student LM taken the first round of learning. Similarly, “Last” refers to the last round of learning. Results\ncolored in green indicate the in-domain reasoning, while blue indicate the out-of-domain reasoning.\nQuestion: Is Europa linked to Viennese waltzes?\nWrong Solution: Europa is a moon of Jupiter, and it is named after a Phoenician queen of Crete. Viennese waltzes are a type\nof dance that originated in Austria. So, the answer is No.\nPlease correct the wrong solution by using better reasoning steps.\nHint: The final answer should be yes.\nBetter Reasoning: While Europa is a moon of Jupiter and named after a Phoenician queen of Crete, it is also the title of a\nfamous waltz composed by Austrian composer Johann Strauss II in 1889. The waltz is named after the mythical princess\nEuropa from Greek mythology, who was abducted by Zeus and taken to Crete. So, the answer is Yes.\nQuestion: Would a honey badger fit inside an oven?\nWrong Solution: Honey badgers are large animals that can grow up to be about the size of a small dog. An oven is a small,\nenclosed space. So, the answer is No.\nPlease correct the wrong solution by using better reasoning steps.\nHint: The final answer should be yes.\nBetter Reasoning: The answer is actually Yes, a honey badger could fit inside an oven, but the size of a honey badger can\nvary and an oven can also vary in size. So, the answer is yes.\nQuestion: Would a pacifist be opposed to hunting?\nWrong Solution: Pacifists believe in non-violence and do not support the use of violence. Hunting is a form of violence. So,\nthe answer is Yes.\nPlease correct the wrong solution by using better reasoning steps.\nHint: The final answer should be no.\nBetter Reasoning: While pacifists do believe in non-violence, their opposition is generally focused on human-on-human\nviolence. Some pacifists may be opposed to hunting due to their belief in non-violence towards animals, but it is not\nnecessarily a universal position among pacifists. So, the answer is No.\nQuestion: { question x }\nWrong Solution: { mistake ˆr }\nPlease correct the wrong solution by using better reasoning steps.\nHint: The final answer should be { answer y }.\nBetter Reasoning: { Expected LLM’s customized feedback }\nTable 12: An example of the prompt template introduced in Fig. 3 for asking the LLM to generate customized\nrationales, for a given question x, student’s mistake ˆr, and the ground truth answer yfrom the StrategyQA dataset.\n1963\nGSM8K Example#2 Ground Truth: 12\nQuestion Janet bought some muffins at the bakery. Each muffin is 75 cents. Janet paid 20 and got 11 in change\nback. How many muffins did Janet buy?\nStudent’s Feedback Janet bought some muffins at the bakery. Each muffin is 75 cents. Janet paid 20 and got 11 in change\nback. We know that:1) Janet bought some muffins at the bakery.2) Each muffin is 75 cents.3) Janet\npaid 20 and got 11 in change back.From these three pieces of information, we can infer that: Janet\nbought 9 muffins.\nTeacher’s Response We know that Janet paid 20 and got 11 in change back. To find how much money Janet paid to buy\nmuffins, we can subtract 11 from 20 which results in 9. If 1 muffin costs 75 cents and Janet paid 9\nthen she must have bought 12 muffins ( 9 divided by 75 cents per muffin = 12 muffins).\nTeacher’s Response\nw/o Feedback\nThe change Janet got back is 11. This means Janet spent 20 - 11 = 9 on muffins. If each muffin costs\n75 cents, then Janet bought 9 / 0.75 = 12 muffins.\nSV AMP Example#1 Ground Truth: 5\nQuestion A waiter had 11 customers. After some left he still had 3 customers. How many more customers left\nthan those that stayed behind?\nStudent’s Feedback To find the number of customers who left, we need to subtract the number of customers who stayed\nbehind from the number of customers who left. The number of customers who left is 11 - 3 = 8.\nTeacher’s Response Let X be the number of customers who left the restaurant. We know that 11 - X = 3, as 3 customers\nstayed behind. We can solve for X by adding X to both sides of the equation, resulting in X = 11 - 3 =\n8. Therefore, the number of customers who left is 8 and the number who stayed is 3. 8 - 3 = 5 is the\nnumber of customers who left than those that stayed behind.\nTeacher’s Response\nw/o Feedback\nTo find the number of customers who left, we must subtract the number of customers who stayed\nfrom the total number of customers the waiter had: 11 - 3 = 8. Therefore, 8 more customers left than\nthose that stayed behind.\nSV AMP Example#2 Ground Truth: 9\nQuestion 5 children were riding on the bus. At the bus stop 63 children got off the bus while some more got on\nthe bus. Then there were 14 children altogether on the bus. How many more children got on the bus\nthan those that got off?\nStudent’s Feedback To find the number of children that got on the bus, we need to subtract the number of children that got\noff the bus from the total number of children on the bus. Therefore, the number of children that got on\nthe bus is 14 - 63 = -49.\nTeacher’s Response Let X be the number of children who got on the bus. Initially, there were 5 children on the bus and 63\nchildren got off. Therefore, after getting off the bus, the number of children on the bus is 5 - 63 =\n-58. Adding the children that got on the bus, we get -58 + X = 14. Therefore, X = 72. And we can\ncalculate the difference between children that got on the bus and children that got off. 72 - 63 = 9.\nTherefore, 9 more children got on the bus than those that got off.\nTeacher’s Response\nw/o Feedback\nFirst, we know that 5 children were riding on the bus before the bus stop, then 63 children got off the\nbus. So, there were 5 - 63 = -58 children on the bus. Then, 14 children were on the bus, so we know\nthat 14 - (-58) = 72 children got on the bus. To find the number of children who got on the bus than\nthose who got off, we find the difference between the number of children who got on and the number\nwho got off which is 72 - 63 = 9 children.\nStrategyQA Example#2 Ground Truth: Yes\nQuestion Do mail carriers need multiple uniforms?\nStudent’s Feedback Mail carriers work for the United States Postal Service (USPS) and they have one uniform. So, the\nanswer is No.\nTeacher’s Response Mail carriers typically have multiple uniforms, with some designed for indoor use, some for outdoor\nuse, and others for different weather conditions. Some mail carriers may also have different clothing\nitems for specialized tasks such as carrying mail bags. The answer is Yes.\nTeacher’s Response\nw/o Feedback\nMail carriers work outdoors in various weather conditions. They need to wear different clothes to be\nprotected from different weather conditions. So, the answer is Yes.\nTable 13: Case study of the impact of student’s feedback on the teacher LLM’s response. The incorrect reasoning\nsteps and answers are colored in red. With the student’s feedback, the teacher can often generate more detailed and\nprecise reasoning steps.\n1964\nGSM8K Example#1\nQuestion: The number of short students in a class is 2/5 of the total number of students. If there are 90 tall students,\nand the class has 400 students, calculate the total number of students with average height. Ground Truth: 150\nRound Student’s Generation Correctness: /enc-37\n0th The number of short students is 2/5 of the total number of students.The number of tall students is 2/5 of the total\nnumber of students. The number of students with average height is 2/5 of the total number of students.\nTeacher’s Feedback\n1st The number of short students in the class is 2/5 of the total number of students, which means there are (2/5) * 400\n= 160 short students. If there are 90 tall students, then the number of students with average height can be calculated\nby subtracting the number of short and tall students from the total number of students. The total number of students\nwith average height is 400 - 160 - 90 = 150 students.\nStudent’s Generation Correctness: /enc-37\n1st There are 90 tall students in the class. There are 2/5 of 400 students, which is 2/5 * 90 = 36 short students. So the\ntotal number of students in the class is 90 + 36 = 126.\nTeacher’s Feedback\n2nd There are 90 tall students in the class. Short students are 2/5 of the total number of students. The total number of\nstudents in the class is 400. Short students: 2/5 * 400 = 160. We know the total number of students in the class\nis 400, so 90 + 160 = 250 is the number of tall and short students. Therefore, the total number of students with\naverage height is 400 - 250 = 150.\nStudent’s Generation Correctness: /enc-37\n2nd The number of short students in a class is 2/5 of the total number of students. So, 2/5 * 400 = 160. The number of\ntall students in a class is 90. So, the total number of students with average height is 400 - 160 - 90 = 170.\nTeacher’s Feedback\n3rd The total number of students in the class is 400. The number of short students is 2/5 of the total number of students,\n2/5 * 400 = 160. The number of tall students is 90. The total number of short and tall students is 160 + 90 = 250.\nSo the total number of students with average height is 400 - 250 = 150.\nStudent’s Generation Correctness: /enc-34\n4th The number of short students is 2/5 of the total number of students, which is 2/5 * 400 = 160. The total number of\nstudents with short height is 160 + 90 = 250. The total number of students with average height is 400 - 250 = 150.\nCSQA Example#1\nQuestion: Where do students keep rubber bands?\n(a) trees (b) drug store (c) trash (d) beside table (e) classroom Ground Truth: (e)\nRound Student’s Generation Correctness: /enc-37\n0th The answer should be a place where students would keep rubber bands. Bedside tables are often where students\nkeep their rubber bands. Therefore, the answer is bedside table (d).\nTeacher’s Feedback\n1st The answer should be a place where students might keep rubber bands. Students might keep rubber bands in their\ndesks in the classroom. Therefore, the answer is classroom (e).\nStudent’s Generation Correctness: /enc-34\n1st The answer should be a place where students would keep rubber bands. Rubber bands are often used for school\nprojects, so students would keep them in their classroom. Therefore, the answer is classroom (e).\nTable 14: Case study of the student LM’s generation and teacher LLM’s feedback by multi-round learning. The\nteacher can generate more tailored training data thanks to being aware of the student’s deficiencies. And the student\ncan gradually improve its reasoning ability with the help of these customized feedback.\n1965\n3-shot demonstrations of GSM8K dataset\nQuestion: Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16\nslices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day? \\n Reasoning: A large pizza has 16 slices, so 2\nlarge pizzas have 32 slices. A small pizza has 8 slices, so 2 small pizzas have 16 slices. If we add 32 slices and 16 slices, we get 48 slices.\nTherefore, he will eat 48 slices of pizza in that day. \\n Answer: 48\nQuestion: Mary does her grocery shopping on Saturday. She does her shopping only at a specific store where she is allowed a credit of 100,\nwhich must be paid in full before her next shopping trip. That week she spent the full credit limit and paid 15 of it on Tuesday and 23 of it on\nThursday. How much credit will Mary need to pay before her next shopping trip? \\n Reasoning: Mary spends her entire credit limit of\n100 on Saturday. On Tuesday, she pays 15 towards her debt. On Thursday, she pays 23 towards her debt. This leaves her with a remaining\nbalance of 100 - 15 - 23, which is equal to 62. \\n Answer: 62\nQuestion: Ralph is going to practice playing tennis with a tennis ball machine that shoots out tennis balls for Ralph to hit. He loads up the\nmachine with 175 tennis balls to start with. Out of the first 100 balls, he manages to hit 2/5 of them. Of the next 75 tennis balls, he manages\nto hit 1/3 of them. Out of all the tennis balls, how many did Ralph not hit? \\n Reasoning: Ralph hits 2/5 of the first 100 balls, so he hits 40\nballs. Then, Ralph hits 1/3 of the next 75 balls, so he hits 25 more balls. In total, Ralph hits 40 + 25 = 65 balls. Finally, we know that Ralph\nstarted with 175 balls, so 175 - 65 = 110 balls not hitted. \\n Answer: 110\n3-shot demonstrations of MultiArith dataset\nQuestion: There are 64 students trying out for the school’s trivia teams. If 36 of them didn’t get picked for the team and the rest were put\ninto 4 groups, how many students would be in each group? \\n Reasoning: The number of students who got picked for the team is 64 - 36 =\n28. To find how many students would be in each group, we need to divide the number of students by the number of groups, which is 28 / 4 =\n7. \\n Answer: 7\nQuestion: Cody bought 7 boxes of chocolate candy and 3 boxes of caramel candy. If each box has 8 pieces inside it, how much candy did he\nhave total? \\n Reasoning: First, we need to find the total number of boxes Cody bought, which is 7 + 3 = 10 boxes. Then, we can multiply\nthe number of boxes by the number of pieces of candy in each box to find the total amount of candy. Therefore, Cody had 10 x 8 = 80 pieces\nof candy in total. \\n Answer: 80\nQuestion: For Halloween Robin scored 23 pieces of candy. She ate 7 pieces the first night and then her sister gave her 21 more pieces. How\nmany pieces of candy does Robin have now? \\n Reasoning: We need to add the number of pieces of candy she had after the first night to the\nnumber of pieces her sister gave her. Therefore, the total number of pieces of candy Robin has now is 23 - 7 + 21 = 37. \\n Answer: 37\n3-shot demonstrations of SV AMP dataset\nQuestion: Paul had 50 books. After buying some in a garage sale he had 151 left. How many books did he buy? \\n Reasoning: The number\nof books Paul bought can be found by subtracting the final number of books from the initial number of books: 151 - 50 = 101. Therefore,\nPaul bought 101 books in the garage sale. \\n Answer: 101\nQuestion: Luke played a trivia game and scored 154 points. If he gained the 11 points in each round. How many rounds did he play? \\n\nReasoning: We need to divide Luke’s total score by the number of points he gained in each round. Therefore, the number of rounds Luke\nplayed is 154 / 11 = 14. \\n Answer: 14\nQuestion: Julia played tag with 17 kids on monday, 15 kids on tuesday and 2 kids on wednesday. How many kids did she play with\naltogether? \\n Reasoning: To find the total number of kids Julia played with, we need to add the number of kids she played with on each day.\nTherefore, the total number of kids Julia played with is 17 + 15 + 2 = 34. \\n Answer: 34\n5-shot demonstrations of CSQA dataset\nQuestion: What do people use to absorb extra ink from a fountain pen? \\n Answer Choices: \\n (a) shirt pocket \\n (b) calligrapher’s hand \\n\n(c) inkwell \\n (d) desk drawer \\n (e) blotter \\n Answer: The answer must be used to absorb extra ink. Blotters are designed to absorb liquids.\nTherefore, the answer is blotter (e).\nQuestion: What home entertainment equipment requires cable? \\n Answer Choices: \\n (a) radio shack \\n (b) substation \\n (c) television \\n\n(d) cabinet \\n (e) desk \\n Answer: The answer must require cable. Cable is used to provide satellite channels to televisions. Therefore, the\nanswer is television (c).\nQuestion: Sammy wanted to go to where the people were. Where might he go? \\n Answer Choices: \\n (a) populated areas \\n (b) race track\n\\n (c) desert \\n (d) apartment \\n (e) roadblock \\n Answer: The answer must be a place with many people. Populated areas, by definition, have\na lot of people. Therefore, the answer is populated areas (a).\nQuestion: Where do you put your grapes just before checking out? \\n Answer Choices: \\n (a) mouth \\n (b) grocery cart \\n (c) super market\n\\n (d) fruit basket \\n (e) fruit market \\n Answer: The answer should be the place where grocery items are placed before checking out. Of the\nabove choices, grocery cart makes the most sense for holding grocery items. Therefore, the answer is grocery cart (b).\nQuestion: Google Maps and other highway and street GPS services have replaced what? \\n Answer Choices: \\n (a) united states \\n (b)\nmexico \\n (c) countryside \\n (d) atlas \\n (e) oceans \\n Answer: The answer must be something that used to do what Google Maps and GPS\nservices do, which is give directions. Atlases were also used to give directions. Therefore, the answer is atlas (d).\n4-shot demonstrations of StrategyQA dataset\nQuestion: Are chinchillas cold-blooded? \\n Reasoning: Chinchillas are rodents, which are mammals. All mammals are warm-blooded. So,\nthe answer is No. \\n Answer: No\nQuestion: Would Janet Jackson avoid a dish with ham? \\n Reasoning: Janet Jackson follows an Islamic practice. Islamic culture avoids\neating pork. Ham is made from pork. So, the answer is Yes. \\n Answer: Yes\nQuestion: Can a honey bee sting a human more than once? \\n Reasoning: Human skin is tough, and the bee’s stinger gets lodged in the skin.\nThe stinger becomes separated from the bee which dies soon after. So, the answer is No. \\n Answer: No\nQuestion: Is average number of peas in a pod enough commas for a billion? \\n Reasoning: The average number of peas in a pod is 6 or 7. A\nbillion is a number that has only 3 commas in it. So, the answer is Yes. \\n Answer: Yes\nTable 15: The demonstrations used for each dataset. The “\\n” indicates a line break. The key token is marked\nin bold for clear view. The prompt for CSQA is slightly different from others since we adopt the original prompt\ntemplate of STaR (Zelikman et al., 2022). And we only use 5 out of 7 demonstrations from STaR.\n1966",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.8865565061569214
    },
    {
      "name": "Computer science",
      "score": 0.5620429515838623
    },
    {
      "name": "Natural language processing",
      "score": 0.5409027934074402
    },
    {
      "name": "Artificial intelligence",
      "score": 0.515956461429596
    },
    {
      "name": "Natural language",
      "score": 0.4119531810283661
    },
    {
      "name": "Linguistics",
      "score": 0.3436400294303894
    },
    {
      "name": "Cognitive science",
      "score": 0.33306288719177246
    },
    {
      "name": "Psychology",
      "score": 0.24165642261505127
    },
    {
      "name": "Philosophy",
      "score": 0.22587305307388306
    },
    {
      "name": "China",
      "score": 0.18213167786598206
    },
    {
      "name": "Political science",
      "score": 0.13289308547973633
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    }
  ],
  "cited_by": 6
}