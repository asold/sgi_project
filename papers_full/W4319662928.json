{
    "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
    "url": "https://openalex.org/W4319662928",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2507324429",
            "name": "Tiffany H. Kung",
            "affiliations": [
                "Massachusetts General Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2534736992",
            "name": "Morgan Cheatham",
            "affiliations": [
                "Brown University"
            ]
        },
        {
            "id": "https://openalex.org/A3102107377",
            "name": "Arielle Medenilla",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142558",
            "name": "Czarina Sillos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142559",
            "name": "Lorie De Leon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142560",
            "name": "Camille Elepaño",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142561",
            "name": "Maria Madriaga",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142562",
            "name": "Rimel Aggabao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142563",
            "name": "Giezel Diaz-Candido",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142564",
            "name": "James Maningo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1904159857",
            "name": "Victor Tseng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2507324429",
            "name": "Tiffany H. Kung",
            "affiliations": [
                "Massachusetts General Hospital",
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A2534736992",
            "name": "Morgan Cheatham",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3102107377",
            "name": "Arielle Medenilla",
            "affiliations": [
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A4316142558",
            "name": "Czarina Sillos",
            "affiliations": [
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A4316142559",
            "name": "Lorie De Leon",
            "affiliations": [
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A4316142560",
            "name": "Camille Elepaño",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316142561",
            "name": "Maria Madriaga",
            "affiliations": [
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A4316142562",
            "name": "Rimel Aggabao",
            "affiliations": [
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A4316142563",
            "name": "Giezel Diaz-Candido",
            "affiliations": [
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A4316142564",
            "name": "James Maningo",
            "affiliations": [
                "eHealth Africa"
            ]
        },
        {
            "id": "https://openalex.org/A1904159857",
            "name": "Victor Tseng",
            "affiliations": [
                "eHealth Africa"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3138114698",
        "https://openalex.org/W2936086693",
        "https://openalex.org/W2557738935",
        "https://openalex.org/W2901612843",
        "https://openalex.org/W3025011581",
        "https://openalex.org/W2766760598",
        "https://openalex.org/W4286233477",
        "https://openalex.org/W3088056511",
        "https://openalex.org/W94062176",
        "https://openalex.org/W2166888237",
        "https://openalex.org/W2949163152",
        "https://openalex.org/W3041565340",
        "https://openalex.org/W4297997268",
        "https://openalex.org/W2920897817",
        "https://openalex.org/W2793981925",
        "https://openalex.org/W4280572991",
        "https://openalex.org/W2958109836",
        "https://openalex.org/W3028484854",
        "https://openalex.org/W3177525997",
        "https://openalex.org/W2599674900",
        "https://openalex.org/W4240958693",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W4392359953",
        "https://openalex.org/W3105070630",
        "https://openalex.org/W3013597571"
    ],
    "abstract": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.",
    "full_text": "RESEA RCH ARTICL E\nPerformance of ChatGPT on USMLE: Potential\nfor AI-assisted medical education using large\nlanguage models\nTiffany H. Kung\n1,2\n, Morgan Cheatham\n3\n, Arielle Medenilla\n1\n, Czarina Sillos\n1\n, Lorie De Leon\n1\n,\nCamille Elepaño\n1\n, Maria Madriaga\n1\n, Rimel Aggabao\n1\n, Giezel Diaz-Candido\n1\n,\nJames Maningo\n1\n, Victor Tseng\nID\n1,4\n*\n1 AnsibleHe alth, Inc Mountain View, Californi a, United States of America, 2 Department of Anesthesi ology,\nMassach usetts General Hospital, Harvard School of Medicine Boston, Massachus etts, United States of\nAmerica, 3 Warren Alpert Medical School; Brown University Providenc e, Rhode Island, United States of\nAmerica, 4 Department of Medical Educatio n, UWorld, LLC Dallas, Texas, United States of America\n* victor@ ansiblehealth. com\nAbstract\nWe evaluated the performance of a large language model called ChatGPT on the United\nStates Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step\n2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams\nwithout any specialized training or reinforcement . Additionally, ChatGPT demonstrated a\nhigh level of concordance and insight in its explanations. These results suggest that large\nlanguage models may have the potential to assist with medical education, and potentially,\nclinical decision-making .\nAuthor summary\nArtificial intelligence (AI) systems hold great promise to improve medical care and health\noutcomes. As such, it is crucial to ensure that the development of clinical AI is guided by\nthe principles of trust and explainability. Measuring AI medical knowledge in comparison\nto that of expert human clinicians is a critical first step in evaluating these qualities. To\naccomplish this, we evaluated the performance of ChatGPT, a language-based AI, on the\nUnited States Medical Licensing Exam (USMLE). The USMLE is a set of three standard-\nized tests of expert-level knowledge, which are required for medical licensure in the\nUnited States. We found that ChatGPT performed at or near the passing threshold of 60%\naccuracy. Being the first to achieve this benchmark, this marks a notable milestone in AI\nmaturation. Impressively, ChatGPT was able to achieve this result without specialized\ninput from human trainers. Furthermore, ChatGPT displayed comprehensible reasoning\nand valid clinical insights, lending increased confidence to trust and explainability. Our\nstudy suggests that large language models such as ChatGPT may potentially assist human\nlearners in a medical education setting, as a prelude to future integration into clinical deci-\nsion-making.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 1 / 12\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Kung TH, Cheatham M, Medenilla A,\nSillos C, De Leon L, Elepaño C, et al. (2023)\nPerformanc e of ChatGPT on USMLE: Potential for\nAI-assisted medical education using large language\nmodels. PLOS Digit Health 2(2): e0000198. https://\ndoi.org/10.13 71/journal.pdi g.0000198\nEditor: Alon Dagan, Beth Israel Deaconess Medical\nCenter, UNITED STATES\nReceived: December 19, 2022\nAccepted: January 23, 2023\nPublished: February 9, 2023\nCopyright: © 2023 Kung et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: The data analyzed in\nthis study were obtained from USMLE sample\nquestions sets which are publicly available . We\nhave made the question indices, raw inputs, and\nraw AI outputs, and special annotation s available in\nS1 Data.\nFunding: The authors received no specific funding\nfor this work.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nIntroduction\nOver the past decade, advances in neural networks, deep learning, and artificial intelligence\n(AI) have transformed the way we approach a wide range of tasks and industries ranging from\nmanufacturing and finance to consumer products. The ability to build highly accurate classifi-\ncation models rapidly and regardless of input data type (e.g. images, text, audio) has enabled\nwidespread adoption of applications such as automated tagging of objects and users in photo-\ngraphs [1], near-human level text translation [2], automated scanning in bank ATMs, and\neven the generation of image captions [3].\nWhile these technologies have made significant impacts across many industries, applica-\ntions in clinical care remain limited. The proliferation of clinical free-text fields combined\nwith a lack of general interoperability between health IT systems contribute to a paucity of\nstructured, machine-readable data required for the development of deep learning algorithms.\nEven when algorithms applicable to clinical care are developed, their quality tends to be highly\nvariable, with many failing to generalize across settings due to limited technical, statistical, and\nconceptual reproducibility [4]. As a result, the overwhelming majority of successful healthcare\napplications currently support back-office functions ranging from payor operations, auto-\nmated prior authorization processing, and management of supply chains and cybersecurity\nthreats. With rare exceptions–even in medical imaging–there are relatively few applications of\nAI directly used in widespread clinical care today.\nThe proper development of clinical AI models [5] requires significant time, resources, and\nmore importantly, highly domain and problem-specific training data, all of which are in short\nsupply in the world of healthcare. One of the key developments that enabled image-based AI\nin clinical imaging has been the ability of large general domain models to perform as well as,\nor even outperform, domain-specific models. This development has catalyzed significant AI\nactivity in medical imaging, where otherwise it would be challenging to obtain sufficient anno-\ntated clinical images. Indeed today, Inception-V3 serves as the basic foundation of many of the\ntop medical imaging models currently published, ranging from ophthalmology [5,6] and\npathology [7] to dermatology [8].\nIn the past three weeks, a new AI model called ChatGPT garnered significant attention due\nto its ability to perform a diverse array of natural language tasks [9]. ChatGPT is a general\nLarge Language Model (LLM) developed recently by OpenAI. While the previous class of AI\nmodels have primarily been Deep Learning (DL) models, which are designed to learn and rec-\nognize patterns in data, LLMs are a new type of AI algorithm trained to predict the likelihood\nof a given sequence of words based on the context of the words that come before it. Thus, if\nLLMs are trained on sufficiently large amounts of text data, they are capable of generating\nnovel sequences of words never observed previously by the model, but that represent plausible\nsequences based on natural human language. ChatGPT is powered by GPT3.5, an LLM trained\non the OpenAI 175B parameter foundation model and a large corpus of text data from the\nInternet via reinforcement and supervised learning methods. Anecdotal usage indicates that\nChatGPT exhibits evidence of deductive reasoning and chain of thought, as well as long-term\ndependency skills.\nIn this study, we evaluate the performance of ChatGPT, a non-domain specific LLM, on its\nability to perform clinical reasoning by testing its performance on questions from the United\nStates Medical Licensing Examination (USMLE). The USMLE is a high-stakes, comprehensive\nthree-step standardized testing program covering all topics in physicians’ fund of knowledge,\nspanning basic science, clinical reasoning, medical management, and bioethics. The difficulty\nand complexity of questions is highly standardized and regulated, making it an ideal input sub-\nstrate for AI testing. The examination is well-established, showing remarkably stable raw\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 2 / 12\nscores and psychometric properties over the previous ten years [10]. The Step 1 exam is typi-\ncally taken by medical students who have completed two years of didactic and problem-based\nlearning and focuses on basic science, pharmacology, and pathophysiology; medical students\noften spend approximately 300–400 hours of dedicated study time in preparation for this\nexam [11]. The Step 2CK exam is usually taken by fourth-year medical students who have\nadditionally completed 1.5 to 2 years of clinical rotations; it emphasizes clinical reasoning,\nmedical management, and bioethics. The Step 3 exam is taken by physicians who generally\nhave completed at least a 0.5 to 1 year of postgraduate medical education.\nUSMLE questions are textually and conceptually dense; text vignettes contain multimodal\nclinical data (i.e., history, physical examination, laboratory values, and study results) often\nused to generate ambiguous scenarios with closely-related differential diagnoses. Due to its lin-\nguistic and conceptual richness, we reasoned that the USMLE would serve as an excellent chal-\nlenge for ChatGPT.\nOur work aims to provide both qualitative and quantitative feedback on the performance of\nChatGPT and assess its potential for use in healthcare.\nMethods\nArtificial Intelligence\nChatGPT (OpenAI; San Francisco, CA), is a large language model that uses self-attention\nmechanisms and a large amount of training data to generate natural language responses to text\ninput in a conversational context. It is particularly effective at handling long-range dependen-\ncies and generating coherent and contextually appropriate responses. ChatGPT is a server-\ncontained language model that is unable to browse or perform internet searches. Therefore, all\nresponses are generated in situ, based on the abstract relationship between words (“tokens”) in\nthe neural network. This contrasts to other chatbots or conversational systems that are permit-\nted to access external sources of information (e.g. performing online searches or accessing\ndatabases) in order to provide directed responses to user queries.\nInput source\n376 publicly-available test questions were from the June 2022 sample exam release, termed\nUSMLE-2022, were obtained from the official USMLE website. Therefore, all inputs repre-\nsented true out-of-training samples for the GPT3 model. This was further confirmed by ran-\ndomly spot checking the inputs to ensure that none of the answers, explanations, or related\ncontent were indexed on Google prior to January 1, 2022, representing the last date accessible\nto the ChatGPT training dataset. All sample test questions were screened, and questions con-\ntaining visual assets such as clinical images, medical photography, and graphs were removed.\nAfter filtering, 350 USMLE items (Step 1: 119, Step 2CK: 102, Step 3: 122) were advanced to\nencoding. Assuming a normal distribution of model performance, this affords 90% power at α\n= 0.05 to detect a 2.5% increase in accuracy against a baseline rate of 60 ± 20% (σ).\nEncoding\nQuestions were formatted into three variants and input into ChatGPT in the following\nsequence:\n1. Open-ended (OE) prompting: Created by removing all answer choices, adding a variable\nlead-in interrogative phrase. This format simulates free input and a natural user query pat-\ntern. Examples include: “What would be the patient’s diagnosis based on the information\nprovided?”; or “In your opinion, what is the reason for the patient’s pupillary asymmetry?”\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 3 / 12\n2. Multiple choice single answer without forced justification (MC-NJ) prompting: Created by\nreproducing the original USMLE question verbatim. Examples include: “Which of the fol-\nlowing best represent the most appropriate next step in management?”; or “The patient’s\ncondition is mostly caused by which of the following pathogens?”\n3. Multiple choice single answer with forced justification (MC-J) prompting: Created by add-\ning a variable lead-in imperative or interrogative phrase mandating ChatGPT to provide a\nrationale for each answer choice. Examples include: “Which of the following is the most\nlikely reason for the patient’s nocturnal symptoms? Explain your rationale for each choice”;\nor “The most appropriate pharmacotherapy for this patient most likely operates by which\nof the following mechanisms? Why are the other choices incorrect?”\nEncoders employed deliberate variation in the lead-in prompts to avoid systematic errors\nintroduced by rigid wording. To reduce memory retention bias, a new chat session was started\nin ChatGPT for each entry. Ordinary 2-way ANOVA of AI response accuracy were performed\npost hoc to evaluate for systematic covariation between encoders and question prompt type\n(S3 Data). Encoders were first considered as individuals (n = 8 inputters), and then subse-\nquently as groups classified by level of medical expertise (n = 4 groups: physician, medical stu-\ndent, nurse, or nonmedical generalist).\nAdjudication\nAI outputs were independently scored for Accuracy, Concordance, and Insight (ACI) by two\nphysician adjudicators using the criteria enumerated in S2 Data. The physicians were blinded\nto each other. A subset of 20 USMLE questions were used for collective adjudicator training.\nPhysicians were not blinded for this subset, but interrater cross-contamination was suppressed\nby forcing staggered review of output measures. For instance, Physician 1 adjudicated Accu-\nracy while Physician 2 adjudicated Concordance. The roles were then rotated such that each\nadjudicator provided a complete ACI rating for the entire dataset. To minimize within-item\nanchoring bias, adjudicators scored Accuracy for all items, followed by Concordance for all\nitems, followed by Insight for all items. If consensus was not achieved for all three domains,\nthe item was referred to a final physician adjudicator. A total of 21 items (6.2% of the dataset)\nrequired arbitration by a third physician. Interrater agreement between physicians was evalu-\nated by computing the Cohen kappa (κ) statistic for OE and MC questions (S4 Data).\nA schematic overview of the study protocol is provided in Fig 1.\nResults\nChatGPT yields moderate accuracy approaching passing performance on\nUSMLE\nExam items were first encoded as open-ended questions with variable lead-in prompts. This\ninput format simulates a free natural user query pattern. With indeterminate responses cen-\nsored/included, ChatGPT accuracy for USMLE Steps 1, 2CK, and 3 was 75.0%/45.4%, 61.5%/\n54.1%, and 68.8%/61.5%, respectively (Fig 2A).\nNext, exam items were encoded as multiple choice single answer questions with no forced\njustification (MC-NJ). This input is the verbatim question format presented to test-takers.\nWith indeterminate responses censored/included, ChatGPT accuracy for USMLE Steps 1,\n2CK, and 3 was 55.8%/36.1%, 59.1%/56.9%, and 61.3%/55.7%, respectively.\nFinally, items were encoded as multiple choice single answer questions with forced justifica-\ntion of positive and negative selections (MC-J). This input format simulates insight-seeking\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 4 / 12\nuser behavior. With indeterminate responses censored/included, ChatGPT accuracy was\n64.5%/ 41.2%, 52.4%/49.5%, and 65.2%/59.8%, respectively (Fig 2B).\nAt the encoding stage, there were no statistically significant interactions between encoders\nand question prompt type, regardless of whether encoders were analyzed as individuals or\nwhen grouped by level of medical expertise (S3 Data). As expected, inter-individual variation\ndominated over inter-group variation, but the overall contribution was insignificant relative to\nresidual error. At the adjudication stage, physician agreement was substantial for OE prompts\n(κ range from 0.74 to 0.81) and nearly perfect for MC prompts (κ >0.9) (S4 Data).\nChatGPT demonstrates high internal concordance\nConcordance was independently adjudicated by two physician reviewers by inspection of the\nexplanation content. Overall, ChatGPT outputted answers and explanations with 94.6% con-\ncordance across all questions. High global concordance was sustained across all exam levels,\nand across OE, MC-NJ, and MC-J question input formats (Fig 3A).\nNext, we analyzed the contingency between accuracy and concordance in MC-J responses.\nChatGPT was forced to justify its answer choice preference, and to defend its rejection of alter-\nnative choices. Concordance amongst accurate responses was nearly perfect, and significantly\ngreater than amongst inaccurate responses (99.1% vs. 85.1%, p<0.001) (Fig 3B).\nFig 1. Schematic of workflow for sourcing, encodin g, and adjudicati ng results. Abbrevia tions: QC = quality control; MCSA-NJ = multiple choice single\nanswer without forced justification; MCSA-J = multiple choice single answer with forced justification; OE = open-ended question format.\nhttps://d oi.org/10.1371 /journal.pdig.0 000198.g001\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 5 / 12\nThese data indicate that ChatGPT exhibits very high answer-explanation concordance,\nlikely reflecting high internal consistency in its probabilistic language model.\nExplanations generated by ChatGPT contain nonobvious insights\nHaving established the accuracy and concordance of ChatGPT, we next examined its potential\nto augment human learning in the domain of medical education. AI-generated explanations\nwere independently adjudicated by 2 physician reviewers. Explanation content was examined\nfor significant insights, defined as instances that met the criteria (see S2 Data) of novelty, non-\nobviousness, and validity. The perspective of the target test audience was adopted by the\nFig 2. Accuracy of ChatGP T on USMLE . For USMLE Steps 1, 2CK, and 3, AI outputs were adjudicated to be\naccurate , inaccurate, or indetermi nate based on the ACI scoring system provided in S2 Data. A: Accuracy distribution\nfor inputs encoded as open-ended questions. B: Accuracy distributi on for inputs encoded as multiple choice single\nanswer without (MC-NJ ) or with forced justifica tion (MC-J).\nhttps://d oi.org/10.1371/j ournal.pdig. 0000198.g002\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 6 / 12\nadjudicator, as a second-year medical student for Step 1, fourth-year medical student for Step\n2CK, and post-graduate year 1 resident for Step 3.\nWe first examined the frequency (prevalence) of insight. Overall, ChatGPT produced at\nleast one significant insight in 88.9% of all responses. Insight frequency was generally consis-\ntent between exam type and question input format (Fig 3C). In Step 2CK however, insight\ndecreased by 10.3% (n = 11 items) between MC-NJ and MC-J formulations, paralleling the\ndecrement in accuracy (Fig 1B). Review of this subset of questions did not reveal a discernible\npattern for the paradoxical decrease (see specifically annotated items [\n�\n] in S1 Data).\nNext, we quantified the density of insight (DOI) contained within AI-generated explana-\ntions. A density index was defined by normalizing the number of unique insights against the\nnumber of possible answer choices. This analysis was performed on MC-J entries only. High\nquality outputs were generally characterized by DOI >0.6 (i.e. unique, novel, nonobvious, and\nFig 3. Concordanc e and insight of ChatGP T on USMLE . For USMLE Steps 1, 2CK, and 3, AI outputs were adjudicated on concordanc e and density of insight (DOI)\nbased on the ACI scoring system provided in S2 Data. A: Overall concordanc e across all exam types and question encoding formats. B: Concordanc e rates stratified\nbetween accurate vs inaccurat e outputs, across all exam types and question encoding formats. p <0.001 for accurate vs inaccurate outputs by Fisher exact test. C:\nOverall insight prevalence , defined as proporti on of outputs with �1 insight, across all exams for questions encoded in MC-J format. D: DOI stratified between\naccurate vs inaccurat e outputs, across all exam types for questions encoded in MC-J format. Horizonta l line indicates the mean. p-value determined by parame tric\n2-way ANOVA testing with Benjamini -Krieger-Yek utieli (BKY) post hoc to control for false discovery rate.\nhttps://d oi.org/10.1371/j ournal.pdig. 0000198.g003\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 7 / 12\nvalid insights provided for >3 out of 5 choices); low quality outputs were generally character-\nized by DOI �0.2. The upper limit on DOI is only bounded by the maximum length of text\noutput. Across all exam types, we observed that mean DOI was significantly higher in ques-\ntions items answered accurately versus inaccurately (0.458 versus 0.199, p <0.0001) (Fig 3D).\nThe high frequency and moderate density of insights indicate that it may be possible for a\ntarget learner (e.g., such as a second-year medical student preparing for Step 1) to gain new or\nremedial knowledge from the ChatGPT AI output, particularly if answering incorrectly.\nDiscussion\nIn this study, we provide new and surprising evidence that ChatGPT is able to perform several\nintricate tasks relevant to handling complex medical and clinical information. To assess\nChatGPT’s capabilities against biomedical and clinical questions of standardized complexity\nand difficulty, we tested its performance characteristics on the United States Medical Licensing\nExamination (USMLE).\nOur findings can be organized into two major themes: (1) the rising accuracy of ChatGPT,\nwhich approaches or exceeds the passing threshold for USMLE; and (2) the potential for this\nAI to generate novel insights that can assist human learners in a medical education setting.\nThe rising accuracy of ChatGPT\nThe most recent iteration of the GPT LLM (GPT3) achieved 46% accuracy with zero prompt-\ning [12], which marginally improved to 50% with further model training and extensive prompt\ntuning. Previous models, merely months prior, performed at 36.7% [13]. In this present study,\nChatGPT performed at >50% accuracy across all examinations, exceeding 60% in some analy-\nses. The USMLE pass threshold, while varying by year, is approximately 60%. Therefore,\nChatGPT now approaches the passing range. Being the first experiment to reach this bench-\nmark, we believe this is a surprising and impressive result. Moreover, we provided no prompt-\ning or training to the AI, minimized grounding bias by expunging the AI session prior to\ninputting each question variant, and avoided chain-of-thought biasing by requesting forced\njustification only as the final input. Further model interaction and prompt tuning could often\nproduce more accurate results. Given this trajectory, it is likely that AI performance will con-\ntinue to improve as LLM models continue to mature.\nParadoxically, ChatGPT outperformed PubMedGPT [14] (accuracy 50.3%), a counterpart\nLLM with similar neural structure, but trained exclusively on biomedical domain literature.\nWe speculate that domain-specific training may have created greater ambivalence in the Pub-\nMedGPT model, as it absorbs real-world text from ongoing academic discourse that tends to\nbe inconclusive, contradictory, or highly conservative or noncommittal in its language. A\nfoundation LLM trained on general content, such as ChatGPT, may therefore have an advan-\ntage because it is also exposed to broader clinical content, such as patient-facing disease prim-\ners and provider-facing drug package inserts, that are more definitive and congruent.\nAn additional explanation for the observed difference in performance may be the disparate\nAI testing datasets. Our present study tested ChatGPT against contemporary USMLE exami-\nnations (publicly available no earlier than 2022, 5 answer choices per question), whereas previ-\nous reports tested language models against the MedQA-USMLE dataset [13] (publicly\navailable 2009–2020, 4 answer choices per question). Although we did not perform a direct\ncomparison against MedQA-UMSLE, our approach nonetheless has several advantages. It is\nguaranteed that none of our inputs were previously seen by GPT3, whereas many of the inputs\nfrom MedQA-USMLE would have likely been ingested during model pretraining. Considering\nthat medical knowledge proliferates at a faster-than-exponential rate [15] and previous\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 8 / 12\nevidence-based practice is frequently debunked [16,17], some concepts tested by Med-\nQA-USMLE are already antiquated and not representative of present-day examination con-\ntent. Finally, the higher accuracy of ChatGPT on USMLE-2022 despite a greater number of\nanswer choices (5 versus 4) may indicate even more impressive performance of this model rel-\native to other domain-specific language models such as PubMedGPT and BioBERT.\nConsistent with the mechanism of generative language models, we observed that the accu-\nracy of ChatGPT was strongly mediated by concordance and insight. High accuracy outputs\nwere characterized by high concordance and high density of insight. Poorer accuracy was\ncharacterized by lower concordance and a poverty of insight. Therefore, inaccurate responses\nwere driven primarily by missing information, leading to diminished insight and indecision in\nthe AI, rather than overcommitment to the incorrect answer choice. These findings indicate\nthat model performance could be significantly improved by merging foundation models, such\nas ChatGPT, with a domain-specific LLM or other model trained on a voluminous and highly\nvalidated medical knowledge resources, such as UpToDate, or other ACGME-accredited\ncontent.\nInterestingly, the accuracy of ChatGPT tended to be lowest for Step 1, followed by Step\n2CK, followed by Step 3. This mirrors both the subjective difficulty and objective performance\nfor real-world test takers on Step 1, which is collectively regarded as the most difficult exam of\nthe series. The low accuracy on Step 1 could be explained by an undertrained model on the\ninput side (e.g. underrepresentation of basic science content on the general information space)\nand/or the human side (e.g. insufficient or invalid human judgment at initial reinforcement\nstages). This result exposes a key vulnerability in pre-trained LLMs, such as ChatGPT: AI abil-\nity becomes yoked to human ability. ChatGPT’s performance on Step 1 is poorer precisely\nbecause human users perceive its subject matter (e.g., pathophysiology) as more difficult or\nopaque.\nThe potential for AI-assisted human learning in medical education\nWe also examined the ability of ChatGPT to assist the human learning process of its target\naudience (e.g., a second year medical student preparing for USMLE Step 1). As a proxy for the\nmetric of helpfulness, we assessed the concordance and insight offered by the AI explanation\noutputs. ChatGPT responses were highly concordant, such that a human learner could easily\nfollow the internal language, logic, and directionality of relationships contained within the\nexplanation text (e.g., adrenal hypercortisolism increased bone osteoclast activity increased\ncalcium resorption decreased bone mineral density increased fracture risk). High internal\nconcordance and low self-contradiction is a proxy of sound clinical reasoning and an impor-\ntant metric of explanation quality. It is reassuring that the directionality of relationships is pre-\nserved by the language processing model, where each verbal object is individually lemmatized.\nAI-generated responses also offered significant insight, role-modeling a deductive reason-\ning process valuable to human learners. At least one significant insight was present in approxi-\nmately 90% of outputs. ChatGPT therefore possesses the partial ability to teach medicine by\nsurfacing novel and nonobvious concepts that may not be in learners’ sphere of awareness.\nThis qualitative gain provides a basis for future real-world studies on the efficacy of generative\nAI to augment the human medical education process. For example, longitudinal exam perfor-\nmance can be studied in a quasi-controlled in AI-assisted and unassisted learners. Unit eco-\nnomic analysis may clarify the cost-effectiveness of incremental student performance gain in\ncomparison to existing tools such as virtual tutors and study aids.\nMedical education, licensing examinations, and test preparation services form a large\nindustrial complex eclipsing a nine-figure market size annually. While its relevance remains\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 9 / 12\ndebated, standardized testing has emerged as an important end-target of medical learning. In\nparallel, of the didactic techniques, a socratic teaching style is favored by medical students\n[18]. The rate-limiting step for fresh content generation is the human cognitive effort required\nto craft realistic clinical vignettes that probe “high-yield” concepts in a subtle way, engage criti-\ncal thinking, and offer pearls of knowledge even if answered incorrectly. Demand for new\nexamination content continues to increase. Future studies may investigate the ability of gener-\native language AI to offload this human effort by assisting in the question-explanation writing\nprocess or, in some cases, writing entire items autonomously.\nFinally, the advent of AI in medical education demands an open science research infra-\nstructure to standardize experimental methods, readouts, and benchmarks to describe and\nquantify human-AI interactions. Multiple dimensions must be covered, including user experi-\nence, learning environment, hybridization with other teaching modes, and effect on cognitive\nbias. In this report, we provide an initial basic protocol for adjudicating AI-generated\nresponses along axes of accuracy, concordance, and insight.\nOur study has several important limitations. The relatively small input size restricted the\ndepth and range of analyses. For example, stratifying the output of ChatGPT by subject taxon-\nomy (e.g., pharmacology, bioethics) or competency type (e.g., differential diagnosis, manage-\nment) may be of great interest to medical educators, and could reveal heterogeneities in\nperformance across language processing for different clinical reasoning tasks. Similarly, a\nmore robust AI failure mode analysis (e.g., language parsing error) may lend insight into the\netiology of inaccuracy and discordance. In addition to being laborious, human adjudication is\nerror-prone and subject to greater variability and bias. Future studies will undoubtedly apply\nunbiased approaches, using quantitative natural language processing and text mining tools\nsuch as word network analysis. In addition to increasing validity and accelerating throughput\nby several orders of magnitude, these methods are likely to better characterize the depth,\ncoherence, and learning value of AI output. Finally, to truly assess the utility of generative lan-\nguage AI for medical education, ChatGPT and related applications must be studied in both\ncontrolled and real-world learning scenarios with students across the engagement and knowl-\nedge spectrum.\nBeyond their utility for medical education, AIs are now positioned to soon become ubiqui-\ntous in clinical practice, with diverse applications across all healthcare sectors. Investigation of\nAI has now entered into the era of randomized controlled trials [19]. Additionally, a profusion\nof pragmatic and observational studies supports a versatile role of AI in virtually all medical\ndisciplines and specialties by improving risk assessment [20,21], data reduction, clinical deci-\nsion support [22,23], operational efficiency, and patient communication [24,25].\nInspired by the remarkable performance of ChatGPT on the USMLE, clinicians within\nAnsibleHealth, a virtual chronic pulmonary disease clinic, have begun to experiment with\nChatGPT as part of their workflows. Inputting queries in a secure and de-identified manner,\nour clinicians request ChatGPT to assist with traditionally onerous writing tasks such as com-\nposing appeal letters to payors, simplifying radiology reports (and other jargon-dense records)\nto facilitate patient comprehension, and even to brainstorm and kindle insight when faced\nwith nebulous and diagnostically challenging cases. We believe that LLMs such as ChatGPT\nare reaching a maturity level that will soon impact clinical medicine at large, enhancing the\ndelivery of individualized, compassionate, and scalable healthcare.\nSupporting information\nS1 Data. Raw data files containing unprocessed question inputs and ChatGPT outputs.\n(PDF)\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 10 / 12\nS2 Data. Adjudication criteria for accuracy, concordance, and insight.\n(PDF)\nS3 Data. ANOVA for systematic encoder effects.\n(PDF)\nS4 Data. Kappa statistic for interrater agreement between adjudicating physicians.\n(PDF)\nAcknowledgmen ts\nThe authors thank Dr. Kristine Vanijchroenkarn, MD and Ms. Audra Doyle RRT, NP for\nfruitful discussions and technical assistance. We also thank Mr. Vangjush Vellahu for technical\nassistance with graphical design and preparation.\nAuthor Contributions\nConceptualization: Tiffany H. Kung, Morgan Cheatham, Victor Tseng.\nData curation: Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria\nMadriaga, Giezel Diaz-Candido, James Maningo, Victor Tseng.\nFormal analysis: James Maningo, Victor Tseng.\nInvestigation: Rimel Aggabao, Victor Tseng.\nMethodology: Tiffany H. Kung, Arielle Medenilla, Czarina Sillos, James Maningo, Victor\nTseng.\nProject administration: Czarina Sillos.\nSoftware: James Maningo.\nSupervision: Tiffany H. Kung, Morgan Cheatham, Victor Tseng.\nValidation: Tiffany H. Kung, Arielle Medenilla, James Maningo, Victor Tseng.\nVisualization: James Maningo, Victor Tseng.\nWriting – original draft: Tiffany H. Kung, Morgan Cheatham, Victor Tseng.\nWriting – review & editing: Tiffany H. Kung, Morgan Cheatham, Victor Tseng.\nReferences\n1. Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inception architectur e for com-\nputer vision. 2016 IEEE Conferen ce on Compute r Vision and Pattern Recognition (CVPR). IEEE;\n2016. https://d oi.org/10.110 9/cvpr.2016 .308\n2. Zhang W, Feng Y, Meng F, You D, Liu Q. Bridging the gap between training and inference for neural\nmachine translation. Proceedings of the 57th Annual Meeting of the Association for Computat ional Lin-\nguistics. Strouds burg, PA, USA: Association for Comp utational Linguistics; 2019. https://doi.or g/10.\n18653/v1 /p19-1426\n3. Bhatia Y, Bajpaye e A, Raghuvanshi D, Mittal H. Image captionin g using Google’s inception -resnet-v2\nand recurrent neural network. 2019 Twelft h International Conferen ce on Contemp orary Computing\n(IC3). IEEE; 2019. https://doi.or g/10.110 9/ic3.2019. 8844921\n4. McDermo tt MBA, Wang S, Marinsek N, Ranganath R, Foschini L, Ghassemi M. Reprodu cibility in\nmachine learning for health research : Still a ways to go. Sci Transl Med. 2021;13 . https://doi.or g/10.\n1126/scitra nslmed.abb 1655 PMID: 33762434\n5. Chen P-HC, Liu Y, Peng L. How to develop machine learning models for healthcar e. Nat Mater. 2019;\n18: 410–414. https://doi.or g/10.1038/ s41563-019- 0345-0 PMID: 3100080 6\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 11 / 12\n6. Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanasw amy A, et al. Developm ent and valida-\ntion of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photogr aphs.\nJAMA. 2016; 316: 2402. https://doi.or g/10.1001/ jama.2016. 17216 PMID: 27898976\n7. Nagpal K, Foote D, Liu Y, Chen P-HC, Wulczyn E, Tan F, et al. Developm ent and validation of a deep\nlearning algorithm for improving Gleason scoring of prostate cancer. NPJ Digit Med. 2019; 2: 48. https://\ndoi.org/10.10 38/s41746-019 -0112-2 PMID: 313043 94\n8. Liu Y, Jain A, Eng C, Way DH, Lee K, Bui P, et al. A deep learning system for differential diagnos is of\nskin diseases. Nat Med. 2020; 26: 900–908. https:// doi.org/10.10 38/s4159 1-020-084 2-3 PMID:\n32424212\n9. [cited 26 Jan 2023]. Available: https://opena i.com/b log/chatgpt /\n10. Performanc e data. [cited 26 Jan 2023]. Available: https://www .usmle.org/p erforma nce-data\n11. Burk-Rafe l J, Santen SA, Purkiss J. Study Behavio rs and USMLE Step 1 Performanc e: Implications of\na Student Self-Dir ected Parallel Curriculum . Acad Med. 2017; 92: S67–S74. https://doi.or g/10.1097 /\nACM.000000 000000 1916 PMID: 29065026\n12. Lie ´ vin V, Hother CE, Winther O. Can large langua ge models reason about medical questions? arXiv\n[cs.CL]. 2022. Availab le: http://arxiv.or g/abs/220 7.08143\n13. Jin D, Pan E, Oufattole N, Weng W-H, Fang H, Szolovits P. What Disease does this Patien t Have? A\nLarge-sc ale Open Doma in Question Answerin g Dataset from Medical Exams . arXiv [cs.CL]. 2020.\nAvailable: http://arxiv .org/abs/2 009.13081\n14. Stanford CRFM. [cited 18 Jan 2023]. Available: https://c rfm.stanford .edu/202 2/12/15/pubm edgpt.html\n15. Densen P. Challeng es and opportun ities facing medical education . Trans Am Clin Climat ol Assoc.\n2011; 122: 48–58. Available: https://ww w.ncbi.nlm. nih.gov/pubm ed/21686208 PMID: 21686208\n16. Prasad V, Vandros s A, Toomey C, Cheung M, Rho J, Quinn S, et al. A decade of reversal: an analysis\nof 146 contradi cted medical practices. Mayo Clin Proc. 2013; 88: 790–79 8. https://doi.or g/10.101 6/j.\nmayocp.2 013.05.012 PMID: 238712 30\n17. Herrera-P erez D, Haslam A, Crain T, Gill J, Livingston C, Kaestner V, et al. A comprehe nsive review of\nrandomiz ed clinical trials in three medical journals reveals 396 medical reversal s. Elife. 2019; 8. https://\ndoi.org/10.75 54/eLife.45183 PMID: 31182188\n18. Abou-Ha nna JJ, Owens ST, Kinnucan JA, Mian SI, Kolars JC. Resuscita ting the Socrati c Method: Stu-\ndent and Faculty Perspe ctives on Posing Probing Question s During Clinical Teaching . Acad Med.\n2021; 96: 113–11 7. https://doi.or g/10.109 7/ACM.00000 000000 03580 PMID: 33394663\n19. Plana D, Shung DL, Grimshaw AA, Saraf A, Sung JJY, Kann BH. Random ized Clinical Trials of Machine\nLearning Interventi ons in Health Care: A System atic Review. JAMA Netw Open. 2022; 5: e22339 46.\nhttps://doi.or g/10.100 1/jamane tworkopen.2 022.33946 PMID: 36173632\n20. Kan HJ, Kharrazi H, Chang H-Y, Bodycombe D, Lemke K, Weiner JP. Exploring the use of machine\nlearning for risk adjustment : A comparison of standard and penali zed linear regression models in pre-\ndicting health care costs in older adults. PLoS One. 2019; 14: e0213258. https://doi.or g/10.1371/\njournal.pon e.02132 58 PMID: 30840682\n21. Delahanty RJ, Kaufman D, Jones SS. Development and Evaluati on of an Automate d Machine Learning\nAlgorithm for In-Hospital Mortality Risk Adjustmen t Among Critical Care Patien ts. Crit Care Med. 2018;\n46: e481–e48 8. https://doi.or g/10.1097 /CCM.0000 00000000301 1 PMID: 294195 57\n22. Vasey B, Nagendran M, Campbell B, Clifton DA, Collins GS, Denaxas S, et al. Reporting guideline for\nthe early-sta ge clinical evaluatio n of decision support systems driven by artificial intelligenc e: DECIDE-\nAI. Nat Med. 2022; 28: 924–933. https:/ /doi.org/10.10 38/s4159 1-022-017 72-9 PMID: 35585198\n23. Garcia-Vi dal C, Sanjuan G, Puerta-Al calde P, Moreno -Garcı ´ a E, Soriano A. Artificial intelligence to sup-\nport clinical decision-m aking processes. EBioMedicine. 2019; 46: 27–29. https://do i.org/10.1016 /j.\nebiom.20 19.07.019 PMID: 31303500\n24. Bala S, Keniston A, Burden M. Patient Percept ion of Plain-Lan guage Medical Notes Generated Using\nArtificial Intelligenc e Software: Pilot Mixed-Metho ds Study. JMIR Form Res. 2020; 4: e16670. https://\ndoi.org/10.21 96/16670 PMID: 3244214 8\n25. Milne-Ives M, de Cock C, Lim E, Shehadeh MH, de Pennington N, Mole G, et al. The Effectiveness of\nArtificial Intelligenc e Conversation al Agents in Health Care: Systema tic Review. J Med Internet Res.\n2020; 22: e20346 . https://doi.or g/10.219 6/20346 PMID: 330901 18\nPLOS DIGI TAL HEALT H\nChatGP T and medical education\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000019 8 Februa ry 9, 2023 12 / 12"
}