{
  "title": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding",
  "url": "https://openalex.org/W3112776819",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2100002096",
      "name": "He Bai",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A1988150995",
      "name": "Peng Shi",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2163619555",
      "name": "Jimmy Lin",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2134276039",
      "name": "Yu-Qing Xie",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2095969387",
      "name": "Luchen Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129229820",
      "name": "Kun Xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108823143",
      "name": "Wen Gao",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1996079675",
      "name": "Ming Li",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2100002096",
      "name": "He Bai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988150995",
      "name": "Peng Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163619555",
      "name": "Jimmy Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134276039",
      "name": "Yu-Qing Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095969387",
      "name": "Luchen Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129229820",
      "name": "Kun Xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108823143",
      "name": "Wen Gao",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1996079675",
      "name": "Ming Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2894175714",
    "https://openalex.org/W6776122419",
    "https://openalex.org/W6631239350",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W6744707562",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2970908088",
    "https://openalex.org/W6766803791",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W6758609666",
    "https://openalex.org/W6749621233",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W2798836595",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2966892770",
    "https://openalex.org/W6752342493",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3046423960",
    "https://openalex.org/W1521626219",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4297788867",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W3015163529",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W4230872509",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W3023786569",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963430354",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2911966030",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segment-aware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning. Our code is available on GitHub.",
  "full_text": "Segatron: Segment-Aware Transformer for\nLanguage Modeling and Understanding\nHe Bai,1 Peng Shi,1 Jimmy Lin,1, 2 Yuqing Xie,1 Luchen Tan,2 Kun Xiong,2 Wen Gao,3 Ming Li1, 2\n1David R. Cheriton School of Computer Science, University of Waterloo\n2 RSVP.ai\n3 School of Electronics Engineering and Computer Science, Peking University\nfhe.bai, peng.shi, jimmylin, yuqing.xie, mlig@uwaterloo.ca, flctan, kung@rsvp.ai, wgao@pku.edu.cn\nAbstract\nTransformers are powerful for sequence modeling. Nearly\nall state-of-the-art language models and pre-trained language\nmodels are based on the Transformer architecture. However,\nit distinguishes sequential tokens only with the token posi-\ntion index. We hypothesize that better contextual representa-\ntions can be generated from the Transformer with richer po-\nsitional information. To verify this, we propose a segment-\naware Transformer (Segatron), by replacing the original to-\nken position encoding with a combined position encoding\nof paragraph, sentence, and token. We ﬁrst introduce the\nsegment-aware mechanism to Transformer-XL, which is a\npopular Transformer-based language model with memory\nextension and relative position encoding. We ﬁnd that our\nmethod can further improve the Transformer-XL base model\nand large model, achieving 17.1 perplexity on the WikiText-\n103 dataset. We further investigate the pre-training masked\nlanguage modeling task with Segatron. Experimental results\nshow that BERT pre-trained with Segatron (SegaBERT) can\noutperform BERT with vanilla Transformer on various NLP\ntasks, and outperforms RoBERTa on zero-shot sentence rep-\nresentation learning. Our code is available on GitHub.1\nIntroduction\nLanguage modeling (LM) is a traditional sequence model-\ning task which requires learning long-distance dependencies\nfor next token prediction based on the previous context. Re-\ncently, large neural LMs trained on a massive amount of text\ndata have shown great potential for representation learning\nand transfer learning, and also achieved state-of-the-art re-\nsults in various natural language processing tasks.\nTo the best of our knowledge, state-of-the-art language\nmodels (Dai et al. 2019; Baevski and Auli 2019; Rae et al.\n2020) and pre-trained language models (Radford 2018; De-\nvlin et al. 2019; Yang et al. 2019; Lan et al. 2020) all use\na multi-layer Transformer (Vaswani et al. 2017). The Trans-\nformer network was initially used in the seq2seq architec-\nture for machine translation, whose input is usually a sen-\ntence. Hence, it is intuitive to distinguish each token with\nits position index in the input sequence. However, the in-\nput length can grow to 1024 or more tokens and come from\nCopyright c\r 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1https://github.com/rsvp-ai/segatron aaai\ndifferent sentences and paragraphs for language modeling.\nAlthough vanilla position encoding can help the transformer\nbe aware of the token position by assigning a unique index\nto each token, the token index in a sentence, sentence in-\ndex in a paragraph, and paragraph index in a document are\nall implicit. Such segmentation information is essential for\nlanguage modeling, as tokens in different segments of con-\ntext hold different signiﬁcance for next token prediction. If\nthe Transformer model can be aware of the segment posi-\ntion of each context token, we hypothesize that the Trans-\nformer model will model language more efﬁciently and suc-\ncessfully, and will generate better context representations. It\nshould be noticed that, although punctuations and paragraph\nbreakers can provide boundary information to some extent,\nthe boundary is not as straightforward as segment position,\nespecially for the self-attention’s dot-product operation in\nTransformer.\nHence, we propose a novel segment-aware Trans-\nformer (Segatron), which encodes paragraph index in a doc-\nument, sentence index in a paragraph, and token index in a\nsentence all together for the input sequence. We ﬁrst ver-\nify the proposed method with relative position encoding on\nthe language modeling task. By applying the segment-aware\nmechanism to Transformer-XL (Dai et al. 2019), our base\nmodel trained with the WikiText-103 dataset (Merity et al.\n2017) outperforms Transformer-XL base by 1.5 points in\nterms of perplexity. Our large model achieves a perplexity of\n17.1, the same score as Compressive Transformer (Rae et al.\n2020), which is a more complicated model with longer input\ncontext and additional training objectives. We also pre-train\nmasked language models with Transformer (BERT-base \u0000)\nand Segatron (SegaBERT-base \u0000) with English Wikipedia\nfor 500K training steps. According to experimental results,\nSegaBERT outperforms BERT on both general language un-\nderstanding (GLUE) and machine reading comprehension\ntasks. We further pre-trained a large model SegaBERT-large\nwith the same data used in BERT. Experimental results show\nthat SegaBERT-large not only outperforms BERT-large on\nall the above tasks, but also outperforms RoBERTa-large on\nzero-shot Semantic Textual Similarity tasks, where we use\nless data and no more than 10% computational resources of\nRoBERTa. These results demonstrate the value of segment\nencodings in Transformers.\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n12526\nModel\nIn this section, we show how to apply our proposed segment-\naware Transformer to language modeling. More speciﬁ-\ncally, we ﬁrst introduce our Segatron-XL (Segment-aware\nTransformer-XL) with non-learnable relative position en-\ncoding for autoregressive language modeling. Then we in-\ntroduce our pre-trained Segatron (SegaBERT) with learn-\nable absolute position encoding for masked language mod-\neling (MLM).\nSegatron-XL\nWe ﬁrst introduce our method in the context of autore-\ngressive language modeling, by replacing the vanilla Trans-\nformer index in Transformer-XL (Dai et al. 2019) with Sega-\ntron. Transformer-XL is a memory augmented Transformer\nwith relative position encoding:\nArel\ni;j = ET\nxiWT\nq Wk;EExj + ET\nxiWT\nq Wk;RRi\u0000j\n+uT Wk;EExj + vT Wk;RRi\u0000j\n(1)\nwhere Arel\ni;j is the self-attention score between query iand\nkey j. Exi and Exj are the input representations of query i\nand key j, respectively.Ri\u0000j is the relative position embed-\nding. Wk;E and Wk;R are transformation matrices for input\nrepresentation and position embedding, respectively. u and\nv are learnable variables. The position embeddings are non-\nlearnable and deﬁned as:\nRi\u0000j;k =\n\u001a sin( i\u0000j\n100002k=dim ) k< 1\n2 dim\ncos( i\u0000j\n100002k=dim ) k\u00151\n2 dim (2)\nwhere dim is the dimension size of Ri\u0000j, and k is the di-\nmension index.\nOur proposed method introduces paragraph and sentence\nsegmentation to the relative position encoding. The new po-\nsition embeddings RI;J are deﬁned as:\nRI;J;k =\n8\n<\n:\nRtti\u0000tj;k k< 1\n3 dim\nRs\nsi\u0000sj;k\u00001\n3 dim\n2\n3 dim>k \u00151\n3 dim\nRp\npi\u0000pj;k\u00002\n3 dim k\u00152\n3 dim\n(3)\nwhere I = fti;si;pig, J = ftj;sj;pjg. t, s, and pare to-\nken position index, sentence position index, and paragraph\nposition index, respectively. Rt, Rs, and Rp are the rela-\ntive position embeddings of token, sentence, and paragraph.\nThese embeddings are deﬁned in Eq. 2 and the dimensions\nof each are equal to 1/3 of RI;J. The input representation of\nour model is shown in Figure 1(a).\nTo equip the recurrence memory mechanism of\nTransformer-XL with the segment-aware relative posi-\ntion encoding, the paragraph position, the sentence position,\nand the token position indexes of the previous segment\nshould also be cached together with the hidden states. Then,\nthe relative position can be calculated by subtracting the\ncached position indexes from the current position indexes.\nPre-trained Segatron\nWe will introduce how to pre-train a language model with\nour proposed Segatron in this section.\n(a) Concating Relative Position Embedding\nInput\tSequence \nW ord \nEmbeddings \nRelative\tPosition \nEmbeddings \n(b) Summing Absolute Position Embedding\nInput\tSequence \nParagraph\t Index\t \nEmbeddings \nSentence\t Index\t \nEmbeddings \nT oken\t Index\t \nEmbeddings \nW ord \nEmbeddings \nFigure 1: Input representation of Segatron-XL and\nSegaBERT.\nFirst, pre-training a masked language model in the setting\nof BERT is a practical choice, as BERT is a popular base-\nline model and requires less computational resources com-\npared with more recent large models. For example, BERT-\nlarge only needs about 10% of the resources of RoBERTa-\nlarge (Liu et al. 2019). Hence, in this paper, we ﬁrst pre-train\ntwo base size models: SegaBERT-base \u0000 and BERT-base\u0000\nwith only English Wikipedia data for 500K training steps,\nto compare BERT pre-trained with Transformer and Sega-\ntron fairly. We then pre-train a large size model SegaBERT-\nlarge with Wikibooks dataset and 1M training steps, same as\nBERT-large.\nInput Representation. Input X of SegaBERT is a sequence\nof tokens, which can be one or more sentences or para-\ngraphs. The representation xt for token t is computed by\nsumming the corresponding token embedding Et, token in-\ndex embedding Pt\nt, sentence index embeddingPs\nt , and para-\ngraph index embedding Pp\nt , as shown in Figure 1(b). Two\nspecial tokens [CLS] and [SEP] are added to the text se-\nquence before the ﬁrst token and after the last token, and\ntheir paragraph/sentence indexes are the same as their adja-\ncent tokens. Following BERT, the text is tokenized into sub-\nwords with WordPiece and the maximum sequence length is\n512.\nTraining Objective. Following BERT, we use the masked\nLM as our training objective. However, next sentence pre-\ndiction (NSP) is not used in our model, as our input contains\nmore than two sentences.\nData preparation. For the pre-training corpus we use En-\nglish Wikipedia and Bookcorpus (Zhu et al. 2015). For each\ndocument, we ﬁrstly split each into Np paragraphs, and all\nthe sub-tokens in the i-th paragraph are assigned the same\nParagraph Index Embedding Pp\ni . The paragraph index starts\nfrom 0 for each document. Similarly, each paragraph is fur-\n12527\nModel #Param. PPL\nLSTM+Neural cache (Grave, Joulin, and Usunier 2017) - 40.8\nHebbian+Cache (Rae et al. 2018) - 29.9\nTransformer-XL base, M=150 (Dai et al. 2019) 151M 24.0\nTransformer-XL base, M=150 (ours) 151M 24.4\nSegatron-XL base, M=150 151M 22.5\nAdaptive Input (Baevski and Auli 2019) 247M 18.7\nTransformer-XL large, M=384 (Dai et al. 2019) 257M 18.3\nCompressive Transformer, M=1024 (Rae et al. 2020) 257M 17.1\nSegatron-XL large, M=384 257M 17.1\nTable 1: Comparison with Transformer-XL and competitive baseline results on WikiText-103.\nther segmented into Ns sentences with NLTK (Bird, Klein,\nand Loper 2009), and all the sub-tokens in the i-th sentence\nare assigned the same Sentence Index Embedding Ps\ni . The\nsentence index starts from 0 for each paragraph. Within each\nsentence, all the sub-tokens are indexed from 0; thei-th sub-\ntoken will have its Token Index EmbeddingPt\ni.\nWhen building a training example, we randomly (length\nweighted) sample a document from the corpus and randomly\nselect a sentence in that document as the start sentence.\nThen, the following sentences are added to that example un-\ntil the example meets the maximum length limitation (512)\nor runs out of the selected document. If any position index\nin that example exceeds the maximum index, all such posi-\ntion indexes will be subtracted by one until they meet the\nmaximum requirements. The maximum position index of\nparagraph, sentence, and token are 50, 100, and 256, respec-\ntively.\nTraining Setup. Liu et al. (2019) have shown that BERT\npre-trained with document input (more than two sentences)\nwithout NSP performs better than the original BERT on\nsome tasks. Hence, we not only pre-train a SegaBERT-large,\nbut also pre-train two base models with the same setting for\nfair comparison. Similar to BERT, the base model is 12 lay-\ners, 768 hidden size, and 12 self-attention heads. The large\nmodel is 24 layers, 1024 hidden size, and 24 self-attention\nheads. For optimization, we use Adam with learning rate 1e-\n4, \f1=0.9, \f2=0.999, with learning rate warm-up over the\nﬁrst 1% of the total steps and with linear decay of the learn-\ning rate.\nExperiments\nIn this section, we ﬁrst conduct autoregressive language\nmodeling experiments with our proposed Segatron and also\nconduct an ablation study with this task. Then, we show the\nresults of pre-trained SegaBERT on general language un-\nderstanding tasks, semantic textual similarity tasks, and ma-\nchine reading comprehension tasks.\nAutoregressive Language Modeling\nDataset WikiText-103 is a large word-level dataset with\nlong-distance dependencies for language modeling. This\ndataset preserves both punctuations and paragraph line\nbreakers, which are essential for our segmentation pre-\nModel PPL\nTransformer-XL base 24.35\n+ paragraph position encoding 24.07\n+ sentence position encoding 22.51\nSegatron-XL base 22.47\nTable 2: Ablation over the position encodings using\nTransformer-XL base architecture.\nprocessing. There are 103M tokens, 28K articles for train-\ning. The average length is 3.6K tokens per article.\nModel Conﬁguration Following Transformer-XL, we\ntrain a base size model and a large size model. The base\nmodel is a 16 layer Transformer with a hidden size of 410\nand 10 self-attention heads. This model is trained for 200K\nsteps with a batch size of 64. The large model is an 18 layer\nTransformer with a hidden size of 1024 and 16 attention\nheads. This model is trained with 350K steps with a batch\nsize of 128. The sequence length and memory length dur-\ning training and testing all equal 150 for the base model\nand 384 for the large model. The main differences between\nour implementation and Transformer-XL are: we use mixed-\nprecision mode; our input/memory lengths between training\nand testing are the same; the large model training steps of\nTransformer-XL are 4M according to their implementation.\nMain Results Our results are shown in Table 1. As we\ncan see from this table, the improvement with the segment-\naware mechanism is quite impressive: the perplexity de-\ncreases 1.5 points for the Transformer-XL base and de-\ncreases 1.2 for Transformer-XL large. We also observe that\nour large model achieves 18.3 PPL with only 172K train-\ning steps. We ﬁnally obtain a perplexity of 17.1 with our\nlarge model – comparable to prior state-of-the-art results of\nCompressive Transformer (Rae et al. 2020), which is based\non Transformer-XL but trained with longer input length\nand memory length (512) and a more complicated memory\ncache mechanism.\nIt is worth noting that we do not list methods with ad-\nditional training data or dynamic evaluation (Krause et al.\n2018) which continues training the model on the test set.\nWe also note that there is a contemporaneous work Rout-\ningTransformer (Roy et al. 2020), which modiﬁes the self-\n12528\n0 5k 10k 15k 20k\nTraining Steps\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0Valid PPL\nTransformer-XL\nSegatron-XL\nFigure 2: Valid perplexities during the training processes of\nlanguage modeling.\n25 50 100 150\nInput Length\n20\n25\n30\n35\n40Test PPL\n36.29\n39.24\n28.06\n29.83\n23.12\n25.23\n22.50\n24.40\nTransformer-XL\nSegatron-XL\nFigure 3: Test perplexities of Segatron-XL and Transformer-\nXL trained with different input lengths.\nattention to local and sparse attention with a clustering\nmethod. However, their implementations are not available.\nWe believe our method is orthogonal to their work and can\nbe introduced to their model.\nAnalysis We plot the valid perplexity of Segatron-XL base\nand Transformer-XL base during training in Figure 2. From\nthis ﬁgure, we can see that the segment-aware model out-\nperforms the base model all the time, and the gap between\nthem becomes larger as training progresses. Segatron-XL\nat 10K steps approximately matches the performance of\nTransformer-XL at 20K steps. We then test the effective-\nness of Segatron over different input lengths (25, 50, 100,\nand 150 input tokens) by comparing Transformer-XL and\nSegatron-XL base models. As we can see from Figure 3, the\nimprovements are consistent and signiﬁcant. There is no ev-\nidence showing our method prefers shorter or longer input.\nAblation Study We ﬁnally conduct an ablation study with\nSegatron-XL base, to investigate the contributions of the\nsentence position encoding and the paragraph position en-\ncoding, respectively. Experimental results are shown in Ta-\nble 2. From this table, we ﬁnd that the PPL of Transformer-\n0 100k 200k 300k 400k 500k\nTraining Steps\n1.4\n1.6\n1.8\n2.0Valid Loss\nBERT\nSegaBERT\nFigure 4: Valid losses during the pre-training.\nXL decreases from 24.35 to 24.07/22.51 after adding para-\ngraph/sentence position encoding, and further decreases to\n22.47 by encoding paragraph and sentence positions simul-\ntaneously. The results show that both the paragraph position\nand sentence position can help the Transformer to model\nlanguage. Sentence position encoding contributes more than\nparagraph position encoding in our experiments.\nPre-trained Masked Language Model\nWe ﬁrst plot the valid losses of BERT-base \u0000 and\nSegaBERT-base\u0000 during pre-training in Figure 4. The\noverall trends between Figure 2 and Figure 4 are simi-\nlar, which demonstrates that our proposed segment-aware\nmethod works on both auto-regressive language modeling\nand masked language modeling. We will detail our experi-\nments with our pre-trained models in the following sections.\nGeneral Language Understanding The General\nLanguage Understanding Evaluation (GLUE) bench-\nmark (Wang et al. 2019) is a collection of resources for\nevaluating natural language understanding systems. Fol-\nlowing Devlin et al. (2019), we evaluate our model over\nthese tasks: linguistic acceptability CoLA (Warstadt, Singh,\nand Bowman 2019), sentiment SST-2 (Socher et al. 2013),\nparaphrase MRPC (Dolan and Brockett 2005), textual\nsimilarity STS-B (Cer et al. 2017), question paraphrase\nQQP, textual entailment RTE (Bentivogli et al. 2009) and\nMNLI (Williams, Nangia, and Bowman 2018), and question\nentailment QNLI (Wang et al. 2019). We ﬁne-tune every\nsingle task only on its in-domain data without two-stage\ntransfer learning.\nOn the GLUE benchmark, we conduct the ﬁne-tuning\nexperiments in the following manner: For single-sentence\nclassiﬁcation tasks, such as sentiment classiﬁcation (SST-2),\nthe sentence will be assigned Paragraph Index 0 and Sen-\ntence Index 0. For sentence pair classiﬁcation tasks, such as\nquestion-answer entailment (QNLI), the ﬁrst sentence will\nbe assigned Paragraph Index 0 and Sentence Index 0 and\nthe second sentence will be assigned Paragraph Index 1 and\nSentence Index 0.\nWe conduct grid search with the GLUE dev set for small\ndata tasks: CoLA, MRPC, RTE, SST-2, and STS-B. Our grid\n12529\nModel MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B A VG\nBERT-base\u0000 83.2 90.4 86.5 68.3 91.3 92.6 55.0 88.9 82.0\nSegaBERT-base\u0000 83.8 91.5 87.0 71.8 92.1 92.4 54.7 89.0 82.8\nBERT-large (best of 3) 87.3 93.0 91.4 74.0 94.0 88.7 63.7 90.2 85.3\nSegaBERT-large 87.6 93.6 89.1 78.3 94.7 92.3 65.3 90.3 86.4\nTable 3: Fair comparison on GLUE dev. The two base models are pre-trained in the same setting. For large models comparison,\nwe choose the best of 3 BERT-large models: the original BERT, whole word masking BERT, and BERT without NSP task.\nResults of BERT-large (best of 3) are from Yang et al. (2019).\nModel MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B A VG\nBERT-base\u0000 82.9 90.1 70.8 65.4 91.2 88.9 43.5 83.9 77.1\nSegaBERT-base\u0000 83.5 90.8 71.4 68.1 91.5 89.3 50.7 84.6 78.7\nBERT-large 86.7 92.7 72.1 70.1 94.9 89.3 60.5 86.5 81.6\nSegaBERT-large 87.9 94.0 72.5 71.6 94.8 89.7 62.6 88.6 82.7\nTable 4: Results on GLUE test set. Results of BERT-large are from Devlin et al. (2019).\nModel STS-12 STS-13 STS-14 STS-15 STS-16 STS-B SICK-R A VG\nS-BERT-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55\nS-BERT-large* 72.39 78.06 75.26 81.79 76.35 78.64 73.85 76.62\nS-RoBERTa-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68\nS-SegaBERT-large 74.49 78.64 74.88 83.28 77.10 79.42 73.77 77.37\nTable 5: Zero-shot spearman’s rank correlation\u001a\u0002100 between the negative distance of sentence embeddings and the gold la-\nbels. STS-B: STS benchmark, SICK-R: SICK relatedness dataset. Results of BERT-large and RoBERTa-large are from Reimers\nand Gurevych (2019).\nsearch space is as follows:\n\u000f Batch size: 16, 24, 32;\n\u000f Learning rate: 2e-5, 3e-5, 5e-5;\n\u000f Number of epochs: 3-10.\nFor QQP, MNLI, and QNLI, we use the default hyper-\nparameters: 3e-5 learning rate, 256 batch size, and 3 epochs.\nThe other hyper-parameters are the same as in the Hugging-\nFace Transformers library.2\nWe compare BERT and SegaBERT in a fair setting to de-\ncouple the effects of document-level inputs and the removal\nof NSP. In Table 3, two base models are pre-trained by us\nand the only difference is the position encoding. We can\nsee that our SegaBERT-base\u0000 outperforms BERT-base\u0000 on\nmost tasks. We also notice that SegaBERT-base \u0000 is lower\nthan BERT-base\u0000 by over 2.5 points on CoLA. However,\nthis gap decreases to 0.1 on the test set, which is shown in\nTable 4. This is because the size of CoLA is quite small and\nnot as robust as other datasets. Improvements can also be\nobserved easily when comparing SegaBERT-large with the\nbest score of 3 BERT-large models.\nThese results demonstrate SegaBERT’s effectiveness in\ngeneral natural language understanding. The improvements\non these sentence and sentence pair classiﬁcation tasks show\nthat our segment-aware pre-trained model is better than\nvanilla Transformer on sentence-level tasks.\n2https://github.com/huggingface/transformers\nSentence Representation Learning Since our SegaBERT\nhas shown great potential on sentence-level tasks, in\nthis section, we further investigate whether SegaBERT\ncan generate better sentence representations. Following\nSentence-BERT (Reimers and Gurevych 2019), we ﬁne-tune\nSegaBERT in a siamese structure on the combination of\nSNLI (Bowman et al. 2015) and MNLI datasets. The ﬁne-\ntuned model is named S-SegaBERT. We then evaluate the\nzero-shot performance of S-SegaBERT and other baselines\non Semantic Textual Similarity (STS) tasks using the Spear-\nman’s rank correlation between the cosine similarity of the\nsentence embeddings and the gold labels.\nIn Table 5, the results of S-BERT-large and S-RoBERTa-\nlarge are from Reimers and Gurevych (2019). The results of\nS-BERT-large* are re-implemented by us, which is similar\nto Sentence-BERT’s results. We can see that our SegaBERT\nachieves the highest average scores on STS tasks, even out-\nperforms RoBERTa, which uses much more training data,\nlarger batch size, and dynamic masking. These results con-\nform with our improvements on GLUE benchmarks, which\nindicate that a language model pre-trained with Segatron can\nlearn better sentence representations (single sentence encod-\ning) than the original Transformer.\nReading Comprehension We ﬁnally test our pre-trained\nmodel on machine reading comprehension tasks. For these\ntasks, the question is assigned Paragraph Index 0 and Sen-\ntence Index 0. For a context with n paragraphs, Paragraph\nIndex 1 to n+ 1 are assigned to them accordingly. Within\n12530\n(a) BERT-Layer 1\n (b) BERT-Layer 6\n (c) BERT-Layer 12\n(d) SegaBERT-Layer 1\n (e) SegaBERT-Layer 6\n (f) SegaBERT-Layer 12\nFigure 5: Self-attention heat maps of the ﬁrst, the sixth, and the last layer of SegaBERT and BERT when encoding the ﬁrst 512\ntokens of a Wikipedia article.\nSystem SQUAD1.1 SQUAD2.0\nModel EM F1 EM F1\nBERT-base 80.8 88.5 72.3 75.6\nBERT-base\u0000 81.9 89.4 75.4 78.2\nSegaBERT-base\u0000 83.2 90.2 76.3 79.2\nBERT-large 84.1 90.9 78.7 81.9\nBERT-large wwm 86.7 92.8 80.6 83.4\nSegaBERT-large 86.0 92.6 81.8 85.2\nTable 6: Evaluation results on SQUAD v1.1 and v2. Re-\nsults of BERT-base and BERT-large are from Devlin et al.\n(2019). Results of BERT-large wwm on SQUAD v1.1 are\nfrom BERT’s github repository. There are no ofﬁcial results\nof BERT-large wwm on SQUAD v2 and here we report our\nﬁne-tuning results.\neach paragraph, the sentences are indexed from 0.\nWe ﬁrst ﬁne-tune our SegaBERT model with SQUAD\nv1.1 (Rajpurkar et al. 2016) for 4 epochs with 128 batch size\nand 3e-5 learning rate. The ﬁne-tuning setting of SQUAD\nv2.0 (Rajpurkar, Jia, and Liang 2018) is the same as SQUAD\nv1.1. Results are shown in Table 6. As we can see from\nTable 6, our pre-trained SegaBERT-base\u0000 outperforms our\npre-trained BERT-base\u0000 on both dataset: 1.3 EM and 0.8\nF1 improvements on SQUAD v1.1; 0.9 EM and 1.0 F1 im-\nprovements on SQUAD v2. It should be noticed that our\npre-trained BERT-base\u0000 outperforms the original BERT-\nbase model, although ours is pre-trained with fewer data and\nsteps. This conﬁrms Liu et al. (2019)’s ﬁnding that BERT\nModel Acc-Dev Acc-Test\nBERT-large 72.7 72.0\nSegaBERT-large 74.5 73.8\nTable 7: Accuracy on dev and test sets of RACE. Results of\nBERT-large are from Pan et al. (2019).\npre-trained with document-level input can contribute to per-\nformance improvements on SQUAD. For large models, as\nwe cannot afford to train a new BERT-large model in the\nsame setting as BERT-base\u0000, we compare our model with\nBERT-large wwm (with whole word masking), which is a\nstronger baseline model. We can see that SegaBERT large is\nslightly lower than BERT-large wwm on SQUAD v1.1 but\noutperforms it on SQUAD v2 over 1.2 EM and 1.8 F1.\nWe further test our models with RACE (Lai et al. 2017),\nwhich is a large-scale reading comprehension dataset with\nmore than 28,000 passages. RACE has signiﬁcantly longer\ncontexts than SQUAD. Our results are shown in Table 7. The\noverall trend is similar to SQUAD.\nVisualization We further visualize the self-attention\nscores of BERT-base \u0000 and SegaBERT-base\u0000 in different\nlayers. Figure 5 shows the average attention scores across\ndifferent attention heads. By comparing Figure 5(d) with\nFigure 5(a), we ﬁnd that SegaBERT can capture context ac-\ncording to the segmentation, for example, tokens tend to\nattend more to tokens in its paragraph than tokens in the\nother paragraphs. A similar trend can be observed at the sen-\ntence level but is more prominent in the shallow layers On\n12531\nthe other hand, the BERT model seems to pay more atten-\ntion to its neighbors: the attention weights of the elements\naround the main diagonal are larger than other positions in\nFigure 5(a), and a band-like contour around the main diago-\nnal can be observed in this ﬁgure.\nFrom Figure 5(f) and Figure 5(c), we can see the attention\nstructure in the ﬁnal layer is different from the shallow lay-\ners, and SegaBERT pays more attention to its context than\nBERT. We also notice that a fractal-like structure can be ob-\nserved in the ﬁrst 10 layers of SegaBERT, while the last two\nlayers of SegaBERT have a striped structure.\nThese attention behaviors show that: in the shallow layers,\nour model is segment-aware while BERT is neighborhood-\naware; in the top layers, both of these two models focus on\nsome tokens across the article rather than local neighbors,\nbut our model can capture more contextual tokens.\nRelated Work\nLanguage modeling is a traditional natural language pro-\ncessing task which requires capturing long-distance depen-\ndencies for predicting the next token based on the context.\nMost of the recent advances in language modeling are\nbased on the Transformer (Vaswani et al. 2017) decoder\narchitecture. Al-Rfou et al. (2019) demonstrated that self-\nattention can perform very well on character-level language\nmodeling. Baevski and Auli (2019) proposed adaptive word\ninput representations for the Transformer to assign more ca-\npacity to frequent words and reduce the capacity for less\nfrequent words. Dai et al. (2019) proposed Transformer-\nXL to equip the Transformer with relative position encod-\ning and cached memory for longer context modeling. Rae\net al. (2020) extended the Transformer-XL memory segment\nto ﬁne-grained compressed memory, which further increases\nthe length of the context and obtains a perplexity of 17.1 on\nWikiText-103.\nAlthough these works prove that longer context can be\nhelpful for the language modeling task, how to generate bet-\nter context representations with richer positional informa-\ntion has not been investigated.\nOn the other hand, large neural LMs trained with a mas-\nsive amount of text have shown great potential on many\nNLP tasks, beneﬁting from the dynamic contextual repre-\nsentations learned from language modeling and other self-\nsupervised pre-training tasks. GPT2 (Radford et al. 2019)\nand BERT (Devlin et al. 2019) are two representative mod-\nels trained with the auto-regressive language modeling task\nand the masked language modeling task, respectively. In ad-\ndition, BERT is also trained with an auxiliary task named\nnext sentence prediction (NSP). ALBERT (Lan et al. 2020)\nthen proposed to share parameters across layers of BERT\nand replaced NSP with sentence order prediction (SOP). Ac-\ncording to their experiments, SOP is more challenging than\nNSP, and MLM together with other downstream tasks can\nbeneﬁt more from replacing NSP with SOP. Concurrently to\nALBERT, Wang et al. (2020) proposed two auxiliary objec-\ntives to provide additional structural information for BERT.\nAll these powerful pre-trained models encode input to-\nkens with token position encoding, which was ﬁrst proposed\nby Vaswani et al. (2017) to indicate the position index of\nthe input tokens in the context of machine translation and\nconstituency parsing. After that, Transformer has been ex-\ntensively applied in machine translation and other sequence\ngeneration tasks (Li et al. 2019; Liu and Lapata 2019; Roller\net al. 2020). However, the input length of language mod-\neling tasks are much longer than these tasks, and simply\nassigning 0–512 token position embeddings is not enough\nfor LMs to learn the linguistic relationships among these to-\nkens. Bai et al. (2020) show that incorporating segmentation\ninformation with paragraph separating tokens can improve\nthe LM generator (GPT2) in the context of story genera-\ntion. However, compared with punctuation and paragraph\nbreaker, segment position indexes are more straightforward\nfor dot-product self-attention based Transformers. In this\nwork, we try to encode segmentation information into the\nTransformer with the segment-aware position encoding ap-\nproach.\nConclusion\nIn this paper, we propose a novel segment-aware Trans-\nformer that can encode richer positional information for lan-\nguage modeling. By applying our approach to Transformer-\nXL, we train a new language model, Segatron-XL, that\nachieves 17.1 test perplexity on WikiText-103. Addition-\nally, we pre-trained BERT with our SegaBERT approach\nand show that our model outperforms BERT on general\nlanguage understanding, sentence representation learning,\nand machine reading comprehension tasks. Furthermore,\nour SegaBERT-large model outperforms RoBERTa-large on\nzero-shot STS tasks. These experimental results demonstrate\nthat our proposed method works on both language models\nwith relative position embeddings and pre-trained language\nmodels with absolute position embeddings.\nAcknowledgments\nThis work was partially supported by NSERC\nOGP0046506, the National Key R&D Program of China\n2016YFB1000902 and 2018YFB1003202. We would like\nto thank Wei Zeng and his team in Peng Cheng Laboratory\n(PCL) for computing resources to support this project.\nReferences\nAl-Rfou, R.; Choe, D.; Constant, N.; Guo, M.; and Jones,\nL. 2019. Character-Level Language Modeling with Deeper\nSelf-Attention. In The Thirty-Third AAAI Conference on\nArtiﬁcial Intelligence, AAAI 2019, The Thirty-First Innova-\ntive Applications of Artiﬁcial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019, 3159–3166. AAAI\nPress.\nBaevski, A.; and Auli, M. 2019. Adaptive Input Represen-\ntations for Neural Language Modeling. In 7th International\nConference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net.\n12532\nBai, H.; Shi, P.; Lin, J.; Tan, L.; Xiong, K.; Gao, W.; Liu,\nJ.; and Li, M. 2020. Semantics of the Unwritten. Arixv\nabs/2004.02251.\nBentivogli, L.; Magnini, B.; Dagan, I.; Dang, H. T.; and\nGiampiccolo, D. 2009. The Fifth PASCAL Recognizing\nTextual Entailment Challenge. In TAC 2009, Gaithersburg,\nMaryland, USA, November 16-17, 2009.\nBird, S.; Klein, E.; and Loper, E. 2009. Natural Language\nProcessing with Python. O’Reilly.\nBowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D.\n2015. A large annotated corpus for learning natural lan-\nguage inference. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing, 632–\n642. Lisbon, Portugal: Association for Computational Lin-\nguistics.\nCer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,\nL. 2017. SemEval-2017 Task 1: Semantic Textual Similar-\nity Multilingual and Crosslingual Focused Evaluation. In\nProceedings of the 11th International Workshop on Seman-\ntic Evaluation (SemEval-2017), 1–14. Vancouver, Canada:\nAssociation for Computational Linguistics.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2978–2988. Florence, Italy: Associ-\nation for Computational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nDolan, W. B.; and Brockett, C. 2005. Automatically Con-\nstructing a Corpus of Sentential Paraphrases. In Proceed-\nings of the Third International Workshop on Paraphrasing\n(IWP2005).\nGrave, E.; Joulin, A.; and Usunier, N. 2017. Improving Neu-\nral Language Models with a Continuous Cache. In 5th In-\nternational Conference on Learning Representations, ICLR\n2017, Toulon, France, April 24-26, 2017, Conference Track\nProceedings. OpenReview.net.\nKrause, B.; Kahembwe, E.; Murray, I.; and Renals, S. 2018.\nDynamic Evaluation of Neural Sequence Models. In Dy,\nJ. G.; and Krause, A., eds., Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML 2018,\nStockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018,\nvolume 80 of Proceedings of Machine Learning Research,\n2771–2780. PMLR.\nLai, G.; Xie, Q.; Liu, H.; Yang, Y .; and Hovy, E. 2017.\nRACE: Large-scale ReAding Comprehension Dataset From\nExaminations. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, 785–\n794. Copenhagen, Denmark: Association for Computational\nLinguistics.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma,\nP.; and Soricut, R. 2020. ALBERT: A Lite BERT for\nSelf-supervised Learning of Language Representations. In\n8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\nOpenReview.net.\nLi, J.; Wang, X.; Yin, D.; and Zong, C. 2019. Attribute-\naware Sequence Network for Review Summarization. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3000–3010. Hong Kong, China: Asso-\nciation for Computational Linguistics.\nLiu, Y .; and Lapata, M. 2019. Text Summarization with\nPretrained Encoders. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 3730–3740. Hong\nKong, China: Association for Computational Linguistics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. Arixv abs/1907.11692.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.\nPointer Sentinel Mixture Models. In 5th International Con-\nference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nPan, X.; Sun, K.; Yu, D.; Chen, J.; Ji, H.; Cardie, C.; and\nYu, D. 2019. Improving Question Answering with External\nKnowledge. In Proceedings of the 2nd Workshop on Ma-\nchine Reading for Question Answering, 27–37. Hong Kong,\nChina: Association for Computational Linguistics.\nRadford, A. 2018. Improving Language Understanding by\nGenerative Pre-Training. OpenAI blog. Accessed: 2018-06-\n11.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI blog. Accessed: 2019-02-14.\nRae, J. W.; Dyer, C.; Dayan, P.; and Lillicrap, T. P. 2018.\nFast Parametric Learning with Activation Memorization. In\nDy, J. G.; and Krause, A., eds., Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML 2018,\nStockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018,\nvolume 80 of Proceedings of Machine Learning Research,\n4225–4234. PMLR.\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; Hillier, C.;\nand Lillicrap, T. P. 2020. Compressive Transformers for\nLong-Range Sequence Modelling. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\n12533\nRajpurkar, P.; Jia, R.; and Liang, P. 2018. Know What You\nDon’t Know: Unanswerable Questions for SQuAD. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), 784–\n789. Melbourne, Australia: Association for Computational\nLinguistics.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension\nof Text. In Proceedings of the 2016 Conference on Empir-\nical Methods in Natural Language Processing, 2383–2392.\nAustin, Texas: Association for Computational Linguistics.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT:\nSentence Embeddings using Siamese BERT-Networks. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3982–3992. Hong Kong, China: Asso-\nciation for Computational Linguistics.\nRoller, S.; Dinan, E.; Goyal, N.; Ju, D.; Williamson, M.; Liu,\nY .; Xu, J.; Ott, M.; Shuster, K.; Smith, E. M.; et al. 2020.\nRecipes for building an open-domain chatbot.arXiv preprint\narXiv:2004.13637 .\nRoy, A.; Saffar, M.; Vaswani, A.; and Grangier, D. 2020. Ef-\nﬁcient Content-Based Sparse Attention with Routing Trans-\nformers. arXiv abs/2003.05997.\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Models\nfor Semantic Compositionality Over a Sentiment Treebank.\nIn Proceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing, 1631–1642. Seattle,\nWashington, USA: Association for Computational Linguis-\ntics.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Guyon, I.; von Luxburg, U.;\nBengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.\nV . N.; and Garnett, R., eds., Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, 5998–6008.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\n7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Open-\nReview.net.\nWang, W.; Bi, B.; Yan, M.; Wu, C.; Xia, J.; Bao, Z.; Peng,\nL.; and Si, L. 2020. StructBERT: Incorporating Language\nStructures into Pre-training for Deep Language Understand-\ning. In 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nWarstadt, A.; Singh, A.; and Bowman, S. R. 2019. Neural\nNetwork Acceptability Judgments. Transactions of the As-\nsociation for Computational Linguistics 7: 625–641.\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), 1112–1122. New Orleans, Louisiana:\nAssociation for Computational Linguistics.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J. G.; Salakhutdinov,\nR.; and Le, Q. V . 2019. XLNet: Generalized Autoregres-\nsive Pretraining for Language Understanding. In Wallach,\nH. M.; Larochelle, H.; Beygelzimer, A.; d’Alch ´e-Buc, F.;\nFox, E. B.; and Garnett, R., eds., Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, 5754–5764.\nZhu, Y .; Kiros, R.; Zemel, R. S.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning Books and\nMovies: Towards Story-Like Visual Explanations by Watch-\ning Movies and Reading Books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015, Santi-\nago, Chile, December 7-13, 2015, 19–27. IEEE Computer\nSociety.\n12534",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7893688082695007
    },
    {
      "name": "Transformer",
      "score": 0.7566102147102356
    },
    {
      "name": "Language model",
      "score": 0.7456688284873962
    },
    {
      "name": "Security token",
      "score": 0.7156796455383301
    },
    {
      "name": "Perplexity",
      "score": 0.6375532150268555
    },
    {
      "name": "Sentence",
      "score": 0.5665565729141235
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5396736860275269
    },
    {
      "name": "Natural language processing",
      "score": 0.5104848146438599
    },
    {
      "name": "Speech recognition",
      "score": 0.3423864245414734
    },
    {
      "name": "Voltage",
      "score": 0.09136945009231567
    },
    {
      "name": "Engineering",
      "score": 0.09095838665962219
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}