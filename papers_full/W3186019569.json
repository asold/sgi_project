{
    "title": "LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation",
    "url": "https://openalex.org/W3186019569",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2099669530",
            "name": "Xu Guoping",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1509959744",
            "name": "Wu Xing-rong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1902469158",
            "name": "Zhang Xuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2656119552",
            "name": "He Xinwei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2964227007",
        "https://openalex.org/W2907965334",
        "https://openalex.org/W3112503277",
        "https://openalex.org/W2907750714",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3040304705",
        "https://openalex.org/W2419448466",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2963881378",
        "https://openalex.org/W3034399482",
        "https://openalex.org/W2464708700",
        "https://openalex.org/W3146091044",
        "https://openalex.org/W2809446072",
        "https://openalex.org/W2999972254",
        "https://openalex.org/W3045264897",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W3025670151",
        "https://openalex.org/W2899279931",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W2963800917",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2395611524",
        "https://openalex.org/W3044738063",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3097510987",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W3031562395",
        "https://openalex.org/W2969825080",
        "https://openalex.org/W3021040258",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W3023158620",
        "https://openalex.org/W3160284783",
        "https://openalex.org/W2804488433",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W3021822555"
    ],
    "abstract": "Medical image segmentation plays an essential role in developing computer-assisted diagnosis and therapy systems, yet still faces many challenges. In the past few years, the popular encoder-decoder architectures based on CNNs (e.g., U-Net) have been successfully applied in the task of medical image segmentation. However, due to the locality of convolution operations, they demonstrate limitations in learning global context and long-range spatial relations. Recently, several researchers try to introduce transformers to both the encoder and decoder components with promising results, but the efficiency requires further improvement due to the high computational complexity of transformers. In this paper, we propose LeViT-UNet, which integrates a LeViT Transformer module into the U-Net architecture, for fast and accurate medical image segmentation. Specifically, we use LeViT as the encoder of the LeViT-UNet, which better trades off the accuracy and efficiency of the Transformer block. Moreover, multi-scale feature maps from transformer blocks and convolutional blocks of LeViT are passed into the decoder via skip-connection, which can effectively reuse the spatial information of the feature maps. Our experiments indicate that the proposed LeViT-UNet achieves better performance comparing to various competing methods on several challenging medical image segmentation benchmarks including Synapse and ACDC. Code and models will be publicly available at https://github.com/apple1986/LeViT_UNet.",
    "full_text": "LEVIT-UNET: M AKE FASTER ENCODERS WITH TRANSFORMER\nFOR MEDICAL IMAGE SEGMENTATION\nGuoping Xu\nSchool of Computer Sciences and Engineering\nWuhan Institute of Technology\nWuhan, Hubei, China, 430205\nxugp2006@126.com\nXingrong Wu*\nSchool of Computer Sciences and Engineering\nWuhan Institute of Technology\nWuhan, Hubei, China, 430205\nxwu@wit.edu.cn\nXuan Zhang\nSchool of Computer Sciences and Engineering\nWuhan Institute of Technology\nWuhan, Hubei, China, 430205\nXinwei He\nSchool of Electronic Information and Communications\nHuazhong University of Science and technology\nWuhan, Hubei, China, 430074\nJuly 20, 2021\nABSTRACT\nMedical image segmentation plays an essential role in developing computer-assisted diagnosis and\ntherapy systems, yet still faces many challenges. In the past few years, the popular encoder-decoder\narchitectures based on CNNs (e.g., U-Net) have been successfully applied in the task of medical\nimage segmentation. However, due to the locality of convolution operations, they demonstrate\nlimitations in learning global context and long-range spatial relations. Recently, several researchers\ntry to introduce transformers to both the encoder and decoder components with promising results, but\nthe efﬁciency requires further improvement due to the high computational complexity of transformers.\nIn this paper, we propose LeViT-UNet, which integrates a LeViT Transformer module into the U-Net\narchitecture, for fast and accurate medical image segmentation. Speciﬁcally, we use LeViT as the\nencoder of the LeViT-UNet, which better trades off the accuracy and efﬁciency of the Transformer\nblock. Moreover, multi-scale feature maps from transformer blocks and convolutional blocks of\nLeViT are passed into the decoder via skip-connection, which can effectively reuse the spatial\ninformation of the feature maps. Our experiments indicate that the proposed LeViT-UNet achieves\nbetter performance comparing to various competing methods on several challenging medical image\nsegmentation benchmarks including Synapse and ACDC. Code and models will be publicly available\nat https://github.com/apple1986/LeViT_UNet.\n1 Introduction\nAutomated medical image segmentation has been widely studied in the medical image analysis community which would\nsigniﬁcantly reduce the amount of tedious and error-prone work by radiologists. In the past few years, Convolutional\nNeural Networks (CNNs) have made substantial progress in medical image segmentation. Fully convolutional networks\n(FCNs)[1] and its variants (e.g., U-Net[2], SegNet[3], DeepLab[4], CCNet[5]) are extensively used architectures. They\nhave been applied in cardiac segmentation from MRI[ 6], liver and tumor segmentation from CT[ 7], and abnormal\nlymph nodes segmentation from PET/CT[8] and etc.\nAlthough powerful representation learning capabilities, local translation invariance and ﬁlter sharing properties have\nmade CNN-based approaches the de facto selection for image segmentation, they still have their own limitations. For\ninstance, the insufﬁcient capability to capture explicit global context and long-range relations owing to the intrinsic\nlocality of convolution operations. Some studies tried to employ dilated convolution[ 4], image pyramids[9], prior-\nguided[6, 10, 11], multi-scale fusion[12, 13], and self-attention mechanisms[14, 15] based CNN features to address\narXiv:2107.08623v1  [cs.CV]  19 Jul 2021\nA PREPRINT - JULY 20, 2021\nthese limitations. However, these studies exist weakness to extract global context features in the task of medical image\nsegmentation, especially for the objects that have large inter-patient variation in terms of shape, scale and texture.\nTransformers[16], initially is proposed for sequence-to-sequence modeling in nature language processing (NLP) tasks,\nsuch as machine translation, sentiment analysis, information extraction, and etc. Recently, the vision transformer (ViT)\narchitecture [17, 18, 19], which tries to apply transformer to vision tasks, has achieved state-of-the-art results for image\nclassiﬁcation via pre-training on the large-scale dataset. Later, Transformer-based architectures have also been studied\nfor semantic segmentation, such as SETR[20], Swin Transformer[21], Swin-UNet[22], TransUNet[23]. However, the\nmain limitation of these Transformer-based methods lies in the high requirement of computation power, which impedes\nthem to run in real-time applications, for example, radiotherapy.\nRecently, LeViT[19] is proposed for fast inference image classiﬁcation with hybrid transformer and convolution blocks,\nwhich optimizes the trade-off between accuracy and efﬁciency. However, this architecture has not fully leveraged\nvarious scales of feature maps from transformer and convolution blocks, which are conducive to image segmentation.\nInspired by the LeViT, we propose LeViT-UNet for 2D medical image segmentation in this paper, which aims to\nmake faster encoder with transformer and improve the segmentation performance. To the best of our knowledge,\nLeViT-UNet is the ﬁrst work that studies the speed and accuracy with transformer-based architecture for the medical\nimage segmentation task. A comparison of the speed and performance operated in various convolution-based and\ntransformer-based methods for Synapse dataset in shown in Figure 1. We can see that the our LeViT-UNets achieve\ncompetitive performance compared the fast CNN-based models. Meanwhile, performance of LeViT-UNet-384 surpasses\nthe previous state-of-the-art transformer-based method, such as TransUnet and Swin-UNet.\n(a) DSC vs Speed\n (b) HD vs Speed\nFigure 1: Speed and accuracy for convolution-based and visual transformers-based method, testing on Synapse dataset.\nLeft: The speed and Dice similarity coefﬁcient (DSC). Right: The speed and Hausdorff distance (HD).\nThe proposed LeViT-UNet consists of an encoder, a decoder and several skip connections. Here, the encoder is\nbuilt based on LeViT transformer blocks, and the decoder is built based on convolution blocks. Motivated by the\nU-shape architecture design, the various resolution feature maps, which are extracted from the transformer blocks of\nLeViT is then upsampled, are concatenated and passed into decode blocks with skip connections. We ﬁnd that such\ndesign could integrate the merits of the Transformer for global features extraction and the CNNs for local feature\nrepresentation. Our experiments demonstrate that LeViT-UNet could improve both accuracy and efﬁciency of the\nmedical image segmentation task. The main contributions of our work can be summarized as follows: (1) We propose a\nnovel light-weight, fast and high accuracy transformer-based segmentation architecture, named LeViT-UNet, which\nintegrates a multi-stage transformer block in the encoder with LeViT; (2) We present a new method to fuse multi-scale\nfeature maps extracted from the transformer and convolutional blocks, which could sufﬁciently integrate the global and\nlocal features in various scales; (3) Extensive experiments are conducted which demonstrated that the proposed method\nis competitive with other state-of-the-art methods in terms of accuracy and efﬁciency.\n2\nA PREPRINT - JULY 20, 2021\n2 Related Works\nCNN-based methods: CNNs served as the standard network model have been extensively studied in medical image\nsegmentation. The typical U-shaped network, U-Net[2], which consists of a symmetric encoder and decoder network\nwith skip connections, has become the de-facto choice for medical image analysis. Afterwards, various U-Net like\narchitectures is proposed, such as Res-UNet[24], Dense-UNet[25], V-Net[26] and 3D-UNet[27]. While CNN-based\nmethods have achieved much progress in medical image segmentation, they still cannot fully meet the clinical application\nrequirements for segmentation accuracy and efﬁciency owing to its intrinsic locality of convolution operations and its\ncomplex data access patterns.\nSelf-attention mechanisms to complement CNNs:Several works have attempted to integrate self-attention mech-\nanism into CNNs for segmentation. The main purpose is to catch the attention weight in terms of channel-wise or\nspatial shape. For instance, the squeeze-and-excite network built an attention-like module to extract the relationship\nbetween each feature map of a layer[28]. The dual attention network appended two types of attention modules to model\nthe semantic interdependencies in spatial and channel dimensions respectively[29]. The Attention U-Net proposed an\nattention gate to suppress irrelevant regions of a feature map while highlighting salient features for segmentation task.\nAlthough these strategies could improve the performance of segmentation, the ability of extracting long-rang semantic\ninformation still need to be addressed.\nTransformers: Recently, Vision Transformer (ViT) achieved state-of-the-art on ImageNet classiﬁcation by using\ntransformer with pure self-attention to input images[17]. Afterward, different ViT variants have been proposed, such as\nDeiT[18], Swin[21], and LeViT[19]. Some works attempted to apply transformer structure to medical segmentation.\nFor example, Medical Transformer (MedT) introduced the gated axial transformer layer into existing architecture.\nTransUNet[23] integrated the Transformers into U-Net, which utilized the advantage from both Transformers and CNN.\nSwin-UNet[22] was proposed which employed pure transformer into the U-shaped encoder-decoder architecture for\nglobal semantic feature learning. In this paper, we attempt to apply LeViT transformer block as basic unit in the encoder\nof a U-shaped architecture, which trade-off the accuracy and efﬁciency for medical image segmentation. Our work\nwill likely provide a benchmark comparison for the fast segmentation with Transformer in the ﬁeld of medical image\nanalysis.\n3 Method\nGiven an input image of height (H) x width (W) x channel (C), the goal of the image segmentation task is to predict the\ncorresponding pixel-wise label of H x W. Unlike the conventional UNet which employs convolutional operations to\nencode and decode features, we apply LeViT module in the encoder part to extract the features and keep the decoder\npart same as UNet. In the following part, we will introduce the overall LeViT-UNet architecture in Section 3.1. Then,\nthe component of encoder and decoder in the LeViT-UNet will be elaborated in Section 3.2 and 3.3, respectively.\n3.1 The overall Architecture of LeViT-UNet\nThe architecture of LeViT-UNet is present in Figure 2. It is composed of an encoder and a decoder. Here, we apply\nLeViT module in the encoder part to extract long-range structural information from the feature maps. The LeViT is a\nhybrid neural network which is composed of convnets and vision transformers.\n3.2 LeViT as Encoder\nFollowing [19], we apply LeViT architecture as the encoder, which consists of two main parts of components:\nconvolutional blocks and transformer blocks. Speciﬁcally, there are 4 layers of 3x3 convolutions with stride 2 in the\nconvolutional blocks, which could perform the resolution reduction. These feature maps will be fed into the transformer\nblock, which could decrease the number of ﬂoating-point operations (FLOPs) that is known large in transformer\nblocks. Depending on the number of channels fed into the ﬁrst transformer block, we design three types of LeViT\nencoder, which are named as LeViT-128s, LeViT-192 and LeViT-384, respectively. The block diagram of the LeViT-192\narchitecture is shown in Figure 3. Note that we concatenate the features from convolution layers and transformer blocks\nin the last stage of the encoder, which could fully leverage the local and global features in various scales.\nThe transformer block can be formulated as:\nˆzn = MLP (BN (zn−1)) +zn−1, (1)\n3\nA PREPRINT - JULY 20, 2021\nFigure 2: The architecture of LeViT-UNet, which is composed of encoder (LeViT block), decoder and skip connection.\nHere, the encoder is constructed based on LeViT module.\nFigure 3: Figure 3: Block diagram of LeViT-192 architecture. A sampling is applied before transformation in the\nsecond and third Trans-Block, respectively.\n4\nA PREPRINT - JULY 20, 2021\nzn = MSA (BN (ˆzn)) + ˆzn, (2)\nWhere ˆzn and zn represent the outputs of MLP (Multiple Layer Perceptron) module and the MSA (Multi-head\nAttention) module of the nth block, respectively. BN means the batch normalization. Similar to the previous work [19],\nself-attention is computed as follows:\nAttention(Q, K, V) = Softmax (QKT\n√\nd\n+ B)V, (3)\nWhere Q, K, V are the query, key and value matrices, whose sizes are M2xd. M2 and d denote the number of patches\nand the dimension of the query or key. B represents attention bias, which takes place of positional embedding and\ncould provide positional information within each attention block.\n3.3 CNNs as Decoder\nSimilar to U-Net, we concatenate the features from the decoder with skip connection. The cascaded upsampling strategy\nis used to recover the resolution from the previous layer using CNNs. For example, there are feature maps with the\nshape of H/16 x w/16 x D from the encoder. Then, we use cascaded multiple upsampling blocks for reach the full\nresolution of H x W, where each block consists of two 3x3 convolution layers, batch normalization layer, ReLU layer,\nand an upsampling layer.\n4 Experiments and Results\n4.1 Dataset\nSynapse multi-organ segmentation dataset (Synapse):This dataset includes 30 abdominal CT scans with 3779\naxial contrast-enhanced abdominal clinical CT images in total. Following the splitting in [23], we use 18 cases for\ntraining and the other 12 cases for validation. We evaluated the performance with the average Dice Similarity Coefﬁcient\n(DSC) and the average Hausdorff Distance (HD) on 8 abdominal organs, which are aorta, gallbladder, spleen, left\nkidney, right kidney, liver, pancreas, spleen, and stomach respectively.\nAutomated cardiac diagnosis challenge dataset (ACDC):the ACDC dataset is collected from 150 patients using\ncine-MR scanners, splitting into 100 volumes with human annotations and the other 50 volumes which are private for\nthe evaluation purpose. Here, we split the 100 annotated volumes into 80 training samples and 20 testing samples.\n4.2 Implementation details\nIn this paper, we run all experiments based on Python 3.8, PyTorch 1.8.0 and Ubuntu 18.04.1 LTS. For all training\nsamples, we apply augmentation strategies such as random ﬂipping and rotations to increase data diversity. All trainings\nare performed on images size of 224x224 by using a Nvidia 3090 GPU with 24GB memory. For all models are trained\nby Adam optimizer with learning rate 1e-5 and weight decay of 1e-4. The cross entropy and Dice loss are use as\nobjective function. We set input resolution 224x224, and batch size of 8. The training epochs are set as 350 and 400 for\nSynapse and ACDC dataset respectively. All transformer backbones in the LeViT were pretrained on ImageNet-1k to\ninitialize the model parameters. Following [23], all 3D volume datasets are trained by slice and the predicted 2D slice\nare stacked together to build 3D prediction for evaluation.\n4.3 Experiment results on Synapse dataset\nWe perform experiments with other state-of-the-art (SOTA) methods in terms of accuracy and efﬁciency as the\nbenchmark for comparison with LeViT-UNet. Three variants of LeViT-UNet were designed. We identify them by the\nnumber of channels input to the ﬁrst transformer block: LeViT-UNet-128s, LeViT-UNet-192, and LeViT-UNet-384,\nrespectively. Following to [22][23], we report the average DSC and HD to evaluate our method on this dataset to\ndemonstrate the generalization ability and robustness of our proposed method.\n4.3.1 Compare state-of-the-art methods\nThe comparison of the proposed LeViT-UNet with other SOTA methods on the Synapse multi-organ CT dataset can\nbe observed in Table 1. Experimental results show that LeViT-UNet-384 achieves the best performance in terms\n5\nA PREPRINT - JULY 20, 2021\nFigure 4: Figure 4: Qualitative comparison of various methods by visualization From Left to right: Ground Truth,\nLeViT-UNet-384, TransUNet, UNet, and DeepLabv3+.\nTable 1: Segmentation accuracy (average DSC% and average HD in mm, and DSC for each organ) of different methods\non the Synapse multi-organ CT dataset.\nMethods DSC↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nV-Net [26] 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98\nDARR [30] 69.77 - 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96\nU-Net [2] 76.85 39.70 89.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58\nR50 U-Net [23] 74.68 36.87 87.74 63.66 80.60 78.19 93.74 56.90 85.87 74.16\nR50 Att-UNet [23] 75.57 36.97 55.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95\nAtt-UNet [14] 77.77 36.02 89.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75\nR50-Deeplabv3+ [4] 75.73 26.93 86.18 60.42 81.18 75.27 92.86 51.06 88.69 70.19\nR50 ViT [23] 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nTransUnet [23] 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nSwinUnet [21] 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nLeVit-UNet-128s 73.69 23.92 86.45 66.13 79.32 73.56 91.85 49.25 79.29 63.70\nLeVit-UNet-192 74.67 18.86 85.69 57.37 79.08 75.90 92.05 53.53 83.11 70.61\nLeVit-Unet-384 78.53 16.84 87.33 62.23 84.61 80.25 93.11 59.07 88.86 72.76\nof average HD with 16.84 mm, which is improved by about 14.8 mm and 4.7 mm comparing the recently SOTA\nmethods. It indicates that our approach can obtain better edge predictions. Comparing the transformer-based method,\nlike TransUNet and SwinUNet, and other convolution-based method, like U-Net and Att-UNet, our approach still could\nachieve the competition result in terms of DSC.\nThe segmentation results of different methods on the Synapse dataset are shown in the Figure 4. We can see that\nthe other three methods are more likely to under-segment or over segment the organs, for example, the stomach is\nunder-segmented by TransUNet and DeepLabV3+ (as indicated by the red arrow in the third panel of the upper row),\nand over-segmented by UNet (as indicated by the red arrow in the fourth panel of the second row). Moreover, results in\nthe third row demonstrate that our LeViT-UNet outputs are relatively smoother than those from other methods, which\nindicates that our method has more advantageous in boundary prediction.\n4.3.2 Compare with fast segmentation methods\nFirstly, it can be seen that LeViT-UNet-384 achieves 78.53% mDSC and 16.84mm mHD, which is the best among all\nmethods in Table 2. Particularly, we can ﬁnd our proposed method is much faster than TransUNet, which integrates\nTransformer block into CNN. Then, to demonstrate the performance of accuracy-efﬁciency, we compare LeViT-UNet\nwith other fast segmentation methods, such as ENet, FSSNet, FastSCNN and etc. In terms of the amount of parameters,\n6\nA PREPRINT - JULY 20, 2021\nTable 2: Mean DSC and HD of the proposed LeViT-UNet compared to other state-of-the-art semantic segmentation\nmethods on the Synapse dataset in terms of parameters and inference speed by FPS (frame per second). Number of\nparameters are listed in millions.\nMethods DSC↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach# params(M) FLOPs(G) FPS\nCGNet [31] 75.08 24.99 83.48 65.32 77.91 72.04 91.92 57.37 85.47 67.15 0.49 0.66 124\nContextNet [32]71.17 36.41 79.92 51.17 77.58 72.04 91.74 43.78 86.65 66.51 0.87 0.16 280\nDABNet [33] 74.91 26.39 85.01 56.89 77.84 72.45 93.05 54.39 88.23 71.45 0.75 0.99 221\nEDANet [34] 75.43 29.31 84.35 62.31 76.16 71.65 93.20 53.19 85.47 77.12 0.69 0.85 213\nENet [35] 77.63 31.83 85.13 64.91 81.10 77.26 93.37 57.83 87.03 74.41 0.36 0.50 141\nFPENet [36] 68.67 42.39 78.98 56.35 74.54 64.36 90.86 40.60 78.30 65.35 0.11 0.14 160\nFSSNet [37] 74.59 35.16 82.87 64.06 78.03 69.63 92.52 53.10 85.65 70.86 0.17 0.33 213\nSQNet [38] 73.76 40.29 83.55 61.17 76.87 69.40 91.53 56.55 85.82 65.24 16.25 18.47 241\nFastSCNN [39]70.53 32.79 77.79 55.96 73.61 67.38 91.68 44.54 84.51 68.76 1.14 0.16 292\nTransUNet [23]77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62 105.28 24.64 50\nLeViT-UNet-128s73.69 23.92 86.45 66.13 79.32 73.56 91.85 49.25 79.29 63.70 15.91 17.55 114\nLeViT-UNet-19274.67 18.86 85.69 57.37 79.08 75.90 92.05 53.53 83.11 70.61 19.90 18.92 95\nLeViT-UNet-38478.53 16.84 87.33 62.23 84.61 80.25 93.11 59.07 88.86 72.76 52.17 25.55 85\nTable 3: Ablation study w/o Transformer blocks.\nMethods DSC↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach# params(M) FLOPs(G)\nLeViT-UNet-128s-Conv72.44 41.63 84.29 59.11 77.70 69.20 91.93 44.18 87.60 65.52 5.46 14.54\nLeViT-UNet-192-Conv74.42 35.41 85.34 62.90 81.39 72.80 91.76 44.95 88.84 67.36 5.97 15.30\nLeViT-UNet-384-Conv74.59 30.19 85.49 62.52 83.00 73.87 91.91 43.47 88.75 67.69 11.94 17.70\nLeViT-UNet-128s73.69 23.92 86.45 66.13 79.32 73.56 91.85 49.25 79.29 63.70 15.91 17.55\nLeViT-UNet-192 74.67 18.86 85.69 57.37 79.08 75.90 92.05 53.53 83.11 70.61 19.90 18.92\nLeViT-UNet-384 78.53 16.84 87.33 62.23 84.61 80.25 93.11 59.07 88.86 72.76 52.17 25.55\nTable 4: Ablation study on the number of skip-connection in LeViT-UNet. ( ’_N’ means the number of skip connections)\nNumber of skip-connectionDSC↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nLeViT-UNet-384_N0 67.190 27.887 73.700 47.080 69.850 65.030 89.920 45.530 82.220 64.180\nLeViT-UNet-384_N1 68.720 27.973 73.590 48.730 75.050 67.960 91.150 45.030 84.130 64.090\nLeViT-UNet-384_N2 74.540 25.845 84.980 59.270 75.430 69.160 92.530 57.200 87.180 70.580\nLeViT-UNet-384_N3 76.910 20.866 86.890 61.010 81.570 76.180 92.860 56.000 87.620 73.190\nLeViT-UNet-384_N4 78.530 16.838 87.330 62.230 84.610 80.250 93.110 59.070 88.860 72.760\nour method is still needed to be improved, comparing to other fast segmentation methods, like CGNet, ContextNet\nand ENet. However, our method has much fewer parameters than TransUNet. Moreover, we evaluate the runtime at\ndifferent methods. Here, ENet (114 fps) and FPENet (160 fps) are slightly faster than LeViT-UNet-128s (114 fps), yet\nthe HD are still needed to improve. Therefore, we conclude that LeViT-UNet is competitive with the current pure CNN\nefﬁcient segmentation method with better performance.\n4.3.3 Ablation study\nWe conduct a variety of ablation studies to thoroughly evaluate the proposed LeViT-UNet architecture and validate\nthe performance under different settings, including: 1) without and with transformer blocks; 2) the number of skip-\nconnections; 3) without and with pretraining.\nEffect of the number of transformer blocks:Here, we compare the performance when Transformer blocks are\nutilized or not. We can see that adding transformer blocks leads to a better segmentation performance in terms of\nDSC and HD in the Table 3. These results show that the transformer block could improve performance owing to its\ninnate global self-attention mechanisms. Moreover, the channel number of feature maps that input to the transformer\nblock could improve the HD performance signiﬁcantly. It reduced the HD about 7.08mm and 11.44mm with/ without\ntransformer blocks respectively from the channel number of 128 to 384. Meanwhile, we ﬁnd that the number of channels\ngives more inﬂuence on the LeViT-UNet method than LeViT-UNet, which did not include transformer blocks. It can\nbe seen that the DSC is boosted to 1.25%, 0.25%, and 4.84% with transformer blocks, respectively. Particularly, the\nperformance of HD is improved to 17.71mm, 16.55 and 13.35 from LeViT-UNet 128s to LeViT-UNet-384, respectively\nEffect of the number of skip connections:We investigate the inﬂuence of skip-connections on LeViT-UNet. The\nresults can be seen in Table 4. Note that “1-skip” setting means that we only apply one time of skip-connection at the\n1/2 resolution scale, and “2-skip”, “3-skip” and “4-skip” are inserting skip-connections at 1/4, 1/8 and 1/16, respectively.\nWe can ﬁnd that adding more skip-connections could result in better performance. Moreover, the performance gain\nof smaller organs, like aorta, gallbladder, kidneys, is more obvious than that of larger organs, like liver, spleen and\nstomach.\n7\nA PREPRINT - JULY 20, 2021\nTable 5: Ablation study of inﬂuence of pretrained strategy. (’-N’ means without pretraining on ImageNet)\nMethods DSC↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nLeViT-UNet-128s-N 76.30 23.77 85.49 62.91 82.74 75.57 93.03 54.76 87.16 68.75\nLeViT-UNet-192-N 76.88 23.44 86.91 66.24 82.80 75.75 92.89 50.81 88.61 71.01\nLeViT-UNet-384-N 77.98 23.69 86.05 65.99 82.89 76.99 93.24 58.01 89.78 70.89\nLeViT-UNet-128s 73.69 23.92 86.45 66.13 79.32 73.56 91.85 49.25 79.29 63.70\nLeViT-UNet-192 74.67 18.86 85.69 57.37 79.08 75.90 92.05 53.53 83.11 70.61\nLeViT-UNet-384 78.53 16.84 87.33 62.23 84.61 80.25 93.11 59.07 88.86 72.76\nTable 6: Segmentation performance of different methods on the ACDC dataset.\nMethods DSC↑ RV Myo LV\nR50 U-Net 87.55 87.10 80.63 94.92\nR50 Att-UNet 86.75 87.58 79.20 93.47\nR50 ViT 87.57 86.07 81.88 94.75\nTransUnet 89.71 88.86 84.53 95.73\nSwinUnet 90.00 88.55 85.62 95.83\nLeViT-UNet-128s 89.39 88.16 86.97 93.05\nLeViT-UNet-192 90.08 88.86 87.50 93.87\nLeViT-UNet-384 90.32 89.55 87.64 93.76\nEffect of pre-training:The pre-training affected the performance of Transformer-based models, which can be attributed\nthat they do not have an inductive bias to focus on nearby image elements[ 19]. Hence, a large dataset is needed to\nregularize the model. Interestingly, we found that pre-training did not cause much inﬂuence of performance with\nLeViT-UNet, especially on the evaluation of DSC. We can see that the DSC is higher without pre-training by the\nLeViT-UNet-128s and LeViT-UNet-192. However, as the LeViT-UNet-384, we found that the pre-training is helpful\nto improve the performance. It indicated that the pre-training causes much inﬂuence to the transformer-based model\nwhich have larger parameters, like LeViT-UNet-384, which has about 52.1 million parameters, in contrast with 15.9\nmillion and 19.9 million parameters in LeViT-UNet-128s and LeViT-UNet-192, respectively.\n4.4 Experiment results on ACDC dataset\nTo demonstrate the generalization ability of LeViT-UNet, we train our model on ACDC MR dataset for automated\ncardiac segmentation. We can observe that our proposed LeViT-UNet could achieve the better results in terms of DSC in\nthe Table 6. Compared with Swin-UNet[22] and TransUNet[23], we can see that our LeViT-UNet achieve comparable\nDSC; for instance, the LeViT-UNet-192 and LeViT-Unet-384 achieve 90.08% and 90.32% DSC.\n4.5 Discussion\nIn this work, we apply LeViT as theencoder into UNet architecture. The feature maps from three Transformer blocks are\ndirectly concatenated after upsampling. In the future work, we will explore the ways to fuse multi-scale global feature\nmaps from Transformer blocks. Moreover, the resolution of input image is down-scaled to 1/16 before the Transformer\nblocks in order to reduce the computation complexity, which may have effect on the performance of segmentation. We\nexpect to design more efﬁcient architectures that could keep that balance between the speed and the accuracy by using\nTransformer-based methods. Lastly, we would like to explore the applications of LeViT-UNet in 3D medical image\nsegmentation.\n5 Conclusion\nTransformers are good at modeling long-range dependency with self-attention mechanism. In this paper, we present\nthe ﬁrst study that integrate LeViT into UNet-like architecture for the general medical image segmentation task. The\nproposed LeViT-UNet makes fully leverage of the advantage of Transformers to build strong global context while\nkeeping the merit of CNN to extract low-level features. Extensive experiments demonstrate that compared with current\nSOTA methods, the proposed LeViT-UNet has superior performance and good generalization ability. Moreover, the\nproposed LeViT-UNet shows the ability of trade-off between accuracy and efﬁciency. In the future, we’d like to optimize\nfurther the structure of LeViT-UNet, which could compete with other CNN-based fast segmentation methods.\n8\nA PREPRINT - JULY 20, 2021\nReferences\n[1] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,”IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 640–651, 2015.\n[2] O. Ronneberger, P.Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,”\nin Medical Image Computing and Computer-Assisted Intervention (MICCAI), ser. LNCS, vol. 9351. Springer,\n2015, pp. 234–241.\n[3] V . Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional encoder-decoder architecture for\nimage segmentation,”IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2017.\n[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully connected crfs,”IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 40, no. 4, pp. 834–848, 2018.\n[5] Z. Huang, X. Wang, Y . Wei, L. Huang, and T. S. Huang, “Ccnet: Criss-cross attention for semantic segmentation,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PP, no. 99, pp. 1–1, 2020.\n[6] F. Cheng, C. Chen, Y . Wang, H. Shi, Y . Cao, D. Tu, C. Zhang, and Y . Xu, “Learning directional feature maps for\ncardiac mri segmentation,” 2020.\n[7] Q. Jin, Z. Meng, C. Sun, H. Cui, and R. Su, “Ra-unet: A hybrid deep attention-aware network to extract liver and\ntumor in ct scans,”Frontiers in Bioengineering and Biotechnology, vol. 8, p. 1471, 2020.\n[8] G. Xu, J. K. Udupa, Y . Tong, D. Odhner, H. Cao, and D. A. Torigian, “Aar-ln-dq: Automatic anatomy recognition\nbased disease quantiﬁcation in thoracic lymph node zones via fdg pet/ct images without nodal delineation,”\nMedical Physics, vol. 47, no. 8, pp. 3467–3484, 2020.\n[9] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,” in 2017 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017, pp. 6230–6239.\n[10] S. Li, C. Zhang, and X. He, “Shape-aware semi-supervised 3d semantic segmentation for medical images,”Lecture\nNotes in Computer Science, p. 552–561, 2020. [Online]. Available: http://dx.doi.org/10.1007/978-3-030-59710-8_\n54\n[11] J. Wan, Y . Liu, D. Wei, X. Bai, and Y . Xu, “Super-bpd: Super boundary-to-pixel direction for fast image\nsegmentation,” 2020.\n[12] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y . Zhao, D. Liu, Y . Mu, M. Tan, X. Wang, W. Liu, and B. Xiao,\n“Deep high-resolution representation learning for visual recognition,” 2020.\n[13] B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang, “Higherhrnet: Scale-aware representation learning\nfor bottom-up human pose estimation,” 2020.\n[14] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y . Hammerla,\nB. Kainz, B. Glocker, and D. Rueckert, “Attention u-net: Learning where to look for the pancreas,” IMIDL\nConference, 2018.\n[15] H. Li, P. Xiong, J. An, and L. Wang, “Pyramid attention network for semantic segmentation,” 2018.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention\nis all you need,” 2017.\n[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” inInternational Conference on Learning Representations, 2021.\n[18] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou, “Training data-efﬁcient image\ntransformers & distillation through attention,” 2021.\n[19] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Jégou, and M. Douze, “Levit: a vision transformer\nin convnet’s clothing for faster inference,” 2021.\n[20] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng, T. Xiang, P. H. S. Torr, and L. Zhang,\n“Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers,” 2021.\n[21] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision\ntransformer using shifted windows,”CoRR, vol. abs/2103.14030, 2021.\n[22] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, “Swin-unet: Unet-like pure transformer for\nmedical image segmentation,” 2021.\n9\nA PREPRINT - JULY 20, 2021\n[23] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and Y . Zhou, “Transunet: Transformers\nmake strong encoders for medical image segmentation,”CoRR, vol. abs/2102.04306, 2021.\n[24] X. Xiao, S. Lian, Z. Luo, and S. Li, “Weighted res-unet for high-quality retina vessel segmentation,”2018 9th\nInternational Conference on Information Technology in Medicine and Education (ITME), pp. 327–331, 2018.\n[25] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng, “H-denseunet: Hybrid densely connected unet for\nliver and tumor segmentation from ct volumes,” IEEE Transactions on Medical Imaging, vol. 37, no. 12, pp.\n2663–2674, 2018.\n[26] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional neural networks for volumetric medical\nimage segmentation,” in2016 Fourth International Conference on 3D Vision (3DV), 2016, pp. 565–571.\n[27] Ö. Çiçek, A. Abdulkadir, S. Lienkamp, T. Brox, and O. Ronneberger, “3d u-net: Learning dense volumetric seg-\nmentation from sparse annotation,” inMedical Image Computing and Computer-Assisted Intervention (MICCAI),\nser. LNCS, vol. 9901. Springer, Oct 2016, pp. 424–432.\n[28] H. Jie, S. Li, S. Gang, and S. Albanie, “Squeeze-and-excitation networks,”IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. PP, no. 99, 2017.\n[29] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu, “Dual attention network for scene segmentation,” inIEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019.\nComputer Vision Foundation / IEEE, 2019, pp. 3146–3154. [Online]. Available: http://openaccess.thecvf.com/\ncontent_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html\n[30] S. Fu, Y . Lu, Y . Wang, Y . Zhou, W. Shen, E. K. Fishman, and A. L. Yuille, “Domain adaptive relational\nreasoning for 3d multi-organ segmentation,” inMedical Image Computing and Computer Assisted Intervention -\nMICCAI 2020 - 23rd International Conference, Lima, Peru, October 4-8, 2020, Proceedings, Part I, ser. Lecture\nNotes in Computer Science, A. L. Martel, P. Abolmaesumi, D. Stoyanov, D. Mateus, M. A. Zuluaga, S. K.\nZhou, D. Racoceanu, and L. Joskowicz, Eds., vol. 12261. Springer, 2020, pp. 656–666. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-59710-8_64\n[31] T. Wu, S. Tang, R. Zhang, J. Cao, and Y . Zhang, “Cgnet: A light-weight context guided network for semantic\nsegmentation,”IEEE Transactions on Image Processing, vol. 30, pp. 1169–1179, 2021.\n[32] R. P. K. Poudel, U. Bonde, S. Liwicki, and C. Zach, “Contextnet: Exploring context and detail for semantic\nsegmentation in real-time,” inBritish Machine Vision Conference 2018, BMVC 2018, Newcastle, UK, September\n3-6, 2018. BMV A Press, 2018, p. 146. [Online]. Available: http://bmvc2018.org/contents/papers/0286.pdf\n[33] G. Li and J. Kim, “Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation,” in 30th\nBritish Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019. BMV A Press, 2019,\np. 259. [Online]. Available: https://bmvc2019.org/wp-content/uploads/papers/0955-paper.pdf\n[34] S. Lo, H. Hang, S. Chan, and J. Lin, “Efﬁcient dense modules of asymmetric convolution for real-time semantic\nsegmentation,” in MMAsia ’19: ACM Multimedia Asia, Beijing, China, December 16-18, 2019, C. Xu, M. S.\nKankanhalli, K. Aizawa, S. Jiang, R. Zimmermann, and W. Cheng, Eds. ACM, 2019, pp. 1:1–1:6.\n[35] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep neural network architecture for real-time\nsemantic segmentation,”CoRR, vol. abs/1606.02147, 2016. [Online]. Available: http://arxiv.org/abs/1606.02147\n[36] M. Liu and H. Yin, “Feature pyramid encoding network for real-time semantic segmentation,” in 30th British\nMachine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019. BMV A Press, 2019, p. 260.\n[37] X. Zhang, Z. Chen, Q. M. J. Wu, L. Cai, D. Lu, and X. Li, “Fast semantic segmentation for scene perception,”\nIEEE Transactions on Industrial Informatics, vol. 15, no. 2, pp. 1183–1192, 2019.\n[38] M. Treml, J. Arjona-Medina, T. Unterthiner, R. Durgesh, and S. Hochreiter, “Speeding up semantic segmentation\nfor autonomous driving,” inNIPS 2016 Workshop - MLITS, 2016.\n[39] R. P. K. Poudel, S. Liwicki, and R. Cipolla, “Fast-scnn: Fast semantic segmentation network,” in 30th British\nMachine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019. BMV A Press, 2019, p. 289.\n10"
}