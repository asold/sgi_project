{
  "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models",
  "url": "https://openalex.org/W4385572063",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101859101",
      "name": "Zhuo Zhang",
      "affiliations": [
        "Harbin Institute of Technology",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2255454991",
      "name": "Yuanhang Yang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2149655215",
      "name": "Yong Dai",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2127284166",
      "name": "Qifan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096181728",
      "name": "Yue Yu",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2129017027",
      "name": "Lizhen Qu",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2607121186",
      "name": "Zenglin Xu",
      "affiliations": [
        "Harbin Institute of Technology",
        "Peng Cheng Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4293575793",
    "https://openalex.org/W4287887646",
    "https://openalex.org/W3102031770",
    "https://openalex.org/W4312352414",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W2807006176",
    "https://openalex.org/W4297687186",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W3203265613",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W3202088367",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4221156340",
    "https://openalex.org/W3184328775",
    "https://openalex.org/W4309208182",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4294106961",
    "https://openalex.org/W3101177651",
    "https://openalex.org/W4320477347",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3012721701",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3209564325",
    "https://openalex.org/W4283455321",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3093034895",
    "https://openalex.org/W2904760378",
    "https://openalex.org/W4302307546",
    "https://openalex.org/W3038028469",
    "https://openalex.org/W4385573610",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3193647133",
    "https://openalex.org/W4372349720",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4287021126",
    "https://openalex.org/W2970408908",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "With increasing concerns about data privacy, there is an increasing necessity of fine-tuning pre-trained language models (PLMs) for adapting to downstream tasks located in end-user devices or local clients without transmitting data to the central server. This urgent necessity therefore calls the research of investigating federated learning (FL) for PLMs. However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system. To this end, we investigate the parameter-efficient tuning (PETuning) of PLMs and develop a corresponding federated benchmark for four representative PETuning methods, dubbed FedPETuning. Specifically, FedPETuning provides the first holistic empirical study of representative PLMs tuning methods in FL, covering privacy attacks, performance comparisons, and resource-constrained analysis. Intensive experimental results have indicated that FedPETuning can efficiently defend against privacy attacks and maintains acceptable performance with reducing heavy resource consumption. The open-source code and data are available at https://github.com/SMILELab-FL/FedPETuning.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 9963â€“9977\nJuly 9-14, 2023 Â©2023 Association for Computational Linguistics\nFedPETuning: When Federated Learning Meets the Parameter-Efficient\nTuning Methods of Pre-trained Language Models\nZhuo Zhang1,2,âˆ— Yuanhang Yang1,âˆ— Yong Dai4 Qifan Wang5 Yue Yu2\nLizhen Qu3,â€  Zenglin Xu1,2,â€ \n1Harbin Institute of Technology, Shenzhen, China\n2Peng Cheng Lab, Shenzhen, China\n3Monash University, Melbourne, Australia\n4Tencent, Shenzhen, China\n5Meta AI, CA, USA\n{iezhuo17, ysngkil}@gmail.com daiyongya@outlook.com wqfcr@fb.com\nyuy@pcl.ac.cn Lizhen.Qu@monash.edu.cn xuzenglin@hit.edu.cn\nAbstract\nWith increasing concerns about data privacy,\nthere is an increasing necessity of fine-tuning\npre-trained language models (PLMs) for adapt-\ning to downstream tasks located in end-user\ndevices or local clients without transmitting\ndata to the central server. This urgent necessity\ntherefore calls the research of investigating fed-\nerated learning (FL) for PLMs. However, large\nPLMs bring the curse of prohibitive commu-\nnication overhead and local model adaptation\ncosts for the FL system. To this end, we investi-\ngate the parameter-efficient tuning (PETuning)\nof PLMs and develop a corresponding feder-\nated benchmark for four representative PETun-\ning methods, dubbed FedPETuning. Specifi-\ncally, FedPETuning provides the first holistic\nempirical study of representative PLMs tuning\nmethods in FL, covering privacy attacks, perfor-\nmance comparisons, and resource-constrained\nanalysis. Intensive experimental results have\nindicated that FedPETuning can efficiently de-\nfend against privacy attacks and maintains ac-\nceptable performance with reducing heavy re-\nsource consumption. The open-source code\nand data are available at https://github.\ncom/SMILELab-FL/FedPETuning.\n1 Introduction\nPre-trained Language Models (PLMs), such as\nBERT (Devlin et al., 2018) and RoBERTa (Liu\net al., 2019a), have demonstrated exceptional per-\nformance on a multitude of natural language pro-\ncessing (NLP) benchmarks (e.g., GLUE (Radford\net al., 2019)). Consequently, PLMs have becomede\nfacto backbones for real-world applications. Gen-\nerally, in most real-world NLP applications, PLMs\nare centrally fine-tuned on the enormous quantity\n*Equal contribution.\nâ€ Corresponding authors.\nClient 1\nClient 2\n...Client N\nServer\nTrainableWeightsFrozen PLMs Communication\nð’²!\nPrivacyLocalData\nð’²! ð’²!\nð’²!\nð’²!ð’²! ð’²!\nð’²\"\nð’²! ð’²! ð’²!ð’²! ð’²! ð’²!\n=\"( )ð’²! ð’²!ð’²!...ð’²!Aggregation\nFigure 1: An overview of FedPETuning where a client\nexchanges a light amount of parameters of PLMs with\nthe server while keeping most parameter frozen.\nof data collected from individual customers, small\nbusinesses, or large enterprises (Qu et al., 2021).\nHowever, with the rising privacy concerns and the\nenactment of data protection laws1, enterprises or\ninstitutions are not allowed to collect data from end\ndevices or local clients to a centralized server for\nfine-tuning PLMs.\nTo break this barrier, federated learn-\ning (Kone Ë‡cn`y et al., 2016; McMahan et al.,\n2017) (FL) has emerged as a privacy-aware\ntechnique designed to collaboratively train models\nwithout transmitting the numerous end-user or\nclient data to a centralized place. In FL, the decen-\ntralized clients only need to periodically compute\nand send model information (i.e., parameters or\ngradients) to a server which is responsible for\naggregating them to produce a global model. With\nthe notion of privacy preserving, FL is appealing\nfor privacy-sensitive NLP applications (Sui\net al., 2020; Basu et al., 2021), as in the case of\nhealthcare (Ge et al., 2020), finance (Long et al.,\n2020), and mobile keyboard (Ji et al., 2019).\n1Such as the EUâ€™s GDPR or the USâ€™s HIPAA.\n9963\nAlthough fine-tuning PLMs in FL, namely\nFedFT, presents promising opportunities, there are\ntwo significant challenges that cannot be over-\nlooked, including (1) communication overhead in\nthe FL system, and (2) computational and storage\ncosts for local clients. Fine-tuning such PLMs usu-\nally requires distributed clients and services high-\nfrequently exchange model gradients or parameters\nwhich are usually in the scale of millions or even\nbillions. The limited communication bandwidth 2\nin the FL system may cause excessive delays dur-\ning frequent uploading and downloading phases\nof the federated fine-tuning procedure. Mean-\nwhile, it is often impractical for local clients to\nfine-tune the entire PLMs because of their limited\ncomputing resources. Moreover, fully fine-tuning\nPLMs is extremely memory-intensive when a local\nclient wants to store different instances for different\ntasks (Hu et al., 2022). As such, it is imperative\nto explore suitable PLMs-empowered FL methods\nunder resource constraints (i.e., communication,\nparameters adaption, and storage).\nTo this end, we investigate parameter-efficient\ntuning (PETuning) methods of PLMs under the\nFL setting. PETuning methods, such as adapter\ntuning (Houlsby et al., 2019), prefix tuning (Li\nand Liang, 2021), LoRA (Hu et al., 2022), Bit-\nFit (Zaken et al., 2021), freeze most parameters of\nPLMs and update only a few additional parame-\nters or a part of the original model parameters for\ndownstream tasks (Ding et al., 2022). These prop-\nerties make PETuning methods potentially appeal-\ning to satisfy the resource constraints since local\nclients just need to tune lightweight parameters and\ncommunicate with the server for updates. Never-\ntheless, it is crucial to address pertinent concerns.\nFirst, as the performance of federated models is\ngreatly affected by the data heterogeneity (Kairouz\net al., 2021) among clients, it is not yet known that\nPETuning methods can achieve acceptable perfor-\nmance in FL with such challenging data hetero-\ngeneity. Second, as private data could be recovered\nfrom model gradients uploaded by clients via gradi-\nent inversion attacks (Zhu et al., 2019), it is unclear\nwhether PETuning methods that upload only partial\nparameters of the entire model can resist gradient\ninversion attacks.\nTo address these concerns, we present the frame-\nwork of Federated Parameter-Efficient Tuning\n2For example, the communication bandwidth between\nclients and server is constrained from a hundred Kbps to a few\nMbps in most situations (Sui et al., 2020).\n(named FedPETuning for short), as illustrated in\nFigure 1, and conduct in-depth experiments of var-\nious PETuning methods under the FL setting, mea-\nsuring privacy-preserving capability, performance,\nand resource costs. Intensive experimental results\non the GLUE benchmark reveal that (1) FedPETun-\ning can reduce the considerable resource costs in\nFL settings while still achieving acceptable perfor-\nmance (e.g., FedAP reduces 97.4% of communi-\ncation with only 0.7% performance degradation\ncompared with FedFT) as shown in Table 2, and\n(2) FedPETuning has an appealing ability to de-\nfend against gradient inversion attacks, which can\nreduce the prevision of recovered words by an av-\nerage of 40.7% compared to FedFT, as shown in\nSection 4.2. In summary, the major contributions\nof this paper are shown as follows:\nâ€¢ FedPETuning is the first benchmark to pro-\nvide a holistic review of PETuning methods\nfor PLMs under FL settings, covering pri-\nvacy attacks, performance comparisons, and\nresource-constrained analysis.\nâ€¢ FedPEtuning can serve as a suite of baselines\nfor efficient-parameter tuning of PLMs in a\nfederated setting, and guide the community to\ndesign FL-tailored efficient parameter tuning\nalgorithms.\nOur research findings demonstrate the poten-\ntial of combining large PLMs with FL, provid-\ning a promising training paradigm for privacy-\npreserving learning in the era of large language\nmodels (Ouyang et al., 2022; OpenAI, 2023).\n2 Related Work\nFederated Learning Federated learn-\ning (Kone Ë‡cn`y et al., 2016; McMahan et al.,\n2017) (FL), a widespread distributed learning\ntechnique used in privacy-sensitive tasks, has been\nhindered by not Independently and Identically\nDistributed (non-IID) (Kairouz et al., 2021),\nwhich results in accuracy discrepancies compared\nto centralized training. Extensive optimization\nstudies have been conducted to address the non-IID\nissue, including data optimization (Zhao et al.,\n2018), model updating optimization (Chai et al.,\n2020), and model training optimization (Sahu et al.,\n2018). Recently, some work has tried to solve this\nproblem from a pre-trained model initialization\nperspective (Chen et al., 2022b; Nguyen et al.,\n2022) and transformer model structure (Qu et al.,\n9964\n2022). Weller et al. (2022) experimentally show\nthat using PLMs could reduce non-IID adverse\neffects and narrow down its accuracy gap to\ncentralized learning. However, the significant\ncommunication overhead of large-scale PLMs are\nless considered in FL systems, leading to slow\nand impractical federated training in real-world\ntasks. Additionally, PLMs can pose challenges for\nlocal clients with limited hardware capabilities for\ncomputation and storage. In contrast, our study\ninvestigates PLMsâ€™ training in the FL context\nunder resource constraints.\nInjecting parameters-efficient tuning methods\ninto federated learning Parameter-efficient tun-\ning (PETuning) seeks to keep most parameters\nof PLMs frozen and fine-tune only additional\nlightweight parameters or a fraction of the param-\neters for downstream tasks (Houlsby et al., 2019;\nLi and Liang, 2021; Hu et al., 2022; Zaken et al.,\n2021). With this trait, PETuning methods can be\nutilized to mitigate the communication overhead\nin FL, which primarily relies on the size of model\nupdate parameters. In the field of computer vision,\nSun et al. (2022) present the FedPEFT framework\nby injecting three PETuning methods (i.e., Bais,\nAdapter, Prompt) of the visual pre-trained models\ninto FL, and find that lightweight PETuning meth-\nods in FL can significantly reduce the communi-\ncation burden while maintaining performance and\nperforming better in different FL settings. Mean-\nwhile, Chen et al. (2022c) extend PETuning meth-\nods to the visual language model in FL and show\nthat PETuning can facilitate a fast convergence\nrate. However, these studies ignore the important\nprivacy attack issue existing in FL. With increas-\ning attention to privacy concerns, validation of the\nprivacy-preserving capabilities of federated PETun-\ning methods is paramount and facilitates their prac-\ntical deployment in real-world scenarios.\nIn the context of NLP, Zhao et al. (2022) first\nexplore the effect of prompt-tuning under the FL\nsetting and achieve acceptable performance results\ncompared with fine-tuning. Xu et al. (2022) show\nthat it is possible to train large vocabulary language\nmodels while preserving accuracy and privacy by\nadopting the low-rank adaptation (Hu et al., 2022).\nHowever, there has been no comprehensive inves-\ntigation into the FL performance of the PETuning\nmethod for PLMs. Our research aims to bridge\nthis gap and provide access to our code and data to\ninspire further exploration of the potential of this\nnew paradigm for efficient federated NLP.\n3 Federated Parameter-Efficient Tuning\nIn this section, we present how different PETuning\nmethods work, followed by the training process of\nFedPETuning.\n3.1 PETuning Methods\nDenote the original PLM parameters by Wp =\n{w1,w2,...,w N }and the updated parameters by\nW\nâ€²\np = {w\nâ€²\n1,w\nâ€²\n2,...,w\nâ€²\nM }after training on the\ndataset D. Define We as trainable model parame-\nters. In vanilla fine-tuning, |We|are equal to the\nnumber of original model parameters and N = M,\nwhere |Â·|refers to the number of parameters. In the\nPETuning methods, most parameters of the PLM\nkeep frozen and only a few added parameters or a\npart of the original model parameters are updated,\nthat is, M â‰¥N and |We|â‰ª N. Following the\ntaxonomy of Ding et al. (2022), PETuning methods\ncould be divided into three groups, i.e., addition-\nbased methods, specification-based methods, and\nreparameterization-based methods.\nAddition-based methods introduce new trainable\nparameters into the frozen PLMs. These meth-\nods have two representative branches: Adapter\ntuning and Prompt tuning. The adapter tuning\nproposed by Houlsby et al. (2019) inserts adapter\nlayers to vanilla Transformers. Specifically, two\nadapter layers are inserted into each Transformer\nblock, wherein each adapter layer contains a down-\nprojection and an up-projection. Given the input\nfeature hâˆˆRd, the down-projection Wd âˆˆRdÃ—r\nprojects the input hin to a r-dimensional space and\nthe up-projection Wu âˆˆRrÃ—d maps back to the in-\nput size. Mathematically, the computation process\nis as follows,\nh â†WT\nu f\n(\nWT\nd h\n)\n, (1)\nwhere f(Â·) is the nonlinear activation function. In\nthis strategy, adapter tuning could only fine-tune\nadapter layers We = {Wu,Wd}(about 0.5% to 8%\nparameters of the whole model) during the tuning\nprocess while keeping parameters of PLMs frozen.\nOn the contrary, prompt-tuning (Li and Liang,\n2021; Liu et al., 2021) adds extra parameters with-\nout modifying the model architecture. Prompt-\ntuning converts the training objective of down-\nstream tasks into a form similar to the pre-trained\nstage (Devlin et al., 2019; Liu et al., 2019b), by\nattaching trainable vectors P, namely prompt, to\n9965\nthe original input. During model training, prompt-\ntuning only adapts light-weight prompt vectors\nWe = {P}, which scale to within 10% of the\nnumber of PLMs parameters.\nAlgorithm 1: Training process of FedPETuning\nParameters: Client set C; Communication round\nT; Local epoch number E; The PLMs parameters\nWp; The local trainable and efficient parameters We\nand the local dataset Dk of the k-th client; Local\nPETuning Method P;\nBefore Training: Initialize W0\ne on the server and\nWp on each client in C.\nServerGlobalAggregation:\nfor each communication round t = 1to Tdo\nCt â†(randomly sample K clients from C)\nfor each user k âˆˆCt in paralleldo\nClientLocalTuning(k, Wtâˆ’1\ne )\nend\nReceive local updated parameters Wk,t\ne\nPerform global aggregation by Eq. (3)\nend\nClientLocalTuning (k, Wt\ne):\nWt â†(assemble Wt\ne and Wp)\nfor epoch e = 1to Edo\nWk,t+1\ne â†P(Dk, Wt)\nend\nSend Wk,t+1\ne to the server\nSpecification-based methods aim to fine-tune a\nfraction of the parameters while keeping others\nfrozen. In particular, Zaken et al. (2021) propose\nBitFit and empirically demonstrate that only tuning\nthe bias terms We = {bâ„“,(Â·)\n(Â·) }of PLMs could still\nachieve competitive performance.\nReparameterization-based methods argue that\nthe PLMsâ€™ adaptions can be re-parameterized\ninto optimization within a low-dimensional sub-\nspace (Aghajanyan et al., 2020). Based on this\nhypothesis, LoRA (Hu et al., 2022) optimizes the\nlow-rank decomposition for weight update matri-\nces âˆ†Wduring model training. For a pretrained\nweight matrix WâˆˆR dÃ—k, we have\nW+ âˆ†W= W+ BA, (2)\nwhere B âˆˆ RdÃ—r, A âˆˆ RrÃ—k, and the rank\nr â‰ªmin(d,k). In this way, LoRA could make\ntraining more efficient with less than 1% train-\nable parameters We = {B,A}and match the fine-\ntuning performance.\n3.2 FedPETuning\nOur work considers a conventional FL system that\ncomprises a central server responsible for manag-\ning participated clients for local training and dis-\ntributing the shared model parameters. Instead of\ncommunicating all cumbersome PLMs parameters,\nFedPETuning resorts to PETuning methods for ex-\nchanging lightweight parameters, as illustrated in\nFigure 1.\nBefore training, FedPETuning initializes the\nbackbone PLM with Wp and the PETuning method\nP with efficient-parameter We. Then global ag-\ngregation in the server and local updating in local\nclients are executed alternately.\nServer Global Aggregation. The third-party\nservice first randomly selects Kclients from Cand\ndistributes trainable parameter We to these chosen\nclients. Then the server performs the federated\naggregation based on the received parametersWk,t\ne\nfrom clients, and updates the We by\nWt+1\ne =\nKâˆ‘\nk=1\n|Dk|\nâˆ‘K\nk=1 |Dk|Wk,t\ne . (3)\nClient Local Tuning. When selected clients\ndownload the trainable parameters, they assemble\na whole model with lightweight parameters We\nand local PLM parameters Wp. Then, selected\nclients train the assembled model with local private\ndata Dk. After local training, the k-th client sends\nits updated efficient parameters Wk\ne to the central\nserver for federated aggregation.\nThe training process described above is repeated\nuntil a specific criterion (e.g., the maximum num-\nber of communication rounds T) is satisfied. This\nprocess can be summarized in Algorithm 1.\n4 Experiments\nIn this section, we conduct extensive experiments\nfor evaluating the performance of PETuning in the\nFL setting, covering privacy attacks (see Section\n4.2), performance comparisons (see Section 4.3),\nand resource-constrained analysis (see Section 4.4).\nBesides, we also provide an in-depth ablation anal-\nysis of FedPETuning in terms of data heterogeneity\n(see Section 4.5), local training epochs (see Section\n4.6), and different FL scenarios (see Section 4.7).\n4.1 Experiments Setup\nDataset and non-IID partitioning Our experi-\nments use six datasets from the GLUE benchmark.\nThe reasons are as follows: (1) the GLUE datasets\nhave emerged as the de facto standard benchmark\nfor assessing PLMsâ€™ effectiveness in natural lan-\nguage understanding; (2) They have been exten-\n9966\nTask # Train # Dev. # Test Metric\nRTE 2,241 249 277 Accuracy\nMRPC 3,301 367 408 F1 Score\nSST-2 66,675 674 872 Accuracy\nQNLI 103,695 1,048 5,463 Accuracy\nQQP 360,210 3,639 40,430 Accuracy\nMNLI 388,774 3,928 9,815 Accuracy\nTable 1: Dataset descriptions and statistics.\nsively leveraged to validate various PETuning meth-\nods (Li and Liang, 2021; Liu et al., 2021; Zaken\net al., 2021; Houlsby et al., 2019), and (3) these\ndatasets are large enough and convenient for FL\ndata partitioning (non-IID). Due to the limitation\nof the unpublished test set in GLUE, we follow the\nprevious studies (Liu et al., 2022; Zhao et al., 2022)\nand use the original validation set as the new test\nset and split a part of the training set as the valida-\ntion set. The data breakdown for this benchmark is\nin Table 1.\nFor the non-IID data partitioning, we follow Lin\net al. (2021) and partition the datasets by using\nthe Dirichlet distribution as the class priors. In\nparticular, we sample Dâˆ¼ Dir(Î±) and allocate\ndata Dk to k-th client. Î±determines the degree of\nnon-IID, and a lower value of Î±generates a high\nlabel distribution shift. Unless otherwise specified,\nwe maintain a default setting of Î± = 1.0 for the\nDirichlet parameter throughout our experiments.\nImplementation Details We adopt the bench-\nmark FL system FedAvg to simulate the FL set-\nting, which has been applied to commercial prod-\nucts (Bonawitz et al., 2019). In FedAvg, clients\nupload local model parameters to the central server\nand download the aggregated model for the next\ntraining round. Following Lin et al. (2021), we set\nthe communication round to 100 and the local train-\ning epoch to 1 for all tuning methods. Following\nChen et al. (2022a), we utilize Roberta-Base (Liu\net al., 2019a) as the local model released by Hug-\ngingface3. The FedAvg implementation is based\non FedLab (Zeng et al., 2023).\nUnder the FedAvg training protocol, we pri-\nmarily evaluate the full fine-tuning (FedFT) and\nfour representative PETuning methods, covering\nadapter tuning (FedAP), prefix tuning (FedPF),\nLoRA (FedLR), and BitFit (FedBF). For FedAP,\nwe follow the architecture in Houlsby et al. (2019),\nwhich interposes adapter modules into both the\nmulti-head attention module and the feed-forward\n3https://github.com/huggingface/transformers\n1 2 4 8 16 32\nBatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Recall\nFedFT\nFedPF\nFedAP\nFedBF\nFedLR\n1 2 4 8 16 32\nBatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Precision\nFedFT\nFedPF\nFedAP\nFedBF\nFedLR\n1 2 4 8 16 32\nBatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n F1\nFedFT\nFedPF\nFedAP\nFedBF\nFedLR\nFigure 2: The data reconstruction attack results on the\nattack dataset with different tuning methods. Low per-\nformance is better. FedPETuning can effectively defend\nagainst data reconstruction attacks.\nnetwork in each Transformer layer. We use pre-\nfix tuning (Lester et al., 2021) as the representa-\ntive of prompt tuning because it has better perfor-\nmance on the small PLMs, e.g., base versions of\nRoberta. For LoRA and BitFit, we take the ar-\nchitectures from their origin papers (Zaken et al.,\n2021; Hu et al., 2022). All PETuing methods are\nbased on OpenDelta4, which is a plug-and-play\nframework for parameter-efficient tuning.\nTo make a fair and reasonable comparison, we\nrun a hyperparameter sweep for each dataset and\ntuning method. We select the best model accord-\ning to the metric on the validation set and report\ntest set metric. Especially, the learning rate is se-\nlected from {5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}.\nWe search the reduction factor from {16, 64} for\nFedAP, the prompt length from {8, 16, 64} for\nFedPF, and the scaling factor and rank from {8, 16}\nfor FedLR. All experiments are done on a server\nwith 8 Nvidia Tesla V100 GPUs with 32GB RAM\neach.\n4.2 Privacy-Preserving Results\nWe first investigate the privacy-preserving capa-\nbilities of FedPETuing and FedFT. In this privacy\nattack experiment, we adopt DLG (Zhu et al., 2019)\nas our base attack method. Due to space limitations,\nwe have omitted the working process of DLG and\nrecommend the reader read the original paper (Zhu\net al., 2019).\nSetup and Metrics As the goal for the attacker\nwith DLG is to recover text from client-uploaded\ngradients, we follow Song and Raghunathan (2020)\nand evaluate FedPETuning and FedFT in terms\nof precision (the average percentage of recovered\nwords in the target texts), recall (the average per-\ncentage of words in the target texts are predicted)\nand F1 score ( the harmonic mean between preci-\n4https://github.com/thunlp/OpenDelta\n9967\nMethods RTE MRPC SST-2 QNLI QQP MNLI Avg. Rel. Com.\nFedFT 70.31.2 90.70.3 94.00.6 91.00.4 89.50.1 86.40.2 83.1 100.0% 1x\nFedAP 69.42.6 89.11.2 93.30.6 90.90.4 88.40.2 86.00.4 82.4 99 .1% â†‘60x\nFedLR 67.44.2 84.54.5 93.60.5 90.80.3 87.40.3 84.90.4 81.0 97 .5% â†‘141x\nFedPF 58.62.2 86.81.0 93.00.6 87.60.5 85.70.3 82.20.3 78.4 94 .3% â†‘12x\nFedBF 61.41.7 84.62.7 92.50.7 87.20.5 84.50.5 81.70.2 77.8 93 .6% â†‘190x\nCenFT 73.01.4 90.90.6 92.90.2 90.80.5 91.10.2 86.00.2 83.6 100 .0% -\nCenAP 76.01.8 90.60.8 94.60.5 92.90.1 91.10.1 87.50.2 84.7 101.3% -\nCenLR 74.42.4 91.70.6 94.00.4 92.70.6 90.10.3 87.00.2 84.4 100 .9% -\nCenPF 65.65.1 90.20.9 93.70.8 91.50.2 89.50.1 86.70.2 82.2 98 .3% -\nCenBF 70.91.0 91.30.8 94.10.3 91.30.2 87.40.2 84.60.1 82.6 98 .8% -\nTable 2: Performance results of the PETuning and FT on GLUE benchmark under the federated (upper half) and\nthe centralized (bottom half) settings. With significantly reducing communication overhead, FedPETuning still\nmaintains acceptable performance. The Rel. denotes the percentage of PETuning in terms of performance relative to\nFT. The blue value indicates more than 95% performance of FT, and red value indicates performance in excess of\nFT. The Com. denotes the normalized values of communication overhead of FedPETuning and FedFT. Mean and\nstandard deviation are computed over 5 runs.\nsion and recall). Specifically, we randomly selected\n128 samples from the MNLI as the attack dataset.\nResults Figure 2 shows the results of DLG on the\nattack dataset with different tuning methods. The\nresults show that FedPETuning can effectively\ndefend against data reconstruction attacks com-\npared to FedFT. Since FedPETuning communi-\ncates a fraction of the entire model parameters with\nthe server during the federated training process,\nit is intractable for the attacker to reconstruct the\noriginal text from such lightweight model param-\neters. Surprisingly, among FedPETuning, DLG is\nmore likely to reconstruct private data from FedAP.\nWe speculate that this result may be related to the\ndesign of PETuning methods. Adapter tuning in-\nserts adapter layers to vanilla Transformers, which\ncan encode the input data separately during model\ntraining. Compared to other PETuning methods,\nAdapter tuning is easier to â€œrememberâ€ the model\ninput, thus causing more severe privacy leaks.\nThere is a clear and consistent trend across all\ntuning methods: DLG performance decreases as\nthe batch size increases. The reason is that the DLG\nattack requires more parameters to be optimized in\nlarger batch sizes. Therefore, increasing the batch\nsize is a good defense strategy against the DLG\nattack. However, clients (e.g., mobile phone) in the\nFL system are usually resource-constrained in real-\nworld applications. There is a trade-off between\nlimited computing resources and large batches. In\ncontrast, the FedPETuning method does not vary\nsignificantly across different batch sizes. In this\nsense, FedPETuning can provide a practical de-\nfense strategy for clients with restricted computing\nresources.\n4.3 Performance Comparison\nTable 2 shows the performance for four PETuning\nand FT under the federated (upper half table) and\ncentralized settings (bottom half table). The results\ndemonstrate that FedPETuning maintains accept-\nable performance (more than 95% of FedFTâ€™s\nperformance) while reducing the communica-\ntion overhead substantially.\nFrom the upper half of Table 2, we find that\nalthough FedPETuning methods lag behind FedFT\nin the federated setting, the performance gap is\nrelatively acceptable. For instance, FedAP reduces\n60 times the communication overhead by only 0.7%\nperformance degradation compared with FedFT.\nThis result also shows that FedPETuning methods,\nespecially FedAP and FedLR (more than 95% of\nFedFTâ€™s performance), can achieve a good level of\ntrade-off between performance and communication\noverhead in practice.\nIn the centralized setting, some PETuning meth-\nods achieve competitive performance compared\nwith CenFT. Specifically, CenAP outperforms\nCenFT on five out of six datasets, and the CenAP\nand CenLR achieve better performances on aver-\nage. This result also supports our motivation that\nPETuning, stimulating colossal models with only a\nsmall portion of tunable parameters, can naturally\nbe a promising way for FL under communication\nand resource constraints.\nComparing the upper and bottom parts of Table\n2, we find that all tuning methods endure a decline\nin performance under the federated setting. Re-\nmarkably, the FedPETuning exhibits a significant\n9968\n102\n103\n104\n105\nCommunication Budget / MB\n0.50\n0.55\n0.60\n0.65Accuracy (%)\nRTE\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n102\n103\n104\n105\nCommunication Budget / MB\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy (%)\nMNLI\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n102\n103\n104\n105\nCommunication Budget / MB\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy (%)\nQNLI\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n102\n103\n104\n105\nCommunication Budget / MB\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy (%)\nQQP\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\nFigure 3: Accuracy versus Communication Budget for\nall tuning methods. The horizontal dashed line indi-\ncates the acceptable performance, which is 95% of the\nperformance of CenFT. FedPETuning makes federated\ntraining more efficient by significantly reducing the com-\nmunication overhead compared to FedFT.\n0 25 50 75 100 125 150 175\nRTE\nMNLI\n190\n190\n12.1\n12.1\n140.1\n140.4\n103.8\n41.9\n1\n1\nFedBF FedPF FedLR FedAP FedFT\n0 25 50 75 100 125 150 175\nRTE\nMNLI\n190\n190\n12.1\n12.1\n140.1\n140.4\n103.8\n41.9\n1\n1\nFedBF FedPF FedLR FedAP FedFT\nFigure 4: Normalized values of storage efficiencies of\nFedPETuning and FedFT on RTE and MNLI. Higher is\nbetter. FedPETuning lowers the storage cost to entry by\n12~190 times.\ndrop in performance in the data heterogeneous FL\ncontext, which aligns with the results observed in\ndata heterogeneity experiments (see Section 4.5).\nThis result suggests that the FL community may de-\nsign FL-tailored PETuning methods to bridge the\nperformance gap caused by the non-IID problem.\nThis will be explored in our future work.\n4.4 Resource Costs\nWe next show the resource cost by different tuning\nmethods under FL settings, including communica-\ntion budget and client storage overhead. Figure\n3 shows accuracy versus communication budget\nfor all tuning methods on RTE and MNLI 5. As\nshown in Figure 3, the communication budgets are\nranked as FedFT â‰«FedPF >FedAP >FedLR >\nFedBF. The experimental results show that Fed-\nPETuning renders federated training remark-\nably efficient by significantly reducing the com-\nmunication overhead as opposed to FedFT.More\ncommunication overhead entails increased training\ntime during uploading and downloading. For FL\nthat necessitates high-frequency communication,\n5The plots of remaining tasks can be found in Appendix B,\nwhich have similar results.\nFedFT FedAP FedLR FedPF FedBF\n10\n5\n0\nRelative Accuracy (%)\nRTE\n=0.1\n =10.0\nFedFT FedAP FedLR FedPF FedBF\n4\n2\n0\nMNLI\n=0.1\n =10.0\nFedFT FedAP FedLR FedPF FedBF\n40\n20\n0\nRelative Accuracy (%)\nQNLI\n=0.1\n =10.0\nFedFT FedAP FedLR FedPF FedBF\n20\n10\n0\nQQP\n=0.1\n =10.0\nFigure 5: Performance variation of FedPETuning and\nFedFT under different non-IID with respect to Î±= 1.0\non RTE and MNLI. FedPETuning is more susceptible\nto data heterogeneity than FedFT.\n0.0 0.2 0.4 0.6 0.8\nDistance\n0\n1\n2\n3\n4\n5\n6\nDensity\nRTE =0.1\n0.0 0.2 0.4 0.6 0.8\nDistance\n0.0\n0.5\n1.0\n1.5\n2.0\nRTE =1.0\n0.0 0.1 0.2 0.3 0.4 0.5\nDistance\n0\n2\n4\n6\n8\n10\nRTE =10.0\n0.0 0.2 0.4 0.6 0.8\nDistance\n0\n2\n4\n6\n8\nDensity\nMNLI =0.1\n0.0 0.2 0.4 0.6 0.8\nDistance\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nMNLI =1.0\n0.0 0.2 0.4 0.6\nDistance\n0\n1\n2\n3\nMNLI =10.0\nFigure 6: Data distribution under different Dirichlet\nparameter Î±. We report the pairwise Jensenâ€“Shannon\ndistance of the label distribution between two clients.\nFedFT is extremely time-consuming, making the\nfederated training slow. In this regard, FedPETun-\ning is more pragmatic in real-world applications,\nparticularly in communication-constrained FL chal-\nlenges.\nFigure 4 shows normalized values of storage ef-\nficiencies of FedPETuning and FedFT on RTE and\nMNLI. FedPETuning lowers the storage cost to\nentry by 12~190 times. This appealing character-\nistic is practiced for local clients of real-world FL\nsystems. When deploying multiple tasks on a local\nclient, FedPETuning can share PLM between dif-\nferent tasks, enabling the client to maintain only a\nfew parameters for each task, thus reducing storage\nrequirements.\n4.5 Impact of Data Heterogeneity\nAs data heterogeneity is a fundamental challenge\nin FL, we also evaluate the performance of Fed-\nPETuning and FedFT under different data hetero-\ngeneity. Following Lin et al. (2021), we consider\nthree Dirichlet distributions in this experiment by\nchoosing Î±from {0.1,1.0,10.0}where a smaller\nÎ±indicates a sharper non-IID distribution among\nclients.\nThe performance of Î± = 1.0 has already been\ndiscussed, Figure 5 illustrates performance changes\nof FedPETuning and FedFT with respect to the\n9969\n1 2 3\nEpoch\n60\n65\n70\n75Accuracy (%)\nRTE\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n1 2 3\nEpoch\n82\n84\n86\nMNLI\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n1 2 3\nEpoch\n80\n85\n90Accuracy (%)\nMRPC\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n1 2 3\nEpoch\n86\n88\n90\nQQP\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\nFigure 7: Performance comparison of FedPETuning\nand FedFT under different epochs on RTE and MNLI.\nMost tuning methods under the FL setting benefit from\nincreased local training epochs.\nother two Î±values. The outcomes corresponding\nto other tasks can be found in Appendix B. Figure 5\nreveals that FedPETuning is more susceptible to\ndata heterogeneity than FedFT. As shown in Fig-\nure 5, although the increase in data heterogeneity\n(Î±from 1.0 to 0.1) degrades the performance of all\ntuning methods, FedPETuning degrades more dra-\nmatically compared to FedFT. This suggests more\nthe federated model necessitates more trainable pa-\nrameters to tackle intricate data heterogeneity, and\nit may be arduous for FedPETuning to confront\ncomplex data heterogeneity in FL.\nAnother noteworthy phenomenon is the perfor-\nmance of FedPETuning and FedFT do not change\nmuch when increasingÎ±from 1.0 to 10.0. To figure\nout this, we report the Probability Density Function\n(PDF) of the data distribution under different Î±in\nFigure 6 (See Appendix A for other datasets). As\nshown in Figure 6, the data heterogeneity is similar\nfor Î±equal to 1.0 and 10.0, while the distribution\ngap is large for Î±equal to 0.1. We think similar\ndata heterogeneity between Î±= 1.0 and Î±= 10.0\ncontributes to this result.\n4.6 Impact of Local Training Epochs\nFigure 7 shows the performance of FedPETuning\nand FedFT with different local training epochs on\nRTE and MNLI. More results are provided in Ap-\npendix B. This result reveals that most tuning\nmethods under the FL setting benefit from in-\ncreased local training epochs. Federated models\ntrained with more local epochs consistently yield a\nperformance gain across different tuning methods\nwhen clients with heigh-resource data (like MNLI).\nIn contrast, training with more epochs may incur a\ndrop in performance for the low-resource dataset\n(like FedLR and FedAP on RTE). This may be due\nto the probability of PLMs becoming more sus-\nceptible to overfitting when clients with few data\nundergo more training epochs. On the contrary,\nMethods RTE MRPC SST-2Avg. Rel. Com.\nFedFT 70.4 90.2 94.3 85.0 100.0% 1x\nFedAP 71.5 88.5 94 .0 84.7 99 .7% â†‘70x\nFedLR 70.8 89 .8 94.4 85.0 100.0% â†‘141x\nFedPF 66.4 88 .1 93 .7 82.7 97 .3% â†‘12x\nFedBF 55.2 88 .6 92 .8 78.9 92 .8% â†‘189x\nTable 3: Performance results of the FedPETuning and\nFedFT in the cross-silo setting. Significantly reduc-\ning the communication overhead, FedPETuning still\nachieves acceptable performance in the cross-silo FL\nscenario.\nMethods QNLI QQP MNLI Avg. Rel. Com.\nFedFT 87.9 88.8 85.6 87.4 100.0% 1x\nFedAP 85.9 87 .0 84 .9 85.9 98 .3% â†‘52x\nFedLR 86.0 86 .5 84 .7 85.7 98 .1% â†‘140x\nFedPF 84.6 81 .8 80 .4 82.3 94 .2% â†‘12x\nFedBF 80.5 84 .0 80 .7 81.7 93 .5% â†‘190x\nTable 4: Performance results of FedPETuing and FedFT\nin the large-scale cross-device setting. With significantly\nreducing the communication overhead, FedPETuning\nstill achieves acceptable performance in large-scale\ncross-device FL scenarios.\nclients with adequate data require more local train-\ning epochs to enhance the FL performance.\nWith the data scale into analysis, we also find\nthat all tuning methods are more unstable on small\ndatasets in Figure 7. For instance, the standard de-\nviation performance of FedLR on RTE and MRPC\nis over 4.0% while other datasets are no more than\n0.5%. This phenomenon is consistent with pre-\nvious work (Chen et al., 2022a), and they exper-\nimentally show that increasing training steps can\neffectively mitigate the instability of the PETuning.\nHowever, this strategy fails in the FL setting. Train-\ning PLMs more stably in FL is a good research\ndirection we will explore in our future work.\n4.7 Various FL Scenarios\nTo verify the effectiveness of PETuning under dif-\nferent FL scenarios, we mainly consider two FL\nscenarios in this experiment, i.e., the cross-silo\nFL scenario (Kairouz et al., 2021) and the large-\nscale cross-device FL scenario (Lai et al., 2022).\nCross-silo (Kairouz et al., 2021) is a vital applica-\ntion scenario for FL, suitable for several users (no\nmore than 100). In Cross-silo, the server selects all\nclients for training in each communication round.\nLarge-scale cross-device FL (Lai et al., 2022) is\nanother federated scenario for deployment across\nthousands of clients. In the large-scale cross-device\nFL, data held by the local client is more scarce. In\n9970\nthis experiment, we chose RTE, MRPC, and SST-\n2 to simulate the cross-silo FL scenarios, while\nQNLI, QQP, and MNLI to simulate the large-scale\ncross-device FL scenarios. The total number of\nclients in the cross-silo and large-scale settings\nis set to 10 and 1000, respectively. For both FL\nscenarios, ten clients are involved in each com-\nmunication round for local training and federated\naggregation.\nTable 3 and Table 4 show the performance re-\nsults of PETuning and FT under these two FL sce-\nnarios, respectively. The results show that Fed-\nPETuning can significantly reduce the commu-\nnication overhead while achieving acceptable\nperformance in cross-silo and large-scale cross-\ndevice FL scenarios. More noteworthy between\nthe different FL scenes is the cross-silo setting. As\nshown in Table 3, the performance gap between\nthe FedPETuning and FedFT diminishes under the\ncross-silo setting, compared to our standard setting\nin Section 4.3. For example, FedPF lags behind\nFedFT by 2.3% and 4.7% under the cross-silo FL\nscenario and the FL scenario of Table 2, respec-\ntively. We attribute this observation to relieving the\ndata scarcity issue since each client has ten times\nmore training samples in the cross-silo setting than\nin the standard setting.\n5 Conclusion\nThis paper investigates parameter-efficient tuning\nmethods of PLMs in the FL setting with extensive\nexperiments for in-depth measurement of these\nmethods under FL settings, covering privacy at-\ntacks, performance comparisons, and resource-\nconstrained analysis. Experimental results unveil\nthat FedPETuning can (1) achieve acceptable per-\nformance while reducing the colossal communica-\ntion overhead and local storage cost, and (2) pro-\nvide strong privacy-preserving capacity under dif-\nferent FL settings. To facilitate FL-tailored PETun-\ning research, we have released our code and parti-\ntioned datasets, aiming to facilitate the use of Fed-\nPETuning and inspire the broader community to\ndevelop more suitable PETuning methods in future\nfederated learning research.\nLimitation\nOne limitation of this paper is that we do not val-\nidate FedPETuning on large scale models, (e.g.,\nT5 (Raffel et al., 2019), LLaMa (Touvron et al.,\n2023), Vicuna (Chiang et al., 2023), etc). Although\nthe parameter efficiency of PETuning is more im-\npressive on larger pre-trained models, we have to\nconsider the limited computational resources of\nclients in FL, making it challenging to deploy such\na large-scale model. In addition, with the increas-\ning size of modern pre-trained models, the commu-\nnity needs to design FL-friendly PETuning meth-\nods. In this sense, our work can serve as a bench-\nmark and guide for future exploration of PETuning\nin FL.\nAcknowledgements\nWeâ€™d like to thank all the anonymous review-\ners for their careful readings and valuable com-\nments. This work was partially supported by\nthe National Key Research and Development\nProgram of China (No. 2018AAA0100204), a\nkey program of fundamental research from Shen-\nzhen Science and Technology Innovation Com-\nmission (No. JCYJ20200109113403826), the Ma-\njor Key Project of PCL (No. 2022ZD0115301),\nand an Open Research Project of Zhejiang Lab\n(NO.2022RC0AB04).\nReferences\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal\nGupta. 2020. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. arXiv\npreprint arXiv:2012.13255.\nPriyam Basu, Tiasa Singha Roy, Rakshit Naidu, and\nZumrut Muftuoglu. 2021. Privacy enabled financial\ntext classification using differential privacy and fed-\nerated learning. arXiv preprint arXiv:2110.01643.\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,\nDzmitry Huba, Alex Ingerman, Vladimir Ivanov,\nChloe Kiddon, Jakub KoneË‡cn`y, Stefano Mazzocchi,\nBrendan McMahan, et al. 2019. Towards federated\nlearning at scale: System design. Proceedings of\nMachine Learning and Systems, 1:374â€“388.\nZheng Chai, Yujing Chen, Liang Zhao, Yue Cheng, and\nHuzefa Rangwala. 2020. Fedat: A communication-\nefficient federated learning method with asyn-\nchronous tiers under non-iid data. CoRR,\nabs/2010.05958.\nGuanzheng Chen, Fangyu Liu, Zaiqiao Meng, and\nShangsong Liang. 2022a. Revisiting parameter-\nefficient tuning: Are we really there yet? arXiv\npreprint arXiv:2202.07962.\nHong-You Chen, Cheng-Hao Tu, Ziwei Li, Han-Wei\nShen, and Wei-Lun Chao. 2022b. On pre-training for\nfederated learning. arXiv preprint arXiv:2206.11488.\n9971\nJinyu Chen, Wenchao Xu, Song Guo, Junxiao Wang,\nJie Zhang, and Haozhao Wang. 2022c. Fed-\ntune: A deep dive into efficient federated fine-\ntuning with pre-trained transformers. arXiv preprint\narXiv:2211.08025.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171â€“4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2022. Delta tuning:\nA comprehensive study of parameter efficient meth-\nods for pre-trained language models. arXiv preprint\narXiv:2203.06904.\nSuyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi, Yongfeng\nHuang, and Xing Xie. 2020. Fedner: Medical named\nentity recognition with federated learning. arXiv\npreprint arXiv:2003.09288.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790â€“2799. PMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nShaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing\nJiang, and Zi Huang. 2019. Learning private neural\nlanguage modeling with attentive aggregation. In\n2019 International joint conference on neural net-\nworks (IJCNN), pages 1â€“8. IEEE.\nPeter Kairouz, H Brendan McMahan, Brendan Avent,\nAurÃ©lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,\nKallista Bonawitz, Zachary Charles, Graham Cor-\nmode, Rachel Cummings, et al. 2021. Advances and\nopen problems in federated learning. Foundations\nand TrendsÂ® in Machine Learning, 14(1â€“2):1â€“210.\nJakub KoneË‡cn`y, H Brendan McMahan, Felix X Yu, Pe-\nter RichtÃ¡rik, Ananda Theertha Suresh, and Dave\nBacon. 2016. Federated learning: Strategies for im-\nproving communication efficiency. arXiv preprint\narXiv:1610.05492.\nFan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu,\nXiangfeng Zhu, Harsha Madhyastha, and Mosharaf\nChowdhury. 2022. Fedscale: Benchmarking model\nand system performance of federated learning at\nscale. In International Conference on Machine\nLearning, pages 11814â€“11827. PMLR.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582â€“\n4597, Online. Association for Computational Lin-\nguistics.\nBill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin\nWang, Yufen Huang, Mahdi Soltanolkotabi, Xi-\nang Ren, and Salman Avestimehr. 2021. Fednlp:\nBenchmarking federated learning methods for nat-\nural language processing tasks. arXiv preprint\narXiv:2104.08815.\nRuixuan Liu, Fangzhao Wu, Chuhan Wu, Yanlin\nWang, Lingjuan Lyu, Hong Chen, and Xing Xie.\n2022. No one left behind: Inclusive federated learn-\ning over heterogeneous devices. arXiv preprint\narXiv:2202.08036.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2021. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. CoRR, abs/2110.07602.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019a.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nGuodong Long, Yue Tan, Jing Jiang, and Chengqi\nZhang. 2020. Federated learning for open banking.\nIn Federated learning, pages 240â€“254. Springer.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Aguera y Arcas. 2017.\nCommunication-efficient learning of deep networks\nfrom decentralized data. In Artificial intelligence and\nstatistics, pages 1273â€“1282. PMLR.\n9972\nJohn Nguyen, Jianyu Wang, Kshitiz Malik, Maziar San-\njabi, and Michael Rabbat. 2022. Where to begin? on\nthe impact of pre-training and initialization in feder-\nated learning. arXiv preprint arXiv:2210.08090.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nChen Qu, Weize Kong, Liu Yang, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Natural\nlanguage understanding with privacy-preserving bert.\nIn Proceedings of the 30th ACM International Con-\nference on Information & Knowledge Management,\npages 1488â€“1497.\nLiangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia,\nFeifei Wang, Ehsan Adeli, Li Fei-Fei, and Daniel\nRubin. 2022. Rethinking architecture design for tack-\nling data heterogeneity in federated learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10061â€“10071.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nAnit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Za-\nheer, Ameet Talwalkar, and Virginia Smith. 2018. On\nthe convergence of federated optimization in hetero-\ngeneous networks. CoRR, abs/1812.06127.\nCongzheng Song and Ananth Raghunathan. 2020. In-\nformation leakage in embedding models. In Pro-\nceedings of the 2020 ACM SIGSAC Conference on\nComputer and Communications Security, pages 377â€“\n390.\nDianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuan-\ntao Xie, and Weijian Sun. 2020. Feded: Federated\nlearning via ensemble distillation for medical relation\nextraction. In Proceedings of the 2020 conference on\nempirical methods in natural language processing\n(EMNLP), pages 2118â€“2128.\nGuangyu Sun, Matias Mendieta, Taojiannan Yang, and\nChen Chen. 2022. Exploring parameter-efficient fine-\ntuning for improving communication efficiency in\nfederated learning. arXiv preprint arXiv:2210.01708.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,\nBaptiste RoziÃ¨re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nOrion Weller, Marc Marone, Vladimir Braverman,\nDawn Lawrie, and Benjamin Van Durme. 2022. Pre-\ntrained models for multilingual federated learning.\narXiv preprint arXiv:2206.02291.\nMingbin Xu, Congzheng Song, Ye Tian, Neha Agrawal,\nFilip Granqvist, Rogier van Dalen, Xiao Zhang, Ar-\nturo Argueta, Shiyi Han, Yaqiao Deng, et al. 2022.\nTraining large-vocabulary neural language models by\nprivate federated learning for resource-constrained\ndevices. arXiv preprint arXiv:2207.08988.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitfit: Simple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nDun Zeng, Siqi Liang, Xiangjing Hu, Hui Wang, and\nZenglin Xu. 2023. Fedlab: A flexible federated learn-\ning framework. Journal of Machine Learning Re-\nsearch, 24(100):1â€“7.\nHaodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and\nGongshen Liu. 2022. Reduce communication costs\nand preserve privacy: Prompt tuning method in fed-\nerated learning. arXiv preprint arXiv:2208.12268.\nYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Da-\nmon Civin, and Vikas Chandra. 2018. Federated\nlearning with non-iid data. CoRR, abs/1806.00582.\nLigeng Zhu, Zhijian Liu, and Song Han. 2019. Deep\nleakage from gradients. Advances in neural informa-\ntion processing systems, 32.\n9973\n0.0 0.2 0.4 0.6 0.8\nDistance\n0\n2\n4\n6\nDensity\nMRPC =0.1\n0.0 0.2 0.4 0.6 0.8\nDistance\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nMRPC =1.0\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nDistance\n0\n1\n2\n3\n4\n5\n6\nMRPC =10.0\n0.0 0.2 0.4 0.6 0.8\nDistance\n0\n1\n2\n3\n4\n5\n6\nDensity\nSST-2 =0.1\n0.0 0.2 0.4 0.6 0.8\nDistance\n0.0\n0.5\n1.0\n1.5\nSST-2 =1.0\n0.0 0.1 0.2 0.3 0.4 0.5\nDistance\n0\n1\n2\n3\n4\n5\nSST-2 =10.0\n0.0 0.2 0.4 0.6 0.8\nDistance\n0\n1\n2\n3\n4\n5\nDensity\nQNLI =0.1\n0.0 0.2 0.4 0.6 0.8\nDistance\n0.0\n0.5\n1.0\n1.5\nQNLI =1.0\n0.0 0.2 0.4 0.6\nDistance\n0\n1\n2\n3\n4\nQNLI =10.0\n0.0 0.2 0.4 0.6 0.8\nDistance\n0\n1\n2\n3\n4\n5\nDensity\nQQP =0.1\n0.0 0.2 0.4 0.6 0.8\nDistance\n0.0\n0.5\n1.0\n1.5\n2.0\nQQP =1.0\n0.0 0.2 0.4 0.6 0.8\nDistance\n0\n1\n2\n3\n4\n5\nQQP =10.0\nFigure 8: Data distribution under different Î±. We report\nthe pairwise Jensenâ€“Shannon distance of the label dis-\ntribution between two clients.\n0 25 50 75 100 125 150 175\nMRPC\nSST-2\nQNLI\nQQP\n190\n190\n190\n190\n12.1\n12.1\n12.1\n12.1\n140.6\n140.5\n140.5\n140.5\n103.8\n42\n103.8\n42\n1\n1\n1\n1\nFedBF FedPF FedLR FedAP FedFT\n0 25 50 75 100 125 150 175\nMRPC\nSST-2\nQNLI\nQQP\n190\n190\n190\n190\n12.1\n12.1\n12.1\n12.1\n140.6\n140.5\n140.5\n140.5\n103.8\n42\n103.8\n42\n1\n1\n1\n1\nFedBF FedPF FedLR FedAP FedFT\nFigure 9: Normalized values of storage efficiencies of\nFedPETuning and FedFT on the other four tasks. Higher\nis better.\nA Non-IID Partitionings Results\nFigure 8 illustrates the data distribution under dif-\nferent Î±. It is observed that data distributed in\nclients with Î±as 0.1 has a large distance with each\nother, which has impact on the performance. Both\nÎ±as 1.0 and 10.0 are considered to produce a more\nuniform distribution.\nB Extra Results\nCommunication Analysis In this section, we il-\nlustrate the accuracy given the communication bud-\nget on the remaining four tasks in Figure 10. As can\nbe seen, PETuning methods consistently reduce the\ncommunication budget over several orders of mag-\nnitude while providing comparable performance.\nMoreover, PETuning methods (apart from FedBF)\nachieve acceptable accuracy ( 90% of fine-tuning)\non all the tasks, demonstrating the effectiveness of\nthese methods.\n102\n103\n104\n105\nCommunication Budget / MB\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90F1 Score\nMRPC\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n102\n103\n104\n105\nCommunication Budget / MB\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy (%)\nSST-2\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n102\n103\n104\n105\nCommunication Budget / MB\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy (%)\nQNLI\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n102\n103\n104\n105\nCommunication Budget / MB\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy (%)\nQQP\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\nFigure 10: Accuracy versus Communication Budget for\nall tuning methods. The horizontal dashed line indicates\nthe acceptable performance, which is 95% of the accu-\nracy of CenFT.\nFedFT FedAP FedLR FedPF FedBF\n5\n0\nRelative Accuracy (%)\nMRPC\n=0.1\n =10.0\nFedFT FedAP FedLR FedPF FedBF\n4\n2\n0\nSST-2\n=0.1\n =10.0\nFedFT FedAP FedLR FedPF FedBF\n40\n20\n0\nRelative Accuracy (%)\nQNLI\n=0.1\n =10.0\nFedFT FedAP FedLR FedPF FedBF\n20\n10\n0\nQQP\n=0.1\n =10.0\nFigure 11: Relative performance of FedPETuning and\nFedFT on RTE and MNLI under different Dirichlet\ndistributions.\n1 2 3\nEpoch\n80\n85\n90Accuracy (%)\nMRPC\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n1 2 3\nEpoch\n92\n93\n94\nSST-2\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n1 2 3\nEpoch\n88\n90\n92Accuracy (%)\nQNLI\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\n1 2 3\nEpoch\n84\n86\n88\n90\nQQP\nFedLR\nFedPF\nFedAP\nFedBF\nFedFT\nFigure 12: Accuracy comparison of PETuning methods\nand FT under different epochs.\n9974\nNon-IID Analysis Figure 11 presents the com-\nparisons of accuracy under different distributions\nby varying Î±. All the methods obtain better per-\nformance with Î± as 1.0 than Î± as 0.1, showing\nthat non-IID distribution has a negative impact on\nthe modelâ€™s performance. On the contrary, vary-\ning Î± from 1.0 to 10.0 has little impact in most\ncircumstances.\nLocal Training Epoch Analysis In Figure 12,\nwe show the accuracy of tuning methods trained\nwith different numbers of training epochs on the\nlocal clients. The accuracy of the relatively small\ndatasets (MRPC, SST-2) shows a faint decreasing\ntrend because of the over-fitting issue. All the tun-\ning methods benefit from more training epochs on\nrelatively large datasets (QNLI, QQP).\n9975\nACL 2023 Responsible NLP Checklist\nA For every submission:\nâ–¡\u0013 A1. Did you describe the limitations of your work?\nLimitation Section\nâ–¡\u0017 A2. Did you discuss any potential risks of your work?\nOur work does not involve potential risks\nâ–¡\u0013 A3. Do the abstract and introduction summarize the paperâ€™s main claims?\nAbstract and introduction\nâ–¡\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB â–¡\u0017 Did you use or create scientiï¬c artifacts?\nLeft blank.\nâ–¡ B1. Did you cite the creators of artifacts you used?\nNo response.\nâ–¡ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\nâ–¡ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciï¬ed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\nâ–¡ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiï¬es individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\nâ–¡ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\nâ–¡ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiï¬cant, while on small test sets they may not be.\nNo response.\nC â–¡\u0013 Did you run computational experiments?\n4\nâ–¡\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9976\nâ–¡\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.1.2\nâ–¡\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\nâ–¡\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4.1.2\nD â–¡\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\nâ–¡ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\nâ–¡ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participantsâ€™ demographic\n(e.g., country of residence)?\nNo response.\nâ–¡ D3. Did you discuss whether and how consent was obtained from people whose data youâ€™re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\nâ–¡ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\nâ–¡ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9977",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8706108331680298
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7260397672653198
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.6834856867790222
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5425781011581421
    },
    {
      "name": "Machine learning",
      "score": 0.49288320541381836
    },
    {
      "name": "Language model",
      "score": 0.48223838210105896
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46018537878990173
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4572886526584625
    },
    {
      "name": "Reinforcement learning",
      "score": 0.4383047819137573
    },
    {
      "name": "Federated learning",
      "score": 0.43419113755226135
    },
    {
      "name": "Code (set theory)",
      "score": 0.41291457414627075
    },
    {
      "name": "Source code",
      "score": 0.4124007225036621
    },
    {
      "name": "Distributed computing",
      "score": 0.3678750693798065
    },
    {
      "name": "Data mining",
      "score": 0.34344106912612915
    },
    {
      "name": "Computer network",
      "score": 0.1701115369796753
    },
    {
      "name": "Operating system",
      "score": 0.09004053473472595
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}