{
  "title": "LLMaAA: Making Large Language Models as Active Annotators",
  "url": "https://openalex.org/W4389518684",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2111773010",
      "name": "Ruoyu Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2106227931",
      "name": "Yanzeng Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2150204707",
      "name": "Yong-Liang Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2096867225",
      "name": "Ming Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2022120215",
      "name": "Lei Zou",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385573991",
    "https://openalex.org/W3176214425",
    "https://openalex.org/W2948367246",
    "https://openalex.org/W4206648492",
    "https://openalex.org/W3101345273",
    "https://openalex.org/W2251435463",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4318351203",
    "https://openalex.org/W1872312298",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3098049952",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W2951911250",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W3176770275",
    "https://openalex.org/W4321524373",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W3164309533",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4366735603",
    "https://openalex.org/W2012878613",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W4385572090",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4361807267",
    "https://openalex.org/W4287124696",
    "https://openalex.org/W3205626500",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3198490223",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2952370363",
    "https://openalex.org/W2343514605",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W2774918944",
    "https://openalex.org/W2951786554",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W4386566609",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W2625559849",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W4382603217",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W2795282075",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4312343407",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W4281790610",
    "https://openalex.org/W3194309076",
    "https://openalex.org/W4376167433",
    "https://openalex.org/W3210129272",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4399971973"
  ],
  "abstract": "Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13088–13103\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLLM AAA: Making Large Language Models as Active Annotators\nRuoyu Zhang1∗, Yanzeng Li1, Yongliang Ma2, Ming Zhou2, Lei Zou1\n1Wangxuan Institute of Computer Technology, Peking University, Beijing, China\n2Langboat Technology, Beijing, China\n{ry_zhang, zoulei}@pku.edu.cn, liyanzeng@stu.pku.edu.cn,\n{mayongliang, zhouming}@langboat.com\nAbstract\nPrevalent supervised learning methods in natu-\nral language processing (NLP) are notoriously\ndata-hungry, which demand large amounts of\nhigh-quality annotated data. In practice, acquir-\ning such data is a costly endeavor. Recently, the\nsuperior performance of large language mod-\nels (LLMs) has propelled the development of\ndataset generation, where the training data are\nsolely synthesized from LLMs. However, such\nan approach usually suffers from low-quality is-\nsues and requires orders of magnitude more la-\nbeled data to achieve satisfactory performance.\nTo fully exploit the potential of LLMs and make\nuse of massive unlabeled data, we propose LL-\nMAAA, which takes LLMs as annotators and\nputs them into an active learning loop to deter-\nmine what to annotate efficiently. To learn ro-\nbustly with pseudo labels, we optimize both the\nannotation and training processes: (1) we draw\nk-NN samples from a small demonstration pool\nas in-context examples, and (2) we adopt the\nautomatic reweighting technique to assign train-\ning samples with learnable weights. Compared\nwith previous approaches, LLM AAA features\nboth efficiency and reliability. We conduct\nexperiments and analysis on two classic NLP\ntasks, named entity recognition and relation ex-\ntraction. With LLM AAA, task-specific models\ntrained from LLM-generated labels can outper-\nform their teacher LLMs within only hundreds\nof annotated examples, which is much more\ncost-effective than other baselines1.\n1 Introduction\nLarge language models (LLMs) have exhibited re-\nmarkable few-shot performance in a wide range\nof tasks, with only a few demonstrations and well-\ndesigned prompts (Brown et al., 2020; Ding et al.,\n2022; Liu et al., 2023). However, with rapid ad-\n∗This work was done during an internship at Langboat\nTechnology.\n1Our code and data are available at https://github.\ncom/ridiculouz/LLMAAA.\nPrompt\nPLM\nLabeled \nData\nHuman\nAnnotator\nIn-Context \nExamples\nUnlabeled \nData\nLabeled \nData\nUnlabeled \nData PLM\nLabeled \nData\nSpecific\nModel\nActive Data \nAcquisition\n(a) Human annotation as supervision.\nPrompt\nPLM\nLabeled \nData\nHuman\nAnnotator\nIn-Context \nExamples\nUnlabeled \nData\nLabeled \nData\nUnlabeled \nData PLM\nLabeled \nData\nSpecific\nModel\nActive Data \nAcquisition\n(b) Text generation as supervision.\nPrompt\nPLM\nLabeled \nData\nHuman\nAnnotator\nIn-Context \nExamples\nUnlabeled \nData\nLabeled \nData\nUnlabeled \nData PLM\nLabeled \nData\nSpecific\nModel\nActive Data \nAcquisition\nIn-Context \nExamples\n(c) LLM AAA: Active LLM annotation as supervision.\nFigure 1: Comparing LLM AAA with other frameworks.\nWe actively acquire annotations from LLM for effi-\nciency, requiring little human effort.\nvancements comes vast potential risks in adopt-\ning LLMs for widespread downstream produc-\ntion applications. One of the main concerns is\nabout data privacy and security. Under the preva-\nlent “Language-Model-as-a-Service” (LMaaS, Sun\net al., 2022) setting, users are required to feed their\nown data, potentially including sensitive or private\ninformation, to third-party LLM vendors to access\nthe service, which increases the risk of data leak-\nage (Lyu et al., 2020; Yu et al., 2022; Li et al., 2023).\nBesides, LLMs usually consume abundant tokens\nby continuous requests to APIs, where the marginal\ncost and latency become substantial in large-scale\nor real-time applications, hindering LLMs’ practi-\ncal deployment in cost-sensitive scenarios (Goyal\net al., 2020; Cao et al., 2023).\nOn the other hand, training task-specific mod-\nels (TAMs) for NLP tasks necessitates extensive\namounts of labeled data. Due to the superior gener-\native capacity of LLMs, some researchers attempt\nto synthesize training data with text generation\n(Meng et al., 2022; Ye et al., 2022), as depicted\n13088\nin Figure 1. However, the generated text usually\nstruggles with low-quality issues and may exhibit\ndomain shifts with test data (Gao et al., 2023). To\nexploit the abundant unlabeled corpus, an alterna-\ntive is to employ LLMs as annotators, which gener-\nate labels in a zero-shot or few-shot manner. While\nthis approach seems promising, it is important to\nacknowledge that LLM-generated labels inevitably\ncontain noise, especially when applied to challeng-\ning tasks and domain-specific data (Agrawal et al.,\n2022; Kazemi et al., 2023). Besides, larger models\ncome with heavier expenses, and it is also crucial\nto reduce the annotation cost when the budget is\nrestricted.\nTo enhance the reliability (i.e. accuracy) of\nTAMs’ performance as well as to ensure the data\nefficiency in annotation cost, we propose LL-\nMAAA, an innovative framework that integrates\nactive learning into the LLM annotation process,\ni.e., making LLMs as Active Annotators. By ex-\nploring different active acquisition strategies, LL-\nMAAA enables the LLM to annotate more informa-\ntive instances that benefit model performance more.\nTo train TAMs reliably, we optimize both the an-\nnotation and training processes within LLM AAA\nframework. Firstly, we employ prompt engineer-\ning techniques to enhance LLMs’ performance by\n(1) selecting k-NN samples from a demonstration\npool as in-context examples, and (2) building fine-\nlevel descriptions aligned with natural language for\nunnatural labels (e.g., category labels in the RE\ntask). The valuable contextual information helps\nimprove the quality of LLM annotations substan-\ntially. During training, we adopt the automatic\nreweighting technique (Ren et al., 2018) to assign\nlearnable weights to the silver2 training samples.\nThis strategy allows the model to prioritize more\ninformative and representative samples while si-\nmultaneously reducing the impact of noisy annota-\ntions.\nWe evaluate LLM AAA on two practical NLP\ntasks: named entity recognition (NER) and relation\nextraction (RE). Experiments show that: (1) with\nsmall-scale gold data (~100 examples) serving for\ndemonstration and validation, the trained TAMs\ncan outperform their teacher LLMs within hun-\ndreds of silver samples via LLM AAA; (2) our\napproach is significantly more data efficient com-\npared to prevalent data generation methods, which\n2We refer gold data to ground-truth/human-labeled sam-\nples, and silver data to LLM-labeled samples.\nusually require large-scale synthetic training data\n(size varying from 10k to 200k, Ye et al., 2022; Gao\net al., 2023). These results confirm the potential\nof LLM AAA as a practical and cost-efficient solu-\ntion to make LLMs as good annotators. The TAMs\ncreated through our framework offer advantages in\nterms of task-specific performance, data privacy,\nand inference costs, which release the capacity of\nLLMs for real-world productivity.\nWe summarize our contributions as follows:\n• We propose LLM AAA, a framework to employ\nLLMs as annotators, featuring both efficiency\nand reliability.\n• LLM AAA is capable to train TAMs that outper-\nform teacher LLMs within hundreds of annotated\nsamples, on classic NLP tasks like NER and RE.\n• LLM AAA sheds light on the practical substi-\ntution of LLMs, with a cost-effective, privacy-\nensured, yet well-performing solution.\n2 Related Work\nLLM and In-Context Learning Large language\nmodels (LLMs), usually pretrained on large-scale\ncorpus to capture rich linguistic patterns and gener-\nate coherent text (Brown et al., 2020; Raffel et al.,\n2020; Chowdhery et al., 2022; OpenAI, 2023; Tou-\nvron et al., 2023), have shown remarkable perfor-\nmance in a wide range of NLP tasks (Min et al.,\n2021; Zhao et al., 2023). With the proposal of\nin-context learning (Brown et al., 2020), prompt\nengineering has been extensively explored to steer\nLLMs’ behavior for desired outcomes. These tech-\nniques design specific prompts or instructions to\nguide models’ outputs (Ding et al., 2022; Liu et al.,\n2023), either in rule-based (Shin et al., 2020) or\nlearning-based (Lester et al., 2021) manners. Re-\ncent trend focuses on the strong reasoning capabili-\nties of LLMs and enhances LLMs’ performance on\ncomplex task with chain-of-thought (CoT) prompt-\ning (Wei et al., 2023a). In general, prompt engineer-\ning improves the controllability and performance\nof LLMs in few-shot and zero-shot settings (Zhong\net al., 2021), and enables LLMs to solve spe-\ncific tasks, e.g. information extraction (Wei et al.,\n2023b; Wang et al., 2023).\nDataset Synthesis Supervised learning methods\nin NLP are often limited by high-quality anno-\ntated data. To address the bottleneck, researchers\nhave explored techniques to synthesize training\n13089\n…\nInput:     Soccer - late goals give Japan win over Syria.\nOutput:  [{\"span\": \"Japan\", \"type\": \"LOC\"},\n               {\"span\": \"Syria\", \"type\": \"LOC\"}]\nCompletion\nYou are a highly intelligent and accurate news domain \nnamed-entity recognition (NER) system. You take passage \nas input and your task is to recognize and extract specific \ntypes of named entities in that given passage and classify \ninto a set of following predefined entity types:\n[PER, LOC, ORG, MISC]\nYour output format must be in json form of:\n[{\"span\": span, \"type\": type}, ...]\nInput:     …\nOutput:  …\nInput:     Soccer - late goals give Japan win over Syria.\nOutput:\nPrompt Engineering\nLLM Annotator\nTask-Specific Model\nLarge \nunlabeled data\n Small \nlabeled data\nRobust TrainingActive Acquisition\nFigure 2: LLM AAA puts the LLM annotator in an active learning iteration, which mainly consists of three novel\ncomponents: (1) an LLM annotator optimized with prompt engineering that generates pseudo labels, (2) an active\nacquisition mechanism for efficient data selection, and (3) an automatic reweighting technique to ensure robust\nlearning with noisy labels. The annotation and training stages run iteratively and gradually produce labeled data for\ntask-specific models.\ndata with LLMs, either by annotation or by gen-\neration. Following the first line of research, Feng\net al. (2021); Chen et al. (2023) employ LLMs\nas unsupervised annotators to generate dialogue\ndatasets. Recently, AnnoLLM (He et al., 2023)\nmakes LLMs’ performance on par with crowd-\nsource annotators by chain-of-thought prompting\nand self-generated explanations. As a contempo-\nrary work, Bansal and Sharma (2023) use LLMs for\nannotation in the domain transfer setting. Under\nthe formulation of active learning, they propose\na new metric, conditional informativeness, that\nworks well with noisy labels. Among generation-\nbased methods, Wang et al. (2021) first use LLM\nwith few-shot prompts to generate training data.\nSchick and Schütze (2021) attempt to generate la-\nbeled text counterparts and text pairs for seman-\ntic textual similarity tasks. ZERO GEN (Ye et al.,\n2022) and SUNGEN (Gao et al., 2023) further ex-\ntend this practice to zero-shot learning by train-\ning small models with zero-shot LLM-generated\ndatasets. However, these approaches still suffer the\nlow-quality and domain-shift issues of the synthetic\ndata, and none of them consider the cost efficiency\nof data generation via LLMs.\nLabor Efficiency and Active Learning Active\nlearning is a technique proposed to minimize the\nannotation cost during the labeling process (Set-\ntles, 2009; Ren et al., 2021). A popular setting for\nactive learning is the pool-based paradigm, which\naims to select the most beneficial samples from\nan unlabeled data pool based on criteria including\nuncertainty (Lewis and Gale, 1994; Houlsby et al.,\n2011; Gal et al., 2017), diversity (Huang et al.,\n2010; Sener and Savarese, 2018), and hybrid ob-\njectives (Du et al., 2017; Yang et al., 2017; Ash\net al., 2020; Margatina et al., 2021). The selected\nsamples are annotated by human annotators and\nthen added into the labeled dataset iteratively.\n3 LLM as Active Annotator\nTo exploit LLMs’ superior few-shot performance\nand leverage abundant unlabeled data, we attempt\nto take LLM as annotator and train task-specific\nmodels for inference. An ideal process should be\nboth efficient and reliable: we want to learn TAMs\nrobustly with minimal LLM-generated labels.\nConcretely, our solution is to make LLMs as\nActive Annotator. As shown in Figure 2, LL-\nMAAA comprises three key components: (1) an\nLLM annotator that generates pseudo labels of\ngiven data, (2) an active acquisition mechanism\nfor efficient data selection, and (3) an automatic\nreweighting technique to ensure robust learning\nwith noisy labels. LLM AAA iterates the three\nstages to gradually produce stronger TAMs.\n3.1 Optimizing LLM as Better Annotator\nIn-context learning (i.e. PROMPTING ) enables\nLLM to conduct few-shot inference without fine-\ntuning. Given a manually-designed prompt T(·,·),\na demonstration set S= {xi,yi}k\ni=1 and the query\nexample xq, PROMPTING first builds a sentence\nT(S,xq), conditioned on which LLM then gener-\nates a text sequence\nyq = argmax\ny\nPLM(y|T(S,xq)).\nFinally, yq is mapped to the label space Y.\nDespite the decent abilities, previous studies\nshow that the design of task-specific prompts has\n13090\na large impact on performance, varying between\nnear state-of-the-art and random guess (Gao et al.,\n2021; Lu et al., 2022b). Finding the best prompts\nfor given tasks and given data points is intractable.\nHowever, there are several principles turn out to be\neffective, compared with plain instruction.\nk-NN Example Retrieval To select good in-\ncontext examples, Liu et al. (2022) propose ak-NN\nretrieval strategy, which first embeds the demon-\nstration pool Ddemo and query sample to vector\nrepresentations, and then retrieves the nearest k\nneighbors of the query to form its exemplars. The\nrationale behind this is that semantically similar\nexamples may help LLM answer the query better.\nFollowing their practice, we use Sentence-BERT\n(Reimers and Gurevych, 2019, 2020) to build the\nrepresentations.\nLabel Verbalizer In classification tasks, the sur-\nface forms of labels may induce difficulties and\nambiguities. Taking relation classification for in-\nstance, the label “ per:parents” can indicate ei-\nther “subject is the parent of object” or “object\nis the parent of subject”, depending on its defini-\ntion. To address this problem, we utilize a label\nverbalizer to transform the surface forms to natural\nlanguage descriptions with pre-defined templates\n(Sainz et al., 2021; Lu et al., 2022a), serving as\nfine-level guidance. The semantic templates we\nuse are shown in Table 7.\n3.2 Active Data Acquisition\nActive learning (AL) seeks to reduce labeling ef-\nforts by strategically choosing which examples to\nannotate. We consider the standard pool-based\nsetting, assuming that a large pool of unlabeled\ndata Dpool is available. AL loop starts with a seed\nlabeled set Dlabeled. At each iteration, we train\na model M on Dlabeled and then use acquisition\nfunction f(·,M) to acquire a batch Bconsisting of\nbexamples from Dpool. We then query the LLM\nannotator to label B. The labeled batch is then re-\nmoved from the pool Dpool and added to labeled\nset Dlabeled, and will serve as training data for the\nnext iteration. The process is repeated for ttimes.\nActive acquisition strategies generally maxi-\nmize either uncertainty or diversity. On one hand,\nuncertainty-based methods leverage model predic-\ntions to select hard examples. On the other hand,\ndiversity-based methods exploit the heterogeneity\nof sampled data. We will cover some common\nstrategies for thorough comparisons, and illustrate\nwith classification task for simplicity3.\nRandom We consider random selection as base-\nline, which samples uniformly from Dpool. Typi-\ncally pool data and test data share the same distri-\nbution, thus the sampled batch is expected to be\ni.i.d. with test data.\nMaximum Entropy Entropy is one of the most\nwidely used estimations of uncertainty (Settles,\n2009). Data for which the model Mhas the highest\nentropy are sampled for annotation according to\nargmax\nx∈Dpool\n−\n∑\ny∈Y\nPM(y|x) logPM(y|x).\nLeast Confidence Culotta and McCallum (2005)\npropose to sort examples with the probability as-\nsigned by M to predicted class ˆy, which samples\nargmax\nx∈Dpool\n(1 −PM(ˆy|x)) .\nK-Means Diversity sampling intends to select\nbatches of data that is heterogeneous in the feature\nspace. Following Yuan et al. (2020), we apply k-\nmeans clustering to the l2-normalized embeddings\nof M4, and sample the nearest neighbors of the k\ncluster centers.\n3.3 Robust Learning with Noisy Labels\nLLM annotators inevitably produce noisy labels,\nespecially with harder tasks and domain-specific\ndata. To stay robust against training label bias, we\nadopt the automatic reweighting technique (Ren\net al., 2018) to assign different weights to training\nexamples adaptively.\nWe assume that a small-scale validation set Dval\nwith clean labels (e.g. human annotations) is avail-\nable throughout learning, with |Dval|≪|D pool|.\nConcisely, automatic reweighting learns sample\nweights w by a meta-learning objective that min-\nimizes validation loss w.r.t. w, and uses online\napproximation to eliminate the nested loop of opti-\nmization. The training process of TAM is shown\nin Algorithm 1.\n4 Tasks\nWe instantiate LLM AAA with two tasks: named\nentity recognition (NER) and relation extraction\n3Adaptation to other settings (e.g. sequence tagging) will\nbe introduced in § 4.\n4We use BERT family asM’s encoder, and the embeddings\nrefer to BERT output.\n13091\nAlgorithm 1: Automatic Reweighting\nInput: Noisy data Dtrain, clean data Dval,\nbatch size n, m, initial parameter θ0,\nstep S\nOutput: Trained parameter θS\nfor s= 0,...,S −1 do\nBtrain ←SampleBatch(Dtrain,n)\nBval ←SampleBatch(Dval,m)\n{ˆyi\ntrain}n\ni=1 ←Forward(Btrain,θs)\n// build computation graph with\nautomatic differtiation\nϵ←0; ltrain ←∑n\ni=1 ϵiL(yi\ntrain,ˆyi\ntrain)\n∇θs ←Backward(ltrain,θs)\nˆθs ←θs −α∇θs\n{ˆyi\nval}m\ni=1 ←Forward(Bval,ˆθs)\nlval ← 1\nm\n∑m\ni=1 L(yi\nval,ˆyi\nval)\n∇ϵ←Backward(lval,ϵ)\n// truncate weights to zero, and\nnormalize to one\n˜w←max(−∇ϵ,0);\nw← ˜w∑\nj ˜w+δ(∑\nj ˜w)\nˆltrain ←∑n\ni=1 wiL(yi\ntrain,ˆyi\ntrain)\n∇θs ←Backward(ˆltrain,θs)\nθs+1 ←OptimizerStep(θs,∇θs)\n(RE). We opt for two simple yet effective models\nas TAMs, and leave other choices for future study.\n4.1 Named Entity Recognition\nFormulation NER aims to extract entities {ei}\nfrom text x, where ei can be expressed as a con-\ntinuous span of sequences with predefined type.\nWe consider the flat scenario (i.e. no overlapping\nentities), in which NER can be reformulated as a\nsequence tagging problem with BIO label.\nTo smoothly adapt uncertainty-based active func-\ntions from classification task to sequence tagging,\nwe provide three pooling options: average, sum,\nand max. In practice, we adopt average and sum\noperations for better empirical performance.\nModel Following Devlin et al. (2019), we lever-\nage BERT to convert tokens into vectorized fea-\ntures, and use a linear classifier with activation to\npredict the {class}-BIO label for each token.\n4.2 Relation Extraction\nFormulation Given subject entity esubj and ob-\nject entity eobj in a sentence, RE classifies their\nrelation into a predefined set R∪{NA}.\nModel We use the same model architecture as\nBaldini Soares et al. (2019), which first encloses\nentity spans with special tokens[E] and [\\E], then\nencodes the sentence with BERT. The concatenated\nembedding of subject and object is fed into a linear\nclassifier with activation for final prediction.\n5 Experiments and Analysis\n5.1 Setup\nDataset We experiment with three different NLP\ndatasets: Chinese OntoNotes 4.0 (Weischedel et al.,\n2011) and English CoNLL03 (Tjong Kim Sang\nand De Meulder, 2003) for NER, and Re-TACRED\n(Stoica et al., 2021) for RE. For Re-TACRED, we\nselect a subset describing personal relationships\nand balance the NA relation instances to the orig-\ninal portion. Details of dataset statistics are de-\nscribed in Appendix A. We report the precision,\nrecall, and micro F1 for both tasks.\nBaselines We compare LLM AAA with the fol-\nlowing baselines: (1) PROMPTING . The prompt-\nbased direct inference on test data, using the same\nengineering techniques as LLM AAA’s teacher\nLLMs. (2) SUPERVISED . The TAMs are trained\non clean-labeled data Dval used in LLM AAA’s\ndemonstration/validation. (3) ZERO GEN (Ye et al.,\n2022). Zero-shot data synthesis method via text\ngeneration. (4) FEWGEN. A data synthesis method\nthat enhances ZERO GEN with in-context examples\nuniformly sampled from the demonstration pool.\nImplementation We use ChatGPT5 as LLM an-\nnotator for main experiments, and adopt BERT\n(Devlin et al., 2019; Cui et al., 2021) as TAM’s en-\ncoder. We also explore with other LLM annotators,\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI,\n2023), in §6. We randomly sample 100 exam-\nples from the original validation sets as gold data,\nreusing the same set for demonstration Ddemo and\nvalidation Dval. We use the original training sets\nas Dpool and randomly initialize seed labeled set\nDlabeled with a size of 50 and acquire 50 samples\nper batch for 9 iterations, which generates 500 sil-\nver annotated samples in total. We generate 500\nand 5,000 samples viaZERO GEN and FEWGEN for\ncomparison. TAMs under all settings are trained\nthree times with different random seeds, and we\nreport the mean and standard deviation in the re-\nsults. The training process and hyperparameters\nare detailed in Appendix B.\n5https://openai.com/blog/chatgpt\n13092\nMethod #Data Chinese OntoNotes 4.0 English CoNLL03 Re-TacRED-subset Avg.\nF1P R F1 P R F1 P R F1\nPROMPTING 100 / - 67.72 74.02 70.73 79.18 83.59 81.33 64.21 86.68 73.77 75.28\nSUPERVISED 100 / - 70.541.33 75.661.14 73.000.84 77.160.31 78.520.52 77.940.10 62.362.35 91.881.90 74.282.05 75.07\nZEROGEN - / 500 62.101.70 71.870.68 66.621.05 71.142.64 71.102.08 71.070.36 61.607.21 78.255.37 68.573.14 68.75\n- / 5000 62.000.92 72.842.50 66.970.61 74.233.32 71.781.97 72.992.61 51.460.82 94.280.65 66.570.66 68.84\nFEWGEN 100 / 500 71.784.34 71.061.66 71.351.80 73.062.31 69.872.23 71.432.21 69.212.49 77.8411.21 73.126.46 71.97\n100 / 5000 68.050.81 75.170.48 71.430.52 75.932.67 72.931.80 74.402.20 68.073.08 92.245.23 78.200.99 74.68\nLLMAAA-random 100 / 500 68.852.36 71.632.02 70.212.00 77.692.11 80.751.49 79.171.32 63.239.60 97.752.63 76.416.48 75.26\nLLMAAA-confidence 100 / 50072.662.42 75.491.67 74.000.44 82.910.83 82.780.63 82.840.31 71.494.76 93.285.18 80.792.63 79.21\nTable 1: Evaluation results for LLM AAA and other baselines across three different datasets, using ChatGPT as\nLLM backbone. We report the mean and standard deviation of three separate runs for each method. Since we set the\ntemperature to 0 in PROMPTING , its results are deterministic and we only run evaluation once. We also denote the\namount of data (gold/silver) that TAM used for training.\nRelation Generated Data\nper:parents\nMarysubj’s father isAdamobj.\nTomsubj’s mother,Maryobj, lives in New York.\nMichelle Obamasubj’s parents areFraser C. Robinson III and Marian Shields Robinsonobj.\nper:children\nMikesubj’s son is namedJackobj.\nLilysubj’s children areAlex and Bellaobj.\nSarahsubj has a daughter namedEmilyobj.\nTable 2: A case study of generated data withZERO GEN on Re-TACRED. We leverage ChatGPT as the text generator,\nand the full prompts we use can be found in Appendix B.3.\nWe follow consistent principles in prompt de-\nsign. Empirically, we find that in-context examples\nbring marginal benefit to RE, while label verbalizer\nis a technique specifically designed for the classi-\nfication task. Therefore, We apply k-NN example\nretrieval to NER and label verbalizer to RE sepa-\nrately. We set kto 5 for all experiments, including\nFEWGEN. Refer to Appendix B.3 for full prompts.\n5.2 Overall Results\nTable 1 denotes our main experiment results. LL-\nMAAA with least confidence as acquisition func-\ntion outperforms all comparative baselines across\nall datasets, with 74.00%, 82.84% and 80.79%\nF1 scores on Chinese OntoNotes 4.0, English\nCoNLL03 and Re-TACRED-subset, respectively.\nComparing with PROMPTING (i.e. the LLM an-\nnotator), LLM AAA shows steady improvement\n(4% in average score) with TAMs of much fewer\nparameters and lower inference latency, indicating\nthat LLM AAA provides a decent substitute for\nLLMs in real-world deployments. LLM AAA also\nsurpasses SUPERVISED , where TAMs are trained\non clean-labeled but smaller-scale data. This sug-\ngests that LLM AAA is capable of deriving rich\nknowledge beyond the limited demonstration/vali-\ndation set on unlabeled data, which benefits gener-\nalization.\nWe also notice that generation-based methods,\ni.e. ZERO GEN and FEWGEN, fail to establish on-\npar results, even with 10×more data in zero-shot\nsetting. We argue that the text-generation abilities\nof LLMs are exaggerated in complex scenarios. To\ndemystify the illusion, we devise a case study on\nRe-TACRED, as is shown in Table 2. ZERO GEN\ntends to generate simple templated sentences that\ndeviate from the news domain, i.e. the original\ncorpus of Re-TACRED. These results may induce\nlow-quality and domain-shift issues that hamper\nTAMs’ performance. FEWGEN’s performance im-\nproves with in-context examples, however, it still\nlags far behind LLM AAA. In contrast, exploit-\ning the unlabeled data effectively alleviates these\nproblems with much higher efficiency, where only\nhundreds of annotated samples are sufficient for\nsatisfactory performance.\n5.3 Ablations\n5.3.1 Effects of Prompt Engineering\nThough ChatGPT can well follow human instruc-\ntions in general, it still struggles with difficult tasks\nand domain-specific data. We compare the infer-\n13093\n100 200 300 400 500\n# of labeled samples\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0F1 score\nChinese OntoNotes 4.0\n100 200 300 400 500\n# of labeled samples\n55\n60\n65\n70\n75\n80\n85\nEnglish CoNLL03\n100 200 300 400 500\n# of labeled samples\n50\n55\n60\n65\n70\n75\n80\n85\nRe-TACRED-subset\nMaximum Entropy Least Confidence K-Means Random\nFigure 3: LLM AAA’s performance with different active acquisition strategies, shown by F1 scores. The dashed\nlines denote PROMPTING ’s results. For each method, we report the mean and standard deviation of three runs\ninitialized with different random seeds.\n60\n65\n70\n75F1 score\nChinese OntoNotes 4.0, Maximum Entropy\n60\n65\n70\n75\nChinese OntoNotes 4.0, Least Confidence\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\nChinese OntoNotes 4.0, K-Means\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\nChinese OntoNotes 4.0, Random\n55\n60\n65\n70\n75\n80F1 score\nEnglish CoNLL03, Maximum Entropy\n55\n60\n65\n70\n75\n80\n85\nEnglish CoNLL03, Least Confidence\n55\n60\n65\n70\n75\n80\nEnglish CoNLL03, K-Means\n55\n60\n65\n70\n75\n80\nEnglish CoNLL03, Random\n100 200 300 400 500\n# of labeled samples\n50\n60\n70\n80F1 score\nRe-TACRED-subset, Maximum Entropy\n100 200 300 400 500\n# of labeled samples\n50\n60\n70\n80\nRe-TACRED-subset, Least Confidence\n100 200 300 400 500\n# of labeled samples\n40\n50\n60\n70\n80\nRe-TACRED-subset, K-Means\n100 200 300 400 500\n# of labeled samples\n50\n60\n70\n80\nRe-TACRED-subset, Random\nw/ reweighting w/o reweighting\nFigure 4: Results for analyzing the effects of automatic reweighting. We remove the online-approximated sample\nweights and train TAMs with standard loss objectives for ablation. The dashed lines denote PROMPTING ’s\nperformance. For each method, we report the mean and standard deviation of F1 scores within three different runs.\nOntoNotes CoNLL Re-TacRED\nBase Instruction 49.62 55.74 70.94\n+k-NN Examples 70.73 81.33 -\n+Label Verbalizer - - 73.77\nTable 3: Comparison results between plain instructions\nand optimized prompts in F1 scores.\nence performance of plain instructions with opti-\nmized prompts in Table 3. Without k-NN example\nretrieval module (i.e. in zero-shot manners), the\nLLM annotator is unable to extract entities well in\nNER task, shown by a drastic drop in F1 scores\n(21% on OntoNotes and 25% on CoNLL). This re-\nsult highlights the need for demonstrations, where\nLLMs’ zero-shot performance is unsatisfactory. In\naddition, the label verbalizer can help align unnatu-\nral labels with natural language descriptions, which\nimproves the performance in RE (from 70.94%\nto 73.77% in F1). These findings emphasize that\nprompt engineering is crucial for building strong\nannotators, and incorporating similar and aligned\ncontexts contributes to better inference.\n5.3.2 Accelerating with Active Learning\nFigure 3 shows LLM AAA performance with dif-\nferent active learning strategies across all datasets.\nUncertainty-based methods, i.e. maximal en-\ntropy and least confidence, perform significantly\nbetter than the random baseline, with faster con-\n13094\nBackbone Method P R F1\nGPT-3 PROMPTING 41.82 22.77 29.49\nLLMAAA-confidence57.26 56.09 56.63\nChatGPT PROMPTING 67.72 74.02 70.73\nLLMAAA-confidence72.66 75.49 74.00\nGPT-4 PROMPTING 68.70 79.42 73.68\nLLMAAA-confidence73.47 76.42 74.90\nTable 4: Results on Chinese OntoNotes 4.0 forPROMPT -\nING and LLM AAA with different LLMs. LLM AAA\nuses least confidence as the acquisition function, and\nannotates 500 samples for TAM training.\nvergence and higher F1 scores at the end of itera-\ntions. It is worth noting that (1) uncertainty-based\nmethods are able to achieve on-par performance\nwith random selection with only 30%~40% train-\ning data, (2) they surpass PROMPTING consistently\nwithin 500 LLM-annotated training samples. In\nsummary, uncertainty-based active learning strate-\ngies enable LLM AAA to be more efficient and\nmore capable.\nThough k-means clustering encourages diversity\nin feature space, it only outperforms random sam-\npling on OntoNotes, while yielding similar results\non CoNLL03 and Re-TacRED. This suggests that it\nmay require more training data for finetuned BERT\nto learn informative representations, and such a\ndiversity-based method may fail in low-resource\nenvironments, e.g. at early iterations of the loop.\n5.3.3 Reweighting Helps Robust Training\nFigure 4 depicts the learning trials with and without\nthe automatic reweighting technique. We observe\nthat reweighting training samples consistently help\nimprove performance across all datasets and meth-\nods. This finding proves that the training process\nof TAMs is more noise-tolerant with automatic\nreweighting, even with only a small-scale clean-\nlabeled set (100 samples) serving for validation.\nIn particular, the performance gain from auto-\nmatic reweighting is more prominent on Onto-\nNotes and Re-TACRED, and diminishes on Co-\nNLL03. We argue that automatic reweighting plays\na crucial role when the LLM annotators are rela-\ntively poor (as in OntoNotes and Re-TACRED).\nIn such scenarios, the online approximation of the\nvalidation set serves as an effective estimation of\nunbiased data distribution, and helps prevent TAMs\nfrom overfitting noisy labels.\n6 Analysis\n6.1 LLM AAA with Different Annotators\nTo guarantee the universal effectiveness of LL-\nMAAA, we further investigate the performance\nwith other LLM annotators, i.e. GPT-3 (Brown\net al., 2020) and GPT-4 (OpenAI, 2023). Due to\nbudgetary considerations, we opt to restrict our ex-\nperiments to OntoNotes. The precision, recall and\nF1 score are shown in Table 4. The results indicate\nthat LLM AAA benefits from better annotators with\ncontinuous improvements, and more importantly,\nTAMs trained by LLM AAA outperform the LLM\nannotators consistently. The student outperforms\nthe weak teacher by a large margin (27% in F1 for\nGPT-3). As the teacher grows stronger, this gap\nnarrows down. This trend meets our expectations:\nsince student TAMs are trained with a fixed budget\nof data (500 samples), enhancing the capabilities\nof teacher LLMs will gradually approach the per-\nformance ceiling of the students. More annotation\nbudget and more powerful TAMs can help extend\nthis limit, while we leave the exploration for future\nresearch.\n6.2 Why Can Students Outperform Teachers?\nAn interesting observation across our experiments\nis that student TAMs trained with generated labels\ncan outperform teacher LLMs, i.e. LLM AAA >\nPROMPTING , even without sample reweighting, as\nshown by Figure 4. Such results partially align with\nprevious findings in knowledge distillation (Wang,\n2021; Song et al., 2021) and pseudo-label-based\nlearning (Lee, 2013; Sanyal et al., 2022; Min et al.,\n2023), which share similar yet slightly different\nsettings with LLM AAA.\nWe attempt to further explain the phenomenon\nin a simplified setting, where we consider a binary\nclassification task that predicts y for x ∼D(x),\nwhere D(x) is discrete as in language space. For\nsimplicity, we let y = 1denote the correct label\nand y = 0 otherwise. We first make the natural\nassumption that the teacher’s performance is above\nchance, i.e. the accuracy p> 0.5. Querying teacher\nfor target sample xt will generate pseudo label\nyt ∼Bernoulli(p). If the student is a universal\nfunction approximator S(x; θ) that outputs a scalar\nas probability that ˆy = 1, then minimizing the\n13095\ncross-entropy loss\nmin\nθ\nExt∼D(x),yt∼B(p)[−ytlog(S(xt; θ))\n−(1 −yt) log(1−S(xt; θ))]\nwill reach optimal with S(x; θ) =p. Usually we\npredict with heuristics that ˆy= 1if S(x; θ) > 0.5.\nWith the previous assumption, we have ˆy = 1,\nwhich means that Salways predicts correctly. This\ntoy case nonetheless explains that an ordinary\nteacher can raise better students. Though teacher\nLLMs are deterministic for specific x when the\ntemperature is set to 0, their predictions are yet\nstatistically random in D(x), where the same con-\nclusion holds.\nWe shall point out that the above discussion con-\nsiders a much-relaxed setting, where we attempt\nto account for an intuitive understanding on why\nstudents outperform teachers in the hard label dis-\ntillation problem. We leave the rigorous theoretical\nanalysis for future work.\n7 Conclusion\nIn this work, we propose LLM AAA, a framework\nthat uses LLMs as active annotators to address the\nchallenges of data scarcity in NLP tasks. With ac-\ntive learning strategies, LLM AAA allows LLMs to\nlabel more informative samples that promote TAMs\nperformance efficiently. We also optimize for reli-\nability within the framework, which uses prompt\nengineering techniques and automatic reweighting\nto improve annotation quality and to reduce the im-\npact of noisy labels, respectively. Experiments on\nNER and RE tasks demonstrate the effectiveness of\nLLM AAA. The evaluation results highlight the ef-\nficiency and reliability of LLM AAA. Trained with\njust hundreds of LLM-annotated samples, TAMs\nare able to outperform their teacher LLMs sub-\nstantially. Besides, LLM AAA is also much more\nefficient compared to prevalent data generation\nmethods, which usually require orders of mag-\nnitude more synthetic training data. These find-\nings reveal that LLM AAA offers a cost-effective,\nprivacy-ensured, yet well-performing solution to\napply LLMs in practical scenarios.\nLimitations\nAlthough LLM AAA demonstrates success in trans-\nferring and exceeding LLMs’ capabilities with\ncheaper TAMs, it does come with certain limita-\ntions. The main difference between the setting\nin LLM AAA and previous zero-shot generation-\nbased methods, e.g. ZERO GEN and SUNGEN,\nis that we use an unlabeled data pool Dpool and\noracle-annotated data Ddemo/Dval, to provide ex-\ntra knowledge. However, we shall point out that\nunlabeled text is readily available in many real-\nworld scenarios, thus it is practical to make the\npool-based assumption. Additionally, in complex\ntasks where zero-shot inference fails (like NER\nin our experiments), it is costly yet necessary to\nincorporate demonstrations for LLMs. In LL-\nMAAA, we strive for minimizing human efforts\nby restricting the oracle-annotated data to a small\nscale (100 samples), and exploiting the same data\nfor demonstration and validation. Another bottle-\nneck is the model capacities of teacher LLMs and\nstudent TAMs. On one hand, a weak teacher is\nunable to teach excellent students that are ready\nto be used for applications (e.g. GPT-3). On the\nother hand, TAMs are bounded depending on their\narchitectures. When the teacher surpasses the ceil-\ning, it will be theoretically impossible for students\nto outperform teachers. Despite these cases, we\nare optimistic that LLM AAA is effective in most\nsituations.\nWe adopt the proprietary GPT family as annota-\ntors in experiments, which are provided by OpenAI\nin a black-box manner. Though powerful, this prac-\ntice may raise several concerns, e.g. the potential\nexposure to test data. Nevertheless, we believe that\ngiven the comprehensive analysis in §6.1, it does\nnot affect the effectiveness of our method.\nEthics Statement\nThis work utilizes publicly available benchmark\ndatasets, and we respect and adhere to their licenses\nand agreements. Our proposed method involves\nthe use of LLMs for data annotation, as discussed\nin GPT3Mix (Yoo et al., 2021). This paradigm\nstill poses several challenges, such as the poten-\ntial biases or toxic content in the generated data.\nTherefore, it is crucial to exercise caution when\nemploying our method to invoke LLMs for generat-\ning data and when utilizing TAMs trained on such\ngenerated data. Applying our work to downstream\ntasks such as NER and RE may result in issues such\nas mis-extraction and false information, and may\nfail in some cases. When employing our method,\nit is essential to consider using debiasing (Schick\net al., 2021) or manual checking to mitigate these\nconcerns.\n13096\nAcknowledgements\nWe appreciate the anonymous reviewers for their\nvaluable advice on this manuscript. We would\nlike to thank Tianhao Wu for the insightful dis-\ncussion and feedback. This work was supported\nby NSFC under grant 61932001 and U20A20174.\nThe corresponding author of this paper is Lei Zou\n(zoulei@pku.edu.cn).\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are few-shot clinical information extractors.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1998–2022, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\nJordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy,\nJohn Langford, and Alekh Agarwal. 2020. Deep\nbatch active learning by diverse, uncertain gradient\nlower bounds. In International Conference on Learn-\ning Representations.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling,\nand Tom Kwiatkowski. 2019. Matching the blanks:\nDistributional similarity for relation learning. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2895–\n2905, Florence, Italy. Association for Computational\nLinguistics.\nParikshit Bansal and Amit Sharma. 2023. Large lan-\nguage models as annotators: Enhancing generaliza-\ntion of nlp models at minimal cost.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nQingqing Cao, Bhargavi Paranjape, and Hannaneh Ha-\njishirzi. 2023. Pumer: Pruning and merging tokens\nfor efficient vision language models.\nWanxiang Che, Mengqiu Wang, Christopher D. Man-\nning, and Ting Liu. 2013. Named entity recognition\nwith bilingual constraints. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 52–62, Atlanta,\nGeorgia. Association for Computational Linguistics.\nMaximillian Chen, Alexandros Papangelis, Chenyang\nTao, Seokhwan Kim, Andy Rosenbaum, Yang Liu,\nZhou Yu, and Dilek Hakkani-Tur. 2023. PLACES:\nPrompting language models for social conversation\nsynthesis. In Findings of the Association for Com-\nputational Linguistics: EACL 2023, pages 844–868,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and\nZiqing Yang. 2021. Pre-training with whole word\nmasking for chinese bert. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n29:3504–3514.\nAron Culotta and Andrew McCallum. 2005. Reducing\nlabeling effort for structured prediction tasks. In Pro-\nceedings of the 20th National Conference on Artifi-\ncial Intelligence - Volume 2, AAAI’05, page 746–751.\nAAAI Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022.\nOpenPrompt: An open-source framework for prompt-\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics:\n13097\nSystem Demonstrations, pages 105–113, Dublin, Ire-\nland. Association for Computational Linguistics.\nBo Du, Zengmao Wang, Lefei Zhang, Liangpei Zhang,\nWei Liu, Jialie Shen, and Dacheng Tao. 2017. Ex-\nploring representativeness and informativeness for\nactive learning. IEEE Transactions on Cybernetics,\n47(1):14–26.\nXiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin,\nand Ting Liu. 2021. Language model as an annota-\ntor: Exploring DialoGPT for dialogue summarization.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1479–1491, Online. Association for Computational\nLinguistics.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017.\nDeep Bayesian active learning with image data. In\nProceedings of the 34th International Conference\non Machine Learning , volume 70 of Proceedings\nof Machine Learning Research , pages 1183–1192.\nPMLR.\nJiahui Gao, Renjie Pi, Lin Yong, Hang Xu, Jiacheng\nYe, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang,\nZhenguo Li, and Lingpeng Kong. 2023. Self-guided\nnoise-free data generation for efficient zero-shot\nlearning. In The Eleventh International Conference\non Learning Representations.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan T. Chakaravarthy, Yogish Sabhar-\nwal, and Ashish Verma. 2020. Power-bert: Accel-\nerating BERT inference via progressive word-vector\nelimination. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research , pages 3690–3699.\nPMLR.\nXingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin,\nHang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan\nDuan, and Weizhu Chen. 2023. Annollm: Making\nlarge language models to be better crowdsourced\nannotators.\nNeil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and\nMáté Lengyel. 2011. Bayesian active learning for\nclassification and preference learning.\nSheng-Jun Huang, Rong Jin, and Zhi-Hua Zhou. 2010.\nActive learning by querying informative and represen-\ntative examples. In Advances in Neural Information\nProcessing Systems 23: 24th Annual Conference on\nNeural Information Processing Systems 2010. Pro-\nceedings of a meeting held 6-9 December 2010, Van-\ncouver, British Columbia, Canada, pages 892–900.\nCurran Associates, Inc.\nMehran Kazemi, Sid Mittal, and Deepak Ramachandran.\n2023. Understanding finetuning for factual knowl-\nedge extraction from language models.\nDong-Hyun Lee. 2013. Pseudo-label: The simple and\nefficient semi-supervised learning method for deep\nneural networks. In Workshop on challenges in rep-\nresentation learning, ICML, volume 3, page 896.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nDavid D. Lewis and William A. Gale. 1994. A se-\nquential algorithm for training text classifiers. In\nProceedings of the 17th Annual International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR ’94, page 3–12, Berlin,\nHeidelberg. Springer-Verlag.\nYansong Li, Zhixing Tan, and Yang Liu. 2023. Privacy-\npreserving prompt tuning for large language model\nservices.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nKeming Lu, I-Hung Hsu, Wenxuan Zhou,\nMingyu Derek Ma, and Muhao Chen. 2022a.\nSummarization as indirect supervision for relation\nextraction. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 , pages\n6575–6594, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022b. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n13098\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nLingjuan Lyu, Xuanli He, and Yitong Li. 2020. Differ-\nentially private representation for NLP: Formal guar-\nantee and an empirical study on privacy and fairness.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 2355–2365, Online.\nAssociation for Computational Linguistics.\nKaterina Margatina, Giorgos Vernikos, Loïc Barrault,\nand Nikolaos Aletras. 2021. Active learning by ac-\nquiring contrastive examples. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 650–663, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language mod-\nels: Towards zero-shot language understanding. In\nAdvances in Neural Information Processing Systems.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heinz, and Dan Roth. 2021. Re-\ncent advances in natural language processing via\nlarge pre-trained language models: A survey. arXiv\npreprint arXiv:2111.01243.\nZeping Min, Qian Ge, and Cheng Tai. 2023. Why the\npseudo label based semi-supervised learning algo-\nrithm is effective?\nOpenAI. 2023. Gpt-4 technical report.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4512–4525,\nOnline. Association for Computational Linguistics.\nMengye Ren, Wenyuan Zeng, Bin Yang, and Raquel\nUrtasun. 2018. Learning to reweight examples for\nrobust deep learning. In Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML\n2018, Stockholmsmässan, Stockholm, Sweden, July\n10-15, 2018, volume 80 of Proceedings of Machine\nLearning Research, pages 4331–4340. PMLR.\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao\nHuang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and\nXin Wang. 2021. A survey of deep active learning.\nACM computing surveys (CSUR), 54(9):1–40.\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka,\nAnder Barrena, and Eneko Agirre. 2021. Label ver-\nbalization and entailment for effective zero and few-\nshot relation extraction. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1199–1212, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nSunandini Sanyal, Sravanti Addepalli, and R Venkatesh\nBabu. 2022. Towards data-free model stealing in a\nhard label setting. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 15284–15293.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in NLP. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nOzan Sener and Silvio Savarese. 2018. Active learn-\ning for convolutional neural networks: A core-set\napproach. In International Conference on Learning\nRepresentations.\nBurr Settles. 2009. Active learning literature survey.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nDandan Song, Jing Xu, Jinhui Pang, and Heyan Huang.\n2021. Classifier-adaptation knowledge distillation\nframework for relation extraction and event detec-\ntion with imbalanced data. Information Sciences,\n573:222–238.\nGeorge Stoica, Emmanouil Antonios Platanios, and\nBarnabas Poczos. 2021. Re-tacred: Addressing\nshortcomings of the tacred dataset. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(15):13843–13850.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022. Black-box tuning for\nlanguage-model-as-a-service. In Proceedings of the\n39th International Conference on Machine Learning,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 20841–20855. PMLR.\n13099\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,\nFei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.\n2023. Gpt-ner: Named entity recognition via large\nlanguage models.\nZi Wang. 2021. Zero-shot knowledge distillation from\na decision-based black-box model. In Proceedings of\nthe 38th International Conference on Machine Learn-\ning, ICML 2021, 18-24 July 2021, Virtual Event ,\nvolume 139 of Proceedings of Machine Learning\nResearch, pages 10675–10685. PMLR.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023a. Chain-of-thought prompting\nelicits reasoning in large language models.\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, et al. 2023b. Zero-\nshot information extraction via chatting with chatgpt.\narXiv preprint arXiv:2302.10205.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2011. Ontonotes 4.0. Linguistic\nData Consortium LDC2011T03.\nLin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang,\nand Danny Z. Chen. 2017. Suggestive annotation: A\ndeep active learning framework for biomedical image\nsegmentation. In Medical Image Computing and\nComputer Assisted Intervention MICCAI 2017: 20th\nInternational Conference, Quebec City, QC, Canada,\nSeptember 11-13, 2017, Proceedings, Part III, page\n399–407, Berlin, Heidelberg. Springer-Verlag.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022. ZeroGen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11653–11669, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo\nLee, and Woomyoung Park. 2021. GPT3Mix: Lever-\naging large-scale language models for text augmen-\ntation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2225–2239,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A. Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\nSergey Yekhanin, and Huishuai Zhang. 2022. Differ-\nentially private fine-tuning of language models. In\nThe Tenth International Conference on Learning Rep-\nresentations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net.\nMichelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-\nGraber. 2020. Cold-start active learning through self-\nsupervised language modeling. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7935–7948,\nOnline. Association for Computational Linguistics.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n35–45, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.\n2021. Adapting language models for zero-shot learn-\ning by meta-tuning on dataset and prompt collections.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2856–2878, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nA Dataset Statistics\nIn this section, we describe the statistics and pre-\nprocessing of each dataset in detail.\nOntoNotes 4.0 (Weischedel et al., 2011) is a\nlarge corpus comprising various genres of text\n(news, web text, etc.) in three languages (English,\nChinese, and Arabic) with structural information\nand shallow semantics, and has been widely used\nfor NER. We use the Chinese text data and take\nthe same data split as Che et al. (2013), which uses\nfour most common entity types, i.e. PER (person),\nLOC (location), ORG (organization) and GPE (geo-\npolitical entities). We truncate token length within\n512 (i.e. split long input to multiple chunks) to\n13100\nTraining Validation Testing\nper:age 421 256 208\nper:nationality 295 222 115\nper:parents 182 69 106\nper:children 275 114 55\nper:siblings 211 33 66\nper:spouse 271 189 73\nno_relation 2,482 1,324 934\nTotal 4,137 2,207 1,557\nTable 5: Statistics of relation types in each split of Re-\nTACRED-subset. We replace the label \"per:origin\"\nwith \"per:nationality\" for clarity.\nfit in BERT input limit. The processed data con-\ntains 15,724/4,301/4,346 samples for training/vali-\ndation/testing, respectively.\nThe English CoNLL 2003 shared task (Tjong\nKim Sang and De Meulder, 2003) is a NER\ndataset that contains four entity types: PER (per-\nson), LOC (location), ORG (organization), and\nMISC (miscellaneous entities), which consists of\n14,041/3,250/3,453 sentences for training/valida-\ntion/testing.\nRe-TACRED (Stoica et al., 2021) is a revised\nversion of TACRED (Zhang et al., 2017), a large-\nscale crowdsource-annotated RE dataset. It orig-\ninally has 40 relation types. Including all these\ntypes will lead to much longer prompts, which may\nexceed the API length limit and receive responses\nwith higher latency. Therefore, we opt to select a\nsubset of relations that describe personal relation-\nships for study. We keep all these relation instances\nin training/validation/testing sets, and balance the\nNA relation instances to the original portion. The\nstatistics for each relation type is shown in Table 5.\nFor all three datasets, we randomly sample 100\nexamples from the original validation sets and\nreuse the same data for demonstration Ddemo and\nvalidation Dval. We use the full training sets as\nthe initial Dpool, from which we randomly sample\nactive learning’s seed labeled sets Dlabeled with a\nsize of 50.\nB Implementations\nB.1 LLM Inference APIs\nWe access OpenAI APIs by Azure service. The\nAPI we use for each model is depicted in Table\n6. Since ChatGPT and GPT-4 will continue to be\nupdated, they may generate different responses as\ntime changes, even when the temperature is 0.\nModel API\nGPT-3 text-curie-001\nChatGPT gpt-35-turbo\nGPT-4 gpt-4\nTable 6: Azure OpenAI service API that we use.\nB.2 Training Task-Specific Models\nFor all experiments that train TAMs for inference\n(i.e. LLM AAA, ZERO GEN, FEWGEN and SUPER -\nVISED ), we repeat each with three random seeds,\nresulting in different parameter initialization and\nrandom data sampling. We report the mean and\nstandard deviation in our results.\nWe use bert-base-cased (Devlin et al., 2019)\nas TAMs’ encoders with a learning rate of 5e-5 for\nEnglish data (CoNLL03 and Re-TACRED), and\nchinese-bert-base-wwm (Cui et al., 2021) with a\nlearning rate of 2e-5 for Chinese data (OntoNotes\n4.0). The learning rate of other parameters (i.e.\nlinear classifiers) is set to 1e-4. We optimize the\nmodels via AdamW (Loshchilov and Hutter, 2019),\nwith ϵ= 1e-6, under a linear warmup schedule for\nthe first 6% steps. We train all TAMs with a batch\nsize of 8 for 40 epochs and take the checkpoint\nwith the highest validation performance for final\nprediction.\nB.3 Prompts\nThe full prompts we use for annotation are shown\nin Table 7. In Re-TACRED, we provide prompts\nboth with and without verbalized labels. To add\ndemonstration, we insert each sample’s text into\ninput and label to output. The target sample is\nadded to the last input, and the last output is left\nblank for prediction.\nWe also show the prompts for generation in Ta-\nble 8. We use them similarly to annotation. In the\nzero-shot setting, to help models generate desired\noutputs, we use a default example to inform LLMs\nabout the output format.\nC Annotation Examples\nWe show two annotation examples of correct/par-\ntially wrong annotations from the CoNLL 2003\nNER dataset in Listing 1. The first example is\nexactly correct, and the second example contains\nhallucinations that do not exist in ground truth:\n\"April\", \"March\", and \"Thursday\".\n13101\nTask Prompting\nCoNLL 03\nDescription\nYou are a highly intelligent and accurate news domain named-entity recognition (NER)\nsystem. You take passage as input and your task is to recognize and extract specific types of\nnamed entities in that given passage and classify into a set of following predefined entity\ntypes: [person (PER), location (LOC), organization (ORG), miscellaneous entity (MISC)]\nYour output format must be in json form of: [“span”: span, “type”: type, ...]\nInstructionThe span must be exactly the same as the original text, including white spaces.\nFormat Input: “Input: {}”\nOutput: “Output: {}”\nOntoNotes 4.0\nDescription\n你是一名通用领域的命名实体识别（NER）标注者，给定一段输入文本和NER类\n型，你需要以json格式返回NER的span和类型。\n类型：[人物（PER），组织机构（ORG），地缘政治实体（GPE），地理位置\n（LOC）]\n输出格式：[\"span\": span, \"type\": type, ...]\nFormat Input: “输入：{}”\nOutput: “输出：{}”\nRe-TACRED\n(Original)\nDescription\nGiven a sentence, and two entities within the sentence, classify the relationship between the\ntwo entities based on the provided sentence. If no relation of interest exists, strictly return\n“no_relation”. All possible relationships are listed below:\nInstruction\n- per:age\n- per:parents\n- per:spouse\n- per:siblings\n- per:children\n- per:nationality\n- no_relation\nFormat\nInput: “Sentence: {}”\nOutput: “Relationship: {}”\nStruct: “e1: {}\ne2: {}”\nRe-TACRED\n(Verbalized)\nDescription\nGiven a sentence, and two entities within the sentence, classify the relationship between the\ntwo entities based on the provided sentence. If no relation of interest exists, strictly return\n“no_relation”. All possible relationships and explanations are listed below:\nInstruction\n- per:age : the age of {e1} is {e2}\n- per:parents : {e1}’s parent is {e2}\n- per:spouse : {e1}’s spouse is {e2}\n- per:siblings : {e1} is the sibling of {e2}\n- per:children : {e1}’s children is {e2}\n- per:nationality: {e1}’s nationality is {e2}\n- no_relation : {e1} has no known relations to {e2}\nFormat\nInput: “Sentence: {}”\nOutput: “Relationship: {}”\nStruct: “e1: {}\ne2: {}”\nTable 7: Annotator’s prompts for each task.\n{\n\" text \":\" Celtic ’s Jackie McNamara , who did well with last season ’s successful\nunder -21 team , earns a call -up to the senior squad .\" ,\n\" labels \":[{\" span \":\" Celtic \" ,\" type \":\" ORG \"} ,{\" span \":\" Jackie McNamara \" ,\" type \":\" PER\n\"}]\n},\n{\n\" text \":\" Finland ’s trade surplus rose to 3.83 billion markka in April from 3.43\nbillion in March , the National Customs Board ( NCB ) said in a statement on\nThursday .\" ,\n\" labels \":[{\" span \":\" NCB \" ,\" type \":\" ORG \"} ,{\" span \":\" Finland \" ,\" type \":\" LOC \"} ,{\" span \":\"\nNational Customs Board \" ,\" type \":\" ORG \"} ,{\" span \":\" April \" ,\" type \":\" MISC \"} ,{\" span\n\":\" March \" ,\" type \":\" MISC \"} ,{\" span \":\" Thursday \" ,\" type \":\" MISC \"}]\n}\nListing 1: Annotation examples.\n13102\nTask Prompting\nCoNLL 03\nDescription\nYou are an intelligent text data generator. Generate {} high-quality and diverse sentences in\nnews domain containing entities for the following types:\n[person (PER), location (LOC), organization (ORG), miscellaneous entity (MISC)]\nWrite one sample per line. No other output.\nFormat Example: “Example: {}”\nOutput: “Output: {}”\nDefault\nExample {“text”: text, “entities”: [{“name”: name, “type”: type}]}\nOntoNotes 4.0\nDescription\n你是一名新闻领域的文本生成助手。生成{}个流畅、通顺、多样的中文句子，并包\n含下面这些类型的命名实体（entity）：\n[人名（PER），组织机构名（ORG），地缘政治实体（GPE），地理位置\n（LOC）]\n每行输出一个样本，不要有任何额外的输出。\nFormat Example: “示例：{}”\nOutput: “输出：{}”\nDefault\nExample {“text”: text, “entities”: [{“name”: name, “type”: type}]}\nRe-TACRED\nDescription\nYou are an intelligent text data generator. Generate {} high-quality and diverse sentences in\nnews domain containing relational triplet for the following relation types:\n- per:age : the age of SUBJ is OBJ\n- per:parents : SUBJ’s parent is OBJ\n- per:spouse : SUBJ’s spouse is OBJ\n- per:siblings : SUBJ is the sibling of OBJ\n- per:children : SUBJ’s children is OBJ\n- per:nationality: SUBJ’s nationality is OBJ\n- no_relation : SUBJ has no known relations to OBJ\nWrite one sample per line in json format. Subject and object must appear in the sentence. No\nother output.\nFormat Example: “Example: {}”\nOutput: “Output: {}”\nDefault\nExample {“text”: text, “subject”: subject, “object”: object, “relation”: relation}\nTable 8: Generator’s prompts for each task.\n13103",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8451642990112305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6631386280059814
    },
    {
      "name": "Annotation",
      "score": 0.6616513729095459
    },
    {
      "name": "Task (project management)",
      "score": 0.6528531312942505
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6457735300064087
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.5545462965965271
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5450950860977173
    },
    {
      "name": "Exploit",
      "score": 0.5324443578720093
    },
    {
      "name": "Natural language processing",
      "score": 0.503983199596405
    },
    {
      "name": "Machine learning",
      "score": 0.4969375431537628
    },
    {
      "name": "Relationship extraction",
      "score": 0.48125407099723816
    },
    {
      "name": "Labeled data",
      "score": 0.47382840514183044
    },
    {
      "name": "Relation (database)",
      "score": 0.44506359100341797
    },
    {
      "name": "Training set",
      "score": 0.4121130406856537
    },
    {
      "name": "Information extraction",
      "score": 0.3383433222770691
    },
    {
      "name": "Data mining",
      "score": 0.19419434666633606
    },
    {
      "name": "Power (physics)",
      "score": 0.08894360065460205
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}