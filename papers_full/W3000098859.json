{
  "title": "Language Models Are An Effective Patient Representation Learning Technique For Electronic Health Record Data",
  "url": "https://openalex.org/W3000098859",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4296539907",
      "name": "Steinberg, Ethan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4298501228",
      "name": "Jung, Ken",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4229192131",
      "name": "Fries, Jason A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296563962",
      "name": "Corbin, Conor K.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226819782",
      "name": "Pfohl, Stephen R.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226819787",
      "name": "Shah, Nigam H.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2078841894",
    "https://openalex.org/W2189162242",
    "https://openalex.org/W2404901863",
    "https://openalex.org/W2768083064",
    "https://openalex.org/W2514071032",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2947607756",
    "https://openalex.org/W2259469853",
    "https://openalex.org/W2481271618",
    "https://openalex.org/W2985962305",
    "https://openalex.org/W2906311950",
    "https://openalex.org/W1956559956",
    "https://openalex.org/W168564468",
    "https://openalex.org/W2935996710",
    "https://openalex.org/W3201352722",
    "https://openalex.org/W2511950764",
    "https://openalex.org/W2963208729",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2557074642",
    "https://openalex.org/W2395172628",
    "https://openalex.org/W2284851926",
    "https://openalex.org/W1028494640",
    "https://openalex.org/W2768348081",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963421326",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3000686922",
    "https://openalex.org/W2255847468",
    "https://openalex.org/W2897007327",
    "https://openalex.org/W2577592800",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2132755184",
    "https://openalex.org/W2963271116",
    "https://openalex.org/W2964301648",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2072773380",
    "https://openalex.org/W3001393923",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W2518582440",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2969881216",
    "https://openalex.org/W2950035161",
    "https://openalex.org/W2769265955",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2799486981",
    "https://openalex.org/W342727659"
  ],
  "abstract": "Widespread adoption of electronic health records (EHRs) has fueled the development of using machine learning to build prediction models for various clinical outcomes. This process is often constrained by having a relatively small number of patient records for training the model. We demonstrate that using patient representation schemes inspired from techniques in natural language processing can increase the accuracy of clinical prediction models by transferring information learned from the entire patient population to the task of training a specific model, where only a subset of the population is relevant. Such patient representation schemes enable a 3.5% mean improvement in AUROC on five prediction tasks compared to standard baselines, with the average improvement rising to 19% when only a small number of patient records are available for training the clinical prediction model.",
  "full_text": "Language Models Are An Eﬀective Representation Learning\nTechnique For Electronic Health Record Data\nEthan Steinberg, Ken Jung, Jason A. Fries, Conor K. Corbin, Stephen R. Pfohl, Nigam H. Shah\nMay 14, 2020\nAbstract\nWidespread adoption of electronic health records (EHRs) has fueled the development of using machine\nlearning to build prediction models for various clinical outcomes. This process is often constrained by\nhaving a relatively small number of patient records for training the model. We demonstrate that using\npatient representation schemes inspired from techniques in natural language processing can increase\nthe accuracy of clinical prediction models by transferring information learned from the entire patient\npopulation to the task of training a speciﬁc model, where only a subset of the population is relevant.\nSuch patient representation schemes enable a 3.5% mean improvement in AUROC on ﬁve prediction\ntasks compared to standard baselines, with the average improvement rising to 19% when only a small\nnumber of patient records are available for training the clinical prediction model.\nKeywords\nElectronic health record, representation learning, transfer learning, risk stratiﬁcation, machine learning\n1 Introduction\nThe widespread use of electronic health records (EHRs) combined with the power of machine learning has the\npotential to reduce healthcare costs and improve quality of care [1, 2, 3, 4]. EHR data has been used to learn\nprediction models for outcomes such as mortality [5], sepsis [6], future cost of care [7], 30-day readmission [8]\nand others [9, 10]. The outputs of these clinical prediction models facilitate risk stratiﬁcation and targeted\nintervention to improve the quality of care. [11, 12]. To date, most clinical prediction models use a small\nnumber of features and are trained using a small number of patient records [13].\nThe complexity of EHRs poses many obstacles for training clinical prediction models. EHR data is\nvariable length, high dimensional and sparse, with complex temporal and hierarchical structure. They are\ncomprised of irregularly spaced visits spread across years, with each visit containing a subset of thousands of\npossible diagnosis, procedure, and medication codes, as well as laboratory test results, unstructured text, and\nimages. In contrast, most oﬀ-the-shelf machine learning algorithms expect a ﬁxed length vector of features as\ninput. Deﬁning a transformation of patient records into such a ﬁxed length representation is often a manual\nprocess that is time consuming and task-dependent, leaving much of the temporal and hierarchical structure\nof EHRs underutilized when training clinical prediction models.\nRecent work on training clinical prediction models has used deep neural networks in an attempt to\nleverage the information inherent in the structure of EHRs, to directly capture the structure of medical\ndata while training the model for a given clinical outcome (e.g., mortality or 30 day readmissions) [9].\nSuch an “end-to-end” formulation is appealing because it has led to ground-breaking accuracy in computer\nvision and natural language processing (NLP) without requiring manual feature engineering. However, this\napproach does not seem to provide consistent gains when applied to electronic health records. Comparisons\nwith simple yet strong baselines [9, 14] found that end-to-end neural network models provide minimal or no\naccuracy advantage over count-based representations combined with logistic regression or gradient boosted\ntrees. One possible explanation for this limited improvement is that deep learning models typically require\n1\narXiv:2001.05295v2  [cs.CL]  12 May 2020\nlarge training datasets and EHR datasets are limited by the number of patients with a given outcome in a\nparticular health system’s data.\nResearchers in NLP and computer vision, when faced with small datasets, often use the technique of\ntransfer learning to achieve gains in accuracy in small data situations [15, 16]. Transfer learning posits\nthat it is possible to train a model for one task on a large dataset and then ﬁne tune that model for a\ndiﬀerent task using a smaller dataset in order to achieve better performance on the second task than would\nbe achieved by training a model de novo. The choice of task used for pre-training is critical, with much of the\ncurrent work in transfer learning focusing on designing a good task that helps capture useful structure that\ncan be shared across tasks [16]. One common pre-training task that performs reasonably well for natural\nlanguage processing is language modeling. Language modeling consists of learning a generative sequence\nmodel for text. After performing pre-training, the learned information stored in that model then needs to\nbe transferred to a particular task of interest. Representation learning is a type of transfer learning that\nfocuses on performing that transfer by constructing ﬁxed length representations which are then reused for\ndownstream tasks. Transfer learning is especially compelling for training clinical prediction models using\nEHR data because it is often the case that the number of patients available in a training set for a given\noutcome is a small fraction of all the patients in an institution’s EHR system [17].\nOur core hypothesis is that it is possible to use data from large numbers of patients to learn reusable,\nﬁxed length representations that improve the accuracy of clinical prediction models trained on smaller\nsubsets of patients. There has been some prior work on applying representation learning methods to EHR\ndata [18, 19, 20, 21]. However, these proposed representation learning techniques only capture parts of the\nEHR (such as visits [19] or codes [20, 21]), and the relative performance of these methods against each other\nand against simple, count based representations is unknown.\nIn this work we propose an improved generative sequence model for EHR data (a “clinical language\nmodel”) and show that this clinical language model can be used to derive representations in an approach we\nrefer to as clinical language model based representations(CLMBR). We empirically evaluated the eﬀectiveness\nof this approach for training models on ﬁve prediction tasks as compared to published representation learning\ntechniques. We compared clinical prediction models trained using CLMBR with clinical prediction models\ntrained using simple count representations and with end-to-end trained deep neural networks for the same\noutcomes, as illustrated in Figure 1. We investigated how the performance gains on clinical prediction models\nusing learned representations varied as a function of the amount of data available for training the clinical\nprediction model. Finally, we showed how the clinical language model used in this work provides better\nrepresentations than a previously published clinical language model [22].\n1.1 Related Work\n1.1.1 Deep Neural Network Based Clinical Prediction Models Using EHR Data\nRecent work on training clinical prediction models using EHR data focuses on deep neural network models\ntrained in an end-to-end manner for an outcome of interest. Clinical prediction models have been built for\nmany outcomes, such as all-cause mortality [5], heart failure [23, 24, 25], COPD [26], unplanned readmissions\n[27, 28], and future hospital admissions [29]. These eﬀorts generally propose novel neural net architectures\nand report performance gains over baseline models. However, Rajkomar et al [9] reported that logistic\nregression using a simple bag of words based representation performed very close to or within the margin of\nerror of an ensemble of three complex neural net models for three outcomes (inpatient mortality, readmissions\nand long length of stay). Similarly, Chen et al [14] found that neural networks were consistently outperformed\nby gradient boosted trees and random forests on a range of clinical outcomes. These seemingly conﬂicting\nresults require further investigation of the beneﬁts of using neural networks.\n1.1.2 Representation Learning\nRepresentation learning is used in computer vision and NLP to mitigate the impact of limited training\ndata [15]. Prior work on representation learning for EHR data primarily follows work in natural language\nprocessing because of similarities in the structure of data. For example, a document in natural language\ncan be viewed as a sequence of words, and representations can be learned for either single words or entire\nsequences. Analogously, a patient’s longitudinal EHR can be seen as a “document” consisting of a sequence\n2\nof diagnosis, procedure, medication, and laboratory codes. Note that this discussion is not about processing\nthe textual content of clinical notes via natural language processing.\nRepresentation learning for documents in natural language settings commonly focuses on learning word\nand document level representations. Word level representations are ﬁxed length vectors for each word\nlearned through information theory and linear algebra [30] or neural networks [31]. Here, the aim is to\nlearn a representation that anticipates surrounding context words (e.g., in this sentence, the context of\n“representation” includes “learn” and “anticipates”). The end result is a ﬁxed length vector representation\nof each word which can then be used for tasks such as question answering and sentiment analysis [32, 33]. In\ncontrast, document level representations are ﬁxed length vectors that capture salient properties of the whole\ndocument. A classic technique for doing so is Latent Semantic Indexing (LSI) [34], which combines both\nsingular value decomposition and term frequency-inverse document frequency to learn a low dimensional\nvector representation of a document with the goal of maximizing the ability to reconstruct document term\nfrequencies.\nCurrently, the most eﬀective representation learning techniques for natural language focus on building\nbetter document level representations by learning language models. A language model is a probabilistic\nmodel of sequences of words, often formulated as a neural network with millions of parameters that capture\n(or model; hence the phrase language model) the language generation process by predicting a word at a time,\neither sequentially with recurrent neural networks [15] or via masking with transformer models [16].\nRepresentation Learning For Electronic Health Records\nAnalogous to word level representations, it is possible to treat medical codes in the EHR as words and learn\nrepresentations for medical codes by adapting word2vec to deal with the lack of ordering of medical codes\nwithin an encounter [20, 21]. Choi et al [21] used the code vectors to learn models that predict heart failure.\nExtending to document level representations, in follow up work, Choi et al simultaneously learned medical\n‐ ‐\nFigure 1: An overview of the diﬀerent approaches to training a predictive model for clinical outcomes: feature\nengineering, end-to-end neural network modeling, and representation learning through an approach such as\nclinical language modeling based representations (CLMBR).\n3\ncode and patient level representations [19]. However, later evaluations found this approach was only a little\nbetter than several other baselines in predicting congestive heart failure [35]. Miotto et al [18] learned patient\nlevel representations using autoencoders, reporting signiﬁcantly better performance for training models that\npredict future diagnosis codes over the next year. However, in Choi et al [19], stacked autoencoders were\nfound to be no better than other baselines at predicting the next encounter’s diagnosis codes.\nResearchers have also applied language modeling to EHRs. Prior work by Choi et al [22] proposed a\nlanguage model (named DoctorAI) that predicts a subset of medical codes appearing in a sequence of patient\nencounters. They reported that a simple Gated Recurrent Unit (GRU) architecture performs quite well for\nthis task. The DoctorAI language model used high level (i.e., 3 digit) diagnosis and medication codes and did\nnot use laboratory tests or procedure codes. These choices enabled DoctorAI to make assumptions allowing a\nsoftmax probability transformation and a ﬂat code output space to reduce computational complexity. They\nmeasure how well this language model captures the series of codes in EHR data and show that a GRU\ndoes better than several simpler baselines. However, Choi et al never evaluated whether such a language\nmodel could be used to improve performance on clinical prediction models. Therefore, the utility of learning\ngeneral purpose representations of EHR data for developing more accurate clinical prediction models remains\nunclear.\n2 Materials and Methods\nWe evaluated the performance of four categories of representations ( Counts, Word2Vec, LSI, and CLMBR)\nused as inputs to a logistic regression and to gradient boosted trees for predicting ﬁve outcomes. Logistic\nregression and gradient boosted trees were chosen because they are widely used to train clinical prediction\nmodels and often perform quite well [14]. As an additional baseline, we also report results of a clinical\nprediction model trained as an end-to-end GRU, which directly used the raw EHR data and internally\nlearned a representation during the process of training for a particular clinical outcome. Figure 2 shows an\noverview of the experimental set up.\nEHR Data\nX\nRepresentation\nCounts\nLSI\nWord2Vec\nDemographics\nDiagnoses\nProcedures\nMed orders\nLab test orders\nLogistic\nRegression\nGradient \nBoosted Trees\nClinical Outcome\nAbnormal HbA1c\nICU Transfer\nLong Admission\nMortality\nX\nExperiments\nCLMBR\nClinical Prediction \nModel Type\nReadmission\nFigure 2: Overview of experiments evaluating representation learning methods using EHR data. We evalu-\nated four representation learning methods with two model types to train clinical prediction models for ﬁve\noutcomes.\n2.1 Data\nAll experiments were conducted on de-identiﬁed EHR data from Stanford Hospital and Lucile Packard\nChildren’s Hospital. The data comprises 3.4 million patient records spanning from 1990 through 2018. The\nstudy was done with approval by Stanford University’s Institutional Review Board. We treated each patient’s\nrecord as a sequence of days d1, . . . , dN, ordered by time. Each day was associated with a set of medical\ncodes for diagnoses, procedures, medication orders, and laboratory test orders (ICD10, CPT or HCPCS,\n4\nTable 1: Deﬁnitions of Clinical Outcomes\nOutcome Deﬁnition Time of Prediction\nInpatient Mortality A patient death occurring during an inpatient stay At admission\nLong Admission A patient stay of seven or more days in the hospital At admission\nICU Transfer Transfer of the patient to the ICU the following day Every day of an inpatient stay\n30-day Readmission A patient readmitted to the hospital within 30 days At discharge\nAbnormal HbA1c An HbA1c value > 6.5% for a non-diabetic patient Before the test result is returned\nTable 2: Characteristics of the dataset for each clinical outcome\nOutcome Name Num Labels Num Positives Num Unique Patients\nInpatient Mortality 212,599 4,294 130,708\nLong Admission 212,636 48,508 130,719\nICU Transfer 761,658 8,094 101,999\n30-day Readmission 187,866 29,693 112,264\nAbnormal HbA1c 83,550 1,651 51,654\nRXCUI, and LOINC codes respectively) recorded on that day. In this study, we did not use quantitative\ninformation such as laboratory test results or vital sign measurements. We also did not use clinical notes,\nimages, or explicit linkages between codes (e.g., diagnosis codes entered to justify procedures, as used in Choi\net al [35]). In total, there were 21,664 codes after ﬁltering for codes that occurred in the records of at least\n25 patients. Patient demographic data (gender, race and ethnicity) was encoded by assigning corresponding\ncodes to the date of birth of the patient.\n2.2 Experimental Setup\nWe compared the eﬀectiveness of simple count based and learned representations in terms of the discrimi-\nnation accuracy of predictive models for ﬁve clinical outcomes across a range of training set sizes. Table 1\nprovides a description of the clinical outcomes; Table 2 describes the dataset for each clinical outcome.\n2.2.1 Data Splits\nData was split into training, development, and test sets by time: data through December 31, 2015 was used\nfor training, data from January 1, 2016 through July 1, 2016 was used for hyperparameter tuning, and data\nfrom August 1, 2016 through August 1, 2017 was used as a held out test set. We adopted this design because\nof potential non-stationarity in EHRs, such that this scheme provides a more unbiased estimate of real world\nperformance than time-agnostic patient splits [36]. Note that even though patients may have been included\nin multiple splits, examples consist of both a patient and a time of prediction, and the times of prediction\ndo not overlap between the splits.\n2.2.2 Clinical Prediction Models\nWe used each representation (details in section 2.3) as input to two types of models to train a clinical\nprediction model for each clinical outcome. The ﬁrst was a simple linear model: logistic regression with L2\nregularization. The L2 strength parameter was swept in a grid for every power of 10 between 10 −6 to 106.\nWe used the Sci-kit Learn’s logistic regression implementation with the LBFGS algorithm [37]. The second\nmodel type was gradient boosted trees, which can model interactions and non-linearities in the data. We\nperformed hyperparameter tuning by grid search, varying the learning rate between 0.02, 0.1, and 0.5; and\nthe number of leaf nodes in each base tree between 10, 25, and 100. Early stopping with 500 max trees was\nused for selecting the number of trees. We used the LightGBM [38] implementation of gradient boosting.\n5\n2.2.3 Subsampling Experiments\nWe also evaluated clinical prediction models trained using representations derived from smaller datasets\nto test the hypothesis that representation learning provides greater beneﬁts as sample sizes decrease. We\nperformed experiments in which training and development sets were subsampled without replacement, with\nstratiﬁed sampling of the training and development sets to enforce a ﬁxed positive label prevalence of 10%.\nThe total sample sizes were 100, 200, 400, 800, 1,600, and 3,200, with 70% and 30% of each sample drawn\nfrom the training and development splits respectively. The subsampled training and development splits were\nthen used for clinical prediction model tuning and ﬁtting. This process was repeated 10 times in order to\nprovide estimates of variance due to sampling of the training and development sets for the performance\nmetrics.\n2.2.4 End-to-End Neural Network Clinical Prediction Models\nTo conﬁrm the utility of general purpose learned representations, it is necessary to quantify the degree to\nwhich they diﬀer from the end-to-end setup, especially for large sample sizes where end-to-end models tend\nto perform especially well. To this end, we also trained end-to-end recurrent neural net models for each\noutcome. These models did not use any of the learned representations, and instead operated directly on the\nraw data itself, i.e., a sequence of observed codes. We used the same architecture as the language models\nexcept that the output of the GRU was fed directly into a simple logistic regression layer to predict the\nclinical outcomes. A hyperparameter search was performed independently for each outcome in order to\nprovide a fair comparison. See Appendix C for the hyperparameter grid of the end-to-end GRU models. See\nAppendix D for the best performing hyperparameters for each clinical prediction model. The diﬀerence with\nthe general purpose representation was that each clinical prediction model could have learned a diﬀerent\npatient representation scheme.\n2.3 Representations\nWe examine four categories of representations for their utility in terms of the accuracy of predictive models\nfor ﬁve clinical outcomes.\n2.3.1 Count Based Representations\nThe simplest representation we considered was counts of each code in the EHRs. This representation is\nwidely used as a baseline, and in Rajkomar et al [9] it resulted in excellent accuracy of regularized logistic\nregression based predictive models for three clinical outcomes.\nWe also evaluated two enhancements to the basic counts representation: time binning and ontology ex-\npansion. Time binning counts occurrences of a code in diﬀerent time buckets separately, and has been used\nin prior work [5, 9]. We used time buckets of 0-30 days, 30-180 days, 180-365 days, and 365+ days from\nthe reference time. These representations were very high dimensional and sparse because there are many\ncodes, most of which occurred in a very few patients. Ontology expansion is a commonly used technique that\nmitigates this problem by using ontologies (knowledge bases that specify hierarchical relationships between\nconcepts, e.g., “Type 1 diabetes mellitus with ketoacidosis” is a type of “Type 1 diabetes mellitus”) to\n“densify” these representations [23]. For example, if we observed the ICD10 code E10.1 (“Type 1 diabetes\nmellitus”), we also counted that as an occurrence of the ancestor codes E10 (“Type 1 diabetes mellitus”) and\nE08-E13 (“Diabetes mellitus”). We used the Uniﬁed Medical Language System (UMLS) [39] and mapped\ncodes to their ancestors within their respective hierarchies when applicable (ICD10 for diagnoses, CPT or\nMTHH for procedures, and ATC for medications). Note that this procedure increased the dimensionality of\nthe representation to 36,617 codes because many ancestor codes were not present in the original representa-\ntion.\nWe thus evaluated four variations of count based representations, one for each combination of ontology\nexpansion and time binning. These representations were used as the baseline, comprising simple, non-learned\nrepresentations.\n6\n2.3.2 Word2Vec Representation\nAdapting Word2Vec to patient EHR data requires managing the unordered nature of codes occurring on\na given day. Prior work [21, 20] recommends randomly ordering the codes assigned on a given day into a\nsequence for input to word2vec. We implemented this strategy to construct embeddings for every code in our\ndata, with an embedding size of 300 using gensim’s word2vec implementation [40]. We also evaluated code\nembeddings generated from data augmented by ontology expansion as described above. Finally, in order to\nconstruct patient level representations from the code embeddings, we evaluated combining code embeddings\nby taking the element-wise mean, and by concatenating the element-wise min, max and mean vectors as\ndescribed in [32]. These two ways of combining code emebeddings along with (or without) the ontology\nexpansion resulted in four variations of word2vec based representations.\n2.3.3 Latent Semantic Indexing Representations\nWe applied LSI to construct patient level representations by treating each patient’s EHR up to a randomly\nsampled time point as a “document” in which each code is a “word.” Following our count based representa-\ntions 2.3.1, we ran LSI with and without ontology expansion and evaluated representation sizes of 400 and\n800. We thus evaluated four diﬀerent LSI representations, again using gensim’s implementation of LSI [40].\n2.3.4 Clinical Language Model Based Representations - CLMBR\nThe core idea behind language model based representations is that they can capture global information about\nthe sequence of tokens (such as words or disease codes) that is later useful for training a model to predict\na clinical outcome. In EHRs, the sequences that we are trying to capture consist of days when a patient\ninteracted with the health system. Mathematically, the language modeling objective is to estimate the\nprobability of seeing a particular patient record, i.e., p(d1, . . . , dN). To train the language model, probability\ndistributions over sequences are factorized into a sequence of predictions where only a single element of the\nsequence is predicted at a time. Using EHR data, this corresponds to predicting the next day in a patient\nrecord given the previous days, i.e., p(di|d1, ...di−1). Each di is composed of a set of codes as opposed to a\nsingle token. Thus, the problem is a multi-label prediction problem. There is a large body of literature on\nmulti-label problems []. For our experiments, we chose the simplest possible technique of transforming this\nproblem into a binary classiﬁcation problem through the binary relevance method [41], modeled as\np(di|d1, ...di−1) =\n∏\nc∈C\nI(c ∈di)p(c|d1, ...di−1) + I(c ̸∈di)(1 −p(c|d1, ...di−1)).\nOne issue with this approach is that this factorization will only be able to correctly model p(di|d1, . . . , di−1)\nwhen the probability of each code is independent conditioned on the history d1, . . . , di−1. That assumption\nis likely to be violated in EHR data, where there are strong correlations among certain codes that co-occur\nwithin an encounter. Nonetheless, we found that this approach seems to work well in practice.\nLike prior work, we used a GRU-based neural network as our language model [22]. Figure 3 shows an\noverview of the model architecture. The main modiﬁcation we made is that we introduced a linear layer after\nthe GRU layer in order to extract patient representations of a lower dimension from the internal GRU state.\nThe ﬁrst layer of our network was an embedding bag layer which took as input the sets of codes for each day\nand output the mean representation for that day using an embedding matrix W with a tuned embedding\nsize. As in Rajkomar et al [9], day representations were then concatenated with a ﬁve element vector for\neach day that contained the age at a particular day, the log transform of the age at that particular day, the\ntime delta from the previous day, the log transform of that time delta and a binary indicator of whether\nor not that day was the ﬁrst day of the sequence. All variables were normalized to a mean of zero and a\nstandard deviation of one. The purpose of adding these variables was to provide some time information to\nthe neural network due to the fact that there were diﬀerent amounts of real time passing between each day.\nThe day representations plus demographic data were then fed into a single layer GRU with a set number of\nhidden units. A patient representation at each time step was computed by passing the output of the GRU\nthrough a GELU [42] activation function and a linear layer with output size equal to the embedding size.\nThis patient representation was then used to compute the probability of each code in the code set in\norder to satisfy the language modeling objective. While it is ideal to compute these probabilities as a sigmoid\n7\ntransformation of the dot product between the patient representation and the code representation, naively\nperforming this computation is problematic due to both memory and computational needs caused by the\nlarge number of codes. Prior work [43] has shown that it is possible to use a hierarchical decomposition\nfor computing large scale softmax operations when there is pre-existing hierarchical structure. We applied\na hierarchical decomposition to our code probability space using the ontologies in UMLS. We then applied\nthat algorithm on a code matrix and a patient representation in order to obtain the probability of each\ncode. Following prior work on text language models, we used the same embedding matrix for both this\ncomputation and when computing our mean embeddings on the input side. After the language model was\ntrained, we extracted patient representations by taking the output of the linear layer prior to the hierarchical\nsigmoid layer. More sophisticated approaches involving layer-wise ﬁne-tuning were investigated, but did not\nappear to perform better than this simpler approach. Thus, we use the trained language model as a ﬁxed\nfeature extractor.\nClinical Language Model-based Representation (CLMBR)\nPatient Embeddings\nLanguage \nModel\nObjective\nCLMBR\nGRU\nMean Code\nEmbedding\nTime Delta\nFeatures\nDay t Codes\n|codes| × dimW ∈ \nCode 1\nCode 2\nCode 3\nCode 4\n.\n.\n.\n.\n.\nCode N\n0\n0\n1\n1\n.\n.\n.\n.\n.\n1\nMean \nPooling\nEmbedding Bag Day  t Embedding\no\no\no\no\n1\n1\no\no\no\n1\no\n1\n0\no\no\n1\no\no\n1\n0\n1\n1\no\no\no\n1\n1\no\no\no\no\no\n1\n1\no\nDay EmbeddingsPatient Timeline\n1 2 3 4 tDay 1 2 3 4 tDay\nt1 2 3 4Day\nEmbed\nCodes\nFigure 3: The ﬁgure shows how patient representations were constructed using the CLMBR language model.\nRepresentations for individual patients were created by extracting ﬁxed length vectors generated by the\nlinear layer after the GRU.\nWe implemented this model in PyTorch and optimized it using OpenAI’s version of the Adam algorithm\n[44] using L2 regularization. We applied dropout between the input embedding and the GRU and between\nthe GRU and the linear layer. Two models were evaluated: a small model with an embedding size of 400,\nand a larger model with an embedding size of 800. For each model, the learning rate, dropout rate, L2\nregularization strength, and hidden layer size were tuned using grid search. We trained each model for 50\nepochs with linear learning rate decay to zero with a two epoch linear learning rate warmup. A batch size\nof 2,000 days (using as many patients as possible in a greedy manner) was used. Xavier initialization was\nused for the code representations and the default PyTorch initialization was used for the other parameters.\nThe full grid of evaluated hyperparameters is speciﬁed in Appendix A. The optimal hyperparameters used\nin the following experiments can be found in Appendix B.\nWe also implemented and evaluated the language modeling objective used in DoctorAI [22] to study\nthe eﬀect of the choice of the language modeling objective. Following DoctorAI, we ignored the multi-label\nnature of the problem and instead used a softmax loss function. In addition, we mirrored DoctorAI by\nsimplifying our target space by only predicting high level diagnosis (3 token ICD10) and medication codes\n8\n(leaf ATC) as opposed to the full code space considered in our main language model. As in DoctorAI,\nwe used a simpliﬁed ﬂat softmax, as the reduced code space renders techniques like hierarchical softmax\nunnecessary. In the experiments comparing the two language model variants, we used a ﬁxed embedding\nsize of 800.\n2.4 Tuning and Evaluation\nWe carried out our experiments in three stages — language model tuning, clinical prediction model tuning,\nand evaluation of the clinical prediction models on held out test data — carefully designed to prevent\nleakage from the test set into the training and development datasets used to develop the language and\nclinical prediction models. Language models such as DoctorAI and CLMBR have many hyperparameters\nthat must be tuned. We tuned them by ﬁtting language models with diﬀerent hyperparameter settings to\nthe training set and selecting optimal settings based on the language model loss on the development set.\nThe ﬁnal language models were re-trained on the combined training and development set data using these\noptimal settings. The hyperparameters for the clinical prediction models were tuned in a corresponding\nmanner through training on the training set and selecting the optimal settings based on AUROC on the\ndevelopment set. We performed a separate hyperparameter search for each clinical prediction model in order\nto fairly measure performance. We then re-trained the clinical prediction models using the optimal settings\non the combined training and development set data. Finally, the re-trained clinical prediction models were\nevaluated on the held out test set. We calculated uncertainty estimates of the resulting performance by\ntaking 1,001 bootstrap samples of the test set.\n3 Results\nOur work primarily aims to measure the extent that clinical prediction models that leverage language model\nbased representations outperform those that rely on engineered features or simpler representation learning\ntechniques. In addition, we explore whether the magnitude of this eﬀect changes as a function of both the\namount of labeled training data available and the class of supervised learning algorithm used for the clinical\nprediction model. We also explore the importance of our more complicated clinical language model compared\nto prior clinical language modeling work.\n3.1 Diﬀerence in Performance of Clinical Prediction Models with Diﬀerent Rep-\nresentations\nWe ﬁrst evaluated the diﬀerence in performance of clinical prediction models trained using alternative rep-\nresentations when a large amount of training data was available. Each outcome in the pool of ﬁve clinical\noutcomes was chosen on the basis of having a large number of labels to both aid this analysis and to reduce\nthe variance of our performance estimates. Table 3 shows the AUROC on the test set for each representation\ncategory when trained with all of the data, with the best performing representation presented in bold font.\nAll performance metrics were calculated pair-wise, relative to the counts representation, in order to reduce\nvariance and better quantify diﬀerences between representations. We report standard deviations estimated\nby 1,001 bootstrap samples of the test set. Appendix E lists the best hyperparameter settings for each\noutcome and representation combination. We found that models trained using CLMBR representations\nperformed best for all ﬁve outcomes, although the improvement over alternatives was minimal for some of\nthe outcomes. Surprisingly, models trained using CLMBR representations were uniformly superior to the\nend-to-end GRU models. Word2vec and LSI representations on the other hand were usually worse than\nother representations.\n3.2 Eﬀect of Training Set Size\nWe also performed experiments in which we artiﬁcially reduced the dataset set sizes used for training clinical\nprediction models through subsampling. These experiments explored the hypothesis that learned represen-\ntations are especially eﬀective when there is only a limited amount of data available for training a clinical\n9\nprediction model. Figure 4 shows the AUROC of the clinical prediction models trained using diﬀerent rep-\nresentations as dataset size changes. We calculated the mean AUROC across ten subsamples of the training\nset and show the 95% t-distribution conﬁdence interval for the mean. Across all representation choices,\nwe found that the AUROC decreased as the data set decreased in size. However, as expected, models\ntrained using CLMBR representations fared best. The other learned representation classes, word2vec and\nLSI, seemed to provide some beneﬁt relative to count based representations at smaller sample sizes (with\nword2vec outperforming LSI).\nTable 3: Diﬀerence in AUROC of clinical prediction models trained on diﬀerent representations\nRelative Compared To Counts Baseline\nOutcome Name Counts Word2Vec LSI CLMBR End-to-end GRU\nInpatient Mortality 0 .834 −0.010 ±0.006 −0.046 ±0.007 0.018 ± 0.006 −0.030 ±0.008\nLong Admission 0 .783 −0.020 ±0.002 −0.055 ±0.002 0.009 ± 0.002 −0.013 ±0.002\nICU Transfer 0 .792 −0.041 ±0.006 −0.086 ±0.007 0.045 ± 0.005 0.039 ±0.006\n30-day Readmission 0 .809 −0.018 ±0.002 −0.051 ±0.003 0.005 ± 0.002 −0.001 ±0.002\nAbnormal HbA1c 0 .700 0 .015 ±0.015 −0.011 ±0.016 0.056 ± 0.013 −0.019 ±0.017\n10\n100 200 400 800 1600 3200\nNumber of labels\n0.6\n0.8\n1.0AUROC\nInpatient Mortality\n100 200 400 800 1600 3200\nNumber of labels\n0.6\n0.8\n1.0AUROC\nLong Admission\n100 200 400 800 1600 3200\nNumber of labels\n0.6\n0.8\n1.0AUROC\nICU Transfer\n100 200 400 800 1600 3200\nNumber of labels\n0.6\n0.8\n1.0AUROC\n30-day Readmission\n100 200 400 800 1600 3200\nNumber of labels\n0.6\n0.8\n1.0AUROC\nAbnormal HbA1c\nCounts Word2Vec LSI CLMBR CLMBR With All Data\nFigure 4: AUROC (y-axis) for ﬁve clinical prediction models. For each clinical prediction model, the AUROC\nof models trained using a given representation type is plotted as a function of training set size (x-axis). Note\nthat CLMBR (red) matched or outperformed all other approaches. In each plot, the dashed line shows\nperformance of clinical prediction models trained using the CLMBR representation with the full dataset and\nrepresents best case performance. We found that using the CLMBR representation increased the AUROC\nof the clinical prediction model for all outcomes and training set sizes, but the magnitude of the beneﬁt was\nlarger at smaller sample sizes and diminished at larger sample sizes.\n3.3 Eﬀect of the Type of the Prediction Model as a Function of the Represen-\ntation\nWe evaluated L2 regularized logistic regression and gradient boosted tree for training the clinical prediction\nmodels, to identify which of these two performed best with which types of representations. This analysis\nis important because simpler models such as logistic regression are easier and faster to train than gradient\nboosted trees. Figure 5 shows the relative performance of these model types for all ﬁve outcomes, over\nmultiple sample sizes for count based representations and CLMBR. As before, we computed the mean\nAUROC from ten subsamples of the training set and report the 95% t conﬁdence interval for that mean.\n11\nWhen using count based representations, there was a consistent performance beneﬁt in using gradient boosted\ntree models versus logistic regression, with gaps ranging from 4% for 30 day readmission to 20.7% for inpatient\nmortality. With CLMBR representations, the best performing clinical prediction model type was a logistic\nregression model. More complex gradient boosted tree models oﬀered no improvement even with large sample\nsizes, and often hurt performance at smaller sample sizes.\n0.5\n0.7\n0.9AUROC\nInpatient Mortality\nCounts\n CLMBR\n0.5\n0.7\n0.9AUROC\nLong Admission\n0.5\n0.7\n0.9AUROC\nICU Transfer\n0.5\n0.7\n0.9AUROC\n30-day Readmission\n100 200 400 800 1600 3200\nNumber of labels\n0.5\n0.7\n0.9AUROC\nAbnormal HbA1c\n100 200 400 800 1600 3200\nNumber of labels\nLogistic Regression\nLightGBM\nLogistic Regression With All Data\nLightGBM With All Data\nFigure 5: AUROC for logistic regression and gradient boosted tree clinical prediction models trained using\ncount based and CLMBR representations. For count based representations, we observed signiﬁcant beneﬁts\nfrom using gradient boosted trees versus L2 regularized logistic regression. In contrast, we found that with\nCLMBR logistic regression outperformed gradient boosting models. The dashed lines show performance of\nthe clinical prediction models trained on the full datasets using CLMBR representations and represents best\ncase performance given available data.\n3.4 Performance Diﬀerence Between CLMBR’s Language Model and DoctorAI\nCLMBR’s language model and DoctorAI [22] are both clinical language models. In order to measure the\nbeneﬁts of CLMBR’s more complicated language model we implemented the DoctorAI language model as\ndescribed in the methods and used it to construct representations. We then trained clinical prediction models\nusing representations from the two diﬀerent language models. Table 4 shows the AUROCs of the prediction\nmodels along with the diﬀerence between the two and the standard deviation of that diﬀerence computed\nfrom bootstrap samples. We observed a consistent improvement in performance across all outcomes with\nthe complicated language modeling objective, with a larger improvement for two of the ﬁve outcomes (ICU\ntransfer, abnormal HbA1c).\n12\nTable 4: AUROC of prediction models with diﬀerent language modeling objectives\nOutcome Name CLMBR Language Model DoctorAI Language Model Diﬀerence\nInpatient Mortality 0 .852 0 .844 −0.008 ±0.003\nLong Admission 0 .792 0 .788 −0.004 ±0.001\nICU Transfer 0 .837 0 .813 −0.024 ±0.002\n30-day Readmission 0 .814 0 .807 −0.007 ±0.002\nAbnormal HbA1c 0 .756 0 .742 −0.014 ±0.008\n4 Discussion\nPrior work employing deep neural networks to train prediction models for clinical outcomes using EHR data\nhas focused mostly on end-to-end prediction models and used large datasets [9]. There has been much less\nwork on learning general purpose representations using the entire EHR dataset that can then be re-used to\ntrain better prediction models. We have shown that language model based representations (such as DoctorAI\nand CLMBR), which capture the sequential nature of EHR data, are signiﬁcantly better than a wide array of\nalternative representations for training clinical prediction models across a range of training set sizes. We also\ndetermined that the choice of the language model objective does matter, with the more expansive language\nmodel, CLMBR, providing better representations. The beneﬁts of a language model based representation\nsuch as CLMBR are largest with small sample sizes (with an average improvement of 19% in AUROC), but\nalso hold with quite large sample sizes, including when training clinical prediction models with over 200,000\nsamples.\nSomewhat surprisingly, language model based representations also proved superior to end-to-end trained\nneural nets in the large sample regime. In contrast, other learned representations proved to be of little value\nrelative to simpler count based representations when enough data was available. Finally, we found that\nwith enough training data, clinical prediction models using simple count based representations can perform\nvery well, being only 3.5% worse than models trained using CLMBR representations. However, with count\nbased representations, it is important to use a model type with suﬃcient expressive power. Note that\ngradient boosted trees performed much better than L2 regularized logistic regression with the simple counts\nbased representations. This observation may explain some of the discrepancies in reported performance gaps\nbetween deep neural network models and baselines in prior work [9, 14, 45, 23].\nCurrently, language model based representations come with signiﬁcant upfront computation costs to\ntrain and tune [16]. However, this process is a one time cost per institution and can be amortized over\nmany clinical outcomes. Moreover, recent language model work has demonstrated considerable reduction in\ntraining costs [46].\nOur conclusions have important limitations. First, our ﬁndings are limited to the ﬁve clinical outcomes\nused in this work and ﬁndings may not generalize to all other possible EHR-based model types. Second,\nthis work does not explore how well CLMBR representations learned from data from one institution will\ngeneralize to other sites. In addition, we can expect the volume of EHR data available for training clinical\nprediction models to increase steadily, which might erase the gains from using more complex representation\nregimes. In particular, we note that end-to-end neural net models may regain the advantage when more\ntraining data is available.\n5 Conclusion\nIn this work we developed and evaluated language model based representations for EHR data and found\nthat the resulting patient representations were better than three other representation schemes as well as\nend-to-end neural network models for training prediction models for a variety of clinical outcomes at varying\ndataset sizes. The improvement in accuracy was especially signiﬁcant at small sample sizes, with an average\nimprovement of 19% in AUROC at the smallest sample sizes. We also found that logistic regression models\nworked particularly well with language model based representations, potentially enabling faster and cheaper\ndevelopment of models for predicting clinical outcomes. These results suggest that language model based\n13\nrepresentations are a useful technique for developing better clinical prediction models using EHR data.\nAcknowledgments\nThis work was funded under NLM R01-LM011369-05. GPU resources were provided by Nero, a secure data\nscience platform made possible by the Stanford School of Medicine Research Oﬃce and Stanford Research\nComputing Center. We would also like to thank Erin Craig, Agata Foryciarz and Sehj Kashyap for providing\nuseful comments on the paper.\nAuthor Information\nAﬃliations\nStanford University, Stanford, CA, USA:\nEthan Steinberg, Ken Jung, Jason A. Fries, Conor K. Corbin, Stephen R. Pfohl, Nigam H. Shah\nContributions\nE.S. designed and conducted the primary experiments and created the initial drafts of the manuscript. K.J.\nhelped work on further drafts and helped contribute code for some experiments. C.C. and S.P. contributed\ncode for some of the experiments. J.F. and N.S. provided helpful discussion and helped design some of the\nﬁgures. All authors contributed to revising the paper.\nCorresponding Author\nCorrespondence to Ethan Steinberg (ethanid@stanford.edu).\nCompeting Interests\nThe authors have no conﬂicts of interest to declare.\nReferences\n[1] S. Shilo, H. Rossman and E. Segal, Axes of a revolution: challenges and promises of big data in\nhealthcare, Nature Medicine 26, 29 (2020).\n[2] B. Norgeot, B. S. Glicksberg and A. J. Butte, A call for deep-learning healthcare, Nature Medicine 25,\n14 (January 2019).\n[3] J. Wiens, S. Saria, M. Sendak, M. Ghassemi, V. X. Liu, F. Doshi-Velez, K. Jung, K. Heller, D. Kale,\nM. Saeed, P. N. Ossorio, S. Thadaney-Israni and A. Goldenberg, Do no harm: a roadmap for responsible\nmachine learning for health care, Nature Medicine 25, 1337 (August 2019).\n[4] S. K. M. G. M. N. K. C. W. R. Mark P. Sendak, Joshua DArcy and S. Balu, A path for translation of\nmachine learning products into healthcare delivery, EMJ Innovations (January 2020).\n[5] A. Avati, K. Jung, S. Harman, L. Downing, A. Ng and N. H. Shah, Improving palliative care with\ndeep learning, in 2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) ,\nNov 2017.\n[6] M. B. Dhudasia, S. Mukhopadhyay and K. M. Puopolo, Implementation of the sepsis risk calculator at\nan academic birth hospital, Hospital Pediatrics 8, 243 (2018).\n14\n[7] S. Tamang, A. Milstein, H. T. Sørensen, L. Pedersen, L. Mackey, J.-R. Betterton, L. Janson and\nN. Shah, Predicting patient ‘cost blooms’ in denmark: a longitudinal population-based study, BMJ\nOpen 7 (2017).\n[8] P. Cronin, J. Greenwald, G. C. Crevensten, H. Chueh and A. Zai, Development and implementation\nof a real-time 30-day readmission predictive model, AMIA Annual Symposium proceedings / AMIA\nSymposium. AMIA Symposium 2014, 424 (11 2014).\n[9] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu, X. Liu, J. Marcus, M. Sun,\nP. Sundberg, H. Yee, K. Zhang, Y. Zhang, G. Flores, G. E. Duggan, J. Irvine, Q. Le, K. Litsch,\nA. Mossin, J. Tansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig, S. L. Volchenboum, K. Chou,\nM. Pearson, S. Madabushi, N. H. Shah, A. J. Butte, M. D. Howell, C. Cui, G. S. Corrado and J. Dean,\nScalable and accurate deep learning with electronic health records, npj Digital Medicine 1, p. 18 (2018).\n[10] J. M. Banda, A. Sarraju, F. Abbasi, J. Parizo, M. Pariani, H. Ison, E. Briskin, H. Wand, S. Dubois,\nK. Jung, S. A. Myers, D. J. Rader, J. B. Leader, M. F. Murray, K. D. Myers, K. Wilemon, N. H.\nShah and J. W. Knowles, Finding missed cases of familial hypercholesterolemia in health systems using\nmachine learning, npj Digital Medicine 2, p. 23 (2019).\n[11] S. S. Paulson, B. A. Dummett, J. Green, E. Scruth, V. Reyes and G. J. Escobar, What do we do after\nthe pilot is done? implementation of a hospital early warning system at scale, The Joint Commission\nJournal on Quality and Patient Safety (2020).\n[12] D. W. Shimabukuro, C. W. Barton, M. D. Feldman, S. J. Mataraso and R. Das, Eﬀect of a machine\nlearning-based severe sepsis prediction algorithm on patient survival and hospital length of stay: a\nrandomised clinical trial, BMJ Open Respiratory Research 4 (2017).\n[13] B. A. Goldstein, A. M. Navar, M. J. Pencina and J. P. A. Ioannidis, Opportunities and challenges in\ndeveloping risk prediction models with electronic health records data: a systematic review, Journal of\nthe American Medical Informatics Association 24, 198 (May 2016).\n[14] D. Chen, S. Liu, P. Kingsbury, S. Sohn, C. B. Sorlie, E. B. Haberman, J. M. Naessens, D. W. Larson\nand H. Liu, Deep learning and alternative learning strategies for retrospective real-world clinical data,\nNature Digital Medicine 2 (2019).\n[15] J. Howard and S. Ruder, Universal language model ﬁne-tuning for text classiﬁcation, in Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\n(Association for Computational Linguistics, Melbourne, Australia, July 2018).\n[16] J. Devlin, M. Chang, K. Lee and K. Toutanova, Bert: Pre-training of deep bidirectional transformers\nfor language understanding, in NAACL-HLT, 2018.\n[17] J. Wiens, J. Guttag and E. Horvitz, A study in transfer learning: leveraging data from multiple hospitals\nto enhance hospital-speciﬁc predictions, Journal of the American Medical Informatics Association 21,\n699 (July 2014).\n[18] R. Miotto, L. Li, B. A. Kidd and J. T. Dudley, Deep Patient: An Unsupervised Representation to\nPredict the Future of Patients from the Electronic Health Records, Sci Rep 6, p. 26094 (05 2016).\n[19] E. Choi, M. T. Bahadori, E. Searles, C. Coﬀey, M. Thompson, J. Bost, J. T and J. Sun, Multi-layer\nrepresentation learning for medical concepts, in Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , KDD ’16 (ACM, New York, NY, USA, 2016).\n[20] Y. Choi, C. Y. Chiu and D. Sontag, Learning low-dimensional representations of medical concepts,\nAMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational\nScience 2016, 41 (Jul 2016), 27570647[pmid].\n[21] E. Choi, A. Schuetz, W. F. Stewart and J. Sun, Medical concept representation learning from electronic\nhealth records and its application on heart failure prediction, CoRR abs/1602.03686 (2016).\n15\n[22] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart and J. Sun, Doctor ai: Predicting clinical events\nvia recurrent neural networks, in Proceedings of the 1st Machine Learning for Healthcare Conference ,\neds. F. Doshi-Velez, J. Fackler, D. Kale, B. Wallace and J. Wiens, Proceedings of Machine Learning\nResearch, Vol. 56 (PMLR, Children’s Hospital LA, Los Angeles, CA, USA, 18–19 Aug 2016).\n[23] E. Choi, M. T. Bahadori, L. Song, W. F. Stewart and J. Sun, Gram: Graph-based attention model for\nhealthcare representation learning, in Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , KDD ’17 (ACM, New York, NY, USA, 2017).\n[24] E. Choi, A. Schuetz, W. F. Stewart and J. Sun, Using recurrent neural network models for early detection\nof heart failure onset, J Am Med Inform Assoc 2, 361 (2016).\n[25] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart and J. Sun, RETAIN: interpretable predictive\nmodel in healthcare using reverse time attention mechanism, CoRR abs/1608.05745 (2016).\n[26] Y. Cheng, F. Wang, P. Zhang and J. Hu, Risk prediction with electronic health records: A deep learning\napproach, in Proceedings of the 2016 SIAM International Conference on Data Mining , 2016.\n[27] T. Pham, T. Tran, D. Phung and S. Venkatesh, Deepcare: A deep dynamic memory model for predictive\nmedicine, in Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining , 2016.\n[28] P. Nguyen, T. Tran, N. Wickramasinghe and S. Venkatesh, Deepr: a convolutional net for medical\nrecords, IEEE journal of biomedical and health informatics 21, 22 (2016).\n[29] J. Zhang, K. Kowsari, J. H. Harrison, J. M. Lobo and L. E. Barnes, Patient2vec: A personalized\ninterpretable deep representation of the longitudinal electronic health record, IEEE Access 6, 65333\n(2018).\n[30] J. Pennington, R. Socher and C. D. Manning, Glove: Global vectors for word representation, inEmpirical\nMethods in Natural Language Processing (EMNLP) , 2014.\n[31] T. Mikolov, I. Sutskever, K. Chen, G. Corrado and J. Dean, Distributed representations of words\nand phrases and their compositionality, in Proceedings of the 26th International Conference on Neural\nInformation Processing Systems - Volume 2 , NIPS’13 (Curran Associates Inc., USA, 2013).\n[32] D. Shen, G. Wang, W. Wang, M. R. Min, Q. Su, Y. Zhang, C. Li, R. Henao and L. Carin, Baseline needs\nmore love: On simple word-embedding-based models and associated pooling mechanisms, in Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\n(Association for Computational Linguistics, Melbourne, Australia, July 2018).\n[33] Y. Kim, Convolutional neural networks for sentence classiﬁcation, in Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP) , (Association for Computational\nLinguistics, Doha, Qatar, October 2014).\n[34] M. W. Berry, S. T. Dumais and G. W. O’Brien, Using linear algebra for intelligent information retrieval,\nSIAM Rev. 37, 573 (December 1995).\n[35] E. Choi, C. Xiao, W. F. Stewart and J. Sun, Mime: Multilevel medical embedding of electronic health\nrecords for predictive healthcare, CoRR abs/1810.09593 (2018).\n[36] E. Sherman, H. Gurm, U. Balis, S. Owens and J. Wiens, Leveraging clinical time-series data for pre-\ndiction: a cautionary tale, in AMIA Annual Symposium Proceedings , (American Medical Informatics\nAssociation, 2017).\n[37] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot and\nE. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research 12,\n2825 (2011).\n16\n[38] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and T.-Y. Liu, Lightgbm: A highly\neﬃcient gradient boosting decision tree, in Proceedings of the 31st International Conference on Neural\nInformation Processing Systems , NIPS’17 (Curran Associates Inc., USA, 2017).\n[39] O. Bodenreider, The uniﬁed medical language system (umls): integrating biomedical terminology, Nu-\ncleic Acids Research 32, D267 (2004).\n[40] R. ˇReh˚ uˇ rek and P. Sojka, Software Framework for Topic Modelling with Large Corpora, inProceedings\nof the LREC 2010 Workshop on New Challenges for NLP Frameworks , (ELRA, Valletta, Malta, May\n2010).\n[41] G. Varando, C. Bielza and P. Larra˜ naga, Expressive power of binary relevance and chain classiﬁers\nbased on bayesian networks for multi-label classiﬁcation, in Probabilistic Graphical Models, eds. L. C.\nvan der Gaag and A. J. Feelders (Springer International Publishing, Cham, 2014).\n[42] D. Hendrycks and K. Gimpel, Bridging nonlinearities and stochastic regularizers with gaussian error\nlinear units, CoRR abs/1606.08415 (2016).\n[43] F. Morin and Y. Bengio, Hierarchical probabilistic neural network language model, in AISTATS, 2005.\n[44] A. Radford, Improving language understanding by generative pre-training2018.\n[45] R. Miotto, L. Li, B. A. Kidd and J. T. Dudley, Deep patient: an unsupervised representation to predict\nthe future of patients from the electronic health records, Scientiﬁc reports 6, p. 26094 (2016).\n[46] K. Clark, M.-T. Luong, Q. V. Le and C. D. Manning, Electra: Pre-training text encoders as discrimi-\nnators rather than generators, in International Conference on Learning Representations , 2020.\n17\nA Language Model Hyperparameter Grid\nTable 5: Language Model Hyperparameters\nHyperparameter Name Hyperparameter Values\nEmbedding Size [400, 800]\nGRU Hidden Size [400, 800, 1600]\nLR [10 −2, 10−3, 10−4, 10−5]\nL2 [0.1, 0.01, 0.001]\nDropout [0, 0.1, 0.2]\nB Best Language Model Hyperparameters\nTable 6: Best Language Model Hyperparameters\nHyperparameter Name Size 400 Model Value Size 800 Model Value\nEmbedding Size 400 800\nGRU Hidden Size 800 1600\nLR 10 −3 10−3\nL2 0.01 0.1\nDropout 0.1 0.1\nEpochs 20 40\nC End-to-end GRU Model Hyperparameter Grid\nTable 7: End-to-end GRU Model Model Hyperparameters\nHyperparameter Name Hyperparameter Values\nEmbedding Size [100, 200, 400]\nGRU Hidden Size [100, 200, 400]\nLR [10 −2, 10−3, 10−4, 10−5]\nL2 [0.1, 0.01, 0.001]\nDropout [0, 0.1, 0.2]\nD Best End-to-end GRU Model Hyperparameters\nTable 8: Inpatient Mortality GRU Best Hyperparameters\nHyperparameter Name Value\nEmbedding Size 100\nGRU Hidden Size 400\nLR 10 −2\nL2 0.1\nDropout 0.1\nEpochs 21\n18\nTable 9: Long Admission GRU Best Hyperparameters\nHyperparameter Name Value\nEmbedding Size 400\nGRU Hidden Size 100\nLR 10 −2\nL2 0.1\nDropout 0.1\nEpochs 28\nTable 10: ICU Transfer GRU Best Hyperparameters\nHyperparameter Name Value\nEmbedding Size 400\nGRU Hidden Size 400\nLR 10 −3\nL2 0.001\nDropout 0\nEpochs 0\nTable 11: 30-day Readmission GRU Best Hyperparameters\nHyperparameter Name Value\nEmbedding Size 400\nGRU Hidden Size 100\nLR 10 −2\nL2 0.1\nDropout 0\nEpochs 24\nTable 12: Abnormal HbA1c GRU Best Hyperparameters\nHyperparameter Name Value\nEmbedding Size 400\nGRU Hidden Size 200\nLR 10 −3\nL2 0.01\nDropout 0.1\nEpochs 1\n19\nE Best Prediction Model/Representation Hyperparameters On\nAll Data\nTable 13: Inpatient Mortality Best Hyperparameters\nRepresentation Name Representation Hyperparameters Best Model Type Best Hyperparameters\nCounts with ontology expansion LightGBM num leaves: 100\nnum boost round: 317\nlearning rate: 0.02\nWord2Vec concat max mean min Logistic C: 0.01\nLSI size: 800 LightGBM num leaves: 10\nnum boost round: 250\nlearning rate: 0.02\nCLMBR size: 800 Logistic C: 0.001\nTable 14: Long Admission Best Hyperparameters\nRepresentation Name Representation Hyperparameters Best Model Type Best Hyperparameters\nCounts with time bins LightGBM num leaves: 100\nnum boost round: 292\nlearning rate: 0.02\nWord2Vec concat max mean min LightGBM num leaves: 100\nnum boost round: 360\nlearning rate: 0.02\nLSI size: 800 LightGBM num leaves: 100\nnum boost round: 494\nlearning rate: 0.02\nCLMBR size: 800 LightGBM num leaves: 100\nnum boost round: 397\nlearning rate: 0.02\nTable 15: ICU Transfer Best Hyperparameters\nRepresentation Name Representation Hyperparameters Best Model Type Best Hyperparameters\nCounts with time bins LightGBM num leaves: 100\nnum boost round: 43\nlearning rate: 0.02\nWord2Vec with ontology expansion,concat max mean min Logistic C: 1.0\nLSI size: 800 Logistic C: 1000000.0\nCLMBR size: 800 Logistic C: 1e-05\n20\nTable 16: 30-day Readmission Best Hyperparameters\nRepresentation Name Representation Hyperparameters Best Model Type Best Hyperparameters\nCounts with time bins LightGBM num leaves: 100\nnum boost round: 159\nlearning rate: 0.02\nWord2Vec concat max mean min LightGBM num leaves: 100\nnum boost round: 215\nlearning rate: 0.02\nLSI size: 400 LightGBM num leaves: 100\nnum boost round: 188\nlearning rate: 0.02\nCLBMR size: 800 LightGBM num leaves: 100\nnum boost round: 282\nlearning rate: 0.02\nTable 17: Abnormal HbA1c Best Hyperparameters\nRepresentation Name Representation Hyperparameters Best Model Type Best Hyperparameters\nCounts with ontology expansion LightGBM num leaves: 100\nnum boost round: 73\nlearning rate: 0.1\nWord2Vec concat max mean min LightGBM num leaves: 25\nnum boost round: 21\nlearning rate: 0.1\nLSI size: 800 LightGBM num leaves: 10\nnum boost round: 63\nlearning rate: 0.1\nCLMBR size: 800 Logistic C: 0.01\n21",
  "topic": "Health records",
  "concepts": [
    {
      "name": "Health records",
      "score": 0.7285423874855042
    },
    {
      "name": "Computer science",
      "score": 0.6838495135307312
    },
    {
      "name": "Representation (politics)",
      "score": 0.6623849868774414
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6252883672714233
    },
    {
      "name": "Machine learning",
      "score": 0.6034882664680481
    },
    {
      "name": "Electronic health record",
      "score": 0.5679826736450195
    },
    {
      "name": "Task (project management)",
      "score": 0.5582841634750366
    },
    {
      "name": "Process (computing)",
      "score": 0.522616446018219
    },
    {
      "name": "Language model",
      "score": 0.4662337005138397
    },
    {
      "name": "Population",
      "score": 0.46079736948013306
    },
    {
      "name": "Natural language processing",
      "score": 0.45719897747039795
    },
    {
      "name": "Health care",
      "score": 0.23848125338554382
    },
    {
      "name": "Medicine",
      "score": 0.1983480155467987
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Environmental health",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}